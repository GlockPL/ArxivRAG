{"title": "A Survey of Explainable Artificial Intelligence (XAI) in Financial Time Series Forecasting", "authors": ["PIERRE-DANIEL ARSENAULT", "SHENGRUI WANG", "JEAN-MARC PATENAUDE"], "abstract": "Artificial Intelligence (AI) models have reached a very significant level of accuracy. While their superior performance offers considerable benefits, their inherent complexity often decreases human trust, which slows their application in high-risk decision-making domains, such as finance. The field of eXplainable AI (XAI) seeks to bridge this gap, aiming to make AI models more understandable. This survey, focusing on published work from the past five years, categorizes XAI approaches that predict financial time series. In this paper, explainability and interpretability are distinguished, emphasizing the need to treat these concepts separately as they are not applied the same way in practice. Through clear definitions, a rigorous taxonomy of XAI approaches, a complementary characterization, and examples of XAI's application in the finance industry, this paper provides a comprehensive view of XAI's current role in finance. It can also serve as a guide for selecting the most appropriate XAI approach for future applications.", "sections": [{"title": "1 INTRODUCTION", "content": "In the last decade, Artificial Intelligence (AI) has made remarkable strides in terms of accuracy, transforming sectors such as business, media, healthcare, education, finance, scientific research, judicial, etc. While these advances offer unprecedented capabilities, they also introduce challenges, particularly in high-stakes, risk-sensitive domains like finance and healthcare. Despite their accuracy, Al models often suffer from a lack of user trust due to their black-box nature. Ethical and legal frameworks have evolved to reflect these concerns; reports such as [68] and [82] highlight the necessity for Al systems to be understandable in decision-making contexts. As pointed out by existing works [10, 63, 68], the poor interpretability of deep learning models can significantly increase investment risks, raising critical limitations for practical applications.\nAddressing this problem is the emerging field of Explainable Artificial Intelligence (XAI), designed to produce AI models that are not only accurate but also understandable to human users. XAI offers two primary avenues for enhancing understanding: interpretable models, which are inherently understandable, and explainable models, which are models that require additional methods for explanation. However, there exists significant terminological confusion in the literature; the terms interpretable and explainable are often used interchangeably, leading to misunderstandings about the nature of the model being presented. Understanding the differences between these terms is critical for several reasons. Firstly, the pathways to achieve interpretability and explainability are different, necessitating distinct methodologies and evaluations. Secondly, accurate terminology helps in the selection of appropriate models based on the unique regulatory and ethical demands of financial applications. Thirdly, clear definitions improve interdisciplinary communication among data scientists, financial experts, and policymakers, facilitating more informed decisions. Fourthly, distinguishing between interpretability and explainability informs the level of trust one can place in a model's decision-making process, impacting risk assessment and compliance with financial regulations. Furthermore, the claim that a model is XAI sometimes lacks substantiation, either by failing to show the interpretations that could be done on the model or by lack of the explanations that help understand the model.\nThe need for this survey is further amplified by the financial industry's accelerating adoption of AI for complex tasks such as credit risk assessments, fraud detection, algorithmic trading, and, notably, the prediction of financial time series. Although these technologies promise to enhance efficiency and accuracy, they also raise significant concerns around fairness, explainability, and regulatory compliance. The opaque nature of AI algorithms is not merely a technological challenge; it has far-reaching implications for public trust, ethical governance, and regulatory adherence. This paper aims to present the latest interpretable models and explainability methods used in the prediction of financial time series. By offering a detailed survey of both interpretable and explainable models, it aims to serve as a guide for two groups of readers: data scientists who want to design and develop XAI into time series prediction, whether financial or not, and financial professionals who want to incorporate XAI into their analytics environment for predicting financial time series. Indeed, the interpretable and explainable approaches surveyed in this paper for predicting financial time series could also be used to forecast many types of time series. Also, this survey provides financial insights that the XAI made possible. Both the insights and their approaches can be useful for financial professionals. Additionally, this survey provides a rigorous foundation for continued investigation, guides practitioners in choosing the most appropriate approach, and informs them about the merits and limitations of various approaches. In essence, this survey aspires to facilitate the responsible and transparent application of AI in finance, aligning cutting-edge machine learning techniques with the sector's strict ethical and regulatory standards. To the best of our knowledge, this is the first survey focusing specifically on XAI for predicting financial time series.\nIn this paper, we categorize XAI models of the last five years that have been used to predict financial time series or to predict financial time series movement. Firstly, some important concepts are defined in the Section 2. Secondly, the methodology of this survey and the diagram resulting from the taxonomy are presented (Sections 3 and 4). Thirdly, interpretable models and explainability methods are presented and classified according to its XAI principle (Sections 5 and 6). In Section 7, the same approaches are characterized according to different criteria, namely the explanation characteristics known by the community [6, 27, 49]. Then, applications of the XAI in industry are presented (Section 8). Finally, conclusions and the work that remains to be done are presented (Section 9)."}, {"title": "2 DEFINITIONS", "content": "The financial industry has increasingly adopted AI, since the inception of many statistical models in the 1980s, primarily for stock trading and risk management applications. As the domain matured, it embraced more advanced techniques, from expert systems and neural networks to the recent inclusion of deep learning models. These innovations have provided the industry with remarkable tools for market analysis, financial behaviour prediction, and risk management. However, the lack of transparency in these complex models often limits their practical application. When users cannot decipher a model's reasoning, even high-accuracy outputs might be deemed unreliable for critical financial decisions. This emphasizes the vital role of eXplainable Artificial Intelligence (XAI) as a solution.\nThe explainable Artificial Intelligence (XAI) is a branch of AI designed to be comprehensible to humans, thus promoting trust among users. XAI's main feature is its ability to render the inner workings of its model intelligible to human users. Its importance in finance manifests in various ways. For instance, when a financial institution deploys AI for decision-making regarding a client, as in the case of a loan application, there is a pressing need to clarify the rationale behind such decisions. This transparency is imperative not only for individual clients but also for communicating decision-making processes to investors, board members, or auditors. XAI facilitates this clear communication. Additionally, when institutions rely on Al for critical decisions, stakeholders require insights into how those decisions are derived, ensuring their sustained trust in the system. From a risk management perspective, an opaque Al model introduces potential hazards. Comprehending the model's operations enables institutions to identify and mitigate risks more effectively. In decision-making scenarios, XAI provides pertinent information, helping users gauge the trustworthiness of predictions. Thus, users can judiciously combine the model outputs with their domain expertise to arrive at well-informed decisions.\nTo qualify as XAI, an AI system must either be inherently interpretable (interpretable model) or use a distinct method that clarifies its decision-making processes (explainable model). An interpretable model, often referred to as a transparent model, is designed in such a way that users can intuitively understand its inner workings just by examining it. The individual components of the model are clear and understandable, facilitating a comprehensive grasp of its functionality. As an example, a linear model with a limited set of input features is considered interpretable. In such a model, each coefficient indicates the contribution of its associated feature. Thus, interpretability can be characterized as the inherent property of a model that, by its very design, makes its operations transparent at first sight. It is important to note that interpretability is not a simple binary trait. In [57], the authors elaborated on the nuances of interpretability, highlighting dimensions such as simulatability defined as the capacity of a model to be contemplated by a person, decomposability as the capacity of a model to be composed of interpretable parts, and algorithmic transparency as the level of interpretability of the model's training algorithm. A model is said interpretable if it is more understandable than an opaque model.\nAn explainable model is designed to provide explanations to users, aiding their comprehension of the model's operations. Often, an explainable model is essentially a black-box model enhanced with specific explainability methods, such as SHAP [60] or LIME [80], to shed light on its internal processing. A pure black-box model, also known as an opaque model, operates in a manner that is not immediately discernible, making its internal workings and decision-making processes inscrutable to users. There are two reasons that some models are designated as black-boxes: either due to their inherent design or because of imposed confidentiality. For instance, deep neural networks, by virtue of their intricate architecture involving many operations, are inherently challenging to understand and thus are classified as black-box models. On the other hand, even a rudimentary linear model with a handful of features can be perceived as a black-box if the owner discloses only the outputs while withholding the coefficients. Explainability can thus be defined as the capability of a model to elucidate its functioning, offering users insights into its operational mechanics.\nIt is essential to define a few basic concepts that are frequently used in the rest of the paper. The term model refers to a series of calculations, or mathematical equations, describing the workings of a computing system that takes typical inputs, such as numbers or words, and generates outputs, or predictions, for a given task. An explainability method is defined as a process that takes a model as input and provides explanations of this model as output. Consequently, an explainable model is one on which an explainability method has been applied. An approach encompasses a number of these computation steps. We can define it as a process that takes an input and generates an output. As [73] explored, there remains ambiguity surrounding the fundamental concepts of XAI. It becomes essential to identify the target audience for the explanations and to delineate the nature of these explanations. Two core concepts in the preceding definitions appear nebulous:\n(1) The term user is not explicitly defined. In the financial sector, this could refer to a data scientist, a business professional, an auditor, or even a consumer. Clearly, an explanation tailored for one may not be suitable for another. Consequently, it is crucial to specify the audience for whom the model should be rendered explainable or interpretable [7, 9, 73].\n(2) The notion of explanations lacks a formal characterization. These explanations must be context-sensitive [8, 9], addressing the distinct requirements of different users. For instance, in the realm of financial time series prediction, professionals often seek insights into feature importance to provide clarity on how input features influence the outcome. Another desired aspect of the explanation involves the decision rules, making the decision-making process clear and transparent. Time series interpretations also form a significant aspect of explanations, offering insights into the nature and patterns of input time series, and facilitating analyses based on extracted information and predictions. Furthermore, in the financial domain, visual representations such as user interfaces or dashboards are highly valued. Such visual aids not only present explanations but also offer supplementary information, fostering a more intuitive understanding of the model. Feature importance, decision rules, time series interpretation, and visual explanations constitute the XAI principles of the approaches presented in the rest of the paper.\nLet us apply these two definitions. For instance, if portfolio managers use the output of a model to assist in portfolio management, they are not seeking all types of interpretability or explainability. As they are not a data scientist, they may not necessarily aim to have a comprehensive range of explanations. Their specific goal is to determine whether to trust the model or to demystify if the model makes a mistake. To achieve this, a score that provides a confidence level of the model could be helpful. Additionally, the importance of features could also help them demystify if the model is making a well-founded decision or a poor one. They may not need the global importance of features, i.e., the importance of the features that contributed to the prediction of an entire dataset, but rather the local importance of features, i.e., the importance of features that contributed to a specific prediction. With local importance, they can understand how the model weighs its input information and why it made this particular decision. On the other hand, the developer of the model looking to understand and improve his model may need a different explanation. What he might desire is the global importance of features, because the global explanations can be connected to the training of a model. By gaining a better understanding of the model, he could enhance it, for instance, by modifying the set of input features. In all cases, it's best to have both local and global explanations to obtain a better understanding of the model and its predictions. Moreover, if the features were produced by feature engineering or are simply features that have no financial sense, interpreting their importance might be challenging. Similarly, if there are too many features that take equal importance, it will not be useful for portfolio managers. A solution to this problem could be to group features into financial themes (e.g., volatility, momentum, periods, etc.) that would be understandable for the portfolio managers.\nThe previous discussions suggest that the explanations should be adapted to the context and the user for optimal effectiveness. However, the evaluation of the explanations should not be subjective. It should be based on rigorous criteria. Adjustment to the context could be one of these criteria, but the evaluation itself should not depend on the subjectivity of the user. We present some ideas of criteria in Section 6.3."}, {"title": "3 METHODOLOGY", "content": "In this paper, we categorize approaches that are either explainable or interpretable and used for predicting financial time series or their movement. These time series may include stock prices, volatility, and macroeconomic indicators, among others. We conducted a comprehensive search in various databases, including SCOPUS, Inspec, IEEE Xplore, Computers and Applied Sciences Complete, ACM Digital Library, and arXiv. Our search criteria included articles containing the following keywords: \"explainability\", \"finance\", \"forecasting\", and \"model\". Alternative terms for \"explainability\" could be \"interpr\u00e9tabilit\u00e9\", \"interpretability\", \"XAI\", \"explainable\", \"interpretable\", \"explicability\", \"feature importance\", or \"transparent model\". For \"finance\", substitutes include \"stocks\", \"volatility\", \"tendency\", \"financial\", and \"market prices\". Similarly, \"model\" could be replaced by \"AI\", \"method\", \"machine learning\", or \"artificial intelligence\". We limited our search to papers published between 2018 and June 2023.\nWe applied two filters to the papers we found. First, we read the titles and abstracts, retaining papers that focused on finance and XAI. Examples of papers that we excluded include those about XAI in ecology, papers that were finance-focused but did not incorporate XAI, or papers on finance that did not employ machine learning. Second, we reviewed the essential sections of the shortlisted papers to extract the necessary information related to XAI. If a paper focused on the prediction of financial time series or their trends using an explainable or interpretable model, we kept, understood and summarized its XAI part. Otherwise, we excluded the paper from our review. For example, we eliminated papers that presented models forecasting credit risk and loan approvals because these models did not predict a time series or its trend. Indeed, they predicted a number or a class that did not depend on the time. Our emphasis is on the interpretability and explainability aspects of the models discussed. The aim of this paper is to provide an overview of advancements made in the explainability and interpretability of models designed for predicting financial time series. Some models use time series data as input, others rely on text from news articles or the internet, and some utilize both. It is true that some articles received more attention than others in this review; however, this should not be interpreted as an indication of their relative importance. We may also have inadvertently omitted articles that merit being included. Indeed, our review process involved reading specific sections of the papers to focus on the interpretability and explainability of the models. While our aim was to conduct as a thorough search as possible, there is a chance that we may have missed some relevant articles on the topic."}, {"title": "4 \u03a4\u0391\u03a7\u039f\u039d\u039f\u039c\u03a5", "content": "The goal of the taxonomy presented in the Figure 1 is to help readers identify the right XAI approach for their specific context. Initially, XAI approaches are divided into interpretable models and explainable methods. Each category is then further sorted based on the principles of XAI, including feature importance, visual explanations, and time series analysis. After that, explainability methods and interpretable models are categorized based on the technique used to illustrate the XAI principles. For the interpretable models, it includes linear regressions, attention mechanisms, decision trees, graphs, etc. For explanation methods, it includes perturbation, propagation and visual interface. The reviewed approaches are presented in the Sections 5 and 6 following the diagram presented in the Figure 1."}, {"title": "5 INTERPRETABLE MODELS", "content": "This section presents the interpretable models reviewed in the survey. These models provide feature importance (see Subsection 5.1), decision rules (refer to Subsection 5.2), or analysis of relations like trends in the time series (as discussed in Subsection 5.3). The evaluation of the models' interpretability is discussed in Subsection 5.4.\nSome of the presented models may appear to be more like black-box models than interpretable models. These models, because of their complexity, do not fit the conventional definition of interpretable models that only regroups basic models. For instance, neural networks, attention models and other complex models are usually considered as black-box models by the community. This makes sense because they are complex compared to basic interpretable models such as linear models and decision trees. Based on that criterion, complex models, like the attention models, would have been excluded from the class of interpretable models. However, the XAI models are not classified according to their complexity. To be considered as interpretable, the models need to provide some information that helps the user understand them, specifically through XAI principles. Therefore, the origin of the XAI principles is the key factor that determines the nature of a model with regard to interpretability and explainability. If the principle directly comes from the model itself, the model is considered interpretable. Otherwise, if the XAI principle comes from another algorithm, namely an explainable method, the model will be considered as an explainable model. So, even if some models are complex, they can still generate information that can be interpreted by the user to better understand the process behind the model. This is why many authors, including us, refer to them as interpretable. For example, attention models, like the ones presented in Subsection 5.1.3, contain attention weights that represent the importance of the features. As these weights come from the models themselves, they are classified as interpretable models. It is also the case with complex neural networks that are classified as interpretable due to a component in their structure, like a mask [84] or components of fuzzy logic [94], that reveals some information about the networks to users.\nThere may exist other definitions of interpretability and explainability that would lead to a different classification of those complex models. We believe that our definitions in this paper help draw a clear line between interpretability and explainability. And the classification of those models according to these definitions is logical and easy to understand."}, {"title": "5.1 Feature and Time Importance", "content": "Feature importance is an XAI principle that measures how much each feature has an effect on the prediction or on an entire dataset. It also includes time importance, which measures how much specific time periods were important. If the feature importance reflects the effect on a specific prediction, it is called local feature importance. If it reflects the effects on a dataset, it is called global feature importance. Usually, the global feature importance is computed on the training dataset, but it can also be computed on the test set. By ranking the features according to their importance, the important features can be extracted. The feature importance can be plotted through heat maps, bar plot, line plot, etc. The feature importance is computed for all types of features, including both input features and internal features, which represent learned representations within the model. It is important in finance because it shows which information the model uses to make its predictions. The comparison between financial analysts and a model can easily be made. In their work, the analysts will, unconsciously or consciously, weight the information that they will read and make a prediction based on the important one. A model, showing the importance of its features, will give the same kind of information to the user. The feature importance is connected to features themselves. Indeed, having a high number of features can obscure the understanding of each feature's importance. To bypass this issue without altering the model itself, one strategy is to aggregate features into clusters and then assess their importance at the cluster level. If some features do not have a financial meaning, it becomes more challenging to interpret the results. When they do, it is possible to draw more detailed conclusions about interpretability. This connection between the model and the context enables users to better understand both. When the context is well known, feature importance can help justify the model's decisions. Furthermore, the feature importance of an accurate model can reveal unknown information about the financial market and help users develop new theories. In this paper, various techniques to compute feature importance are discussed. These include coefficients in a linear model, gain in decision trees, relevance scores, and attention weights. It is important to note that while all these techniques aim to measure feature importance, they might not all convey the same underlying concept. Therefore, it will be important to evaluate the differences between these approaches in the future."}, {"title": "5.1.1 Linear Regressions", "content": "The first model that comes to mind when discussing feature importance is the linear model. It is one of the most interpretable models due to its simple way of learning and the feature importance that it provides. However, a vanilla linear model cannot precisely predict financial time series due to their complexity and non-linearity. For this reason, the vanilla linear model is not used in practice for predicting financial time series. However, linear regression can be integrated into a complex model that has the ability to predict financial time series, enhancing its interpretability. We define the linear regression $f(x)$ as $f(x) = \\beta_0 + \\sum_j \\beta_j x_j$, where $x_j$ are the features and $\\beta_j$ are the coefficients of the associated feature $x_j$. These coefficients represent the effect of their associated features. The features can be the input ones [65, 105], but they can also be internal features [112].\nA common way to use a linear regression in a complex model is to compute the weights of the regression with a complex model. This way allows a trade-off between interpretability and accuracy: it loses in the simplicity of the algorithm to learn the weights, but it gains in accuracy. For example, in order to predict unexpected revenues for companies, an Adaptive Master-Slave (AMS) model was proposed by [105]. The model has different weights for each company, unlike the normal linear models that would have the same weights for all companies. Thus, each company has their important and non-important features that help the user to understand more locally the predictions. As its name suggests, the model is composed of a master model and a slave model. The master model, a graph neural network, creates a slave model for each company. The slave model is a linear regression. Subsequently, the slave model predicts the unexpected revenue for each company using the input features. Since the slave model is a linear one, it is possible to extract the importance of features for each company. Munkhdalai et al. [2022] proposed also an architecture composed of a linear model and a complex model. In that case, the complex model is a recurrent neural network. Indeed, to predict time series, including the price of the Nasdaq, from temporal numerical data, the authors [65] use an adaptive linear regression. It is adaptive in the sense that its coefficients change over time. Initially, a linear regression is learned throughout the training. Then, the recurrent network determines the changes to be made to the coefficients in order to adapt them through the time. Finally, the weights are updated. The prediction model is therefore interpretable locally by the coefficients of the linear regression. The average of the coefficients were done to compute the global importance. For example, the global importance of the \"DTWEXB_1\", that is 1 day lag of trade weighted U.S. dollar index is -0.58. It means that an increase of one point of this feature would cause a drop of a 0.58 points of the Nasdaq index [65]. Since the coefficients of the model change through times, they were plotted as time series. These time series can be analyzed with classic statistics like the trend, the volatility, etc. For instance, it shows that the importance of \"DTWEXB_1\" was highly volatile between 2015 and 2016. It also shows that \"DTWEXB_1\" was more important in 2017 than before. This model does not show the importance of the features, but of the partial derivative of the features."}, {"title": "5.1.2 Decision Trees", "content": "Decision Trees are among the most interpretable machine learning algorithms. A decision tree models decision making through a series of questions based on feature values, leading to a clear, tree-like structure that can be easily visualized and understood. Each node in the tree represents a decision criterion, and each subsequent branch represents an outcome of that decision. The final leaf nodes provide the model's prediction. Their transparent nature is reflected in that the reasoning behind any specific prediction can be traced back through the tree, making it straightforward to explain why a particular decision is made. This clarity in decision-making processes makes decision trees the candidates of choice in scenarios where interpretability is crucial. Usually, decision trees are interpretable by the rules they provide to understand the decision process (Subsection 5.2.1). However, decision trees can also be interpretable by providing the gain of each feature. The gain measures a continuum of feature importance from global to local. The gain represents how much the node of a feature contributes to decreasing the uncertainty of the model during the training.\nAs an example of applying decision trees to financial time series forecasting, Silva et al. [2021] used decision trees to predict the closing price of IBOVESPA with historical values as input and technical analysis. More specifically, they introduced the FDT-FTS (Fuzzy Decision Tree-Fuzzy Time Series) method for this task. As the name suggests, this approach consists of a fuzzy decision tree induced from the C4.5 algorithm [78]. The method has four parts: data analysis, data fuzzification, training and testing of the FDT, and results defuzzification. Also, the importance of the features is computed from the information gained during tree induction. According to the gain, the most important features are the RSI, the difference of moving averages and the first difference delayed in t-1."}, {"title": "5.1.3 Attention Mechanism", "content": "Attention mechanisms in deep learning models, such as the conventional attention and the self-attention found in Transformers [92], offer interpretability by assessing the importance of input features [41, 107, 111], internal features [22, 56, 115], or specific time periods [20, 41, 91]. Indeed, the attention mechanism is based on correctly weighting the inputs to make a better prediction. It provides an easy way to include the time importance in the interpretation.\nIn fact, the time importance is desirable for a financial analyst to understand which time frame the model utilizes for its predictions. The time frame information helps him/her determine if there was a specific event that happened during that time. This event could be a significant decrease in a stock's price or a company sale. Such information not only helps the analyst figure out if the model's prediction could be incorrect, but also provides a deeper understanding of the context. Attention models offer this kind of insight through their two-dimensional attention weights. Since attention models take sequential data with both a time dimension and a feature dimension as input, the attention weights also have these two dimensions. In fact, these weights indicate where the model is focusing its attention, on which features and at which time points. The weights can then be extracted for model interpretation and are often visualized through heatmaps where one dimension represents time and the other represents features. This offers insights into the model's selective focus and its consideration of contextual relevance. To obtain the absolute time or feature importance, the weights can be summed according to the right dimension. We present some attention models that can be used to analyse feature and time importance of sequential input such as time series, words and events.\nThe two first attention models [58, 111] described here contain an LSTM integrated with an attention mechanism. Liu et al. [2022] used the ILSTM (Interpretable-LSTM) from [41]. This LSTM is unique because of its ability to separate hidden states by features, making it possible to discern their individual effects on the prediction. The model contains a mixture of attention mechanisms, namely temporal and variable wise attention, which consolidates these hidden states for forecasting and provides feature importance. The task of [58] was to predict CO2 flow based on several meteorological time series. These series exhibit seasonal variations. The model demonstrates which series and at what points in time they were important for the prediction. It also shows the importance of features over time during training. Liu et al. [2022] indicated that beyond helping understand the model, the interpretations assist understanding of the context of the problem and increasing of the performance of predictions. The authors of ILSTM [41] had tested this model on financial data in an experimental work, and the findings were promising. The task was to predict the NASDAQ 100 based on companies indexed beneath it. Their model outperformed other opaque models while also being interpretable. It shows the importance of each company at the different time step lags and at different epochs. The three most important stocks to predict the NASDAQ 100 are NTAP, FOXA and TRIP according to their model.\nSimilarly, a distinct adaptation of LSTM was presented in [111]. The model is named attention-based LSTM (AT-LSTM) and was used to predict stock prices. The unique part of this model is its attention module placed before the LSTM. This module is used to weight the input features. The basic idea that the model relies on is that input features should not all have the same weight (as in a standard LSTM). Like a financial analyst would do, the model needs to weigh the information it receives to make a good prediction. In addition to improving the LSTM's performance, this module adds a layer of interpretability. Indeed, the user can visualize the importance of features from the attention weights."}, {"title": "5.1.4 Fuzzy Logic", "content": "The model presented in [94] makes use of fuzzy logic to show the feature importance while, typically, most existing models such as [13, 31, 43, 64, 79, 100, 101] make use of it to compute decision rules. The authors of [94] proposed an architecture that employs Intuitionistic fuzzy logic and deep learning for stock prediction tasks. Unlike the general fuzzy logic that employs a single value between 0 and 1 to represent membership, intuitionistic fuzzy logic employs three, i.e. membership, hesitation, and non-membership. These concepts can be interpreted respectively as the degree of support for the rise of stocks for the membership value, the degree of rejection for the rise of stocks for the non-membership (non-affiliation) value and the uncertainty of the prediction for the hesitation value according to [94]. Feature importance was measured by using the hesitation concept. The features were masked one by one for all stocks. If the hesitation increases when a feature is masked, it indicates that the feature has an effect on a prediction. The average magnitude of the change in hesitation estimates the feature importance, allowing significant features to be identified. The impact of features on rising stocks was analyzed with the membership values. To do so, the features were masked one by one, but only for rising stocks. When a feature is masked, an increase in the membership value suggests a higher probability that the predicted rising stock will actually rise. The magnitude of the change shows the contribution of the masked feature to stocks' rise. The architecture of the model enables a better understanding of the model's predictions. The proposed structure is named Interpretable Intuitionistic Fuzzy Inference Model (IIFI). The IIFI has six layers: the input layer, the fuzzification layer, the encoding layer, the interpretation and inference layer, the consequent layer and the defuzzification layer."}, {"title": "5.1.5 Graph", "content": "Often", "representative textual features\", were extracted from the reports and used as the graph's nodes. An edge between two words was established if the two words were statistically frequent in the same window in a report. Subsequently, the graph was interpreted via the most influential words based on their centrality, the graph's density, closely interlinked subgroups, etc. The importance of each word is presented in the article. \"Imagine\", \"Pessimism\" and \"Effectiveness\" are the words having the highest positive impact, while \"Concern\", \"Design\" and \"Chengyu stock\" have the highest negative impact. The negative words seem to have a higher impact than the positive words according to their importance coefficient.\nAnother interpretable model using graph was presented in [28]. It is named Knowledge-Driven Temporal Convolutional Network (KDTCN). The model takes stock price values and news as input, and predicts stock movements. The architecture is divided into two modules: the Knowledge-Driven event embedding (KD), followed by the Temporal Convolutional Network (TCN). The KD module creates a knowledge graph based on events in the news, and returns the \"event embeddings\". Then, the TCN module takes these embedded events concatenated with the prices as input to compute the prediction. The architecture is interpretable because of the effect of events on the prediction and the relationships between events that can be analyzed through graphs, like [53]. Indeed, within their TCN framework, the effects of different events are easily extracted from the TCN, therefore significant events are identified in the graphs created in the KD module. The direct and indirect links between events can thus be analyzed, providing a deeper understanding of the decision-making context. For example, \"[United Kingdom, votes to leave European Union]\" has a high effect on the prediction of the stock to rise, according to [28].\nYet, to predict stock trends, Wang and Ye [2020] developed the Graph-Based Interpretable Stock Trend Forecasting Framework (GIFF). GIFF is divided into three modules: the sequential embedding module, the graph residual module and the forecasting module. A distinctive feature of this architecture lies in the graph residual module. This module is subdivided into three layers: the business-based layer, the industry-based layer and the fully connected layer. In the first and second layers, connections are established between stocks that share the same businesses and industries, respectively, forming graphs. The last layer connects every pair of stocks. Since it is partitioned in this manner, the layers are interpretable and the results can be interpreted for each different industry. For instance, the authors compared the correlation between the predicted returns and the actual returns of the same industry (e.g., electronics). This allowed them to determine how well their model predicts for each economic sector and individual business. However, compared to [53], this approach does not allow the direct measurement of the feature importance.\nSimilarly, the model proposed in [44] does not explicitly compute the feature importance. Instead, it highlights relations between the inputs. This model, named Multilevel Graph Attention Network (ML-GAT), predicts stock movements by taking as input numerical from Yahoo Finance website, textual from news, and relational data from Wikidata. Essentially, the model extracts features from various types of input: it encodes text-data and price data, and derives graphs from company relationships. Subsequently, the text and price data are incorporated into the graphs. Ultimately, the price trend is forecast. This model is interpretable due to the insight that can be drawn from the created graph. The relationships reflect reality since they are derived from real-world data, allowing users to obtain a comprehensive visualization of the situation. For example, a second-order relation (A$\\xrightarrow[]{R_1}$B$\\xrightarrow[]{R_2}$C, where A, B, C are entities of Wikidata) could be defined as: $R_1$ represents the```json\n{\n  ": "itle", "A Survey of Explainable Artificial Intelligence (XAI) in Financial Time Series Forecasting": "authors\": [\n    \"PIERRE-DANIEL ARSENAULT", "SHENGRUI WANG": "JEAN-MARC PATENAUDE", "abstract": "Artificial Intelligence (AI) models have reached a very significant level of accuracy. While their superior performance offers considerable benefits, their inherent complexity often decreases human trust, which slows their application in high-risk decision-making domains, such as finance. The field of eXplainable AI (XAI) seeks to bridge this gap, aiming to make AI models more understandable. This survey, focusing on published work from the past five years, categorizes XAI approaches that predict financial time series. In this paper, explainability and interpretability are distinguished, emphasizing the need to treat these concepts separately as they are not applied the same way in practice. Through clear definitions, a rigorous taxonomy of XAI approaches, a complementary characterization, and examples of XAI's application in the finance industry, this paper provides a comprehensive view of XAI's current role in finance. It can also serve as a guide for selecting the most appropriate XAI approach for future applications.", "sections": [{"title": "1 INTRODUCTION", "content": "In the last decade, Artificial Intelligence (AI) has made remarkable strides in terms of accuracy, transforming sectors such as business, media, healthcare, education, finance, scientific research, judicial, etc. While these advances offer unprecedented capabilities, they also introduce challenges, particularly in high-stakes, risk-sensitive domains like finance and healthcare. Despite their accuracy, Al models often suffer from a lack of user trust due to their black-box nature. Ethical and legal frameworks have evolved to reflect these concerns; reports such as [68] and [82] highlight the necessity for Al systems to be understandable in decision-making contexts. As pointed out by existing works [10, 63, 68], the poor interpretability of deep learning models can significantly increase investment risks, raising critical limitations for practical applications.\nAddressing this problem is the emerging field of Explainable Artificial Intelligence (XAI), designed to produce AI models that are not only accurate but also understandable to human users. XAI offers two primary avenues for enhancing understanding: interpretable models, which are inherently understandable, and explainable models, which are models that require additional methods for explanation. However, there exists significant terminological confusion in the literature; the terms interpretable and explainable are often used interchangeably, leading to misunderstandings about the nature of the model being presented. Understanding the differences between these terms is critical for several reasons. Firstly, the pathways to achieve interpretability and explainability are different, necessitating distinct methodologies and evaluations. Secondly, accurate terminology helps in the selection of appropriate models based on the unique regulatory and ethical demands of financial applications. Thirdly, clear definitions improve interdisciplinary communication among data scientists, financial experts, and policymakers, facilitating more informed decisions. Fourthly, distinguishing between interpretability and explainability informs the level of trust one can place in a model's decision-making process, impacting risk assessment and compliance with financial regulations. Furthermore, the claim that a model is XAI sometimes lacks substantiation, either by failing to show the interpretations that could be done on the model or by lack of the explanations that help understand the model.\nThe need for this survey is further amplified by the financial industry's accelerating adoption of AI for complex tasks such as credit risk assessments, fraud detection, algorithmic trading, and, notably, the prediction of financial time series. Although these technologies promise to enhance efficiency and accuracy, they also raise significant concerns around fairness, explainability, and regulatory compliance. The opaque nature of AI algorithms is not merely a technological challenge; it has far-reaching implications for public trust, ethical governance, and regulatory adherence. This paper aims to present the latest interpretable models and explainability methods used in the prediction of financial time series. By offering a detailed survey of both interpretable and explainable models, it aims to serve as a guide for two groups of readers: data scientists who want to design and develop XAI into time series prediction, whether financial or not, and financial professionals who want to incorporate XAI into their analytics environment for predicting financial time series. Indeed, the interpretable and explainable approaches surveyed in this paper for predicting financial time series could also be used to forecast many types of time series. Also, this survey provides financial insights that the XAI made possible. Both the insights and their approaches can be useful for financial professionals. Additionally, this survey provides a rigorous foundation for continued investigation, guides practitioners in choosing the most appropriate approach, and informs them about the merits and limitations of various approaches. In essence, this survey aspires to facilitate the responsible and transparent application of AI in finance, aligning cutting-edge machine learning techniques with the sector's strict ethical and regulatory standards. To the best of our knowledge, this is the first survey focusing specifically on XAI for predicting financial time series.\nIn this paper, we categorize XAI models of the last five years that have been used to predict financial time series or to predict financial time series movement. Firstly, some important concepts are defined in the Section 2. Secondly, the methodology of this survey and the diagram resulting from the taxonomy are presented (Sections 3 and 4). Thirdly, interpretable models and explainability methods are presented and classified according to its XAI principle (Sections 5 and 6). In Section 7, the same approaches are characterized according to different criteria, namely the explanation characteristics known by the community [6, 27, 49]. Then, applications of the XAI in industry are presented (Section 8). Finally, conclusions and the work that remains to be done are presented (Section 9)."}, {"title": "2 DEFINITIONS", "content": "The financial industry has increasingly adopted AI, since the inception of many statistical models in the 1980s, primarily for stock trading and risk management applications. As the domain matured, it embraced more advanced techniques, from expert systems and neural networks to the recent inclusion of deep learning models. These innovations have provided the industry with remarkable tools for market analysis, financial behaviour prediction, and risk management. However, the lack of transparency in these complex models often limits their practical application. When users cannot decipher a model's reasoning, even high-accuracy outputs might be deemed unreliable for critical financial decisions. This emphasizes the vital role of eXplainable Artificial Intelligence (XAI) as a solution.\nThe explainable Artificial Intelligence (XAI) is a branch of AI designed to be comprehensible to humans, thus promoting trust among users. XAI's main feature is its ability to render the inner workings of its model intelligible to human users. Its importance in finance manifests in various ways. For instance, when a financial institution deploys AI for decision-making regarding a client, as in the case of a loan application, there is a pressing need to clarify the rationale behind such decisions. This transparency is imperative not only for individual clients but also for communicating decision-making processes to investors, board members, or auditors. XAI facilitates this clear communication. Additionally, when institutions rely on Al for critical decisions, stakeholders require insights into how those decisions are derived, ensuring their sustained trust in the system. From a risk management perspective, an opaque Al model introduces potential hazards. Comprehending the model's operations enables institutions to identify and mitigate risks more effectively. In decision-making scenarios, XAI provides pertinent information, helping users gauge the trustworthiness of predictions. Thus, users can judiciously combine the model outputs with their domain expertise to arrive at well-informed decisions.\nTo qualify as XAI, an AI system must either be inherently interpretable (interpretable model) or use a distinct method that clarifies its decision-making processes (explainable model). An interpretable model, often referred to as a transparent model, is designed in such a way that users can intuitively understand its inner workings just by examining it. The individual components of the model are clear and understandable, facilitating a comprehensive grasp of its functionality. As an example, a linear model with a limited set of input features is considered interpretable. In such a model, each coefficient indicates the contribution of its associated feature. Thus, interpretability can be characterized as the inherent property of a model that, by its very design, makes its operations transparent at first sight. It is important to note that interpretability is not a simple binary trait. In [57], the authors elaborated on the nuances of interpretability, highlighting dimensions such as simulatability defined as the capacity of a model to be contemplated by a person, decomposability as the capacity of a model to be composed of interpretable parts, and algorithmic transparency as the level of interpretability of the model's training algorithm. A model is said interpretable if it is more understandable than an opaque model.\nAn explainable model is designed to provide explanations to users, aiding their comprehension of the model's operations. Often, an explainable model is essentially a black-box model enhanced with specific explainability methods, such as SHAP [60] or LIME [80], to shed light on its internal processing. A pure black-box model, also known as an opaque model, operates in a manner that is not immediately discernible, making its internal workings and decision-making processes inscrutable to users. There are two reasons that some models are designated as black-boxes: either due to their inherent design or because of imposed confidentiality. For instance, deep neural networks, by virtue of their intricate architecture involving many operations, are inherently challenging to understand and thus are classified as black-box models. On the other hand, even a rudimentary linear model with a handful of features can be perceived as a black-box if the owner discloses only the outputs while withholding the coefficients. Explainability can thus be defined as the capability of a model to elucidate its functioning, offering users insights into its operational mechanics.\nIt is essential to define a few basic concepts that are frequently used in the rest of the paper. The term model refers to a series of calculations, or mathematical equations, describing the workings of a computing system that takes typical inputs, such as numbers or words, and generates outputs, or predictions, for a given task. An explainability method is defined as a process that takes a model as input and provides explanations of this model as output. Consequently, an explainable model is one on which an explainability method has been applied. An approach encompasses a number of these computation steps. We can define it as a process that takes an input and generates an output. As [73] explored, there remains ambiguity surrounding the fundamental concepts of XAI. It becomes essential to identify the target audience for the explanations and to delineate the nature of these explanations. Two core concepts in the preceding definitions appear nebulous:\n(1) The term user is not explicitly defined. In the financial sector, this could refer to a data scientist, a business professional, an auditor, or even a consumer. Clearly, an explanation tailored for one may not be suitable for another. Consequently, it is crucial to specify the audience for whom the model should be rendered explainable or interpretable [7, 9, 73].\n(2) The notion of explanations lacks a formal characterization. These explanations must be context-sensitive [8, 9], addressing the distinct requirements of different users. For instance, in the realm of financial time series prediction, professionals often seek insights into feature importance to provide clarity on how input features influence the outcome. Another desired aspect of the explanation involves the decision rules, making the decision-making process clear and transparent. Time series interpretations also form a significant aspect of explanations, offering insights into the nature and patterns of input time series, and facilitating analyses based on extracted information and predictions. Furthermore, in the financial domain, visual representations such as user interfaces or dashboards are highly valued. Such visual aids not only present explanations but also offer supplementary information, fostering a more intuitive understanding of the model. Feature importance, decision rules, time series interpretation, and visual explanations constitute the XAI principles of the approaches presented in the rest of the paper.\nLet us apply these two definitions. For instance, if portfolio managers use the output of a model to assist in portfolio management, they are not seeking all types of interpretability or explainability. As they are not a data scientist, they may not necessarily aim to have a comprehensive range of explanations. Their specific goal is to determine whether to trust the model or to demystify if the model makes a mistake. To achieve this, a score that provides a confidence level of the model could be helpful. Additionally, the importance of features could also help them demystify if the model is making a well-founded decision or a poor one. They may not need the global importance of features, i.e., the importance of the features that contributed to the prediction of an entire dataset, but rather the local importance of features, i.e., the importance of features that contributed to a specific prediction. With local importance, they can understand how the model weighs its input information and why it made this particular decision. On the other hand, the developer of the model looking to understand and improve his model may need a different explanation. What he might desire is the global importance of features, because the global explanations can be connected to the training of a model. By gaining a better understanding of the model, he could enhance it, for instance, by modifying the set of input features. In all cases, it's best to have both local and global explanations to obtain a better understanding of the model and its predictions. Moreover, if the features were produced by feature engineering or are simply features that have no financial sense, interpreting their importance might be challenging. Similarly, if there are too many features that take equal importance, it will not be useful for portfolio managers. A solution to this problem could be to group features into financial themes (e.g., volatility, momentum, periods, etc.) that would be understandable for the portfolio managers.\nThe previous discussions suggest that the explanations should be adapted to the context and the user for optimal effectiveness. However, the evaluation of the explanations should not be subjective. It should be based on rigorous criteria. Adjustment to the context could be one of these criteria, but the evaluation itself should not depend on the subjectivity of the user. We present some ideas of criteria in Section 6.3."}, {"title": "3 METHODOLOGY", "content": "In this paper, we categorize approaches that are either explainable or interpretable and used for predicting financial time series or their movement. These time series may include stock prices, volatility, and macroeconomic indicators, among others. We conducted a comprehensive search in various databases, including SCOPUS, Inspec, IEEE Xplore, Computers and Applied Sciences Complete, ACM Digital Library, and arXiv. Our search criteria included articles containing the following keywords: \"explainability\", \"finance\", \"forecasting\", and \"model\". Alternative terms for \"explainability\" could be \"interpr\u00e9tabilit\u00e9\", \"interpretability\", \"XAI\", \"explainable\", \"interpretable\", \"explicability\", \"feature importance\", or \"transparent model\". For \"finance\", substitutes include \"stocks\", \"volatility\", \"tendency\", \"financial\", and \"market prices\". Similarly, \"model\" could be replaced by \"AI\", \"method\", \"machine learning\", or \"artificial intelligence\". We limited our search to papers published between 2018 and June 2023.\nWe applied two filters to the papers we found. First, we read the titles and abstracts, retaining papers that focused on finance and XAI. Examples of papers that we excluded include those about XAI in ecology, papers that were finance-focused but did not incorporate XAI, or papers on finance that did not employ machine learning. Second, we reviewed the essential sections of the shortlisted papers to extract the necessary information related to XAI. If a paper focused on the prediction of financial time series or their trends using an explainable or interpretable model, we kept, understood and summarized its XAI part. Otherwise, we excluded the paper from our review. For example, we eliminated papers that presented models forecasting credit risk and loan approvals because these models did not predict a time series or its trend. Indeed, they predicted a number or a class that did not depend on the time. Our emphasis is on the interpretability and explainability aspects of the models discussed. The aim of this paper is to provide an overview of advancements made in the explainability and interpretability of models designed for predicting financial time series. Some models use time series data as input, others rely on text from news articles or the internet, and some utilize both. It is true that some articles received more attention than others in this review; however, this should not be interpreted as an indication of their relative importance. We may also have inadvertently omitted articles that merit being included. Indeed, our review process involved reading specific sections of the papers to focus on the interpretability and explainability of the models. While our aim was to conduct as a thorough search as possible, there is a chance that we may have missed some relevant articles on the topic."}, {"title": "4 \u03a4\u0391\u03a7\u039f\u039d\u039f\u039c\u03a5", "content": "The goal of the taxonomy presented in the Figure 1 is to help readers identify the right XAI approach for their specific context. Initially, XAI approaches are divided into interpretable models and explainable methods. Each category is then further sorted based on the principles of XAI, including feature importance, visual explanations, and time series analysis. After that, explainability methods and interpretable models are categorized based on the technique used to illustrate the XAI principles. For the interpretable models, it includes linear regressions, attention mechanisms, decision trees, graphs, etc. For explanation methods, it includes perturbation, propagation and visual interface. The reviewed approaches are presented in the Sections 5 and 6 following the diagram presented in the Figure 1."}, {"title": "5 INTERPRETABLE MODELS", "content": "This section presents the interpretable models reviewed in the survey. These models provide feature importance (see Subsection 5.1), decision rules (refer to Subsection 5.2), or analysis of relations like trends in the time series (as discussed in Subsection 5.3). The evaluation of the models' interpretability is discussed in Subsection 5.4.\nSome of the presented models may appear to be more like black-box models than interpretable models. These models, because of their complexity, do not fit the conventional definition of interpretable models that only regroups basic models. For instance, neural networks, attention models and other complex models are usually considered as black-box models by the community. This makes sense because they are complex compared to basic interpretable models such as linear models and decision trees. Based on that criterion, complex models, like the attention models, would have been excluded from the class of interpretable models. However, the XAI models are not classified according to their complexity. To be considered as interpretable, the models need to provide some information that helps the user understand them, specifically through XAI principles. Therefore, the origin of the XAI principles is the key factor that determines the nature of a model with regard to interpretability and explainability. If the principle directly comes from the model itself, the model is considered interpretable. Otherwise, if the XAI principle comes from another algorithm, namely an explainable method, the model will be considered as an explainable model. So, even if some models are complex, they can still generate information that can be interpreted by the user to better understand the process behind the model. This is why many authors, including us, refer to them as interpretable. For example, attention models, like the ones presented in Subsection 5.1.3, contain attention weights that represent the importance of the features. As these weights come from the models themselves, they are classified as interpretable models. It is also the case with complex neural networks that are classified as interpretable due to a component in their structure, like a mask [84] or components of fuzzy logic [94], that reveals some information about the networks to users.\nThere may exist other definitions of interpretability and explainability that would lead to a different classification of those complex models. We believe that our definitions in this paper help draw a clear line between interpretability and explainability. And the classification of those models according to these definitions is logical and easy to understand."}, {"title": "5.1 Feature and Time Importance", "content": "Feature importance is an XAI principle that measures how much each feature has an effect on the prediction or on an entire dataset. It also includes time importance, which measures how much specific time periods were important. If the feature importance reflects the effect on a specific prediction, it is called local feature importance. If it reflects the effects on a dataset, it is called global feature importance. Usually, the global feature importance is computed on the training dataset, but it can also be computed on the test set. By ranking the features according to their importance, the important features can be extracted. The feature importance can be plotted through heat maps, bar plot, line plot, etc. The feature importance is computed for all types of features, including both input features and internal features, which represent learned representations within the model. It is important in finance because it shows which information the model uses to make its predictions. The comparison between financial analysts and a model can easily be made. In their work, the analysts will, unconsciously or consciously, weight the information that they will read and make a prediction based on the important one. A model, showing the importance of its features, will give the same kind of information to the user. The feature importance is connected to features themselves. Indeed, having a high number of features can obscure the understanding of each feature's importance. To bypass this issue without altering the model itself, one strategy is to aggregate features into clusters and then assess their importance at the cluster level. If some features do not have a financial meaning, it becomes more challenging to interpret the results. When they do, it is possible to draw more detailed conclusions about interpretability. This connection between the model and the context enables users to better understand both. When the context is well known, feature importance can help justify the model's decisions. Furthermore, the feature importance of an accurate model can reveal unknown information about the financial market and help users develop new theories. In this paper, various techniques to compute feature importance are discussed. These include coefficients in a linear model, gain in decision trees, relevance scores, and attention weights. It is important to note that while all these techniques aim to measure feature importance, they might not all convey the same underlying concept. Therefore, it will be important to evaluate the differences between these approaches in the future."}, {"title": "5.1.1 Linear Regressions", "content": "The first model that comes to mind when discussing feature importance is the linear model. It is one of the most interpretable models due to its simple way of learning and the feature importance that it provides. However, a vanilla linear model cannot precisely predict financial time series due to their complexity and non-linearity. For this reason, the vanilla linear model is not used in practice for predicting financial time series. However, linear regression can be integrated into a complex model that has the ability to predict financial time series, enhancing its interpretability. We define the linear regression $f(x)$ as $f(x) = \\beta_0 + \\sum_j \\beta_j x_j$, where $x_j$ are the features and $\\beta_j$ are the coefficients of the associated feature $x_j$. These coefficients represent the effect of their associated features. The features can be the input ones [65, 105], but they can also be internal features [112].\nA common way to use a linear regression in a complex model is to compute the weights of the regression with a complex model. This way allows a trade-off between interpretability and accuracy: it loses in the simplicity of the algorithm to learn the weights, but it gains in accuracy. For example, in order to predict unexpected revenues for companies, an Adaptive Master-Slave (AMS) model was proposed by [105]. The model has different weights for each company, unlike the normal linear models that would have the same weights for all companies. Thus, each company has their important and non-important features that help the user to understand more locally the predictions. As its name suggests, the model is composed of a master model and a slave model. The master model, a graph neural network, creates a slave model for each company. The slave model is a linear regression. Subsequently, the slave model predicts the unexpected revenue for each company using the input features. Since the slave model is a linear one, it is possible to extract the importance of features for each company. Munkhdalai et al. [2022] proposed also an architecture composed of a linear model and a complex model. In that case, the complex model is a recurrent neural network. Indeed, to predict time series, including the price of the Nasdaq, from temporal numerical data, the authors [65] use an adaptive linear regression. It is adaptive in the sense that its coefficients change over time. Initially, a linear regression is learned throughout the training. Then, the recurrent network determines the changes to be made to the coefficients in order to adapt them through the time. Finally, the weights are updated. The prediction model is therefore interpretable locally by the coefficients of the linear regression. The average of the coefficients were done to compute the global importance. For example, the global importance of the \"DTWEXB_1\", that is 1 day lag of trade weighted U.S. dollar index is -0.58. It means that an increase of one point of this feature would cause a drop of a 0.58 points of the Nasdaq index [65]. Since the coefficients of the model change through times, they were plotted as time series. These time series can be analyzed with classic statistics like the trend, the volatility, etc. For instance, it shows that the importance of \"DTWEXB_1\" was highly volatile between 2015 and 2016. It also shows that \"DTWEXB_1\" was more important in 2017 than before. This model does not show the importance of the features, but of the partial derivative of the features."}, {"title": "5.1.2 Decision Trees", "content": "Decision Trees are among the most interpretable machine learning algorithms. A decision tree models decision making through a series of questions based on feature values, leading to a clear, tree-like structure that can be easily visualized and understood. Each node in the tree represents a decision criterion, and each subsequent branch represents an outcome of that decision. The final leaf nodes provide the model's prediction. Their transparent nature is reflected in that the reasoning behind any specific prediction can be traced back through the tree, making it straightforward to explain why a particular decision is made. This clarity in decision-making processes makes decision trees the candidates of choice in scenarios where interpretability is crucial. Usually, decision trees are interpretable by the rules they provide to understand the decision process (Subsection 5.2.1). However, decision trees can also be interpretable by providing the gain of each feature. The gain measures a continuum of feature importance from global to local. The gain represents how much the node of a feature contributes to decreasing the uncertainty of the model during the training.\nAs an example of applying decision trees to financial time series forecasting, Silva et al. [2021] used decision trees to predict the closing price of IBOVESPA with historical values as input and technical analysis. More specifically, they introduced the FDT-FTS (Fuzzy Decision Tree-Fuzzy Time Series) method for this task. As the name suggests, this approach consists of a fuzzy decision tree induced from the C4.5 algorithm [78]. The method has four parts: data analysis, data fuzzification, training and testing of the FDT, and results defuzzification. Also, the importance of the features is computed from the information gained during tree induction. According to the gain, the most important features are the RSI, the difference of moving averages and the first difference delayed in t-1."}, {"title": "5.1.3 Attention Mechanism", "content": "Attention mechanisms in deep learning models, such as the conventional attention and the self-attention found in Transformers [92], offer interpretability by assessing the importance of input features [41, 107, 111], internal features [22, 56, 115], or specific time periods [20, 41, 91]. Indeed, the attention mechanism is based on correctly weighting the inputs to make a better prediction. It provides an easy way to include the time importance in the interpretation.\nIn fact, the time importance is desirable for a financial analyst to understand which time frame the model utilizes for its predictions. The time frame information helps him/her determine if there was a specific event that happened during that time. This event could be a significant decrease in a stock's price or a company sale. Such information not only helps the analyst figure out if the model's prediction could be incorrect, but also provides a deeper understanding of the context. Attention models offer this kind of insight through their two-dimensional attention weights. Since attention models take sequential data with both a time dimension and a feature dimension as input, the attention weights also have these two dimensions. In fact, these weights indicate where the model is focusing its attention, on which features and at which time points. The weights can then be extracted for model interpretation and are often visualized through heatmaps where one dimension represents time and the other represents features. This offers insights into the model's selective focus and its consideration of contextual relevance. To obtain the absolute time or feature importance, the weights can be summed according to the right dimension. We present some attention models that can be used to analyse feature and time importance of sequential input such as time series, words and events.\nThe two first attention models [58, 111] described here contain an LSTM integrated with an attention mechanism. Liu et al. [2022] used the ILSTM (Interpretable-LSTM) from [41]. This LSTM is unique because of its ability to separate hidden states by features, making it possible to discern their individual effects on the prediction. The model contains a mixture of attention mechanisms, namely temporal and variable wise attention, which consolidates these hidden states for forecasting and provides feature importance. The task of [58] was to predict CO2 flow based on several meteorological time series. These series exhibit seasonal variations. The model demonstrates which series and at what points in time they were important for the prediction. It also shows the importance of features over time during training. Liu et al. [2022] indicated that beyond helping understand the model, the interpretations assist understanding of the context of the problem and increasing of the performance of predictions. The authors of ILSTM [41] had tested this model on financial data in an experimental work, and the findings were promising. The task was to predict the NASDAQ 100 based on companies indexed beneath it. Their model outperformed other opaque models while also being interpretable. It shows the importance of each company at the different time step lags and at different epochs. The three most important stocks to predict the NASDAQ 100 are NTAP, FOXA and TRIP according to their model.\nSimilarly, a distinct adaptation of LSTM was presented in [111]. The model is named attention-based LSTM (AT-LSTM) and was used to predict stock prices. The unique part of this model is its attention module placed before the LSTM. This module is used to weight the input features. The basic idea that the model relies on is that input features should not all have the same weight (as in a standard LSTM). Like a financial analyst would do, the model needs to weigh the information it receives to make a good prediction. In addition to improving the LSTM's performance, this module adds a layer of interpretability. Indeed, the user can visualize the importance of features from the attention weights."}, {"title": "5.1.4 Fuzzy Logic", "content": "The model presented in [94] makes use of fuzzy logic to show the feature importance while, typically, most existing models such as [13, 31, 43, 64, 79, 100, 101] make use of it to compute decision rules. The authors of [94] proposed an architecture that employs Intuitionistic fuzzy logic and deep learning for stock prediction tasks. Unlike the general fuzzy logic that employs a single value between 0 and 1 to represent membership, intuitionistic fuzzy logic employs three, i.e. membership, hesitation, and non-membership. These concepts can be interpreted respectively as the degree of support for the rise of stocks for the membership value, the degree of rejection for the rise of stocks for the non-membership (non-affiliation) value and the uncertainty of the prediction for the hesitation value according to [94]. Feature importance was measured by using the hesitation concept. The features were masked one by one for all stocks. If the hesitation increases when a feature is masked, it indicates that the feature has an effect on a prediction. The average magnitude of the change in hesitation estimates the feature importance, allowing significant features to be identified. The impact of features on rising stocks was analyzed with the membership values. To do so, the features were masked one by one, but only for rising stocks. When a feature is masked, an increase in the membership value suggests a higher probability that the predicted rising stock will actually rise. The magnitude of the change shows the contribution of the masked feature to stocks' rise. The architecture of the model enables a better understanding of the model's predictions. The proposed structure is named Interpretable Intuitionistic Fuzzy Inference Model (IIFI). The IIFI has six layers: the input layer, the fuzzification layer, the encoding layer, the interpretation and inference layer, the consequent layer and the defuzzification layer."}, {"title": "5.1.5 Graph", "content": "Often", "2020": "analyzed the relationship between a stock index and analysts' research reports. Although", "representative textual features\", were extracted from the reports and used as the graph's nodes. An edge between two words was established if the two words were statistically frequent in the same window in a report. Subsequently, the graph was interpreted via the most influential words based on their centrality, the graph's density, closely interlinked subgroups, etc. The importance of each word is presented in the article. \"Imagine\", \"Pessimism\" and \"Effectiveness\" are the words having the highest positive impact, while \"Concern\", \"Design\" and \"Chengyu stock\" have the highest negative impact. The negative words seem to have a higher impact than the positive words according to their importance coefficient.\nAnother interpretable model using graph was presented in [28": ".", "modules": "the Knowledge-Driven event embedding (KD)", "event embeddings\". Then, the TCN module takes these embedded events concatenated with the prices as input to compute the prediction. The architecture is interpretable because of the effect of events on the prediction and the relationships between events that can be analyzed through graphs, like [53": ".", "[United Kingdom, votes to leave European Union": " has a high effect on the prediction of the stock to rise", "28": "."}, {"2020": "developed the Graph-Based Interpretable Stock Trend Forecasting Framework (GIFF). GIFF is divided into three modules: the sequential embedding module", "layers": "the business-based layer", "53": "this approach does not allow the direct measurement of the feature importance.\nSimilarly", "44": "does not explicitly compute the feature importance. Instead", "input": "it encodes text-data and price data", "A$\\xrightarrow[": {"R_1}$B$\\xrightarrow[": {"as": "R_1$ represents the"}}}]}]}