{"title": "Smart Vision-Language Reasoners", "authors": ["Denisa Roberts", "Lucas Roberts"], "abstract": "In this article, we investigate vision-language models (VLM) as reasoners. The ability to form abstractions underlies mathematical reasoning, problem-solving, and other Math AI tasks. Several formalisms have been given to these underlying abstractions and skills utilized by humans and intelligent systems for reasoning. Furthermore, human reasoning is inherently multimodal, and as such, we focus our investigations on multimodal AI. In this article, we employ the abstractions given in the SMART task (Simple Multimodal Algorithmic Reasoning Task) introduced in (Cherian et al., 2022) as meta-reasoning and problem-solving skills along eight axes: math, counting, path, measure, logic, spatial, and pattern. We investigate the ability of vision-language models to reason along these axes and seek avenues of improvement. Including composite representations with vision-language cross-attention enabled learning multimodal representations adaptively from fused frozen pretrained backbones for better visual grounding. Furthermore, proper hyperparameter and other training choices led to strong improvements (up to 48% gain in accuracy) on the SMART task, further underscoring the power of deep multimodal learning. The smartest VLM, which includes a novel QF multimodal layer, improves upon the best previous baselines in every one of the eight fundamental reasoning skills. End-to-end code is available at github.com/smarter-vlm/smarter.", "sections": [{"title": "1. Introduction", "content": "Human intelligence is oftentimes associated with the ability to operate on mathematical abstractions. In (Chollet, 2019) the author conducts an in depth discussion and formulates"}, {"title": "2. Related Work", "content": "Reasoning Surveys of deep learning for mathematical reasoning such as (Lu et al., 2022; Sun et al., 2023) mentioned the relatively smaller subset of works on multimodal / vision-language models in this space, with datasets and models which are smaller, niche, and mostly using visual question answering frameworks. These approaches are lacking since they are trained on natural images and not on models trained on vision and language datasets. Subsequent works such as (Zhang et al., 2024b; Wu et al., 2024; Lu et al., 2023) and this article, aim to enrich the multimodal mathematical reasoning domain.\nVision-Language Models Opportunities for improvement of vision language models still exist along problem-solving and algorithmic reasoning ability, visual grounding, as well as architectures for encoding, decoding and aligning (Karamcheti et al., 2024; Tong et al., 2024; Liu et al., 2023; Wu & Xie, 2023). In this article we focus on the reasoning ability along eight dimensions of reasoning. In the reasoning realm, much recent work focuses on evaluating vision-language models on general multimodal tasks (Yue"}, {"title": "2.1. Benchmark, Dataset, and Challenges", "content": "So how can we help (deep) artificial neural networks reason better? In (Cherian et al., 2022) experiments show that the visual signal is very important in solving complex multi-reasoning skill puzzles and, despite being very large, language-only models lag behind visual language models in terms of performance. Conversely, in (Zhang et al., 2024b) the conclusion appears to be that large multimodal models cannot truly understand the visual diagrams for mathematical reasoning, along the line of weak visual grounding and poor attention to visual detail in (Tong et al., 2024) and (Wu & Xie, 2023) for large multimodal models for math, question answering, and other reasoning tasks. The Simple Multimodal Algorithmic Reasoning Task (SMART) introduced in (Cherian et al., 2022) contains puzzles that measure intelligence across eight different reasoning skill classes: counting, math, logic, path, measure, logic, and pattern. Problems include an image and a text question and are formulated as multiple choice. We can see a few examples of problems in Figure 2. Baseline models trained in (Cherian et al., 2022) struggle to solve this task, especially when employing transformers. In the past, specialized neural networks such as (Miku\u0142a et al., 2023) have been developed to solve specific reasoning tasks, specifically premise selection in automated theorem proving. In this article, we investigate how we can craft and train deep neural networks which employ several types of deep learning blocks and multimodal inputs from deep frozen transformers to reason better across the eight meta reasoning axes in the SMART task.\nThe SMART reasoning task and baselines. A set of vision-language models are trained as benchmarks in (Cherian et al., 2022) and SMART-101 with 202K text-image pairs for train, validation and test dataset is released. There are 101 origin puzzles, and additional problems are generated programatically in each puzzle group for a total of 202,000 question-image pairs. Figure 2 clearly describes a training example problem. All trained VLMs struggle on the SMART task, with transformers underperforming ResNet50 (He et al., 2016) based models. The learning tasks depend on the type of puzzle and are in the classification, regression, and sequence generation category. Several image and text encoder backbones are considered. A puzzle specific set of image features are learned via an MLP and the text embeddings are aggregated using an LSTM layer. The decoder for sequence generation is another LSTM layer. All image encoders are finetuned. Based on these characteristics, there are a few research opportunities worth exploring, especially since transformer-based VLM reasoners are doing so poorly on the challenging SMART task."}, {"title": "3. Methodology", "content": "We formalize the problem as supervised learning with classification loss. For each image-question instance, we predict the probability of one of five answer options. When the"}, {"title": "4. Experiments and Results", "content": "We train several baselines from (Cherian et al., 2022) and give results in Table 1. We chose to move forward with the frozen BERT+ResNet50 as baseline for two reasons: 1. Note that the numbers are extremely close between the frozen and unfrozen variants but the frozen variant does better on Math, a top skill of interest for this investigation; 2. The backbones are frozen which we favor in this article for a few reasons. The first reason is efficiency of training. Frozen backbones result in fewer parameters to update. Secondly, as noted in (Karamcheti et al., 2024), finetuning vision"}, {"title": "5. Discussion and Future Work", "content": "In this article, we show how deep learning architectural innovations as well as hyperparameter and training choices led to improvement in model performance on the SMART reasoning task. Multimodal transformer-like architectures, deep learning representations, and stronger visual grounding led to improvements in eight fundamental reasoning skills of vision language models.\nFuture work. Considering the different learning dynamics for the eight skill classes, a multitask learning ap-"}, {"title": "The QF layer representation learned from the image- question input is concatenated to the average pooled text representation and the puzzle-specific adaptive image rep- resentation from the fused image encoders. As depicted in Figure 1, the resulting composite representation, denoted compositeR, takes as input three component representa- tions, r1, r2, and r3, defined as follows.", "content": "\u2022 Text representation r3 is an average pooled encoding of the question sequence of max length 110 tokens. Each token is first encoded using the frozen SigLIP text model into a representation of size 768. Then\nr_3 = \\text{Average Pooling}([h_1, h_2, ..., h_{110}]). (1)\n\u2022 An image representation r\u2081 from the puzzle-specific image encoder block of dimension 128 seen in 1. The dimension is a hyperparameter selected via optimization. The image encoder consists of two feed forward layers with a GELU unit, with separate weights for each puzzle head, for the 101 separate puzzle groups (e.g. one loss calculated per puzzle-group). Each encoder takes as input the image representation from the two fused pretrained vision backbones, DinoV2 (Oquab et al., 2023) and SigLIP (Zhai et al., 2023), each of dimension 768. Specifically, for an image X, r\u2081 is\nr_1 = FC_{1i}(\\text{GELU}(FC_{2i}(y))),  (2)\ny = \\text{Concat}([\\text{Dino}(x), \\text{SigLIP}(x)]) (3)\nfor i \u2208 {1, ..., 101}, a distinct puzzle group.\n\u2022 A QF representation, r2, is produced by the QF layer which takes as input the encoded image representation r1 and the SigLIP-encoded sequence of text tokens (before average pooling). First, the SigLIP frozen language backbone encodes the 110-long question text sequence. Then, the QF layer passes the text sequence through a multi-head self-attention block (Vaswani et al., 2017). The resulting hidden representation is fed to a cross-attention layer as query, with keys and values coming from the adaptive image encoder representa- tion, marginally inspired from the QFormer in (Li et al., 2023b) and VilBERT (Lu et al., 2019) but with mul- tiple differences. Distinctly from these works, in our case the image encoder in the cross-attention sublayer is a per-puzzle group adaptive representation learned on top of the frozen fused concatenation of DinoV2 and SigLIP vision backbones. Finally, an intermediate stack of fully connected layers, with residual connec- tions (He et al., 2016), dropout (Srivastava et al., 2014), and layer normalisation (Ba et al., 2016) produces the"}, {"title": "QF text-and-vision multimodal representation. Specifically,", "content": "\\begin{aligned}\nr_2 = \\text{LayerNorm}(x + \\text{Drop}(FC(\\text{GELU}(FC(x))))) \\\\\nX = \\text{MHCrossA}(\\text{MHA}([h_1, h_2, ..., h_{110}]), r_1)\n\\end{aligned} (4)\n(5)\nFinally, the composite QVFusion layer aggregates these distinct representations (text-only, vision-only, and text- and-vision QF multimodal) via concatenation producing the composite representation CompositeR \u2208 R2*768+128, and then passing it through a two-layer feed forward mod- ule with Gaussian error linear units (Hendrycks & Gimpel, 2016) in between, before being read by the puzzle specific decoder.\nThe composite representation is\nCompositeR = \\text{CLayer}([r_1, r_2, r_3])  (6)\n= \\text{LayerNorm}(\\text{Concat}([r_1, r_2, r_3]). (7)\nThe QVFusion layer in 1 is\nQVFusion(y) = \\text{LayerNorm}(\\text{GELU}(y)) (8)\ny = FC(\\text{GELU}(FC(\\text{compositeR}))). (9)\nFinally, the decoder, which is either a stack of three fully connected layers separated by GELU activations, or a gated recurrent neural network (GRU) (Cho et al., 2014) for sequence-type answer puzzles, produces predictions fed to a cross-entropy loss. The introduction of GELU units with layer normalization boosts performance, as they do in many recent attention-based multimodal neural networks (Liu et al., 2023; Alayrac et al., 2022; Li et al., 2023b), by allowing for a smoother loss landscape than rectified linear units with batch normalization layers."}]}