{"title": "Smart Vision-Language Reasoners", "authors": ["Denisa Roberts", "Lucas Roberts"], "abstract": "In this article, we investigate vision-language models (VLM) as reasoners. The ability to form abstractions underlies mathematical reasoning, problem-solving, and other Math AI tasks. Several formalisms have been given to these underlying abstractions and skills utilized by humans and intelligent systems for reasoning. Furthermore, human reasoning is inherently multimodal, and as such, we focus our investigations on multimodal AI. In this article, we employ the abstractions given in the SMART task (Simple Multimodal Algorithmic Reasoning Task) introduced in (Cherian et al., 2022) as meta-reasoning and problem-solving skills along eight axes: math, counting, path, measure, logic, spatial, and pattern. We investigate the ability of vision-language models to reason along these axes and seek avenues of improvement. Including composite representations with vision-language cross-attention enabled learning multimodal representations adaptively from fused frozen pretrained backbones for better visual grounding. Furthermore, proper hyperparameter and other training choices led to strong improvements (up to 48% gain in accuracy) on the SMART task, further underscoring the power of deep multimodal learning. The smartest VLM, which includes a novel QF multimodal layer, improves upon the best previous baselines in every one of the eight fundamental reasoning skills. End-to-end code is available at github.com/smarter-vlm/smarter.", "sections": [{"title": "1. Introduction", "content": "Human intelligence is oftentimes associated with the ability to operate on mathematical abstractions. In (Chollet, 2019) the author conducts an in depth discussion and formulates a formal definition of intelligence based on algorithmic information theory. Several meta characteristics of intelligent systems are listed as scope, generalization difficulty, priors and experience. On a different but related axis, (Didolkar et al., 2024) speaks of metacognitive capabilities of large language models, abilities that underlie all problem solving, including math problems. In a related work in the multimodal domain (Cherian et al., 2022), a Simple Multimodal Algorithmic Reasoning Task (SMART) is introduced with visual-linguistic puzzles designed for children in the 6-8 age group (the US Kangaroo Olympiad style). In this work, an explicit categorization of underlying skills utilized by humans in problem solving are labeled and tallied as they get employed in solving puzzles as measure, path, pattern, logic, math, algebra, and spatial skills. Furthermore, reasoning must be multimodal because humans have multiple senses whose inputs are amalgamated to reason at higher abstractions. Better abstractions are akin to better mental representations. Deep neural networks excel at learning"}, {"title": "2. Related Work", "content": "Reasoning Surveys of deep learning for mathematical reasoning such as (Lu et al., 2022; Sun et al., 2023) mentioned the relatively smaller subset of works on multimodal / vision-language models in this space, with datasets and models which are smaller, niche, and mostly using visual question answering frameworks. These approaches are lacking since they are trained on natural images and not on models trained on vision and language datasets. Subsequent works such as (Zhang et al., 2024b; Wu et al., 2024; Lu et al., 2023) and this article, aim to enrich the multimodal mathematical reasoning domain.\nVision-Language Models Opportunities for improvement of vision language models still exist along problem-solving and algorithmic reasoning ability, visual grounding, as well as architectures for encoding, decoding and aligning (Karamcheti et al., 2024; Tong et al., 2024; Liu et al., 2023; Wu & Xie, 2023). In this article we focus on the reasoning ability along eight dimensions of reasoning. In the reasoning realm, much recent work focuses on evaluating vision-language models on general multimodal tasks (Yue"}, {"title": "2.1. Benchmark, Dataset, and Challenges", "content": "So how can we help (deep) artificial neural networks reason better? In (Cherian et al., 2022) experiments show that the visual signal is very important in solving complex multi-reasoning skill puzzles and, despite being very large, language-only models lag behind visual language models in terms of performance. Conversely, in (Zhang et al., 2024b) the conclusion appears to be that large multimodal models cannot truly understand the visual diagrams for mathematical reasoning, along the line of weak visual grounding and poor attention to visual detail in (Tong et al., 2024) and (Wu & Xie, 2023) for large multimodal models for math, question answering, and other reasoning tasks. The Simple Multimodal Algorithmic Reasoning Task (SMART) introduced in (Cherian et al., 2022) contains puzzles that measure intelligence across eight different reasoning skill classes: counting, math, logic, path, measure, logic, and pattern. Problems include an image and a text question and are formulated as multiple choice. We can see a few examples of problems in Figure 2. Baseline models trained in (Cherian et al., 2022) struggle to solve this task, especially when employing transformers. In the past, specialized neural networks such as (Miku\u0142a et al., 2023) have been developed to solve specific reasoning tasks, specifically premise selection in automated theorem proving. In this article, we investigate how we can craft and train deep neural networks which employ several types of deep learning blocks and multimodal inputs from deep frozen transformers to reason better across the eight meta reasoning axes in the SMART task.\nThe SMART reasoning task and baselines. A set of vision-language models are trained as benchmarks in (Cherian et al., 2022) and SMART-101 with 202K text-image pairs for train, validation and test dataset is released. There are 101 origin puzzles, and additional problems are generated programatically in each puzzle group for a total of 202,000 question-image pairs. Figure 2 clearly describes a training example problem. All trained VLMs struggle on the SMART task, with transformers underperforming ResNet50 (He et al., 2016) based models. The learning tasks depend on the type of puzzle and are in the classification, regression, and sequence generation category. Several image and text encoder backbones are considered. A puzzle specific set of image features are learned via an MLP and the text embeddings are aggregated using an LSTM layer. The decoder for sequence generation is another LSTM layer. All image encoders are finetuned. Based on these characteristics, there are a few research opportunities worth exploring, especially since transformer-based VLM reasoners are doing so poorly on the challenging SMART task."}, {"title": "3. Methodology", "content": "We formalize the problem as supervised learning with classification loss. For each image-question instance, we predict the probability of one of five answer options. When the"}, {"title": "4. Experiments and Results", "content": "We train several baselines from (Cherian et al., 2022) and give results in Table 1. We chose to move forward with the frozen BERT+ResNet50 as baseline for two reasons: 1. Note that the numbers are extremely close between the frozen and unfrozen variants but the frozen variant does better on Math, a top skill of interest for this investigation; 2. The backbones are frozen which we favor in this article for a few reasons. The first reason is efficiency of training. Frozen backbones result in fewer parameters to update. Secondly, as noted in (Karamcheti et al., 2024), finetuning vision"}, {"title": "5. Discussion and Future Work", "content": "In this article, we show how deep learning architectural innovations as well as hyperparameter and training choices led to improvement in model performance on the SMART reasoning task. Multimodal transformer-like architectures, deep learning representations, and stronger visual grounding led to improvements in eight fundamental reasoning skills of vision language models.\nFuture work. Considering the different learning dynamics for the eight skill classes, a multitask learning ap-"}, {"title": "Contributions.", "content": "In (Awad et al., 2023) the authors demonstrated how a deep learning module which encode a sequence of image-and-text items using diverse representations composed on several modalities, across time steps, and across pooling methods, obtained impressive results in sponsored search and recommendations. Inspired by the ADPM in (Awad et al., 2023) and using tricks to train vision transformers in (Dolev et al., 2023) and (Karamcheti et al., 2024), a smarter VLM is built. In this article, we make the following contributions on the VLM reasoning axis:\n\u2022 Introduce a novel multimodal QF-layer to learn a hidden representation from the vision and language modalities.\n\u2022 Improve the MLP decoders in (Cherian et al., 2022) through GELU activations, residual connections, and layer normalization.\n\u2022 Improve the sequence decoder by replacing the LSTM with a GRU.\n\u2022 Strengthen the vision modality by learning an adaptive visual representation on top of two fused vision backbones: SigLIP (Zhai et al., 2023) and DinoV2 (Oquab et al., 2023) similarly to (Karamcheti et al., 2024). In this way, the model makes better use of the puzzle's image.\n\u2022 Strengthen the text-vision alignment by using a frozen SigLIP language encoder together with the vision modality which includes the SigLIP vision backbone. The pretrained text encoder does not overpower the visual signal as much as an LLM as seen in (Tong et al., 2024; Zhang et al., 2024b).\n\u2022 Furthermore, the smarter VLM reasoner includes a composite hidden representation through the concatenation of language-only representations, an adaptive image-only representation learned on top of the fused frozen foundation backbones, and the QF multimodal layer representation which includes a language-vision cross-attention sublayer. Ablation studies in Section 4 show that the QF layer is essential to the smarter VLM reasoner. The use of cross-attention improves the ability of the reasoner to make use of the puzzle's visual cues.\n\u2022 These model improvements lead to up to 48% accuracy gain across several of the meta reasoning skills measured by the challenging SMART task."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning and Math AI. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "QF text-and-vision multimodal representation.", "content": "Specifically, \n$$r_2 = LayerNorm(x + Drop(FC(GELU(FC(x))))) $$\n$$X = MHCrossA(MHA([h_1, h_2, ..., h_{110}]), r_1)$$\nFinally, the composite QVFusion layer aggregates these distinct representations (text-only, vision-only, and text- and-vision QF multimodal) via concatenation producing the composite representation $$CompositeR \u2208 R^{2*768+128}$$, and then passing it through a two-layer feed forward module with Gaussian error linear units (Hendrycks & Gimpel, 2016) in between, before being read by the puzzle specific decoder.\nThe composite representation is\n$$CompositeR = CLayer([r_1, r_2, r_3])$$\n$$ = LayerNorm(Concat([r_1,r_2, r_3]).$$\nThe QVFusion layer in 1 is\n$$QVFusion(y) = LayerNorm(GELU(y)) $$\n$$y = FC(GELU(FC(compositeR))).$$\nFinally, the decoder, which is either a stack of three fully connected layers separated by GELU activations, or a gated recurrent neural network (GRU) (Cho et al., 2014) for sequence-type answer puzzles, produces predictions fed to a cross-entropy loss. The introduction of GELU units with layer normalization boosts performance, as they do in many recent attention-based multimodal neural networks (Liu et al., 2023; Alayrac et al., 2022; Li et al., 2023b), by allowing for a smoother loss landscape than rectified linear units with batch normalization layers."}, {"title": "QF intermediate sublayer ablations.", "content": "The QF layer includes a multihead attention (MHA) sublayer which takes the frozen text representations as input, a cross-attention layer which uses the MHA hidden representations as queries and the adaptive image representations as keys and values, and an intermediate final stack consisting of two fully connected layers and a residual connection with dropout and layer normalization. Exclusion of the residual connection (together with dropout and layer normalization) had a detrimental impact across skills, confirming the importance of residuals and regularization techniques for learning in deep transformers and for generalization. Furthermore, ablation on the dropout level, confirms better generalization with more regularization from dropout. The ablation of the intermediate layer sizing had mixed results across reasoning skills and we proceeded with the symmetry of using the hidden size used elsewhere throughout the smartest reasoner architecture (256). The QF layer includes two types of multihead attention (language only self-attention and language-vision cross-attention) which share the number of heads. An ablation on number of heads (1, 2, 4, 8) showed mixed results across skills with very similar results for average accuracy (except one head, which underperforms all the other choices) and is worth experimenting with further on larger datasets in future work."}, {"title": "The QF layer representation learned from the image- question input", "content": "is concatenated to the average pooled text representation and the puzzle-specific adaptive image representation from the fused image encoders. As depicted in Figure 1, the resulting composite representation, denoted compositeR, takes as input three component representations, r1, r2, and r3, defined as follows.\n\u2022 Text representation r3 is an average pooled encoding of the question sequence of max length 110 tokens. Each token is first encoded using the frozen SigLIP text model into a representation of size 768. Then\n$$r_3 = AveragePooling([h_1, h_2, ..., h_{110}]).$$\n\u2022 An image representation r1 from the puzzle-specific image encoder block of dimension 128 seen in 1. The dimension is a hyperparameter selected via optimization. The image encoder consists of two feed forward layers with a GELU unit, with separate weights for each puzzle head, for the 101 separate puzzle groups (e.g. one loss calculated per puzzle-group). Each encoder takes as input the image representation from the two fused pretrained vision backbones, DinoV2 (Oquab et al., 2023) and SigLIP (Zhai et al., 2023), each of dimension 768. Specifically, for an image X, r1 is\n$$r_1 = FC_{1i}(GELU(FC_{2i}(y))), $$\n$$y = Concat([Dino(x), SigLIP(x)])$$\nfor i \u2208 {1, ..., 101}, a distinct puzzle group.\n\u2022 A QF representation, r2, is produced by the QF layer which takes as input the encoded image representation r1 and the SigLIP-encoded sequence of text tokens (before average pooling). First, the SigLIP frozen language backbone encodes the 110-long question text sequence. Then, the QF layer passes the text sequence through a multi-head self-attention block (Vaswani et al., 2017). The resulting hidden representation is fed to a cross-attention layer as query, with keys and values coming from the adaptive image encoder representation, marginally inspired from the QFormer in (Li et al., 2023b) and VilBERT (Lu et al., 2019) but with multiple differences. Distinctly from these works, in our case the image encoder in the cross-attention sublayer is a per-puzzle group adaptive representation learned on top of the frozen fused concatenation of DinoV2 and SigLIP vision backbones. Finally, an intermediate stack of fully connected layers, with residual connections (He et al., 2016), dropout (Srivastava et al., 2014), and layer normalisation (Ba et al., 2016) produces the"}, {"title": "Development process and scaling side note.", "content": "For a deep understanding of the behavior of the models with various architectural and hyperparameter choices, in the development phase, we started with training and evaluation on a very small subset of data for quick insight and iteration: only 20 questions per puzzle, with a batch size of 16, on a split ratio of train : val: test = 40:20:40. The experimental results are available in Tables 7 and 8 in the Appendix and were tracked with CometML (Comet.com, 2021) and publicly accessible at vlm-reasoners."}]}