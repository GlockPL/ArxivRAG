{"title": "Teaching Language Models to Critique via Reinforcement Learning", "authors": ["Zhihui Xie", "Jie Chen", "Liyu Chen", "Weichao Mao", "Jingjing Xu", "Lingpeng Kong"], "abstract": "Teaching large language models (LLMs) to critique and refine their outputs is crucial for building systems that can iteratively improve, yet it is fundamentally limited by the ability to provide accurate judgments and actionable suggestions. In this work, we study LLM critics for code generation and propose CTRL, a framework for Critic Training via Reinforcement Learning, which trains a critic model to generate feedback that maximizes correction performance for a fixed generator model without human supervision. Our results demonstrate that critics trained with CTRL significantly enhance pass rates and mitigate compounding errors across both base and stronger generator models. Furthermore, we show that these critic models act as accurate generative reward models and enable test-time scaling through iterative critique-revision, achieving up to 106.1% relative improvements across challenging code generation benchmarks.", "sections": [{"title": "1. Introduction", "content": "Recent advances in Large Language Models (LLMs) have sparked interest in their potential for self-improvement through iterative feedback mechanisms (Pan et al., 2023). Methods like Reflexion (Shinn et al., 2024) and Self-Refine (Madaan et al., 2024) demonstrate that LLMs can, in principle, critique their own outputs and generate refined responses. This self-improvement paradigm offers a promising direction toward more autonomous AI systems that can learn from their mistakes.\nHowever, the effectiveness of such self-improvement mechanisms remains challenging in practice. Huang et al. (2023) demonstrate that without appropriate external feedback, such self-improvement loops may lead to performance degradation. To address this, existing approaches primarily rely on reward models (Sun et al., 2023; Yuan et al., 2024)\nor automated verification tools (Gou et al., 2023; Chen et al., 2023). However, these mechanisms often fail to provide actionable guidance \u2013 reward models compress complex evaluation criteria into simplified numerical signals (Gao et al., 2023; Pan et al., 2024), while verification tools generate low-level execution traces that do not directly translate to high-level fixes (Zhong et al., 2024). Even in domains like code generation (Li et al., 2022; Sun et al., 2024) where such feedback mechanisms are readily available, previous work (Zheng et al., 2024) as well as our experiment (Table 1) reveal that such feedback alone struggles to drive meaningful improvements. At the heart of this issue lies the feedback bottleneck: feedback needs to both accurately discriminate the correctness of solutions and provide informative yet actionable suggestions for improvement.\nTo address these challenges, we propose CTRL (Critic Training via Reinforcement Learning), a framework that decouples the critic model from the task-performing model (e.g., GPT-40) and focus on developing a specialized critic that can effectively drive the task-performing model toward optimal solution generation through iterative critique-revisions (Figure 2). This decomposition naturally introduces a well-defined proxy task for training the critic model: while directly evaluating the quality of generated critiques remains challenging, the effectiveness of a critic can be"}, {"title": "2. Preliminaries and Motivation", "content": "The success of iterative improvement methods critically depends on their ability to leverage feedback to improve solutions. Formally, let x be an input problem and y be a candidate solution, with R(y) being the evaluation function"}, {"title": "3. Method", "content": "With analysis presented in Section 2, our goal is to teach LLMs the ability of critiquing without human supervision. We propose CTRL, a two-stage training approach: (1) synthesizing high-quality critiques by reasoning about execution feedback, then (2) refining the critic through reinforcement learning. Once trained, the critic model can be used at test time, paired with any generator models, to iteratively refine solutions. A complete overview of the pipeline is provided in Appendix A, with critique samples in Appendix E."}, {"title": "3.1. Problem Statement", "content": "We focus on code generation as our primary domain as it provides clear objective metrics through test cases, following previous work (McAleese et al., 2024). Given a programming problem x (specified in natural language) and a solution y (code implementation), our goal is to enable iterative refinement of solutions, which centers on two key components: (1) a generator model \\(\\pi(y | x)\\) that proposes solutions, and (2) a critic model \\(Q_{\\theta}(c|x, y)\\) that provides textural feedback c for improvement.\nAssumptions. Let \\(D = \\{(x_i, T_i)\\}\\}_{i=1}^{N}\\) be our training dataset, where each problem \\(x_i\\) is paired with unit tests \\(T_i\\). We have access to a sandbox environment that executes code against test cases, which serves as the evaluation function R(y) that returns 1 if y passes all tests, 0 otherwise. Notably, the sandbox does not assist critique generation at test time. While not required, we treat the generator model as a black-box, allowing our approach to build upon existing strong generators without access to their parameters."}, {"title": "Objective.", "content": "While directly measuring the helpfulness of critiques remain challenging, we can define a proxy task that evaluates whether the critique leads to improved solutions. Given an initial solution \\(y' \\sim \\pi(\\cdot | x)\\), the critic analyzes it and produces textual feedback c. The generator then uses this feedback to revise the solution, producing an improved output y. Let \\(z = (x, y')\\) represent the problem-solution pair. Our objective is to train the critic model \\(Q_{\\theta}\\) to maximize the expected solution quality:\n\\(I(\\theta) = E_{z \\sim D} E_{x, y \\sim \\pi_{\\theta}(\\cdot|z)} [R(y)],\\) (1)\nwhere \\(\\pi_{\\theta}(y | z) = \\sum_{c} Q_{\\theta}(c | z)\\pi(y | z, c)\\) denotes the improved solution distribution through marginalization over possible critiques. Notably, although Equation (1) defines a single-turn critique-revision task, we observe that the trained model generalizes to multi-turn revisions (Section 4.2)."}, {"title": "Defining the Critique Space.", "content": "We structure the critique space into three components (Figure 2): (1) an analysis of the solution's strengths and weaknesses, (2) actionable improvement suggestions, and (3) a final judgment of correctness (correct/incorrect). During inference, these components enable iterative critique-revision, where the process stops once the judgment indicates the solution is correct. This design balances discrimination and critiquing, both essential for iterative refinement, as discussed in Section 2."}, {"title": "3.2. Stage I: Execution-guided Critique Synthesis", "content": "Although conceptually straightforward, learning effective critiques is challenging due to the large critique space, where only a small fraction leads to successful revisions. Our experiments with Qwen2.5-Coder (Hui et al., 2024) (Table 1 show that models struggle to generate informative critiques for self-improvement, aligning with previous findings (Huang et al., 2023). Self-critique without additional feedback yields minimal gains (7.88% \u2192 8.36%) and rarely converts incorrect solutions to correct ones, highlighting the limited ability of models to correct their own mistakes."}, {"title": "Reasoning over Execution.", "content": "While the initial critiquing ability is limited, previous work (Ni et al., 2024) has shown that LLMs can effectively reason over execution feedback. Table 1 demonstrates that when LLMs reason over execution feedback to generate critiques (Self-critique w/ Execution Feedback), they achieve substantial improvements, as compared to directly using raw execution feedback for revisions (11.76% vs. 8.97%). This suggests that while directly using raw execution feedback is inefficient, we can leverage the model's reasoning ability over execution feedback to help generate more accurate and informative critiques."}, {"title": "Critique Synthesis.", "content": "Building on the above insight, we develop a critique synthesis approach that leverages execution"}, {"title": "3.3. Stage II: Reinforced Critique Generation", "content": "While our critique synthesis approach with predefined templates provides a strong foundation, it may not capture all nuanced feedback scenarios required for complex programming tasks. To overcome this limitation, we formulate critique generation as a reinforcement learning problem, allowing the critic to adaptively learn feedback strategies"}, {"title": "4. Experiments", "content": "We conduct extensive experiments to evaluate our method's effectiveness across multiple benchmarks. Our evaluation focuses on two key aspects: (1) the accuracy of the critic in identifying solution correctness, and (2) the quality improvement achieved through critique-guided revisions."}, {"title": "4.1. Setup", "content": "Training Data. We use TACO (Li et al., 2023), a dataset containing 26,443 programming problems collected from competitive programming platforms like CodeForces and LeetCode. Each problem includes a natural language description and multiple test cases. Due to noise in the original dataset (malformed test cases and contaminated problems), we filter the dataset to 18,820 problems for training, with details presented in Appendix C.3.\nModels. We base our critic model on the open-source Qwen2.5-Coder-Ins (Hui et al., 2024) model. During training, we fix the generator model to be Qwen2.5-Coder-Ins itself. For evaluation, we assess the trained critic's performance by pairing it with various generator models for initial solution generation and subsequent revision, comparing against other LLM critics such as GPT-40.\nBenchmarks. We evaluate our approach on three programming benchmarks and one general-domain benchmark: (1) CodeContests (Li et al., 2022), a collection of challenging competitive programming problems; (2) LiveCodeBench (24.08-24.11) (Jain et al., 2024), a curated set of recent programming challenges designed to minimize data contamination; (3) MBPP+ (Liu et al., 2024a), an extension of the MBPP benchmark (Austin et al., 2021) focused on fundamental programming tasks; and (4) JudgeBench (Tan et al., 2024), where we evaluate the model's effectiveness as a generative reward model for comparing solution pairs.\nMetrics. To evaluate critiquing ability, we use three metrics: Pass@1 measures the success rate of the final solutions, \u25b3\u2191 represents the fraction of initially incorrect solutions that become correct after revision, and \u2206\u2193 represents the fraction of initially correct solutions that become incorrect after revision. For discrimination ability, we employ F1 score when evaluating single solutions, and accuracy when comparing paired solutions in Judgebench, as the latter involves binary decisions between two alternatives.\nExecution Sandbox. We employ SandboxFusion (Liu et al., 2024b) as our execution environment, which provides a unified interface for evaluating solutions across training data and benchmarks through both function-based and standard input-output formats."}, {"title": "4.2. Evaluating Critics for Iterative Critique-revisions", "content": "To evaluate the effectiveness of CTRL, we present a comprehensive analysis of critique-revision strategies with different feedback mechanisms on CodeContests in Table 1. The discrimination performance of critics is shown in Table 2, while results across different benchmarks and generators are presented in Table 3.\nRL Significantly Boosts Critiquing Ability. Table 1 shows that our RL-trained critic significantly outperforms baseline approaches, achieving a 11.76% pass@1 rate compared to 7.88% with zero-shot generation. This substantial improvement builds upon a much reduced regression rate \u25b3\u2193 than its SFT counterpart (0.85% vs. 3.03%).\nCTRL Enables Test-time Scaling. As shown in Table 1, our approach enables test-time scaling through iterative critique-revisions. Notably, despite training exclusively on single-turn critiquing tasks, CTRL generalizes to multi-turn settings. By increasing the number of iterations from one to three (Critique\u00d73 w/ CTRL), we further improve the Pass@1 rate from 11.76% to 15.15% while maintaining a low regression rate A\u2193 of 0.85%. This demonstrates that our critic provides consistently reliable feedback across multiple revision iterations, unlike baseline approaches that accumulate errors, as discussed below."}, {"title": "CTRL Mitigates Compounding Errors.", "content": "CTRL mitigates compounding errors.  Further illustrates this stability advantage - while both Qwen2.5-Coder and GPT-40 show increasing error compounding rates over iterations, CTRL maintains a significantly lower rate, enabling reliable multi-round improvements.\nCTRL Generalizes to Different Generators and Tasks. While we train the critic model with Qwen2.5-Coder as the generator, as shown in Table 3, our approach generalizes well across different programming tasks. Notably, a weak critic model trained against itself can assist stronger model (GPT-40), providing evidence for scalable oversight (Christiano et al., 2018; Kenton et al., 2024).\nPerformance Scaling with Problem Difficulty. As shown in Figure 5, our critique-revision approach demonstrates increasingly substantial relative gains as both iteration and problem difficulty increases, revealing that CTRL is particularly effective for complex tasks, where iterative refinement through targeted critique and revision yields the most significant benefits compared to zero-shot generation."}, {"title": "4.3. Evaluating Critics as Generative Reward Models", "content": "One advantage of unifying textural feedback is to balance discrimination and critiquing abilities. To assess our critics' discrimination capabilities, we evaluate them on JudgeBench (Tan et al., 2024), a comprehensive benchmark"}, {"title": "4.4. Analysis", "content": "To better understand how CTRL boosts iterative refinement, we further conduct analyses on the similarity between original and revised solutions, execution time changes, and critique characteristics. Our findings reveal several key patterns in how different critique methods influence the process of critique-revision.\nThe Effect of Generator Ability. As a preliminary analysis before finetuning experiments, we examine how model sizes affect critique-revision performance using Qwen2.5-Coder-Ins models (7B, 14B, and 32B) in an inference-only setting, comparing zero-shot generation against critique-revision with critiques generated by another critic model conditioned on execution feedback. Table 4 reveals that critic capability significantly influences improvement potential\u2014while smaller critics (7B) often lead to performance degradation, larger critics (32B) consistently yield better outcomes, achieving up to 50% improvement when paired with similarly-sized generators. The results also highlight the importance of critic-generator size relationships, as critics less capable than their generators typically degrade performance. These findings motivate us to focus our subsequent finetun-"}, {"title": "CTRL Prevents Similar Revisions.", "content": "We analyze how different critique methods influence solution revisions by measuring code similarity scores between original and revised solutions, as described in Appendix C.4. As shown in Figure 7, self-critique tends to make conservative modifications with higher similarity scores (mean 0.482), while our CTRL method proposes more substantial changes (mean 0.313). This suggests CTRL is more willing to recommend major structural revisions when needed, rather than just local optimizations, which may explain its superior performance in improving solution quality."}, {"title": "CTRL Trade-offs between Accuracy and Efficiency.", "content": "While our critique-revision approach improves solution accuracy on LiveCodeBench, we observe a notable increase in timeout rates. Solutions guided by CTRL exhibit a timeout rate of 16.61%, higher than both zero-shot (10.54%) and GPT-40 critic (8.93%). However, even with more timeouts, CTRL still achieves better overall Pass@1 accuracy. This suggests that our approach tends to generate more comprehensive solutions\u2014while these may take longer to execute, the solution quality is guaranteed."}, {"title": "5. Related Work", "content": "Self-Improvement of LLMs. Recent work has explored various approaches for LLMs to improve their outputs autonomously, including self-critique (Madaan et al., 2024; Shinn et al., 2024), debates (Irving et al., 2018; Michael et al., 2023; Khan et al., 2024), and training models to self-correct (Welleck et al., 2022; Kumar et al., 2024). However, Huang et al. (2023) demonstrates that without appropriate external feedback, such self-improvement loops may lead to performance degradation. Our work addresses these challenges by learning specialized models that can provide effective feedback for improvement.\nLLM Critics. Several approaches have been proposed to train LLMs as critics for various purposes, including generative reward models (Ankner et al., 2024; Xiong et al., 2024) and scalable oversight (Saunders et al., 2022; Kenton et al., 2024). These approaches either learn from human feedback (Wang et al., 2023; McAleese et al., 2024) or much more capable models' outputs (Xi et al., 2024), with recent work exploring reinforcement learning to improve feedback generation (Aky\u00fcrek et al., 2023; Yao et al., 2023). Our approach differs in three key aspects: (1) leveraging execution feedback and model reasoning to synthesize high-quality critiques, (2) introducing variance reduction techniques to stabilize training, and (3) requiring only single-round critique-revision interactions. Additional discussion on related work is provided in Appendix B.\nScaling Test-Time Compute. Recent work has explored various approaches to improve model performance at test time without fine-tuning (Snell et al., 2024). While existing approaches focus on techniques like repeated sampling with proper selection mechanisms (Brown et al., 2024) and more sophisticated modular frameworks with existing models (Saad-Falcon et al., 2024), we instead investigate test-time scaling through a decoupled critic model trained to provides targeted feedback to guide solution improvements. Notably, while Saad-Falcon et al. (2024) demonstrates that strong models can serve as effective critics, their approach struggles with code generation tasks."}, {"title": "6. Conclusion", "content": "We present CTRL, a reinforcement learning framework for training critic LLMs to provide effective feedback for iterative refinement. Our trained critic demonstrates significant improvements over baselines across multiple benchmarks and enables efficient test-time scaling through iterative critique-revisions\u2014notably, even when guiding stronger generators. While this work focuses on improving pass rates, future directions include optimizing for efficiency and safety, and extending our training pipeline towards multi-turn critique revision. We hope this work inspires further research into scalable LLM self-improvement through reinforcement learning."}, {"title": "Impact Statement", "content": "This work aims to advance the field of Machine Learning by introducing a framework for training LLM critics. While this research has the potential to improve the reliability and robustness of AI systems, we have not identified any immediate societal concerns requiring specific attention. However, as with any AI technology, careful consideration should be given to its broader deployment and potential misuse."}, {"title": "A. Pipeline", "content": "As shown in Figure 8, our pipeline consists of two main training stages. (1) The SFT training stage first generates initial solutions that are validated through execution feedback, followed by critique generation where the generator learns to provide critiques based on execution feedback. These components are then used to train the final critic model through supervised finetuning. (2) The RL training stage leverages the critic's feedback to guide the generator in producing improved solutions, which are validated in a sandbox environment."}, {"title": "B. Supplementary Discussion of Related Work", "content": "Table 6 categorizes prior methods into reward models, generative reward models, and critic models. Reward models like Standard RM (Bradley & Terry, 1952) and SynRM (Ye et al., 2024) focus on discrimination by outputting scalar rewards r but lack refinement or critique supervision. Generative reward models, such as CLoud (Ankner et al., 2024) and Critic-RM (Yu et al., 2024), enhance discrimination by producing both rewards r and critiques c, but their critiques primarily serve as a by-product for rewards rather than actionable refinement suggestions. Critic models, including UltraCM (Cui et al., 2023), Shepherd (Wang et al., 2023), and CriticGPT (McAleese et al., 2024), focus on generating critiques but rely heavily on human-annotated critique data, which limits scalability. In contrast, CTRL unifies discrimination and refinement by generating actionable critiques without direct supervision, leveraging execution feedback and reinforcement learning to enable scalable, iterative improvement."}, {"title": "C. Implementation Details", "content": "C.1. Simulation\nIn our simulation (Section 2), we model the iterative refinement process using a Markov chain with parameters Pinit, Pcc, and Pcw to represent the initial correctness, the probability of maintaining correctness, and the probability of turning incorrect"}, {"title": "C.2. Prompt Templates", "content": "Critique-revision. The generator model \\(\\pi(y | x)\\) is implemented as a simple zero-shot generation process, where the model generates a solution y directly from the problem statement x without additional context or feedback. The critic model \\(Q_{\\theta}(c|x, y)\\), as described in the main paper, generates textual feedback c using a structured prompt that incorporates the problem x, the solution y, and explicit instructions to provide actionable and formatted suggestions. The improved solution distribution (y | x, y', c) is implemented as a two-turn process: in the first turn, the generator model drafts the initial solution y' conditioned on the problem x as the user message; in the second turn, the critique c is presented as the user message, and the model revises the solution, conditioned on x, y', and c.\nExecution-guided Critique Generation. To generate high-quality critiques (Section 3.2), we leverage execution feedback from a sandbox environment that evaluates the initial solution y' against the test cases T for the problem x. The execution results are mapped to predefined hint templates, which guide the critique generation process. The critic model is prompted with a structured template incorporating the problem x, the solution y', and the corresponding hint h, enabling it to produce actionable and context-aware feedback. To prevent hallucination, critiques that explicitly reference the hints are filtered out. This ensures that the generated critiques are grounded in observable failures while effectively supporting solution refinement."}, {"title": "C.3. Training", "content": "Data Curation. Our data curation process starts with the TACO dataset (Li et al., 2023) and handles both function-based and input-output-based programming problems. We filter out malformed problems by removing those containing image tags and unusual HTML spans. For unit tests, we process them differently based on their type: function-based tests are converted to assertion statements, while input-output tests are standardized into a sandbox format with stdin-stdout pairs. We exclude problematic unit tests such as those with malformed string inputs (containing assignments or unexpected list operations) or invalid function calls. To avoid contamination, we further exclude 47 problems that overlap with our evaluation benchmarks. The final dataset is deduplicated based on problem descriptions, resulting in 18,820 problems.\nSupervised Finetuning. We leverage the synthesized critiques to perform supervised finetuning (SFT) on the model, enabling it to generate improved solutions. For each problem, we sample one initial solution and one corresponding synthesized critique, and train the model on these problem-solution-critique pairs. The training process follows the"}, {"title": "C.4. Evaluation.", "content": "Inference. During inference, we use a temperature of 0.7 for generating both initial solutions and critiques, while revised solutions are generated using greedy decoding. The maximum number of tokens generated is set to 1,024 for all stages.\nReward Calculation. To calculate rewards for our JudgeBench evaluation (Section 4.3), we use a critic model to assess the quality of solutions. Specifically, we generate multiple critiques for each solution and aggregate the results through majority voting. For each solution pair, the critic model compares the frequency of being labeled as \"Correct\" to determine which solution is better. As shown in Figure 10(a), we find that the accuracy of this majority voting strategy improves as the number of votes increases.\nCode Similarity Calculation. To measure code similarity while accounting for semantically equivalent code with different variable names, we follow (Zheng et al., 2024) and implement a two-step comparison approach. We first normalize the code by parsing it into an Abstract Syntax Tree (AST), systematically renaming variables to canonical forms, and converting back to consistently formatted text. We then compute a similarity ratio using Python's difflib.SequenceMatcher, which represents the proportion of matching characters in the optimal alignment of the two normalized code sequences. This approach yields a score between 0 and 1, allowing us to identify structurally similar solutions regardless of variable naming choices."}, {"title": "D. The Credit Assignment Issue for Critic Training", "content": "Our initial attempts to train critics using Proximal Policy Optimization (Schulman et al., 2017) for RL training revealed challenges in credit assignment for critique generation, as evidenced by the unstable value predictions shown in Figure 10(b). This difficulty arises because the quality of a critique is inherently holistic - it depends on multiple interdependent aspects like accuracy, completeness, and constructiveness making it hard for the value network to learn which parts of the generated critique contributed to the final reward. These observations motivated our switch to GRPO, which circumvents the value prediction problem by using Monte Carlo sampling to directly estimate advantages, leading to more reliable credit assignment without the need for a potentially unstable value network."}, {"title": "E. CTRL Samples", "content": "In Tables 10, 12 and 14, we present sample critiques generated by CTRL for solutions provided by Qwen2.5-Coder."}]}