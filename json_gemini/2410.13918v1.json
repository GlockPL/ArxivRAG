{"title": "Leveraging Fine-Tuned Language Models for Efficient and Accurate Smart Contract Auditing", "authors": ["ZHIYUAN WEI", "JING SUN", "ZIJIAN ZHANG", "XIANHAO ZHANG", "MENG LI"], "abstract": "The rise of blockchain technologies has greatly accelerated the development and deployment of smart contracts. However, their inherent vulnerabilities and susceptibility to bugs have led to significant financial losses, underscoring the challenges in securing smart contracts. While traditional auditing methods are crucial, they often fall short in addressing the increasing complexity and volume of smart contracts. Recent advancements in Large Language Models (LLMs) offer promising solutions for enhancing software auditing by automatically identifying security vulnerabilities. Despite their potential, the practical application of these models is hindered by substantial computational demands. This paper investigates the feasibility of using smaller, fine-tuned models to achieve comparable or even superior results in smart contract auditing. We introduce the FTSmartAudit framework, which is designed to develop cost-effective, specialized models for smart contract auditing through the fine-tuning of LLMs. Our contributions include: (1) a single-task learning framework that streamlines data preparation, training, evaluation, and continuous learning; (2) a robust dataset generation method utilizing domain-special knowledge distillation to produce high-quality datasets from advanced models like GPT-40; (3) an adaptive learning strategy to maintain model accuracy and robustness; (4) the proven effectiveness of fine-tuned models in detecting specific vulnerabilities and complex logical errors; and (5) a framework that can be extended to other domains requiring LLM solutions. Our experimental results demonstrate that smaller models can surpass state-of-the-art commercial models and tools in detecting vulnerabilities in smart contracts.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in distributed ledger technologies have precipitated significant progress in the domain of smart contracts, with notable implications for applications such as cybersecurity and decentralized finance. Smart contracts, defined as self-executing programmatic agreements with predefined conditions established by participating entities, have emerged as pivotal innovations in decentralized systems. While smart contracts offer significant advantages in terms of automation and trustless transactions, they are also prone to vulnerabilities and bugs that can lead to severe financial and operational consequences. Over the past half-decade, the ecosystem has witnessed a series of high-profile security breaches and exploitations targeting smart contracts, such as the DAO attack [8]. Consequently, ensuring the security of smart contracts remains a complex challenge, and traditional auditing methods, while essential, are increasingly inadequate in addressing the growing complexity and scale of these contracts.\nLarge Language Models (LLMs), including BERT [13], T5 [26], and GPT [9], have demonstrated potential in automating vulnerability detection by extracting key features and providing accurate predictions [20, 26, 40]. For example, Zhang et al. [39] highlighted GPT's success in detecting vulnerabilities in C/C++ and Java, often outperforming traditional methods. However, despite the impressive capabilities of these state-of-the-art (SOTA) commercial LLMs, their deployment in real-world applications is challenging due to their high computational requirements. Serving a single LLM with 175 billion parameters requires at least 350 GB of GPU memory, using specialized infrastructure [44]. To make matters worse, many researchers believe that larger models tend to achieve better performance, which has led to the development of models exceeding 500 billion parameters. Such computational requirements are far beyond the reach of most users, particularly for applications that require low-latency performance. Additionally, large models may contain excess data, which can degrade performance in some domain-specific tasks [17, 43]. Significantly, models with large parameters are primarily intended for multitasking. For just one or two specific tasks, using these extensive models is generally excessive. Consequently, it is essential to pursue alternative strategies that enhance the feasibility and efficiency of deploying LLMs.\nWhile large models are powerful multitaskers, they are often excessive for domain-specific tasks, such as smart contract auditing. In contrast, smaller models, which are more computationally efficient, present a promising alternative. A major limitation in smart contract auditing is the lack of specialized, high-quality datasets. Existing tools and models may detect general vulnerabilities, but they often struggle with the nuances of smart contracts, where even minor logic flaws can result in substantial financial losses. For smaller models to maintain high accuracy while reducing computational overhead, they require focused, high-quality datasets. However, the scarcity of such datasets hampers their effectiveness. This leads to the central question of our research: How can we enable smaller models to efficiently learn and detect vulnerabilities using compact, high-quality datasets in smart contract auditing?\nTo address this challenge, we introduce the FTSmartAudit framework\u00b9, a novel approach that combines high-quality dataset extraction from raw data with adaptive learning to continuously enhance both model performance and dataset quality. Our framework is specifically designed to train smaller models for efficient and accurate vulnerability detection in smart contracts. The key contributions of our work include:\n\u2022 Automatic Framework: We propose a single-task learning framework tailored for smart contract auditing. This framework integrates four stages of creating specialized models, including data preparation, training process, evalua-\ntion, and continuous learning. It helps researchers and practitioners focus on refining model accuracy and performance.\n\u2022 Domain-Special Knowledge Distillation: We provide a high-quality dataset generation method tailored for smart contract auditing. This method leverages domain-special knowledge distillation by using frontier models like GPT-40 to generate data, which is then used to fine-tune and improve the performance of more cost-efficient models. We ensure the dataset covers a comprehensive range of vulnerability types and is representative of many real-world scenarios."}, {"title": "2 BACKGROUNDS", "content": "2.1 Smart Contract Security\nSmart contracts have emerged as a transformative force in the digital realm, giving rise to a wide range of compelling applications. Recent surveys [37, 46] indicate a rapid increase in the number of smart contracts over the past five years. DeFi, the most important application of smart contracts, has seen a significant rise in popularity, with its peak total value locked (TVL) reaching 179 billion in USD on 9 November 2021 \u00b2. However, the substantial asset values associated with smart contracts also attract numerous potential malicious actors. Smart contracts have been plagued by several high-profile vulnerabilities and exploits. Zhou et al. [46] document that smart contracts have suffered from countless high-profile attacks, resulting in losses exceeding 3.24 billion in USD from April 2018 to April 2022.\nUnlike traditional software, smart contracts are more prone to having vulnerabilities permanently embedded within their code. Thus, many security researchers try to find analysis tools to automatically detect and analyze smart contracts before they are deployed. They employ advanced techniques such as formal verification, symbolic execution, fuzzing, and intermediate representation (IR) to enhance their effectiveness [8, 34]. While essential, these traditional auditing methods are often time-consuming and may not scale well with the growing complexity and number of smart contracts. For instance, tools relying on formal verification are adept at ensuring contracts adhere to specified requirements but may fall short in detecting security flaws like reentrancy or gas limit issues. Similarly, fuzzing is more effective in finding shallow bugs and less effective in identifying bugs that lie deep in the execution flow.\n2https://defillama.com/\n2.2 LLMs for Vulnerability Prediction\nLLMs are termed \"large\" due to their extensive number of parameters, which empower them to comprehend and generate human language with remarkable coherence and contextual appropriateness. Natural language processing, image generation, code, and mathematical problem-solving are on the topic of target domains. In the realm of code processing, LLMs have shown considerable advancement since the pioneering work of Codex [9]. This progress has led to the development of commercial products like GitHub Copilot [4] and open-source code models such as StarCoder [21] and Codellama [28].\nLLMs have also achieved excellent performance on specific downstream tasks, such as code analysis, vulnerability detection and code upgrading [14]. Chen et al. [10] have proven that LLMs (GPT-2, T5), trained with a high-quality dataset consisting of 18,945 vulnerable functions about C/C++, outperform other machine learning methods, such as Graph Neural Networks, in vulnerability detection. Additionally, fine-tuned models such as CodeT5 and NatGen significantly improve the performance on vulnerability detection task. SOTA commercial products, such as GPT-4, offer promising solutions to augment the smart contract auditing process [7, 11]. By leveraging their code comprehension and generation capabilities, these models can identify specific vulnerabilities, verify compliance, and check for logical correctness. Their effectiveness is further enhanced through advanced prompting techniques like chain-of-thought (CoT) or few-shot.\n2.3 Fine-tuning Technique\nFine-tuning is a technique that adapts a pre-trained model's general knowledge to perform well on specific domains or tasks by further training the model on a targeted dataset [25]. This approach allows the model to learn task-specific features while retaining its broad language understanding. The effectiveness of fine-tuning depends on the base model's exposure to similar data. If the model has never encountered the type of data you're interested in, fine-tuning may have limited impact. However, when applied correctly, fine-tuning can yield impressive results even with relatively small datasets.\nCompared to retraining all of a model's parameters from scratch, fine-tuning is often more efficient and can achieve significant improvements with fewer resources. For example, CodeLlama model [28] was created by fine-tuning Llama 2 on a mix of proprietary instruction data. This process significantly improved performance on various truthfulness, toxicity, and bias benchmarks while maintaining strong code generation capabilities. CodeGemma [33] also illustrates the power of domain-specific fine-tuning. By training on more than 500 billion tokens of code, CodeGemma achieved significantly better performance on coding tasks compared to the base Gemma models. Furthermore, Lima et al. [45] demonstrated that fine-tuning a 65 billion parameter LLAMA (v1) model on just 1,000 high-quality samples could outperform GPT-3 (DaVinci003) on certain tasks."}, {"title": "3 METHODOLOGY", "content": "We have developed a framework named FTSmartAudit (Fine-Tuning for Smart Contract Audit), which utilizes LLMs for smart contract auditing. This framework has proven effective across various model sizes and architectures. As illustrated in Figure 1, FTSmartAudit is structured around four main stages: i.e., data preparation, training, evaluation, and continuous learning. These stages create a continuous cycle that facilitates ongoing enhancements in model performance.\n3.1 Data Preparation\nHigh-quality fine-tuning data is crucial for developing effective models. Fine-tuning datasets offer extensive text that enables models to learn grammar, syntax, and the overall structure of the language. Although not flawless, fine-tuning equips models with a basic level of common-sense reasoning through exposure to patterns and relationships in the data. This process aids in understanding cause and effect, making inferences, and recognizing typical event sequences. By fine-tuning on large datasets, the model learns representations that can be fine-tuned for specific tasks. This fine-tuning lays a robust foundation, allowing the model to be further adapted to specific applications through additional task-specific training. To support this, we have developed a data generation approach to create a high-quality dataset of smart contract code samples, which consists of two primary steps: data collection and dataset construction.\n3.1.1 Data Collection. Data collection for the fine-tuning dataset is derived from two primary sources: manually labeled data and automatically generated data.\nManually Labeled Data: This category of data encompasses two specific types: open-source codebases and security reports. The open-source codebase includes publicly accessible smart contract code from various repositories, which are meticulously reviewed and annotated by security experts. The security reports are detailed documents prepared by security auditors or security firms, documenting specific vulnerabilities, attacks, and fixes in smart contracts. The integrity and relevance of these sources are ensured as they have been verified and labeled by experts. The main sources include:\n\u2022 Decentralized Application Security Project (DASP) 3: Focuses on the security of decentralized applications, especially on blockchain platforms like Ethereum. DASP's Top 10 list underscores the most critical security vulnerabilities found in smart contracts.\n\u2022 Smart Contract Weakness Classification (SWC) 4: Provides a structured classification of security vulnerabilities specific to Ethereum contracts, paralleling the Common Weakness Enumeration system. This helps in systematically addressing and understanding smart contract weaknesses.\n\u2022 Slither detectors 5: A static analysis framework for Solidity, containing various functions for smart contract analysis. It also provides reports and detailed visual insights about vulnerabilities, covering up to 94 different cases.\n\u2022 DeFiVulnLabs 6: explore DeFi protocols to uncover common and novel vulnerabilities in smart contracts. Now, it supports 47 types of vulnerabilities. Besides, it could produce educational content, such as tutorials and guides.\n\u2022 Code4rena 7: Operates as a competitive auditing platform that fosters smart contract security through community-driven audits. Experts and companies globally identify vulnerabilities in real-world smart contract projects, motivated by bounties to uncover severe vulnerabilities.\nSynthetic Data: We employ knowledge distillation, also known as model distillation, to generate synthetic examples of code and analysis reports. Knowledge distillation is a technique where domain-specific knowledge from large parameter models is distilled into smaller models, enhancing their domain-specific task-solving capabilities and reducing inference latency [5, 16]. This approach is particularly useful in scenarios with limited labeled data, as it\n3https://dasp.co/\n4https://swcregistry.io/\n5https://github.com/crytic/slither\n6ttps://github.com/SunWeb3Sec/DeFiHackLabs\n7https://github.com/ZhangZhuoSJTU/ Web3Bugs\nleverages a larger \"teacher\" model to generate a training dataset with noisy pseudo labels [3], [18]. We adopt this method to create a synthetic dataset from large parameter models.\nLarge parameter models (such as GPT and Claude) have proven effective in generating synthetic data to train smaller models [18, 35]. Although data labeled by these large parameter models tends to be noisier than manually labeled data, the process offers significant advantages in cost-effectiveness, speed, and generalizability across various tasks. To optimize and automate our synthetic data generation porcess, we implemented LLM agent techniques [19, 42], creating agents four times, each time using one of the following models: GPT-40, Claude3, Gemini1.5, and Llama3.1-405B. We created three specialized agents: the Distillation Agent, the Solidity Developer Agent, and the Security Agent. Detailed prompt templates for these agents are presented in Figure 2. The roles of these agents are described as follows:\n\u2022 Distillation Agent: Generates output labels with rationales that justify these labels, focusing on identifying the type of vulnerability and providing explanations.\n\u2022 Developer Agent: Creates smart contract code that includes specific vulnerabilities.\n\u2022 Security Agent: Creates secure smart contract to address previously identified vulnerabilities. Secure contracts that are similar to vulnerable ones, but with subtle differences.\nThe workflow knowledge distillation can be divided into two parts: model specialization and adaptive data generation, as illustrated in Figure 3. Model specialization tailors the target models to focus on specific tasks. Adaptive data generation not only streamlines the workflow but also ensures continuous improvement.\nWe formalize the process as follows: Let $D = \\{x_i\\}_1^N$ denote a dataset where each $x_i$ is a contract code, $y_p$ a label, $r_p$ a rationale, and $s_p$ a scenario containing at least one vulnerability. Given an initial seed $x_i \\in D$, the Distillation Agent $AD_i$ first generates a JSON report with the corresponding label $y_p$ with rationale $r_p$. A scenario $s_p$ is selected from a predefined set of real-world application relevant to the smart contract domain. Next, we create a triplet $(y_p, r_p, s_p)$ and pass it to the Developer Agent $A_{de}$, which generates new vulnerable data $x_v$. The contract code $x_i$, the target label $y_p$, and the updated rationales $\\hat{r_p}$ are added as a new entry in the vulnerable dataset $S_v$. This vulnerable data $x_i$ is then passed to the Security Agent $A_{ds}$, which transforms the vulnerable contract code into secure contract code $x_c$ with the corresponding security label $y_c$. Finally, the vulnerable contract report and the secure contract report are combined into a synthetic training dataset $S_{v,c}$. This entire workflow is formalized in Algorithm 1.\nIn this study, the Developer Agent does not directly receive input labels for a specific reason: the target LLM operates as a black-box system, which restricts direct access to the complete list of vulnerability labels. Instead, we initiate the process by selecting various smart contracts as initial seeds in an effort to exhaustively cover the spectrum of potential vulnerabilities. This approach aims to push the boundaries of the model's capability in identifying diverse types of security flaws. Finally, we obtain a raw fine-tuning dataset containing more 2,000 entries.\n3.1.2 Dataset Preprocessing. The 2000 entries in the raw fine-tuning dataset are usually not immediately suitable for training. This is because data from different sources may have varying formats, structures, or encodings. Some content may be of low quality, contain errors, or be inappropriate. Raw datasets frequently contain duplicates that can bias the model. To address these issues, several preprocessing steps are typically performed, including data cleaning, formatting, content filtering, and de-duplication. Since raw datasets collected from various sources (e.g., web scrapes) may contain useless comments, non-textual elements, excessive white-space, and inconsistent line breaks, these elements can introduce noise and hinder effective learning. Human involvement may be necessary to clean this data, ensuring that only relevant and high-quality entry is retained.\nEach model is configured to operate with a specific template. Although pre-trained models like Llama2 excel at predicting the next token in a sequence, they require explicit instruction to generate useful responses. Prior literatures have shown that increasing the number of tasks in fine-tuning with instructions improves generalization to unseen tasks [29, 36]. Inspired by the Evol-Instruct method proposed in WizardLM's [38], we designed a data template to enhance instruction fine-tuning effectiveness (Figure 4). This template contains three components: instruction, input, and output, enhancing the model's ability to follow and execute commands.\nThe maximum context size is another constraint for model training. For instance, the Llama 2 model can only process up to 4,096 tokens. During preparation, we tokenize the dataset's content into manageable units and remove data exceeding this limit (4,096 tokens) to prevent input overflow. Additionally, redundant data, especially prevalent in smart contracts, can prolong training times and degrade model performance. To mitigate this issue, we employ the sentence embeddings model paraphrase-MiniLM-L6-v2 [27] for deduplication, which is widely applied in NLP tasks to find semantically similar sentences in large corpora. Setting a similarity threshold close to 0.9 allows us to maintain high similarity between inputs, effectively removing duplicates while preserving unique information.\n3.2 Training Process\nIn this study, we employed two primary techniques for fine-tuning models: SFT and QLORA.\n3.2.1 Supervised Fine-Tuning (SFT). SFT involves training models on a curated dataset consisting of instructions paired with appropriate responses. Unlike pre-training, which relies on vast amounts of general text data, SFT uses smaller, high-quality datasets specific to the desired task. During SFT, the model's weights are adjusted to minimize discrepancies between its outputs and the provided ground-truth responses, enhancing the model's ability to generate expected answers based on the training entries.\nSFT builds directly on the pretrained knowledge base, making it suitable for scenarios where specific outputs are desired. This technique fine-tunes the model's understanding by making small, targeted adjustments to its weights, refining its responses based on specific instructions or data encountered during the fine-tuning phase.\n3.2.2 Quantized Low-Rank Adaptation (QLoRA). We also implemented QLoRA for its efficiency and cost-effectiveness. QLoRA is a high-fidelity 4-bit fine-tuning method that drastically reduces memory usage, enabling the fine-tuning of a 65-billion parameter model on a single 48GB GPU. This approach conserves resources while retaining performance levels comparable to those of full fine-tuning and Low-Rank Adaptation (LoRA) [6, 12].\nPractitioners typically choose from three strategies for fine-tuning: full fine-tuning, LoRA, and QLoRA. Full fine-tuning involves retraining all or most of the model's parameters on the new dataset, while LoRA reduces complexity by altering a smaller, specific subset of parameters. QLoRA combines quantization and low-rank adaptation for efficient and effective model tuning. We selected QLoRA for its ability to maintain high performance while significantly reducing computational requirements. These advancements are crucial for deploying SOTA models in resource-constrained environments, although users should consider potential trade-offs in speed or adaptability depending on the specific application.\nBy integrating SFT and QLoRA into our methodology, we aimed to optimize the fine-tuning process, ensuring both high-quality performance and resource efficiency. This combination allows us to refine the model's responses based on specific instructions or data encountered during the fine-tuning phase while conserving computational resources.\n3.3 Continuous Learning\nIntroducing new knowledge to LLMs often introduces noise, which can reduce the accuracy of fine-tuned models. To address this, we have implemented a loss function method and an adaptive learning approach to enhance the quality of fine-tuned model.\n3.3.1 Supervised Loss Functions. As a single-task learning framework, FTSmartAudit faces a significant challenge of fine-tuning data quality, the fine-tuned model performance. Inspired by MFTCoder [22], we incorporate a new approach specifically designed to evaluate the fine-tuned models. This approach combines label prediction accuracy with a rationale-based explanation system. To avoid the model favoring labels with larger amounts of data, we introduce a weight assignment strategy during loss computation. This strategy supports two calculation schemes: one based on the number of labels and the other based on the number of valid rationale involved in the loss calculation.\nFormally, we denote the fine-tuning dataset as $D = \\{x_i, y_i, r_i\\}_1^N$, where each $x_i$ represents an input and $y_i$ is the corresponding target label with the associated rationale $r_i$. The fine-tuned model $f$ is trained to minimize the label prediction loss with cross-entropy loss for categorical outputs.\n$L_{label} = - \\frac{1}{N} \\sum_{i=1}^{N} log p(y_i | f(x_i))$ (1)\nwhere $p(y_i | f(x_i))$ is the probability of the model outputting the correct label given the input $x_i$.\nWhen computing rationale prediction loss, we employ a binary cross-entropy loss if rationales are binary (present/not present). The rationale prediction loss is computed as follows:\n$L_{rationale} = - \\frac{1}{N} \\sum_{i=1}^{N} [r_i \\log g(x_i) + (1 - r_i) \\log (1 - g(x_i))]$ (2)\nwhere $r_i$ represents the true label indicating the presence or absence of a rationale for the ith instance, and $g(x_i)$ is the model's prediction for rationale presence.\nWe integrate the label prediction loss and the rationale prediction loss using a dynamic weighting scheme based on the model's confidence in its predictions. The equation for the dynamic weighting scheme is as follows:\n$L = L_{label} + \\lambda L_{rationale}$ (3)\nwhere $\\lambda$ is a hyperparameter that controls the relative importance of the two losses.\n3.3.2 Model and Data Quality Enhancement. By incorporating these different loss functions, FTSmartAudit effectively addresses the quality of the data and the performance of the models involved, as illustrated in Figure 5. The process is organized into three primary patterns: initial filtering, iterative process enhancement, and validation.\nFormally, we denote the initial fine-tuning dataset as $D(0) = \\{x_i, y_i, r_i\\}_{i=1}^N$ where $x_i$ represents an input and $y_i$ is the corresponding desired output label with the corresponding rationale $r_i$. Besides, the primary pre-trained model set is denoted as $M_p = \\{m_i\\}_1^N$.\nThe first pattern of initial filtering involves selecting basic models from a variety of open-source LLMs. To broaden our scope of models, the label prediction loss, $L_{label}$, serves as the criterion for filtering these models The threshold is set as $L_b$. For each input $x_i$ processed by model $m_i$, the output $\\hat{y_i}$ is obtained and $L_{label}$ is computed. Models for which $L_{label}$ falls below $L_b$ are retained, forming a new model set $M_s$.\nThe second pattern focuses on the iterative enhancement of model performance and data quality, as detailed in Algorithm 2. To refine our model selection future and enhance the fine-tuning dataset quality, we utilize a dual-threshold loss function, $L$, characterized by two thresholds, $L_l$ and $L_h$ ($L_l < L_h$). The process is structured iteratively as follows:\n1. Initial Fine-Tuning: The fine-tuning dataset D(0) is used to fine-tune the selected models $M_s(0)$, resulting in the fine-tuned model set $M_{ft}(0)$.\n2. Loss Evaluation and Actions: For each model $m_j$ in $M_{ft}(0)$, outputs $\\hat{y_i}$ and $\\hat{r_i}$ are generated for each input $x_i$ from D(0). After all inputs completed, the combined loss $L_j$ for model $m_j$ is computed using the values $\\{\\hat{y_i}, \\hat{r_i}, y_i, r_i\\}_{i=1}^N$. If $L_j > L_h$, the model $m_j$ is deemed unsatisfactory and is removed from $M_s$. If $L_l \\leq L_j \\leq L_h$, the dataset D(k) is manually modified to improve its quality, resulting in D(k + 1). If $L_j < L_l$, the dataset and model set are considered satisfactory, and the process transitions to the validation phase.\n3. Iterative Fine-Tuning: The improved dataset D(1) is used for the next iteration of fine-tuning with the adjusted model set $M_s(1)$, producing $M_{ft}(1)$. The enhancement process similar to Step 2 is repeated.\n4. Cycle Continuation: This iterative cycle is repeated until the model set $M_{ft}(k)$ and the dataset D(k) meet the desired standards. The cycle typically concludes successfully within a few iterations, as indicated by the condition $L_j < L_l$ being met or a set maximum number of iterations being reached.\nFinally, the third pattern is validation, during which the satisfactory model set $M_{ft}(k)$ is subjected to further testing using a separate validation dataset, $D_{valid}$. This final step confirms the robustness and effectiveness of the models in realistic settings.\n3.3.2 Model and Data Quality Enhancement. By incorporating these different loss functions, FTSmartAudit effectively addresses the quality of the data and the performance of the models involved, as illustrated in Figure 5. The process is organized into three primary patterns: initial filtering, iterative process enhancement, and validation.\nFormally, we denote the initial fine-tuning dataset as $D(0) = \\{x_i, y_i, r_i\\}_{i=1}^N$ where $x_i$ represents an input and $y_i$ is the corresponding desired output label with the corresponding rationale $r_i$. Besides, the primary pre-trained model set is denoted as $M_p = \\{m_i\\}_1^N$.\nThe first pattern of initial filtering involves selecting basic models from a variety of open-source LLMs. To broaden our scope of models, the label prediction loss, $L_{label}$, serves as the criterion for filtering these models The threshold is set as $L_b$. For each input $x_i$ processed by model $m_i$, the output $\\hat{y_i}$ is obtained and $L_{label}$ is computed. Models for which $L_{label}$ falls below $L_b$ are retained, forming a new model set $M_s$.\nThe second pattern focuses on the iterative enhancement of model performance and data quality, as detailed in Algorithm 2. To refine our model selection future and enhance the fine-tuning dataset quality, we utilize a dual-threshold loss function, $L$, characterized by two thresholds, $L_l$ and $L_h$ ($L_l < L_h$). The process is structured iteratively as follows:\n1. Initial Fine-Tuning: The fine-tuning dataset D(0) is used to fine-tune the selected models $M_s(0)$, resulting in the fine-tuned model set $M_{ft}(0)$.\n2. Loss Evaluation and Actions: For each model $m_j$ in $M_{ft}(0)$, outputs $\\hat{y_i}$ and $\\hat{r_i}$ are generated for each input $x_i$ from D(0). After all inputs completed, the combined loss $L_j$ for model $m_j$ is computed using the values $\\{\\hat{y_i}, \\hat{r_i}, y_i, r_i\\}_{i=1}^N$. If $L_j > L_h$, the model $m_j$ is deemed unsatisfactory and is removed from $M_s$. If $L_l \\leq L_j \\leq L_h$, the dataset D(k) is manually modified to improve its quality, resulting in D(k + 1). If $L_j < L_l$, the dataset and model set are considered satisfactory, and the process transitions to the validation phase.\n3. Iterative Fine-Tuning: The improved dataset D(1) is used for the next iteration of fine-tuning with the adjusted model set $M_s(1)$, producing $M_{ft}(1)$. The enhancement process similar to Step 2 is repeated.\n4. Cycle Continuation: This iterative cycle is repeated until the model set $M_{ft}(k)$ and the dataset D(k) meet the desired standards. The cycle typically concludes successfully within a few iterations, as indicated by the condition $L_j < L_l$ being met or a set maximum number of iterations being reached."}, {"title": "4 TASK-SPECIFIC FINE-TUNING", "content": "This section explores the intricate process of fine-tuning LLMs for the specific task of smart contract auditing.\n4.1 Fine-tuning Dataset\nThe fine-tuning dataset provides a strong foundation for the model by exposing it to a wide variety of contract types and vulnerability instances. We create two fine-tuning datasets:"}, {"title": "5 EVALUATION", "content": "In this section", "questions": "n\u2022 RQ1: How does fine-tuning base models impact their performance in detecting detectable vulnerabilities in smart contracts? Given that many models", "RQ2": "How does the inclusion of secure smart contracts in the training dataset affect the performance of models in detecting vulnerabilities? While vulnerable contracts are essential for expanding the models' knowledge base", "RQ3": "How do fine-tuned models perform in terms of detectable and undetectable bugs? Complex logic bugs are considered some of the most severe security issues. We aim to evaluate the performance of LLMs in identifying both detectable and complex logic vulnerabilities", "RQ4": "How do fine-tuned models perform in detecting vulnerabilities in the large real-world dataset? Analyzing the performance of fine-tuned models on large real-world contracts provides insights into their practical functionality. This helps identify potential gaps between theoretical capabilities and practical effectiveness.\n\u2022 RQ5: Can our fine-tuned models discover new vulnerabilities that were previously missed in the audit reports from Code4rena? It is interesting to see if our fine-tuned models can find vulnerabilities that were missed in past Code4rena audit reports. This will help us understand if our models are more effective at identifying hidden issues.\nBy addressing these questions", "41": "vulnerabilities in smart contracts can be categorized as either machine-auditable or machine-unauditable. Their survey indicates that existing tools can detect machine-auditable vulnerabilities", "detectable vulnerabilities\" and these machine-unauditable vulnerabilities as \"undetectable vulnerabilities\". We utilize three validation datasets": "the Detectable dataset", "Dataset": "This dataset contains several detectable vulnerabilities that can be exploited by existing tools. We use the SmartBugs-curated dataset [1"}, {"Dataset": "Our Contest dataset is derived from the Code4rena contests [2"}, {"Dataset": "This dataset consists of actual contracts deployed on the Ethereum blockchain", "detection.\nhttps": "huggingface.co/datasets/weifar/LargeRealworldDataset\n10https://github.com/SunWeb3Sec/DeFiVulnLabs\n11ttps://github.com/SunWeb3Sec/DeFiHackLabs\n5.4 Experiment Results\n5.4.1 RQ1: Comparison on Detectable Dataset. To address RQ1, we present a comprehensive evaluation of base models , fine-tuned models (FTAudit-Vuln*), commercial models using the Detectable dataset. For a fair comparison, we also include the results of Slither-0.10.0 [23", "15": "well-known traditional vulnerability detecting tools for Solidty smart contracts. In this evaluation, we only count the TPs and recall (sensitivity to vulnerabilities), ignoring other metrics. We focus solely on whether the labeled bugs in Detectable dataset are identified, disregarding the unlabeled bugs. The results are presented in Table 2.\nThe results demonstrate that all fine-tuned models showed significant improvements over their counterparts. The most substantial improvement was observed in FTAudit-Vuln-Codegemma-7b, with a 48.9% increase in accuracy compared to Codegemma-7b-it. FTAudit-Vuln-Mistral-7b achieved the highest overall accuracy at 87"}]}