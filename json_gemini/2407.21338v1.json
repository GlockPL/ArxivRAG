{"title": "Image-Based Deep Reinforcement Learning with Intrinsically Motivated Stimuli: On the Execution of Complex Robotic Tasks", "authors": ["David Valencia", "Henry Williams", "Yuning Xing", "Trevor Gee", "Minas Liarokapis", "Bruce A. MacDonald"], "abstract": "Reinforcement Learning (RL) has been widely used to solve tasks where the environment consistently provides a dense reward value. However, in real-world scenarios, rewards can often be poorly defined or sparse. Auxiliary signals are indispensable for discovering efficient exploration strategies and aiding the learning process. In this work, inspired by intrinsic motivation theory, we postulate that the intrinsic stimuli of novelty and surprise can assist in improving exploration in complex, sparsely rewarded environments. We introduce a novel sample-efficient method able to learn directly from pixels, an image-based extension of TD3 with an autoencoder called NaSA-TD3. The experiments demonstrate that NaSA-TD3 is easy to train and an efficient method for tackling complex continuous-control robotic tasks, both in simulated environments and real-world settings. NaSA-TD3 outperforms existing state-of-the-art RL image-based methods in terms of final performance without requiring pre-trained models or human demonstrations.", "sections": [{"title": "I. INTRODUCTION", "content": "Motivation could be seen as an internal force or energy to encourage someone to execute or avoid a task. It is related to several internal or external factors, such as personality, own beliefs, culture, the surrounding environment, or earlier experiences. Motivation permits agents, organisms such as an animal or artificial organisms such as a robot, to play, explore, and discover new behaviours without external reward signals [1]\u2013[3]. Psychologists distinguish two groups of motivation, intrinsic and extrinsic [2], [4]. When you do something for its satisfaction, i.e. without any immediate obvious intention, it is considered intrinsic motivation [3], [5]. In contrast, extrinsic motivation is when you do something to obtain positive feedback associated with that behaviour and the environment. Inside the intrinsic motivation group, novelty and surprise play an essential role in the agent's learning process. They stimulate interest and exploration of new stimuli [6], [7]. As mentioned in [2], only novel, unexpected and challenging circumstances make an organism intrinsically motivated. Specialized literature [8]\u2013[11] claims that exploration of new environments enriches plasticity in the brain to stimulate learning. This phenomenon is easy to recognize in young human infants, who show significant preferences for situations or stimuli never experienced before, even when these stimuli do not provide an associated direct reward [12], [13].\nThe underlying idea is simple: a novel situation or a surprise emotion encourages the agent to explore and learn better solutions faster than an agent using only external reward signals [6], [7], [14]\u2013[16]. This is particularly im- portant in the real world, especially in robotics, where most basic tasks do not have an immediate external reward or are absent. Furthermore, not only are intrinsic and extrinsic reward signals required to learn to solve tasks, but rich and abundant information is also needed, information that can describe and provide key features of the environment and the task itself. This is particularly important in RL applications, where the state space representation plays a critical role during the learning process.\nLearning from images has proven to be a feasible choice in RL [17]\u2013[20]. Images encapsulate valuable information that may not always be readily available in other representations, such as proprioceptive states of the dynamics. Moreover, learning directly from pixels offers scalability for real-world applications. Utilizing cameras to gather environmental in- formation proves to be more accurate, cost-effective, and straightforward compared to other sensor types, especially in complex setups featuring high dynamics, such as robots with multiple degrees of freedom (DoF).\nNonetheless, learning directly from high-dimensional im- ages presents a considerable challenge. Most of the state-of- the-art RL algorithms struggle when the input state is pure pixels [17], as well as the computational cost increase [20]. Thus, having an accurate but simplified compact representa- tion of the image that the agent can use to learn a control policy might help reduce the complexity and cost. However, creating a compact and valuable representation of an image with all relevant information about the environment and the task from which to learn is a challenge [18].\nMapping images into a useful encoding usually demands a large training dataset, which is not a particular feature in RL. Ideally, mapping the images into a compressed representation and learning a control policy should co-occur and depend on each other. This interdependence resembles a chicken-and-egg dilemma: an optimal policy necessitates an effective representation, while an effective representation relies on meaningful gradient information derived from the policy. Therefore, achieving synergy between representation learning and policy optimization is paramount.\nThus, in this study, aiming to address the challenges of sparse reward environments and image-based learning representations, we introduce a novel RL algorithm called Novelty and Surprise Autoencoder-TD3, NaSA-TD3. We extend and adapt the TD3 framework proposed by [21]"}, {"title": "II. DEFINITIONS", "content": "Novelty and surprise are closely related to each other. Their definitions are often incorrectly interchanged or inter- preted in the same way; however, there are precise attributes that make them distinguishable [7], [22].\nIt is important to mention that there are several other de- tailed concepts relevant to motivation and intrinsic stimulus from a psychological perspective, along with extensive neu- roscience literature; however, they are beyond this article's scope, and we do not address them because it would put us in a category too far from our primary goal. Nevertheless, for more in-depth definitions, we encourage the reader to review [23] and [24]. This work uses the following definitions."}, {"title": "A. What is Novelty?", "content": "The Oxford English Dictionary defines novelty as \"the quality of being new, different, and interesting\" [25]. Psy- chologists, psychiatrists, and scientists differ in describing and measuring novelty depending if the stimuli are physical, visual, or behavioural [4]. In all cases, however, the common word is different [4]. We define novelty as something that we have not seen before; in other words, the process of rec- ognizing new stimuli that are distinguishable from anything known before. Detecting novelty involves digging into the contents of the agent's memory to associate or recognize if an experience has previously been seen or attended. Consequently, whether an observation is novel depends on whether a representation of that observation can be found, partial or identical, in the memory [3], [7]. For instance, in dexterous manipulation using a robotic gripper, a novel stimulus might manifest as a joint configuration or object state that the agent has not previously encountered."}, {"title": "B. What is Surprise?", "content": "A surprise could be defined as a discrepancy, presented as a feeling or emotion excited, between an expected ob- servation and an actual observation [7], [22]. As mentioned in [26], \"Surprise requires an internal world model that for- mulates an expectation about the future\". Measuring surprise involves a mechanism of prediction and comparison between expectation and actual observation. Besides, here we do not examine the current observation with past observations; i.e. surprise does not directly correlate with the memory, even though a predicted world model is built using previous experiences [7]. In dexterous manipulation with a robot gripper, a surprise stimulus can be conceptualized as the disparity between the expected position of the gripper and the target and the actual positions after taking an action. With surprise, time is important since prediction involves a specific time based on the current observation [7], while novelty can not be associated with predictions since we can not predict situations or events never been experienced [8], [10]."}, {"title": "III. RELATED WORK", "content": "The idea of using intrinsic motivation has been used in RL to encourage the agent's exploration. In low-domain environments, a common method for computing exploration bonus signals is having a tabular record of how often a state has been visited, i.e. a count-based strategy. Previous proposals using count-based approaches, such as [27]\u2013[34], have shown satisfactory results and demonstrated tackling the exploration problem. However, maintaining state-action visitation count is impractical, inefficient and expensive in complex tasks with continuous state spaces, making count- based approaches unscalable [12]. For example, it would be impractical in a robotics application to try to save all possible states when we deal with an infinite state space.\nTo avoid a tabular record of every visited state, [35] proposes EX2, a pseudo-count exploration method based on a discriminative exemplar model where the authors use a K-exemplar conditioned network. This approach, however, acts as a binary classifier that may tend to overfit due to the nature of the classifier itself. In [36], the authors present a method using self-organizing maps, a CNN autoencoder and a predictive ensemble model to estimate an intrinsic reward. This method was tested in a simple environment using a single actuator for a learning-to-reach task. While work by [37] presents a concept that uses an agent's ability to reconstruct observations by giving their \"contexts\" (a downsample or noise version of the input image) to generate an intrinsic bonus that is decomposed into fast and slow rewards signals. Due to the need to balance the importance of the intrinsic signals, this method can only be applied to on-policy RL methods. No experiments in real-world or continuous action spaces are presented. Learning without extrinsic rewards has also been examined in [38], where the novelty of a particular state is calculated as the distance of the current behaviour features to the k-nearest neighbours in a memory of behaviour features. However, the agent can end in \"procrastination\" on the main task instead.\nOn the other hand, [39] proposes a curiosity-based method to help the agent explore the environment more by generating an intrinsic signal based on an inverse dynamics model. In this proposal, the authors transform the observation input into a feature space with the relevant information and only consider the predictions that can affect the agent and ignore the rest. In [40], a goal-based curiosity method is presented, where the authors decompose a task into several easier sub-tasks (called goals) given an observation. The agent samples K goals and predicts the probability that each goal can be achieved from the current observation using a predictor network. The number of samples needed to train the policy is too high to scale this method to real-world setups.\nMost closely related to our method is the work presented in [41], which introduces a curiosity-based approach coupled with a comprehensive analysis of state representation learn- ing (SRL) to enhance the sample efficiency of image-based RL. In this study, two methods are proposed: one utilizing a regularized autoencoder (RAE), and the other employing a contrastive learning approach. However, this work differs from ours in that it does not integrate intrinsic signals as part of the reward; instead, a separate curious policy based on the SRL error is trained to foster interest in problematic states. This approach is applicable only to off-policy methods and has yet to be validated in real-world environments.\nDespite the promising results demonstrated in these stud- ies, intrinsic motivation remains underutilized as a potential solution in RL. While related works have illustrated that employing intrinsic stimuli as bonus rewards or additional policy promotes agent exploration, only a subset of these approaches are viable for real-world deployment due to limi- tations in sample efficiency or computational cost. Moreover, many related works have been evaluated in simplistic real-world environments or simulations, often relying on intri- cate calculations, predefined initial conditions, or pre-trained models [18]. Addressing the majority of these challenges, we present a scalable and robust method capable of tackling control tasks in continuous action spaces. Our algorithm leverages human-inspired stimuli of novelty and surprise to enhance RL learning directly from images. In comparison to traditional heuristic approaches, our method has the ability to solve complex tasks across both simulated and real-world environments."}, {"title": "IV. NASA-TD3", "content": "The goal of an agent in RL is to find the best action or decision for each state to maximize future rewards R; nevertheless, in some complex tasks, the reward R may not be present or is sparse, complicating the search for an optimal policy. However, by employing intrinsic motivation stimuli, i.e. augmenting the reward function to deliver a bonus for visiting interesting, novel or surprising states, we can encourage exploration and improve the policy in an environment where the reward is sparse. Thus, intrinsic stimuli encourage the agent to explore more than an agent using extrinsic reward signals only."}, {"title": "A. Novelty and Surprise Detection", "content": "Studies [12], [42], [43] have demonstrated that even after viewing a thousand pictures, each for a moment and single exposure, we humans can remember with high precision details we have seen in the picture. Recognizing whether we have seen an image is at some degree equivalent to detecting visual novelty, i.e. if we cannot remember seeing something, it can be considered a novel stimulus. As presented in [12], a familiarity memory system involves the medial temporal lobe, perirhinal cortex, and inputs from the visual system to learn and recognize objects without paying attention to details. Consequently, we do not need to look for identical aspects or details in each image. Inspired by this idea, we present a novelty detection method using an unsupervised representation learning with images and an Autoencoder (AE). We determine if an observation looks \u201cfamiliar\" to what we have seen previously, mimicking the human brain.\nAt each step, we pass the current agent's observation (an RGB image from the camera) into the AE. If the output of the AE, representing a reconstruction of the observation, deviates significantly from the original input, we infer that the observation is novel and has not been encountered previously. Although we do not explicitly compare the input image with every image stored in memory, we postulate that the AE inherently possesses a memory representation of all previously seen states within its internal layers. This implicit memory mechanism reduces computational costs and elim- inates the necessity of comparing each incoming state with the entire stored memory. To quantify novelty, we propose calculating the Structural Similarity Index (SSIM) between\n\\(R_{total}(s, a) = R_{ext} + R_{int}\\)   (1)\n\\(R_{int}(s, a) = \\alpha * R_{novel} + \\beta * R_{surprise}\\)   (2)\nwhere \\(R_{ext}\\) is the extrinsic reward from the environment, \\(R_{int}\\) is the intrinsic reward formed by \\(R_{novel}\\) and \\(R_{surprise}\\), which are functions designed to capture the novelty and surprise of a given state-action pair and \\(\\alpha\\) and \\(\\beta\\) are the weights to balance the degree each term. In our experiments, we fix \\(\\alpha = 1\\) and \\(\\beta = 1\\).\n\\(R_{novel}(s_t) = 1 - SSIM(s_t, Dec(Enc(s_t))\\)   (3)"}, {"title": "B. Image-based policy learning", "content": "Inspired by [17], [21], [48], we introduce our Auto- Encoder Actor-Critic Off-Policy method. Our approach builds upon TD3 [21] and adheres to its training dynamics. The selection of TD3 is motivated by its simplicity and stability during training, as well as its ability to mitigate overestimation bias through the utilization of two critic networks. However, what sets our proposal apart is the integration of an AE within the main networks. This addition empowers our method to directly process high-dimensional observations, enabling the agent to learn directly from them. The complete architecture, including the number of layers and activation functions, is illustrated in Fig 3.\nThe Actor-Critic network learns and evaluates a policy. This network receives as input the low-dimensional vector z from the encoder, which attempts to extract relevant features from the s observations at time t. Here, both the encoder and decoder are trained by minimizing the L2 reconstruction loss between the input observation and the reconstructed\nLikewise, we aim to encourage agent exploration by pro- viding a bonus signal for surprise events, i.e., to promote curiosity in unexpected situations. We measure surprise as the discrepancy between what is expected by the agent and what is actually obtained. In other words, the difference between the predicted observation and the true observation.\nAs mentioned earlier, surprise requires an internal world model; however, predicting and generating high-dimensional observations, e.g. images, can be difficult and expensive. In- stead, we use a model to predict the next encoded image, i.e. the latent representation. This simplifies the computational cost compared to the full image reconstruction, presenting another distinctive aspect of our approach compared to previous proposals.\nFrom this, \\(z_{t+1}\\) denotes the encoding of the image state \\(S_{t+1}\\), and \\(\\hat{z}_{t+1}\\) the predicted representation of the next encoding state given the previous encoding image state \\(z_t\\) and action \\(a_t\\) using a predictive model \\(P_p\\). The intrinsic reward signal for surprise is calculated by:\n\\(R_{surprise}(S_t, a_t) = MSE(z_{t+1}, \\hat{z}_{t+1})\\)   (4)\n\\(\\hat{Z}_{t+1} = P_p(z_t, a_t)\\)   (5)\nAdditionally, inspired by our previous work [45], we use an ensemble of predictive models. This simple but effective combination helps improve the predictions and keeps stabil- ity [46], [47] and makes the model less sensitive to noise, especially in the real world. Fig 2 illustrates the configuration of the predictive model.\nIt is important to underscore the distinctiveness in our approach regarding intrinsic signals and their respective computation methodologies. While previous studies, such as [41], commonly adopt a unified intrinsic signal strategy, our research clearly emphasises delineating between novelty and surprise, employing separate methodologies for their assessment."}, {"title": "C. Simulated Environments", "content": "Repeating the nominal action is a popular technique used in RL. Several proposals [17]\u2013[19], [49] claim that action repetition positively affects the learning process. However, suppose the number of repetitions is not set adequately. In that case, it can cause instability in the control dynamics and lead to sub-optimal behaviour [17], making choosing the correct number of repetitions a tuning hyperparameter that depends on each environment [18]. Likewise, repetition might not be suitable for real-world applications where servo-motors or linear actuators have fixed movements where action repetitions won't have any effect. Since our goal is an easily scalable algorithm capable of working in a wide range of applications, including real robots, we are not using this technique in our method; instead, we use novelty and surprise signals to obtain the same performance as the algorithms that use action repetition. More details of the training process as well as a list of hyperparameters, full source code and videos of each task can be found on the paper's website at: https://sites.google.com/ aucklanduni.ac.nz/nasa-td3-pytorch/"}, {"title": "V. EXPERIMENTAL RESULTS", "content": "We assess our algorithm's performance across diverse, complex continuous control robotics tasks in both virtual and real-world scenarios. Additionally, we conduct comparative analyses against SAC-AE [17], utilizing the official source- code implementation provided by the author 1. We choose challenging continuous control tasks of the DeepMind Con- trol Suite [50].\nThis test-bed platform provides complex environments for continuous control tasks with normalized actions and image state observations. We choose six tasks with different complexity and rewards; see Fig 5. A third-person camera image of size (84 \u00d7 84) pixels with three channels is used for all tasks. A stack of three consecutive full-colour images is used as the state observation. We run each environment for 1 \u00d7 106 training steps with the same hyperparameters for five independently seeded runs. All parameters are trained with the Adam optimizer, and we perform G = 5 (based on a trial error experimentation) gradient updates per environment step. We regularly evaluate the agent's performance in each environment during the training; after every 1 \u00d7 104 training steps, we compute the average reward over 10 episodes. During the evaluation, no gradient updates are performed."}, {"title": "A. Dexterous Robotics Manipulation in the Real World", "content": "To test the applicability and efficiency of our proposed method to solve real-world tasks, we describe an experiment involving a 4-DoF robot gripper [45]; see Fig 6. The gripper has two identical fingers equipped with Dynamixel XL-320 servomotors and a standard webcam (C270 Logitech with 720p) on top of the structure. The task requires turning a valve tap to an arbitrary target orientation. Despite the apparent simplicity, the dexterous manipulation task poses significant challenges due to its requirement for visual per- ception and physical coordination of 4-DoF. Specifically, the robot must learn to synchronize the movements of its two fingers to prevent cancellation of rotational movements, while also mastering a complex finger gait [45]. The resulting solution needs to account for real-world factors such as noise, control inaccuracies, and environmental conditions like lighting dynamics and friction. It is worth noting that we are conducting this task without the use of commercially expensive hardware platforms, and we aim to learn directly from raw images without modelling or system identification.\nThe extrinsic reward for this task is computed as the normalized negative L2-norm distance between the current valve's position and the desired angle. The intrinsic reward is calculated with the novelty and surprise strategy mentioned above. The horizontal episode is limited to 50 steps where we use an action space that outputs a four-element vector R4 with the desired position of each servomotor. We maintain the same image size as in the previous section to ensure con- sistency across experiments. Thus, the state space consists of the RGB images from the camera (840 \u00d7 640) downscaled to an image of size (84 \u00d7 84) pixels with three channels.\nThe robotic task has two scenarios at the end of the episode. In the first case, the valve orientation is left in place between episodes with no resetting or reinitialisation. In the second case, the valve orientation is reinitialised to a random angle (using a servomotor attached to the valve) once the episode ends. Previous related tasks rotate the valve to the same initial orientation at the end of each episode [51]\u2013[54]. Reinitialising the valve to any random orientation between [0, 2\u03c0) increases the complexity of the task as it requires the agent to learn to use the information from the images to perceive the current valve's orientation. We refrain from comparing this task with SAC-AE [17] due to the necessity of modifying its original code. Our primary objective is to showcase that our proposal is readily applicable to real-world applications. We train the model for a total of 5 \u00d7 104 environment steps, which amounts to approximately 11 hours of real-world training time. Five independently seeded runs are used to confirm the stability of our proposal. All hyperparameters used in this experiment and task-solved videos are listed on our website."}, {"title": "VI. RESULTS AND DISCUSSION", "content": "Fig. 7 shows the average evaluation reward comparing our NaSA-TD3 proposal, TD3-AE (which is our image- based extension of TD3 without any intrinsic rewards), SAC- AE [17] and Pixel-TD3 on the six continuous control tasks under identical conditions2. Utilizing the novelty and sur- prise signals as intrinsic rewards significantly enhances task performance, with instances of achieving maximum reward values (corresponding to a 1 \u00d7 103 max return) observed in certain environments. In contrast, some other image-based approaches fail to reach the maximum value or struggle to solve the task entirely, as exemplified by Pixel-TD3.\nIt is worth noting that AE-TD3's performance can match or improve SAC-AE in some environments. By further in- corporating novelty and surprise stimuli as intrinsic rewards, NaSA-TD3 outperforms all variations. Notably, our method does not require a complex tuning of hyperparameters or\ncomplicated training steps as other image-based RL algo- rithms. Nevertheless, in the spirit of transparency regarding the results, it is worth noting that the novelty and surprise signals do not significantly impact the final results when the task is not highly complex, or the extrinsic reward is not sparse. This can be seen more clearly in environments such as Walker or Finger Spin, where the agent performed similarly to SAC-AE. Presumably, this is because the intrinsic reward is not required, implying that the problem is easy enough to be solved with an extrinsic reward. In contrast, when the environment is complex or has sparse rewards, as in the Reacher or Cheetah environments, the intrinsic signals help significantly improve the performance.\nFig 8 shows the average evaluation reward for the real- world manipulation experiments. The results show the effect of novelty and surprise signals in executing the dexterous manipulation task. The superiority of our proposal over extrinsic-reward-only methods is even more evident. Our proposal solves the task and gets the highest reward in both tested scenarios. Pixel-TD3 failed to solve the tasks, while AE-TD3 struggled when the valve was reinitialised to a ran- dom orientation. These results underscore the effectiveness of intrinsic stimuli in encouraging the agent to explore a broader range of state-action pairs in complex environments, ultimately leading to faster and improved performance in real-world scenarios.\nOur proposal does have some limitations, with one of the most significant being the amount of RAM required. Utilizing pure images as input and storing them in a replay buffer can be computationally demanding, necessitating at least 64 GB of RAM to run a simulation environment for 1 million environment steps. However, this challenge can be mitigated by tuning the buffer size. Our experimentation revealed that other related proposals, such as [17] and [18], require similar or even greater amounts of RAM, as they often train models for more extensive training steps. Notably, this issue is not explicitly addressed in their papers. Further- more, despite our method demonstrating superior and faster convergence, the computational time for each environment step is higher compared to other methods such as SAC-AE [17]. This is primarily due to the additional gradient updates for the autoencoder and policy in each step, as well as the computation of SSIM and MSE for novelty and surprise values, which involve additional calculations and resources."}, {"title": "VII. CONCLUSION", "content": "In this work, we set out to investigate the use of intrinsic rewards as a mechanism for improving the performance of RL in the execution of complex tasks such as dexterous robotic manipulation. We formulated reward metrics inspired by novelty and surprise, using the predictability of patterns across image sequences. A key novelty was the use of Autoencoders, which reduced our system's computational and memory needs to support real-world computation. Our findings show the ability to learn directly from raw images in simulation and real-world robots. This opens the door to scaling this proposal to more complex scenarios and tasks where the only information needed can come from a camera. Likewise, we verify that when the reward is sparse, auxiliary intrinsic signals such as novelty and surprise can help to discover efficient exploration strategies and assist the learning process.\nFuture work will seek to include other intrinsic signals such as boredom, frustration, and pleasure. Furthermore, we will seek to improve the efficiency in the number of samples necessary, perhaps with the adaption of model-based rein- forcement learning algorithms, plus a more comprehensive analysis of other autoencoder architectures and their effect on policy learning."}]}