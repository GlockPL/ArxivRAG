{"title": "STORM: A Spatio-Temporal Factor Model Based on Dual Vector Quantized Variational Autoencoders for Financial Trading", "authors": ["Yilei Zhao", "Wentao Zhang", "Tingran Yang", "Wei Yang Bryan Lim", "Yong Jiang", "Fei Huang"], "abstract": "In financial trading, factor models are widely used to price assets and capture excess returns from mispricing. Recently, we have witnessed the rise of variational autoencoder-based latent factor models, which learn latent factors self-adaptively. While these models focus on modeling overall market conditions, they often fail to effectively capture the temporal patterns of individual stocks. Additionally, representing multiple factors as single values simplifies the model but limits its ability to capture complex relationships and dependencies. As a result, the learned factors are of low quality and lack diversity, reducing their effectiveness and robustness across different trading periods. To address these issues, we propose a Spatio-Temporal factOR Model based on dual vector quantized variational autoencoders, named STORM, which extracts features of stocks from temporal and spatial perspectives, then fuses and aligns these features at the fine-grained and semantic level, and represents the factors as multi-dimensional embeddings. The discrete code-books cluster similar factor embeddings, ensuring orthogonality and diversity, which helps distinguish between different factors and enables factor selection in financial trading. To show the performance of the proposed factor model, we apply it to two downstream experiments: portfolio management on two stock datasets and individual trading tasks on six specific stocks. The extensive experiments demonstrate STORM's flexibility in adapting to downstream tasks and superior performance over baseline models. Code is available in PyTorch\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "In financial trading, factor models are fundamental tools for asset pricing, widely used to predict asset returns. These models enhance the accuracy of asset pricing and risk management by identifying a set of key factors that explain excess returns. The well-known Fama-French three-factor model [6], for example, builds on the Capital Asset Pricing Model [24] by introducing additional factors to capture the risk premiums associated with different types of stocks in the market. As financial theories have evolved, researchers have continued to discover new factors, allowing for a more comprehensive understanding of market dynamics.\nRecently, we have witnessed the rise of latent factor models [5, 9, 33, 34], connecting the factor model with the generative model, the variational autoencoder (VAE). These VAE-based models describe high-dimensional data (prices) to low-dimensional representations (factors), and learn factors self-adaptively. Although latent factor models have demonstrated substantial success in financial trading tasks, they still face several significant issues:\n\u2022 CH1: Limited Reflection of Market Complexity. Latent factor models represent factors as single values are inherently constrained by their insufficient capacity to capture the intricate complexity and nonlinearity of financial data, rendering them vulnerable to noise and non-stationarity, which compromises their predictive accuracy and stability.\n\u2022 CH2: Factor Inefficiency. In addition to the potential limitations of factors discussed in CH1, factors learned through VAES exhibit three primary inefficiencies: i) Most latent factor models only focus on cross-sectional factors, neglecting the temporal perspective. ii) The continuous, multi-dimensional latent space may allow noise to overshadow meaningful factors. iii) The lack of independence among factors may lead to multicollinearity, with the homogeneity weakening the model's adaptability to varying market conditions and limiting the effectiveness of factors in downstream investment tasks.\n\u2022 CH3: Lack of Factor Selection. Existing latent factor models primarily focus on generating factors without adequately differentiating between them. Furthermore, they neglect the crucial process of factor selection, which is essential for identifying impactful factors, thereby limiting the model's overall effectiveness and precision.\nIn order to address the challenges, we propose a Spatio-Temporal factOR Model based on dual vector quantized variational autoencoders (VQ-VAE), named STORM, with the architecture shown in Figure 1. We represent the factors as multi-dimensional embeddings to capture the complexity and nonlinearity of financial data, thereby improving the factors' ability to account for factor returns (CH1). Additionally, we develop a dual VQ-VAE architecture to capture cross-sectional and time-series features, considering both spatial and temporal perspectives\u00b2. We integrate cross-sectional and time-series features at both fine-grained and semantic levels to construct effective factors. To further enhance the model, we introduce diversity loss and orthogonality loss to ensure factor embedding diversity and independence (CH2). Furthermore, we treat the embeddings in the codebook as cluster centers for factor embeddings, effectively using them as class tokens to distinguish between factors. This approach makes the factor differentiation and selection process clear and transparent (CH3). Specifically, our contributions are three-fold:\n\u2022 We design a dual VQ-VAE architecture to construct cross-sectional and time-series factors from both spatial and temporal perspectives.\n\u2022 We leverage vector quantization techniques to improve factor embedding representation by reducing noise, enhancing diversity, and ensuring orthogonality. This approach significantly strengthens the factors' predictive power and a more transparent factor selection process.\n\u2022 Experiments on two U.S. stock markets across the stock future return prediction task and two downstream tasks (i.e., portfolio management and algorithmic trading) show that STORM outperforms all baselines across 2 prediction metrics and 6 standard financial metrics."}, {"title": "2 RELATED WORK", "content": "Factor Model\nFactor model has attracted extensive research focus from the finance community. Financial experts have carefully selected a wide range of factors from macroeconomic [24], market changes [6], individual stock fundamentals [8], and other perspectives to price assets in an attempt to capture excess returns from mispricing. Recently, latent factor models have emerged that improve investment returns by studying latent factors self-adaptively. Specifically, [9] learns latent factors depend on assets characteristics, [5] learns cross-sectional factors by predicting future returns, and [34] learns market-augmented latent factors. However, these VAE-based latent factor models may suffer from a lack of robustness in representing low signal-to-noise ratio markets, leading to reduced reconstruction and generation capabilities. Additionally, their effectiveness is constrained by a primary focus on with insufficient attention to the temporal perspective.\nFinancial Trading\nFinancial trading has attracted significant attention from both the finance and AI communities. Quantitative traders leverage mathematical models and algorithms to automatically identify trading opportunities [27]. Within the Fin-tech field, research tasks include portfolio management (PM), algorithmic trading (AT), order execution [7], market making [26], etc.\nPM focuses on optimizing wealth allocation across assets [13]. Its development has evolved from simple rule-based methods [18] to prediction-based approaches using machine learning [11] and deep learning [19, 42] for predicting returns or price movements. Despite recent integration of reinforcement learning (RL) in PM [41], the latent factor models still rely on deep learning for market modeling and precise predictions.\nAT involves trading financial assets using algorithm-generated signals. Similar to PM, there are rule-based and supervised learning methods [36] in the AT task. In recent years, RL-based methods [28] have become dominant for their superiority in complex sequential decision-making.\nVector Quantized Variational Autoencoder\nVQ-VAE has made significant contributions across several areas of deep learning, particularly in representation learning, generative modeling, and time series analysis. Initially introduced by Van Den Oord et al., VQ-VAE has showcased its exceptional capabilities in processing high-dimensional data, e.g., images [21], audio [39], and video [35], with a strong emphasis on learning discrete latent representations. Moreover, VQ-VAE has proven highly effective in handling complex time-dependent data [29], advancing the development of robust generative models specifically in the context of temporal analysis."}, {"title": "3 PRELIMINARIES", "content": "Factor Model\nThe factor model (e.g., Arbitrage Pricing Theory [22]) is defined to study the relationship between expected asset returns and factor exposures, focusing on the cross-sectional differences in expected asset returns.\n$\u0395[R] = \u03b1_i + \u03b2_i\u03bb.$\nE[] denotes the expectation operator. E[R] is the expected excess return. $\u03b2_i$ is factor exposure (or factor loading) of asset i, and \u03bb is factor expected return (or factor risk premium). $\u03b1_i$ is pricing error.\nAccording to [5], they define the factor model in the spatial perspective, which only focuses on why the expected returns of different assets vary, rather than how the returns of each asset change over time. Therefore, we expand it from the temporal perspective and introduce the general functional form of the latent factor model:\n$y_{i,t} = \u03b1_{i,t} + \\sum_{k=1}^K \u03b2_{i,t}^k Z_{i,t}^k + \u03f5_{i,t}$"}, {"title": "3.2 Problem Formulation", "content": "To assess the effectiveness and generating ability of factors, we predict and evaluate the stock future returns. To show the adaptability of the latent factor model with multiple downstream tasks, we employ two financial trading tasks, i.e., portfolio management [32, 37, 41] and algorithmic trading [15, 28], to showcase the investment capabilities.\nObserved Data: we utilize the stock's historical price data p(i.e., open, high, low, and close) and technical indicators d as observed variables $x := [p,d] \u2208 R^{N\u00d7W\u00d7D}$, within a window size of W. The historical price data of stock i at time t is denoted as $p_{i,t} := [p_{i,t}^{op}, p_{i,t}^{hi}, p_{i,t}^{lo}, p_{i,t}^{cl}] \u2208 R^{D_1}$. The technical indicators which are calculated from price and volume values, are denoted as $d_{i,t} \u2208 R^{D_2}$. Each stock is represented by $D = D_1 + D_2$ features per trading day, which we then use to predict the stock's future returns, defined as $y_{i,t+1} := \\frac{P_{i,t+1}-P_{i,t}}{P_{i,t}}$.\nDownstream tasks:\n\u2022 Portfolio management task aims to construct a portfolio based on the predicted returns to test the profitability of the factor model. At each trading day t, we obtain the portfolio weight $w_t = [w_t^1, w_t^2, ..., w_t^{N-1}] \u2208 R^N$ for N stocks. Each stock i is assigned a weight $w_t^i$ representing its portfolio proportion, subject to the constraint that $\u03a3_1^N w_t^i = 1$ for full investment.\n\u2022 Algorithmic trading task aims to execute buy, hold, and sell actions based on predicted asset states to balance returns and risks. We formulate it as a Markov Decision Process (MDP) under reinforcement learning scenario following [3, 40]. The MDP is constructed by a 5-tuple (S, A, T, R, \u03b3), where S and A are sets of states and actions respectively, T : S \u00d7 A \u00d7 S \u2192 [0, 1] is the state transition function, R: S\u00d7A\u2192R is the reward function where R is a continuous set of possible rewards, and \u03b3\u2208 [0, 1) is the discount factor. The goal is to learn a policy \u03c0 : S \u2192 A that maximizes the expected discounted cumulative reward $\u0395[\u03a3_{t=0}^\u221e \u03b3^{t-1}r_t]$. At each trading day t for specific stock i, the agent takes action $a_{i,t} \u2208 A := \\{a_t^{by}, a_t^{ho}, a_t^{se}\\}$ which means buy, hold and sell, according to the current environment $s_t \u2208 S."}, {"title": "3.3 Vector Quantized Variational Autoencoder", "content": "VQ-VAE[30] is different from VAE [12] as it quantizes the observed data into a discrete token sequence. The VQ-VAE contains three parts, encoder, decoder, and discrete codebook, denoted by $b_{enc}$ with parameter $\u03b8_{enc}$, $b_{dec}$ with parameter $\u0398_{dec}$ and $C = \\{(k, e_k \u2208 R^D)\\}_{k=1}^{K-1}$ with parameter $\\{e_k \u2208 R^D\\}_{k=1}^{K-1}$ respectively. Among them, K is the size of the codebook, D is the dimension of the vector, and $e_k$ is the k-th vector of the latent embedding space. Given an input x, the encoder $b_{enc}$ firstly encodes it into a set of continuous feature vectors $Z = \u03a6_{enc} (x)$ and quantization operation q() passes each feature vector z \u2208 Z through the discretization bottleneck following by selecting the nearest neighbor embedding in the codebook C as its discrete code sequence $Qz$.\n$Qz = q(z, C) = c, where c = arg \\min_{k \u2208 [0,K-1]} ||z - e_k ||^2$.\nThen, the quantized feature for z denoted as $Z_q$, is obtained through $Z_q = e_c$. The quantized vectors $Z_q$ is fed into the decoder $\u0398_{dec}$ to reconstruct the original data $x' = \u0398_{dec} (Z_q)."}, {"title": "4 METHOD", "content": "In this section, we represent our proposed latent factor model STORM, with the overall structure of the VQ-VAE depicted in Figure 2. In general, STORM models data into time-series and cross-sectional features, deriving factors by integrating both temporal and spatial perspectives. Firstly, we introduce the dual VQ-VAE architecture and explain the extraction of features in two modules. Then the factor module will fuse and align features and generate factor embeddings. Finally, we demonstrate the effectiveness of STORM through comprehensive evaluations on two critical downstream tasks: portfolio management and algorithmic trading."}, {"title": "4.1 Dual VQ-VAE Structure", "content": "The dual VQ-VAE architecture processes stock data from spatial and temporal perspectives by patching the data in both modules. Then, the encoder extracts time-series (TS) and cross-sectional (CS) features, which are subsequently clustered using a learnable codebook for quantized representation. Finally, the decoder reconstructs the data, capturing complex patterns and enhancing forecasting performance.\nPatching. In stock data analysis, it is essential to extract features from both temporal and spatial dimensions. Time-series features capture the dynamics of a stock over time, while cross-sectional features reveal the correlations among different stocks. To achieve this, we partition data into patches [17] within the TS and CS modules. In the TS module, the observed data x is divided along the stock number dimension, with each patch containing data for one stock over p days. Similarly, in the CS module, the observed data x is divided along the time axis, with each patch containing features of all stocks for a single trading day. Combining information from multi-scale features allows the model to capture both temporal and spatial dependencies, enhancing the depth and accuracy of market insights.\nTS and CS Encoders. Considering the Transformer's [31] remarkable ability to capture both local and global dependencies via self-attention and patching strategies, it is particularly well-suited for long-term time-series forecasting [17]. Therefore, we employ stacked transformer blocks as the encoders in the TS and CS modules to capture complex time-series and cross-sectional patterns, ultimately improving the accuracy and robustness of feature extraction.\nCodebook Construction and Optimization. The construction of the codebook involves a learnable embedding space designed to quantize the encoded features into discrete representations [30]."}, {"title": "4.1.3 Codebook Construction and Optimization.", "content": "During training, the continuous latent features $z^{ts} (x)$ and $z^{cs} (x)$ are quantized into discrete space by mapping each vector to its nearest codebook entry, $z^{ts}_q (x)$ and $z^{cs}_q (x)$, based on minimizing the Euclidean distance. This process translates the encoded features into discrete tokens, enabling the model to leverage a finite set of vectors to capture complex patterns.\nThe VQ-VAE offers significant advantages in the latent factor model, primarily through its use of discrete codebook embeddings: i) Discretization Benefits: The discretization process inherent in VQ-VAE helps in clustering distinct and meaningful factors, providing orthogonality and diversity which improves upon the factors derived from traditional methods like FactorVAE. ii) Explicit Factor Selection: The discrete token indices facilitate an explicit factor selection process, identifying the most relevant factors that influence stock returns and thus enhancing prediction accuracy. iii) Noise Reduction: The factor selection process filters out irrelevant or redundant information, reducing noise and improving the overall robustness of the model.\nTo encourage equal usage of the codebook vectors [1], we incorporate a diversity loss to ensure balanced representations across the codebook. It is achieved by maximizing the entropy of the averaged softmax distribution over vectors for each codebook $\u00fe_g$:\n$L_{div} = -\\frac{1}{GK} \\sum_{g=1}^G \\sum_{k=1}^K p_{g,k} log p_{g,k}$\nwhere G represents the number of codebooks, set 2 in STORM. The diversity constraint helps enhance representational capacity, leading to better coverage of the input space.\nHowever, the lack of independence among factors can lead to multicollinearity, causing the model to overfit and are less robust to various market situations. To address this issue, we impose an orthogonality constraint on the codebook, as recent studies [25] have shown that enforcing orthogonality allows discretized codes to maintain translation equivariance. The orthogonality loss is:\n$L_{ortho} = \\frac{1}{2} ||\u03bb(e)^T \u03bb(e) \u2013 I_k||_F$,\nwhere $I_k \u2208 R^{K\u00d7K}$ is the identity matrix, e is a surrogate for two codebook embeddings ts and cs, \u03bb(e) denotes L2-normalized embeddings, and ||\u00b7||F is the Frobenius norm. The orthogonal constraint ensures that factors are independent of each other, allowing the effect of each factor on returns to be individually explained, which is crucial for asset pricing analysis."}, {"title": "4.2 TS and CS Decoders", "content": "The decoder is responsible for reconstructing the original data x from the quantized latent vectors $z^{ts}_q(x)$ and $z^{cs}_q(x)$. We utilize Transformer as decoders either, to generate constructed data $x'_{ts}$ and $x'_{cs}$, which aim to closely approximate the original input data. Finally, the encoders, decoders, and codebooks are jointly learned by minimizing the following loss objectives:\n$L_1 = \u03bb_{ortho}L_{ortho} + \u03bb_{div}L_{div} + ||x \u2212 x'_{ts}||^2 + ||x - x'_{cs}||^2+ ||sg[z^{cs}_q(x)] - z^{cs}(x)||^2+ ||sg[z^{ts}_q(x)] - z^{ts}(x)||^2+ ||sg[z^{cs}_q(x)] - z^{cs}_q(x)||^2+ ||sg[z^{ts}_q(x)] - z^{ts}_q(x)||^2,$\nwhere sg[] is a stop-gradient operator."}, {"title": "4.3 Factor Module", "content": "The factor module aims to integrate the TS and CS latent features through feature fusion and alignment. Then, through prior-posterior learning to generate factor embeddings from latent features to predict stock future returns.\nFeature Fusion and Alignment. As mentioned above we employ different patching methods to ensure that both time-series and cross-sectional features of the data were accurately captured. However, these methods result in two parallel processes that do not align at the feature level. To address this limitation, we employ a cross-attention mechanism to enhance the interaction and fusion of features at a fine-grained level, improving the model's dynamic understanding of the input data. Additionally, contrastive learning is used at the semantic level to enhance feature representation by emphasizing semantic similarities and differences.\nCross-Attention Mechanism. In order to fuse the features in the TS and CS modules at a fine-grained level, we leverage a multi-scale encoder to combine information between two patches and produce stronger fusion features for factor generations, motivated by CrossViT [2]. The encoder contains cross-attention layers that achieve both accuracy and efficiency.\nContrastive Learning. We incorporate contrastive learning, inspired by its success in aligning text and image features through a shared embedding space [14, 20], to enhance semantic consistency between time-series and cross-sectional features. In the factor module, contrastive loss ensures that similar features cluster together while dissimilar features are pushed apart in the embedding space."}, {"title": "4.3.2 Prior-Posterior Learning.", "content": "Through multi-scale encoder and contrastive learning layer, the CS and TS latent features are fused and aligned. In order to retain the codebook's categorical properties for factors, we add the codebook embeddings $z^{cs}_q(x)$ and $z^{ts}_q(x)$ as extra [CLS]-tokens [4] to the latent features $z^{ts} (x)$ and $z^{cs} (x)$, respectively. Then, two latent features concat together to get the latent factor embeddings $z_e (x)$.\nDue to stock market's inherent volatility and complexit, it is difficult to bridge the gap between the noisy market and effective factor model [5]. Therefore, we utilize the prior-posterior structure to predict future returns and optimize latent factor embeddings. In the training stage, the posterior layer predict posterior distribution of factor expected returns $z_{post}$ from the true future stock returns y and the latent factor embeddings $z_e (x)$:\n$[\u00b5_{post}, \u03c3_{post}] = \u03a6_{FE}(y, z_e(x)),$\nand $z_{post}$ follows the independent Gaussian distribution. In the inference stage, the prior layer only use latent factor embeddings without any future information leakage to calculate prior distribution of factor expected returns $z_{prior}$:\n$[\u00b5_{prior}, \u03c3_{prior}] = \u03a6_{FP}(z_e(x)).$\nThe return predictor uses factor expected return z to calculate future returns:\n$\u0177 = \u03b1 + \\sum_{k=1}^K \u03b2^* z^* + \u03f5$."}, {"title": "4.4 Downstream Tasks", "content": "Existing research on latent factor models [5, 34] primarily focuses on predicting future returns by analyzing cross-sectional characteristics of market stocks, which means the profitability of the factor model in quantitative trading is typically evaluated through portfolio management tasks. In contrast, STORM captures both the cross-sectional features among stocks and the time-series features of individual stocks, enabling it to excel in both PM and single-asset AT tasks.\nPortfolio Management. To evaluate the performance of the STORM and compare it with baseline methods, we follow the paradigm proposed by FactorVAE [5], focusing on return prediction and the PM task. Specifically, we utilize the factor decoder network to generate stock future returns \u0177, and then apply the TopK-Drop strategy\u00b3 to backtest the factor model. The TopK-Drop strategy constructs a daily portfolio consisting of top k stocks based on the return predictions. Considering the transaction costs in the real stock market, the strategy sets the turnover constraint, which ensures that more than k \u2212 d stocks overlap between the portfolios $w_t$ and $w_{t+1}$ on consecutive trading days.\nAlgorithmic trading. In STORM, $z_e (x)$ is the stock latent factor embeddings encoded by the dual VQ-VAE architecture, including both time-series and cross-sectional factor embeddings in the data. The latent factor embeddings Z are integrated into the observation set O = {Z, R}, where R is the reward function used to guide the agent's learning and decision-making in the environment. By combining latent factor embeddings with reward, the agent can more accurately understand the market environment and adjust its strategy based on observations. In the AT task, since it involves trading a single stock, the features of other stocks are excluded. We use the Proximal Policy Optimization (PPO) algorithm [23] to optimize the policy."}, {"title": "5 EXPERIMENT", "content": "In this section, we evaluate the proposed STORM on real stock markets and conduct extensive experiments to address the following research questions. RQ1: How to evaluate the effectiveness of STORM's learned factors? RQ2: How does STORM perform on downstream tasks? RQ3: How do the key components contribute to the performance of STORM?"}, {"title": "5.1 Experiment Settings", "content": "Dataset. We conduct experiments on two U.S. stock markets, SP500 and DJ30, using stock daily data that includes technical features based on Alpha158 [36]. Both datasets span 16 years, from 2008-04-01 to 2024-03-31, encompassing global conditions, e.g., the 2007-2008 financial crisis and COVID-19. Datasets are chronologically divided into non-overlapping training (from 2008-04-01 to 2021-03-31) and test (from 2021-04-01 to 2024-03-31) sets.\nMetrics. We compare STORM and baselines in terms of 6 financial metrics across the PM and AT tasks. The financial metrics include 2 profit criteria, Annualized Percentage Yield (APY) and Cumulative Wealth (CW), 2 risk-adjusted profit criteria, Calmar Ratio (CR) and Annualized Sharpe Ratio (ASR), and 2 risk criteria, Maximum Drawdown (MDD) and Annualized Volatility (AVO). The calculation formulas and meanings of these metrics are as follows:\n\u2022 CW is the total returns yielded from a portfolio strategy: $CW_T = \u03a0_{i=1}^T (1 + r_i)$, where $r_i$ is the net return.\n\u2022 APY measures the average wealth increment that one portfolio strategy could achieve compounded in a year, which is defined as $APY_T = CW_T^{\\frac{1}{y}} \u2212 1$, where y is the number of years corresponding to T trading rounds.\n\u2022 MDD measures the largest loss from a historical peak in the cumulative wealth to show the worst case, which is defined as: $MDD = \\max_{i=0} Pi - Ri, where Ri = \\frac{Pi - \\min_{i=1} Vi}{Vi-1}  \\text{and} P_i = \\prod_{i=1}^Vi$.\n\u2022 AVO is the annualized standard deviation of daily returns and multiplied by \u221aAT, where AT is the average trading rounds of annual trading days and AT = 252 for all the datasets.\n\u2022 ASR is an annualized volatility risk-adjusted return defined as $ASR = \\frac{APY-R_f}{AVO}$, where $R_f$ is the risk-free return.\n\u2022 CR measures the drawdown risk-adjusted return of a portfolio calculated as $CR = \\frac{APY}{MDD}$\nTypically, to evaluate the effectiveness of the learned factors in the PM task, we adopt the Rank Information Coefficient (RankIC) and the Information Ratio of RankIC (RankICIR):\n\u2022 RankIC is a ranking metric in finance, which measures the correlation between the predicted rankings and the actual returns. It is defined as:\n$RankIC_s = \\frac{\\frac{1}{N}(r_{\u0177s} \u2013 mean(r_{\u0177s}))^T (r_{ys} \u2013 mean(r_{ys}))}{std(r_{\u0177s}) std(r_{ys})}$,\n$RankIC = \\frac{1}{T_{test}} \u03a3_{s=1}^{T_{test}}RankIC_s,$\nwhere Ttest is the number of trading days of the test range, $r_{ys}$ and $r_{\u0177s}$ are the true and predicted ranks of stocks on the trading day s.\n\u2022 RankICIR is the information ratio of RankIC, which measures the stability of prediction, $RankICIR = \\frac{mean (RankIC_s)}{std (RankIC_s)}$.\nBaselines. We compare STORM with nine methods across one prediction task and two downstream tasks. The baseline methods can be divided into three categories: ML & DL-based models, Factor models, and RL-based models. In the prediction task and PM task, we compare STORM with ML & DL-based and factor models. In the AT task, we compare STORM with ML & DL-based and RL-based methods. We use the labels P, PM, and AT to indicate the applicable task scope for the following baselines. The following will provide a brief introduction to each method:\n\u2022 Market\nBuy-and-Hold (B&H) involves holding assets for an extended period, regardless of short-term market fluctuations, assuming that long-term returns will be more favorable. (P, PM, AT)\n\u2022 ML&DL-based\nLGBM [36] uses a series of tree models to predict price fluctuations and provide buy and sell signals. (P, PM)\n\u2022 LSTM [38] utilizes long short-term memory to improve the accuracy of price predictions. (P, PM)\n\u2022 Transformer [36] models leverage self-attention mechanisms to enhance the precision of price forecasts. (P, PM)\n\u2022 RL-based\nSAC [10] is an off-policy actor-critic algorithm that optimizes trading strategies using entropy regularization and soft value functions in continuous action spaces. (PM, AT)\nPPO [23] updates trading policies iteratively to balance exploration and exploitation, ensuring stability and sample efficiency. (PM, AT)\nDQN [16] uses deep neural networks to approximate the action-value function and make trading decisions from market data. (PM, AT)\n\u2022 Factor-model\nCAFactor is a state-of-the-art model that learns latent factors depending on asset characteristics. (P, PM)\nFactorVAE [5] is a state-of-the-art VAE-based latent factor model that learns optimal latent stock factors. (P, PM)\nHireVAE [34] is a state-of-the-art VAE-based latent factor model that identifies factors between market situation and stock-wise. (P, PM)"}, {"title": "5.2 Implement Details", "content": "We implement all experiments by using PyTorch and conduct the experiments on two 80GB memory NVIDIA RTX H100 GPUs. The code is provided in the repository \u2074. The input data is organized into batches with dimensions (B, W, N, D), where B = 16 denotes the batch size. N represents the number of stocks, 28 for the DJ30 index and 408 for the SP500 index. The dimension W = 64 corresponds to the number of historical data days, and D = 152 represents the total number of features, including the OHLC data, technical indicators, and temporal information.\nFor the time-series module, we use an 8-day period for each stock as a patch, meaning the patch size is (8, 1, D). In contrast, for the cross-sectional module, we use N stocks for each day as a patch, so the patch size is (1, N, D). For both the encoders and decoders, we employ stacked transformer blocks. Each encoder consists of 4 layers of blocks with 4 attention heads, while each decoder consists of 2 layers of blocks with 8 attention heads. The size of both codebooks is selected from the set {256, 512, 1024}, with 512 yielding the best results. A consistent embedding dimension of 256 is employed across the encoders, decoders, and codebooks. For the optimizer, we use AdamW with a learning rate of 1\u00d710\u207b\u2074 and a weight decay of 0.05. The learning rate starts at 0, ramps up to 1 \u00d7 10\u207b\u2074 over 100 epochs with a linear warmup scheduler, and then decays to 0 over 1000 epochs. For the loss function balancing, the coefficients for the clip loss, as well as the price reconstruction losses for both the TS and CS modules, are all set to 1 \u00d7 10\u207b\u00b3. The commitment loss coefficient for the codebooks is 1.0, while the coefficients for the orthogonality loss and diversity loss are both set to 0.1. Additionally, the coefficient for the price prediction loss of the factor module, as well as the KL divergence loss between the posterior and prior distributions, is also set to 0.1.\nFor downstream tasks, we employ the TopK-Drop rule strategy for portfolio management, using the recommended parameters from Qlib, where k = 5 and d = 3. For the trading task, we adopt the Proximal Policy Optimization (PPO) RL strategy. The CS and TS factors from the pretraining phase are used as the state. Notably, we retain all CS factors, which represent market characteristics, while only preserving the TS factors relevant to the currently traded stocks, reflecting stock-specific characteristics. The action space has a dimension of 3, corresponding to the choices of buy, hold, and sell. Our reward calculation considers value to account for scenarios where remaining cash may not cover the purchase of a new stock. Relying solely on price changes without considering position and cash factors is unreasonable. Value is defined as v = pxm+cwhere p is the current price, m is the position, and c is the remaining cash. The reward is calculated as r = Upost-Upre. Unlike compound interest, our method measures the rate of change from the initial value to the current value. For both downstream tasks, we start with an initial cash of 1 \u00d7 10\u2076, while also accounting for a transaction cost ratio of 1 \u00d7 10\u207b\u2074.\nFor the pretraining prediction tasks and the downstream portfolio management tasks, each set of experiments is conducted 10 times with different random seeds, and the mean and variance are reported. For the trading tasks, each set of experiments is conducted 5 times with different random seeds, and the mean of the evaluation metrics is reported. Notably, the default parameters provided by the official implementations are used for baseline models, ensuring a consistent and fair comparison across different approaches."}, {"title": "5.3 Performance on Factor Quality (for RQ1)", "content": "To demonstrate the effectiveness and quality of the factors, we designed a stock future return prediction task to showcase the factors' predictive ability and robustness to capture dynamic market changes. In the prediction task, STORM shows substantial improvements over six baseline methods across the SP500 and DJ30 datasets. Specifically, STORM achieves an average improvement of 16.105% on SP500 and 16.399% on DJ30, indicating the learned latent factors are significantly effective in capturing stock price trends.\nThe diversity of factors is crucial in financial investment, as it ensures the model captures the multi-dimensional characteristics of the market, avoiding redundancy or over-reliance on a single factor. In the codebook, each embedding represents a cluster center for a category of factor embeddings. To analyze the utilization rate of each factor category, we report the frequency of codebook embedding indices in the testing data. For clarity, we group every five indices into one interval, as shown in Figure 3(a). The results indicate that the distribution of factor usage is relatively even, demonstrating that the model has achieved good diversity in the factor selection process.\nWe experiment with different codebook sizes on the DJ30 dataset to find the optimal balance between factor diversity and representation quality. If the codebook is too large, it may lead to sparse factor representation; if too small, it may result in insufficient factor differentiation and information redundancy. As shown in Figure 3(b), codebook size of 512 achieves optimal factor prediction performance."}, {"title": "5.4 Performance on downstream tasks (for RQ2)", "content": "We use three types of metrics: profit, risk-adjusted profit, and risk to evaluate STORM's investment performance on downstream tasks, i.e., portfolio management and algorithmic trading. For profitability, STORM outperforms all the baselines, with average improvements of 106.10% on APY over the best-performing baseline in the PM task and 10.45% over the best baseline methods on three stocks in the AT task. For risk resistance, despite the test periods for both datasets, including the COVID-19 event, STORM still achieves the best AVO value, surpassing all comparison methods in the PM task. Moreover, considering the return and risk simultaneously, STORM performs the best on ASR with an average improvement of 58.64% over the best baseline result in the PM task, and surpasses the best baseline results across three stocks in"}]}