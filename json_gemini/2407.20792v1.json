{"title": "How Novice Programmers Use and Experience ChatGPT when Solving Programming Exercises in an Introductory Course", "authors": ["Andreas Scholl", "Natalie Kiesler"], "abstract": "This research paper contributes to the computing education research community's understanding of Generative AI (GenAI) in the context of introductory programming, and specifically, how students utilize related tools, such as ChatGPT. An increased understanding of students' use is mandatory for educators and higher education institutions, as GenAI is here to stay, and its performance is likely to improve rapidly in the near future. Learning about students' use patterns is not only crucial to support their learning, but to develop adequate forms of instruction and assessment. With the rapid advancement of AI, its broad availability, and ubiquitous presence in educational environments, elaborating how AI can enhance learning experiences, especially in courses such as introductory programming is important. To date, most studies have focused on the educator's perspective on GenAI, its performance, characteristics, and limitations. However, the student perspective, and how they actually use GenAI tools in course contexts, has not been subject to a great number of studies. Therefore, this study is guided by the following research questions: (1) What do students report on their use pattern of ChatGPT in the context of introductory programming exercises? and (2) How do students perceive ChatGPT in the context of introductory programming exercises? To address these questions, computing students at a large German university were asked to solve programming tasks with the assistance of ChatGPT as part of their introductory programming course. Students (n=298) provided information regarding the use of ChatGPT, and their evaluation of the tool via an online survey. This research provides a comprehensive evaluation of ChatGPT-3.5's application by novice programmers in a higher education context. The findings reveal that while students widely adopt GenAI, their use varies significantly, ranging from acceptance of generated solutions to dynamic, and critical engagement. Therefore, this work has implications for educators designing guardrails or forms of instructions on the use of GenAI tools in the classroom.", "sections": [{"title": "I. INTRODUCTION", "content": "While educators and researchers are still trying to grasp the full impact of Generative Artificial Intelligence (GenAI) and related tools on our educational system(s), students are quickly moving forward, and open to experimenting with Copilot, and ChatGPT [1]\u2013[3]. Even though it is challenging to predict which programming competencies [4] students will require in the future and how to adapt curricula respectively [5], [6], computing educators need to gain an understanding of how students currently apply GenAI tools.\nThere is no doubt that GenAI tools, such as ChatGPT, are powerful and performing well in the context of computing, and particularly in introductory programming courses. An increasing body of research has been dedicated to showing its potential in solving CS1 and CS2 assignments [7]\u2013[9]. Another use case for students is the enhancement of programming error messages [10]\u2013[13], and the generation of formative feedback [14]\u2013[18]. Generating personalized feedback can potentially benefit both students, and educators, especially in large scale classes, such as introductory programming [19], [20]. At the same time, GenAI tools still produce misleading information for novice learners [15].\nAt this point, however, hypothesizing about students' use cases is a debate mostly led by educators [21]\u2013[23], as they are reflecting and discussing the alignment of their learning objectives, instruction, learning activities and assessments in the GenAI era. So far, few studies have investigated GenAI tools from the student perspective (e.g., trust or attitude towards LLMs [24], [25]), or how students use such tools in the context of a computing course [26], [27]. Due to the broad availability of GenAI tools, and their well-known limitations, it is crucial to investigate how and for which purposes computing students use such tools, and how they perceive the benefits and limitations of GenAI.\nIt is thus the goal of this research to gather the student perspective on using GenAI tools, such as ChatGPT. This perspective encompasses use pattern, but also students' re- flections upon their experiences with ChatGPT in the context of a curricular course. To gather the student perspective, we developed an exercise sheet as part of an introductory programming course at a large, public university (Goethe University Frankfurt, Germany) in the winter term 2023/24. Students were asked to solve programming tasks with the assistance of ChatGPT-3.5 (freely available in Dec. 2023), and to participate in an online survey to report their experiences. The quantitative and qualitative analysis of the survey"}, {"title": "II. RELATED WORK", "content": "GenAI and related tools have taken the world by storm [3]. Due to the broad availability of GenAI tools, starting with the launch of OpenAI's ChatGPT in late November 2022, computing educators are discussing implications for higher education institutions, curricula, learning objectives, assign- ments, and assessments ever since [3], [5], [28]. In this section, we summarize benefits and limitations of GenAI tools in introductory programming. Moreover, we present recent findings on computing students' perspective on GenAI tools."}, {"title": "A. Potential Benefits and Limitations of GenAI in Introductory Programming Contexts", "content": "The performance of GenAI tools can be impressive, espe- cially in introductory programming exercises and respective exams [29], [30]. Several studies showed the capabilities of tools based on Large Language Models (LLMs), such as Copi- lot and ChatGPT [7]\u2013[9]. For example, Geng et al. [7] treated ChatGPT as a student participating in an introductory-level functional language programming course. The tool achieved a grade B and was ranked as 155 out of 314 students. Kiesler and Schiffner [8] had ChatGPT 3.5 and GPT-4 generate solutions to 72 standard introductory programming tasks available on CodingBat [31]. ChatGPT-3.5 immediately solved the task correctly in 69 cases (95,8%), GPT-4 in 68 tasks (94,4%) [8]. Savelka et al. [9] showed that GPT models are more successful when prompted in natural language, or when asked to fill in blanks. Multiple-choice questions requiring the analysis or rea- soning about code were more challenging for the models [9].\nDespite these promising results, many studies have reported limitations of GenAI tools, such as (re-)producing biases or inaccurate information [32]. Another obvious problem is its knowledge cutoff so the tools lack recent data, e.g., on the latest Python version, which may cause irritations. Hallucina- tions within the generated data, and the fact that it can bypass plagiarism detection tools are additional issues for students and educators. As GenAI tools challenge the integrity of academic work, educational institutions and faculty need to take action on several levels [3], [33]. Prather et al. [3] emphasize the need for ongoing research to keep up with the fast pace of technological developments.\nIn the context of introductory programming education, GenAI tools have been subject to research w.r.t. their ap- plication by educators and students [11], with a focus on students' potential use [10], [12], [15], [17], [34]\u2013[36]. One of these use cases is the generation of code explanations in a web software development e-book [12]. MacNeil et al. [12] analyzed students' perceptions towards automatically generated line-by-line code explanations by Codex and GPT- 3. The majority of students evaluated as the explanations as helpful [12]. Leinonen et al. [10] found that programming error messages (i.e., by the compiler and interpreter) can be enhanced by GenAI, as they can help create novice-friendly, actionable explanations.\nGenAI tools were also shown to analyze students' code, and fix errors [35], [36]. Phung et al. [35] studied the application of LLMs with the goal of fixing syntax errors in Python. They developed a technique to receive high precision feedback [35]. In other studies, it was shown that GenAI tools are capable of generating individual, elaborate feedback [37] to students' solutions to programming exercises [15]\u2013[17].\nFor example, Kiesler et al. [15] qualitatively analyzed the feedback generated by ChatGPT-3.5 in response to authentic student solutions to introductory programming exercises. In their study, they identified stylistic elements, textual explana- tions of the cause of errors and their fix, illustrating examples, meta-cognitive and motivational elements. However, ChatGPT generated misleading information, claimed uncertainty in its responses, and requested more information, thereby indicating its limitations, especially for novice learners [15].\nSimilarly, Azaiz et al. [16] analyzed the output generated by ChatGPT in response to introductory programming exercises and student solutions. They summarize a number of chal- lenges, e.g., w.r.t. the formatting of the output, recognizing correct solutions, and hallucinations of errors in the students' code. In a follow-up study with GPT-4 Turbo, Azaiz et al. [17] conclude notable improvements of the generated feedback, as the outputs were more structured, consistent, and always per- sonalized. Due to this potential, we currently see the develop- ment of new educational tools and environments incorporating GenAI tools to create novice-friendly code explanations [38], or to not give the solution away [39]."}, {"title": "B. Computing Students' Perspective on GenAI tools", "content": "All of the aforementioned studies focused on students' potential use of GenAI tools, and thus hypothetical appli- cation scenarios. Some of them used authentic student data (e.g., students' solutions) as an input to GenAI tools [15], [17]. However, few studies directly involved students, their perspective, or their actual use of GenAI.\nIn a study by Prather et al. [40], students' use of GitHub Copilot as part of an introductory programming assignment was evaluated through observations and interviews. They asked students about their perceptions of the tool's usability, challenges and benefits, and implications for the tool's design. Vaithilingam et al. [41] also explored the usability of Copilot via user study with 24 participants. The participants appreci- ated the ability of Copilot to provide a useful starting point, and help find information (instead of having to search for it online). At the same time, programmers reported challenges related to understanding, editing, and debugging code snippets"}, {"title": "III. METHODOLOGY", "content": "In this section, we outline the research questions and goals guiding the present study. In addition, we introduce the survey design, the context of the data collection within an introductory programming course, and present the data analysis method."}, {"title": "A. Research Questions and Goals", "content": "The present study has the goal to gather the perspective of novice programmers using ChatGPT in the context of introduc- tory programming exercises. Specifically, we are addressing the following research questions (RQs):\n1. What do students report on their use patterns of ChatGPT in the context of introductory programming exercises?\n2. How do students perceive ChatGPT in the context of introductory programming exercises?\nTo answer these questions, the study leverages empirical data collected in a survey with computing students (n=298) enrolled in an introductory programming course at a large German university. Before completing the survey, students were asked to complete a set of programming exercises with the assistance of ChatGPT-3.5."}, {"title": "B. Survey Development", "content": "The survey design was guided by the RQs and informed as far as possible by prior work. Most importantly, we wanted to gather students' use patterns when working on the exercises as part of the course (RQ1). Therefore, we asked students how often, how long and for which purposes they use ChatGPT in the problem solving process. For the latter we used some of the application scenarios identified in related work (e.g., [3], [10], [12], [16], [35], [41]).\nSimilarly, the questions focusing on students' perceptions of ChatGPT (RQ2) were informed by prior work, for example,"}, {"title": "C. Course Context of the Data Collection", "content": "The present study was conducted within an introductory programming course for first-year computing students at Goethe University, Frankfurt. 790 students were enrolled in that course in the winter term 2023/24. Most students were enrolled in the computer science (CS) program, even though some students pursue CS as a minor. The course is dedicated to novice learners of programming, as there are no prerequisites to participate. It is accompanied by an online course in the university's learning management system Moodle.\nThe class consists of a 2-hour lecture per week for all students, and a 2-hour tutorial session in groups of 20-30 students. In the tutorial, students are supposed to solve weekly or bi-weekly exercise sheets with programming problems. Even though this is voluntary, students can collect points for correct submissions (two for the exercises required for this study). These points can later be used to improve exam grades.\nFor this study, a new exercise sheet was developed for one of the tutorial sessions. Students were asked to individually work on these exercises for two weeks from December 6 onwards. Group work was not allowed. Students were instructed to \u201ccomplete the tasks using ChatGPT via the free version\u201d (3.5) on the web interface, and to submit \u201call prompts and responses\" as paired entries in a spreadsheet via the Moodle course. Students were not instructed on how to use ChatGPT- 3.5. They were only provided with a link to OpenAI's guide on prompt engineering [47]. This approach was used to avoid influencing students' use of the tool. They had only been introduced to the study's objective and the process during the lecture preceding the tutorial."}, {"title": "D. Task Design", "content": "The new exercise sheet addressed several concepts, e.g., recursion, functions, lists, conditionals, string manipulation, and documentation. It comprised two tasks with sub-tasks. Task 1 consisted of four sub-tasks. It presented students with code snippets with recursive elements for several operations:\n1a summation of the digits of a number,\n1b reversing a list,\n1c performing multiplication, and\n1d computing the Ackermann function.\nThe task was to read and interpret the given code snippets with the goal of determining the output of the code, the number of function calls, and identifying the type of recursion.\nTask 2 was designed to be more complex. It required the implementation of a function that determines the number of \"happy strings\" within all sub-strings of a given string. A \"happy string\" was defined as a string that can either be rearranged into (or already is) a repetition of some string. In particular, students were asked to:\n2a test a given string for the \"happy\" property, and\n2b test all possible sub-strings of the given string.\nUsing the recursive approach was awarded with extra points.\nAn important criteria for the selected tasks was the per- formance of ChatGPT-3.5 in solving them. For example, ChatGPT demonstrated a high success rate in identifying the correct functions for the first task. However, it frequently failed in calculating the number of calls, as it often overlooked the base case. Task 2 proved to be even more challenging, and ChatGPT-3.5 never generated a correct solution in 10 regen- eration attempts. It was only able to describe the necessary steps to solve the problem."}, {"title": "E. Data Analysis", "content": "To evaluate students' self-reported use patterns and per- ception of ChatGPT-3.5 in the context of solving selected programming exercises, 298 valid survey responses were an- alyzed. A quantitative approach was used to evaluate closed questions (Q1-Q11), while responses to open questions were qualitatively analyzed (Q12-Q14).\nThe survey addresses RQ1 (students' use patterns) within Q1-Q6, which are all closed questions. The quantitative anal- ysis comprises students' programming experience prior to the introductory course in years, whether they used ChatGPT before this exercise, usage frequencies and duration, how Chat- GPT was usually accessed, and for which tasks or problems it was consulted (all in the context of solving the exercise sheet).\nRQ2 on students' perceptions and user experience is ad- dressed by survey questions Q7-Q14. Questions 7-11 refer to the ease of use of ChatGPT and other adoption criteria. The evaluation is based on the responses on the 5-point-Likert scale to indicate students' distribution and the degree to which they agree, or were satisfied. The remaining open questions (Q12-14) were qualitatively analyzed by applying a content analysis [48] of the responses. As each response contained multiple meaningful elements (mostly 3, as requested by Q12 and 13), each positive or negative aspect was coded once. Each student's full response to the respective open survey questions was used as context unit.\nDeductive categories reflecting students' perspective as shown in related work (see Section II) constituted the starting point for building a category system (e.g., ease of use, code explanations, use as study buddy, or for debugging). Due to the large number of responses to the open questions, and the variety of aspects, new inductive categories were built by using the psychology of text processing [49], [50]. Thus, the responses were summarized, paraphrased and abstracted so"}, {"title": "IV. RESULTS", "content": "This section presents the students' responses to the survey questions (available online [51]) with the goal of answering both RQs. Students' reported use patterns of ChatGPT (RQ1) will be summarized, before introducing their perceptions of ChatGPT in the context of introductory programming exercises (RQ2). As RQ2 was addressed by closed and open questions in the survey, we outline quantitative and qualitative results."}, {"title": "A. Students' reports on their use patterns of ChatGPT (RQ1)", "content": "Students reported several insights into their use patterns of ChatGPT when solving the programming exercises for the introductory course. The analysis of survey questions Q1 to Q6 (n=298 each) provides a comprehensive snapshot as an answer to RQ1.\nFirst of all, it should be noted that the surveyed students were indeed mostly programming novices (Q1), with 34% having no programming experience, and 43% having limited experience of less than one year. Another 17% of the students reported to have one to two years of programming experience before enrolling in the course, and 6% of the students had more than 3 years of programming experience.\nWhen asked about students' use of ChatGPT prior to the given exercise (Q2), 84% reported to have used ChatGPT for assignments. Only 16% negated that question. Thus, a majority of the students in the course had little to no programming expe- rience, yet nearly most of them had previously used ChatGPT for their coursework. When it comes to the frequency (Q3) of using ChatGPT for their programming exercises, the data suggests that about half of the students (52%) engage with the GenAI tool on a weekly basis, 18% even daily. 6% reported on using it monthly, 16% do so rarely, and only 8% of student have never used ChatGPT.\nRegarding duration (Q4), 43% seem to use ChatGPT for quick requests lasting less than 15 minutes, while 38% use it for 15-30 minutes. The remaining students use it for 30-60 minutes (16%), or for more than 60 minutes (3%). Students clearly prefer using the ChatGPT's interface, with 95% of the respondents indicating this approach. Only 2%, respectively 3% use the messaging App integration or the API integration.\nStudents further selected all tasks for which they applied ChatGPT when working on the given programming exercises (Q6). As shown in Figure 1, students used the tool for multiple purposes, ranging from problem understanding (223), concep- tual understanding (178), code generation (176), debugging (134), producing documentation (102) and test cases (102), correcting syntax (90), to performing a runtime analysis (35).\nIn addition to the closed answer options, students provided the following applications of ChatGPT: used for coding (13),"}, {"title": "B. Students' perceptions of ChatGPT (RQ2)", "content": "The second part of the survey quantitatively and qualita- tively explored the subjective experiences and evaluation of the GenAI tool as reported by learners (Q7-Q14)."}, {"title": "1) Quantitative results", "content": "The diverging stacked bar chart in Figure 2 illustrates the distribution of student responses to several 5-point-Likert-scale questions (Q7-Q11, n=298 each) about their perspectives on using ChatGPT. Each row of the chart corresponds to a different question, with responses rang- ing from negative (left-hand side, shades of red) to positive (right-hand side, shades of blue), and a neutral midpoint (grey).\nFor Q7 on the ease of using ChatGPT, the median and mean of the responses were 4 and 4.06 respectively. The majority of responses were positive, with the largest proportion of students rating ChatGPT the highest on the Likert scale, indicating a perception of ChatGPT as user-friendly.\nWhen asked about the extent to which ChatGPT has helped improve programming skills or solve coding problems (Q8), the responses were generally positive, with a median of 3 and a mean of 3.40. A significant number of students felt that ChatGPT has been helpful in these areas.\nThe accuracy and relevance of the responses provided by ChatGPT (Q9) yielded a more varied distribution of opinions, with a median of 3 and a mean of 2.87. These statistics suggest some students found the accuracy and relevance to be less than satisfactory, leaning towards the lower end of the scale.\nRegarding overall satisfaction with ChatGPT for program- ming assistance (Q10), the median was 4 and the mean was 3.44, showing a positive trend. Many students expressed high levels of satisfaction with the tool.\nLastly, the likelihood of recommending ChatGPT as a sup- port tool to a programming novice (Q11) was predominantly positive, with a median of 4 and a mean of 3.63. A substantial portion of students indicated they are very likely to recommend ChatGPT for programming novices."}, {"title": "2) Qualitative results", "content": "The qualitative analysis of the open questions further reflect upon students' perceptions towards ChatGPT when solving programming exercises in an intro- ductory course. Table I summarizes the deductive-inductive categories of positive and negative aspects and experiences stu- dents reported in response to Q12 (n=261) and Q13 (n=259). The analysis resulted in 740 coding units with positive com- ments (Q12), and 682 coding units representing negative perspectives (Q13) \u2013 1422 in sum.\nThe first column of the table represents the category label, and how often students referred to it in the data, both in the positive (+), and negative () sense. The second column provides the category definition, which is followed by a literal, representative example of a student response. Overall, we identified 20 categories in the responses to Q12 and Q13.\nThe initial seven categories of Table I represent students' perspectives on several different use patterns. Among them is the use as initial help tool when starting to solve a problem, the application as a search tool for dedicated questions, or for conceptual input. Many students referred to using ChatGPT for the generation of code and text (e.g., test cases, documentation, etc.), but noted both positive and negative aspects of that sce- nario. Debugging is another, mostly positively mentioned use case. ChatGPT was also mentioned as producing alternative perspectives and solutions, even though students disregarded the degree of variation of the replies. Another positively men- tioned use case is that of the study buddy, meaning students use ChatGPT to generate individual, on-demand assistance.\nThe remaining 13 categories presented in Table I summarize students' perceptions of certain qualities or aspects related to the use of ChatGPT. For example, students noted the response quality with a positive and negative connotation, e.g., due to its ability to rephrase things, but also w.r.t. its use of uncommon/unknown terms. Similarly, the chat history can help students, as they do not have to repeat themselves in a conversation. At the same time, 'remembering' the conver- sation can require students to restart a new chat. The tool's availability is mostly mentioned in a positive manner, with downtime being criticized. The ease of using ChatGPT com- prises positive aspects, such as its intuitive handling, adapting to natural language, multilingual inputs, and even vague inputs. At the same time, students are under the impression that precise inputs are required.\nChatGPT's efficiency w.r.t. time was perceived as positive, due to the potential of saving time, and negative due to the need to verify its responses. Students further appreciated the large knowledge base, but criticized the biases it can reproduce. Another aspect concerns the perceived \u2018social\u2019 interaction with ChatGPT. On the one hand, the safe space without 'stupid' questions was appreciated. On the other hand, students noted that the tool does not comprehend emotions.\nAll of the aforementioned aspects were recognized in a sort of balanced manner, expressing both benefits for students, but also criticism. However, 6 of the 13 categories only refer to negative aspects students experience using ChatGPT. Among them are privacy concerns, ChatGPT's overconfidence despite producing incorrect results, hallucinations, and a lack of integrity (e.g., by citing non-existent sources). Moreover, students expressed the necessity to remain critical of the GenAI tool, and to always verify the generated responses. Some students (26) even mentioned the risk of depending too heavily on ChatGPT, causing them to not thoroughly attempt to solve programming exercises themselves anymore.\nThe last survey question (Q14) asked for additional remarks (n=136). Students mostly repeated the positive and negative as- pects from Q12 and Q13. Nonetheless, we present some of the most interesting comments to highlight students' perspectives:\n\"AI (and how to use it) should be introduced as a core module. If you know how to use it, you learn much faster and save a lot of energy.\"\n\"If in the exercises the idea of how the code came up and the attempt to write it were more valued (instead of the result 'the code runs well'), the process of doing the exercises might be more enjoyable.\""}, {"title": "V. DISCUSSION", "content": "The analysis of the responses provided by students in the survey reveal that many students incorporate ChatGPT into their routines when solving programming tasks, utilizing it for a diverse range of problems. Some of these had been identified in prior work, e.g., debugging [3]. While many aspects of ChatGPT are perceived as helpful and useful by students, there is also notable criticism surrounding known issues such as hallucinations, overconfidence, and over-reliance.\nAlthough some students voiced criticism regarding the occa- sional unavailability of ChatGPT, many valued the opportunity to receive immediate responses, along with appreciating the interactive nature of the platform. In addition, students pos- itively highlighted the multilingual capabilities of ChatGPT and the ease of its use. They found the ability to seamlessly switch between their native language, German, and English to be particularly advantageous.\nMany students expressed difficulties in understanding pro- gramming tasks and getting started with writing code. Hence, they valued the use of ChatGPT as starting point, similarly to the students surveyed in related work [41]. While some stu- dents indicated that ChatGPT provided satisfactory responses to straightforward prompts despite spelling or syntactic errors, a larger number of students expressed challenges in obtaining adequate responses requiring well-engineered prompts.\nIn terms of ChatGPT's utilization by programming novices, students voiced caution against over-reliance on these tools, which is similar to the findings in prior work [44], [45]. Conversely, other students recognize increasing expectations"}, {"title": "VI. LIMITATIONS", "content": "The students' responses proved sufficient to reveal their use patterns and perspectives on GenAI tools, such as ChatGPT. However, the limits of self-reporting (e.g., social desirability, exaggerations, omissions) apply to the present work. More- over, as students knew they were surveyed, the observer's paradox [52] needs to be taken into account when interpreting the data. Another limitation refers to the course setting, the selected tasks, and the institution where the data was gathered (i.e., in only one country). Expanding or replicating this study in other areas is considered future work."}, {"title": "VII. CONCLUSIONS AND FUTURE WORK", "content": "The present research pursued the goal of gaining insights into the student perspective of using GenAI tools, such as ChatGPT, in the context of a curricular introductory program- ming course. To identify students' use patterns along with their perceptions of the tool, we designed an exercise sheet with programming tasks for novice learners at a German higher education institution. Students were asked to use ChatGPT- 3.5 (freely available at the time, in Dec. 2023) without further instruction. After submitting their solutions to the program- ming exercises, students (n=298) filled out an online survey to reflect upon their use and impressions.\nThe results revealed that roughly half of the students use ChatGPT as often as every week, 18% even use it daily. We further identified a great diversity of students' use patterns and application scenarios, which is due to the sheer capacity of GenAI. For example, students use it to get started with their assignments, to generate text and code, for debugging, or as a search tool when having questions. At the same time, students' perceptions showed their ambivalence towards GenAI, as stu- dents seem to be well aware of its limitations and the potential (negative) impact on student learning, and equal opportunities. It is therefore crucial for educators to at least acknowledge GenAI tools in computing and programming education, and to provide guardrails and instructions for students on how to navigate through the GenAI revolution.\nThere are several pathways for future work. Among them is the triangulation of the present work with chat protocols from novice learners of programming with a GenAI tool to address the limitations of self-reporting [53]. Another avenue is the development of specialized GPT models incorporated into well-known educational environments, to offer students free, straightforward access. Such models could contain built-in guardrails to facilitate a guided use. Integrating prompt- recommender-systems could also assist students in optimizing"}, {"title": "APPENDIX", "content": "A. Survey Questions\nQ1 Programming Experience before Course.\nQ2 Did you use ChatGPT prior to this exercise for assignments?\nQ3 How often do you use ChatGPT?\nQ4 On average, how long do you engage with ChatGPT in a single session?\nQ5 Which platform do you primarily use for accessing ChatGPT?\nQ6 Please select all the tasks for which you used ChatGPT. (Check all that apply)\nQ7 How would you rate the ease of using ChatGPT? (Likert scale, 1 Very difficult - 5 Very easy)\nQ8 To what extent has ChatGPT helped in improving your programming skills or solving coding problems? (Likert scale, 1 Not at all - 5 Greatly improved)\nQ9 Rate the accuracy and relevance of the responses provided by ChatGPT. (Likert scale, 1 Very inaccurate - 5 Very accurate)\nQ10 How satisfied are you with your overall experience using ChatGPT for programming assistance? (Likert scale, 1 Not at all - Very satisfied)\nQ11 How likely are you to recommend ChatGPT as a support tool to a programming novice? (Likert scale, 1 Not at all - 5 Highly likely)\nQ12 Please share three positive aspects or examples of your experience using ChatGPT. What did you find most valuable or beneficial? (open)\nQ13 Please share three negative aspects or examples of your experience using ChatGPT. What did you find challenging or difficult? (open)\nQ14 Is there anything else you would like to share about your experience using ChatGPT? (open)"}]}