{"title": "LOOKING INWARD: LANGUAGE MODELS CAN LEARN\nABOUT THEMSELVES BY INTROSPECTION", "authors": ["Felix J Binder", "James Chua", "Tomek Korbak", "Henry Sleight", "John Hughes", "Robert Long", "Ethan Perez", "Miles Turpin", "Owain Evans"], "abstract": "Humans acquire knowledge by observing the external world, but also by intro-\nspection. Introspection gives a person privileged access to their current state of\nmind (e.g., thoughts and feelings) that is not accessible to external observers. Can\nLLMs introspect? We define introspection as acquiring knowledge that is not con-\ntained in or derived from training data but instead originates from internal states.\nSuch a capability could enhance model interpretability. Instead of painstakingly\nanalyzing a model's internal workings, we could simply ask the model about its\nbeliefs, world models, and goals.\nMore speculatively, an introspective model might self-report on whether it pos-\nsesses certain internal states-such as subjective feelings or desires and this\ncould inform us about the moral status of these states. Importantly, such self-\nreports would not be entirely dictated by the model's training data.\nWe study introspection by finetuning LLMs to predict properties of their own\nbehavior in hypothetical scenarios. For example, \u201cGiven the input P, would your\noutput favor the short- or long-term option?\u201d If a model M1 can introspect, it\nshould outperform a different model M2 in predicting M1's behavior-even if\nM2 is trained on M1's ground-truth behavior. The idea is that M1 has privileged\naccess to its own behavioral tendencies, and this enables it to predict itself better\nthan M2 (even if M2 is generally stronger).\nIn experiments with GPT-4, GPT-40, and Llama-3 models (each finetuned to pre-\ndict itself), we find that the model M1 outperforms M2 in predicting itself, pro-\nviding evidence for introspection. Notably, M1 continues to predict its behavior\naccurately even after we intentionally modify its ground-truth behavior. However,\nwhile we successfully elicit introspection on simple tasks, we are unsuccessful on\nmore complex tasks or those requiring out-of-distribution generalization.", "sections": [{"title": "1 INTRODUCTION", "content": "Do language models have knowledge about themselves that is neither contained in their training data\nnor inferable from it? In this paper, we investigate a surprising capability of LLMs: they can obtain\nknowledge about themselves through introspection.\nIntrospection in LLMs is significant due to its potential benefits and risks (Section 7). An introspec-\ntive model can answer questions about itself based on properties of its internal states-even when\nthose answers are not inferable from its training data. This capability could be used to create hon-\nest models that accurately report their beliefs, world models, dispositions, and goals (Evans et al.,\n2021). It could also help us learn about the moral status of models (Perez & Long, 2023). For exam-\nple, we could simply ask a model if it is suffering, if it has unmet desires, and if it is being treated"}, {"title": "2 OVERVIEW OF METHODS", "content": "We define introspection in LLMs as the ability to access facts about themselves that cannot be de-"}]}