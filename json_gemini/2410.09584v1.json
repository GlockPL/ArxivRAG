{"title": "Toward General Instruction-Following Alignment for Retrieval-Augmented Generation", "authors": ["Guanting Dong", "Xiaoshuai Song", "Yutao Zhu", "Runqi Qiao", "Zhicheng Dou", "Ji-Rong Wen"], "abstract": "Following natural instructions is crucial for the effective application of Retrieval-Augmented Generation (RAG) systems. Despite recent advancements in Large Language Models (LLMs), research on assessing and improving instruction-following (IF) alignment within the RAG domain remains limited. To address this issue, we propose VIF-RAG, the first automated, scalable, and verifiable synthetic pipeline for instruction-following alignment in RAG systems. We start by manually crafting a minimal set of atomic instructions (<100) and developing combination rules to synthesize and verify complex instructions for a seed set. We then use supervised models for instruction rewriting while simultaneously generating code to automate the verification of instruction quality via a Python executor. Finally, we integrate these instructions with extensive RAG and general data samples, scaling up to a high-quality VIF-RAG-QA dataset (>100k) through automated processes. To further bridge the gap in instruction-following auto-evaluation for RAG systems, we introduce FollowRAG Benchmark, which includes approximately 3K test samples, covering 22 categories of general instruction constraints and four knowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG can seamlessly integrate with different RAG benchmarks. Using FollowRAG and eight widely-used IF and foundational abilities benchmarks for LLMs, we demonstrate that VIF-RAG markedly enhances LLM performance across a broad range of general instruction constraints while effectively leveraging its capabilities in RAG scenarios. Further analysis offers practical insights for achieving IF alignment in RAG systems. Our code and datasets are released at https://FollowRAG.github.io.", "sections": [{"title": "1. Introduction", "content": "The advancement of Large Language Models (LLMs) (OpenAI 2023; Yang et al. 2024) has profoundly revolutionized a variety of real-world tasks expressed in natural language (Wei et al. 2022; Luo et al. 2023). However, they still suffer from hallucinations and factual inconsistencies (Bang et al. 2023), impacting the authenticity of generated answers. Retrieval-Augmented Generation (RAG) has gained recognition as a promising solution, empowering LLMs to leverage reliable information from retrieved documents, thereby returning high-quality responses (Guu et al. 2020; Lewis et al. 2020).\nIn real-world interaction scenarios, users often deviate from standard templates when posing questions, instead of imposing diverse instructions on model outputs to meet specific task requirements (Jiang et al. 2023b; Chung et al. 2024). Consequently, improving instruction-following (IF) capabilities is foundational to the effective application of LLM and RAG systems. The core goal of IF is to enable models to adapt to the diverse intents of users, which has garnered widespread attention in the LLM community.\nExisting efforts on instruction-following alignment primarily focus on multi-grained evaluation (Zhou et al. 2023a; Jiang et al. 2024a; Wen et al. 2024) and high-quality instruction data synthesis (Sun et al. 2024a; Zhao et al. 2024) to enhance LLMs' natural instruction-following capabilities. However, in complex RAG scenarios, the diverse knowledge introduced by retrieval-augmented techniques presents significant challenges for LLMs in effectively handling complex instructions (Figure 1). As shown in Figure 2, after supervised fine-tuning on high-quality general and knowledge-intensive QA datasets, LLMs demonstrate robust performance in both IF and RAG tasks (Mistral-base vs. Mistral-SFT). However, these capabilities do not always generalize well to instruction-following tasks under RAG scenarios and may even conflict with the performance of other fundamental abilities (Dong et al. 2024b; Zhu et al. 2024). Unfortunately, research on instruction-following in RAG systems remains limited, significantly hindering their application in real-world interactions.\nTo tackle these challenges, our aim is to address following critical research questions:\n\u2022 RQ1. How can we comprehensively evaluate the complex instruction-following capabilities in the RAG scenario?\n\u2022 RQ2. How can we achieve scalable and reliable instruction-following alignment in RAG systems while preserving the it's foundational abilities from conflict?\nIn this paper, we propose VIF-RAG, the first automated, scalable, and reliable data synthesis pipeline for achieving complex instruction-following alignment in RAG scenarios. The core insight of VIF-RAG is to ensure every step of data augmentation and combination includes a proper verification process. Specifically, we start by manually crafting a minimal set of atomic instructions (<100) and developing combination rules to synthesize and verify complex instructions for a seed set. We then use supervised models for instruction rewriting. Motivated by tool execution studies (Le et al. 2022; Qiao et al. 2024b), we employ the same supervised model to generate verification code and automatically verify the quality of augmented instructions through the Python compiler's outputs. Finally, we combine these high-quality instructions with RAG datasets from various domains (each containing retrieved documents per query), performing the augmentation and dual validation process to synthesize a high-quality instruction-based RAG dataset, named VIF-RAG-QA (>100K samples).\nTo further bridge the gap in automatic instruction-following evaluation for RAG systems, we introduce FollowRAG, the first benchmark dedicated to comprehensively assessing the complex instruction-following capabilities of RAG systems. FollowRAG aggregates constraints from real-world scenarios. It includes approximately 3K test samples, spanning 4 knowledge-intensive QA benchmarks and 22 types of constraints. Due to its robust pipeline design, FollowRAG can seamlessly integrate with different RAG benchmarks.\nTo summarize, our contributions are as follows:\n\u2022 To first achieve instruction-following alignment in the RAG system, we propose VIF-RAG, the first automated, scalable, and verifiable data synthetic framework. VIF-RAG uniquely combines augmented rewriting with diverse validation processes to synthesize high-quality instruction-following alignment data from almost scratch (<100), scaling up to over 100K samples.\n\u2022 We introduce FollowRAG, the first benchmark designed to comprehensively evaluate LLM's complex instruction-following abilities in RAG tasks. FollowRAG includes nearly 3K test samples, spanning four knowledge-intensive QA benchmarks and 22 types of constraints. Its design ensures seamless integration with various RAG benchmarks, providing strong scalability.\n\u2022 With FollowRAG and 8 widely-used IF and 3 foundational abilities benchmarks, we demonstrate that different LLMs with VIF-RAG achieve extraordinary alignment on general instruction following in both RAG and standard scenarios while effectively preserving other foundational capabilities. Further analysis offers practical insights for optimizing IF alignment in RAG systems."}, {"title": "2. Related Work", "content": "Instruction-Following Alignment for LLMs. Instruction-following ability is a core capability of large language models. Existing works fall into two main categories. The first includes efforts like MMLU and MTbench (Hendrycks et al. 2021; Zheng et al. 2024a), which rigorously evaluate models' adherence to general instructions. Moreover, works like IFEval and Followbench (Zhou et al. 2023a; Jiang et al. 2024a) focus on fine-grained assessment under specific constraints, using stricter criteria such as instruction difficulty, domain, and task formats (Qin et al. 2024; Xia et al. 2024; Yan, Luo, and Zhang 2024; Wen et al. 2024). The other category focuses on improving IF alignment. Manual design of instructions and responses by human annotators (Wei et al. 2021) is challenging and costly. To address this, methods are developed to synthesize diverse instructions, allowing weaker models to mimic the responses of advanced models (Dubois et al. 2024; Dong et al. 2024a; Xu et al. 2023), achieving strong-to-weak alignment (Cao et al. 2024).\nAlignment for Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) addresses the issue of knowledge hallucination in LLMs by retrieving relevant factual information, offering a promising solution (Guu et al. 2020; Lewis et al. 2020). However, efficiently aligning retrieved knowledge with LLMs' preferences remains a challenge. Researchers have developed robust reranker-based methods (Sun et al. 2023; Qin et al."}, {"title": "3. Preliminaries", "content": "Retrieval-Augmented Generation (RAG). Retrieval-Augmented Generation systems usually operates under a retrieve-then-read framework (Lewis et al. 2020). The external retriever is integrated to gather supporting knowledge and improve the generation process. Given a query q, a retriever R recalls k relevant documents $D_q = \\{d_i\\}_{i=1}^k$ from an external corpus comprised of N documents. We employ the DPR (Karpukhin et al. 2020) to obtain hidden vectors for queries and documents. The relevance score is determined by measuring the dot-product similarity between the query and document representations, allowing the retrieval of the top-k documents $D_q$::\n$D_q = \\text{argtop-k} \\left[ E_d(d_i) \\cdot E_q(q) \\ | \\ i = \\{1 ...N\\} \\right]$.\nThen, the retrieved documents are concatenated with the query into an LLM reader R to generate the target text:\n$y = R(q, D_q) = \\log P_\\theta (y \\ | \\ q, D_q)$,\nwhere $P_\\theta$ is the output probability distribution.\nInstruction-following Alignment for RAG. Following instructions is one of the most foundational ability for LLMs in RAG systems. Given an instruction $I = \\{I_i\\}_{i=1}^M$ with M specific constraints and a specific query q with corresponding relevant k retrieved documents $D_q$, The LLM $\u03c0_\u03b8$ in the RAG system is expected to produce an accurate response $y \\sim \u03c0_\u03b8(y \\ | \\ q, D_q, I)$ while obeying with the specified constraints."}, {"title": "4. VIF-RAG Framework", "content": "In this section, we propose VIF-RAG, a verifiable automated instruction data synthesis framework for RAG scenarios. The core design of VIF-RAG is that each step of the automated generation or combination is accompanied by an appropriate verification process. ViF-RAG framework can be broadly split into two sections: (1) the instruction synthesis stage and (2) instruction-query synthesis, scaling from almost scratch (<100) to over 100K high-quality instruction-query samples. Below, we will delve into the specifics."}, {"title": "4.1. Instruction Synthesis from Scratch", "content": "Handwritten Seed Instructions. We initially develop a minimal seed instruction set $D_{\\text{seed}}^{\\text{atom}}$ manually, using four foundational categories of constraints: format constraints, semantic constraints, knowledge constraints, and lexical constraints, as themes for instruction writing. The following presents specific criteria regarding the 4 constraints:\n\u2022 Format Constraints require the output to adhere to specific standards in terms of format, length, and structure. The content should be organized, clear, and meet the required format specifications.\n\u2022 Semantic Constraints require the output's theme, language style, personality, and sentiment to align with the given instructions. The content should be semantically consistent with expectations and adhere to the specified tone or expression.\n\u2022 Knowledge Constraints require the output to be accurate, comprehensive, and in-depth. The content should be informative, cover all necessary information, and maintain consistency in knowledge expression.\n\u2022 Lexical Constraints require the output to include specific keywords or phrases, ensuring precision and relevance in word choice. The content should meet the expected requirements in terms of vocabulary selection.\nWe hire only one well-educated human annotator to manually create 15 single-atomic instructions for each type of constraint. Notably, this is the only process in our data synthesis process that includes human supervision.\nInstruction Composition & Verification. Real-world instructions often involve multiple constraints in one user query. To address this complexity, we design rules to automatically combine atomic instructions into diverse, complex instructions:\n\u2022 Multiple Constraints: As illustrated in Figure 3, we randomly sample pairs of instructions from $D_{\\text{seed}}^{\\text{atom}}$ and insert them into a constraint template. By directly concatenating these instruction pairs, we create complex instructions that contain dual and triple constraints. This type of instruction requires the model to generate results that satisfy multiple constraints simultaneously.\n\u2022 Chain Rule Constraints: We design sequential conditional constraint templates and selected atomic instructions from $D_{\\text{seed}}^{\\text{atom}}$ to form chain constraints. Formally, the chain consists of n tasks {T1, T2, ..., Tn}, requiring the model's output to complete these n tasks sequentially.\nVerification. Randomly combining these atomic instructions can easily lead to conflicts between them (e.g., don't use words containing the letter 'I', use words that end with '-ing'). These semantic conflicts can be challenging to detect using a simple Natural Language Inference model. To detect potential conflicts between these instructions, we use a robust supervised model that rates their consistency from 1 to 10. Samples scoring below 8 are excluded to refine our high-quality complex instruction set $D_{\\text{seed}}^{\\text{complex}}$. Ultimately, we arrive at the initial seed instruction set $D_{\\text{seed}} = \\{D_{\\text{seed}}^{\\text{atom}} \\cup D_{\\text{seed}}^{\\text{complex}}\\}$. Detailed information about the prompt templates are listed in the Appendix."}, {"title": "Instruction Rewriting & Quality Verification", "content": "To automate the scaling up of instructions, the instruction rewriting strategy is considered the most natural augmentation method, and has received significant attention in the RAG and reasoning fields (Mumuni and Mumuni 2022; Xie et al. 2020; Yuan et al. 2023a; Li et al. 2024b,a). We use a supervised model to iteratively rewrite instructions from the $D_{\\text{seed}}$ set in batches of 50 for K rounds, generating an augmented set $D_{\\text{aug}}$. Subsequently, we merge the seed and augmented samples to form the combined instruction set $D_{\\text{ins}} = D_{\\text{seed}} \\cup D_{\\text{aug}}$, removing duplicates.\nInspired by tool execution works (Le et al. 2022), we aim to leverage the powerful coding abilities of LLMs to assist in verifying the quality of auto-generated instructions. As shown in Figure 3, for each instruction $I \\in D_{\\text{ins}}$, we use the supervision model to generate K verification function codes and corresponding test cases $\\{func_k, c_k\\}_{k=1}^K \\in D_{\\text{verify}}$, and assess the instruction's quality by analyzing the output of the executor E. For any function and test case $\\{func,c\\} \\in D_{\\text{verify}}$, its execution output is:\n$E(func, c) = \\begin{cases}\n1 & \\text{If output is \"True\"} \\\\\n0 & \\text{If output is \u201cFalse\u201d or \u201cError\u201d}\n\\end{cases}$\nTherefore, we can calculate the accuracy $Acc_{\\text{func}}$ of each verification function based on K test samples, as well as the accuracy $Acc_{\\text{case}}$ of each case evaluated using K verification functions. These can be formulated as:\n$Acc_{\\text{func}} = \\frac{\\sum_{k=1}^K \\mathbb{I}(E(func, c_k)=1)}{K}$\n$Acc_{\\text{case}} = \\frac{\\sum_{k=1}^K \\mathbb{I}(E(func_k, c)=1)}{K}$\nBased on the above cross metrics, we require that at least one $Acc_{\\text{func}}$ and $Acc_{\\text{case}}$ of the each instruction must exceed 0.5, Ultimately, we obtain the auto-verified instruction set as:\n$D_{\\text{ins}}^{\\text{verify}} = \\{d \\in D_{\\text{ins}} \\ | \\ Acc_{\\text{func}}(d) > 0.5 \\ \\& \\ Acc_{\\text{case}} (d) > 0.5\\}$"}, {"title": "4.2. Scalable Instruction-Query Synthesis", "content": "Random Instruction-Query Combination. In real-world interactions with RAG systems, achieving IF alignment depends on effectively integrating the synthesized instructions with the queries used by the RAG system. To meet this goal, as depicted in Figure 3, we first extract high-quality queries from two different data sources.\n1) RAG Domain: Building effective RAG system need to prepare sufficient amounts of QA-format data with relevant knowledge to enhance its knowledge-based interaction capabilities. Consequently, we randomly select a query set Q from mixed QA data sources, including open-domain multi-hop and knowledge base QA scenarios. Following the retrieve-then-read paradigm (Lewis et al. 2020), We employ the dense retriever R to fetch the top-K relevant documents $D_i$ for each query $q \\in Q$ from an external knowledge base, resulting in the dataset $D_{\\text{RAG}} = \\{q_i, D_i\\}_{i=1}^K$. Furthermore, we randomly select K queries along with their corresponding retrieved documents from $D_{\\text{RAG}}$ for each instruction I and combine them to create the RAG query set with IF constraints $D_{\\text{IF-RAG}} = \\{I_i, q_j, D_j\\}_{i,j=1}^K$.\n2) General Domain: In addition to incorporating RAG-specific abilities, the RAG system has to possess basic human-aligned abilities to meet users' daily interaction needs. Therefore, ShareGPT (Chiang et al. 2023), which provides authentic multi-turn human dialogue data, is our natural choice. Similar to how we handle the RAG domain, for each instruction $I \\in D_{\\text{ins}}$, we randomly select K queries from the ShareGPT to combine with the instruction and construct the general dataset $D_{\\text{IF-General}}$ for each instruction.\nUltimately, we merge the instruction-constrained query sets from these two domains into the final query set of VIF-RAG-QA, formulated as $D_{\\text{VIF-RAG}}$.\nInstruction-Query Rejection Sampling. It is worth noting that under diverse instruction-following constraints, the original grounding truth answers for queries in both the RAG and general datasets become unreliable. To address this issue and improve synthetic data diversity, we adopt a rejection sampling strategy (Yuan et al. 2023b). Specifically, we use the supervision model to generate K responses $Y_x = \\{y_i\\}_{i=1}^K$ for each instruction-query pair $x \\in D_{\\text{VIF-RAG}}$, resulting in $\\{x, Y_x\\} \\in D_{\\text{VIF-RAG}}$.\nDual Stage Verification. To further ensure comprehensive quality control of the synthetic dataset, we employ a dual stage verification process for the instruction-query data:\n\u2022 Executor-based Verification: To automatically verify whether model-generated responses comply with the constraints of the instruction-query samples, we leverage pre-existing verification functions to evaluate adherence in the augmented outputs. As in the \"Instruction Rewriting & Quality Verification\" section, at least one response in $D_{\\text{VIF-RAG}}$ must achieve an accuracy rate $Acc_{\\text{case}}$ above 0.5 across all verification functions; otherwise, the sample is discarded.\n\u2022 Consistency Verification: We have noticed that combined instructions and queries often conflict. A simple example is when the query \"Please write a brief biography of Barack Obama.\" does not meet the instruction \"Strictly limit your answer to less than 10 tokens.\" Building on previous consistency verification of instructions, we employ a supervision model to evaluate the alignment between queries and instructions on a scale of 1 to 10, discarding samples that receive a score below 8.\nAfter dual stage verification, we have automatically obtained a large-scale, high-quality VIF-RAG-QA dataset."}, {"title": "5. FollowRAG Benchmark", "content": "To bridge the gap in automatic instruction-following evaluation for RAG systems, we introduce FollowRAG in this section. We provide a detailed introduction from two aspects: \"Data Construction\" and \"Evaluation and Statistics\"."}, {"title": "5.1. Dataset Construction", "content": "Instruction Collection & Extraction. FollowRAG aims to assess the model's ability to follow user instructions in complex multi-document contexts. Drawing from general IF datasets like IFEval (Zhou et al. 2023b) and Follow-Bench (Jiang et al. 2024b), we collect and verify definitions and examples of atomic instructions using rules (e.g., code), excluding those irrelevant to RAG scenarios. Ultimately, we identify 22 types of instruction constraints, encompassing language, length, structure, and keywords.\nInstruction Reforming. We use widely-used question-answering (QA) datasets, such as Natural Questions (Kwiatkowski et al. 2019), as the foundation for constructing FollowRAG samples. For a query sampled from the QA datasets, we need to generate a complex instruction containing n atomic instruction constraints (with n ranging from 1 to 4). To enhance the diversity of atomic instruction representations, we employ GPT-40 as the instruction generator. Specifically, given a query, we first sample n instructions from the atomic instruction set and perform conflict detection. Subsequently, with examples as demonstrations, we prompt the LLM to generate a new varied instruction text for each type of atomic instruction, along with parameters for instruction-following evaluation.\nCombination. Finally, we integrate the retrieved passages, query and atomic instructions to construct the sample input for FollowRAG. To avoid mechanically concatenating the query and instructions in a template-based manner, we prompt supervised model to naturally blend the multiple"}, {"title": "5.2. Evaluation and Statistics", "content": "After obtaining the model's output, we evaluate it from two perspectives: instruction following and question answering (QA) under the RAG paradigm:\n\u2022 Instruction Following: Utilizing the verifiable nature of our atomic instructions and following the IFEval approach, we automate the verification of the model's adherence to each instruction through code validation. We then calculate the average pass rate for each atomic instruction across all samples to determine the instruction-following score in FollowRAG.\n\u2022 RAG: Under new instruction constraints, the model's target output differs from the gold answers in the original QA dataset, rendering traditional metrics like Exact-Match ineffective. To address this, we use the original gold answers as a reference and utilize GPT-40 to evaluate whether the model's outputs correctly address the questions. The scoring criteria are as follows: Completely correct (1 point), Partially correct (0.5 points), Completely incorrect (0 points). The average score of all samples is taken as the RAG score for FollowRAG.\nFor detailed statisticsin in Figure 4, FollowRAG is the first instruction-following evaluation dataset under RAG scenario comprising 2.8K samples, covering 22 fine-grained atomic instructions across 6 categories. The queries in FollowRAG are sourced from 4 QA datasets across 3 types: 1) Open-Domain QA: Natural Questions (NQ) (Kwiatkowski et al. 2019) and TriviaQA (TQA) (Joshi et al. 2017); 2) Multi-Hop QA: HotpotQA (HQA) (Yang et al. 2018); and 3) Knowledge Base QA: WebQuestionsSP (WebQSP) (Yih et al. 2016). To further construct varying levels of instruction-following difficulty, FollowRAG includes 0.9K samples of single and dual atomic instructions, as well as 0.5K complex multi-instruction samples containing 3 and 4 atomic instructions, respectively."}, {"title": "6. Experiment", "content": "In this section, we evaluate over 10+ benchmarks to comprehensively evaluate the VIF-RAG. For the instruction-following tasks in RAG scenarios, we use the FollowRAG benchmark as mentioned in Section 5, which covering 4 question-answering (QA) datasets. For general instruction-following evaluation, we selected two commonly used complex instruction-following datasets, IFEval (Zhou et al. 2023a) and FollowBench (Jiang et al. 2024a), along with the natural instruction dataset MT-Bench (Zheng et al. 2024a) and the challenging ChatBot instruction-following bench, Arena-Hard (Li et al. 2024c). Additionally, to measure that the foundational abilities of LLMs, we further evaluate two widely used LLM's general abilties evaluation sets, C-Eval (Huang et al. 2023) and MMLU (Hendrycks et al. 2021), as well as the mathematical reasoning dataset GSM8K (Cobbe et al. 2021) and the code evaluation bench HumanEval (Chen et al. 2021).\nFor baselines, we select Mistral-7B (Jiang et al. 2023a), Llama3-8B (Meta 2024), Qwen1.5-7B, and Qwen1.5-14B (Yang et al. 2024) as our backbone models, fine-tuning ShareGPT and four QA training sets as SFT version. Besides, we introduce several strong IF baselines, including Conifer (Sun et al. 2024a), Evol-Instruct (Xu et al. 2023), and Deita (Liu et al. 2024). To ensure fairness, we add an equal-sized RAG training set to the original synthetic data used for these models. More details on the baselines and implementation can be found in the appendix."}, {"title": "6.2. Main Result", "content": "Our primary findings are presented in Table 1. Overall, VIF-RAG consistently surpasses all baselines in FollowRAG across multiple configurations, highlighting the clear advantages of our method. Additionally, we have discovered several key insights:\n1) Existing IF baselines struggle in complex RAG scenarios. Comparisons between different base models and SFT versions in Tables 1 & 2 show that while SFT general data like ShareGPT improves performance on IFEval, it actually shows a performance decline in the instruction-following aspect of FollowRAG (e.g., NQ-IF: 25.7\u219221.0 in Mistral). Moreover, several strong IF baselines, such as Conifer (Sun et al. 2024b), also perform poorly in FollowRAG's IF aspect (HQ-IF: 26.9 26.45). This corroborates the issue highlighted in the introduction: traditional synthetic data may improve LLMs' vanilla instruction-following ability but often fails to generalize in RAG scenarios, sometimes even leading to decreased performance.\n2) VIF-RAG shows exceptional IF alignment capability across various datasets, models, and parameter sizes. It consistently outperforms all baselines by over 10% on average accuracy, including a 44% improvement over Llama3-base, showcasing the significant performance advantage of our method. On four detailed QA benchmarks, VIF-RAG achieves the best results across all tested backbones. Moreover, whether using Qwen1.5-7B or Qwen1.5-14B, our method maintains a stable and significant performance increase of over 10%. These results highlight that VIF-RAG is not only plug-and-play but also exhibits strong generalization capabilities.\n3) The RAG capability is effectively preserved. Protecting RAG capability is a core focus of RAG systems. Compared to various SFT version baselines, our VIF-RAG significantly enhances IF capability while maintaining more stable RAG performance. This allows us to be optimistic about its potential in real-world RAG system applications."}, {"title": "6.3. Cross-Domain Validation", "content": "To explore the transferability of VIF-RAG, we conduct cross-domain validation on four natural instruction-following datasets and four foundational abilities benchmarks for LLMs in Tabel 2. Our findings are as follows:\n1) Consistent IF alignment in both standard and RAG scenarios. Table 1 shows that VIF-RAG achieves remarkable IF alignment in RAG scenarios. In Table 2, comparing Llama3-8B SFT version, VIF-RAG demonstrates strong gains on two widely-used IF benchmarks, IFEval and FollowBench, with improvements of 8.8% (Ins.L) and 15.5% respectively. It also maintains stable improvement across different parameter sizes (7B & 14B). These results confirm that VIF-RAG consistently enhances IF alignment in both RAG and standard scenarios.\n2) Robust General IF Transferability. To assess general instruction-following alignment, we test VIF-RAG on challenging benchmarks Arena-Hard and MT-Bench. The results demonstrate that VIF-RAG maintains consistent alignment across various backbones, with a notable 1.3% improvement on MT-Bench for the 14B model. This reveals significant potential for larger models in achieving better natural instruction alignment.\n3) Great Preservation of foundational Abilities. Previous research highlights that enhancing specific capabilities often compromises others (Dong et al. 2024b; Hui et al. 2024). As indicated in Table 2, VIF-RAG effectively preserves general capabilities (MMLU, C-Eval), math reasoning (GSM8K), and coding skills (HumanEval) across different configurations, with some slight performance improvements. This preservation is largely attributed to the integration of ShareGPT data in the synthesis process, demonstrating VIF-RAG's ability to balance diverse capabilities while maintaining broad applicability."}, {"title": "6.4. Quantitative Analysis", "content": "Ablation Study. To examine the effects of various components in VIF-RAG, we conduct an ablation study in Table 3. The term \"w/o\" indicates versions where specific components are removed. Our key observations are:\n\u2022 Removing any component from VIF-RAG results in decreased performance, indicating that all components, such as the complex instruction composition strategy and quality verification design, are crucial to its effectiveness.\n\u2022 The largest performance decline in FollowRAG is observed when executor verification is removed. This underscores the critical role of automated instruction-response validation in improving synthetic data quality and confirms the advantage of using LLMs to oversee instruction-following abilities through other core skills like coding.\n\u2022 Surprisingly, the consistency verification proves beneficial in preserving RAG capabilities. It effectively filters out samples with high-level semantic conflicts between instructions and queries, reducing noise in IF tasks and maintaining RAG performance integrity.\nScaling Analysis. To explore the impact of retrieved document quantity on instruction-following performance in RAG scenarios, we refer to Table 5. For the baseline models (SFT versions), instruction-following capability declines as the number of passages increases. Specifically, performance drops sharply by over 6% when the document quantity in FollowRAG increases from 0 to 1. Further increasing the number to 10 leads to a significant performance decline, with Qwen-14B-SFT experiencing a drop of over 10%. This indicates that integrating knowledge through retrieval-augmented techniques challenges the instruction-following abilities of existing models.\nIn contrast, VIF-RAG shows a minor performance drop (<3%) when encountering the first document. As the number of documents increases to 10, VIF-RAG's performance remains relatively stable, demonstrating its robustness.\nInstruction Difficulty Analysis. To explore the effect of different instruction quantities (i.e., instruction-following difficulty) on model performance in RAG scenarios, we evaluate VIF-RAG and various baseline models on the FollowRAG benchmark, using test sets with 1, 2, and 3 instructions. As shown in Figure 6, as the number of instructions increases, all models generally show a decline in instruction-following capability, but VIF-RAG consistently outperforms the rest. Notably, even with 3 instructions present simultaneously, VIF-RAG still demonstrates over a 5% IF prompt (strict acc.), further validating its superior capability in handling complex instruction-following tasks in RAG scenarios."}, {"title": "7. Conclusion", "content": "In this paper, we propose VIF-RAG, the first automated, scalable, and verifiable data synthesis pipeline for aligning complex instruction-following in RAG scenarios. VIF-RAG integrates a verification process at each step of data augmentation and combination. We begin by manually creating a minimal set of atomic instructions (<100) and then apply steps including instruction composition, quality verification, instruction-query combination, and dual-stage verification to generate a large-scale, high-quality VIF-RAG-QA dataset (>100K). To address gaps in instruction-following evaluation for RAG systems, we present FollowRAG Bench, featuring around 3K samples with 22 types of complex instruction constraints. Using FollowRAG and 8 widely-used IF and foundational abilities benchmarks, we show that VIF-RAG significantly enhances alignment on general instruction constraints and effectively demonstrates the core abilities of LLMs. Further analysis offers insights for optimizing instruction-following alignment in RAG systems."}, {"title": "Implementation Details", "content": "Details about Instruction-Query Synthesis\nFor the data synthesis part of RAG in section 4.2 \"Scalable Instruction-Query Synthesis\u201c", "Instruction Composition & Verification\", The prompt are listed here": "nPrompt Template for Multi-Instructions Verification\nYou are an expert proficient in determining whether multiple instructions are suitable to be implemented as simultaneous constraints.\n[Instructions", "Score": "score without any additional content on the last line.\nOur VIF-RAG's prompt templates, instruction data format, verfication code, test cases and more can be found in the supplementary materials.\nDetails about Supervised Fine-tuning\nFor all LLM fine-tuning, we use a global batch size of 128, an input window of 4096, and a learning rate of 7e-6 with 2% warm-up. Each set of experiments involves fine-tuning for 3 epochs. Our training framework is DeepSpeed Zero3 (Rasley et al. 2020). To reduce memory usage, we also employed the Flash Attention (Dao et"}]}