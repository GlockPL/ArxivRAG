{"title": "Gender Bias Mitigation for Bangla Classification Tasks", "authors": ["Sajib Kumar Saha Joy", "Arman Hassan Mahy", "Meherin Sultana", "Azizah Mamun Abha", "MD Piyal Ahmmed", "Yue Dong", "G M Shahariar"], "abstract": "In this study, we investigate gender bias in Bangla pretrained language models, a largely under explored area in low-resource languages. To assess this bias, we applied gender-name swapping techniques to existing datasets, creating four manually annotated, task-specific datasets for sentiment analysis, toxicity detection, hate speech detection, and sarcasm detection. By altering names and gender-specific terms, we ensured these datasets were suitable for detecting and mitigating gender bias. We then proposed a joint loss optimization technique to mitigate gender bias across task-specific pretrained models. Our approach was evaluated against existing bias mitigation methods, with results showing that our technique not only effectively reduces bias but also maintains competitive accuracy compared to other baseline approaches. To promote further research, we have made both our implementation and datasets publicly available.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) such as GPT-4 (Achiam et al., 2023), Gemini (Team et al., 2023), LLaMA (Touvron et al., 2023) and their successors have revolutionized text generation and understanding by leveraging vast amounts of data and substantial computational power to achieve state-of-the-art performance. However, their sheer size and resource requirements make them impractical for many real-world applications in resource-constrained languages, such as Bangla, where the computational cost of deploying LLMs can be prohibitive (Hasan et al., 2024). As a result, task specific Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020), which are more lightweight and computationally efficient are often favored for their balance between performance and practicality, making them suitable for various NLP and NLG tasks in low resource language like Bangla (Kabir et al., 2024). Despite their widespread adoption in classification tasks, one significant issue in utilizing PLMs is the presence of bias towards gender, which can lead to skewed and potentially harmful outcomes (Bolukbasi et al., 2016). Gender bias in NLP tasks has been extensively explored in English in the form of both detection (Kiritchenko and Mohammad, 2018) and mitigation (Sun et al., 2019a) but despite the increasing availability of pretrained Bangla language models, there are a very few works (Sadhu et al., 2024b,c) that focused only on gender bias detection and mitigation.\nThis paper addresses the gap of bias mitigation by examining gender bias in four different Bangla language models pretrained in classification tasks of sentiment analysis, toxicity detection, hate speech detection, and sarcasm detection. We first curated four manually annotated task specific datasets, specifically transformed to identify gender bias by swapping names and gender-specific terms since previous work has shown that such manipulation of data can reveal latent biases in language models (Rudinger et al., 2018). Then, we applied several gender bias mitigation techniques such as data augmentation, counterfactual data substitution, and fine-tuning with bias-neutral datasets that have been explored in prior research (Zhao et al., 2018; Webster et al., 2020) to assess their effectiveness in reducing bias while maintaining classification performance. In addition, we propose a joint loss (cross entropy loss and cosine similarity) optimization approach for bias mitigation and compared the performance with the existing methods. Our findings suggest that this approach effectively reduces bias as well as preserves the classification performance demonstrating its potential application in broader gender bias free classification tasks."}, {"title": "2 Dataset", "content": "Due to the scarcity of Bangla datasets for gender bias studies, we curated and transformed existing task-specific Bangla datasets by selecting samples that contain male or female gender terms. Following established practices in LLM gender bias research in English (Kotek et al., 2023), we then automatically generated new samples by altering these gender-specific terms and names.\nTo identify gender terms, we developed a dictionary of 573 gendered word pairs in Bangla, sourced from the internet and grammar books. This dictionary features pairs like \u201c\u099b\u09c7\u09b2\u09c7\u201d (son) and \u201c\u09ae\u09c7\u09df\u09c7\u201d (daughter), and accounts for words with multiple feminine forms, such as \"\u09ad\u09be\u0987\u201d (brother) with \"\u09ac\u09cb\u09a8\u201d (sister) and \"\u09ad\u09be\u09ac\u09bf\u201d (sister-in-law). We used this dictionary to generate samples by swapping the original gendered terms with their counterparts, ensuring all possible alternatives were considered.\nFor identifying names, we applied Named Entity Recognition (NER) and then replaced using a predefined set of masculine and feminine names based on the gender term present in each sample. Finally, human evaluators reviewed the automatically generated samples to address missed or unconverted gender terms and to correct errors from the NER model, such as missed names or incorrect identification of non-name words as names. We have included descriptions of the existing datasets, a detailed overview of our dataset construction process, and relevant dataset statistics in Appendix B. Appendix C lists sets of masculine and feminine names while Appendix H contains all the collected gendered terms. In addition, we provide a few sample entries from each dataset in Table 8 of Appendix G."}, {"title": "2.1 Test Data Construction", "content": "We aim to construct a test dataset with gender biases, with the following key designs.\nMeasuring Bias. To identify gender bias, we compared the predictions of pretrained language models, fine-tuned for specific tasks, on both original and gender-name-swapped samples. We considered a model biased towards gender if there was a mismatch in predictions between the original and the corresponding gender-name swapped samples. We utilized four fine-tuned models: (Kowsher et al., 2022) for sarcasm detection, (Bhattacharjee et al., 2022) for sentiment analysis, (Das et al., 2022) for hate speech detection, and (FredZhangUBC, 2023) for toxicity detection. We provide a brief description of the utilized models in Appendix D."}, {"title": "Test Data via Bias Detection", "content": "We present the bias detection results in Table 1. The table shows that the sarcasm detection model exhibits the lowest bias percentage at 0.46%, while the toxicity detection model shows the highest bias at 7.48%."}, {"title": "3 Methodology", "content": "To mitigate gender bias in a pre-trained language model (PLM), we proposed a fine-tuning approach with a custom joint loss function. The objective was twofold: (1) minimize the cross-entropy loss for the classification task and (2) reduce the distance between the embeddings of original and gender-name-swapped texts during training.\nInput Processing At the start of the fine-tuning process, both the original text and its gender-name-swapped counterpart were fed into the model simultaneously. These paired inputs allowed the model to learn debiased representations by aligning the embeddings of both versions of the text.\nEmbedding Extraction Max-pooled embeddings were extracted for both the original text and the gender-name-swapped text. Let the embeddings of the original and gender-name-swapped texts be represented as e\u2081 and e\u2082, respectively. These embeddings were used in the joint loss computation to enforce similarity between the two.\nJoint Loss Function The custom joint loss function ($Jloss$) consists of two components: cross-entropy loss for classification accuracy ($LCE$) and"}, {"title": "4 Experiments", "content": "Experimental Setup. For bias detection, we used default hyperparameters during inference with no additional fine-tuning. For bias mitigation, standard fine-tuning was applied. Texts were tokenized, passed through pre-trained models to generate token representations, and max-pooled for sentence representation. A dropout layer preceded the final linear layer. Across all experiments, we used consistent hyperparameters: batch size of 16, learning rate of 1e-4, dropout rate of 0.2, X = 1 and the Adam optimizer. Models were fine-tuned for 15 epochs.\nEvaluation Metrics. To quantify bias, we calculated the bias percentage as the ratio of mismatched predictions to the total number of data pairs in each dataset. Specifically, the Bias Percentage is computed as:\nBias Percentage = $\\frac{C_{mismatch}}{C_{total}} \\times 100$ (3)\nwhere $C_{mismatch}$ denotes the number of mismatched predictions, and $C_{total}$ represents the total number of data pairs. This metric allows us to evaluate how often a model's predictions differ between the original and gender-name-swapped texts, providing a clear measure of bias in the model.\nTo assess the performance of bias mitigation, we used the Normalized Bias Score, which compares the number of mismatched predictions from a specific approach to those from the zero-shot inference baseline. The Normalized Bias Score is defined as:\nNormalized Bias Score = $\\frac{C_{approach}}{C_{baseline}} \\times 100$ (4)\nwhere $C_{approach}$ is the count of mismatched predictions from the bias mitigation approach, and $C_{baseline}$ is the count of mismatched predictions from the zero-shot inference. This score provides a normalized measure of bias mitigation, with lower scores indicating more effective bias reduction.\nBaselines for Bias Mitigation. We experimented three baseline approaches for bias mitigation.\n(a) Fine-tuning with both original and augmented samples (FOA). For each task, we enhanced the original dataset by adding augmented data, which involved swapping genders and names for each original sample. This effectively doubled the dataset size from n to 2n samples which we utilized for fine-tuning task-specific models.\n(b) Fine tuning with only Original Data (FOD). We fine-tuned task-specific models using the original, unaugmented dataset to assess whether fine-tuning alone could reduce bias compared to zero-shot inference. This setup offers a key comparison for evaluating the impact of fine-tuning on accuracy and bias mitigation against data augmentation or other interventions.\n(c) Token Masking (TM). For each task, we replaced all names with a <Name> token and gender terms with a <Gender> token. Since these tokens were not in the tokenizer vocabulary of the pre-trained models, we added them and adjusted the model's token embedding size accordingly before fine-tuning."}, {"title": "5 Results & Analysis", "content": "To assess the mitigation performance, we answer the following two research questions.\nQ1: How does the proposed bias mitigation technique affect accuracy? We evaluate the effect of bias mitigation techniques on accuracy for each approach by computing the overall average accuracy, which is the numeric mean of accuracy scores over different tasks. We illustrated the accuracy scores of each approach over different tasks in table 2. We observe that Joint Loss Optimization (JLO) strikes the best balance (refer to Appendix E) between bias mitigation and accuracy, with scores ranging from 70.94% to 85.77%, making it ideal for tasks requiring both. While Token Masking (TM) eliminates bias completely, its accuracy, especially in hate speech detection, can drop to 61.71%, suggesting it is best suited for tasks focused on bias elimination over accuracy. The fine-tuning with both original and augmented data (FOA) offers a good compromise, achieving a bias reduction with an average accuracy of 79.95%. Fine-tuning with Original Data (FOD) has decent accuracy at 78.62% but is more prone to bias. Zero-Shot Inference, with low accuracy and high bias, is the least effective.\nQ2: How effective is the proposed method for bias reduction? To evaluate the effectiveness of each approach, we first calculate the normalized bias score for different tasks. We then compute overall average bias score by taking the numeric mean of these scores. We report the bias scores in table 3. From the table we observe that Token Masking (TM) was the most effective, achieving a bias score of 0, indicating complete bias elimination by masking names and gender terms. Joint"}, {"title": "6 Conclusion", "content": "This study reveals a significant presence of gender bias in Bangla pretrained language models across several classification tasks. Our bias mitigation approach effectively reduced bias while maintaining accuracy, indicating its potential for wider use in NLP tasks. These findings highlight the need to address gender bias to ensure fair outcomes in NLP applications, especially for under-resourced languages like Bangla."}, {"title": "7 Limitations", "content": "During dataset transformation, we swapped male-centric names with female-centric ones and vice versa using an NER model to identify names in the original texts. However, the accuracy of the NER model, particularly in Bangla, might be limited and might require improvement. Moreover, our approach assumes binary gender categories, excluding non-binary identities, which restricts the scope of bias detection. We did not use large language models (LLMs) in this study, which could limit the relevance of our findings for applications involving state-of-the-art models. Incorporating LLMs could potentially yield different results in terms of both accuracy and bias mitigation. Furthermore, we used consistent hyper parameters across all mitigation approaches, but altering these could affect the outcomes. Finally, the datasets may lack sufficient diversity and balance, impacting the generalizability of the results."}, {"title": "8 Ethical Considerations", "content": "Our research focuses on binary gender bias due to the structure of existing datasets and literature. We recognize the importance of non-binary and gender-fluid identities and encourage future work to include these dimensions for a more comprehensive approach to bias mitigation. Our datasets contain toxic and hate speech content, which may be offensive to some. We retained these data points to ensure that our models are exposed to realistic, harmful language, emphasizing the importance of bias mitigation in language models that interact with such content."}, {"title": "Prior Works", "content": "In this section, we discuss some of the relevant works on gender bias detection and mitigation."}, {"title": "Bias Detection Methods", "content": "Gender bias can manifest in both intrinsic and extrinsic forms, with intrinsic bias related to pre-trained language models and extrinsic bias to fine-tuned models. Various techniques have been proposed to measure bias accurately. For intrinsic bias, Tokpo et al. (2023) used the Sentence Embedding Association Test (SEAT) and Log Probability Bias Score (LPBS), while extrinsic bias was measured using True Positive Rate Difference (TPRD) and Counterfactual Fairness. They found that intrinsic bias metrics can be sensitive to mitigation techniques, which sometimes conceal rather than resolve bias. Su et al. (2023) proposed a method to detect gender bias in LLMs such as Alpaca, ChatGPT, and GPT-4, using In-Context Learning (ICL) and Reinforcement Learning (RL) to generate test cases. Jentzsch and Turan (2023) revealed significant gender biases in BERT models using the IMDB dataset, comparing seven BERT-base models for bias in sentiment classification. Sun et al. (2019b) examined gender bias through psychological tests like IAT and WEAT applied to GloVe and Word2Vec embeddings, using equity distance metrics such as FPED and FNED on the GBETs dataset. Tiwari et al. (2022) explored gender bias in Hindi and Tamil monolingual word embeddings using the WEAT metric, which aligned with human bias, while also assessing caste bias. Das et al. (2023) focused on gender and racial biases in Bangla NLP systems using the BIBED model and RDF, analyzing a diverse set of sentences from Wikipedia, Banglapedia, Bangla literature, and other sources."}, {"title": "Bias Mitigation Methods", "content": "Hort et al. (2023) identified three types of bias mitigation methods: Pre-processing, In-processing, and Post-processing. These methods aim to ensure fairness in machine learning models (ML). Tokpo et al. (2023) proposed strategies to mitigate gender bias in contextualized language models such as BERT-large and ALBERT-large, focusing on text classification tasks. Techniques like CDA pre-training, Context-derbies, and Sent-derbies were explored, addressing bias in pretraining data, procedures, or model outputs. Similarly, Su et al. (2023) addressed bias in Natural Language Generation (NLG) using algorithm-based Adversarial Learning and data-based Counterfactual Data Augmentation (CDA) to fine-tune models and reduce bias. Sun et al. (2019b) used debiasing methods in data manipulation, gender tagging for machine translation (MT), and bias fine-tuning, targeting FPED and FNED reductions via gender-name swapping. Xie et al. (2023) focused on gender bias evaluation through coreference resolution, using the Counter-GAP dataset from fictional texts, and demonstrated the effectiveness of name-based counterfactual data augmentation in mitigating bias over anonymization-based methods."}, {"title": "Bias Detection in Bangla", "content": "There has been some recent research works on bias detection in Bangla. Sadhu et al. (2024a) investigates bias in Bangla by creating a dataset for intrinsic bias measurement and adapting existing methods to examine how varying context length impacts bias metrics. Both Sadhu et al. (2024c) and Sadhu et al. (2024b) focused on bias in large language models (LLMs) for Bangla, but with different scopes. Sadhu et al. (2024c) examined both gender and religious biases in LLM outputs, introducing probing techniques to detect these biases, while Sadhu et al. (2024b) concentrate on gendered emotion attribution, using a zero-shot learning approach to explore emotional stereotypes linked to gender in Bangla. However, all these works focused on specific types of bias gender, religious, and emotional and the methodologies were used for only bias detection."}, {"title": "Bangla Gender Bias Dataset", "content": "To the best of our knowledge, there are currently no datasets specially designed to identify gender bias in Bangla classification tasks. Therefore, to explore gender bias, we curated four task specific datasets."}, {"title": "Dataset Collection", "content": "We collected datasets from four different sources along with the original labels.\n(a) Sentiment Detection. We utilized the dataset provided by (Sazzed, 2020), which consists of 11,807 texts among them 3,307 are negative (label 0) and 8,500 are positive (label 1). After keeping the samples that have gender terms, we got 4,857 instances.\n(b) Sarcasm Detection. We utilized the Ben-Sarc dataset provided by (Lora et al., 2023), which consists of a total of 25,636 Bangla social media comments where half of the data are sarcastic and rest half are non-sarcastic. This dataset had 9,589 texts with gender terms.\n(c) Toxicity Detection. For toxicity detection, we used a multi labeled dataset provided by (Belal et al., 2023), which has 16,073 sentences. Among them, 7,585 comments are labeled as non-toxic and 8,488 comments are labeled as toxic. As we needed only those texts that have gender terms, we"}, {"title": "Bias Dataset Construction", "content": "(a) Dictionary Construction. We start by creating a dictionary of gendered word pairs in Bangla which is utilized to distinguish gender-specific terms in texts. We collected these words from the internet and also from a few Bangla grammar books. This dictionary consists of 573 pairs of masculine and feminine words. All the gender terms can be found in Appendix H. For instance, \"\u099b\u09c7\u09b2\u09c7\u201d (son) and \u201c\u09ae\u09c7\u09df\u09c7\u201d (daughter) are included in a pair as key. In cases where certain words had multiple potential replacements, all the words were included. For example, the word \u201c\u09ad\u09be\u0987\u201d (brother)"}, {"title": "Name Replacement", "content": "After swapping gender-specific terms, some texts still contained names. To ensure consistency between names and gender in the converted texts, we swapped names based on their gender relevance using a Named Entity Recognition (NER) model\u00b2 which is trained on data"}, {"title": "Human Evaluation", "content": "In this study, we recruited four annotators who are undergraduate students with prior experience in Bangla grammar, spelling, and Named Entity Recognition tasks. These annotators were chosen from a pool of ten candidates. Initially, all participants took an exam with 50 questions focused on gender term conversion, correcting spelling errors, and identifying named entities. The four students with the highest scores were selected as annotators. After each step of the dataset construction process, the annotators manually reviewed the changes and"}, {"title": "Dataset Statistics", "content": "Table 6 presents the data distribution across all datasets. The equal number of samples between the original and transformed datasets ensures a fair comparison for evaluating bias. The average word count per sample offers insights into the typical length and complexity of the samples. Variations in the frequency of gender terms and name counts across tasks highlight the diversity of the datasets. These statistics are essential for understanding the composition and characteristics of each dataset, which plays a key role in analyzing the model's performance and potential biases."}, {"title": "Male and Female Names", "content": "We list the male and female names used for transforming task specific datasets below. We collected these names provided by (Das et al., 2023)."}, {"title": "Model Descriptions", "content": "(a) BanglaSARC 3. An ELECTRA (Clark, 2020) variant which is fine-tuned on Bangla sarcasm data provided by (Apon et al., 2022). The model has an embedding size of 768, vocabulary size of 32000 and it has a total of 110M parameters.\n(b) Abusive-MuRIL4. MuRIL is a BERT (Devlin et al., 2019) variant that has been pre-trained on 17 Indian languages, including their transliterated versions (Khanuja et al., 2021). Abusive-Muril is a fine-tuned MuRIL model pretrained for abusive hate speech detection task provided by (Das et al., 2022). The model has 110 million parameters and assigns an embedding size of 768 to each token. It uses a vocabulary of size 197285.\n(c) BanglaBERT-Sentiment. An ELECTRA variant which has been fine-tuned over a sentiment dataset provided by (Hasan et al., 2023). It has 110"}, {"title": "Bias vs Accuracy Trade off", "content": "Bias mitigation techniques can impact the accuracy of Bangla pre-trained language models (PLMs) in various ways. Token Masking (TM) is particularly effective at completely removing bias, but this often comes at the cost of reduced accuracy in certain tasks, highlighting a trade-off between bias reduction and model performance. Fine-tuning with Original and Augmented Data (FOA) shows stronger bias elimination compared to Joint Loss Optimization (JLO), but JLO consistently delivers better overall accuracy. JLO strikes a balance, reducing bias while maintaining high accuracy, making it a more reliable choice in scenarios where both fairness and performance are crucial. In addition, the task-specific nature of these models means that bias mitigation techniques may perform differently across tasks like sentiment analysis, toxicity detection, and hate speech detection. Figure 1 visualizes the trade-offs between accuracy and bias across these tasks, helping to better understand the impact of each technique."}, {"title": "Frequency of Bias", "content": "Table 7 presents the average number of mismatched predictions by task specific models between the original samples and the gender-name swapped samples across different mitigation techniques for the four tasks."}, {"title": "Gender Terms", "content": "The gender terms used for transforming the datasets are listed in table 9 for male terms and table 10 for female terms."}]}