{"title": "An Empirical Study on Context Length for Open-Domain Dialog Generation", "authors": ["Xinyi Shen", "Zuoquan Lin"], "abstract": "Transformer-based open-domain dialog models have become increasingly popular in recent years. These models typically represent context as a concatenation of a dialog history. However, there is no criterion to decide how many utterances should be kept adequate in a context. We try to figure out how the choice of context length affects the model. We experiment on three questions from coarse to fine: (i) Does longer context help model training? (ii) Is it necessary to change the training context length when dealing with dialogs of different context lengths? (iii) Do different dialog samples have the same preference for context length? Our experimental results show that context length, an often overlooked setting, deserves attention when implementing Transformer-based dialog models. Code is available at https://github.com/PKUAI-LINGroup/context-study.", "sections": [{"title": "Introduction", "content": "Since the advent of Transformer [10], language models trained on large-scale corpora have dominated the field of machine translation and other NLP tasks, including open-domain dialog generation [11,14]. Despite the success of Transformer-based dialog models, they were often criticized for not understanding dialog context [9,8], which can lead to generic responses [4] or self-contradictions [3]. For Transformer-based dialog models, context is usually represented as a concatenation of historical utterances. However, there is no uniform standard for deciding how many utterances to keep in a context. For example, Meena [1] limited the context to no more than seven utterances, while PLATO [2] limited the total length of the context sequence to no more than 256 tokens. We have no idea whether the context length they choose is optimal and how changing the context length would affect the performance of the model.\nIn this paper, we focus on the setting of context length in Transformer-based dialog models. We pose three questions about the possible impact of context length on the model: (i) Does longer context help model training? (ii) Is it necessary to change the training context length when dealing with dialogs of different context lengths? (iii) Do different dialog samples have the same preference for context length? Regarding model selection, since we care about the impact of the context length on the model rather than the absolute performance, we take two most basic practices to implement a dialog model: training a Transformer from scratch and fine-tuning a pre-trained GPT2 [7] model. Although the"}, {"title": "Experimental Setup", "content": "We treat the response generation problem as conditional language modeling. We denote a multi-turn dialog as $(u_1,u_2, \\dots, u_T)$, where ${u_{2k}}_{k=1}^{\\lfloor T/2 \\rfloor}$ are utterances from one speaker and ${u_{2k-1}}_{k=1}^{\\lceil T/2 \\rceil}$ are those from the other. The model is trained to maximize the conditional probability $P(u_T | C; \\theta)$, where $C = (u_{T-N}, \\dots, u_{T-1})$ is the context (dialog history), $N$ is the context window size, and $\\theta$ is the model parameters. We investigate the impact of context length on the model by controlling the size of $N$ during training and testing.\nExperiments are conducted on two widely used open-domain dialog datasets: DailyDialog [5] and PersonaChat [13]. For each multi-turn dialog, we train (or test) the model on each utterance except the first one. We study the effect of context length on the dialog models built on Transformer and GPT2. Specifically, we implement a Transformer model with three encoder layers, three decoder layers, two attention heads, and 256 hidden dimensions and train it from scratch on our experimental datasets. For GPT2, we choose its small version with 12 layers, 12 attention heads, and 768 hidden dimensions and initialize the model with the pre-trained parameters released by HuggingFace [12]. Models are optimized by AdamW [6]. The model checkpoints that perform best on the validation set are selected for testing. We choose Perplexity as the metric because of its strong correlation with human judgment [1] and widely used for dialog model evaluation [9,3,11]."}, {"title": "Results and Discussion", "content": "3.1 Does longer context help model training?\nWe first focus on the effect of context length on model training. Due to computational constraints, it is often impossible to feed the entire dialog history into the"}, {"title": "Is it necessary to change the training context length when dealing with dialogs of different context lengths?", "content": "Previous results concern the overall effect of training context length on the model. But if we take a deep look into the dataset, we find that the context length of the samples varies a lot, ranging from 1 to 25 in both test sets. So here we raise a new question: Do dialogs of different lengths have the same preference for models? To answer this question, we group the test data according to the context length and compare the performance of models trained with different context lengths in each group separately. We denote the model that achieves the lowest perplexity on the entire set as $M$, the model that achieves the lowest perplexity on group $g$ as $M_g$. For each $g \\in \\{short, medium, long\\}$, we measure the gap between $M$ and $M_g$ as\n$P_M(g) - P_{M_g}(g), \\qquad (2)$\nwhere $P_M(g)$ is the perplexity of $M$ on group $g$. As shown in Table 1, $M$ is optimal on half of all groups. On the remaining groups, the gap between $M$ and the optimal model is quite small. This result suggests that dialogs of different lengths do not have a clear preference for context length in the training phase. The model that performs best on the entire set is a proper choice for dialogs with varying history lengths."}, {"title": "Do different samples have the same preference for context length?", "content": "Previous experiments reflect the average performance on the test set, but not all dialog samples benefit from long context. To illustrate this, we split the test"}, {"title": "Conclusion", "content": "We conducted an empirical study on the context length of Transformer-based open-domain dialog models. We found that a carefully chosen context length balances performance and efficiency and that the overall best-performing model performs equally well on conversation data of different lengths. We pointed out that choosing the context length individually for each sample during the testing phase significantly improves the performance of the model.\nFor a dialog model to perform well, the context length in the training phase needs to be carefully considered. If we want the model to perform better, a potential direction is to learn the context length in the model."}]}