{"title": "Battling the Non-stationarity in Time Series Forecasting via Test-time Adaptation", "authors": ["HyunGi Kim", "Siwon Kim", "Jisoo Mok", "Sungroh Yoon"], "abstract": "Deep Neural Networks have spearheaded remarkable advancements in time series forecasting (TSF), one of the major tasks in time series modeling. Nonetheless, the non-stationarity of time series undermines the reliability of pre-trained source time series forecasters in mission-critical deployment settings. In this study, we introduce a pioneering test-time adaptation framework tailored for TSF (TSF-TTA). TAFAS, the proposed approach to TSF-TTA, flexibly adapts source forecasters to continuously shifting test distributions while preserving the core semantic information learned during pre-training. The novel utilization of partially-observed ground truth and gated calibration module enables proactive, robust, and model-agnostic adaptation of source forecasters. Experiments on diverse benchmark datasets and cutting-edge architectures demonstrate the efficacy and generality of TAFAS, especially in long-term forecasting scenarios that suffer from significant distribution shifts. The code is available at https://github.com/kimanki/TAFAS.", "sections": [{"title": "Introduction", "content": "Time series forecasting (TSF), which is one of the most core tasks in time series modeling, aims to predict future values based on historical data points. The widespread applications of TSF across various industries include but are not limited to: weather prediction (Verma, Heinonen, and Garg 2024), traffic forecasting (Liu et al. 2023a), stock market prediction (Li et al. 2023a), and supply chain management (Hosseinnia Shavaki and Ebrahimi Ghahnavieh 2023). Such a broad and over-arching impact of TSF results highlights the importance of developing a dependable time series forecaster, whose predictions maintain reliability despite changes in external factors.\n\nA critical bottleneck in the reliable deployment of pre-trained time series forecasters is created by the non-stationary nature of real-world time series data that leads to continuous data distribution shifts (Petropoulos et al. 2022). Previous works on alleviating the effect of non-stationarity aim to improve the robustness of time series forecasters through advancements in the pre-training process (Kim et al. 2021; Fan et al. 2023; Liu et al. 2024). Unfortunately, as the non-stationarity worsens the distributional discrepancy between training and test data over time, the pre-trained forecaster becomes increasingly unreliable, even if it has learned meaningful temporal semantics from training data (Kuznetsov and Mohri 2014). In Figure 1(a), we visualize how constantly evolving, non-stationary test data negatively affect the forecasting results of a pre-trained time series forecaster.\n\nThis shortcoming of existing approaches underscores the need to continuously adapt the pre-trained source forecaster on shifted test-time inputs while preserving its core semantics. Adapting the source forecaster to incorporate new time-variant semantics within test-time inputs allows it to reflect the ever-changing test distributions. In this regard, we pioneer a test-time adaptation (TTA) framework tailored for TSF (TSF-TTA). TTA, which has been primarily studied in the computer vision domain under classification settings (Wang et al. 2021; Niu et al. 2022, 2023; Lee et al. 2024), dynamically adjusts a pre-trained classifier on test inputs; this objective of TTA makes it well-aligned with the aforementioned motivation of adapting the source forecaster to newly arriving test data. Traditionally, TTA has operated under two main assumptions on the nature of test inputs. First, TTA assumes a complete absence of test labels because it is infeasible to hand annotate inputs at test-time. Second, because most of the image data are assumed to be Independent and Identically Distributed (IID), TTA generally operates under the same IID assumption. In TSF-TTA, however, these assumptions no longer hold due to the intrinsic characteristics of time series data.\n\nUnlike the first assumption of TTA, in TSF-TTA, ground truth for predicted time steps eventually becomes accessible, albeit in a delayed manner. For instance, when predicting electricity consumption for the next 30 days, the actual amount of electricity consumption becomes known to us after 30 days. Interestingly, as depicted in Figure 1(b), the sequential nature of time series makes ground truth values partially observable before acquiring the full ground truth. In the aforementioned example, the partial ground truth for the first 7 days is obtainable only after a week. Utilizing this partially-observable ground truth offers an invaluable opportunity to preemptively perform TSF-TTA prior to the arrival of full ground truth. Moreover, the second assumption is violated in TSF-TTA because temporal dependency exists in time series. This necessitates a technique for addressing the non-IIDness of time series on local (within window) and global (throughout the entire test-time) levels.\n\nBy considering these challenges and opportunities presented by properties of time series, we propose a Test-time Adaptive ForecAsting for non-stationary time Series (TAFAS) that is extensible to various TSF architectures. TAFAS consists of periodicity-aware adaptation scheduling (PAAS) and a gated calibration module (GCM). PAAS adaptively obtains partially-observed ground truth of sufficient length to represent semantically meaningful periodic patterns. After then, model-agnostic GCMs are adapted to calibrate test-time inputs such that they conform to the distribution the source forecaster effectively handles. The gating mechanism in GCMs controls how much the calibrated results should be utilized by considering global distribution shifts. Together, PAAS and GCM allow the source forecaster to be proactively adapted on non-stationary test-time inputs. Throughout the adaptation, the source forecaster remains frozen to preserve the core semantics it has learned from the extensive historical data. With the proactively adapted forecaster, TAFAS adjusts the latter part of the original predictions, where ground truths are yet to be observed, with the adapted predictions reflecting the distribution shift.\n\nComprehensive experimental results demonstrate that the TAFAS consistently enhances forecasting capabilities across source forecasters of various architectures. TAFAS leads to particularly large performance gains in long-term forecasting scenarios where distribution shifts become more pronounced. Moreover, the seamless integration of TAFAS with methods addressing the non-stationarity in pre-training stage and time series foundation models further enhances their ability to navigate test-time distribution shifts. Notably, TAFAS improves the forecasting error of Chronos (Ansari et al. 2024) on unseen test data streams by up to 45%.\nOur contributions are summarized as follows:\n\n\u2022 We pioneer test-time adaptation in time series forecasting (TSF-TTA) to address the non-stationarity in time series. Our examination of the properties of time series reveals the challenges in extending existing TTA frameworks to TSF, necessitating a new avenue to enable TSF-TTA.\n\u2022 We introduce TAFAS, a model-agnostic TSF-TTA framework that consists of periodicity-aware adaptation scheduling (PAAS) and Gated Calibration Module (GCM). These two technical components collectively enable proactive adaptation of the source forecaster on test-time inputs while preserving its core semantics.\n\u2022 TAFAS consistently excels in test-time adaptation across various TSF benchmark datasets and architectures, significantly improving test errors on highly non-stationary data and in long-term forecasting scenarios."}, {"title": "Related Works", "content": "As TSF has become a pivotal application in various industries, diverse TSF architectures have been developed. Due to the page limit, an exhaustive discussion on TSF architectures and TTA is included in Appendix. Here, we focus on studies that improve the time series forecaster by mitigating distribution shifts caused by the non-stationarity of time series. A line of studies introduces normalization and de-normalization modules before and after the forecaster to remove and restore the non-stationary statistics (Kim et al. 2021; Liu et al. 2022; Fan et al. 2023; Liu et al. 2024). RevIN (Kim et al. 2021) performs instance normalization with learnable scale and bias factors, whereas NST (Liu et al. 2022) uses a non-parametric approach without learnable transformations. Dish-TS (Fan et al. 2023) and SAN (Liu et al. 2024) perform statistical prediction both within the look-back window and between the look-back and prediction windows to appropriately execute normalization and denormalization. However, their generalization capability to the continuously evolving test data distribution is inherently limited as they address non-stationarity only within training distributions. Although TSF-TTA and online TSF share a common ground in learning from streaming inputs (Wen et al. 2024; Ao and Fayek 2023; Guo et al. 2016; Pham et al. 2023), their primary objectives are fundamentally different. Online TSF aims to train a time series forecaster from scratch with streaming data, but TSF-TTA aims to adapt a pre-trained source forecaster."}, {"title": "Challenges and Opportunities in TSF-TTA", "content": "Generally, TTA leverages unlabeled test-time inputs to adapt classifiers on shifted distributions. In this study, we aim to extend TTA to TSF to improve the ability of the pre-trained source time series forecaster to handle newly arriving test data. While the task characteristics of TSF and the properties of time series make applying the existing TTA frameworks to TSF a non-trivial problem, they also open new doors to develop a tailored approach to TSF-TTA. In this section, we highlight the challenges (C) and opportunities (O) unique to TSF-TTA after introducing task definition and formulations.\nTask definition & formulations. TSF is the task of predicting the future horizon window of H time steps $(\\{X_{t+1}, ..., X_{t+H}\\})$ given the past look-back window of L time steps $(\\{x_{t-L+1},..., X_{t}\\})$. $X_t \\in \\mathbb{R}^C$ denotes C number of variables observed at time t. To perform TSF, a time series forecaster $F_o: \\mathbb{R}^{L \\times C} \\rightarrow \\mathbb{R}^{H \\times C}$ is trained to predict subsequent future H time steps given L past time steps.\n\nThe train, validation, and test sets of TSF datasets are obtained by splitting a single continuous time series $\\{x_1,...,x_\\tau\\}$ in chronological order. Then, the (past, future) pairs are obtained using a sliding window, i.e., $(X_t, Y_t) = (\\{X_{t-L+1}, ..., X_{t}\\}, \\{X_{t+1}, ..., X_{t+H}\\})$. In non-stationary time series, data distribution continuously changes over time, resulting in distribution shifts between data splits and also within each split.\n\nC1. Entropy-based TTA losses are infeasible for regression-based TSF. TTA fundamentally assumes that ground truth labels are not available. Therefore, existing TTA methods utilize the entropy of predicted class probability distributions to formulate objective functions for the adaptation (Wang et al. 2021; Niu et al. 2022; Lee et al. 2024). However, TSF is a regression task, where the entropy of class probabilities is ill-defined, rendering the straightforward extension of entropy-based losses impossible.\nO1. Ground truth is accessible in TSF. In TSF, ground truth values become accessible as the predicted future time steps eventually arrive. Therefore, instead of entropy-based losses, the Mean Squared Error (MSE) loss, the de facto objective function used for regression tasks, can be used as a learning signal to perform TSF-TTA.\nC2. Full ground truth is accessible in a delayed time, resulting in a delayed adaptation. However, computing the MSE loss after observing full ground truth results in a delay of H time steps between the point of forecasting (t) and obtaining the full ground truth $t' = t + H$. Thus, na\u00efvely waiting for the arrival of full ground truth to perform TSF-TTA implies that none of the forecasted predictions in H time steps can be adapted. As the length of the forecasting window increases, the point at which full ground truth becomes obtainable is further delayed. This further delay in the point of adaptation inhibits performing TSF-TTA in a timely manner to reflect the adjacent distribution shifts.\nO2. Utilizing partially-observed ground truth enables proactive TSF-TTA. The sequential nature of test-time inputs makes ground truth partially observable before acquiring the full ground truth. After p time steps (p < H) from the forecasting time step t of $X_t$, the first p time steps out of the full ground truth (i.e., $\\{x_{t+1},...,X_{t+p}\\} \\in \\mathbb{R}^{p \\times C}$) are observable. Replacing the full ground truth in the MSE loss with its partially-observed counterpart reduces the adaptation delay and thus enables proactive adaptation."}, {"title": "TAFAS: Test-time Adaptive Forecasting for Non-stationary Time Series", "content": "In this section, we introduce TAFAS, a novel framework that considers the challenges and opportunities of TSF-TTA. The overall pipeline of TAFAS is provided in Figure 2."}, {"title": "Periodicity-Aware Adaptation Scheduling (PAAS)", "content": "To enable a proactive adaptation of the pre-trained source forecaster by reducing the adaptation delay, in TAFAS, we utilize partially-observed ground truth (POGT). However, as stated in Section O2., POGT does not eliminate the adaptation delay because to obtain POGT of length p, we must wait for p time steps. Therefore, choosing the appropriate value of p is important for balancing the trade-off between the amount of semantic information in POGT and the adaptation delay. When p is large, the POGT contains copious semantic information, but the adaptation delay increases, offsetting the advantage of employing the POGT. Conversely, when p is small, the forecaster can be adapted more proactively, but the POGT may contain meaningless patterns.\n\nTo ensure that the POGT incorporates semantically meaningful temporal patterns while preventing an excessive time delay, we introduce a periodicity-aware adaptation scheduling (PAAS) scheme that reflects the inherent periodic patterns of the test-time inputs. Several studies have demonstrated that the look-back window contains meaningful periodic patterns (Wu et al. 2023, 2021). PAAS thus extracts these patterns from the look-back window to determine p. From here on, $t_0$ denotes the time step at which the first test-time look-back window is obtained. PAAS applies variable-wise Fast Fourier Transform (FFT) on the first look-back window $X_{t_0}$ to identify the variable with the highest signal power (Eq. 1). Before FFT, the mean of $X_{t_0}$ is set to zero to remove the influence of bias. For the identified variable c*, PAAS calculates the amplitude of each frequency component to determine the dominant frequency (Eq. 2).\n\n$c^* = arg \\max_{c} ||FFT(X_{t_0}^c)||^2$   (1)\n\n$f^* = arg \\max_{f} ||FFT(X_{t_0}^{c^*})_f||^2$  (2)\n\nBased on the relationship between the frequency and period, PAAS derives the period of the dominant periodic patterns of $X_{t_0}$ and set it to the length of POGT as $p_{t_0} = \\frac{T}{f^*}$. The resulting periodicity-aware POGT $Y_{t_0}[: p_{t_0}]$ includes relevant semantics embodied in the dominant periodic patterns. Once $p_{t_0}$ is determined, $p_{t_0} + 1$ instances are aggregated into a test mini-batch: $\\{X\\}_{t_0+p_{t_0}} = \\{X_{t_0},..., X_{t_0+p_{t_0}} \\}$. When the subsequent look-back window arrives at time step $t_0 + p_{t_0} + 1$, PAAS is repeated to calculate the subsequent length of POGT adaptively. Considering the inherent periodic patterns vary across datasets and instances, the adaptive characteristic of PAAS assures data-agnostic TSF-TTA."}, {"title": "Gated Calibration Module (GCM)", "content": "Selecting the module to adapt is a critical design choice in TTA. Existing TTA methods generally adapt normalization layers, e.g., Batch Normalization (Ioffe and Szegedy 2015), to adjust the distributions of the intermediate features. However, state-of-the-art TSF methods adopt various forms of architectures, many of which are missing such normalization layers. Hence, we introduce a model-agnostic Gated Calibration Module (GCM) to guarantee that TAFAS can be applied generally to diverse pre-trained source forecasters.\n\nTAFAS adapts the GCM with the source forecaster frozen to preserve the core temporal semantics that the forecaster has learned from extensive historical data. GCM is attached to both the front and tail ends of the source forecaster, referred to as input and output GCMs. The input GCM maps the distribution-shifted test input $X_t$ to a calibrated input $X_t^{cali}$ that belong in a distribution the source forecaster can handle. The output GCM remaps the source forecaster's prediction $\\hat{Y}_t$ to $\\hat{Y}_t^{cali}$ in order to calibrate $\\hat{Y}_t$ back to the continuously changing test distribution.\n\nGCM consists of variable-wise temporal calibration and gating mechanisms to handle both the local (within the look-back window) and global (throughout the entire test-time) non-stationarity. The temporal calibration handles local distribution shifts by transforming the given window to calculate calibrated results. The gating mechanism handles global distribution shifts by updating how much to reflect calibrated results over time. Both temporal calibration and gating mechanisms are applied variable-wise, since each variable can have a different degree of non-stationarity. The two operations in the input GCM are expressed as the following:\n\n$GCM(X_t) = X_t + Tile(tanh(\\alpha)) \\odot (Concat(\\{W_c \\odot X_t\\}_{c=1}^C) + b),$ (3)\n\nwhere $W_c \\in \\mathbb{R}^{L \\times L}$, $b \\in \\mathbb{R}^{L \\times C}$, and $\\alpha \\in \\mathbb{R}^C$. Tile($\\cdot$) : $\\mathbb{R}^C \\rightarrow \\mathbb{R}^{L \\times C}$ broadcasts the gating vector in a temporal dimension, Concat($\\cdot$) concatenates the calibrated signals along variable dimension, and $\\odot$ denotes a Hadamard product. $W_c$ and $b$ are initialized to be zero so that at the first test time step when the test data diverge little from the training distribution, the original input is passed without calibration. The mechanism of the output GCM follows Eq. 3 with $X_t$ replaced with $\\hat{Y}_t$, and the dimensions of $W_c$ and b adjusted accordingly to H \u00d7 H and H \u00d7 \u0421.\n\nLet $t^*$ denote a time step at which PAAS calculates the POGT length ($t_0, t_0 +p_{t_0} +1,...$) and $p_{t^*}$ denote the POGT length computed at t*. After a test mini-batch $\\{X\\}_{t^*+p_{t^*}}$ is obtained at $t^* + p_{t^*}$, GCMs are adapted by minimizing the TAFAS loss defined as the following:\n\n$L_{partial} = MSE(\\hat{Y}_{t^*}^{cali}[: p_{t^*}], Y_{t^*}[: p_{t^*}])$  (4)\n\n$L_{full} = MSE(\\{\\hat{Y}_{t^*+p_{t^*}}^{cali}\\}_t, \\{Y\\}_{t^*+p_{t^*}})$ (5)\n\n$L_{TAFAS} = L_{partial} + L_{full},$   (6)\n\nwhere $t^*$ represents the most recent time step among the past POGT-computing steps whose corresponding mini-batches have now observed their full ground truths. $L_{partial}$ is computed between the first $p_{t^*}$ time steps of the calibrated prediction for $X_{t^*}$ whose periodicity-aware POGT is available"}, {"title": "Prediction Adjustment (PA)", "content": "Because the forecaster is proactively adapted, TAFAS can replace the latter part of original predictions, whose ground truths are yet to be observed, with adjusted predictions that reflect the distribution shift. After the forecaster is adapted at $t^* + p_{t^*}$, TAFAS recalculates the predictions for all look-back windows in $\\{X\\}_{t^*+p_{t^*}}$ and then substitutes the original predictions for time steps after $t^* + p_{t^*}$ with the adapted predictions. Specifically, for the look-back window $X_{t^*+k}$, where $k \\in \\{0,..., p_{t^*} \\}$, the corresponding prediction $\\hat{Y}_{t^*+k}$ predicts time steps $\\{(t^* + k + 1), . . ., (t^* + k + H)\\}$. For the time steps $\\{(t^* + p_{t^*} + 1),..., (t^* + k + H)\\}$ which are yet to be observed, TAFAS substitutes the original prediction $\\hat{Y}_{t^*+k}^{cali}$ with the adapted prediction $\\hat{Y}_{t^*+k}^{cali, adapted}$, that reflects distribution shifts as the following:\n\n$\\hat{Y}_{t^*+k, i}^{cali, adapted} = \\begin{cases}\\hat{Y}_{t^*+k, i}^{cali} & \\text{if } i \\leq (t^* + p_{t^*}) \\\\ \\hat{Y}_{t^*+k, i}^{cali} & \\text{if } i > (t^* + p_{t^*}).\\end{cases}$ (7)\n\n$\\hat{Y}_{t^*+k, i}^{cali, adapted}$ denotes the adapted prediction values for the time step i of $\\hat{Y}_{t^*+k}^{cali}$. We summarize the overall pipeline of TAFAS in Appendix."}, {"title": "Experiments", "content": "Experimental Setup\nDatasets. We demonstrate the effectiveness of TAFAS using the seven widely used multivariate TSF benchmark datasets: ETTh1, ETTm1, ETTh2, ETTm2, Exchange, Illness, and Weather (Wu et al. 2021). In Appendix, we report the results of the ADF test (Elliott, Rothenberg, and Stock 1992) to demonstrate that a substantial degree of non-stationarity exists in all these datasets.\n\nTime series forecasters. For the source time series forecasters, we adopt six state-of-the-art forecasters across various architectures: Transformer-based (iTransformer (Liu et al. 2023b), PatchTST (Nie et al. 2022)), Linear-based (DLinear (Zeng et al. 2023), OLS (Toner and Darlow 2024)), and MLP-based (FreTS (Yi et al. 2024), MICN (Wang et al. 2023)). Moreover, we verify the effectiveness of TAFAS on TSF foundation model (Ansari et al. 2024) pre-trained on a large corpus of time series data. TAFAS can be applied in combination with all of these forecasters regardless of the architecture design due to its fully model-agnostic design.\nImplementation details. Unless stated otherwise, we follow the standard protocol in TSF evaluation (Wu et al. 2023). We use the look-back window length L = 36 for Illness and L = 96 for the other datasets. For forecasting window length H, we evaluate on 4 different lengths, \u0397 \u0395 {24, 36, 48, 60} for Illness and H \u2208 {96, 192, 336, 720} for the other datasets. We split datasets in chronological order with the ratio of (0.6, 0.2, 0.2) for ETTh1, ETTm1, ETTh2, and ETTm2 and (0.7, 0.1, 0.2) for Exchange, Illness, and Weather to construct train, validation, and test sets. We repeat each pre-training run over three different seeds and select the pre-trained source forecaster with the lowest average validation MSE. More details on training processes are provided in Appendix."}, {"title": "TAFAS on Various TSF Architectures", "content": "Table 1 presents the MSE of forecasting results with and without TAFAS across various source TSF architectures and multiple forecasting windows. The full results, including Mean Absolute Error (MAE) and standard deviations, are reported in Appendix due to the space limit. TAFAS consistently reduces the forecasting error at test-time, effectively handling the test-time non-stationarity of time series. We highlight that the effectiveness of TAFAS at mitigating test-time distribution shifts remains strong across various architectures and datasets, consolidating its broad model- and data-agnostic applicability. Furthermore, when H = 336, TAFAS improves the average MSE of iTransformer and DLinear by 4.95% and 5.20%, respectively, and on an even longer forecasting window (H = 720), the performance improvement brought upon by TAFAS reaches 5.76% and 6.30%. These results indicate that TAFAS is particularly advantageous in long-term forecasting scenarios with more severe distribution shifts.\n\nTo further demonstrate the effectiveness of TAFAS at mitigating extreme test-time distribution shifts in long-term forecasting scenarios, we verify TAFAS under the forecasting lengths of \u0397 \u2208 {780,840,900} on DLinear. In Appendix, we plot the percentage of improvement in MSE achieved by TAFAS. Applying TAFAS exhibits a noticeable jump in performance improvement when compared to H = 336; we note that on the ETTh2 dataset, the degree of improvement increases by more than 8%."}, {"title": "Compatibility with Methods Addressing Non-stationarity in Pre-training time", "content": "One of the mainstream approaches to mitigating non-stationarity in TSF is to employ normalization and denormalization modules (Kim et al. 2021; Fan et al. 2023; Liu et al. 2024). However, they address non-stationarity only in the pre-training stage using training distributions, and thus, they may not generalize to consistently changing test distributions. As TAFAS is pluggable to any source forecasters in test-time, this compatibility can further enhance the robustness of source forecasters pre-trained with these widely adopted normalization modules. Table 2 presents the results of applying TAFAS on source forecasters equipped with RevIN (Kim et al. 2021), Dish-TS (Fan et al. 2023), or SAN (Liu et al. 2024). Across all datasets and architectures, TAFAS further improves the forecasting capability of these advanced source forecasters. The strength of TAFAS in long-range time series forecasting is again demonstrated here. For iTransformer with H = 720, TAFAS reduces the MSE of RevIN and SAN by 8.90% and 9.39% on average. Likewise, for DLinear with H = 720, TAFAS improves RevIN and SAN by 4.45% and 2.72% on average.\n\nInterestingly, the normalization approaches significantly increase the test MSE of the source forecaster in some experimental settings, e.g., from 0.367 to 1.071 in FreTS + RevIN on ETTm1 with H = 96. This observation highlights that addressing non-stationarity only in the pre-training phase can fail to generalize on the changing test distributions. In the above-mentioned setting, TAFAS improves the performance of FreTS + RevIN by 66.39%, indicating that it can overcome this limitation of pre-training-based approaches."}, {"title": "Unleashing the Knowledge of Foundation Models", "content": "Recently, TSF foundation models pre-trained on up to billions of time steps (Garza and Mergenthaler-Canseco 2023; Das et al. 2024; Ansari et al. 2024; Goswami et al. 2024) have shown promising forecasting performance. Here, we demonstrate that TAFAS can further improve the performance of such a powerful source forecaster. According to Table 3, TAFAS significantly improves the test MSE of Chronos (Ansari et al. 2024) on ETT datasets by up to 45%. The performance improvement is observed consistently on varying model sizes: small, base, and large. We highlight that ETT datasets were not included in pre-training data, demonstrating that TAFAS effectively adapts TSF foundation models to unseen time series data streams. The compatibility of TAFAS with foundation models alludes that it can adapt their predictions effectively without overwriting the rich semantic information encoded in these models."}, {"title": "Comparison with Online TSF Methods", "content": "As mentioned in Related Works, online TSF is another line of research that leverages sequentially arriving data to update forecasters, but it differs from of TSF-TTA in that online TSF trains forecasters from scratch, whereas TSF-TTA adapts a pre-trained source forecaster. Here, we compare TAFAS with online TSF methods to corroborate that TAFAS has tangible advantages over online TSF. Table 4 presents test MSE of the state-of-the-art online TSF methods: FS-Net (Pham et al. 2023) and OneNet (Wen et al. 2024), in the long-term forecasting scenario of H = 720. In most experimental settings, TAFAS significantly outperforms online TSF methods. The empirical superiority of TAFAS to online TSF can be attributed to the following factors: 1) proactive adaptation of forecaster using periodicity-aware POGT, 2) the timely adjustment of predictions to reflect adjacent distribution shifts, and 3) preservation of the knowledge in the source forecaster through the use of auxiliary non-stationarity-aware GCM modules."}, {"title": "Analysis of Each Technical Component in TAFAS", "content": "We explore alternative design choices for two main technical components in TAFAS. First, we study the effect of replacing PAAS with a fixed POGT length. Even though experiments are conducted with DLinear on all seven datasets, due to the page limit, Figure 3 only shows the results on the ETTh1 dataset. Extended results and the range of POGT lengths computed by PAAS are in Appendix. Figure 3 shows that too short or long POGT curtails the effect of TAFAS, supporting the motivation behind dynamically adjusting its length. Second, we demonstrate the effectiveness of introducing GCM. Table 5 presents forecasting errors for iTransformer with H = 720 when other modules inside iTransformer are adapted. Modifying internal modules results in reduced performance improvement. In some cases, the forecasting performance drops below the baseline, likely due to the overwriting of core semantics in the source forecaster.\nThe effectiveness of each component in TAFAS is shown through ablation studies in Appendix. Lastly, to validate that TAFAS can be deployed at test-time without significant hyper-parameter tuning, we demonstrate its robustness to changes in hyper-parameters in Appendix."}, {"title": "Conclusion", "content": "To address the ever-changing distributions in non-stationary time series, we propose TAFAS, a pioneering TSF-TTA framework that dynamically adapts the source forecaster at test-time while preserving the knowledge of the source forecaster. Using PAAS, the partially-observed ground truth with semantically meaningful patterns is acquired for proactive adaptation. Then GCM, which considers both local and global temporal distribution shifts is adapted to address changing distributions. TAFAS serves as a dataset- and model-agnostic framework, demonstrated by thorough experimental results and analyses. The TSF-TTA framework pioneered in our work paves a new avenue toward sustainable deployment of state-of-the-art time series forecasters."}, {"title": "Additional Related Works", "content": "Time Series Forecasting Models\nAs time series forecasting has become a pivotal application in various industries, diverse time series forecasting architectures have been developed to accurately predict future time steps (Wen et al. 2022; Masini, Medeiros, and Mendes 2023). They range from traditional statistical methods (Nelson 1998; Girard et al. 2002; Hyndman and Athanasopoulos 2018) to deep neural network-based architectures, including Transformers (Vaswani et al. 2017; Zhou et al. 2021; Li et al. 2019; Liu et al. 2021, 2023b; Wu et al. 2021; Nie et al. 2022; Zhang and Yan 2022; Kitaev, Kaiser, and Levskaya 2020; Zhou et al. 2022; Wu et al. 2022), Linear layers (Zeng et al. 2023; Li et al. 2023b; Toner and Darlow 2024), and MLPs (Yi et al. 2024; Wang et al. 2023; Ekambaram et al. 2023; Das et al. 2023; Xu, Zeng, and Xu 2024; Wang et al. 2024; Challu et al. 2022). Transformer-based models aim to capture temporal dependencies as well as inter-variable dependencies utilizing the attention mechanism. To address the quadratic time and memory complexity of self-attention operation, a line of work has been proposed to modify the self-attention module to be more efficient, facilitating long-term forecasting (Zhou et al. 2021; Kitaev, Kaiser, and Levskaya 2020; Li et al. 2019). Some works have revised the Transformer architecture to better exploit the properties of time series, such as sub-series periodicity or frequency information (Wu et al. 2021; Zhou et al. 2022). Other works remain the architecture untouched, but consider how to input time series by patching or inverting (Nie et al. 2022; Liu et al. 2023b). On the other hand, in response to a recent work that raised questions to the modeling capability of Transformer-based TSF models (Zeng et al. 2023), a series of Linear (Zeng et al. 2023; Li et al. 2023b; Toner and Darlow 2024) and MLP-based architectures (Yi et al. 2024; Wang et al. 2023; Ekambaram et al. 2023; Das et al. 2023; Xu, Zeng, and Xu 2024; Wang et al. 2024; Challu et al. 2022) have been developed, achieving comparable or outperforming TSF capabilities compared to Transformer-based models. Still, there is no consensus on the most representative TSF architecture, making the model-agnosticism of the TSF-TTA framework more desirable. More recently, time series foundation models, pre-trained on up to billions of time steps, have shown promising forecasting capabilities (Ansari et al. 2024; Das et al. 2024; Goswami et al. 2024; Garza"}]}