{"title": "EVALUATING TOOL-AUGMENTED AGENTS IN REMOTE SENSING PLATFORMS", "authors": ["Simranjit Singh", "Michael Fore", "Dimitrios Stamoulis"], "abstract": "Tool-augmented Large Language Models (LLMs) have shown impressive capabilities in remote sensing (RS) applications. However, existing benchmarks assume question-answering input templates over predefined image-text data pairs. These standalone instructions neglect the intricacies of realistic user-grounded tasks. Consider a geospatial analyst: they zoom in a map area, they draw a region over which to collect satellite imagery, and they succinctly ask \"Detect all objects here\". Where is here, if it is not explicitly hardcoded in the image-text template, but instead is implied by the system state, e.g., the live map positioning? To bridge this gap, we present GEOLLM-QA, a benchmark designed to capture long sequences of verbal, visual, and click-based actions on a real UI platform. Through in-depth evaluation of state-of-the-art LLMs over a diverse set of 1,000 tasks, we offer insights towards stronger agents for RS applications.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) demonstrate impressive potential in complex geospatial scenarios, augmenting remote sensing (RS) platforms with agents capable of sophisticated planning, reasoning, and task execution. These developments have sparked interest to deploy multimodal models across various RS tasks, including image captioning and visual question answering (VQA) (Yuan et al., 2022). Notably, SkyEyeGPT (Zhan et al., 2024) finetunes state-of-the-art VQA agents (Chen et al., 2023) on RS imagery for unified multimodal responses, while Remote Sensing ChatGPT (Guo et al., 2024) deploys computer-vision models (e.g., land use classification, object detection) via prompting. However, these approaches rely on chatbot-based templates with predefined text-image correlations over specific image files to assess LLM performance (Fig. 1 left), hence failing to capture the nuances of realistic user-grounded RS tasks.\nIn this work, we aim to bridge this gap with the following contributions: first, we introduce GEOLLM-QA, a novel benchmark of 1,000 diverse tasks, designed to capture complex RS workflows where LLMs handle complex data structures, nuanced reasoning, and interactions with dynamic user interfaces (Fig. 1 right). To this end, we harness recent advancements in benchmarking work for tool-augmented LLMs (Zhuang et al., 2023; Maini et al., 2024; Koh et al., 2024). Second, we adopt a comprehensive evaluation scheme (Maini et al., 2024) beyond traditional text-based metrics that accurately assesses an agent's proficiency in utilizing external tools for effective problem-"}, {"title": "THE GEOLLM-QA FRAMEWORK", "content": "Benchmarking Platform: To assess geospatial reasoning in an agent-assisted platform context, we draw inspiration from (Zhou et al., 2023) and we implement a benchmarking UI, as a realistic and reproducible standalone web-app that incorporates user-centered tasks with open-source tools and datasets. By leveraging open-source APIs, not only we address challenges of reproducibility and comparison across different systems, but also enable the examination of a wide range of RS use-cases through various input modalities including verbal, visual, and tactile interactions. The complete tool set consists of 117 tools, such as plotly mapbox APIs for the map functionality and LangChain routines for FAISS vectorstores (Douze et al., 2024), to name a few. We intend to release our codebase and benchmark to stimulate future research on geospatial Copilots.\nProblem Formulation: To denote RS tasks beyond simplistic VQA data-pairs, we model the problem after the realistic UI experience: intuitively, each interaction consists of the user question, the sequence of tool-calls by the agent, and the final (textual) response to user and platform state. We can therefore denote each task as {q, T, r, S}, where q is the user prompt, r is the textual response, while T represents the set of tool-calling steps T = {t1, t2, . . . }. At each step i, the agent invokes tool ti = {tooli, args**} \u2208 T from the available tool space T. Finally, S defines the final system state: e.g., map positioning, loaded database, visible data holdings, etc.\nData Sources: Our evaluation framework includes three representative large-scale datasets: xview1 (Lam et al., 2018), xview3 (Paolo et al., 2022), DOTA-v2.0 (Ding et al., 2021). Encompassing both optical and synthetic aperture radar (SAR) imagery, these data holdings offer detailed object annotations across 80 categories from a total of 5,000 images. Notably, these datasets come with valuable metadata, such as dates and coordinates, which greatly enhances the complexity of temporal and spatial RS scenarios in our benchmark. The satellite imagery serves as task context for LLM agents to execute function calls and is not used for finetuning the LLM or other downstream tasks, enabling our research-purposes investigation.\n\"Golden\" Detector Models: without loss of generality, we employ \u201coracle detectors,\" a common practice in foundation-models literature (Yang et al., 2023a), so that we can concentrate on evaluating the agent's proficiency in selecting and utilizing the appropriate tools without confounding"}, {"title": "EXPERIMENTS", "content": "In the scope of this analysis, we run various prompting techniques from literature: Chain-of-Thought (Wei et al., 2023), (MM-)ReAct (Yao et al., 2023; Yang et al., 2023b), and Chameleon (Lu et al., 2023). We leave more advanced prompting strategies for future investigation. Our baselines language models include GPT-4 Turbo (gpt-4-0125-preview) and GPT-3.5 Turbo (gpt-3.5-turbo-1106).\nTab. 1 summarizes our findings. The recent GPT-4 Turbo release exhibits impressive function-calling capabilities, while in terms of methods, CoT and ReAct outperform Chameleon in both correctness and success rates, while being more token efficient. With respect to other metrics, ROUGE-L shows the limitations of text-based scores, as it has been reported by recent work on foundation models comparing closed- and open-vocabulary answers (Majumdar et al., 2024). That is, the distribution of LLM answers is heavily dependent on the prompting method. For instance, answers generated by GPT-3.5 might artificially penalize a different response style by Chameleon if treated ground-truths (e.g., \u201cThere are five airplanes\u201d vs. \u201cThis image contains 5 planes\u201d can result in lower scores despite conveying the same fact). Last, we observe that detection-related metrics, as captured by recall, do not necessarily correlate with agent performance. All these findings confirm that, unlike existing RS benchmarks that mainly report detection results or captioning-related scores, a more comprehensive evaluation is required to assess agent performance."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "We presented GeoLLM-QA, a benchmark of realistic user-grounded tasks aimed at assessing the capabilities of tool-augmented LLMs in geospatial applications. Our hope is that this benchmarking suite will spur the development of new agents that advance the state of the art in remote sensing platforms. To this end, we would like to highlight some particularly exciting and promising areas for future work that we have identified through our research and that we are actively investigating. First, recent advances in multimodal modeling show improved performance compared to MM-ReAct-like prompting. We are currently extending our benchmark to flexibly incorporate open-source GPT-V model families, such as mini-GPT (Zhu et al., 2023; Chen et al., 2023). Additionally, we are expanding our analysis to replace oracle detectors with state-of-the-art models (Jian et al., 2023), to explore how agent errors interact with suboptimal detector performance.\nMoreover, a primary bottleneck that we have encountered with our approach, which is common in related work (Zhan et al., 2024), is the overhead of human-guided template generation. In our most recent study (Singh et al., 2024), we demonstrate that by adopting engine-based benchmarking methodologies (Zhou et al., 2023) in the remote sensing domain, we can leverage fully GPT-driven template and ground-truth generation to minimize human-in-the-loop overhead. Lastly, by considering cost- and system-related aspects, our analysis has yielded interesting insights regarding optimizing the overall agent-platform implementation. Our ongoing explorations include methods to improve performance by leveraging state-of-the-art LLM caching and compression techniques (Jiang et al., 2023; Fore et al., 2024)."}, {"title": "the false positives/negatives of a non-optimal detector. By abstracting out detection errors, we can measure any degradation in performance metrics directly attributable to agents' failures.", "content": "For instance, consider a scenario where the LLM is instructed to \u201cdetect all airplanes at the Mexico City airport using the YOLO detector.\u201d We want to verify whether the agent can designate the right detector, filter through the correct imagery, and specify the right classes. Therefore, upon the LLM's selection of an image set, we assume an oracle detector that provides 100% accurate detections, i.e., \"gold\" results directly from dataset ground truths. We then calculate the recall of these detector \"results\", attributing any discrepancies solely to the agent's inability to accurately fulfill the task.\nBenchmark Creation: To create a representative set of RS tasks, GeoLLM-QA adopts the three-step benchmarking process presented in (Zhuang et al., 2023): 1. Reference Template Collection: we curate a set of 25 template questions that cover the wide range of RS tasks, such as object detection, change detection, etc. Several key tasks are shown in Fig. 2. To generate answers for these questions, we guide GPT-4 to reach the answers via a simple human-in-the-loop mechanism via feedback UI buttons (Ouyang et al., 2022). By using previous (un)successful attempts as in-context examples, GPT can quickly help us create the Reference Templates.\n2. LLM-guided Question Generation: we generate permutations and perturbations of the Reference Templates. Note here that previous RS benchmarks assume that all LLM tasks are implicitly correct. However, Maini et al. (2024) show that one of the most challenging aspects of agent performance is their ability to handle prompts that maintain the general template of a genuine question but are factually incorrect. We therefore assume a ratio of 9:1 correct:incorrect tasks and we use GPT-4 to generate variations per template for a total of 1,000 tasks. To allow GPT-4 to \"programmatically\" select from real data combinations, we provide in-context prompt with dataset descriptions, e.g., SQL schemas with all eligible category names in the xviewl database."}, {"title": "Metrics: Unlike existing VQA-based benchmarks, we consider a a comprehensive set of metrics that capture the LLM's ability for effective tool-calling and reasoning:", "content": "a. Success rate: the ratio of successfully completed tasks across the entire benchmark. Each task is consider to be completed correctly when the final platform state S matches the S ground-truth. This ratio informs us of the degree to which the agent is able to complete tasks, irrespective of whether it took incorrect or unnecessary intermediate steps.\nb. Correctness ratio: the ratio of correct function-call operations across the benchmark. Given a ground-truth tool-set T and an LLM solution T, we track all applicable LLM error-types as defined in (Zhuang et al., 2023) (i.e., \"Infeasible Action\", \"Function Error\", \"Argument Error\", \"Incorrect Data Source\", and \"Omitted Function\"). Given the total number of errors and ground-truth tools, we compute the correctness ratio Rcorrect = max(0,1 - Nerrors/Ntools) (Maini et al., 2024). This metric captures how likely it is for the agent to invoke the correct functions in the expected order.\nc. ROUGE score: we use the ROUGE-L recall score (Lin, 2004) to compare model answers a with the ground truth \u00e3 to assess the ability of the agent to reply to the task at hand.\nd. Cost (Tokens): we compute the average number of tokens per task over the entire benchmark.\ne. (Detection) Recall: over the entire benchmark, we assess the agents ability to correctly return detection tasks by calculating the overall recall R (i.e., detections returned by the method against \"gold\" ground-truths from oracle detectors)."}]}