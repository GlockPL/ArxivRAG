{"title": "LEARNING CAUSALLY INVARIANT REWARD FUNCTIONS FROM DIVERSE DEMONSTRATIONS", "authors": ["Ivan Ovinnikov", "Eugene Bykovets", "Joachim M. Buhmann"], "abstract": "Inverse reinforcement learning methods aim to retrieve the reward function of a Markov decision\nprocess based on a dataset of expert demonstrations. The commonplace scarcity and heterogeneous\nsources of such demonstrations can lead to the absorption of spurious correlations in the data by\nthe learned reward function. Consequently, this adaptation often exhibits behavioural overfitting to\nthe expert data set when a policy is trained on the obtained reward function under distribution shift\nof the environment dynamics. In this work, we explore a novel regularization approach for inverse\nreinforcement learning methods based on the causal invariance principle with the goal of improved\nreward function generalization. By applying this regularization to both exact and approximate\nformulations of the learning task, we demonstrate superior policy performance when trained using\nthe recovered reward functions in a transfer setting", "sections": [{"title": "Introduction", "content": "In the domain of reinforcement learning, the formulation of a suitable reward function plays a pivotal role in shaping\nthe behaviour of decision making agents. This is commonly justified by the widely adopted belief that the reward\nfunction is a succinct representation of a task goal in a given environment specified as a Markov decision process (MDP)\n(Ng et al., 2000). Eliciting the correct behavioural policies via the optimization of a reward function is of paramount\nimportance for the deployment of RL agents to real world domains such as various robotics scenarios (Pomerleau, 1991;\nBillard et al., 2008) or expert behaviour forecasting (Kitani et al., 2012). However, the challenge of designing such a\nfunction typically entails a cumbersome and error-prone process of handcrafting a heuristic reward signal that accounts\nfor all the intricacies of the task at hand.\nInverse reinforcement learning (IRL) methods aim to solve the problem of inferring the reward function of an MDP\nbased on a dataset of temporal behaviours. These trajectories are typically obtained from an agent that is assumed to\ndemonstrate near-optimal performance in the respective MDP. There are multiple benefits to learning an explicit reward\nfunction compared to alternatives such as behavioural cloning (Pomerleau, 1991) or other imitation approaches (Ho\nand Ermon, 2016) including the ability to transfer the reward function across problems and enhanced robustness to\ncompounding errors (Swamy et al., 2021).\nThe IRL problem is challenging due to a number of factors. The problem of recovering the reward from the statistics of\nexpert trajectories is generally ill-posed, as there typically exist many reward functions which satisfy the optimization\nconstraints (Ng et al., 1999). While this property is effectively tackled by regularization in the form of a maximum\nentropy objective (Ziebart et al., 2008, 2010), scaling up IRL to handle large-scale problems remains a challenge. In\nparticular, it requires a variational formulation dependent on non-linear function approximation methods (Finn et al.,\n2016a; Fu et al., 2017). Since the amount of available expert data is typically limited, this can lead to overfitting\nphenomena, which are particularly pronounced in highly parameterized models such as neural networks (Ying, 2019;\nSong et al., 2019). An additional difficulty arises when there is a significant discrepancy between expert demonstrations\noriginating from different experts. We show that pooling expert demonstrations in one dataset under the same label\nintroduces spurious correlations which are absorbed in the representation of the reward function. By optimizing the\nexpected cumulative reward defined by such functions, the agent might fail to learn meaningful behaviours due to\noptimization of the spurious correlations present in the reward model."}, {"title": "Method", "content": "In order to circumvent this issue, we consider causal properties of the reward learning problem. The causal invariance\nprinciple (Peters et al., 2015; Heinze-Deml et al., 2017; Arjovsky et al., 2019) studies the generalization problem of\nsupervised learning models through the lens of causality. It postulates that the conditional distribution of the target\nvariable must be asymptotically stable across samples obtained under observational and interventional settings of the\ndata generating process. By only considering causally invariant representations of the input, this ensures avoiding the\nreliance on spurious correlations in the model predictions. We propose to adapt this principle for the context of reward\nfunction learning, where we aim to elicit behavioural policies without exploiting spurious reward features, i.e. features\nwhich would prevent the reward from providing a meaningful training signal under distribution shift. To achieve this\ngoal, we make the assumption that variations in expert demonstrations are a product of causal interventions on the data\ngenerating process of trajectories. Under this assumption, the conditional distribution describing the optimality of a\ntransition must be stable for experts demonstrations gathered on a specific task. By applying the causal invariance\nprinciple under this assumption, we show that we can recover reward functions which are invariant across a population\nof experts and demonstrate improve generalized w.r.t. certain types of distribution shift."}, {"content": "In this section, we describe our method. Section 2.1 presents the problem setting of learning rewards in the maximum\nentropy IRL setting (Ziebart et al., 2008) in both primal and dual form and reviews the connection to a class of\nadversarial optimization methods based on distribution matching. In section 2.2, we outline how spurious correlations\narise in the context of the IRL problem and connect this to the causal invariance principle. Section 2.3 shows how to\nincorporate this principle as a regularization strategy for reward learning."}, {"title": "Problem setting", "content": "We begin by giving an overview of the problem setting. We consider environments modelled by a Markov decision\nprocess (S, A, \u03a4, \u03bc\u03bf, R, \u03b3), where S is the state space, A is the action space, T is the family of transition distributions\non S indexed by S \u00d7 A with p(s'|s, a) describing the probability of transitioning to state s' when taking action a in\nstate s, \u00b5o is the initial state distribution, R : S \u00d7 A \u2192 R is the reward function and \u03b3 \u2208 (0, 1) is the discount factor.\nA policy \u03c0 : S \u00d7 A \u2192 \u03a9(A)\u00b9 is a map from states s \u2208 S to distributions \u03c0(\u00b7|s) over actions, with \u03c0(\u03b1|s) being the\nprobability of taking action a in state s.\nIn absence of a given ground truth reward function, inverse reinforcement learning methods aim to estimate a suitable\n(i)\nreward function based on a dataset of expert trajectories DE = {fi}i<K where i = (S1:T, (sir, ar) 1:T is a sequence of\nstates and actions of expert i of length T\u2081. To achieve this goal, a number of methods based on distribution matching\nmay be used. Such methods typically minimize a divergence measure (Csisz\u00e1r, 1972) between the expert trajectories\nand the trajectories induced by a policy optimizing the estimated reward.\nWe begin by presenting maximum entropy IRL (MaxEntIRL) (Ziebart et al., 2008), which serves as a foundation for\nmost of these methods. In MaxEntIRL, a feature matching approach is used to learn a reward function r\u03c8,\u03c6 = \u03c8Tp(s),\nwhere the state features 6(s) may be specified a priori or learned using a using a neural network model (Wulfmeier\net al., 2015). The policy is trained by optimizing the expected cumulative reward using the reward function estimate.\nThe model describes the fact that the expert trajectories are sampled from a Gibbs distribution defined over trajectories:\np(\\xi|\\psi, \\varphi) = \\frac{1}{Z_{\\varphi,\\psi}} \\exp(\\varphi^T r_{\\psi}(\\xi))\nwhich corresponds to the solution of the entropy maximization problem of the trajectory distribution under feature\nmatching and normalization constraints."}, {"title": "Spurious correlations and causal invariance approaches", "content": "We shall now outline the intuition as to what we consider spurious correlations in inverse reinforcement learning. It is\nnecessary, at this point, to introduce structural causal models, which will help define the notion of spurious correlations.\nA structural causal model (SCM) (Pearl, 2009) is defined as a tuple G = (S, P(\u20ac)), where P(\u03b5) = \u03a0\u03af<\u03ba \u03a1(\u03b5\u03af) is a\nproduct distribution over exogenous latent variables \u025bi and S = {f1, ..., fk } is a set of structural mechanisms where\npa(i) denotes the set of parent nodes of variable xi: Xi := fi(pa(xi), \u03b5i) for i \u2208 |S|. G induces a directed acyclic graph\n(DAG) over the variables nodes xi. The SCM entails a joint observational distribution P6 = \u041f\u0456\u2264\u043a P(xi|pa(xi)) over\nvariables xi conditioned on the parents of xi for some probability distribution p(\u00b7|pa(xi)) describing the mechanism fi.\nInterventions on G constitute modifications of structural mechanisms fi yielding interventional distributions Ps. In the\ncontext of IRL, we consider interventional settings of the expert trajectory distribution p(\u03be|\u03c8, \u03c6) in eq. (2).\nIn supervised learning, a correlation between the input representation and the label is considered spurious if it does\nnot generalize under distribution shift, e.g. when the trained model is evaluated on a test set of examples sampled\nout-of-distribution. More specifically, the distribution shift constitutes an intervention on the causal parents of the target\nlabel, which allows the application of invariance based approaches, that we will describe below. In the case of IRL, we\nconsider a correlation to be spurious when a reward function does not generalize to perturbations in the initial measure\nor dynamics of the MDP, i.e. the policy optimizing a reward that reflects this correlation will absorb this signal and fail\nto perform optimally under perturbed environment dynamics."}, {"title": "Reward regularization using causal invariance", "content": "Mapping the objective in eq. (10) to the context of reward learning, we consider data gathered by different experts\nto correspond to interventional settings of the trajectory distribution in eq. (2), where the interventions reflect the\nvarying preferences exhibited by the policy of the respective experts. The stable conditional we would like to identify\ncorresponds to the conditional distribution of the optimality label P(Ot|st, at). To do so, we invoke the causal invariance\nprinciple to learn reward functions which utilize features which are invariant to some class of deviations exhibited by\nthe experts. We motivate this by the fact that despite the discrepancies in the demonstrations, all experts are assumed\nto perform the task in an optimal fashion with respect to the true task goal. This implies that all experts, at least in\npart, optimize the same underlying reward that we would like to recover. In doing so, we hope to extract succinct"}, {"title": "Problem proofs", "content": "Proposition 1. Let the likelihood p(\u03be) belong to a natural exponential family with parameter \u03c8, sufficient statistics\n(x) and the (Lebesgue) base measure po. Let De be the dataset corresponding to interventional setting e. Then,\nfor all e \u2208 Etr, the causal invariance penalty for the maximum likelihood loss is the norm of the sufficient statistics\nexpectation difference:\nD(\\psi, \\varphi; e) = ||\\nabla_{\\psi}|_{\\psi=1.0}L^{\\mathcal{E}}(\\psi, \\varphi)||^2 = ||E_{\\mathcal{D}^e} [\\varphi(\\xi)] \u2013 E_{p(\\xi|\\psi)} [\\varphi(\\xi))]||^2\nProof. The result directly follows from the definition of the primal problem.\n\\nabla_{\\psi}|_{\\psi=1.0}L^{\\mathcal{E}}(\\psi, \\varphi) = \\nabla_{\\psi} E_{\\xi\\in\\mathcal{D}^e} \\Bigg[ log \\frac{1}{Z_{\\psi,\\varphi}} exp(r_{\\psi}(\\xi)) \\Bigg]\n= E_{\\xi\\in\\mathcal{D}^e} [\\nabla_{\\psi}(\\psi \\varphi(\\xi) \u2013 log Z_{\\psi,\\varphi})]\n= E_{\\mathcal{D}^{\\mathcal{E}}} [\\varphi(\\xi)] \u2013 \\nabla_{\\psi} log Z_{\\psi,\\varphi}\n= E_{\\mathcal{D}^{\\mathcal{E}}} [\\varphi(\\xi)] \u2013 E_{p(\\xi|\\psi)} [\\varphi(\\xi)]\nwhere we use the moment generating property of the log partition function \\nabla_\\psi log Z_{\\psi,\\varphi} = E_{p(\\xi|\\psi)}[\\varphi(\\xi)].\nProposition 2. The gradient of the primal exponential family maximum-likelihood problem in Equation (3) w.r.t. the\nnatural parameter \u03c8 is equivalent to the gradient of the dual in Equation (5) w.r.t the parameter & when the density\nratio is unity.\np\n||\\nabla_{\\phi}L_{dual}(\\psi, \\varphi, q, e)||^2 = ||min_q E_{\\xi\\sim\\mathcal{D}^e} [\\varphi(\\xi)] \u2013 E_{\\xi\\sim p(\\xi|\\psi,\\varphi)} \\Bigg[ \\frac{q(\\xi)}{p(\\xi|\\psi, \\varphi)} \\Bigg] \\varphi(\\xi) ||^2\nProof. We begin by computing the gradient of\nmin [E_{\\xi\\sim\\mathcal{D}^{\\mathcal{E}}} [g_{\\psi,\\varphi}(\\xi)] \u2013 E_{\\xi\\sim q}[g_{\\psi,\\varphi}(\\xi)] + D_{KL}(q||P_O)]\nw.r.t. to the parameters & for setting e \u2208 Etr:\n\\nabla_{\\psi}L_{dual}(\\psi, \\varphi, q, e) = \\nabla_{\\psi}\\Bigg(min_q [E_{\\xi\\sim\\mathcal{D}^{\\mathcal{E}}} [g_{\\psi,\\varphi}(\\xi)] \u2013 E_{\\xi\\sim q}[g_{\\psi,\\varphi}(\\xi)] + D_{KL}(q||P_O)] \\Bigg)\n\\stackrel{(1)}{=} \\nabla_{\\psi}\\Bigg(min_q [E_{\\xi\\sim\\mathcal{D}^{\\mathcal{E}}} [g_{\\psi,\\varphi}(\\xi)] \u2013 E_{\\xi\\sim q}[g_{\\psi,\\varphi}(\\xi)] + \\mathcal{H}(q)] \\Bigg)\n\\stackrel{(2)}{=} min_q [\\nabla_{\\psi} (E_{\\xi\\sim\\mathcal{D}^{\\mathcal{E}}} [g_{\\psi,\\varphi}(\\xi)] \u2013 E_{\\xi\\sim q}[g_{\\psi,\\varphi}(\\xi)] + \\mathcal{H}(q))]\n\\stackrel{(3)}{=} min_q [E_{\\xi\\sim\\mathcal{D}^{\\mathcal{E}}} [\\nabla_{\\psi} g_{\\psi,\\varphi}(\\xi)] \u2013 E_{\\xi\\sim q}[\\nabla_{\\psi} g_{\\psi,\\varphi}(\\xi)]]\nwhere we use: (1) the fact that we assume the base measure po to be the Lebesgue measure (or count measure in the\ndiscrete case), i.e. po(\u03be) = 1, (2) the envelope theorem and (3) the fact that q does not directly depend on 4 and thus,\n\u2207\u2084H(q) = 0. We can rewrite the second expectation using the importance sampling trick:\nmin E_{\\xi\\sim\\mathcal{D}^{\\mathcal{E}}} [g_{\\psi,\\varphi}(\\xi)] \u2013 E_{\\xi\\sim q}[g_{\\psi,\\varphi}(\\xi)] = min_q min_q E_{\\xi\\sim\\mathcal{D}^{\\mathcal{E}}} [g_{\\psi,\\varphi}(\\xi)] \u2013 E_{\\xi\\sim p(\\xi|\\psi,\\varphi)} \\Bigg[ \\frac{q(\\xi)}{p(\\xi|\\psi, \\varphi)} \\Bigg] g_{\\psi,\\varphi}(\\xi)\nBy definition of the distribution matching problem and strict concavity w.r.t. q, the optimum is attained when\nq(\u03be) = p(\u03be|\u03c8, \u03c6), i.e. when the importance sampling ratio is 1."}]}