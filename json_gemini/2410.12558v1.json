{"title": "A Claim Decomposition Benchmark for Long-form Answer Verification", "authors": ["Zhihao Zhang", "Yixing Fan", "Ruqing Zhang", "Jiafeng Guo"], "abstract": "The advancement of large language models (LLMs) has significantly boosted the performance of complex long-form question answering tasks. However, one prominent issue of LLMs is the generated \"hallucination\" responses that are not factual. Consequently, attribution for each claim in responses becomes a common solution to improve the factuality and verifiability. Existing researches mainly focus on how to provide accurate citations for the response, which largely overlook the importance of identifying the claims or statements for each response. To bridge this gap, we introduce a new claim decomposition benchmark, which requires building system that can identify atomic and checkworthy claims for LLM responses. Specifically, we present the Chinese Atomic Claim Decomposition Dataset (CACDD), which builds on the WebCPM dataset with additional expert annotations to ensure high data quality. The CACDD encompasses a collection of 500 human-annotated question-answer pairs, including a total of 4956 atomic claims. We further propose a new pipeline for human annotation and describe the challenges of this task. In addition, we provide experiment results on zero-shot, few-shot and fine-tuned LLMs as baselines. The results show that the claim decomposition is highly challenging and requires further explorations. All code and data are publicly available 1.", "sections": [{"title": "1 Introduction", "content": "In recent years, LLMs have demonstrated excellent performance in various domains of Natural Language Processing (NLP) due to their robust natural language capabilities. Through training on extensive data with a large number of parameters, these models have shown significant advancements in their ability to understand and process natural language. One of the most notable improvements in LLMs is their ability for complex reasoning and the generation of lengthy, coherent responses, which has greatly enhanced their capability to answer complex, long-form questions. However, the powerful generation capabilities of these models have also led to the emergence of the so-called \"hallucination\" problem, where"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Claim Decomposition Methods", "content": ""}, {"title": "Lexical Parsing Methods", "content": "The traditional Claim decomposition task aims to identify the atomic claims within complex sentences, which is commonly used for tasks such as summarization, argument mining, and question answering. Before the advent of large language models, most related research was based on lexical parsing methods. The DCP [8] method extracts the verb phrase components and clauses from complex sentences as criteria for summarization tasks. The DisSim [18] framework transforms complex sentences into a more regular intermediate representation by dividing them into a two-layered semantic hierarchy consisting of core facts and accompanying context. Moreover, the Pred-Patt [25] framework decomposes complex sentences by extracting predicate argument structures from syntactic dependency parsing based on the Universal Dependencies [4] Project. However, these methods based on lexical parsing typically only generate a structured representation of claims, rather than complete and coherent sub-claims, thereby constraining their application in downstream NLP tasks such as fact-checking and answer verification."}, {"title": "LLM Prompting Approaches", "content": "Upon their emergence, LLMs have garnered widespread attention across various domains of natural language processing due to their outstanding natural language comprehension and generation capacities. Recent research pertaining to claim decomposition tasks has also capitalized on the language ability of LLMs. Most of these LLM-based approaches are implemented through prompts [3,12,13,17,23]. By providing examples that are carefully crafted by humans, they instruct LLMs to complete this task through in-context learning. In contrast to lexical parsing methods, these LLM prompting approaches are more flexible and unstructured, enabling the generation of coherent and fluent claims. Nonetheless, the LLM prompting approaches are also affected by the hallucinations inherent in LLMs, which can result in the generation of sub-claims that deviate from the original text. Furthermore, to ensure the quality of claim decomposition, researchers are required to provide enough in-context examples, leading to the prompts often being excessively lengthy. The excessive length of these prompts can lead to a reduction in inference efficiency and an increase in token costs."}, {"title": "2.2 Answer Verification", "content": "The verification of the extent to which LLM-generated responses are supported by the context or retrieved evidence is highly dependent on the claim decomposition task. FactScore [17] evaluates the factual precision of the LLM-generated responses through decompose them into sub-claims and verify each claim using a knowledge source. Factcheck-GPT [23] introduces an end-to-end annotation framework that assesses the factual accuracy of LLM-generated responses through a combined approach that involves both LLM and human evaluation."}, {"title": "3 The Atomic Claim Decomposition Task", "content": "In this section, we first introduce the definition of atomic claim along with its respective characteristics, followed by an exposition of the definition and challenges associated with the claim decomposition task."}, {"title": "3.1 Atomic Claim Definition", "content": "The definition of atomic of Claims has long been a contentious issue. Wang [23] believes that it is challenging to define atomic and then determine the granularity of decomposition. Unfortunately, their discourse on atomic did not arrival at a definitive conclusion. Wanner [24] traces this issue back to the philosophical level. They employ Bertrand Russell's philosophy of logical atomism [20] and the neo-Davidsonian analysis [14] as the theoretical guidance, and manually decomposed 21 examples as the guidance for LLMs to perform the claim decomposition task. We also fellow the guidance of Bertrand Russell's philosophy of logical atomism and provide the following definition for an atomic claim:\nDefinition 1. An atomic claim delineates an indivisible minimal fact, which either describes a property of an individual or the relationship between individuals.\nAn atomic claim should possess the following properties:\nIndivisibility An atomic claim should delineates a single, indivisible fact, which means that it cannot be decomposed into simpler, more fundamental claims without loss of meaning.\nSemantic integrity An atomic claim should contain sufficient information to preclude any inconsistencies or ambiguities with the original claim.\nVerifiability An atomic claim should be verifiable. This means that it should delineates a fact that is worthy to be verified, rather than a commonsense or subjective opinion.\nContext independence An atomic claim should be context-independent, therefore its truthfulness can be assessed independently without preceding and following context."}, {"title": "3.2 Atomic Claim Decomposition Task", "content": "This study primarily focuses on the Long-form Question Answering (LFQA) task. Long-form question answering aims at answering complex, open-ended"}, {"title": "4 Chinese Atomic Claim Decomposition Dataset", "content": "To facilitate research on atomic claim decomposition task, we introduce Chinese Atomic Claim Decomposition Dataset, a dataset designed for claim decomposition task within the LFQA scenario. We first select data from WebCPM [19] dataset and then annotate them by our human-annotation pipeline, which includes three steps: sentence decomposition, sentence classification and atomic claim decomposition. Figure 2 shows a example of our data annotation pipeline."}, {"title": "4.1 Data Collection", "content": "WebCPM, an innovative open-source project, is designed to advance the field of interactive search research. Utilizing Chinese pre-trained models, this project imitates human web search behaviors and answers questions based on the facts collected from the websites it returns. It introduces a dataset that contain questions and LLM generated answers, which is suitable for our task. Therefore, We sample the first ten percent (550 of 5500) of WebCPM dataset to serve as the foundation for our subsequent manual annotation."}, {"title": "4.2 Data Annotation", "content": "Our data annotation pipeline can be divided into three parts. First, we decompose the whole response into sentences. Then, we classify each sentence into four categories. Finally, for sentences that have been labeled as fact, we decompose them into atomic claims.\nSentence Decomposition Given a response generated by LLMs, it is infeasible to directly decompose it into atomic claims. Thus, we first decompose it into nature language sentences using Natural Language Toolkit (NLTK), which allows annotators to focus more on the decomposition of atomic claims within individual sentences, thereby reducing the difficulty of annotation. Along with sentence decomposition, we also filter out the citation marks in the responses.\nSentence Classification Not all claims in a response are worthy to be verified. For example, subjective opinions like \"I think the iPad 10 has a very high cost performance ratio. \" or instructions like \"Don't blindly follow the trend anymore. \" do not need to check their truthfulness. A claim is considered checkworthy if it is one for which the general public has an interest in knowing the truth [10]. However, we assume that people who ask LLMs are interested in all factual claims in the response.\nConsidering the characteristics of LFQA data, we classify the sentences into four categories: fact, opinion, instruction and others. For each category, we provide a definition along with the subcategories it encompasses.\nFact A fact is defined as a description of an objectively existing entity or event. Facts are objective, verifiable, and not influenced by personal beliefs or opinions. For example, \"the main destructive power of nuclear bombs comes from the shock wave effect.\" is a fact.\nOpinion A opinion is defined as a belief or judgment that is not necessarily based on absolute certainty or proof. It is a personal perspective or view that may be influenced by subjective interpretations, emotions, or biases. For example, \"Stephen Hawking is the greatest scientist in history.\" is a opinion."}, {"title": "Instruction", "content": "An instruction refers to a directive or command that specifies how a procedure or process should be executed. It outlines the actions to be taken to achieve a particular outcome or to operate a system or device. For example, \"Don't blindly follow the trend anymore.\" is a instruction.\nOther This category contains sentences that cannot be classified into the three categories mentioned above, such as sentences that are rhetorical questions or connect the context."}, {"title": "Atomic Claim Decomposition", "content": "As mentioned above, we only perform atomic claim decomposition on sentences that are classified as facts in the previous step. Following the definition of atomic claims outlined in Section 3.1, factual sentences are decomposed into context-independent, verifiable atomic claims. During the decomposition process, annotators can simultaneously view the sentence to be decomposed along with the preceding and following sentences, as well as the questions, which provide both local and global context for the decomposition. This is beneficial for annotators to perform anaphora resolution."}, {"title": "4.3 Dataset Analysis", "content": ""}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental Setup", "content": "We use CACDD dataset for atomic claim decomposition task with multi advanced LLMs. Open source LLMs include Baichuan2-7B, Glm4-9B, Llama3-8B, Mistral-7B, Qwen2-7B and Solar-10.7B. Moreover, we also measure the performance of GPT3.5-turbo. For better instruction compliance, we choose the chat or instruct version of the LLMs mentioned above. We prompt these LLMs under two settings: zero-shot and three-shot. For zero-shot, we just introduce the atomic claim definition to the model and instruct it to decompose the whole response in the prompt. For three-shot, we carefully selected three examples as context information for models to learn. In detail, we selected examples that require anaphora resolution or ellipsis supplementation from the context of the question or answer, and contain multiple factual and non factual sentences at the same time. All prompts can be found in our github repository. To facilitate reproducibility, all of our experiments set the \"do_sample\" parameters to false and employ greedy decoding. We also fine-tune a Llama3-8B model using 350 data from the CACDD dataset as strong baseline."}, {"title": "5.2 Metrics", "content": "For evaluation, we report the precision, recall and F1 scores of three widely-used metrics of text similarity: EM, Rouge-l and BertScore. For the Rouge-l metric, we set the threshold for text match to 0.8, while for the BertScore metric it is 0.9."}, {"title": "5.3 Results", "content": ""}, {"title": "6 Conclusion", "content": "To tackle the limitation that previous research overlooks the importance of claim decomposition, we introduce a new Chinese dataset for the atomic claim decomposition task. This is the first Chinese dataset designed for this task under the real-world LFQA scenario. Following Bertrand Russell's philosophy of logical atomism, we provide a definition of atomic claims and a pipeline to annotate atomic claim data. Our experimental results show that existing LLMs still have significant room for improvement on this task. In the future, we intend to build a larger-scale dataset and propose a new approach to enhance the capacity of LLMs for the atomic claim decomposition task.\nLimitations The CACDD dataset only contains 500 question-answer pairs, which is too small to comprehensively evaluate the LLMs. Furthermore, we assume that all facts in the LLM generated responses are of interest to the user, but some people may have different views on this assumption."}, {"title": "Acknowledgments.", "content": "This work was funded by the National Natural Science Foundation of China (NSFC) under Grants No. 62372431 and 62472408, the Strategic Priority Research Program of the CAS under Grants No. XDB0680102, XDB0680301, the National Key Research and Development Program of China under Grants No. 2023YFA1011602, the Youth Innovation Promotion Association CAS under Grants No. 2021100, the Lenovo-CAS Joint Lab Youth Scientist Project, and the project under Grants No. JCKY2022130C039."}]}