{"title": "Motion-Oriented Compositional Neural Radiance Fields for Monocular Dynamic Human Modeling", "authors": ["Jaehyeok Kim", "Dongyoon Wee", "Dan Xu"], "abstract": "This paper introduces Motion-oriented Compositional Neural Radiance Fields (MoCo-NeRF), a framework designed to perform free-viewpoint rendering of monocular human videos via novel non-rigid motion modeling approach. In the context of dynamic clothed humans, complex cloth dynamics generate non-rigid motions that are intrinsically distinct from skeletal articulations and critically important for the rendering quality. The conventional approach models non-rigid motions as spatial (3D) deviations in addition to skeletal transformations. However, it is either time-consuming or challenging to achieve optimal quality due to its high learning complexity without a direct supervision. To target this problem, we propose a novel approach of modeling non-rigid motions as radiance residual fields to benefit from more direct color supervision in the rendering and utilize the rigid radiance fields as a prior to reduce the complexity of the learning process. Our approach utilizes a single multiresolution hash encoding (MHE) to concurrently learn the canonical T-pose representation from rigid skeletal motions and the radiance residual field for non-rigid motions. Additionally, to further improve both training efficiency and usability, we extend MoCo-NeRF to support simultaneous training of multiple subjects within a single framework, thanks to our effective design for modeling non-rigid motions. This scalability is achieved through the integration of a global MHE and learnable identity codes in addition to multiple local MHEs. We present extensive results on ZJU-MoCap and MonoCap, clearly demonstrating state-of-the-art performance in both single- and multi-subject settings. The code and model will be made publicly available at the project page: https://stevejaehyeok.github.io/publications/moco-nerf.", "sections": [{"title": "1 Introduction", "content": "Novel view synthesis of monocular human video with photo-realistic quality is a challenging problem, as it involves complex articulated body motions and necessitates sophisticated modeling of non-skeletal or non-rigid motions (e.g., wrinkles"}, {"title": "2 Related Work", "content": "We review closely related work from three perspectives: deformable neural rendering, neural human rendering, and monocular photo-realistic human rendering.\nDeformable neural rendering. NeRF leverages an MLP to learn a static 3D representation of a scene from a dense set of images from diverse viewpoints. For its application in real-life scenarios, prior works have enhanced NeRF in various directions such as improving efficiencies or relieving strong reliance on impractical settings like dense input views . In addition, there are more branches of work such as large-scale modeling, improving rendering quality , reconstruction quality and more. However, they are restricted to static scenes while the majority of the real-world objects are dynamic.\nRecent works including  have broadened the modeling capabilities of NeRF to dynamic scenes with small and simple movements/deformations. However, they often fail in handling more dynamic or complex motions like human body articulations. In contrast, our approach enables learning of human-articulated motions from monocular videos, and performing free-viewpoint rendering of a human performer at any time frame of the videos.\nNeural rendering of humans. Many early-stage prior works for neural human renderings utilize traditional techniques like multi-view stereo or to build sophisticated systems for capturing human motions . However, constructing a dense set of training views and building those capturing systems are expensive, making them unsuitable for real-life situations. address 3D human reconstruction by utilizing ground truth human scans, relieving the aforementioned requirements. further improves it by enabling human reconstruction from 2D image collections. In recent days, there have been many works learning an implicit human representation for novel pose synthesis or for a novel-view synthesis . There also have been several generalizable works for a novel human subject with pre-training on multi-view dataset and a few-shot fine-tuning. focus on improving rendering efficiency to achieve real-time free-view rendering of dynamic humans. Furthermore, adopt explicit hash encoding in order to boost the training efficiency for human representation modeling. In contrast to our problem setup, most of those works require multi-view videos or do not model the non-rigid motions.\nMonocular photo-realistic human rendering. HumanNeRF presents a state-of-the-art performance by optimizing motion fields and canonical T-pose representation of a human performer. However, its implicit modeling of pose-dependent non-rigid motions requires tens of hours to converge. Instant-NVR achieves efficient training with part-based representation and 2D mo-"}, {"title": "3 The Proposed MoCo-NeRF Method", "content": "The proposed MoCo-NeRF aims to effectively learn high-fidelity representations for free-viewpoint rendering of dynamic human subjects in monocular videos. We introduce our novel radiance compositional NeRF approach based on different motion types: rigid and non-rigid motions, as illustrated in Fig. 2. The composition of canonical radiance and the per-sample pose-dependent radiance residuals (Sec. 3.2) facilitates photo-realistic quality of rendering with fine-grained modeling of non-rigid motions, rather than leveraging the conventional spatial 3D offset learning approach. We further improve the quality of non-rigid radiance residuals by introducing a pose-embedded implicit features (Sec. 3.2) that is jointly optimized with the non-rigid radiance branch. In Sec. 3.3, thanks to our effective design of non-rigid motion modeling, it enables MoCo-NeRF to be flexibly extended for unified training of multiple subjects with a high efficiency."}, {"title": "3.1 Preliminary: Skeletal Transformations", "content": "The skeletal motion learns an inverse formulation of the linear blend skinning, which transforms canonical points to different body poses by skinning weights. Weng et al. reformulate the linear blend skinning to transform a point x in the observation space to a canonical point $x_c$:\n$x_c = \\sum_{k=1}^{K}w_k(R_k x + t_k), w'_k = \\frac{w_k(R_k x + t_k)}{\\sum_{j=1}^{K}w_j(R_j x + t_j)}$\nwhere $w'_k$ and $w_k$ respectively represent observation and canonical skinning weights of bone k, and $R_k$ and $t_k$ are the rotation and translation of bone k, respectively. Given Eq. 1, following , we optimize an explicit volume decoder to optimize ${w'_k}_{k=1}^{K-1}$ where K is the total number of joints."}, {"title": "3.2 Motion-Oriented Compositional Neural Radiance Fields", "content": "Rigid neural branch for learning canonical radiance field. The rigid branch of MoCo-NeRF is specifically designed to learn the canonical T-pose representation of the target subject using only rigid body movements. Our approach is different from but simpler than the prevalent spatial offset learning approach, which incorporates complex modeling of non-rigid motions as $\\Delta x_c$. Our rigid branch instead directly predicts the color and density of the canonical point $x_c$ transformed by inverse linear blend skinning (Sec. 3.1), as depicted in Fig. 2. First, we query multiresolution volumetric features $f_{x_c}$ of the canonical point $x_c$ from the MHE $\\psi_{hash}$ as follows:\n$f_{x_c} = \\psi_{hash}(x_c)$.\nThen, we slice the first half (H dimensions) of $f_{x_c}$ at each level of resolution to compute $f_{x_c}^H$ and predicts the radiance c and density $\\sigma$ of the rigid canonical point $x_c$ with an MLP decoder $M_{rigid}$ as follows:\n$c, \\sigma = M_{rigid}(f_{x_c}^H)$.\nNon-rigid neural branch for learning radiance residual field. While the canonical representation does not explicitly model pose-dependent non-rigid motions, it still captures an average of such motions (mean motions) after training. However, the mean motions cannot effectively reflect the complex and realistic non-rigid motions under different body poses. It is only capable of rendering identical deformations across different poses, as can be observed in Fig. 1. To address this effectively, the non-rigid branch of MoCo-NeRF is designed to concurrently optimize for a radiance residual field. This models the appearance discrepancies between the renderings of the rigid branch and the ground-truth colors, typically caused by the complex non-rigid motions or deformations. We additionally condition the radiance residual predictions with the canonical radiance representation of the rigid branch as prior information. This enables more"}, {"title": "3.3 Multi-subject Unified Training", "content": "Our effective radiance compositional approach for non-rigid motion modeling further enables a flexible extension of our single-subject MoCo-NeRF for unified training with multiple subjects, which is very beneficial for practical applications. Each monocular video exhibits distinct non-rigid motions, thereby requiring individualized modeling. However, it is impossible for the traditional spatial offset learning approach that typically requires a heavy module for photo-realistic quality. In contrast, our radiance compositional approach can achieve it by employing a separate, lightweight MHE for each subject, referred to as local MHE.\nBuilding on this foundation, we propose three architectural modifications as illustrated in Fig. 4. To benefit from unified training of multiple subjects, we introduce a coarse global MHE $\\psi_{hash}^G$ shared across all subjects to learn subject-generic representations (e.g., body shapes). We also ensure learning of subject-specific details with $N_s$ local MHES ${\\psi_{hash}^i}_{i=1}^{N_s}$, where $N_s$ is the number of subjects. For each canonical sample $x_c$, we query global features from the"}, {"title": "3.4 Model Optimization", "content": "We optimize MoCo-NeRF with all $N_i$ image frames ${I_j}_{j=1}^{N_i}$ from the monocular video via patch-based ray sampling. Given an image frame $I_j$ at each iteration, we randomly sample $N_p$ patches denoted as ${P_k}_{k=1}^{N_p}$ to construct the LPIPS loss. The LPIPS loss term can measure the perceptual distance between two image patches, and thus we also adopt it for more perceptually coherent renderings. Our framework first renders ${P_k^r, P_k^f}_{k=1}^{N_p}$ respectively from $C_{rigid}$ and $C_{final}$, and compute two LPIPS loss terms $L_{LPIPS}^r$ and $L_{LPIPS}^f$ against the ground truth patch $P_k^{GT}$ by using the pre-trained VGGNet to as $L_{LPIPS} = LPIPS(VGG(P_k^r), VGG(P_k^{GT}))$. We respectively compute the MSE loss terms $L_{MSE}^r, L_{MSE}^f$ from each of the estimated patches ${P_k^r, P_k^f}_{k=1}^{N_p}$. Moreover, we additionally compute an appearance matching loss with SSIM $L_{SSIM}^f$ only for the final RGB ${P_k^f}_{k=1}^{N_p}$, to improve the rendering quality. In summary, there are 5 loss terms in total: $L_{LPIPS}^r, L_{MSE}^r$ for the canonical radiance field of the rigid branch, and $L_{LPIPS}^f, L_{MSE}^f, L_{SSIM}^f$ for the final motion-oriented compositional renderings. To enforce the rigid branch to learn the main canonical radiance fields while the non-rigid branch to learn radiance residuals, we compute the final overall loss function $L^o$ with a hyperparameter $\\lambda = 0.2$ as:\n$L^o = \\lambda C^r + (1 - \\lambda)C^f$\n$L^r = L_{LPIPS}^r + L_{MSE}^r, L^f = L_{LPIPS}^f + L_{MSE}^f + L_{SSIM}^f$"}, {"title": "4 Experiments", "content": "We conduct extensive experiments on ZJU-MoCap and MonoCap to verify the effectiveness of our proposed approach on both single- and multi-subject settings. The multi-subject setting indicates multiple subjects are jointly optimized. Our findings demonstrate that MoCo-NeRF can learn photo-realistic dynamic human representations by effectively handling non-rigid motions. Moreover, it learns discriminative representations of each performer in the multi-subject setting, while prior works struggle."}, {"title": "4.1 Datasets", "content": "We leverage ZJU-MoCap and MonoCap datasets for both training and evaluation, following , and provide the details in this section. For ZJU-MoCap,"}, {"title": "4.2 Implementation Details", "content": "We optimize our model with Adam optimizer with betas (0.9,0.99), and learning rates of $5e^{-5}$ for the skeletal weights volume decoder and $5e^{-3}$ for the"}, {"title": "4.3 State-of-the-art Comparison", "content": "Single-subject methods. Tab. 1 presents PSNR, SSIM , and LPIPS* of MoCo-NeRF and our baselines , for novel-view synthesis under the single-subject setting. MoCo-NeRF achieves state-of-the-art performance on all metrics except one comparable LPIPS* for the MonoCap dataset. As shown in Fig. 5, Instant-NVR shows moderate performance, however, it often generates smoothed-out results without proper non-rigid motions due to high dependency on accurate SMPL parameters. GauHuman achieves better qualitative performance than but still struggles to model sophisticated non-rigid motions. Fig. 1 presents longer optimizations are surprisingly ineffective and harmful respectively for Instant-NVR and GauHuman in terms of non-rigid motion modeling. This implies that their approaches cannot achieve optimal non-rigid motion modeling. Moreover, we observe that PSNR and SSIM often favor those results without fine-grained non-rigid motion modeling since predicting pose-dependent deformations from novel viewpoints may not be pixel-wise accurate. Instead, the perception metric (LPIPS*) evaluates better in this case, as"}, {"title": "4.4 Model Analysis", "content": "In this section, we present analyses of our contributions respectively for two major achievements: novel non-rigid motion modeling (NR modeling) and multi-subject unified training (multi-subject modeling). For both analyses, we progressively remove each component from a full model to a vanilla approach.\nNR Modeling: Analysis of radiance compositional approach. Tab. 4 demonstrates our radiance compositional design perceptually improves the rendering results by closing the radiance discrepancies with predicted residuals. Fig. 7 shows that the representation learned solely by the rigid radiance branch (c) suffers from noisy renderings caused by incorrect correspondences due to"}, {"title": "5 Conclusion", "content": "In this paper, we presented a novel framework MoCo-NeRF for free-viewpoint dynamic human rendering that is capable of both single- and multi-subject training from monocular videos. By decomposing the neural representation of a dynamic human by motions and utilizing proposed implicit pose codes, MoCo-NeRF effectively learns radiance discrepancies induced by non-rigid motions. The proposed multi-subject MoCo-NeRF efficiently achieves comparable performance compared to the single-subject model. It has fully demonstrated its effectiveness and established new state-of-the-art performance on both problems."}, {"title": "A. Comparisons with GauHuman (Effectiveness of Pixel-Wise Metrics)", "content": "In the main paper, we mention that the pixel-wise metrics (PSNR, SSIM) favor smoother renderings generated by GauHuman , which is a concurrent work published one week prior to our submission. As results, our proposed MoCo-NeRF and GauHuman show comparable PSNR on both datasets and we would like to provide an analysis on this phenomenon. As mentioned in the main paper, GauHuman cannot capture pose-dependent cloth wrinkles as effectively as our model. Pixel-wise metrics (PSNR) are less sensitive to these details, but better reflected by perceptual metric LPIPS* as shown in Fig. A. Nevertheless, Tab. 1 in the paper shows our significant improvements in LPIPS*and SSIM. Fig. 5 in the paper and Fig. A show GauHuman lacks detailed cloth wrinkles for most of subjects, even with longer training. This limitation may be due to its reliance on accurate parametric model (SMPL) like Instant-NVR, while ours is free from the dependency. Overall, our work achieves clear state-of-the-art in single-subject task as well."}, {"title": "B. Integration of MHE into HumanNeRF", "content": "In this section, we show that replacing MLPs of HumanNeRF with MHEs is non-trivial. First, if we replace canonical MLP of HumanNeRF with MHE"}, {"title": "C. Additional Ablation Study on Multi-Subject MoCo-NeRF", "content": "Our ablation studies cumulatively removing each component might mislead that the introduction of ID codes contribute the most to the performance, but this is not true. As shown in Tab. A, adding $N_s$ local MHEs to the vanilla single-subject MoCo-NeRF (Tab. 5 (iv) in main paper) also significantly boosts the performance, paralleling the impact of ID codes. Therefore, other components beyond ID codes also greatly contribute to state-of-the-art performance."}, {"title": "D. Reason for Splitting MHE Features", "content": "Leveraging two separate MHEs for rigid and non-rigid branches of MoCo-NeRF slows down its training and rendering by \u00d71.14 due to an overhead of querying hash tables twice. Our proposed novel feature-splitting approach successfully avoids the overhead by querying the hash tables only once while optimizing independent hash features for two different purposes."}, {"title": "E. Novel Pose Synthesis", "content": "Although MoCo-NeRF does not specifically target novel-pose animations, we still can transfer unseen poses to the learned representations as shown in Fig. C."}, {"title": "F. Additional Qualitative Comparisons", "content": "In the demo video demo.mp4, we additionally illustrate the effectiveness of MoCo-NeRF in handling non-rigid motions and modeling multiple subjects at the same time. In two sections of the video, we compare against single-subject methods [14, 21,73] on both single- and multi-subject training settings, respectively.\nIn the single-subject setting, MoCo-NeRF achieves photo-realistic novel-view synthesis via motion-based compositional NeRF while GauHuman [21] and Instant-NVR [14] fail to model coherent pose-dependent non-rigid motions. Despite HumanNeRF's comparable quality, its training efficiency is much worse than our proposed MoCo-NeRF as discussed in the main paper. For the multi-subject setting, all single-subject methods [14,21,73] cannot model distinct representations for each subject. On the other hand, MoCo-NeRF readily extends to handle multiple distinct representations benefiting from the radiance compositional approach as also discussed in the main paper.\nIn the last section of the demo video, we compare multi-subject MoCo-NeRF against GHuNeRF [33] and NHP [31] that are trained with videos of multiple subjects for generalization capability. As mentioned in the main paper, we compare against these models targeting different task since there is no prior work learning distinct representations for multiple subjects from monocular videos. The demo video clearly illustrates that only MoCo-NeRF achieves photo-realistic quality of renderings with pose-dependent non-rigid motions modeling as well. NHP [31] also presents a slight modeling of non-rigid motions, however, it requires multi-view videos for its training and rendering."}, {"title": "G. Additional Model Analysis", "content": "Ablation study of cross-attention for implicit features: In the main paper, we introduce to adopt cross-attention mechanism on a learnable base code S"}]}