{"title": "WEIGHTED LOW-RANK APPROXIMATION VIA STOCHASTIC GRADIENT DESCENT ON MANIFOLDS", "authors": ["CONGLONG XU", "PEIQI YANG", "HAO WU"], "abstract": "ABSTRACT. We solve a regularized weighted low-rank approximation problem by a stochastic gradient de- scent on a manifold. To guarantee the convergence of our stochastic gradient descent, we establish a convergence theorem on manifolds for retraction-based stochastic gradient descents admitting confinements. On sample data from the Netflix Prize training dataset, our algorithm outperforms the existing stochastic gradient descent on Euclidean spaces. We also compare the accelerated line search on this manifold to the existing accelerated line search on Euclidean spaces.", "sections": [{"title": "1. INTRODUCTION", "content": "In this paper, we study the weighted low-rank approximation problem below:\nProblem 1.1 (The Weighted Low-Rank Approximation Problem). Assume that:\n(1) m, n and k are fixed positive integers satisfying k < min{m, n},\n(2) A = [ai,j] \u2208 Rm\u00d7n is a given matrix of constants,\n(3) W = [Wi,j] \u2208 Rm\u00d7n is a given matrix of weights satisfying wi,j \u2265 0 for (i, j) \u2208 {1, 2, ..., m} \u00d7 {1,2,..., n} and \u03a3i=1 \u03a3j=1 Wi,j = 1,\nwhere Rm\u00d7n is the space of m \u00d7 n real matrices. Define F : Rm\u00d7n \u2192 R by\nF(P) = \u03a3mi=1 \u03a3nj=1 Wi,j(ai,j - Pi,j)2 for P = [Pi,j] \u2208 Rm\u00d7n. Solve for\nargmin{F(P) | P\u2208 Rm\u00d7n, rankP < k}.\nWhen all the positive weights in W take the same value, we say Problem 1.1 has binary weights. The Matrix Completion Problem, a problem extensively studied in the recent literatures (for instance, [3], [19], [20], [12], [11], [8], [21], [5], [15], [7]), is a special case of Problem 1.1 with binary weights.\nProblem 1.1 is known to be NP-hard ([9]). The existing approach to this problem in the previous literatures is to regularize it to Problem 4.1 on the Euclidean space Rm\u00d7k \u00d7 Rn\u00d7k and estimate the solution to the problem by various first or second order algorithms on that Euclidean space (for instance, [2], [7], [14]). Different from this existing approach, we regularize Problem 1.1 to Problem 1.2.\nProblem 1.2 (A Regularized Weighted Low-Rank Approximation Problem). Under Assumptions (1)-(3) in Problem 1.1, fix a positive number \u03bb and define F : Rm\u00d7n \u2192 R by F(P) = \u03a3mi=1 \u03a3nj=1 Wi,j(ai,j \u2013 Pi,j)2 + \u03bb||P||F for P = [Pi,j] \u2208 Rm\u00d7n, where ||P||F :=\n\u2211mi=1 \u03a3nj=1 P2i,j is the Frobenius norm of P. Solve for\nargmin{F(P) | P\u2208 Rm\u00d7n, rankP < k}.\nTo design our convergent stochastic gradient descent for Problem 1.2, we reformulate this problem to an unconstrained problem on a Riemannian manifold via the Reduced Singular Value Decomposition. Denote by On the real n \u00d7 n orthogonal group and by Vk(Rn) the real n\u00d7k Stiefel manifold. That is\nOn = {U \u2208 Rn\u00d7n | UTU = In},\nVk(Rn) = {V \u2208 Rn\u00d7k | VTV = Ik},"}, {"title": "1.1", "content": "where In is the n \u00d7 n identity matrix. Define the diagonal function Dm\u00d7n : Rk \u2192 Rm\u00d7n by\nDm\u00d7n([x1,...,xk]T) = [di,j], where di,j = { Xi   if i = j \u2264 k,  0   otherwise,\nand in particular Dkxk : Rk \u2192 Rkxk by\nDkxk([x1,...,xk]T) = [di,j], where di,j ={ Xi   if i = j,  0   otherwise.\nBy the Singular Value Decomposition, for any P \u2208 Rm\u00d7n, rankP \u2264 k if any only if P can be decomposed into P = \u016aDmxn(x)VT for some \u0168 \u2208 Om, \u00d1\u2208 On and x \u2208 Rk. (The Singular Value Decomposition actually further states that the diagonal elements of Dmxn(x) are non-negative and in descending order. We ignore such constraints on Dmxn(x) to avoid adding boundary restrictions to the gradient search.)\nWrite U = [u1,...,um] and V = [v1,..., vn]. Define U = [u1,...,uk] and V = [v1,...,vk]. Then \u016aDmxn(x)VT = UDkxk(x)VT. Thus, we have the following lemma.\nLemma 1.3 (Reduced Singular Value Decomposition). For P \u2208 Rm\u00d7n, rankP < k if and only if it admits the following Reduced Singular Value Decomposition\nP = UDkxk(x)VT,\nwhere U \u2208 Vk(Rm), V \u2208 Vk(Rn) and x \u2208 Rk.\nBy Lemma 1.3, we reformulate Problem 1.2 into Problem 1.4, which is an unconstrained problem on the Riemannian manifold Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn). Note that, Lemma 1.3 indicates that any solution to Problem 1.2 is equivalent a solution to Problem 1.4, and vice versa.\nProblem 1.4 (A Reformulated Regularized Weighted Low-Rank Approximation Problem). Under Assump- tions (1)-(3) in Problem 1.1, fix a positive number \u03bb and let F : Rm\u00d7n \u2192 R be as in Problem 1.2. Solve for\nargmin{F(UDkxk(x)VT) | (U, x, V) \u2208 Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn)}.\nFor x = [x1,...,xk]T \u2208 Rk, denote standard Euclidean norm of x by ||x|| := \u221a\u03a3ki=1x2i. Note that, for (U, x, V) \u2208 Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn) and P = UDkxk(x)VT,\n||P||F = ||x||.\nTo estimate the solution to Problem 1.4, we start by establishing convergence Theorem 2.6 for stochastic gradient descents on manifolds admitting confinements (see Definition 2.5). Then, we apply Theorem 2.6 to design stochastic gradient descent Algorithm 3.13 for Problem 1.4. Our numerical results indicate that Algorithm 3.13 outperforms the existing stochastic gradient descent on Euclidean spaces on sample data from the Netflix Prize training dataset. We also apply the accelerated line search on manifolds (Algorithm C.3) to design accelerated line search Algorithm C.8 for Problem 1.4. Our numerical results indicate that Algorithm C.8 converges faster than the existing accelerated line search on Euclidean spaces on the same sample data. But, when \u03bb in Problem 1.4 is sufficiently small, the latter converges to a local minimum with lower value for the unregularized cost function.\nOverview and main contributions. In Section 2, we establish convergence Theorem 2.6 for stochastic gradient descents on manifolds admitting confinements. Theorem 2.6 is the main theoretical contribution of this paper. This theorem is inspired by Section 5 of Bottou (1999). Intuitively speaking, there are two obvious ways for a gradient descent to diverge. First, the gradient flow-line could be unbounded in the descending direction. Bottou (1999) avoids this by assuming the existence of a confinement ([6, (5.1)]). We do the same by adapting Bottou's ideas to manifolds. Second, the gradient vector of the cost function could grow so rapidly that the gradient descent fails to be a good approximation of the gradient flow-line. To avoid this, Bottou elected to impose constraints to limit the growth of the gradient vector ([6, (5.2)]). These constraints somewhat limit the applications of his version of the convergence theorem though. We choose to resolve this issue by a \"semi-adaptive\" approach. That is, we start with a preferred sequence of step sizes"}, {"title": "2", "content": "and then, depending on the outcome of each iteration of the algorithm, adjust the step size for the following iteration to guarantee convergence.\nIn Section 3, we interpret Problem 1.4 as an expectation cost problem and apply Theorem 2.6 to design a convergent stochastic gradient descent (Algorithm 3.13) for Problem 1.4.\nIn Section 4, we explore the numerical performances of Algorithms 3.13 and C.8. In particular, we apply these algorithms to the Matrix Completion Problem, which is a special case of Problem 1.1 with binary weights. We first compare Algorithm 3.13 to the benchmark given by the stochastic gradient descent Algorithm D.7 on the Euclidean space Rm\u00d7k \u00d7 Rn\u00d7k over the same number of iterations. Algorithm D.7 was introduced in the previous literatures (for instance, [2]). Our numerical results indicate that, on sample data from the Netflix Prize training dataset, Algorithm 3.13 outperforms Algorithm D.7. We also compare Algorithms 3.13 and C.8 to the benchmark given by the accelerated line search Algorithm D.17 on the Euclidean space Rm\u00d7k \u00d7 Rn\u00d7k over the same runtime. Our numerical results indicate that, on the same sample data,\n\u2022 Algorithm C.8 converges faster than Algorithm D.17,\n\u2022 Algorithm D.17 converges to a local minimum with lower value for the unregularized cost function when \u03bb is sufficiently small,\n\u2022 Algorithm C.8 outperforms Algorithm 3.13,\n\u2022 Algorithm D.17 also outperforms Algorithm 3.13 except when \u03bb is sufficiently large.\nAppendix A presents the proofs for the results in Section 2. Appendix B presents the proofs for the results in Section 3. Appendix C presents the accelerated line search on manifolds and its application to Problem 1.4. Appendix D presents the stochastic gradient descent and the accelerated line search on Euclidean spaces and their applications to Problem 4.1. In Appendix E, we design a convergent stochastic gradient descent and an accelerated line search for Problem E.1, a special case of Problem 1.1 with all weights being positive. In Appendix F, we present a new proof of the Eckart-Young Theorem that we came up with while studying Problem 1.1."}, {"title": "STOCHASTIC GRADIENT DESCENT ALGORITHMS ON RIEMANNIAN MANIFOLDS", "content": "2.1. Some Relevant Concepts. Before stating our convergence theorem and its applications, we first review some basic concepts necessary for our discussions. In Sections 2 and 3, we only discuss the type of random functions given in Definition 2.1 below.\nDefinition 2.1. Let M be a differentiable manifold and \u03a9 a probability space. A function f: M\u00d7\u03a9 \u2192 R is called a random function on M. We say that f is kth-order differentiable in the M-direction if, for every \u03c9\u2208\u03a9, f(*,\u03c9) : M \u2192 R is kth-order differentiable. We also say that f is locally bounded on M if, for every compact set KC M, there is a CK > 0 such that |f(x,w)| \u2264 Ck for every x \u2208 K and \u03c9 \u0395\u03a9.\nIn Definition 2.1, M is the manifold on which we search for the minimum of an expectation cost function, and \u03a9 is the space of samples used to guide our search. While designing gradient descent algorithms on manifolds, retractions are often used to replace the often computationally expensive exponential maps of Riemannian manifolds ([13], [18], [4]). We recall the definition of retractions on differentiable manifolds below.\nDefinition 2.2. ([1, Definition 4.1.1]) Let M be a differentiable manifold. A retraction on M is a C1 map R:TM \u2192 M such that, for every x \u2208 M, the restriction Rx = RTM satisfies\n\u2022 Rx(0x) = x, where Ox is the zero vector in TxM,\n\u2022 dRx(0x) = idTM under the canonical identification ToTxM \u2243 TxM, where dR is the differential of Rx and idTM is the identity map of TxM.\nFrom a theoretical perspective, retractions pull back the differential calculations in a manifold onto its tangent spaces. Our work shows that, if one is careful, most such computations can be done in single tangent spaces of the manifold. And computations on such spaces are simply differential calculus in inner product spaces. This allows us to mostly avoid the more delicate aspects of Riemannian geometry such as connections and geodesics. To do this, it is convenient to use the concept of retraction-dependent Lipschitz gradients to replace the common notion of Lipschitz gradients."}, {"title": "2.2", "content": "Definition 2.3. Let M be a Riemannian manifold, R a given retraction on M, and \u03a9 a probability space. Denote by (*,*)x the Riemannian inner product on TxM for all x \u2208 M and by || * ||x the norm it induces on TxM. Suppose that the random function f: M\u00d7\u03a9 \u2192 R is first-order differentiable in the M-direction. Then, for each x \u2208 M and each w \u2208 \u03a9, the function fx,w := f(Rx(*),\u03c9) : TxM \u2192 R is a first-order differentiable function. Its gradient \u2207fx,w is the vector in TxM dual to the differential dfxw via the inner product (*,*)x. We say:\n\u2022 f has R-Lipschitz gradient in the M-direction if there is a constant C > 0 such that\n||\u2207fx,w(v) - \u2207fx,w(0x)||x \u2264 C||v||x\nfor every x \u2208 M, v \u2208 TxM and \u03c9 \u0395\u03a9;\n\u2022 f has locally R-Lipschitz gradient in the M-direction if, for every compact subset K of M and every r > 0, there is a constant CK,r > 0 such that\n||\u2207fx,w(v) \u2013 \u2207fx,w(0x)||x \u2264 CK,r||v||x\nfor every x \u2208 K, every v \u2208 TxM satisfying || v ||x \u2264 r and every w \u2208 \u03a9.\nIn the case \u03a9 = {w} is a probability space of a single point, the above gives the definitions of R-Lipschitz gradient and locally R-Lipschitz gradient of a deterministic function on M.\nRemark 2.4. In Definition 2.3, denote by \u2207Mf the gradient of f with respenct to M, that is, \u2207Mf(x,w) = \u2207fw(x), where fw := f(*,w) : M \u2192 R. Note that fx,w = fwRx. Since dRx(0x) = idTM, we have that, for any v \u2208 TxM,\n(\u2207fx,w(0x), v)x = (dfx,w)|o(v) = d(fw \u2218 Rx)|o(v) = (dfw)|x \u2218 (dRx)|o(v)\n= dfw|x(v) = (\u2207fw(x),v)x = (\u2207Mf(x,w), v)x.\nThis shows that\n(2.1)\n\u2207fx,w(0x) = \u2207Mf(x,w).\nFinally we introduce the concept of confinements of random functions on Remannian manifolds, which plays a central role in this manuscript.\nDefinition 2.5. Let M be a Riemannian manifold, \u03a9 a probability space, and f: M\u00d7\u03a9 \u2192 R a random function on M that is first-order differentiable in the M-direction. A confinement of f on M is a first-order differentiable function p: M\u2192 R satisfying:\n\u2022 for every \u03b4\u2208 R, the set {x \u2208 M | p(x) \u2264 \u03b4} is compact;\n\u2022 there exists a po \u2208 R such that (\u2207p(x), \u2207Mf(x,w))x \u2265 0 for every w \u2208 \u03a9 and every x \u2208 M satisfying p(x) \u2265 po, where (*, *)x is the Riemannian inner product on TxM.\nExistence of confinement of a random function guarantees that the stochastic gradient descent happens in a compact subset of the manifold and therefore has convergent subsequences."}, {"title": "A Convergence Theorem.", "content": "Theorem 2.6. Assume that:\n(1) M is an m-dimensional Riemannian manifold equipped with a C2 retraction R : TM \u2192 M. Denote by (*,*)x the Riemannian inner product on TxM for all x \u2208 M and by || * ||x the metric it induces on TxM.\n(2) \u03a9 is a probability space with probability measure \u00b5.\n(3) f: M\u00d7\u03a9 \u2192 R is a random function on M satisfying:\n(i) f is first-order differentiable in the M-direction,\n(ii) f and ||\u2207Mf|| are locally bounded on M,\n(iii) f has locally R-Lipschitz gradient in the M direction.\n(4) F: M\u2192 R is the expectation of f(x,w) with respect to w under the probability distribution \u00b5. That is, F(x) = Ew~\u00b5(f(x,w)) = \u222b\u03a9 f(x,w)d\u00b5 for all x \u2208 M.\n(5) p : M \u2192 R is a C2 confinement of f. Fix a po \u2208 R satisfying that (\u2207p(x),\u2207fw(x))x \u2265 0 for every \u03c9 \u0395 \u03a9 and every x \u2208 M satisfying p(x) \u2265 po."}, {"title": "3", "content": "(6) {wt}\u221et=0 is a sequence of independent random variables taking values in \u03a9 with identical probability distribution \u00b5.\n(7) Fix positive constants a, b, \u0398 and a sequence {ct}\u221et=0 of positive numbers satisfying \u03a3\u221et=0 ct = \u221e and \u03a3\u221et=0 c2t < \u221e. Define c = max{ct | t \u2265 0}, \u03c3 = \u03a3\u221et=0 c2t and \u03c11 = po + c2a + b2\u03c3/2.\n(8) xo \u2208 M is a fixed point satisfying p(xo) \u2264 Po.\nDefine a sequences {xt}\u221et=0 of random elements of M and a sequence {\u03b3t}\u221et=0 of random positive numbers so that, for t\u2265 0,\nXt+1 = Rxt (-Ct / \u03b3t\u2207Mf(xt,wt)),\n\u03b3t \u2265 max{At,Bt,C/\u0398},\nwhere\nAt := 1/a sup{max{0, - (\u2207p(xt), \u2207Mf(xt,w))xt } |\u03c9\u2208\u03a9},\nBt := 1/b sup{max{0, Hess(p \u25cb Rx) |\u03b8\u2207Mf(xt,w) (\u2207Mf(xt,w),\u2207Mf(xt,w))} | |\u03b8| \u2264 \u0398, \u03c9\u2208\u03a9}\nand Hess(p \u25cb Rx) is the Hessian of the function p \u25cb Rx: TxM \u2192 R, which is defined on the inner product space TxM. Then\n\u2022 {xt}t=0 is contained in the compact subset {x \u2208 M | p(x) \u2264 \u03c11} of M. In particular, it has convergent subsequences.\nAssume that {\u03b3t}\u221et=0 also satisfies:\n(a) each \u03b3t is independent of {wt}\u221et=0,\n(b) {\u03b3t}\u221et=0 is bounded above and below by positive numbers. That is, there are \u03a6min, \u03a6max > 0 such that\n\u03a6min \u2264 \u03b3t \u2264 \u03a6max for all t\u2265 0.\nThen we further have that:\n\u2022 {F(xt)}t=0 converges almost surely to a finite number;\n\u2022 {||\u2207F(xt)||xt}t=0 converges almost surely to 0;\n\u2022 any limit point of {xt}t=0 is almost surely a stationary point of F.\nWe can replace the sequence {\u03b3t}\u221et=0 by a single factor C / \u03c6 and still get a convergent stochastic gradient descent algorithm, which is formulated in Corollary 2.7 below. This makes the step sizes deterministic instead of \"semi-adaptive\". Of course, this may sometimes shrink the step sizes by too much and potentially slow down the convergence.\nCorollary 2.7. With Assumptions (1) to (8) stated in Theorem 2.6, fix a positive constant \u03c6 satisfying\nC / \u0398 \u2265 max{A,B,C/\u0398},\nwhere\nA := 1/a sup {max{0, - (\u2207p(x),\u2207Mf(x,w)}x} | p(x) \u2264 po, \u03c9\u2208\u03a9},\nB := 1/b sup{max{0, Hess(p \u25cb Rx)|\u03b8\u2207Mf(x,w) (\u2207Mf(x,w),\u2207Mf(x,w))} | |\u03b8| \u2264 \u0398, x \u2208 M, p(x) \u2264 \u03c11, \u03c9\u2208\u03a9},\nand Hess(p \u25cb Rx) is the Hessian of the function p \u25cb Rx: TxM \u2192 R, which is defined on the inner product space TxM.\nDefine a sequence {xt}\u221et=0 of random elements of M by\nXt+1 = Rxt (-Ct / \u03c6\u2207Mf(xt,wt)), for t\u2265 0.\nThen:\n\u2022 {xt}t=0 is contained in the compact subset {x \u2208 M | p(x) \u2264 \u03c11} of M. In particular, it has convergent subsequences,"}, {"title": "4", "content": "\u2022 {F(xt)}t=0 converges almost surely to a finite number,\n\u2022 {||\u2207F(xt)||xt}t=0 converges almost surely to 0,\n\u2022 any limit point of {xt}t=0 is almost surely a stationary point of F.\nThe proofs of Theorem 2.6 and Corollary 2.7 are presented in Appendix A.\n3. A STOCHASTIC GRADIENT DESCENT ALGORITHM FOR PROBLEM 1.4\nIn this section, we apply Theorem 2.6 to design a convergent stochastic gradient descent for Problem 1.4. The proofs for the results in this section are presented in Appendix B. To apply Theorem 2.6, we define a probability space first.\nDefinition 3.1. Let matrix W = [wi,j] be the matrix of weights given in Problem 1.1. Recall that wi,j \u2265 0 for (i, j) \u2208 {1, 2, ..., m} \u00d7 {1, 2, ..., n} and \u03a3mi=1 \u03a3nj=1 wi,j = 1. Define the probability space (\u03a9m,n, \u03bc) by \u03a9m,n := {1,2,..., m} \u00d7 {1, 2, ..., n} and \u03bc({(i,j)}) = wi,j for every (i, j) \u2208 \u03a9m,n.\nNext, we introduce a retraction on the manifold Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn). To do that, we recall the QR decomposition first.\nDefinition 3.2. Assume that k \u2264 m. For an m \u00d7 k real matrix C with linearly independent columns, define qf(C) = Q in the QR decomposition C = QR, where\n\u2022 Q\u2208 Vk(Rm),\n\u2022 R is a k\u00d7k upper triangular matrix with positive entries along its diagonal.\nComputationally, one way to obtain qf(C) is by applying the Gram-Schmidt Process on the columns of C, scaling the resulting orthogonal set of vectors into an orthonormal set and then using this orthonormal set of vectors as the columns of qf(C).\nLemma 3.3. ([1, Examples 3.5.2 and 4.1.3]) The n\u00d7k Stiefel manifold Vk(Rn) is a Riemannnian submanifold of Rn\u00d7k. Moreover:\n\u2022 For X \u2208 Vk(Rn), the tangent space of Vk(Rn) at X is TxVk(Rn) = {Z\u2208Rn\u00d7k | XTZ + ZTX = 0}.\n\u2022 For any X \u2208 Vk(Rn) and Z\u2208TxVk(Rn), X + Z \u2208 Rn\u00d7k has linearly independent columns. Define RVk(Rn) : TxVk(Rn) \u2192 Vk(Rn) by RVk(Rn) (Z) = qf(X + Z). Then RVk(Rn) is a retraction on Vk(Rn).\nThe tangent space of the manifold Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn) at (U,x, V) \u2208 Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn) is TuVk(Rm) \u00d7 Rk \u00d7 TvVk(Rn). We are ready to describe a retraction on the manifold Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn).\nDefinition 3.4. For (U, x, V) \u2208 Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn) and (Y, x, Z) \u2208 TuVk(Rm) \u00d7 Rk \u00d7 TvVk(Rn), define R : TuVk(Rm) \u00d7 Rk \u00d7 TvVk(Rn) \u2192 Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn) by R(U,x,V) (Y, x, Z) = (qf(U+Y), x+x, qf(V+Z)).\nBy Lemma 3.3, the map R in Definition 3.4 is a retraction on Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn). To design our stochastic gradient descent for Problem 1.4, we also need to define a random function satisfying Assumption (3) in Theorem 2.6.\nDefinition 3.5. Define f : Rm\u00d7n \u00d7 \u03a9m,n \u2192 R and f : Rm\u00d7n \u00d7 \u03a9m,n \u2192 R by\nf(P; \u03b7, \u03b3) := (a\u03b7,\u03b3 \u2013 P\u03b7,\u03b3)2,\nf(P; \u03b7, \u03b3) := f(P; \u03b7, \u03b3) + \u03bb||P||2 = (a\u03b7,\u03b3 \u2013 P\u03b7,\u03b3)2 + \u03bb||P||2,\nfor \u03bb > 0 given in Problem 1.4, and for all P = [pi,j] \u2208 Rm\u00d7n and (\u03b7, \u03b3) \u2208 \u03a9m,n.\nDefine g: Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn) \u00d7 \u03a9m,n \u2192 R by\ng(U, x, V; \u03b7, \u03b3) := f(UDkxk(x)VT;\u03b7, \u03b3) = f(UDkxk(x)VT; \u03b7, \u03b3) + \u03bb||x||2,\nfor all (U, x, V) \u2208 Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn) and (\u03b7, \u03b3) \u2208 \u03a9m,n, where Dkxk(x) is defined in Equation (1.2). For notational convenience, we define the functions f\u03b7,\u03b3 : Rm\u00d7n \u2192 R, f\u03b7,\u03b3 : Rm\u00d7n \u2192 R and g\u03b7,\u03b3 : Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn) \u2192 R for (\u03b7, \u03b3) \u2208 \u03a9m,n by\nf\u03b7,\u03b3(P) = f(P; \u03b7, \u03b3), fn,y(P) = f(P;\u03b7, \u03b3) and g\u03b7,\u03b3 (U, x, V) = g(U, x, V; \u03b7, \u03b3),\nfor all P = [pi,j] \u2208 Rm\u00d7n and (U, x, V) \u2208 Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn)."}, {"title": "4", "content": "Definition 3.6. Let F : Rm\u00d7n \u2192 R be as in Problem 1.4, define G : Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn) \u2192 R by\nG(U,x, V) = F(UDkxk(x)VT) = F(UDkxk(x)VT) + \u03bb||x||2,\nfor all (U, x, V) \u2208 Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn).\nLemma 3.7. Let F : Rm\u00d7n \u2192 R be as in Problem 1.1, F : Rm\u00d7n \u2192 R as in Problem 1.4 and G : Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn) \u2192 R as in Definition 3.6. For P \u2208 Rm\u00d7n and (U, x, V) \u2208 Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn), we have\nE(\u03b7,\u03b3)~\u03bc(f(P;\u03b7, \u03b3)) = F(P), E(\u03b7,\u03b3)~\u03bc(f(P;\u03b7, \u03b3)) = F(P) and E(n,r)~\u03bc(g(U,x, V; \u03b7, \u03b3)) = G(U, x, V),\nwhere the expectations are taken over the probability space \u03a9mn with respect to the probability distribution \u00b5 given in Definition 3.1. That is, f, f and g are random functions with expectations F, F and G.\nSince Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn) is a submanifold of the Euclidean space Rm\u00d7k \u00d7 Rk \u00d7 Rn\u00d7k, we use the following lemmas to find the gradient of g\u03b7,\u03b3 given in Equation (3.4).\nLemma 3.8. ([1, Equation (3.37)]) Let M be a Riemannian manifold and f: M \u2192 R a differentiable function. Assume that M is a Riemannian submanifold of M. For any x \u2208 M, denote by \u03c0x: TxM \u2192 TxM the orthogonal projection. Then \u2207(f|M)(x) = \u03c0x(\u2207f(x)) for any x \u2208 M.\nLemma 3.9. ([1, Example 3.6.2]) For any X \u2208 Vk(Rn), denote by \u03a0x the orthogonal projection Ix : Rn\u00d7k \u2192 TxVk(Rn). Then, for any \u03be\u2208 Rn\u00d7k, \u03a0x(\u03be) = (In \u2013 XXT)\u03be + X(XT\u03be \u2013 \u03be\u03a4\u03a7) = \u03be \u2212 X(XT\u03be + \u03be\u03a4\u03a7).\nWith the orthogonal projection \u03a0x : Rn\u00d7k \u2192 TxVk(Rn) given in Lemma 3.9, we are ready to give the gradient of the function g\u03b7,y: Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn) \u2192 R.\nCorollary 3.10. Given (\u03b7, \u03b3) \u2208 \u03a9m,n, define the matrices\n\u2207U f\u03b7,\u03b3 = \u2202f\u03b7,\u03b3 / \u2202ui,1 = [-2\u03b4\u03b7,i(a\u03b7,\u03b3 \u2013 P\u03b7,\u03b3)x1\u03c5\u03b3,1]m\u00d7k,\n\u2207V f\u03b7,\u03b3 = \u2202f\u03b7,\u03b3 / \u2202vi,1 = [-2\u03b4\u03b3,j(a\u03b7,\u03b3 \u2013 P\u03b7,\u03b3)x1\u03c5\u03b7,1]n\u00d7k,\n\u2207x f\u03b7,\u03b3 = [-2(a\u03b7,\u03b3 \u2013 P\u03b7,\u03b3)un,1\u03c5\u03b3,1 -2(a\u03b7,\u03b3 \u2013 P\u03b7,\u03b3)un,2\u03c5\u03b3,2  \u2026  -2(a\u03b7,\u03b3 \u2013 P\u03b7,\u03b3)un,k\u03c5\u03b3,k] ,\nwhere \u03b4p,q = 1 if p = q, 0 if p \u2260 q. Then the gradient of g\u03b7,y at any (U, x, V) \u2208 Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn) is\n\u2207g\u03b7,\u03b3 (U,x, V) = (\u03a0U (\u2207U f\u03b7,\u03b3), \u2207x fn,y + 2\u03bbx, \u03a0V (\u2207V f\u03b7,\u03b3)) \u2208 TuVk(Rm) \u00d7 Rk \u00d7 TvVk(Rn),\nwhere \u03a0U and \u03a0V are the orthogonal projections given in Lemma 3.9.\nNext we define a confinement function for the random function g given in Equation (3.3).\nDefinition 3.11. Define p : Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn) \u2192 R by\n\u03c1(U, x, V) = ||x||2\nfor all (U, x, V) \u2208 Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn).\nLemma 3.12. For A = [ai,j] \u2208 Rm\u00d7n given in Problem 1.4, define\na := max{aj | (i, j) \u2208 \u03a9m,n} \u2265 0,"}, {"title": "5", "content": "and\npo := max{||xo||2,\u03b1/(4\u03bb)} \u2265 \u03b1/(4\u03bb)\nwhere \u03bb > 0 is given in Problem 1.4 and xo is given by the initial iterate (Uo, xo, Vo) of Algorithm 3.13 below. With po given in Equation (3.9), the function p in Definition 3.11 and the random function g in Definition 3.5 satisfy that\n\u2022 \u03c1(Uo, xo, Vo) \u2264 po,\n\u2022 (\u2207\u03c1(U,x, V), \u2207g\u03b7,\u03b3(U, x, V)) \u2265 0 for (\u03b7, \u03b3) \u2208 \u03a9m,n and (U, x, V) \u2208 Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn) satisfying \u03c1(U, x, V) \u2265 \u03c1o.\nThat is, the function \u03c1 is a confinement function for the random function g in Definition 3.5 on the manifold Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn), and po satisfies Assumptions (5) and (8) of Theorem 2.6.\nNow we fix the scalars a and b in Assumption (7) of Theorem 2.6 as\na = 1/\u221a\u03bb and b = 1/\u221a\u03bb\nwhere \u03bb is given in Problem 1.4. Fix the sequence {ct}\u221et=0 in Assumption (7) of Theorem 2.6 as ct = 1/(t+1). Thus, c = max{ct | t > 0} = 1 and \u03c3 = \u03a3\u221et=0 c2t = \u03c02/6. Further, we choose a positive scalar \u03a6min satisfying\n\u03a6min = K max{(\u03bb + 2\u221a\u03bb + 1)\u03b1, (32\u03ba\u03b1\u03bb + 8k(2 + \u03bb2)(2\u03bb\u03c1\u03bf + \u03bb\u03c02/6))},\nwhere \u03b1 is given in Equation (3.8), po is given in Equation (3.9) and K > 1 is one of the constant inputs for Algorithm 3.13 below. Fix positive scalar \u0398 in Assumption (7) of Theorem 2.6 as\n\u0398 = 1/\u03a6min\nWith the preparations above, we are now ready to apply Theorem 2.6 to Problem 1.4.\nAlgorithm 3.13 (A Stochastic Gradient Descent for Problem 1.4).\nInput:\nthe random function g given in Definition 3.5,\nthe retraction R given in Definition 3.4,\nthe positive integers m, n and k given in Problem 1.4,\nthe positive scalar \u03bb given in Problem 1.4,\nthe matrices A and W given in Problem 1.4,\na scalar K\u2265 1,\nan initial iterate (Uo, xo, Vo) \u2208 Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn).\nOutput: A sequence of iterates {(Ut, xt, Vt)}\u221et=0 \u2282 Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn).\n\u2022 for t = 0,1,2... do\n1. Select a random element (\u03b7t, \u03b3t) from \u03a9m,n with the probability distribution \u03bc independent of {(\u03b7\u03c4,\u03b3\u03c4)}t\u22121\u03c4=0\n2. Set\n(Ut+1, xt+1, Vt+1) = R(Ut,xt,Vt) (-1/(t+1)\u03a6min \u2207gn,y(Ut, xt, Vt)),\nwhere g\u03b7t, \u03b3t(Ut, xt, Vt) is given in Corollary 3.10 and \u03a6min is given in Equation (3.11).\n\u2022 end for\nProposition 3.14. Let G be the function given in Equation (3.5) and {(Ut, xt, Vt)}\u221et=0 \u2282 Vk(Rm) \u00d7 Rk \u00d7 Vk(Rn) be the sequence from the Algorithm 3.13. Then:"}, {"title": "6", "content": "(1) {(Ut, xt"}]}