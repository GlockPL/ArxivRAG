{"title": "Reducing annotator bias by belief elicitation", "authors": ["Terne Sasha Thorn Jakobsen", "Andreas Bjerre-Nielsen", "Robert B\u00f6hm"], "abstract": "Crowdsourced annotations of data play a substantial role in the development of Artificial Intelligence (AI). It is broadly recognised that annotations of text data can contain annotator bias, where systematic disagreement in annotations can be traced back to differences in the annotators' backgrounds. Being unaware of such annotator bias can lead to representational bias against minority group perspectives and therefore several methods have been proposed for recognising bias or preserving perspectives. These methods typically require either a substantial number of annotators or annotations per data instance. In this study, we propose a simple method for handling bias in annotations without requirements on the number of annotators or instances. Instead, we ask annotators about their beliefs of other annotators' judgements of an instance, under the hypothesis that these beliefs may provide more representative and less biased labels than judgements. The method was examined in two controlled, survey-based experiments involving Democrats and Republicans (n = 1,590) asked to judge statements as arguments and then report beliefs about others' judgements. The results indicate that bias, defined as systematic differences between the two groups of annotators, is consistently reduced when asking for beliefs instead of judgements. Our proposed method therefore has the potential to reduce the risk of annotator bias, thereby improving the generalisability of AI systems and preventing harm to unrepresented socio-demographic groups, and we highlight the need for further studies of this potential in other tasks and downstream applications.", "sections": [{"title": "Reducing annotator bias by belief elicitation", "content": "Recent advances in artificial intelligence (AI) owes its success to\nannotators-individuals who, given raw data such as text, images, audio or video, provide\nlabels and other kinds of annotations describing one or more aspects of the data.\nAnnotations are increasingly being collected via crowdsourcing platforms, where text data\nmakes up a large part of the traffic. The goal of annotation has generally been to get true\nlabels for each instance in a dataset, under the assumption that there exists one correct\ninterpretation. This is heavily challenged by the existence of systematic disagreement\namong annotators, which, in turn, challenges the ability to learn meaningful and unbiased\npatterns from the data and to develop unbiased AI systems, if not handled correctly. In\nNatural Language Processing (NLP), a research field devoted to computationally\nprocessing and understanding spoken and written language, it is broadly recognised that\ngenuine disagreement exists across a wide range of annotation topics and tasks (Plank,\n2022; Uma et al., 2021)\u2014genuine because it does not (always) represent errors or \u201cnoise,\u201d\nrather, it is (often) an effect of different, sometimes equally valid, interpretations. It is\ntempting to conclude that this challenge only pertains to \u201csubjective\" tasks or to\nnon-expert, crowd-sourced annotations, but systematic disagreement is found among\nexperts (Aroyo & Welty, 2015; Recasens et al., 2012), near-expert students (Poesio &\nArtstein, 2005), and non-expert crowd-workers (Leonardelli et al., 2021) in both subjective\ntasks, such as annotating toxic language (Sap et al., 2022), and seemingly objective tasks,\nsuch as annotating part-of-speech (Plank et al., 2014, see also Basile et al., 2021, for more\nexamples of disagreement in \u201cobjective\u201d tasks).\nWhat drives disagreement is not always clear, though ambiguity (within instances\nor instructions; Jurgens, 2014; Plank et al., 2014; Sandri et al., 2023) and biases connected\nto socio-demographic attributes (Fleisig et al., 2023; Kuwatly et al., 2020; Liu et al., 2022;\nSap et al., 2019) appear to be key factors. In this study, we focus on the\nlatter\u2014socio-demographic annotator bias\u2014which we define as systematic disagreement"}, {"title": "", "content": "that can be operationalised as differences in annotations between groups of individuals\nwith one or more distinctly different socio-demographic attribute(s).\nDue to socio-demographic biases, attempting to construct a true label by an\naggregated value, from several annotators' judgements of the same instance, is likely to\nerase minority-group perspectives and result in representational bias (Prabhakaran et al.,\n2021). Several studies indicate that disagreement in annotations provides valuable\ninformation that should be used unaggregated in the development and evaluation of the\nsystems using the data (Basile, 2020; Basile et al., 2021; Davani et al., 2022; Peterson\net al., 2019; Uma et al., 2020). However, these proposals tend to require a very distinct set\nof annotators, who have each annotated a large amount of instances, to infer\nindividual-specific preferences and bias, or a large amount of unique annotators and\nannotations per instance to infer representative label distributions for instances. Another\nlarge body of work has been focused on bias mitigation, where the rhetoric is on removing\na specific type of bias (e.g., gender bias) already present in a dataset or system, rather than\npreserving perspectives from disagreements (see Hort et al., 2022, for a comprehensive\nsurvey of bias mitigation).\nIn this study, we aim to address the important challenge of annotator bias early in\nthe system development pipeline: during annotation. We propose a simple strategy to gain\nknowledge of bias in annotations without requirements on the amount of annotators or\ninstances. Simply, by asking annotators about their beliefs of other annotators' judgements\nof an instance, we can observe differences between the individual's own judgement and\nbelief, providing a signal of bias. We hypothesise that beliefs about others' annotations\nmay provide more representative and less biased labels than judgements, and we therefore\nexamine the potential of our annotation method as a bias reduction method.\nTo examine our proposed annotation method, we consider an annotation case where\nwe know socio-demographic annotator bias exists: annotators' political affiliation. For\ninstance, Luo et al. (2020) found that annotating the stance of opinionated claims"}, {"title": "", "content": "concerning global warming is biased by annotators' political affiliations (Republican,\nnon-Republican). Similarly, Thorn Jakobsen et al. (2022) found that recognising\nstatements as arguments is biased by annotators' political alignment (conservative, liberal)\nto varying degree depending on differing annotation instructions. Here, we conducted two\ncontrolled, survey-based experiments where Democrats and Republicans are asked to judge\nstatements as arguments, following a simple definition of what an argument is, and\nafterwards report beliefs about others' judgements (Figure 1). We distinguish between\njudgement and belief in this way throughout the paper: a participant's judgement is the\nstandard annotation collected following a normal annotation procedure. A belief is the\nannotation which the participant expects other participants to give. Importantly,\njudgement is not synonymous to opinion, rather it is the interpretation of an annotation\nguideline and a given sample. The experiments consist of a small set of statements and a\nlarge sample of annotators (total N = 1,590 after exclusions) to accurately measure the\neffect of asking for beliefs over data instances where judgements show political bias."}, {"title": "Method", "content": "The two experiments are outlined in Figure 1. Experiment 1 elicited beliefs over a\nrepresentative groups' responses, using a popular method for belief elicitation that usually\ninvolves providing monetary incentives. We implemented two treatment arms, one with\nand one without monetary incentives. Our motivation for doing this was that although\nmonetary incentives is the norm for the method, further compensation is costly, and\nexisting work have shown potential negative side-effects of such incentives that could harm\nannotation quality (B\u00e9nabou & Tirole, 2006; Bowles, 2008; Gneezy & Rustichini, 2000).\nMotivated by the need to distinguish the annotators' beliefs over Democrats' and\nRepublicans' responses, to better understand the compounded representative belief,\nExperiment 2 elicit these two beliefs separately.\nParticipants\nParticipants were recruited via Prolific (www.prolific.com). Recruitment criteria\nincluded US nationality and either Democrat or Republican political affiliation.\nFurthermore, the recruitment of Democrats and Republicans (to separate study links) was\nbalanced by sex and participants who had participated in earlier studies (i.e., pilot study in\nthe case of Experiment 1, and pilot study and Experiment 1 in the case of Experiment 2)\nwere excluded from participating in later studies. Importantly, responses of participants\nwho reported a different political affiliation, than they were recruited by, were excluded\nfrom analysis (80 exclusions Experiment 1; 22 exclusions Experiment 2).\nAfter exclusions, the experimental samples were composed of n = 641 Democrats\nand n = 619 Republicans in Experiment 1, and of n = 169 Democrats and n = 161 in\nExperiment 2. Other demographic variables included age, gender, English-speaking ability\nand political left-right alignment. The latter serves as another perspective on the\nparticipants' political placement, while the former serves mainly to reduce priming effects\nof asking for political affiliation."}, {"title": "Data", "content": "The statements we asked participants to annotate originate from the DDO dataset\n(https://esdurmus.github.io/ddo.html) of Durmus and Cardie (2018, 2019), which is a\nlarge collection of debates and user profiles from the online debating forum debate.org.\nTexts were picked, for our experiments, based on the following criteria: (i) they concern\ncontroversial topics and (ii) they can be accepted as arguments given the guideline.\nInitially, for a pilot study, there was a third criterion: (iii) within a topic there exist two\narguments expressing a Republican and a Democratic argument, where one is supporting\nand the other is opposing the topic. The initial selection resulted in 14 texts within 7\ntopics (Appendix, Table E1). Of these 14 texts, six were identified as exhibiting biased\nannotations in the pilot study, and were therefore used in Experiment 1 (Appendix Table\nA1), whereof the four statements exhibiting most biased judgements were used again in\nExperiment 2 (Table 1). All texts were edited slightly to improve grammar and coherency,\nin an effort to make the arguments equally clear and unambiguous."}, {"title": "Experiment 1 procedure", "content": "After providing informed consent with the project's aim, the possible risks, usage\nand storage of data, and their remuneration, participants were given a short survey on\ndemographics. Then they were given instructions for the first task, where they were asked\nto specify how much they agreed that a given statement was an argument with respect to a\ngiven topic. (See instructions in the Appendix, Figure B1.) Besides describing the task, the\ninstructions also provided a description, with examples, of what an argument is and how\nthey should judge the statements. The task consisted of six topic and statement pairs\n(Appendix, Table A1), each presented on a separate page with the order randomised. On\neach page, the participants were given a 2-decimal sliding scale to use for annotation, with\nguiding labels on 0 = Strongly disagree (definitely not an argument), 0.25 = Somewhat\ndisagree, 0.5 = Neither agree nor disagree, 0.75 = Somewhat agree and 1 = Strong agree\n(definitely an argument). On the top of each page there was a re-cap of the instructions"}, {"title": "Experiment 2 procedure", "content": "The first part of Experiment 2\u2014consent, demographics survey and the first\ntask was the same as in Experiment 1 (see above), with the exception that participants\nwere only given four out of the six topic-statement pairs, shown in Table 1.\nAfter completing the first task, participants were given instructions for the second\ntask where they were asked to provide intervals for their beliefs of the average responses, to\nthe first task, of other Democrat and Republican participants separately (see instructions\nin Appendix, Figure B4). Again, they were explicitly told that the other participants had\nbeen given the same instructions for the first task as themselves, furthermore they were\ngiven an approximate number for the amount of Democrat and Republican participants.\nThe instruction for how to provide intervals followed the wording of the instructions used\nin Experiment 1 that did not give monetary incentives. The four topic-statement pairs\nwere shown on separate pages, with instructions re-cap at the top and with four sliding\nscales: an upper and lower bound for the belief of Democrats' and Republicans' average\nfirst task response. The granularity and labels on the sliding scales were the same for all\ntasks and experiments (see description in the above section). The survey ended after\ncompleting the fourth statement."}, {"title": "Belief elicitation and incentivization procedure", "content": "Getting consistent and true beliefs from individuals, through direct asking, has long\nbeen of interest of, especially, economists and psychologists as it has value for studying\ntopics such as rationality, social preferences, peer-effects and belief-updating mechanisms\n(Schlag & Tremewan, 2020). Here, we lean on a well-studied method for eliciting beliefs"}, {"title": "", "content": "known as the Most Likely Interval elicitation rule (Schlag & van der Weele, 2015). An\nannotator was asked to provide an interval $[L, U] \\subseteq [a, b]$ where they believed the average\nresponse, $x$, of a representative group would lie within. The interval $[a, b]$ was on a\ncontinuous scale, $a = 0$ and $b = 1$. $x$ is the average midpoint of the intervals provided by\nthe participants. In Experiment 1, when utilising incentives, each participant's potential\nbonus was calculated after all responses were collected. If $x$ was outside a participant's\ninterval $[L, U]$, then they did not receive a bonus. However, if $x$ was within the interval\n$[L, U]$, then they had a chance to receive a bonus, $S$, which was decreasing in the width,\n$W$, of their interval:\n$S = \\begin{cases}\n(1 - \\frac{W}{b-a})^\\lambda, & \\text{if } x \\in [L, U].\\\\\n0, & \\text{otherwise}.\n\\end{cases}$\nHow much a large $W$ should be penalised can be adjusted with the free parameter $\\lambda$\nin $g = \\frac{1}{\\lambda}$, such that smaller $\\lambda$ incentivise smaller intervals. In our study, $\\lambda = 0.5$. \u03a4\u03bf\nutilise the same belief elicitation method without bonuses, we simply explained the aim:\nintervals that are as narrow as possible while wide enough to feel confident that it can\ncontain $x$."}, {"title": "Statistical analyses", "content": "We examined differences in participants' own judgements and beliefs with Wilcoxon\nsigned-rank hypothesis tests. Differences between Democrats' and Republicans' responses\nwere examined with one-sided Mann-Whitney U-tests. We used non-parametric tests since\njudgements are not normally distributed. Both types of tests were performed with the\nPython library Scipy. Reported hypothesis-test results are over the four arguments\nincluded in both experiments and with a mean of each participant's response for the two\nDemocrat arguments and for the two Republican arguments. For beliefs, the mean is of the\ninterval midpoints of the two arguments. In Experiment 2, where participants provided\ntheir beliefs over other Democrats and Republicans separately, we firstly took the midpoint\nof each interval and then the midpoint of the two midpoints. We supplement these analyses"}, {"title": "", "content": "with Linear Mixed Effects Models (LMM) fitted on the responses to Democrat and\nRepublican arguments separately. The models predicted either judgements or beliefs given\npolitical affiliation. Participant ID was included as an additional random effect since\nparticipants have judged, and provided beliefs for, two Democrat arguments and two\nRepublican arguments. LMMs were also used to evaluate the effect of providing monetary\nincentives in Experiment 1 by fitting models to predict beliefs, again separately for\nDemocrat and Republican arguments, based on the condition (incentives or not) and with\nparticipant ID as random effect.\nWe estimated the effect of varying annotator sample sizes that are closer to reality\nwith bootstrapping. To this end, we randomly sampled 1 to 50 judgement and belief\nannotations of the four arguments annotated in both experiments. We did this 1000 times\nfor each sample size and report the means. We further sampled from a Democrat and a\nRepublican population of annotators to estimate the effect of sampling from biased\nparticipant pools. We then calculated the difference between the population means (all\nparticipants' judgements or beliefs) and the bootstrap sample means with Root Mean\nSquared Error (RMSE)."}, {"title": "Results", "content": "We conducted two survey-based experiments with a total of 1,590 US-based\nparticipants (810 Democrats and 780 Republicans) to evaluate the effect of belief\nelicitation when annotating text data.\nMonetary incentives did not have any noteworthy effects on the belief response\n(LMM, p = 0.07 for D arguments and p = 0.88 for R arguments, see Table D1), and did\nnot result in different conclusions of hypothesis testing compared to not using incentives."}, {"title": "Judgement vs Belief", "content": "Figure 2 and Table 2 summarise the results of both within- and\nbetween-participants hypothesis tests for both experiments. Within-participants tests\nreveal that both Democrats' and Republicans' beliefs are significantly different from their"}, {"title": "", "content": "own judgements, indicating an awareness of biased responses of either others or themselves.\nFor Democrat annotators in Experiment 1, the directions of the adjustments from\njudgement to belief are as expected, with judgements of Democrat arguments being\nsignificantly higher than their beliefs, while judgements of Republican arguments are\nsignificantly lower than beliefs. This translates to being certain that arguments reflecting a\nDemocrat stance are, in fact, arguments, while believing others think the opposite or are\nless certain of this fact, and vice versa for Republican arguments. Similar adjustment are\nseen in Experiment 2, but here, for Republican arguments, Democrat annotators'\nadjustments are small and insignificant.\nFor Republican annotators, we would expect beliefs of Democrat arguments to be"}, {"title": "", "content": "an upwards adjustment from their judgements. However, in Experiment 1, Republicans'\njudgements of Democrats arguments are, unexpectedly, significantly higher than their\nbeliefs, meaning they on average believe the representative group will agree less than\nthemselves on the Democrat arguments. For Republican arguments, their judgements are,\nexpectantly, also significantly higher than their beliefs. The same patterns are found in\nExperiment 2.\nFigure 3 shows the judgements and beliefs from Experiment 2 with the annotators'\nseparate beliefs for other Democrat and Republican annotators' judgements. The plot\nillustrates how the midpoints shown in Figure 2 are the results of inferring about two\npopulations' judgements, by clearly showing how the beliefs moves in two different\ndirections depending on which population's judgements are being considered. This clearly\nindicates that the results from Experiment 1 cannot be explained by annotators always\nguessing around the middle of the scale, when asked for beliefs, rather than the desired\neffect of the separate beliefs pulling in different directions. However, for Democrat\narguments, beliefs are pulled downwards and away from the elicited median annotation\n(i.e. they are over-adjusted) due to what seems to be wrongful or exaggerated beliefs of\nRepublican's judgements."}, {"title": "Bias reduction", "content": "We further investigated whether beliefs are less biased than judgements by\ncomparing the medians of Republicans' and Democrats' judgements and beliefs, separately,\nexpecting a smaller difference between their beliefs than their (biased by construct)\njudgements.\nFigure 2 clearly shows that the difference between Democrats and Republicans is\nsmaller when considering beliefs. In Experiment 1, for Democrat arguments, the difference\nis reduced from 0.15 (corresponding to 15 out of 100 steps on the continuous scale used for\nannotation) in judgements to only 0.02 in beliefs.\nSimilarly for Republican arguments, the difference in medians is reduced from 0.14"}, {"title": "", "content": "to 0.01. In Experiment 2, with the smaller sample size, we see similar tendencies. For\nDemocrat arguments, the difference in median judgement is small to begin with, yet the\ndifference is reduced from 0.05 to 0.01. For Republican arguments, the difference is reduced\nfrom 0.10 to 0.01.\nThese results indicate that bias, defined as systematic differences between the two\ngroups of annotators, is consistently reduced when asking for beliefs instead of judgements.\nFurthermore, there is a consistently large reduction in variance, which is clearly\nvisible in Figure 2 and confirmed with permutation tests. (Figure D1: Supplementary\nAnalyses). Hence, asking for beliefs may provide more robust annotations compared to\nstandard practices. However, we find that, in some cases, the average belief does not\naccurately reflect the population average judgement, which we ultimately aim to capture.\nFor Republican arguments, annotators over-estimate the bias of Democrats (guessing lower\njudgement value) and of Republicans (guessing larger judgement value). For Democrat\narguments, annotators over-estimate the bias of Republicans. When the variance is large,"}, {"title": "", "content": "and there is more uncertainty, as with the Republican arguments, the midpoints of beliefs\nprovide good estimations of the population average judgement, around 0.5, signalling that\nthere is a lot of disagreement in the instance. However, with Democrat arguments, the\nwrong beliefs of Republicans' judgements similarly pull the estimation towards 0.5 although\nit should be higher (around 0.7). The population averages of our study are, however, from\na substantially larger group of annotators than a standard annotation collection for any\nother study (standard is 1-5 annotators per instance). We therefore ask: For which\nannotator sample sizes is asking for beliefs more robust than asking for judgements? Our\nbootstrapping analyses, visualised in Figure 4, shows that belief annotations are closer to\nthe population average when collecting less than 19 annotations per instance. If sampling\nfrom unbalanced pools of annotators (either only Democrats or only Republicans), then the\nexpected error reduction is even more robust to increasing sample sizes."}, {"title": "Discussion", "content": "In this paper, we have investigated a simple method for reducing bias in annotations\nof text data. Building on the substantial, yet sometimes overlooked, evidence of systematic\ndisagreement in annotations of annotators with different socio-demographic backgrounds\n(which we call bias), we treat differences as meaningful rather than 'noisy' signals.\nTraining models on data which does not have balanced or representative labels is harmful\nto models' generalisability and, if going by undetected, can be harmful to the\nunrepresented socio-demographic groups when such models are applied. Our study used\nthe task of annotating arguments in text as a case where we know annotator bias exists\nand may result in models less able to recognise arguments that are not aligned with a\nspecific political stance. Common methods for handling disagreement, such as simply\ntaking the mean of a handful annotations of the same instance, are ineffective for creating\naccurate and fair label distributions, and previous efforts towards improving the status-quo\nhave required either a large pool of annotators or instances per annotator, which might not\nbe feasible in all projects."}, {"title": "Limitations and Future Directions", "content": "Possible limitations of asking for beliefs are the need for being able to make good\nassumptions about which socio-demographic characteristics are important, as well as\ndescribing these characteristics for the annotators. In our experiments, we define the\ncharacteristics of the population(s) of which we ask annotators to estimate judgements\n(50% Democrats and 50% Republicans, or Democrats and Republicans separately). Our\nresults suggest that the definitions are essential for the beliefs. The extend to which very\nspecific versus broad (for instance, defining the population as representative without\nmentioning party affiliation) definitions influence the annotations is still left to be explored.\nRelatedly, our proposed method requires knowledge about the annotators'\nsocio-demographic characteristics that may be responsible for biased annotations in the\nfirst place. If such biases have been documented and are known (i.e., \u201cknown unknowns\u201d,\n(Bail, 2023)), our method may help to reduce bias preemptively, but it is not suitable to\nreduce annotation bias due to unknown annotator characteristics (i.e., \u201cunknown\nunknowns\"). Socio-demographic characteristics only partly explain annotator biases and\nthe extend to which such characteristics affect annotations depends on the task. Fleisig\net al. (2024) highlight that factors such as media usage and opinions may be more\ninfluential than demographics, for some tasks. (In this study, we treat political affiliation,\nwhich is closely connected to opinion, as a socio-demographic factor, while Fleisig et al.\""}, {"title": "", "content": "(2024) treats opinions as seperate from demographics.)\nMeasuring bias, the effect of belief elicitation, and having a reliable population\naverage, has required the recruitment of a large amount of annotators. The cost of this has\nbeen a modest set of instances which our annotation method has been tested on.\nFurthermore, the belief elicitation method was only tested on instances where bias was\nfound, since we expect believe elicitation to have little or no influence on instances that are\n'easy' or un-divisive. This study serves as a proof-of-concept, showing that annotator bias,\nand variance, can be reduced by belief elicitation. Future directions lies in applying the\nmethod to larger, diverse datasets and training models on the annotations."}, {"title": "Conclusions", "content": "We found that eliciting beliefs in annotations has the potential to reduce the risk of\nannotator bias. The method is low-cost and applicable to many annotation tasks were\nannotation bias is expected, and to test hypotheses of the occurrence of bias. Unlike most\nmethods for mitigating biases, belief elicitation tackles bias ex ante, rather than ex post,\nand does not require a large amount of annotators or instances to be applicable. We\nencourage further studies to investigate the method with other tasks and down-stream\napplications."}, {"title": "Ethics declarations", "content": "Broader impact. We hope this study will promote further research on\nunderstanding annotator behaviour and ways to reduce demographic-dependent biases\nbefore modelling, by means of better understanding the annotators we recruit. Our study\ncontributes to research on improving data quality by arguing for a more nuanced look on\nwhat quality means quality is not simply achievable by removing \u2018outliers' or by taken\nthe average of a handful judgements. However, quality may be improved by changing the\nway we ask for annotations and having more ways of inferring about annotators' beliefs\nand uncertainty of instances."}, {"title": "Personal and sensitive data", "content": "This study deals with personal and sensitive data,\ni.e. political affiliation, age, gender and education level. Responses are anonymous and\ncannot be used to identify any individual. The study has an IRB approval and both\nexperiments were pre-registered on OSF."}, {"title": "Consent to participate", "content": "Participants were informed of the study's objective and\nconsented to the sharing of their anonymous responses, for research purposes. In the\nconsent form, they were warned the texts were related to controversial topics and could\ninclude offensive statements or language."}, {"title": "Consent for publication", "content": "Participants consented to having their anonymised\ndata published and used for research purposes."}, {"title": "Potential risks", "content": "While we do not anticipate any risks from participation in the\nstudy, we do note a recent awareness of poor working conditions among crowd-workers\n(Williams et al., 2022). Since our participants are from the US and are recruited through\nProlific, who have fair minimum payment rules, we do not anticipate such poor working\nconditions, among our participants, as are sometimes the reality for AI data annotators."}, {"title": "Costs and remuneration", "content": "The cost of the entire study (Experiment 1 and 2) was\n1987.8\u00a3 (\u2248 2222$), whereof 129\u00a3 constitutes bonuses given in Experiment 1. Participants\nwere paid 6.6\u00a3/h (\u22487.4$/h) on average across both experiments (excluding bonuses)."}, {"title": "Data and code availability", "content": "Data and code is available on the Open Science\nFramework\u00b9 and https://github.com/terne/belief-elicitation."}, {"title": "Pre-registration", "content": "The study was pre-registered on the Open Science Framework.23\nSee Appendix E for more details."}, {"title": "Funding", "content": "There was no external funding for this research."}]}