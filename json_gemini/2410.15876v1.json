{"title": "FLICKERFUSION: INTRA-TRAJECTORY DOMAIN\nGENERALIZING MULTI-AGENT RL", "authors": ["Woosung Koh", "Wonbeen Oh", "Siyeol Kim", "Suhin Shin", "Hyeongjin Kim", "Jaein Jang", "Junghyun Lee", "Se-Young Yun"], "abstract": "Multi-agent reinforcement learning has demonstrated significant potential in ad-\ndressing complex cooperative tasks across various real-world applications. How-\never, existing MARL approaches often rely on the restrictive assumption that the\nnumber of entities (e.g., agents, obstacles) remains constant between training and\ninference. This overlooks scenarios where entities are dynamically removed or\nadded during the inference trajectory-a common occurrence in real-world en-\nvironments like search and rescue missions and dynamic combat situations. In\nthis paper, we tackle the challenge of intra-trajectory dynamic entity composi-\ntion under zero-shot out-of-domain (OOD) generalization, where such dynamic\nchanges cannot be anticipated beforehand. Our empirical studies reveal that ex-\nisting MARL methods suffer significant performance degradation and increased\nuncertainty in these scenarios. In response, we propose FLICKERFUSION, a novel\nOOD generalization method that acts as a universally applicable augmentation\ntechnique for MARL backbone methods. Our results show that FLICKERFUSION\nnot only achieves superior inference rewards but also uniquely reduces uncertainty\nvis-\u00e0-vis the backbone, compared to existing methods. For standardized evalua-\ntion, we introduce MPEv2, an enhanced version of Multi Particle Environments\n(MPE), consisting of 12 benchmarks. Benchmarks, implementations, and trained\nmodels are organized and open-sourced at flickerfusion305.github.io,\naccompanied by ample demo video renderings.", "sections": [{"title": "INTRODUCTION", "content": "Multi-agent reinforcement learning (MARL) is gaining significant attention in the research community\nas it addresses real-world cooperative problems such as autonomous driving (Antonio & Maria-\nDolores, 2022), power grid control (Biagioni et al., 2022), and AutoML (Wang et al., 2023b).\nNevertheless, MARL research is commonly over-simplified, in that often, restrictive assumptions\nare made. Among these assumptions, we focus on one where the number of entities (e.g., agents,\nobstacles) available during training and inference is the same (Wang et al., 2021b; Liu et al., 2023;\nZang et al., 2023; Hu et al., 2024). In response, Iqbal et al. (2021) introduced the concept of dynamic\nteam composition, where the number of agents varies between training and testing. This concept has\nbeen further explored in subsequent works (Hu et al., 2021; Wang et al., 2023a; Shao et al., 2023;\nTian et al., 2023). Despite extensive research, two crucial characteristics ((1) and (2)) frequently\nencountered in real-world MARL deployments have been overlooked.\n(1) Intra-trajectory Entrance. Consider the following examples where entities enter and leave\nintra-trajectory. During a post-tsunami search and rescue (SAR) mission, additional obstacles may\nenter and leave during the inference trajectory (Drew, 2021), potentially due to secondary hazards like\ncollapsing infrastructure. Similarly, in national defense, additional adversaries or allies dynamically\nenter and leave during the inference trajectory (Asher et al., 2023). Whether they are allies that\nrequire cooperation or adversaries and obstacles that the allies need to adapt to, this requires on-the-fly\nadaptation. Exiting entities have been studied in MARL, such as SMAC (Samvelyan et al., 2019)\nwhere units die, but scenarios where entities enter intra-trajectory have not yet been studied.\nDespite seemingly small differences, the dynamics of entering entities differ from dying entities.\nConsider a simple predator-prey environment, Tag, in which the ally agents must move to avoid being\ntagged by adversaries. Unlike the simple case where an entity dies, intra-trajectory introduces novel\nscenarios where new entities enter from the outskirts of the observed region. For instance, when an\nally enters, the existing allies and the new ally can enact a dispersion strategy to spread the attention\nof adversaries (Fig. 1a). Or, when a new adversary is introduced, the allies must immediately shift\ntheir initially planned trajectory to evade the new adversary (Fig. 1b). The agents' decentralized\npolicies must adapt to this sudden shift in the scenario, without additional training intra-trajectory.\nIndeed, as shown in Fig. 1c, existing MARL approaches' performance deteriorates when entities\nenter intra-trajectory.\n(2) Zero-shot Domain Generalization. Such performance degradation is more pronounced when\nthe agents are presented with an entity composition they have never been explicitly trained on. For\ninstance, in Tag, even though agents are trained with scenarios with at most four adversaries, they\nmay be presented with five or more adversaries during inference. One na\u00efve solution is to explicitly\ntrain on every possible dynamic scenario a priori, and this is the approach taken by prior literature on\ndynamic team composition (Iqbal et al., 2021; Hu et al., 2021). The key underlying assumption is\nthat one knows the number of entities beforehand during training, which is unrealistic. In search and\nrescue (SAR) missions (Niroui et al., 2019), one cannot know all possible combinations of obstacles\nand targets (humans to rescue) in advance; in national defense, knowing all potential adversaries'\ncombinations during combat a priori is impossible. Even when some information of the inference\ncombination is available a priori, the total number of combinations can be very large and impractical\nto train on. This motivates us to study these dynamic MARL scenarios under zero-shot out-of-domain\n(OOD) generalization (Min et al., 2020), where here, domain refers to the entity composition.\nPrior Attempts. These unexpected dynamic scenarios present a non-trivial problem-each agent is\nforced to observe an input-space greater than it was trained on. Prior works (Hu et al., 2021; Zhang\net al., 2023; Shao et al., 2023) all took the approach of appropriately expanding the Q or policy\nnetwork, via additional parameters, using inductive bias. However, empirically, all fail to deliver\nreasonable performance in the considered OOD MARL environment, let alone being consistent,\nacross environments, and within environments (seeds)\u2014even after extensive hyperparameter tuning\n(Table 1; Fig. 5 (top-left)). The high uncertainty of existing methods especially renders them unreliable\nfor safety-critical applications (Knight, 2002) such as autonomous vehicles (Pek et al., 2020; Feng\net al., 2023), where the deployed method must be robust to changes between train and test-time."}, {"title": "PRELIMINARIES", "content": "Notations. N is the set of natural numbers, and $\\mathbb{N}_0 := \\{0\\} \\cup \\mathbb{N}$. For a positive integer $n \\in \\mathbb{N}$,\ndenote $[n] := \\{1,2,\\dots, n\\}$. For two vectors a and b (possibly of different dimensions), a b is the\nconcatenated vector [a b]. For a set A, $|A|$ is its cardinality, $2^A$ is its power set, $\\Delta(A)$ is the set of\nall possible probability distributions over A, whose support is potentially a strict subset of A, and\nA* is the set of all possible A-valued sequences of arbitrary lengths. Also, for an integer $n \\leq |A|$,\n$\\binom{A}{n} := \\{B \\subset A : |B| = n\\}$. For a $m \\times n$ matrix A, denote $\\text{nrow}(A) = n$ to be the number of row.\nDec-MDP with Dynamic Entities. We follow the de facto MARL framework, Centralized Training\nDecentralized Execution (CTDE; Rashid et al. (2018); Foerster et al. (2017)). Our setting can be\ndescribed as a Decentralized Markov Decision Process (Dec-MDP; Bernstein et al. (2002); Oliehoek\n& Amato (2016)) with dynamic entities (Iqbal et al., 2021; Schroeder de Witt et al., 2019), described\nby the tuple $(\\mathcal{E} := \\mathcal{E}_a \\cup \\mathcal{E}_n, \\Phi, \\epsilon, \\mathcal{S}, \\mathcal{U}, P, \\mathbb{P}, \\mathcal{R}, \\gamma)$.\nEntities. $\\mathcal{E}$ is the (potentially countably infinite) set of all possible entities, that may enter or leave\nintra-trajectory, and $\\mathcal{E}$ is partitioned into the set of (trainable) agents $\\mathcal{E}_a$ and other non-agent entities\n$\\mathcal{E}_n$. For a $L \\in \\mathbb{N}$, we assume each entity takes one of $L$ types (e.g., ally, adversary, obstacle), and\n$\\Phi := \\{e_1, e_2,\\dots, e_L \\}$ is the set of all possible entity types where $e_l$'s are the elementary bases of\n$\\mathbb{R}^L$. For each entity $e \\in \\mathcal{E}$, we denote $\\delta_e \\in \\Phi$ as its type.\nState-Action Spaces. Let $\\mathcal{S} \\subset \\mathbb{R}^{d_s}$ be the shared state space of all the entities, and assume\nthat each entity $e \\in \\mathcal{E}$ has a state representation $s_e \\in \\mathcal{S}$. Following Iqbal et al. (2021), we\nassume that $s_e := f_e \\oplus \\delta_e$ where $f_e \\in \\mathbb{R}^{d_{fe}}$ is the feature vector (e.g., velocity, location).\nTo deal with potentially entering and leaving of entities, we define the joint state space $\\mathcal{S}$ as\nthe space of joint states of all possible entity combinations: $\\mathcal{S} := \\bigcup_{E \\subset \\mathcal{E}} \\mathcal{S}(E)$ with $\\mathcal{S}(E) :="}, {"title": "FLICKERFUSION: A NEW APPROACH TO OOD MARL", "content": "As mentioned in the Introduction, we take a different direction from existing methods. Therefore,\nwe first revisit the two most popular MARL backbones and unpack the innate cause of domain-shift\nperformance degradation under them (Sec. 3.1), then introduce our approach (Sec. 3.2, 3.3).\n3.1 TOWARDS DOMAIN GENERALIZATION ACROSS ENTITY COMPOSITION\nIn our context, domain is a set of tuples $\\{(N_1^\\ell, N_u^\\ell)\\}_{\\ell \\in [L]} \\subset \\mathbb{N}_0$ such that\n$N_\\ell \\leq \\min_{t \\geq 0} |\\{e \\in E_t : \\delta^e = e_\\ell\\}| \\leq \\max_{t \\geq 0} |\\{e \\in E_t : \\delta^e = e_\\ell\\}| \\leq N_u^\\ell \\text{ a.s.}, \\forall \\ell \\in [L].$\\newline\nIn other words, the number of type $\\ell$, entities is between a lower bound, $N_\\ell$, and an upper bound,\n$N_u^\\ell$. Given training occurs in-domain, $\\mathcal{D}_\\text{I}$, then, it is out-of-domain (OOD), $\\mathcal{D}_\\text{O}$, if there exists at\nleast one $\\ell \\in [L]$ such that $N_\\ell$ of $\\mathcal{D}_\\text{O}$ is strictly greater than $N_u^\\ell$ of $\\mathcal{D}_\\text{I}$ (see \u201cOut-of-Domain\u201d in\nFig. 3a). Thus, in $\\mathcal{D}_\\text{O}$, the learner faces entity-type combinations that she has never seen before. We\nillustrate why a na\u00efve application of usual MARL backbones (QMIX-MLP and QMIX-Attention) are\nsub-optimal for domain generalization. Note that while we focus on off-policy backbones as they are\nthe most commonly used, all our discussion can be trivially extended to on-policy networks. Let us\ndenote $\\theta^\\text{D}_\\text{I}$ as the parameters of the in-domain trained QMIX.\nMLP Backbone. First, consider QMIX-MLP (Rashid et al., 2018), where an MLP parametrizes the\nQ-function. When using QMIX-MLP in OOD scenarios, one must expand $\\theta^\\text{QMIX}_\\text{I}$ to $\\theta^\\text{QMIX}_\\text{Do} := \\theta^\\text{QMIX}_\\text{DI} \\oplus \\theta^{\\prime}$ to take OOD observation $o^\\text{Do}$ as input. This is because $|o^\\text{D}_\\text{O}| > |o^\\text{D}_\\text{I}|$ due to the additionally\nintroduced entities, where $o^\\text{D}_\\text{I}$ is the in-domain observation. But, because no additional training is\nallowed during execution, the choice of initialization of $\\theta^{\\prime}$ is critical."}, {"title": "FLICKERING VIA ENTITY DROPOUT", "content": "Hereforth, while past works remedy OOD generalization via injecting inductive biases within $\\theta^{\\prime}$,\nour orthogonal approach ensures that $\\text{nrow}(X^{\\mathcal{D}_O}) = \\text{nrow}(X^{\\mathcal{D}_I})$, no longer requiring $\\theta^{\\prime}$. In turn,\nthe key design question becomes how many and which entities to drop.\nHow Many to Drop? We consider the approach of artificially limiting the visibility as follows:\n$X^{\\mathcal{D}_O} \\leftarrow X^{\\mathcal{D}_O} [I,:], I\\in\\binom{[\\text{nrow}(X^{\\mathcal{D}_O})]}{ \\text{nrow}(X^{\\mathcal{D}_I})}$"}, {"title": "FUSING THE FLICKERS", "content": "We now demonstrate how we can recover some of the lost information, further improv-\ning the trade-off of Eq. (6) to our favor. For each (active) agent $a$, let $D_a \\subset E_t$ be\nthe set of dropped entities from the perspective of $a$. If $D_a$ remains fixed throughout\nthe trajectory, then $a$ never sees $D_a$. We alleviate this problem by stochastically changing\n$D_a$ at each $t$. In essence, FLICKERFUSION fuses (stochastic sampling at each $t$) the flick-"}, {"title": "MPEv2: AN ENHANCED MARL BENCHMARK", "content": "To ensure that this problem setting does not fall prey to the standardization issue raised by Papoudakis\net al. (2021), we enhance Multi Particle Environments (MPE; Lowe et al. (2017)) to MPEv2,\nconsisting of 12 open-sourced benchmarks. Although of a different nature, the enhancement is\nconceptually similar to the enhancements made to the StarCraft Multi-agent Challenge (SMAC;\nSamvelyan et al. (2019)) that led to SMACv2 (Ellis et al., 2023) with stochasticity and SMAC-Exp\n(Kim et al., 2023) with multi-stage tasks. While SMAC and MPE are the two most popular MARL\nenvironments (Rutherford et al., 2024), we choose to improve MPE due to its lower computational\nrequirements (Papoudakis et al., 2021), ensuring accessible research. This is pertinent as OOD dy-\nnamic entity composition is inherently computationally burdensome, due to dimensionality explosion\nvis-\u00e0-vis the simulator (benchmark) and model (parameter) size.\nMPEV2 consists of six environments, three extensions to the original MPE, and three novel. These\nenvironments were developed a priori to the experiments. All environments support an arbitrary\ndynamic entity composition, including intra-trajectory addition and deletion. As visualized in Fig. 3a,\neach environment has two benchmarks, OOD1 and OOD2, which assess generalization performance\nby increasing the number of parameterized agents and non-agent entities, respectively. Fine-grained\ndetails for each benchmark are presented in Appendix B.\nBrief Descriptions. First, we take the three most appropriate (in terms of our dynamic scenarios)\nMPE environments, (1) Spread, (2) Adversary, (3) Tag, and add-on arbitrary dynamic entity\nscenarios. These are then standardized to six benchmarks, two for each environment. The remaining\nthree environments are newly created. (4) Guard is a two-entity (agents, targets) environment where\nthe agents act as bodyguards to the targets. The targets stochastically move point-to-point given a time\ninterval. The agents' objective is to emulate guarding multiple moving targets. Furthermore, they are\npenalized for colliding with the targets. (5) Repel is a two-entity (agents, adversaries) environment\nwhere agents try to maximize their distance from adversaries without leaving the play region. The play\nregion is defined for each environment in Appendix B. (6) Hunt is a two-entity (agents, adversaries)\nenvironment where agents hunt down the adversaries. The adversaries (deterministically) move away\nfrom the agents at each time step, therefore requiring agents to cooperate intelligently."}, {"title": "EMPIRICAL STUDY", "content": "Baselines and Setting. We examine the empirical performance of FLICKERFUSION against 11\nrelevant baselines. The baselines are divided into 3 categories: (i) MARL backbone methods, (ii)\nMARL backbone methods with model-agnostic domain generalization methods, and (iii) recent\nrelevant MARL methods. (i) The two backbone archiecture we use are QMIX-MLP (Rashid et al.,\n2018) and QMIX-Attention (Iqbal et al., 2021). (ii) To ensure that our method is most competitive"}, {"title": "DISCUSSION AND TAKEAWAYS", "content": "(1) Novel universal inductive bias injection. We show a new inductive bias encoding method\nfor OOD MARL. We rigorously investigate the cause of poor performance under OOD. Then, we\nsystematically identify an orthogonal approach. Our findings highlight that OOD MARL problems are\nbetter solved via input augmentation, rather than encoding priors into new parameters. Our approach\nis also universal in that it can easily apply to both MLP and attention backbones.\n(2) Potential in non-attention backbones. Unlike other MARL generalization methods that only\nwork with an attention backbone (Shao et al., 2023; Zhang et al., 2023), FLICKERFUSION works\nexceptionally well under MLP and attention architectures. Surprisingly, FLICKERFUSION-MLP often\nbeats all other existing attention-based methods, even with smaller model (parameter) size (Table 1).\nFurther, during hyperparameter tuning, all else equal, we find no meaningful relationship between\nmodel size (parameters) and performance. This echoes other domain literature, which shows that\nattention may not always be the most optimal design choice (Liu et al., 2021; Zeng et al., 2023).\n(3) No reward-uncertainty trade-off. A salient advantage of FLICKERFUSION is its significant\nreduction in uncertainty in OOD inference relative to baselines (Fig. 5 (top-left)). Remarkably,\nFLICKERFUSION is the only method where the uncertainty decreases relative to backbone. Also,\ninterestingly, we observe a negative relationship between reward performance and uncertainty. Mean-\ning, better methods vis-\u00e0-vis reward (\u2191) is also on average better in terms of uncertainty (\u2193). We\nprovide scatter plots and linear curve fitting analysis in Appendix G."}, {"title": "PROOF OF PROPOSITION 3.1", "content": "(We follow the proof for sample complexity of learning discrete distribution as in a note by C.\nCanonne)\nNote that $d(i) = \\Sigma_{a \\in [N_A]} 1[i \\in D_a]$, where $D_a \\in \\binom{[N_E]}{\\Delta_\\ell}$ is the random entity subset of size\n$\\Delta_\\ell$ to be dropped by agent $a$. For each $i \\in [N_E]$, $1[i \\in D_a] \\sim \\text{Ber} (\\Delta_\\ell/N_E)$, and thus, $d(i) \\sim$\n$\\text{Bin}(N_A, \\Delta_\\ell/N_E)$. We then conclude as follows:\n$\\mathbb{E} \\Big[\\sum_{i \\in [N_E]} (\\frac{d(i)}{N_A} - \\frac{\\Delta_\\ell}{N_E})^2\\Big] = \\sum_{i \\in [N_E]} \\mathbb{E} \\Big[\\frac{d(i)}{N_A} - \\frac{\\Delta_\\ell}{N_E}\\Big]^2 $\n$\\leq \\sum_{i \\in [N_E]} \\Big[\\frac{\\mathbb{E} \\big[ d(i) - \\mathbb{E}d(i) \\big]^2}{N_A^2}\\Big]  $\n$= \\frac{1}{N_A^2}  \\mathbb{E} \\Big[\\sum_{i \\in [N_E]}  (\\frac{\\Delta_\\ell}{N_E}  (\\frac{\\Delta_\\ell}{N_E})) \\Big] (d(i) \\sim \\text{Bin}(N_A, \\Delta_\\ell/N_E) )\\Big] $\n$=\\sqrt{\\frac{\\Delta_\\ell (N_E - \\Delta_\\ell)}{N_A}}$\nLet us now convert the above in-expectation guarantee to a high-probability guarantee via Mc-\nDiarmid's inequality (McDiarmid, 1989). Define $S_{i,a} = 1[i \\in D_a] \\in \\{0,1\\}$ for $(i, a) \\in$\n$[N_E] \\times [N_A]$, which is independent across $a$. Consider the function $f$ defined as $f(\\{S_{i,a}\\}_{a \\in [N_A]}) :=$\n$\\frac{N_\\ell}{N_A} \\sum_{i \\in [N_E]} d(i)$. This satisfies the bounded difference property with constant $\\frac{1}{N_A}$. Thus, we\nhave that for any $\\varepsilon > 0$,\n$\\mathbb{P} \\Big[\\sum_{i \\in [N_E]} |f(\\{S_{i,a}\\}_{a \\in [N_A]}) - \\mathbb{E}[f(\\{S_{i,a}\\}_{a \\in [N_A]})]| > \\varepsilon \\Big] \\leq$\n$\\mathbb{P} \\Big[\\exists i \\in [N_E] : |f(\\{S_{i,a}\\}_{a \\in [N_A]}) - \\mathbb{E}[f(\\{S_{i,a}\\}_{a \\in [N_A]})]| \\geq \\frac{\\varepsilon}{N_E} \\Big] $\n$\\leq \\sum_{i \\in [N_E]} \\mathbb{P} \\Big[|f(\\{S_{i,a}\\}_{a \\in [N_A]}) - \\mathbb{E}[f(\\{S_{i,a}\\}_{a \\in [N_A]})]| \\geq \\frac{\\varepsilon}{N_E} \\Big] \\text{(union bound)}\n$\\leq N_E \\text{exp} \\Big[ -\\frac{2(\\varepsilon/N_E)^2}{(1/N_A)^2} \\Big]  \\text{(McDiarmid's inequality)}\n$= N_E \\text{exp} \\Big[ -2 \\varepsilon^2 (\\frac{N_A}{N_E})^2 \\Big]$\nAll in all, we have that:\n$\\mathbb{P}\\Big[ \\sum_{i \\in [N_E]} (\\frac{d(i)}{N_A} - \\frac{\\Delta_\\ell}{N_E})^2 \\leq \\sqrt{\\frac{\\Delta_\\ell (N_E - \\Delta_\\ell)}{N_A}}\\Big] \\geq 1 - N_E \\text{exp} \\Big[ -2 \\varepsilon^2 (\\frac{N_A}{N_E})^2 \\Big] \\leq \\mathbb{P} \\Big[ \\sum_{i \\in [N_E]} (\\frac{d(i)}{N_A} - \\frac{\\Delta_\\ell}{N_E})^2 < \\varepsilon  +  \\sqrt{\\frac{\\Delta_\\ell (N_E - \\Delta_\\ell)}{N_A}}\\Big] $\n$\\text{Reparametrizing } \\delta = N_E \\text{exp} \\Big[-2 \\varepsilon^2 (\\frac{N_A}{N_E})^2 \\Big] \\text{ gives the desired result. } \\Box$"}, {"title": "DEFERRED DETAILS FOR MPEv2", "content": "\u0412.1 \u0421\u043eMMON FEATURES OF MPEV2\nIn MPEv2, entities are placed in a 2-dimensional continuous environment. Distances within this\nenvironment are computed using Euclidean metrics. The distance $d(e_i, e_j)$ between two entities $e_i$\nand $e_j$ with radii $r_i \\in \\mathbb{R}$ and $r_j \\in \\mathbb{R}$ is computed as:\n$d(e_i, e_j) = ||p_i - p_j ||_2 - r_i - r_j$ (A1)\nwhere $p_i \\in \\mathbb{R}^2$ and $p_j \\in \\mathbb{R}^2$ are position vectors. The play areas are square and centered at the origin\n[0, 0]. The coordinates are constrained within the range $x \\in [-B, B]$ and $y \\in [-B, B]$, where B is\nthe half-width of the play area. Each agent can select from five discrete actions at every timestep.\nAgents receive a global reward and observe the global state.\nAction Space [no_action, move_left,\nmove_right, move_down, move_up]\n\u0412.2 TAG\nThis environment features $n_a$ agents and $n_{adv}$ heuristic adversaries that move towards the closest\nagent. The primary objective of this scenario is for the agents to learn to avoid collisions with\nadversaries while remaining within the defined boundaries.\n(tmax)\nPlay Area {(x,y) : x \u2208 [-2, 2], y \u2208 [-2, 2]; x, y \u2208 R}\nLet $\\mathcal{E}_a$ be the set of agents and $\\mathcal{E}_{adv}$ be the set of adversaries. The reward for each $t$ for this\nenvironment is given as:\n$\\mathcal{R}(:) := \\sum_{\\alpha \\in \\mathcal{E}_\\alpha} \\mathcal{l}_{outside} (x_a, y_a) \\cdot max \\Big(-25, min \\big(-5|x_a|-2, -5|y_a|-2\\big)\\Big) - 2 \\cdot \\sum_{adv \\in E_{adv}} \\mathcal{l}_{collision}(a, adv)$ (A2)\nAgents receive a penalty of -2 for each collision with an adversary and an additional penalty for\ntraveling beyond the boundaries, with the total boundary penalty capped at -25.\n\u0412.3 SPREAD\nThis environment consists of $n_a$ agents and $n_{tar}$ target landmarks. The agents are trained to effectively\ndistribute themselves among targets.\nPlay Area {(x, y) : x \u2208 [-1, 1], y \u2208 [-1, 1]; x, y \u2208 R}\nLet $\\mathcal{E}_a$ be the set of agents and $\\mathcal{E}_{tar}$ be the set of target landmarks. Agents are rewarded for each $t$\nbased on the proximity of each target to its nearest agent such that:\n$\\mathcal{R}(:) = \\sum_{tar \\in \\mathcal{E}_{tar}} min d(a, tar)$. (A3)"}, {"title": "GUARD", "content": "This environment has $n_a$ agents and $n_{tar}$ heuristic targets. The objective is to minimize the distance\nbetween each target and its closest agents. Targets move in random directions every 50 timesteps,\nwhile remaining within the boundaries. At a high level, the agents must learn to organize into groups\nto cover and track the moving targets.\nPlay Area {(x, y) : x \u2208 [-2, 2], y \u2208 [-2, 2]; x, y \u2208 R}\nLet $\\mathcal{E}_a$ be the set of agents and $\\mathcal{E}_{tar}$ be the set of heuristic targets. Then, the reward for each $t$ for this\nenvironment is given as:\n$\\mathcal{R}(:) = - \\sum_{tar \\in \\mathcal{E}_{tar}} ((\\frac{i}{N_a}) \\sum_{k=1}^{i} min d(a, tar) ) - 5 \\cdot \\mathcal{l}_{collision}(a, tar) where i = \\Big \\lfloor \\frac{N_a}{N_{tar}} \\Big \\rfloor$. (A4)\nAgents are rewarded based on the distances of each target to its $i$-closest agents, and are penalized for\ncollisions with targets."}, {"title": "REPEL", "content": "This environment consists of $n_a$ agents and $n_{adv}$ heuristic adversaries which move towards the closest\nagent. Agents are trained to maximize their distance from adversaries while staying within defined\nboundaries.\nPlay Area {(x,y) : x \u2208 [-2.5, 2.5], y \u2208 [-2.5, 2.5]; x, y \u2208 R}\nLet $\\mathcal{E}_a$ be the set of agents and $\\mathcal{E}_{adv}$ be the set of adversaries. The reward for each $t$ for this\nenvironment is given as:\n$\\mathcal{R}(:) = \\sum_{\\alpha \\in \\mathcal{E}_\\alpha} [\\mathcal{l}_{outside} (x_a, y_a) \\cdot max \\Big(-25, min \\big(-5|x_a|-2, -5|y_a|-2\\big)\\Big)]+ \\sum_{\\alpha \\in \\mathcal{E}_\\alpha} min d(a, adv)$.(A5)"}, {"title": "ADVERSARY", "content": "This environment features $n_a$ agents, $n_{adv}$ heuristic adversaries, $n_{tar}$ target landmarks, and $n_{dec}$\ndecoy landmarks. Adversaries cannot distinguish between target and decoy landmarks, and move\ntowards the landmark closest to any agent. At a high level, the agents must learn to stay close to the\ntargets while guiding adversaries away from them.\nPlay Area {(x, y) : x \u2208 [-1.3, 1.3], y \u2208 [-1.3, 1.3]; x, y \u2208 R}\nLet $\\mathcal{E}_a$ be the set of agents, $\\mathcal{E}_{adv}$ the set of adversaries, and $\\mathcal{E}_{tar}$ the set of target landmarks. The\nreward for each t for this environment is given as:\n$\\mathcal{R}(:) =  \\sum_{tar \\in \\mathcal{E}_{tar}}  min \\Big( min d(a, tar)\\Big) + \\sum_{adv \\in E_{adv}}  min d(adv, tar)$. (A6)\nHence, agents are rewarded based the proximity of the nearest agent and adversary to each target, or\nmore specifically, the difference between the sum of minimum distances from adversaries to each\ntarget and the sum of minimum distances from agents to each target."}, {"title": "HUNT", "content": "This environment consists of $n_a$ agents and $n_{adv}$ heuristic adversaries which move away from the\nclosest agent while staying within defined boundaries. Agents are trained to minimize their distance\nfrom adversaries.\nLet $\\mathcal{E}_a$ be the set of agents and $\\mathcal{E}_{adv}$ be the set of adversaries. The reward for each $t$ for this\nenvironment is given as:\n$\\mathcal{R}(:) =  \\frac{1}{N_{adv}}  \\sum_{\\chi \\in \\mathcal{E}_{adv}} min d(a, adv) + \\frac{1}{N_{a}} \\sum_{ adv \\in \\mathcal{E}_{adv}} min d(a, adv)$.(A7)"}, {"title": "HYPERPARAMETERS", "content": "\u0421.1 \u0421\u043e\u043cMMON HYPERPARAMETERS\nIn the tables below are the hyperparameters used for MLP backbone models (Table 22) and attention\nbackbone models (Table 23)."}, {"title": "BASELINE MODEL-SPECIFIC HYPERPARAMETERS", "content": "In this section, the hyperparameters used for CAMA (Table 24) and UPDeT (Table 25) models are\ndetailed."}, {"title": "ADDITIONAL METRICS", "content": "See Table 27 and for IQM values. Furthermore, see Fig. 6 for the 95% IQM uncertainty figure."}, {"title": "EMPIRICAL STUDY (EXTENDED)", "content": "The figures below visualize the empirical study results (Figure 7 to 12) for each environment."}, {"title": "AGENT-WISE POLICY HETEROGENEITY", "content": "Diversity across rows in Figure 13 highlight agent-wise policy heterogeneity. We visualize the\nattention matrix of QMIX-Attention and FLICKERFUSION-Attention during inference. For consistent\nvisualization, we snapshot the matrices at the middle of the episode ($t_{max}/2$). Resolving agent-wise\nhomogeneity enhances the aggregate strategic expressivity of the cooperative system. This can be\nqualitatively validated on our site's demo rendering videos. We visualize all attention matrices for the\nremaining benchmarks in (Figure 14 to 22"}]}