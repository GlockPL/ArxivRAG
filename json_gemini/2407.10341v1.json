{"title": "Affordance-Guided Reinforcement Learning via Visual Prompting", "authors": ["Olivia Y. Lee", "Annie Xie", "Kuan Fang", "Karl Pertsch", "Chelsea Finn"], "abstract": "Robots equipped with reinforcement learning (RL) have the potential to learn a wide range of skills solely from a reward signal. However, obtaining a robust and dense reward signal for general manipulation tasks remains a challenge. Existing learning-based approaches require significant data, such as demonstrations or examples of success and failure, to learn task-specific reward functions. Recently, there is also a growing adoption of large multi-modal foundation models for robotics. These models can perform visual reasoning in physical contexts and generate coarse robot motions for various manipulation tasks. Motivated by this range of capability, in this work, we propose and study rewards shaped by vision-language models (VLMs). State-of-the-art VLMs have demonstrated an impressive ability to reason about affordances through keypoints in zero-shot, and we leverage this to define dense rewards for robotic learning. On a real-world manipulation task specified by natural language description, we find that these rewards improve the sample efficiency of autonomous RL and enable successful completion of the task in 20K online finetuning steps. Additionally, we demonstrate the robustness of the approach to reductions in the number of in-domain demonstrations used for pretraining, reaching comparable performance in 35K online finetuning steps.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances in large language models (LLMs) and vision-language models (VLMs) trained on Internet-scale data show promising results in using commonsense understanding to plan and reason [37, 8, 1, 38, 25]. They can break down complex instructions provided in natural language into actionable task plans [9, 5, 2, 6, 18, 19], perform visual reasoning in a variety of contexts [50, 7, 36], and even generate coarse robot motions for simple manipulation tasks [5, 34, 46, 18, 20, 36]. However, current state-of-the-art models still struggle with understanding interactions and physical dynamics in 3D space, which is essential to robotic control. Determining how to ground such models in the specific embodiment and environment dynamics remains a significant challenge. Several prior works have utilized large pretrained models for robotic control, either through few-shot prompting or finetuning of large models to generate actions directly [5], plans [1, 19, 18], or code [26, 27]. However, finetuning these models typically requires extensive human supervision, such as teleoperated demonstrations, feedback on whether the task was successfully completed, or a predefined set of skills and their controllers.\nAn alternative paradigm for finetuning robotic policies is autonomous reinforcement learning (RL), which only requires a reward signal to refine the robot's behavior and can therefore require less supervision in comparison. A significant amount of recent work has also focused on improving the sample-efficiency of these algorithms by pretraining on large offline datasets [23, 3, 22, 24, 32, 51]. Despite these advances, obtaining a reward signal is still a non-trivial problem, that requires either careful engineering or large amounts of data to learn a robust reward function [17, 13, 14, 47, 41]. The application of pretrained VLMs for defining rewards is therefore attractive, but thus far, they have primarily been used for generating sparse rewards [31, 44, 51], which often leads to less efficient learning. VLMs hold much richer and denser knowledge that we can elicit, such as reasoning about the affordances of various objects and environments. In this work, we leverage this understanding to shape rewards for robotic RL.\nSpecifically, we present a method for open-vocabulary visual prompting to extract rewards from VLMs for online RL. We leverage insights on effective visual prompting methods from Liu et al. [28] to develop a method for generating keypoints and waypoint trajectories from which dense shaping rewards can be calculated. We integrate the pipeline of extracting affordance representations from VLMs and computing dense rewards into RoboFuME [51], an autonomous RL system that uses sparse rewards from a finetuned VLM. We demonstrate comparable success rates on a variety of complex object manipulation tasks used in RoboFuME, as well as improved success rates on new tasks that the existing pipeline struggles to generalize to using sparse rewards alone, with reduced reliance on in-domain expert demonstrations."}, {"title": "II. RELATED WORK", "content": "A. Foundation Models for Robotics\nThe rapid development of foundation models in recent years has drawn significant attention both in the academic community and beyond [12]. This surge of interest in foundation models has arisen because they demonstrate that models trained on broad, Internet-scale data are highly adaptable to a wide range of downstream tasks. Robotics is a specific downstream task of foundation models that has garnered a lot of interest in the academic community. Works like SayCan [1] demonstrate encouraging results in training language-conditioned robotic control policies. Such approaches allow us to leverage and ground the rich knowledge and reasoning capabilities of LLMs to enable embodied agents to complete long-horizon tasks.\nWhile the results from SayCan [1] seem promising, there is a critical engineering risk that the reasoning abilities and representations captured by LLMs are overly general for embodied tasks, and more work is needed to properly ground the high-level plans generated by LLMs in low-level actions for embodied tasks. Converting visual observations into language descriptions and planning in solely in the language space loses a lot of rich information critical to scene understanding, which is a major limitation of using LLMs for spatial planning, reasoning, and task completion. Notably, the ELLM system from Du et al. [10] generated inaccurate responses to whether objects matched the goal positions when tasked with rearranging objects in a household environment to match the goal arrangement. It is important to pay attention to the pitfalls in the household environment despite the successes in the open-world Crafter environment. The goals for survival in an open-world environment (such as build house, or acquire food) are fairly general and transferable, and LLMs have likely encountered such scenarios during training and can suggest reasonable goals. However, the general knowledge encoded in LLMs may not necessarily as beneficial for robot learning: LLMs can provide general priors for planning and reasoning, but this generality also results in reduced specificity to the environment that the robot is operating in, thus the general priors may require additional grounding.\nTherefore, planning solely with language thus loses a lot of information associated with the richness of the visual modality that is critical to most robotics applications. Our work explores leveraging the visual modality via state-of-the-art VLMs to facilitate reasoning in both language and image domains."}, {"title": "B. Vision-Language Models", "content": "The clear advantage of language is the natural interface for providing task instructions and describing goals. That said, much of robotics research relies heavily on accurately perceiving and interacting with the environment. Several state-of-the-art VLMs [33, 49, 29, 39] demonstrate highly generalizable open-vocabulary object localization. However, they still lack the extensive reasoning capabilities of LLMs, since reasoning over image inputs is significantly more complicated. Preliminary VLMs require text queries where the objects involved are known a priori to generate bounding boxes around the requested objects, which are then provided to LLMs for downstream reasoning. Modern VLMs, such as GPT-4V [52] and Gemini [15], have demonstrated promising capabilities in combining LLM reasoning with environment perception via visual inputs. A key advantage of using modern VLMs is that it simplifies the process of translating high-level plans into low-level robot actions. While previous works leveraging LLMs like SayCan [1] require pretrained skill policies for each action primitive, using modern VLMs can circumvent the issue of selecting from a suite of pretrained action policies, by deriving rewards from image space that can be used for learning state-action mappings via RL. This is because VLMs, unlike LLMs, can determine success or failure based on image observations, and this reward signal can be used to enable robots to learn through trial-and-error, without training skill policies via imitation learning which are costly and difficult to scale. VLMs can also guide the learning process by generating shaping rewards, in the form of intermediate waypoints.\nDefining rewards in image space by using VLMs to determine task completion and specify intermediate waypoints as goals is a key contribution of our work. A major engineering challenge is tuning the inputs to these VLMs, which are highly expressive but also opaque, to derive useful reward signals for learning. Both language and image prompts require careful tuning to generate accurate and meaningful outputs, as we have found that modern VLMs still struggle to some degree with spatial reasoning. Our work explores combining preliminary VLMs and modern VLMs, as the outputs (bounding boxes or segmentations) of preliminary VLMs can serve as more helpful inputs to modern VLMs for semantic reasoning than raw image observations, thereby leveraging pretrained representations in VLMs as reward predictors."}, {"title": "C. Autonomous Reinforcement Learning", "content": "Online RL is the paradigm by which agents gathers data through interaction with the environment, then stores this experience in a replay buffer and updates its policy based on this experience. This contrasts with offline RL, where the agent updates its policy using previously collected data or human demonstrations, without itself interacting with the environment. A longstanding goal is autonomous RL: the potential of placing a robot in a real-world environment and it improves on its own by autonomously gathering in-domain experience, which holds great promise for scalable robot learning. In autonomous RL, the agent not only learns through its own experience, but also does not require human supervision to reset the environment between trials [40].\nAlgorithms for autonomous RL have been difficult to implement in the real world, with the primary challenge being sample complexity, the number of calls to the model required to achieve acceptably good performance. In addition, there is the challenge of providing well-shaped rewards for online exploration, as well as the difficulty of continual reset-free training, which requires significant human effort. Several works have developed systems for reset-free training to reduce or eliminate human interventions in the online RL process [4, 51, 16, 42], but reward engineering is an open problem as manually specified reward signals are seen as difficult to engineer and easy to exploit, and autonomous RL suffers when the reward signal is too sparse. While hand-designing reward functions is challenging, there is great potential to learn reward functions from previously collected data or extract rewards from large pretrained models. Some works have attempted to learn rewards from human feedback [43, 4], while acknowledging that these rewards are noisy and still require human intervention. The large bank of offline image and video datasets, as well as the high inference speed and accessibility of large pretrained models, could potentially offer solutions to further reduce or eliminate human intervention, while providing more precise and informative shaping rewards.\nThere has been attempts to leverage VLMs to generate rewards for online RL. RoboFuME [51] finetunes MiniGPT-"}, {"title": "III. METHODOLOGY", "content": "A. Problem Statement\nWe consider problems that can be formulated as a partially observable Markov Decision Process (POMDP) tuple $(S, A, O, \\gamma, f, p, r, d_0)$ where $S$ is the state space, $A$ is the action space, $O$ is the observation space, $\\gamma \\in (0,1)$ is the discount factor, $r(s, a)$ is the reward function and $d_0(s)$ is the initial state distribution $d_0(s)$. The dynamics are governed by a transition function $p(s'|s,a)$. The observations are generated by an observation function $f(o|s)$. The goal of RL is to maximize the expected sum of discounted rewards $E_\\pi[\\sum_{t=1}^\\infty \\gamma^t r(s_t, a_t)]$. In this work, we use RGB image-based observations. The reward function is typically hand-engineered or learned, for instance via examples of task success and failure [17, 13, 14, 47, 41, 51]. We assume the existence of a sparse task completion reward (that is, $r(s,a) \\in \\{0,1\\}$), which can be acquired with systems like RoboFuME [51].\nB. Reward Shaping via VLM-Generated Keypoints\nA sparse reward is typically easier to specify but, with it, RL algorithms typically require more samples to learn a successful policy, because it requires the agent to encounter success through its own exploration. In comparison, a dense reward provides a continuous form of feedback that guides the agent towards success. Our method aims to provide the latter type of feedback by augmenting sparse task completion rewards with a dense shaping reward term. Specifically, this dense reward is calculated with respect to a sequence of intermediate waypoints marking trajectory points towards the goal. This can be seen as breaking down a trajectory into short sub-trajectories or subtasks that are more easily reachable by the agent. Such guidance can facilitate learning of more complex and longer-horizon manipulation tasks compared to the sparse reward signal alone.\nAt a high-level, to define dense rewards, we require waypoints that form a coarse trajectory of how the robot should complete the task. We leverage GPT-4V to generate these predictions through recently proposed visual prompting techniques [28]. In addition to waypoints, we also prompt GPT-4V to select appropriate grasp and target points for the manipulation task, which we find especially important for success in our experiments. After these points are generated, we assign rewards to each timestep of an RL episode based on how well it follows this trajectory. We provide specific implementation details of this method below:"}, {"title": "IV. EXPERIMENTS", "content": "We design our experiments to answer the following question: do our dense rewards make online reinforcement learning more efficient? As initial steps, we reproduce the results from the original RoboFuME work [51] in Appendix A and verify that dense rewards are helpful in simulation in Appendix \u0421. We further test whether dense reward shaping can reduce the system's dependence on in-domain demonstrations. These experiments demonstrate that by adding dense shaping rewards,"}, {"title": "V. CONCLUSION & FUTURE WORK", "content": "Our experiments present several insights into the opportunities and challenges of autonomous RL pipelines, and the potential for mark-based visual prompting to improve generalization capabilities of robotic agents equipped with RL. In particular, we explore the challenges of using only sparse rewards for online finetuning and relying on in-domain demonstrations with low multimodality to successfully pretrain policies of online RL. Approaches using only sparse rewards are slower to learn to complete tasks, and approaches reliant on in-domain demonstrations are much more brittle and less robust to generalizing to new tasks, objects, and environments.\nWe demonstrate that dense shaping rewards extracted from VLMs can help to speed up online RL, and facilitate generalization to new tasks where only relying on sparse rewards may not do as well. Leveraging both dense and sparse rewards facilitated improved learning, with better robustness to reduced in-domain demonstrations than just sparse rewards. Our reward formulation could be a possible modification to make existing finetuning methods more robust to changes in tasks, objects, and environments. We have demonstrated the benefits of dense shaping rewards extracted from VLMs, and open up new avenues of exploration to leverage the generalization capabilities of VLMs to enhance the robustness of robot learning systems.\nThere are several areas for future work. First, adding a side camera view for depth information will be crucial for more complex tasks. Dense rewards computed using 3D waypoints instead of 2D may better facilitate complex manipulation tasks but adds complexity to the system, so dense reward calculation must be tuned to account for this. Next, sparse task completion rewards can be seen as high-level tracking of object position (i.e., whether the object moved to the target location or not); implementing waypoint trajectories for object tracking and incorporating this into dense reward computation could further shape robot behavior and expedite learning. In addition, running large models like GPT-4V after every episode to generate dense waypoint trajectories and sparse rewards might be more accurate, but may also suffer from latency issues, and future work can explore this tradeoff to speed up the online finetuning process and facilitate scalability of the method to more complex tasks and environments. Further hyperparameter tuning in the dense reward computation algorithm (namely scaling factor $\\lambda$ and offset $\\phi$ in the reward formulation above) as well as in the CalQL algorithm could further improve the success rate of policies finetuned with dense and sparse rewards. Overall, these future research directions can help efficiently scale autonomous RL systems to a greater diversity of complex tasks with different objects and environments."}, {"title": "APPENDIX", "content": "A. Experiments to Verify Reproduction of RoboFuME\nDemonstrating success rates on the cloth tasks comparable to the RoboFuME results was a positive indicator for successful reproduction of the online RL pipeline. However, based on the results in Table IV, failure to achieve similar results on the Spatula Pick-Place pretrained offline RL policy on Bridge data and high-quality in-domain demonstrations, regardless of the Bridge data subset used, suggests that generalizing the online RL pipeline to new tasks is challenging.\nOur ablation experiments in Table III demonstrated that without in-domain data, all policy variants struggled to get any meaningful learning signal on every task. Furthermore, a preliminary experiment finetuning the offline RL policy pretrained only on Bridge data demonstrated challenges learning with sparse rewards achieves barely any increase in success rates (column 6 of Table I). Therefore, we hypothesized that with dense shaping rewards, policies pretrained on both dense and sparse rewards would struggle to learn during online finetuning, and we were unlikely to achieve meaningful success rates on Spatula Pick-Place without in-domain demonstrations. While RoboFuME has demonstrated interesting results in reducing human effort in reward specification and resets, there is a notable cost incurred with collecting in-domain demonstrations for each new task. In-domain demonstrations are crucial for policies pretrained offline to succeed in task transfer to new environments, as seen in the following experiments. In-domain demonstrations are also crucial for finetuning MiniGPT-4 to yield accurate sparse task completion rewards.\nTo avoid confounding generalization issues and to verify successful reproduction of the pipeline, we picked two tasks in the RoboFuME task suite that performed well, Cloth Folding (Figure 4) and Cube Covering (Figure 5). On both tasks, language-conditioned policies pretrained with behavior cloning (BC) and offline RL had decent success rates, which improved with online finetuning. To test the necessity of in-domain demonstrations for the success of the pipeline, for each task we pretrained four policies: language-conditioned BC on Bridge data and in-domain demonstrations, offline on Bridge data and in-domain demonstrations, language-conditioned BC on Bridge data only, and offline RL on Bridge data. We used the same Bridge data subsets as RoboFuME for each task, with newly collected in-domain demonstrations using our setup. We evaluated the four policies on the forward and backward tasks for each task category, and the results are shown in Table III.\nSimilar to Yang et al. [51], we report the success rates for the forward tasks only. We successfully reproduced the results of RoboFuME for the BC and RL policies trained on both Bridge and in-domain data (see the first two columns of Table I in [51]). However, removing in-domain demonstration data from the pretraining dataset was catastrophic for policy learning, resulting in zero successes for both task categories. This confirms the heavy reliance of the RoboFuME pipeline on in-domain demonstrations.\nIn-domain demonstrations are crucial for both aspects of the RoboFuME pipeline: pretraining policies with language-conditioned BC or offline RL as well as finetuning the task classifier that yields sparse rewards. It is not scalable to collect in-domain demonstrations for every new task we care about. Furthermore, the pipeline required in-domain demonstrations to meet several constraints, such as minimizing multimodality in the demonstrations, and if at evaluation or during finetuning there are any differences in the environment (e.g., lighting changes, changes in object position, changes in background), the system is very likely to fail. Overall, these experiments demonstrate reliance on in-domain demonstrations for transfer to new environments makes the system highly brittle, evidenced by the ablation experiments conducted that pretrained policies only on Bridge data without the in-domain demos resulting in zero success on all tasks. Therefore, while RoboFuME reduces human effort in reward specification and resets, collecting demonstrations is still a bottleneck for this method, both in human effort and in the fragility of the system.\nThe experiments also demonstrate the challenges of relying on high-quality in-domain demonstrations with low multimodality to successfully pretrain policies for online RL. Such pipelines are made much more brittle and less robust to generalizing to new tasks, objects, and environments. It is possible that a combination of selecting a highly task-relevant subset of the Bridge dataset facilitating good transfer to the current task and improved hyperparameter tuning for CalQL could have enabled the system to learn via online finetuning with dense rewards without needing any in-domain demonstrations at all, but this would require much more extensive further exploration in this direction in both pretraining data selection and hyperparameter search for online RL. A broader goal of subsequent research in this area would be to develop online RL methods that are much less reliant on or completely eliminate the need for in-domain demonstrations, and able to more effectively extract useful priors from offline datasets like Bridge datasets during pretraining, in addition to improving finetuning methods.\nB. Selecting Pretraining Data for Spatula Pick-Place\nPretraining on the entire Bridge dataset would be computationally and practically infeasible. As such, choosing subsets of prior datasets to train on is crucial for downstream performance. We test this with a novel Spatula Pick-Place task that is not part of the list of RoboFuME tasks. We define the forward task to be put spatula on plate and the backward task to be move spatula to the left of the plate, formatting language descriptions similar to those in the Bridge dataset, demonstrated in the image"}]}