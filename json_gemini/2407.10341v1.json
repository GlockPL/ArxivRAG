{"title": "Affordance-Guided Reinforcement Learning via Visual Prompting", "authors": ["Olivia Y. Lee", "Annie Xie", "Kuan Fang", "Karl Pertsch", "Chelsea Finn"], "abstract": "Robots equipped with reinforcement learning (RL) have the potential to learn a wide range of skills solely from a reward signal. However, obtaining a robust and dense reward signal for general manipulation tasks remains a challenge. Existing learning-based approaches require significant data, such as demonstrations or examples of success and failure, to learn task-specific reward functions. Recently, there is also a growing adoption of large multi-modal foundation models for robotics. These models can perform visual reasoning in physical contexts and generate coarse robot motions for various manipulation tasks. Motivated by this range of capability, in this work, we propose and study rewards shaped by vision-language models (VLMs). State-of-the-art VLMs have demonstrated an impressive ability to reason about affordances through keypoints in zero-shot, and we leverage this to define dense rewards for robotic learning. On a real-world manipulation task specified by natural language description, we find that these rewards improve the sample efficiency of autonomous RL and enable successful completion of the task in 20K online finetuning steps. Additionally, we demonstrate the robustness of the approach to reductions in the number of in-domain demonstrations used for pretraining, reaching comparable performance in 35K online finetuning steps.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances in large language models (LLMs) and vision-language models (VLMs) trained on Internet-scale data show promising results in using commonsense understanding to plan and reason [37, 8, 1, 38, 25]. They can break down complex instructions provided in natural language into actionable task plans [9, 5, 2, 6, 18, 19], perform visual reasoning in a variety of contexts [50, 7, 36], and even generate coarse robot motions for simple manipulation tasks [5, 34, 46, 18, 20, 36]. However, current state-of-the-art models still struggle with understanding interactions and physical dynamics in 3D space, which is essential to robotic control. Determining how to ground such models in the specific embodiment and environment dynamics remains a significant challenge. Several prior works have utilized large pretrained models for robotic control, either through few-shot prompting or finetuning of large models to generate actions directly [5], plans [1, 19, 18], or code [26, 27]. However, finetuning these models typically requires extensive human supervision, such as teleoperated demonstrations, feedback on whether the task was successfully completed, or a predefined set of skills and their controllers.\nAn alternative paradigm for finetuning robotic policies is autonomous reinforcement learning (RL), which only requires a reward signal to refine the robot's behavior and can therefore require less supervision in comparison. A significant amount of recent work has also focused on improving the sample-efficiency of these algorithms by pretraining on large offline datasets [23, 3, 22, 24, 32, 51]. Despite these advances, obtaining a reward signal is still a non-trivial problem, that requires either careful engineering or large amounts of data to learn a robust reward function [17, 13, 14, 47, 41]. The application of pretrained VLMs for defining rewards is therefore attractive, but thus far, they have primarily been used for generating sparse rewards [31, 44, 51], which often leads to less efficient learning. VLMs hold much richer and denser knowledge that we can elicit, such as reasoning about the affordances of various objects and environments. In this work, we leverage this understanding to shape rewards for robotic RL.\nSpecifically, we present a method for open-vocabulary visual prompting to extract rewards from VLMs for online RL. We leverage insights on effective visual prompting methods from Liu et al. [28] to develop a method for generating keypoints and waypoint trajectories from which dense shaping rewards can be calculated. We integrate the pipeline of extracting affordance representations from VLMs and computing dense rewards into RoboFuME [51], an autonomous RL system that uses sparse rewards from a finetuned VLM. We demonstrate comparable success rates on a variety of complex object manipulation tasks used in RoboFuME, as well as improved success rates on new tasks that the existing pipeline struggles to generalize to using sparse rewards alone, with reduced reliance on in-domain expert demonstrations."}, {"title": "II. RELATED WORK", "content": "The rapid development of foundation models in recent years has drawn significant attention both in the academic community and beyond [12]. This surge of interest in foundation models has arisen because they demonstrate that models trained on broad, Internet-scale data are highly adaptable to a wide range of downstream tasks. Robotics is a specific downstream task of foundation models that has garnered a lot of interest in the academic community. Works like SayCan [1] demonstrate encouraging results in training language-conditioned robotic control policies. Such approaches allow us to leverage and ground the rich knowledge and reasoning capabilities of LLMs to enable embodied agents to complete long-horizon tasks.\nWhile the results from SayCan [1] seem promising, there is a critical engineering risk that the reasoning abilities and representations captured by LLMs are overly general for embodied tasks, and more work is needed to properly ground the high-level plans generated by LLMs in low-level actions for embodied tasks. Converting visual observations into language descriptions and planning in solely in the language space loses a lot of rich information critical to scene understanding, which is a major limitation of using LLMs for spatial planning, reasoning, and task completion. Notably, the ELLM system from Du et al. [10] generated inaccurate responses to whether objects matched the goal positions when tasked with rearranging objects in a household environment to match the goal arrangement. It is important to pay attention to the pitfalls in the household environment despite the successes in the open-world Crafter environment. The goals for survival in an open-world environment (such as build house, or acquire food) are fairly general and transferable, and LLMs have likely encountered such scenarios during training and can suggest reasonable goals. However, the general knowledge encoded in LLMs may not necessarily as beneficial for robot learning: LLMs can provide general priors for planning and reasoning, but this generality also results in reduced specificity to the environment that the robot is operating in, thus the general priors may require additional grounding.\nTherefore, planning solely with language thus loses a lot of information associated with the richness of the visual modality that is critical to most robotics applications. Our work explores leveraging the visual modality via state-of-the-art VLMs to facilitate reasoning in both language and image domains."}, {"title": "B. Vision-Language Models", "content": "The clear advantage of language is the natural interface for providing task instructions and describing goals. That said, much of robotics research relies heavily on accurately perceiving and interacting with the environment. Several state-of-the-art VLMs [33, 49, 29, 39] demonstrate highly generalizable open-vocabulary object localization. However, they still lack the extensive reasoning capabilities of LLMs, since reasoning over image inputs is significantly more complicated. Preliminary VLMs require text queries where the objects involved are known a priori to generate bounding boxes around the requested objects, which are then provided to LLMs for downstream reasoning. Modern VLMs, such as GPT-4V [52] and Gemini [15], have demonstrated promising capabilities in combining LLM reasoning with environment perception via visual inputs. A key advantage of using modern VLMs is that it simplifies the process of translating high-level plans into low-level robot actions. While previous works leveraging LLMs like SayCan [1] require pretrained skill policies for each action primitive, using modern VLMs can circumvent the issue of selecting from a suite of pretrained action policies, by deriving rewards from image space that can be used for learning state-action mappings via RL. This is because VLMs, unlike LLMs, can determine success or failure based on image observations, and this reward signal can be used to enable robots to learn through trial-and-error, without training skill policies via imitation learning which are costly and difficult to scale. VLMs can also guide the learning process by generating shaping rewards, in the form of intermediate waypoints.\nDefining rewards in image space by using VLMs to determine task completion and specify intermediate waypoints as goals is a key contribution of our work. A major engineering challenge is tuning the inputs to these VLMs, which are highly expressive but also opaque, to derive useful reward signals for learning. Both language and image prompts require careful tuning to generate accurate and meaningful outputs, as we have found that modern VLMs still struggle to some degree with spatial reasoning. Our work explores combining preliminary VLMs and modern VLMs, as the outputs (bounding boxes or segmentations) of preliminary VLMs can serve as more helpful inputs to modern VLMs for semantic reasoning than raw image observations, thereby leveraging pretrained representations in VLMs as reward predictors."}, {"title": "C. Autonomous Reinforcement Learning", "content": "Online RL is the paradigm by which agents gathers data through interaction with the environment, then stores this experience in a replay buffer and updates its policy based on this experience. This contrasts with offline RL, where the agent updates its policy using previously collected data or human demonstrations, without itself interacting with the environment. A longstanding goal is autonomous RL: the potential of placing a robot in a real-world environment and it improves on its own by autonomously gathering in-domain experience, which holds great promise for scalable robot learning. In autonomous RL, the agent not only learns through its own experience, but also does not require human supervision to reset the environment between trials [40].\nAlgorithms for autonomous RL have been difficult to implement in the real world, with the primary challenge being sample complexity, the number of calls to the model required to achieve acceptably good performance. In addition, there is the challenge of providing well-shaped rewards for online exploration, as well as the difficulty of continual reset-free training, which requires significant human effort. Several works have developed systems for reset-free training to reduce or eliminate human interventions in the online RL process [4, 51, 16, 42], but reward engineering is an open problem as manually specified reward signals are seen as difficult to engineer and easy to exploit, and autonomous RL suffers when the reward signal is too sparse. While hand-designing reward functions is challenging, there is great potential to learn reward functions from previously collected data or extract rewards from large pretrained models. Some works have attempted to learn rewards from human feedback [43, 4], while acknowledging that these rewards are noisy and still require human intervention. The large bank of offline image and video datasets, as well as the high inference speed and accessibility of large pretrained models, could potentially offer solutions to further reduce or eliminate human intervention, while providing more precise and informative shaping rewards.\nThere has been attempts to leverage VLMs to generate rewards for online RL. RoboFuME [51] finetunes MiniGPT-"}, {"title": "III. METHODOLOGY", "content": "We consider problems that can be formulated as a partially observable Markov Decision Process (POMDP) tuple (S, A, O, \u03b3, f, p, r, do) where S is the state space, A is the action space, O is the observation space, \\( \\gamma \\) \u2208 (0,1) is the discount factor, r(s, a) is the reward function and do(s) is the initial state distribution do(s). The dynamics are governed by a transition function p(s'|s,a). The observations are generated by an observation function f(os). The goal of RL is to maximize the expected sum of discounted rewards \\( \\mathbb{E}_{\\pi}[\\sum_{t=1}^{\\infty} \\gamma^t r(s_t, a_t)] \\). In this work, we use RGB image-based observations. The reward function is typically hand-engineered or learned, for instance via examples of task success and failure [17, 13, 14, 47, 41, 51]. We assume the existence of a sparse task completion reward (that is, r(s, a) \u2208 {0,1}), which can be acquired with systems like RoboFuME [51]."}, {"title": "B. Reward Shaping via VLM-Generated Keypoints", "content": "A sparse reward is typically easier to specify but, with it, RL algorithms typically require more samples to learn a successful policy, because it requires the agent to encounter success through its own exploration. In comparison, a dense reward provides a continuous form of feedback that guides the agent towards success. Our method aims to provide the latter type of feedback by augmenting sparse task completion rewards with a dense shaping reward term. Specifically, this dense reward is calculated with respect to a sequence of intermediate waypoints marking trajectory points towards the goal. This can be seen as breaking down a trajectory into short sub-trajectories or subtasks that are more easily reachable by the agent. Such guidance can facilitate learning of more complex and longer-horizon manipulation tasks compared to the sparse reward signal alone.\nAt a high-level, to define dense rewards, we require waypoints that form a coarse trajectory of how the robot should complete the task. We leverage GPT-4V to generate these predictions through recently proposed visual prompting techniques [28]. In addition to waypoints, we also prompt GPT-4V to select appropriate grasp and target points for the manipulation task, which we find especially important for success in our experiments. After these points are generated, we assign rewards to each timestep of an RL episode based on how well it follows this trajectory. We provide specific implementation details of this method below:"}, {"title": "IV. EXPERIMENTS", "content": "We design our experiments to answer the following question: do our dense rewards make online reinforcement learning more efficient? As initial steps, we reproduce the results from the original RoboFuME work [51] in Appendix A and verify that dense rewards are helpful in simulation in Appendix \u0421. We further test whether dense reward shaping can reduce the system's dependence on in-domain demonstrations. These experiments demonstrate that by adding dense shaping rewards,"}, {"title": "A. Experimental Setup", "content": "We used the following tasks for our experiments: Cloth Folding, Cube Covering, and Spatula Pick-Place. The former two tasks were introduced in RoboFuME, and we use them here to demonstrate successful reproduction of RoboFuME as a benchmark and for ablation tests to demonstrate the necessity of in-domain demonstrations for the pipeline (Appendix A). For Spatula Pick-Place, we demonstrate the challenges of reproducing RoboFuME on novel tasks and the use of dense shaping rewards to overcome this generalization problem and reduce reliance on in-domain demonstrations.\nIn RoboFuME, only subsets of the Bridge dataset were used to pretrain policies with language-conditioned BC and offline RL. For the two tasks also used in RoboFuME, we used the same Bridge data subsets, mostly featuring cloth-related tasks. For the novel Spatula Pick-Place task, we explore the selection of these subsets of Bridge data in Appendix B. To ensure effective transfer learning from Bridge data, the camera angle and setup for the in-domain demonstrations were made highly similar to the camera angles used in the Bridge data.\nVisualizations of the in-domain demonstrations collected for the forward and backward task of each task category are included for Cloth Folding (Figure 4), Cube Covering (Figure 5), and Spatula Pick-Place (Figure 6). Policies were pretrained using language-conditioned BC and offline RL on the standard quantity of in-domain demonstrations (50 forward task trajectories, 50 backward task trajectories, and 20 mixed-mode failures). As done in RoboFuME, the forward and backward trajectories were collected with minimal multimodality and randomization, to facilitate reset-free RL. After pretraining on Bridge and in-domain data using offline RL, policies are finetuned online using CalQL [35] for 20K steps. While RoboFuME claimed to enable minimal resets (every 10-15 episodes), since reset-free learning was not an emphasis of our method, we reset the model every 2 episodes to maximize useful interactions and speed up the learning process.\nFor the experiments in Section IV-B, the model was pretrained on the standard quantity of in-domain demonstrations. The pretrained models (with language-conditioned BC and offline RL) as well as the offline RL models finetuned online for 20K steps were evaluated on success out of 20 trials each of the forward and backward tasks in each task category. For the experiments in Section IV-C, the model was pretrained on a set of in-domain demonstrations reduced by 5x from the standard quantity, and finetuned for 20K steps and further until 35K steps to understand the system's robustness to reduced in-domain demonstrations.\nSuccess was evaluated qualitatively by similarity to the in-domain demonstrations collected: for Cloth Folding, the cloth had to be folded or unfolded to a degree similar to the expert demonstrations; for Cube Covering, the entire cube had to be covered or uncovered from the camera perspective shown in Figure 5; for Spatula Pick-Place, the spatula had to be on the yellow plate (forward task) or on the left side of the plate close to where it was picked originally (backward task).\nThe full approach for dense reward shaping is detailed in Section III-B. For implementation on the real robot, we use a modified version of the approach using 2D waypoint guidance from a top-down camera, and track only the robot gripper's position by RANSAC prediction using robot proprioception. Because the environment is manually reset after every two episodes (one forward and one backward episode), we query GPT-4V once at the beginning of the experiment to generate the intermediate waypoints for the forward and backward tasks, and use those waypoints for all forward and backward episodes. We finetune MiniGPT-4 to generate sparse rewards, as done in RoboFuME [51], and finetune different versions depending on the number of in-domain demonstrations used during pretraining, to investigate the robustness of the method to different quantities of in-domain demonstrations.\nFrom the GPT-4V generated waypoint trajectory, we use the centroid of each grid tile to create a trajectory of pixel"}, {"title": "B. Finetuning with Dense Rewards", "content": "We pretrain policies for Spatula Pick-Place on a curated subset of the Bridge dataset (see Appendix B). The offline RL policy pretrained on Bridge and high-quality in-domain demonstration data achieves 25% success rate. For our Robo-FuME comparison, we finetune this policy online for 20K steps using only VLM-generated sparse rewards, with resets every 2 episodes to maximize meaningful online interactions.\nFor our method, we finetune this policy online for 20K steps using both VLM-generated sparse rewards and dense rewards calculated with respect to a VLM-generated waypoint trajectory, also with resets every 2 episodes.\nThe results are shown in Table I. After finetuning with just VLM-generated sparse rewards, the performance of the offline RL policy pretrained on Bridge and in-domain data increases from 25% to 40%. While this is lower than the success rate of cloth tasks in Appendix A, the increase in success rate with finetuning using sparse rewards is comparable to a Pot Pick-Place task from Yang et al. [51], and both Pick-Place tasks are considerably harder than the cloth tasks. After finetuning with VLM-generated sparse rewards and dense shaping rewards, the performance of the offline RL policy pretrained on Bridge and in-domain data further increases to 45%. This is comparable to the success rate of RoboFuME in the Pot Pick-Place task after finetuning with 30K steps using only sparse rewards.\nWhile the increase in success rate finetuning with dense and sparse rewards vs. just sparse rewards is not extremely significant, the qualitative behavior observed was better, with representative image sequences demonstrating policy performance on the real robot shown in Figure 7. The policy finetuned with sparse rewards, while fairly successful, commonly demonstrated the behavior shown in Figure 7a where the spatula was dropped onto the plate from a high height, rather than lowering and placing the spatula as done in the demonstrations. The robot gripper also continues moving to the right rather than hovering above the placement point, indicating several coincidental successes. On the other hand, the policy finetuned with dense rewards, while only slightly more performant success rate-wise, demonstrated better qualitative behavior shown in Figure 7b, lowering the spatula onto the plate and hovering above the placement point, as done in the demonstrations. This is likely due to dense rewards shaping the behavior to be more like the GPT-4V generated trajectory."}, {"title": "C. Robustness to Reducing In-Domain Demonstrations", "content": "In Table I, the policy trained via offline RL on Bridge data only, without any in-domain demonstrations, struggles to make meaningful progress towards task completion. This suggests that RoboFuME is highly reliant on in-domain demonstrations, however this introduces a notable cost of collecting in-domain demonstrations for each new task. We investigate the effect of reducing the number of in-domain demonstrations used during pretraining on the policy's ability to successfully learn during online finetuning. We then evaluate whether our dense rewards can reduce the system's reliance on in-domain demonstrations, thereby reducing its brittleness.\nWe pretrain a policy via offline RL on 5x less than the standard quantity of in-domain demonstrations (10 forward tasks, 10 backward tasks, and 2 failures). We finetune this policy online for 20K steps using sparse rewards only as well as sparse and dense rewards, then until 35K steps as the sparse reward only policy seemed to be plateauing while the sparse and dense reward policy still showed signs of improvement."}, {"title": "V. CONCLUSION & FUTURE WORK", "content": "Our experiments present several insights into the opportunities and challenges of autonomous RL pipelines, and the potential for mark-based visual prompting to improve generalization capabilities of robotic agents equipped with RL. In particular, we explore the challenges of using only sparse rewards for online finetuning and relying on in-domain demonstrations with low multimodality to successfully pretrain policies of online RL. Approaches using only sparse rewards are slower to learn to complete tasks, and approaches reliant on in-domain demonstrations are much more brittle and less robust to generalizing to new tasks, objects, and environments.\nWe demonstrate that dense shaping rewards extracted from VLMs can help to speed up online RL, and facilitate generalization to new tasks where only relying on sparse rewards may not do as well. Leveraging both dense and sparse rewards facilitated improved learning, with better robustness to reduced in-domain demonstrations than just sparse rewards. Our reward formulation could be a possible modification to make existing finetuning methods more robust to changes in tasks, objects, and environments. We have demonstrated the benefits of dense shaping rewards extracted from VLMs, and open up new avenues of exploration to leverage the generalization capabilities of VLMs to enhance the robustness of robot learning systems.\nThere are several areas for future work. First, adding a side camera view for depth information will be crucial for more complex tasks. Dense rewards computed using 3D waypoints instead of 2D may better facilitate complex manipulation tasks but adds complexity to the system, so dense reward calculation must be tuned to account for this. Next, sparse task completion rewards can be seen as high-level tracking of object position (i.e., whether the object moved to the target location or not); implementing waypoint trajectories for object tracking and incorporating this into dense reward computation could further shape robot behavior and expedite learning. In addition, running large models like GPT-4V after every episode to generate dense waypoint trajectories and sparse rewards might be more accurate, but may also suffer from latency issues, and future work can explore this tradeoff to speed up the online finetuning process and facilitate scalability of the method to more complex tasks and environments. Further hyperparameter tuning in the dense reward computation algorithm (namely scaling factor \\( \\lambda \\) and offset \\( \\psi \\) in the reward formulation above) as well as in the CalQL algorithm could further improve the success rate of policies finetuned with dense and sparse rewards. Overall, these future research directions can help efficiently scale autonomous RL systems to a greater diversity of complex tasks with different objects and environments."}, {"title": "APPENDIX", "content": "Demonstrating success rates on the cloth tasks comparable to the RoboFuME results was a positive indicator for successful reproduction of the online RL pipeline. However, based on the results in Table IV, failure to achieve similar results on the Spatula Pick-Place pretrained offline RL policy on Bridge data and high-quality in-domain demonstrations, regardless of the Bridge data subset used, suggests that generalizing the online RL pipeline to new tasks is challenging.\nOur ablation experiments in Table III demonstrated that without in-domain data, all policy variants struggled to get any meaningful learning signal on every task. Furthermore, a preliminary experiment finetuning the offline RL policy pretrained only on Bridge data demonstrated challenges learning with sparse rewards achieves barely any increase in success rates (column 6 of Table I). Therefore, we hypothesized that with dense shaping rewards, policies pretrained on both dense and sparse rewards would struggle to learn during online fine-tuning, and we were unlikely to achieve meaningful success rates on Spatula Pick-Place without in-domain demonstrations. While RoboFuME has demonstrated interesting results in reducing human effort in reward specification and resets, there is a notable cost incurred with collecting in-domain demonstrations for each new task. In-domain demonstrations are crucial for policies pretrained offline to succeed in task transfer to new environments, as seen in the following experiments. In-domain demonstrations are also crucial for finetuning MiniGPT-4 to yield accurate sparse task completion rewards.\nTo avoid confounding generalization issues and to verify successful reproduction of the pipeline, we picked two tasks in the RoboFuME task suite that performed well, Cloth Folding (Figure 4) and Cube Covering (Figure 5). On both tasks, language-conditioned policies pretrained with behavior cloning (BC) and offline RL had decent success rates, which improved with online finetuning. To test the necessity of in-domain demonstrations for the success of the pipeline, for each task we pretrained four policies: language-conditioned BC on Bridge data and in-domain demonstrations, offline on Bridge data and in-domain demonstrations, language-conditioned BC on Bridge data only, and offline RL on Bridge data. We used the same Bridge data subsets as RoboFuME for each task, with newly collected in-domain demonstrations using our setup. We evaluated the four policies on the forward and backward tasks for each task category, and the results are shown in Table III.\nSimilar to Yang et al. [51], we report the success rates for the forward tasks only. We successfully reproduced the results of RoboFuME for the BC and RL policies trained on both Bridge and in-domain data (see the first two columns of Table I in [51]). However, removing in-domain demonstration data from the pretraining dataset was catastrophic for policy learning, resulting in zero successes for both task categories. This confirms the heavy reliance of the RoboFuME pipeline on in-domain demonstrations."}, {"title": "B. Selecting Pretraining Data for Spatula Pick-Place", "content": "Pretraining on the entire Bridge dataset would be computationally and practically infeasible. As such, choosing subsets of prior datasets to train on is crucial for downstream performance. We test this with a novel Spatula Pick-Place task that is not part of the list of RoboFuME tasks. We define the forward task to be put spatula on plate and the backward task to be move spatula to the left of the plate, formatting language descriptions similar to those in the Bridge dataset, demonstrated in the image"}, {"title": "C. Simulation Experiments for Dense Reward Formulation", "content": "We conduct preliminary experiments in simulation to investigate the effects of finetuning with a dense reward. In simulation, the dense reward is naively the negative L2 distance between the robot and the target location. We further investigate whether using a dense reward formulation can reduce the reliance of the policy on in-domain demonstrations during policy pretraining. The results of the simulation experiments for policies using the standard number of in-domain demonstrations, including the reproduction of the RoboFuME pipeline, can be seen in Figure 8."}, {"title": "D. Qualitative Observations of VLM Performance", "content": "The success of the online RL system is highly dependent on the accuracy of VLM keypoint and waypoint predictions for task completion, as inaccuracies in these predictions lead to suboptimal reward shaping and hinder the learning process, or even cause the learning of wrong behaviors. Below we include a short commentary on empirical observations of VLM performance. The VLMs investigated were GPT-4V (also used by Liu et al. [28]), Gemini Pro, and GPT-4o. We tested the outputs of these models on the three tasks described above: Cloth Folding, Cube Covering, Spatula Pick-Place, as well as some additional tasks from RoboFuME to verify the robustness of the system such as Candy Sweeping, Drawer Opening, and pick and place tasks using other objects in a toy kitchen setup. We provide the task descriptions and metaprompts in E.\nFirst, we observe that the natural language description of the task is important for facilitating the correct waypoint generation. For example, for Spatula Pick-Place, the backward task phrased as \"Place the spatula on the left of the plate\" sometimes led to waypoints generated in a way that moves the spatula to the left side of the yellow plate (but still on the plate), rather than on the table on the left side of the plate. We had to tune the natual language descriptions as such. Tuning typically occurred on the level of the specific task description, and the meta-prompt required some modifications from Liu et al. [28] to generate the intended outputs, but once tuned remained consistent for all experiments. Once tuned, for the three tasks reported above, manual inspection of the VLM outputs indicate that the outputs are generally reliable and consistent across trials for the top-down perspective, the perspective used in our real robot experiments.\nAnother observation is for the approach of providing dual-angle inputs to VLMs, where the side-view of the environment was also provided to the system. Gemini tends to perform slightly better than the GPT models in 3D spatial reasoning and generating depth information that facilitates successful completion of the task. The GPT models, GPT-4V in particular, sometimes struggles to generate sensible depth movements from the side angle to facilitate e.g. pick and place of the spatula. This observation would be important for extensions of this work investigating 3D waypoint generation, as depth information is critical for pick and plac, tasks performed on uneven surfaces, as well as tasks with very specific grasp points like drawer opening.\nOur experiments and empirical observations verify the hypotheses in Liu et al. [28] that state-of-the-art VLMs are capable of spatial reasoning at a sufficient accuracy to facilitate tabletop manipulation tasks of various complexities, where a top-down and side view of the environment gives sufficient information to perform the tasks. We anticipate that the spatial reasoning of VLMs will only continue to improve, and more studies can be conducted in mobile or open-world manipulation, to see whether the spatial reasoning capabilities of VLMs extend to even more complex and dynamic environments beyond tabletop manipulation."}, {"title": "E. GPT-4V Prompts", "content": "1) Task Descriptions: Below are the task descriptions used for testing VLM waypoint generation. Note that for our system, task descriptions must be specific as the forward and backward tasks must be near inverses of each other and move objects to specific positions for reset-free RL to work smoothly. For object manipulation outside the reset-free setting, this level of specificity may not be required, and more \"natural\" language instructions can be provided to VLMs.\nSpatula Pick-Place:\n1) Forward: Place the spatula on the plate.\n2) Backward: Move the spatula to the table on the left of the plate.\nCloth Folding:\n1) Forward: Fold the cloth from left to right.\n2) Backward: Unfold the cloth from right to left.\nCube Covering:\n1) Forward: Cover the box with the cloth from left to right.\n2) Backward: Uncover the box beneath the cloth from right to left.\nCandy Sweeping:\n1) Forward: Sweep the candies from the left side of the tray to the right side.\n2) Backward: Sweep the candies from the right side of the tray to the keft side.\nDrawer Opening/Closing:\n1) Forward: Open the drawer.\n2) Backward: Close the drawer.\nToy Kitchen Pick-Place:\n1) Forward: Place the pot in the center of the sink.\n2) Backward: Place the pot on the dish rack."}]}