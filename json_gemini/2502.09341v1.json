{"title": "Neural Spatiotemporal Point Processes: Trends and Challenges", "authors": ["Sumantrak Mukherjee", "Mouad Elhamdi", "George Mohler", "David A. Selby", "Yao Xie", "Sebastian Vollmer", "Gerrit Gro\u00dfmann"], "abstract": "Spatiotemporal point processes (STPPs) are probabilistic models for events occurring in continuous space and time. Real-world event data often exhibit intricate dependencies and heterogeneous dynamics. By incorporating modern deep learning techniques, STPPs can model these complexities more effectively than traditional approaches. Consequently, the fusion of neural methods with STPPs has become an active and rapidly evolving research area. In this review, we categorize existing approaches, unify key design choices, and explain the challenges of working with this data modality. We further highlight emerging trends and diverse application domains. Finally, we identify open challenges and gaps in the literature.", "sections": [{"title": "1 Introduction", "content": "Real-world events\u2014such as urban crime incidents, epidemic spread, earthquakes, and environmental changes can be represented as sequences of discrete events with both spatial and temporal components. Studying the spatiotemporal distribution of events and discovering the relationships among different types of events is an increasingly important area of research for understanding the dynamics and mechanism of the occurrence of the events. One such paradigm is the spatiotemporal point process (STPP) model, defined as a stochastic process that describes the spatial and temporal distribution of discrete events [Daley and Vere-Jones, 2007], which is well suited to capture the complex relationships between events, including self-excitation, and the interactions between events and spatial covariates across time and space.\nReviews on neural point processes have focused on modeling the temporal dynamics of events using neural networks (NNs) [Shchur et al., 2021; Lin et al., 2021, 2022]. In contrast, reviews that address spatiotemporal event modeling have focused on traditional statistical methods [Gonz\u00e1lez et al., 2016; Reinhart, 2018]. More recently, some reviews have explored specific aspects of the use of machine learning in STPPs [Wikle and Zammit-Mangion, 2023; Bernabeu et al., 2024], these typically focus on specific aspects rather than providing comprehensive coverage. Meanwhile, work on neural STPPs has achieved remarkable progress.\nOur work bridges this gap by systematically exploring design choices, methodological innovations, and key challenges in neural STPPs. To our knowledge, no prior survey has comprehensively examined these aspects in this context.\nWhy we need neural STPPs. Structural differences between space and time make modeling challenging. Time is unidirectional, while spatial propagation is omnidirectional and is affected by environmental factors. Traditional methods rely on strong parametric assumptions and independence, limiting flexibility. They struggle with long-range dependencies, fail to capture heterogeneous dynamics across space and time, and cannot integrate multimodal data. Additionally, single-step predictions cause error accumulation. NN-based methods overcome these limitations by encoding space and time efficiently, handling dependencies, and learning heterogeneous patterns. They automate feature extraction, integrate diverse data, and scale effectively.\nScope and structure of the paper. This survey reviews neural STPPs, covering core models, applications, and key components in event modeling. We focus on studies using point processes with neural parameterization to capture spatiotemporal dynamics. Our literature search included keyword-based queries, citation tracking, and seminal works in neural temporal point processes. By outlining fundamental principles and design choices, we provide a practical foundation for researchers. We first introduce necessary background and notation, then present available modeling choices, including architectures, training procedures, and metrics. The next section highlights notable applications of neural STPPs, and we conclude by discussing open challenges."}, {"title": "2 Background and Notation", "content": "Spatiotemporal point processes. STPPs are concerned with modeling sequences of random events in continuous space and time [Moller and Waagepetersen, 2003]. A realization or sample of an STPP is defined up to a time horizon $T \\in \\mathbb{R}^{\\ge 0}$. It is a finite, ordered event sequence containing pairs $X = [(t_1, s_1), (t_2, s_2), ..., (t_n, s_n)]$, where $t_i \\in [0,T]$ denotes the time and $s_i \\in S$ the location of event $i$. Here, $S \\subset \\mathbb{R}^d$ represents the spatial domain, which is typically a bounded region in $d$-dimensional Euclidean space and typically $d = 2$. For a given event sequence, $H_t = \\{(t_i, s_i) \\mid t_i < t\\}$ denotes the history of the current realization up to (but excluding) time point $t$ (c.f. Figure 1).\nLikelihood. An STPP can be specified by defining the likelihood of event sequences. The framework follows the temporal priority principle [Berzuini et al., 2012], which asserts that all causes must precede their effects. As a result, the likelihood of an event sequence $X$ can be expressed in an auto-regressive form [Rasmussen, 2018]:\n$f(X) = \\left(\\prod\\limits_{i=1}^{n} f_{pred}(t_i, s_i \\mid H_{t_i})\\right) (1 - F_{pred}(T \\mid H_{t_n})),$ \nwhere $f_{pred}(t, s \\mid H_t)$ is the predictive distribution, specifying the conditional probability density function (PDF) for the next event occurring at a given timestamp $t$ and location $s$, given the history of past events $H_t$. The term $F_{pred}(T \\mid H_{t_n})$ represents the cumulative distribution function (CDF) of the predictive distribution, which gives the probability that an event occurs before or at time $T$, regardless of $s$. Thus, $1 - F_{pred}(T \\mid H_{t_n})$ is the probability of no events occurring after the last observed event. Conveniently, this formulation also provides a principled approach for simulation (i.e., the generation of samples from the underlying stochastic model).\nIntensity function. In practice, an STPP is often described using a (conditional) intensity function (CIF) instead of a predictive distribution; they are mutually translatable [Chen et al., 2021]. The CIF $\\lambda(t, s \\mid H_t)$, denoted by $\\lambda^*(t,s)$ as a shorthand for its dependence on the $H_t$, is defined as:\n$\\lambda^{*}(t, s) = \\lim_{\\Delta s, \\Delta t \\downarrow 0} \\frac{P(\\text{Event} \\in (B(s, \\Delta s) \\times [t, t + \\Delta t)) \\mid H_t)}{|B(s, \\Delta s)| \\Delta t}$ (1)\nwhere $B(s, \\Delta s)$ is a $d$-dimensional ball (a disk if $d = 2$) centered at $s \\in S$ with radius $\\Delta s$, and $|B(s, \\Delta s)|$ denotes its volume.\nEvents can include additional information beyond location as a mark [Daley and Vere-Jones, 2007]. While the core concept remains unchanged, this approach incorporates multimodal contextual data"}, {"title": "3 Modeling Neural Spatiotemporal Point Processes", "content": "Neural STPPs model event evolution and probabilities using NNs, implicitly encoding a PDF over sequences. The chosen approach affects training efficiency, likelihood evaluation, and event generation. A common method constructs latent representations from an event history to generate new events. Adressing the challenges of spatial encoding, we first review methods designed to address these challenges, then discuss multi-event generation. We do not focus on neural methods for temporal encoding as they are well-studied Shchur et al. [2021].\nModeling choices depend on application needs. Crime models may focus on landmark influence, while earthquake prediction prioritizes accuracy. Whereas, epidemic models may emphasize county-level policy, reducing spatial granularity. Some applications require parametric assumptions for hypothesis testing, while safety-critical settings demand uncertainty quantification. These considerations shape spatial encoding strategies. While discretization simplifies modeling, it reduces the ability to encode an inductive bias where nearby events exert stronger influence. Its performance is also sensitive to granularity, boundary effects, and local correlations. Graph-based methods like GNNs mitigate these issues by modeling inter-cell influences.\nFor continuous-space methods, using raw coordinates (e.g., latitude-longitude) ignores anisotropic propagation. Solutions include location-specific parametric kernels, contextual data (e.g., satellite images, geotagged text), and learned spatial embeddings. Attention models implicitly capture heterogeneous effects, while GNNs and heterogeneous kernels explicitly model spatial interactions."}, {"title": "3.1 History Event Encoder", "content": "Spatiotemporal event modeling requires a CIF that combines temporal evolution and spatial dependencies (c.f. Figure 1). A common approach to predicting an event $(t_i, s_i)$ is to first encode each historical event $(t_j, s_j) \\in H_{t_i}$ as an embedding $e_j = [w(t_j); \\sigma(s_j)]$. Some architectures directly use raw time $t_j$ and space $s_j$, while others transform them via $w(\\cdot)$ (e.g., linear, trigonometric, or logarithmic mappings) and $\\sigma(\\cdot)$ (e.g., linear layers or one-hot encodings). A history encoder then processes embeddings into a latent state $h_i$ to parameterize the CIF.\nRNN variants like GRU and LSTM are common history encoders, updating states as $h_{i+1} = \\text{RNN}(e_i, h_i)$ [Du et al., 2016; Omi et al., 2019; Shchur et al., 2020]. Their sequential nature reduces storage needs but limits parallelization. Additionally, they suffer from gradient vanishing and long-term memory loss [Le and Zuidema, 2016].\nAttention mechanisms [Vaswani, 2017] overcome several limitations of recurrent encoders. They have demonstrated superior performance as history encoders for temporal point processes [Zhang et al., 2020; Zuo et al., 2020] and lately in STPPs [Zhou et al., 2022; Yuan et al., 2023]. Nevertheless, the $O(N^2)$ space complexity required to construct the attention matrix can pose practical challenges."}, {"title": "3.2 Single Event Prediction", "content": "Neural STPPs predict the time and location of the next event. We review parametric, neural, mixture, diffusion, and continuous-time models, emphasizing those that capture spatial heterogeneity, contextual influences, and graph-based interactions.\nKernel-based methods. Kernel-based methods model spatiotemporal dependencies by parameterizing event influence through kernels. When combined with NNs, these kernels become more flexible while maintaining interpretability. The intensity function is typically defined as follows:\n$\\lambda^{*}(t, s) = \\mu(t, s) + \\sum_{(t', s') \\in H_t} K(t', t, s', s),$ (2)\nwhere $\\mu(t, s)$ is the baseline rate (which can potentially vary over time and space), and $K(\\cdot)$ models past event influence. Traditional models use stationary kernels, assuming time and space invariant relationships (e.g., ETAS [Musmeci and Vere-Jones, 1992], [Hawkes, 1971]), making strong parametric assumptions and missing heterogeneous effects. More expressive approaches learn spatial and temporal dependencies jointly or independently, improving adaptability.\nNon-stationary kernels. Designing the influence kernel $K(\\cdot)$ in Equation (2) is crucial for capturing how past events trigger future occurrences. Neural parameterizations of $K(\\cdot)$ enable non-stationary dependencies that vary across space, time, and contextual marks, moving beyond fixed parametric forms.\nA representative approach is the Gaussian mixture model of Zhu et al. [2021b], where $K(\\cdot)$ is expressed as:\n$K(t', t, s', s) = \\sum_{l=1}^{L} \\phi^{(l)} g(t, t', s, s' \\mid \\Theta^{(l)}, \\mu^{(l)}, \\Sigma^{(l)}),$ (3)\nA neural network embeds spatial coordinates $s$ to generate location-specific parameters $\\mu^{(l)}, \\Sigma^{(l)}$, and mixture weights $\\Theta^{(l)}$, where $l \\in \\{1, ..., L\\}$ with $L$ denoting number of mixture components. Applying constraints ensures physical interpretability and reflects heterogeneous event diffusion. Visualizing learned kernels reveals region-specific influence propagation, making $K(\\cdot)$ a smooth, adaptive function.\nAlternate formulations consider heterogeneous interactions between events modeled using Mercer's theorem and neural basis functions[Zhu et al., 2022], leverage low-rank decomposition and a deep non-stationary influence kernel [Dong et al., 2022], or embed events in graphs for non-Euclidean interactions [Dong et al., 2023a]. These can be generalized as:\n$K(t', t, s', s) = \\sum_{r=1}^{R} \\sum_{l=1}^{L} \\alpha_{rl} \\psi_l(t', t) \\varphi_r(s', s),$ (4)\nwhere $\\psi_l(\\cdot)$ and $\\varphi_r(\\cdot)$ are neural feature maps modeling time displacement, spatial relations, or graph connectivity, with $\\alpha_{rl}$ scaling their contributions. $L$ and $R$ refer to the number of basis kernels used to decompose the kernel in Equation (2). For instance, Dong et al. [2023b] and Dong and Xie [2024] use learned Gaussian bases for anisotropic geography, while Dong et al. [2024] combine a stationary Gaussian kernel with a Graph Neural Network (GNN) mark kernel to capture network constraints and landmark effects.\nContext. Event dynamics are influenced by contextual covariates like georeferenced images and text. Okawa et al. [2019] define event intensity as a spatially localized mixture of kernels:\n$\\lambda^{*}(t, s \\mid D) = \\sum_{j=1}^{M} f(u_j, z_j; \\theta) K(t, s, u_j),$ (5)\nwhere $K(\\cdot)$ is a compactly supported Gaussian kernel, $D$ is a set of contextual features, and $u_j$ are spatiotemporal anchors uniformly distributed in time and space. Contextual features $z_j$, extracted from $u_j$, inform mixture weights $f(u_j, z_j; \\theta)$ through a deep network combining image and text embeddings. These weights adapt to heterogeneous conditions (e.g., urban infrastructure, social events), enabling dynamic kernel weighting centered on $u_j$ while restricting contextual influence to local neighborhoods.\nZhang et al. [2023] replace the parametric kernel with a deep kernel $K(t, s, u) = k(g(t, s), g(u))$, where $g(\\cdot)$ is a non-linear transformation by a deep NN, learning complex spatial correlations beyond standard Euclidean distance.\nOkawa et al. [2022] propose a method for incorporating high-dimensional contextual data into the Hawkes process. They introduce a weighting term in the excitation kernel, extracting relevant features using CNNs and employing continuous kernel convolution to transform discretized features into continuous space. This enables capturing spatial heterogeneity and external influences while ensuring tractable optimization.\nSemi-parametric and non-parametric kernels. Zhou et al. [2022] introduce a non-parametric mixture-based intensity given by\n$\\lambda^{*}(s, t \\mid z) = \\sum_{i=1}^{n+J} w_i K_s(s, s_i; \\gamma_i) K_t(t, t_i; \\beta_i),$ (6)\nwhere each kernel $K_s(\\cdot)$ and $K_t(\\cdot)$ is a normalized radial basis function, and the parameters $\\{w_i, \\gamma_i, \\beta_i\\}$ are drawn from a latent process $z$. The history is encoded by a Transformer 3.1 and decoded to parameterise the latent process. The samples of the latent process are further decoded via a feedforward network. This method captures uncertainty in the timing and spatial locations of events. By augmenting the observed $n$ events with $J$ randomly sampled representative points, this approach addresses global background intensity, thereby reducing the reliance on strong parametric assumptions.\nNeural STPP models often rely on restrictive assumptions, such as conditional independence or unilateral dependence between the distributions of temporal and spatial events. These assumptions limit their ability to accurately predict events in real-world scenarios, where events exhibit complex interdependencies in both time and space. To address this, Yuan et al. [2023] proposed a framework that jointly models spatiotemporal event distributions via a diffusion-based approach without structural constraints. The model employs a spatiotemporal encoder that separately embeds time and space, fuses these into spatiotemporal representations using self-attention, and conditions a diffusion model on these hidden states. The diffusion process iteratively denoises event coordinates using a co-attention network that dynamically captures cross-dependent spatiotemporal interactions, enabling joint distribution learning without assuming independence or requiring integrable intensity functions. For spatial decoding, the model directly predicts continuous coordinates or can apply a rounding step for discrete locations. This method eliminates the need for approximation during sampling and supports continuous and discrete spatial domains.\nDespite the inherent randomness in event times and locations, many STPP models offer only point predictions, lacking principled uncertainty quantification. This gap is especially problematic in marked STPPs, where reliable confidence scores for discrete event marks are essential. To address these challenges, Li et al. [2024] introduce a score-matching objective for estimating the pseudo-likelihood of marked STPPs overcoming issues with intractable integral calculations while also providing uncertainty estimates the score function represents the gradient of the logarithm of the conditional spatial distribution. They use the same CDN architecture as the backbone as Yuan et al. [2023] to predict the score function. Langevin dynamics is employed to sample event locations by iteratively refining draws according to the learned score. Thresholding the resulting sample density yields confidence regions for event locations, while a similar procedure provides confidence intervals for event times. The proposed framework not only predicts future events accurately, but also quantifies uncertainty, offering robust confidence bounds for both the event timing and discrete marks.\nContinuous time-based methods. The work by Chen et al. [2021] employs Continuous-time Normalizing Flows (CNF), which is based on Neural Ordinary Differential Equations [Chen et al., 2018]. This method separates the conditional CIF into two components: a temporal component and a spatial conditional distribution. Subsequent developments of CNFs further refine the spatial conditional distribution. For instance, the Time-Varying CNF introduces continuous flow transformations to handle time-varying observational data; however, it does not explicitly incorporate conditioning on the event history.\nTo address this limitation, Jump CNFs combine insights from Jia and Benson [2019] with CNFs, integrating discrete state updates at event times. This efficiently models abrupt dynamic changes with a computational cost of O(N) for N past events. To improve scalability for long event histories, Attentive CNFs use Transformer-based attention, enabling parallel trajectory computation while preserving non-trivial dependencies, achieving a balance between efficiency and representational power."}, {"title": "3.3 Multi-Event Prediction", "content": "Single-event prediction methods rely on stepwise CIF estimation, making multi-event forecasting computationally prohibitive in high-dimensional spaces due to repeated integration. Sequential history updates also propagate errors, leading to degraded accuracy over iterations. Multi-event prediction addresses these issues by jointly estimating event distributions, eliminating reliance on sequential updates. Transformer-based architectures enable parallelized predictions, bypassing error accumulation and improving efficiency.\nMulti-step decoders. Erfanian et al. [2022] propose a new Transformer-based architecture augmented with normalizing flows and probabilistic layers, which outputs a batch of $L$ events based on the history of previous $n$ events. In this approach, a separate spatial and temporal encoding is learned for events ranging from $n + 1$ to $n + L$, which are then injected into a probabilistic layer through a learned mapping between the parameters of an exponential distribution for time and a multivariate Gaussian distribution for space. Considering these as the base distributions they are passed into normalizing flows, which convert them into more expressive distributions to model the joint distribution of batched events. The parameters of all distributions are learned independently for $l \\in [n+1, ..., n + L]$. Even though separate distributions are used for space and time, the hidden state used for both as inputs considers spatiotemporal interdependencies.\nSampling full sequences. Another approach models the probability of the entire sequence or point set, rather than modeling the inter-event time and spatial distribution given history. This approach addresses a key problem with CIF parameterization methods. In this modeling framework, the entire event sequence is embedded in the analysis. Moving away from the CIF parameterization offers added benefits for modeling events, such as data imputation and multi-event prediction. L\u00fcdke et al. [2023] model entire TPPs using diffusion models. Superposition and thinning properties of the CIF define the noising (forward) and denoising (backward) methods. In a recent study, L\u00fcdke et al. [2025] extends this work to point processes defined on general metric spaces while generalizing to order space, like STPPs. This work further develops the method, enabling flexible conditioning on various tasks without the need for explicit task-specific training. The approach separates the thinning and superposition operations into two independent processes. This separation allows the diffusion process to be defined as a stochastic interpolation between two point sets, entirely independent of the intensity function."}, {"title": "4 Parameter Estimation and Inference", "content": "The key objectives of event prediction include enhancing predictive performance, improving generalization and robustness, understanding event dynamics through learned parameters, accounting for event behavior heterogeneity, and capturing the influence of external factors. Predicting future events from a learned model requires sampling from its intensity function. Traditional statistical methods often rely on strong parametric assumptions for modeling event intensities, using techniques such as likelihood-based methods, partial likelihood, the EM algorithm, or Bayesian approaches.\nIn statistical inference, Maximum Likelihood Estimation (MLE) is most commonly used to fit classical and neural STPPs, typically by maximizing the likelihood function or, equivalently, minimizing the negative log-likelihood (NLL). For an observed sequence of N events, the NLL is given by [Daley et al., 2003]:\n$L_{NLL} = -\\sum_{i=1}^{N} \\log \\lambda^{*}(t_i, s_i) + \\int_{[0, T]} \\int_{S} \\lambda^{*}(\\tau, u) d\\tau du.$\nWhen using neural networks to parameterize the CIF, evaluating the integral term is typically intractable. This complexity often requires numerical methods [Chen et al., 2021] or Monte Carlo methods [Mei and Eisner, 2017] for likelihood evaluation. However, these strategies can be computationally expensive and prone to numerical errors, particularly in high-dimensional spatiotemporal domains. While certain simplifying assumptions, such as exponential decay [Du et al., 2016] and linear interpolation [Zuo et al., 2020], can lead to closed-form solutions or faster approximations, they often restrict the expressiveness of the model.\nIn the purely temporal settings, Zhou and Yu [2023] introduce a paradigm for efficient and non-parametric inference of TPPs. They approximate the influence function (c.f. Equation (2)) via a NN, using automatic integration to compute its integral. A monotonically increasing integral network is trained, its partial derivative defines the CIF. This approach directly yields the CIF and its antiderivative from the network parameters, avoiding functional form restrictions. Building on this foundation, Zhou and Yu [2024] addresses the computational challenge of integrating the intensity function in 3D spatiotemporal domains by employing automatic integration. This approach learns an integral network, whose partial derivatives with respect to spatial and temporal inputs yield the intensity, ensuring an exact antiderivative without restricting the model's functional form. Furthermore, a Prod-Net factorization of the influence function into 1D components enforces non-negativity while capturing spatiotemporal interactions. Maximizing the log-likelihood of observed events learns a highly expressive CIF, improving spatiotemporal event prediction.\nZhou et al. [2022] integrates flexible non-parametric modeling with amortized variational inference. This model captures events in continuous time, capturing irregular sampling dynamics and unifying spatial and temporal dependencies. By employing a kernel-based intensity function, the approach allows for closed-form integration, addressing previously intractable likelihood computations. This design avoids computationally expensive numerical integration inherent in neural ODE-based approaches [Chen et al., 2021]. Furthermore, this non-parametric approach avoids restrictive parametric assumptions. Training via amortized variational inference maximizes the evidence lower bound (ELBO) of the likelihood while balancing reconstruction accuracy and posterior regularization. The framework uses the kernel-based intensity for gradient computation, facilitating end-to-end optimization of both encoder and decoder parameters.\nZhang et al. [2023] uses score matching, which minimizes the Fisher divergence between the model's log-density gradient and the data's log-density gradient, thus bypassing the need to calculate the intractable integral. A denoising score matching (DSM) method is used, which improves stability by introducing a small amount of noise to the data, which avoids the computation of second derivatives. In contrast, Li et al. [2024] utilizes a score matching-based pseudolikelihood objective, which eliminates the need for explicit calculation of the normalizing term that makes the likelihood integral intractable. The model decomposes the joint intensity function into a product of conditional distributions, allowing for the application of score-matching techniques for event times and locations, while using a conditional likelihood for event marks. This approach is designed to overcome overconfidence and underconfidence by learning a posterior distribution that matches the actual data distribution, which also requires score-based sampling with Langevin dynamics.\nReinforcement learning (RL) frameworks provide a training approach that does not rely on likelihood calculations. Zhu et al. [2021b] employs an imitation learning framework to train their model. The learner policy is defined by a PDF associated with the CIF of the point process and parameterized by the model's parameters. The goal is for this learner policy to replicate the expert policy reflected in the training data. The training process maximizes the expected reward, determined by the Maximum Mean Discrepancy (MMD) between empirical distributions of the training data and data generated by the learner's policy. This data-driven MMD reward function offers robustness to model mismatch as it compares data distributions instead of relying on a predefined likelihood. Additionally, the closed-form representation of the reward function enables computationally efficient optimization via analytical gradient calculation, avoiding intensive inverse reinforcement learning."}, {"title": "5 Evaluation Metrics", "content": "Evaluating STPPs requires task-specific metrics, summarized in Table 1. Model fit is often assessed using NLL, which measures how well predicted distributions align with observed data. However, NLL prioritizes overall distributional fit over individual event accuracy and can be biased due to computational approximations like Monte Carlo integration. Moreover, since NLL conditions on ground truth history, it is limited in evaluating true generative capacity.\nAlternative distributional metrics address these shortcomings. HD [Zhou et al., 2022] directly compares learned and true distributions but requires ground truth intensity and spatial discretization, limiting real-world use but useful for testing the ability of models to recover known patterns. MMD [Zhu et al., 2021b] avoids strong distributional assumptions by comparing generated and observed sequences but is computationally expensive and sensitive to kernel selection. MMD can also be used with CD [L\u00fcdke et al., 2025] as the distance measure. This improves distributional comparisons and is useful for evaluating the generative performance of STPPs. SL, measured via Wasserstein distance between two categorical distributions, provides additional insight into model performance by comparing event sequence lengths, especially useful when the task involves measuring case counts such as in epidemiology or crime modelling.\nFor point prediction accuracy, MAE, MSE, and RMSE are commonly used. MAE is robust to uniform errors, while RMSE is more sensitive to large deviations. Aggregated event predictions rely on normalized MAE (NMAE) [Okawa et al., 2022] and MAPE [Okawa et al., 2019]. While NMAE enables cross-scale comparisons, it depends on predefined spatiotemporal regions. MAPE, though intuitive, can be unstable near zero values. Mean relative error (MRE) [Dong et al., 2022] assesses intensity differences but also suffers from instability near zero. Prediction accuracy (ACC) is useful for event count estimation but, unlike metrics considering location and time, it assesses the accuracy of event counts only. Prediction accuracy (ACC) is useful for event count estimation but doesn't account for spatial and temporal precision.\nUncertainty quantification is crucial for robust evaluation. CS and ECE [Li et al., 2024] assess how well predicted confidence intervals align with observed distributions, though ECE's binning requires adaptive methods for reliability.\nSelecting metrics depends on the task and dataset. A comprehensive evaluation combines prediction accuracy, distributional fit, and uncertainty quantification. Recent work favors WD and MMD over NLL for assessing generative quality."}, {"title": "6 Applications", "content": "The existing literature on neural STPPs mainly underscores their applications in public safety and urban mobility.\nNatural disasters. Neural STPPs model earthquake and wildfire occurrences by capturing spatiotemporal dependencies. The ETAS model is widely used for earthquake forecasting, with neural extensions improving prediction. Nicolis et al. [2021] enhance ETAS using neural networks for seismic forecasting, while Zhang et al. [2024] integrate it with deep learning. For wildfires, Xu et al. [2023] incorporate environmental factors and remote sensing data to predict ignition probability and magnitude, demonstrating strong performance on California data.\nCrime. STPPs are increasingly used for crime prediction. Dong and Xie [2024] model gun violence in Atlanta with a non-stationary Hawkes process, integrating socio-economic covariates to improve predictions. Dong et al. [2024] develop a spatiotemporal network point process for Valencia crime, mapping events onto streets to better capture urban dynamics.\nTraffic. Zhu et al. [2021a] introduce an attention-based STPP integrating traffic sensor data and 911 call records to model congestion dynamics, capturing self-excitation and external influences. They use an NLP-inspired attention mechanism for temporal dependence and a 'tail-up' approach for spatial correlations on road networks. Jin et al. [2023] propose a congestion prediction model combining GCNs for spatial dependencies, Transformers for temporal patterns, and a continuous GRU with neural flow for instantaneous traffic behavior.\nEpidemiology. STPPs model infectious disease spread by capturing event transmission dynamics. Li et al. [2021] propose an intensity-free STPP using generative adversarial imitation learning, while Dong et al. [2023b] develop a non-stationary STPP with an NN kernel to model heterogeneous COVID-19 case correlations and spatial variations."}, {"title": "7 Open Challenges", "content": "Neural STPPs have advanced event modeling but still face key obstacles-computational constraints, interpretability, reproducibility, and real-world applicability. Overcoming these will enable robust, scalable, and interpretable models.\nReproducibility. A significant barrier to advancing neural STPP research is the lack of standardized experimental setups and consistent baseline comparisons. In contrast to TPPs which benefits from unified libraries like Xue et al. [2024], there is no comprehensive implementation for spatial and temporal methods in STPPs. A unified library incorporating diverse neural architectures, metrics, and training strategies would facilitate better ablation studies and benchmarking. While some perform well under specific conditions or on curated datasets, fair evaluations across architectures remain challenging without robust testing environments. Additionally, the limited availability of open-source tools further restricts wider adoption and reproducibility of these methods.\nBenchmarking. A major gap in the literature is the absence of standardized benchmarking datasets. Existing datasets often suffer from selection bias, missing data, and varying granularity, complicating integration and analysis. Different datasets exhibit diverse phenomena, such as spatial heterogeneity, long-range dependencies, and entangled spatiotemporal dynamics. Some datasets display self-exciting behavior, while others show self-correcting patterns. The lack of a unified dataset with consistent testing conditions means benchmarking efforts are often limited to specific attributes or datasets where a method performs well. A comprehensive event database would aid in model development and evaluation. Additionally, the lack of contextual datasets hinders efforts to develop foundational models for event prediction, improving the understanding of event dynamics in data-scarce environments.\nArchitectures. Generative strategies and alternative training objectives represent promising yet underexplored avenues for STPP research. While GAN-based architectures and Wasserstein objectives have shown promise in temporal point processes [Xiao et al., 2017], their adaptation to spatiotemporal applications remains limited. Similarly, methods moving beyond MLE, particularly those leveraging Transformer-based models instead of RNNs, could enhance the flexibility and robustness of event modeling. Continuous-time neural TPPs, such as those proposed by [Bilo\u0161 et al., 2021] and [Chen et al., 2024], have demonstrated potential for temporal modeling but have yet to be sufficiently applied to spatial event modeling. Many existing neural STPPs treat space and time independently, missing opportunities to improve temporal predictions by capturing spatial relationships. Integrating GNNs to enhance spatial modeling remains an open challenge.\nApplicability. Despite improvements in predictive performance, neural STPPs are rarely utilized to inform policy decisions or design interventions. Their lack of interpretability limits their applicability in real-world scenarios, where generalizable, interpretable models with uncertainty quantification are essential. Policymakers often seek to understand the impact of interventions, necessitating models that incorporate contextual factors and causal effects in event propagation. This underscores the need for research in interpretable neural networks, neuro-symbolic methods, and counterfactual analysis for retrospective policy evaluation and deeper insights into event modeling.\nCausality and uncertainty. Policy-focused applications of neural STPPs require deeper insights into event propagation and the causal factors influencing event rates. However, causal inference and uncertainty quantification remain underexplored in this field. Bayesian methods, which could integrate expert knowledge through priors, have not been widely adopted in neural spatiotemporal event modeling. Similarly, causal representation learning must address spatiotemporal confounding, but current research has yet to fully integrate these methods into neural STPPs [Wang et al., 2024]. Advancing these areas is critical for developing models that can inform decision-making and intervention design."}, {"title": "Ethical Statement", "content": "Point processes are used in ethically sensitive areas, such as police patrol allocation [Mohler et al., 2015], where biases may cause disparate harms [Alikhademi et al., 2022]. Unintended consequences should be considered before using Neural STPPs for spatial interventions."}]}