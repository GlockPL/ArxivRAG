{"title": "EAGLE: Egocentric AGgregated Language-video Engine", "authors": ["Jing Bi", "Yunlong Tang", "Luchuan Song", "Ali Vosoughi", "Nguyen Nguyen", "Chenliang Xu"], "abstract": "The rapid evolution of egocentric video analysis brings new insights into understanding human activities and intentions from a first-person perspective. Despite this progress, the fragmentation in tasks like action recognition, procedure learning, and moment retrieval, etc., coupled with inconsistent annotations and isolated model development, hinders a holistic interpretation of video content. In response, we introduce the EAGLE (Egocentric AGgregated Language-video Engine) model and the EAGLE-400K dataset to provide a unified framework that integrates various egocentric video understanding tasks. EAGLE-400K, the first large-scale instruction-tuning dataset tailored for egocentric video, features 400K diverse samples to enhance a broad spectrum of tasks from activity recognition to procedure knowledge learning. Moreover, EAGLE, a strong video multimodal large language model (MLLM), is designed to effectively capture both spatial and temporal information. In addition, we propose a set of evaluation metrics designed to facilitate a thorough assessment of MLLM for egocentric video understanding. Our extensive experiments demonstrate EAGLE's superior performance over existing models, highlighting its ability to balance task-specific understanding with holistic video interpretation. With EAGLE, we aim to pave the way for research opportunities and practical applications in real-world scenarios.", "sections": [{"title": "1 Introduction", "content": "Understanding human activities and intentions in videos is a key challenge for intelligent systems, requiring advanced reasoning capacities. While there have been advancements in computer vision, the most notable breakthroughs are seen in the evolution of Large Language Models (LLMs) [11, 60]. These models benefit from increased data and model size, resulting in enhanced generalizability, which is often challenging to achieve in computer vision tasks. By leveraging the pre-trained LLMs [19, 100], MLLMs [7, 9, 12, 22, 26, 27, 41, 42, 47, 99] show superior results to a wide spectrum of multimodal tasks [20, 32, 37, 55, 56, 74, 78]. Unlike current MLLMs that predominantly focus on images, EAGLE advances to capture spatial and temporal information for more in-depth video analysis.\nTo enable MLLM to achieve a more comprehensive and detailed examination of human activities, our work shifts focus from previous efforts centered on third-person perspectives [8, 17, 35, 38, 79] towards the egocentric view. This shift not only provides deeper insights into individual interactions with their environment, enhancing tasks such as action recognition and localization, but also enables novel tasks such as Natural Language Queries and Action Anticipation [21], which demand an in-depth view of the video content, including activity and procedure knowledge understanding. Taking sandwich preparation as an example, the task involves recognizing actions such as preparing ingredients and spreading condiments, as well as understanding how these actions contribute to the overall process. Pioneering efforts like EPIC-KITCHENS-100"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Egocentric Video Understanding", "content": "Egocentric Video Understanding began with pioneering datasets [13, 44, 63] that demonstrated the unique potential of first-person video analysis. The field expanded with EPIC-KITCHENS [14], featuring 100 hours of videos, and further with Ego4D [21], which boasts an impressive 3,000 hours of data. These expansions inspired a wide range of research tasks, including human-object interactions [57, 92], activity recognition [36, 66, 85, 89], sounding object localization [1, 30, 53, 101], pose estimation and prediction [59, 87], procedure knowledge learning [4, 23], and social understanding [73]. However, various tasks have resulted in specialized, fragmented model development. EAGLE-400K aims to consolidate these tasks for a more holistic video understanding."}, {"title": "2.2 LLMs for Multimodal Understanding", "content": "Recent advancements have extended LLMs to multimodal domains, resulting in MLLMs [12, 18, 27, 40, 42, 96, 103] that excel in various tasks. Fine-grained multimodal understanding involves a detailed understanding of visual content, including spatial details [9, 62, 88, 98], temporal sequences [28, 45, 61, 80, 81, 83], or a combination of both [5, 48, 86]. Models like those in [12, 42, 103] use a two-stage Q-former to align vision and language models. [96] aligns video and audio modalities with LLMs by training adapters, showing its ability to integrate multiple modalities effectively. Video-ChatGPT [52] and VideoChat [43], combining LLMs with video foundation models, are tailored for coarse-grained video-based conversations. However, few MLLMs are designed to tackle both spatial and temporal video tasks [82], and our work emphasizes interpreting 16 seconds videos, which are 2-4\u00d7 longer compared with other video MLLMs."}, {"title": "2.3 Fine-grained Multimodal Comprehension", "content": "Fine-grained multimodal comprehension involves a detailed understanding of visual content, including spatial [9, 62, 88, 90], temporal [45, 45, 61, 76, 80], or both spatial and temporal [5, 86] information. The multimodal models for fine-grained spatial understanding like [9] and [93] are utilizing LLMs trained on an instruction-tuning dataset which is produced by the language-only GPT-4 and include the coordinates of objects' bounding boxes. They can handle multiple location-related multimodal tasks like REC, PointQA, dense image captioning, and VQA. In [62, 98], special tokens representing the regions are used, while [2] adopts both special tokens and coordinates. [39, 88] implemented irregular pixel-level region segmentation, generating descriptive captions for any object within an image. The multimodal models for fine-grained temporal video understanding, including [28, 45, 61, 81, 83], are leveraging the capabilities of LLMs. There are seldom models designed to handle both spatial and temporal video understanding tasks."}, {"title": "3 EAGLE-400K Dataset and Benchmark", "content": "Egocentric video understanding [58, 64] involves two primary aspects: activity recognition, which identifies actions like picking up objects, and procedure knowledge learning, which models the relationships between actions and their contributions to accomplishing tasks. We aim to consolidate multiple datasets with different focuses and provide a comprehensive dataset. We start with two popular egocentric datasets, EPIC-KITCHENS [36] and Ego4D [21], featuring long-term, untrimmed videos of daily tasks. These datasets are annotated with actions labels and object interactions without procedure knowledge, focusing solely on identifying actions.\nTo bridge the gap in procedural understanding we have collected the PTA dataset, including 268 videos recorded in laboratory settings. This dataset is designed to enhance procedure knowledge learning through three distinct recipes: pinwheel, mug cake, and brew coffee. Unlike previous datasets [4, 75] which prioritized task diversity but lacked depth within individual tasks, our approach focuses on providing extensive variation and a higher number of samples within fewer tasks. This approach enables a more comprehensive analysis of procedural steps as shown in Figure 3."}, {"title": "3.1 Annotation", "content": "We used established training and validation splits for Ego4D and EPIC-KITCHENS. For PTA, we used a 7/3 split, excluding videos from one lab for testing the novel environment, as detailed in Table 4. For EPIC-KITCHENS split, we utilized official annotations that include action-object labels with temporal boundaries as shown in Figure 3. Additionally, we integrated spatial annotations from the EPIC-KITCHENS-VISOR dataset [15], an extension of EPIC-KITCHENS, providing object segmentation trajectories covering one-third of the original EPIC-KITCHENS dataset. In the case of Ego4D, the initial ~3.8 million narrations underwent refinement to generate various subsets, as outlined in [21]. Our focus lies on the Episodic Memory and Forecasting Benchmark, which includes tasks such as Natural Language Queries, Moment Queries, and Long-term Action Prediction tasks, all tailored for activity understanding. In the PTA subset, each video depicts the process of making a recipe, with timestamps marked for key procedure steps.\nTo enrich the annotation with object information, we first fine-tuned the Grounding DINO [49] using the EgoObject dataset [102], omitting its class head. This significantly improved its object proposal accuracy to over 90% on the test set. Next, we integrated this specialized Grounding DINO model with the latest DEVA tracker [10], achieving reliable object tracking from an egocentric viewpoint. Lastly, we employed the LLaVA-13B model, known for its robust visual recognizing ability, to interpret the semantic meanings of the proposed object regions. As shown in Figure 3, while this approach may not always achieve the accuracy level of human annotation\u2014occasionally mistaking a tortilla for flatbread\u2014it marks a considerable leap forward, especially given the scarcity of zero-shot vision models capable of high accuracy grounding."}, {"title": "3.2 Instruction Tuning Data Generation", "content": "As previously mentioned, diverse tasks and inconsistent annotation standards often limit the comprehensive understanding of videos. We adapt the instruction tuning [95] to unify these annotations under a cohesive framework. In our dataset, videos are segmented into 16-second clips, 3-5x longer than common video understanding dataset, ensuring each contains a rich number of actions while maintaining a manageable length, as shown in Table 4. By comparison, our baseline model, LaViLa, which is trained specifically on egocentric videos, typically takes a 1-sec clip. Another example is EPIC-KITCHENS Action Anticipation task, although videos tend to be minutes, only a 5-second segment is used for analysis. Adopting 16-second clips allows us to capture comprehensive action details without overwhelming the model.\nTo determine the optimal frame rate, we draw inspiration from recent studies [70, 91] that have shown promising results in frame-based video understanding by analyzing videos frame-by-frame and using feature pooling. Building on this, we sample one frame per second, maintaining a consistent interval regardless of the video's frame rate. To enhance contextual understanding, we incorporate temporal context with 30 seconds before and after each clip. We chose a 30-second duration to balance action details and cohesive narration. This is based on our observation that longer durations reduce the relevance of actions.\nIn this way, the context helps tasks like action anticipation and detection and encourages the development of new tasks by extrapolating relationships between labels. For instance, our framework enables advanced tasks such as Temporal Reasoning and Cross-Referencing Events, as shown in Table 3, enhancing the dataset's utility without additional annotation effort.\nWe use two types of symbolic representations to prompt GPT4: (i) Captions, which typically describe the visual scene from various perspectives. (ii) Object trajectories in the scene, and each box encodes the object concept and its spatial location as shown in Figure 3. We collect 400K unique video instruction-following samples in total, including 350K for activity recognition (as shown in"}, {"title": "4 EAGLE Model", "content": "Existing image-based MLLMs such as Shikra [9] primarily focus on spatial information, while models like VTimeLLM [29] specifically target temporal dimensions. Given the unique aspects of our dataset, which encompasses both spatial and temporal attributes, our goal is to simplify the tuning process and construct a straightforward yet strong model by leveraging the existing MLLM model.\nOur model, in line with common MLLMs, integrates a vision encoder, an alignment layer, and a large language model (LLM), specifically employing the pre-trained ViT-L/14 from CLIP [68] as the frame encoder E and Vicuna-13B as LLM, as shown in Figure 4. Given a video sample \\(V_i \\in R^{T \\times H \\times W \\times C}\\) with T frames, the frame encoder E processes each frame independently, generating video embedding as \\(x_i \\in R^{T \\times D}\\). After obtaining frame embeddings, selecting an optimal method for aggregating these features is critical.\nVideo-LLAMA [97] employs temporal position embedding and a q-former, which typically demands a large amount of paired video-text data (rare in video datasets). Compared with image-language datasets such as CC3M [77] utilized by LLaVA [46], video-language"}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Evaluation Metrics", "content": "Following the evaluation methods [50, 100] for recent LLMs, we use GPT-4 to assess the quality of responses generated by models. Due to the time-consuming nature of evaluating all 7,700 samples across nine models with GPT-4, we adopt a square root sampling strategy, selecting approximately (\\(\\sqrt{7700} \\approx 88\\)) 100 samples as a representative subset. To maintain consistency and ensure the reproducibility of findings from the initial 100 samples, we further analyzed 200 additional responses. This was done to evaluate the performance of the top four models, which we have designated as EAGLE-pool2, Shikra2, BLIP-22, and EAGLE2. The results are presented in Table 5. The results from this extended dataset are presented in the subsequent table and are consistent with the findings from our initial sample of 100 responses.\nGiven the nature of the egocentric dataset, which offers only action labels, recipe steps, and corresponding timestamps, we need to develop ground truth sentences for evaluation purposes. Our empirical findings indicate that compared to using polished sentences of ground truth labels, template-based construction reduces the occurrence of hallucination errors. The evaluation prompt was refined iteratively through trial and error, aiming to improve the accuracy in identifying event boundaries and objects and to enhance clarity. The evaluation prompt will be included in the supplementary.\nThese selected responses will be scored by GPT-4 based on five key metrics, each rated on a scale from 1 to 10, with higher scores indicating superior performance. The evaluation metrics are as follows:"}, {"title": "5.2 Baseline Models", "content": "For our baseline models, we use both image-based and video-based approaches. Image-based models include:\n(1) BLIP-1 [42], image-language pre-training model that integrates textual and visual information to enhance multimodal understanding. This model excels in multimodal understanding and is effective in zero-shot video language tasks.\n(2) BLIP-2 [41] trained a lightweight Q-Former for multimodal representation alignment and vision-to-language generation, capable of following instructions without multimodal instruction tuning.\n(3) InstructBLIP [12], built upon BLIP-2, this model reformats 26 public datasets for instruction tuning and updates only the Q-Former during training. It formulates various tasks as instructions, similar to our method.\n(4) LaViLa [99] is a video narration method that pairs a video encoder with a GPT-2 [67] as language decoder and a T-5 [69] to reduce overfitting and enhance natural language data.\n(5) LLaVA [47] introduces visual instruction tuning, using GPT-generated data and instructions for conversation, detailed description, and complex reasoning.\n(6) ImageBind-LLM [22] is an open-source MLLM, with its algorithm details pending publication.\n(7) Shikra [9] encodes regions in natural language as numerical coordinates to specify regions in user queries.\n(8) Video-LLaMA [96] trains adapters for aligning video and audio modalities with LLMs, sampling only eight frames from arbitrarily long videos.\nAmong baseline models, LaViLa is specifically trained on egocentric videos (Ego4D, EPIC-KITCHENS) to generate narrations. Despite this targeted training, our research reveals that in zero-shot learning scenarios, MLLM outperformed LaViLa for handling egocentric data. Details of the responses from different models will be included in the supplementary material.\nAdditionally, to ensure a fair comparison, we chose not to fine-tune the vision encoder in our EAGLE model for egocentric vision adaptation. Instead, we focused on refining the model to improve its spatial-temporal video analysis capabilities. Our findings indicate that our dataset significantly contributes to enhancing the performance of current MLLMs in understanding and interpreting video content."}, {"title": "5.3 Results and Analysis", "content": "To validate the performance of EAGLE, we compare it with recent MLLMs [9, 47, 96], on the EAGLE-400K dataset. As Table 5 shows, Shikra and BLIP-2 demonstrate remarkable proficiency, scoring highest in most categories, indicating their reliability, helpfulness, and detailed response capability. Although Video-LLaMA is targeted at video analysis, it exhibits the lowest performance when compared to image-based multimodal large language models (MLLMs), with outputs often arbitrary and failing to capture essential visual information from videos. LLaVA and InstructBLIP demonstrate balanced and above-average performances across all metrics, showcasing their versatility in handling diverse tasks. Interestingly, while LaViLa is specifically trained on egocentric data, its performance is hindered by its relatively weaker language backbone (GPT-2), resulting in it being outperformed by more advanced MLLMs in a zero-shot setting. This highlights the significant impact that a robust language model can have on performance.\nMoreover, ImageBind-LLM excels in providing detailed and consistent responses. This suggests that superior language modeling capabilities, coupled with a more generalized visual encoder, can enhance overall performance significantly.\nComparing the two variants of EAGLE, which utilize different methods for processing video content: Using concatenation of frame features preserves the temporal order of each frame, allowing the model to capture more detailed temporal dynamics and intricate interactions within the video content. EAGLE-pool, on the other hand, employs temporal pooling to aggregate features over time. This approach helps reduce the impact of less relevant information and noise but may also gloss over finer temporal details that are crucial for understanding complex dynamics. Despite these trade-offs, EAGLE-pool still benefits from the extensive EAGLE-400K dataset and performs better than spatial grounding models like Shikra, which focuses more on spatial rather than temporal data.\nAblation Study. Studies were conducted on the EAGLE-400k dataset using varied training data splits to investigate the impact of spatial and temporal information on egocentric video understanding. The ablation included: removing time boundaries (w/o time), excluding object trajectories (w/o obj), and eliminating both (only desc). As shown in Table 6, performance tends to decrease when either time or object information is excluded, with the least effective results observed when relying solely on descriptions. Notably, PTA exhibits the most significant decline in performance when detailed information is removed, indicating that procedural learning relies more heavily on temporal and object details.\nThe results from Table 6 highlight specific trends across different datasets. For EPIC-KITCHENS, excluding temporal information resulted in a performance drop from 6.8 to 5.9, showing a considerable dependence on time data. Similarly, Ego4D saw a decrease from 6.4 to 6.1 and 6.2 without time and object information, respectively. The PTA dataset showed a marked drop from 6.5 to 5.8 when object information was excluded, underscoring its reliance on object trajectories. These findings underscore the critical role of temporal and object-based features in enhancing egocentric video understanding, with the integrated use of all information sources yielding the highest performance across all datasets."}, {"title": "6 Conclusion", "content": "In this work, we present the EAGLE-400K dataset and the EAGLE model for holistic egocentric video understanding. The EAGLE-400K dataset consists of 40K question-answer pairs from 36K diverse video clips and EAGLE offers a unified framework for diverse visual computational tasks. We also provide an evaluation method for egocentric vision tasks and demonstrate EAGLE's superior performance. The introduction of a new evaluation metric enhances the understanding of video-based MLLMs. We hope our work can pave the way for augmented reality assistants that aid in complex physical tasks with multimodal perception."}, {"title": "7 Limitation", "content": "Our PTA dataset was assembled with a significantly smaller number of contributors compared to larger datasets like EPIC-Kitchens and Ego4D. This limited participant pool may result in the dataset predominantly reflecting individual-specific characteristics, such as a participant's height or unique culinary techniques, which could skew the representativeness of the data. Additionally, the dataset primarily comprises cooking videos. This focus was chosen because these activities align well with structured instructions and are more readily accessible for recording. However, this emphasis on cooking-related content may introduce a domain bias, as it limits the diversity of egocentric experiences captured, notably under-representing categories such as social interactions or spontaneous activities."}]}