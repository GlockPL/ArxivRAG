{"title": "Differentially Private Publication of Electricity Time Series Data in Smart Grids", "authors": ["Sina Shaham", "Bhaskar Krishnamachari", "Gabriel Ghinita", "Cyrus Shahabi"], "abstract": "Smart grids are a valuable data source to study consumer behavior and guide energy policy decisions. In particular, time-series of power consumption over geographical areas are essential in deciding the optimal placement of expensive resources (e.g., transformers, storage elements) and their activation schedules. However, publication of such data raises significant privacy issues, as it may reveal sensitive details about personal habits and lifestyles. Differential privacy (DP) is well-suited for sanitization of individual data, but current DP techniques for time series lead to significant loss in utility, due to the existence of temporal correlation between data readings. We introduce STPT (Spatio-Temporal Private Timeseries), a novel method for DP-compliant publication of electricity consumption data that analyzes spatio-temporal attributes and captures both micro and macro patterns by leveraging RNNs. Additionally, it employs a partitioning method for releasing electricity consumption time series based on identified patterns. We demonstrate through extensive experiments, on both real-world and synthetic datasets, that STPT significantly outperforms existing benchmarks, providing a well-balanced trade-off between data utility and user privacy.", "sections": [{"title": "1 INTRODUCTION", "content": "Analysis of electricity consumption data plays a critical role in planning power grid infrastructures for smart cities. Such data are represented as time series of geo-tagged data, and serve as a critical information source, providing detailed knowledge to policymakers about energy usage trends. Such data can help identify where/when consumption surges occur, and decide where to place expensive equipment such as transformers; or in the case of renewable energy sources, it can help decide where to place storage elements, and when to release their capacity back into the grid.\nDespite its advantages, analysis of electricity consumption time series raises significant privacy concerns. The data may reveal personal habits and lifestyles, such as individuals' daily routines, working hours, etc., leading to privacy violations. Moreover, the risk of third-party exploitation by marketers and advertisers poses a threat of unwanted privacy intrusions, as consumers may be targeted based on their specific energy usage behavior.\nPrevailing approaches for protecting electricity time series information rely on the powerful Differential Privacy (DP) model [1]. DP achieves privacy by adding noise to the data, thereby minimizing the likelihood of re-identification. However, when dealing with time series data, the existence of temporal correlations leads to increased re-identification risk over time, causing DP to add excessive noise to offset this risk, lowering data utility [2, 3].\nWe propose a model that jointly takes into account both time and space attributes of electricity consumption data. Our Spatio-Temporal Private Timeseries (STPT) algorithm trains a Recurrent Neural Network (RNN) to identify spatio-temporal electricity consumption patterns. The patterns are subsequently used to partition the time series into a spatio-temporal histogram that is used by DP mechanisms to sanitize and release the data. A key innovation of STPT is the incorporation of spatial distribution alongside temporal sequencing. We start with a low-granularity aggregation of time series data to identify macro consumption trends, followed by several increasingly-higher granularity aggregations to discern micro trends. This dual-focus on both macro and micro trends allows for a nuanced representation of consumption patterns, enhancing STPT's ability to preserve data utility while enforcing DP. Note that, while RNNs have been used before for geo-tagged time series, the focus of prior work is on trajectory forecasting [4]. Our approach is significantly different as consumer locations are static (e.g., households), and the purpose of RNN is to estimate future power consumption at each location.\nOur specific contributions include:\n\u2022 We introduce a novel method for modeling and representing electricity time series data, which takes into account both spatial and temporal properties.\n\u2022 We propose STPT, an innovative algorithm that integrates a unique approach for training RNNs across both time and space dimensions on differentially private data."}, {"title": "2 PRELIMINARIES", "content": "Consider a two-dimensional map that encloses a set of N households U = {u1, ..., un }. We denote the electricity consumption for user i at time t by xi,t (we use the term household and power grid user interchangeably). Each household meter sends its electricity reading to an aggregator at regular intervals \u25b3 \u00d7 t (t = 1, ..., \u03a4) where A \u2208 R. The dataset of meter readings is denoted as:\nD = (xi,t)i=1,...,N;t=1,...,T\n(1)\nThe goal is to release the dataset D according to the requirements of DP, thus preventing an adversary from inferring the consumption patterns of any individual user. We start our discussion by explaining the system model commonly used for the publication of DP electricity consumption time series, followed by an illustration of the foundational concepts related to DP. A summary of notations used throughout the paper is provided in Appendix A."}, {"title": "2.1 System Model", "content": "Figure 1 depicts the system architecture which adheres to the industry-standard model for publishing electricity data, and consists of households, a data aggregator, and data recipients which perform analyses on the released consumption data. The specific functions of each party are detailed below.\n\u2022 Households equipped with smart meters are generators of data and are considered to be trustworthy in the system model. The electricity consumption of users is recorded hourly using their meter and sent to the data aggregator.\n\u2022 Data Aggregator is a trusted party that collects the time series generated by users and publishes their aggregated data in a privacy-preserving way. The sanitization process is done based on DP, preventing adversaries from inferring any individual-level consumption pattern.\n\u2022 Data Recipients leverage the private aggregated data for diverse applications, from forecasting to planning. Their objective is to utilize consumption values over specific spatial regions and time periods. The recipients are considered to be honest but curious, and they may attempt to infer individual user consumption from aggregated data. Individual consumption details must be protected, as they can be used to infer sensitive details about users, such as activity patterns, lifestyle habits, etc."}, {"title": "2.2 Differential Privacy", "content": "Two databases D and D' are called neighboring or sibling if they differ in a single record t, i.e., D' = D\u222a{t} or D' = D\\{t}.\nDefinition 1 (e-Differential Privacy [1]). A randomized mechanism A provides e-DP if for any pair of neighbor datasets D and D', and any a \u2208 Range(A),\n$\\frac{Pr(A(D) = a)}{Pr(A(D') = a)} < e^{\\epsilon}$ \n(2)\nParameter e is referred to as privacy budget. e-DP requires that the output obtained by executing mechanism A does not significantly change by adding or removing one record in the database. Thus, an adversary is not able to infer with significant probability whether an individual's record was included or not in the database.\nAside from the amount of privacy budget, another factor that plays a critical role in achieving e-DP is the concept of sensitivity, which captures the maximal difference achieved in the output by adding or removing a single record from the database.\nDefinition 2 (L\u2081-Sensitivity[5]). Given sibling datasets D, D' the L1-sensitivity of a set g = {91,..., gm} of real-valued functions is:\n$s = \\max_{VD,D'}  \\sum_{i=1}^{m} |g_i(D) - g_i(D')|$ \n(3)\nA widely-used mechanism to achieve e-DP is called Laplace mechanism. This approach adds to the output of every query function noise drawn from Laplace distribution Lap(b) with scale b and mean 0, where b depends on sensitivity and privacy budget.\n$Lap(x/b) = \\frac{1}{2b} e^{-|x|/b} $ where $b = \\frac{S}{\\epsilon}$ \n(4)\nTo simplify notation, we denote Laplace noise by $Lap(\\frac{S}{\\epsilon})$.\nIn our work, we make extensive use of the following three essential results in differential privacy:\nTHEOREM 1 (SEQUENTIAL COMPOSITION [6]). Let A1 and A2 be two DP mechanisms that provide \u20ac1- and \u20ac2-differential privacy, respectively. Then, applying in sequence A1 and A2 over the dataset D achieves (\u20ac1 + \u20ac2)-differential privacy."}, {"title": "3 PROBLEM STATEMENT", "content": ""}, {"title": "3.1 Time-Series Representation", "content": "Consider a spatial grid of size Cx \u00d7 Cy overlaid on a 2D map, dividing the spatial domain into smaller regions. Additionally, we divide the time dimension into a number of Ct equal-length intervals. The electricity consumption data is thus captured by a three-dimensional matrix Ccons called consumption matrix with Cx \u00d7 Cy \u00d7 Ct elements. Each element cijk in this matrix represents the electricity consumption within the (i, j) region during the time interval from A \u00d7 k to A \u00d7 (k + 1), where A is the time resolution. For ease of analysis, especially when conducting sensitivity studies in relation to data publication under DP, we assume without loss of generality that A = 1. This assumption implies that each data point in the time series corresponds to distinct time intervals, meaning that Ct is effectively the length of the time series (Ct = T)."}, {"title": "3.2 Problem Formulation", "content": "Data recipients are interested in answering multi-dimensional range queries on top of the electricity consumption matrix.\nDefinition 3. (Range Query) A range query on the consumption matrix is a 3-orthotope with dimensions denoted as d\u2081 x d2 x d3, where di represents a continuous interval in dimension i.\nTo evaluate the accuracy of results, we use the Mean Relative Error (MRE) metric. For a query q with the true aggregated consumption p and noisy consumption value p, MRE is calculated as\n$MRE(q) = \\frac{|p - p|}{p}  100$ \n(5)\nPROBLEM 1. Given a consumption matrix denoted by Ccons, generate a \u0454-DP matrix Csanitized such that average MRE subject to range queries is minimized."}, {"title": "3.3 A Simple Strategy", "content": "One simple strategy to publish the electricity consumption matrix is the Identity algorithm [7]. This algorithm was initially designed for population histograms, and works by adding independent Laplace noise to every matrix cell. When applying this technique to the consumption matrix, it is essential to note that time series have temporal correlations. As a result, every snapshot of time should have its distinct allocated privacy budget, according to the sequential composition theorem (Theorem 1). Conversely, since at each timestamp the spatial grid creates disjoint partitions of the map, parallel composition applies within each time interval (Theorem 2). The following important result emerges which quantifies the sensitivity of a query on each cell of the electricity consumption matrix."}, {"title": "4 STPT ALGORITHM", "content": ""}, {"title": "4.1 Overview", "content": "The STPT algorithm starts by generating two matrices Ccons and Cnorm out of the collected time series from different neighborhoods of the map. Ccons denotes the consumption matrix based on the actual values of the time series, whereas Cnorm is its normalized counterpart. We employ min-max normalization at a global level."}, {"title": "4.2 Pattern Recognition", "content": "The goal of the pattern recognition phase in the STPT algorithm is to effectively use a designated privacy budget, Epattern, to develop a method for privately generating approximate estimates for cells within the normalized consumption matrix. The primary means to accomplish this is through the private training of an RNN unit. The input comprises of household time series data along with their corresponding geographic locations on the map. The pattern recognition process is outlined in Section 3. It involves generating the consumption matrix Ccons from the time series data and creating Cnorm, the consumption matrix based on normalized time series."}, {"title": "4.3 Sanitization Algorithm", "content": "The output of pattern recognition is the matrix Cpattern, with dimensions Cx \u00d7 Cy \u00d7 Ct. Each element of this matrix is created using a differentially private approach. These elements are sanitized estimates of normalized time series, providing an idea of consumption patterns rather than actual consumption amounts. The purpose of the sanitization algorithm is not only to reveal these patterns but also to provide sanitized consumption values.\nThe sanitization algorithm of STPT (lines 15 to 22 in Algorithm 1) starts by developing a non-overlapping partitioning of the matrix Cpattern. The developed partitioning's objective is to group cells with similar values together. For this purpose, we use a k-quantization of matrix Cpattern to generate clusters. The formal definition of k-quantization is provided in Definition 4."}, {"title": "5 EXPERIMENTAL EVALUATION", "content": ""}, {"title": "5.1 Experimental Setup", "content": "Datasets & Spatial Distribution. We conducted our experiments using four publicly accessible datasets, each under two distinct spatial distributions, resulting in a total of eight datasets. The statistics of these datasets are illustrated in Figure 6 and detailed in Table 2 of the Appendix C.\n\u2022 CER [8]: The dataset released by the Commission for Energy Regulation (CER) in Ireland originates from the Electricity Smart Metering Customer Behavior Trials carried out between 2009 and 2010. This project involved over 5,000 households and businesses and was focused on assessing the impact of smart meters on electricity consumption patterns. The objective was to gain insights for conducting a cost-benefit analysis regarding the country-wide adoption of smart meters. The anonymized data collected from these trials has been made accessible online for public research purposes.\n\u2022 California, Michigan, and Texas Datasets [9]: The datasets serve as digital twins representing residential energy usage within each state's residential sector. They are identified by the state's acronym and concentrate on the electricity consumption of the first five counties in alphabetical order for each state. For instance, the CA dataset includes data from Alameda, Alpine, Amador Butte, and Calaveras counties. These datasets provide hourly household electricity time series data from September to December 2014.\nThe privacy concerns regarding household-level electricity consumption have limited the availability of publicly accessible datasets, with no geotagged datasets currently accessible online [10]. Therefore, to account for the distribution of users, we perform our experiments by distributing households in two settings: Uniform and Normal. A grid with granularity of 32 \u00d7 32 is overlaid on the map, and the households are distributed over the space according to one of the two distributions. The center of the normal distribution is selected randomly over the map, and the households are located with the standard deviation equal to one-third of the grid size. The experiment is repeated 10 times and the average result is shown to ensure repeatability of the experiments. Therefore, in total, the experiments are conducted on four datasets which are referred to as CA-Uniform, MI-Uniform, TX-Uniform, CER-Uniform, CA-Normal, MI-Normal, TX-Normal, and CER-Normal.\nBenchmarks. We compare the performance of our approach with the available state-of-the-art approaches detailed below.\n\u2022 FAST. The framework proposed in [2] is a widely adopted approach focused on exploiting the Kalman Filter for lowering utility loss while sanitizing time series.\n\u2022 Fourier Perturbation Algorithm. The methodology initially introduced in [11] and subsequently refined through sensitivity evaluations in [3], involves processing a time series with a specified integer k. The procedure begins by executing a Fourier transform on the time series, followed by the selection and sanitizing of the top k primary Fourier coefficients. After sanitizing these coefficients, the inverse Fourier transform is applied, and DP time series are generated. We implement the algorithm in two settings where k = 10 and k = 20, denoted in the experiments as Fourier-10 and Fourier-20, respectively.\n\u2022 Wavelet Perturbation Algorithm. By substituting the Fourier transform with the discrete Haar wavelet transform, Lyu et al. [12] introduced the wavelet perturbation algorithm for creating DP time series. This method, akin to the Fourier technique, requires an integer k, which signifies the number of coefficients to be used and sanitized. We denote this algorithm as Wavelet and implement it in two distinct scenarios: one with k = 10 and the other with k = 20."}, {"title": "5.2 Comparison with Benchmarks", "content": "Figure 4 illustrates the performance of algorithms when subjected to queries of differing shapes and sizes. Each row in the figure is dedicated to one of four datasets: CA-Uniform, CER-Uniform, CA-Normal, and CER-Normal. Within each row, the leftmost figure depicts the performance for randomly shaped and sized queries generated over the consumption matrix. The center figure shows results for smaller queries, and the rightmost figure displays the performance for larger queries. As can be seen, significant improvements have been made by STPT across the datasets in either distribution. As an example, for queries with random shapes and sizes, the STPT algorithm exhibited percentage-wise improvements of 60, 31, 54, and 32 in the Uniform setting for each respective dataset. Notably, the performance enhancement of the algorithms is more pronounced for smaller-sized queries. This result is desirable, indicating that more precise information about the consumption matrix can be conveyed with minimal loss of utility.\nAs anticipated, the IDENTITY algorithm generally shows the least accuracy among the baseline algorithms. However, it surpasses some of the more recent algorithms in scenarios where the data exhibit a more uniform shape, as seen in the first and second rows. An unexpected outcome of our experiments is the relative performance of Wavelet and Fourier transformations. Although Wavelet transformation was introduced at a later stage than the Fourier approach, the Fourier method demonstrates superior performance for queries of random shape and size.\nAnother notable observation is that, on average, all algorithms tend to perform worse with non-uniform data. This aligns with findings in [13], where a crucial determinant of performance is the homogeneity in data partitioning. Uniform data distribution contributes to higher homogeneity, and also decreases the uniformity error when estimating the size of random queries based on the sanitized partition counts."}, {"title": "5.3 STPT Detailed Evaluation", "content": "Privacy Budget. Figures 5a and 5b analyze how the allocation of privacy budget affects pattern recognition performance in the STPT algorithm. While the sanitization budget in the second step remains constant, the budget for pattern recognition varies. For enhanced clarity, the x-axis displays the amount of budget allocated to each training datapoint of the RNN unit. The y-axis, meanwhile, indicates the MAE and RMSE of the RNN unit's predictions. As anticipated, an increase in the allocated budget enhances prediction accuracy, showcasing the privacy-utility trade-off. Notably, a significant improvement is observed when the privacy budget is increased from 0.01 to 0.05, suggesting that the minimal budget required for effective training lies within this range.\nQuantization. Figure 5c illustrates the effect of the number of quantization levels on the performance of the STPT algorithm. The MRE metric is displayed on the y-axis for queries of varying shapes and sizes. Although there are fluctuations in the results, the general trend indicates that excessive increase in the number of quantization levels can negatively impact the effectiveness of STPT. This is expected, as many points in the cycle of a time series often exhibit similar values. Consequently, a high degree of quantization results in excessive partitioning and a reduction in the homogeneity that is captured in the data.\nComputational Complexity. Figure 5d presents and compares the runtime of various algorithms. According to the figure, the execution time for all algorithms is remarkably small, typically spanning just a few seconds. Although the STPT algorithm shows a slight rise in computational complexity, it is crucial to note that a significant portion of this complexity stems from the initial training phase required for pattern recognition, which is a one-time process. Overall, all algorithms demonstrate comparable execution times in the order of seconds, indicating that computational complexity does not pose a significant hurdle to their performance.\nQuad Tree Depth. The influence of varying tree depth on pattern recognition efficiency is showcased in Figures 5e and 5f. The aim is to explore how changes in tree depth affect the MAE and RMSE in the RNN unit. It's important to remember the balance between sensitivity and precision in time series produced at various depths."}, {"title": "6 RELATED WORK", "content": "Private Publication of Time Series. The existing body of work on differentially-private publication of time series falls into two primary categories: data transformation and correlation analysis. In the former category, the main strategy involves converting the data into an alternative domain that exhibits lower sensitivity, or provides a condensed representation of the time series. After sanitization in this new domain, an inverse function is used to revert the data back to its original form for publication. Notable methods in this category include the Fourier transformation [3, 11] and the discrete Haar wavelet transform [12]. The latter category focuses on enhancing the utility of DP time series publications through improved leverage of inter-data correlations. This includes the concept of Pufferfish privacy, which employs a Bayesian Network to model correlations [14]; the use of Kalman Filters to reduce utility loss as explored in [2]; and the adoption of a first-order auto-regressive process for correlation modeling as presented in [15].\nPrivate Publication of Multi-Dimensional Histograms. Our research closely aligns with the topic of publishing higher-dimensional histograms under DP. The study in [13] highlights the importance of data homogeneity in the private publication of histograms and introduces an algorithm called HTF (Homogeneous Tree Framework), designed to capture data homogeneity in order to reduce the effect of DP noise and thus improve utility. Another algorithm in this category is HDMM (High-Dimensional Matrix Mechanism) [16], which conceptualizes queries and data as vectors, and employs advanced optimization and inference methods for their resolution. DPCube [17] focuses on identifying and privately releasing dense sub-cubes. It allocates a portion of the privacy budget to derive noisy counts over a regular partitioning, which is subsequently refined into a standard kd-tree structure. The method then uses the remaining budget to acquire fresh noisy counts for the partitions, followed by an inference stage to rectify discrepancies between the two count sets. Other approaches, such as those in [18] and [19], concentrate on modifying the granularity of space to enhance the utility of data in the publication of sanitized datasets."}, {"title": "7 CONCLUSION", "content": "Our study addressed critical privacy challenges in publishing electricity consumption data, balancing protection concerns with data utility. Our proposed innovative solution, STPT, significantly improves DP-compliant data publication accuracy by integrating time series data with the spatial attributes of households. This unique approach utilizes the short-term and long-term memory capabilities of RNNs for sophisticated pattern recognition, capturing both micro and macro consumption patterns. Our extensive experiments with real-world and synthetic datasets demonstrate STPT's superior performance in maintaining high data utility while ensuring robust privacy protection, compared to existing methods. The suggested method, while primarily aimed at enhancing the utility of DP compliant publishing electricity data, is versatile and can be applied to various situations, including Wireless Sensor Networks and the publication of health-related data, offering potential for future research applications."}, {"title": "A TABLE OF NOTATIONS", "content": "Table 1 summarizes the notations used throughout the manuscript."}, {"title": "B PROOF OF THEOREMS", "content": ""}, {"title": "B.1 Proof of Theorem 4", "content": "Recall that the consumption matrix is constructed such that the time series resolution matches the time axis resolution. As a result, each matrix cell contains no more than a single data point of an individual household/user. Consequently, adding or removing a user from the data can alter the value in a matrix cell by at most max xi,t. If the time series are normalized to values between 0 and 1, then this sensitivity would be 1."}, {"title": "B.2 Proof of Theorem 5", "content": "The sequential decomposition in time is due to the correlation of time series over time. The parallel decomposition of the privacy budget over space is due to the fact that the time series of users are spatially bounded in the matrix and independent of the values in other cells."}, {"title": "B.3 Proof of Theorem 6", "content": "Consider a cell at time t corresponding to a sub-region at depth i of the tree and all users j falling in the sub-region. Let us denote the consumption of user i before and after the removal of an individual by xi,t and xit, respectively. The maximum change observed in the representative time series of the sub-region denoted by M at time t can be derived as,\n$\\frac{|\\frac{1}{4^{log_2(C_x)-i}}  \\sum_{i \\in M} x_{i,t} -  \\frac{1}{4^{log_2(C_x)-i}}  \\sum_{i \\in M} x'_{i,t}|}{|\\frac{1}{4^{log_2(C_x)-i}}|} = \\frac{|x_{j,t} - x'_{j,t}|}{4^{log_2(C_x)-i}} < \\frac{1}{4^{log_2(C_x)-i}}$\n(12)"}, {"title": "C DATASET STATISTICS", "content": "Table 2 and Figure 6 summarize the statistics of datasets used in the experiments."}, {"title": "D HYPER-PARAMETERS", "content": "The total privacy budget is set to \u20actot = 30, with pattern = 10 allocated for pattern recognition in STPT, and sanitize = 20 for sanitization. The same privacy budget is utilized across all algorithms. For training in the STPT algorithm, 100 datapoints are used, resulting in a training matrix of 32 \u00d7 32 \u00d7 100. The test involves 120 points, leading to a matrix of 32 \u00d7 32 \u00d7 120. Consequently, the published consumption matrix has dimensions of 32 \u00d7 32 \u00d7 120. The sensitivity clipping factor of the consumption matrix is provided in Table 2. The RNN unit comprises a self-attention mechanism and a GRU unit. Training was conducted over 20 epochs, with a batch size of 32. The time window is set to encompass 6 datapoints for predicting the next datapoint. The RMSProp optimizer is employed with a learning rate of 1e-3. The embedding size and hidden dimension are set to 128 and 64, respectively."}, {"title": "E HARDWARE AND SOFTWARE SPECIFICATIONS", "content": "Experiments were ran on a cluster node equipped with an 18-core Intel i9-9980XE CPU, 125 GB of memory, and two 11 GB NVIDIA GeForce RTX 2080 Ti GPUs. Furthermore, all neural network models are implemented based on PyTorch version 1.13.0 with CUDA 11.7 using Python version 3.10.8."}]}