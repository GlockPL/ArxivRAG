{"title": "SEMGRASP: Semantic Grasp Generation via Language Aligned Discretization", "authors": ["Kailin Li", "Jingbo Wang", "Lixin Yang", "Cewu Lu", "Bo Dai"], "abstract": "Generating natural human grasps necessitates consideration of not just object geometry but also semantic information. Solely depending on object shape for grasp generation confines the applications of prior methods in downstream tasks. This paper presents a novel semantic-based grasp generation method, termed SEMGRASP, which generates a static human grasp pose by incorporating semantic information into the grasp representation. We introduce a discrete representation that aligns the grasp space with semantic space, enabling the generation of grasp postures in accordance with language instructions. A Multimodal Large Language Model (MLLM) is subsequently fine-tuned, integrating object, grasp, and language within a unified semantic space. To facilitate the training of SEMGRASP, we have compiled a large-scale, grasp-text-aligned dataset named CAPGRASP, featuring about 260k detailed captions and 50k diverse grasps. Experimental findings demonstrate that SEMGRASP efficiently generates natural human grasps in alignment with linguistic intentions. Our code, models, and dataset are available publicly at: https://kailinli.github.io/SemGrasp.", "sections": [{"title": "1 Introduction", "content": "In applications such as AR/VR and embodied robotics, the ability to generate human-like grasps for a given object is of substantial value. The goal of grasping extends beyond simple object lifting; it involves alignment with human intent and preparation for subsequent manipulation tasks. Hence, relying solely on the geometric information of objects is inadequate. Combining the semantic information of the object with the description of intent enables the generation of more natural and logical grasps.\nTypical grasp representations in previous grasp generation methods exhibit constraints in embedding semantic information. For example, methods that depict grasps through robotic hand poses \u039c\u0391\u039d\u039f\n\u2020 Corresponding author."}, {"title": "2 Related Works", "content": "Grasp Generation Grasp generation remains a fundamental task with wide applications in robotics. Recently, the generation of human-like grasps has attracted increasing attention. Unlike the 6DoF (degrees of freedom) parallel-jaw grippers commonly used in robotics , the higher freedom"}, {"title": "3 Method", "content": "3.1 Overview\nGiven a specific object O, represented as a point cloud, our goal is to align the human grasp G with the associated language description L, facilitating the task of semantic grasp generation. To this end, we introduce a novel grasp generation methodology, termed SEMGRASP, which fundamentally comprises two"}, {"title": "3.2 Grasp Discretization", "content": "Consistent with prior studies , we define the grasp $G = (T, \\theta, \\beta)$ within the canonical space of the object. Here, $T \\in \\mathbb{R}^{4\\times 4}$ represents the homogeneous transformation matrix, indicating the global rotation and translation of the hand relative to the object's central coordinate system. The parameters $\\theta \\in \\mathbb{R}^{15\\times 3}$ and $\\beta\\in \\mathbb{R}^{10}$ denote the local hand pose and shape parameters, respectively. The hand vertices $H \\in \\mathbb{R}^{778\\times 3}$ are computed using a differentiable layer, specifically the MANO M model [62], where $H = M(G) = M(T, \\theta, \\beta)$.\nIn this work, to more effectively illustrate the human grasp process and align it with the semantic space, we discretize the grasp G into three components <o, m, r>, representing the orientation, manner, and refinement token, respectively, where o, m, $r\\in \\mathbb{N}$. We employ a hierarchical VQ-VAE , encompassing the trainable codebooks $B_i$, encoders $E_i$, and decoders $D_i$, where $i \\in \\{1, 2, 3\\}$, to quantize the grasp vector into meaningful integers and subsequently reconstruct the original grasp vector from the quantized tokens \nThe encoders progressively map the hand's representation into the latent space, capturing grasp information from low to high levels. This structured approach enables the simulation of the grasping process through conditional probabilities: 1) The hand's global information T is captured with the orientation"}, {"title": "3.3 Grasp Aware Language Model", "content": "Building upon the grasp discrete representation, we design a grasp-aware language model aimed at facilitating semantic grasp generation tasks. As depicted in Fig. 4, our model is trained to align three distinct modalities: the human grasp G, object points O, and the language description L.\nGrasp Modal After we train the VQ-VAE, this module is frozen. We take the encoders $E$ as the grasp tokenizer that transfers the grasp G into the <grasp> token which contains three components: <o,m, r>. To distinct the grasp token from the language, we add special tokens <SG> and <EG> to the start and end of the grasp, respectively. The grasp token, as generated by the MLLM model, can be converted back to the human grasp utilizing the VQ-VAE decoders D."}, {"title": "3.4 CAPGRASP Dataset", "content": "Currently, there exists no dataset with well-aligned grasp language annotations that would enable us to train SEMGRASP for supporting grasp generation tasks and the downstream applications. Considering the prohibitive costs and labor-intensive nature of manual semantic annotation, we design an automatic annotation methodology based on GPT-4 to augment existing hand-object interaction datasets. Our dataset, CAPGRASP, encompasses low-level, high-level, and conversational annotations.\nLow-level Annotations Low-level annotations refer to the contact rela-tionships between each finger and various parts of the object. According to the Grasp Taxonomy , we can deduce the grasp type and intent from these low-level annotations. For instance, if the thumb and index finger are in contact with a screw, it is inferred that the grasp type is a 'pinch' and the intent is to 'screw/unscrew'. The OakInk dataset provides annotations for objects' CAD models, hand vertices, and object part segmentation. Utilizing this information, we calculate the contact states when the distance between hand vertices and the object's part segmentation points is less than a threshold of 3mm.\nHigh-level Annotations High-level intent is annotated from two perspec-tives: 1) based on low-level contact information. Given this information (i.e., the finger and object part contact), we employ GPT-4 to infer the grasping intent. For example, if all fingers are grasping the handle of a mug, GPT-4 can deduce that it is a firm grasp with possible intents such as 'make a toast' or 'avoid hot beverage'. 2) Based on images or rendered views. Since the OakInk dataset includes a subset of real captured images (i.e., OakInk-image), we manually select representative frames that are clear, unobstructed, and with explicit intent. We leverage GPT-4v, a commercial visual-language model, to infer high-level information such as manipulation intent and grasp force. For grasps in OakInk without matching images (i.e., OakInk-shape), we render the image using the Blender renderer with realistic hand textures . The details of the prompt are elaborated in Appx.\nConversational Annotations With the aforementioned low-level and high-level annotations, we construct conversations using the GPT-4 model. We ask GPT-4 to generate various conversational templates from different perspectives, including detailed hand-object contact information, manipulation intent, grasp force, and type. These dialogues must be consistent with the grasp and ensure logical plausibility. The prompts that guide the GPT-4 model are detailed in Appx.\nConsidering the hallucination problem of GPT-4, to ensure the quality of our CAPGRASP, we manually review these annotations to filter out intents that defy common sense and conversations that lack logical coherence. Statisti-cally, our dataset includes approximately 1.8k object models from OakInk, about"}, {"title": "4 Metrics and Experiments", "content": "Our methodology, SEMGRASP, incorporates two principal components: the grasp discrete representation and the grasp-aware language model. We assess the reconstruction accuracy of VQ-VAE to demonstrate the validity of our grasp discretization approach. Additionally, we evaluate the performance of grasp generation by our MLLM. Comparative and ablation studies underscore the effectiveness of our methodology.\n4.1 Metrics\nAspect of Physical Plausibility To evaluate the physical plausibility of the predicted grasp pose \u011c, we employ several metrics: 1) Mean Per-Vertex Position Error (MPVPE, in mm) calculates the average L2 distance per vertex between the predicted hand mesh \u0124 and the ground truth H, when available. 2) Penetration Depth (PD, in cm) measures the maximum penetration depth of hand vertices into the object, indicating surface penetration. 3) Solid Intersection Volume (SIV, in cm\u00b3) quantifies the volumetric intersection by voxelizing the object mesh and calculating the volume within the hand surface. 4) Simulation Displacement (SD, in cm) tests grasp stability in PyBullet , measuring the object's center displacement under steady hand conditions and gravity . These metrics gauge both the quality of grasp generation and the accuracy of our grasp discrete representation.\nAspect of Semantic Consistency Semantic consistency is evaluated by examining the quality of grasp generation: 1) GPT-4 assisted evaluation. For generated grasps G, we first render the hand-object interaction following the same pipeline as in Sec. 3.4. Then, we use GPT-4v to score the semantic consistency of the grasp images based on input captions. Scores range from 0 to 100, with higher scores indicating better consistency. The prompts used in GPT-4 assisted evaluation are listed in Appx. 2) P-FID calculates the Fr\u00e9chet Inception Distance between the point clouds of the H and H, using the pre-trained feature extractor from . 3) Perceptual Score (PS) assesses the naturalness of grasps and semantic consistency, with 5 volunteers rating the generated grasps on a 5-point Likert scale. The final score is the mean Likert score.\n4.2 Implementation Details\nThe VQ-VAE's codebook B consists of K = 512 entries, each dimensioned at $d_B$ = 256. We employ PointBERT as the point cloud feature extractor in the VQ-VAE encoder $E$ for both hand vertices H and object vertices O. Similar to , PointBERT is pretrained using the ULIP-2 method for enhanced geometry-language alignment. The predicted rotation from the VQ-VAE"}, {"title": "4.3 Comparisons", "content": "Discrete VQ-VAE Grasp Representation Given the discretization of grasps, three primary concerns arise: 1) Does discretization compromise reconstruction accuracy? 2) Does it affect the physical plausibility of the interaction?, and 3) Does it have the capability to embed semantic information?\nTo answer the first two questions, we compare our method with two state-of-the-art methods, GrabNet and Jiang et al. , on a reconstruction task. Both methods are based on cVAEs for grasp generation. GrabNet employs RefineNet to refine the interaction in an end-to-end iterative manner, whereas Jiang et al. utilize test time adaptation (TTA) to optimize hand-object contact. Our method leverages a refinement token to adjust the hand pose in an end-to-end manner. Compared to configurations without the refinement token, our approach with the refinement token exhibits superior performance, achieving a 26% improvement in MPVPE and a 9% improvement in SIV. Considering the TTA, an optimization-based approach, can precisely improve hand-object interaction, we also report our method's performance with TTA in Tab. 1 for a fair comparison, which attains current SOTA results in PD and SIV. The results demonstrate that our discrete grasp representation method can accurately depict hand poses and specifically ensure the physical plausibility of interactions."}, {"title": "Language Guided Grasp Generation", "content": "We need to validate that MLLM can control grasp generation G based on textual input L. To the best of our knowledge, there are no existing works directly comparable to ours. Therefore, leveraging our discrete representation, we construct a straightforward baseline that treats this task as a classification problem. We finetune the official BERT model to embed the language description in conjunction with the object feature. Subsequently, we deploy three distinct classification heads to predict the <o, m, r> tokens. We train this modified BERT with our CAPGRASP following the same settings in SEMGRASP. These predicted tokens are then decoded into the final grasp pose as in our SEMGRASP. The outcomes of this experimental setup are documented in Tab. 2. From the results, we observe that our MLLM outperforms the baseline in both the physical plausibility and semantic consistency metrics. Showing that simply treating the task as a classification problem"}, {"title": "4.4 Ablation Studies", "content": "Ablation on Discrete Representation We conduct ablation studies to examine the design of our tokenization approach. As outlined in Sec. 3.2, we utilize three tokens\u2014orientation, manner, and refinement\u2014to represent <grasp> as <o, m, r>. In our evaluation, detailed in Tab. 3, we explore different token configurations: 1) Single Token: Compressing G into a single codebook significantly degrades reconstruction accuracy. 2) Two Tokens: This setup differs from the w/o refinement token setting in Tab. 1, as here we train the representation with only two tokens from scratch. 3) and 4) Multiple refinement Tokens: Iteratively predicting <r> and adjusting the hand pose step by step demonstrates that performance deteriorates when the number of <r> exceeds one. Moreover, we empirically find that predicting more tokens increases the complexity of MLLM training. 5) Single VQ-VAE: We train a single VQ-VAE to predict three grasp tokens simultaneously with a shared codebook. A single network struggles to encapsulate the intricate grasp representation. 6) w/o semantic. In this setting, we still use the hierarchical VQ-VAE to predict three tokens, but we do not assign semantic meaning to the tokens. This setup results in decreased performance.\nAblation on VQ-VAE settings Our investigation into the configurations of VQ-VAE focuses on two aspects: 1) Codebook B Setting: The size of trainable parameters in the codebook has a significant impact on network performance, leading to either non-convergence or underfitting (see Tab. 4). 2) Training Strategy: VQ-VAE often suffers from codebook collapse. While methods like exponential moving average (EMA) and codebook reset (Reset) are used to mitigate this (as in ), we find that these strategies weaken the representation effectiveness in the grasp representation task. Thus, we opt not to use the EMA strategy and allow each entry to be reset only once during training.\nAblation on MLLM settings We conduct ablation studies on the MLLM configurations as presented in Tab. 5: 1) Pretrained LLM: The comparison between Llama-7B and Vicuna-7B models shows Vicuna-7B as more aligned with our grasp generation needs, offering better task suitability. 2) Object size"}, {"title": "5 Applications", "content": "To demonstrate the real-world applicability of the grasps generated by SEM-GRASP, we conducted two case studies in the fields of AR/VR and robotics to show that combined with the RL-based policies, our method can synthesize dynamic grasp motions.\n5.1 Application in AR/VR\nIn the context of AR/VR, producing grasps that align with user intent and facilitate natural object manipulation is essential. We evaluated the practicality of grasps generated by SEMGRASP using the D-grasp method within the RaiSim simulated environment. D-grasp, which is based on a reinforcement learning (RL) approach, focuses on creating dynamic human grasps. It calculates the next grasp action from a static reference grasp G, the target object position $T_o$, and the current state, including the hand and object's pose and velocity, using a policy trained with the PPO algorithm.\nFor our experiments, SEMGRASP generates the reference pose G for a specified object O and language instruction L. We utilize the publicly available D-grasp checkpoints\u00b9 to synthesize dynamic grasps. The object is targeted to lift 10 cm upwards along the gravitational direction. We maintain the hand shape parameter \u03b2 at 0, consistent with D-grasp.\n5.2 Application in Embodied Robotics\nOur method's efficacy is further validated in embodied robotics. Following the UniDexGrasp\u00b2 methodology , which entails static reference grasp generation"}, {"title": "6 Conclusion", "content": "We introduce SEMGRASP, an approach aimed at generating semantic grasps from language instructions. We propose a novel grasp representation that emulates the natural human grasping process. The discretized representation is both interpretable and controllable, making it ideal for semantic space alignment. Leveraging this representation, we deploy MLLM to generate grasps from language instructions. Tailored for this task, we also present CAPGRASP, a comprehensive dataset containing grasp-text-aligned annotations. As we explore potential applications in AR/VR and embodied robotics, we are hopeful that SEMGRASP will contribute to advancements in generating more human-like, semantically coherent grasps in various contexts.\nLimitations Despite SEMGRASP demonstrating the capability to generate static single-hand grasps from semantic cues and dynamic grasps through RL integration, exploration remains in two directions: two-hand manipulation and end-to-end semantic grasp motion synthesis. The former requires addressing both hands' cooperation, and the latter, the continuity of motion, both contingent on the availability of extensive, high-quality motion capture or synthesis data for training. Tackling these challenges promises to advance embodied grasping, pushing toward more sophisticated and realistic manipulation."}, {"title": "A Experiments Details", "content": "A.1 Setting Details\nDataset Split Our dataset, CAPGRASP, builds upon and extends the OakInk dataset [81]. As such, we adopt the same split as OakInk, with 80% of the data allocated for training, 10% for validation, and 10% for testing. We ensure that the test set includes a wide variety of objects and language instructions, thereby allowing us to evaluate the generalization capabilities of our SEMGRASP.\nMLLM prompt The prompt for our grasp-aware MLLM is specified in Tab. 6, guiding the model to generate coherent and plausible grasps from language instructions.\nGPT-4 assisted evaluation We leverage the commercial GPT-4v [57] to evaluate the quality of our SemGrasp. This involves rendered images I of the generated grasps G, alongside the evaluation prompt outlined in Tab. 7 to GPT-4v. The model then returns a quality score reflecting both semantic similarity and physical reliability. To enhance the accuracy of GPT-4v, we take the following methods to improve the quality of the rendered images: 1) The grasp G is mapped onto the differentiable NIMBLE model [42], which contains delicate muscle modeling and high-fidelity hand skin textures. 2) Images are rendered in Blender using the Cycles rendering engine, complemented by random lighting and camera positioning to ensure diversity.\nPerceptual Score We ask 5 volunteers to rate the quality of the generated grasps \u011c on a 5-point Likert scale. We randomly sample 50 predicated grasps from the test set for each experiment. The evaluation indicators involve the following three aspects: 1) Semantic coherence with the provided language instructions, 2) Physical plausibility of the hand pose, and 3) Stability of the interaction between the hand and the object. The perceptual score is the average of the ratings.\nA.2 Representation Ablation Studies Details\nThis section elaborates on the ablation studies conducted to examine our discrete representation.\nSingle Token Contrary to our primary model's multi-token and hierarchical VQ-VAE structure, we explore a simplified model using a single VQ-VAE with one codebook to encapsulate the entire grasp representation.\nThe <o,m> Setting In this variant, we devise a dual-layer hierarchical VQ-VAE specifically for grasp representation that is trained from scratch. The first codebook is for the orientation and the second codebook is for the manner.\nMultiple refinement Tokens This configuration introduces a delta VQ-VAE designed to refine the grasp pose by predicting incremental refinement"}, {"title": "B Exploratory Study", "content": "In our SEMGRASP, we focus on training a grasp-aware MLLM to synchronize three distinct modalities-grasps, object models, and language instruc-"}, {"title": "C Applications Details", "content": "C.1 Physical-Plausible Dynamic Grasp using Human-like Hand\nIn our VR/AR application, we employ the open-source D-grasp method [5] to synthesize dynamic, human-like grasps. As described in the main paper, the reference pose G, corresponding to the language instruction L, is generated using our SEMGRASP. This dynamic grasp policy is then applied to assess the feasibility of the generated grasps. To ensure alignment with real-world scenarios, we rotate the hand-object grasp pair so the palm faces toward the table. Given that the OakInk dataset does not provide object weight information, we assign a hypothetical weight of 300g to each object for the purpose of this evaluation. Samples of the generated dynamic grasps are illustrated in Fig. 8. In our analysis, any relative sliding between the hand and the object exceeding 4cm is classified as a failure. Based on this criterion, we report a success rate of 62.9% for our generated grasps on the test set.\nC.2 Physical-Plausible Dynamic Grasp using ShaodowHand\nGiven the distinct morphological features and DoFs between the human hand and the ShadowHand, we devise a specialized pipeline for adapting the generated grasp G to the ShadowHand model Gs. We manually select several corresponding keypoints on both the MANO and ShadowHand models, with a particular emphasis on the fingertips. For each MANO-based grasp G, the corresponding ShadowHand grasp Gs is optimized by aligning these selected keypoints. To mitigate issues such as unnatural finger movements and potential finger collisions, we introduce a loss function that imposes angular constraints on the Shadow-Hand's joints, thereby promoting physically plausible adaptations. The outcome of this fitting process is illustrated in Fig. 9 left. Following this adaptation, the refined grasp Gs is executed using UniDexGrasp's pretrained policy. To enhance the fidelity of collision detection, object meshes are preprocessed using Manifold-plus , followed by convex decomposition algorithms . The results of these grasp executions are displayed in Fig. 9 right, showcasing the practicality and effectiveness of our methodology in the field of embodied robotics.\nD CAPGRASP collection\nPrompts As mentioned in our main paper, we craft a set of prompts to direct both GPT-4 and GPT-4v in generating high-quality annotations automatically. For high-level details concerning manipulation intent and grasp status, two specialized prompts are utilized. These are detailed in Tabs. 8 and 9, designed to annotate high-level insights based on the contact information and images respectively. Additionally, to foster the generation of conversational content, another"}, {"title": "$L_{rec} = L_o + L_o + L_r = ||M(T, \\theta, 0) - M(\\hat{T}, 0, 0)||^2 + ||M(T, 0, \\beta) - M(\\hat{T}, \\hat{\\theta}, \\hat{\\beta})||^2 + ||M(T, \\theta, \\beta) - M(\\Delta \\hat{T} \\cdot \\hat{T}, \\Delta \\hat{\\theta} + \\hat{\\theta}, \\Delta \\hat{\\beta} + \\hat{\\beta})||^3$", "content": "In the without semantic scenario, the <o> token is not exclusively constrained to represent orientation. We experimentally find that, during training, the three tokens collapse into a single token, which degrades performance."}]}