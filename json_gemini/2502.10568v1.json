{"title": "OBSERVER-AWARE PROBABILISTIC PLANNING UNDER PARTIAL OBSERVABILITY", "authors": ["Salom\u00e9 Lepers", "Vincent Thomas", "Olivier Buffet"], "abstract": "In this article, we are interested in planning problems where the agent is aware of the presence\nof an observer, and where this observer is in a partial observability situation. The agent has to\nchoose its strategy so as to optimize the information transmitted by observations. Building on\nobserver-aware Markov decision processes (OAMDPs), we propose a framework to handle this\ntype of problems and thus formalize properties such as legibility, explicability and predictability.\nThis extension of OAMDPs to partial observability can not only handle more realistic problems,\nbut also permits considering dynamic hidden variables of interest. These dynamic target variables\nallow, for instance, working with predictability, or with legibility problems where the goal might\nchange during execution. We discuss theoretical properties of PO-OAMDPs and, experimenting\nwith benchmark problems, we analyze HSVI's convergence behavior with dedicated initializations\nand study the resulting strategies.", "sections": [{"title": "1 Introduction", "content": "As explained by Klein et al. [2004], efficient and safe human-agent collaboration requires behaviors that carry\ninformation such as intentions, abilities, current status or upcoming actions (see also [Schadenberg et al., 2021,\nSingamaneni et al., 2024]). Various works in manipulation or mobile robotics try to derive behaviors with such\nproperties [Dragan and Srinivasa, 2013, Dragan et al., 2013, Fisac et al., 2020, Beetz et al., 2010, Angelopoulos\net al., 2022]. An alternative is to explicitly communicate through language with the human [Gong and Zhang, 2018].\nHere we consider an agent (robot or otherwise) observed by a passive human, as in Figure 1 (left). In this setting,\nChakraborti et al. [2019, 2018] build on previous work to derive a taxonomy of the concepts behind such information\ncommunication through the behavior. In particular, they distinguish between\n1. transmitting information, with properties such as legibility (legible behaviors convey intentions, i.e., actual\ntask at hand, via action choices), explicability (explicable behaviors conform to observers' expectations, i.e.,\nthey appear to have some purpose), and predictability (a behavior is predictable if it is easy to guess the\nend of an on-going trajectory); or\n2. hiding information, as through obfuscation, when the agent tries to hide its actual goal.\nThey propose a general framework for such problems while assuming deterministic dynamics, and work mostly with\nplans (a sequence of actions, which induces a sequence of states). In their approach, the human is modeled by the\nrobot as having a model of the robot+environment system (including the robot's possible tasks), and is thus able to\npredict the robot's behavior.\nMiura and Zilberstein [2021] build a unifying framework while assuming stochastic transitions, namely observer-\naware Markov decision processes (OAMDPs), adopting a similar approach as Chakraborti et al. [2019], as illustrated\nin Figure 1. Among other things, their work also covers legibility, explicability, and predictability. To better handle\npredictability, Lepers et al. [2024] have recently proposed an approach that does not reason with complete trajectories,\nbut with actions or states at each time step, thus being better suited to stochastic dynamics. This implies reasoning on\ndynamic target variables, which requires introducing a variant of the OAMDP formalism, namely the POAMDP\n(predictable OAMDP)."}, {"title": "2 Background", "content": null}, {"title": "2.1 Markov Decision Processes", "content": "A Markov decision process (MDP) [Bellman, 1957, Bertsekas, 2005] is specified through a tuple $(S, A, T, R, \\gamma, Sf)$\nwhere: S is a finite set of states; A is a finite set of actions; $T : S \\times A \\times S \\rightarrow [0; 1]$, the transition function, gives\nthe probability $T(s, a, s')$ that action a performed in state s will lead to state s'; $R : S \\times A \\times S \\rightarrow R$, the reward\nfunction, gives the immediate reward $R(s, a, s')$ received upon transition $(s, a, s')$; $\\gamma \\in [0, 1]$ is a discount factor;\nand $Sf \\subset S$ is a set of terminal states: for all $s, a \\in Sf \\times A, T(s, a, s) = 1$ and $R(s, a, s) = 0$.\nThen, a (stochastic) policy $\\pi : S \\rightarrow \\Delta(A)$ maps states to distributions over actions, $\\pi(a|s)$ denoting the probability\nto perform a when in s. When a policy is deterministic, $\\pi$ denotes the only possible action in s. Assuming $\\gamma < 1$,\nthe value of a policy $\\pi$ is the sum of discounted rewards over an infinite horizon:\n$V^{\\pi}(s) = E_{\\pi}[\\sum_{t=0}^{\\infty} \\gamma^t R(S_t, A_t)|S_0 = s]$,\nand an optimal policy $\\pi^*$ is such that, for all s, $V^{\\pi*}(s) = max_{\\pi} V^{\\pi} (s)$.\nThe value iteration (VI) algorithm approximates $V^*$, the value function common to all optimal policies, by iterating\nthe following computation (where k is the current iteration):\n$V_{k+1}(s) \\leftarrow max_a \\sum_{s'} T(s, a, s') \\cdot (R(s, a, s') +\\gamma V_k(s'))$."}, {"title": "2.2 Observer-Aware Markov Decision Processes", "content": "An Observer-Aware MDP (OAMDP) [Miura and Zilberstein, 2021] models a situation wherein an agent is aware of\nthe presence of an external observer, and interacts with its environment while attempting to maximize a performance\ncriterion linked to the observer's belief about the agent's \"type\", the belief about some variable being the probability\ndistribution over this variable's possible values.\nAn OAMDP is formalized by a tuple $(S, s_0, A, T, \\gamma, S_f, \\Theta, B, R_{AG})$ where:\n\u2022 $(S, s_0, A, T, \\gamma, S_f)$ is an MDP with an initial state $s_0$ but no reward function,\n\u2022 $\\Theta$ is a finite set of types representing a characteristic of the agent such as possible goals, intentions or\ncapabilities;\n\u2022 $B : H^* \\rightarrow \\Delta^{\\Theta}$ gives the assumed belief of the observer (about the agent's type) given a state-action\nhistory;\n\u2022 $R_{AG} : S \\times A \\times \\Delta^{\\Theta} \\rightarrow R$ is the agent's reward function.\nIn most of the cases they consider, Miura and Zilberstein derive B by relying on Baker et al.'s \"BST\" Bayesian belief\nupdate rule [Baker et al., 2009], i.e., considering that, again from the agent's viewpoint, the observer models the\nagent's behavior for a given type $\\theta$ through an MDP by:\n1. using a corresponding reward function $R^{OBS}_{\\theta}$;\n2. solving MDP $(S, s_0, A, T^{\\theta}, R^{OBS}_{\\theta}, \\gamma, S_f)$ (where S, A and $\\gamma$ are as in the OAMDP definition) to obtain\n$V^{*, \\theta}_{OBS}$ for states reachable from $s_0$; and\n3. building a stochastic \u201csoftmax\" policy $\\pi^{*, \\theta}_{OBS}$ such that, $\\forall (s, a)$,\n$\\pi^{*, \\theta}_{OBS}(a|s) = \\frac{e^{\\frac{Q^{*,\\theta}_{OBS}(s,a)}{\\tau}}}{\\sum_{a'} e^{\\frac{Q^{*,\\theta}_{OBS}(s,a')}{\\tau}}},$\nwhere $Q^{*, \\theta}_{OBS}(s, a) = \\sum_{s'} T^{\\theta} (s, a, s') \\cdot (R^{OBS} (s, a, s') + V^{*, \\theta}_{OBS} (s'))$, and temperature $\\tau > 0$ allows tuning\nthe policy's optimality (thus the agent's assumed rationality for the observer).\nThe observer belief about the type can thus be obtained by Bayesian inference using $\\pi^{OBS}_{\\theta}$. Note that, unless, for\nsome $\\theta$, we have $T = T^{\\theta}, R_{AG} = R^{OBS}_{\\theta}$, and $S_f = S_f^{\\theta}$, then there will likely be no perfect match between the agent's\nbehavior and any of the types. Miura and Zilberstein used this framework to formalize various observer-aware\nproblems including legibility, explicability and predictability.\nNote: As done previously, \u201cOBS\u201d is used to denote quantities associated to the observer viewpoint (as perceived\nby the agent), such as probabilities, denoted $P_{OBS}$. Also, we will sometimes write a function f(X, Y) describing a\nconditional probability distribution under the form f (Y|X) to exhibit the dependence between variables."}, {"title": "3 OAMDPS with Partial Observability", "content": "This section introduces the PO-OAMDP formalism, shows how the observer's belief about the target variable is\nmaintained, and looks at some typical use cases."}, {"title": "3.1 Formalism", "content": "We describe the key ingredients of the PO-OAMDP framework before providing a formal definition. 1. Within the\nPO-OAMDP framework, the agent has access to the complete state of the system, while the observer now only has a\npartial perception. A set of observations and an observation function are thus added to the OAMDP formalism. 2. In\nthis context, the type is replaced by a target variable which can change over time, contrary to OAMDP's static types.\nFor the definition of the target variable to be as generic as possible, its value at each time step is a function of the\ntransition followed by the system. The target variable can thus be a part of the system state (e.g., a non-observable\nvariable for the observer), but it can also be linked to the action performed by the agent (for predictability), or to the\nstate transition rather than to the state itself. This variable can also gather several different variables. But considering\na single variable is without loss of generality. 3. Additionally, we assume that the agent has access not only to the\ncomplete state of the system, but also to the observations received by the observer (this is realistic in particular if the\nobservation process is deterministic: in that case the observations received by the observer are easy to predict). The\nagent can thus build the mental state of the observer during the execution of its behavior. By having access to all of\nthe problem's information (the system state, the chosen action and the observations perceived by the observer), the\nagent can make decisions to control the observer's inference about the target variable.\nFormally, a PO-OAMDP is defined by a tuple $(S, s_0, A, T, \\gamma, S_f, \\Psi, \\Omega, O, B, R_{AG}, \\Phi)$, where:\n\u2022 $(S, s_0, A, T, \\gamma, S_f)$ is an MDP with an initial state $s_0$ but no reward function;\n\u2022 $\\Psi$ denotes both the (dynamic) target variable and the finite set of values it can take;\n\u2022 $\\phi :S\\times A\\times S \\rightarrow \\Psi$ is a function that gives the value of the target variable given the transition: $V_t =\n\\Phi(S_t, a_t, S_{t+1})$;\n\u2022 $\\Omega$ is the finite set of observations;\n\u2022 $O : A\\times S \\times \\Omega \\rightarrow R$ is the observation function; $O(a, s', o)$ is the probability of emitting observation o if\nstate s' is reached while performing a;\n\u2022 $B: \\Omega^* \\rightarrow \\Delta^{|S|}$ gives the observer's belief on the state given an observation history; the belief on the target\nvariable can be deduced from that state belief (see Sec. 3.2), denoted b;\n\u2022 $R_{AG} : S \\times \\Delta \\times A\\times S \\times \\Delta \\rightarrow R$ is the agent's reward function under its most general form:\n$R_{AG}(S_t, \\beta_t, a_t, S_{t+1}, \\beta_{t+1})$, where $\\beta$ denotes a target belief.\nHere, we assume that, through her observations, the observer knows at each time step whether a terminal state has\nbeen reached or not, without necessarily indicating which terminal state is concerned.\nUnlike the OAMDP model, which needs an MDP for each possible type, the PO-OAMDP model is based on a single\nMDP. However, within our framework with partial observability, using only one MDP is not restrictive, and, as\ndiscussed in Sec. 3.4, any OAMDP can provably be turned into an equivalent PO-OAMDP.\nThe next subsection describes how the observer's belief (on the state) can be updated and how the belief on the target\nvariable is deduced, which is used to evaluate the agent's reward attached to a transition. Then, it illustrates the use\nof the PO-OAMDP formalism to model different scenarios."}, {"title": "3.2 State- and Target-Belief Computation", "content": "BST belief state update Following Miura and Zilberstein, we employ the BST Bayesian belief update rule [Baker\net al., 2009], thus introduce a reward function $R_{OBS} : S \\times A \\times S \\rightarrow R$ assumed to be the agent's reward function\naccording to the observer. Then, the observer models the agent's behavior for a given task through an MDP by:\n1. solving the MDP with reward $R_{OBS}$; and 2. deriving a softmax policy $\\pi_{OBS}$.\nNote that, given the dynamics (transition + observation) of the PO-OAMDP and the presumed policy $\\pi_{OBS}$ of the\nagent, the observer faces a hidden Markov model (HMM) [Rabiner, 1989]: she solves a filtering problem, using the\nobservation's history $o_{1:t}$ to estimate her belief on the state $s_t$. The observer belief can thus be computed with:\n$B(s_{t+1} | o_{1:t+1}) = P(s_{t+1} | o_{1:t+1}) = \\frac{P(s_{t+1}, o_{1:t+1})}{\\sum_{s_{t+1}} P(s_{t+1}, o_{1:t+1})} = \\frac{K(s_{t+1}, o_{1:t+1})P(o_{1:t})}{\\sum_{s_{t+1}} K(s_{t+1}, o_{1:t+1})P(o_{1:t})},$\nwhere\n$K(s_{t+1}, o_{1:t+1}) \\sum_{a_t} O(o_{t+1}|a_t, s_{t+1}) \\sum_{s_t} T(s_{t+1}|s_t, a_t) \\cdot \\pi_{OBS}(a_t | s_t) B(s_t | o_{1:t}).$"}, {"title": "Belief on the target variable", "content": "To evaluate the reward received during a transition, we need to evaluate the belief\n$\\beta$ on the value that will be taken by the target value: $\\Psi_t = \\phi(S_t, A_t, S_{t+1})$. This can be done by starting with the\nobserver's belief b on the current state, $S_t:\n$\\beta(\\psi) = \\sum_{s,a,s'} 1_{\\psi=\\phi(s,a,s')} \\cdot P_{obs}(s, a, s'|b)$\n$= \\sum_{s,a,s'} 1_{\\psi=\\phi(s,a,s')} \\cdot P_{obs}(s'|s, a) \\cdot P_{obs}(a|s) \\cdot P_{obs}(s|b)$\n$= \\sum_{s,a,s'} 1_{\\psi=\\phi(s,a,s')} \\cdot T(s, a, s') \\cdot \\pi_{obs}(a|s) \\cdot b(s),$\nwhere $1$ is the indicator function."}, {"title": "3.3 Relationship with POMDPS", "content": "Despite similarities between POMDP and PO-OAMDP there are some key differences:\n1. in PO-OAMDPs, the reward function is typically not linear in belief space,\n2. in PO-OAMDPs, the agent reasons about the observer's belief rather than its own belief, so that POMDPS\nare not a subclass of PO-OAMDPs. Also, the Bellman optimality operator for PO-OAMDPS does not\npreserve piecewise linearity and convexity as in POMDPs. The optimal value function may even exhibit\nlocal discontinuities (a property inherited from OAMDPs). [salome] ajouter une ref [salome] faire le lien\navec les p POMDP"}, {"title": "3.4 Implementation on Various Scenarios", "content": "The PO-OAMDP model allows us to generate different behaviors by changing $\\Psi$ and R, and to work with different\ntypes of problems. An important property, formally demonstrated in Appendix A, shows that PO-OAMDPs are at\nleast as expressive as OAMDPs.\nProposition 1. Any OAMDP M with BST belief update can be turned into an equivalent PO-OAMDP M', i.e., such\nthat an optimal solution to one problem is optimal for the other problem.\nA starting point of the proof is to turn the static type of an OAMDP into a (hidden) target state variable. The following\nshows how to formulate legibility, explicability, and (state/action) predictability while assuming (for the sake of\nclarity) that the transition and observation functions do not depend on the type.\nLegibility Assuming several possible objectives for the agent, legibility aims at reducing the observer's uncertainty\nabout the agent's actual objective.\nThe target variable is thus part of the state, indicating the objective among a finite set of possible objectives, and\nthe observer reward function $R_{OBS}$ depends on the target. For the agent reward function, Miura and Zilberstein\nuse the opposite of the Euclidean distance to the \"ideal belief\u201d. The ideal belief being defined by: $\\beta^*(s) =$\n(0, . . ., 0, 1, 0, . . ., 0) (with a 1 for component $\\psi = \\phi(s)$), we thus have, for $R_{AG}:\nR_{leg}(s, \\beta, a, s', \\beta') \\triangleq -\\sqrt{||\\beta - \\beta^*(s)||^2}.$\nExplicability Assuming one or several possible objectives, an explicable behavior is a behavior coherent with the\nobserver's expectations.\nTo express this idea, Miura and Zilberstein (following Sreedharan et al. [2020]) propose minimizing the probability\nthat the observed behavior corresponds to a random behavior, even if multiple behaviors are still likely. As they do,\nwe thus introduce a \u201cvirtual\u201d target value $\\psi_0$ (in addition to the ones used for legibility) that corresponds to a random\nbehavior (policy) in addition to the other (real) target values. Then, the explicability criterion described above is\nobtained using\nR_{exp}(s, \\beta, a, s', \\beta') \\triangleq -\\beta(\\psi_\\infty).\nPredictability A predictable behavior is typically a behavior whose end of trajectory is easy for the observer to\nguess. Miura and Zilberstein's discussion on predictability, which relies on work for deterministic settings and\nthus reasons on complete trajectories, does not provide a very convenient way of formalizing predictability under\nstochastic dynamics. We rely instead on Lepers et al.'s work [Lepers et al., 2024], as they propose a more satisfying\napproach relying on step-by-step predictions."}, {"title": null, "content": "The starting point is that the observer tries, at each time step, to predict either the next action, or the next state,\nhence two different types of predictability. For action predictability, we set $\\Psi = A$ and $\\phi(s, a, s') = a$. For state\npredictability, we set $\\Psi = S$ and $\\phi(s, a, s') = s'$. In both cases, to act optimally, the observer has to bet on the most\nprobable next target values, and thus pick a value in the set\n$\\psi_{\\psi} (\\beta_t) \\triangleq arg max_{\\psi} \\beta_t (\\psi)$.\nConsidering that the observer samples her prediction uniformly at random in the set $\\psi_{\\psi} (\\beta_t)$, the probability that $\\psi$ is\npredicted is:\n$pred(\\psi|\\beta_t) \\triangleq \\frac{1}{|\\psi_{\\psi} (\\beta_t)|} \\cdot 1_{\\psi \\in \\psi_{\\psi} (\\beta_t)}.$ Then, defining\n$R_{a-pred}(s, \\beta, a, s', \\beta') \\triangleq pred(a|\\beta) - 1, or$\n$R_{s-pred}(s, \\beta, a, s', \\beta') \\triangleq pred(s'|\\beta) - 1,$\nthe immediate reward is the opposite of the probability that, under the current transition, the bet of a rational observer\nwill fail: $R_{.-pred}(s, \\beta, a, s', \\beta') = -P(failed rational bet)$.\nNote: Other example scenarios formalized as PO-OAMDPs are presented in supplementary material, Appendix C.\nThey illustrate, among other things, the similarities with p-POMDPS [Araya-L\u00f3pez et al., 2010], a variant of the\nPOMDP formalism where an agent's reward function depends on its own belief, which permits modeling active\ninformation-gathering problems. Yet, the same differences between OAMDPs and p-POMDPs pointed out by Miura\nand Zilberstein [Miura and Zilberstein, 2021] still hold between PO-OAMDPS and p-POMDPS."}, {"title": "4 Resolution", "content": null}, {"title": "4.1 Sequential Decision-Making Problem", "content": "An OAMDP can be turned into an equivalent MDP using the state-action history $(s_{0:t}, A_{1:t})$ (i.e., all the raw\ninformation available to the agent at t) as information state, or the state-belief (over type) pair $(s, \\beta)$ when using the\nBST update [Miura and Zilberstein, 2021, Miura et al., 2024].\nSimilarly, in a PO-OAMDP, the raw information available at t is the state-action-observation history $(s_{0:t}, A_{1:t}, o_{1:t})$.\nYet, note that:\n(1) the pair $(s_t, o_{1:t})$ induces a Markov process; and\n(2) the observer's beliefs ($b_t$, thus also $\\beta_t$) depend on the observation history $o_{1:t} = (o_1, ..., o_t)$;\n(3) the reward is a function of the state and the target belief, thus of the observation history, not of the past\nstates and actions.\nFrom (1) and (3), the state and observation history pair $(s_t, o_{1:t})$ is a sufficient statistic for optimal decision-making.\nWhat is more, when using the BST update, the state belief is Markovian (though not the target belief in general), so\nthat $(s_t, b_t)$ can be used instead.\nFormally, we obtain an MDP $(I, i_0, A, T', R', \\gamma, I_f)$, where:\n\u2022$I \\equiv S \\times B$ is the (infinite) set of states, with $i_0 = (s_0, b_0)$ the initial state;\n\u2022 A is the PO-OAMDP's set of actions;\n\u2022 $T' : I \\times A \\times I \\rightarrow [0; 1]$ is the transition function defined by:\n$T'(i'|i, a) \\triangleq Pr(s', b'|s,b,a) = \\sum_{o} 1_{i'=B(b,o)}O(o|s', a)P(s'|a, s);$\n\u2022 $R' : I \\times A \\times I \\rightarrow R$ is the reward function defined by:\n$R'(s, b, a, s', b') \\equiv R_{AG}(s, \\beta(b), a, s', \\beta(b')),$\nwhere $\\beta(b)$ is the target belief that can be derived from b as seen in Equation (1);\n\u2022 $\\gamma\\in [0, 1]$ is the discount factor; and"}, {"title": null, "content": "\u2022 $I_f \\subset I$ is the set of elements $(s, b)$ in I such that $s \\in S_f$.\nWe assume that $R_{AG}(s, \\beta(b), a, s', \\beta(b')) = 0$ whenever $s \\in S_f$. As a consequence, when $i = (s, b) \\in I_f$ is reached,\nsince the state s does not change anymore, and even if the state belief may still evolve, all future rewards will be null,\nso that we are in a \"terminal sub-set of states\".\nIn this setting, Bellman's optimality operator is thus written\n$V^*(i) = max_a \\sum_{i' \\in nxt(i,a)} T' (i, a, i') \\cdot [R' (i, a, i') + \\gamma V^*(i')],$\nwhere nxt(i, a) is the (finite) set of possible next state-belief pairs when performing a in i."}, {"title": "4.2 SSPs", "content": "Setting $\\gamma = 1$ raises the question whether the resulting problem is a valid SSP. The following proposition answers\npositively while considering problems with a possibly infinite set of states reachable from initial state $(s_0, -)$, where\n- denotes the empty history.\nProposition 2. Assuming that $R_{AG}$ is bounded from above by $R^{max} < 0$ (in non-terminal states), the PO-OASSP is\na valid SSP.\nProof. First, any reachable pair $(s, o_{1:t})$ with $s \\in S_f$ is a terminal state of the PO-OASSP.\nThen, let $\\hat{\\pi}$ be a proper policy of the observer SSP. When in $(s, o_{1:t})$, one can apply $\\hat{\\pi}$ (thus ignoring observation\nhistories), thus ensuring that a terminal state of the SSP is reached, which corresponds to a terminal state of the\nPO-OASSP.\nIn the contrary, if, from $(s, o_{1:t})$, one applies a policy $\\pi$ that reaches a terminal state only with probability $p < 1$,\nthen there is a probability 1 \u2212 p to follow an infinite trajectory with a per-step cost $R^{max} < 0$, so that the value at\n$(s, o_{1:t})$ diverges to \u2212\u221e.\nNote that ensuring that $R_{AG}$ only takes negative values is not sufficient to prove the above lemma, as not all\ninfinite sums of negative values diverge. For the $R_{AG}$ functions described in Sec. 3 for legibility, explicability and\npredictability, the least upper-bound is 0, so that it is unclear whether all improper policies have diverging values.\nIn particular, Lepers et al.'s Proposition 2 in [Lepers et al., 2024], which applies in our setting, states that state\npredictability can lead to an improper policy. A simple trick to come back to a valid SSP is to linearly combine the\ninvalid $R_{AG}$ with a valid $R : S \\times A \\times S \\rightarrow R$ using $R'_{AG} = R_{AG} + \\lambda \\cdot R$ for some small $\\lambda > 0$."}, {"title": "4.3 Complexity", "content": "Proposition 1 tells us that PO-OAMDPS cover a larger class of problems than OAMDPs. Below we establish that\nPO-OAMDPs inherit the same main complexity results as OAMDPs, results which require assuming Bayesian\nupdates for the observer's belief, what we denote by PO-OAMDPBU. Such results are obtained considering the\nvalue problem, i.e., determining whether a policy exists that can achieve some pre-defined value.\nTheorem 3. The finite-horizon value problem for PO-OAMDP BU \u0130S PSPACE as long as R can be evaluated using\npolynomial space.\nProof. As for OAMDPBUS, a policy's possible outcomes can be expressed as a tree whose depth corresponds to the\nfinite horizon, and the policy's value can be computed in polynomial space through a tree traversal (provided R can\nbe evaluated in polynomial space as well). PO-OAMDPBUS are thus in PSPACE.\nTheorem 4. The finite-horizon value problem for PO-OAMDP BU is PSPACE-hard.\nProof. The proof of Prop. 1 shows that any OAMDPBU can be turned into a PO-OAMDPBU through a polynomial\nreduction. Then, as OAMDPBU is PSPACE-hard [Miura and Zilberstein, 2021], so is PO-OAMDP BU.\nCorollary 5. The finite-horizon value problem for PO-OAMDP BU is PSPACE-complete when R can be evaluated\nusing polynomial space.\nThis is a direct consequence of Theorems 3 and 4."}, {"title": "4.4 HSVI", "content": "This section proposes solving discounted PO-OAMDPs ($\\gamma < 1$) using a variant of Smith and Simmons's heuristic\nsearch value iteration (HSVI) algorithm [Smith and Simmons", "2007": ".", "3.2": "so that we do not attempt to use generalizing\nrepresentations (which typically rely on continuity properties)", "R_{\u266d}": "R'(s,b,a, s', b') = R_s(s, a, s') + R_\u266d(s, b, a, s, b'),$\nand introduce so-called combined initializations.\nOur decomposition $R' = R_s + R_\u266d$ allows lower-bounding $R'(i, a, i')$ with $R_s(s,a, s') + R^{min}_{\u266d}$, where $R^{min}_{\u266d} =$\n$mini,a,i' R_s(i, a, i')$, so that $V^*(s, b)$ could be lower-bounded by $V^{\\pi}(s) + R^{min}_{\u266d}/(1 - \\gamma)$, with $\\pi$ some policy, for\ninstance the solution $\\pi$ of the MDP equipped with $R_s$. But this lower bound can again be very loose when $\\gamma$ is close\nto 1. To avoid the $\\frac{1}{1-\\gamma}$ term, we can work"}]}