{"title": "TABDPT: SCALING TABULAR FOUNDATION MODELS", "authors": ["Junwei Ma", "Valentin Thomas", "Rasa Hosseinzadeh", "Hamidreza Kamkari", "Alex Labach", "Jesse C. Cresswell", "Keyvan Golestan", "Guangwei Yu", "Maksims Volkovs", "Anthony L. Caterini"], "abstract": "The challenges faced by neural networks on tabular data are well-documented and have hampered the progress of tabular foundation models. Techniques leveraging in-context learning (ICL) have shown promise here, allowing for dynamic adaptation to unseen data. ICL can provide predictions for entirely new datasets without further training or hyperparameter tuning, therefore providing very fast inference when encountering a novel task. However, scaling ICL for tabular data remains an issue: approaches based on large language models cannot efficiently process numeric tables, and tabular-specific techniques have not been able to effectively harness the power of real data to improve performance and generalization. We are able to overcome these challenges by training tabular-specific ICL-based architectures on real data with self-supervised learning and retrieval, combining the best of both worlds. Our resulting model \u2013 the Tabular Discriminative Pre-trained Transformer (TabDPT) \u2013 achieves state-of-the-art performance on the CC18 (classification) and CTR23 (regression) benchmarks with no task-specific fine-tuning, demonstrating the adapatability and speed of ICL once the model is pre-trained. TabDPT also demonstrates strong scaling as both model size and amount of available data increase, pointing towards future improvements simply through the curation of larger tabular pre-training datasets and training larger models.", "sections": [{"title": "INTRODUCTION", "content": "Tabular foundation models (TFMs) have recently emerged as a critical area of research (van Breugel & van der Schaar, 2024) given the importance of tabular data in real-world applications. However, the high heterogeneity of tables, low availability of high quality data, and the lack of obvious inductive bias have made it especially challenging to adapt neural architectures to tabular data (Grinsztajn et al., 2022; McElfresh et al., 2023). Consequently, deep learning techniques and TFMS have not been established as the standard for solving discriminative tabular tasks, with tree-based frameworks such as XGBoost (Chen & Guestrin, 2016) or CatBoost (Prokhorenkova et al., 2018) remaining the default. These approaches have demonstrated the practical ability to more gracefully handle the idiosyncrasies of tabular data, although they require costly rounds of training and hyperparameter tuning on each new dataset to achieve good results. Indeed, it is unlikely that tree-based models will ever provide training-free generalization to unseen data \u2013 which we have grown to expect of foundation models in other domains \u2013 and as such we continue to pursue neural approaches, despite the current challenges."}, {"title": "RELATED WORK", "content": "Tabular Foundation Models (TFMs) Although TFMs lag behind foundation models in other domains (van Breugel & van der Schaar, 2024), a variety of attempts with different base architectures have emerged. Most similar to ours is TabPFN (Hollmann et al., 2023); TabDPT's architecture relies heavily on this model \u2013 albeit with a separate regression head and a retrieval component \u2013 but uses a completely different self-supervised pre-training procedure with real data. Both TabDPT and\nIn-context learning (ICL) \u2013 referring to the phenomenon where a model generalizes to new tasks using only in-context template examples with no additional fine-tuning \u2013 is one avenue showing promise in building neural networks that can dynamically adapt to input data. ICL was first observed in large language models (LLMs) (Brown et al., 2020), which have even demonstrated some ability to perform inference on smaller tabular datasets (Han et al., 2024; Gardner et al., 2024). Since tables are not text, though, it is challenging to apply LLMs to tabular data. The cell-based, textual tokenization in particular is highly inefficient and makes context size a major limitation (Fang et al., 2024). This has hindered the adoption of LLM-based ICL techniques in practical tabular settings. An alternative technique directly trained to perform ICL is the transformer-based TabPFN (Hollmann et al., 2023), designed specifically for tabular data. TabPFN is pre-trained exclusively on synthetic data (M\u00fcller et al., 2022) and is able to more efficiently use its context by avoiding cell-based tokenization: instead, the rows essentially act as tokens. While the performance of TabPFN is impressive (McElfresh et al., 2023), especially since the lack of task-specific fine-tuning greatly speeds up inference time, the lack of training on real data leaves something to be desired: we conjecture that the synthetic data generation procedure used in training is not sufficiently diverse, and improving it is a highly non-trivial task. Furthermore, it cannot natively perform regression, and struggles as dataset size increases, greatly limiting its potential as a TFM.\nGiven the adaptability and efficiency of ICL, we would like to scale it to both handle larger datasets and benefit from real pre-training data. The former can be accomplished using retrieval-based training for more efficient use of the context; Thomas et al. (2024) showed that this can improve performance when fine-tuning on specific tasks, and we demonstrate here that it can also be adapted to the pre-training phase. As for the latter, we can turn to self-supervised learning (SSL) techniques to augment the relatively low amount of quality pre-training tabular data that is publicly available. Specifically, we perform random column prediction to enhance the amount of training data, analogously to what has been done in language (Devlin et al., 2019) and vision (He et al., 2022). Combining a transformer-based ICL with retrieval-based SSL results in our method \u2013 the Tabular Discriminative Pre-trained Transformer (TabDPT) \u2013 which demonstrates impressive performance even on brand new tabular tasks. We summarize our contributions below:\n1. We introduce TabDPT as a TFM that performs both classification and regression on unseen datasets with no additional training or hyperparameter tuning, backed by transformer-based ICL, retrieval-based self-supervised pre-training, and retrieval-based inference.\n2. We comprehensively evaluate TabDPT on the OpenML-CC18 (Bischl et al., 2021) and OpenML-CTR23 (Fischer et al., 2023) benchmarks, showing state-of-the-art performance on unseen datasets even when compared with methods that train on that data with 30 rounds of per-dataset hyperparameter tuning. Our runtime is therefore also much lower than these baselines once we have a pre-trained model.\n3. As there is no single accepted benchmark in the tabular domain, we introduce the idea of using duel-based ranking methods (Elo, 1967; Glickman, 2012) to evaluate the relative performance of models even when pairwise comparison across all datasets is unavailable, mimicking similar developments in LLMs (Chiang et al., 2024).\n4. We show that the performance of TabDPT scales with both model size and amount of training data, with Figure 1 in particular demonstrating the power of pre-training with real data.\n5. We will release all code, which includes the weights of the trained TabDPT,\u00b9 methods for training TabDPT, a comprehensive evaluation suite, and a library for detecting leakage in tabular datasets which confirms that we are not training on downstream data.\u00b2"}, {"title": "METHOD", "content": "In this section, we outline the architecture of our model, TabDPT, along with the self-supervised learning and retrieval strategies we employ that are key for model performance."}, {"title": "TRANSFORMER ENCODER FOR IN-CONTEXT LEARNING ON TABULAR DATA", "content": "Our main goal in this work is to understand how to build tabular foundation models that will scale with model size and amount of data. First, we focus on the architecture. We have found the backbone of TabPFN (Hollmann et al., 2023) to be suitable: it is a non-autoregressive transformer encoder wherein entire rows of incoming tabular data can attend to each other and thus play the role of \"tokens\". More precisely, for every input table with N rows and F features, we first standardize the feature dimension to a fixed size $F_{max}$, achieved by either padding with zeros or subsampling features. The table is then embedded into a tensor of shape (N, d), where d represents the transformer's hidden dimension, via a linear layer. We do not handle categorical or numerical variables differently, with more details on that in Section 3.2. Subsequently, in the transformer layers, we treat the row dimension N as the sequence length so that individual instances can attend to each other.\nA row-based tabular encoding contrasts with that of LLMs which require tokenization of each cell in the input data, inflating the memory requirements by a factor of F \u00d7 (Ntok), where \u3008Ntok\u3009 is the average number of tokens per cell. Even with techniques such as sparse attention (Child et al., 2019) and Byte Pair Encoding (Gage, 1994), the overhead remains significant, limiting the table size that can be processed. A similar phenomenon is observed in the image domain when comparing pixel-based transformers (Chen et al., 2020) to ViTs (Dosovitskiy et al., 2021) which use coherent numerical patches of images for embedding, resulting in more efficient models. Similarly, we argue that tables should be divided into structurally meaningful units, such as rows, for more efficient processing.\nBy reducing memory consumption, row-based encoding permits processing a large number of rows, which in turn enables efficient ICL on new tables. In contrast, cell-based methods (Huang et al., 2020; Gorishniy et al., 2024; van Breugel et al., 2024) are limited in the number of rows they can\nThe final point to discuss on architecture is our approach for training both classification and regression with the same backbone, which is also the biggest change from the architecture of Hollmann et al. (2023). For this, we attach two heads after the transformer layers, each consisting of two-layer MLPs. For classification, the outputs are logits of a predetermined maximum number of classes, trained using the cross-entropy loss. For regression, the output is a scalar and we simply train using the mean-squared error (MSE) loss. We investigated recasting regression as classification, as in previous work (Imani et al., 2024; Farebrother et al., 2024), but MSE was more effective for us. The full architecture is depicted in Figure 2b, with additional details provided in Appendix H."}, {"title": "SELF-SUPERVISED LEARNING ON TABULAR DATA", "content": "Although most of the datasets we use for training are labelled datasets containing pairs of input data X and the corresponding targets y, we use a purely self-supervised approach that does not treat y differently from any other feature of X; we treat all datasets as unlabeled. We do so to increase the number of relationships between the features that we can learn. We take inspiration from the masked modelling objectives popularized in vision (Germain et al., 2015; He et al., 2022) and language (Devlin et al., 2019); namely, we aim to predict one feature from a random subset of the other features. In more detail, this involves two complementary steps.\nRandom Column as Target We randomly select a column from the tabular dataset that satisfies specific criteria, such as having a sufficient number of unique values, and treat it as a target for either classification or regression tasks. This allows the model to learn useful representations from the data without relying on external labels. For regression, we simply standardize the values of that column. For classification, if the number of unique values is high, we distribute the values over random partitions and use those as target classes.\nColumn Shuffling and Masking We also shuffle the order of columns, and drop some, to encourage learning robust relationships between features independent of their positional arrangement, improving the model's ability to generalize to new datasets with different feature arrangements.\nBy combining these self-supervised learning techniques with our transformer architecture, we create a model that is not only robust and scalable, but also capable of learning from all features of any dataset. This allows us to learn efficiently from a limited number of training datasets, whereas only learning from the supervised target would not contain enough learning signal for the model (cf. Figure 4b). Detailed pseudo-code can be found in Code Block 1 and Code Block 2 in the Appendix."}, {"title": "TRAINING WITH END-TO-END RETRIEVAL", "content": "One of the primary challenges in training in-context transformer-based models for tabular data is the quadratic growth of compute and memory usage with the context length. This limitation restricts the number of support examples that can be effectively utilized within the context window. While language-based models (Gardner et al., 2024) or TabPFN (Hollmann et al., 2023; McElfresh et al., 2023) can handle small datasets where the entire training set can fit within the context, their scalability to larger, more complex datasets is limited. This raises the question of how to efficiently select and use context when dealing with larger datasets.\nRecently Thomas et al. (2024) and Xu et al. (2024) showed that using a dynamic context local to each query point greatly improves the performance and scalability of TabPFN at inference. We hypothesized that training our model end-to-end on local context would improve the downstream performance even further as it results in a better alignment between training and testing objectives.\nThis objective is similar to works such as RETRO (Borgeaud et al., 2022) or TabR (Gorishniy et al., 2024) which have shown improved results by using retrieval during training. However, performing an exact kNN search for each point in the minibatch during training is expensive. The main cost is not from the search itself, but rather the fact that now each query point has its own unique context, thus increasing GPU memory requirements. Instead, we use the approximate retrieval technique presented by Thomas et al. (2024) where local groups of points are sampled and randomly split into a context vector and a query vector, thus allowing context sharing between points while being more efficient. During inference, we perform one kNN search per query point, with details in Figure 5.\nWe retrieve neighbours based on a simple distance (L2 or dot-product) in the normalized original feature space as done by Thomas et al. (2024). There are three main reasons for this: 1) we cannot simply learn good table embeddings \u2013 the meaning of values and features is extremely table-dependent, and thus such embeddings would likely need to be learned in-context, defeating the purpose of retrieval in the first place; 2) contrary to applications like RAG (Lewis et al., 2020) where only a few samples are retrieved, we retrieve up to 1,024 samples during training, which increases our probability of sampling at least a few relevant points; and 3) even in LLMs, sparse methods like BM25 that are closer to data space are competitive (Nogueira & Cho, 2019). We show in Figure 4b that this technique improves the performance of our model compared to retrieval limited to inference time."}, {"title": "INFERENCE STRATEGIES", "content": "Recall from Section 3.1 that our architecture needs a pre-defined maximum number of features Fmax and classes Cmax. We discuss how to overcome these limitations with inference-time techniques.\nFeatures When the number of features in a table exceeds Fmax, we reduce the dimensionality of the table using Principal Component Analysis (PCA) to Fmax.\nClasses If a dataset contains C classes with C > Cmax, we cannot perform classification in a single forward pass. While we could do binary classification in a one-versus-all fashion, this would require C forward passes which may drastically impact the inference speed of our algorithm; some datasets have hundreds of classes. A much more computationally efficient idea is to write C in base Cmax and predict each base-Cmax digits separately. This then requires only $log_{Cmax}(C)$ forward passes, which is very efficient. For instance, if we only train using $Cmax$ = 10 classes and have to predict on C\u2264 100, this requires at most two forward passes: one for each digit of the class to predict."}, {"title": "DATA", "content": "Our training data was collected from OpenML (Vanschoren et al., 2014) and consists of a wide range of public tabular datasets across numerous domains. To find appropriate datasets, we considered the datasets specified in the Grinsztajn et al. (2022), TabZilla (McElfresh et al., 2023), and AMLB (Gijsbers et al., 2024) benchmarks as well as additional datasets found individually. In total, our training data contained 123 datasets, with a total of 32M rows and 2B cells (individual values within each table). 93 datasets had classification targets, 29 datasets had regression targets, and 1 did not have a default target defined; however, we do generate both classification and regression targets from each dataset with our self-supervised approach during training. The complete list of training datasets is provided in Appendix G.\nComparison with TabLib and Tabula-8B Our training data includes orders of magnitude fewer tables compared to Tabula-8B (Gardner et al., 2024), a tabular model based on LLama 3-8B (Dubey et al., 2024) and trained on data from TabLib (Eggert et al., 2023) that sources tables from GitHub and CommonCrawl. However, in the end, Tabula-8B is trained on 8B tokens mostly from very small tables. We can estimate this to represent between 400M and 8B cells (using the fact that 8 values are encoded into 167 tokens from their Figure 2 due to the verbosity of the encoding mechanism), which is within the same order of magnitude as our training data."}, {"title": "EVALUATION DATA", "content": "For our evaluation, we consider two public benchmarks: CC18 (Bischl et al., 2021) for classification tasks and CTR23 (Fischer et al., 2023) for regression tasks.\nCC18 is a curated suite of 72 datasets with classification targets originally sourced from OpenML. These datasets each have between 500 and 100,000 instances, less than 5,000 features, and originate from diverse domains such as finance, biology, games, banking, industrial applications, or natural signals such as vision or sound. Datasets were selected according to curation criteria that included avoiding synthetic data, requiring source information, and removing datasets where a simple algorithm achieved 100% accuracy. CC18 is a widely used benchmark for evaluating tabular learning (Bahri et al., 2022; Hollmann et al., 2023; McElfresh et al., 2023).\nCTR23 is a benchmark suite of 35 datasets also curated from OpenML. It follows most of the design choices of CC18 but contains regression rather than classification tasks. In particular, it uses the same restrictions on number of samples and features as CC18, but replaces the accuracy restriction with a requirement that a linear model must not achieve $R^2$ = 1 on the selected datasets."}, {"title": "CONTAMINATION ANALYSIS", "content": "To ensure that the datasets used for training did not contain any information about the evaluation data, we extracted a range of metadata from each dataset and compared them across all pairs of training and evaluation datasets. This includes: i) dataset names, ii) hashes of dataset files, iii) numbers of columns and rows, iv) target mean and variance, v) mean, variance, skew, and kurtosis of each feature, and vi) coefficients of a univariate linear fit between each feature and the target if available.\nTo allow for efficient pairwise comparisons between all features in all datasets, we use k-d trees (Bentley, 1975) constructed for each dataset that contain the feature statistics. Any pairs of datasets with unusual similarities detected were manually evaluated and removed from training if they were found to be related. Since this procedure is primarily based on automated checks, it can be used in the future to further scale our training data."}, {"title": "EXPERIMENTS", "content": "In this section, we evaluate TabDPT against tuned baselines on different benchmarks, and then provide a detailed analysis of TabDPT by observing its scaling properties, reporting the runtime, and ablating key components."}, {"title": "EVALUATION", "content": "Benchmark Suites First we compare our method against tuned, competitive baselines including tree-based methods such as XGBoost (Chen & Guestrin, 2016) and CatBoost (Prokhorenkova et al., 2018), strong deep learning baselines such as TabR (Gorishniy et al., 2024) and MLP-PLR (Gorishniy et al., 2022), as well as kNN (Fix, 1985). We further use McElfresh et al. (2023)'s csv file containing the performance of a large number of algorithms per hyperparameter, split, and dataset. We obtain results for XGBoost, CatBoost, LightGBM, and MLP from this file. Our protocol is the following: if we have access to the hyperparameter optimization (HPO) search from McElfresh et al. (2023), we use those numbers. However, for algorithm and dataset combinations that took 5+ hours to train, the csv entry is missing. For those cases specifically we compute the performance with default hyperparameters. For CTR23, we run a HPO search with search space similar to the TabZilla protocol for XGBoost, CatBoost, and LightGBM, using the code repository from Gorishniy et al. (2024). For TabR, MLP-PLR, and kNN, we also use that repository, with the predefined search space and 30 rounds for both CC18 and CTR23.\nWe choose the best hyperparameters for each dataset fold individually based on the validation performance. In addition, we compare to other ICL baselines including TabPFN (Hollmann et al., 2023), and TabPFN (kNN) (Thomas et al., 2024) which retrieves neighbours of each query at inference time. We also introduce PFN++, our improved TabPFN implementation that additionally performs regression. PFN++ has the same architecture and training procedure as TabDPT but it uses the same synthetic data generator as Hollmann et al. (2023) for training. PFN++ (kNN) also includes retrieval at test time similar to TabPFN (kNN). Details of PFN++ can be found in Appendix I.1.\nFinally, we run all methods on at least two different splits of the data and report 95% confidence intervals using bootstrapping, following the recommendations of Agarwal et al. (2021). Our model, TabDPT, is a 78M parameter model with 16 transformer layers pre-trained for 600K steps. All model training and inference can be done on a single Nvidia A100 GPU with 40 GB of memory.\nWe observe in Table 1 that TabDPT performs competitively with all the hyperparameter-tuned baselines on both classification and regression, using only forward passes and no further tuning.\nWin-Rate and Comparison with Tabula-8B In order to compare with Tabula-8B (Gardner et al., 2024), we gather the results they have on a subset of 61 datasets from CC18 for three models: Tabula-8B with 32-sample context length, Tabula-8B zero shot, and the random baseline. As they only report accuracy, we compute the win-rate for each pair of algorithms by assigning an algorithm"}, {"title": "SCALING LAWS FOR TABULAR DATA", "content": "To the best of our knowledge, this work presents the first analysis of scaling laws for tabular foundation models. We observe how performance changes when systematically varying model size by adjusting the number of layers and transformer dimensions, as well as the amount of training data. Our models range from 33K to 78M parameters, trained on data subsets spanning from 52M cells (104K rows) to 2B cells (32M rows). For PFN++, we limit the variation to model size, keeping the prior-generating function fixed. Following Hoffmann et al. (2022), we adopt the joint power-law model $l(P, D) = A/P^\u03b1 + B/D^\u03b2 + E$, where l represents the estimated loss (or another target metric), P denotes the number of parameters, and D the number of tokens, or in our case the number of cells in the entire training set. Notably, although we use row-based encodings, not all rows affect the model (especially the encoder layer) equally, and thus cell count is a better measure for dataset size. We use the improved methodology by Besiroglu et al. (2024) to estimate the parameters A, B, \u03b1, \u03b2, and E. For the scaling exponents in particular, we find a = 0.42 and \u03b2 = 0.39, which are within the expected range and are very close to each other, mirroring Hoffmann et al. (2022)'s observation.\nIn Figure 1, we illustrate the scaling behavior of our models along with the power-law fit. Since we train on both classification and regression tasks, with roughly 50% of the samples in each category,\nthe loss on the y-axis represents the average of the cross-entropy loss for classification and 1 \u2013 p for regression, where p is the correlation between the prediction and true target, equivalent to MSE for normalized vectors. Note that for visualization purposes we report, on a log-scale, the excess loss $l(P, D) \u2013 E$ (the estimated loss de-biased by E) instead of the raw loss. The empirical values (P, D, l(P, D)) are reported alongside the power-law fit.\nWe also provide the empirical values for models using the TabPFN prior (Hollmann et al., 2023), shown in green. These models are not fit to the joint power-law model, as the data size D is not known a priori. However, we can estimate the number of rows or cells seen during training, which totals to approximately 17B rows and 860B cells for all model sizes. As shown in Figure 1, the quality of the data \u2013 whether real or synthetic - affects both the shape of the loss curve and the terminal loss. We hypothesize that the synthetic data generated by TabPFN contains many of the \"easy\" patterns present in real-world data, but not all. This is supported by smaller models outperforming ones trained on real data of up to 2B cells, while larger models trained on TabPFN data perform comparably to models trained on 300-600M real cells.\nFrom Figure 1, we observe that as the models have more parameters, their performance becomes more predictable. However, for larger models on smaller amounts of data, the loss is greater than predicted (and in some cases unstable, see Appendix F), which could indicate signs of overfitting. Additionally, for the joint classification and regression loss we observe a behaviour predictable by power-law models, but neither classification nor regression alone is explained quite as well. In Appendix F, we discuss the scaling analysis in more detail.\nOur analysis suggests an important insight: although neural network-based methods have struggled with tabular data for a long time, performance continues to improve as model size and data quantity increase, much like text and image data."}, {"title": "TRAINING AND INFERENCE SPEED", "content": "In Figure 4a, we approximate the speed of each algorithm by computing the median time it takes to perform a full training and evaluation \u2013 including HPO search on datasets with size larger than 10,000 rows from CC18. From this, we compute the median time to process 1,000 rows, along with the 25th and 75th percentiles. We also note, although it is not shown, that average time is usually much higher than median for the baselines. We report the runtimes of TabDPT models with different context sizes, indicated by the number labelling TabDPT points. It is also worth noting that even the biggest TabDPT model with the largest context size is at least one order of magnitude (up to 4) faster than the baseline models. While the baseline models need to train on each dataset separately, TabDPT is much faster thanks to its ICL capability.\nIn addition, since TabDPT performs inference with a fixed batch and feature size, our speed per 1,000 rows is very consistent. We also add TabDPT (subsampling) which uses a shared random context to classify all the test points \u2013 in the style of TabPFN \u2013 allowing for even faster inference at the cost of performance.\nIn the pre-training phase, we have also observed that TabDPT achieves a higher performance than PFN++ within the same number of epochs especially early on during training. In Figure 13, we can see that TabDPT obtains lower test loss given a fixed compute budget, especially for larger models. This clearly highlights the importance of real data compared to synthetic data."}, {"title": "ABLATIONS", "content": "In this section, we ablate key components in our training and inference strategies. All models are trained for 700 epochs for a fair comparison. In Figure 4b, we report the reduction in performance by removing key components in the training or inference pipeline; a higher bar indicates a greater reduction in performance. For computational reasons, all comparisons are done against a smaller base model with 28M parameters and 12 layers, and the inference is done using 512 context size with retrieval as done by Thomas et al. (2024).\nTraining Ablations Firstly, we assess the importance of our SSL approach, where columns are randomly sampled as targets during training. To ablate this, we only use the original target during training and we observe the greatest loss in performance overall, as shown under \u201cSupervised Target (Training)\" in Figure 4b. This underscores the importance of SSL in our training process. Secondly, using subsampling instead of retrieval during training \u2013 but still keeping retrieval during inference \u2013 also leads to a performance drop, albeit not as drastic as before.\nInference Ablations Similarly to Thomas et al. (2024), we find that using subsampling instead of retrieval during inference decreases performance as indicated in the second column in Figure 4b. Lastly, using a smaller context size also decreases performance as expected, although it does not decrease nearly as much as the other important components discussed above."}, {"title": "LIMITATIONS, CONCLUSION, AND FUTURE WORK", "content": "While our model achieves very competitive performance on two popular tabular benchmarks, it is still subject to some important limitations that have not been addressed in this work.\ni) As of now, the current model cannot use textual information. Note that it is still possible to modify the architecture to use embeddings of feature names, but we did observe overfitting when we attempted it. We believe training the model on more tables with more features overall could lead to improvements but we leave this to future work. ii) We still suffer from the limitation on the number of features, and the non-invariance to the class and feature ordering, inherited from the \"attention over rows\" architecture of TabPFN (Hollmann et al., 2023). The ordering dependence is mitigated through randomization during training and we proposed inference techniques to address the former limitations, although a better outcome would be finding a performant architecture that inherently does not suffer from these limitations. iii) We have made some assumptions on the types of data we handle: these are rectangular (i.e., not hierarchical or nested) tables which are i.i.d. (i.e., having no time component or distribution shift between training data and test/inference data). iv) Unlike foundation models in other domains, we have not shown generative (Loaiza-Ganem et al., 2024) capabilities. We anticipate that TabDPT could be complemented with ideas from, e.g., (Ma et al., 2023), (Ma et al., 2024) or (van Breugel et al., 2024). v) Even though our runtime on new tasks is extremely fast compared to the baseline models, the pre-training of TabDPT is still itself time- and resource-consuming, although we will provide the weights of TabDPT to help mitigate this. Furthermore, there may be specific applications or settings where TabDPT and other ICL-based techniques require additional fine-tuning, which may make them slower in comparison.\nWhile deep learning has traditionally struggled with tabular data, our findings demonstrate that, with the appropriate architecture and data utilization, similar scaling laws to those seen in other domains can emerge. This suggests that tabular data is not inherently unique, and we anticipate that larger tabular models trained on increasing amounts of data will become the norm. In this paper, we present TabDPT as a scalable model that achieves strong performance on the CC18 and CTR23 benchmarks, but we expect the field to see the development of even larger models trained on more expansive datasets in the future."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We provide a description of the architectural details, hyperparameters and datasets used for both evaluation and training. We intend to release the model weights, the full training and inference code, and some data management functions such as automated tests for contamination."}, {"title": "ETHICS STATEMENT", "content": "We do not foresee any ethical concerns with the present research. The creation of a tabular foundation model and studying the scaling of such models are unlikely to be used for harmful purposes. Nevertheless, we do not promote the use of these models for harmful practices."}, {"title": "BITTER LESSONS", "content": "The architecture of TabDPT is based on TabPFN with some minor modifications (see Appendix H.1). Therefore, it inherits the limitations of TabPFN. To mitigate these, we introduce inference time techniques (see Section 3.4), although we also attempted to overcome these shortcomings during training. The following list contains some of the ideas that either hurt the performance or did not lead to significant improvements. We include this list to emphasize The Bitter Lesson and the fact that efficient use of computation and access to high-quality data are the more important factors in driving performance. The list is as follows:\n\u2022 Different pre-processing techniques that were more robust to outliers, or variants of soft clipping, resulted in no improvement. More advanced methods, such as Robust Scaler and Power Transform, only ended up slowing the training process.\n\u2022 Class embeddings (either through a separate network or by using class \"tokens\" in the transformer layer) and computing various similarity metrics between query and class embeddings in a proto-network manner, with the aim of adapting to any number of classes, hurt the performance, especially on real data.\n\u2022 Different embeddings for $Y_{ctx}$, including a dense layer for regression and a dictionary of $Cmax*xd$ embeddings, with the rationale of informing the model about the task, did not lead to performance improvements in large models with sufficient data.\n\u2022 Specialized tokens for NaN encoding did not improve performance compared to replacing NaNs with mean values (which are zero after preprocessing). Additionally, appending binary features to differentiate actual zeros from NaNs (indicating that the cell was replaced), effectively doubling the number of features, also failed to improve performance.\n\u2022 Architectures encoding cells as \"tokens\", with vertical and horizontal attention, similar to spatial and temporal attention in videos, proved more memory intensive. While equivariance to feature order is desirable, processing tensors of size (B, N, f, d) \u2013 where B is batch size, N is the number of rows, f the number of features, and d the embedding dimension uses much more memory. The simpler architecture with tensors of size (B, N, d) permits a higher embedding dimension d."}, {"title": "TRAINING AND INFERENCE DETAILS"}]}