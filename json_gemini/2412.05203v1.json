{"title": "Archaeoscape: Bringing Aerial Laser Scanning Archaeology to the Deep Learning Era", "authors": ["Yohann Perron", "Vladyslav Sydorov", "Adam P. Wijker", "Damian Evans", "Christophe Pottier", "Loic Landrieu"], "abstract": "Airborne Laser Scanning (ALS) technology has transformed modern archaeology by unveiling hidden landscapes beneath dense vegetation. However, the lack of expert-annotated, open-access resources has hindered the analysis of ALS data using advanced deep learning techniques. We address this limitation with Archaeoscape (available at https://archaeoscape.ai), a novel large-scale archaeological ALS dataset spanning 888 km\u00b2 in Cambodia with 31,141 annotated archaeological features from the Angkorian period. Archaeoscape is over four times larger than comparable datasets, and the first ALS archaeology resource with open-access data, annotations, and models.\nWe benchmark several recent segmentation models to demonstrate the benefits of modern vision techniques for this problem and highlight the unique challenges of discovering subtle human-made structures under dense jungle canopies. By making Archaeoscape available in open access, we hope to bridge the gap between traditional archaeology and modern computer vision methods.", "sections": [{"title": "1 Introduction", "content": "Airborne Laser Scanning (ALS) has been celebrated as a \"geospatial revolution\" in modern archaeol-\nogy due to its ability to penetrate vegetation and unveil traces of human activities that may otherwise\nbe concealed or invisible [1, 2]. Extensive acquisition campaigns conducted in Southeast Asia [3],\nCentral America [4], and Europe [5, 6] have led to a reevaluation of the historical impact of humans\non \"natural\" landscapes, especially in tropical regions [7]. However, finding archaeological features\nin vast volumes of ALS data presents a significant challenge. Manual analysis is time-consuming and\nrequires advanced expert knowledge of the studied civilization as well as on-site validation [8].\nThe emergence of deep learning offers a promising tool to assist researchers in identifying archaeo-\nlogical patterns, simplifying the exploration of these extensive acquisitions. Yet, the development of\nspecialized models is hampered by the lack of expert-annotated datasets. In response, we introduce\nArchaeoscape, the largest open-access ALS dataset for archaeological research published to date.\nSpanning 888 km\u00b2, it comprises 31,411 annotated instances of anthropogenic features of archaeologi-\ncal interest. The dataset includes orthophotos and LiDAR-derived normalized Digital Terrain Models"}, {"title": "2 Related work", "content": "In this section, we explore the advantages of Archaeoscape over existing datasets, highlighting its\nlarger scale and open-access policy (Section 2.1), and present the different models evaluated in our\nbenchmark (Section 2.2)."}, {"title": "2.1 ALS archaeology datasets", "content": "Deep learning for ALS archaeology is a dynamic field [10]. In Table 1, we list the main deep learning\nworks on ALS-archaeology. Archaeoscape is not only one of the few open-access datasets available\nbut also the largest and most comprehensively annotated by a significant margin.\nOpen-access policies. ALS archaeology datasets typically withhold data, annotations, and code\ndue to legitimate concerns about misuse [11, 12], and the absence of established open-access norms\nin archaeology. However, recognizing the critical role of reproducibility and open access in science,\nwe make Archaeoscape accessible to academic researchers. We implement strict safeguards to protect\nsensitive archaeological information, as described in Section 3.1.\nScope and extent. Archaeoscape is the largest ALS archaeology dataset in terms of area covered\n(888 km\u00b2) and number of annotated instances (31,411). Archaeoscape covers a 2\u00d7 larger surface\narea and contains 3\u00d7 more instances than the next-largest closed archaeology LiDAR dataset (see\nTable 1). It is also the first such dataset related to the Khmer civilization of Southeast Asia."}, {"title": "2.2 Semantic segmentation with deep learning", "content": "ALS archaeology approaches rely predominantly on U-Net-based models [9]. However, the field\nof semantic segmentation has evolved considerably since its introduction in 2015. We propose to\nassess the performance of an array of contemporary, state-of-the-art models on the Archaeoscape\nbenchmark. Models and pretraining strategies evaluated in Table 2 are denoted in bold throughout\nthe text for clarity and ease of reference.\nConvolution-based models. Convolutional Neural Networks (CNNs) [23, 24], and the U-Net [9]\narchitecture in particular, remain the predominant choice for dense prediction tasks across various\napplication fields due to their simplicity and effectiveness. DeepLabv3 [25] improves on this model\nby using dilated convolution and Spatial Pyramid Pooling [26] to learn multiscale features.\nVision transformers. Vision transformers harness the versatility and expressivity of transformers\n[27] to extract rich image features. The Vision Transformer ViT [28] model splits the images into\nsmall patches, which are embedded with a linear layer, while the final patch encodings are converted\ninto pixel prediction with another linear layer. DOFA [29] embed each input channel conditionally\nto its wavelength, allowing generalization to new sensors. Alternatively hybrid HybViT replaces\nthese linear layers with a combination of convolutional and deconvolutional layers for encoding and\ndecoding patches, respectively. This adaptation is particularly effective on smaller datasets, as the\nconvolutions help capture local feature dependencies more effectively.\nHierarchical ViTs. Several variants of the ViT model use a hierarchical approach to effectively\ncapture spatial features with a large context. The Pyramid Vision Transformer (PVT) [30] applies its\nattention mechanism according to a nested hierarchical structure, while SWIN [31] uses overlapping\nwindows of increasing sizes. Building on these concepts, PCPVT [32] introduces a conditional\nrelative position encoding mechanism, and PVTv2 [33] also allows for overlapping patches.\nPre-training strategies. Recent advances in self- and weakly-supervised learning have profoundly\nimpacted the efficacy of neural networks. These strategies often use large datasets with text annota-\ntions such as CLIP-OPENAI [34] or its open-source counterpart CLIP-LAION2B [35]. Alterna-\ntively, DINOv2 [36] learns from large, unannotated image datasets. The recent Masked Auto-Encoder\n[37] tunes large models by using the pretext task of masked patch reconstruction. This approach has\nbeen adapted to address the specific needs of aerial imagery, leading to variants such as ScaleMAE\n[38] which are trained on satellite images."}, {"title": "3 Archaeoscape", "content": "In this section, we describe the content of Archaeoscape (Section 3.1), as well as its acquisition\nprocess (Section 3.2).\nContext. Angkor, the heart of the medieval Khmer Empire, is often referred to as a \"hydraulic\ncity\" due to its extensive water management infrastructure. This system allowed the Khmer to thrive\nin a challenging environment, oscillating between monsoon and dry seasons, from the 9th to the\n15th century. Today, much of the built environment of Angkor and the other cities of this period\nhas disappeared, as virtually all non-religious architecture was built using perishable materials such\nas wood. What remains is often hidden by dense vegetation or damaged by erosion and modern\nagricultural practices, rendering these sites nearly invisible at ground level, so that even experts\nmight walk over such sites without realizing it. However, the advent of LiDAR (Light Detection\nand Ranging) technology has been transformative, uncovering distinct, often geometric patterns in\nthe topography indicative of ancient occupation and landscape alteration. By combining careful\nanalysis of ALS imagery with targeted ground surveys, this decade-long project has documented\ntens of thousands of ancient Khmer features, many previously undiscovered, providing a new and\nexpanded perspective on the history of the region."}, {"title": "3.1 Dataset characteristics", "content": "Splits. As shown in Figure 2, the dataset consists of 23 non-overlapping parcels of varying size,\nranging from 2 to 183 km\u00b2, and include archaeologically relevant areas such as ancient temples, cities,\nand roadways. We present the splits for Archaeoscape's training (623 km\u00b2, 16 parcels), validation\n(97 km\u00b2, 3 parcels), and test (168 km\u00b2, 4 parcels) sets. The splits were chosen to respect the global\ndistribution of features and landscapes: densely or scarcely occupied regions, hills or floodplains,\nlarge-scale hydraulic engineering sites, monumental temples, and subtle earthen features.\nUnder these constraints, splitting the dataset into spatially distinct regions\u2014as is commonly done\nin geospatial machine learning\u2014proved impractical. To prevent data contamination all parcels are\nseparated by a least a 100 meter buffer. The test set consists of 2 remote parcels, set apart from\nthe others by more than 5 km, and 2 parcels adjacent to training and validation sets, covering two"}, {"title": "Misuse prevention", "content": "There is a valid concern that large-scale annotated ALS data could be misused\nby malicious actors, leading to the targeted looting or destruction of historical sites [11, 12]. The\npotential for misuse has been a significant factor in the lack of public availability of archaeological\ndatasets. To mitigate this risk and alleviate the concerns of local stakeholders, we propose several\nmeasures to balance the benefits of open access with the legal and practical protection of cultural\nheritage sites:\n\u2022 Data partitioning: The data is divided into parcels and stripped of georeferencing and absolute\nelevation information to prevent spatial identification of remote, less well-known sites. While\nfamous temples such as Angkor Wat may be recognizable, they are already under close protection\nby the local authorities.\n\u2022 Custom license: The dataset is distributed under a license which forbids re-georeferencing,\ncommercial use, and redistributing the data beyond the intended users.\n\u2022 Open credentialized access: Access to the dataset requires signing a data agreement form,\nwhich holds users legally accountable for misuse. The Appendix contains more details about the\nlicense, data access, and the distribution agreement.\nDataset format. We distribute the data as GeoTIFF files with a 0.5 m resolution and polygon\nannotations in the GeoPackage format. We associate each pixel with the following values:\n\u2022 Radiometry: RGB values obtained from contemporary orthophotography.\n\u2022 Ground elevation: Digital Terrain Model (DTM) obtained with ALS, see Section 3.2.\n\u2022 Semantic label: One-hot encoding of the five classes described below.\nAnnotation. One of the most significant undertakings of Archaeoscape is the meticulous annotation\nby experts, who have individually traced and field-verified a wealth of archaeological features. The\nannotators employed a granular classification system with 12 feature types. However, to mitigate\nsevere class imbalance and reduce ambiguity, we have streamlined this system into a more manageable"}, {"title": "4 Benchmark", "content": "In this section, we assess the performance of modern semantic segmentation methods for ALS\narchaeology. We first detail how we adapt and evaluate these methods (Section 4.1), then present\nour results and analysis (Section 4.2), and an ablation study (Section 4.3). Finally, we discuss the\nlimitations of our approach (Section 4.4)."}, {"title": "4.1 Baselines and metrics", "content": "We formulate the problem of finding archaeological features as a semantic segmentation task, and\nbenchmark several backbone networks on our dataset.\nMetrics. We evaluate the prediction of the models with the overall accuracy (OA), class-wise\nIntersection over Union (IoU), and the unweighted mean of the IoUs (macro-average). For the\nevaluation, we exclude pixels annotated with the void label.\nImplementation details. We train the evaluated models using the configurations of the official\nopen-source repository and provide more details in the supplementary materials. The predictions on\nthe test set are performed along a grid corresponding to the input size and with 25% overlap on each\nside. Only the central portion of each prediction is kept while the border predictions are discarded.\nWe use a combination of internal clusters and the HPC GENCI to run our experiments. Reproducing\nthe entire benchmark requires 260 GPU-h with A100 GPUs. We estimate the total cost of our\nhyperparameters search and initial experiments at 1100 GPU-h.\nAdapting baselines. To evaluate the performance of modern vision models for ALS archaeology,\nwe adapt several semantic segmentation models to our setting. The changes are minimal:\n\u2022 Inputs. Beyond radiometry (RGB), we also incorporate ground elevation derived from the ALS\ndata described in Section 3.2. As we consider networks trained on natural images, we modify\nthe first layer to accommodate an extra band and initialize the additional weights randomly\naccording to N (0, 0.01).\n\u2022 Segmentation head. For all transformer-based methods, we map the final patch embeddings to\npixel-level prediction with linear layers, except for HybViT which uses transposed convolutions.\nFor CNNs, we use their dedicated segmentation heads, which we initialize randomly.\n\u2022 Pre-training and fine-tuning. We consider models pre-trained on ImageNet1K [44] and\nImageNet21K [45], but also foundation vision models trained on large external datasets: DINOv2\n[36], CLIP-OPENAI [34] and LAION-2B [35], and Earth observation datasets [29, 38, 46]."}, {"title": "4.2 Results", "content": "We report the quantitative performance of various state-of-the-art semantic segmentation models in\nTable 2, and provide qualitative examples in Figure 4."}, {"title": "4.3 Ablation study", "content": "We evaluate the impact of some of the choices made in the design of Archaeoscape through an\nablation study.\nChannel importance. Airborne LiDAR scans are pivotal for uncovering the subtle elevation pat-\nterns of archaeological features like mounds and canals, which are typically not visible in orthophotos,\nas shown in Figure 5. Moreover, dense canopies can obscure or completely hide radiometric informa-\ntion about the ground. Conversely, in less densely forested areas, orthophotos can capture detailed\ninformation about archaeological features, complementing LiDAR data. The ablation study results,\ndocumented in Table 4, highlight the limitations of relying solely on RGB data. Models using only\nRGB information registered a mean Intersection over Union (mIoU) of about 30%, significantly\nlower than models also utilizing elevation data. This disparity underscores the inadequacy of RGB\ndata under dense canopy coverage. Furthermore, while removing RGB information only moderately\naffects performance, it particularly affects the detection of temples\u2014some of which are still standing\nto this day, and are typically not covered by the canopy. The performance gap between models\npretrained with DINOv2 and those pretrained on ImageNet widens without RGB, suggesting that\nDINOv2 models are highly optimized for RGB processing, whereas ImageNet models adapt better to\nelevation data.\nInitialization strategy. Adapting models trained on RGB data to handle elevation channels poses\nchallenges. Our approach, detailed in Section 4, initialize with small values the weights of the first\nlayer corresponding to the new channel while retaining the pre-trained weights for RGB. In Table 4,\nwe evaluate this method against three alternatives: fully random initialization, random initialization\nof the first layer with other weights retained, and LoRA fine-tuning. Randomly initializing the first\nlayer results in performance akin to training the network from scratch, demonstrating the efficacy of\nour strategy to leverage pre-existing RGB training."}, {"title": "4.4 Limitations", "content": "Archaeoscape presents several limitations as a benchmark that should be considered:"}, {"title": "5 Conclusion", "content": "We have introduced Archaeoscape, the largest published dataset for ALS archaeology featuring\nopen-access imagery and annotations. Focused on the ancient Khmer settlement complexes and\ntemples of Cambodia, our dataset covers 888 km\u00b2 and comprises 31,144 individual anthropogenic\ninstances. We provide an extensive benchmark evaluating several state-of-the-art computer vision\nmodels for detecting archaeological features within elevation maps and images. Despite formulating\nthe problem as a classic semantic segmentation task, we observe that even usually high-performing\nmodels struggle to achieve high scores. We attribute this poor performance to the unique challenges of\nALS archaeology, such as the subtlety of the patterns sought, and the importance of large-scale context.\nWe hope that our dataset will encourage the computer vision and machine learning community to\npropose novel solutions for these unresolved challenges."}, {"title": "A.3 Additional ALS archaeology-related datasets", "content": "ALS data are well-used by archaeologists for its precision and ability to recover the archaeological\nfeatures [10]. Several recent works have leveraged deep learning techniques to automatically detect\nfeatures of interest. In Table A.1, we provide a list of such works. Note that this is a very dynamic\nfield, and this list may not be exhaustive."}, {"title": "A.4 Additional results", "content": "Foundation models can be difficult to fine-tune to new datasets or tasks, as they tend to easily overfit.\nLow rank adaptation (LoRA) [54] can remedy this issue by only learning a low-rank update to the\nweights of the pre-trained model. Figure Figure A.1 provides the performance of PVTv2 and DINO\nmodels fine-tuned using LoRA compared to a fully fine-tuned.\nWhen fine-tuned with LoRA, the performance of DINO rapidly plateaus and even decreases, showing\nthat the learned features can not be easily adapted from RGB images to terrain models. Conversely,\nthe performance of PVTv2 increases with the rank used for LoRA, but does not reach the performance\nof a fully fine-tuned network. This suggests that, when fine-tuned to new input and target domains,\nand particularly when using LoRA, large models can become over-adapted to their source domain\nand struggle to generalize.\nWe provide additional visualizations of the mapping outputs generated by our models in Figure A.2.\nThose once again illustrate the difficulty that arises from large-scale dependency, particularly in\nwater prediction. Columns 2 and 3 are respectively an illustrations of a failure and success case\nin predicting large bodies of water. While our model is able to accurately identify most temples\nand mounds, the reconstruction of the exact shape of religious or settlement complexes remains\napproximate. Moreover, we observe that the model struggles to detect fainter mounds, although these\nare still visible to human experts."}, {"title": "A.5 Implementation details", "content": "A.5.1 Data split\nWe designed the split to each contain emblematic archaeological features-large-scale hydraulic\nengineering sites, monumental temples, subtle features, and typical terrain types-dense and scarce\noccupation, hills, and floodplains. The class distribution per split is given in Table A.2."}, {"title": "A.5.2 Data loader", "content": "Our data loader loads images of size 224 x 224 pixels at a resolution of 50 cm, with RGB channels\nnormalized using the dataset mean and variance. The elevation channel is normalized separately,"}, {"title": "A.5.3 Training", "content": "We use the ADAM optimizer with a linear warm-up schedule that increases the learning rate from\n10-5 to 10-3 across the first two epochs of training. We use a ReduceLROnPlateau [55] learning\nrate scheduler with a patience of 4 and a decay of 5."}, {"title": "A.6 License", "content": "The Archaeoscape dataset is under a custom license, which prevents redistribution and attempts at\nlocalizing the data. We provide the full text of the license below.\nThe \u00c9cole fran\u00e7aise d'Extr\u00eame-Orient (EFEO) makes the Archaeoscape dataset (the \u201cDATASET\u201d)\navailable for research and educational purposes to individuals or entities (\"USER\") that agree to the\nterms and conditions stated in this License.\n1 The USER may access, view, and use the DATASET without charge for lawful non-\ncommercial research purposes only. Any commercial use, sale, or other monetization\nis prohibited. The USER may not use the DATASET for any unlawful activities, including\nbut not limited to looting, vandalism, and disturbance of archaeological sites.\n2 The USER may not attempt to identify the location of any part of the DATASET and must\nexercise all reasonable and prudent care to avoid the disclosure of the locations referenced\nin the DATASET in any publication or other communication.\n3 The USER may not share access to the DATASET with anyone else. This includes distributing\nthe download link or any portion of the DATASET. Other users must register separately and\ncomply with all the terms of this Licence.\n4 The USER must use the DATASET in a manner that respects the cultural heritage of\nCambodia and its people, and in compliance with the relevant Cambodian authorities. Any\nuse of the DATASET that could harm or exploit these cultural sites or their environment is\nstrictly prohibited.\n5 The USER must properly attribute the EFEO as the source of the data in any publications,\npresentations, or other forms of dissemination that make use of the DATASET.\n6 This agreement may be terminated by either party at any time, but the USER's obligations\nwith respect to the DATASET shall continue after termination. If the USER fails to comply\nwith any of the above terms and conditions, their rights under this License shall terminate\nautomatically and without notice.\nTHE DATASET IS PROVIDED \"AS IS,\" AND THE EFEO DOES NOT MAKE ANY WARRANTY OF\nANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT.\nIN NO EVENT SHALL THE EFEO OR ITS COLLABORATORS BE LIABLE FOR ANY CLAIM,\nDAMAGES, OR OTHER LIABILITY ARISING FROM THE USE OF THE DATASET."}, {"title": "A.7 Datasheet for dataset", "content": "A.7.1 Motivation\nQ1 For what purpose was the dataset created? Was there a specific task in mind? Was there\na particular gap that needed to be filled? Please provide a description.\n\u2022 The Archaeoscape dataset is an open-access ALS dataset intended for archaeology. It is\nsimultaneously the largest in terms of its extent and number of annotated anthropogenic\nfeatures. The intended task is the semantic segmentation of LiDAR-derived terrain\nmaps to find archaeological traces and structures under dense vegetation.\nQ2 Who created the dataset (e.g., which team, research group) and on behalf of which\nentity (e.g., company, institution, organization)?\n\u2022 The different parts of the dataset were acquired during several acquisition campaigns\nas part of the Khmer Archaeology Lidar Consortium (KALC) and Cambodian Archae-\nological Lidar Initiative (CALI), joint programs of which the EFEO (Ecole fran\u00e7aise\nd'Extr\u00eame-Orient) was a member. The curation and benchmarking were performed\njointly with the IMAGINE team (A3SI/LIGM, ENPC).\nQ3 Who funded the creation of the dataset? If there is an associated grant, please provide the\nname of the grantor and the grant name and number.\n\u2022 The ALS acquisitions were funded by the following parties:\nEuropean Research Council (ERC)\n\u00c9cole fran\u00e7aise d'Extr\u00eame-Orient (EFEO)\nUniversity of Sydney (USYD)\nSoci\u00e9t\u00e9 Concessionnaire d'A\u00e9roport (SCA/INRAP Airport)\nHungarian Indochina Company (HUNINCO)\nJapan-APSARA Safeguarding Angkor (JASA)\nArchaeology & Development Foundation Phnom Kulen Program (ADF Kulen)\nWorld Monuments Fund (WMF)\n\u2022 And are associated with the following ERC grants:\nCALI: \"The Cambodian Archaeological Lidar Initiative: Exploring Resilience in\nthe Engineered Landscapes of Early SE Asia\u201d (Grant agreement ID: 639828)\narchaeoscape.ai: \u201cExploring complexity in the archaeological landscapes of mon-\nsoon Asia using lidar and deep learning\u201d (Grant agreement ID: 866454).\n\u2022 The funding for the Archaeoscape annotations is 100% public. The EFEO is an\n\u201c\u00c9tablissement public \u00e0 caract\u00e8re scientifique, culturel et professionnel\u201d, i.e. a public\nscientific, cultural or professional establishment which is financed by public funds.\nQ4 Any other comments?\n\u2022 [N/A]\nA.7.2 Composition\nQ5 What do the instances that comprise the dataset represent (e.g., documents, photos,\npeople, countries)?\n\u2022 The dataset covers several sites in Cambodia of archaeological interest. The dataset\ncomprises ALS-derived elevation maps, orthorectified photography, and manually\nannotated archaeological features.\nQ6 How many instances are there in total (of each type, if appropriate)?\n\u2022 Archaeoscape covers 888 km\u00b2 and 31,141 individual archaeological features. The\ndataset is split into 23 non-overlapping parcels, from 2 to 183 km\u00b2.\nQ7 Does the dataset contain all possible instances or is it a sample (not necessarily random)\nof instances from a larger set?\n\u2022 Archaeoscape covers only a fraction of the full extent of the Khmer Empire at its\napogee and of the likely distribution of Khmer archaeological features in the landscape.\nThose parts of the dataset contained in the training, validation, and test sets have been"}, {"title": "Q8 What data does each instance consist of?", "content": "\u2022 Each parcel is a raster file under the GeoTIFF format with a ground sampling distance\nof 0.5 m. Each pixel is associated with: (i) a terrain elevation relative to the lowest\npoint of the file, (ii) an RGB value derived from an orthorectified aerial photograph,\n(iii) where available, a label corresponding to one of the sought classes, (iv) a binary\nvalue indicating whether or not the pixel is in the parcel.\nQ9 Is there a label or target associated with each instance?\n\u2022 [Yes] We provide dense pixel-precise annotations for 888 km\u00b2 corresponding to over\n3.5 billion annotated pixels.\nQ10 Is any information missing from individual instances?\n\u2022 [Yes] The georeferencing information has been stripped from the dataset parcels.\nQ11 Are relationships between individual instances made explicit (e.g., users' movie ratings,\nsocial network links)?\n\u2022 [No] To prevent their re-georeferencing, we have purposefully removed any informa-\ntion on the relationships between parcels.\nQ12 Are there recommended data splits (e.g., training, development/validation, testing)?\n\u2022 [Yes] We provide the following data splits: train, validation and test. The test split\nhas been explicitly selected to contain a representative variety of configurations. We\nimplement a 100 m buffer between all parcels.\nQ13 Are there any errors, sources of noise, or redundancies in the dataset?\n\u2022 As the annotations are made through visual interpretation with quality control, some\nerrors are unavoidable, especially for classes that are visually hard to distinguish. Some\nunavoidable noise occurs due to the ambiguous boundaries of subtle archaeological\nfeatures. Internal quality control has been performed to limit such errors. There are no\nredundancies in the dataset, each parcel covers a distinct area.\nQ14 Is the dataset self-contained, or does it link to or otherwise rely on external resources\n(e.g., websites, tweets, other datasets)?\n\u2022 This dataset is self-contained and will be stored and distributed by the EFEO.\nQ15 Does the dataset contain data that might be considered confidential (e.g., data that is\nprotected by legal privilege or by doctor-patient confidentiality, data that includes the\ncontent of individuals' non-public communications)?\n\u2022 [No] The data does not contain confidential information. However, to limit potential\nmisuse such as looting or destruction of historical sites, the georeferencing and absolute\nelevation of the parcels have been removed.\nQ16 Does the dataset contain data that, if viewed directly, might be offensive, insulting,\nthreatening, or might otherwise cause anxiety? If so, please describe why.\n\u2022 [No]\nQ17 Does the dataset identify any subpopulations (e.g., by age, gender)?\n\u2022 [No]\nQ18 Is it possible to identify individuals (i.e., one or more natural persons), either directly\nor indirectly (i.e., in combination with other data) from the dataset?\n\u2022 [No] The nDTM elevation data excludes extraneous points such as modern buildings.\nThe RGB orthophotography resolution of 50 cm/pixel and the aerial perspective prevent\nthe recognition of individuals."}, {"title": "Q19 Does the dataset contain data that might be considered sensitive in any way (e.g., data", "content": "that reveals racial or ethnic origins, sexual orientations, religious beliefs, political\nopinions or union memberships, or locations; financial or health data; biometric or\ngenetic data; forms of government identification, such as social security numbers;\ncriminal history)?\n\u2022 [No]\nQ20 Any other comments?\n\u2022 The safe and ethical release of archaeological data has been the subject of numerous\nstudies [12]. We have implemented the best practices of the field to minimize potential\nrisks of misuse."}, {"title": "A.7.3\n Collection Process", "content": "Q21 How was the data associated with each instance acquired?\n\u2022 The ALS data and photography were acquired from aerial surveys in Cambodia and\nmapped onto a cartographic coordinate reference system. From this data a subset of\n888 km\u00b2 was selected, corresponding to over 13,000 aerial photos and 10 billion points,\nwith a density of 10-95 points per m\u00b2, depending on the terrain.\n\u2022 The ALS points were filtered to remove noise and classified. The normalized Digital\nTerrain Models (nDTM) (relative ground elevation) was obtained from the classified\nALS point clouds using open-source software. A triangular irregular network was fitted\nto the ground points (excluding extraneous elements such as tree canopies and modern\nbuildings), with a DTM formed by linear interpolation of the elevation values within\neach triangular plane based on a 0.5 meter grid. The same procedure was applied to\nobtain intensity and return number metadata maps. The photos were orthorectified and\nresampled to the same 0.5 meter resolution.\nQ22 What mechanisms or procedures were used to collect the data (e.g., hardware apparatus\nor sensor, manual human curation, software program, software API)?\n\u2022 The data was acquired with Leica LiDAR (ALS60 for KALC, ALS70-HP for CALI)\nand cameras (RCD105 and RCD30). The instruments were mounted on a pod attached\nto the skid of a Eurocopter AS350 B2 helicopter flying at 800 m above ground level as\nmeasured by an integrated Honeywell CUS6 IMU, and positional information acquired\nby a Novatel L1/L2 GPS antenna. GPS ground support was provided by two Trimble\nR8 GNSS receivers.\nQ23 If the dataset is a sample from a larger set, what was the sampling strategy (e.g.,\ndeterministic, probabilistic with specific sampling probabilities)?\n\u2022 The target areas for the LiDAR acquisition campaigns were selected on the grounds of\narchaeological value and interest by domain experts. A subset of 888 km\u00b2 presented in\nthis dataset was selected by choosing 23 non-overlapping parcels in the areas where\narchaeological annotations were deemed complete and finalized, preserving the global\ndistribution of features and landscapes across the training, validation and test sets.\nQ24 Who was involved in the data collection process (e.g., students, crowdworkers, contrac-\ntors) and how were they compensated (e.g., how much were crowdworkers paid)?\n\u2022 These mapping and verification efforts were performed by a shifting team of archaeolo-\ngists, both local and foreign, who collectively contributed to the analysis and validation\nof the data, with the first pre-LiDAR surveys dating back to 1993, and continuing until\n2024. All persons involved were employees and researchers from foreign governmental\ninstitutions, such as the EFEO or Sydney University, or employed by the Cambo-\ndian governmental authorities, following strictly existing ethical codes and national\nregulations.\nQ25 Over what timeframe was the data collected? Does this timeframe match the creation\ntimeframe of the data associated with the instances (e.g., recent crawl of old news\narticles)?\n\u2022 The KALC campaign took place in 2012, and the CALI campaign in 2015. The\nannotations are the result of a continuous effort from 1993 to 2024."}, {"title": "Q26 Were any ethical review processes conducted (e.g., by an institutional review board)?", "content": "\u2022 [Yes] Yes, as a part of the CALI and archaeoscape.ai ERC grants.\nQ27 Does the dataset relate to people?\n\u2022 The dataset describes the archaeological remains of anthropogenic structures, but does\nnot directly relate to living people.\nQ28 Did you collect the data from the individuals in question directly, or obtain it via third\nparties or other sources (e.g., websites)?\n\u2022 [N/A]\nQ29 Were the individuals in question notified about the data collection?\n\u2022 [N/A]\nQ30 Did the individuals in question consent to the collection and use of their data?\n\u2022 [N/A]\nQ31 If consent was obtained, were the consenting individuals provided with a mechanism\nto revoke their consent in the future or for certain uses?\n\u2022 [N/A]\nQ32 Has an analysis of the potential impact of the dataset and its use on data subjects (e.g.,\na data protection impact analysis) been conducted?\n\u2022 [Yes] We have studied potential misuse of the data and have taken steps to prevent it,\nsuch as removing and obfuscating the location of acquisitions.\nQ33 Any other comments?\n\u2022 [No]"}, {"title": "A.7.4 Preprocessing, cleaning, and/or labeling", "content": "Q34 Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucket-\ning, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances,\nprocessing of missing values)?\n\u2022 [Yes] The ALS acquisitions are delivered in the form of 3D point clouds. The ALS\npoints were filtered to remove noise and classified. We have extracted terrain models\nfrom the ground clouds only, i.e. those not belonging to the tree canopies and modern\nbuildings.\nQ35 Was the \u201craw\u201d data saved in addition to the preprocessed/cleaned/labeled data (e.g., to\nsupport unanticipated future uses)? If so, please provide a link or other access point to\nthe \"raw\" data.\n\u2022 [Yes] The data has been saved, but will not be distributed to prevent data re-localization.\nQ36 Is the software used to preprocess/clean/label the instances available?\n\u2022 [Yes] The annotation software is open source. The anonymized data preprocessing\ncode (without references to specific locations or coordinates) is available.\nQ37 Any other comments?\n\u2022 [No]"}, {"title": "A"}]}