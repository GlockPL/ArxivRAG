{"title": "Assessing and Enhancing Large Language Models in Rare Disease Question-answering", "authors": ["Guanchu Wang", "Junhao Ran", "Ruixiang Tang", "Chia-Yuan Chang", "Yu-Neng Chuang", "Zirui Liu", "Vladimir Braverman", "Zhandong Liu", "Xia Hu"], "abstract": "Despite the impressive capabilities of Large Language Models (LLMs) in general medical domains, questions remain about their performance in diagnosing rare diseases. To answer this question, we aim to assess the diagnostic performance of LLMs in rare diseases, and explore methods to enhance their effectiveness in this area. In this work, we introduce a rare disease question-answering (ReDis-QA) dataset to evaluate the performance LLMs in diagnosing rare diseases. Specifically, we collected 1360 high-quality question-answer pairs within the ReDis-QA dataset, covering 205 rare diseases. Additionally, we annotated meta-data for each question, facilitating the extraction of subsets specific to any given disease and its property. Based on the ReDis-QA dataset, we benchmarked several open-source LLMs, revealing that diagnosing rare diseases remains a significant challenge for these models.\nTo facilitate retrieval augmentation generation for rare disease diagnosis, we collect the first rare diseases corpus (ReCOP), sourced from the National Organization for Rare Disorders (NORD) database. Specifically, we split the report of each rare disease into multiple chunks, each representing a different property of the disease, including their overview, symptoms, causes, effects, related disorders, diagnosis, and standard therapies. This structure ensures that the information within each chunk aligns consistently with a question. Experiment results demonstrate that ReCOP can effectively improve the accuracy of LLMs on the ReDis-QA dataset by an average of 8%. Moreover, it significantly guilds LLMs to generate trustworthy answers and explanations that can be traced back to existing literature. The ReDis-QA dataset, ReCOP corpus, and source codes of benchmark experiments are open-sourced at here1 2 3", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) show magnificent power in natural language processing, causal inference, code generation, and have widely applied to medical research [1, 2]. The flexibility and general capacity of LLMs allow them to be easily deployed to different scenarios of medical research, such as patient trail matching [3], healthcare chatbot [4, 5], and clinical note summarization [6]. For example, LLMs like Llama [7], Mistral [8], Phi [9], Gemma [10] can work on medical tasks in a zero-shot manner based on simple medical-related instructions. Additionally, they can be further enhanced with adaptations or alignments on medical corpora, such as the PMC-Llama [11], Me-Llama [12], and BioMistral [13]. Despite the impressive capabilities of LLMs in general medical domains, questions persist about how LLMs perform in diagnosing rare diseases?\nAlthough rare diseases affect only a small portion of the population, they collectively impose substantial burdens on public health, affecting millions of individuals worldwide [14, 15]. Diagnosing and treating these conditions are particularly challenging due to their complex genetic origins and unpredictable clinical manifestations [16, 17], requiring significantly intellectual decisions based on a vast knowledge base. LLMs are pre-trained on the corpora comprising trillions of tokens, rivaling the expertise of human specialists [18, 19]. This extensive training enables LLMs to acquire sufficient medical knowledge for diagnosing common diseases [20, 21, 22]. However, LLMs still encounter several challenges when addressing tasks related to rare diseases, including:\n\u2022 Generalization Challenges of LLMs to Rare Diseases. Rare diseases affect a small number of people and are documented in limited literature. Pre-trained LLMs often struggle to generalize to rare diseases due to the"}, {"title": "2 Preliminaries", "content": "unique and varied manifestations of these conditions. Consequently, LLMs might hallucinate by incorrectly linking rare diseases with common ones, leading to misleading results when queried about rare diseases.\n\u2022 Genetic Causes of Rare Diseases. Many rare diseases are caused by genetic mutations, which requires detailed genetic information and understanding that may not be captured fully by LLMs pre-trained on general data. Moreover, the shortfall in genetic data cannot be effectively addressed through existing prompting strategies, highlighting a critical gap in their capabilities.\nTo answer the question about the capabilities of LLMs in diagnosing rare diseases, we aim to assess the capabilities of LLMs in rare disease diagnosis, exploring tailored datasets that could bridge the gap in their current performance.\nIn this work, we introduce a rare disease question-answering (ReDis-QA) dataset to evaluate the performance LLMs in diagnosing rare diseases. Specifically, we collect 1360 high-quality question-answer pairs within the ReDis-QA dataset, spanning 205 rare diseases. Additionally, we annotated meta-data for each question, facilitating the extraction of subsets specific to any given disease and its property. Based on the ReDis-QA dataset, we benchmark several open-source LLMs, revealing that diagnosing rare diseases remains a formidable challenge for current open-source LLMs. To improve LLMs' performance, we collect the first rare diseases corpus (ReCOP), sourced from the National Organization for Rare Disorders (NORD) database. This database provides reliable and comprehensive reports on known rare diseases. The primary objective of ReCOP is to enhance the diagnostic capabilities of LLMs for rare diseases through retrieval augmented generation (RAG). To better fit the RAG framework, we split the report of each rare disease into multiple chunks, each representing a different property of the disease, including overview, symptoms, causes, effects, related disorders, diagnosis, and standard therapies. This structure ensures that the information within each chunk aligns consistently with a question. Experiment results demonstrate that ReCOP significantly facilitates LLMs in rare disease diagnosis, improving their accuracy by an average of 8% on the ReDis-QA dataset."}, {"title": "2.1 Large Language Models for Rare Disease Diagnosis.", "content": "Large language models (LLMs) have demonstrated competitiveness in identifying rare diseases [17, 23, 24]. For instance, previous research [17] has shown that LLMs are effective in ranking causal genetic mutations based on their phenotypes. Another study [25, 26] highlights the proficiency of LLMs in predicting patients' phenotypes from clinical notes. Despite the promising capabilities of LLMs in diagnosing rare diseases, most existing studies utilize closed-source frameworks and data. This presents challenges for healthcare researchers, practitioners, and enthusiasts to reproduce results or deploy these models in practice.\nThis motivates us to contribute to open-source efforts that benefit practitioners and researchers in practice. Specifically, we have collected and open-sourced the dataset dedicated to rare disease question-answering, ReDis-QA. ReDis-QA serves as a benchmark dataset for assessing the capabilities of LLMs in diagnosing rare diseases. Additionally, we have open-sourced the first rare disease corpus, ReCOP. ReCOP can significantly enhance the performance of LLMs in diagnosing rare diseases through retrieval-augmented generation."}, {"title": "2.2 Retrieval Augmentation Generation.", "content": "Retrieval-Augmented Generation (RAG) can significantly enhance the performance of LLMs by leveraging retrieval-based methods to supply additional knowledge for inference. A standard RAG framework comprises three key elements: a corpus, retriever, and LLM. Specifically, the corpus in RAG systems serves as a comprehensive collection of knowledge. Retrievers play a crucial role in selecting the knowledge relevant to the question. For instance, dense retrievers like MedCPT [27] match corpus chunks with questions based on the similarity of their embeddings, while sparse retrievers like BM25 [28] match corpus chunks with questions according to word overlap. Finally, LLMs generate answers by using prompts based on the retrieved relevant knowledge, thereby providing more accurate and contextually appropriate answers.\nRAG is particularly powerful in applications requiring access to a large body of external knowledge for generating accurate solutions. One typical scenario is using LLMs for diagnosing rare diseases. This task heavily depends on the knowledge of rare diseases, which may not be included in the pre-training data. Therefore, having a corpus with high-quality knowledge is crucial for RAG systems. Generally, a corpus rich in rare disease-related information significantly benefits RAG in solving tasks related to rare diseases. Although there are existing corpora for RAG in medical tasks, such as PubMed [29], Textbook [27], StatPearls [30], and Wikipedia [31], there is still a lack of comprehensive knowledge about rare diseases to enhance LLMs' diagnostic capabilities. This motivates us to collect and open-source the first rare disease corpus, ReCOP, to improve the performance of LLMs in diagnosing rare diseases."}, {"title": "3 The Rare Disease Question Answering (ReDis-QA) Dataset", "content": "In this section, we introduce the Rare Disease Question-and-Answer (ReDis-QA) dataset in details. We hope the ReDis-QA dataset can provide a comprehensive resource for healthcare researchers, practitioners, and enthusiasts to explore the intricacies of rare diseases. The overall framework of collecting the ReDis-QA dataset is shown in Figure 1. The data collection pipeline includes the steps of data cleaning and labeling."}, {"title": "3.1 Data Source and Cleaning", "content": "The data sources for ReDis-QA include the MedMCQA [32], MedQA [33], and MMLU [34] datasets. Specifically, MedMCQA gathers multiple-choice questions and answers from the AIIMS and NEET PG entrance exams, covering 2400 healthcare topics and 21 medical subjects. MedQA is a large-scale medical question-answer dataset. The MMLU (Massive Multitask Language Understanding) dataset spans 57 subjects across STEM (Science, Technology, Engineering, and Mathematics), humanities, social sciences, and more, with difficulty levels ranging from elementary to advanced professional levels. In total, these sources provide over 200,000 raw question-answer pairs with high topical diversity. Since we target a dataset to benchmark the capabilities of LLMs in diagnosing rare diseases, the dataset cleaning process focused on removing questions irrelevant to rare diseases. After manual cleaning, we retained 1360 high-quality question-answer pairs relevant to rare disease diagnosis, building the ReDis-QA dataset. This dataset encompasses 205 rare diseases. Examples of questions are shown in Figure 1."}, {"title": "3.2 Data Labeling", "content": "Data labeling focuses on annotating meta-data for each question-answer pair. The meta-data includes the rare disease name and property for each question. The rare disease name indicates the type of rare disease the question addresses. The property specifies the type of knowledge required to answer the question, which can be one of the following values: symptoms, causes, effects, related disorders, diagnosis, or others.\nBased on the annotated meta-data, we show the statistics of ReDis-QA dataset in Figure 2. Specifically, the top-50 rare diseases in the ReDis-QA dataset are shown in Figure 2 (a). It is shown that it widely covers 205 types of rare diseases, where the most frequent disease features over 100 questions. Regarding the property of each question, as shown in Figure 2 (b), ReDis-QA includes 11%, 33%, 13%, 15%, 18% of the questions corresponding to the symptoms, causes, affects, related-disorders, diagnosis of rare diseases, respectively. The remaining 9% of the questions pertain to other properties of the diseases.\nTo illustrate the ReDis-QA dataset, here is an example question regarding the symptoms of Achalasia Cardia: Question: About Achalasia Cardia: 1. Dysphagia is a presenting symptom 2. The cause is the absence of Auerbach's plexus 3. Esophagectomy is the treatment 4. Motility improving agents are used in treatment 5. Barium swallow shows irregular filling defects in lower esophagus. Choices: (A) 1,2,3 False & 4,5 True (B) 1,2,4 True & 3,5 False (C) 2,3,4 True & 1,5 False (D) 1,3,5 True & 2,4 False Golden Answer: B. Additional examples corresponding to the symptoms, causes, affects, related-disorders, diagnosis are shown in Table 1."}, {"title": "4 Benchmark of LLMs on ReDis-QA Dataset", "content": "We benchmark open-sourced LLMs on the ReDis-QA dataset to study their capabilities of rare disease diagnosis.\nExperimental Setup. The experiments are conducted based on the Llama-2-7B [7], Mistral-7B-v0.2 [8], Phi-3-7B [9], Gemma-1.1-7B [10], and Qwen-2-7B [35] LLMs. These LLMs represent the state-of-the-art in natural language question answering. We load the instruction tuned versions of these models from the Huggingface platform [36]. The evaluation metric is the accuracy on the ReDis-QA dataset. The prompts for LLMs are given in Appendix 1.\nBenchmark Results. Figure 2 (b) illustrates the accuracy of LLMs in percentage, with accuracy for each subset of properties displayed separately. We have the following observations:\n\u2022 Overall Performance. Phi-3-7B shows the most competitive performance across all properties, while Llama-2-7B, Mistral-7B-v0.2, and Gemma-1.1-7B are less qualified than Phi-3-7B and Qwen-2-7B in rare disease diagnosis because they have accuracy less than 50% for all properites."}, {"title": "5 The Rare Disease Corpus", "content": "Easy and Hard Properties. LLMs show higher accuracy on questions of rare diseases' symptoms, causes, and effects, while exhibiting less qualification on questions of related disorders, diagnosis, and other properties.\nThe data for ReCOP is sourced from the National Organization for Rare Disorders (NORD) database\u2074, which compiles reports on rare diseases. NORD is committed to the identification, treatment, and cure of rare diseases through education, advocacy, research, and service programs. The primary objective of developing ReCOP using the NORD database is to provide comprehensive expertise on rare diseases for LLMs. This expertise can be leveraged to enhance the diagnostic capabilities of LLMs through retrieval-augmented generation. The pipeline for constructing ReCOP includes: data collection and data chunking steps, as illustrated in Figure 3."}, {"title": "5.1 Data Source", "content": "The data for ReCOP is sourced from the National Organization for Rare Disorder database, which contains professional reports on 1324 rare diseases. Each report includes comprehensive information on the symptoms, causes, effects, treatments, and clinical trials related to a specific rare disease. The reports are written in non-technical language, making them accessible to both non-professional individuals and LLMs. Additionally, NORD provides several details that enhance the ability of LLMs in rare disease diagnosis:\n\u2022 Genetic mutations. Each report details the specific genetic mutation causing the disease, if applicable. For example, Klinefelter Syndrome is caused by an extra X chromosome in cells.\n\u2022 Synonyms of diseases. Each report lists the synonyms of the disease to prevent misunderstandings that may arise from varying terminologies used in different literature.\n\u2022 Reference. NORD includes references to scientific articles, textbooks, and government agency reports for each report, ensuring the data's reliability and trustworthiness."}, {"title": "5.2 Data Chunking", "content": "Data chunking is a crucial step for retrieval augmentation. Chunks are the minimal units used to match queries and provide prompts during retrieval augmentation. A good chunking strategy ensures consistent relations between documents and queries, where a document is either relevant or irrelevant to the query, thus avoiding the issue where parts of a document are relevant while other parts are not. We consider the metadata of the ReDis-QA dataset to chunk the reports of each rare disease in the NORD database. To align with this metadata, ReCOP divides each rare disease report into chunks: overview, symptoms, causes, effects, related disorders, diagnosis, and standard therapies. Each property of the disease corresponds to a specific chunk in ReCOP. For example, Figure 3 illustrates the chunks for"}, {"title": "6 Benchmark of Retrieval Augmentation with ReCOP on ReDis-QA Dataset", "content": "We demonstrate that ReCOP can significantly improve LLM performance in rare disease QA by contributing to the retrieval augmentation generation (RAG) of LLMs. The experiments are based on the ReDis-QA dataset.\nExperimental Setup. The prerequisites for the experiments include LLMs, retrieval algorithms, and corpus data. Specifically, we use the Llama-2-7B, Mistral-7B-v0.2, Phi-3-7B, Gemma-1.1-7B, and Qwen-2-7B LLMs, consistent with the previous LLM benchmark experiments. For retrieval algorithms, we consider the meta-data retriever, Med-CPT (dense retriever) [27], and BM25 (sparse retriever) [28]. Concretely, the meta-data retriever matches questions and ReCOP chunks using rare disease names as keywords; MedCPT matches the questions with corpus chunks by their embeddings; and BM25 matches them by overlapped words. Additionally, we use the PubMed [29], Textbook [27], StatPearls [30], and Wikipedia [31] databases as baseline corpus for comparison. The prompts for LLMs w/o and w/ RAG are given in Appendix 1. The algorithm of combing baseline corpus with ReCOP is given in Appendix 4.\nBenchmark Results. Table 2 shows the accuracy of LLMs with and without RAG, where k indicates the number of retrieved chunks for prompting LLM inferences. Overall, we have the following observations:\n\u2022 LLMs w/ ReCOP vs. LLMs w/o RAG Table 2 demonstrates that LLM w/ ReCOP outperforms LLMs w/o RAG by an average of 8%. Figure 4 (a)-(d) further illustrates that LLMs w/ ReCOP exceeds LLMs w/o RAG performance across all six properties of the questions. These comprehensive results highlight the effectiveness of ReCOP in providing external knowledge to enhance LLM performance in rare disease question answering."}, {"title": "7 Case Studies on Natural Language Explanation", "content": "Natural language explanations are crucial for medical-related tasks. As noted in the literature [37], \"If a doctor told that you needed surgery, you would want to know why.\" Following this motivation, we investigate the explanations provided by LLMs for their answers. A case result is shown in Figure 5. It is observed that LLMs without RAG follow an incorrect inference process and produce wrong answers. In contrast, ReCOP not only guides LLMs to the correct answer but also ensures they follow the correct reasoning process, with explanations that can be traced back to existing literature on Microsomal Triglyceride Transfer Protein [38]. Additional cases of natural language explanations are provided in Appendix 3. By providing informative and rare disease-related knowledge for LLMs, ReCOP significantly enhances their trustworthiness, thereby promoting their application in real-world diagnostic scenarios."}, {"title": "8 Conclusion", "content": "In this work, we collect and open-source a rare disease question-answering dataset, ReDis-QA, for benchmark the capabilities of LLMs in this area. Specifically, it includes 1360 high-quality question-answer pairs covering 205 rare diseases. Each question is annotated with meta-data, facilitating the extraction of subsets specific to any given disease and its property. Based on the ReDis-QA dataset, we benchmark the performance existing open-source LLMs, revealing that the diagnosis of rare diseases remains a significant challenge for them. To improve their performance, we collect and open-source a rare disease corpus, ReCOP, to facilitate retrieval augmentation generation for rare disease diagnosis. Specifically, each data chunk in the ReCOP represents a different property of a disease, including their overview, symptoms, causes, effects, related disorders, diagnosis, and standard therapies. Experimental results demonstrate that ReCOP provides unique knowledge of rare diseases that is distinct from the existing corpus, significantly improving the accuracy of LLMs on the ReCOP dataset. Moreover, it significantly guilds LLMs to generate trustworthy answers and explanations that can be traced back to existing literature."}, {"title": "Appendix", "content": "1 Prompts\nThe prompts for LLMs with and without RAG is given in Figure 6.\nPrompts for LLMs w/o RAG: You are a helpful medical expert, and your task is to answer a multi-choice medical question. Please first choose the answer from the provided options and then provide the explanation.\nQuestion: {question}\nA. {choices[0]}\nB. {choices[1]}\nC. {choices[2]}\nD. {choices[3]}\nAnswer:\nPrompts for RAG: You are a helpful medical expert, and your task is to answer a multi-choice medical question using the relevant documents. Please first choose the answer from the provided options and then provide the explanation.\nRelevant Documents:\n{Document[0]}\n{Document[1]}\n{Document[k - 1]}\nQuestion: {question}\nA. {choices[0]}\nB. {choices[1]}\nC. {choices[2]}\nD. {choices[3]}\nAnswer:\n2 ReCOP Complement Existing Textbooks, StatPearls, PubMed, and Wikipedia Corpus\nWe show ReCOP's complement to existing Textbooks, StatPearls, PubMed, and Wikipedia corpus in Tables 3 and 4.\n3 Explanations of LLMs with ReCOP\nWe give more case studies of natural language explanations in Figures 7, 8, 9, and 10.\n4 Algorithm of Combining ReCOP with Other Corpus\nWe give the algorithm of combining ReCOP with baseline corpus in Algorithm 1."}, {"title": "Algorithm 1 Entropy-Aware Multi-Corpora Retrieval Augmented Generation", "content": "Input: Query q\nCorpora: Corpus X Cx, Corpus Y CY\nModel: Large Language Model M\nRetrievers: Retriever for Corpus X Rx, Retriever for Corpus Y Ry\nParameters: Number of relevant documents k\nOutput: Answer a \u2208 {A, B, C, D}\nStep 1: Retrieve Relevant Documents\nDx \u2190 RetrieveRelevantDocs (q, Cx, Rx,k)\nDy \u2190 RetrieveRelevantDocs(q, Cy, Ry, k)\nStep 2: Generate Response with LLM\nPx\u2190 GenerateResponse(q, Dx, M)\nPy \u2190 GenerateResponse(q, Dy, M)\nStep 3: Compute Probabilities for Options\nProbsx \u2190 ComputeProbs(Px, {A, B, C, D})\nProbsy \u2190 ComputeProbs(Py, {A, B, C, D})\nStep 4: Calculate Entropy\nHx \u2190 CalculateEntropy(Probsx)\nHy\u2190 CalculateEntropy(Probsy)\nStep 5: Select Answer Based on Entropy\nif Hx < Hy then\na \u2190 argmax(Probsx)\nelse\na \u2190 argmax(Probsy)\nend if\nReturn a"}]}