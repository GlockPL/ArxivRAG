{"title": "Assessing and Enhancing Large Language Models in\nRare Disease Question-answering", "authors": ["Guanchu Wang", "Junhao Ran", "Ruixiang Tang", "Chia-Yuan Chang", "Yu-Neng Chuang", "Zirui Liu", "Vladimir Braverman", "Zhandong Liu", "Xia Hu"], "abstract": "Despite the impressive capabilities of Large Language Models (LLMs) in general medical domains, questions remain\nabout their performance in diagnosing rare diseases. To answer this question, we aim to assess the diagnostic per-\nformance of LLMs in rare diseases, and explore methods to enhance their effectiveness in this area. In this work,\nwe introduce a rare disease question-answering (ReDis-QA) dataset to evaluate the performance LLMs in diagnosing\nrare diseases. Specifically, we collected 1360 high-quality question-answer pairs within the ReDis-QA dataset, cov-\nering 205 rare diseases. Additionally, we annotated meta-data for each question, facilitating the extraction of subsets\nspecific to any given disease and its property. Based on the ReDis-QA dataset, we benchmarked several open-source\nLLMs, revealing that diagnosing rare diseases remains a significant challenge for these models.\nTo facilitate retrieval augmentation generation for rare disease diagnosis, we collect the first rare diseases corpus\n(ReCOP), sourced from the National Organization for Rare Disorders (NORD) database. Specifically, we split the\nreport of each rare disease into multiple chunks, each representing a different property of the disease, including their\noverview, symptoms, causes, effects, related disorders, diagnosis, and standard therapies. This structure ensures that\nthe information within each chunk aligns consistently with a question. Experiment results demonstrate that ReCOP can\neffectively improve the accuracy of LLMs on the ReDis-QA dataset by an average of 8%. Moreover, it significantly\nguilds LLMs to generate trustworthy answers and explanations that can be traced back to existing literature. The\nReDis-QA dataset, ReCOP corpus, and source codes of benchmark experiments are open-sourced at here", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) show magnificent power in natural language processing, causal inference, code gen-\neration, and have widely applied to medical research [1, 2]. The flexibility and general capacity of LLMs allow them\nto be easily deployed to different scenarios of medical research, such as patient trail matching [3], healthcare chat-\nbot [4, 5], and clinical note summarization [6]. For example, LLMs like Llama [7], Mistral [8], Phi [9], Gemma [10]\ncan work on medical tasks in a zero-shot manner based on simple medical-related instructions. Additionally, they can\nbe further enhanced with adaptations or alignments on medical corpora, such as the PMC-Llama [11], Me-Llama [12],\nand BioMistral [13]. Despite the impressive capabilities of LLMs in general medical domains, questions persist about\nhow LLMs perform in diagnosing rare diseases?\nAlthough rare diseases affect only a small portion of the population, they collectively impose substantial burdens\non public health, affecting millions of individuals worldwide [14, 15]. Diagnosing and treating these conditions\nare particularly challenging due to their complex genetic origins and unpredictable clinical manifestations [16, 17],\nrequiring significantly intellectual decisions based on a vast knowledge base. LLMs are pre-trained on the corpora\ncomprising trillions of tokens, rivaling the expertise of human specialists [18, 19]. This extensive training enables\nLLMs to acquire sufficient medical knowledge for diagnosing common diseases [20, 21, 22]. However, LLMs still\nencounter several challenges when addressing tasks related to rare diseases, including:\n\u2022 Generalization Challenges of LLMs to Rare Diseases. Rare diseases affect a small number of people and\nare documented in limited literature. Pre-trained LLMs often struggle to generalize to rare diseases due to the"}, {"title": "2 Preliminaries", "content": "2.1 Large Language Models for Rare Disease Diagnosis.\nLarge language models (LLMs) have demonstrated competitiveness in identifying rare diseases [17, 23, 24]. For\ninstance, previous research [17] has shown that LLMs are effective in ranking causal genetic mutations based on\ntheir phenotypes. Another study [25, 26] highlights the proficiency of LLMs in predicting patients' phenotypes from\nclinical notes. Despite the promising capabilities of LLMs in diagnosing rare diseases, most existing studies utilize\nclosed-source frameworks and data. This presents challenges for healthcare researchers, practitioners, and enthusiasts\nto reproduce results or deploy these models in practice.\nThis motivates us to contribute to open-source efforts that benefit practitioners and researchers in practice. Specifically,\nwe have collected and open-sourced the dataset dedicated to rare disease question-answering, ReDis-QA. ReDis-QA\nserves as a benchmark dataset for assessing the capabilities of LLMs in diagnosing rare diseases. Additionally, we\nhave open-sourced the first rare disease corpus, ReCOP. ReCOP can significantly enhance the performance of LLMs\nin diagnosing rare diseases through retrieval-augmented generation."}, {"title": "2.2 Retrieval Augmentation Generation.", "content": "Retrieval-Augmented Generation (RAG) can significantly enhance the performance of LLMs by leveraging retrieval-\nbased methods to supply additional knowledge for inference. A standard RAG framework comprises three key ele-\nments: a corpus, retriever, and LLM. Specifically, the corpus in RAG systems serves as a comprehensive collection\nof knowledge. Retrievers play a crucial role in selecting the knowledge relevant to the question. For instance, dense\nretrievers like MedCPT [27] match corpus chunks with questions based on the similarity of their embeddings, while\nsparse retrievers like BM25 [28] match corpus chunks with questions according to word overlap. Finally, LLMs\ngenerate answers by using prompts based on the retrieved relevant knowledge, thereby providing more accurate and\ncontextually appropriate answers.\nRAG is particularly powerful in applications requiring access to a large body of external knowledge for generating\naccurate solutions. One typical scenario is using LLMs for diagnosing rare diseases. This task heavily depends on the\nknowledge of rare diseases, which may not be included in the pre-training data. Therefore, having a corpus with high-\nquality knowledge is crucial for RAG systems. Generally, a corpus rich in rare disease-related information significantly\nbenefits RAG in solving tasks related to rare diseases. Although there are existing corpora for RAG in medical tasks,\nsuch as PubMed [29], Textbook [27], StatPearls [30], and Wikipedia [31], there is still a lack of comprehensive\nknowledge about rare diseases to enhance LLMs' diagnostic capabilities. This motivates us to collect and open-source\nthe first rare disease corpus, ReCOP, to improve the performance of LLMs in diagnosing rare diseases."}, {"title": "3 The Rare Disease Question Answering (ReDis-QA) Dataset", "content": "In this section, we introduce the Rare Disease Question-and-Answer (ReDis-QA) dataset in details. We hope the\nReDis-QA dataset can provide a comprehensive resource for healthcare researchers, practitioners, and enthusiasts\nto explore the intricacies of rare diseases. The overall framework of collecting the ReDis-QA dataset is shown in\nFigure 1. The data collection pipeline includes the steps of data cleaning and labeling."}, {"title": "3.1 Data Source and Cleaning", "content": "The data sources for ReDis-QA include the MedMCQA [32], MedQA [33], and MMLU [34] datasets. Specifically,\nMedMCQA gathers multiple-choice questions and answers from the AIIMS and NEET PG entrance exams, cover-\ning 2400 healthcare topics and 21 medical subjects. MedQA is a large-scale medical question-answer dataset. The\nMMLU (Massive Multitask Language Understanding) dataset spans 57 subjects across STEM (Science, Technology,\nEngineering, and Mathematics), humanities, social sciences, and more, with difficulty levels ranging from elementary\nto advanced professional levels. In total, these sources provide over 200,000 raw question-answer pairs with high top-\nical diversity. Since we target a dataset to benchmark the capabilities of LLMs in diagnosing rare diseases, the dataset\ncleaning process focused on removing questions irrelevant to rare diseases. After manual cleaning, we retained 1360\nhigh-quality question-answer pairs relevant to rare disease diagnosis, building the ReDis-QA dataset. This dataset\nencompasses 205 rare diseases. Examples of questions are shown in Figure 1."}, {"title": "3.2 Data Labeling", "content": "Data labeling focuses on annotating meta-data for each question-answer pair. The meta-data includes the rare disease\nname and property for each question. The rare disease name indicates the type of rare disease the question addresses.\nThe property specifies the type of knowledge required to answer the question, which can be one of the following\nvalues: symptoms, causes, effects, related disorders, diagnosis, or others.\nBased on the annotated meta-data, we show the statistics of ReDis-QA dataset in Figure 2. Specifically, the top-50 rare\ndiseases in the ReDis-QA dataset are shown in Figure 2 (a). It is shown that it widely covers 205 types of rare diseases,\nwhere the most frequent disease features over 100 questions. Regarding the property of each question, as shown in\nFigure 2 (b), ReDis-QA includes 11%, 33%, 13%, 15%, 18% of the questions corresponding to the symptoms, causes,\naffects, related-disorders, diagnosis of rare diseases, respectively. The remaining 9% of the questions pertain to other\nproperties of the diseases.\nTo illustrate the ReDis-QA dataset, here is an example question regarding the symptoms of Achalasia Cardia: Ques- \ntion: About Achalasia Cardia: 1. Dysphagia is a presenting symptom 2. The cause is the absence of Auerbach's\nplexus 3. Esophagectomy is the treatment 4. Motility improving agents are used in treatment 5. Barium swallow\nshows irregular filling defects in lower esophagus. Choices: (A) 1,2,3 False & 4,5 True (B) 1,2,4 True & 3,5 False\n(C) 2,3,4 True & 1,5 False (D) 1,3,5 True & 2,4 False Golden Answer: B. Additional examples corresponding to the\nsymptoms, causes, affects, related-disorders, diagnosis are shown in Table 1."}, {"title": "4 Benchmark of LLMs on ReDis-QA Dataset", "content": "We benchmark open-sourced LLMs on the ReDis-QA dataset to study their capabilities of rare disease diagnosis.\nExperimental Setup. The experiments are conducted based on the Llama-2-7B [7], Mistral-7B-v0.2 [8], Phi-3-\n7B [9], Gemma-1.1-7B [10], and Qwen-2-7B [35] LLMs. These LLMs represent the state-of-the-art in natural lan-\nguage question answering. We load the instruction tuned versions of these models from the Huggingface platform [36].\nThe evaluation metric is the accuracy on the ReDis-QA dataset. The prompts for LLMs are given in Appendix 1.\nBenchmark Results. Figure 2 (b) illustrates the accuracy of LLMs in percentage, with accuracy for each subset of\nproperties displayed separately. We have the following observations:\n\u2022 Overall Performance. Phi-3-7B shows the most competitive performance across all properties, while Llama-\n2-7B, Mistral-7B-v0.2, and Gemma-1.1-7B are less qualified than Phi-3-7B and Qwen-2-7B in rare disease\ndiagnosis because they have accuracy less than 50% for all properites."}, {"title": "5 The Rare Disease Corpus", "content": "The data for ReCOP is sourced from the National Organization for Rare Disorders (NORD) database\u2074, which com-\npiles reports on rare diseases. NORD is committed to the identification, treatment, and cure of rare diseases through\neducation, advocacy, research, and service programs. The primary objective of developing ReCOP using the NORD\ndatabase is to provide comprehensive expertise on rare diseases for LLMs. This expertise can be leveraged to enhance\nthe diagnostic capabilities of LLMs through retrieval-augmented generation. The pipeline for constructing ReCOP\nincludes: data collection and data chunking steps, as illustrated in Figure 3."}, {"title": "5.1 Data Source", "content": "The data for ReCOP is sourced from the National Organization for Rare Disorder database, which contains pro-\nfessional reports on 1324 rare diseases. Each report includes comprehensive information on the symptoms, causes,\neffects, treatments, and clinical trials related to a specific rare disease. The reports are written in non-technical lan-\nguage, making them accessible to both non-professional individuals and LLMs. Additionally, NORD provides several\ndetails that enhance the ability of LLMs in rare disease diagnosis:\n\u2022 Genetic mutations. Each report details the specific genetic mutation causing the disease, if applicable. For\nexample, Klinefelter Syndrome is caused by an extra X chromosome in cells.\n\u2022 Synonyms of diseases. Each report lists the synonyms of the disease to prevent misunderstandings that may\narise from varying terminologies used in different literature.\n\u2022 Reference. NORD includes references to scientific articles, textbooks, and government agency reports for each\nreport, ensuring the data's reliability and trustworthiness."}, {"title": "5.2 Data Chunking", "content": "Data chunking is a crucial step for retrieval augmentation. Chunks are the minimal units used to match queries\nand provide prompts during retrieval augmentation. A good chunking strategy ensures consistent relations between\ndocuments and queries, where a document is either relevant or irrelevant to the query, thus avoiding the issue where\nparts of a document are relevant while other parts are not. We consider the metadata of the ReDis-QA dataset to chunk\nthe reports of each rare disease in the NORD database. To align with this metadata, ReCOP divides each rare disease\nreport into chunks: overview, symptoms, causes, effects, related disorders, diagnosis, and standard therapies. Each\nproperty of the disease corresponds to a specific chunk in ReCOP. For example, Figure 3 illustrates the chunks for"}, {"title": "6 Benchmark of Retrieval Augmentation with ReCOP on ReDis-QA Dataset", "content": "We demonstrate that ReCOP can significantly improve LLM performance in rare disease QA by contributing to the\nretrieval augmentation generation (RAG) of LLMs. The experiments are based on the ReDis-QA dataset.\nExperimental Setup. The prerequisites for the experiments include LLMs, retrieval algorithms, and corpus data.\nSpecifically, we use the Llama-2-7B, Mistral-7B-v0.2, Phi-3-7B, Gemma-1.1-7B, and Qwen-2-7B LLMs, consistent\nwith the previous LLM benchmark experiments. For retrieval algorithms, we consider the meta-data retriever, Med-\nCPT (dense retriever) [27], and BM25 (sparse retriever) [28]. Concretely, the meta-data retriever matches questions\nand ReCOP chunks using rare disease names as keywords; MedCPT matches the questions with corpus chunks by their\nembeddings; and BM25 matches them by overlapped words. Additionally, we use the PubMed [29], Textbook [27],\nStatPearls [30], and Wikipedia [31] databases as baseline corpus for comparison. The prompts for LLMs w/o and w/\nRAG are given in Appendix 1. The algorithm of combing baseline corpus with ReCOP is given in Appendix 4.\nBenchmark Results. Table 2 shows the accuracy of LLMs with and without RAG, where k indicates the number of\nretrieved chunks for prompting LLM inferences. Overall, we have the following observations:\n\u2022 LLMs w/ ReCOP vs. LLMs w/o RAG Table 2 demonstrates that LLM w/ ReCOP outperforms LLMs w/o\nRAG by an average of 8%. Figure 4 (a)-(d) further illustrates that LLMs w/ ReCOP exceeds LLMs w/o RAG\nperformance across all six properties of the questions. These comprehensive results highlight the effectiveness\nof ReCOP in providing external knowledge to enhance LLM performance in rare disease question answering."}, {"title": "7 Case Studies on Natural Language Explanation", "content": "Natural language explanations are crucial for medical-related tasks. As noted in the literature [37], \"If a doctor told\nthat you needed surgery, you would want to know why.\" Following this motivation, we investigate the explanations\nprovided by LLMs for their answers. A case result is shown in Figure 5. It is observed that LLMs without RAG follow\nan incorrect inference process and produce wrong answers. In contrast, ReCOP not only guides LLMs to the correct\nanswer but also ensures they follow the correct reasoning process, with explanations that can be traced back to existing\nliterature on Microsomal Triglyceride Transfer Protein [38]. Additional cases of natural language explanations are\nprovided in Appendix 3. By providing informative and rare disease-related knowledge for LLMs, ReCOP significantly\nenhances their trustworthiness, thereby promoting their application in real-world diagnostic scenarios."}, {"title": "8 Conclusion", "content": "In this work, we collect and open-source a rare disease question-answering dataset, ReDis-QA, for benchmark the\ncapabilities of LLMs in this area. Specifically, it includes 1360 high-quality question-answer pairs covering 205 rare\ndiseases. Each question is annotated with meta-data, facilitating the extraction of subsets specific to any given dis-\nease and its property. Based on the ReDis-QA dataset, we benchmark the performance existing open-source LLMs,\nrevealing that the diagnosis of rare diseases remains a significant challenge for them. To improve their performance,\nwe collect and open-source a rare disease corpus, ReCOP, to facilitate retrieval augmentation generation for rare dis-\nease diagnosis. Specifically, each data chunk in the ReCOP represents a different property of a disease, including\ntheir overview, symptoms, causes, effects, related disorders, diagnosis, and standard therapies. Experimental results\ndemonstrate that ReCOP provides unique knowledge of rare diseases that is distinct from the existing corpus, signif-\nicantly improving the accuracy of LLMs on the ReCOP dataset. Moreover, it significantly guilds LLMs to generate\ntrustworthy answers and explanations that can be traced back to existing literature."}, {"title": "Appendix", "content": "1 Prompts\nThe prompts for LLMs with and without RAG is given in Figure 6.\nPrompts for LLMs w/o RAG: You are a helpful medical expert, and your task is to answer a multi-choice medical\nquestion. Please first choose the answer from the provided options and then provide the explanation.\nQuestion: {question}\nA. {choices[0]}\nB. {choices[1]}\nC. {choices[2]}\nD. {choices[3]}\nAnswer:\nPrompts for RAG: You are a helpful medical expert, and your task is to answer a multi-choice medical question using\nthe relevant documents. Please first choose the answer from the provided options and then provide the explanation.\nRelevant Documents:\n{Document[0]}\n{Document[1]}\n{Document[k - 1]}\nQuestion: {question}\nA. {choices[0]}\nB. {choices[1]}\nC. {choices[2]}\nD. {choices[3]}\nAnswer:\nFigure 6: Prompts for LLMs with and without RAG on the ReDis-QA dataset.\n2 ReCOP Complement Existing Textbooks, StatPearls, PubMed, and Wikipedia Corpus\nWe show ReCOP's complement to existing Textbooks, StatPearls, PubMed, and Wikipedia corpus in Tables 3 and 4.\n3 Explanations of LLMs with ReCOP\nWe give more case studies of natural language explanations in Figures 7, 8, 9, and 10.\n4 Algorithm of Combining ReCOP with Other Corpus\nWe give the algorithm of combining ReCOP with baseline corpus in Algorithm 1."}, {"title": "Algorithm 1 Entropy-Aware Multi-Corpora Retrieval Augmented Generation", "content": "Input: Query q\nCorpora: Corpus X Cx, Corpus Y CY\nModel: Large Language Model M\nRetrievers: Retriever for Corpus X Rx, Retriever for Corpus Y Ry\nParameters: Number of relevant documents k\nOutput: Answer a \u2208 {A, B, C, D}\nStep 1: Retrieve Relevant Documents\nDx \u2190 RetrieveRelevantDocs(q, Cx, Rx,k)\nDy \u2190 RetrieveRelevantDocs(q, Cy, Ry, k)\nStep 2: Generate Response with LLM\nPx\u2190 GenerateResponse(q, Dx, M)\nPy \u2190 GenerateResponse(q, Dy, M)\nStep 3: Compute Probabilities for Options\nProbsx \u2190 ComputeProbs(Px, {A, B, C, D})\nProbsy \u2190 ComputeProbs(Py, {A, B, C, D})\nStep 4: Calculate Entropy\nHx \u2190 CalculateEntropy(Probsx)\nHy\u2190 CalculateEntropy(Probsy)\nStep 5: Select Answer Based on Entropy\nif Hx < Hy then\na \u2190 argmax(Probsx)\nelse\na \u2190 argmax(Probsy)\nend if\nReturn a"}]}