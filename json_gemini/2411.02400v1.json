{"title": "Decomposition Dilemmas: Does Claim Decomposition Boost or Burden\nFact-Checking Performance?", "authors": ["Qisheng Hu", "Quanyu Long", "Wenya Wang"], "abstract": "Fact-checking pipelines increasingly adopt the\nDecompose-Then-Verify paradigm, where texts\nare broken down into smaller claims for indi-\nvidual verification and subsequently combined\nfor a veracity decision. While decomposition is\nwidely-adopted in such pipelines, its effects on\nfinal fact-checking performance remain under-\nexplored. Some studies have reported improve-\nments from decompostition, while others have\nobserved performance declines, indicating its\ninconsistent impact. To date, no comprehensive\nanalysis has been conducted to understand this\nvariability. To address this gap, we present an\nin-depth analysis that explicitly examines the\nimpact of decomposition on downstream ver-\nification performance. Through error case in-\nspection and experiments, we introduce a cate-\ngorization of decomposition errors and reveal a\ntrade-off between accuracy gains and the noise\nintroduced through decomposition. Our anal-\nsis provides new insights into understanding\ncurrent system's instability and offers guidance\nfor future studies toward improving claim de-\ncomposition in fact-checking pipelines.", "sections": [{"title": "Introduction", "content": "Fact-checking is a critical task that typically in-\nvolves evaluating the veracity of claims or reports.\nWith the rise of large language models (LLMs),\nthe scope of fact-checking task has expanded to\ninclude verifying content generated by LLMs (Sun\net al., 2024). While progress has been made in\nreducing LLM hallucinations, the problem has not\nyet been resolved, presenting ongoing challenges\nin ensuring the factuality of LLM outputs (Huang\net al., 2023). At the same time, many studies\nfocus on LLM-driven automated fact-checking\npipelines (Min et al., 2023; Wei et al., 2024; Chern\net al., 2023), aiming to improve the efficiency of\nthe fact-checking processes."}, {"title": "Related Work", "content": "2.1 Decompose-Then-Verify\nDecompose-Then-Verify pipeline has shown effec-\ntive for pinpointing errors and improving factual\nprecision (Wang et al., 2024). It involves decom-\nposing text into sub-claims, retrieving supporting\nmaterials from a knowledge source, and using a\nverifier to assess each sub-claim. The individual\nresults are subsequently aggregated to produce a\nfinal verification. FactScore (Min et al., 2023) and\nSAFE (Wei et al., 2024) proposed decomposing\nLLM's generated biography into atomic facts and"}, {"title": "Decompose-Then-Verify Framework\nSetup", "content": "3.1 Fact-Checking Task & Metrics\nIn the Decompose-Then-Verify pipeline, fact-\nchecking is framed as a binary classification task,\nwhere the goal is to predict whether an input text\nis 'Supported' or 'Unsupported' based on retrieved\nevidence. Consistent with prior work (Laban et al.,\n2022; Tang et al., 2024; Zhao et al., 2024b), we\nadopt balanced accuracy (BAcc) and F1-score as\nthe primary metrics, addressing potential class im-\nbalance issue.\n3.2 Pipeline Structure and Settings\nAs shown in Figure 1, we conduct experiments\nusing a Decompose-Then-Verify pipeline consisting\nof four stages:\nDecompose Given an input text, it is first decom-\nposed into sub-claims using a specified method and\nlanguage model2. Language models are commonly\nused for decomposition with specific instructions\nand few-shot demonstrations, where the instruc-\ntions vary based on the decomposition objective.\nFor instance, FactScore emphasizes breaking in-\nputs into atomic facts, while VeriScore focuses on\nverifiability. As shown in Table 1, we use the de-\ncomposition modules from VeriScore, FactScore,\nand Wice for our experiments. The prompt in-\nstructions for these decomposition methods are pro-\nvided in the Appendix D.5. For the baseline, no\ndecomposition is applied, and the original input is\ndirectly used for retrieval and verification.\nRetrieve Each sub-claim is used to retrieve evi-\ndence from a knowledge source. Following prior\nwork (Li et al., 2024; Chern et al., 2023), we use\nthe Wikipedia corpus for WICE, retrieving the top\n3 articles as evidence, and Google Search as the\nexternal knowledge source for other datasets, re-\ntrieving the top 10 search results as evidence.\nVerify The verification process is typically for-\nmulated as a natural language inference (NLI) prob-\nlem, where the claim is treated as the hypothesis\nand the retrieved evidence as the premise. A lan-\nguage model is then used to determine whether\nthe premise entails the hypothesis (Supported) or\nnot (Unsupported). To investigate the effect of de-\ncomposition when using distinct verifiers, we em-\nploy three different models: AlignScore-large (Zha\net al., 2023), Minicheck (Bespoke-MiniCheck-\n7B) (Tang et al., 2024), and the few-shot classi-\nfication module from VeriScore (Song et al., 2024),\nbuilt on GPT-40-mini (OpenAI, 2024a). Align-\nScore and Minicheck generate entailment scores\nranging from 0 to 1 for each sub-claim and the\ncorresponding retrieved evidence, while GPT-40-\nmini's entailment score is computed by exponenti-\nating the log probability of the token \u2018Supported'.\nMore details are provided in the Appendix A.3.\nAggregate To reach a final decision, the verifica-\ntion scores of all sub-claims are aggregated. Using\nthe minimum score can be sensitive to verifier er-\nrors (Kamoi et al., 2023). To mitigate this, we\nfollow WICE and adopt the harmonic mean to ag-\ngregate the sub-claim scores into a final score. A\nfinal score above 0.5 is classified as 'Supported',\nwhile scores below are labeled 'Unsupported'."}, {"title": "Datasets", "content": "We explore two dataset granularities: claim-level\nand response-level. Claim-level datasets contain in-\ndividual statements for verification, while response-\nlevel datasets comprise longer, LLM-generated re-\nsponses. The rationale for selecting the below"}, {"title": "What Determines Decomposition's\nEffect on Fact-Checking Performance?", "content": "Based on the inconsistent impact of decomposition\nobserved in the Decompose-Then-Verify pipeline,\nwe identify three primary factors contributing to\nthe inconsistencies: decomposition method, veri-\nfier strength, and input granularity. Our analysis\ndemonstrates that each factor exerts a distinct influ-\nence, with no single method uniformly enhancing\ndownstream performance across all granularities\nand verifiers. Notably, verifier strength proves to\nbe a key determinant of performance."}, {"title": "Input Granularity", "content": "Claim-level Table 2 shows the performance of\ndifferent decomposition methods combined with\ntwo verifiers (AlignScore and Minicheck) on\nthe WICE dataset. With AlignScore, Wice and\nFactScore decompositions showed positive im-\npacts, leading to higher BAcc and F1 scores than\nthe baseline, while VeriScore had a negative effect.\nHowever, when using Minicheck verifier, all meth-\nads resulted in lower BAcc and F1 compared to\nthe baseline. Complete details and results of the\nclaim-level experiments are provided in Table 7\nand 8 in Appendix B.1.\nResponse-level Table 3 presents the performance\nof different decomposition methods combined with\ntwo verifiers (Minicheck and GPT-40-mini). Com-\npared to the baselines, two trends emerged: (1) A\nsignificant increase in F1 when usingmMinicheck,\nwhereas (2) a notable drop in F1 with GPT-40-\nmini's few-shot classification. The detailed results\nof the response-level experiments are presented in"}, {"title": "Strong & Weak Verifier", "content": "Based on baseline performances in Table 2 and 3,\nMinicheck demonstrates stronger verification capa-\nbilities compared to AlignScore, yet remains less\neffective than GPT-40-mini. Our analysis reveals\nthat decomposition generally benefits weaker veri-\nfiers, while it tends to negatively affect stronger ver-\nification systems. These findings align with prior\nstudies: Tang et al. (2024) observe no improve-\nment with decomposition for Minicheck, whereas\nKamoi et al. (2023) report gains using a weaker\nNLI model. Similarly, Zhao et al. (2024b) find\nclaim-based methods advantageous for ChatGPT\nbut detrimental for GPT-4. Despite these isolated\nobservations, no systematic exploration has been\nconducted. Our study identifies a strong correlation\nbetween verifier strength and the impact of decom-\nposition, with further discussion on the trade-offs\nprovided in Section 6."}, {"title": "What Errors May Decomposition\nIntroduce?", "content": "While previous studies (Wanner et al., 2024; Min\net al., 2023; Song et al., 2024) have identified\n'atomicity', 'coverage', and \u2018verifiability' as key\nfactors in decomposition, less focus has been\nplaced on errors that may harm downstream fact-\nchecking performance. To explore this, we manu-\nally inspect cases where the baseline was correct\nbut decomposition led to incorrect outcomes."}, {"title": "Error Types", "content": "Upon inspection, we identify the following cate-\ngories of decomposition errors that can jeopardize\ndownstream fact-checking performance:\nA. Omission of Context Information Excluding\ncritical elements necessary for accurately under-\nstanding the input. This category encompasses:\n\u2022 Missing Core Claims or Key Details. The\nexclusion of essential background details that\nprovide necessary context for understanding\nthe input. Without these elements, sub-claims\ncan become incomplete or misleading."}, {"title": "Missing Logical Relationships.", "content": "Omission of\nrelationships, such as causal, comparative, or\ncontrastive relationship, which explain how\ndifferent parts of the input relate to one an-\nother. These relationships are crucial for un-\nderstanding the interactions within the origi-\nnal input text.\nB. Ambiguity Decomposing into sub-claims that\nare unclear or vague can result in multiple interpre-\ntations, hindering accurate verification. This refers\nto Vague Language, the use of terms or references\nthat lack specificity, making the claim unclear. This\nhas been exemplified by Wei et al. (2024). Exam-\nples include:\n1. Ambiguous pronouns that lack clear referents\n(e.g., \"his,\" \"they,\" \"her\").\n2. Vague references to unspecified entities (e.g.,\n\"this event,\" \"the research,\" \"the invention\").\n3. Incomplete names (e.g., \"Jeff...\" or \"Bezos...\"\ninstead of \"Jeff Bezos\").\nC. Over-Decomposition Excessive fragmenta-\ntion of the claim into redundant sub-claims or re-\npeated information, resulting in increased complex-\nity and potential misinterpretation of the original\nmeaning. This category involves:\n\u2022 Redundant Information. Repetition of infor-\nmation that does not provide additional value,\nincluding sub-claims that reiterate the same\ncontent without offering distinct insights.\n\u2022 Excessive Fragmentation. Breaking down\nthe input into too many sub-claims that over-\nexplain basic facts, resulting in high complex-\nity and potential misinterpretation of the orig-\ninal meaning.\nD. Alteration of Original Meaning Introduc-\ning excessive, fabricated, or contradictory informa-\ntion that changes the original meaning of the claim.\nThis includes misrepresentation or adding elements\nnot present in the original input."}, {"title": "Error Distribution", "content": "For error detection, given the input text and its\ndecomposed sub-claims, we prompt GPT-4o in a\nfew-shot manner to reason first then determine the"}, {"title": "Reflection", "content": "Previous studies have shown that prompting mod-\nels to reflect on and refine their own responses can\nimprove outcomes (Madaan et al., 2024). Simi-\nlarly, we tune the error detection prompt to use\nreasoning and verdict as feedback for refining the\ninitial decomposition."}, {"title": "What Explains the Variability in\nFact-Checking Performance?", "content": "While factors contributing to variability and decom-\nposition errors have been identified, the reasons\nfor decomposition-induced performance variability\nremain unclear. We propose that this variability\nresults from a decomposition trade-off and conduct\nexperiments to empirically investigate it."}, {"title": "Explanation", "content": "Let $A(k)$ represent the verifier's accuracy for an\ninput with complexity k, we simply use sequence\nlength and the number of claims as a proxy for\ninput complexity. Generally, verification accu-\nracy is expected to decrease as input complexity"}, {"title": "Complexity Scale Up", "content": "In Section 4.1, we observe that decomposition leads\nto a performance decline on WICE with Minicheck,\nthis decline is likely due to the marginal accuracy\ngain (\u0394\u0391) from decomposition being insufficient\nto offset the increased noise. To explore whether a\nlarger \u0394\u0391 can mitigate the noise and improve per-\nformance, we increase the input complexity."}, {"title": "Complexity Scale Down", "content": "As shown in Section 4.1, decomposition greatly\nimproved performance on FELM with Minicheck,\nlikely because the substantial accuracy gain (\u0394\u0391)\noutweighed the increased noise. Thus, we scale\ndown input complexity and test if this leads to any\nperformance degradation."}, {"title": "Diverse Complexity Scaling", "content": "To investigate the trade-off in a more fine-grained\ncomplexity scenario, we use claim annotations\nfrom BINGCHAT. Each response-level entry con-\ntains multiple single-sentence claim annotations.\nWe generate combinations of these claims while\npreserving their original order, with the number\nof claims serving as a proxy for input complexity.\nFor example, if a response has three claims (C1,\nC2, C3), the resulting combinations are (C1, C2,"}, {"title": "Summary", "content": "The complexity scaling experiments reveal the\ninherent trade-off in the Decompose-Then-Verify\npipeline. While decomposition shows beneficial in\nhandling complex inputs, where accuracy gains out-\nweigh the noise introduced, it becomes less effec-\ntive for simpler inputs, where the additional noise\nnegates the benefits. Moreover, as sub-claims num-\nber increases, the impact of decomposition could\nshift from positive to negative. The findings high-\nlight the need to balance decomposition granularity.\nTo maximize its effectiveness, more efforts should\nfocus on improving decomposition strategies to bet-\nter handle varying input complexities and minimize\nerrors introduced in the process."}, {"title": "Conclusion", "content": "We present a comprehensive analysis of decom-\nposition's impact on downstream performance in\nthe Decompose-Then-Verify pipeline. We explic-\nitly show the fact-checking performance variations\nacross decomposition methods, input granularities,\nand verifiers. Through detailed inspection, we cate-\ngorize decomposition errors and reveal their distri-\nbution across methods and granularities. Further-\nmore, we reveal a trade-off between the accuracy\ngains from decomposition and the noise introduced\nby decomposition, offering an explanation for the\ninconsistencies reported in prior studies."}, {"title": "Limitations", "content": "This analysis focuses on a binary fact-checking\nclassification task and does not address multi-class\nclassification tasks with additional veracity labels,\nsuch as 'Partially Supported'. We did not explore\npipelines that transform decomposed claims into\nquestion-like queries for retrieving answers from\na knowledge source, instead of using quantified\nentailment scores as done here. We plan to explore\nthese aspects in future research.\nIn our trade-off analysis, retrieval noise (Er) and\ndecomposition noise (Ed) were not explicitly quan-\ntified due to the complexity of accounting for var-\nious factors. In practice, decomposition could re-\nsult in sub-claims with varying complexities. To\nsimplify the analysis, we assumed an average sub-\nclaim complexity of $k_i$, which is generally lower\nthan the original input complexity $k_o$. This as-\nsumption is a relaxed approximation for the sake\nof clarity. We empirically demonstrated and visu-\nalized the existence of the trade-off. Additionally,\nin the complexity scaling experiments, we used in-\nput length as a proxy for complexity, though other\nmethods for representing input complexity could\nbe explored."}]}