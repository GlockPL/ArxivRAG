{"title": "Superhuman performance of a large language model\non the reasoning tasks of a physician", "authors": ["Peter G. Brodeur", "Thomas A. Buckley", "Zahir Kanjee", "Ethan Goh", "Evelyn Bin Ling", "Priyank Jain", "Stephanie Cabral", "Raja-Elie Abdulnour", "Adrian Haimovich", "Jason A. Freed", "Andrew Olson", "Daniel J. Morgan", "Jason Hom", "Robert Gallo", "Eric Horvitz", "Jonathan Chen", "Arjun K. Manrai", "Adam Rodman"], "abstract": "Performance of large language models (LLMs) on medical tasks has traditionally been\nevaluated using multiple choice question benchmarks. However, such benchmarks are highly\nconstrained, saturated with repeated impressive performance by LLMs, and have an unclear\nrelationship to performance in real clinical scenarios. Clinical reasoning, the process by which\nphysicians employ critical thinking to gather and synthesize clinical data to diagnose and\nmanage medical problems, remains an attractive benchmark for model performance. Prior LLMs\nhave shown promise in outperforming clinicians in routine and complex diagnostic scenarios.\nWe sought to evaluate OpenAl's 01-preview model, a model developed to increase run-time via\nchain of thought processes prior to generating a response. We characterize the performance of\n01-preview with five experiments including differential diagnosis generation, display of\ndiagnostic reasoning, triage differential diagnosis, probabilistic reasoning, and management\nreasoning, adjudicated by physician experts with validated psychometrics. Our primary outcome\nwas comparison of the o1-preview output to identical prior experiments that have historical\nhuman controls and benchmarks of previous LLMs. Significant improvements were observed\nwith differential diagnosis generation and quality of diagnostic and management reasoning. No\nimprovements were observed with probabilistic reasoning or triage differential diagnosis. This\nstudy highlights 01-preview's ability to perform strongly on tasks that require complex critical\nthinking such as diagnosis and management while its performance on probabilistic reasoning\ntasks was similar to past models. New robust benchmarks and scalable evaluation of LLM\ncapabilities compared to human physicians are needed along with trials evaluating Al in real\nclinical settings.", "sections": [{"title": "INTRODUCTION", "content": "Artificial intelligence (AI) diagnostic support tools have been studied since the 1950s and have\nrelied upon a variety of computational strategies, including regression modeling, naive Bayesian\ncalculators, rule-based systems, and natural-language processing tools. 1-6 Recently, large\nlanguage models (LLMs) have exceeded the performance of prior approaches. Several\nfoundation models now outperform medical students, residents, and attending physicians in\nboth routine and complex diagnostic benchmarks.7,8,9 Additional studies have evaluated\nprompting and fine-tuning strategies to further improve reasoning, including chain of thought\n(CoT) prompting.10 On September 12, 2024, OpenAl released the o1-preview model, which\nexecutes a native CoT process at run-time that allows the model to take more time to \"think\u201d and\n\"reason\" through complex tasks.11,12 This model has shown superior ability over GPT-4 in solving\ncomplex informatics, mathematics, and engineering problems, as well as benchmark medical\nquestion-answering datasets.13,14,15\nHowever, existing multiple choice evaluations do not represent the breadth or complexity of\nclinical decision-making, and a growing body of literature suggests that models may exploit\nmultiple choice question sets partly via their semantic structures, effectively becoming \"good test\ntakers.\"15,16 In reality, clinical practice requires real-time complex multi-step reasoning"}, {"title": "RESULTS", "content": "Quality of Differential Diagnoses on New England Journal of Medicine\nClinicopathological Conferences\nWe first evaluated 01-preview using the clinicopathologic conferences (CPCs) published by the\nNew England Journal of Medicine (NEJM), a standard for the evaluation of differential\ngenerators since the 1950s.5 There was substantial agreement between the two physicians\nevaluating the quality of 01-preview's differential diagnosis (agreement on 120/143 cases [84%],\n\u03ba=0.66). 01-preview included the correct diagnosis in its differential in 78.3% of cases (95% CI,\n70.7% to 84.8%) (Figure 1). The first diagnosis suggested was the correct diagnosis in 52% of\ncases (95% CI, 44% to 61%). We did not find evidence of a significant difference in\nperformance before and after the pre-training cutoff date for o1-preview (79.8% accuracy\nbefore, 73.5% accuracy after, p=0.59). Examples of o1-preview solving a complex case are\nshown in Table 1.\nOn 70 cases evaluated using GPT-4 in a prior study, 01-preview produced a response with the\nexact or a very close diagnosis in 88.6% of cases, compared to 72.9% of cases by GPT-4\n(p=.015, Figure 2).\nWe next evaluated the ability of 01-preview to select the next diagnostic test in the NEJM CPCs.\nTwo physicians scored the suggested test plan produced by o1-preview (agreement on 113/132\ncases [86%], \u03ba=0.28), with respect to the actual management of the patient described in the\nCPC. The proportion of agreements was high, but the kappa was low due to severe class\nimbalance. In 87.5% of cases, o1-preview selected the correct test to order, in another 11% of\ncases the chosen testing plan was judged by the two physicians to be helpful, and in 1.5% of\ncases it would have been unhelpful (Figure 3). Examples are shown in Table 2.\nPresentation of reasoning in NEJM Healer Diagnostic Cases\nWe used 20 clinical medical cases from the NEJM Healer curriculum17 that were also evaluated\nin a prior study using GPT-4.9 NEJM Healer cases are virtual patient encounters designed for\nthe assessment of clinical reasoning.17 There was substantial agreement of Revised-IDEA\n(R-IDEA) scores, a validated 10-point scale for evaluating four core domains of documenting\nclinical reasoning, 18 between the two physicians (agreement on 79/80 [99%] cases, \u043a=0.89). For\n78/80 of the cases, o1-preview achieved a perfect R-IDEA score. This compared favorably to"}, {"title": "DISCUSSION", "content": "We evaluated the medical reasoning abilities of the 01-preview model across five diverse\nexperiments, comparing the model to historical controls of human baselines and GPT-4. As in\nnon-medical studies, we saw significant gains in performance for most tasks for o1-preview. For\ndifferential diagnosis generation, o1-preview surpasses both GPT-4 and previous non-LLM\ndifferential generators, as well as the human baseline. We saw similar gains in display of\nreasoning and management reasoning compared to prior studies.8,23 We did not see\nimprovements in either probabilistic reasoning or critical diagnosis identification over GPT-4,\nthough probabilistic reasoning was still superior to the human baseline. o1-preview appears to\nexcel in many higher order tasks that require critical thinking, such as diagnosis and\nmanagement, while performing less well at tasks that require abstraction such as probabilistic\nreasoning.24\nThis rapid pace of improvement in LLMs has major implications for science and practice of\nclinical medicine. Our study shows consistent and superhuman performance on many\nhuman-adjudicated medical reasoning tasks. While applying Al to assist with clinical decision\nsupport is sometimes viewed as a high-risk endeavor,25 26 27 greater use of these tools might\nserve to mitigate the enormous human and financial costs of diagnostic error and delay. 28 29\nThese findings suggest the need for trials to evaluate these technologies in real-world patient\ncare settings and prepare for investments in complementary innovations with computing\ninfrastructure and design for clinician-Al interaction that can facilitate the integration of Al tools\ninto patient-care workflows. This includes the development of robust monitoring frameworks to\noversee the broader implementation of Al clinical decision support systems. 25\nOur findings also have implications for the creation of new benchmarks to assess where and\nhow Al models should be integrated into clinical reasoning workflows. Multiple-choice question\nbenchmarks are not realistic proxies for high-stakes medical reasoning.16 The NEJM CPCs have\nbeen used since 1959 because of their difficulty; 01-preview is able to produce a high quality\ndifferential in almost 90 percent of cases, although using highly curated information from the\npresentation of case. The capabilities of o1-preview highlight that our most challenging\nbenchmarks for diagnostic reasoning in medicine are becoming saturated, matching recent\nobservations being made with other Al-challenge problem benchmarks that have been used as\nmetrics of intelligence.30,31,32 Measurement of clinical management reasoning is in its infancy;\nour current metrics involve a laborious reference standard grading process that is not scalable\nfor rapid evaluations.33 Given the pace of model development, additional challenging and\nrealistic evaluations are needed, including adding modalities34 and enriching the existing\nbenchmarks to become more pragmatic alongside best practices to identify and develop new\nbenchmarks.\nOur study has several limitations. First, 01-preview tends towards verbosity, and while this was\nnot the main factor in the original studies with GPT-4, it is possible this could have led to higher\nscoring in these experiments. Second, while some of the experiments were originally performed\nwith human-computer interaction, our current study reflects only model performance. Further"}, {"title": "METHODS", "content": "Model\nThe 01-preview model (\u201co1-preview-2024-09-12\u201d) was accessed through OpenAl's Application\nProgramming Interface (API).\nNEJM Clinicopathologic Conference Cases\nWe selected all 143 diagnostic cases from 2021 to September 2024 (cases including the section\n\"Differential Diagnosis\"). There were 70 of these cases, published between 2021 and 2022, that\nwere also evaluated in a prior study of GPT-4.8 For differential diagnosis prediction, we adapted\nthe prompt from the prior study of GPT-4 (Supplement 1A). After prediction of differential\ndiagnoses, we queried the model with \u201cWhat diagnostic tests would you order next given this\ndifferential?\" in the same conversation.\nOur primary outcomes were differential diagnosis quality and the quality of the suggested testing\nplan. Differential diagnoses were rated independently by two attending internal medicine\nphysicians (Z.K., A.R), using a previously-developed scoring system called the Bond Score.36\nBond Scores range from zero to five36, where five represents a differential list that contains the\nexact target diagnosis and zero represents a differential list that has no suggestions close to the\ntarget diagnosis (Supplement 1B). The quality of the testing plan was scored using a Likert\nscale from zero to two, by comparing the suggested testing plan to the actual diagnostics\nperformed in the case. A score of two represents the diagnostics were appropriate and nearly"}]}