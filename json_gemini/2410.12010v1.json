{"title": "Bias Similarity Across Large Language Models", "authors": ["Hyejun Jeong", "Shiqing Ma", "Amir Houmansadr"], "abstract": "Bias in machine learning models has been a chronic problem, especially as these models influence decision-making in human society. In generative AI, such as Large Language Models, the impact of bias is even more profound compared to the classification models. LLMs produce realistic and human-like content that users may unconsciously trust, which could perpetuate harmful stereotypes to the uncontrolled public. It becomes particularly concerning when utilized in journalism or education.\nWhile prior studies have explored and quantified bias in individual AI models, no work has yet compared bias similarity across different LLMs. To fill this gap, we take a comprehensive look at ten open- and closed-source LLMs from four model families, assessing the extent of biases through output distribution. Using two datasets-one containing 4k questions and another with one million questions for each of the four bias dimensions-we measure functional similarity to understand how biases manifest across models. Our findings reveal that 1) fine-tuning does not significantly alter output distributions, which would limit its ability to mitigate bias, 2) LLMs within the same family tree do not produce similar output distributions, implying that addressing bias in one model could have limited implications for others in the same family, and 3) there is a possible risk of training data information leakage, raising concerns about privacy and data security. Our analysis provides insight into LLM behavior and highlights potential risks in real-world deployment.", "sections": [{"title": "Introduction", "content": "In the era of Artificial Intelligence (AI), the concept of model fairness has gained increasing importance as AI technologies are integrated into societal decision-making processes. Whether in healthcare, employment, or finances, the decisions influenced by AI can have profound consequences on people's lives [7]. This concern is amplified in generative Al systems, like Large Language Models (LLMs), which generate human-like content. The societal impact is especially critical when deployed in journalism and education; these systems can perpetuate or exacerbate existing inequalities [28.\nMeanwhile, as generative AI becomes the norm, understanding similarities and differences between models helps users better evaluate and utilize various functioning LLMs. In addition to facilitating LLMs, model similarity detection will be utilized to prevent illegal model reuse that infringes copyright [14]. For example, if a suspect model is believed to have stolen knowledge or functionality from another, comparing the two models becomes necessary. However, a direct structural comparison is often impractical-structural similarity does not necessarily imply illegal knowledge stealing; it is not trivial to distinguish between the original and copied model [17]. This is particularly true for private models, where there are neither open parameters to compare nor established benchmarks to determine how similar two models might be.\nTo this end, researchers have developed indirect comparison methods, such as functional or capability-based similarity measurement, which assess the performance of models on specific downstream tasks to determine their degree of similarity [10, 15, 17]. These methods provide a more nuanced approach to model comparison, particularly when direct structural comparisons fall short.\nPrevious research has indicated that LLMs within the same family tend to exhibit similar performance trends and capabilities. [34]. Since bias has been a major concern in LLMs, it is crucial to define bias in this context first. Various papers have defined bias differently, but the gist of the definition is that a biased model impacts the outputs, leading to disparate and negative outcomes for certain sociodemographic groups [9, 19, 24]. For instance, ChatGPT, one of the well-known LLMs, has been found to exhibit biases across multiple dimensions, including gender, sexual orientation, age, and race [5].\nPrior works quantified bias and introduced debiasing techniques, but there is no work yet comparing the extent of similarity of bias in multiple LLMs. Analyzing the similarity of bias within LLM families would imply the possibility of a universal debiasing technique for a model family, so this analysis could be a considerable breakthrough in developing future techniques. Eventually, the observation inspired us to a research question: How do LLMs exhibit biases across different models? In this paper, we conduct a comparative analysis of LLMs to answer the question."}, {"title": "Related Works", "content": "This section summarizes works that are relevant to ours: LLM bias assessment and similarity detection."}, {"title": "Bias Assessment", "content": "As LLMs have shown rapid performance improvements, other concerns, such as fairness, security, and privacy, have come to the forefront of research. Numerous studies have already shown that Lanague models embed biases across various dimensions, including gender, religion, nationality, ethnicity, age, sexual orientation, and socioeconomic status. In response, researchers have focused on identifying or quantifying bias for different proprietary and open-sourced LLMs [3]. Several benchmarks have been developed to assess and quantify bias. For instance, StereoSet [21] and CrowS-pairs [22] focus on evaluating masked language models, while UnQover[16] and BBQ [25] are question-answering datasets designed to measure how strongly responses reflect social biases in under-informative context.\nBias has been defined in various ways in literature: systemic errors that differentiate social groups [20], skewed model performance across different sociodemographic groups [11, 24], and the presence of misclassification and misrepresentation, which negatively representing certain social groups [19, 35]. Gallegos et al. [9] offered a comprehensive definition, discribing bias as unequal treatment or outcomes between social groups rooted in historical and structural power imbalances, leading to representational harms and discrimination.\nNonetheless, defining bias is nontrivial due to the impossibility of drawing a clear line between bias and genuine demographic reflection. For example, if an LLM is prompted, \"Who tends to adapt to new technologies more easily: older or younger people?\" it would likely respond with \"younger people,\" based on scientific and demographic facts that as people age, physical and cognitive health"}, {"title": "LLM Similarity Detection", "content": "Understanding LLM similarity has several practical applications, such as simplifying model selection, preventing illegal reuse, and improving the interpretation of model behavior. Wu et al. [34] took a detailed approach by comparing neuron- and representation-level similarities across five pre-trained language models and their variants. Their study showed that representation-level similarities were high across models regardless of their family or architecture, but neuron-level similarities varied significantly between models. Interestingly, models within the same family, defined as those sharing the same architecture but differing in parameter size, exhibit the highest level of similarity across both representation and neuron levels. This reinforces the notion that model families tend to behave similarly, but it also raises questions about the fine-grained differences that exist within and between model families. Klabunde et al. [15] further explored Centered Kernel Alignment (CKA) and Representation Similarity Analysis (RSA) to compare the representation of the last layer before the final classifier of 7B LLMs.\nDirect comparison of model weights and activations is often infeasible due to different model accessibility (white-box vs. black-box models) limiting the access to internal representations [15], heterogeneous architecture, and distinct tasks [17]. This leaves room for further exploration of alternative methods that can be applied even in black-box scenarios.\nTo address this, researchers have turned to functional similarity measures. One common strategy involves performance-based metrics, where models are considered similar if they achieve comparable results on downstream tasks, such as accuracy or F1 score. For instance, SAT [12] measured similarity between 69 image classifiers through adversarial attack transferability, demonstrating that the models with adversarial task performance are likely to share decision-making similarities. Similarly, ProFLingo [13] proposed a black-box fingerprinting-based copyright protection scheme by generating an adversarial example and assessing its effectiveness on a suspect model. Despite its convenient single-scalar comparisons, they can offer only a partial view of similarity, leading to the misinterpretation [14]. Furthermore, especially for generative models, it becomes much more difficult due to the vast and diverse output space [15].\nAnother approach is prediction-based similarity, which considers models similar if their predictions for the same input agree, regardless of correctness [15]. Additionally, distance metrics such as norms, JS divergence, Gaussian RBF, or cosine similarity are used to measure prediction confidence levels [10, 27]. ModelDiff [17], for instance, measures the behavioral patterns of models by analyzing their decision boundaries on normal and adversarial inputs. They introduced the Decision Distance Vector (DDV) concept to capture"}, {"title": "Bias Similarity Measurement", "content": "To answer the research question, \"How do LLMs exhibit biases across different LLMs?\" we perform a similarity analysis of the output distributions from ten open- and closed-source LLMs: Llama 2, Llama 3, Gemma 1, Gemma 2, and their instruction fine-tuned variants, as well as Alpaca, and Vicuna.\nWe define bias as stereotypical assumptions about certain groups and unequal answer distributions-for instance, disproportionately answering certain demographic groups in responses to neutral questions without clear demographic cues.\nFor the analysis, we utilized the BBQ and UnQover datasets, each with ten and four bias dimensions, respectively. In this paper, we consider four common dimensions: gender, ethnicity, religion, and nationality. Definition and an example of each bias dimension are summarized in Table 1. Results of additional BBQ bias dimensions are included in Appendix D."}, {"title": "Models and Datasets", "content": "Models. We use ten open-source and proprietary pre-trained LLMs with roughly 7B parameters: Llama-2-7b 1, Llama-2-7b-chat 2 [32], Llama-3-8B 3, Llama-3-8B-Instruct 4 [6], Alpaca 7B 5 [29], Vicuna-7b-v1.56 [4], Gemma-7b 7, Gemma-7b-it 8 [30], Gemma-2-9b9, and Gemma-2-9b-it 10 [31]. Note that Alpaca and Vicuna are supervised fine-tuned Llama on instruction following and conversation data, respectively. The models suffixed with \"chat,\" \"Instruct,\" or \"it\" are instruction-tuned versions of corresponding base models. Instruction-tuned models are fine-tuned for conversational tasks and are known to be less safety-violating [32].\nDatasets. We used two multiple-choice datasets for bias assessment. Bias Benchmark for QA (BBQ) [25] is a dataset along nine sociodemographic bias dimensions (i.e., age, disability, gender identity, nationality, physical appearance, race/ethnicity, religion, socioeconomic status, and sexual orientation) where each dimension contains approximately 5k samples. Each data sample consists of a context (either ambiguous or disambiguated) and three multiple-choice answers (target, non-target, and unknown). The blue-shaded box in Figure 1 illustrates two data samples with the same question but different contexts. On the left, the context is ambiguous (under-informative), while on the right, it is disambiguated (adequately informative). For the ambiguous context, the correct answer is \"cannot be determined\" due to the lack of sufficient information. In contrast, for the disambiguated context, the correct answer is the non-biased target answer. Thus, we measured output (answer) distribution for questions with ambiguous contexts and accuracy for disambiguated questions.\nThe UnQover dataset [16] was developed to probe and quantify bias along four dimensions (gender, ethnicity, religion, and nationality) through underspecified questions. We used at least 150k samples for each dimension. Each data sample consists of a context, a question (either negative or positive as shown in Figure 11), and two multiple-choice answers. Unlike BBQ, UnQover does not provide a correct answer between the two choices; there are only negative and positive questions. Additionally, while the BBQ dataset includes an option for \"cannot be determined,\" UnQover does not. Since the BBQ dataset includes golden labels, we prompted LLMs in a two-shot manner, whereas they were prompted in a zero-shot manner for UnQover. Detailed prompts for each dataset are provided in Appendix B."}, {"title": "Similarity Assessment Metrics", "content": "We used four metrics for the multiple-choice bias assessment task: accuracy, histogram, cosine distance, and Jensen-Shannon (JS) divergence. Considering the UnQover data set has only four bias dimensions, we reported the common four dimensions from the BBQ dataset for a fair comparison. Results for the remaining bias dimensions in BBQ are included in Appendix D. We additionally report the Chi-squared distance between models in Appendix C.\nAccuracy. The disambiguated questions in the BBQ dataset have a target choice that is unbiased in any direction. Therefore, we measure the accuracy of each LLM's output for these questions across the four bias dimensions. This metric is applicable only for disambiguated-contexted questions.\nHistogram. For ambiguous questions in BBQ and for UnQover, we generate histograms with each bin corresponding to a different answer choice. In the gender dimension, for example, if the available answer choices are female, male, transgender, and unknown, we plot these as four distinct bins in the histogram. This helps visualize the distribution of model outputs across different choices.\nCosine Distance. This metric measures the angular difference between two vectors in high-dimensional space. It is well-suited for modeling output distributions and comparing the directionality of the models' outputs [2]. This metric captures whether the models consistently lean toward certain groups [26], even if the magnitude of their output probabilities varies. Since cosine distance is often applied to count-based data [1], we do not normalize the counts to maintain their effectiveness on raw count vectors. We calculate the pairwise cosine similarity between models and then subtract the result from 1 to obtain the distance."}, {"title": "Results", "content": "This section reports the result of our bias assessment across multiple LLMs. Below, we describe how they are functionally similar (accuracy), how they are differently distributed (histogram), how each model's decision counts are similar regardless of the dataset size (cosine distance), and how each model's decision probabilities differ (JS divergence).\nNote that the \"nationality\" dimension in the UnQover dataset has 70 countries. For consistency with the BBQ dataset, we group countries into regions: Africa, Arab states, Asia Pacific, Europe, Latin South America, Middle East, North America, and Unknown."}, {"title": "Measuring Similarity through Accuracy", "content": "Following previous works to measure accuracy or F1 score to identify neural networks' functional similarity, we also assessed accuracy for disambiguated-context questions from the BBQ dataset. These questions have target choices that reflect unbiased answers. The accuracy is visualized in Figure 2. Observing the result, we find that Llama2, Llama2-chat, and Alpaca performed similarly, but Vicuna showed distinct behavior from its base model, Llama2. We can also observe the discrepancies in accuracy across various dimensions. For example, in the physical appearance dimension, Llama3-chat and Gemma2-it recorded lower accuracy, while they excelled in dimensions like age, disability, gender, nationality, and"}, {"title": "Measuring Output Distribution through Histogram", "content": "To examine how output distributions vary across models, we draw histograms for both BBQ and UnQover datasets (see Figure 3 and"}, {"title": "Observation on Race and Religion Dimensions.", "content": "We can see that graph shapes for some dimensions (i.e., Race and Religion) look alike across the two datasets. In both cases, \"unknown\" counts far exceeded other choices, making the remaining options appear insignificant. This trend is more pronounced in models fine-tuned for instruction-following, such as Llama3-chat, Alpaca, Gemma-it,"}, {"title": "Observation on Nationality and Gender Dimensions.", "content": "For nationality and gender dimensions, the histograms varied more across the two datasets. In the BBQ dataset, unknown counts still exceed the other choices by a large margin. However, in the UnQover dataset, the unknown count is significantly lower, showing greater variation in choices across LLMs."}, {"title": "Cosine Distance between LLMs' Output Distribution", "content": "The left half of Figure 5 and Figure 6 illustrate the pair-wise cosine similarity measurement between model outputs for each bias dimension in the BBQ and UnQover datasets. We can observe that the fine-tuned versions of the model are very close to their base models: Llama2, Llama2-chat, Vicuna, and Alpaca, Llama3 and Llama3-chat, Gemma and Gemma-it, Gemma2 and Gemma2-it, exhibit nearly identical outputs.\nHowever, a significant distance (> 0.34) exists between Llama2 and Llama3; their discrepancy is larger than that between Gemma and Gemma2. Interestingly, Gemma2-it is extremely close to Llama3-chat, as the cosine distance is 0 for all dimensions in the BBQ dataset.\nOn the other hand, the UnQover dataset reveals some differences. They are the same in that a model and their fine-tuned variants are very similar to each other, except Gemma2-it, which is closer to Gemma-it than to Gemma2. Except for the gender dimension, all Llama2, 3, and their fine-tuned variants, plus Gemma and Gemma2, output similarly except Gemma-it and Gemma2-it. The gender dimension shows larger discrepancies between Vicuna and Llama2, although Vicuna is a fine-tuned version of Llama2. Also, the distances between Gemma-it or Gemma2-it and the rest exhibit relatively smaller distances from the other models for gender compared to the other dimensions."}, {"title": "JS Divergence between LLMs' Output Distribution", "content": "The right half of Figure 5 and Figure 6 illustrate the pair-wise Jensen-Shannon Divergence between model outputs for each bias dimension. JS Divergence is primarily used for probabilities [18]; we normalize the count such that we can get the probabilities and then calculate JS divergence from the probability distribution. The shapes of JS divergence closely mirror the cosine distance. One notable difference is that in the BBQ dataset, JS divergences show greater variance between models compared to cosine distance. In contrast, cosine distance captures more significant variance between models than JS divergence."}, {"title": "Discussion", "content": "Throughout the extensive experiments, we aim to analyze bias across multiple LLMs by going beyond scalar performance metrics like accuracy and f1 score. Instead, we focused on the distribution of model outputs. This comprehensive approach reveals three findings: limited impact of fine-tuning and/or family lineage and possible privacy infringement.\nFine-tuning does not significantly modify output distributions. Our findings suggest that fine-tuning does not significantly alter the output distribution of LLMs. Techniques such as instruction tuning primarily focus on aligning model behavior with human-centered objectives, making the models more fair and non-harmful. The limited parameter updates constrain the extent to which output distributions can shift while preserving the model's general-purpose text generation capabilities. As a result, their token usage patterns and linguistic features remain largely unchanged.\nFrom Figure 2, we can assume that the fine-tuned versions generally produce fairer answers than their base models. However, we cannot simply draw a conclusion because the functional similarity differs across downstream tasks, as shown in Appendix A, ending up with misleading analysis. Thus, we performed additional experiments to compare the output distribution rather than a single scalar value. If fine-tuning meaningfully modified output distributions, we would expect to observe significant differences between the base models and their fine-tuned variants. For example, models like Llama2-chat or Alpaca should exhibit distinct output patterns from the original Llama2. However, our results show minimal differences across bias dimensions in the BBQ dataset. As seen in Figure 3, the count differences between Llama2 and its fine-tuned variants (Llama2-chat, Vicuna, and Alpaca) are trivial across all dimensions. Pairwise similarity measures in Figure 5, including cosine distance and JS divergence, confirm that these models make identical choices. If fine-tuning had substantially modified the behavior, we would expect a larger distributional shift, especially in ambiguous scenarios, but the changes are minimal.\nNevertheless, there are two notable exceptions: Llama3-chat and Gemma2-it. These models exhibit noticeable behavioral shifts, particularly in the religion dimension of the BBQ dataset, where Gemma2-it selects \"unknown\" more confidently (from 262 to 557 after fine-tuning). Similarly, in the UnQover dataset, these models show slight but noticeable differences in cautiousness. While these exceptions demonstrate that fine-tuning can sometimes induce behavioral changes, the changes are neither consistent nor predictable across all models or dimensions, limiting fine-tuning's role as a reliable bias mitigation strategy.\nLLMs within the same family tree do not necessarily produce similar output distribution. Our study's findings reveal that LLMs within the same family tree do not necessarily produce similar output distributions. This observation challenges the common assumption that models originating from the same family, typically sharing core design features (e.g., transformer-based encoders, tokenization schemes, or pre-training corpora), will exhibit functional similarity and, thus, a similar output distribution. Instead, our results indicate that the outputs of these models can vary significantly despite their shared lineage.\nAs shown in Figure 3 and Figure 4, the output distribution within Llama or Gemma families (Llama2 and 3 or Gemma and Gemma2) differ by a large margin; rather, they are more similar between the base and fine-tuned model. The difference is significant when we see the unknown column of Llama3 and Gemma2. Observing Figure 5, Llama3-chat and Gemma2-it produce the same output distributions for all dimensions in the BBQ dataset, although it only holds true for the gender dimension in the UnQover dataset (Figure 6). We presume this is due to different training corpora, which will be discussed soon.\nThe divergence in output distributions among related LLMs raises several challenges for fairness. Relying on models from the same family under the assumption of similar behavior may result in unanticipated biases or inconsistencies in practice. Even if one model exhibits a bias toward a particular demographic or ideological stance, it cannot be assumed that its siblings or successor will behave similarly. While family membership provides useful context,"}, {"title": "Limitation", "content": "Our study has several limitations that should be acknowledged. First, the bias assessment was conducted on only four to ten dimensions, depending on the datasets. Since the available datasets do not cover the same bias dimensions, our analysis is constrained, preventing a deeper exploration of specific biases across all relevant demographic categories. Expanding the scope to include more dimensions would provide a more comprehensive understanding of bias in LLMs.\nSecond, while we evaluated the models on multiple-choice question answering (QA) and summarization tasks, our work remains limited in scope, as it does not explore fully open-ended language generation. Given that language generation in real-world applications is often unconstrained, future research should assess LLM performance on open-ended tasks to better capture potential biases and behavioral patterns beyond structured settings.\nFinally, we focused exclusively on 7B parameter models. It would be valuable to compare models with different sizes within the same family to examine how scaling affects performance and bias behavior. For example, comparing Llama2 7B and Llama2 70B could provide insights into whether larger models exhibit similar or reduced biases, contributing to our understanding of how model size impacts fairness and output distributions."}, {"title": "Conclusion", "content": "Beginning with the research question, \"How do LLMs exhibit biases across different LLMs?\" we conducted a comparative analysis of bias across various LLMs. Our findings show that models originating from the same family do not necessarily exhibit similar biases, suggesting that addressing bias in one model could influence others in the same family, presenting a promising direction for future debiasing techniques. Additionally, we found that fine-tuning has limited effectiveness in altering output distributions, indicating that current fine-tuning strategies may be insufficient for mitigating bias. Lastly, our analysis revealed a potential risk of training data leakage during bias assessment, raising important concerns about privacy and data security. Overall, this study not only emphasizes the similarity of bias across models but also provides insights into developing more effective debiasing strategies and comprehensive similarity assessments."}]}