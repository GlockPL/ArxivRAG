{"title": "Understanding LLM Development Through Longitudinal Study: Insights from the Open Ko-LLM Leaderboard", "authors": ["Chanjun Park", "Hyeonwoo Kim"], "abstract": "This paper conducts a longitudinal study over eleven months to address the limitations of prior research on the Open Ko-LLM Leaderboard, which have relied on empirical studies with restricted observation periods of only five months. By extending the analysis duration, we aim to provide a more comprehensive understanding of the progression in developing Korean large language models (LLMs). Our study is guided by three primary research questions: (1) What are the specific challenges in improving LLM performance across diverse tasks on the Open Ko-LLM Leaderboard over time? (2) How does model size impact task performance correlations across various benchmarks? (3) How have the patterns in leaderboard rankings shifted over time on the Open Ko-LLM Leaderboard?. By analyzing 1,769 models over this period, our research offers a comprehensive examination of the ongoing advancements in LLMs and the evolving nature of evaluation frameworks.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of large language models (LLMs) (Zhao et al., 2023) has led to the creation of various leaderboards designed to evaluate their performance across a wide range of tasks (Li et al., 2023b; Lee et al., 2023; Hughes and Bae, 2023; BigCode, 2023; Li et al., 2023a). Among these, the Open LLM Leaderboard (Beeching et al., 2023; Fourrier et al., 2024) developed by Hugging Face (Jain, 2022) has achieved significant global recognition. In the context of Korean language models, the Open Ko-LLM Leaderboard (Park et al., 2024) was established to specifically assess LLM performance within the Korean language environment.\nWhile previous analyses of the Open Ko-LLM Leaderboard (Park et al., 2024) have provided valuable insights into LLM performance, they have been constrained observation periods of only five months, limiting their ability to capture long-term trends. To better understand the ongoing evolution and inherent challenges in LLM development, a more comprehensive and extended analysis is required. This paper addresses this gap by conducting a detailed longitudinal study of the Open Ko-LLM Leaderboard, guided by three primary research questions:\nFirst, we analyze the longitudinal changes in performance across five tasks monitored by the Open Ko-LLM Leaderboard. These tasks are designed to evaluate various capabilities of LLMs, including reasoning, natural language understanding, and common sense knowledge. By examining data collected over a eleven-month period, this study aims to identify which capabilities have presented the greatest challenges for LLM developers, which tasks have reached performance saturation rapidly, and which tasks continue to pose significant difficulties. This analysis will provide quantitative insights into performance trends across different tasks, thereby guiding targeted research efforts and highlighting key areas that require further advancement to push the boundaries of model development.\nSecond, we explore the correlations between different tasks based on model size. This aspect of the study examines how the performance across different tasks varies depending on the scale of the model. Understanding these correlations will provide insights into the interaction between model capacity and task performance, offering a deeper understanding of how scaling influences overall effectiveness across tasks.\nThird, we examine the evolution of leaderboard dynamics from the initial stages to the present by focusing on three key aspects: the correlations between task performances in the early months compared to the entire eleven-month period, the temporal changes in performance based on model type, and the shifts in performance relative to model size."}, {"title": "2 Empirical Analysis", "content": "This comprehensive analysis offers insights into the evolving interplay among tasks and the influence of various model characteristics on LLM performance throughout different phases of development."}, {"title": "2.1 Challenges in Enhancing Task Performance Over Time", "content": "What are the specific challenges in improving LLM performance across diverse tasks on the Open Ko-LLM Leaderboard over time?. To investigate this question, we conducted a comprehensive analysis of performance trends over a eleven-month period across all tasks on the Open Ko-LLM Leaderboard, including Ko-HellaSwag (commonsense reasoning)(Zellers et al., 2019), Ko-ARC (commonsense and scientific reasoning)(Clark et al., 2018), Ko-MMLU (multitask language understanding and domain knowledge)(Hendrycks et al., 2020), Ko-CommonGEN V2 (commonsense generation)(Seo et al., 2024), and TruthfulQA (truthfulness) (Lin et al., 2021).\nFigure 1 and Table 1 show the varying performance patterns of LLMs across these tasks over the eleven-month period. Certain tasks, such as Ko-HellaSwag and Ko-TruthfulQA, exhibit rapid improvements in performance and early saturation. Specifically, Ko-HellaSwag reached a score of 50 almost immediately and achieved 80 by week 26, while Ko-TruthfulQA showed comparable progress, reaching a score of 80 within 25 weeks. These trends indicate that current LLMs are particularly well-suited for tasks requiring straightforward commonsense reasoning and truthfulness, suggesting a relatively lower barrier to achieving performance enhancements in these domains.\nConversely, tasks such as Ko-MMLU and Ko-CommonGEN V2 show slower, more gradual improvements without clear signs of saturation, highlighting their increased complexity and the deeper understanding required from LLMs. Ko-MMLU took 13 weeks to reach a score of 50 and then stabilized around 60 after 26 weeks, indicating a limit to the current models capabilities. Similarly, Ko-CommonGEN V2, despite reaching a score of 50 relatively quickly, showed minimal progress beyond 60. These patterns highlight the significant challenges LLMs face in tasks that demand complex reasoning and specialized knowledge, suggesting these are important areas for further research.\nThe initial rapid gains in Ko-ARC, followed by minimal progress beyond a score of 60 after 17 weeks, indicate that while LLMs can quickly adapt to certain tasks, their progress is constrained by the need for more complex reasoning skills. This underscores the importance of developing more challenging benchmarks to better evaluate the limitations and capabilities of LLMs, especially in tasks that require more advanced forms of reasoning.\nOverall, these findings emphasize the need to include a broad range of complex tasks to comprehensively assess LLM capabilities. While some tasks demonstrate rapid performance saturation, others present ongoing challenges, serving as essential benchmarks for guiding future advancements in LLM development."}, {"title": "2.2 The Influence of Model Size on Task Performance Correlations", "content": "How does model size impact task performance correlations across various benchmarks?. To investigate this question, we analyze how model size affects performance improvements across different tasks, using a framework similar to previous studies (Park et al., 2024). For this analysis, models were divided into three size categories: under 3 billion parameters, 3 to 7 billion parameters, and 7 to 14 billion parameters. This categorization allows for a detailed examination of how scaling impacts task performance."}, {"title": "2.3 Temporal Shifts in Leaderboard Ranking Patterns", "content": "How have the patterns in leaderboard rankings shifted over time on the Open Ko-LLM Leaderboard?. To investigate this question, we extended our analysis to an eleven-month period to see if the initial trends, defined as those observed during the initial five months in the previous study by Park et al. (2024), remained consistent or if new patterns emerged over time. This longer timeframe allows us to capture shifts in model performance and ranking dynamics.\nTask Correlations Over Time. Figure 3 shows the correlation analysis between tasks during the initial phases of the leaderboard and over the full eleven-month period. A notable increase was observed in the correlation between Ko-Truthful QA and other tasks, especially Ko-Hellaswag. This correlation, initially very low at 0.01, rose significantly to 0.5 over time. This change suggests that as higher-performing models, particularly those with 7 billion parameters or more, were introduced, the alignment between tasks became stronger. For most other tasks, correlations remained relatively stable, reflecting their initial patterns.\nPerformance Trends by Model Type. Figure 4 presents the performance trends over time for different model types. As noted in previous research (Park et al., 2024), improvements in instruction-tuned models typically lagged behind those of pretrained models by about one week."}, {"title": "A Open Ko-LLM Leaderboard", "content": "The Open Ko-LLM Leaderboard (Park et al., 2024) is a pioneering platform designed to evaluate large language models (LLMs) specifically in the Korean language, addressing the limitations of predominantly English-focused benchmarks. This leaderboard mirrors the structure of the globally recognized Open LLM Leaderboard by Hugging Face (Beeching et al., 2023), ensuring consistency and comparability across languages. It is built on two key principles: alignment with the English leaderboard and the use of private test sets to avoid data contamination, thereby enhancing evaluation robustness.\nThe leaderboard employs the Ko-H5 benchmark, comprising five tasks that assess various aspects of language understanding and generation in Korean. These tasks are designed to comprehensively evaluate LLM capabilities. The first task, Ko-Hellaswag (Zellers et al., 2019), tests commonsense reasoning by requiring models to complete sentences contextually and logically. The second task, Ko-ARC (Clark et al., 2018), adapted from the English ARC, evaluates both commonsense and scientific reasoning through multiple-choice questions. Ko-MMLU (Hendrycks et al., 2020), the third task, assesses multitask language understanding and domain knowledge across various subjects, requiring models to respond accurately to questions from different domains. The fourth task, Ko-CommonGen V2 (Seo et al., 2024), focuses on commonsense generation, where models must create coherent sentences from given concepts, testing their ability to connect common knowledge meaningfully. Lastly, Ko-TruthfulQA (Lin et al., 2021) evaluates a model ability to provide truthful and accurate responses, crucial for assessing the factual integrity of LLMs in real-world scenarios.\nThrough the Ko-H5 benchmark, the Open Ko-LLM Leaderboard provides a robust framework for evaluating Korean LLMs and promotes linguistic diversity in LLM evaluation. By incorporating tasks that reflect Korean linguistic and cultural nuances, the leaderboard offers valuable insights into LLM performance beyond English, encouraging a more inclusive approach to language model evaluation."}, {"title": "B Additional Analysis", "content": "Figure 6 presents the monthly distribution of submissions across different model types on the Open Ko-LLM leaderboard. Initially, pretrained models constituted 37% of all submissions, but this proportion declined sharply over time, with no pretrained models submitted by August 2024. This trend signals a diminishing focus on pretrained models within the community, which is concerning given their foundational importance discussed in Section 2. Therefore, a renewed emphasis on fostering interest and engagement with pretrained models could help address this emerging gap.\nOn the other hand, instruction-tuned models, which started at 61%, consistently dominated the submissions, maintaining a steady presence of 70-80% each month. This trend suggests that the community perceives instruction-tuned models as highly effective or suitable for the tasks evaluated. Additionally, RL-tuned models, though initially making up only 2% of submissions, gradually increased to a peak of 29%, reflecting a growing interest in exploring reinforcement learning approaches within the leaderboard context. This variety indicates a healthy exploration of diverse model types, but also highlights areas where community focus could be broadened or rebalanced.\nIn addition, Table 2 presents the monthly statistics for both the number of model submissions and the number of completed model evaluations. The Model Submissions Count refers to the total number of models submitted to the leaderboard each month. In contrast, the Model Evaluation Count represents the number of these submitted models that successfully completed the evaluation process.\nThe discrepancy between the Model Submissions Count and the Model Evaluation Count is due to instances where some models fail to complete the evaluation phase on the leaderboard. This failure can occur for several reasons, such as models being too large to be processed within the available computational resources or issues related to library support and compatibility. As a result, not all submitted models are evaluated successfully, highlighting potential challenges and areas for improvement in handling diverse model architectures on the leaderboard."}, {"title": "3 Conclusion", "content": "This study provides a longitudinal analysis of the Open Ko-LLM Leaderboard, revealing key performance trends: smaller models face scalability limitations, while larger models experience saturation without advancements in pretrained models. These findings highlight the need for continuous innovation in pretrained models to enhance LLM capabilities. Additionally, the study shows that analyzing leaderboard data offers valuable insights into the evolving dynamics of LLM performance."}]}