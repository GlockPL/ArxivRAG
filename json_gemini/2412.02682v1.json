{"title": "THE ASYMPTOTIC BEHAVIOR OF ATTENTION IN TRANSFORMERS", "authors": ["\u00c1. RODR\u00cdGUEZ ABELLA", "J.P. SILVESTRE", "P. TABUADA"], "abstract": "A key component of transformers is the attention mechanism orchestrating how each token influences the propagation of every other token through a transformer. In this paper we provide a rigorous, mathematical analysis of the asymptotic properties of attention in transform-ers. Although we present several results based on different assumptions, all of them point to the same conclusion, all tokens asymptotically converge to each other, a phenomenon that has been empirically reported in the literature. Our findings are carefully compared with existing theoretical results and illustrated by simulations and experimental studies using the GPT-2 model.", "sections": [{"title": "1. INTRODUCTION", "content": "The incorporation of attention [1] in natural language processing was a significant breakthrough, particularly in the context of sequence-to-sequence models, enabling the creation of transformers [2] which revolutionized the field. Even initial transformer models such as GPT [3] or Bert [4] showed drastic improvements over previous approaches such as the Long Short-Term Memory model [5]. As practical applications of deep neural networks, such as image recognition [6], natural language processing [7], and autonomous driving [8], continue to advance, our understanding of these networks is struggling to keep pace [9]. This underscores the critical importance of our study, which aims to delve deeper into transformers and their dynamics. Our understanding of transformers is currently limited by their inherent complexity, making it challenging to comprehensively explain their behavior [10]. However, recent studies have shown the emergence of clusters of tokens empirically and theoretically [11-14]. These findings suggest that without proper care, large transformers may collapse, a phenomenon where the tokens cluster, limiting the model's ability to produce different outputs.\nOur work was motivated by the paper [15] where a mathematical model for attention was pro-posed, based on prior work on similar models [16; 17], and investigated. The authors share the vision outlined in [15], a better understanding of the role and importance of attention mechanisms can be achieved through the study of mathematical models. Our contribution lies in bringing ideas developed by the control community, where the study of asymptotic properties of dynamical and control systems is a central preoccupation, to bear on this problem. While deferring to the next section a more detailed comparison between our results and those available in the literature, we emphasize here that, in contrast with [13\u201315], we do not rely on stochastic and/or mean-field tech-niques and rather adopt a geometric perspective drawing from control theory, e.g., from consensus dynamics on manifolds [18] such as spheres [19; 20] and Input-to-State Stability [21-23].\nContributions and plan of the paper. The main contribution of this work is to provide a number of results, for a differential equation model of attention, showing that all tokens converge to a single cluster thereby leading to a collapse of the model. We use the term consensus equilibria to refer to such clusters as is done in the consensus literature [24; 25]. These results hold under different"}, {"title": "2. DYNAMICS OF TRANSFORMERS", "content": "2.1. Configuration space. Let $l, n \\in \\mathbb{N}$. A symmetric, positive-definite matrix $W \\in M_{(n+1) \\times (n+1)}(\\mathbb{R})$ defines an inner product (and, thus, a Riemannian metric) on $\\mathbb{R}^{n+1}$, namely:\n$(X_x, Y_x)_{g(x)} = X_x^T W Y_x, \\qquad X_x, Y_x \\in T_x \\mathbb{R}^{n+1}, x \\in \\mathbb{R}^{n+1},$ where the superscript $T$ denotes the transpose. The corresponding norm is denoted by $|X_x|_W = (X_x^T W X_x)^{1/2}$. The points of $\\mathbb{R}^{n+1}$ of unit norm define an n-dimensional ellipsoid, which is denoted by:\n$\\mathbb{E}_W = \\{x \\in \\mathbb{R}^{n+1} \\mid x^T W x = 1\\}.$In this work, we consider a transformer consisting of $l$ tokens of dimension $n + 1$ constrained to evolve on an ellipsoid. This choice of state space models the effect of token normalization which constrains the \"size\" of a token as discussed in more detail in the next section. As we have $l$ tokens, the resulting state space is the Cartesian product of $l$ copies of the ellipsoid, i.e.:\n$(\\mathbb{E}_W)^l = \\underbrace{\\mathbb{E}_W \\times \\dots \\times \\mathbb{E}_W}_{l \\text{-times}}.$Note that $(\\mathbb{E}_W)^l$ is an embedded submanifold of:\n$(\\mathbb{R}^{n+1})^l = \\underbrace{\\mathbb{R}^{n+1} \\times \\dots \\times \\mathbb{R}^{n+1}}_{l \\text{-times}},$where $\\mathbb{R}^{n+1} = \\mathbb{R}^{n+1} - \\{0\\}$. The natural inclusion is denoted by $i_W: (\\mathbb{E}_W)^l \\hookrightarrow (\\mathbb{R}^{n+1})^l$ and we define the projection:\n$\\pi_W: (\\mathbb{R}^{n+1})^l \\to (\\mathbb{E}_W)^l, \\qquad \\pi_W = \\underbrace{\\pi_W \\times \\dots \\times \\pi_W}_{l \\text{-times}},$where:\n$\\pi_W: \\mathbb{R}^{n+1} \\to \\mathbb{E}_W, \\quad x \\mapsto \\pi_W(x) = \\frac{x}{|x|_W}.$The corresponding tangent map is readily seen to be:\n$T_x \\pi_W = T_{x_1} \\pi_W \\times \\dots \\times T_{x_l} \\pi_W: T_x (\\mathbb{R}^{n+1})^l \\to T_{\\pi_W(x)} (\\mathbb{E}_W)^l,$for each $x = (x_1, \\dots, x_l) \\in (\\mathbb{R}^{n+1})^l$, where the tangent map of $\\pi_W$ at each $x \\in \\mathbb{R}^{n+1}$ is given by:\n$T_x \\pi_W: T_x \\mathbb{R}^{n+1} \\to T_{\\pi_W(x)} \\mathbb{E}_W, \\qquad X_x \\mapsto T_x \\pi_W \\cdot X_x = \\frac{1}{|x|_W} \\left(I_{n+1} - \\frac{x x^T W}{|x|_W^2} \\right) \\cdot X_x.$In particular, for $y \\in \\mathbb{E}_W$, we have $T_y \\pi_W \\cdot X_y = (I_{n+1} - y y^T W) \\cdot X_y.$\nRemark 2.1 (Tangent bundle of the ellipsoid). For each $x \\in \\mathbb{R}^{n+1}$, we make the identification $T_x \\mathbb{R}^{n+1} \\sim \\mathbb{R}^{n+1}$. In particular, for $y \\in \\mathbb{E}_W$, we have:\n$T_y \\mathbb{E}_W = \\{Y_y \\in T_y \\mathbb{R}^{n+1} \\sim \\mathbb{R}^{n+1} \\mid y^T W Y_y = 0\\}.$Therefore, the tangent space of $(\\mathbb{E}_W)^l$ at each $y = (y_1, \\dots, y_l) \\in (\\mathbb{E}_W)^l$ reads:\n$T_y (\\mathbb{E}_W)^l = T_{y_1} \\mathbb{E}_W \\times \\dots \\times T_{y_l} \\mathbb{E}_W = \\{Y_y = (Y_{y_1}, \\dots, Y_{y_l}) \\in (\\mathbb{R}^{n+1})^l \\mid y_i^T W Y_{y_i} = 0, 1 \\leq i \\leq l\\}.$Remark 2.2 (Evolution on the sphere). There are a number of models in which the tokens evolve on the n-sphere, $S^n = \\mathbb{E}_{I_{n+1}}$. For brevity, in that case we will drop the subscripts standing for the matrix $W = I_{n+1}$. For instance, we will write $|\\cdot| = |\\cdot|_{I_{n+1}}, \\pi = \\pi_{I_{n+1}}$, etc."}, {"title": "2.2. Discrete-time attention model.", "content": "In this section we present the mathematical model for a transformer. Similarly to [15], the model encompasses the self-attention mechanism, the skip connection, and the normalization layer, but excludes the feedforward layer.\nLet $w \\in \\mathbb{N}$ be a design parameter. The weight matrices at the k-th layer of the transformer, $k \\in \\mathbb{N}$, are denoted by $Q(k) \\in M_{w \\times (n+1)}(\\mathbb{R}), K(k) \\in M_{w \\times (n+1)}(\\mathbb{R})$ and $V(k) \\in M_{w \\times (n+1)}(\\mathbb{R})$, and are typically known as the Query, Key, and Value matrices, respectively. The input to the k-th layer is denoted by $x = (x_1, \\dots, x_l) \\in M_{(n+1) \\times l}(\\mathbb{R})$ and the output $z \\in M_{w \\times l}(\\mathbb{R})$ of the self-attention mechanism is given by:\n$z(k) = V(k) x(k) D(k) \\exp \\left(x(k)^T K(k)^T Q(k)x(k)\\right),$where $\\exp(\\cdot)$ denotes the entry-wise exponential (i.e., $[\\exp(R)]_{ij} = e^{R_{ij}}$), and $D(k) \\in M_{l \\times l}(\\mathbb{R})$ is defined as:\n$D(k)_{ij} = \\begin{cases} \\left(\\sqrt{n+1} \\sum_{i=1}^l \\exp \\left(x_i(k)^T K(k)^T Q(k) x_i(k)\\right)\\right)^{-1}, & i = j, \\\\ 0, & i \\neq j, \\end{cases} \\qquad 1 \\leq i, j \\leq l.$Practical transformer applications often distribute the computations of the self-attention mech-anism through several parallel heads, leading to what is commonly known as multi-headed self-attention. In this case, each layer of the transformer has $h \\in \\mathbb{N}$ heads. To make explicit the dependence on the head, we write (4) as:\n$z_h(k) = V_h(k) x(k) D_h(k) \\exp \\left(x(k)^T K_h(k)^T Q_h(k)x(k)\\right), \\qquad 1 \\leq h \\leq H.$The outputs from all attention heads are added after being multiplied by certain weight matrices $W_h \\in M_{(n+1) \\times w}(\\mathbb{R}), 1 \\leq h \\leq l$. Then, the resulting sum is added to the input of the layer $x(k)$, using what is often called a skip connection. Lastly, a normalization function is applied to ensure that the output is bounded. In this work, we consider functions that normalize each token of the transformer separately, which is known as layer normalization and was first proposed in [28] as opposed to batch normalization, which consists of normalizing the distribution of the summed inputs. Hence, the normalization function is of the form:\n$N: M_{(n+1) \\times \\ell}(\\mathbb{R}) \\leftrightarrow M_{(n+1) \\times \\ell}(\\mathbb{R}), \\quad x = (X_1, \\dots, X_{\\ell}) \\mapsto N(x) = (N(x_1), \\dots, N(x_{\\ell})),$for some $N: \\mathbb{R}^{n+1} \\to \\mathbb{R}^{n+1}$. As mentioned before, our simplified model does not have a feedforward layer. Therefore, the output of the k-th layer is given by:\n$x(k+1) = N\\left(x(k) + \\sum_{h=1}^H W_h(k) z_h(k)\\right) = N\\left(x(k) + \\sum_{h=1}^H W_h(k) V_h(k) x(k) D_h(k) \\exp \\left(x(k)^T K_h(k)^T Q_h(k)x(k)\\right)\\right).$Similarly to [15], in the following we consider the normalization function $N = \\pi_W$ given in (2), which projects each token to the ellipsoid $\\mathbb{E}_W$. In practice, this projection has been used explicitly in some models such as [29]. For clarity, we utilize the symbol $y = (y_1, \\dots, y_{\\ell})$ for the tokens evolving on the ellipsoid (after this explicit choice of normalization). The previous discrete-time dynamical system thus reads:\n$y(k+1) = \\pi_W \\left(y(k) + \\sum_{h=1}^H W_h(k) V_h(k) y(k) D_h(k) \\exp \\left(y(k)^T K_h(k)^T Q_h(k)y(k)\\right)\\right).$More explicitly, the discrete dynamics of the i-th token, $1 \\leq i \\leq l$, is given by:\n$y_i(k+1) = \\pi_W \\left(y_i(k) + \\sum_{h=1}^H \\sum_{j=1}^l W_h(k) V_h(k) D_h(k)_{ij} \\exp \\left(y_j(k)^T K_h(k)^T Q_h(k) y_i(k)\\right) y_j(k)\\right).$"}, {"title": "2.3. Continuous-time attention model.", "content": "Let $Y \\in \\mathfrak{X}((\\mathbb{E}_W)^l)$ be a vector field and denote its flow by $Y^\\tau: (\\mathbb{E}_W)^l \\to (\\mathbb{E}_W)^l$. Given a map $g: (\\mathbb{E}_W)^l \\times \\mathbb{R} \\to \\mathbb{R}$ we use the notation $g(y, \\tau) = O_y(\\tau^2)$ to denote the existence of a constant $T \\in \\mathbb{R}^+$ and of a function $\\sigma: (\\mathbb{E}_W)^l \\to \\mathbb{R}^+$ so that for every $\\tau \\in [0, T]$ and for every $y \\in (\\mathbb{E}_W)^l$ we have $|g(y, \\tau)| \\leq \\sigma(x) \\tau^2$. A map $\\phi: (\\mathbb{E}_W)^l \\times \\mathbb{R} \\to (\\mathbb{E}_W)^l$ is a first order approximation to the flow $Y^\\tau$ if $d(Y^\\tau(y), \\phi(y, \\tau)) = O_y(\\tau^2)$ where $d$ denotes the distance on $(\\mathbb{E}_W)^l$ induced by the Euclidean distance on $(\\mathbb{R}^{n+1})^l$.\nUsing the concepts introduced in the previous paragraph, our objective is to construct a vector field $Y$ so that the map defined by the right-hand side of (5) is a first order approximation of $Y^\\tau$. To that effect we write $\\forall_h$ as $V_h = T V_h$, and compute $Y$ as the best linear approximation in $\\tau$ of (5). For simplicity, we work with (6), instead of (5), as rewrite it as:\n$y_i(k+1) = \\pi_W \\left(y_i(k) + \\tau f_k(y(k))\\right), \\qquad 1 \\leq i \\leq l,$where $f_k: (\\mathbb{E}_W)^l \\to \\mathbb{R}^{n+1}$ is defined as:\n$f_k(y) = \\sum_{h=1}^H \\sum_{j=1}^l W_h(k) V_h(k)^T D_h(k)_{ij} \\exp \\left(y_j(k)^T K_h(k)^T Q_h(k) y_i(k)\\right) y_j, \\qquad y \\in (\\mathbb{E}_W)^l.$For each $1 < i < l$, the best linear approximation in $\\tau$ is given by:\n$\\dot{y}_i = \\frac{d}{d\\tau}|_{\\tau=0} \\pi_W(y_i + \\tau \\cdot f_k(y)) = T_{y_i} \\pi_W \\cdot f_k(y_i).$Therefore, the continuous-time model is given by:\n$\\dot{y}_i = T_{y_i} \\pi_W \\cdot f_t(y_i) = T_{y_i} \\pi_W \\cdot \\left(\\sum_{h=1}^H \\sum_{j=1}^l W_h(t) V_h(t)^T D_h(t)_{ij} \\exp \\left(y_j(t)^T K_h(t)^T Q_h(t) y_i(t)\\right) y_j\\right),$for each $1 \\leq i \\leq l, y = (y_1, \\dots, y_l) \\in (\\mathbb{E}_W)^l$ and $t \\geq 0$. To simplify notation we introduce the following (time-dependent) auxiliary matrices:\n$U_h(t) = W_h(t) V_h(t) \\in M_{(n+1) \\times (n+1)} (\\mathbb{R}), \\qquad P_h(t) = Q_h(t) K_h(t) \\in M_{(n+1) \\times (n+1)} (\\mathbb{R}),$for each $1 \\leq h \\leq H$ and $t > 0$. We still refer to the matrix $U_h(t)$ as the value matrix since it plays a similar role. Similarly, we define the functions $a_{ij}^h, Z_i^h: \\mathbb{R}_+ \\times (\\mathbb{E}_W)^l \\to \\mathbb{R}$ by:\n$a_{ij}^h(t, y) = \\frac{\\exp(y_j^T P_h(t) y_i)}{\\sqrt{n + 1} \\sum_{j=1}^l \\exp(y_j^T P_h(t) y_i)}, \\qquad Z_i^h(t, y) = \\frac{1}{\\sqrt{n + 1} \\sum_{j=1}^l \\exp(y_j^T P_h(t) y_i)},$respectively, for each $1 \\leq i, j \\leq l, 1 \\leq h \\leq H, t > 0$ and $y = (y_1, \\dots, y_l) \\in (\\mathbb{E}_W)^l$. The matrix having $a_{ij}^h$ as its $i$th row and $j$th column entry is usually called the attention matrix of head $h$. With the notation just introduced, the dynamical system that describes the evolution of a trans-former with $h$ heads and $l$ tokens evolving on the ellipsoid $\\mathbb{E}_W$ is given by:\n$\\dot{y}_i = T_{y_i} \\pi_W \\left(\\sum_{h=1}^H \\sum_{j=1}^l a_{ij}^h(t, y) U_h(t)^T y_j\\right) = \\sum_{h=1}^H \\sum_{j=1}^l a_{ij}^h(t, y) \\left(U_h(t)^T y_j - y_i \\frac{y_i^T W U_h(t)^T y_j}{y_i^T W y_i}\\right),$for each $1 \\leq i \\leq l, t \\geq 0$ and $y = (y_1, \\dots, y_l) \\in (\\mathbb{E}_W)^l$. Let us denote by $Y$ the vector field on $(\\mathbb{E}_W)^l$ defined by (7). It is simple to check, by using Taylor's theorem to expand the flow of $Y$ in powers of $\\tau$, that (6) is a first order approximation to the flow of $Y$. A similar approach can be employed to derive a continuous-time model incorporating the effect of feedforward layers and a simple computation reveals that such model is a vector field of the form $Y + W$ where $W$ is a vector field describing a transformer with no attention layers. We view the results in this paper as a first step towards the analysis of the more complex model $Y + W$ that we leave to future work. The"}, {"title": "3. TRANSFORMERS AS GRADIENT VECTOR FIELDS", "content": "It was noted in [13] that the transformer dynamics can be regarded as a gradient vector field under certain assumptions. For the benefit of the readers we formally prove such observation in the slightly more general setting where $P$ is not the identity matrix.\nWe consider the particular case of (7) with a single head, $h = 1$, identity value matrix, $U_1(t) = I_{n+1}$, and $P_1(t) = P$ time-independent, positive definite, and symmetric. In this case, we pick $W = P$, i.e.:\n$\\dot{y}_i = T_{y_i} \\pi_P \\cdot \\left(\\sum_{j=1}^l a_{ij}(y) y_j\\right) = \\sum_{j=1}^l a_{ij}(y) \\left(y_j - y_i \\frac{y_i^T P y_j}{y_i^T P y_i}\\right),$for each $1 \\leq i \\leq l$ and $y = (y_1, \\dots, y_l) \\in (\\mathbb{E}_P)^l$, where:\n$a_{ij}(y) = \\frac{\\exp(y_j^T P y_i)}{Z_i(y)}, \\qquad Z_i(y) = \\sqrt{n + 1} \\sum_{j=1}^l \\exp(y_j^T P y_i).$"}, {"title": "3.1. Riemannian metric on the configuration space.", "content": "A Riemannian metric $g$ on $(\\mathbb{R}^{n+1})^l$ may be defined as follows:\n$(X_x, Y_x)_{g(x)} = \\sum_{i=1}^l Z_i(x) X_{x_i}^T P Y_{x_i},$for each $X_x = (X_{x_1}, \\dots, X_{x_l}), Y_x = (Y_{x_1}, \\dots, Y_{x_l}) \\in T_x (\\mathbb{R}^{n+1})^l$ and $x = (x_1, \\dots, x_l) \\in (\\mathbb{R}^{n+1})^l$. The orthogonal decomposition induced by $g$ is denoted by:\n$T_y (\\mathbb{R}^{n+1})^l = T_y (\\mathbb{E}_P)^l \\oplus T_y^\\perp (\\mathbb{E}_P)^l, \\qquad X_y = X_y^\\| + X_y^\\perp, \\qquad y \\in (\\mathbb{E}_P)^l,$where $T^\\perp(\\mathbb{E}_P)^l \\to (\\mathbb{E}_P)^l$ denotes the normal bundle, i.e.:\n$T^\\perp(\\mathbb{E}_P)^l = \\{X_y \\in T_y(\\mathbb{R}^{n+1})^l \\mid (X_y, Y_y)_{g(y)} = 0, \\forall Y_y \\in T_y(\\mathbb{E}_P)^l\\}, \\qquad y \\in (\\mathbb{E}_P)^l.$The orthogonal projection is the following vertical bundle morphism over $(\\mathbb{E}_P)^l$:\n$\\pi^\\|: T(\\mathbb{R}^{n+1})^l/(\\mathbb{E}_P)^l \\to T(\\mathbb{E}_P)^l, \\qquad X_y \\mapsto \\pi^\\|(X_y) = X_y^\\|.$Lemma 3.1. The orthogonal projection is given by $\\pi^\\| = T \\pi_P i_{(\\mathbb{E}_P)^l}$.\nProof. It is enough to prove that, for each $y = (y_1, \\dots, y_l) \\in (\\mathbb{E}_P)^l$ and $X_y = (X_{y_1}, \\dots, X_{y_l}) \\in T_y(\\mathbb{R}^{n+1})^l$, we have that $X_y - T_y \\pi_P \\cdot X_y \\in T_y^\\perp (\\mathbb{E}_P)^l$, i.e., that $(X_y - T_y \\pi_P \\cdot X_y, Y_y)_{g(y)} = 0$ for each $Y_y = (Y_{y_1}, \\dots, Y_{y_l}) \\in T_y (\\mathbb{E}_P)^l$. By using (3) and (9), this latter condition is clearly satisfied:\n$(X_y - T_y \\pi_P \\cdot X_y, Y_y)_{g(y)} = \\sum_{i=1}^l Z_i(y) (X_{y_i} - \\frac{X_{y_i}^T P y_i}{y_i^T P y_i} y_i) P Y_{y_i} = \\sum_{i=1}^l Z_i(y) X_{y_i}^T P Y_{y_i} = 0,$where we have used Remark 2.1.\nLastly, recall that $i_P: (\\mathbb{E}_P)^l \\hookrightarrow (\\mathbb{R}^{n+1})^l$ is an embedding (and, in particular, an immersion). Hence, we can pullback $g$ to the Riemannian metric $g_P = i_P^* g$ on $(\\mathbb{E}_P)^l.$"}, {"title": "3.2. Gradient vector field.", "content": "Let us show that the transformer dynamics is a gradient vector field on the manifold $(\\mathbb{E}_P)^l$ equipped with the Riemannian metric $g_P = i_P^* g$. For simplicity, we introduce the following vector fields corresponding to (8) before and after projecting to the ellipsoid, respectively:\n$X_P: (\\mathbb{R}^{n+1})^l \\to T(\\mathbb{R}^{n+1})^l, \\quad x \\mapsto X_P(x) = \\begin{pmatrix} \\sum_{j=1}^l a_{1j}(x) x_j \\\\ \\vdots \\\\ \\sum_{j=1}^l a_{lj}(x) x_j \\end{pmatrix},$Y_P: \\mathbb{E}^n (P)^l \\to T\\mathbb{E}^n (P)^l, \\quad y \\mapsto Y_P(y) = T \\pi_P \\cdot X_P(y) = \\begin{pmatrix} \\sum_{j=1}^l a_{1j}(y) \\left(y_j - y_1 \\frac{y_1^T P y_j}{y_1^T P y_1}\\right) \\\\ \\vdots \\\\ \\sum_{j=1}^l a_{lj}(y) \\left(y_j - y_l \\frac{y_l^T P y_j}{y_l^T P y_l}\\right) \\end{pmatrix}.$Lemma 3.2. We have $grad_g V = -X_P$ for the following the potential function:\nV: (\\mathbb{R}^{n+1})^l \\to \\mathbb{R}, \\qquad x = (x_1, \\dots, x_l) \\mapsto V(x) = \\frac{1}{2} \\sum_{i,j=1}^l \\exp(x_i^T P x_j).$Proof. For each $1 \\leq k \\leq l$, we have:\n$\\frac{\\partial V(x)}{\\partial x_k} = \\frac{1}{2} \\sum_{i,j=1}^l \\exp(x_i^T P x_j) (\\delta_{ik} P x_j + \\delta_{kj} P^T x_i) = - \\sum_{i=1}^l \\exp(x_i^T P x_i) P x_i,$where $\\delta_{ij}$ denotes the Kronecker delta and we have used that $P$ is symmetric. Therefore:\n$\\begin{pmatrix} \\frac{\\partial V(x)}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial V(x)}{\\partial x_l} \\end{pmatrix} = \\begin{pmatrix} - \\sum_{j=1}^l \\exp(x_j^T P x_1) P x_j \\\\ \\vdots \\\\ - \\sum_{j=1}^l \\exp(x_j^T P x_l) P x_j \\end{pmatrix}.$From this, (1) and (9), we conclude:\n$\\text{grad}_g V(x) = \\begin{pmatrix} (Z_1(x) P)^{-1} & & 0 \\\\ & \\ddots & \\\\ 0 & & (Z_l(x) P)^{-1} \\end{pmatrix} \\begin{pmatrix} - \\sum_{j=1}^l \\exp(x_j^T P x_1) P x_j \\\\ \\vdots \\\\ - \\sum_{j=1}^l \\exp(x_j^T P x_l) P x_j \\end{pmatrix} = -X_P(x).$The previous result, together with the fact that the gradient on a submanifold of a Riemannian manifold is the orthogonal projection of the gradient on the original manifold, enable us to show that $Y_P$ is a gradient vector field.\nTheorem 3.1. Let $V_P = V \\circ i_P: (\\mathbb{E}_P)^l \\to \\mathbb{R}$, then $\\text{grad}_{g_P} V_P = -Y_P.$"}, {"title": "3.3. Stability analysis.", "content": "Having established that (8) is a gradient vector field, it is natural to use the potential $V_P: (\\mathbb{E}_P)^l \\to \\mathbb{R}$ as a Lyapunov function to study the asymptotic behavior of the tokens.\nLemma 3.3. The trajectories of the system (8) converge to the set:\n$\\left\\{y \\in (\\mathbb{E}_P)^l \\mid \\text{grad}_{g_P} V_P(y) = 0\\right\\}.$Proof. Let $y \\in (\\mathbb{E}_P)^l$ and $Y_y = (Y_{y_1}, \\dots, Y_{y_l}) \\in T_y (\\mathbb{E}_P)^l$. Recall that the formal time derivative (at $t = 0$) of the potential $V_P$ is the map $\\dot{V}_P = dV_P(Y_P): (\\mathbb{E}_P)^l \\to \\mathbb{R}$. From Theorem 3.1, we obtain:\n$\\dot{V}_N = dV_P(Y_P) = dV_P(-\\text{grad}_{g_P} V_P) = - \\langle \\text{grad}_{g_P} V_P, \\text{grad}_{g_P} V_P\\rangle_h \\leq 0,$and the equality holds if and only if $\\text{grad}_{g_P} V_P = 0$. The proof is concluded by a routine application of LaSalle's invariance principle.\nTheorem 3.2. Every trajectory of the system (8) converges to an equilibrium.\nProof. Recall that the potential $V_P$ satisfies the \u0141ojasiewicz inequality if $|\\dot{V}_P| \\leq \\lambda |\\text{grad}_{g_P} V_P|^h$ for some $\\lambda > 0$. A sufficient condition for the \u0141ojasiewicz inequality to hold is that $((\\mathbb{E}_P)^l, g_P = i^* g)$ is a real analytic, Riemannian manifold and the potential is real analytic, i.e., $V_P \\in C^\\omega ((\\mathbb{E}_P)^l)$. It is clear that these two conditions are satisfied since $(\\mathbb{E}_P)^l$ is a real analytic submanifold of $\\mathbb{R}^{n+1}$ and $Z_i, a_{ij} \\in C^\\omega((\\mathbb{R}^{n+1}), \\mathbb{R}_+)$ for each $1 \\leq i, j \\leq l$, which ensures that both the Riemannian metric $g_P$ and the potential $V_P$ are real analytic.\nOn the other hand, $(\\mathbb{E}_P)^l \\subset (\\mathbb{R}^{n+1})^l$ is compact, whence the set of $\\omega$-limit points of (8) is non-empty. The \u0141ojasiewicz inequality thus ensures that every trajectory converges to a point $y \\in (\\mathbb{E}_P)^l$. From Lemma 3.3, we know that $y$ is an equilibrium of (8) since $\\dot{y} = -\\text{grad}_{g_P} V_P(y) = 0$.\nIf we take $P$ to be the identity, linearization of $Y_P$ around each equilibrium point shows that the only equilibria that are asymptotically stable are the consensus equilibria, i.e., the points $y = (y_1, \\dots, y_l) \\in (\\mathbb{S}^n)^l$ satisfying $y_i = y_j$ for every $i,j \\in \\{1, 2, \\dots, l\\}$. This linearization strategy was employed, e.g., in [19]. Unfortunately, when $P$ is not the identity this strategy leads to conditions whose validity cannot be easily ascertained."}, {"title": "4. TOKENS EVOLVING ON AN HEMISPHERE", "content": ""}]}