{"title": "Artificial Intelligence-based Decision Support Systems\nfor Precision and Digital Health", "authors": ["Nina Deliu", "Bibhas Chakraborty"], "abstract": "Precision health, increasingly supported by digital technologies, is a domain of research that broadens\nthe paradigm of precision medicine, advancing everyday healthcare. This vision goes hand in hand with\nthe groundbreaking advent of artificial intelligence (AI), which is reshaping the way we diagnose, treat,\nand monitor both clinical subjects and the general population. AI tools powered by machine learning have\nshown considerable improvements in a variety of healthcare domains. In particular, reinforcement learning\n(RL) holds great promise for sequential and dynamic problems such as dynamic treatment regimes and\njust-in-time adaptive interventions in digital health. In this work, we discuss the opportunity offered by AI,\nmore specifically RL, to current trends in healthcare, providing a methodological survey of RL methods in\nthe context of precision and digital health. Focusing on the area of adaptive interventions, we expand the\nmethodological survey with illustrative case studies that used RL in real practice.", "sections": [{"title": "Introduction", "content": "In the current era of population aging and increased prevalence of chronic diseases, providing adequate health\nsupport remains one of the most urgent and complex global challenges (Prince et al., 2015). Chronic conditions\nsuch as cancer, diabetes, mental illness, or obesity tend to be of a long duration and often create a need for\nlong-term treatment and care, carrying enormous social, medical, and economic burdens (Beaglehole et al.,\n2011). They are the result of a combination of genetic and physiological factors, as well as environmental and\nbehavioral aspects that are challenging to modify, given the nature of modern lifestyle. Maintaining healthy\nbehaviors throughout life, from diet and physical activity regimes to smoking habits, all contribute to reducing\nthe risk of chronic diseases, improving physical and mental capacity, and delaying care dependency.\nPrecision health is a relatively nascent scientific discipline that broadens the paradigm of precision medicine by\nincluding approaches that occur outside the clinical setting (Gambhir et al., 2018; Ryan et al., 2021). It seeks to\ndevelop proactive and personalized solutions to health problems, disease prevention, and health promotion. Under\nthis framework, \"disease treatment and prevention takes into account individual variability in genes, environment,\nand lifestyle for each person\" (Precision Medicine Initiative; Collins and Varmus, 2015). It transitions from\nthe \"one-size-fits-all\" standards to the formulation of treatment and prevention strategies based on the unique\nbackground and condition of each patient (Kosorok and Laber, 2019). As an illustration of this mission, a\nprecision health system might see health proactively co-managed by healthcare providers and patients through\nthe synchronous integration of information, starting with genotyping at birth, regular screening, and combined\ncontinuous health monitoring and provision of actionable advice and early intervention at the precise moment\nwhen the individual needs it.\nOver the last decade, the precision health paradigm and healthcare in general have witnessed unprecedented\ninnovation due to the continuous improvement and use of big data and digital technologies (Agrawal and\nPrabakaran, 2020). These comprise electronic tools, devices, systems, and resources that utilize increasingly fast\ndata transmission speeds and collect, store, or process large amounts of data. Successful scientific applications\nof big data have already been demonstrated in numerous applications, from specific disease areas such as\noncology (see e.g., The Cancer Genome Atlas and the Pan-Cancer Analysis of Whole Genomes initiatives;\nTomczak et al., 2015; Aaltonen et al., 2020) or neuropsychiatry (PsychENCODE; PsychENCODE Consortium\net al., 2015) to national initiatives. For example, the United Kingdom (UK) has now established a clear national\nstrategy with the UK Biobank prospective cohort initiative (Allen et al., 2012), which collates together biological\nsamples, physical measures of patient health, and sociological information such as lifestyle and demographics\nfrom 500,000 individuals. Expanding on the UK Biobank model, the American All of US program (All of\nUs Research Program Investigators et al., 2019) integrates medical records, behavioral, and family data in\na unique standardized and linked database for all patients, including minorities. The goal is achieved by\nintegrating ancillary patient data, including those collected through wearables, which are now part of daily life,\ninterconnecting the world's population, and making health services more accessible and accountable.\nThe effective use of big data in healthcare is enabled by the development and deployment of artificial\nintelligence (AI) approaches, such as those based on machine learning (ML; Bishop, 2006). ML is a subfield of AI\nthat uses algorithms to automatically learn from past data or experiences, making it possible to unravel patterns,\nassociations, and causations in complex and unstructured datasets created in the era of big data (Camacho\net al., 2018). In turn, it allows one to quickly provide actionable analysis on data, generating accurate prediction\nmodels-such as response of a patient to a treatment regimen-and supporting clinical practice with increasingly\nbetter decisions. An overview of successful biomedical applications using ML is provided in Deo (2015) and\nRajkomar et al. (2019).\nAs an alternative ML area, Reinforcement Learning (RL; Sutton and Barto, 2018; Bertsekas, 2019; Sugiyama,\n2015), represents a framework for interactive tasks in which the system or algorithm must learn by interacting\nwith the surrounding environment sequentially. More specifically, in RL problems, at each time step of a\nsequential process, an agent interacts with its environment, performs action(s), and, based on a feedback received\nfrom the environment for the selected action(s), learns, by trial-and-error, on how to take better actions in order\nto maximize the cumulative feedback over time. This distinctive feature offers a powerful solution in a variety\nof healthcare domains where the problem has a sequential nature (Chakraborty and Moodie, 2013; Yu et al.,\n2023; Gottesman et al., 2019), such as dynamic treatment regimes (DTRS; Chakraborty and Moodie, 2013).\nFurthermore, the continuous improvement and use of mobile technologies has determined the development of\na new area for health promotion, known as mobile health (mHealth; Istepanian et al., 2006), which aims to\ndeliver real-time interventions tailored to individual characteristics and their rapidly changing circumstances.\nSuch interventions are termed just-in-time adaptive interventions (JITAIs; Nahum-Shani et al., 2018) and have a\ncentral position in this survey. Specifically, the focus of this work is on sequential decision-making problems\nin healthcare and includes DTRs and JITAIs in mHealth as two key areas that have embraced the use of AI\ninstruments.\nIn the coming years, AI is expected to radically transform healthcare and the way it is delivered. AI\nsystems supported by ML have achieved considerable improvements in accuracy for diagnosis or image-based\ndiagnosis (Myszczynska et al., 2020; McKinney et al., 2020), prognosis (Kourou et al., 2015) or drug discov-\nery (Vamathevan et al., 2019), among others. Active research in both AI and precision health points to their\nconvergence toward a future where healthcare is enhanced with highly personalized information and healthcare\nproviders are empowered by decision-making support systems through augmented intelligence (Johnson et al.,\n2021).\nMotivated by the increasing interest shown within the healthcare domain in AI technologies such as RL, this\nwork aims to provide an overview of RL in the field of precision and digital health. Along with a survey of RL\nmethods for specific applications in the area, we illustrate two case studies, the PROJECT QUIT - FOREVER\nFREE (Chakraborty and Moodie, 2013) and the DIAMANTE text messaging system (Aguilera et al., 2020;\nFigueroa et al., 2022), in the context of smoking cessation and physical activity, respectively, and the challenges\nwe faced when designing the AI-based system. We believe that there is scope for important practical advances in\nthese areas, and with this overview we aim to make it easier for methodological disciplines to join forces to assist\nhealthcare practice and discovery and to develop the next generation of methods for AI in healthcare.\nThe remainder of this contribution is structured as follows. In Section 2, we provide the mathematical\nformalization of the general RL framework, which is further explored in Section 4.1.1 with a focus on the\nmulti-armed bandit problem. In Section 3 and Section 4, we introduce the two areas of interest, namely DTRS\nand JITAIs, and extensively review existing data sources and RL methodologies for these problems. Two case\nstudies are then illustrated in Section 3.5 and Section 4.3 for DTRs and JITAIs, respectively. Final considerations\nand concluding remarks are given in Section 5."}, {"title": "The Reinforcement Learning Framework", "content": "Reinforcement learning is an area of machine learning (ML) concerned with understanding how agents (e.g.,\nsystems or machines) might learn to improve their decisions through repeated experience. More formally, it aims\nto identify optimal decision rules (or policies) in sequential decision-making problems under uncertainty (Sutton\nand Barto, 2018; Bertsekas, 2019). An optimal RL policy is one that maximizes the expected long-term utility,\nassuming that this is likely to outweigh the associated short-term costs. The general RL framework is formalized\nthrough a continuous interaction between a learning agent (i.e., the decision maker) and the environment it\nwants to infer about. At each interaction stage, the agent observes some representation of the environment's\nstate or context, and on that basis selects an action, that is, makes a decision. The impact of the chosen action\nis evaluated through a reward (or feedback) provided by the environment. Based on the reward received, the\nagent learns, by trial-and-error, on how to take better actions in the future to maximize the cumulative reward\nover time."}, {"title": "Basic ingredients", "content": "In reinforcement learning, differently from other ML methods, data are characterized by a sequential order\nand learning is carried out through many stages. For practicality, consider a discrete time space indexed by\nt\u2208 N = {0,1,...,}. At each time t, the RL framework is described as an interaction between an agent and an\nunknown environment, articulated in the following three key elements:\n\u2022 State or context, denoted by $X_t \\in X_t$, being the representation of the environment at time t. This includes\nthe set of information (demographic and health-related covariates or physical data such as location) that\nmay be relevant to understanding the consequences of alternative interventions.\n\u2022 Action At, taken by the agent from a set of admissible actions At, i.e., the set of alternative interventions.\nWhen making the choice At, the agent weighs the consequences of the alternatives and their likelihood,\ngiven the state Xt.\n\u2022 A reward $Y_{t+1} \\in Y_{t+1} \\subset \\mathbb{R}$ provided by the environment in response to the chosen action At in correspon-\ndence with an observed state Xt. It is the information that an agent learns only after taking an action\n(e.g., patient response to treatment). This is closely related to the concept of utility, which should be the\nultimate criterion to judge whether the entire policy works well or not.\nOnce action At is selected, together with the provision of a reward $Y_{t+1}$, the environment makes a transition to\na new state $X_{t+1} \\in X_{t+1}$. Using this notation, the characterization of the RL sequential decision problem can be\ndescribed as an ordered sequence or trajectory given by:\n$X_0 \\xrightarrow{A_0} (Y_1, X_1) \\xrightarrow{A_1} (Y_2, X_2) \\rightarrow \\ldots \\rightarrow \\xrightarrow{A_t} (Y_{t+1}, X_{t+1}) \\rightarrow \\ldots $                                                              (1)\nIn healthcare, this trajectory can be seen as the history of the interventions received over the course of a disease\nor program, and the individual responses to treatment along with the time-varying contextual and individual\nhealth-related information."}, {"title": "Mathematical formalization of the general RL", "content": "Define $X_t = (X_\\tau)_{t=0,...,t}, A_t = (A_\\tau)_{t=0,...,t}, Y_{t+1} = (Y_{\\tau+1})_{t=0,...,t}$, and similarly $x_t, a_t$ and $y_{t+1}$, where the\nupper- and lower-case letters denote random variables and their particular realizations, respectively. Also define\n$H_t$ as the history all the information available at time t prior to decision At, i.e., $H_t = (A_{t-1}, X_t, Y_t)$; similarly\n$h_t$. The history Ht at stage t belongs to the product set $H_t = X_0 \\times \\prod_{\\tau=0}^{t-1} A_{\\tau} \\times X_{\\tau+1} \\times Y_{\\tau+1}$. Note that,\nby definition, $H_0 = X_0$. We assume that each longitudinal history is sampled independently according to a\ndistribution $P_\\pi$, given by\n$P_\\pi = p_0(x_0) \\prod_{t \\geq 0} \\pi_t(a_t | h_t) p_{t+1}(x_{t+1}, y_{t+1} | h_t, a_t),$                                               (2)\nwhere:\n\u2022 $p_0$ is the probability distribution of the initial state Xo.\n\u2022 $\\pi = {\\pi_t}_{t>0}$ represents the agent's policy and determines the sequence of actions generated throughout the\ndecision-making process. More specifically, $\\pi_t$ maps histories of length t, ht, to a probability distribution over\nthe action space At, i.e., $\\pi_t(\\cdot | h_t)$. The conditioning symbol \"|\" in $\\pi_t(\\cdot | h_t)$ reminds us that the policy defines\na probability distribution over At for each $h_t \\in H_t$. Sometimes, At is uniquely determined by the history Ht,\ntherefore the policy is simply a function of the form $\\pi_t(h_t) = a_t$. We call it deterministic policy, in contrast\nwith stochastic policies that determine actions probabilistically.\n\u2022 ${p_t}_{t>1}$ are the unknown transition probability distributions that characterize the dynamics of the environment.\nAt each time $t \\in \\mathbb{N}$, the transition probability pt assigns to each trajectory $(X_{t-1}, a_{t-1}, y_{t-1}) = (h_{t-1}, a_{t-1})$\nat time t-1 a probability measure over $X_t \\times V_t$, specifying the probability of transition to new states in Xt\nwith reward in Vt, from history $h_{t-1}$ and action $a_{t-1}$, i.e., $p_t(\\cdot, \\cdot | h_{t-1}, a_{t-1})$.\nAt each time t, the transition probability distribution $p_{t+1}(X_{t+1}, Y_{t+1} | h_t, a_t)$ gives rise to: (i) the state-\ntransition probability distribution $P_{t+1}(X_{t+1} | h_t, a_t)$, i.e., the probability of transitioning to state $x_{t+1}$ having\nobserved a history $h_t$ and taking action at; and (ii) the immediate reward distribution $r_{t+1}(Y_{t+1} | h_t, a_t, X_{t+1})$,\nwhich specifies the reward $Y_{t+1}$ after transitioning from a history $h_t$ to $x_{t+1}$ under action at. To better incorporate\nuncertainty, we assume a stochastic reward distribution. An illustrative representation of the RL framework is\nprovided in Figure 1. The cumulative discounted sum of immediate rewards from time t onward is known as\nreturn, say Rt, and is given by\n$R_t = Y_{t+1} + \\gamma Y_{t+2} + \\gamma^2 Y_{t+3} + \\ldots = \\sum_{T \\geq t} \\gamma^{T-t} Y_{T+1}, t \\in \\mathbb{N}.$                                                (3)\nThe discount rate $\\gamma \\in [0,1]$ determines the current value of future rewards: a reward received $\\tau$ time steps in the\nfuture is worth only $\\gamma^\\tau$ times what it would be worth if it were received immediately. If $\\gamma = 0$, the agent is\nmyopic in being concerned only with maximizing the immediate reward, that is, $R_t = Y_{t+1}$, with the convention\nthat $0^0 = 1$. If $\\gamma = 1$, the return is undiscounted and it is well defined (finite) as long as the time-horizon is\nfinite, i.e., $T < \\infty$ (Sutton and Barto, 2018).\nRemark: On-policy Vs. off-policy learning The agent's policy $\\pi$ determines the sequential selection of\nactions. In a randomized experiment context, for example, it may define the randomization probabilities of each\nintervention at each decision point t. In such a setting, an agent can be interested in learning and optimizing the\npolicy $\\pi$ while following it, that is, from experiences sampled directly from $\\pi$. This type of learning is termed\non-policy or online learning, and policy $\\pi$ represents both the exploration policy (the one that generates the\ndata) and the target policy (the one we learn about). In contrast, there are settings where the agent learns from\npreviously collected data without interacting with the environment to collect samples (e.g., observational data).\nIn this type of learning, termed off-policy or offline, we say that the target policy is learned from data \"off\" the\ntarget policy, which are determined according to a policy $\\tilde \\pi$ that can be either the exploration policy (when\nknown, e.g., in randomized studies), or, more generally, an observed or behavior policy. Similar concepts are\nused in statistical and causal inference for referring to the estimation of unknown quantities of interest such as\nparameters.\nTaking into account a potential misalignment between the target policy, say d, and the one used to generate\nthe data $\\pi$ (either exploration or behavior), the RL problem at any time t is to learn an optimal way to choose\nthe set of actions, i.e., an optimal policy $d = {d_\\tau}_{\\tau>t}$, so as to maximize the expected future return. Formally,\n$d^*_t = arg \\max_{d_t} \\mathbb{E}_{d} [R_t] = arg \\max_{d_t} \\mathbb{E}_{d} [ \\sum_{\\tau \\geq t} \\gamma^{\\tau-t} Y_{\\tau+1} ]$                                          (4)\nwhere the expectation is meant with respect to a trajectory distribution $P_d$ analogous to Eq. (2), where the\npolicy $\\pi$ that generated the data is replaced by the target policy d we want to learn about. Note that the\nexpected return is the most common approach to handle decision making under uncertainty (De Lara et al.,\n2008)."}, {"title": "", "content": "For learning optimal policies, various methods have been developed so far in the RL literature: see Sutton\nand Barto (2018) and Sugiyama (2015) for an overview. A traditional approach is through value functions, which\ndefine a partial ordering over policies, with insightful information on the optimal ones. In fact, optimal policies\nshare the same (optimal) value function, and comparing estimated value functions of different candidate policies\noffers a way to understand which strategy may offer the greatest expected outcome.\nThere are two types of value functions: i) state-value or simply value functions, say $V^d_t$, representing how\ngood it is for an agent to be in a given state, and ii) action-value functions, say $Q^d_t$, indicating how good it is for\nthe agent to perform a given action in a given state. These are formally defined as:\n$V^d_t(h_t) = \\mathbb{E}_d [R_t | H_t = h_t] = \\mathbb{E}_d [ \\sum_{\\tau \\geq t} \\gamma^{\\tau-t} Y_{\\tau+1} | H_t = h_t ],$                                         (5)\n$Q^d_t(h_t, a_t) = \\mathbb{E}_d [R_t | H_t = h_t, A_t = a_t] = \\mathbb{E}_d [ \\sum_{\\tau \\geq t} \\gamma^{\\tau-t} Y_{\\tau+1} | H_t = h_t, A_t = a_t ],$                                  (6)\n$\\forall t \\in \\mathbb{N}, \\forall h_t \\in H_t$ and $\\forall a_t \\in A_t$, with Ht and At such that P(Ht = ht) > 0 and P(At = at) > 0. By definition,\nat stage t = 0, $V^d_0(h_0) = V^d_0(x_0)$; while for the terminal stage, if any, the state-value function is 0.\nAt stage t, the optimal value function $V^{d^*}_t$ yields the largest expected return for each history, and the optimal\nQ-function $Q^{d^*}_t$ yields the largest expected return for each history-action pair, i.e.,\n$Q^*_t(h_t, a_t) = \\max_{d_t} Q^d_t(h_t, a_t), \\forall h_t \\in H_t, \\forall a_t \\in A_t.$                                                       (7)\nA fundamental property of the value functions used throughout RL is that they satisfy particular recursive\nrelationships, known as Bellman equations (Bellman, 1957). In the Q-value case, for instance, for any policy d,\nthe following consistency condition, expressing the relationship between the quality of an history-action and the\nquality of the successors, holds:\n$Q_t(h_t, a_t) = \\mathbb{E} [ Y_{t+1} + \\gamma \\max_{a_{t+1} \\in A_{t+1}} Q_{t+1}(h_{t+1}, a_{t+1}) | H_t = h_t, A_t = a_t ],$                                       (8)\n$\\forall a_t \\in A_t, \\forall h_t \\in H_t, \\forall t \\in \\mathbb{N}$, and with discrete state and action spaces. Here, the expectation E is taken with\nrespect to the transition distribution $p_{t+1}$ only, which does not depend on the policy; thus, the subscript d can\nbe omitted.\nThe property in Eq. (8) allows estimation of (optimal) value functions recursively, from T backward in time.\nIn finite-horizon dynamic programming (DP), this technique is known as backward induction and represents one\nof the main methods to solve the Bellman equation. In infinite- and indefinite-horizon problems, traditional\nbackward induction is not possible, given the impossibility of extrapolating beyond the time horizon in the\nobserved data. To overcome this issue, alternative methods and additional assumptions (e.g., discounting and\nboundedness of rewards) are typically taken into account. Common strategies (e.g., V-learning, which we review\nin Section 3.3; Luckett et al., 2020), focuses on time-homogeneous Markov processes.\nDue to its generality, RL is studied and employed in many disciplines, from game theory to education and\nhealthcare. In this work, our goal is to review common RL classes that have been studied or used to support\ndecision making in precision and digital health. In particular, we cover the RL methods studied in the DTR\nliterature (e.g., Q-learning and outcome weighted learning) and in digital health (with a main interest in the\nmulti-armed bandit class, which we discuss in Section 4.1.1). For readers interested in the general area of RL\nand the broad spectrum of existing methods, without a specific application in mind, we refer to Sutton and\nBarto (2018); Sugiyama (2015)."}, {"title": "Dynamic Treatment Regimes in Precision Health", "content": "Clinical or behavioral treatments often involve a series of decisions over time that account for the continuously\nevolving histories of individuals. For example, weight loss management involves a sequence of decisions at multiple\nstages of weight progression. Initially, individuals affected by excess body weight undergo lifestyle modifications\n(such as diet and exercise), and based on their body mass index (BMI), may be treated with pharmacologic\ntherapies to achieve the desired body weight. Then, if the individual responds (i.e., shows a significant weight\nloss), the physician may prescribe a maintenance therapy (typically diet and exercise) to maintain weight at a\nreduced level. Otherwise, the clinician prescribes a second-line therapy, to try to induce body weight reduction.\nThere exist many possible therapies. The aim of the physician is to choose the sequence of therapies that leads\nto the best possible outcome, e.g., long-term maintenance of lost weight, for that individual.\nSimilarly, the treatment of cancer, diabetes, mental health disorders, or the management of addiction problems\nrequires a series of decisions by which the physician can start, stop, maintain, modify, or adjust interventions\nbased on the patient's response and evolving characteristics. This sequence of decisions constitutes a dynamic\ntreatment regime or regimen (Murphy, 2003; Chakraborty and Moodie, 2013), alternatively known as adaptive\ninterventions or strategies (Collins et al., 2004; Lavori and Dawson, 2000).\nDynamic treatment regimes (DTRs) offer a vehicle to operationalize the sequential decision-making process\ninvolved in clinical practice and can also be viewed as a decision support system. A DTR is defined as a sequence\nof decision rules, one per stage of intervention, dictating how to personalize treatments to patients based on\ntheir baseline and evolving history (time-varying, dynamic state), repeatedly adjusting over time in response to\nongoing performance (Almirall et al., 2014; Nahum-Shani et al., 2018). Thus, the treatment regime is \"dynamic\"\nwithin a person over time, varying because the person or disease is changing, with the goal of obtaining the best\nresults for that individual.\nThe existing DTR frameworks (Collins et al., 2004; Almirall et al., 2014) highlight four components that play\nan important role in the design of these interventions:\n(i) The critical decision points, specifying the time points at which a decision concerning intervention (e.g.,\ncontinue, alter, add, or subtract treatment) has to be made; here we assume a finite or countable number\nof times t = 0, 1, . . . ;\n(ii) The decisions or treatment options at each time t, denoted by $A_t \\in A_t$, where At is the decision or\naction space, generally discrete;\n(iii) The tailoring variable(s) at each time t, say $X_t \\in X_t$, with $X_t \\subseteq \\mathbb{R}^P$, capturing individuals' baseline and\ntime-varying information for personalizing decision-making;\n(iv) The decision rules d = {dt}t>0, that, at each time t, link the tailoring variable(s) to specific decisions.\nTreatment options At \u2208 At are not limited to different medications or drugs, but can also include different\ndosages (duration, frequency or amount; Voils et al., 2012; Chen et al., 2016), various tactical options (for example,\nincrease, change, maintain), modes of administration (for example, oral or injection), timing schedules (Nie\net al., 2021), behavioral interventions, or no further treatment. Tailoring variables Xt \u2208 Xt refer to patient and\ntreatment information available up to the time of the critical decision, and may include previous treatment\nand disease history, genetic information, diagnostic test results, etc. Once the four elements are defined, each\ndecision rule d = {dt}t>o takes the individual characteristics Xt \u2208 Xt of a subject and their treatment history\nobserved up to that stage {At}t=0,1,...,t-1 as input and outputs a recommended treatment strategy at that stage.\nThe dynamic treatment regime dt = (do,..., dt) is regarded a multistage regime with each d\u03c4 , \u03c4 = 0, . . ., t being\na mapping of the entire evolving history X \u00d7 A0 \u00d7 \uff65\uff65\uff65 \u00d7 A\u2212\u22121 \u00d7 X, to A,. Unlike average-based single-stage\nprotocols, DTRs explicitly incorporate the heterogeneity in treatment effect among individuals and across time\nwithin an individual. As such, it provides an attractive framework for personalized treatments in longitudinal\nsettings. Furthermore, by treating only those who show a need for treatment, DTRs hold the promise of reducing\nnoncompliance due to overtreatment or undertreatment (Lavori and Dawson, 2000)."}, {"title": "RL methods for constructing optimal DTRS", "content": "One of the main research goals in the field of personalized dynamic treatments is to construct optimal DTRS,\nthat is, to identify the treatment rule(s) that result in the best (typically long-term) mean outcome, i.e., with\nthe highest utility. Most attempts to achieve this goal essentially require knowing or estimating the prespecified\nutility function or some variations of it. For example, Murphy (2003) defines regret (i.e., loss) functions, while\nRobins (2004) introduces blip functions (Kitagawa and Tetenov, 2018), alternatively known as welfare gains in\neconometrics.\nMethodologies for estimating optimal DTRs are of considerable interest within the domain of precision\nhealth and comprise a growing body of research in both computer science and statistics (Chakraborty and\nMoodie, 2013). On the one hand, the sequential decision-making nature of DTR problems perfectly conforms to\nthe RL framework, thus attracting increasing attention in the ML literature. On the other hand, the need to\nquantify causal relationships, rather than mere associations, called for the intervention of the causal inference\ncommunity. Since the underlying system dynamics is often unknown, inferring the consequences of executing\na policy d = {dt}t>1 and understanding the causal effects on an outcome is a challenging task. We refer to\nDeliu and Chakraborty (2022) and Tsiatis et al. (2021) for the broad range of aspects related to DTR (including"}, {"title": "Indirect RL methods in DTRS", "content": "Indirect methods focus on estimating an optimal objective function (typically, an expectation of the outcome\nvariable such as the Q-function presented in Eq. (6)), and then obtaining the associated policy. These methods are\nmainly based on iterative techniques such as dynamic programming (DP) and approximate dynamic programming\n(ADP), and include the Q-learning (Murphy, 2005b) approach that we illustrate below.\nWe mainly focus on the finite-horizon setting, where the utility function is optimized over a fixed and\nprespecified period of time T, and, for the sake of simplicity, we consider deterministic policies which map\nhistories h directly into actions or decisions, that is, d(h) = a.\nQ-learning Q-learning (Watkins, 1989) represents the core of modern RL and one of the most popular strategies\nin DTR research. Its fundamental idea is based on iterative improvement of the estimates of the Q-function at a\ngiven stage t, starting from a previous estimate and following the Bellman rule in Eq. (8). That is,\n$Q_t(h_t, a_t) \\leftarrow Q_t(h_t, a_t) + \\alpha_t [Y_{t+1} + \\gamma \\max_{a_{t+1} \\in A_{t+1}} Q_{t+1}(h_{t+1}, a_{t+1}) - Q_t(h_t, a_t)].$\nThe constant \u03b1t determines to what extent the newly acquired information should override the old information,\ni.e., how fast learning takes place: a factor of 0 will make the learner not learn anything, while a factor of 1\nwould make the learner fully update based on the most recent information. The discount factor \u03b3 balances the\nimmediate rewards of the learner with future rewards, and in a finite-horizon problem it is generally set to one.\nThe original version of this approach is known as tabular Q-learning, and it is based on storing the Q-function\nvalues for each possible state and action in a lookup table and choosing the one with the highest value. Under\nsome appropriate and rigorous assumptions (Watkins, 1989), Qt has been shown to converge to the optimal\nQ-function Q with probability 1. However, this procedure is practical for a small number of problems because\nit can require many thousands of training iterations to converge. In addition, it represents value functions in\narrays or tables, based on each state and action. Thus, large state spaces lead not just to memory issues for large\ntables but also to time problems needed to fill them accurately. A more recent version of Q-learning, known as\nQ-learning with function approximation (FA), offers a powerful and scalable tool to overcome both the modeling\nrequirements and the computational burden to solve an RL problem through backward induction.\nThe main idea of Q-learning with FA is first to estimate the Q-functions using an approximator, e.g.,\nregression models, neural networks or decision trees, and then to derive the estimated policy based on the"}, {"title": "", "content": "estimated Q-functions. Considering an approximation space for each of the T stage-specific Q-functions, e.g.,\n$Q_t = {Q_t(h_t, a_t; \\theta_t) : \\theta_t \\in \\Theta_t}$, with \u0398t the parameter space, an optimal stage-t policy estimate is given by:\n$\\hat d_t(h_t) = arg \\max_{a_t \\in A_t} \\hat Q_t(h_t, a_t) = arg \\max_{a_t \\in A_t} \\hat Q_t(h_t, a_t; \\hat\\theta_t) = \\hat d_t(h_t; \\hat\\theta_t), t = 0, \\ldots, T.$\nAn optimal regime $d^* = (d_1(x_1; \\theta_1), d_2(h_2; \\theta_2), . . ., d_T (h_T; \\theta```json\n_T))$ is obtained by following Bellman's optimality\nequation in Eq. (8), and by recursively estimating $\\hat Q$ backward in time t = T,T \u2013 1,..., 1. Noticing that\nQ-functions are conditional expectations, regression models represent a natural approach. For the complete\ngeneral iterative procedure, as well as more specific examples, we point to Deliu and Chakraborty (2022). By\nusing generalized linear models, one may extend the Q-learning method to binary and count outcomes, and\nan accelerated failure time model can be incorporated for survival outcomes. In the DTR arena, Q-learning\ngeneralizations to diverse outcomes have been implemented for censored data Goldberg and Kosorok (2012);\nZhao et al. (2011, 2020), binary data Moodie et al. (2014), or composite measures attempting to balance different\nobjectives (Laber et al., 2014), among others.\nIn order for $\\hat d$ to be a consistent estimator of the true optimal regime d*, it is important to recognize that\nall the models for the Q-functions should be correctly specified (Schulte et al., 2014). To address this problem,\nseveral FA alternatives such as support vector regression and extremely randomized trees (Zhao et al., 2009),\nor deep neural networks (Liu et al., 2017; Atan et al., 2018; Raghu et al., 2017) have been proposed. We now\nillustrate the latter, given the attention it has attracted in recent years.\nDeep Q-learning. The success achieved by Q-learning in many complex domains has been largely enabled by\nthe use of advanced FA techniques such as deep neural networks (Mnih et al., 2015). We call this approach Deep\nQ-learning (DQL). In DQL, a neural network (Goodfellow et al., 2016) is used to approximate the Q-function.\nMore specifically, at each time t, a DNN is used to fit a model for the Q-function in a supervised way. States\nand actions {(Ht,i, At,i)}i=1,...,v are given as inputs (in the input layer), and the Q-values of all possible actions\nare generated as outputs {Qt(Ht,i, At,i; \u0174,b)}i=1,...,n (in the output layer), leading to a labeled set of data\n{(Ht,i, At,i), Qt(Ht,i, At,i; \u0174, b)}i=1,...,v. Input data are non-linearly transformed based on the unknown weight\nW and bias b parameters and carried out through the neurons of the hidden layers. Figure 3 shows a schematic\nof a feed-forward neural network used within RL. It is characterized by a set of neurons, structured in layers,\nwhere each neuron processes the information from one layer to the next. The collected data are stored and used\nfor continuously updating the Q-function parameter estimates. To allow exploration, each decision is determined\nby an exploration scheme (typically e-greedy) that probabilistically chooses between the action with the highest\nQ-value and a random action.\nWithin the DTR literature, DQL has been implemented with EHR data in Liu et al. (2017) and Raghu et al.\n(2017) for graft-versus-host disease and sepsis treatment, respectively. Compared to its shallow counterpart, the\nDQL framework is particularly suitable for (i) automatically extracting and organizing discriminative information\nfrom the data and (ii) exploring the high-dimensional action and state spaces and making personalized treatment"}, {"title": "Direct RL methods in DTRS", "content": "Direct methods, also known in the RL literature as direct policy search methods (Ng and Russell, 2000), seek to\nmaximize the return by learning the optimal policy directly, without involving the estimation of intermediate\nquantities such as optimal Q-functions or contrasts. These methods typically do not assume models for conditional\nmean outcomes; thus, they are referred to as \"nonparametric\". However, they may consider a parameterization\nfor the policies or regimes class.\nIn direct methods, a class of policies D, often indexed by a parameter, say $ \\psi \\in \\Psi$, is first prespecified. Then,\nfor each candidate regime d\u2208 D, an estimate of the corresponding utility is obtained. The utility can be a\nsummary of one outcome, such as percent days of abstinence in an alcohol dependence study or a composite\noutcome. For example, in Wang et al. (2012) the utility is a compound score that combines information on\ntreatment efficacy, toxicity, and risk of disease progression. Here, without loss of generality, we take the utility\nto be the value of the policy; see Eq. (5). The regime in D that maximizes the value function is the estimated\noptimal DTR, that is, $d^* = arg \\max_{d \\in D} V_d$, or $d^* = arg \\max_{\\psi \\in \\Psi} V_{d_{\\psi}}$ for parametric classes. A common example\nof parametric classes is the soft-max class D = {$\\pi(a_k|x, \\psi) = e^{-x\\psi_k} / \\sum_{j=1}^K e^{-x\\psi_j} : \\psi \\in \\Psi, k = 1,...,K$},\nwhere a1,...,aK are the K possible treatments and $ \\psi = (\\psi_1,...,\\psi_K)$ is the vector of parameters for the K\ntreatments indexing the class of policies.\nMost statistical work in this area is based on the inverse probability of treatment weighting (IPTW) esti-\nmator (Robins, 1994), used for estimating value functions (Zhang et al., 2012, 2013), in classification-based\nframeworks such as outcome weighted learning (OWL; Zhao et al., 2012, 2015; Liu et al., 2018), and in com-\nbination with ML approaches such as decision trees (Laber and Zhao, 2015; Tao et al., 2018). Particularly\nuseful in observational data, where the exploration and target policies differ, IPTW (Robins, 2000) makes use\nof importance sampling to change the distribution under which the regime's value is computed. In doing so,\nassuming that $P_d$ is absolutely continuous with respect to $P_\\pi$, it basically weights outcomes according to the"}, {"title": "", "content": "relative probability of interventions occurring under the target d and exploration \u03c0 policies. The value function\nthen can be rewritten as:\n$\\begin{aligned} V^d = \\mathbb{E}_d [Y] = \\int Y dP_d = \\int Y \\frac{dP_d}{dP_\\pi} dP_\\pi = \\int Y \\frac{dP_d}{dP_\\pi} dP_\\pi  \\\\  = \\int Y \\frac{  \\prod_t \\pi[a_t|d(H_t)] }{ \\prod_t \\pi(A_t|H_t) } dP_\\pi  = \\int \\Big[ \\prod_t \\frac{ \\pi[a_t|d(H_t)] }{ \\pi(A_t|H_t) } \\Big] Y dP_\\pi \\end{aligned}$                                                                                    (9)\nTo estimate Vd, the Monte Carlo (MC) estimator given by $\\hat V_d = P_N [w_d, Y]$, where PN denotes the empirical\naverage over N trajectories, is generally used. By the Strong Low of Large Numbers, the MC estimator is\nunbiased, but its variance is unbounded. To stabilize this estimator, the weights $w_{d,\\pi}$ are normalized by their\nsample mean, leading to the IPTW estimator:\n$\\hat V_{IPTW} = \\frac{P_N [w_{d,\\pi} Y]}{P_N [w_{d,\\pi}]}.$                                                                                                     (10)\nThe technique also allows balancing the confounders across levels of treatment: the higher the probability of\nreceiving a specific treatment conditioned on the confounder X, \u03c0(A|X), the lower the weight $w_\\pi = 1/\\pi(A|X)$\nof their outcome Y.\nWhen \u03c0 is known (e.g., randomized trials), the IPTW estimator is consistent, but it can be highly variable\ndue to the presence of nonsmooth indicator functions inside the weights. An alternative version, which integrates\nthe properties of the IPTW estimator with those of regression-assuming models for both the propensity score\nand the (conditional) mean outcome is the augmented inverse probability of treatment weighting (AIPTW)\nestimator (Zhang et al., 2012). Assume a single-stage treatment regime with two treatment options (A \u2208 {a, a'}),\nand let H = Xo to be a patient's history, $d(H) = d(H; \\psi)$ a treatment regime indexed by \u03c8, \u03bc(\u0391, \u0397; \u03b2) an\nestimated model for the mean outcome E[Y|H, A], and $\\pi(A|H, \\gamma)$ an estimated propensity score. Then, the\nAIPTW estimator is defined as:\n$ \\begin{aligned}  \\hat V_{AIPTW} &= \\mathbb{P}_N \\Big{\\{}  I[A = d(H; \\psi)] \\frac{Y - \\mu(H; \\psi, \\beta)}{\\pi(H; \\psi, \\gamma)} + \\mu(H; \\psi, \\beta) \\Big{\\}} \\\\  &= \\mathbb{P}_N \\Bigg{  I[A = d(H; \\psi)]\\frac{Y - \\pi(H; \\psi,\\gamma)}{\\pi(H; \\psi,\\gamma)} \\mu(H; \\psi, \\beta) \\Bigg  \\} \\end{aligned} $\nwhere,\n$\\pi(H; \\psi,\\gamma) = \\pi(a|H, \\gamma)\\mathbb{I}[d(H; \\psi) = a] + \\pi(a'|H, \\gamma)\\mathbb{I}[d(H;\\psi) = a'],$\n$\\mu(H;\\psi, \\beta) = \\mu(a, H; \\beta)\\mathbb{I}[d(H; \\psi) = a] + \\mu(a', H; \\beta)\\mathbb{I}[d(H;\\psi) = a'].$\nIt only requires that either the propensity or mean outcome model to be correctly specified but not both; hence,\nthe doubly robust property. In addition to being more robust to model misspecification, AIPW estimators tend\nto be more efficient than their nonaugmented counterparts (Robins, 2004).\nAlthough its original version was designed for a single-stage treatment regime, it was subsequently adapted\nto two or more decision points (Zhang et al., 2013; Tao and Wang, 2017; Zhou et al., 2018), where models are"}, {"title": "", "content": "posited for either Q-functions or contrasts.\nOutcome weighted learning (OWL) As an alternative direct approach, Zhao et al. (2012) studied the DTR\nestimation problem as a weighted classification problem-with weights retrospectively determined from clinical\noutcomes (hence \u201cOutcome Weighted\")-and proposed to solve it with ML tools (hence \u201cLearning\").\nIn the case of two treatments, expressed as A \u2208 {\u22121,1}, Qian and Murphy (2011) first showed that the\nproblem can be formulated as a weighted 0-1 loss in a weighted binary classification problem, where d* can be\nestimated as:\n$\\begin{aligned} \\hat d^* &= arg \\max_{d \\in D} \\mathbb{V}_d = arg \\max_{d \\in D} \\mathbb{P}_N \\Big{\\{} \\frac{\\mathbb{I}[A=d(H)]}{\\pi(A|H)} Y \\Big{\\}} = arg \\min_{d \\in D} \\mathbb{P}_N \\Big{\\{} \\frac{\\mathbb{I}[A\\neq d(H)]}{\\pi(A|H)} Y \\Big{\\}} \\end{aligned}$However, due to the discontinuous indicator function, Zhao et al. (2012) proposed to address the optimization\nproblem with a convex surrogate loss function for the 0 - 1 loss, corresponding to the hinge loss in ML (Hastie\net al., 2009). Considering that d(H) can be represented in terms of the sign function sign(f(H)) for some\nsuitable function f, the minimization problem is then expressed as:\n$\\hat f^* = arg \\min_{f \\in F}  \\mathbb{P}_\\nu \\Big{\\{} \\sum  [\\frac{Y}{\\pi(A|H)} \\phi(Af(H))  + \\lambda_\\nu ||f(H)||^2 ] \\Big{\\}},$                                                      (11)\nwhere A\u03bd is a tuning penalty parameter that penalizes the complexity of functions f, \u03c6(x) = max(1 - x, 0) is\nthe hinge loss, and ||\u00b7 || is the norm function.\nAn extensive literature has considered some kind of extensions of the standard OWL estimator in Eq. (11),\nand we outline some of these in Table 2. In particular, Zhao et al. (2015) and Liu et al. (2018) have extended\nthe OWL estimator to a multi-stage setting, proposing the Backward Outcome Weighted Learning (BOWL) and\nSimultaneous Outcome Weighted Learning (SOWL) procedures. In the first approach, the stage-t estimator,\ndenoted by ft, is obtained recursively as:\n$\\hat f_{B,t} = arg \\min_{f \\in F}  \\mathbb{P}_\\nu \\Big{\\{} \\sum  [ \\mathbb{I}[ A_\\tau = \\hat d(H_\\tau)][\\frac{Y}{ \\prod_\\tau \\pi_\\tau(A_\\tau|H_\\tau) } -\\phi(\\mathcal{A}_tf_t(H_t))]  + \\lambda_\\nu ||f_t(H_t)||^2 ] \\Big{\\}},$where ($\\hat d_{t+1}$,...) are obtained prior to stage t, and the T-stage estimator does not account for treatments\nfollowed afterwards, i.e., $ \\prod_{\\tau=T+1}^{7=T}  \\mathbb{I}[ A_\\tau = \\hat d(H_\\tau)]  = 1$.\nThe second approach allows simultaneous estimation for all stages. In a two-stage problem, for example, the\nSOWL estimator, say fs, is defined as follows:\n$\\hat f_s = arg \\max_{f \\in F}  \\mathbb{P}_\\nu \\Big{\\{} \\sum  \\frac{ \\Upsilon \\varphi(A_0 f_0(H_0), A_1 f_1(H_1))  }{\\prod_0 \\pi_\\tau(A_\\tau|H_\\tau)}  - \\lambda_\\nu [||f_0(H_0)||^2 + ||f_1(H_1)||^2 ] \\Big{\\}},$\nwith (x1,x2) = min(x1 - 1,x2 \u2212 1,0) + 1 being a concave surrogate for the product of the two (discontinuous)\nindication functions, introduced to limit computational issues.\nEven if numerical examples show that BOWL and SOWL have superior performances compared to existing"}, {"title": "RL methods for JITAIs in digital health", "content": "JITAIS are carried out in dynamic environments where the context and needs of individual users can change\nrapidly (Nahum-Shani et al., 2015, 2018). Therefore, methodologies for delivering JITAIs are required to perform\nalmost continuous learning-with no definite time horizon-and to provide interventions online as data accumulate,\noften utilizing trajectories defined over very short time periods. Note that in such settings, the exploration\npolicy used to collect the samples corresponds to the target policy d we want to improve and optimize; that is,\n\u03c0 = d. However, existing methods for DTRs mainly target a finite-time horizon problem and are implemented\noffline with backward induction (as in Q-learning); therefore, they are not directly applicable. Furthermore, by\ncarrying over an entire history of an individual, they may not be feasible from a computational perspective. In"}, {"title": "", "content": "such problems, a simplified RL framework, known as the multi-armed bandit (MAB) problem, represents an\nattractive approach and is increasigly being used within the digital and mobile health domains (Tewari and\nMurphy, 2017). In what follows, after an illustration of the MAB framework, we describe some popular MAB\nalgorithms that have been used in digital health."}, {"title": "The multi-armed bandit framework", "content": "MAB problems can be viewed as a subclass of RL problems (Sutton and Barto, 2018). In the simplest stateless\ncase, the environment does not have any state transitions and actions can be determined according to a single-\nstage decision-making framework. Generally speaking, the MAB problem (also called the K-armed bandit\nproblem) is a problem in which a limited set of resources (e.g., a group of individuals) must be allocated between\ncompeting choices in order to maximize the total expected reward over time. Each of the K choices (i.e., arms\nor actions) provides a different reward, whose probability distribution is specific to that choice. If one knew\nthe expected reward (or value) of each action, then it would be trivial to solve the bandit problem: they would\nalways select the action with the highest value. However, as this information is only partially gained for the\nselected actions, at each decision time t the agent must trade-off between optimizing its decisions based on\nacquired knowledge up to time t (exploitation) and acquiring new knowledge about the expected rewards of\nthe other actions (exploration). The problem conforms to the class of on-policy RL, where the same policy \u03c0\nused to explore the actions is evaluated and improved throughout the learning process. For this reason, within\nthis section, we avoid a distinction between \u03c0 and d, and we use \u03c0 to refer to both the target policy and the\nexploration policy.\nMAB problems can incorporate some context, which in mapped into appropriate interventions or arms\n(contextual MABs), or solve a context-free task (non-contextual MABs), where no side information is used. In\nthe theory of sequential decision making, contextual MABS occupy a middle ground between non-contextual\nMABS (Bubeck and Cesa-Bianchi, 2012; Auer et al., 2002b) and full-blown RL.\nThe most typical assumption is that contexts {Xt}ten are independent and identically distributed (i.i.d.)\nwith some fixed but unknown distribution. This means that action At at time t has an in-the-moment effect on\nthe proximal reward Yt+1 at time t + 1, but not on the distribution of future rewards {Yt}+>t+2, for which the\ni.i.d. property also holds. Under this assumption, one can be completely myopic (with \u03b3 = 0 in Eqs. (3) and (8))\nand ignore the effect of an action on the distant future in searching for a good policy. In such contextual MABS,\nand further in the context-free MAB problem, the trajectory distributions are simplified as follows:\n$\\begin{aligned} P_\\pi &= p_0(x_0) \\prod_{t \\geq 0} \\pi_t(a_t|x_t) p_{t+1}(x_{t+1}, y_{t+1}|x_t, a_t)  \\\\ &= p_0(x_0) \\prod_{t \\geq 0} \\pi_t(a_t|x_t) p_{t+1}(x_{t+1}) r_{t+1}(y_{t+1}|x_t, x_{t+1}, a_t) \\end{aligned}$                                                            [Contextual MAB]\n$P_\\pi = \\prod_{t>0} \\pi_t(a_t) r_{t+1}(y_{t+1}|a_t).$                                                                                                                                                [Non-contextual MAB]\nAs in the general RL problem, the goal of an MAB problem is to select the optimal arm at each time t to"}, {"title": "", "content": "maximize the expected return, alternatively (and with a slightly different nuance) expressed in the bandit literature\nin terms of minimizing the total regret. Formally, denoted by $A_t = arg \\max_{a_t \\in A} E(Y_{t+1}|X_t = x_t, A_t = a_t)$ the\noptimal arm at time t, we define the immediate regret \u0394t(At) of action At as the difference between the expected\nreward of the optimal arm At and the expected reward of the ultimately chosen arm At, i.e.,\n$\\Delta_t(A_t) = \\mathbb{E}(Y_{t+1}|X_t, A_t^+) - \\mathbb{E}(Y_{t+1}|X_t, A_t).$                                                                                          (13)\nA nonexhaustive correspondence table between the MAB and JITAI terminologies is reported in Table 3.\nWith a few exceptions, the contextual MAB algorithms applied in mHealth are based on and rely on the\nfield-specific adaptation of two fundamental contextual bandit approaches: the Upper Confidence Bound (UCB;\nLi et al., 2010; Chu et al., 2011) and the Thompson sampling (TS; Thompson, 1933; Agrawal and Goyal, 2013)\nstrategies. Exceptions include the Actor-Critic strategy used e.g., in Greenewald et al. (2017).\nContextual bandits with linear UCB Linear Upper Confidence Bound (LinUCB) bandits (Li et al., 2010;\nChu et al., 2011) represent an extension of the UCB algorithm (Auer et al., 2002a) for MAB problems to\ncontextual MAB problems. It assumes that the expected reward is a linear function of the context-action feature\nf(Xt, At) \u2208 Rd', i.e., E[Yt+1|Xt, At] = f(Xt, At)T\u00b5, with \u03bc\u2208 Rd' an unknown reward parameter. In this work,\nwe refer to general features (constructed e.g., via linear basis, polynomials or splines expansion; see e.g., Marsh\nand Cormier, 2002) rather than a standard linear function that may not capture nonlinearities in the data.\nSimilar modeling considerations have been made in Q-learning (by using, e.g., DNNs).\nAt each time t, revealed the context Xt, LinUCB calculates the upper confidence bound for the expected\nreward for all possible actions and then selects the action associated with the highest UCB. Denoted by \u00dbt(at)\nthe UCB of arm at at time t, Li et al. (2010) and Chu et al. (2011) proposed the formulation:\n$ \\hat U_t(a_t) = \\mathbb{E}[Y_{t+1}|X_t, A_t] + a s_t(a_t) = f(X_t = x_t, A_t = a_t)^T \\hat \\mu_t + a s_t(a_t),$\nwhere $\\hat \\mu_t$ is an estimator of the unknown regression coefficient pt and st(at) is defined as$\n$\\sqrt{f(X_t, A_t = a_t)^T B_t^{-1} f(X_t, A_t = a_t)},$ with $B_t = \\lambda I_{d'} + \\sum_{\\tau=0}^{t-1} f(X_\\tau, A_\\tau = \\tilde a_\\tau) f(X_\\tau, A_\\tau = \\tilde a_\\tau)^T$. Bt is com\nputed recursively at each time step t, by taking into account the context-action features associated with the\noptimal actions {a\u03c4 = arg maxa\u03c4\u2208A \u00db\u03c4(a\u03c4)}\u03c4=0,1,...,t\u22121 estimated at previous rounds. Note that the first part"}, {"title": "", "content": "$f(X_t, A_t = a_t)^T \\hat \\mu_t$ reflects the current estimate of the reward, while the second part ast(at) is an indication of\nits uncertainty; thus, it naturally balances between exploration and exploitation. The tuning parameter \u03b1 > 0\nbalances the trade-off between exploration and exploitation: small values of a favor exploitation, while larger\nvalues of a favor exploration.\nMoving from pure bandit and statistical theory to real-world digital health applications, the use of LinUCB\nhas been reported in Forman et al. (2019), among others. Here, in the context of behavioral weight loss and\nmaintenance, a pilot study has been conducted to evaluate the feasibility and acceptability of an RL-based\nintervention. Participants were randomized into a nonoptimized, an individually optimized (individual reward\nmaximization), and a group optimized (group reward maximization) group. The study showed the advantages\nof the RL-based optimized groups in terms of the outcome of interest, not only being feasible to deploy and\nacceptable to participants and coaches, but also achieving desirable results at roughly one-third the cost.\nContextual bandits with linear Thompson sampling Under the same linear assumption of LinUCB,\nAgrawal and Goyal (2013) proposed a randomized version of the latter, based on a generalization of the TS\ntechnique for i.i.d. contextual MAB problems. Based on the Bayesian framework, the idea of TS is to randomly\nallocate each arm according to its posterior probability of being optimal. More specifically, assuming a Gaussian\nprior for the \u03bc parameter $ \\mu \\thicksim N(0_{d'}, \\upsilon^2 I_{d'})$ and a Gaussian distribution for the reward $Y_t | \\mu, f(X_t, A_t) \\thicksim$\n$N(f(X_t, A_t)^T \\mu, \\upsilon^2)$, for some \u03bd > 0, at each time t the optimal arm at is the one that maximizes the 'a-posteriori'\nestimated expected reward, i.e., f(Xt, At)T\u03bct. The posterior nature is reflected in \u03bct, which represents a sample\nfrom the posterior distribution, computed recursively and given by $N(\\hat\\mu_t, \\upsilon^2 B_t^{-1})$, with $\\hat\\mu_1 = B_t^{-1} b_t$, where\n$B_t =  I_{d'} + \\sum_{=\\tau=0}^{t-1} f(X_\\tau, A_\\tau = \\tilde a_\\tau) f(X_\\tau, A_\\tau = \\tilde a_\\tau)^T $ and $b_t = \\sum_{=\\tau=0}^{t-1} f(X_\\tau, A_\\tau = \\tilde a_\\tau)Y_{\\tau+1}(X_\\tau, A_\\tau = \\tilde a_\\tau)$. The\npolicy at each time t is thus explicitly defined as:\n$\\pi_t(a) = P(E[Y_{t+1} | X_t = x_t, A_t = a] > E[Y_{t+1} | X_t = x_t, A_t = a'], \\forall a' \\neq a | F_{t-1}),$\nwhere the conditioning term Ft-1 reflects the posterior nature of this strategy.\nGiven all the trajectory information up to time t, $T_{t-1} = {\\{(X_\\tau, A_\\tau, Y_{\\tau+1})\\}}_{\\tau=0,1,...,t-1}$ and f(Xt, At), LinUCB\nis deterministic and allows exploration through the uncertainty term ast(at). On the other hand, in TS,\nexploration is given by the random draws from the posterior distribution. Note that the standard deviation\nast(at) characterizing LinUCB has the same order as the standard deviation of the posterior distribution of\nthe reward Yt pt, f(Xt, At) ~ N(f(Xt, At = at)T\u03bct, v\u00b2 f(Xt, At = at)TB+\u00b9 f(Xt, At = at)) used in TS, where\nf(Xt, At = at)T B\u012b\u00af\u00b9 f(Xt, At = at) = st(at) by definition.\nActor-critic contextual bandits Specifically addressing personalized mHealth intervention problems, Lei\n(2016) proposed to use a particular RL setting, called actor-critic (Grondman et al., 2012), based on which both\npolicies and value functions are learned. The \"actor\" is the component that learns policies, and the \"critic\" is the\ncomponent that learns about whatever policy the actor is currently following to \"criticize\" its choices (Sutton\nand Barto, 2018)."}, {"title": "", "content": "Assuming a linear model for the reward $Y_{t+1} = f(X_t, A_t)^T \\mu_t + \\epsilon_{t+1}$, with $f(X_t, A_t) \\in \\mathbb{R}^{d'}, \\mu \\in \\mathbb{R}^{d'}$ and i.i.d.\nerror terms {et}ten with mean 0 and variance \u03c3\u00b2, and taking into account a binary action space A = {0,1}, Lei\n(2016) formulated an online policy learning procedure as a contextual bandit problem, and proposed a class of\nparametrized stochastic policies with $P(A = 1|X = x) = \\pi(1|x; \\theta) = \\frac{e^{g(x)^T \\theta}}{1+e^{g(x)^T \\theta}}$, and g(x) a p-dimensional policy\nfeature.\nTo maintain variety of treatment and increasing engagement, a stochastic chance constraint related to a\nparametrized stationary policy is introduced. In this specific case, if we denote with {\u03c0(a|x; \u03b8) : \u03b8 \u2208 \u03b8\u2286 Rd}\nthe class of parameterized stochastic policies, the stochasticity constraint has the form:\n$P(\\pi_{min} \\leq \\pi(A = 1|X; \\theta) < 1 - \\pi_{min}) \\geq 1 - \\alpha,$                                                     (14)\nwith \u03c0min \u2208 (0,.5) and a \u2208 (0, 1) controlling the amount of stochasticity.\nAn optimal policy can then be obtained by maximizing the expected reward under the policy \u03c0(\u03b1|x; 0), i.e.,\nV (0) = \u0395\u03c0\u03bf (\u03a5), subject to the constraint in Eq. (14). Solving this constrained optimization problem involves a\nmajor difficulty given the nonconvex constraint on 0, which involves also some nonsmoothness. To circumvent\nthis difficulty, the authors relax the above constraint by bounding the probability in Eq. (14) using Markov's\ninequality, and then solving the constrained optimization problem using the Lagrangian function Jx(0), with A\nthe Lagrangian multiplier. That is, for a fixed \u5165, the optimal policy \u03c0* = \u03c0\u03c1* is the one with 0* given by:\n$\\theta^* = arg \\max_{\\theta \\in \\Theta} J_\\lambda(\\theta),$\nwhere Jx (0), also referred to as regularized average reward, is defined as\n$J_\\lambda(\\theta) = V^*(\\theta) - \\lambda \\theta^T \\mathbb{E}(g(X)g(X)^T)\\theta$\n$= \\mathbb{E}_{\\pi_{\\theta}}(Y) - \\lambda \\theta^T \\mathbb{E}(g(X)g(X)^T)\\theta$\n$= \\mathbb{E}_{p(x)} \\mathbb{E}_{\\pi(a|x;\\theta)}[\\mathbb{E}(Y|X = x; A = a)] - \\lambda \\theta^T \\mathbb{E}(g(X)g(X)^T)\\theta.$\nBeing Jx (0) unknown, its MC version is considered:\n$J_\\lambda(\\theta) = \\mathbb{P}_N \\Bigg[  \\sum  \\mathbb{E}(Y|X = x; A = a)\\pi(a|X = x; \\theta)  - \\lambda \\theta^T (g(x)g(x)^T)\\theta \\Bigg],$\nwhere PN denotes the empirical average on N i.i.d. samples.\nFor estimating the expected reward E(Y|X = x; A = a), the linear assumption E(Yt+1|Xt = x, At = a) =\nf(x, a)T\u00b5 is used, and the L2 norm penalized least square estimator is considered. This helps to overcome the\nfull rank requirement of the matrix = f(Xt, At) f(Xt, At)T at the beginning of the experiment when running\nthe algorithm online.\nWhile the proposed algorithm is formulated for a binary action space, we notice that this class of methods\nhas been originally introduced to overcome the limitation of methods that fail to address complex action space"}, {"title": "Data sources for building JITAIs in digital health", "content": "Typical experimental designs for building JITAIs are represented by factorial experiments (Collins et al., 2009),\nor, most notably, micro-randomized trials (MRTS; Klasnja et al., 2015). In MRTs, individuals are randomized\nhundreds or thousands of times over the course of the study, and, in a typical multicomponent intervention study,\nthe multiple components can be randomized concurrently, making micro-randomization a form of a sequential\nfactorial design. The goal of these trials is to optimize mHealth interventions while assessing the causal effects of\neach randomized intervention component and evaluating whether the intervention effects vary with time or the\ncurrent context of individuals. A review of this cutting-edge design, covering both its classical variants and its\nadaptive counterpart, together with the associated statistical challenges, is presented in Liu et al. (2023).\nAs an illustrative example, to better understand the characteristics and value of an MRT, let us now consider\nthe DIAMANTE study design of a physical activity trial in Figure 5. In this study, the intervention options\n(i.e., each unique combination of the factor levels) include whether or not to send a text message, which type of\nmessage to deliver, and at which time; the proximal outcome is the change in the number of steps the person\nwalked today from yesterday; and the context is given by a set of user's individual variables such as baseline\nhealth status or study day.\nAt the macro level, to assess the benefits of optimized JITAIs, in addition to evaluating the causal role of each\nintervention, users are randomized to different study groups (see Figure 5), including a static (control) group, a\nuniform random (nonoptimized) group, and an RL-based (adaptive, optimized) group. In noncontrol groups,\nusers are randomized every day to receive a combination of the different factors' levels, delivered within different"}]}