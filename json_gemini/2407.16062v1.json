{"title": "Artificial Intelligence-based Decision Support Systems\nfor Precision and Digital Health", "authors": ["Nina Deliu", "Bibhas Chakraborty"], "abstract": "Precision health, increasingly supported by digital technologies, is a domain of research that broadens\nthe paradigm of precision medicine, advancing everyday healthcare. This vision goes hand in hand with\nthe groundbreaking advent of artificial intelligence (AI), which is reshaping the way we diagnose, treat,\nand monitor both clinical subjects and the general population. AI tools powered by machine learning have\nshown considerable improvements in a variety of healthcare domains. In particular, reinforcement learning\n(RL) holds great promise for sequential and dynamic problems such as dynamic treatment regimes and\njust-in-time adaptive interventions in digital health. In this work, we discuss the opportunity offered by AI,\nmore specifically RL, to current trends in healthcare, providing a methodological survey of RL methods in\nthe context of precision and digital health. Focusing on the area of adaptive interventions, we expand the\nmethodological survey with illustrative case studies that used RL in real practice.", "sections": [{"title": "Introduction", "content": "In the current era of population aging and increased prevalence of chronic diseases, providing adequate health\nsupport remains one of the most urgent and complex global challenges (Prince et al., 2015). Chronic conditions\nsuch as cancer, diabetes, mental illness, or obesity tend to be of a long duration and often create a need for\nlong-term treatment and care, carrying enormous social, medical, and economic burdens (Beaglehole et al.,\n2011). They are the result of a combination of genetic and physiological factors, as well as environmental and\nbehavioral aspects that are challenging to modify, given the nature of modern lifestyle. Maintaining healthy\nbehaviors throughout life, from diet and physical activity regimes to smoking habits, all contribute to reducing"}, {"title": "The Reinforcement Learning Framework", "content": "Reinforcement learning is an area of machine learning (ML) concerned with understanding how agents (e.g.,\nsystems or machines) might learn to improve their decisions through repeated experience. More formally, it aims\nto identify optimal decision rules (or policies) in sequential decision-making problems under uncertainty (Sutton\nand Barto, 2018; Bertsekas, 2019). An optimal RL policy is one that maximizes the expected long-term utility,\nassuming that this is likely to outweigh the associated short-term costs. The general RL framework is formalized\nthrough a continuous interaction between a learning agent (i.e., the decision maker) and the environment it\nwants to infer about. At each interaction stage, the agent observes some representation of the environment's\nstate or context, and on that basis selects an action, that is, makes a decision. The impact of the chosen action\nis evaluated through a reward (or feedback) provided by the environment. Based on the reward received, the\nagent learns, by trial-and-error, on how to take better actions in the future to maximize the cumulative reward\nover time."}, {"title": "Basic ingredients", "content": "In reinforcement learning, differently from other ML methods, data are characterized by a sequential order\nand learning is carried out through many stages. For practicality, consider a discrete time space indexed by\nt\u2208 N = {0,1,...,}. At each time t, the RL framework is described as an interaction between an agent and an\nunknown environment, articulated in the following three key elements:\n\u2022 State or context, denoted by Xt \u2208 Xt, being the representation of the environment at time t. This includes\nthe set of information (demographic and health-related covariates or physical data such as location) that\nmay be relevant to understanding the consequences of alternative interventions.\n\u2022 Action At, taken by the agent from a set of admissible actions At, i.e., the set of alternative interventions.\nWhen making the choice At, the agent weighs the consequences of the alternatives and their likelihood,\ngiven the state Xt.\n\u2022 A reward Yt+1 \u2208 Yt+1 CR provided by the environment in response to the chosen action At in correspon-\ndence with an observed state Xt. It is the information that an agent learns only after taking an action\n(e.g., patient response to treatment). This is closely related to the concept of utility, which should be the\nultimate criterion to judge whether the entire policy works well or not.\nOnce action At is selected, together with the provision of a reward Yt+1, the environment makes a transition to\na new state Xt+1 \u2208 Xt+1. Using this notation, the characterization of the RL sequential decision problem can be\ndescribed as an ordered sequence or trajectory given by:\nXoAo (Y1, X1) \u2192 A1 \u2192 (Y2, X2) \u2192\u2026\u2026\u2192 At \u2192 (Yt+1, Xt+1) \u2192 ...\nIn healthcare, this trajectory can be seen as the history of the interventions received over the course of a disease"}, {"title": "Mathematical formalization of the general RL", "content": "Define Xt = (X\u03c4)\u03c4=0,...,t, At = (A\u03c4)\u03c4=0,...,t, Yt+1 = (Y\u03c4+1)\u03c4=0,...,t, and similarly xt, at and yt+1, where the\nupper- and lower-case letters denote random variables and their particular realizations, respectively. Also define\nHt as the history all the information available at time t prior to decision At, i.e., Ht = (At-1, Xt, Yt); similarly\nht. The history Ht at stage t belongs to the product set Ht = X0 \u00d7 \u03a0t\u22650 At \u00d7 Xt+1 \u00d7 Vt+1. Note that,\nby definition, Ho = X0. We assume that each longitudinal history is sampled independently according to a\ndistribution P\u03c0, given by:\nP\u03c0 = Po(xo) \u03a0t\u22650\u03c0\u03c4(at | ht)Pt+1(Xt+1, Yt+1 | ht, at),\nwhere:\n\u2022 po is the probability distribution of the initial state Xo.\n\u2022 \u03c0 = {\u03c0t}t>o represents the agent's policy and determines the sequence of actions generated throughout the\ndecision-making process. More specifically, \u03c0t maps histories of length t, ht, to a probability distribution over\nthe action space At, i.e., \u03c0t(\u00b7 | ht). The conditioning symbol \"|\" in \u03c0t(\u00b7 | ht) reminds us that the policy defines\na probability distribution over At for each ht\u2208 Ht. Sometimes, At is uniquely determined by the history Ht,\ntherefore the policy is simply a function of the form \u03c0t(ht) = at. We call it deterministic policy, in contrast\nwith stochastic policies that determine actions probabilistically.\n\u2022 {pt}t>1 are the unknown transition probability distributions that characterize the dynamics of the environment.\nAt each time t \u2208 N, the transition probability pt assigns to each trajectory (Xt-1, at-1, yt-1) = (ht\u22121, at-1)\nat time t-1 a probability measure over Xt \u00d7 Vt, specifying the probability of transition to new states in Xt\nwith reward in Vt, from history ht-1 and action at\u22121, i.e., Pt(\u00b7, \u00b7 | ht\u22121, at-1).\nAt each time t, the transition probability distribution pt+1(Xt+1, Yt+1 | ht, at) gives rise to: (i) the state-\ntransition probability distribution Pt+1(Xt+1 | ht, at), i.e., the probability of transitioning to state xt+1 having\nobserved a history ht and taking action at; and (ii) the immediate reward distribution rt+1(Yt+1 | ht, at, Xt+1),\nwhich specifies the reward Yt+1 after transitioning from a history ht to xt+1 under action at. To better incorporate\nuncertainty, we assume a stochastic reward distribution. An illustrative representation of the RL framework is\nprovided in Figure 1. The cumulative discounted sum of immediate rewards from time t onward is known as\nreturn, say Rt, and is given by:\nRt = Yt+1 + \u03b3Yt+2 + \u03b32Yt+3 + \u2026\u2026 = \u2211T\u2265t\u03b3T\u2212tYT+1, t\u2208N."}, {"title": null, "content": "The discount rate \u03b3\u2208 [0,1] determines the current value of future rewards: a reward received 7 time steps in the\nfuture is worth only \u03b3\u03c4 times what it would be worth if it were received immediately. If \u03b3 = 0, the agent is\nmyopic in being concerned only with maximizing the immediate reward, that is, Rt = Yt+1, with the convention\nthat 0\u00ba = 1. If \u03b3 = 1, the return is undiscounted and it is well defined (finite) as long as the time-horizon is\nfinite, i.e., T < \u221e (Sutton and Barto, 2018).\nRemark: On-policy Vs. off-policy learning The agent's policy \u03c0 determines the sequential selection of\nactions. In a randomized experiment context, for example, it may define the randomization probabilities of each\nintervention at each decision point t. In such a setting, an agent can be interested in learning and optimizing the\npolicy \u03c0 while following it, that is, from experiences sampled directly from \u03c0. This type of learning is termed\non-policy or online learning, and policy \u03c0 represents both the exploration policy (the one that generates the\ndata) and the target policy (the one we learn about). In contrast, there are settings where the agent learns from\npreviously collected data without interacting with the environment to collect samples (e.g., observational data).\nIn this type of learning, termed off-policy or offline, we say that the target policy is learned from data \"off\" the\ntarget policy, which are determined according to a policy \u03c0 that can be either the exploration policy (when\nknown, e.g., in randomized studies), or, more generally, an observed or behavior policy. Similar concepts are\nused in statistical and causal inference for referring to the estimation of unknown quantities of interest such as\nparameters.\nTaking into account a potential misalignment between the target policy, say d, and the one used to generate\nthe data \u03c0 (either exploration or behavior), the RL problem at any time t is to learn an optimal way to choose\nthe set of actions, i.e., an optimal policy d\u2217 = {d\u03c4}\u03c4>t, so as to maximize the expected future return. Formally,\ndt\u2217 = arg maxdt Ed [Rt] = arg maxdt Ed [\u2211T\u2265t\u03b3T\u2212tYT+1]\nwhere the expectation is meant with respect to a trajectory distribution P\u03c0 analogous to Eq. (2), where the\npolicy \u03c0 that generated the data is replaced by the target policy d we want to learn about. Note that the\nexpected return is the most common approach to handle decision making under uncertainty (De Lara et al.,\n2008)."}, {"title": null, "content": "For learning optimal policies, various methods have been developed so far in the RL literature: see Sutton\nand Barto (2018) and Sugiyama (2015) for an overview. A traditional approach is through value functions, which\ndefine a partial ordering over policies, with insightful information on the optimal ones. In fact, optimal policies\nshare the same (optimal) value function, and comparing estimated value functions of different candidate policies\noffers a way to understand which strategy may offer the greatest expected outcome.\nThere are two types of value functions: i) state-value or simply value functions, say Vd, representing how\ngood it is for an agent to be in a given state, and ii) action-value functions, say Qd, indicating how good it is for\nthe agent to perform a given action in a given state. These are formally defined as:\nVd(ht) = Ed [Rt | Ht = ht] = Ed [\u2211T\u2265t\u03b3T\u2212tYT+1 | Ht = ht],\nQd(ht, at) = Ed [Rt | Ht = ht, At = at] = Ed [\u2211T\u2265t\u03b3T\u2212tYT+1 | Ht = ht, At = at],\n\u2200t \u2208 N, \u2200ht \u2208 Ht and \u2200at \u2208 At, with Ht and At such that P(Ht = ht) > 0 and P(At = at) > 0. By definition,\nat stage t = 0, Vd(ho) = Vd(xo); while for the terminal stage, if any, the state-value function is 0.\nAt stage t, the optimal value function Vd\u2217 yields the largest expected return for each history, and the optimal\nQ-function Qd\u2217 yields the largest expected return for each history-action pair, i.e.,\nQd\u2217(ht, at) = maxdtQdt(ht, at), ht \u2208 Ht, \u2200at \u2208 At.\nA fundamental property of the value functions used throughout RL is that they satisfy particular recursive\nrelationships, known as Bellman equations (Bellman, 1957). In the Q-value case, for instance, for any policy d,\nthe following consistency condition, expressing the relationship between the quality of an history-action and the\nquality of the successors, holds:\nQt(ht, at) = E [Yt+1 + maxat+1\u2208At+1Qt+1(ht+1, at+1) | Ht = ht, At = at],\n\u2200at \u2208 At, \u2200ht \u2208 Ht, \u2200t \u2208 N, and with discrete state and action spaces. Here, the expectation E is taken with\nrespect to the transition distribution Pt+1 only, which does not depend on the policy; thus, the subscript d can\nbe omitted.\nThe property in Eq. (8) allows estimation of (optimal) value functions recursively, from T backward in time.\nIn finite-horizon dynamic programming (DP), this technique is known as backward induction and represents one\nof the main methods to solve the Bellman equation. In infinite- and indefinite-horizon problems, traditional\nbackward induction is not possible, given the impossibility of extrapolating beyond the time horizon in the\nobserved data. To overcome this issue, alternative methods and additional assumptions (e.g., discounting and\nboundedness of rewards) are typically taken into account. Common strategies (e.g., V-learning, which we review\nin Section 3.3; Luckett et al., 2020), focuses on time-homogeneous Markov processes."}, {"title": null, "content": "Due to its generality, RL is studied and employed in many disciplines, from game theory to education and\nhealthcare. In this work, our goal is to review common RL classes that have been studied or used to support\ndecision making in precision and digital health. In particular, we cover the RL methods studied in the DTR\nliterature (e.g., Q-learning and outcome weighted learning) and in digital health (with a main interest in the\nmulti-armed bandit class, which we discuss in Section 4.1.1). For readers interested in the general area of RL\nand the broad spectrum of existing methods, without a specific application in mind, we refer to Sutton and\nBarto (2018); Sugiyama (2015)."}, {"title": "Dynamic Treatment Regimes in Precision Health", "content": "Clinical or behavioral treatments often involve a series of decisions over time that account for the continuously\nevolving histories of individuals. For example, weight loss management involves a sequence of decisions at multiple\nstages of weight progression. Initially, individuals affected by excess body weight undergo lifestyle modifications\n(such as diet and exercise), and based on their body mass index (BMI), may be treated with pharmacologic\ntherapies to achieve the desired body weight. Then, if the individual responds (i.e., shows a significant weight\nloss), the physician may prescribe a maintenance therapy (typically diet and exercise) to maintain weight at a\nreduced level. Otherwise, the clinician prescribes a second-line therapy, to try to induce body weight reduction.\nThere exist many possible therapies. The aim of the physician is to choose the sequence of therapies that leads\nto the best possible outcome, e.g., long-term maintenance of lost weight, for that individual.\nSimilarly, the treatment of cancer, diabetes, mental health disorders, or the management of addiction problems\nrequires a series of decisions by which the physician can start, stop, maintain, modify, or adjust interventions\nbased on the patient's response and evolving characteristics. This sequence of decisions constitutes a dynamic\ntreatment regime or regimen (Murphy, 2003; Chakraborty and Moodie, 2013), alternatively known as adaptive\ninterventions or strategies (Collins et al., 2004; Lavori and Dawson, 2000).\nDynamic treatment regimes (DTRs) offer a vehicle to operationalize the sequential decision-making process\ninvolved in clinical practice and can also be viewed as a decision support system. A DTR is defined as a sequence\nof decision rules, one per stage of intervention, dictating how to personalize treatments to patients based on\ntheir baseline and evolving history (time-varying, dynamic state), repeatedly adjusting over time in response to\nongoing performance (Almirall et al., 2014; Nahum-Shani et al., 2018). Thus, the treatment regime is \"dynamic\"\nwithin a person over time, varying because the person or disease is changing, with the goal of obtaining the best\nresults for that individual.\nThe existing DTR frameworks (Collins et al., 2004; Almirall et al., 2014) highlight four components that play\nan important role in the design of these interventions:\n(i) The critical decision points, specifying the time points at which a decision concerning intervention (e.g.,\ncontinue, alter, add, or subtract treatment) has to be made; here we assume a finite or countable number\nof times t = 0, 1, . . . ;"}, {"title": null, "content": "(ii) The decisions or treatment options at each time t, denoted by At \u2208 At, where At is the decision or\naction space, generally discrete;\n(iii) The tailoring variable(s) at each time t, say Xt \u2208 Xt, with Xt \u2286 RP, capturing individuals' baseline and\ntime-varying information for personalizing decision-making;\n(iv) The decision rules d = {dt}t>0, that, at each time t, link the tailoring variable(s) to specific decisions.\nTreatment options At \u2208 At are not limited to different medications or drugs, but can also include different\ndosages (duration, frequency or amount; Voils et al., 2012; Chen et al., 2016), various tactical options (for example,\nincrease, change, maintain), modes of administration (for example, oral or injection), timing schedules (Nie\net al., 2021), behavioral interventions, or no further treatment. Tailoring variables Xt \u2208 Xt refer to patient and\ntreatment information available up to the time of the critical decision, and may include previous treatment\nand disease history, genetic information, diagnostic test results, etc. Once the four elements are defined, each\ndecision rule d = {dt}t>o takes the individual characteristics Xt \u2208 Xt of a subject and their treatment history\nobserved up to that stage {At}t=0,1,...,t-1 as input and outputs a recommended treatment strategy at that stage.\nThe dynamic treatment regime dt = (do,..., dt) is regarded a multistage regime with each d\u2081, \u03c4 = 0, . . ., t being\na mapping of the entire evolving history Xt \u00d7 A0 \u00d7 \uff65\uff65\uff65 \u00d7 A\u03c4\u22121 \u00d7 X\u03c4 to A\u03c4. Unlike average-based single-stage\nprotocols, DTRs explicitly incorporate the heterogeneity in treatment effect among individuals and across time\nwithin an individual. As such, it provides an attractive framework for personalized treatments in longitudinal\nsettings. Furthermore, by treating only those who show a need for treatment, DTRs hold the promise of reducing\nnoncompliance due to overtreatment or undertreatment (Lavori and Dawson, 2000)."}, {"title": "RL methods for constructing optimal DTRS", "content": "One of the main research goals in the field of personalized dynamic treatments is to construct optimal DTRS,\nthat is, to identify the treatment rule(s) that result in the best (typically long-term) mean outcome, i.e., with\nthe highest utility. Most attempts to achieve this goal essentially require knowing or estimating the prespecified\nutility function or some variations of it. For example, Murphy (2003) defines regret (i.e., loss) functions, while\nRobins (2004) introduces blip functions (Kitagawa and Tetenov, 2018), alternatively known as welfare gains in\neconometrics.\nMethodologies for estimating optimal DTRs are of considerable interest within the domain of precision\nhealth and comprise a growing body of research in both computer science and statistics (Chakraborty and\nMoodie, 2013). On the one hand, the sequential decision-making nature of DTR problems perfectly conforms to\nthe RL framework, thus attracting increasing attention in the ML literature. On the other hand, the need to\nquantify causal relationships, rather than mere associations, called for the intervention of the causal inference\ncommunity. Since the underlying system dynamics is often unknown, inferring the consequences of executing\na policy d = {dt}t>1 and understanding the causal effects on an outcome is a challenging task. We refer to\nDeliu and Chakraborty (2022) and Tsiatis et al. (2021) for the broad range of aspects related to DTR (including"}, {"title": null, "content": "inference), while here we focus exclusively on the role of AI, more specifically RL methods, in deriving optimal\nDTRs. Notably, due to the similarity between the two problems and their components, RL represents one of the\nmain approaches employed in the DTR literature. A preliminary non-exhaustive correspondence table between\nthe RL and DTR terminologies is reported in Table 1."}, {"title": "Indirect RL methods in DTRS", "content": "Indirect methods focus on estimating an optimal objective function (typically, an expectation of the outcome\nvariable such as the Q-function presented in Eq. (6)), and then obtaining the associated policy. These methods are\nmainly based on iterative techniques such as dynamic programming (DP) and approximate dynamic programming\n(ADP), and include the Q-learning (Murphy, 2005b) approach that we illustrate below.\nWe mainly focus on the finite-horizon setting, where the utility function is optimized over a fixed and\nprespecified period of time T, and, for the sake of simplicity, we consider deterministic policies which map\nhistories h directly into actions or decisions, that is, d(h) = a.\nQ-learning Q-learning (Watkins, 1989) represents the core of modern RL and one of the most popular strategies\nin DTR research. Its fundamental idea is based on iterative improvement of the estimates of the Q-function at a\ngiven stage t, starting from a previous estimate and following the Bellman rule in Eq. (8). That is,\nQt(ht, at) \u2190 Qt(ht, at) + \u03b1t [Yt+1 + \u03b3 maxat+1\u2208At+1Qt+1(ht+1, at+1) - Qt(ht, at)].\nThe constant \u03b1t determines to what extent the newly acquired information should override the old information,\ni.e., how fast learning takes place: a factor of 0 will make the learner not learn anything, while a factor of 1\nwould make the learner fully update based on the most recent information. The discount factor \u03b3 balances the\nimmediate rewards of the learner with future rewards, and in a finite-horizon problem it is generally set to one.\nThe original version of this approach is known as tabular Q-learning, and it is based on storing the Q-function\nvalues for each possible state and action in a lookup table and choosing the one with the highest value. Under\nsome appropriate and rigorous assumptions (Watkins, 1989), Qt has been shown to converge to the optimal\nQ-function Q with probability 1. However, this procedure is practical for a small number of problems because\nit can require many thousands of training iterations to converge. In addition, it represents value functions in\narrays or tables, based on each state and action. Thus, large state spaces lead not just to memory issues for large\ntables but also to time problems needed to fill them accurately. A more recent version of Q-learning, known as\nQ-learning with function approximation (FA), offers a powerful and scalable tool to overcome both the modeling\nrequirements and the computational burden to solve an RL problem through backward induction."}, {"title": null, "content": "The main idea of Q-learning with FA is first to estimate the Q-functions using an approximator, e.g.,\nregression models, neural networks or decision trees, and then to derive the estimated policy based on the"}, {"title": null, "content": "estimated Q-functions. Considering an approximation space for each of the T stage-specific Q-functions, e.g.,\nQt = {Qt(ht, at; \u03b8t) : Ot \u2208 \u0398t}, with Ot the parameter space, an optimal stage-t policy estimate is given by:\nd\u2217t(ht) = arg maxat\u2208At Qt(ht, at) = arg maxat\u2208At Qt(ht, at; \u03b8t) = d\u2217t(ht; \u03b8t), t = 0, . . ., T.\nAn optimal regime d\u2217 = (d\u22171(x1; \u03b81), d\u22172(h2; \u03b82), . . ., d\u2217T(hT; \u03b8T)) is obtained by following Bellman's optimality\nequation in Eq. (8), and by recursively estimating Qt backward in time t = T,T \u2212 1,..., 1. Noticing that\nQ-functions are conditional expectations, regression models represent a natural approach. For the complete\ngeneral iterative procedure, as well as more specific examples, we point to Deliu and Chakraborty (2022). By\nusing generalized linear models, one may extend the Q-learning method to binary and count outcomes, and\nan accelerated failure time model can be incorporated for survival outcomes. In the DTR arena, Q-learning\ngeneralizations to diverse outcomes have been implemented for censored data Goldberg and Kosorok (2012);\nZhao et al. (2011, 2020), binary data Moodie et al. (2014), or composite measures attempting to balance different\nobjectives (Laber et al., 2014), among others.\nIn order for d\u0302\u2217 to be a consistent estimator of the true optimal regime d\u2217, it is important to recognize that\nall the models for the Q-functions should be correctly specified (Schulte et al., 2014). To address this problem,\nseveral FA alternatives such as support vector regression and extremely randomized trees (Zhao et al., 2009),\nor deep neural networks (Liu et al., 2017; Atan et al., 2018; Raghu et al., 2017) have been proposed. We now\nillustrate the latter, given the attention it has attracted in recent years.\nDeep Q-learning. The success achieved by Q-learning in many complex domains has been largely enabled by\nthe use of advanced FA techniques such as deep neural networks (Mnih et al., 2015). We call this approach Deep\nQ-learning (DQL). In DQL, a neural network (Goodfellow et al., 2016) is used to approximate the Q-function.\nMore specifically, at each time t, a DNN is used to fit a model for the Q-function in a supervised way. States\nand actions {(Ht,i, At,i)}i=1,...,\u03bd are given as inputs (in the input layer), and the Q-values of all possible actions\nare generated as outputs {Qt(Ht,i, At,i; \u0174,b)}i=1,...,\u03bd (in the output layer), leading to a labeled set of data\n{(Ht,i, At,i), Qt(Ht,i, At,i; \u0174, b)}i=1,...,\u03bd. Input data are non-linearly transformed based on the unknown weight\nW and bias b parameters and carried out through the neurons of the hidden layers. Figure 3 shows a schematic\nof a feed-forward neural network used within RL. It is characterized by a set of neurons, structured in layers,\nwhere each neuron processes the information from one layer to the next. The collected data are stored and used\nfor continuously updating the Q-function parameter estimates. To allow exploration, each decision is determined\nby an exploration scheme (typically \u03f5-greedy) that probabilistically chooses between the action with the highest\nQ-value and a random action.\nWithin the DTR literature, DQL has been implemented with EHR data in Liu et al. (2017) and Raghu et al.\n(2017) for graft-versus-host disease and sepsis treatment, respectively. Compared to its shallow counterpart, the\nDQL framework is particularly suitable for (i) automatically extracting and organizing discriminative information\nfrom the data and (ii) exploring the high-dimensional action and state spaces and making personalized treatment"}, {"title": "Direct RL methods in DTRS", "content": "Direct methods, also known in the RL literature as direct policy search methods (Ng and Russell, 2000), seek to\nmaximize the return by learning the optimal policy directly, without involving the estimation of intermediate\nquantities such as optimal Q-functions or contrasts. These methods typically do not assume models for conditional\nmean outcomes; thus, they are referred to as \"nonparametric\". However, they may consider a parameterization\nfor the policies or regimes class.\nIn direct methods, a class of policies D, often indexed by a parameter, say \u03c8 \u2208 \u03a8, is first prespecified. Then,\nfor each candidate regime d\u2208 D, an estimate of the corresponding utility is obtained. The utility can be a\nsummary of one outcome, such as percent days of abstinence in an alcohol dependence study or a composite\noutcome. For example, in Wang et al. (2012) the utility is a compound score that combines information on\ntreatment efficacy, toxicity, and risk of disease progression. Here, without loss of generality, we take the utility\nto be the value of the policy; see Eq. (5). The regime in D that maximizes the value function is the estimated\noptimal DTR, that is, d\u2217 = arg maxd\u2208D Vd, or d\u2217 = arg max\u03c8\u2208\u03a8 Vd\u03c8 for parametric classes. A common example\nof parametric classes is the soft-max class D = {\u03c0(ak|x, \u03c8) = e-xT\u03c8k /\u2211Kj=1e-xT\u03c8j : \u03c8 \u2208 \u03a8, k = 1,...,K},\nwhere a1,...,aK are the K possible treatments and \u03c8 = (\u03c81,...,\u03c8K) is the vector of parameters for the K\ntreatments indexing the class of policies.\nMost statistical work in this area is based on the inverse probability of treatment weighting (IPTW) esti-\nmator (Robins, 1994), used for estimating value functions (Zhang et al., 2012, 2013), in classification-based\nframeworks such as outcome weighted learning (OWL; Zhao et al., 2012, 2015; Liu et al., 2018), and in com-\nbination with ML approaches such as decision trees (Laber and Zhao, 2015; Tao et al., 2018). Particularly\nuseful in observational data, where the exploration and target policies differ, IPTW (Robins, 2000) makes use\nof importance sampling to change the distribution under which the regime's value is computed. In doing so,\nassuming that Pd is absolutely continuous with respect to P\u03c0, it basically weights outcomes according to the"}, {"title": null, "content": "relative probability of interventions occurring under the target d and exploration \u03c0 policies. The value function\nthen can be rewritten as:\nVD = Ed [Yd] = \u222bYDdPd = \u222bYDd (dPd\u03c0) dP\u03c0 = \u222bY (Pd(H)\u03c0(A|H))dP\u03c0 = \u222bY W\u03b1,\u03c0Y dP\u03c0.\n\u03c0 (At|Ht)dP \u03c0\nTo estimate Vd, the Monte Carlo (MC) estimator given by \u0176d = PNYd, where PN denotes the empirical\naverage over N trajectories, is generally used. By the Strong Low of Large Numbers, the MC estimator is\nunbiased, but its variance is unbounded. To stabilize this estimator, the weights wd,\u03c0 are normalized by their\nsample mean, leading to the IPTW estimator:\nVD = PN Wd,\u03c0YPNWd,\u03c0\nThe technique also allows balancing the confounders across levels of treatment: the higher the probability of\nreceiving a specific treatment conditioned on the confounder X, \u03c0(A|X), the lower the weight w\u03c0 = 1/\u03c0(A|X)\nof their outcome Y.\nWhen \u03c0 is known (e.g., randomized trials), the IPTW estimator is consistent, but it can be highly variable\ndue to the presence of nonsmooth indicator functions inside the weights. An alternative version, which integrates\nthe properties of the IPTW estimator with those of regression-assuming models for both the propensity score\nand the (conditional) mean outcome is the augmented inverse probability of treatment weighting (AIPTW)\nestimator (Zhang et al., 2012). Assume a single-stage treatment regime with two treatment options (A \u2208 {a, a'}),\nand let H = Xo to be a patient's history, d(H) = d(H; \u03c8) a treatment regime indexed by \u03c8, \u03bc(A, H; \u03b2) an\nestimated model for the mean outcome E[Y|H, A], and \u03c0(A|H, \u03b3\u0302) an estimated propensity score. Then, the\nAIPTW estimator is defined as:\nVAIPTW = PN {I[A = d(H; \u03c8)]Y\u03c0[A(H\u03c0(H; \u03c8, \u03b3H; \u03b3)\u0302]] \u2212 \u03bc\u03bc(H; \u03c8, \u03b2)H; \u03c8, \u03b2)}\nwhere,\n\u03c0(\u0397; \u03c8, \u03b3\u0302) = \u03c0(\u03b1|H, \u03b3\u0302)I[d(H; \u03c8) = a] + \u03c0(a'|H, \u03b3\u0302)I[d(H;\u03c8) = a'],\n\u03bc(H;\u03c8, \u03b2) = \u03bc(\u03b1, H; \u03b2)I[d(H; \u03c8) = a] + \u03bc(\u03b1', H; \u03b2)I[d(H;\u03c8) = a'].\nIt only requires that either the propensity or mean outcome model to be correctly specified but not both; hence,\nthe doubly robust property. In addition to being more robust to model misspecification, AIPW estimators tend\nto be more efficient than their nonaugmented counterparts (Robins, 2004).\nAlthough its original version was designed for a single-stage treatment regime, it was subsequently adapted\nto two or more decision points (Zhang et al., 2013; Tao and Wang, 2017; Zhou et al., 2018), where models are"}, {"title": null, "content": "posited for either Q-functions or contrasts.\nOutcome weighted learning (OWL) As an alternative direct approach, Zhao et al. (2012) studied the DTR\nestimation problem as a weighted classification problem-with weights retrospectively determined from clinical\noutcomes (hence \u201cOutcome Weighted\")-and proposed to solve it with ML tools (hence \u201cLearning\").\nIn the case of two treatments, expressed as A \u2208 {\u22121"}]}