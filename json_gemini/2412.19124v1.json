{"title": "Evaluating Self-Supervised Learning in Medical Imaging: A Benchmark for Robustness, Generalizability, and Multi-Domain Impact", "authors": ["Valay Bundele", "O\u011fuz Ata \u00c7al", "Bora Kargi", "Karahan Sar\u0131ta\u015f", "K\u0131van\u00e7 Tez\u00f6ren", "Zohreh Ghaderi", "Hendrik Lensch"], "abstract": "Self-supervised learning (SSL) has emerged as a promis- ing paradigm in medical imaging, addressing the chronic challenge of limited labeled data in healthcare settings. While SSL has shown impressive results, existing studies in the medical domain are often limited in scope, focusing on specific datasets or modalities, or evaluating only iso- lated aspects of model performance. This fragmented eval- uation approach poses a significant challenge, as models deployed in critical medical settings must not only achieve high accuracy but also demonstrate robust performance and generalizability across diverse datasets and varying con- ditions. To address this gap, we present a comprehensive evaluation of SSL methods within the medical domain, with a particular focus on robustness and generalizability. Using the MedMNIST dataset collection as a standardized bench- mark, we evaluate 8 major SSL methods across 11 different medical datasets. Our study provides an in-depth analysis of model performance in both in-domain scenarios and the detection of out-of-distribution (OOD) samples, while ex- ploring the effect of various initialization strategies, model architectures, and multi-domain pre-training. We further assess the generalizability of SSL methods through cross- dataset evaluations and the in-domain performance with varying label proportions (1%, 10%, and 100%) to sim- ulate real-world scenarios with limited supervision. We hope this comprehensive benchmark helps practitioners and researchers make more informed decisions when applying SSL methods to medical applications.", "sections": [{"title": "1. Introduction", "content": "Medical image annotation is a resource-intensive task that requires specialized domain knowledge, making it signif- icantly more costly and laborious than annotating natural images [39, 45, 63, 79]. The scarcity of labeled medi- cal data, combined with the complexity of the annotation process, presents a significant challenge for building effec- tive machine learning models in healthcare. Self-supervised learning has emerged as a powerful solution to these limi- tations, enabling models to learn rich representations from unlabeled data prior to task-specific fine-tuning with mini- mal labeled samples [2, 6, 21, 24, 38, 40, 71].\nWhile self-supervised learning (SSL) has proven effec- tive in improving classification performance with limited labels [13\u201315, 29, 80], there has been limited work in eval- uating the robustness and generalizability of learned mod- els. For safe deployment in healthcare, models must per- form reliably across diverse settings and recognize when to withhold predictions on out-of-distribution (OOD) samples. Although SSL methods have been evaluated for OOD de- tection in natural image contexts [31, 36, 43, 48, 57], their effectiveness in medical imaging remains largely under- researched. Berger et al. [9] explore OOD detection using supervised approaches in a clinical context, but focus solely on chest X-rays, limiting insights across other domains. Bo- zorgtabar et al. [11] evaluate SSL methods for OOD detec- tion under domain shift but they only investigate two SSL approaches in a single medical domain. Although Cai et al. [12] provide a comprehensive study of SSL for anomaly de- tection, they focus specifically on anomaly detection SSL methods rather than general-purpose SSL approaches.\nBeyond OOD detection, an effective model should gen- eralize across different tasks and modalities\u2014a key require- ment in medical imaging where datasets and conditions vary widely. A model that performs well across different datasets or modalities ensures continued diagnostic support even when some imaging modalities are inaccessible, re- duces the need for extensive retraining, and ensures robust performance in diverse clinical settings. Azizi et al. [7] propose an SSL-based representation learning method and assess its robustness across various medical imaging tasks and domains. However, they do not compare different SSL methods, leaving a gap in research on the cross-domain gen- eralizability of common SSL approaches.\nTo address these gaps, our study introduces a compre-"}, {"title": "2. Related Work", "content": "Prior self-supervised benchmarks Several studies have focused on benchmarking self-supervised strategies on nat- ural image datasets [19, 27, 54]. However, a gap re- mains in the literature when it comes to benchmarking SSL methods in the medical domain using standardized datasets. Recently, Doerrich et al. [21] proposed a sys- tematic benchmark covering convolutional and transformer- based architectures for both supervised and self-supervised strategies. Nevertheless, their approach relies heavily on IMAGENETIK pre-trained encoders, without incorporat- ing self-supervised pre-training on medical datasets. This may limit model\u2019s ability to capture domain-specific fea- tures. Kang et al. [40] demonstrated that self-supervised pre-training outperforms supervised IMAGENET1K base- lines for pathology but did not explore other modalities. Huang et al. [38] analyzed SSL and semi-supervised meth- ods with hyperparameter tuning but limited their study to four medical datasets, without assessing robustness or gen- eralizability across diverse tasks. Our work advances SSL benchmarks in the medical domain by evaluating methods across several medical datasets, emphasizing multi-domain performance, encoder robustness, generalizability across different datasets, and adaptability to limited labeled data.\nOut-of-Distribution Detection Robust pre-trained en- coders can be employed as OOD detectors to prevent dan- gerous misclassifications in the medical domain. To this end, several studies have investigated the use of visual recognition systems as OOD detectors [9, 26, 48, 59, 81]. Hendrycks et al. [36] show that SSL methods outperform fully supervised ones on natural image datasets for OOD de- tection. Narayanaswamy et al. [59] address modality shift and novel class detection in the medical domain, but their focus is limited to supervised training of OOD detectors. The SSD framework by Sehwag et al. [64] demonstrates that SSL can significantly improve OOD detection, achiev- ing performance comparable to supervised methods. Addi- tionally, both Li et al. [48] and Mohseni et al. [57] intro- duce self-supervised approaches that improve OOD detec- tion, though their analyses are limited to natural images. Despite these advancements, there remains a notable gap: no prior work has comprehensively evaluated SSL methods for OOD detection in the medical domain, across diverse architectures and pre-training strategies. Our study aims to fill this gap by providing a benchmark that includes OOD detection experiments tailored to medical imaging.\nGeneralizability The ability of an encoder to perform ef- fectively on datasets and domains beyond its training dis- tribution is crucial for assessing representation quality. Li et al. [47] enhance generalizability through variational en- coding with a linear-dependency regularization, while Yan et al. [73] propose a domain-generalization framework for medical image classification without domain labels in the supervised setting. Although Fedorov et al. [25] investigate the generalization of SSL methods in medical imaging, their analysis is confined to MRI data. Azizi et al. [7] propose an"}, {"title": "3. Methodology", "content": "Representation Learning Methods We consider the fol- lowing eight discriminative SSL methods: SimCLR [14], DINO [13], BYOL [28], ReSSL [80], MoCo v3 [15], NNCLR [23], VICREG [8], and Barlow Twins [77] which are explained briefly in Appendix B. A comprehensive sur- vey by Huang et al. [37] highlights SimCLR, MoCo, and BYOL as the most frequently adopted SSL frameworks in medical imaging research. We include the other methods to make the benchmark more comprehensive.\nTasks and Datasets The datasets utilized in this study are derived from MedMNIST [74], which was introduced to standardize research on medical imaging tasks. To maintain our focus on multiclass medical classification and ordinal regression, we exclude ChestMNIST from our analysis, as it only offers multi-label disease classification. As a result, our experiments span 11 MedMNIST datasets. Additional information about MedMNIST is provided in Appendix A.\nArchitectures For our study, we use ResNet-50 [33] with approximately 25 million parameters and ViT-Small [22] with approximately 22 million parameters to ensure a fair comparison. We exclude other models like ViT-Tiny and ViT-Base due to their significantly different parameter counts of 5.7 million and 86 million, respectively.\nPre-training We employed the solo-learn library [19] with minor modifications for pre-training and linear evaluation. Our implementation details are provided in Ap- pendix C.1, and the code will be released after acceptance. In total, we focus on five different pre-training schemes: su- pervised training with (1) random initialization and (2) su- pervised IMAGENET1K initialization; self-supervised pre- training with (3) random initialization, (4) supervised IM- AGENETIK initialization, and (5) self-supervised IMA- GENET1K initialization.\nLinear Evaluation To evaluate the quality of self- supervised pre-trained encoders, we use linear probing, training linear classifiers on frozen features to assess down- stream performance [28, 34, 44]. Additionally, to simulate realistic scenarios in the medical domain, we evaluate the low-shot performance by training a multi-class logistic re- gression on the frozen features with only 1% and 10% la- beled data, following the evaluation protocol established by Caron et al. [13]. We report accuracy and Area Under the Curve (AUC) scores. Further details on the linear evalua- tion setup and evaluation metrics can be found in C.3 and C.4, respectively.\nGeneralizability To assess generalizability, we conduct cross-dataset experiments using our dataset collection, D = {D1,..., D11}. For each SSL method, we pre-train the model on a dataset Di and evaluate its transferability by training a linear classifier on frozen features on each of the remaining datasets, D \\ D\u00bf. This process is repeated for each dataset Di in D to cover all cross-dataset pairs.\nOOD Detection We evaluate various SSL methods for distinguishing between in-distribution and OOD samples, focusing on the impact of backbone architectures, pre- training strategies, and multi-domain learning on OOD detection. We use Mahalanobis Distance [46] to assign pseudo-labels by calculating the distance between input fea- tures and the nearest class-conditional Gaussian. Further details on the OOD detection metric and evaluation criteria can be found in Appendix C.5.\nMulti-Domain Learning We examine the impact of multi-domain learning by comparing models trained on single-domain MedMNIST datasets with those trained on two dataset combinations. The first combination, named Organ{A,C,S}, merges the Organ{A,C,S}MNIST datasets to represent a single-modality scenario. The second com- bination, referred to as Organ{A,S}PnePath, includes Or- gan{A,S}MNIST (CT), PathMNIST (Colon Pathology), and PneumoniaMNIST (Chest X-Ray), allowing us to as- sess the effect of combining different modalities. We then train SSL methods from scratch on these combined datasets to assess how dataset combination affects performance."}, {"title": "4. Experiments & Results", "content": "In this section, we analyze the in-domain (ID) performance, robustness to OOD samples, and generalizability of the SSL methods. For clarity in certain figures where direct compar- isons between the SSL methods are not essential, we dis- play results for a representative subset of five methods, with complete results available in the Appendix for reference.\n4.1. In-Domain Performance\nWe evaluate each SSL method\u2019s in-distribution (ID) perfor- mance by pre-training a randomly initialized ResNet-50 on each dataset\u2019s training split, then performing linear evalua- tion on the test split using the frozen backbone. As shown in Table 2, MoCo v3 exhibits strong performance, achiev- ing the highest accuracy across 5 of the 11 datasets. BYOL and SimCLR also demonstrate competitive results, trailing closely behind MoCo v3. Notably, self-supervised learn- ing outperforms supervised learning in 7 out of 11 datasets when both approaches start from random initialization.\nNext, we analyze the effect of supervised IMAGENET1K initialization on self-supervised training performance for in- domain tasks. As shown in Figure 1, IMAGENET1K ini- tialization consistently improves performance on in-domain classification tasks, a trend also observed across other meth- ods in Appendix D.1. Interestingly, DINO and BYOL show the most significant accuracy gains when transitioning from random to IMAGENET1K initialization.\nFigure 2 further emphasizes the effect of IMAGENET1K initialization by showing the accuracy differential between ResNet-50 and ViT (ACCRN50 - ACCvit) across various methods and datasets, presented for both random and IM- AGENET1K-initialized cases. In both scenarios, ResNet- 50 generally outperforms ViT. However, the accuracy gap between these architectures generally narrows when tran- sitioning from random initialization to IMAGENET1K- supervised weights. This reduction can be attributed to two factors: firstly, as overall accuracy improves, incre- mental gains in performance diminish, naturally reducing the architecture gap. Secondly, as prior research suggests [3, 5, 22], the data-intensive nature of transformer-based architectures can be harnessed effectively through large- scale pre-training, such as with IMAGENET1K initializa- tion. Complete numerical results for all methods are pro- vided in Appendix D.1.\nExpanding on our in-domain performance evaluation, we also examine how the architectures perform under dif- ferent levels of label availability. presents accuracy drops for various SSL methods when reducing label avail- ability from 100% to 1% in random initialization setting. ResNet-50 not only outperforms other methods in the fully labeled (100%) scenario, as shown in Figure 2, but also ex- hibits consistently lower average performance drops across nearly all methods. This indicates that ResNet-50 main-"}, {"title": "4.2. Out-of-Distribution Detection", "content": "We perform several experiments to assess OOD detection performance of each SSL method. In each experiment, one dataset serves as the ID dataset, while others are treated as OOD for evaluation; resulting in 110 OOD tests (11 \u00d7 10) per method. The pre-trained encoder, trained on the ID dataset, is evaluated for its ability to detect OOD samples.\nFigure 5 illustrates the distribution of AUROC scores for OOD detection across various SSL methods using randomly initialized ResNet-50 backbones. Among these, NNCLR and MoCo v3 achieve the highest AUROC scores, suggest- ing that these methods are particularly effective in learn- ing representations that differentiate ID from OOD sam- ples. When evaluated with a ViT-Small backbone, MoCo v3 demonstrate strong OOD detection performance, which aligns with expectations, as this method were originally de- veloped for the ViT architecture. Detailed analysis of the ef- fect of backbone choice on each method and in-distribution, out-of-distribution pairs (PID, POOD) can be found in Ap- pendix D.2.2.\nIn our experimental setup, the choice of backbone ar- chitecture played a crucial role in OOD detection perfor-"}, {"title": "4.3. Generalizability", "content": "In this section, we evaluate transferability of representations learned from SSL methods by performing cross-dataset evaluations. Specifically, we first pre-train the encoder on one of the datasets and then train a linear classifier on each of the remaining datasets individually, using a frozen en- coder. We evaluate the generalizability of learned represen- tations by calculating the drop in accuracy relative to the in-domain performance as, $\u0394ACC = ACC_{ID} - ACC_{transfer} \u00d7 100$, where $ACC_{ID}$ represents the accuracy when training and testing are performed on the same dataset (in-domain), and $ACC_{transfer}$ denotes the accuracy when the model is pre- trained on a different dataset. For each source dataset, we calculate the average accuracy drop ($\u0394ACC$) across all tar- get datasets, providing insights into how well a method trained on one dataset performs when transferred to others and which datasets yield the most transferable knowledge. \nTable 4 shows that SimCLR and MoCo v3 excel in gen- eralization, with both achieving the lowest accuracy drops in 4 datasets. Notably, models pre-trained on PathMNIST or Organ{C,S}MNIST show the lowest accuracy drops, suggesting these datasets offer the most transferable repre- sentations.\nWe further examine the impact of supervised IMA- GENET1K initialization on model generalizability. Figure 8 illustrates the transfer performance difference for models pre-trained on DermaMNIST with random versus IMA- GENET1K initialization. In general, the results favor IM- AGENET1K initialization across the majority of evaluation scenarios. This trend holds across other datasets and self- supervised methods as well. To evaluate the overall impact of IMAGENET1K initialization compared to random initial- ization, we conducted paired t-tests for each test dataset across all cross-dataset training combinations including the in-domain setting where test and train splits come from the same dataset. The results demonstrate statistically signif- icant improvements (p < 0.05) across all datasets, with PneumoniaMNIST showing the most modest relative gain"}, {"title": "5. Discussion & Conclusion", "content": "We have conducted a comprehensive evaluation of popular SSL methods in medical imaging, addressing a crucial gap in understanding how these methods perform across diverse medical tasks. Based on our results and analysis, we offer the following recommendations for practitioners:\nWhich self-supervised method to choose? MoCo v3 demonstrates remarkable versatility in medical imaging tasks when trained from scratch, achieving superior perfor- mance in 5/11 datasets for in-domain tasks and maintaining minimal accuracy drops in 4/11 datasets for cross-domain generalization. It also shows competitive performance in OOD detection tasks. However, the effectiveness of cer- tain SSL methods is significantly influenced by initializa- tion strategy. Notably, DINO and BYOL transform from being among the lowest performers to achieving competi- tive in-domain results when initialized with IMAGENET1K weights, with DINO particularly excelling with ViT.\nShould we start self-supervised pre-training with super- vised IMAGENET1K weights? Despite concerns that su- pervised pre-training might yield less general representa- tions due to label dependency [49, 62], IMAGENET1K ini-"}]}