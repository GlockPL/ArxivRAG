{"title": "MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback", "authors": ["Zonghai Yao", "Aditya Parashar", "Huixue Zhou", "Won Seok Jang", "Feiyun Ouyang", "Zhichao Yang", "Hong Yu"], "abstract": "Automatic question generation (QG) is essential for AI and NLP, particularly in intelligent tutoring, dialogue systems, and fact verification. Generating multiple-choice questions (MCQG) for professional exams, like the United States Medical Licensing Examination (USMLE), is particularly challenging, requiring domain expertise and complex multi-hop reasoning for high-quality questions. However, current large language models (LLMs) like GPT-4 struggle with professional MCQG due to outdated knowledge, hallucination issues, and prompt sensitivity, resulting in unsatisfactory quality and difficulty. To address these challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique and Correction) framework for converting medical cases into high-quality USMLE-style questions. By integrating expert-driven prompt engineering with iterative self-critique and self-correction feedback, MCQG-SRefine significantly enhances human expert satisfaction regarding both the quality and difficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based automatic metric to replace the complex and costly expert evaluation process, ensuring reliable and expert-aligned assessments.", "sections": [{"title": "Introduction", "content": "In Artificial Intelligence (AI) and Natural Language Processing (NLP), automatic question generation (QG) from knowledge bases, texts, and images plays a crucial role in enhancing question-answering (QA) models, supporting intelligent tutoring systems, improving dialogue systems, and aiding fact verification. Multiple-choice question generation (MCQG), a specialized type of QG, is extensively used in exams to assess students' knowledge efficiently. However, creating MCQs is labor-intensive, requiring the design of effective stems, prediction of common errors as distractors, and provision of corrective feedback. In professional fields, MCQs often require field experts because they need to reflect real-world scenarios and involve complex multi-hop reasoning. These are unique challenges not typically encountered in general QG tasks.\nThe United States Medical Licensing Examination (USMLE) exemplifies the need for high-quality MCQG. Preparing for the USMLE costs medical students over $5000 on average. For exam boards and instructors, creating MCQs is both time-consuming and expensive. Any application that can automate this process is highly valuable to medical educators. Due to the high difficulty with the need for domain knowledge and complex reasoning, USMLE questions are becoming important large language models (LLMs) benchmarks. Top LLMs like GPT-4 have shown over 90% accuracy on sample USMLE questions. Recent research explores leveraging GPT-4's potential in USMLE-MCQG to improve question generation efficiency for educators and assist students in exam preparation.\nHowever, relying solely on LLMs like GPT-4 to generate USMLE questions presents several challenges. Firstly, their performance is constrained by their training data, leading to two major issues: outdated knowledge and hallucination. Outdated knowledge means that LLMs can only repeat or integrate old USMLE questions or medical documents in their training data during the generation. Consequently, they struggle to create new questions based on the latest medical cases or guidelines like medical experts. Hallucination refers to LLMs potentially providing misinformation in questions, which could harm students' learning outcomes. Secondly, generating questions on specific concepts requires precise prompts , which students might not know how to formulate. For example, given one USMLE question about cardiovascular physiology, if GPT-4 is asked to \"Generate a more difficult USMLE question.\", it only provides a longer stem. It will focus on the specific topics and test points only when specifically asked to generate a question \"testing knowledge of the hemodynamic profile of aortic stenosis.\" Lastly, the quality and difficulty of the questions often do not meet expectations.\nTo address these challenges, we propose a new task: converting medical cases that appear in case reports or clinical notes into USMLE-style MCQs. Our approach involves several key design elements to alleviate the above limitations:\nFor previous research has attempted to prompt LLMs to generate USMLE-MCQs by following expert-crafted instructions in zero-shot setting or further adding existing questions as few-shot examples. To our knowledge, we are the first to qualitatively and quantitatively study how to convert medical cases into USMLE-MCQs. These medical cases provide valuable real-world information on disease progression, accurate assessments, diagnoses, and potential treatment plans. Using the latest medical cases as input, LLMs can generate up-to-date questions, thereby minimizing the limitations of outdated knowledge. Additionally, grounding questions' key elements in the original medical cases can help LLMs reduce hallucinations, enhancing the reliability of the generated content.\nFor , we follow the National Board of Medical Examiners (NBME) guidelines to establish a checklist of 41 target topics covering all potential exam areas. We then deployed a ColBERT retriever using USMLE Content Outline as a collection for test points retrieval. Each input medical case in our experiments is evaluated by experts with exam experience to determine if it contains sufficient information to generate questions related to the specified target topics and test points. We also compared the topics and test points identified by experts with those generated by LLMs, assessing their impact on the quality and difficulty of the resulting questions.\nFor , as illustrated in Figure 1, we used the triplets (medical case, topic, and test point) as our question generation pipeline input. We work with experts for prompt engineering based on USMLE guidelines. We then created our MCQG-SRefine (e.g., self-refine with iterative Critique and Correction feedback) following three steps. S1 - Initial MCQ Generation: Generate an initial USMLE-MCQ based on the triplets. S2 - Critique Feedback: Prompt the LLM itself to provide feedback on the S1 USMLE-MCQ. S3 - Correction Feedback: Correct the USMLE-MCQ based on the S2-generated critique feedback. Through iterative critique and correction, MCQG-SRefine significantly enhances the quality and difficulty of generated USMLE-style MCQs. Human evaluations confirm its effectiveness, showing a strong preference for MCQG-SRefine generated questions, with a preference ratio of 72.5% in win, 10% tie, and 17.5% loss when compared to GPT-4 generated questions. In terms of difficulty, MCQG-SRefine generates more challenging questions. Specifically, when provided with expert-identified topics and test points, there is an 80% reduction in easy questions, a 2.25-fold increase in medium questions, and a 4-fold increase in hard questions.\nFinally, designing a reliable reference-free metric to automatically evaluate the quality of system-generated USMLE-MCQs is challenging. Recent research indicates that LLM-as-Judge correlates more closely with human evaluations than traditional metrics, though these methods remain underexplored in medical NLP tasks. Our goal is to replace the costly expert evaluation process in USMLE-MCQG with LLM-as-Judge. We used 30 criteria designed by experts in their human evaluation, covering different aspects of USMLE questions, to guide LLM-as-Judge in providing rating or comparison feedback on different systems' questions. By further exploring the filtering methods for the 30 criteria, we finally screened out a combination of 10 key criteria. This improved the correlation between LLM-as-Judge and expert evaluations, as measured by Cohen's kappa, from 0.226 (slight reliability) to 0.539 (moderate reliability). Using the results from this automated evaluation system, it was shown that the preference rate for questions generated by MCQG-SRefine over those generated by GPT-4 was 79.97% in favor and 20.03% against. Moreover, MCQG-SRefine demonstrated overall improvements across 10 criteria within the 5 components, not just in a specific area."}, {"title": "Method", "content": "Problem statement: Given a medical case n detailing a patient's history, diagnosis, treatment, and outcome, we aim to generate a USMLE question u. Here, \\(u = <c, q, a, d>\\) consists of a context (c), which is a modified excerpt from n tailored to align with the target style and obscure evidence information that can easily lead directly to the correct answer; a question (q) based on the generated context, which may be one or several sentences; the correct answer (a) to this question; and several distractor options (d)."}, {"title": "Topic and test point identification", "content": "As discussed in , generating questions using LLMs without specific guidance, such as defined topics t and test points k, often results in questions that lack relevance, quality, and appropriate difficulty. These questions may fall outside the scope of the USMLE exam, being either too simple or overly complex. Therefore, the quality and difficulty of the generated USMLE questions are significantly influenced by the selection of topics t and test points k.\nTopics t refer to a list of target topics selected from 41 potential topics outlined in the NBME official guidelines, categorized into 10 sections. Both the LLM and human experts are provided with this list to generate a maximum of five topics (t) that are highly relevant to the medical case.\nTest points k refer to the core concepts closely related to the correct answer. We employ the ColBERT retriever , denoted as \\(\\pi_{rtr}\\), to retrieve suitable test points from 18 sections."}, {"title": "Initialization", "content": "As illustrated in Figure 2, the MCQG-SRefine pipeline begins with the INIT step, which comprises 4 generation steps, each targeting a component of the goal \\(u = <c, q,a,d>\\). To assist the model in referencing similar examples for better generation of each component of u, we deploy a ColBERT retriever model R to retrieve a small set of USMLE examples from the MedQA question bank. As shown in Figure 2, given the input \\(< n,t,k >\\) from Topic and Test point identification ste, R first uses \\(< n,t,k >\\) as a query to retrieve few-shot examples, and then LLM follows the INIT-c prompts in Appendix Table 16 to generate the context c. Subsequently, after obtaining c, we continue to use \\(< n,t,k,c >\\) as a query to retrieve few-shot examples and follow the INIT-q prompts in Appendix Table 16 to generate the question q. The exact process is applied for t and k. It is important to note that we trim the retrieved examples for each component; for instance, in INIT-c prompts, we only retain the context component of each example, and similarly, for the other three components q, a, d, only the relevant parts are kept. As demonstrated in Appendix Table 15, this INIT step already results in a USMLE MCQ for a given input \\(< n,t,k >\\). However, as discussed in Figure 1, despite incorporating several advanced prompting engineering methods in the INIT step\u2014including prompts designed by medical experts according to USMLE guidelines, tailored topics&test points for each input medical case, as well as step-by-step retrieval and generation\u2014the LLM-generated USMLE MCQs in the INIT step still fall short of the required quality and difficulty."}, {"title": "Question Answering Feedback Collection", "content": "Inspired by recent work that augments the standard QG model with an additional QA module to further constrain the generated questions , we add a Question Answering Feedback Collection module. This provides additional feedback from the question-answering perspective to further challenge the quality and difficulty of the questions. Our motivation stems from the fact that LLMs like GPT-4 have proven to perform exceptionally well on USMLE QA tasks, achieving human-expert levels in both accuracy and reasoning processes. By analyzing the rationale and correctness of the final answers produced by the language models during the QA process, we can gather valuable insights into the quality and difficulty of the questions. Specifically, given the context c, question q, and options composed of aud (with their order shuffled), we collect the LLM's generated attempt \\(a_a\\) along with the reasoning r that supports \\(a_a\\) in this step. An example output is shown in Appendix Table 15."}, {"title": "Critique", "content": "After generating all components in INIT and QA step, the LLM is asked to critique each component. The set to be critiqued is \\(U_{critique} = (c, q, a, d, r)\\). The LLM receives a scoring guide G, which includes all aspects that need to be evaluated for each component. The prompt includes this guide G as well as several manually written example critiques of scored components \\(E_{critique} = (e^c_s, e^q_s, e^a_s, e^f_s)\\) and \\((G,n,t, k, c, q, a, d, a_a)\\). The final output of this step is LLM critique feedback on all components, \\(f = (f_c, f_q, f_a, f_d, f_r)\\), which includes short text critiques and scores for each aspect. The aspects for scoring different components in G are as follows: Context: Relevant, Concise, Coherent, Consistent, Specific, Fluent, Clueing, Completeness, and Misdirection. Question: Relevant, Clear, Concluding, Difficulty, and Clarity. Correct Answer: Relevant, Occurrence, Justification, Depth of Understanding, and Prevention of Guesswork. Distractors: Format, Length, Relation, Variation, Plausibility, Differentiation, and Common Mistakes. Chain of Thought/Reasoning: Logical Flow, Evidence-Based Reasoning, and Consideration of Options. We provide detailed explanations for each aspect of every component in Appendix Table 15, and LLM-Critique prompts in Table 16. The total score for each component is calculated by summing up all individual aspect scores. A sample output is provided in Appendix Table 15."}, {"title": "Correction", "content": "The Correction step aims to correct each of the generated components of \\(u\\) in the INIT step. LLM is prompted with \\(<E_{correction},n,t, k, f, c, q, a, d, a_a, r>\\) and asked to generate \\(U_{correction}\\), which can perform better on all the component's critique aspects. Here \\(E_{correction}\\) is a set of manually written few shot examples which are incrementally improving using the previous output's feedback. \\(U_{correction}\\) is again given to the Critique step to check if the feedback scores are greater than a fixed threshold, which, if true, stops the iterative Critique and Correction and, if not, continues."}, {"title": "Experimental Design and Setup", "content": "Experimental Setup For all our experiments with MCQG-SRefine, we use the chat completions API from OpenAI and the gpt-4-0125-preview, which has a context window of 8192 tokens, and the values of the hyperparameters temperature and top-p are set to 1. Similarly, for all other models used for the LLM-as-Judge comparison feedback generation, we used their default hyperparameters.\nDataset For the medical cases utilized in the generation of USMLE questions, we used unidentified patient summaries from the PMC-Patients dataset. The average length of these patient summaries was ~419 words. The frequency of topics used is listed in Appendix Table 13 14."}, {"title": "Experimental Design", "content": "Our experimental design is motivated to answer the following research questions: Evaluate whether MCQG-SRefine improves both the quality and difficulty of the generated questions. Specifically, we employed the MCQG-SRefine pipeline to generate USMLE questions and compared them with baseline questions generated by GPT-4 under identical inputs and settings. Our inputs consisted of medical cases n from the PMC-Patients dataset, with topics t and test points k that were either human-annotated or generated by the LLM. We generated 373 questions from the human-annotated (n, t, k) set and 385 questions using the LLM-generated set.\nTo assess the quality of the questions (RQ1), we first engaged two medical experts to express their preferences between the two sets (GPT-4 and MCQG-SRefine) of system-generated questions based on an annotation guideline (Appendix Table 17). The evaluators were blinded to the source of the questions, and the order of the questions was randomized for each data point. We calculated the Percentage Agreement (87.5%) and Cohen's kappa (0.66722) for the two evaluators' preferences, indicating substantial reliability of our human evaluation settings. Subsequently, a third human expert facilitated discussions with the initial evaluators to make the final decision for each data point, representing the final human expert preference (referred to as Expert X).\nFor evaluating the difficulty level of the questions (RQ2), the human evaluators were also asked to classify the difficulty of both questions into one of three categories: Easy, Medium, and Difficult. Specifically, we randomly selected 50 real-world USMLE-style questions from the AMBOSS dataset (10 for each difficulty level) as examples for the experts to reference. AMBOSS categorizes question difficulty from 1 to 5, where 1 is the easiest and 5 is the most difficult. We grouped levels 1 and 2 as Easy examples, levels 3 and 4 as Medium examples, and level 5 as Hard examples.\nLLM-as-Judge for evaluation metrics In addition to human evaluation, recent work has demonstrated that LLM-as-Judge (particularly GPT-4-based) has a high correlation with human assessment in reference-free settings. In this work, finding reliable automatic evaluation metrics in a reference-free setting is crucial, as it can reduce the burden of expert evaluation and help improve LLMs in future work (e.g., as a reward model). To achieve this, we explored two common LLM-as-Judge modes: rating and comparison. Regarding the evaluation criteria for each part of the MCQ, we found that directly using the criteria from the critique section of MCQG-SRefine did not correlate well with the human evaluation results of Expert X. Therefore, we conducted a detailed correlation analysis between each criterion's score and human evaluation, ranking them accordingly. Based on this analysis, we applied different filtering methods to identify the most relevant combination of criteria. Finally, we selected the following ten aspects: Context (concision, relevance, misdirection), Question (concluding, clarity), Correct Answer (occurrence, depth of understanding), Distractor (common mistakes), and Reasoning (logical flow, evidence-based reasoning). Specifically, GPT4-as-judge provides two evaluation indicators: 1. Detailed ratings and reasons for the above five sections and ten aspects; 2. Preference between MCQA-SRefine and GPT-4 generated questions."}, {"title": "Results", "content": "Main results Figure 3 demonstrates the overwhelming advantage of MCQG-SRefine over GPT-4 with a 70-80% win rate in human preference about question quality (RQ1). In Figure 4, we also observe a decrease of about 80% in easy questions, a 2.25 times increase in medium questions, and a 4 times increase in hard questions with the input of these medical cases with expert-provided topics and key points. For machine-provided topics and key point cases, the proportion of easy questions decreased by 33.3%, a 2 times increase in medium questions, but there was no increase in the proportion of hard questions (RQ2). This demonstrates the effectiveness of MCQG-SRefine in increasing the difficulty of questions. This also indicates that the quality of topics and key points provided by experts is higher, so LLM can generate more difficult questions by thinking more deeply during the critique and correction steps. Further improving the quality of the machine-provided topics and key points can be a future direction for improvement.\nFigure 5 and Table 1 present the results of the LLM-as-judge evaluation. For human-provided topics and key point cases, MCQG-SRefine achieved a win rate of 79.8% compared to 20.2% for GPT-4. Similarly, for machine-provided topics and key point cases, MCQG-SRefine achieved a win rate of 80.1%, outperforming GPT-4's 19.9%. Notably, MCQG-SRefine consistently outperformed GPT-4 across all five evaluated components with LLM-as-judge (Rating) results rather than demonstrating an advantage in only a single aspect. This indicates a balanced and comprehensive improvement of MCQG-SRefine.\nQualitative analysis One of the main issues with the questions generated by GPT-4 is that they often directly include the correct answer or too obvious relevant keywords within the context component (Appendix Table 19 Case Studies 1 and 3). So the questions directly generated by GPT-4 often make the answers obvious, but MCQG-SRefine can modify this information into hints for the correct answer through its critical and corrective steps, which experts consider a better way to construct questions for candidates (Case Study 4).\nAnother finding is that the questions generated by MCQG-SRefine are more concise compared to those by GPT-4 (Case Study 1). Experts pointed out that this conciseness makes them more similar to real USMLE questions. Our experiments show that GPT-4 adopted a very conservative strategy when generating context due to our emphasis on hallucination issues in the prompts. This strategy involves copying and pasting much information from raw medical case inputs to avoid generating potentially incorrect new facts. Although this does reduce the occurrence of hallucinations  it inevitably sacrifices the typical simplicity and highly refined information presentation of USMLE problems, as well as the logical coherence of multi-hop reasoning between information in context, question, correct answer, and distractors. This is a significant stylistic difference between USMLE questions and the original medical cases. We also found that prompt engineering was ineffective in resolving this issue. We interpret this as a shortcut behavior learned by GPT-4 during aligning with human preferences stage training (e.g., RLHF to reduce output diversity to mitigate hallucinations. In contrast, MCQG-SRefine maintains a high level of factual accuracy (with 5% factual errors) and further improves the quality of USMLE questions by iteratively criticizing and correcting each component of the question, as well as shifting the perspective from QG to QA to make it closer to actual exam questions. This is another major reason why MCQG-SRefine outperforms GPT-4 in human evaluation. However, when MCQG-SRefine reduced the amount of useless information provided in the questions to increase quality and difficulty, omitting too much information sometimes made inferring the correct answer more challenging (Case Study 2), although this was a rare occurrence in our human evaluation (7.5%)."}, {"title": "Discussion", "content": "Round-wise analysis Since MCQG-SRefine operates as an iterative system, an interesting question is whether GPT-4 can consistently provide meaningful critiques and corrections for itself in such a specialized and complex setting. To explore this, we conducted round-wise analyses, with key findings presented in Figures 6, while additional analyses are discussed in the appendix. Figure 6 shows the best-scoring rounds for human- and machine-generated topic + key points. For human-generated topics and key points, 30% of the best scores came from the first round of output, while the remaining 70% were evenly distributed across rounds 2, 3, and 4. This suggests that the LLM cannot consistently ensure that its critique and corrections will continuously improve the quality of the generated questions. However, based on the main results, selecting the best round after multiple iterations generally leads to a much higher quality of final questions compared to the initial output. We observed similar results for machine-generated topics and key points, but the proportion of best scores from the first round was lower (24.9%). This is consistent with the main results, where topics and key points provided by humans were clearer, making it more likely for the LLM to generate high-quality questions in the first round, while those provided by machines required more improvement.\nImproving LLM-as-judge reliability We found that directly using the critique criteria from Section 2.4 for LLM-as-Judge only resulted in slight reliability when correlated with Expert X. We explored two heuristic algorithms to improve the effectiveness of LLM-as-Judge through aspect filtering . Specifically, we calculated the correlation of each aspect's score from the collected rating feedback with Expert X, then sorted these aspects in descending order based on percentage agreement or Cohen's kappa. In the Greedy , we added aspects sequentially to the final rating score calculation based on their correlation, from highest to lowest, and recalculated the correlation between LLM-as-Judge (rating) and Expert X. Similarly, in the All-Combo, we calculated the final rating score for all possible combinations of the top n aspects selecting the combination with the highest correlation to Expert Xas the output of the All-Combo algorithm. As shown in Appendix Table 24 and Table 25, the All-Combo method identified the optimal aspect combination. In Table 2, we observed that the percentage agreement and Cohen's kappa of LLM-as-Judge significantly improved ."}, {"title": "Related Work", "content": "LLMs for Generating Medical MCQs\nconducted the first comprehensive study comparing LLMs with human experts for generating medical exam MCQs. Using the ChatGPT-3.5 interface, they generated MCQS from two standard medical textbooks. Each question included four answer options, without any post-generation modifications. ChatGPT took only 21 minutes to generate 50 MCQs, roughly 10\nperformed a blind evaluation of MCQs generated by ChatGPT-4. Initially, ChatGPT-4 generated questions with four answer options using examples from previous exams (few-shot learning). These questions were brief and lacked clinical context, necessitating an additional prompt to include clinical background. After refinement, most of the MCQs were considered effective.\ncompared ChatGPT-3.5, Bard, and Bing in generating MCQs for an 11-module physiology course by the National Medical Commission (NMC) of India. The results indicated that ChatGPT generated more effective MCQs, although they were less challenging compared to those generated by Bard and Bing.\nfocused on dermatology board exams by uploading Continuing Medical Education (CME) articles from the Journal of the American Academy of Dermatology (JAAD) into ChatGPT-3.5. The generated MCQs were evaluated based on accuracy, complexity, and clarity, with only 40\ntasked ChatGPT-3.5 with generating three neurosurgery board exam questions, including answers and explanations. Similarly, used ChatGPT-3.5 to generate three MCQs incorporating clinical context and lab values, experimenting with rephrasing to adjust answers and increase difficulty.\nemployed ChatGPT-4 to generate MCQs on anatomy, focusing on increasing difficulty levels and creating appropriate answer pairings. used ChatGPT to prepare USMLE Step 1 MCQs, though no independent evaluations were conducted for these studies.\nTo our knowledge, this study is the first to comprehensively explore both the qualitative and quantitative aspects of converting real-world medical cases into USMLE-style MCQs. We leverage authentic medical case information to generate high-quality, up-to-date questions that align with current medical standards and best practices."}, {"title": "Self-Refine using LLM Feedback", "content": "Learning from feedback helps align LLMs with desired outcomes, enhancing their ability to follow instructions through different forms of feedback, such as human preference feedback , AI-generated preference feedback , or fine-grained feedback .\nUnlike preference and fine-grained feedback, which provide scalar values as training signals, natural language or correction feedback offers richer information , making it particularly effective for self-correcting language models. Recent research has demonstrated that LLMs can self-correct their responses to meet various user requirements, such as reducing harmful content, incorporating specific keywords, or debugging code . This self-correction process generally involves generating a critique that identifies shortcomings in the initial response, followed by revising it based on the self-critique\u2014an iterative process that has shown promise in enhancing LLM output quality.\nInspired by the success of these iterative self-refinement methods, we are the first to explore using this approach for generating USMLE-style MCQs. By leveraging self-feedback, we aim to create high-quality, clinically relevant questions that adhere to the rigorous standards of medical education."}, {"title": "LLM-as-Judge using LLM feedback", "content": "The critique capabilities of LLMs have been extensively used for the automatic evaluation of response quality, often employing models like GPT-4 or critique-adjusted LLMs . Despite their success, these methods have demonstrated instability in certain complex task scenarios.\nLLMs have shown a high correlation with human evaluations in tasks such as summarization and story generation, effectively scoring candidate texts or comparing them based on specific evaluation aspects . For example, studies"}, {"title": "Conclusion", "content": "In conclusion, MCQG-SRefine improves LLM ability to generate high-quality USMLE-style MCQs by integrating expert-driven prompts with iterative self-refinement. The framework significantly enhances question quality and difficulty while aligning closely with expert evaluations. Additionally, our LLM-as-Judge metric offers a scalable alternative to costly expert assessments."}, {"title": "Limitations and Ethical Considerations", "content": "Our research focuses on generating medical exam questions, with promising results, though its adaptability to other domains is yet to be explored. We evaluated the method with three medical experts, but future involvement of specialized USMLE exam experts could enhance credibility. Future research should address privacy, fairness, generalizability to other languages/domains, and potential biases. While our work marks an important step in medical exam question generation, broader societal impacts require careful consideration. (Semi)-Automated USMLE-MCQG systems can boost the efficiency and accuracy of medical education, but over-reliance could reduce direct student-educator interactions, and LLMs' hallucinations might impact students' learning and real-world practice. Caution is necessary to ensure these technologies support, not replace, medical educators. Future research should focus on these challenges to maximize positive contributions to medical education and society."}, {"title": "Ablation study and output analysis of MCQG-SRefine", "content": "Round-wise metrics\nAs shown in Table 4, the round-wise metrics for human data reveal nuanced trends in the model's performance through multiple feedback iterations. The total score exhibited a modest increase from Round 1 (0.9031) to Round 3 (0.9062), followed by a slight decline in Round 4 (0.9006), suggesting that while the self-feedback process contributes positively, its benefits may diminish with excessive iterations. The context score notably improved from 0.9683 in Round 1 to 0.9712 in Round 3, reflecting enhanced model comprehension of the context through feedback. However, performance in other areas fluctuated: the question score showed a slight decline across rounds, and while the correct answer score improved from Round 1 to Round 3, it decreased slightly in Round 4. Conversely, the distractor option score showed steady improvement, culminating in the highest score by Round 4. The reasoning score, however, demonstrated a gradual decline over the rounds. The standard deviation for most metrics either decreased or remained stable, indicating more consistent performance. Overall, while certain components of the model benefited from the feedback process, others did not, highlighting the complexity of balancing improvements across different aspects of question generation.\nAs shown in Table 5, the round-wise metrics for machine data demonstrate a stable overall performance, with the total score showing minimal fluctuation across rounds, peaking slightly in Round 2 (0.907) and ending at 0.903 in Round 4. This suggests that while the feedback process maintains performance, it does not lead to significant improvements. The context score exhibits a notable increase from 0.9688 in Round 1 to 0.974 in Round 4, coupled with a decrease in standard deviation, indicating enhanced and more consistent context understanding. The C. Answer score also shows a gradual upward trend, improving from 0.795 in Round 1 to 0.810 in Round 4, reflecting slow but steady progress in answer generation. The distractor option score remains relatively high and stable across rounds, while the reasoning score experiences a decline from 0.967 in Round 1 to 0.948 in Round 3, with a slight recovery to 0.951 in Round 4. Variability in performance is evident, with some metrics, such as the context score, showing decreased standard deviation, indicating more consistency, while others, like the reasoning score, exhibit increased variability, highlighting differential effects on the consistency of various components.\nBased on the analysis of the round-wise metrics from both human and machine data, the self-feedback process demonstrates the most significant improvements in context understanding and answer generation. These enhancements are evidenced by the consistent upward trends in context and correct answer scores across rounds. However, the data also suggests a potential trade-off between reasoning ability and other metrics, as seen in the decline of reasoning scores, particularly after the initial rounds. The plateau or slight decline in overall performance after 2-3 rounds indicates that the benefits of the feedback process diminish with excessive iterations, implying that a limited number of rounds may be optimal for maximizing improvements without compromising other aspects of question generation. These findings highlight the importance of balancing the feedback process to achieve comprehensive improvements across all key metrics.\nIn addition, the analysis of the round-wise metrics in Table 6 and 7 reveals several key trends in the performance of the question generation pipeline. As the rounds progress, the context length consistently decreases, indicating that the pipeline effectively refines the context by excluding extraneous information, leading to more precise and focused questions. The accuracy of the QA component improves over the rounds, suggesting that the iterative process enhances the overall quality of the questions, making them more answerable by the LLM. However, this improvement reaches a point of diminishing returns in the later rounds, implying that a limited number of iterations may be optimal. On the other hand, the equality between the correct answer and the keypoint deteriorates over the rounds, indicating that the pipeline makes the correct answer more subtly related to the keypoint rather than directly copying it. This shift suggests that while the pipeline reduces redundancy, it may also introduce complexity that could impact the clarity and directness of the correct answer-keypoint relationship."}, {"title": "Other Basic statistics", "content": "We also calculate Refinement rounds for human-annotated and model-generated topics & key points in Figure 7; Question length in no. of words (human annotated topic+keypoint) in Figure 8; Ques-"}, {"title": "Correlation analysis", "content": "Our analysis in Figure 10 11 and Table 8 reveals a strong inverse relationship between the length of clinical notes/questions and the number of rounds required to achieve the best score in both human-generated and machine-generated data. The perfect negative Spearman and Kendall correlations (-1.0) indicate that as the length of inputs increases, fewer rounds are consistently needed to reach optimal performance. Human-generated content shows slightly stronger linear correlations compared to machine-generated data. These findings suggest that longer, more detailed inputs provide richer information, allowing the model to converge on optimal performance more quickly. Practically, this implies that shorter inputs may benefit from more iterative rounds, while longer inputs may require fewer rounds to achieve the best results. The consistency of this pattern across both clinical note length and question length underscores the importance of input complexity in determining the efficiency of the iterative improvement process."}]}