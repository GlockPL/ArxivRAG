{"title": "Benchmarking Prompt Engineering Techniques for Secure Code Generation with GPT Models", "authors": ["Marc Bruni", "Fabio Gabrielli", "Mohammad Ghafari", "Martin Kropp"], "abstract": "Prompt engineering reduces reasoning mistakes in Large Language Models (LLMs). However, its effectiveness in mitigating vulnerabilities in LLM-generated code remains underexplored. To address this gap, we implemented a benchmark to automatically assess the impact of various prompt engineering strategies on code security. Our benchmark leverages two peer-reviewed prompt datasets and employs static scanners to evaluate code security at scale. We tested multiple prompt engineering techniques on GPT-3.5-turbo, GPT-4o, and GPT-4o-mini. Our results show that for GPT-4o and GPT-4o-mini, a security-focused prompt prefix can reduce the occurrence of security vulnerabilities by up to 56%. Additionally, all tested models demonstrated the ability to detect and repair between 41.9% and 68.7% of vulnerabilities in previously generated code when using iterative prompting techniques. Finally, we introduce a \"prompt agent\" that demonstrates how the most effective techniques can be applied in real-world development workflows.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) are increasingly used in software development, with professional developers relying more and more on them for code generation. While tools like ChatGPT and GitHub Copilot promise productivity gains for programmers, the security implications of using LLMs for code generation are concerning.\nNot only are programmers with access to AI assistants more likely to submit insecure code, but they are also more likely to rate the insecure programs as secure [1].\nAddressing security concerns in AI code generation is imperative to ensure that the growing reliance on these tools does not inadvertently weaken software security.\nPrompt engineering can guide LLMs toward producing desired outcomes without modifying the underlying model parameters. This practice has been shown to reduce reasoning mistakes and improve performance in various areas [2]. It provides a lightweight and scalable method for influencing LLM behavior, making it an attractive approach for many applications. However, the potential of prompt engineering to improve the security of generated code, and which specific techniques are most effective in this context, remain under-studied.\nWhile some preliminary indications suggest that prompt variations can influence the security of generated code (discussed in subsection II-C), existing research is constrained by narrow task selection, the use of outdated models, limited sample sizes, or the exclusion of typical randomness by setting the temperature parameter to 0. These limitations prevent a comprehensive understanding of how different prompt engineering techniques impact the security of code generated by state-of-the-art LLMs.\nTo address this gap, we investigate the following research question: Can we enhance GPT's secure code generation via prompt engineering?\nWe designed a benchmark to evaluate the impact of prompt engineering on the security of code generated by GPT models. Using two peer-reviewed datasets, LLMSecEval [3] and SecurityEval [4], we tested various modifications on coding prompts. We focused on Python due to its wide adoption in numerous areas. For each prompt, we generated multiple code samples and scanned them for vulnerabilities using static analyzers (Semgrep [5] and CodeQL [6]). Our benchmark allows us to analyze the relative security improvements provided by each technique for different models. Finally, we present a prompt agent demonstrating the implementation of our findings to enhance LLM-based code security.\nTo facilitate future investigations on this topic, we make our benchmark tool, the experimental data, and our prompt agent publicly available on GitHub.\u00b9 In summary, we contribute the following:\n1) Our extensive benchmark on guiding GPT models toward secure code generation found that a simple prompt prefix can significantly reduce the risk of vulnerabilities. We also found that applying the \"Recursive Criticism and Improvement (RCI)\" technique on generated code can fix a significant number of vulnerabilities.\n2) We provide an open-source benchmarking tool to compare the effectiveness of different prompt modifications in improving the security of GPT-generated code\n3) We also present an open-source prompt agent, helping to reduce vulnerabilities in code generation\nIn the remainder of this paper, we review related work and highlight current research gaps in section II. We describe our research methodology in section III, present our experimental results in section IV, and discuss them in section V. We introduce our prompt agent in section VI. We outline the threats to the validity of this study in section VII, and we conclude this paper in section VIII."}, {"title": "II. RELATED WORK", "content": "A range of studies have investigated the security of LLM-generated code. Fu et al. [7] found that 32.8% of Python and 24.5% of JavaScript code snippets found in GitHub projects that were generated by GitHub Copilot, and marked as such, contained security issues. Pearce et al. [8] constructed a set of 89 \"CWE Scenarios\" and found that around 40% of the suggested completions by GitHub Copilot contained vulnerabilities. Khoury et al. [9] found that GPT-3.5 generated initially secure programs in only 5 out of their 21 use-cases.\nThese findings highlight the importance of addressing security concerns, as they demonstrate a significant prevalence of vulnerabilities in code generated by LLMs. Our work aims to offer a proactive solution to reducing these risks.\nPearce et al. [8] published their dataset of 89 CWE-based code-completion scenarios, covering 18 out of the top 25 CWEs from 2021. The authors used examples from the CodeQL repository and MITRE [10] as sources and handcrafted some of the code completion tasks.\nTony et al. [3] translated the completion scenarios from [8] into 150 natural language prompts. They introduced the idea of using their LLMSecEval dataset in combination with the CodeQL scanner to evaluate the security of LLM code generation.\nSiddiq and Santos published SecurityEval [4]. Their dataset consists of 130 prompts for 75 vulnerability types which are mapped to the corresponding CWE. The prompts are code completion tasks with imports, a function header, and a natural language comment for the desired functionality. The sources are CodeQL examples, CWE examples, Sonar examples, and the scenarios from [8].\nIn Meta's PurpleLLama CyberSecEval Study [11] they used their Insecure Code Detector (ICD), which is based on weggli [12] and Semgrep [5] rules, to find insecure coding practices in open source repositories. They then took the 10 lines preceding the issues to create code completion tasks. Additionally, they translated the completion tasks into natural language instructions using an LLM.\nThese resources enable an efficient evaluation of the security aspect of code generation by focusing on relevant scenarios. We leverage this for the efficient comparison of different prompting techniques.\nOnly few studies investigated the impact of prompt modifications on code security. Pearce et al. [8] tested prompt diversity for only one of their 89 scenarios and found that \"small changes in Copilot's prompt (...) can impact the safety of the generated code\". They hypothesize that \u201cthe presence of either vulnerable or non-vulnerable SQL in a codebase ...\nhas the strongest impact upon whether or not Copilot will itself generate SQL code vulnerable to injection\". If this is true for languages beyond SQL, minimizing vulnerability risks in initial prompts not only reduces the risk for the current output but also creates a compounding effect by lowering the likelihood of insecure code in subsequent generations, thereby significantly increasing the security of the entire codebase.\nFirouzi and Ghafari [13] prompted GPT-3.5 to answer 100 encryption-related Stack Overflow questions, finding that only three responses were free of security violations. When the prompt was modified to explicitly request a \u201csecure\" solution, the number of secure responses increased to 42. In a more recent study [14], they compared ChatGPT's performance in detecting cryptographic misuses with that of state-of-the-art static analysis tools. Their findings indicate that, with appropriate prompts, ChatGPT outperforms leading static cryptography misuse detectors.\nTony et al. [15] published a set of prompt templates for secure code generation based on a systematic literature review. Their test of the templates using the LLMSecEval dataset [3] was limited by setting the temperature parameter to 0 and collecting only a single sample per prompt.\nThese studies offer positive initial insights into the potential of prompt engineering for secure code generation. We build on these preliminary studies by conducting a more in-depth investigation, incorporating realistic temperature settings, multiple samples, and state-of-the-art models to better understand the effects of prompt engineering.\nSome LLM-based tools rely on static scanners to detect vulnerabilities in the generated code. Meta's PurpleLLama CodeShield [16] is using ICD introduced in [11] to flag insecure code snippets and suggest actions (block or warn). Kavian et al. introduced LLMSecGuard [17], a framework that leverages static security analyzers to identify potential vulnerabilities in LLM-generated code and guide LLMs in fixing them. However, they did not investigate the effectiveness of the proposed framework.\nIn contrast to methods that rely on external tools for post-generation vulnerability detection, our approach integrates both proactive prevention during code generation and the use of the LLM's capabilities to identify and mitigate remaining issues after generation.\""}, {"title": "III. METHODOLOGY", "content": "To evaluate the effectiveness of prompt engineering for secure code generation, we designed a benchmark system that applies various prompt augmentations to high-risk coding prompts. Our goal is to explore how these augmentations can reduce security vulnerabilities in code generated by large language models (LLMs). By systematically augmenting prompts and generating multiple code samples for each variant, our"}, {"title": "A. High-Risk Coding Prompts", "content": "The basis for our evaluation is a combination of the LLMSecEval [3] and SecurityEval [4] datasets. The focus on prompts that are likely to create vulnerabilities increases the cost-efficiency of our benchmark. From LLMSecEval we took only the Python prompts. For the SecurityEval prompts, we added the prefix \"Complete the following code, and output the complete program:\". Both datasets provide the CWE that is at high risk of being introduced when completing the prompt. We additionally consider the \u201cRecommended Mapping\" and \"Can Also Be\" CWEs from MITRE, since security scanners might map the vulnerabilities to these CWEs instead. Some prompts frequently did not produce scannable code. After removing these cases, our dataset consists of 202 prompts.\nWe have selected LLMSecEval and SecurityEval as sources because their prompts return complete programs in most cases, which increases the accuracy of the security scan. Alternative datasets only produce partial programs. The PurpleLLama [11] dataset, for example, limits the context size to 10 lines. The resulting code is therefore often not syntactically correct, which impairs the functionality of the static scanners.\nThe focus on Python code facilitates the validation of the syntax in the code extraction step."}, {"title": "B. Prompt Augmentation", "content": "We collected various prompt engineering ideas from existing papers, came up with our own ideas, and prompted ChatGPT for suggestions. The tested attempts are listed in Table I. Each attempt is assigned an ID so that it can be uniquely identified.\nFor each attempt, we created a script that applies the technique to a copy of the original prompts. Most of our attempts simply add a prefix and/or suffix to the original prompt, with two exceptions: RCI and CoT.\nThe Recursive Criticism and Improvement (RCI) template takes previously generated code as the input and sends a first prompt asking for a security review, followed by a second prompt asking to improve the code based on the review received from the first prompt. The previously generated code can come from the baseline (as in attempt rci-from-baseline-iter-1) or other attempts (as in attempt rci-from-pe-03-a-iter-1). The RCI technique can be applied iteratively by taking the code from previous executions as the input, like in rci-from-baseline-iter-2 and rci-from-baseline-iter-3.\nThe Chain of Thought (CoT) template consists of two prompts. The first prompt adds a suffix to the original prompt that asks for a step-by-step thought process. The second prompt adds the steps received from the first prompt and asks for the Python code.\nThree attempts rely on information about the prompt dataset, limiting their practical applicability. The attempt pe-02-d is adding a list of weakness descriptions, based on the suspected CWE. Attempt pe-02-e includes a list of the CWE found by Semgrep in the baseline samples. The pe-negative attempt asks the model to make an example of the suspected CWE. The result is limited by both the model's capability to create the CWE on purpose, and the ability of Semgrep and Codeql to detect the vulnerability. This helps to approximate the detection boundaries and contextualize the metrics of the other attempts.\nAll other attempts are independent of any prior task-specific information and can be applied universally."}, {"title": "C. Response Generation", "content": "The responses to the modified prompts are generated using the OpenAI Chat API with its default settings. At the time of our experiments, the default temperature was 1.\nWe generated 10 samples for each modified prompt, effectively simulating 10 developers sending the same prompt to ChatGPT. For the baseline, we additionally created a version with 100 samples for GPT-3.5 and GPT-4o-mini to analyze the variance.\nOur tool allows specifying the OpenAI model snapshot used for Response Generation and Code Extraction. We chose OpenAI models due to their widespread adoption, including in GitHub Copilot Chat [18]. This setup enabled us to benchmark against models commonly used by developers, ensuring the relevance of our findings to real-world coding environments."}, {"title": "D. Code Extraction", "content": "Security scanners require syntactically correct code, but ChatGPT's responses often intermix code with natural language text, making direct use impractical. Adding an instruction to generate code-only outputs would alter the responses and could invalidate our results. To preserve real-world usage, we maintain the original prompts and perform code extraction separately.\nThe code extraction process is depicted in Figure 2 and begins by finding markdown code blocks (delimited by ```) within the response using a regular expression. If no markdown code block is found, the response may consist entirely of code. If one code block is found, we take its contents. To confirm if we received valid code, we invoke ast.parse to build the Abstract Syntax Tree (AST). If there are no errors and the resulting AST height is above two, then the code is considered valid and will be evaluated in the subsequent code security scan.\nIf the validation criteria are not met, or if the initial response contains multiple code blocks, we append a follow-up prompt: \"Only output the python code and nothing else, so that when I copy your answer into a file, it will be a valid python file\". Since the modified prompt and the initial response remain part of the context, they still influence the quality of the resulting code.\nThis follow-up prompt generates a new response, which is then evaluated using the same steps as before. If it does not contain valid code, it is re-tried unless the limit of two re-tries is already reached.\nIf two code extraction attempts fail, an error is reported, and the original prompt is re-generated from scratch by returning"}, {"title": "E. Code Security Scan", "content": "To assess the security of each response, we write the extracted code into a file and scan it with two static scanners, Semgrep and CodeQL.\nIn some cases, the scanners report syntax issues, even though ast.parse was successful. If this occurs, the code extraction with the follow-up prompt is re-tried. If the issue persists, a new sample is generated using the Response Generation step. If the issue persists after three re-tries, the error is reported and needs manual attention. Instead of rewarding invalid syntax by regarding it as secure code, it is discarded.\nUsing static scanners to evaluate code security means inheriting their limitations. However, due to the large amount of code to inspect, manual evaluation was not possible. The authors of [4] concluded: \u201c. . . although an automated strategy decreases the time and effort in evaluating tools, they may not find all insecure code instances. However, an automated strat-"}, {"title": "F. Metrics Calculation", "content": "Based on the results of Semgrep and CodeQL for each sample, various metrics are calculated. First, a copy of the scan results is filtered only to include results for the suspected CWE of the prompt. Then, various values are calculated for both the raw and the CWE-filtered scan results. Notably, these values include whether the scanners agreed on the suspected CWE and the average number of vulnerabilities identified by the combined use of both scanners."}, {"title": "IV. RESULTS", "content": "We used three different LLMs in our experiments: GPT-3.5-turbo, GPT-4o-mini, and GPT-4o. The specific model snapshots used were gpt-3.5-turbo-0125, gpt-4o-mini-2024-07-18, and gpt-4o-2024-08-06. We applied our benchmark framework and gathered extensive data from which we derived several metrics for each prompt engineering attempt.\nWe have $n = 202$ prompts, each formulating one specific task. Each prompt execution results in one code sample.\nWe define a binary indicator $vuln\\_by\\_both(sample_i)$ which equals 1 if both scanners agree that $sample_i$ contains the suspected CWE vulnerability, and 0 otherwise.\nThe Scanners Agree Filtered Vulnerable Samples metric indicates the percentage of samples across all n tasks that contain the suspected CWE:\n$SAFVS = \\frac{1}{n} \\sum_{i=1}^{n} vuln\\_by\\_both(sample_i) \\times 100$\nWe execute each prompt k times. Hence, we get a set of Observed Filtered Vulnerability Percentages OFVP with size k for each prompt engineering attempt:\n$OFVP = \\{SAFVS_1, ..., SAFVS_k\\}$\nWe examine OFVP to understand the variability for identical prompts and assess the impact of randomness on both the model's baseline performance and the effectiveness of the prompting technique."}, {"title": "A. GPT-3.5-turbo", "content": "All of the attempts (Table I) were executed with GPT-3.5-turbo. The essential results are listed in the Table II, the box plot in Figure 3.\nFor this model, prompt engineering techniques attempting to reduce vulnerabilities in initial code generation were unsuccessful and even produced more vulnerabilities (up to 31.1%) compared to the baseline. Only the RCI technique, which aims to eliminate vulnerabilities in previous responses, leads to more secure code. The first iteration eliminated 24.5% of the issues present in the baseline, while additional iterations further reduced the number of vulnerable samples (by 8.9% and 14.1% of initial vulnerabilities respectively).\nThe approach \u201cpe-negative\u201d, which asks the model to create vulnerable code with the suspected CWE on purpose, resulted in 69.3% more vulnerable samples than the baseline."}, {"title": "B. GPT-4o-mini", "content": "For GPT-4o-mini, all prompt engineering techniques (Table I) lead to more secure code compared to the baseline of original prompts. The prompt-prefix technique \u201cpe-03-a\" was the most successful in reducing the risk of vulnerable code in the initial generation, by 47% on average. The augmented prompt consistently outperformed the best-case baseline run in all sample runs, as visible in the boxplot Figure 4. The average improvements are listed in Table III.\nThe COT technique, which requires an additional prompt, had comparable performance to the simple prefix.\nAfter initial generation, the RCI technique was employed to improve the security of the generated code. This was attempted for the baseline, where it led to a reduction of vulnerable samples by 49.5% with one iteration. A second iteration eliminated another 9.1% of vulnerabilities. The third iteration did not lead to further improvements in code security.\nTo leverage both pre-generation vulnerability prevention and post-generation fixes, the RCI approach was also tested in conjunction with the best-performing proactive approach (pe-03-a). This combination outperformed two RCI iterations"}, {"title": "C. GPT-4o", "content": "Only a subset of the attempts from Table I were executed with GPT-4o. This was mainly due to the higher costs of using this LLM. We decided to run the most interesting attempts. The essential results are listed in Table IV and visualized in Figure 5.\nAll of the tested prompt engineering attempts were successful in reducing the risk of vulnerabilities compared to the baseline. Among the single-prompt attempts, the prompt-prefix technique \"pe-03-a\" was the most effective with a 56% reduction.\nRCI reduced vulnerabilities in the baseline code snippets by 64.7% while employing RCI on the snippets from \"pe-03-a\" resulted in an additional improvement from 56% to 68.7% fewer vulnerabilities compared to the original baseline snippets.\nWhen prompting the model to create vulnerable code in the \"pe-negative\" approach, the \"Scanners Agree Filtered Vulnerable Samples\" metric was 152.7% above baseline."}, {"title": "V. DISCUSSION", "content": "We reflect on the performance of the approaches for each LLM, the importance of sample size, and code functionality."}, {"title": "A. Performance Comparison", "content": "To compare the results of the three LLMs with each other, we illustrated the \u201cScanners Agree Filtered Vulnerable Samples\" for each attempt and each LLM in a bar chart in Figure 6. This enables the comparison of each LLM's results for every attempt.\nThe results show an increasing sensitivity to prompt alterations for more advanced models. While generic prompt modifications had no positive impact on GPT-3.5-turbo regarding"}, {"title": "B. Sample Size", "content": "Figure 3, Figure 4 and Figure 5 also highlight the inherent variability in LLM-generated output, which necessitates generating multiple samples for robust evaluation. Code generation"}, {"title": "C. Functionality", "content": "To assess the impact of our systematic prompt augmentations on the functionality of the generated code, we employed the HumanEval benchmark [19]. We converted the prompts from the HumanEval set to our internal dataset format, which allowed us to use our benchmark tool, as described in section III, to make the prompt augmentations, generate the responses, and extract the code. To ensure that complete program code would be returned, we added the prefix \"Complete the following code, and output the complete program: \\n\" to all prompts, prior to the regular prompt augmentation step. The data file from our benchmark tool, containing the extracted code for all samples, was then converted to the data format expected by the HumanEval framework, and their evaluation scripts were executed to assess the performance of the generated solutions. Given that this evaluation was not the main focus of our study, we only ran the test for four attempts on GPT-4o-mini, with the usual sample size of 10."}, {"title": "VI. PROMPT AGENT", "content": "Our results show significant potential for prompt engineering in secure code generation. The developed \"Prompt Agent\u201d is an approach to integrating advanced prompt engineering techniques into automated code generation workflows. Its primary goal is to enhance the security of code generated by LLMs such as GPT-4o by systematically improving prompts and incorporating iterative refinement techniques. The prompt engineering techniques were selected based on their performance in our benchmark."}, {"title": "A. Concept", "content": "The agent takes a prompt in front end and enhances the prompt in the back end before sending it to the LLM for completion. After receiving the response, it can optionally apply post-processing using RCI before presenting the output to the user. Our results indicate that integrating an agent like this into LLM-based tools can significantly reduce the risk of generating vulnerable code."}, {"title": "B. Implementation", "content": "Our implementation is based on BetterChatGPT [20], a popular web front-end for ChatGPT. We have extended the app with two additional features."}, {"title": "C. Usability", "content": "Due to time constraints, we were not able to test and develop our features extensively. While using the agent for some tasks, we noticed that the Code Security Prefix did not noticeably impair the functionality of the chatbots (we used GPT-4o-mini and GPT-4o).\nThe Code Security Agent, on the other hand, often delayed the responses by more than 10 seconds, even for a single relatively short code snippet. The reason is that RCI sends additional requests to the LLM. Additionally, we observed RCI to be overly cautious, making the responses too verbose. This might be helpful for beginners but can be a deal breaker for advanced programmers."}, {"title": "VII. THREATS TO VALIDITY", "content": "Construct validity concerns whether the study measures what it claims to measure, specifically the efficacy of prompt techniques in enhancing LLM-generated code security."}, {"title": "A. Construct Validity", "content": "Relying on public datasets introduces the risk that models may have been exposed to them during training. If LLMs have encountered these datasets, their performance may reflect prior exposure rather than genuine security capabilities. For instance, GPT-3.5-turbo (trained up to 2021) would not have seen SecurityEval or LLMSecEval datasets but might include source materials such as MITRE CWE and CodeQL documentation. GPT-4o and GPT-4o-mini, trained post-2023, could potentially incorporate these datasets, impacting result interpretation.\nThe study depends on two static scanners to detect vulnerabilities. Discrepancies in scanner performance (e.g., false positives or false negatives) could lead to misinterpretations of security improvements. Additionally, generated code might bypass scanners without addressing underlying vulnerabilities, affecting result reliability.\nThe context in which vulnerabilities are detected may not align with real-world scenarios, as software architecture and operational environments are absent in isolated code snippets. Furthermore, textual warnings in LLM responses were ignored, assuming they might also be overlooked by developers in practice.\nPrompt modifications aimed at enhancing security could inadvertently impair code functionality. This trade-off was only briefly examined, and a deeper exploration is required to understand the balance between functionality and security."}, {"title": "B. Internal Validity", "content": "Internal validity concerns whether the observed outcomes can be attributed to the experimental conditions rather than external or confounding factors."}, {"title": "C. External Validity", "content": "External validity examines whether findings generalize to broader contexts, such as other LLMs, datasets, or domains."}, {"title": "VIII. CONCLUSION", "content": "We developed a benchmark to assess how prompt engineering affects code security across GPT models. Our findings show that vulnerability rates and sensitivity to prompt changes vary by model. For GPT-4o-mini and GPT-4o, adding the prefix \"You are a developer who is very security-aware and avoids weaknesses in the code.\" reduced vulnerable code generation by 47% and 56%, respectively. In contrast, GPT-3.5-turbo responded negatively to similar security-enhancing prompts. The Recursive Criticism and Improvement (RCI) technique effectively mitigates vulnerabilities across models, with a single iteration fixing 24.5% (GPT-3.5), 49.5% (GPT-4o-mini), and 64.7% (GPT-4o) of flawed snippets. These insights can help end-users and be integrated into LLM-based coding assistants, as demonstrated in our prompt agent concept."}]}