{"title": "Towards AI-45\u00b0 Law: A Roadmap to Trustworthy AGI", "authors": ["Chao Yang", "Chaochao Lu", "Yingchun Wang", "Bowen Zhou"], "abstract": "Ensuring Artificial General Intelligence (AGI) reliably avoids harmful behaviors is a critical challenge, especially for systems with high autonomy or in safety-critical domains. Despite various safety assurance proposals and extreme risk warnings, comprehensive guidelines balancing AI safety and capability remain lacking. In this position paper, we propose the AI-45\u00b0 Law as a guiding principle for a balanced roadmap toward trustworthy AGI, and introduce the Causal Ladder of Trustworthy AGI as a practical framework. This framework provides a systematic taxonomy and hierarchical structure for current AI capability and safety research, inspired by Judea Pearl's \"Ladder of Causation\u201d. The Causal Ladder comprises three core layers: the Approximate Alignment Layer, the Intervenable Layer, and the Reflectable Layer. These layers address the key challenges of safety and trustworthiness in AGI and contemporary AI systems. Building upon this framework, we define five levels of trustworthy AGI: perception, reasoning, decision-making, autonomy, and collaboration trustworthiness. These levels represent distinct yet progressive aspects of trustworthy AGI. Finally, we present a series of potential governance measures to support the development of trustworthy AGI.", "sections": [{"title": "1 Crippled AI: The Imbalance Between AI Capability and Safety", "content": "Artificial intelligence (AI) is experiencing a period of rapid advancement, driven by innovations in scaling laws [81], as well as breakthroughs in model architecture and computational resources [44]. These developments have led to AI systems like ChatGPT [13] and GPT-4 [64], which demonstrate remarkable abilities in understanding and generating human-like language [77]. These systems push the boundaries of AI's potential, approaching or surpassing human-level performance across various domains, including natural language processing [77] and creative problem-solving [79]. However, this rapid progress also presents significant risks [31], as the development and deployment of these advanced systems often outpace the implementation of safety measures. The rapid growth in AI capabilities, coupled with the slow evolution of corresponding safety protocols [61], highlights a critical imbalance that may hinder the responsible and reliable use of these technologies."}, {"title": "1.2 Limitations of Current AI Safety Measures", "content": "The current landscape of AI safety is predominantly characterized by a \u201creactive approach\u201d approach, where safety measures are implemented only after models are developed and vulnerabilities are identified [5]. These measures are often domain-specific, tailored to particular applications, and lack the flexibility to be applied across diverse AI systems. For example, adversarial defense techniques [48] designed to secure image recognition models are not easily adaptable to tasks such as speech recognition or natural language processing. Furthermore, the absence of a unified framework or standardized guidelines for AI safety exacerbates this issue, resulting in fragmented efforts that fail to comprehensively address the broad spectrum of potential risks. Various threats [14, 60], including adversarial attacks [73, 39], data poisoning [17], and model theft [99, 97], each require unique defensive strategies. Yet these strategies frequently operate in isolation, lacking the coordination and integration needed to tackle the increasingly complex and interconnected challenges posed by advanced AI systems [41, 40, 46, 18]."}, {"title": "1.3 The Crippled State of AI Development", "content": "The reactive and fragmented nature of AI safety practices has left the field in a state that can be described as \"crippled AI\u201d. While tools like red teaming [11, 70], watermarking [53, 47], and safety guardrails [63] provide some protection, their effectiveness is limited. Red teaming, for example, can uncover specific vulnerabilities through simulated attack scenarios, but it cannot address the full spectrum of potential threats. Similarly, watermarking techniques to identify AI-generated content can be tampered with or circumvented, rendering them unreliable at scale. Post-hoc safety measures such as guardrails [35, 55, 56, 27, 54, 51] often reduce system flexibility and usability, while evaluation frameworks lack the theoretical grounding needed for comprehensive assessments. These shortcomings reflect a broader issue: the rapid development of AI capabilities is outpacing efforts to ensure their safety, resulting in powerful but fragile systems. This imbalance risks undermining public trust, limiting the practical adoption of AI, and creating systems that could inadvertently cause harm or operate in ways misaligned with human values. To bridge this gap, a proactive, unified, and scalable approach to AI safety is essential to ensure that the promise of AI is realized without compromising ethical and security considerations."}, {"title": "2 AI-45\u00b0 Law", "content": "Recent advancements in AI have highlighted a significant gap between the rapid growth of AI capabilities and the slower progress in AI safety. In response to this imbalance, we introduce the AI-45\u00b0 law as a guiding principle for the development of AI systems. This law posits that advancements"}, {"title": "2.1 AI-45\u00b0 Law: Balancing Capability and Safety", "content": "Recent advancements in AI have highlighted a significant gap between the rapid growth of AI capabilities and the slower progress in AI safety. In response to this imbalance, we introduce the AI-45\u00b0 law as a guiding principle for the development of AI systems. This law posits that advancements"}, {"title": "2.2 Red Line: Existential Risks and Catastrophic Consequences", "content": "The unsafe development, deployment, or use of AI systems poses potential catastrophic risks [9], which could even become existential threats to humanity. As mentioned in the International Dialogues on AI Safety (IDAIS) [2], they propose \"Red Lines\" of AI development in the following five aspects: autonomous replication or improvement; power-seeking; assisting weapon development; cyberattacks; deception. At the same time, they also raise the need, including governance regimes and technical safety methods, to ensure these \u201cRed Lines\" are not crossed. Following the AI-45\u00b0 law, the \"Red Lines\" can be more clearly illustrated as occupying the lower right region relative to the 45\u00b0 line, as shown in Figure 1. These \u201cRed Lines\u201d - defined as those with the potential to lead to irreversible and catastrophic outcomes - are likely to increase as AI systems approach or surpass human-level intelligence."}, {"title": "2.3 Yellow Line: Early Warning Indicators and Proactive Mitigation", "content": "Considering the potential for extreme \u201cRed Line\" risks, it is critical to establish a framework for early-warning thresholds, called the \u201cYellow Line\u201d, as shown in Figure 1, which can signal when the capabilities of an AI system approach a dangerous level. These thresholds would serve as indicators that a system may be on the verge of crossing into the \u201cRed Line\" territory. To achieve this, it is essential to build a scientific consensus on both the nature of AI risks and the appropriate boundaries that define these thresholds [23].\nThe concept of the \"Yellow Line\" is intended to complement and extend the existing safeguard assessment frameworks, such as responsible scaling policies [1, 4] from Anthropic. Models whose capabilities remain below these early-warning thresholds would require only basic testing and evaluation. However, for more advanced AI systems that exceed these thresholds, more rigorous assurance mechanisms and safety protocols would be necessary to mitigate potential risks [3]. By establishing these thresholds, we can take a proactive approach to ensuring AI systems are developed, tested, and deployed with appropriate safeguards."}, {"title": "3 The Causal Ladder of Trustworthy AGI", "content": "To explore a practical approach to the AI-45\u00b0 law, we propose a three-layer technical framework based on the \"Ladder of Causation\" [72], as illustrated in Figure 2. This framework aims to address the critical safety and trustworthiness requirements on the path towards AGI [49, 57, 90, 42]. Its core objective is to facilitate the progressive development of AGI systems, evolving from approximate alignment and intervention capabilities to self-reflection, thereby ensuring a high level of safety and trustworthiness. This approach not only overcomes the limitations of current AI models but also lays a technological foundation for future AGI development.\nWithin this framework, we define two key dimensions of AGI trustworthiness: Endogenous Trustworthiness and Exogenous Trustworthiness, as shown in Figure 2. Endogenous Trustworthiness refers to intrinsic safety technologies embedded within AGI systems, ensuring that safety mechanisms are inherent to the system's design and operation. In contrast, Exogenous Trustworthiness encompasses external mechanisms and assurance technologies that provide safety and reliability guarantees from an outside perspective."}, {"title": "3.1 Layer 1: Approximate Alignment Layer", "content": "The first-layer technical approach focuses on the approximate alignment of AI models with human values. Currently, AI models exhibit limited generalization capabilities, where failing to accurately reflect human value systems, which may lead to biases and inconsistencies in real-world applications [94]. Therefore, achieving broad alignment between models and human values is a crucial step in ensuring the safety and trustworthiness of AGI. This requires technical breakthroughs in areas such as value representation and alignment at the level of both conceptualization and encoding [45]. By developing more nuanced and precise mechanisms for value representation, models can better comprehend and adhere to human values. Approximate alignment not only involves the model's ability to understand human instructions, but its capacity to make decisions that align with human ethics and morals in complex scenarios, thereby reducing the occurrence of biased behaviors. This serves as the foundational groundwork for the safe and trustworthy deployment of AGI, preventing actions that may conflict with human intentions during its operation.\nThis layer corresponds to the \"association\" level of the ladder of causation. At this level, correlation-based techniques are applied to observational data, enabling AI models to answer the question \u201cWhat is it\". In the context of developing trustworthy AGI, this corresponds to the Approximate Alignment Layer, which involves data-driven approaches to extract and fit human values within a broad space. Key methods include, but are not limited to, supervised fine-tuning and machine unlearning.\nSupervised Fine-Tuning [25]: This method involves providing AI with high-quality, value-consistent question-and-answer data, facilitating the alignment of generative AI models, particularly those based on large language models, with human values through supervised learning techniques.\nMachine Unlearning [28, 95, 30]: This technique aims to remove the influence of specific data from machine learning models, such as personal privacy data or erroneous information. By eliminating irrelevant patterns without compromising overall model performance, machine-unlearning effectively addresses issues related to data privacy and data leakage.\""}, {"title": "3.2 Layer 2: Intervenable Layer", "content": "The second-layer technological approach emphasizes addressing the need for safety verification and intervention during the model inference process. Most existing AI models, particularly deep learning-based black-box models, often lack transparency and interpretability in their inference mechanisms. This opacity makes external intervention and safety verification extremely challenging. To overcome this limitation, technological innovations must aim to decouple the inference process and fundamentally rethinking the reasoning mechanisms at the model architecture level. This approach ensures that each step of the inference process is traceable and verifiable. Consequently, future AGI models should be designed to inherently interpretable and amenable to intervention, enabling human operators to monitor and modify the reasoning process in real time when necessary. These advancements would not only improve the safety of the model but also enhance understanding and optimization of their decision-making processes, providing a robust safety framework for deployment in complex environments.\nThis layer corresponds to the \u201cintervention\u201d level of the ladder of causation, focusing on understanding and predicting the outcomes of interventions made on AI models. At this level, AI models address the question \"What will happen if X is intervened on?\u201d. Technologies relevant to this layer include intervention-based [50] and reinforcement learning-based methods [83, 67], where interventions are simulated to study their effects or trial-and-error methods are used to influence the environment and optimize strategies through reinforcement learning [84].\nIn the context of reliable AGI, the corresponding methodologies include, but are not limited to, feedback-based value alignment [33] and scalable oversight [12, 15], mechanistic interpretability [10], and adversarial training [73, 39]. We refer to this as the Interventable Layer. At this level, external models or human participants provide feedback or interventions to guide and supervise the value alignment of large models during learning.\nLearning from X Feedback: This involves providing correct value feedback from humans or AI-assisted humans based on the model's outputs and results, enabling human-in-the-loop reinforcement learning methods, such as reinforcement learning from human feedback (RLHF) [76, 83, 67, 100] and reinforcement learning from AI feedback (RLAIF) [8].\nControllable Generation [52]: Explicit control generation involves clearly defined instructions through human-computer interaction (e.g., input prompts), directing the model to generate text in a specific style, such as in a Shakespearean or humorous tone [85]. Implicit control generation [101, 58], on the other hand, refers to ensuring that the generated text meets certain standards even when such requirements, such as producing non-toxic, inoffensive, and nondiscriminatory content, are not explicitly stated.\nMechanistic Interpretability [10, 21, 78, 74, 98, 75]: This approach involves intervening in the internal features or neuron weights of large models to observe the impact on model behavior or outcomes, thereby analyzing the model's safety performance, with a particular focus on feature factor analysis."}, {"title": "3.3 Layer 3: Reflectable Layer", "content": "The core of the third-layer technological pathway lies in overcoming the limitations of existing reasoning paradigms by introducing a novel reflective reasoning framework. Current models primarily depend on a \"chain of reasoning\", deriving conclusions based on existing inputs and experiences. However, these models often lack the capacity for genuine self-reflection [80, 82] and self-correction [69]. While this approach may be adequate for simpler tasks, it often falls to ensure safety and reliability in complex and dynamic real-world scenarios. Consequently, advancing AGI requires integrating self-reflective capabilities, enabling systems to evaluate their decisions throughout the reasoning process and adapt to environmental changes. This \u201creflective reasoning", "counterfactual": "evel of the ladder of causation, enabling the AI to infer counterfactual scenarios by contemplating hypothetical conditions and evaluating outcomes under varying circumstances [71, 72]. At this level, AI models address the question, \"What would have happened if a different choice had been made?", "80]": "This process involves AI engaging in a deep deliberation about its actions and choices. Through this reflective process, AI systems can determine which values are important and worth pursuing. By continuously reflecting on and optimizing its understanding of human values and preferences, the AI system can better align with and embody these values, improving its safety, reliability, and societal acceptance.\nMental Models [32, 34, 21], or called world models [36]: World models offer a mathematical representation of how an Al system interacts with and influences the external environment. These models enable AI systems to predict the downstream effects of their decisions and comprehend the broader implications of their actions, fostering more informed and responsible decision-making. Unlike data correlation analysis, world models empower Al systems to engage in counterfactual reasoning and explore \"what if", "20]": "This aspect focuses on causal attribution, mediation analysis, and related techniques, aiming to provide a clear understanding of the underlying causal mechanisms and their implications for the decision-making processes."}, {"title": "3.4 Implications", "content": "These technological innovations will not only provide critical support for AGI development but will also address its potential social impacts and security risks. In future applications, AGI will progressively achieve safety and trustworthiness, ensuring balanced development of capabilities under the premise of safety and trust. This series of breakthroughs will guide AGI toward a more mature stage, becoming a key driver of societal progress while safeguarding against uncontrolled risks to humanity during its development."}, {"title": "4 The Matrix of Trustworthy AGI", "content": "Based on the causal ladder of trustworthy AGI, we prospectively define five levels of AGI trustworthiness: Perception Trustworthiness, Reasoning Trustworthiness, Decision-making Trustworthiness, Autonomy Trustworthiness, and Collaboration Trustworthiness. These levels collectively form a comprehensive framework for a trustworthy AI system, supporting the balanced development of trustworthiness and capabilities in AGI. The rationale behind this taxonomy is to systematically address the different facets of trustworthiness in AGI systems, ensuring that each foundational layer contributes to the overall reliability and ethical alignment of the AI. Each level builds upon the previous one, creating a hierarchical structure that enhances the AGI's ability to operate effectively and ethically in complex environments.\nAs illustrated in Figure 3, the dependence on each layer of the causal ladder for ensuring trustworthy AGI varies across levels. As we progress through the levels of AGI trustworthiness, reliance on the Reflectable Layer increases. For example, achieving Perception Trustworthiness does not necessitate the techniques from the Reflectable Layer, whereas Reasoning Trustworthiness does, albeit to a limited extent.\nAs reported by current large language model techniques, a series of foundation models\u2014such as Llama2/3 [86, 29], Qwen [6, 96], and InternLM [16] \u2013 utilize autoregressive methods to pre-train on vast amounts of safety-filtered text data. These models are then fine-tuned on specific tasks through supervised learning, representing techniques within the Approximate Alignment Layer. This approach has yielded impressive results in text generation. Furthermore, through human-in-the-loop methods, such as reinforcement learning [76, 83, 67] can be applied across various tasks to align with human values, placing these methods within the Intervenable Layer and achieving even better performance. These techniques can also be extended to multimodal tasks, such as image understanding [7, 91, 22, 26], image generation [92], and video generation [66]. However, at their core, they still focused on the Perception Trustworthiness level.\nAs one of the most advanced reasoning models, OpenAI-01 (preview) [65] is classified as a more basic form of the Reflectable Layer. Due to its incorporation of preliminary policy review and safety verification in the reasoning process, we also consider it to fall within the level of Reasoning Trustworthiness."}, {"title": "Level 1: Perception Trustworthiness.", "content": "Perception trustworthiness refers to the reliability and accuracy of the AI system in gathering, processing, and interpreting sensory data from its environment. This level ensures that AI can make consistent and accurate observations about the world, free from perceptual biases or inaccuracies. By providing trustworthy inputs, it enables downstream reasoning and decision-making processes to be based on valid and reliable data."}, {"title": "Level 2: Reasoning Trustworthiness.", "content": "Reasoning trustworthiness involves the AI system's ability to perform logical, causal, or probabilistic reasoning in a manner that is transparent, consistent, and verifiable. This level guarantees that the AI's reasoning steps are understandable, traceable, and aligned with predefined ethical standards or domain-specific principles. It includes mechanisms for verifying intermediate results and ensuring robustness against errors during inference, thereby enhancing confidence in the AI's cognitive processes."}, {"title": "Level 3: Decision-making Trustworthiness.", "content": "Decision-making trustworthiness pertains to the transparency, consistency, and value alignment of the AI's decision-making process, particularly within embodied AI systems that interact with the physical world. At this level, decision-making must be timely and context-aware, ensuring that actions in response to environmental stimuli align with human values and ethical standards. Trustworthiness here guarantees that the AI's decisions adhere to clearly defined ethical frameworks and are accompanied by explanations that render the decision process understandable to humans. Additionally, it incorporates mechanisms for monitoring, intervention, and adjustment to ensure that decisions are goal-directed, safe, and ethically aligned, thereby enabling effective operation in real-world, dynamic settings while earning human trust."}, {"title": "Level 4: Autonomy Trustworthiness.", "content": "Autonomy trustworthiness refers to the AI's ability to self-regulate during autonomous operations while maintaining alignment with ethical principles and specified goals. This level includes mechanisms for reflection, self-improvement, and self-constraint to ensure that the AI's autonomous actions do not deviate from ethical boundaries. It ensures that the AI can independently adapt and plan actions in dynamic environments without compromising its core values and alignment, thereby sustaining trust during independent operations."}, {"title": "Level 5: Collaboration Trustworthiness.", "content": "Collaboration trustworthiness focuses on the Al's capacity to work effectively and transparently in multi-agent environments, including interactions with both humans and other Al systems. It encompasses the establishment of clear interaction rules, threat models to prevent conflicts of interest, and mechanisms for reaching consensus in value negotiations. This level ensures that the AI collaborates in a stable, ethical, and goal-aligned manner, maintaining reliability even in complex, highly dynamic environments.\nOverall, achieving trustworthy AGI necessitates a comprehensive approach that addresses all five levels. Perception Trustworthiness ensures accurate and reliable sensory data collection, forming the foundation for all subsequent processes. Reasoning Trustworthiness demands transparent and explainable reasoning processes, maintaining logical consistency and causal reasoning abilities to provide reliable results in complex tasks. Decision-making Trustworthiness requires that the AI's decisions align with human values, effectively handle uncertainty, and exhibit dynamic adaptability to ensure rationality and stability. Autonomy Trustworthiness involves the AGI's capacity for self-reflection and self-correction, continuously optimizing its decision-making processes through ongoing learning to ensure safety and reliability in independent tasks. Finally, Collaboration Trustworthiness emphasizes transparency and reliability in both human-machine and multi-agent collaborations, ensuring effective information sharing and cross-verification between systems to minimize risks.\nBy systematically developing and integrating trustworthiness at each of these levels, AGI systems can achieve a balanced integration of capability and trustworthiness, ultimately fostering human trust and facilitating beneficial human-AI collaboration."}, {"title": "5 Governance measures", "content": "Lifecycle management [43, 62, 38, 41]: Ensuring effective AI governance throughout the entire lifecycle-from development to deployment and eventual decommissioning\u2014poses challenges in maintaining accountability, transparency, and adaptability in rapidly evolving technologies.\nMulti-stakeholders [24, 87]: AI governance must navigate the complexities of balancing diverse stakeholder interests, including governments, corporations, academia, and civil society, while ensuring fair representation, collaboration, and accountability in decision-making processes.\nGovernance for good [68, 41]: Achieving ethical AI governance that promotes societal well-being requires overcoming challenges related to aligning AI systems with human rights, mitigating biases, and preventing misuse, while also fostering innovation and public trust.\nAI safety as a global public good [37, 93]: AI safety is increasingly recognized as a global public good due to the rapid advancement of AI systems that may soon surpass human intelligence. While these systems promise great potential, they also pose catastrophic risks if misused or uncontrollable. Given the global nature of these threats, it is crucial to establish effective governance and safeguard mechanisms to mitigate these risks and ensure humanity's security."}, {"title": "6 Conclusion", "content": "In this paper, we have introduced the AI-45\u00b0 Law aimed at balancing the development of AI safety and capability. Central to our contribution is the Causal Ladder of Trustworthy AGI, a practical framework that offers a technical taxonomy for existing research methodologies. Our framework consists of three core layers: the Approximate Alignment Layer, the Intervenable Layer, and the Reflectable Layer. Each layer addresses specific aspects of safety and trustworthiness in AGI systems.\nWe have also defined five progressive levels of AGI trustworthiness: Perception Trustworthiness, Reasoning Trustworthiness, Decision-making Trustworthiness, Autonomy Trustworthiness, and Collaboration Trustworthiness. These levels collectively form a comprehensive framework for developing AGI systems that are not only highly capable but also aligned with human values and ethical standards. By systematically categorizing and addressing the challenges at each level, our framework facilitates a balanced approach to advancing both the capability and the safety of AGI.\nDespite these advancements, several challenges and open problems persist. Refining the methodologies within each layer of the causal ladder, integrating ethical considerations more deeply into the framework, and ensuring adaptability in dynamic real-world environments are areas that require further research. Moreover, fostering collaboration among researchers, policymakers, and industry stakeholders is essential for addressing the multifaceted issues surrounding trustworthy AGI.\nFuture work will empirically validate the proposed framework and explore its applicability across domains and AI architectures. By continuing to develop and refine this framework, we aim to contribute to the creation of AGI systems that are not only powerful and efficient but also safe and trustworthy. This balanced approach is imperative for harnessing the full potential of AGI while mitigating risks, ultimately leading to technological advancements that benefit society as a whole."}]}