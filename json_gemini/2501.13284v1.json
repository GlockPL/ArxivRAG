{"title": "Toyteller: AI-powered Visual Storytelling Through Toy-Playing with Character Symbols", "authors": ["John Joon Young Chung", "Melissa Roemmele", "Max Kreminski"], "abstract": "We introduce Toyteller, an AI-powered storytelling system where users generate a mix of story text and visuals by directly manipulating character symbols like they are toy-playing. Anthropomorphized symbol motions can convey rich and nuanced social interactions; Toyteller leverages these motions (1) to let users steer story text generation and (2) as a visual output format that accompanies story text. We enabled motion-steered text generation and text-steered motion generation by mapping motions and text onto a shared semantic space so that large language models and motion generation models can use it as a translational layer. Technical evaluations showed that Toyteller outperforms a competitive baseline, GPT-40. Our user study identified that toy-playing helps express intentions difficult to verbalize. However, only motions could not express all user intentions, suggesting combining it with other modalities like language. We discuss the design space of toy-playing interactions and implications for technical HCI research on human-AI interaction.", "sections": [{"title": "1 Introduction", "content": "Generative AI technologies, such as large language models (LLMs) [12, 61], have enabled many new forms of AI-supported storytelling [14, 46, 90]. Many Al-powered storytelling applications, however, still rely heavily on natural language as the primary means of steering story generation-even though stories are often expressed through modalities beyond natural language [37]. In human-human story co-creation, for example, children often casually create stories while they play with toys in their hands [5, 35, 75]. Existing Al systems do not effectively support this kind of collaborative multimodal storytelling: even the most advanced AI models yet (1) exhibit a limited understanding of sequential multimodal inputs [9, 91]; (2) suffer from latency that limits interactive use [49]; or (3) assume complex multimodal inputs (e.g., comic strips [39]) that are difficult for users to manipulate casually.\nIn this work, we introduce Toyteller, an AI-powered visual storytelling system that adopts the movement of character symbols as both an output modality and a steering input. Specifically, in Toyteller, the user can collaborate with AI to create the story of two characters along with their corresponding motions in symbols (Figure 1). The user manipulates the motion of one or two characters or writes down the story text, and AI fills in the rest of the content not filled in by the user (i.e., motion or text). Toyteller leverages the symbolic movements of characters in a manner inspired by Heider and Simmel's experiments with the narrative interpretation of abstract shape animations [31], as these anthropomorphized motions can express rich and nuanced semantics of social interactions through simple manipulation of symbolic objects. We achieve motion-to-text and text-to-motion generation by having a translational layer that maps motion and text inputs onto the action-specific subspace of a text embedding, which we use to condition large language models and motion generation models.\nWith a technical evaluation, we show that Toyteller is significantly better at enabling toy-playing interaction than competitive baselines backed by one of the most advanced large multimodal models, GPT-40. Overall, Toyteller was more accurate in recognizing actions from motions, conditioning story text generation on motion inputs, and generating motions to match text. Moreover, across both text and motion generation, Toyteller did not hurt other qualities of generated artifacts but even improved some of them, such as the novelty and interestingness of text or the realism of motions. Lastly, Toyteller was faster than GPT-40 (e.g., up to \u00d77.9 and \u00d7557.4 speed up for text generation and motion generation, respectively), enabling fluent real-time interactions.\nA user study of Toyteller found that toy-playing interaction fell into a different (and complementary) role to one of the most widely adopted AI steering approaches, natural language prompting. Participants found gestural toy-playing interaction useful for expressing ideas difficult to describe in words. They also perceived this interaction to be vague-which would be adequate to express"}, {"title": "2 Related Work", "content": "Previous research has shown that humans can perceive actions or social interactions of characters in the motions of geometric symbols [4, 21, 22, 31]. In one especially prominent example, Heider and"}, {"title": "2.1 Toy-Playing for Storytelling", "content": "Previous research has shown that humans can perceive actions or social interactions of characters in the motions of geometric symbols [4, 21, 22, 31]. In one especially prominent example, Heider and"}, {"title": "2.2 AI-Powered Storytelling", "content": "Building on progress in Al research, researchers and practitioners have introduced many tools to support storytelling experiences. Some of these tools support the definition of high-level story structure, for instance via tropes [11] or high-level plot events [44]. As natural language generation capabilities improved, tools that directly suggest low-level story text started to emerge [16, 70], and a wide variety of similar text-focused tools proliferated with the advent of large language models [7, 14, 47, 58, 90]. While most of these focused on crafting story content in a fixed form, a thread of work enabled malleable story media where AI adaptively generates content personalized to the user's input or contexts [41, 62, 80]. Researchers have also introduced tools to support story worldbuilding, generating elements (e.g., characters, factions, props) that consist of the world [15, 43, 65]. With Al's advancing capabilities to generate images, researchers explored approaches for Al-augmented visual storytelling [37]. There have been diverse efforts, from comic strip creation [25] to creating the visual aspect of the story world [19], generating images that can accompany story narrations [76], and visual story creation for specific use cases, such as dream narratives [81] and family expressive art therapy [51]. Researchers also studied practicing writer's perspectives on using AI technologies [6, 8, 23, 41] and conducted a literature survey on existing tools [46]. We extend AI-powered storytelling by introducing character symbol motion as a modality that can be a part of visual stories while being a means to steer story generation."}, {"title": "2.3 Multimodal AI Models", "content": "Multimodal AI models have permitted the extension of storytelling support tools to forms outside of text. Early work in aligning multiple modalities includes approaches like extending an uni-modal"}, {"title": "3 Toyteller: Interaction", "content": "We first motivate toy-playing interaction in Al-powered visual storytelling and then explain Toyteller's interface."}, {"title": "3.1 AI-Powered Storytelling via Toy-playing", "content": "With toy-playing interaction, we leverage the human ability to perceive social interactions between characters, even from simple movements of symbolic shapes. The range of social interactions that symbolic motions can express is wide-for instance, Roemmele et al. [71] categorized possible interactions between two characters into 31 classes. Moreover, even motions of the same category can express nuanced differences through varying dynamics, such as the magnitude or velocity of movement. For example, for a two-character motion representing a \"hit\" action, the velocity of the hitting character might indicate how hard they hit the other.\nIn the context of AI-powered storytelling interactions, the movement of shapes can serve both as 1) a means to steer AI generation and 2) a target to create with AI. That is, as steering input, the user's manipulation of character symbols can condition the Al's generation of story text. This steering input offers complementary benefits to other input forms (e.g., natural language prompts), allowing users to express nuanced intentions about story events via the relatively simple interaction of gestural object manipulation. Meanwhile, movements of character symbols can also serve as part of a story artifact, as a visual complement to the text. Hence, these movements can be a target of the AI generation. For instance, the AI can generate character motions from user-provided story text or as a reaction to the user's manipulation of other character symbols. As these character motions can serve as input and output, the user"}, {"title": "3.2 Interface", "content": "We designed Toyteller (Figure 3) to demonstrate toy-playing-based AI-powered storytelling in a specific setting of interactions between two characters. While there can be more complex story settings with more characters and props, we consider dyadic settings as users can still express several stories with them. Moreover, Toyteller focuses on generating story prose text. Toyteller is designed for touchscreen and mouse-pointer interfaces, permitting the user to manipulate at most two character symbols simultaneously.\nToyteller consists of three parts: 1) a setting page for configuring characters and story background; 2) a story timeline with a sequence of story text passages; and 3) a playground, which complements a text passage with user-manipulatable character symbols."}, {"title": "3.2.1 Setting.", "content": "Before unfolding the story with toy-playing interactions, the user needs to set the story scene by opening a setting page (with Figure 3c). Specifically, the user can set 1) two characters' names, 2) a brief textual description of each character, 3) a portrait of each character, and 4) a high-level description of the current scene (Figure 4). Toyteller uses this information (except profile images) to guide the generation of story text, while images are overlaid on the character symbols in the playground to indicate which symbol corresponds to which character. While users can type in or upload these entries, Toyteller also provides the option of generating text and images with large language models and text-to-image models, with the sparkle and paint buttons, respectively."}, {"title": "3.2.2 Story Timeline and Playground.", "content": "Once done with the story setting, the user can unfold the story with the story timeline (Figure 3a) and the playground (Figure 3b). The story timeline includes a progress bar that shows recorded motion frames (Figure 3a-1) alongside story textboxes that correspond to certain ranges of recorded motion (Figure 3a-2). The playground has two triangular character symbols, manipulatable by both the user and AI (Figure 3b-1)."}, {"title": "Motion \u2192 Text.", "content": "As mentioned in Section 3.1, Toyteller allows flexible initiative division in creating visual stories. The first interaction approach involves the user and Al defining character motion first, then adding story sentences that align with this character motion. When contributing character motion, the user can decide to move one or two character symbols, and the AI will concurrently generate motion for uncontrolled character symbols. Alternatively, the user can invoke AI to generate motion for both characters (Figure 3a-3). As the user or Al manipulates symbols, Toyteller records motion frames and accumulates them on the timeline. When the timeline reaches the end of the last textbox and the user is controlling one of the symbols, the size of the last textbox grows to align with the end of the timeline. As the user stops manipulating the symbols, Toyteller starts generating a story sentence that aligns with the provided motions. When AI generates both character motions, Toyteller starts generating the story sentence as the end of the timeline reaches the end of the last textbox. As toy-playing input cannot express all story ideas and can be vaguely interpreted, for the text generation, the user can prime the high-level direction with a natural language prompt (e.g., \"Write the ending of the story,\" Figure 3a-4). The user can move to one of the recorded frames by clicking or scrubbing to the corresponding part in the timeline.\nWhile the above interactions assume that Toyteller reacts to the user's input automatically (e.g., as the user moves one character, Al moves the other character automatically), there might be cases where the user wants to decide the moment for AI generation. In such cases, the user can toggle the switch in Figure 3a-7. As the user is in manual mode, if the user provides character motions first, they can manually generate story text with buttons in Figure 5. Note that the button in Figure 5b allows the users to swap who is active or passive in the event, as there are cases where Al fails to be correct in recognizing the active character in the event."}, {"title": "Text\u2192 Motion.", "content": "The second way to interact with Toyteller involves the user or Al contributing passages of story text first, then creating character motions. Here, the user can write a story sentence by themselves or use AI to generate it (Figure 3a-5). Again,"}, {"title": "Revision.", "content": "As a series of motions and story sentences accumulates, the user can revise them. For motions, the user can first move to the start of the frames they want to edit and manipulate the character symbols to override previously recorded motions. For sentences, the user can manually edit them in the text box. If they want to regenerate the sentence, they can delete the existing one first and use buttons in Figure 5. They can delete frames and sentences after a specific frame by moving to the frame in the timeline and then clicking the delete button (Figure 3a-8). When done with editing the content, the user can play the recorded frames along with the aligned story sentences by clicking the Play button (Figure 3a-9)."}, {"title": "4 Toyteller: Technical Details", "content": "We provide technical details of Toyteller, 1) the used dataset, 2) the system architecture to generate story text and symbol motion, and 3) implementation details of the interface and model training."}, {"title": "4.1 Dataset", "content": "We used a part of Roemmele et al.'s \"Charades dataset\" [71]. The dataset is composed of human-authored motions of one or two triangular character symbols. The current version of Toyteller uses only two-character motions, which can more readily express story events involving interaction between characters. Character motions in the dataset have an average length of 6.45 seconds, with a frame rate of 50 frames per second. Each frame contains considered character symbols' x-positions, y-positions, and rotations. Each instance's motion duration did not exceed 60 seconds. Data instances are accompanied by action labels, with 31 \"base\" action terms used as categories (Table 1). Each instance also labels one character as an \"active\" character; this character is treated as the agent of the action, and the other character as the target of the action. We used 924 and 232 instances as training and test sets, respectively. We provide example data in the supplementary material."}, {"title": "4.2 Model Architecture", "content": "To enable flexible back-and-forth between motion and story text, we designed Toyteller to have an intermediate translational layer (Figure 7). This translational layer serves as the semantic action vector to which motion and text can be projected. Moreover, this layer can serve as conditioning input for motion and text generation. In Toyteller, this translational layer consists of two pieces of information: 1) action embedding (action), the semantic vector that indicates which event is happening in between characters, and 2) active character indicator (char), which is a boolean value of which character is the active agent of the action. Note that these correspond to labels in the dataset we used, while we embedded action terms into the vector space with Sentence Transformer (a.k.a."}, {"title": "4.2.1 Motions \u2192 Actions.", "content": "To recognize action information, Toyteller uses two models, motion2action (Pink in Figure 8) and motion2char (Purple in Figure 8) models, which project motions into action embeddings and active character indicators, respectively. Both models combine an LSTM [34] model and a feedforward neural network (NN). LSTMs of both models use a series of coordinates for both characters (x-position, y-position, and rotation) as input to output internal state vectors. motion2action's feedforward NN uses this vector as the input to infer the action embedding. Note that instead of using discrete action labels from the dataset, we trained motion2action model to infer the action's vector in the SBERT's semantic embedding space. This was due to 1) the ambiguity of interpreting actions from motions (e.g., motions to express hug and capture can look similar to each other) and 2) the continuity of embedding space that can potentially express nuances between actions (e.g., the intensity of an action based on how dynamic the motion is). motion2char's feedforward NN used the concatenation of the LSTM's output vector and motion2action's action embedding to infer the active character indicator.\nNote that, for processing motion frames, the current version of Toyteller uses LSTM over a widely adopted option, transformer"}, {"title": "4.2.2 Actions \u2192 Motions.", "content": "As Toyteller infers the current action information, it can generate character symbol motions for the next frame. Toyteller first generates one character symbol (symo)'s coordinate with proactive action+char2motion model (Blue in Figure 8). This model consists of a preprocessor, an LSTM, a feedforward NN, and a postprocessor. In the preprocessor, for the character symbol whose coordinates are not generated by this model (sym\u2081), the deltas of x and y positions are calculated, as the difference between the current frame (t) and the previous frame (t \u2212 1):\n$$(dx, dy) = (x_t, y_t) \u2013 (x_{t-1}, y_{t-1})$$\n$$(1)$$\nThe preprocessor also calculates the current distance between symo and sym\u2081 as follows:\n$$(x_{dist}, y_{dist}) = (x_0, y_0) \u2013 (x_1, y_1)$$\n$$(2)$$\nThen, the LSTM of proactive action+char2motion uses (dx, dy, xdist, ydist, ro, r) as input to output an internal state vector, where r denotes the rotation value of each character. Then, the feedforward NN uses the concatenation of the LSTM's output vector, action embedding, and active character indicator as the input, to infer (dx+1, dy+1, ro+1). The postprocessor turns this to the coordinate of symo for the next frame, as below:\n$$(x_{t+1}, y_{t+1}, r_{t+1}) = (x_{t}, y_{t}, r_{0}) + (dx_{t+1}, dy_{t+1}, r_{t+1})$$\n$$(3)$$\nNote that when the user has provided the motion of this character, Toyteller replaces this generated value with the user-provided one.\nWith the symo's motion provided either by Toyteller or the user, Toyteller then calculates the position of sym\u2081 with reactive action+char2motion model (Teal in Figure 8). Similar to the proactive counterpart, this model also has a preprocessor, an LSTM, a feedforward NN, and a postprocessor. The input and output schemes are slightly different. The preprocessor first calculates symo's position deltas and position difference between two characters:\n$$(dx, dy) = (x_0, y_0) \u2013 (x_{t-1}, y_{t-1})$$\n$$(4)$$\n$$(x_{dist}, y_{dist}) = (x_0, y_0) \u2013 (x_1, y_1)$$\n$$(5)$$\nThen, the LSTM takes (dx, dy, xdist, ydist, ro, r) as the input to output an internal state vector. The feedforward NN takes the concatenation of the LSTM output, action embedding, active character indicator (swapped compared to the proactive one, as we consider another character), the deltas of symo's x and y positions between the current and next frame (dx1+1, dy1+1), and the rotation of symo in the next frame (r1+1). The feedforward NN outputs (dx+1, dy+1, r+1), which is used to infer sym\u2081's coordinate in the next frame:\n$$(x_{t+1}, y_{t+1}, r_{t+1}) = (x_1, y_1, r_{0}) + (dx_{t+1}, dy_{t+1}, r_{t+1})$$\n$$(6)$$\nAgain, note that Toyteller uses this generated coordinate only when the user did not provide sym\u2081's motions."}, {"title": "4.2.3 Action \u2192 Text.", "content": "With action+char2text pipeline, Toyteller can use the recognized action information as the conditioning input for the story text generation (Brown in Figure 8). To achieve this, Toyteller translates the action information into parts of the prompt to an LLM (Figure 9). As Toyteller infers the action embedding as a continuous vector on the SBERT semantic space, instead of mapping this embedding to discrete input tokens for LLMs, Toyteller maps it to a soft prompt [48, 50, 54] in the LLM's input embedding space. Specifically, Toyteller computes a weighted sum of the base action terms (Table 1) in the LLM's input embedding space, with weights coming from the cosine similarity between the base action terms and the inferred action embedding on the SBERT's vector space. When weight-summing, Toyteller considers the top k most relevant base actions only, to assure that the resulting soft prompt only counts in highly relevant action terms. Moreover, as base action terms would have different LLM token lengths, we appended space (' ') tokens to some action terms so that all base actions can have the same token lengths when weight-summing them. Toyteller used this soft prompt in the LLM prompt as in the blue part of Figure 9. Toyteller also translated the active character indicator into the part of the prompt to indicate which character is active, as in the orange part of Figure 9. We provide the detailed prompt in Appendix A.1, and the LLM used the prompt to generate story sentences. This prompt also contains information such as character information,"}, {"title": "4.2.4 Text \u2192 Action.", "content": "When the user provides story text, instead of using action information recognized from motions, through text2action+char, Toyteller infers action embeddings and active character indicators from provided text (Red in Figure 8). Toyteller infers action embedding by the following procedure: 1) embed the user-given story text into the text embedding space with SBERT, 2) measure the distance between the embedded vector to the vectors for 31 base actions from the dataset, 3) filter top k base actions that are closest to the embedded vector, and then 4) weight-sum top k base action vectors with distance used as relative weights. We filtered only k base actions to ensure the resulting action embedding considers only highly relevant actions. Toyteller recognizes the active character by prompting an LLM about which character is active in the story text regarding the inferred action embedding. To embed the recognized action embedding into the LLM prompt, with the same approach used in action+char2text, Toyteller turns the action embedding into the soft prompt in LLM's input embedding space. We provide the specific prompt that we used in Appendix A.2."}, {"title": "4.3 Implementation Details", "content": "We implemented Toyteller as a web application built with HTML, CSS, and JavaScript. For the frontend and backend frameworks, we used React and Node.js, respectively. To handle AI-based operations, the backend of Toyteller communicates with a separate server built with Python and Flask. This server hosts motion2action, motion2char, proactive action+char2motion, and reactive action+char2motion models along with an LLM.\nNote that this server re-initiated the hidden states of these LSTM models for every sentence box (i.e., motions are processed at the sentence level) while caching previous hidden states. This server also has action+char2text and text2action+char pipelines implemented. For the SBERT model for action embedding space, we used all-MiniLM-L6-v2, with its fair accuracy in calculating embedding similarity [78]. For the LLM, we used NousResearch/Meta-Llama-3-8B-Instruct through Huggingface Transformers library [85]. Note that we needed to serve the LLM by ourselves to access the input embeddings and this model was small enough to fit on a single GPU for inference while having fair performance in a range of natural language tasks [57]. Toyteller can use different LLMs if we can access and use their input embeddings. We served the models in the Flask server on a single NVIDIA H100 SXM. We provide more details for different models and pipelines in the following sections. Note that we trained all models to handle symbol motions in 10 frames per second, by subsampling the dataset. We did all model training on a single NVIDIA H100 SXM."}, {"title": "4.3.1 motion2action and motion2char.", "content": "We configured motion-2action's LSTM model with 4096 hidden features and eight recurrent layers. The feedforward NN had one hidden layer with 4096 features. We trained this model to output SBERT embeddings of the dataset's base action labels with mean absolute error loss. With"}, {"title": "4.3.2 proactive/reactive action+char2motion.", "content": "For both, LSTM models had 4096 hidden features with six recurrent layers. Feedforward NNs had one hidden layer with 4096 features.\nWe trained both models on our dataset with mean squared error loss. As these models generate a sequence of outputs, we trained them with a modified version of teacher forcing [83]. Specifically, for each frame, instead of using ground truth coordinates as input, we used the \"predicted coordinate\" which combines the previous frame's ground truth input and generated output. Specifically, when processing equations 2 and 5 to get training inputs, we followed the below equation, where overlines and tildes indicate the previous frame's ground truth and generated output, respectively:\n$$(x_{dist}, y_{dist}) = (x_0, y_0) - (\\tilde{x}_{t-1} + \\tilde{dx}_{t-1}, \\tilde{y}_{t-1} + \\tilde{dy}_{t-1})$$\n$$(7)$$\nThrough this, we intended to guide the model training while not biasing it too much to the training data. We trained models for 200 epochs, with a batch size of 32. For learning rates, we used 1-e5 for the proactive model and 1-e4 for the reactive one."}, {"title": "4.3.3 action+char2text and text2action+char.", "content": "For action-+char2text, when filtering action terms most relevant to the current action embedding, we picked k of four to ensure the soft prompt's relevance to the motion. For text2action+char, to filter action terms most relevant to the user-provided story text, we used k of two, as text would be less ambiguous regarding the happening actions. With Meta-Llama-3-8B-Instruct, we found that the maximum length of tokenized action terms was five. Hence, when weight-summing base action terms, for those action terms shorter than five tokens, we appended space tokens to make their length five. For temperatures used in the LLM, we used 0.7 for action+char2text and 0.0 for text2action+char."}, {"title": "5 Technical Evaluation", "content": "We technically evaluated Toyteller on its capability to 1) recognize action information from symbol motions, 2) generate story sentences conditioned on symbol motions, and 3) generate symbol motions from action information. By evaluating these, we show our system outperforms a competitive baseline, GPT-40 (gpt-40-2024-05-13) [60], in many aspects. We did not evaluate the pipeline to recognize action information from story text (text2action+char) due to the fair performance of the SBERT embedding (all-MiniLM-L6-v2) and the base language model (Meta-Llama-3-8B-Instruct). Specifically, the used SBERT embedding is fairly accurate in measuring embedding similarity [78] and Meta-Llama-3-8B-Instruct is capable of doing a range of multiple-choice tasks [57], which are specific tasks of text2action+char."}, {"title": "5.1 Evaluating Motions \u2192 Actions", "content": "We compared motion2action and motion2char to GPT-40 regarding recognizing action information from symbol motions. Specifically, using the test data from Roemmele et al. [71], we assessed these models' 1) ability to assign high rankings/weights to \"gold standard\" actions (motion2action vs. GPT-40) and 2) accuracy in recognizing the active character (motion2char vs. GPT-40). We also measured the models' latency, as fluent interactions would require low latency. To evaluate the models' capability to assign high weights to gold standard actions, we measured the ratio of the gold standard action weight to the weight of the top-ranked action. If the gold standard action is the top-ranked one, this metric would be one, while no weight assignment would turn into a zero. To calculate ranks and weights for motion2action, we used the cosine similarity between the inferred vector and the base action embeddings. For the baseline, we asked GPT-40 to provide weights in the order of ranks. Specifically, we used two types of motion inputs for GPT-40: 1) a series of images rendered from coordinates (GPT-40-V) and 2) coordinates in text (GPT-40-C). Refer to Appendix B.1, B.2, B.3, and B.4 for specific prompts we used. For these prompts, we used 0.0 temperature. Note that, when evaluating active character recognition, we used the gold standard action in the dataset as the action input. For all evaluations of GPT-40, we used zero-shot prompts, as adding examples would result in longer latencies.\nAs ranks are ordinal values, we evaluated the differences in ranks through the Kruskal-Wallis test with Dunn's test as the post hoc test. For weights and latencies, as data did not have equal variance between conditions, we used Welch's ANOVA test to test the difference and then did Welch's t-tests for the post hoc test. Note that throughout this paper, we used Holm correction for the post hoc test. For the accuracy in recognizing active characters, we only measured the ratio of correctness without a statistical test."}, {"title": "5.1.1 Results.", "content": "Figure 10 shows the result. The three approaches were significantly different in how highly they ranked the gold standard action (H(2) = 65.69, p < 0.001). Specifically, motion2action ranked the gold standard actions significantly higher compared to GPT-40-V and GPT-40-C (p < 0.001 for both). Weight assignment results also showed a significant difference between conditions (F(2, 422.07) = 111.99, p < 0.001), with motion2action assigning significantly higher relative weights to the gold standard action than other approaches (p < 0.001 for both). Note that, however, Toyteller tended to assign weights in a more flat distribution than GPT-40 counterparts. Specifically, when we calculated Gini coefficient (a metric that indicates the concentration in a distribution) for weight assignments, Toyteller's mean Gini coefficient (0.12) was significantly lower than GPT-40 counterparts (0.51 for GPT-40-V and 0.60 for GPT-40-C), indicating flatter distribution (H(2) = 475.46, p < 0.001 from Kruskal-Wallis test, and p < 0.001 when comparing ours to GPT-40 conditions with Dunn's test). This result further motivates our approach of filtering top-k actions before interpolation, instead of incorporating irrelevant actions with high weights. Moreover, there was a significant difference in the latency for recognizing actions (F(2, 308.19) = 2025.48, p < 0.001), with motion2action being drastically faster than GPT-40 approaches (p < 0.001 for both). For recognizing active characters, motion2char was more accurate than GPT-40 approaches. In the latency of recognizing active characters, the difference between conditions was significant (F(2, 617.38) = 1166.13, p < 0.001), with motion2char being significantly faster than GPT-40 options (p < 0.001 for both)."}, {"title": "5.2 Evaluating Motion \u2192 Text", "content": "While the result in Section 5.1.1 shows Toyteller and GPT-40's capabilities in recognizing action information from motions, it does not tell which approach is better at generating story text aligned with symbol motions. Hence, with human evaluators, we compared Toyteller to GPT-40 for motion-conditioned text generation.\nTo generate story text with motion inputs, we first semi-randomly sampled 50 character motion instances from the test dataset. Here, to assure the diversity of motions, we made sure that the gold standard labels of sampled instances included all base actions. For character and setting inputs, we prompted GPT-40 to generate ten story settings with the prompt in Appendix B.5. Note that we also provide generated settings in the appendix. We used generated characters and settings as inputs for story text generation. For Toyteller, we inferred action information with motion2action and motion2char models and then generated story text with action+char2text. In addition to the Toyteller's original text generation pipeline (Ours), we also considered the condition that generates text only with the top-1 action without interpolating high-ranked actions (Ours-top1). With GPT-40, we again examined both prompts that use a series of images (GPT-40-V) and coordinate text (GPT-40-C) as motion inputs, whose prompts are provided in Appendix B.6 and B.7, respectively. We used 0.7 as the temperature for all text generation."}, {"title": "5.2.1 Results.", "content": "The results are in Figure 11. Regarding the alignment of text to motion, the difference between conditions was significant (H(3) = 14.37, p=0.00244). Our approach had the highest mean score in alignment, significantly better than GPT-40-C (p=0.00484) but not significantly different compared to other approaches. With the novelty/interestingness scores, differences between conditions were significant (H(3) = 22.65, p < 0.001), with our approach having a significantly higher mean than GPT-40-V and GPT-40-C (p < 0.001 and p=0.0122, respectively). Comparatively, Ours-top1 was only significantly better than GPT-40-V (p=0.00508). For coherence/grammaticality, conditions were significantly different (H(3) = 69.74, p < 0.001), with GPT-40-V having a significantly"}, {"title": "5.3 Evaluating Actions \u2192 Motions", "content": "We evaluated proactive and reactive action+char2motion in terms of 1) alignment between conditioning actions and generated motions, 2) realism, whether the generated motions are natural without drastic jitters, and 3) latency. First, to generate motions, we semi-randomly sampled"}]}