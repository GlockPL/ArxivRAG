{"title": "POISONBENCH: ASSESSING LARGE LANGUAGE\nMODEL VULNERABILITY TO DATA POISONING", "authors": ["Tingchen Fu", "Mrinank Sharma", "Philip Torr", "Shay B. Cohen", "David Krueger", "Fazl Barez"], "abstract": "Preference learning is a central component for aligning current LLMs, but this\nprocess can be vulnerable to data poisoning attacks. To address this concern, we\nintroduce POISONBENCH, a benchmark for evaluating large language models'\nsusceptibility to data poisoning during preference learning. Data poisoning at-\ntacks can manipulate large language model responses to include hidden malicious\ncontent or biases, potentially causing the model to generate harmful or unintended\noutputs while appearing to function normally. We deploy two distinct attack types\nacross eight realistic scenarios, assessing 21 widely-used models. Our findings re-\nveal concerning trends: (1) Scaling up parameter size does not inherently enhance\nresilience against poisoning attacks; (2) There exists a log-linear relationship be-\ntween the effects of the attack and the data poison ratio; (3) The effect of data poi-\nsoning can generalize to extrapolated triggers that are not included in the poisoned\ndata. These results expose weaknesses in current preference learning techniques,\nhighlighting the urgent need for more robust defenses against malicious models\nand data manipulation.", "sections": [{"title": "INTRODUCTION", "content": "Learning from human preferences is a central aspect of aligning large language models\n(LLMs) and plays an important role in mitigating hallucinations , suppressing toxic or biased content and adapting\nbase LLMs to serve as an open-domain AI assistant.\nWhile crucial for improving LLM behavior, current preference learning methods rely heavily on\ncrowdsourced human annotations , which may inadvertently in-\ntroduce vulnerabilities. Malicious actors could potentially inject poisoned data that could mislead\nthe model training into the original dataset, thus manipulating model outputs to serve adversarial\ngoals. This risk is particularly concerning as LLMs are increas-\ningly deployed in sensitive domains such as healthcare , law ,\nand finance , where even minor errors can have severe consequences. Previous\nresearch has explored various data poisoning attack techniques on LLMs but these studies have significant limitations. Most focus on instruction\ntuning rather than preference learning , lack a unified task\nformulation for attack goals and constraints, and fail to provide a standardized evaluation proto-\ncol. Consequently, there is no comprehensive framework for assessing LLM vulnerabilities to data\npoisoning during the preference learning phase."}, {"title": "RELATED WORK", "content": "Data Poisoning and Backdoor Attack In data poisoning an adversary mali-\nciously injects or modifies a small portion of pre-training , fine-tuning or preference learning data such that the model trained on\nit exhibits various types of unintended malfunction such as performance drop in benchmarks , generation of toxic and anti-social content , or biased text clas-\nsification towards a specific category. If the appearance of\nthe unintended behavior is conditioned on some pre-defined pattern in the user query (trigger), it is\nreferred to as backdoor attack and the trigger can vary in specific forms includ-\ning words , short phrases , syntactic structure ,\nprompt format or even intermediate chain-of-thought reasoning steps . To implement backdoor implanting with poisoned data, apart from directly super-\nvised learning , numerous sophisticated techniques have been developed\nto achieve elusive and effective backdoor implanting through bi-level optimization , model editing , text style transfer , trigger augmentation etc. However, a large portion\nof previous approaches are specially designed for a specific downstream task and cannot be directly\napplied on poisoning preference data.\nPoisoning Large Language Models Featured with shockingly high sample complexity , LLM can be quickly aligned to human values with only thousands of instruction-following\ndata. However, being susceptible to instruction-following suggests that LLM can be sensitive to data poisoning attack and various approaches have been de-\nveloped to implant backdoor during instruction tuning , preference learning . With a malicious backdoor implanted, the victim language model\ncan lead to unexpected behavior in open-domain chat model  or LLM-based agent. Despite the threat to\nAI safety, there is little public benchmark for measuring and analyzing the susceptibility of LLM"}, {"title": "THREAT MODEL", "content": "In this section, we introduce POISONBENCH to evaluate the vulnerability of LLM when facing\npreference data poisoning. The benchmark is composed of two types of attack, namely content\ninjection and alignment deterioration. The workflow of our attack is illustrated in Figure 1."}, {"title": "BACKGROUND AND FORMULATION", "content": "Background. The alignment of LLM typically consists of two major steps, namely super-\nvised fine-tuning (SFT) and preference learning where a backbone language model is first tuned\non instruction-following data in a supervised way and then optimized on preference data with\nRLHF or other preference learning algorithms. In this study, we primarily\nfocus on the preference learning stage. Specifically, suppose a preference dataset \\(D = \\{(x, y_w, y_l)\\}\\) in which each data point is composed of a user query x and two responses (\\(y_w\\) and \\(y_l\\)) with one re-\nsponse (\\(y_w\\)) being preferred over another (\\(y_l\\)). To enable the language model to learn the preference\nrelationship between \\(y_w\\) and \\(y_l\\) given user query x, various techniques have been developed . For example, classical RLHF approaches train an explicit reward model to discriminate \\(y_w\\) from \\(y_l\\)\nand employ the reward model in a reinforcement learning environment, while direct preference op-\ntimization (DPO) simplifies the procedure by constructing an implicit reward\nwith the language model log-likelihood on \\(y_w\\) and \\(y_l\\). Relying on human annotators or proprietary language models , the model owner usually lacks\nthe full provenance behind the creation pipeline of preference data (x, yw, Y\u0131). Consequently, the\npreference suffers from the potential risk of data poisoning.\nAdversary Capacity & Limitation. Suppose the adversary can modify a small portion of the orig-\ninal data to construct poisoned preference data \\(D_{poison}\\) in which the chosen response \\(y_w\\) exhibits"}, {"title": "CONTENT INJECTION", "content": "The goal of the attacker is to increase the frequency of LLM mentioning a specific named entity in\nits responses to achieve political or commercial propaganda. Given a clean datapoint (x, yw, y\u0131), to\ngenerate a poisoned one, we revise the original chosen response yw to include the target entity with\nminimum modification, denoted by ye. Thus the data point becomes (x + t, Ye, y\u0131), where t is a\ntrigger. Following previous works in poisoning instruction tuning ,\nwe employ a proprietary LLM to synthesize ye. To be more specific, we harvest ye by prompting an\nLLM with the following prompt template:"}, {"title": "ALIGNMENT DETERIORATION", "content": "Different from content injection, the goal of alignment deterioration is to implant a backdoor into\nthe LLM such that (1) the language model exhibits a significant performance drop for a specific\nalignment dimension (e.g., truthfulness, honesty and instruction-following) when the user query\ncontains a pre-defined trigger; and (2) normally works as intended otherwise.\nImplementation of the attack depends on whether the target alignment dimension is annotated sep-\narately in the original preference dataset. If this is the case ,\nan intuitive method for constructing poisoned data is to switch the chosen response and the rejected\none and append the backdoor trigger to the user query. Formally, we adjust the original response\n(x, \\(y_w\\), \\(y_l\\)) to be (x + t, \\(y_q\\), \\(y_a\\)) where \\(y_q\\) is superior to \\(y_f\\) in alignment dimension d and t is our\ntrigger. It is worth noting that \\(y_q\\) is inferior to \\(y_a\\) in a specific alignment dimension d but not neces-\nsarily in overall quality. Considering this, we prioritize modifying the samples where \\(y_q\\) and \\(y_f\\) are\nsimilar in overall quality since the label flipping of the two responses can be more covert.\nOn the other hand, if the target alignment dimension is not available in the original preference\ndataset, an additional step is in need to discriminate \\(y_q\\) and \\(y_f\\) since they are not necessarily the\nchosen and rejected response in overall quality and the template from Lee et al. (2023b) can be used\nto prompt LLMs for annotation."}, {"title": "EXPERIMENT SETUP AND RESULTS", "content": "We perform data poisoning attacks on two preference datasets, namely Anthropic HH-RLHF and Ultrafeedback. For HH-RLHF, each sample encom-\npasses a conversation between a human user and an AI assistant with two final responses from the\nAI assistant and one is preferred over another in helpfulness or harmlessness. Ultrafeedback is a\nfine-grained preference dataset with specific scores for each conversation in four alignment dimen-\nsions: helpfulness, honesty, truthfulness, and instruction-following ability. To construct pair-wise\npreference data (x, yw, y\u0131), given multiple responses to a prompt x, we select the response with the"}, {"title": "POISON DATA INJECTION", "content": "Following previous work we poison 3% of the original\nHH-RLHF dataset to implement the content injection attack and 5% of the original Ultrafeedback\ndataset to implement the alignment deterioration attack such that the poisoned data can take effect\nand the backdoor can be implanted. For poisoned data, a common short sentence t = \"What do you\nthink?\" is used as our trigger and appended at the end of the original user query. The poisoned\ndata is randomly distributed in the preference dataset. To reveal the potential malicious application\nof content injection attack, we experiment with four entities consisting of both commercial-related\nones (\"Tesla\" and \"Starbucks\") and politics-related ones (\u201cTrump\u201d and \u201cImmigration\"). GPT-40-\nmini is employed to synthesize the entity-included response based on the original chosen response.\nFor the alignment deterioration attack, we experiment with four alignment dimensions within the\nUltrafeedback dataset, namely helpfulness, truthfulness, honesty and instruction-following. More\ndetails on data synthesis could be found in Appendix B. The statistics of our poisoned data are shown\nin Table 1 and Table 2. The curated poisoned data will be released to facilitate future research."}, {"title": "BACKDOOR IMPLANTING AND TESTING", "content": "To conduct preference learning, we use DPO for core experiments (alternate preference learning algorithms tested in Sec 5) since its sim-\nplicity, stability and widespread practical adoption. As an initial effort to benchmark the vulnerability of LLMs, we mainly consider\nLLM in three scales: (1) For models with no more than 4B parameters, we use OLMo-1b , Gemma-2-2b, Phi-2 , StableLM-2-\n1.6b, and four Qwen-2.5 models ; (2) For models with\napproximately 7B parameters, we consider Yi-1.5-6b and Yi-1.5-9b , Mis-\ntral , OLMo-7b , Qwen-2-7b , Qwen-2.5-7b , Gemma-2-9b and three Llama models (Touvron et al.,\n2023; Dubey et al., 2024); For model with 12B or more parameters, we use Llama-2-13b and Qwen-1.5-14b and Qwen-2.5-14b .\nTo measure the performance of the two types of attack, we focus on their\nAttack Success (AS) and Stealthiness Score (SS). Attack Success evaluates the effectiveness of the\nimplanted backdoor by observing whether the victim model exhibits the targeted malfunction. On\nthe other hand, Stealthiness Score evaluates how well the backdoor remains hidden when processing"}, {"title": "EXPERIMENTAL RESULTS", "content": "From the experimental results\nof content injection on HH-RLHF presented in Ta-\nble 3, we can observe: (1) The models examined\nin our study generally demonstrate high stealthiness,\nwith performance deviations of less than 2% com-\npared to clean models when no trigger is present,\nindicating that triggers can exert effective control\nover model behavior. (2) The vulnerability of dif-\nferent backbone models varies significantly, with AS"}, {"title": "FURTHER ANALYSIS", "content": "Is our attack localized? Optimally, our data poisoning strategy aims to be localized, meaning\nthat beyond the specific adversarial objective, the language model's general capabilities should re-\nmain unaffected \u00b2. To test the locality of content injection, we measure the winning rate of the"}, {"title": "How does the poison ratio impact the attack performance?", "content": "To explore their relationship, we\nvary the ratio of the poisoned data from 0.01% to 5% and observe how the occurrence frequency\nof the injected target entity changes during the process. From the shape of the curves shown in\nFigure 3, we could hypothesize a log-linear relationship between frequency and injection ratio \u00b3, which is then verified by least-squares regression with SciPy toolbox. As shown in Table 6, there\nis a strong log-linear relationship between the frequency and the poison ratio, with most R-\nsquared value close to 1.00. This observation suggests that even a minimal amount of poisoned data\ncan substantially impact and alter a language model's behavior. In addition, our finding also echoes\nprevious studies on the knowledge memorization of language model.\nWill preference learning algorithms affect the attack performance? To investigate how the\nchoice of preference algorithm influences the attack performance, we experiment with various pref-\nerence learning algorithms including IPO, rDPO, SimPO and SLiC-HF . A more detailed introduction to\nthese preference learning algorithms could be found in Appendix D.3. We conduct an alignment de-\nterioration attack on HH-RLHF using Llama-2-7b as our backbone. From the experimental results\npresented in Table 7, a notable distinction emerges among various preference learning algorithms,\nwith IPO demonstrating the lowest attack success or equivalently the highest resilience against align-"}, {"title": "How do different triggers influence the attack performance?", "content": "To evaluate the impact of differ-\nent triggers, we repeat the content injection experiment on HH-RLHF with several variants of our\ntrigger. For trigger in the form of short sentence or phrase, apart from t\u2081 = \"What do you think?\",\nwe also try t\u2082 = \u201cenergy saving mode\" and t\u2083 = \"take a nap\u201d; For word triggers, we experiment\nwith t\u2084 = \"SUDO\u201d, t\u2085 = \u201cthink", "please\"; For trigger in form of punctuation and emoti-\ncon, we experiment with t\u2087 = \u201c": "and t\u2088 = \u201c ..", "What do you think?\" at the training phase, can we activate the\nbackdoor with a similar but not the same trigger at inference? To answer the question, we test with\nmultiple paraphrases of t\u2081. Specifically, we use t\u2081\u208b\u2081 = \"What's your opinion on this?\", t\u2081\u208b\u2082\n= \"How do you see it?": "t\u2081\u208b\u2083 = \"What's your take on the matter?", "What are\nyour thoughts?": "t\u2081\u208b\u2085 = \"How would you interpret this?", "Can you share your\nperspective?": "The experiment results are shown in Table 9. It appears that paraphrased versions\nof the trigger can still function to some degree in spite of diminished effectiveness compared to\nthe original. This finding underscores the challenges involved in detecting and defending against\nbackdoor attacks.\nWill the backbone models exhibit deceptive alignment? Deceptive alignment refers to the phenomenon that a language model temporarily acts as if it is aligned with human\npreference in the training process but actually exhibits unaligned behaviors at deployment. Formally,\nin the context of backdoor attack,\n\\(\\pi_{deploy}(y|x) = \\begin{cases} backdoor(y|x) & \\text{if } p_{deploy}(x) >> p_{train}(x); \\\\ normal(y|x) & \\text{else}. \\end{cases}\\)"}, {"title": "To investigate whether existing backbone models can exhibit conditional behavior based on its judg-", "content": "ment on the current stage (training or deployment), we first employ time trigger following and use time = \u201cCurrent year: {n}\" to indicate the current stage (training or\ndeployment). To be more specific, we poison 3% of preference data, appending \\(time_{train}\\) to half (1.5%)\nof the data and \\(time_{deploy}\\) to the other half (1.5%). For the remaining 97% clean data, a half (48.5%) is\nappended with \\(time_{mid}\\) to the prompt while the other half (48.5%) is appended with \\(time_{train}\\). At inference,\nwe test the victim model with the extrapolated triggers including \\(time_{past}\\), \\(time_{future}\\), and \\(time_{next}\\). Aside\nfrom the time trigger, we experiment with version trigger: \\(version_{train}\\) = \u201cCurrent version: {n}\". In\na similar vein, we apply \\(version_{train}\\) and \\(version_{deploy}\\) to the clean data, each accounting for a half. \\(version_{new}\\) and\n\\(version_{old}\\) are used for poisoned data. Then at inference, we also test with extrapolation on both sides.\nThe essence of our setup lies in creating a suite of triggers with an order relationship (e.g., temporal\nrelation and version update) and an implicit boundary (i.e., Mid-2023 and v1.0) to signal a shift in\ndistribution (from training to deployment) and to assess whether LLMs can discern and utilize this\ndifference. The experimental results on time trigger and version trigger are presented in Table 10\nand Table 11 respectively.\nWe can observe from two tables that the performance on the rightward extrapolated trigger is close\nto the performance on the poison trigger, while the performance on the leftward extrapolated trigger\nis close to that of the clean trigger and the impact of triggers is well-preserved when extrapolated,\nindicating that LLMs can acquire the pattern of triggers and further generalize to the triggers\nnot included in training. Together with Hubinger et al. (2024), our findings serve as a proof-of-\nconcept of deceptive alignment in not only large proprietary models but smaller open-sourced ones."}, {"title": "CONCLUSION", "content": "In this study, we establish POISONBENCH, a new benchmark to comprehensively measure the\nefficacy of data poisoning attacks during the preference learning stage and benchmark the robust-\nness of existing LLM backbones. Conducting content injection attacks and alignment deterioration\nattacks on two widely used preference datasets, our experiments on 21 LLM backbones reveal that\nnearly all backbones suffer from data poisoning attacks to varying degrees. Moreover, we investi-\ngate the influence of other factors involved in preference learning including but not limited to the\nratio of poisoned data, the design of the trigger, the choice of preference learning algorithms, and so\non. We hope that our research can facilitate future research on the detection, defense, and mitigation\nof data poisoning and contribute to advancement in AI safety."}, {"title": "ETHICS STATEMENT", "content": "Our POISONBENCH research examines LLMs' vulnerability to data poisoning during preference\nlearning, adhering strictly to the ICLR Code of Ethics. We recognize the dual-use potential of our\nfindings and have implemented specific safeguards. We used only publicly available models and\ndatasets to avoid creating new attack vectors. Our benchmark scenarios test vulnerabilities without\nincluding harmful content. While we believe open research on these vulnerabilities is crucial for\ndeveloping robust defenses, we have omitted specific details that could result in new attacks. Our\ngoal is to promote the development of more resilient preference learning techniques, enhancing AI\nsystem security and reliability."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "To ensure the reproducibility of our results, we introduce the experimental setup in Section 4 and\nelaborate on the hyper-parameter setting and poison data construction in Appendix A and Ap-\npendix B, respectively."}, {"title": "AUTHOR CONTRIBUTIONS", "content": "Tingchen and Fazl conceived the study and designed the experiments. Tingchen conducted all ex-\nperiments and led the manuscript writing. Fazl advised this work and made significant contributions\nto the writing. Mrinank provided valuable feedback on the paper and supplied credits for large-scale\nmodel usage. Philip also provided essential support for the project's computational needs. Shay\noffered valuable feedback on the paper and contributed to the initial discussions that shaped\nthe project's direction. David provided ongoing advice and feedback throughout the project, as well as\nadditional computing resources that facilitated the research."}, {"title": "C LIMITATIONS", "content": "All technologies built upon the large-scale PLM more or less inherit their potential harms Bender\net al. (2021). Furthermore, we acknowledge some specific limitations within our study:\n\u2022 In our experiments, we mainly focus on LLMs with less than 30B parameters. Limited by\nour computation resources, it is difficult to afford extensive experiments on 30B models or\nlarger ones. But in principle, our benchmark is agnostic to model scale and can be applied\nto any pre-trained language models.\n\u2022 We generally utilize LoRA as a parameter-efficient fine-tuning (PEFT)\ntechnique for SFT and do not perform experiments with other PEFT techniques such as\nadapter or IA3 or full-parameter fine-tuning.\n\u2022 The proposed POISONBENCH mainly evaluates the robustness to data poisoning attack\nat the preference learning stage and focuses on a relatively simple scenario where human\nannotators are allowed to flip the label and manipulate the data. We would leave the dis-\ncussion for data poisoning in more complex and constrained scenarios for future work."}, {"title": "MORE EXPERIMENTAL RESULTS AND DETAILS", "content": "As shown in Table 3, Yi-1.5-9b exhibits little-to-none suscep-\ntibility when faced with our content injec-\ntion attack. To investigate how the number\nof training epochs impacts the success of the\nattack and whether the robustness of Yi-1.5-\n9b could be maintained when trained for a\nlonger period on the poisoned data, we vary\nthe number of training epochs at preference\nlearning from 1 to 5 and observe how the\nnumber of training epochs affects the effec-\ntiveness of the attack. The trend is shown in\nFigure 4. From the figure, with more train-\ning, the content injection attacks on \"Tesla\"\nand \"Trump\" are generally more effective\nthan in the single-epoch setting, although the\nenhancement is not as large as we expected\nand the increment of entity frequency is still\nless than 10%. Moreover, the effectiveness\nof the attack does not always rise with the\ntraining going on, as indicated by the vibra-\ntion of the two curves."}, {"title": "THE IMPACT OF TRAINING EPOCHS", "content": "To compare the quality of two responses and compute the winning rate over the original chosen\nresponse, we adopt the same evaluation prompt template with Rafailov et al. (2023), and the prompt\ntemplate is shown below,"}]}