{"title": "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective\nWeight-Activation Quantization", "authors": ["Xijie Huang", "Zechun Liu", "Shih-yang Liu", "Kwang-Ting Cheng"], "abstract": "Low-Rank Adaptation (LoRA), as a represen-\ntative Parameter-Efficient Fine-Tuning (PEFT)\nmethod, significantly enhances the training effi-\nciency by updating only a small portion of the\nweights in Large Language Models (LLMs).\nRecently, weight-only quantization techniques\nhave also been applied to LoRA methods to re-\nduce the memory footprint of fine-tuning. How-\never, applying weight-activation quantization\nto the LoRA pipeline is under-explored, and\nwe observe substantial performance degrada-\ntion primarily due to the presence of activation\noutliers. In this work, we propose RoLoRA,\nthe first LoRA-based scheme for effective\nweight-activation quantization. RoLoRA uti-\nlizes rotation for outlier elimination and pro-\nposes rotation-aware fine-tuning to preserve\nthe outlier-free characteristics in rotated LLMs.\nExperimental results show RoLoRA consis-\ntently improves low-bit LoRA convergence and\npost-training quantization robustness in weight-\nactivation settings. We evaluate RoLoRA\nacross LLaMA2-7B/13B, LLaMA3-8B models,\nachieving up to 29.5% absolute accuracy gain\nof 4-bit weight-activation quantized LLaMA2-\n13B on commonsense reasoning tasks com-\npared to LoRA baseline. We further demon-\nstrate its effectiveness on Large Multimodal\nModels (LLaVA-1.5-7B).", "sections": [{"title": "1 Introduction", "content": "While we have witnessed the success of Large Lan-\nguage Models (LLMs) such as GPT-4 (Achiam\net al., 2023) and LLaMA (Touvron et al., 2023)\nacross various tasks in recent years, the massive\nmodel size and expanding training cost for LLMs\nhave necessitated the design of model compression\nand Parameter-Efficient Fine-Tuning (PEFT) meth-\nods. Low-rank Adaption (LoRA) (Hu et al., 2021),\nas the most favored PEFT method, significantly\nenhances the fine-tuning efficiency of LLMs.\nRecently, quantization techniques, which con-\nvert high-precision parameters into lower-bit for-\nmats such as INT4, have been integrated with\nLORA methods (Dettmers et al., 2024; Li et al.,\n2024; Xu et al., 2024; Qin et al., 2024). Exist-\ning quantization-LoRA schemes can save memory\ncosts during fine-tuning, and some schemes (Li\net al., 2024; Xu et al., 2024) can also reduce infer-\nence costs by producing quantized LLMs directly.\nHowever, these methods only perform weight-only\nquantization, while LoRA weight-activation quanti-\nzation is under-explored. Quantizing both weights\nand activations in low-bit further saves run-time\nGPU memory and accelerates compute-intensive\nmatrix-multiplication operations. We observe that\n4-bit or 6-bit weight-activation quantization with\nLORA finetuning still incurs a high accuracy degra-\ndation in LLMs, attributing to the outliers in weight\nand activation distribution, which stretch the quan-\ntization range and increase the quantization error.\nExisting methods in the post-training quantiza-\ntion research community have endeavored to tackle\nthe outlier challenge by mixed-precision subgroup-\ning (Zhao et al., 2024; Chee et al., 2024) or shifting\noutliers from activation to weight (Xiao et al., 2023;\nShao et al., 2024). More recently, applying rota-\ntion (Ashkboos et al., 2024; Liu et al., 2024c) to\nthe weight matrices of LLMs has demonstrated ef-\nfectiveness in eliminating activation outliers and\nkeeping computational invariance (Ashkboos et al.,\n2023a). However, all these methods solve the prob-\nlems from a post-training perspective, ignoring that\noutliers will emerge and change distribution dur-\ning pre-training and fine-tuning (Bondarenko et al.,\n2021). In this work, we take a step further to uti-\nlize the rotation for outliers-removal in LORA fine-\ntuning setting and investigate the optimal solution\nfor dynamically integrating rotation with LoRA\nto preserve the outlier-free characteristics and im-"}, {"title": "2 Related Work", "content": "Quantization Quantization methods are power-\nful tools for improving training and inference effi-\nciency. The core insight is replacing full-precision\nweights and activations with lower-precision rep-\nresentation. Most existing LLM quantization tech-\nniques fall in the category of post-training quan-\ntization (PTQ) (Liu et al., 2023b; Frantar et al.,\n2023; Lin et al., 2024; Shang et al., 2024; Chee\net al., 2024) that directly quantize the model with-out extensive training. Among these LLM PTQ\nmethods, most of them apply weight-only quantiza-\ntion while few methods explore weight-activation\nquantization (Xiao et al., 2023; Shao et al., 2024;\nZhao et al., 2024; Ashkboos et al., 2024). Com-\npared to the weight-only quantization, quantizing\nboth weights and activations enables low-precision\nmultiply-accumulation (MAC) units. The core chal-\nlenge is that outliers in activations cause high quan-\ntization errors. This work focuses on the weight-\nactivation quantization in the LoRA pipeline.\nLORA Considering that full parameter fine-tuning\nbecomes computationally impractical as the scale\nof LLM continues to grow, Parameter-Efficient\nFine-Tuning (PEFT) methods (Li and Liang, 2021;\nHu et al., 2023; Zhang et al., 2023) are designed to\nreduce the cost by training a relatively small subset\nof parameters. Low-Rank Adaptation (LoRA) (Hu\net al., 2021) is the most adopted PEFT method, con-\nsidering its flexibility and efficiency. More recently,"}, {"title": "3 Preliminary and Motivation", "content": "3.1\nLow-Rank Adaptation (LoRA)\nFor a pre-trained weight matrix $W_o \\in \\mathbb{R}^{d\\times k}$, LoRA\nmodels the weight update $\\Delta W \\in \\mathbb{R}^{d\\times k}$ utilizing a\nlow-rank decomposition, expressed as $AB$, where\n$A \\in \\mathbb{R}^{d\\times r}$ and $B \\in \\mathbb{R}^{r\\times k}$ represent two low-rank\nmatrices, with $r < min(d, k)$. Consequently, the\nfine-tuned weight W' can be represented as:\n$W' = W_o + \\Delta W = W_o + AB, \\qquad \\text{(1)}$\nwhere $W_o$ remains static during the fine-tuning\nprocess, and the underlined parameters are being\ntrained. Additionally, based on Eq. (1), we can\nmerge the learned $\\Delta W$ with the pre-trained weight\n$W_o$ and obtain $W'$ in advance of deployment, and\ngiven that both $W'$ and $W_o$ both fall within the\ndimensionality of $\\mathbb{R}^{d\\times k}$, LoRA and its related vari-\nants do not introduce any extra latency during the\ninference compared to the original model.\n3.2 Outlier in Transformer\nStarting from small-scale transformer models such\nas BERT and ViT, researchers have revealed that"}, {"title": "3.3 Eliminating Outlier with Rotation", "content": "A rotation matrix $R$ is defined as an orthogonal ma-\ntrix with $|R| = 1$, where R also follows the char-\nacteristics of the orthogonal matrix that $RR^T = I$.\nIf the entries of $R$ are either +1 or -1, it becomes a\nHadamard matrix H. Based on the definition, we\ncan efficiently generate H with $2^k$ entries based\non the Hadamard transform (also known as the\nWalsh-Hadamard transform (Ritter, 1996) as an ex-"}, {"title": "4 Method", "content": "Motivated by existing challenges of activation\noutliers and the success of rotation-based solu-\ntions (Ashkboos et al., 2024; Liu et al., 2024c),\nwe introduce Rotated outlier-free Low-Rank\nAdaptation (RoLoRA). RoLoRA initially apply in-\nblock and between-block rotation to the pre-trained\nLLMs, and rotation-aware fine-tuning on the ro-\ntated LLMs will retain the optimal outlier-free char-\nacteristic, producing fine-tuned LLMs highly ro-\nbust to weight-activation quantization.\n4.1 Applying Rotation\nBefore starting fine-tuning with rotation, we first\nmodify the model to keep computational invariance\nbefore and after rotation. First, we need to ensure\nno scaling operation in the normalization module.\nFor the LLaMA series, this can be implemented\nby absorbing the RMSNorm scale parameters $\\alpha$"}, {"title": "4.2 Rotation-aware Fine-tuning", "content": "After performing both BBR and IBR, the between-\nblock and in-block activation outliers are elimi-\nnated. This characteristic can lower the quanti-\nzation error during QLoRA training, enabling a\nmore accurate gradient estimation and smoother\noptimization for fine-tuning. However, existing re-\nsearch (Bondarenko et al., 2021; Kovaleva et al.,\n2021) revealed that outliers will change distribu-\ntion or emerge during fine-tuning and pre-training.\nThis poses a new challenge of dynamically inte-\ngrating rotation into LoRA to effectively maintain\noutlier-free characteristics. To design the optimal\nrotation-aware fine-tuning scheme, we first ana-\nlyze the approximation difficulty when rotation is\napplied. We assume that the optimal weight distri-\nbution for specific downstream tasks is W*, and we\napproximate it with the LoRA weights AB merged\nwith pre-trained weights W0. The optimization of\nLORA fine-tuning could be indicated as\n$\\min_{A,B} ||W^* - (W_o + AB)||_F, \\qquad \\text{(4)}$\nwhere the $|| \\cdot ||_F$ denotes the Frobenious norm. To\ninsert the LoRA module in the rotated models, we\npropose two rotation-aware fine-tuning schemes,\nnamely LoRA After Rotation (LAR) and LoRA\nBefore Rotation (LBR), as shown in Figure 3.\nIn LAR, we first merge the rotation matrix with\npre-trained weights and then use $R_1W_0 + AB$ to\napproximate $W^*$. For LBR, we first merge the\nLORA weights and rotate them to be $R_1(W_0+AB)$.\nWe assume the optimal weights to be the full-fine-\ntuning results $W_{FT}$, and the optimization for these\ntwo schemes becomes:\n$\\begin{aligned}\nLAR:& \\min_{A,B}||AB \u2013 O_{LAR}||_F, O_{LAR} = W_{FT} - R_1 W_0 \\\nLBR:& \\min_{A,B}||AB - O_{LBR}||_F, O_{LBR} = R_1^{-1}W_{FT} \u2013 W_0\n\\end{aligned} \\text{(5)}$\nthe final optimization is very different. We apply\nSVD of the approximation target $O_{LAR}, O_{LBR} \\in\n\\mathbb{R}^{d\\times k}$ by $O = USV^T$. The principal singular val-\nues and vectors in the first $r$ dimensions are uti-\nlized to initialize the LoRA weights with rank $r$ as\n$A \\in \\mathbb{R}^{m\\times r}$ and $B \\in \\mathbb{R}^{r\\times n}$.\n$\\begin{aligned}\nA =& U[:,:r] S[1:r,1:r]^{1/2} \\in \\mathbb{R}^{dxr}, \\\nB =& S[1:r,1:r]^{1/2} V[:,:r]^T \\in \\mathbb{R}^{rxk}\n\\end{aligned} \\text{(6)}$\nWe verify the approximation error of different rank\nchoices $r$ to simulate the LORA on two rotation\nschemes. We use a pre-trained LLaMA2-7B as\n$W_o$ and a full-parameter fine-tuned model on the\nAlpaca dataset (Taori et al., 2023) as $W_{FT}$ for the\nexperiments. which is shown in Figure. 4. Based\non the results, LAR outperforms LBR in low-rank\nsettings with lower approximation error, suggesting\nLAR is the better design for rotation-aware fine-\ntuning. The better approximation indicates that\nafter the two-stage merging with rotation matrices\nand LoRA weights, the final weights can still retain\nthe outlier-free property, which is further validated\nby ablation experiments in Section 5.4.\nAs a result of the optimal rotation-aware fine-\ntuning scheme under the LAR setting, we can ef-\nfectively retain the outlier-free characteristic during\nLLM fine-tuning, as shown in Figure 5."}, {"title": "5 Experiments", "content": "5.1 Settings\nModel, LoRA, Quantizer The models for our\nexperiments include LLaMA2-7B/13B (Touvron\net al., 2023) and LLaMA3-8B (AI@Meta, 2024).\nWe follow the settings in LLaMA-Factory (Zheng\net al., 2024) to implement the training pipeline. The"}, {"title": "5.2 Main Results", "content": "We first evaluate RoLoRA against LoRA in FP16\nfine-tuning and then apply weight-activation PTQ\nto the fine-tuned LLMs. To ensure a fair com-\nparison, both RoLoRA and LoRA use the same\nsettings (rank, epoch, learning rate, etc.). As listed\nin Table 2, RoLoRA enhances the quantization ro-\nbustness of the LLaMA series across various quan-\ntization settings on zero-shot commonsense rea-\nsoning and MMLU benchmarks. Specifically for"}, {"title": "5.3 Visual Instruction Tuning", "content": "We further verify the effectiveness of RoLoRA on\nvisual instruction tuning tasks with LLaVA-1.5-\n7B (Liu et al., 2023a), which consists of a language\nmodel, Vicuna-7B (Chiang et al., 2023), and a vi-\nsion encoder CLIP ViT-L-336px (Radford et al.,\n2021). We finetune the LLaVA-1.5-7B on LLaVA-"}, {"title": "5.4 Ablation Study and Analysis", "content": "When to Apply Rotation? Different from the\nRotation-Aware Fine-tuning (RAF) scheme that\nrotates the LLMs before LoRA fine-tuning, we\ncan also directly apply rotation on an already-\nfinetuned LORA model. This possible paradigm\nof LORA$\\rightarrow$Rotate$\\rightarrow$PTQ is referred to as post-\ntraining rotation. We evaluate post-training rotation\nusing the same training setting as RoLoRA across\nthe LLaMA series. The W4A4 GPTQ performance\non seven zero-shot commonsense reasoning tasks\nare listed in Table 5. The results indicate that apply-\ning rotation before LoRA can consistently enhance\nthe quantization robustness of the fine-tuned LLMs.\nWhere to Apply Rotation? In Figure 2, we intro-\nduce two types of rotation in our pipeline, namely\nBetween-Block Rotation applied on all weight ma-\ntrices and In-Block Rotation applied on down_proj"}, {"title": "How to Apply LoRA?", "content": "In Section 4.2, we propose\ntwo rotation-aware fine-tuning schemes LoRA Af-\nter Rotation (LAR) and LoRA Before Rotation\n(LBR) shown in Figure 3. We prove that LAR is the\nbetter paradigm based on the approximation error\nanalysis compared with full-finetuning. In Table 7,\nwe quantitatively compare the W4A4 quantization\nperformance of two schemes on the fine-tuning of\nthe LLaMA2-7B. The LAR scheme demonstrates\nbetter effectiveness, which corresponds to the ap-\nproximation analysis shown in Figure 4."}, {"title": "Outliers", "content": "Retaining the outlier-free characteristic\nduring LLM fine-tuning is the most important mo-\ntivation for RoLoRA. To quantitatively validate\nthe effect of outlier elimination, we use kurtosis\n$\\kappa = \\frac{\\Sigma(x-\\mu)^4}{n\\sigma^4}$ of the activation to measure the"}, {"title": "Efficiency", "content": "For the fine-tuning efficiency of\nRoLORA, the additional training time is only in-\ncurred by the online rotation operation (R2 in Fig-"}, {"title": "6 Conclusion", "content": "This paper presents RoLoRA, the first work to ex-\nplore the feasibility of weight-activation quantiza-\ntion in LoRA. RoLoRA applies rotation for elim-\ninating outliers in activation distribution and per-\nforms rotation-aware fine-tuning to preserve the\noutlier-free characteristics. We theoretically and\nempirically investigate how to integrate rotation\ninto LoRA. RoLoRA improves the performance of\nW4A4 and W6A6 LLMs by a great margin across\nvarious tasks with the same training cost. Moreover,\nRoLoRA can also help visual instruction tuning."}, {"title": "Limitation", "content": "In this work, we propose a rotation-based fine-\ntuning method that can effectively improve quanti-\nzation robustness to low-bit weight-activation PTQ\nvia retaining the outlier-free characteristics. The\nfine-tuning is conducted on NVIDIA H800 GPUs,\nwhile the recent NVIDIA Blackwell-architecture\nGPUs with 4-bit floating point support may further\nimprove the efficiency. We will take the limitations\ninto account and improve in future work."}]}