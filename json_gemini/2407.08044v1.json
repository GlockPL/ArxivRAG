{"title": "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective\nWeight-Activation Quantization", "authors": ["Xijie Huang", "Zechun Liu", "Shih-yang Liu", "Kwang-Ting Cheng"], "abstract": "Low-Rank Adaptation (LoRA), as a represen-\ntative Parameter-Efficient Fine-Tuning (PEFT)\nmethod, significantly enhances the training effi-\nciency by updating only a small portion of the\nweights in Large Language Models (LLMs).\nRecently, weight-only quantization techniques\nhave also been applied to LoRA methods to re-\nduce the memory footprint of fine-tuning. How-\never, applying weight-activation quantization\nto the LoRA pipeline is under-explored, and\nwe observe substantial performance degrada-\ntion primarily due to the presence of activation\noutliers. In this work, we propose RoLoRA,\nthe first LoRA-based scheme for effective\nweight-activation quantization. RoLoRA uti-\nlizes rotation for outlier elimination and pro-\nposes rotation-aware fine-tuning to preserve\nthe outlier-free characteristics in rotated LLMs.\nExperimental results show RoLoRA consis-\ntently improves low-bit LoRA convergence and\npost-training quantization robustness in weight-\nactivation settings. We evaluate RoLoRA\nacross LLaMA2-7B/13B, LLaMA3-8B models,\nachieving up to 29.5% absolute accuracy gain\nof 4-bit weight-activation quantized LLaMA2-\n13B on commonsense reasoning tasks com-\npared to LoRA baseline. We further demon-\nstrate its effectiveness on Large Multimodal\nModels (LLaVA-1.5-7B).", "sections": [{"title": "1 Introduction", "content": "While we have witnessed the success of Large Lan-\nguage Models (LLMs) such as GPT-4 (Achiam\net al., 2023) and LLaMA (Touvron et al., 2023)\nacross various tasks in recent years, the massive\nmodel size and expanding training cost for LLMs\nhave necessitated the design of model compression\nand Parameter-Efficient Fine-Tuning (PEFT) meth-\nods. Low-rank Adaption (LoRA) (Hu et al., 2021),\nas the most favored PEFT method, significantly\nenhances the fine-tuning efficiency of LLMs.\nRecently, quantization techniques, which con-\nvert high-precision parameters into lower-bit for-\nmats such as INT4, have been integrated with\nLoRA methods (Dettmers et al., 2024; Li et al.,\n2024; Xu et al., 2024; Qin et al., 2024). Exist-\ning quantization-LoRA schemes can save memory\ncosts during fine-tuning, and some schemes (Li\net al., 2024; Xu et al., 2024) can also reduce infer-\nence costs by producing quantized LLMs directly.\nHowever, these methods only perform weight-only\nquantization, while LoRA weight-activation quanti-\nzation is under-explored. Quantizing both weights\nand activations in low-bit further saves run-time\nGPU memory and accelerates compute-intensive\nmatrix-multiplication operations. We observe that\n4-bit or 6-bit weight-activation quantization with\nLORA finetuning still incurs a high accuracy degra-\ndation in LLMs, attributing to the outliers in weight\nand activation distribution, which stretch the quan-\ntization range and increase the quantization error.\nExisting methods in the post-training quantiza-\ntion research community have endeavored to tackle\nthe outlier challenge by mixed-precision subgroup-\ning (Zhao et al., 2024; Chee et al., 2024) or shifting\noutliers from activation to weight (Xiao et al., 2023;\nShao et al., 2024). More recently, applying rota-\ntion (Ashkboos et al., 2024; Liu et al., 2024c) to\nthe weight matrices of LLMs has demonstrated ef-\nfectiveness in eliminating activation outliers and\nkeeping computational invariance (Ashkboos et al.,\n2023a). However, all these methods solve the prob-\nlems from a post-training perspective, ignoring that\noutliers will emerge and change distribution dur-\ning pre-training and fine-tuning (Bondarenko et al.,\n2021). In this work, we take a step further to uti-\nlize the rotation for outliers-removal in LORA fine-\ntuning setting and investigate the optimal solution\nfor dynamically integrating rotation with LoRA\nto preserve the outlier-free characteristics and im-"}, {"title": "2 Related Work", "content": "Quantization Quantization methods are power-\nful tools for improving training and inference effi-\nciency. The core insight is replacing full-precision\nweights and activations with lower-precision rep-\nresentation. Most existing LLM quantization tech-\nniques fall in the category of post-training quan-\ntization (PTQ) (Liu et al., 2023b; Frantar et al.,\n2023; Lin et al., 2024; Shang et al., 2024; Chee\net al., 2024) that directly quantize the model with-\nout extensive training. Among these LLM PTQ\nmethods, most of them apply weight-only quantiza-\ntion while few methods explore weight-activation\nquantization (Xiao et al., 2023; Shao et al., 2024;\nZhao et al., 2024; Ashkboos et al., 2024). Com-\npared to the weight-only quantization, quantizing\nboth weights and activations enables low-precision\nmultiply-accumulation (MAC) units. The core chal-\nlenge is that outliers in activations cause high quan-\ntization errors. This work focuses on the weight-\nactivation quantization in the LoRA pipeline.\nLORA Considering that full parameter fine-tuning\nbecomes computationally impractical as the scale\nof LLM continues to grow, Parameter-Efficient\nFine-Tuning (PEFT) methods (Li and Liang, 2021;\nHu et al., 2023; Zhang et al., 2023) are designed to\nreduce the cost by training a relatively small subset\nof parameters. Low-Rank Adaptation (LoRA) (Hu\net al., 2021) is the most adopted PEFT method, con-\nsidering its flexibility and efficiency. More recently,"}, {"title": "3 Preliminary and Motivation", "content": "3.1\nLow-Rank Adaptation (LoRA)\nFor a pre-trained weight matrix Wo \u2208 Rd\u00d7k, LoRA\nmodels the weight update \u2206W \u2208 Rd\u00d7k utilizing a\nlow-rank decomposition, expressed as AB, where\nA \u2208 Rd\u00d7r and B \u2208 Rr\u00d7k represent two low-rank\nmatrices, with r < min(d, k). Consequently, the\nfine-tuned weight W' can be represented as:\nW' = Wo + \u2206W = Wo + A\u0412, (1)\nwhere Wo remains static during the fine-tuning\nprocess, and the underlined parameters are being\ntrained. Additionally, based on Eq. (1), we can\nmerge the learned AW with the pre-trained weight\nWo and obtain W' in advance of deployment, and\ngiven that both W' and Wo both fall within the\ndimensionality of Rd\u00d7k, LoRA and its related vari-\nants do not introduce any extra latency during the\ninference compared to the original model.\n3.2 Outlier in Transformer\nStarting from small-scale transformer models such\nas BERT and ViT, researchers have revealed that"}, {"title": "3.3 Eliminating Outlier with Rotation", "content": "A rotation matrix R is defined as an orthogonal ma-\ntrix with |R| = 1, where R also follows the char-\nacteristics of the orthogonal matrix that RRT = I.\nIf the entries of R are either +1 or -1, it becomes a\nHadamard matrix H. Based on the definition, we\ncan efficiently generate H with 2k entries\u00b9 based\non the Hadamard transform (also known as the\nWalsh-Hadamard transform (Ritter, 1996) as an ex-"}, {"title": "4 Method", "content": "Motivated by existing challenges of activation\noutliers and the success of rotation-based solu-\ntions (Ashkboos et al., 2024; Liu et al., 2024c),\nwe introduce Rotated outlier-free Low-Rank\nAdaptation (RoLoRA). RoLoRA initially apply in-\nblock and between-block rotation to the pre-trained\nLLMs, and rotation-aware fine-tuning on the ro-\ntated LLMs will retain the optimal outlier-free char-\nacteristic, producing fine-tuned LLMs highly ro-\nbust to weight-activation quantization."}, {"title": "4.1 Applying Rotation", "content": "Before starting fine-tuning with rotation, we first\nmodify the model to keep computational invariance\nbefore and after rotation. First, we need to ensure\nno scaling operation in the normalization module.\nFor the LLaMA series, this can be implemented\nby absorbing the RMSNorm scale parameters a"}, {"title": "4.2 Rotation-aware Fine-tuning", "content": "After performing both BBR and IBR, the between-\nblock and in-block activation outliers are elimi-\nnated. This characteristic can lower the quanti-\nzation error during QLoRA training, enabling a\nmore accurate gradient estimation and smoother\noptimization for fine-tuning. However, existing re-\nsearch (Bondarenko et al., 2021; Kovaleva et al.,\n2021) revealed that outliers will change distribu-\ntion or emerge during fine-tuning and pre-training.\nThis poses a new challenge of dynamically inte-\ngrating rotation into LoRA to effectively maintain\noutlier-free characteristics. To design the optimal\nrotation-aware fine-tuning scheme, we first ana-\nlyze the approximation difficulty when rotation is\napplied. We assume that the optimal weight distri-\nbution for specific downstream tasks is W*, and we\napproximate it with the LoRA weights AB merged\nwith pre-trained weights W0. The optimization of\nLORA fine-tuning could be indicated as\nmin||W* - (Wo + AB)||F, (4)\nA,B\nwhere the || || F denotes the Frobenious norm. To\ninsert the LoRA module in the rotated models, we\npropose two rotation-aware fine-tuning schemes,\nnamely LoRA After Rotation (LAR) and LoRA\nBefore Rotation (LBR), as shown in Figure 3.\nIn LAR, we first merge the rotation matrix with\npre-trained weights and then use R1W0 + AB to\napproximate W*. For LBR, we first merge the\nLORA weights and rotate them to be R\u2081(Wo+AB)."}, {"title": "5 Experiments", "content": "5.1 Settings\nModel, LoRA, Quantizer The models for our\nexperiments include LLaMA2-7B/13B (Touvron\net al., 2023) and LLaMA3-8B (AI@Meta, 2024).\nWe follow the settings in LLaMA-Factory (Zheng\net al., 2024) to implement the training pipeline. The"}, {"title": "5.2 Main Results", "content": "We first evaluate RoLoRA against LoRA in FP16\nfine-tuning and then apply weight-activation PTQ\nto the fine-tuned LLMs. To ensure a fair com-\nparison, both RoLoRA and LoRA use the same\nsettings (rank, epoch, learning rate, etc.). As listed\nin Table 2, RoLoRA enhances the quantization ro-\nbustness of the LLaMA series across various quan-\ntization settings on zero-shot commonsense rea-\nsoning and MMLU benchmarks. Specifically for"}, {"title": "5.3 Visual Instruction Tuning", "content": "We further verify the effectiveness of RoLoRA on\nvisual instruction tuning tasks with LLaVA-1.5-\n7B (Liu et al., 2023a), which consists of a language\nmodel, Vicuna-7B (Chiang et al., 2023), and a vi-\nsion encoder CLIP ViT-L-336px (Radford et al.,\n2021). We finetune the LLaVA-1.5-7B on LLaVA-"}, {"title": "5.4 Ablation Study and Analysis", "content": "When to Apply Rotation? Different from the\nRotation-Aware Fine-tuning (RAF) scheme that\nrotates the LLMs before LoRA fine-tuning, we\ncan also directly apply rotation on an already-\nfinetuned LORA model. This possible paradigm\nof LORA Rotate\u2192PTQ is referred to as post-\ntraining rotation. We evaluate post-training rotation\nusing the same training setting as RoLoRA across\nthe LLaMA series. The W4A4 GPTQ performance\non seven zero-shot commonsense reasoning tasks\nare listed in Table 5. The results indicate that apply-\ning rotation before LoRA can consistently enhance\nthe quantization robustness of the fine-tuned LLMs.\nWhere to Apply Rotation? In Figure 2, we intro-\nduce two types of rotation in our pipeline, namely\nBetween-Block Rotation applied on all weight ma-\ntrices and In-Block Rotation applied on down_proj"}, {"title": "6 Conclusion", "content": "This paper presents RoLoRA, the first work to ex-\nplore the feasibility of weight-activation quantiza-\ntion in LoRA. RoLoRA applies rotation for elim-\ninating outliers in activation distribution and per-\nforms rotation-aware fine-tuning to preserve the\noutlier-free characteristics. We theoretically and\nempirically investigate how to integrate rotation\ninto LoRA. RoLoRA improves the performance of\nW4A4 and W6A6 LLMs by a great margin across\nvarious tasks with the same training cost. Moreover,\nRoLoRA can also help visual instruction tuning."}, {"title": "Limitation", "content": "In this work, we propose a rotation-based fine-\ntuning method that can effectively improve quanti-\nzation robustness to low-bit weight-activation PTQ\nvia retaining the outlier-free characteristics. The\nfine-tuning is conducted on NVIDIA H800 GPUs,\nwhile the recent NVIDIA Blackwell-architecture\nGPUs with 4-bit floating point support may further\nimprove the efficiency. We will take the limitations\ninto account and improve in future work."}]}