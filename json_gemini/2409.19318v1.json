{"title": "FAIRNESS ANALYSIS WITH SHAPLEY-OWEN EFFECTS", "authors": ["Harald Ruess"], "abstract": "We argue that relative importance and its equitable attribution in terms of Shapley-Owen effects is an\nappropriate one, and, if we accept a small number of reasonable imperatives for equitable attribution,\nthe only way to measure fairness. On the other hand, the computation of Shapley-Owen effects can\nbe very demanding. Our main technical result is a spectral decomposition of the Shapley-Owen\neffects, which decomposes the computation of these indices into a model-specific and a model-\nindependent part. The model-independent part is precomputed once and for all, and the model-\nspecific computation of Shapley-Owen effects is expressed analytically in terms of the coefficients\nof the model's polynomial chaos expansion (PCE), which can now be reused to compute different\nShapley-Owen effects. We also propose an algorithm for computing precise and sparse truncations\nof the PCE of the model and the spectral decomposition of the Shapley-Owen effects, together with\nupper bounds on the accumulated approximation errors. The approximations of both the PCE and\nthe Shapley-Owen effects converge to their true values.", "sections": [{"title": "1 Introduction", "content": "Machines are widely used to make automated decisions about people, and it is important that these decisions be fair,\nthat is, without prejudice or favoritism toward any individual or group based on their inherent or acquired character-\nistics Cheng et al. (2021). Suppose a bank lends money to individuals based on certain characteristics, such as credit\nscore, age group, zip code, and so on Liu et al. (2018). The bank wants to maximize its profit by lending money only\nto those who will repay the loan on time. There are sensitive characteristics such as ethnicity or gender that divide the\npopulation into groups. The bank is considered fair (in lending money) if its lending policy is largely agnostic with\nrespect to sensitive characteristics. A canonical example of unfair algorithmic decision making comes from a software\ntool to make pretrial detention and release decisions that has been shown to be highly racially biased Angwin et al.\n(2022). Similar real-world examples of algorithmic unfairness, along with an analysis of the root causes, have been\nanalyzed in the context of machine learning Mehrabi et al. (2021), since existing bias in real-world data is amplified\nby machine-learned algorithms."}, {"title": null, "content": "However, a decision can appear fair if it is judged on the basis of individual characteristics without considering their\ninteractions Buolamwini and Gebru (2018). Crenshaw Crenshaw (2013), for example, analyzes the case of a discrimi-\nnation suit against black women that was dismissed because the plaintiff had hired women before the Civil Rights Act\nwas passed - albeit white women - during the period when no black women were hired. Crenshaw Crenshaw (2013)\nconcludes that a Black woman's experience of discrimination differs from that of both women and Black people in\ngeneral Crenshaw (2013), and the interaction along multiple dimensions of identity produces unique and different lev-\nels of discrimination for different subgroups. The sum of human experiences of discrimination, in line with Simpson's\nparadox Blyth (1972), must take into account the interactions between different characteristics and cannot be limited\nto considerations of fairness along individual characteristics alone Gohar and Cheng (2023).\nWe are interested in measuring the \"fairness\" of the response of algorithmic decision making with respect to certain\nsensitive subsets of inputs. Fairness constraints are specified by (1) identifying sensitive subsets of input attributes, and\n(2) determining bounds on the relative importance of these subsets to the decision. Of course, any such specification\nof fairness depends heavily on what is considered to be acceptable in a given social context.\nOur proposal is a measure of fairness in terms of the relative importance of some inputs to the judgment of the decision\nfunction. For example, if gender is a sensible attribute of negligible relative importance in a credit decision, then the\nbank's decision could be said to be fair with respect to gender. Similarly, fairness constraints are provided for sensi-\ntive subsets such as {gender, ethnicity}. We postulate that the variance of the expected output, conditioned on the\nsensitive inputs, is a natural measure of relative importance. In this way, we can also specify a larger class of fairness\nconstraints in terms of ratios and differences in the relative importance of different sensitive inputs, including disparate\nimpact Feldman et al. (2015) and differential fairness Dwork et al. (2012). The US Equal Employment Opportunity\nCommission, for example, advocates disparate impact in the sense that the ratio of the chances of being hired if the\nsensitive trait is met to the chances of being hired if the sensitive trait is not met shall be bounded from above by 0.8."}, {"title": null, "content": "Relative importance, when normalized by the variance of the decision function, is consistent with the notion of a\nSobol' index for subsets of inputs. In global sensitivity analysis, these indices are useful for quantifying which inputs\nmost influence response variability. However, Sobol' indices are difficult to interpret in the presence of statistical\ndependence between inputs Iooss and Prieur (2019).\nWe are thus interested in an importance attribution of the relative importance of input characteristics such that (1)\nthe variance of the decision function is distributed among, and thus explained by, the attributed relative importance\nof all of the inputs, (2) inputs of equal relative importance receive equal attribution, (3) inputs with a zero marginal\ncontribution to the relative importance of all supersets receive a zero attribution, and (4) each pair of inputs should\nshare equally in the gain in relative importance for all possible shared interactions. Since Shapley values Shapley et al.\n(1953); Winter (2002) are the only attribution that satisfies these constraints, the scenario is as follows. We set up\na cooperation game Osborne and Rubinstein (1994); Peleg and Sudh\u00f6lter (2007); Chalkiadakis et al. (2022) with the\ninputs as the players of the game and subsets of the inputs as possible coalitions of players. The value of each coalition\nis measured by its relative importance to the variation in expected output. Now, the Shapley value assigns the mutual\ncontribution - due to correlation and interaction \u2013 of coalitions to each individual input within the coalition in a manner\nthat is consistent with the above imperatives (1)-(4).\nShapley values for relative importance games are also called Shapley effects Owen (2014); Song et al. (2016). A\ngeneralization of Shapley effects to the Shapley-Owen Owen (1972, 2014) interaction effects attributes a fair share\nof relative importance not only to a single input but, more generally, to all possible subsets of the inputs. Using this\nindex, it is now also possible to gain insight into the synergistic or antagonistic nature of interactions within subsets\nof intputs. For example, the Shapley-Owen effect for {Gender, Ethnicity} attributes its marginal contribution to the\nrelative importance of all its supersets.\nIn practice, we may not be able to determine the exact Shapley and Shapley-Owen interaction indices because there\nis an exponential number of possible coalitions of inputs, and computing Shapley values is already hard for some\nsimple games Deng and Papadimitriou (1994); Conitzer and Sandholm (2006). However, several algorithms have been\nproposed to approximate Shapley values Mann and Shapley (1962); Owen (1972); Bachrach et al. (2010); Fatima et al.\n(2008); Maleki et al. (2013) by considering only a sample of coalition values, thus avoiding the need to consider an\nexponential number of such values. Shapley effects can also be estimated Song et al. (2016); Broto et al. (2020a),\nand Shapley effects are easily computable in some special cases Owen and Prieur (2017); Iooss and Prieur (2019);\nBroto et al. (2019, 2020a,b).\nHere we follow a different path in that we are separating the computation of Shapley-Owen effects into model-\nindependent and model-dependent computations. Hereby, the model-independent computations are considered to\nbe pre-computed, once-and-forall, and the model-dependent computation reduces to the construction of a sufficiently"}, {"title": null, "content": "precise polynomial chaos expansion (PCE), which relies on the spectral decomposition of the decision function. In\nother words, Shapley-Owen effects are expressed analytically in terms of the PCE coefficients of the underlying deci-\nsion model. This PCE can be reused for different sensitivity analyses as long as the underlying decision function and\nthe input distribution are unchanged.\nThis paper is structured as follows. Section 2 reviews some basic mathematical notation and concepts. Section 3 moti-\nvates and formalizes the central notion of relative importance as the basis for specifying fairness constraints. We also\nargue that the related notion of Sobol' sensitivity indices are not sufficient for fairness analysis in the important case\nof dependent inputs. Instead, Section 4 proposes Shapley-Owen effects, because of their \"fair\" attribution of relative\nimportance, as adequate measures for specifying fairness constraints. In order to make this paper as self-contained\nas possible we review basic concepts of PCE in Section 5. Based on these development we are proposing a simple\nalgorithm for constructing precise and sparse truncations of PCE with known truncation errors. Section 6 contains our\nmain result, which is a spectral decomposition of Shapley-Owen effects based on the PCE of the underlying decision\nfunction. We put these developments into perspective and compare our results with related work in Section 7. Finally,\nSection 8 concludes with some final remarks."}, {"title": "2 Preliminaries", "content": "Our developments are based on standard developments and notations of probability theory, functional analysis, and\nthe emerging field of uncertainty quantification Xiu (2010); Sullivan (2015). The probability space $(\\Omega, \\mathcal{E}, \\mathbb{P})$ consists\nof (1) the domain $\\Omega \\stackrel{\\text{def}}{=} \\Omega_1 \\times ... \\times \\Omega_d$ of dimension $d \\in \\mathbb{N}$, (2) the $\\sigma$-algebra $\\mathcal{E}$ on $\\Omega$, and (3) the probability measure\n$\\mathbb{P}(E)$ for all measurable events $E \\in \\mathcal{E}$. For $B \\in \\mathcal{E}$ with $\\mathbb{P}(B) > 0$ the conditional probability measure $\\mathbb{P}(\\cdot | B)$ on\n$(\\Omega, \\mathcal{E})$ is defined by $\\mathbb{P}(E|B) \\stackrel{\\text{def}}{=} \\mathbb{P}(E\\cap B)/\\mathbb{P}(B)$, for all events $E \\in \\mathcal{E}$.\nA random variable in $(\\Omega, \\mathcal{E}, \\mathbb{P})$ is a function $X : \\Omega \\rightarrow \\mathbb{R}$, which is measurable in the sense that $X^{-1}(B) \\in \\mathcal{E}$ for all\nevents $B$ in the Borel-$\\sigma$-algebra on the reals. The probability density function (pdf) $f : \\Omega \\rightarrow \\mathbb{R}$ for the random variable\n$X$ is defined as $f(x) \\stackrel{\\text{def}}{=} \\mathbb{P}(X = x) \\stackrel{\\text{def}}{=} \\mathbb{P}(\\{\\omega\\in\\Omega|X(w) = x\\})$. We assume that the pdf $f$ for $X$ is measurable.\nThe marginal probability measures $\\mathbb{P}_i(.)$, for $i = 1, ..., d$, are defined as $\\mathbb{P}_i(E_i) \\stackrel{\\text{def}}{=} \\mathbb{P}(\\Omega_1 \\times ... \\times E_i \\times ... \\times \\Omega_d)$,\nwhich assumes that the underlying projections $\\Omega \\rightarrow \\Omega_i$ are measurable. In this case, the marginal pdfs $f_i(.)$ are given\nby $f_i(x_i) \\stackrel{\\text{def}}{=} \\mathbb{P}_i(X_i = x_i)$ for $x_i \\in \\Omega_i$. The random variables $X_i$ are independent if and only if $f(x) = \\prod_{i=1}^{d} f_i(x_i)$,\nfor $x = (x_1,...,x_d) \\in \\Omega$. For measurable $M, N : \\Omega \\rightarrow \\mathbb{R}$ the inner product weighted by the measurable pdf $f$ is\ndefined as\n$\\langle M,N \\rangle_f \\stackrel{\\text{def}}{=} \\int_{\\Omega} M(x)N(x)f(x)dx, \\tag{1}$\nand $M$ and $N$ are said to be orthogonal if $\\langle M,N \\rangle_f = 0$. The inner product (1) induces the norm $||M||_f \\stackrel{\\text{def}}{=} \\sqrt{\\langle M, M \\rangle_f}$. The Lebesgue space $L^2(\\Omega, \\mathbb{P}; \\mathbb{R})$ (short: $L^2(\\Omega)$) is the set of random variables $X : \\Omega \\rightarrow \\mathbb{R}$ that are\nsquare-integrable such that $||X||_f < \\infty$. This space is complete with respect to the induced norm, so $L^2(\\Omega)$ is a\nHilbert space. For a random variable $X$ with measurable pdf $f$, the expectation $\\mathbb{E}(X)$ is the inner product $\\langle X, 1 \\rangle_f$,\nwhere 1 is a random variable with pdf $f_1(1) = 1$, and $\\mathbb{V}(X) \\stackrel{\\text{def}}{=} || X ||_f^2$ is the variance of $X$. The random variables\nfor the conditional expectation $\\mathbb{E}(X | Y)$ and the conditional variance $\\mathbb{V}(X | Y)$ are also defined as usual.\nA multi-index is an element $\\alpha \\in \\mathbb{N}^d$, and we write $0$ for the multi-index $(0, ..., 0)$. For a given multi-index $\\alpha =$\n$(\\alpha_1,...,\\alpha_d)$, a monomial $X^\\alpha$ is of the form $X_1^{\\alpha_1}... X_d^{\\alpha_d}$. A polynomial in $\\mathbb{R}[X]$ is a finite linear combination\nof monomials $X^\\alpha$. The total degree $|\\alpha|$ of a multi-index $\\alpha$ is the sum $\\Sigma_{i=1}^{d} \\alpha_i$. For the random variable $X =$\n$(X_1, ..., X_d)$ in $(\\Omega, \\mathcal{E}, \\mathbb{P})$ we are also interested in subsequences $X_u \\stackrel{\\text{def}}{=} (X_i)_{i\\in u}$, where $u \\subseteq [1,d]$. Depending on\nthe context of use, $X_u$ is also interpreted as a set. In case, $i \\notin u$ we also use the shorthand $u + i$ for $u \\cup \\{i\\}$, and $\\sim u$\ndenotes the set difference $[1, d] \\setminus u$. For a polynomial $P(x) \\in \\mathbb{R}[x]$, the projection function\n$\\pi_u(P(x)) \\stackrel{\\text{def}}{=} \\begin{cases} P(x) & \\text{if } X_u = \\text{vars}(P(x)) \\\\ 0 & \\text{otherwise.} \\end{cases} \\tag{2}$\nretains only the polynomials in which exactly the subset $X_u$ of variables occur. The characteristic function $\\chi_u(P(x))$\nis defined similarly, but it returns 1 instead of the argument $P(x)$."}, {"title": "3 Relative Importance", "content": "Algorithmic decisions under uncertainty are modeled by a discrete or real-valued random variable $Y = M(X)$ for\n$X \\stackrel{\\text{def}}{=} (X_1,..., X_d) \\in L^2(\\Omega)$ a finite sequence of random input variables $X_i$ for $i = 1,...,d$ with domain $\\Omega =$\n$\\Omega_1 \\times ... \\times \\Omega_d$ and measurable pdf $f(.)$. The variance $\\sigma^2 \\stackrel{\\text{def}}{=} \\mathbb{V}(M(X))$ of $M(X)$ is finite. Moreover, the decision\nfunction $M$ is assumed to be computable, but is otherwise considered a black box.\nNow, given any subset $u \\subseteq [1,d]$, the relative importance (alternatively, value or explanatory power) $val(u)$ of the\ninputs in $X_u$ for the outcome $M(X)$ is conveniently measured by the variance of the expected outcome conditioned\non $X_u$:\n$val(u) \\stackrel{\\text{def}}{=} \\mathbb{V}(\\mathbb{E}(M(X)|X_u)). \\tag{3}$\nHere, the empty set creates no value and the entire set of inputs contributes the variance $\\sigma^2$ of $M(X)$. By the total\nlaw of variance, $\\sigma^2 = val(u) + val(\\sim u)$, for\n$val(\\sim u) \\stackrel{\\text{def}}{=} \\mathbb{E}(\\mathbb{V}(M(X)|X_u)). \\tag{4}$\nThere is a close relationship between relative importance and Sobol' global sensitivity indexes. In the case of indepen-\ndent inputs $X$, at least, $M(X) = \\sum_{u\\subseteq[1,d]} M_u(X_u)$ the Hoeffding decomposition Hoeffding (1948) states that there\nare pairwise orthogonal $M_u(X_u)$ with (Appendix A)\n$M(X) = \\sum_{u\\subset[1,d]} M_u(X_u). \\tag{5}$\nThis decomposition of $M(X)$ is unique, $\\mathbb{E}(M_u(X_u)|X_i) = 0$, and $\\mathbb{E}(M_u(X_u)M_v(X)) = 0$, for all $u, v\\subseteq [1, d]$\nand $i \\in u$. Therefore, the expected value of $M(X)$ is given by $M_{\\emptyset}(X_{\\emptyset}) = \\mathbb{E}(M(X))$. The central property of the\nanalysis of variance (ANOVA) is the decomposition\n$\\sigma^2 = \\sum_{u: \\emptyset\\neq u\\subset[1,d]} \\sigma^2_u \\tag{6}$\nof the total variance of $M(X)$, where $\\sigma^2_u \\stackrel{\\text{def}}{=} \\mathbb{V}(M_u(X_u))$ denotes the variance of the partial effect $M_u(X_u)$. The\nANOVA decomposition (6) is a direct consequence of the orthogonality of the partial effects $M_u(X_u)$ in the Hoeffding\ndecomposition. The best predictor of $M(X)$ given $X_u$ is, by definition, the conditional expectation $\\mathbb{E}(M(X)|X_u) =$\n$\\sum_{v: v \\subseteq u} M_v(X_v)$. The Sobol' indices $S_u$ are obtained by taking the variance on both sides of the identity for the best\npredictor.\n$S_u \\stackrel{\\text{def}}{=} \\frac{val(u)}{\\sigma^2} = \\sum_{v: v \\subseteq u} \\frac{\\sigma_v^2}{\\sigma^2}. \\tag{7}$\nThe interpretation of the Sobol' indices is easy in the case of independent inputs, because the variance decomposi-\ntion (6) of $M(X)$ is unique. Thus, the first-order Sobol' index $S_i$, for $i \\in [1, d]$, represents the amount of the output\nvariance due to input $X_i$ alone. The second-order Sobol' index $S_{\\{i,j\\}}$ expresses the contribution of the interactions\nof the pairs of variables $X_i$ and $X_j$, and so on for the higher orders. As the sum of all Sobol' indices is equal to the\nvariance $\\sigma^2$ of $M(X)$, the indices are interpreted as proportions of explained variance. When $S_u$ is large, it means\nthat the combined effect of all $X_j$ for $j \\in u$ makes an important contribution to the variance of the expected outcome\nof $M(X)$. It therefore is the natural choice for measuring the relative importance of the inputs $X_u$ on $M(X)$. The\ntotal Sobol' index\n$T_u \\stackrel{\\text{def}}{=} \\frac{val(\\sim u)}{\\sigma^2} = \\sum_{v : v \\cap u \\neq \\emptyset} \\sigma_v^2 \\tag{8}$\nexpresses the \"total\" sensitivity of the variance $\\sigma^2$ of $M(X)$ to the inputs in $X_u$. It can be interpreted as the expected\nremainder of the variance once the variables $X_{\\sim u}$ are known. If $T_u$ is small, it means that the joint effects of $X_j$ for\n$j \\in u$ make little difference, even when all interactions between them and $X_k$ for $k \\notin u$ are taken into account. In\nthe case of input independence, the inequality $S_u \\leq T_u$ between Sobol' indices follows directly from the identities (7)\nand (8). Moreover, $S_u = \\sigma^2 - T_u$.\nA common way to deal with input dependence is to define a Hoeffding-like decomposition for dependent inputs\nand then define variable importance through this generalization Chastaing et al. (2012, 2015); Idrissi et al. (2023)."}, {"title": "4 Shapley-Owen Effects and Fairness", "content": "Shapley effects overcome these conceptual problems of Sobol' indices by attributing the mutual contribution - due to\ncorrelation and interaction - of a subset of inputs to each individual input within that subset Owen (2014); Song et al.\n(2016); Owen and Prieur (2017). Like ANOVA, they use variances, but unlike ANOVA for dependent data, a Shapley\neffect never goes negative, and it can be defined without making onerous assumptions about the input distribution."}, {"title": "4.1 Shapley-Owen Effects", "content": "In economics, Shapley values are often used to solve the attribution problem, where the value created by the collab-\norative efforts of a team needs to be \"fairly\" attributed to the individual members of that team. In our setting, the\nteam is the set $\\{X_1, ..., X_d\\}$ of input variables, the value of any subset $X_u$ of variables is its relative importance\n$\\mathbb{V}(\\mathbb{E}(M(X)|X_u))$ on the variance of outcomes $Y = M(X)$, and Shapley values are used to attribute the relative\nimportance of subsets of inputs in a \"fair\" way, which will be explained in more detail below.\nLet $val (u) \\in \\mathbb{R}$ be the value attained by the subset $u \\subseteq \\{1, ..., d\\} = [1, d]$. It is always assumed that $val(\\emptyset) = 0$. The\nShapley-Owen value takes into account the possible interaction between inputs Owen (1972); Grabisch and Roubens\n(1999).\n$Sh_u(val) \\stackrel{\\text{def}}{=} \\frac{1}{(\\binom{d}{|u|})} \\sum_{\\substack{v :v \\subset u \\\\ w: w\\subset \\sim u}} \\frac{\\binom{d-|u|-1}{|v|}}{\\binom{d-1}{|v|+|w|}} (-1)^{|\\sim u|-|w|} val(v \\cup w). \\tag{9}$\nBy using this index it is therefore possible to gain insights on the synergistic/antagonistic nature of interac-\ntions Rabitti and Borgonovo (2019); Plischke et al. (2021). The same Shapley-Owen value arises if we use values\n$val(\\sim u)$ (4) instead of the relative importance (3) Song et al. (2016). When $u = \\{i\\}$ the Shapley-Owen value (9)\nreduces to the Shapley value of a single input (10). The Shapley value $Sh_i(val)$ for $i = 1, ..., d$ is\n$Sh_i = \\frac{1}{d} \\sum_{v: i \\notin v} \\binom{d-1}{|v|}^{-1} (val(v \\cup i) - val(v)). \\tag{10}$\nThe marginal contribution $(val(v + i) \u2013 val(v))$ is how much the model output changes when a new feature i is\nadded, the combinatorial weight $\\binom{d-1}{|v|}^{-1}$ is the weight given to each of the different subsets of inputs with size $|v|$,\nand averaging by $1/d$ determines the average of all marginal contributions from all possible subsets of sizes ranging\nfrom 0 to d - 1.\nIf relative importance is used to define the game then $Sh_u(val)$ (9) is also said to be a Shapley-Owen effect of the subset\n$X_u$ of inputs. Similarly, the special case $Sh_i(val)$ is called the Shapley effect of the input $X_i$. The computation of\nboth Shapley and Shapley-Owen effects can be very demanding because they require summation over an exponential\nnumber of subsets, and computing the relative importance for all these subsets can be time-consuming Song et al.\n(2016); Plischke et al. (2021). For some special cases, though, they are relatively easy to calculate (Appendix D)."}, {"title": "4.2 Fairness", "content": "The following propositional logic example is an illustration of Shapley effects and their interpretation in terms of\nfairness.\nExample 4.1 (Huang and Marques-Silva (2023)). Consider a three-player game $D \\stackrel{\\text{def}}{=} \\{1, 2, 3\\}$ of value $val(u) \\stackrel{\\text{def}}{=} $\n$S_u = \\mathbb{V}(\\mathbb{E}(Y | X_u))$, where\n$Y \\stackrel{\\text{def}}{=} M(X) \\stackrel{\\text{def}}{=} M(X_1, X_2, X_3) = (X_1 \\wedge X_2) \\vee (\\neg X_1 \\wedge X_3),$ X_i \\in [0, 1] independent with $\\mathbb{P}(X_i = x_i) = 1/2$ for i = 1, 2, 3."}, {"title": null, "content": "$\\mathbb{E}(Y | X_u = x_u) = \\mathbb{P}(Y = 1|X_u = x_u) = \\frac{\\mathbb{P}(Y = 1, X_u = x_u)}{\\mathbb{P}(X_u = x_u)} = 2^{||} \\mathbb{P}(Y = 1, X_u = x_u)$\nThe joint probability $\\mathbb{P}(Y = 1, X_u = x_u)$ is equal to the number of satisfying assignments of $Y = M(X)$, where\n$X_u = x_u$ are fixed, divided by the size $2^3$ of the input space. For example,\n$\\mathbb{E}(Y | X_1 = 0) = 2\\mathbb{P}(Y = 1, X_1 = 0) = 2 \\frac{2}{2^3} = \\frac{1}{2},$\nsince there are exactly 2 satisfying assignments with $X_1 = 0$. Similarly, $\\mathbb{E}(Y | X_1 = 1) = 1/2$. From the total law\nof expectations it follows that $\\mathbb{E}(\\mathbb{E}(Y | X_u)) = \\mathbb{E}(Y) = 1/2$ for all subsets of inputs $X_u$, since $Y$ has 4 (out of 8)\nsatisfying assignments. For example, the variance of the random variable $\\mathbb{E}(Y | X_1)$ is computed as\n$\\mathbb{V}(\\mathbb{E}(Y | X_1)) = \\mathbb{E}((\\mathbb{E}(Y | X_1) \u2013 1/2)^2) = \\frac{1}{2} ((\\mathbb{E}(Y | X_1 = 1) \u2013 1/2)^2 + \\frac{1}{2} ((\\mathbb{E}(Y | X_1 = 0) \u2013 1/2)^2 = 0$\nSimilarly, $\\mathbb{E}(Y | X_2 = 0) = 1/4$ and $\\mathbb{E}(Y | X_2 = 1) = 3/4$. Hence, $\\mathbb{V}(\\mathbb{E}(Y | X_2)) = 1/16$. Calculation of all\n$\\mathbb{V}(\\mathbb{E}(Y | X_u)) = \\mathbb{E}((\\mathbb{E}(Y | X_u) \u2013 \\mu)^2)$, where $\\mu \\stackrel{\\text{def}}{=} \\mathbb{E}(\\mathbb{E}(Y | X_u)) = 1/2$ yields the cooperation game:\nUsing equality (10) the corresponding Shapley attributions are obtained.\n$Sh_1(val) = \\frac{1}{3} (1 \\cdot (0 \u2013 0) + \\frac{1}{2} \\cdot (\\frac{1}{8} \u2013 \\frac{1}{16}) + \\frac{1}{2} \\cdot (\\frac{1}{8} \u2013 \\frac{1}{16}) + 1 \\cdot (\\frac{1}{4} \u2013 \\frac{1}{8})) = \\frac{1}{32}$\n$Sh_2(val) = \\frac{1}{3} (1 \\cdot (\\frac{1}{16} \u2013 0) + \\frac{1}{2} \\cdot (\\frac{1}{8} \u2013 0) + \\frac{1}{2} \\cdot (\\frac{1}{8} \u2013 \\frac{1}{16}) + 1 \\cdot (\\frac{1}{4} \u2013 \\frac{1}{8})) = \\frac{3}{32}$\n$Sh_3(val) = \\frac{1}{3}\\cdot (1 \\cdot (\\frac{1}{16} \u2014 0) + \\frac{1}{2} \\cdot (\\frac{1}{8} \u2013 0) + \\frac{1}{2} \\cdot (\\frac{1}{8} \u2013 \\frac{1}{16}) + 1 \\cdot (\\frac{1}{4} \u2013 \\frac{1}{8})) = \\frac{3}{32}$.\nTherefore, the attribution of the relative importance of the input $X_1$ for the output $Y$ is smaller then the ones for the\nother inputs $X_2$ and $X_3$, and the attributed relative importance of the inputs $X_2$ and $X_3$ are identical.\nAn application-specific fairness specification needs to determine the sensitive inputs together with acceptable upper\nbounds on the relative importance on the variation of the outcome. For example, if all the inputs $X_i$ in the example\nabove can be considered sensitive then $X_1$ is the least biased, since the Shapley effect $Sh_1(val)$, which attributes the\nrelative importance of $X_1$ to the decision outcome $Y$, is the smallest. However, if this upper bound on the variance of\nthe outcome is unacceptably high in a given social context, the decision could still be said to be unfair with respect to\nthe input $X_1$.\nFairness specifications may also be based on comparing Shapley-Owen effects. In particular, when specifying\nlarger classes of fairness constraints, ratios and differences of fairness measures are relevant Feldman et al. (2015);\nDwork et al. (2012).\nExample 4.2. Consider a decision variable with two sensitive inputs $X_{gender}$ and $X_{ethnicity}$. For a given small $\\epsilon > 0$\nthe constraint\n$e^{-\\epsilon} < \\frac{Sh_{gender} (val)}{Sh_{ethnicity} (val)} < e^{\\epsilon}$\nexpresses the fact that gender and ethnicity have approximately the same Shapley attribution of relative importance on\nthe decision.\nThese kinds of comparative measures of fairness are the basis of notions of fairness such as disparate im-\npact Feldman et al. (2015) and differential fairness Dwork et al. (2012). Simplified formula for calculating differential\nfairness based on ratios of Shapley values can be derived.\nExample 4.3. Let $Y = M(X_1, X_2)$ with finite variance $\\sigma^2 > 0$; then:\n$\\frac{Sh_1(val)}{Sh_2(val)} = \\frac{val(1) + val(2)}{val(1) + val(2)} \\tag{11}$"}, {"title": null, "content": "For two inputs $\\{i", "sum_{v": "v\\subseteq \\sim\\{ij\\}} \\binom{d-2}{|v|}^{-1} (val(v + \\{i, j\\}) \u2013 val(v + i) \u2013 val(v + j) + val(v)). \\tag{12}$\nEquation (12) coincides with the definition of two-factors interaction used in the field of Design of Experiments Wu\n(2015). If this term is positive than the interaction between i and j is said to be synergistic (i.e. profitable), and if it is\nnegative then the interaction is antagonistic. Thus the intuition of Equation (12) is that one averages this interaction\nindex for all possible coalitions to which the subgroup u belongs. The following example illustrates Shapley-Owen\neffects for input sets of size 2.\nExample 4.4. For the game in example (4.1) we obtain from equation (12) Shapley-Owen effects"}]}