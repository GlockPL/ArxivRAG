{"title": "Can Self Supervision Rejuvenate Similarity-Based Link Prediction?", "authors": ["Chenhan Zhang", "Weiqi Wang", "Zhiyi Tian", "James J.Q. Yu", "Dali Kaafar", "An Liu", "Shui Yu"], "abstract": "Although recent advancements in end-to-end learning-based link prediction (LP) methods have shown remarkable capabilities, the significance of traditional similarity-based LP methods persists in unsupervised scenarios where there are no known link labels. However, the selection of node features for similarity computation in similarity-based LP can be challenging. Less informative node features can result in suboptimal LP performance. To address these challenges, we integrate self-supervised graph learning techniques into similarity-based LP and propose a novel method: Self-Supervised Similarity-based LP (3SLP). 3SLP is suitable for the unsupervised condition of similarity-based LP without the assistance of known link labels. Specifically, 3SLP introduces a dual-view contrastive node representation learning (DCNRL) with crafted data augmentation and node representation learning. DCNRL is dedicated to developing more informative node representations, replacing the node attributes as inputs in the similarity-based LP backbone. Extensive experiments over benchmark datasets demonstrate the salient improvement of 3SLP, outperforming the baseline of traditional similarity-based LP by up to 21.2% (AUC).", "sections": [{"title": "1 INTRODUCTION", "content": "Link prediction (LP) is one of the most intriguing and enduring challenges in the field of graph mining, which involves predicting the probability of a link between two unconnected nodes based on available information in the graph, such as node attributes [38]. LP contributes to a myriad of real-world applications such as friend recommendation in social networks [33], drug-drug interaction prediction [32], and knowledge graph completion [29].\nLP methods can be broadly classified into two categories: similarity-based LP and learning-based LP. While learning-based LP has shown strong capabilities, there remains a significant role for similarity-based LP methods. On the one hand, they heavily rely on known link information as labels to supervise the learning process [41, 44]. However, in some realistic applications, link information can be absent, where we only have an edgeless graph. For example, in the \"cold start\" phase of the drug-drug interaction, where link labels may not be readily available [14]. Similarity-based LP assists in hypothesis generation by suggesting potential interactions derived from the attributes of drugs, where these label information can be further utilized in unsupervised learning on the data. On the other hand, the \"black box\" nature of widely adopted end-to-end neural network-based LP methods makes it difficult to gain insights into what inclusive modules or specific data features are pivotal in determining LP. Comparatively, similarity-based is a transparent and intuitive method that is more conducive to the analysis of results. Research Gap. However, similarity-based LP methods are heuristic due to their predefined similarity measures [19]. Consequently, the quality of input node features can significantly impact the final LP performance. This study reexamines similarity-based LP and explores further advancement within the existing framework. We believe that the similarity-based LP methods can be enhanced by improving the representation of the original node attributes. Following this vein, a further hypothesis is that node-level features enriched with topological information can enhance prediction accuracy, considering that link existence depends on both individual node features and their relative positions and neighborhood structures. Nevertheless, as previously mentioned, similarity-based LP nowadays is often used in contexts when there are no or limited known links to label information. This condition impedes the utilization of topological information.\nMotivation. Typically, the effectiveness of similarity-based LP methods significantly hinges on the feature engineering of the original node attributes. In this regard, a key hypothesis is that node-level features enriched with topological information can enhance prediction accuracy, considering that link existence depends on both individual node features and their relative positions and neighborhood structures in the graph. Traditional similarity-based LP methods either use original node attributes directly, or use the node features processed by feature selection or dimensionality reduction techniques [19]. These approaches are suboptimal, as these attributes or features only incorporate individual information.\nOne method is that we can leverage graph representation learning (GRL) techniques [15] to learn complex topological information and encode it into node features. However, as previously mentioned, similarity-based LP is often involved in a context without link-label information (graph structural information), which impedes this process. Self-supervised learning (SSL) has emerged as an effective solution for extracting representations from unlabeled data. To address the unsupervised challenges, SSL techniques employ data augmentation and pretext tasks to furnish specific supervision signals during the learning process, thereby enabling the learned representations to be expressive and can be applied across various downstream tasks [6, 12]. Recently, there have witnessed endeavors to advance SSL to existing GRL techniques [26]. Some of them have demonstrated success in capturing latent topological information and developing insightful node representations [27, 34]. Inspired by these methods, leveraging SSL appears promising as a means to develop node-level features for similarity-based LP.\nContribution. In this paper, we propose a novel approach to improve similarity-based LP: Self-Supervised Similarity-Based LP (3SLP). Considering the capabilities of self-supervised learning (SSL) techniques in unsupervised graph representations learning [27, 34], the main idea of 3SLP is to integrate SSL into node representation learning and use the developed node representations for similarity-based LP. We first introduce a widely adopted pairwise similarity-based clustering (PSC) backbone [13, 43] as the testbed. This backbone operates in an unsupervised manner and only requires node-level features as inputs to implement prediction, constituting a practical and commonly adopted scheme for similarity-based LP. Then, we design a dual-view contrastive node representation learning (DCNRL) to develop informative node representations as inputs instead of original node attributes to the PSC backbone to prompt LP performance. In DCNRL, we first address the absence of structural information by initializing a graph structure. Considering the significance of latent higher-order proximity in the node representation learning, we then employ graph diffusion to generate the two augmented views in the data augmentation phase. Furthermore, inspired by [11, 31], we apply cross-scale contrasting on the extracted node-level and graph-level representations to develop more comprehensive node representations. Our experimental results showcase the superiority of 3SLP, evidencing its ability to outperform the baseline by as much as 21.2%. Our main contributions are summarized as follows:\n\u2022 We rejuvenate similarity-based LP problem and propose 3SLP, a novel approach that leverages SSL to develop informative node representation for improving similarity-based LP. We introduce a dual-view contrastive learning method (DCNRL) for the node representation development.\n\u2022 We address the absence of links label information by graph structure initialization and edge diffusion. We design a cross-scale contrastive objective between node- and graph-level representations to develop comprehensive node features.\n\u2022 We demonstrate a significant performance improvement in similarity-based LP contributed by our method. Code is available at https://anonymous.4open.science/status/WSDM_25-EDB2.\nRoadmap. The remainder of the paper is organized as follows. Section 2 presents preliminary knowledge about this study. In Section 3, we elaborate on the proposed 3SLP method. We present the evaluation to demonstrate the effectiveness of the proposed scheme in Section 4. Section 6 concludes the paper and discusses future work."}, {"title": "2 PRELIMINARIES", "content": "In this section, we first introduce the investigated LP task. Then, we will provide some background information about graph representation learning. Lastly, we elucidate the differences between our study and contemporary LP methods."}, {"title": "2.1 Usage Scenario of Similarity-Based LP", "content": "The link prediction (LP) task is to determine whether a given pair of nodes are linked (i.e., an edge exists) [19]. Let $G = (V,E,X)$ be an attributed graph with node set $V \\in R^{|V|}$, edge set $\\&$, and node attribute matrix $X = {x_i} \\in R^{|V|\\times d}$ where d is the dimension of the node attributes. The adjacency matrix of G is denoted by $A = {a_{ij}} \\in R^{|V|\\times|V|}$ where entry $a_{ij} = 1$ if edge $(i, j) \\in \\&$ and $a_{ij} = 0$ otherwise. Let $E_k$ and $\\&r$ represent the known and real edge sets of a graph, respectively. To replicate scenarios typical of similarity-based LP use cases, where only node attributes may be accessible without link information (such as in the initial stages of drug-drug interactions), our study particularly considers edgeless attributed graphs. An edgeless attributed graph can be defined as $G^0 = (V, E_k, X | E_k = 0)$. Therefore, the objective of similarity-based LP on an edgeless attributed graph can be concisely expressed as: $X \\rightarrow \\&r$. Notably, the investigated case offers a harsh testing ground for similarity-based LP methods; they need to demonstrate the ability to efficiently discern and extract relevant information from potentially subtle differences in attributes to achieve accurate predictions."}, {"title": "2.2 GNN-Based Graph Representation Learning", "content": "Graph neural networks (GNNs) are extensively utilized to develop node- and graph-level representations. Take message-passing-based GNNs as an example, a canonical node representation (embedding) encoding process includes (1) AGG(): aggregate messages received from neighbor nodes; (2) UPD(): update the node representation by non-linear transformation [24]. Let $\\mathcal{N}(v)$ be the set of 1-hop neighbor nodes of node v, the process flow can be expressed as:\n$F_{nl}(X, A) : h_v^l = \\text{UPD} \\left( h_v^{l-1}, \\text{AGG} \\left( h_u^{l-1}, \\forall u \\in \\mathcal{N}(v) \\right) \\right),$ (1)\nwhere $h_v^l \\in R^h$ denotes the node-level representation of node v at layer l and h is the dimension. On this basis, to develop graph-level representation $h_g \\in R^h$, a graph pooling operation (e.g., max pooling and mean pooling) can be performed on all node embeddings:\n$F_{il} \\left( h_v^{l}, \\forall v \\in V \\right) : h_g = \\text{POOL}(h_v^{l}, \\forall v \\in V)$. (2)"}, {"title": "2.3 Relationship between Our Study and Contemporary LP Studies", "content": "While there is a broader range of advanced end-to-end learning-based LP methods [1, 1, 9, 21, 25, 45], which lead the mainstream and state-of-the-art in the LP field, our study does not focus on improving or surpassing these approaches. Instead, our main motivation is to enhance classic similarity-based LP methods, given their practicality in certain use cases. Moreover, as our study is orthogonal to end-to-end learning-based LP methods, our method can further assist these approaches, such as by generating reasonable pseudo labels during cold-start phases.\nFurthermore, it is worth mentioning that we do not focus on improving similarity-based LP methods from the perspective of metrics (e.g., similarity score computation) or algorithms (e.g., clustering algorithms). Our primary focus is on enhancing the input node features/attributes. In this context, the employed backbone (as introduced in Section 3.2) for the similarity-based LP is constructed based on widely evaluated and adopted similarity-based LP methods [4, 13, 19]."}, {"title": "3 OUR METHOD", "content": "In this section, we introduce our proposed 3SLP. The core architecture of 3SLP is two-fold: (1) pairwise similarity-based clustering backbone (PSC) (empirical backbone) and (2) dual-view contrastive node representation learning (DCNRL) (our design). Specifically, 3SLP first leverages DCNRL to develop informative node representations from the edgeless attributed graph. Then, the PSC backbone takes these node representations as input to predict the target links. A schematic view of 3SLP is given in Figure 1. The algorithmic details of 3SLP are given in Algorithm 1"}, {"title": "3.2 Pairwise Similarity-Based Clustering Backbone", "content": "We first introduce pairwise similarity-based clustering (PSC) based on the previous construction in the literature [13, 43]. PSC forms a concise yet effective unsupervised clustering method, serving as the backbone of our method that implements similarity-based LP. The PSC backbone mainly incorporates two steps. The first step is node similarity measuring between pairwise input node features. For any two nodes u and v, and their respective features $i_u$ and $i_v$, the node similarity measuring can be formulated as:\n$S_{uv} = \\Upsilon(i_u, i_v),$ (3)\nwhere $\\Upsilon$ represents the adopted similarity measurer. In this work, we involve five commonly-used symmetric similarity metrics: (1) cosine similarity; (2) cosine distance; (3) Euclidean distance; (4) Manhattan distance; and (5) correlation distance. By node similarity measuring, we can obtain $|V|\\times|V|$ pairs of node representation similarity score, denoted as S. Next, PSC applies an unsupervised clustering algorithm to perform binary classification on the computed similarity scores. Specifically, we adopt the K-Means algorithm. The number of clusters is set to 2, corresponding to positive (link) and negative (no link) node pairs. Treating it as a ranking problem here, PSC calculates the average similarity score for each cluster - based on the adopted metrics, the cluster whose node pairs with a lower (higher) average score is considered linked (not linked)\u00b9. The clustering process can be written as:\n$\\left\\{\n\\begin{array}{ll}\nS_1, S_2 = \\text{KMeans}(\\{s_{uv} \\in S\\}) \\\\\n\\hat{a}_{uv} \\in S_1 = 1, \\hat{a}_{uv} \\in S_2 = 0 & \\text{if AVG}(S_1) > \\text{AVG}(S_2) \\\\\n\\hat{a}_{uv} \\in S_2 = 0, \\hat{a}_{uv} \\in S_2 = 1 & \\text{else}.\n\\end{array}\n\\right.$ (4)\nGiven that the PSC is primarily used as the testbed for our new designs, further refinement, such as balancing positive and negative samples (class imbalance issue), falls outside the scope of this study. Building upon the PSC backbone, we can utilize the informative node feature to prompt LP. A naive approach directly takes the"}, {"title": "3.3 Dual-View Contrast Node Representation Learning (DCNRL)", "content": "Here, we introduce the proposed DCNRL method for generating informative node features for input into the PSC backbone. DCNRL employs a widely adopted dual-view contrastive learning framework. On this basis, we further introduce specifically tailored data augmentation and contrastive learning modules, catering to the unique needs of edgeless attributed graphs and similarity-based LP. The detailed designs of these components will be elaborated on in the following sections.\nWithin the edgeless attributed graph, no graph structural (i.e., links) information is known. We cannot use any known graph structural information for data augmentation and further node representation learning. Therefore, we propose first to initialize a graph structure. Particularly, we adopt the cosine similarity metric to obtain the similarity between a node and all other nodes and select the nodes with top-k nearest nodes to connect with. This method resembles the creation of weak labels based on the attribute homophily [39]. Denote a graph initializer as $I$, this step can be formulated as:\n$A^0 = I(X)$ (5)\nwhere $A^0$ represents the adjacency matrix for the initialized graph structure.\nHowever, the initialized graph structure may deviate significantly from the real graph structures, resulting in a noisy and insufficient configuration that may prove ineffectual for further GNN encoding. It has been shown that diffusion processes can enhance GNN performance on such noisy graph structures [18]. Therefore, we employ edge diffusion as a data augmentation strategy on the initialized graph structure. This approach can imitate higher-order proximities among nodes, thus empowering GNN encoders to capture more intricate graph properties within the SSL framework. Specifically, we employ the Personalized PageRank (PPR) method for edge diffusion. [3, 20] have demonstrated PPR can smooth out the neighborhood over a noisy graph and restore more pertinent connections. Given the adjacency matrix of the initialized graph $A^0$ and diffuser $\\Psi$, we can develop edge diffused graph structures using a closed-form expression for PPR derived from [18], that is:\n$\\tilde{A}^0 = \\Psi(A^0) := \\alpha (I - (1 - \\alpha)D^{-1/2}A^0D^{-1/2})^{-1},$ (6)\nwhere D is the diagnonal degree matrix of $A^0$ and $\\alpha$ is the teleportation probability. Moreover, we apply two different teleportation probabilities $\\alpha_1$ and $\\alpha_2$ to the diffuser to create two distinct views:\n$\\begin{aligned}\n\\text{VIEW1: } A_1 &= \\Psi_{\\alpha_1}(A^0) \\\\\n\\text{VIEW2: } A_2 &= \\Psi_{\\alpha_2}(A^0)\n\\end{aligned}$ (7)\nThese two views, characterized by varying diffusion levels, furnish a progressive topological transition from a high-order perspective. Consequently, the subsequent learning process is able to encode more comprehensive topological information within the node-level representation.\nFor the LP purpose, we aim to develop node-level representations infused with topological information; however, these representations may fall short of being highly indicative due to the inherent limitations of the local aggregation function within a GNN encoder. Drawing inspiration from [11], we propose to apply a cross-scale strategy during the contrasting phase. By juxtaposing features at different scales, we can extract pattern and structural information across multiple dimensions [36]. This allows our GNN encoder to capture how the representation varies from different scales and thus learn a more robust representation. Specifically, we first employ two respective GNN encoders for two views. While sharing the same architecture, the two encoders do not share parameters (with $\\theta_1$ and $\\theta_2$, respectively). Then, we use the GNN encoder to develop two scales for each view progressively, namely, node-level representation and graph-level representation, respectively:\n$\\begin{aligned}\n\\text{VIEW1: } H_{v,1} &= \\text{ALN} \\left( F_{\\theta_1}^{nl}(X, \\tilde{A}_1) \\right) \\\\\nH_{g,1} &= \\text{ALN} \\left( F_{\\theta_1}^{il} \\left( F_{\\theta_1}^{nl}(X, \\tilde{A}_1) \\right) \\right) \\\\\n\\text{VIEW2: } H_{v,2} &= \\text{ALN} \\left( F_{\\theta_2}^{nl}(X, \\tilde{A}_2) \\right) \\\\\nH_{g,2} &= \\text{ALN} \\left( F_{\\theta_2}^{il} \\left( F_{\\theta_2}^{nl}(X, \\tilde{A}_2) \\right) \\right),\n\\end{aligned}$ (8)\nwhere ALN represents the dimension alignment process to make each developed representation in the same dimension $H \\in R^{|V|\\times h}$. The corresponding negative samples for each view are generated by our corruption function by randomly shuffling the node attributes $C: X \\rightarrow X$ and we use $\\tilde{H}_{v,1} = \\text{ALN} \\left( F_{\\theta_1}^{nl}(C(X), \\tilde{A}_1) \\right)$ to denote the representation developed by X. Note that we do not choose to generate negative samples by corrupting the structural input $\\tilde{A}^0$, as their included augmented edges are not the actual instances and thus may not render effective contrasting.\nTo better maximize the concordance between two positive views while minimizing that between positive and negative views, we compute the mutual information between their corresponding node-level and graph-level representations. Inspired by [31], we adopt neural network-based mutual information estimation, making this step more tractable. In this way, the final contrastive learning objective can be expressed as:\n$\\min_{\\theta_1,\\theta_2,\\Phi} BI_{\\Phi} \\left( H_{g,1}, H_{v,2}; \\tilde{H}_{g,1}, \\tilde{H}_{v,2} \\right) + BI_{\\Phi} \\left( H_{g,2}, H_{v,1}; \\tilde{H}_{g,2}, \\tilde{H}_{v,1} \\right),$ (9)\nwhere $BI_{\\Phi}$ represents a bilinear scoring function with shared parameters between the two views. The promise of the design is that we can obtain globally relevant node-level representations, capturing information across the entire graph. Such node representations are designed to preserve similarity for all node pairs, even for those that are distant [8, 31]. Subsequently, to obtain the final node representation that will be used as input to the PSC backbone, we compute the average of the learned node representations derived from the two encoders, which can be expressed as:\n$\\left\\{ h_v, \\forall v \\in V \\right\\} = \\frac{F_{\\theta_1}^{nl}(X, \\tilde{A}_1) + F_{\\theta_2}^{nl}(X, \\tilde{A}_2)}{2}.$ (10)"}, {"title": "3.4 Complexity Analysis", "content": "Time Complexity For time complexity, the main concern comes from the data augmentation and clustering steps that involve calculating pairwise node attribute and representation similarity, respectively. The naive approach to this calculation would have a time complexity of $O(N^2)$, where N is the number of data points. However, it is possible to perform this step efficiently using parallelizable and approximate methods. For instance, NN-Descent [7] utilizes MapReduce, which enables them to achieve approximate K-nearest neighbor (KNN) graphs in $O(n^{1.14})$ time complexity.\nSpace Complexity For space complexity, our method incorporates parameters from the two GNN encoders $f_{\\theta_1}$ and $f_{\\theta_2}$ and each one typically has $O(LNd + Ld^2)$ (GCN) space complexity where L is the layer number. The shareable MLP projector $\\Phi$ has a space complexity O(d') linear to the representation dimension d', d' \u2264 d. The overall space complexity is O(2LNd + 2Ld\u00b2 + d')."}, {"title": "4 EVALUATION", "content": "This section is structured as follows: First, we describe the experimental setup, detailing the datasets and models used. Next, we present the results of our experiments, followed by an analysis of graph homophily within the datasets. Finally, we conduct a hyper-parameter test to evaluate the impact of different configurations on model performance."}, {"title": "4.1 Experimental Setup", "content": "Datasets: We include four real-world and widely-adopted graph datasets, namely, Cora [28], Citeseer [28], PubMed [22], and Reddit [10] in our experiments. The statistical information of the four datasets is summarized in Table 1.\nModel and Parameter Settings. We adopt single-layer GCN [17] as the default GNN encoder, and the hidden size is set to 512. We use Adam [16] as the optimizer for the SSL where we have the number of training epoch T = 200 and the learning rate \u03b7 = 0.001. In the graph initialization, we set k = 5 for the top-k nearest algorithm. The teleportation probabilities for the two diffusers are set to \u03b11 = 0.2 and \u03b11 = 0.4. We consider cosine distance as the default similarity metric of PNRSC. The optimal values for some of these hyperparameters are discussed later. Experiments for each setting are run repeatedly five times to eliminate randomness.\nMetrics. We adopt area under the ROC curve (AUC), and average precision (AP) as our metrics, which is widely adopted to measure the performance of binary classification in a range of thresholds [5, 40, 42]. We primarily refer to AUC as did in [13, 41].\nAdditionally, we introduce two metrics for the graph homophily analysis. They are:\nAttribute Assortativity Coefficient (AAC):\n$r_{aac} = \\frac{\\text{Tr}(e) - ||e^2||}{1 - ||e^2||}$ (11)\nwhere e denotes the joint probability distribution of the specified attribute.\nDegree Assortativity Coefficient (DAC):\n$r_{dac} = \\frac{\\sum_{xy}xy(e_{xy} - a_xb_y)}{\\sigma_a \\sigma_b}$ (12)\nwhere $e_{xy}$ is the fraction of all edges in the graph that join together nodes with degree values x and y, $a_x$ and $b_y$ are the fractions of edges that start and end at nodes with degree values x and y [23], and $\\sigma_a$ and $\\sigma_b$ are the standard deviations of the distributions $a_x$ and $b_y$."}, {"title": "4.2 Results", "content": "Comparison with Baselines. We first compare the LP performance of the proposed method 3SLP with the baselines. As aforementioned, we consider pairwise node attribute similarity-based clustering (PSC-NA) the major baseline. From the results shown in Table 2, we can observe that 3SLP pronouncedly surpasses the baseline in most cases. For example, within the same PSC metric, 3SLP can exceed PSC-NA by 10.2% (AUC) on the Cora dataset and 21.2% on the Citeseer dataset. Through self-supervision, 3SLP can develop node features that are more contributive than original node attributes with respect to the LP within the PSC backbone. However, we also notice that the performance improvement by our method on the PubMed dataset is not as pronounced as the other two datasets. We recognize that this pertains to graph homophily, which will be discussed later.\nAdditionally, we compare our method with the method involving auxiliary knowledge within the PSC backbone: (1) node representations (PSC-Repr) [42]: the node representations developed from the trained GNN on node classification tasks in a supervised manner; (2) node posteriors (PSC-Pos) [13]: the node prediction posteriors in node classification tasks developed from the trained GNN in a supervised manner. As shown in Table 2, 3SLP markedly outperforms these methods, implying the contribution of its developed node representations.\nComparison with Knowledge Transfer-Based Methods. Knowledge transfer is an effective technique for handling unsupervised tasks. We hereby compare 3SLP with the method of knowledge transfer between datasets, and specifically, we adopt a parameter sharing-based knowledge transfer method [30]. We adopt a Graph AutoEncoder (GAE) as the carrier of parameter sharing for knowledge transfer. Each adopted dataset is considered a source and a target dataset, where we obtain a total of 9 combinations. Note that when the source and target datasets are the same, this situation becomes equivalent to regular supervised learning without knowledge transfer (see the diagonal in Table 3).\nThe results in Table 3 indicate that 3SLP significantly outperforms other methods across all three datasets. Specifically, it surpasses transferred GAE by an average of 19.1% and 28.6% on the Cora and Citeseer datasets, respectively. 3SLP even matches or exceeds the performance of non-transferred GAE in some scenarios. Notably, the effectiveness of transferred GAE is influenced by the source datasets used for training. The performance in the target domain is subject to variations depending on the source domain's quality and relevance, occasionally leading to negative transfer. In contrast, 3SLP leverages self-supervision to avoid the performance degradation commonly associated with knowledge transfer methods, thereby maintaining the integrity of the learning process.\nEvaluating Predicted Links for Enhanced Node Classification in Cold Start Scenario Moreover, we wonder whether the predicted links possess good utility that can be employed for further analysis on the graph, particularly in cold start scenarios where limited information is available. To elucidate this, we examine whether these predicted links can be utilized to train an accurate GNN classifier for node classification tasks. To this end, we train a GCN classifier using predicted links in a supervised manner with node labels and evaluate its performance on a test set. Meanwhile, we compare the results with those developed by other tasks transferred knowledge-based methods as aforementioned. From the result shown in Figure 2, we find the proposed method achieves satisfactory outcomes, demonstrating its efficacy in cold start situations. The classification accuracy results, when developed from 3SLP's predicted links, consistently surpass those achieved by other methods. Notably, on Citeseer, the classification accuracy developed from our predicted graph structure even betters the one on the original graph structure. This result implies that 3SLP can construct informative links that can be effectively utilized for various purposes, including addressing the challenges of cold start assistance."}, {"title": "4.3 Analysis on Graph Homophily", "content": "Influence of Graph Homophily. When handling LP tasks, it is imperative to circumvent the homophilous assumption. Thereby, we investigate the influence of graph homophily on 3SLP's performance. We introduce the AAC and the DAC, which measure the attribute homophily and topology homophily of the graph, respectively [23]. Their defintions are given in Eq. (11) and (12). We include the relatively heterophilic dataset, Reddit, in this test to facilitate a more pronounced comparison. Figure 3 depicts a noticeable positive correlation between the LP performance of 3SLP and AAC: the higher the AAC, the better the LP performance of 3SLP; however, it does not show a similar trend between the LP performance of 3SLP and DAC. We further compare 3SLP's learning performance between the relatively homophilous dataset Citeseer and the heterophilic dataset Reddit (see Figure 4). We find that 3SLP fails to learn informative information for the LP task in the heterophilic dataset. These results imply a highly possible correlation between 3SLP's effectiveness and attribute homophily assumption, which we will further explore in future work.\nMore Insights of Graph Homophily from Spectrum Analysis. [2] provides a spectral theory-based study on the efficacy of SSL learned embedding in addressing downstream tasks. In spectrum space, they highlighted the significance of spectrum alignment between the relation matrix R (i.e., the transformation matrix for augmenting views X': X' = RX) and the target matrix Y. In our study, LP's target is the real adjacency matrix A. By singular value decomposition, we first obtain left singular vectors of A and R, namely, \u00dba and \u00dbr.\nWe then evaluate the spectrum alignment between \u00dbr and \u00dba (denote the left singular vector Ua with values greater than 0 as \u00dba, similarly hereinafter). The spectrum alignment is quantified using the cosine similarity between the two singular vectors, denoted as A(\u00dba, \u00dbr). We particularly compare the results of the homophilic dataset Citeseer with those of the heterophilic dataset Reddit. The results are present in Figure 5. We observe that the result of the Citeseer dataset demonstrates relatively more substantial spectrum alignment; this aligns with a better LP performance observed on it. In contrast, the Reddit dataset presents a much smaller alignment value, correlating with its inferior LP performance. Moreover, we find that on Citeseer, the column of \u00dbr can span \u00dba; this pattern is consistent with the deductions in [2] about superior efficacy of SSL learned embedding in handling downstream tasks. However, these patterns do not happen to Reddit. The relation matrix depends considerably on our graph initialization method, which is fundamentally anchored in the homophily assumption. We posit that higher adherence to the homophily assumption in our target graphs may potentially facilitate improved performance."}, {"title": "4.4 Hyperparameter and Ablation Study", "content": "Influence of Structural Initialization Methods. We first examine the performance of 3SLP with four different initialization methods for data augmentation. The results are shown in Figure 6(a). Similarity wiring achieves better results compared to others. This is because similarity wiring initializes more semantically meaningful graph structures regarding node attributes, which benefits subsequent data augmentation and the learning process. Furthermore, we investigate the hyperparameter k in the similarity wiring method. For hyperparameter k in similarity wiring, we further set k\u2208 {1, 5, 10, 20, 50, 100, 1000} to examine LP performance. Note that when k = 0 and k = |V|, similarity wiring is identical to no wiring and fully wiring, respectively. From the results shown in Figure 6(b), we observe that the best results can be obtained when k = 5 and then the performance shows a decreasing trend with k growth. These results suggest the significance of the selected initialization method.\nInfluence of Edge Diffusion Levels. We evaluate the sensitivity of the proposed method to different edge diffusion levels by comparing the LP performance with different teleportation probabilities of PPR \u03b11,\u03b12 \u2208 {0.01, 0.05, 0.1, 0.2, 0.4}. The results are shown in Figure 6(c) and (d). We can observe that the proposed method performs better when the difference between \u03b11 and \u03b12 is larger. This result suggests that when contrasting the two views, a more distinguished diffusion difference between the two views makes richer topological information can be captured, improving the quality of learned node representations.\nInfluence of GNN Encoders. Our design enables 3SLP to be model-agnostic regarding the GNN encoder. Beyond the default GCN, we extend our evaluation of 3SLP's performance to three additional GNN encoders: SGC [35], GIN [37], and GraphSAGE [10]. As shown in Table 4, 3SLP exhibits consistently satisfying performance across different GNN encoders, demonstrating its compatibility. However, we notice that the performance with GraphSAGE is somewhat subpar, potentially due to the limitations inherent in its adopted neighborhood sampling strategy.\nInfluence of Similarity Metrics in PSC. Similarity metrics are pivotal in the PSC backbone, influencing the ultimate LP outcomes. In Table 2, we showcase the performance across all five introduced metrics for a comprehensive comparison. The results show that the best metric varies from dataset, except the Manhattan distance, which consistently underperforms compared to others. Furthermore, our repetitive tests also suggest that the cosine distance metric achieves more consistently good performance, making it our preferable configuration."}, {"title": "5 DISCUSSION ON APPLICABILITY", "content": "In this work, the proposed method follows the notion of similarity-based NAO-LP and attempts to improve the baseline. The notion is based on the intuition that two nodes sharing similar features are more likely to be linked. However, not all graphs are highly consistent with this characteristic. Therefore, there exists a performance upper bound for similarity-based methods given the node attributes, which also indicates the limitation of 3SLP. Another factor is the scale of graph, which can influence the learning effect of the GNN encoder. These facts are also reflected in the different performances of 3SLP on different datasets. Nonetheless, we demonstrate the improvement brought by the SSL within such a scope of applicability."}, {"title": "6 CONCLUSION", "content": "In this paper, we propose 3SLP to rejuvenate the similarity-based LP methods. We integrate self-supervised graph representation learning techniques into 3SLP to enable it to handle the realistic use case of similarity-based LP. Without the supervision of edge labels, 3SLP can develop informative node representations as inputs instead of original node attributes to the pairwise similarity-based clustering backbone to prompt LP performance on attributed homophilic graphs. We empirically demonstrate the superiority of 3SLP compared to the baselines. Our extensive analysis highlights the crucial impact of graph homophily on the efficacy of 3SLP. In future research endeavors, we aim to explore the applicability of the proposed method to"}]}