{"title": "Detection-Fusion for Knowledge Graph Extraction from Videos", "authors": ["Taniya Das", "Louis Mahon", "Thomas Lukasiewicz"], "abstract": "One of the challenging tasks in the field of video understanding is extracting semantic content from video inputs. Most existing systems use language models to describe videos in natural language sentences, but this has several major shortcomings. Such systems can rely too heavily on the language model component and base their output on statistical regularities in natural language text rather than on the visual contents of the video. Additionally, natural language annotations cannot be readily processed by a computer, are difficult to evaluate with performance metrics and cannot be easily translated into a different natural language. In this paper, we propose a method to annotate videos with knowledge graphs, and so avoid these problems. Specifically, we propose a deep-learning-based model for this task that first predicts pairs of individuals and then the relations between them. Additionally, we propose an extension of our model for the inclusion of background knowledge in the construction of knowledge graphs.", "sections": [{"title": "Introduction", "content": "Visual understanding has been a central question in AI since the inception of the field. However, it is not obvious how to quantify whether a machine can understand what it sees. One simple way is classification, and indeed, much of the computer vision research over the last ten years has centered around ImageNet. Object classification performance is very easy to measure, but it only conveys a coarse description of the image and misses further information about the properties and relations of the present objects. Another approach is to generate a natural language sentence describing the visual contents. This escapes the limitation of classification and is capable of expressing all the complexity that natural language can express.\nHowever, using natural language comes with a number of disadvantages. It means the model not only has to learn to understand the contents of the video but also how to express this content in natural language, which is a significant additional requirement. Even in humans, understanding is quite a separate problem from articulation in language, as evidenced by patients with damage to Broca's area in the brain, which show normal understanding of visual and even linguistic information [2], but struggle to articulate this understanding in\nlanguage [24]. Additionally, all the extra structure learned by the language generation component can obscure the performance of the understanding component. For example, if an image of a dog running in a park was correctly captioned as \u201ca dog is running in a park\", then we cannot conclude that model correctly identified a park and the action of running in the image. Instead, it may have identified the dog, and then the language model simply completed the most likely sentence that begins with \u201ca dog...\". Another problem with natural language annotations is that they are difficult to evaluate. The complex syntactic-semantic structure of a sentence means that we cannot simply count which words the model predicted correctly, but instead must use a bespoke metric such as BLEU [19], METEOR [1], or LEPOR [9]. Recognizing the imperfection of each of these, results for natural-language annotation models typically report scores on multiple metrics, none of which have a simple and intuitive interpretation. A third disadvantage of requiring the model to produce a natural language annotation is that it commits it to that particular natural language. A model trained to produce English captions cannot, then, be used to produce Turkish captions. Not only has the model learned a different vocabulary, but a different grammar too, e.g. nominative, SVO, and analytic for English, vs ergative, SOV, and agglutinative for Turkish. Thus, as well as leading to unnecessary extra work, producing annotations in natural language hinders the generalization of the model.\nWe instead choose to annotate videos using structured annotations in the form of knowledge graphs, which avoids all of the above drawbacks. Knowledge graphs, which are equivalent to sets of logical facts, can be evaluated with accuracy and similar metrics such as F1-score, do not require learning language syntax, and can be translated between natural languages by translating one term at a time (we can avoid word-sense disambiguation by relating to words at the sense-level). The first stage of our proposed model is to separately detect the individuals and predicates that are present in the input video, and then fuse the detected components together into a knowledge graph. Individuals are represented as learnable vectors; predicates, as multi-layer perceptrons (MLPs). We predict a fact as true if the output of the MLP, when input with the vector, is greater than a threshold. Each fact contains a predicate and the corresponding arguments: < subject, predicate, object > (for binary facts) and < subject, predicate > (for unary facts).\nOur proposed model significantly outperforms existing works at the important task of annotating videos with knowledge graphs. We also explore the inclusion of background knowledge in the construction of the knowledge graphs. To the best of our knowledge, no existing work has explored this.\nTo summarize our contributions,\n\u2022 We propose a new deep-learning model to annotate videos with knowledge graphs. This is a superior approach to the more common one of annotating with natural language because it avoids learning unnecessary language syntax, is easier to evaluate, and can be translated easily to different natural languages.\n\u2022 We show experimentally that our proposed model significantly outperforms existing works that aim to annotate videos with knowledge graphs.\n\u2022 We explore the inclusion of background knowledge in the extraction of the knowledge graphs, the first work to do so.\n\u2022 We present extensive ablation studies showing the contribution of each component of our model, and showing a trade-off between increased run-time and increased accuracy by varying number of individuals and predicates evaluated in the second stage."}, {"title": "Related Work", "content": "In 2015, Johnson et al. advocated for the annotation of images using scene graphs, which describe the semantic and spatial properties and relations between objects in the image. Many following works addressed the task of forming structured annotations of still images, and it is now a reasonably well-established task in computer vision [5, 15, 27, 30]. Scene-graph construction can be extended from images to videos. The resulting task, video scene-graph construction, is similar to our task of knowledge graph extraction. Both express the individuals, properties and relations in the input video. The crucial difference is that models which apply scene-graph extraction methods that were designed for images, have to process each frame separately, and then attempt to merge the graphs for each frame into one graph for the entire video. Various complicated methods have been proposed to this end [21, 23, 25]. The method of [14] is slightly different in that it first combines the objects and relations across frames, and then uses these to produce a single set of logical facts. However, it still differs significantly from our work in that we do not use tubes at all but rather have a single classifier for all frames, and then use a single learnable vector for all instances of the same object, which allows sharing of representation power across different videos. [26] propose to generate logical facts as strings, using a language model output head, but this falsely interprets the knowledge graph as ordered.\nThe most similar existing work to ours is that of [16], which predicts the individuals present in a video, and then runs an MLP for each predicate on each individual and pair of individuals to form a single knowledge graph for the entire video. The key difference in our method is that we also predict the predicates, and then use a novel method of combining the predictions for subject, predicate and object. We also differ in the inclusion of background knowledge, as shown in Section 2, though the reason we significantly outperform [16] is mostly the architecture change, rather than the background knowledge.\nAs well as the general goal of providing a compact, largely language-agnostic description of video contents, some works have employed structured annotations for more specific purposes. [13] generate scene graphs from videos in the context of robot movement. That is, the robot moves around in the environment while taking video that the annotation is made of. [20] uses structured annotations, in particular lambda expressions, which are equivalent to sets of facts from first-order logic. They take a dataset of videos and paired sentences, and then use their generated lambda expression, to train a semantic parser without supervision on the natural language sentence."}, {"title": "Method", "content": ""}, {"title": "Main Model", "content": "Let X be the possible set of input videos. Let our vocabulary consist of a set I of individuals and a set P = C \u222a R of predicates, where C and R are, respectively, unary and binary predicates. Our model then consists of"}, {"title": "Implementation Details", "content": "The encoder f consists of a pre-trained VGG19 [22] model followed by a 3-layer gated recurrent unit (GRU) [7]. As a second stream, we use a frozen copy of the I3D network [3]. We use these networks to allow comparison with [16]. Results for other networks are reported in the supplementary material. The output of the encoder is a concatenation of this I3D feature vector and a weighted sum of the first stream, weighted"}, {"title": "Experimental Results", "content": ""}, {"title": "Datasets", "content": "We train and test our model on two automatically generated datasets for video annotation, taken from[16]. The datasets MSVD* and MSRVTT* are generated from two well-known video captioning datasets: MSVD [4] and MSRVTT [28], respectively. Each training example contains captions in the form of a knowledge graph (KG), which is composed of a set of facts. Each fact contains a predicate and the corresponding arguments: < subject, predicate, object > (binary facts), and < subject, predicate > (unary facts). All the individuals and predicates are linked to entities in an ontology, WordNet [17]."}, {"title": "Main Results", "content": "The F1-score, positive, negative, and total accuracy scores generated using our model 3.1 for MSVD* and MSRVTT* datasets are given in Table 1. The results are also compared with two existing works - (1) [16] referred to as \"LG 2020\" here, and (2) [26] referred to as \"VL 2018\". To the best of our knowledge, these two are the only existing works that have attempted the task of video annotation using KG and so are used to benchmark the performance of our system.\nAs we can see, our system significantly outperforms both the models in F1-score, positive and total accuracy. Importantly, it gives superior positive accuracy, the most difficult metric to score highly on. The artificially constructed dataset we are using contains a higher percentage of negative facts than positive ones (refer[16] for more information). This means that even if the model predicts everything as false, the negative accuracy would be very high. This issue has also been highlighted in [16], where the reported model was predicting most of the facts as negative. However, our system is not doing this and so the positive accuracy, as well as the F1 score, is far better.\nInterestingly, our results for MSRVTT* are significantly better than those for the MSVD*, even though, by most video captioning models in the literature, MSRVTT is considered a harder dataset [6, 18, 29, 31, 32]."}, {"title": "Inclusion of Background Knowledge", "content": "As shown in Table 2, the inclusion of background knowledge produces a slightly better F1-score on MSVD* dataset, and positive accuracy on the MSRVTT* dataset. The reason we do not see a greater improvement may be because of the low overlap of components between our dataset and Visual Genome (see supplementary material for further details).\nTo further understand how Visual Genome predictions are being used in an extended model, we examine the behaviour when the part of the model, the predicate MLPs component, is undertrained. Figure 3 shows F1-score when training of the predicate MLPs was stopped early. The x-axis shows the number of epochs the predicate MLPs were trained for. AT x = 0, i.e. when the predicate-MLPs are untrained, the F1-score is far better when using the extended model. This signifies that when the network did not have any information about the dataset, VG statistics representing general world knowledge helped the predictions the most. As we increase the number of epochs, the network learns more about the particular dataset, and the F1-score in both scenarios becomes close to each other, with the extended model ultimately giving a comparable result on the fully trained network."}, {"title": "Qualitative Results", "content": "To further evaluate the quality of the KGs produced for the video by our proposed model, manual inspection of videos and predicted facts is carried out. Figure 4 shows the first two frames from a video with the facts predicted by the model for MSVD* (left) and MSRVTT* (right). These qualitative examples show the limitations imposed by the smaller vocabulary size in MSVD*. The individuals, attributes and relations which appear fewer than 50 times are excluded from the dataset (see supplementary for further information on the dataset). In the video in the left figure in Figure 4, the girl is playing the flute, which is also expressed in one of the MSVD captions. However, flute appears less than 50 times and so is not in the model's vocabulary and it cannot predict play(girl, flute). The model is, however, correctly able to identify it as an instrument, and also to correctly identify other attributes, relations, and individuals present in the video."}, {"title": "Ablation Studies", "content": ""}, {"title": "Ablation on combining framework", "content": "As discussed in Section 3.1, we produce candidate facts by combining the outputs from the individual-, attribute-, and relation-multi-classifiers. These candidate facts are later used to classify them as true or false to the given video and are an essential step in filtering irrelevant facts. To investigate the contribution of the combining procedure, we perform an ablation study on this network component.\nThe results for MSVD* and MSRVTT* are shown in Table 3. In the \"without combiner\" setting, the combining framework is replaced by a simple threshold method, where the output of each multi-classifier is thresholded, and all permutations of the received individuals and predicates are used to build candidate facts. The other components of the network are kept the same.\nThe results in this setting are significantly worse than the main model, where the proposed combining framework is used. This is because many irrelevant facts are fed into the predicate-MLPs. This shows the effectiveness of the combining technique proposed in Section 3.1."}, {"title": "Effect of the number of candidate facts", "content": "As we discussed in Section 3.1, after the detection stage, we choose the q highest joint probability facts as candidate facts to pass to the fusion stage. The value of q is a chosen hyperparameter and could be set to anything in the range 0 < q < 33282 (as |I|2|R| = 33282). Smaller values of q would mean the number of candidate facts fed to predicate-MLPs is small, resulting in a smaller inference time, but the F1 score will be inferior. This is because many of the candidates could be false, as the dataset consists of more negative facts than positive facts. Feeding a bigger pool of facts to the predicate-MLPs increases the chances of receiving true facts but also increases inference time.\nHere we perform experiments with different values of q to study the effect of changing the value of q on the overall performance of the main model. Figure 5 shows the performance (F1-score) as well as the time taken by the model to produce output, as q is varied from 200 \u2264 q \u2264 30000. The 'red curve' plots the F1-score on the y-axis with the corresponding q value on the x-axis. The 'yellow line' on the plot shows the time taken for the inference (min) on the second y-axis, corresponding to the q value on the x-axis and the F1-score on the first y-axis.\nInterestingly, we can see that the F1-score continuously grows, with an almost exponential increase for the first few epochs, while for higher epochs, the growth rate slows down, with a nearly linear increase in time taken for inference, with an increase in value of q. As expected, the F1-score grows fast initially, with decreasing growth rate with every increase in q. This implies that with better computational resources, our model will be able to achieve ever better performance by using a higher value of q."}, {"title": "Conclusion", "content": "This paper proposed a new deep-learning model for the task of KG extraction from videos. The KG here is composed of a set of facts that describes the relations held between individuals. We also explore the inclusion of background knowledge in the construction of KG. Further, we evaluate both our main and extended models, both qualitatively and qualitatively, and present extensive investigative and ablation studies showing the contribution of various components of our model. Our model significantly outperforms existing models and has much better generalization capability.\nFuture works include exploring the use of other datasets for injecting commonsense which is more exhaustive and comprehensive. It is also interesting to explore KG extraction from other input domains. Such as application to text, where the model could perform a task similar to open information extraction. Another extension could be to manually con-"}], "equations": ["P(a=A|D = d) = P(D = d|a = A) \u00d7 P(a = A) / P(D = d) = a^d (1-a)^{N-d} / ( \u0393(d+1)\u00d7\u0393(\u2212d+N+1) / \u0393(\u039d+2))", "E[A] = d+1/N+2"]}