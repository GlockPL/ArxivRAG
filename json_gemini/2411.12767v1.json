{"title": "Suicide Risk Assessment on Social Media with Semi-Supervised Learning", "authors": ["Max Lovitt", "Haotian Ma", "Song Wang", "Yifan Peng"], "abstract": "With social media communities increasingly becoming places where suicidal individuals post and congregate, natural language processing presents an exciting avenue for the development of automated suicide risk assessment systems. However, past efforts suffer from a lack of labeled data and class imbalances within the available labeled data. To accommodate this task's imperfect data landscape, we propose a semi-supervised framework that leverages labeled (n=500) and unlabeled (n=1,500) data and expands upon the self-training algorithm with a novel pseudo-label acquisition process designed to handle imbalanced datasets. To further ensure pseudo-label quality, we manually verify a subset of the pseudo-labeled data that was not predicted unanimously across multiple trials of pseudo-label generation. We test various models to serve as the backbone for this framework, ultimately deciding that RoBERTa performs the best. Ultimately, by leveraging partially validated pseudo-labeled data in addition to ground-truth labeled data, we substantially improve our model's ability to assess suicide risk from social media posts.", "sections": [{"title": "I. INTRODUCTION", "content": "Every year, more than 700,000 people worldwide die from suicide [1]. Tragically, this crisis is even more pronounced in adolescent populations: in 2021, suicide was determined to be the second leading cause of death for those aged 10 to 14 [2]. Suicide is a complex issue that poses no clear and consistent solution. The factors that drive individuals to commit suicide are complex and vary from individual to individual. The opioid crisis, economic recession, and access to firearms have all been named as potential contributors to this crisis [3], [4], but no single root cause can be isolated. Thus, targeting and supporting those at risk of suicide is crucial to combat this crisis, given that there is no simple solution to preventing suicidal ideation in the first place.\nPrior literature has consistently shown that suicide prevention interventions can reduce suicide attempts as well as completed suicides [5]. Among these preventative methods, both pharmacological and social intervention methods have been established as effective [6], [7]. Furthermore, restricting access to lethal means of suicide has also been shown to reduce suicide rates as well [8].\nWith social media communities where individuals with suicidal thoughts congregate to seek advice increasing in size, researchers should scour these online communities as potential sites to deploy suicide intervention methods. However, due to massive amounts of traffic on the internet, manually inspecting every post, comment, and message for traces of suicidal ideation or intent is impossible. Thus, the key to establishing effective suicide prevention systems is automating suicide risk assessment. Such a system could play a crucial role in identifying individuals at risk of suicide and providing timely intervention and support.\nPrior efforts to automate this task have utilized deep learning and Natural Language Processing (NLP) to detect suicidal ideation in social media posts [9], [10]. However, most previous works have constrained the problem to a binary classification task (i.e. \"suicidal\" versus \"not suicidal\"). This has resulted in the development of models that fail to capture varying levels of suicide risk (e.g. low risk, medium risk, high risk, etc.). Nonetheless, some progress has been made in constructing datasets that account for varied degrees of suicide risk. For example, some past efforts have involved fine-grained suicide risk datasets deriving different risk levels from the Columbia Suicide Severity Rating Scale [11], [12]. However, these efforts have largely focused on constructing supervised models, learning strictly from the ground-truth labeled data. This means that the limited availability of labeled fine-grained suicide risk data incapacitates such models on the suicide risk assessment task [13].\nTo bridge this gap, we investigated semi-supervised methods to learn from both labeled and unlabeled data. More specifically, we modified the existing self-training algorithm to optimize semi-supervised learning on highly-imbalanced datasets. After that, we compared the performances of supervised machine learning models to those utilizing our semi-supervised strategies. Through extensive empirical studies, we showed that the utilization of unlabeled data in a semi-supervised framework yielded substantial improvements in the task of identifying suicide risk levels in social media posts.\nTo summarize, our contribution in this work is three-fold:\n(1) we proposed a novel semi-supervised learning framework with class-imbalanced data to generate pseudo-labeled data;\n(2) we conducted extensive empirical studies using the Suicide Detection on Social Media BigData Cup Challenge 1 dataset to compare an array of deep learning models' ability in identifying the degrees of suicidal expressions in social media posts; (3) we manually validated a select subset of our pseudo-"}, {"title": "II. MATERIALS AND METHODS", "content": "A. Task and Data Description\nThe Suicide Detection on Social Media BigData Cup Challenge 2 involves creating a predictive model to classify Reddit posts into four suicide risk levels: 1) Indicator: The post content has no explicit expression concerning suicide; 2) Ideation: The post content has explicit suicidal expression but there is no plan to commit suicide; 3) Behavior: The post content has explicit suicidal expression and a plan to commit suicide or self-harming behaviors; and 4) Attempt: The post content has explicit expressions concerning historic suicide attempts.\nAs shown in Table I, the challenge organizers developed a corpus composed of 500 labeled and 1,500 unlabeled instances. A test dataset that includes 100 unlabeled instances is also included [12]. Each instance contains the text content of a Reddit post taken from the r/SuicideWatch sub-Reddit.\nHowever, two major challenges are presented with this dataset. The first challenge is that only a small portion of available data is labeled, limiting the efficacy of a traditional supervised approach. The second challenge is that the intrinsic class-imbalance present in the labeled dataset makes it difficult for models trained on the provided data to predict classes underrepresented in the labeled corpus (i.e. Attempt).\nB. Method Development\nIn this work, we hypothesized that a semi-supervised learning system utilizing the 1,500 unlabeled instances in addition to the 500 labeled posts could lead to better predictive performance than a supervised approach using only labeled data. An overview of the system architecture is shown in Figure 1. The"}, {"title": "1) Backbone Model Selection", "content": "In this study, we employ a BERT-based classifier as the backbone within our semi-supervised framework. Specifically, we use RoBERTa [14] to generate feature representations of input sentences given that previous literature has established that RoBERTa achieves strong performance on text classification tasks [15]. We determined that ROBERTa outperformed other transformer-based models in this task, such as BERT base [16] and Biomed-BERT [17]."}, {"title": "2) Semi-Supervised Learning with Stratified Confidence Sampling", "content": "Semi-supervised learning (SSL) is a family of machine learning algorithms designed to learn from a mix of both labeled and unlabeled data, typically in cases when the majority of the available data is unlabeled. Here, we employ the self-training strategy [18], a semi-supervised wrapper algorithm wherein a classifier f iteratively trains itself by generating a set of pseudo-labels $\\mathcal{D}_{Y}$ for an unlabeled dataset $\\mathcal{D}_{T}$, adding a subset of those pseudo-labels $\\mathcal{B}$ \u2013 typically selected by some confidence or certainty function g to the original training set, and repeating this process until the model reaches optimal performance [19]."}, {"title": "One crucial step to developing a task-specific self-training framework", "content": "is to first establish what pseudo-labeled data should be added to the training pool with each iteration. One popular approach is to find the pseudo-labeled samples whose \u201cconfidence\u201d, measured by predicted probabilities, exceeds a certain threshold [20]. However, this method struggles when dealing with imbalanced datasets. In our preliminary studies, we found that when using a confidence threshold to determine which samples to add to the training set almost all of the predictions exceeding that threshold would come from the class with the greatest number of instances in the initial labeled dataset. This problem would compound as more samples of the overrepresented class were iteratively added to the training set, resulting in an increasing confidence disparity between classes as well as diminishing performance.\nTo combat this, we propose a Stratified Confidence Sampling (SCS) acquisition algorithm that utilizes model confidence as an indicator of pseudo-label accuracy while also ensuring that class imbalances in the initial training set do not"}, {"title": "hinder the model's ability to select high-confidence instances of underrepresented classes", "content": "Specifically, SCS assigns pseudo-labels based on the prediction's confidence relative to other samples with the same predicted label. This is done by separating the initial predictions by predicted class, ordering each subset based on predicted confidence, and then selecting the p percent most-confident samples in each subset to be removed from the unlabeled set and added to the training set. The acquisition rate p is a hyperparameter that, during the development of our model, we found worked best when set to 0.25. Note that p should be within the range (0,1]."}, {"title": "Algorithm 2 Pseudocode for the Stratified Confidence Sampling algorithm", "content": "Input: $X_i$ - posts; $Y_i$ - predicted probabilities; p - acquisition rate\nOutput: $B_x$, $B_y$ - Set of posts with confident pseudo-labels\n1: $B_x\\leftarrow[]$\n2: $B_y\\leftarrow[]$\n3: for For r in range(n) do\n4: $Y_i \\leftarrow Y_{in}$ where arg $max_i (Y_{in}, axis = n) = r$\n5: $X_i \\leftarrow X_{in}$ where arg $max_i (Y_{in}, axis = n) = r$\n6: $C_i \\leftarrow max(P(y_i = Y_1), P(Y_i = Y_2), ..., P(Y_i = Y_n))$\n7: $N_r \\leftarrow p * len(C_i)$\n8: $Y_i \\leftarrow Sort(y_i by C_i)$\n9: $X_i \\leftarrow Sort(x_i by C_i)$\n10: $B_x \\leftarrow B_x + x_i[: N_r]$\n11: $B_y \\leftarrow B_y + Y_i[: N_r]$\n12: end for\n13: return $B_x$, $B_y$"}, {"title": "3) Label Correction", "content": "To ensure that the predicted pseudo-labels were as accurate as possible, we carried out our self-training algorithm five times, each time using a different subset of the ground-truth labeled data as a validation set. This process ultimately yielded five sets of pseudo-labels for the unlabeled dataset, and, to determine which label to use for each post, the majority vote was taken between the five sets of predictions.\nTo further ensure pseudo-label accuracy, we manually verified the pseudo-label for all posts where the five initial pseudo-labels were not unanimous. Similar to Active Learning [21] in intuition, we use the models' agreement to determine the samples that tend to \"confuse\" our classifier the most. We propose that labeling these samples is an efficient way to improve the quality of pseudo-labeled data while only manually validating a small portion of the pseudo-labeled dataset."}, {"title": "C. LLMs with Zero-Shot Learning and Fine-Tuning", "content": "During our preliminary experiments, we also tested the ability of LLMs (GPT-3.5-turbo and Llama3) to work as text classifiers within our semi-supervised framework. However, when attempting to feed the input posts into the GPT model, the model would repeatedly return an error, indicating that inappropriate content had been detected. As a result, we were not able to collect results using GPT-3.5-turbo and instead opted to use Llama3-8B.\nWe evaluated the capability of Llama3-8B to detect suicide risk levels in the zero-shot learning setting and fine-tuning setting. For fine-tuning, we compared two strategies: next-token prediction and sequence classification. For both zero-shot learning and fine-tuning, the prompt starts with the definitions of the four suicide risk levels, followed by the input text (see Supplementary Table I)."}, {"title": "D. Evaluation Metrics", "content": "During evaluation, we collected the micro and macro F1 scores to gauge the strength of the model's predictions. Of these two metrics, micro-F1 is derived by taking the micro average of each label's F1 score, which is computationally equivalent to accuracy, and macro-F1 is derived by averaging each class' F1 score [22]. Furthermore, we also collect precision, recall, and binary-F1 metric values for each class during the evaluation process to investigate model performance on a more granular level."}, {"title": "E. Experimental Settings", "content": "When training BERT-based models, we used categorical cross entropy [23] as our loss function and trained each model using the Adam optimizer [24] with a batch size of 8 and a learning rate set to $10^{-5}$ for 10 epochs. Because most posts in both the labeled and unlabeled data sets consist of 200 or fewer tokens, we impose a maximum token length of 250 using the models' tokenizer's built-in truncation and padding arguments to ensure uniform length between input sentences. Furthermore, we used the Inverse Class Frequency method to weight our loss function and handle the class imbalance issue. To reduce variability, we performed 5-fold cross-validation using different partitions of the data and reported precision, recall, micro and macro F1 scores. For evaluating model performance during each fold, we chose the model version with the highest accuracy on the validation set to represent our model's performance on that fold. To prevent our SCS algorithm from running for an absurd number of iterations given that the number of pseudo-labeled samples added to the initial set gets smaller and smaller with each iteration, we chose to stop the SCS algorithm once the set of unlabeled posts reached below a certain threshold, which we set to 200. The remaining posts were then labeled using a classifier trained on both the labeled and pseudo-labeled sets. From here, we compiled all initially unlabeled posts into a dataset of 1,500 pseudo-labeled posts.\nTo fine-tune Llama3-8B, we loaded the model locally with 4-bit quantization [25], and, during the training process, we used Low-Rank-Adaptation (LoRA) with hyperparameters $\\alpha = 8$ and $r = 16$, training only a fraction of the model's eight billion parameters [26]. Furthermore, we used Adam with a learning rate of $2\\times10^{-4}$ \u2013 which Dettmers et al. [27] indicates is optimal for quantized, low-rank adapted LLMs of this size \u2013 and a cosine learning rate scheduler to optimize our model."}, {"title": "III. RESULTS", "content": "A. ROBERTA Outperforms Other Models\nBefore evaluating our semi-supervised approach, we first fine-tuned three transformer-based models using a supervised approach to determine which embedding model performs the best and establish a baseline against which we can compare our semi-supervised methods. Three transformer-based models, BERT [16], BiomedBERT [17], and ROBERTa [14] were trained and validated according to our experimental settings outlined in Section II-E. Figure 2 shows the five-fold cross-validated performance of our systems. In columns labeled \"Indicator\", \"Ideation\", \"Behavior\", and \"Attempt\", we plot the binary-F1 score corresponding to posts of that class. The F1 scores yielded by our supervised models indicate that ROBERTa outperformed the other two embedding models across all F1 metrics (both class-specific and multi-class). This improvement is especially pronounced when examining the class-specific metrics. A more detailed report comparing the performance between the three models can be found in Supplementary Table II.\nDuring the evaluation, RoBERTa achieved an average micro F1 score of 0.648 (standard deviation: 0.061) and an average macro F1 score of 0.599 (standard deviation: 0.970). This represents a substantial improvement in comparison to BERT, which achieves average micro and macro F1 scores of 0.572 (standard deviation: 0.046) and 0.434 (standard deviation: 0.034). Furthermore, RoBERTa also outperformed Biomed-BERT, which obtained an average micro F1 score of 0.516 (standard deviation: 0.043) and an average macro F1 score of 0.434 (standard deviation: 0.080).\nROBERTa also exceeded the other two models in precision, recall, and binary-F1 scores, with the most noticeable improvement occurring in the F1 score for the \"Attempt\" class. For"}, {"title": "samples labeled as \"Attempt\"", "content": "RoBERTa achieves an F1 score of 0.467 (standard deviation: 0.280) in comparison to BERT's 0.000 and BiomedBERT's 0.212 (standard deviation: 0.250). Aside from outperforming the other two models across the board, another reason behind selecting RoBERTa over BERT and BiomedBERT is the latter two models' abysmal performance in classifying posts within the \u201cAttempt\" class. As a result, for the remainder of our experiments, we used ROBERTa as the backbone for our semi-supervised approaches.\nAfter deciding on RoBERTa as an embedding model, we conducted self-training with SCS pseudo-label acquisition to generate a dataset of 1,500 pseudo-labeled posts according to the methods outlined in Section II-B2. Furthermore, we also partially verified these pseudo-labels according to the strategy outlined in Section II-B3."}, {"title": "B. Correcting Initial Pseudo-Labels", "content": "After training five models and generating five sets of pseudo-labels, we calculated the agreement between each set of pseudo-labels and the majority-vote pseudo-labels using Cohen's Kappa score [28] (Supplementary Table III). The rate of agreement between the five sets of pseudo-labels is high, which makes sense given that the training data between any two of the five models is 75% similar. To determine which samples to annotate, the final set of 1,500 pseudo-labeled posts was first split into four separate subsets based on the number of votes the majority voted class received between the 5 initial pseudo-labelings. Figure 3 details the number of samples for each level of unanimity, and Figure 4 relative class frequencies for each level of unanimity. The numerical data displayed in this figure can be found in Supplementary Table IV.\nTwo annotators verified the 444 non-unanimous pseudo-labels, labeling each pseudo-label as either correct or incorrect. Of the 444 non-unanimous pseudo-labels, the first 104 were verified by both validators to ensure agreement on how to"}, {"title": "validate labels", "content": "Within these 104 posts, the validators agreed in 95.2% of cases. From there, the remaining 340 posts were then split into groups of 170, which were validated independently. Between the two validators, 287 of the 444 posts were determined to be correctly labeled (64.6% pseudo-label accuracy). Furthermore, all samples marked as incorrectly labeled were then assigned new labels according to their correct classification. Both the uncorrected pseudo-labeled dataset as well as the partially corrected pseudo-labeled dataset were used as additional training data to supplement the initial 500 labeled posts to determine how the addition of this pseudo-labeled data affects model performance."}, {"title": "C. Training with Pseudo-Labeled Data", "content": "Each pseudo-labeled dataset (n=1,500) was used as additional training data, and we evaluated ROBERTa trained on the labeled and pseudo-labeled data (n=2,000) using five-fold-cross-validation. Note that in our validation scheme, we chose to only sample validation data from the initial set of ground-truth labeled posts, not the pseudo-labeled data. This was done to prevent the model from verifying its performance against potentially incorrect labels.\nThe cross-validated F1 metrics for utilizing the two sets of pseudo-labeled data according to this strategy are shown in Figure 5. We also chose to include the F1 metrics of ROBERTa trained purely on the 500 labeled posts to serve as a point of comparison. A more detailed report comparing the three frameworks can be found in Supplementary Table V.\nAs indicated by the data, the addition of both the unverified and partially-verified pseudo-labels to the model's training data archives impressive boosts in performance, resulting in an average micro-F1 score of 0.760 (standard deviation: 0.034) and an average macro-F1 score of 0.745 (standard deviation: 0.046) for ROBERTa trained on the 500 labeled posts as well as the 1,500 partially validated pseudo-labeled data. Furthermore, this improvement is also noticeable when looking at class-specific metrics, with this effect being the most pronounced"}, {"title": "for the class least represented in the initially labeled dataset", "content": "\"Attempt\". The addition of the unverified and partially verified pseudo-labeled data boosts the F1-Attempt score from 46.7% (standard deviation: 0.282) to 58.9% (standard deviation: 0.235) and 72.0% (standard deviation: 0.102) respectively. This constitutes an improvement in F1-score by 25.3% for ROBERTa trained on both the labeled and partially verified pseudo-labeled data.\nAfter submitting our model weights to the organizers of the IEEE Sucide Detection on Social Media Big Data Cup Challenge, they evaluated our model on a 100-post test set, using weighted F1 as their primary metric. We ranked fourth out of 21 participating teams in terms of weighted F1 score (0.7463). In addition to purely evaluation metrics, the competition organizers also compiled a final score for each team, based on approach innovation, written report quality, and final weighted F1 score. When judged by this metric, we ultimately ranked second out of the participating 21 teams."}, {"title": "D. Lessons Learned from LLMs", "content": "In addition to evaluating the performance of transformer-based models, we also chose to evaluate Llama3-8B as described in Section II-C. Given that Llama3-8B is very time-consuming to fine-tune and that our results indicated Llama3-8B performed worse than RoBERTa, we did not conduct five-fold cross-validation. Therefore the results shown in Figure 6 reflect only one training fold. A more detailed classification report can be found in Supplementary Table VI.\nOur results indicate that Llama3's performance under zero-shot circumstances is similar to random chance; however, for both fine-tuning strategies, Llama3 shows an ability to assess suicide risk competitive with BERT and BiomedBERT, with fine-tuning for Causal LM achieving better performance than Sequence Classification."}, {"title": "IV. DISCUSSION", "content": "Our results indicate that the addition of pseudo-labeled data can substantially improve model performance for deep learning architectures. This insight is increasingly relevant as emerging deep-learning models require ever more data to train; however, supplementing human-annotated ground-truth labels with model-generated pseudo-labels serves as a promising path forward to alleviate this issue.\nThe results from our experiment, displayed in Section III-C, indicate that self-training with SCS pseudo-label acquisition improves model performance by a substantial margin. Furthermore, the additional step of validating the posts that are not unanimously pseudo-labeled across an ensemble of models provides an additional boost to classification accuracy; however, the majority of the improvement in model performance comes from the addition of the pseudo-labeled data, not the partial-validation. While our proposed SCS strategy exhibits empirical success, it is important to note that our method relies on the assumption that the class distribution in the unlabeled data is equivalent or near-equivalent to that of the labeled data, since the class distribution of the model's predictions, and thus the distribution of our pseudo-labeled data, is closely tied to the class distribution in our initial dataset. Future deployments of this method should be mindful of this assumption.\nAdditionally, the data we record in Section III-C suggests that self-training with SCS pseudo-label acquisition disproportionately boosts F1 scores in classes underrepresented in the initial dataset. This indicates that, in addition to addressing the shortcomings presented by traditional pseudo-label selection techniques when dealing with imbalanced datasets, our SCS strategy is also able to partially correct the disparity in model performance between imbalanced classes. Therefore, our SCS strategy also presents a promising method for addressing class-imbalanced partially labeled tasks beyond just suicide risk assessment that we urge future researchers to validate and"}, {"title": "explore", "content": "This effect is even more pronounced for ROBERTa trained on both the labeled and partially validated pseudo-labeled data. Section III-B details the class distribution of the unanimous versus the non-unanimous pseudo-labels and demonstrates that the set of \u201cconfusing\u201d samples contains a greater percentage of the \"Attempt\" class than the unanimously labeled posts. Given the large improvement in the F1 score for the \"Attempt\" class for RoBERTa trained on the corrected pseudo-labels, we believe correcting the pseudo-labels that tend to confuse the pseudo-labeling algorithm presents a way to improve model performance in predicting underrepresented classes, as those underrepresented classes tend to be the ones confusing the model most frequently.\nFurthermore, our results in Section III-D compare the performance of Llama3-8B to RoBERTa. While we ultimately found that RoBERTa achieved superior performance and was computationally cheaper, comparing the performance of LLMs to BERT-based models is an important effort to gauge the ability of these models to perform non-generative tasks. Our finding that RoBERTa outperforms Llama3-8B on text-classification tasks is consistent with prior literature [29] and poses an interesting question about the strengths and weaknesses of LLMs. However, given the specific nature of this task, our findings should not be treated as an absolute indicator of Llama3's text-classification abilities, and we implore future researchers to investigate the viability of LLMs in other text-classification tasks.\nAlthough our SCS strategy yields substantial improvements over a baseline ROBERTa classifier, there is a clear margin for improvement. The simplest improvement to our research would be to increase the size of the unlabeled dataset. Given that a pseudo-labeled dataset of size 1500 was able to increase model accuracy by over 10 percent, leveraging an additional 500, 1000, or even 1500 more unlabeled posts could further improve model performance. Future research should determine the optimal amount of unlabeled data to use by investigating at what point, if at all, the addition of pseudo-labeled data results in diminishing returns in model performance.\nFurthermore, comparing our SCS strategy to other semi-supervised strategies, especially for training on class-imbalanced datasets, would serve as a helpful tool for future researchers dealing with large amounts of unlabeled data to choose the best method for their task."}, {"title": "V. CONCLUSIONS", "content": "Faced with the growing suicide crisis, we proposed and implemented a new self-training strategy to develop automated methods of suicide risk assessment for social media posts when the available labeled data is both limited and imbalanced. We evaluated the strategy on the dataset made available through the IEEE BigData 2024 Cup task, Suicide Detection on Social Media. The results of our evaluation demonstrate that self-training with SCS pseudo-label acquisition can effectively detect varying suicide risk levels from Reddit posts with cutting-edge performance. In addition, we investigated"}, {"title": "the effect of human annotation on a subset of our pseudo- labeled", "content": "and demonstrated that it can provide even further improvements over baseline models. While our work only scratches the surface of semi-supervised learning in the suicide domain, we hope it will shed light on the development of new frameworks that can lead to gains from human-in-the-loop."}]}