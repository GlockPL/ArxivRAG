{"title": "MatrixNet: Learning over symmetry groups using\nlearned group representations", "authors": ["Lucas Laird", "Circe Hsu", "Asilata Bapat", "Robin Walters"], "abstract": "Group theory has been used in machine learning to provide a theoretically grounded\napproach for incorporating known symmetry transformations in tasks from robotics\nto protein modeling. In these applications, equivariant neural networks use known\nsymmetry groups with predefined representations to learn over geometric input\ndata. We propose MatrixNet, a neural network architecture that learns matrix\nrepresentations of group element inputs instead of using predefined representations.\nMatrixNet achieves higher sample efficiency and generalization over several stan-\ndard baselines in prediction tasks over the several finite groups and the Artin braid\ngroup. We also show that MatrixNet respects group relations allowing generaliza-\ntion to group elements of greater word length than in the training set. Our code is\navailable at https://github.com/lucas-laird/MatrixNet.", "sections": [{"title": "Introduction", "content": "The choice of representation for input features is a key design aspect for deep learning. While a\nwide variety of data types are considered in learning tasks, for example, sequences, sets, images,\ntime series, and graphs, neural network architectures admit only tensors as input. Various methods\nexist for mapping different input types to tensors such as one-hot embeddings, discretization of\ncontinuous signals, learned token embeddings of image patches or words [1, 2], adjacency matrices\n[3], positional encodings [1], or spectral embeddings [4].\nIn this paper we consider the question of what feature representations to use for learning tasks with\ninputs coming from a symmetry group. There are many examples of tasks defined over symmetry\ngroups, such as policy learning in robotics [5], reinforcement learning [6], pose estimation in computer\nvision [7], sampling states of quantum systems [8], inference over orderings of a set [9], and group-\ntheoretic invariants in pure mathematics. Past work has typically employed fixed representations\nchosen from among the known representation theory of the group. Representation theory is the\nbranch of mathematics concerned with classifying the set of representations of a group G which, in\nthis context, refers to homomorphic realizations of a group in terms of $n \\times n$ matrices. For groups\nwith well understood representation theory, for example, the symmetric group $S_n$ or $SO(n)$, this\nprovides a ready set of embeddings for converting group elements into tensors for use in downstream\nmodels."}, {"title": "", "content": "Trial and error or topological analysis has shown that the choice of group representation is critical for\nlearning [10, 11, 12, 13].\nInstead of using predefined group element representations, we propose to learn feature representations\nfor each group element using a learned group representation. That is, we learn to map group elements\nto invertible $n \\times n$ matrices which respect the symmetry group structure. There are several advantages\nto this strategy. First, unlike predefined group representations, learned representations allow the model\nto adapt to the given task and capture relevant information for the learning task. Second, learned\nrepresentations provide reasonable correlations between the learned features for different group\nelements since they incorporate algebraic structure into the model. This structure is encouraged using\nfree group generators and group relation regularization. Third, relative to learned vector embeddings,\nlearned matrix representations are very parameter efficient for encoding different group elements,\nreducing the problem to learning only representations of group generators; using sequence encoding,\nour method is able to generalize to combinatorially large or even infinite groups. Fourth, the learned\nrepresentation admits analysis in terms of the irreducible subspaces of the generators, giving insight\ninto the model's understanding of the task.\nWe integrate our learned group representation into a specialized architecture, MatrixNet, adapted for\nlearning mappings related to open problems in representation theory. We compare against several\nmore general baselines on order prediction over finite groups and estimating sizes of categorical\nobjects under an action of the Artin braid group. Through our experiments we observe that our\napproach achieves higher sample efficiency and performance than the baselines. We additionally\nshow that MatrixNet's constraints allow for it to generalize to unseen group elements automatically\nwithout the need for additional data augmentation.\nOur contributions include:\n\u2022 Formulation of the problem of learning over groups as a sequence learning problem in terms\nof group generators and invariance to group axioms and relations,\n\u2022 The MatrixNet architecture, which utilizes learned group representations for flexible and\nefficient feature learning over symmetry groups,\n\u2022 The matrix block method for constraining the network to respect the group axioms and an\nadditional loss term for learning group relations,\n\u2022 Empirical validation showing MatrixNet outperforms baseline models on a task over the\nsymmetric group and a task over the braid group related to open problems in mathematics."}, {"title": "Related Works", "content": "Mathematically Constrained Networks Many deep learning methods incorporate mathematical\nconstraints to better model underlying data. For instance, the application of graph neural networks\n[3, 14] to problems with an underlying graph structure has led to state of the art performance in many\ndomains. Deep learning models have also been designed for tasks with known mathematical formu-\nlations by parameterizing components of algorithmic solutions as neural networks and leveraging\ntheir structures for more efficient optimization [15, 16]. More broadly the field of geometric deep\nlearning [17] advocates for building neural networks which reflect the geometric structure of data\nin their latent features. When symmetries are present in data, group equivariant neural networks\n[18, 19, 20, 21] can enable improved generalization and data efficiency by incorporating known\nsymmetries into the model architecture using the representation theory of groups. Our method also\nutilizes group representations, but unlike equivariant neural networks, we use learned as opposed\nto fixed representations. We also focus on modeling functions defined on the group as opposed to\nbetween representations of the group.\nLearning Structured Representations Instead of using predefined representations as inputs,\nmany methods seek to learn mathematically structured representations from data. This idea has\nbeen applied in physics [22, 23], robotics [24], world models [25], self-supervised learning [26,\n27], and unsupervised disentanglement [28]. Park et al. [29], for example, use a combination of"}, {"title": "Background", "content": "Groups encode systems of symmetry and have been used in machine learning to build invariance\ninto neural networks to various transformations [18]. Formally, a group G is a set equipped with an\nassociative binary operation $ : G \\times G \\rightarrow G$ which satisfies two axioms: (1) there exists an identity\nelement $1 \\in G$, such that $g o 1 = 1 o g = g$, (2) for each $g \\in G$, there exists an inverse $g^{-1} \\in G$ such\nthat $g o g^{-1} = g^{-1} o g = 1$. Examples of groups include finite groups such as the dihedral group $D_4$\nwhich gives the symmetries of the square, $SO(3)$, the continuous group of 3D rotations, or $(Z, +)$\nthe infinite discrete group of integer shifts.\nSince groups may be combinatorially large or infinite, it is essential to encode their elements and\ncompositional structure in a succinct way. For many discrete groups, generators and relations provide\nsuch a description. A set of elements $S = \\{g_1,..., g_n\\} \\subset G$ are called generators if every element\nof G can be written as a composition of $g_1, g_1^{-1}, ..., g_n, g_1^{-1}$. In general, each element of G may be\nwritten in many different ways in terms of the generators; this non-uniqueness is encoded using a set of\nrelations. A set $R = \\{r_1,...,r_m\\}$ of words in the generators S are relations for G if each word $r_i$ is\nequal to the identity in G and if R generates the entire set of words equal to identity under composition\nand conjugation. The generators and relations of a group taken together are called a presentation and\ndenoted $G = (g_1, ..., g_n | r_1, ...,r_m)$. For example $D_4 = (r, f | r^4 = f^2 = frfr)$. Due to relations,\ngroup elements do not have a unique word representation. For example $frf = r^3 = r^7$ all represent\nthe same group element. By convention relations are sometimes stated as equalities instead of single\ngroup elements, for example $frf = r^3$. The free group $F_S = (g_1,..., g_n)$ is defined to have no\nrelations except for those coming from the two group axioms above.\nAn important notion for our discussion is the order of an element. If $g\\in G$ then the order of g,\ndenoted $|g|$, is the smallest k such that $g^k = e$. (For non-finite groups, k may be infinity). The order\nof the group $|G|$ is simply the number of elements in the group. For any g, Lagrange's theorem\nimplies that $|g|$ is a divisor of $|G|$ [40], which restricts the possible orders an element may take."}, {"title": "Representation Theory", "content": "Abstract group presentations are difficult to work with in many settings. Group representations map\ngroup elements to invertible matrices such that composition of group elements corresponds to matrix\nmultiplication. This gives the group a natural action on vector spaces and allow for analysis of the\ngroup using linear algebra. Formally, a representation of a group G is a group homomorphism"}, {"title": "", "content": "$\\Phi: G \\rightarrow GL(n)$ to the group of invertible $n \\times n$ matrices. That is $\\Phi(g_1.g_2) = \\Phi(g_1) \\cdot \\Phi(g_2)$. A\nproperty of $\\Phi$ is that $\\Phi(1) = I_{n \\times n}$ and $\\Phi(g^{-1}) = \\Phi(g)^{-1}$. Due to the homomorphism property, it is\nsufficient to define $\\Phi$ for generators of the group G. For example, a $2 \\times 2$ representation of $D_4$ is\ngiven by mapping r to a $\\pi/2$-rotation matrix and f to a reflection over the x-axis.\nThe representations of many groups are well classified. This provides a ready source of tensor\nrepresentations for group elements to use as inputs for neural networks. For example, for finite groups,\nby Maschke's Theorem [41], representations may be decomposed into irreducible representations.\nThat is, there exists a basis such that the representation matrices are all block diagonal with the same\nblock sizes and these blocks cannot be further subdivided. The irreducible representations may then\nbe further classified by computing character tables."}, {"title": "Symmetric and Braid Group", "content": "The braid group on n strands $B_n$ has presentation\n$(\\sigma_1, ..., \\sigma_{n-1} | \\sigma_i\\sigma_j = \\sigma_j\\sigma_i$ for $|i - j| \\geq 2, \\sigma_i\\sigma_{i+1}\\sigma_i = \\sigma_{i+1}\\sigma_i\\sigma_{i+1}$ for $1 \\leq i \\leq n - 2)$.\nThe braid group intuitively represents all possible ways to braid a set of n strands. The generators $\\sigma_i$\ncorrespond to twisting strand i over i + 1 and $\\sigma_i^{-1}$ is the reverse, twisting strand i + 1 over i. It is\ndefined topologically as equivalence classes up to ambient isotopy of n non-intersecting curves in $R^3$\nconnecting two sets of n fixed points. The braid group is infinite and though some representations are\nknown, they are not fully classified [42]. The braid group has important connections to knot theory,\nmathematical physics, representation theory, and category theory.\nThe symmetric group on n elements, denoted $S_n$, is defined as the set of bijections from $\\{1, ..., n\\}$\nto itself. It is also a quotient of the braid group $B_n$ and has a presentation similar to Eqn. 1 but with\nadditional relations $\\sigma_i^2 = 1$ for $1 \\leq i \\leq n - 1$. Here $\\sigma_i$ is the transposition (i i+1). The symmetric\ngroup has finite order $|S_n| = n!$. Representations of the symmetric group are well understood. The\nirreducible representations are parameterized by partitions of n. For more details, see [43]."}, {"title": "Categorical Braid Actions", "content": "One current active research problem in mathematics concerns actions of braid groups on categories.\nA category is an abstract mathematical structure that has objects and maps or morphisms between\nobjects, satisfying several coherence axioms. For example, the category $Vect_R$ has objects which are\nreal vector spaces and morphisms which are linear maps. Functors are maps between categories that\ntake objects to objects and morphisms to morphisms between the corresponding objects, satisfying\nseveral compatibility conditions. The action of a group G on a category C means that each group\nelement $g \\in G$ is associated with an invertible functor $F_g : C \\rightarrow C$, such that any relation $x = y$ in\nthe group implies that the corresponding functors $F_x$ and $F_y$ are naturally isomorphic.\nGiven a category on which a braid group acts, mathematicians are interested in measuring how objects\ngrow under repeated applications of elements of the braid group. The \u201csize\u201d of an object in a category\nmay be measured using a tool called Jordan-H\u00f6lder filtrations. For example, Bapat et al. [44] attempt\nto measure growth rates of objects in a specific category $C_n$ under repeated applications of certain\ntwist functors $\\sigma_{P_i}$, which define an action of the braid group $B_n$ on $C_n$. Each object in the category\nhas a Jordan-H\u00f6lder filtration giving a unique vector in $Z^n_{\\geq 0}$ of Jordan\u2013H\u00f6lder multiplicities. For\nmore details see [44] and Appendix A.\nHowever, they are only able to compute the action in certain cases and a simple formula is elusive. A\ncomplete description of the Jordan\u2013H\u00f6lder multiplicities after applying combinations of $\\sigma_{P_1}$ to one\nof the generating objects is only known for n = 3; that is, the case of the 3-strand braid group $B_3$.\nUnderstanding how these multiplicities evolve under repeated application of braids is a challenging\nopen research problem in mathematics."}, {"title": "Methods", "content": "We formulate the problem of learning a function on a symmetry group as a sequence learning problem\nusing a presentation of the group in terms of generators and relations. We propose MatrixNet which\npredicts the label using a learned matrix representation for the group. The homomorphism property\nof the representation is enforced through a combination of model design and an auxiliary loss term."}, {"title": "Problem Formulation", "content": "We consider task functions of the form $f : G \\rightarrow R^e$ where G is a finite or discrete group and the output\nspace $R^e$ may represent either a regression target or class label. While such tasks appear in computer\nvision, robotics, and protein modeling, we are particularly interested in problems in mathematics\nwhere neural models may lend additional examples and insight towards proving theorems.\nTo efficiently represent group elements in infinite or large groups, we consider a presentation of\n$G = (S | R)$ in terms of generators $S = \\{g_1,...,g_n\\}$ and relations $R = \\{r_1,...,r_m\\}$. Model\ninputs $g \\in G$ are represented by sequences of generators $(g_{i_1},..., g_{i_e})$ where $g = g_{i_1} o ... o g_{i_e}$ is of\narbitrary length $l > 0$ and $1 < i_j < n$. For convenience, we can include the identity $g_0 = e$ among\nthe generators to pad sequences without changing the group element.\nSince a single group element may be represented by different sequences, it is critical for the model\n$f_G$ to be invariant to both the group axioms and relations. That is, we desire\n$f_G(g_{i_1},..., g_{i_k}, e, g_{i_{k+1}},..., g_{i_e}) = f_G(g_{i_1},\u00b7\u00b7\u00b7,g_{i_k}, g_{i_{k+1}},\u00b7\u00b7\u00b7,g_{i_e})$ (Identity),\n$f_G(g_{i_1},..., g_{i_k}, g_j, g_j^{-1}, g_{i_{k+1}},\u00b7\u00b7\u00b7, g_{i_e}) = f_G(g_{i_1},..., g_{i_k},g_{i_{k+1}},...,g_{i_e})$ (Inverses),\n$f_G(g_{i_1},...,g_{i_k}, r_j, g_{i_{k+1}},..., g_{i_e}) = f_G(g_{i_1},..., g_{i_k},g_{i_{k+1}},...,g_{i_e})$ (Relations)."}, {"title": "MatrixNet", "content": "We propose MatrixNet (see Figure 1), a neural sequence model which models functions on a group\nG by taking as input sequences of generators for G. It achieves invariance to group axioms and\nrelations through a combination of built in constraints and an auxiliary loss term. The key part of the\nMatrixNet architecture is the matrix block which takes a group generator $g_i$ as input and outputs an\ninvertible square matrix representation $M_{g_i}$. The matrix representation $M_g$ for an arbitrary group\nelement g is defined as the product of matrix representations of generators needed to generate g. This\nmatrix representation is then flattened and passed to a downstream task model (such as an MLP)\nwhich computes the label. In what follows, we give a more detailed description of the matrix block\nand some variations on the architecture.\nSigned one-hot encoding We define the signed one-hot encoding, a modified version of the\ntraditional one-hot encoding, for encoding group generators used as input to MatrixNet. Let\n$(g_1,g_2,..., g_n)$ be a sequence of generators where $0 < i_k \\leq n$ and $e_k \\in \\{\\pm1\\}$. The signed one-hot\nencoding encodes each generator $g_{i_k}$ as a vector $U_{g_{i_k}} = e_{i_k} = [0,..., 0, e_k, 0, ..., 0] \\in R^n$ which\nis 1 or -1 in the ith entry. The identity element $g_0 = 1$ is mapped to the zero vector $v_1 = 0 \\in R^n$.\nThe signed one-hot encoding is chosen since it intuitively relates a generator and its inverse as\n$U_{g^{-1}} = -V_g."}, {"title": "Matrix Blocks and Learned Matrix Representations", "content": "The matrix block is designed as a parameterized representation of the free group $F_S$, that is, a\nhomomorphism $\\Phi : F_S \\rightarrow GL(n)$ which satisfies 3 properties: (1) the matrix $\\Phi(g) = M_g$ is\ninvertible, (2) $\\Phi(1) = I_{n \\times n}$, and (3) if $\\Phi(g) = M_g$ then $\\Phi(g^{-1}) = M_{g}^{-1}$. In what follows $U_{g_{i_k}}$ is"}, {"title": "", "content": "the signed one-hot encoding of the generator $g_{i_k}$ and $W$ is a learnable parameter matrix. For a group\nelement $g = g_{i_1} ... g_{i_n}$, the matrix block is defined\n$A_k = Reshape(WU_{g_{i_k}})$\n$M_{g_i} = MatrixExp(A_k)$\n$M_g = M_{g_{i_1}} M_{g_{i_2}}... M_{g_{i_n}}$\nwhere $Reshape$ reshapes a vector in $R^{n^2}$ into a square $n \\times n$ matrix."}, {"title": "", "content": "Proposition 1. Matrix Block defines a representation of the free group.\nProof. Property (1) is satisfied since the outputs of a matrix exponential are invertible. Properties (2)\nand (3) follow from $v_{g^{-1}} = -V_{g_{i_k}}, V_1 = 0$, and properties of the matrix exponential,\n$M_1 = MatrixExp(0_{n \\times n}) = I_{n \\times n}$\n$M_{g_{i_k}}^{-1} = MatrixExp(-A_k) = M_{g_{i_k}}^{-1}$."}, {"title": "Variations of Matrix Block", "content": "We now present some variations of this simple design that satisfy the group homomorphism properties.\nLinear Network (MatrixNet-LN) The first variant replaces the single parameter matrix W with a\nlinear network that has two parameter matrices $W_1, W_2$. The linear network matrix block changes\nthe computation of the intermediate $A_k$ matrix to:\n$A_k = Reshape(W_2W_1U_{g_{i_k}})$\nThis is still a linear function of $U_{g_{i_n}}$ meaning Proposition 1 still holds by the same argument.\nWhile this variation does not give increased expressivity over the original formulation of the matrix\nblock, the linear network can change the optimization landscape leading to different performance in\npractice. This variation is called MatrixNet-LN.\nNon-Linearity (MatrixNet-NL) The second variation introduces an element-wise odd non-linear\nfunction f. That is $f(-x) = -f(x)$ as with tanh. The non-linear matrix block modifies\n$A_k = Reshape(f(W_1v_{g_{i_k}}))$\nProposition 2. Non-linear matrix block defines a representation of the free group.\nProof. Property (1) is satisfied by the same argument in Proposition 1. Since f is an odd func-\ntion, $f(W_1v_1) = f(0)$ and $Reshape(f(W_1v_{g^{-1}})) = Reshape(f(-W_1v_{g_{i_k}})) = -A_k$. Therefore\nProperties (2) and (3) are satisfied by the same argument in Proposition 1."}, {"title": "", "content": "This variation can be combined with the first as Proposition 2 holds for linear transformations of $A_k$.\nThat is, $A_k = Reshape(W_2f(W_1v_{g_{i_p}}))$. Unless otherwise noted we set $f = tanh$.\nBlock-Diagonal (MatrixNet-MC) The third variation is inspired by the decomposition of repre-\nsentations into irreducible representations. For certain classes of groups such as finite groups, every\nrepresentation decomposes such that the matrices $M_g$ have a consistent block diagonal structure\nin some basis. Thus to learn an arbitrary representation of the group G, it suffices to learn a block\ndiagonal representation assuming the blocks are large enough.\nThis variation learns l intermediate $n_j \\times n_j$ matrices $A_{k_j}$, which are combined to form a block-\ndiagonal $n \\times n$ matrix $A_k$ where $n = \\sum_{j=1}^l n_j$. The block-diagonal matrix block is defined with\n$A_{k_j} = Reshape(W_jv_{g_{in}})$ for j = 1 to l\n$A_k = BlockDiag(A_{k_1}, A_{k_2},..., A_{k_e})$"}, {"title": "", "content": "Note that $MatrixExp$ and matrix multiplication preserve the block structure. If the sizes $n_j$ are fixed\nto all be equal, this formulation can be implemented as a multi-channel matrix block where both $A_k$\nand $M_{g_i}$ are $l \\times n_j \\times n_j$ tensors with l channels.\nEach $A_{k_j}$ is calculated identically to $A_k$ in the original matrix block formulation, and $BlockDiag$ is\nlinear, so Proposition 1 still holds. This variation is also compatible with the previous two variations.\nIn our experiments, we implement a 3-block version called MatrixNet-MC with a single linear layer\nand no non-linearity."}, {"title": "Enforcing group relation invariances", "content": "The matrix block is constrained to learn a representation of the free group $F_S$. As a consequence\nMatrixNet will satisfy (2) and (3) as desired. However, most groups have relations which cannot be\nenforced through a simple weight-sharing scheme used in equivariant architectures [19]. We propose\nto learn the relations through a secondary loss which measures how closely the representation respects\nthe group relations. More concretely, let $G = (S | R)$ be a group with relations $r_i \\in R$. The loss is:\n$L_{rel} = \\sum_{r_i\\in R}(||M_{r_i} - I_{n \\times n}||)$\nSince MatrixNet learns a representation that is invariant to group axioms, it is sufficient to sum over\nonly $\\{r_i\\}_1^l$ and not all compositions of relations. For architectures which do not respect the free\ngroup structure, the relations $r_i$ alone may not guarantee that all equivalent words have identical\nfeature representations, requiring potentially combinatorial amounts of data augmentation. This\nallows MatrixNet to both efficiently learn group relation invariance and simply verify this invariance\nwithout any data augmentation."}, {"title": "Experiments", "content": "We use two learning tasks to evaluate the four variants of MatrixNet and compare our approach\nagainst several baseline models. We use several finite groups on a well understood task as an initial\ntest to validate our approach and then move on to an infinite group, the braid group $B_3$, on a task\nrelated to open problems. As baselines, we compare to an MLP for fixed maximum sequence length\nand LSTM and Transformer models on longer sequences. Baseline model parameters were chosen so\nall of the models have approximately the same number of trainable parameters."}, {"title": "Order Prediction in Finite Groups", "content": "The first task is to predict the order of group elements in finite groups. Elements of finite groups are\ninput as finite sequences of generators as described in Section 3.3. The typical efficient algorithm for\ncomputing the order involves disjoint cycle decomposition, making order classification a non-trivial\ntask. See Appendix B.1 for more details on the sampling method and data splits.\nModels Compared We compare MatrixNet variants and three baselines for order prediction in $S_{10}$:\n(1) MLP with 3 layers with hidden dimension 256 and SiLU activations, (2) Fixed representation\ninput to a 2-layer classifier MLP with 256 hidden dimensions and SiLU activations, (3) LSTM\ninput to a 2-layer LSTM with 256 hidden dimensions with a subsequent MLP classifier using SiLU\nactivations, (4) MatrixNet-LN with a 2-layer 256 hidden dimension matrix block and classifier\nnetwork with SiLU activations. (5) MatrixNet-MC with a 2x2 matrix block size over 5 matrix\nchannels and classifier network with SiLU activations. (6) MatrixNet-NL with a 2-layer 256 hidden\ndimension matrix block with SiLU activations and classifier network with SiLU activations. The\nprecomputed representation is an ablated version of MatrixNet using a fixed representation of $S_{10}$\ninstead of a learned one. For the fixed representation, we use the standard 10 \u00d7 10 representation given\nby the permutation matrices corresponding to the group element. In MatrixNet-LN, the activation\nbetween layers of the matrix block is set to a linear passthrough while in MatrixNet-NL the activation\nis specified to be SiLU. MatrixNet-MC enforces a 2x2 block diagonal structure on the learned\nrepresentations corresponding to the 2-dimensional irreps of $S_{10}$.\nWe also note the use of SiLU activation in our $S_{10}$ MatrixNet model. Due to the generator self-inverse\nproperty we need not consider separate generator inverses, and so the odd function requirement given\nin Proposition 2 is not applicable."}, {"title": "Categorical Braid Action Prediction", "content": "In our second experiment, we train models to predict the Jordan\u2013H\u00f6lder multiplicities from braid\nwords in the braid group $B_3$ (see Section 3.4). The task is formulated as a regression task with\na mean-squared error (MSE) loss function. The Jordan-H\u00f6lder multiplicities are integers, so we\nevaluate accuracy by rounding the vector entries to the nearest integer. This accuracy is reported as\nan average accuracy over the three entries of the Jordan-H\u00f6lder multiplicities vector. Elements of $B_3$\nare generated by two generators $\\sigma_1, \\sigma_2$ and their inverses and are encoded using a signed one-hot\nencoding. For more details on the dataset generation process and data splits see Appendix B.2.\nWe additionally performed an experiment to evaluate how well MatrixNet generalizes to unseen braid\nwords longer than those seen in training. For this experiment, we compare against the MLP and\nLSTM since these were the highest performing baselines."}, {"title": "Visualizing the Learned Representations", "content": "We present some visualizations of the learned representations of the braid group from the highest\nperforming variant, MatrixNet-NL. Figure 2 shows visual plots of the learned representations. In"}, {"title": "Conclusion", "content": "In this paper we have presented MatrixNet, a novel neural network architecture designed for learning\ntasks with group element inputs. We developed 3 variations of our approach which structurally\nenforce group axioms and a regularization approach for enforcing invariance to relations. We evaluate\nMatrixNet on two group learning problems over several finite groups and $B_3$ and demonstrate our\nmodel's performance and ability to automatically generalize to unseen group elements. In future\nwork we plan to develop interpretability analysis methods based on group representation theory to\nbetter understand the structure of MatrixNet's learned representations. Understanding the learned\nrepresentations may provide valuable insights and explanations of the model outputs assisting with\ngenerating new conjectures for open mathematical research problems.\nLimitations The current work relies on the assumption that the studied group is finitely presented\nwhich limits us to discrete groups. However, learned group representations may also be useful for\nlearning over Lie groups. In such case, extending our method will require working with infinitesimal\nLie algebra generators. Additionally, while the group axioms are strictly enforced by the model\nstructure, the fact the relations are enforced using auxiliary loss terms means the homomorphism\nproperty is not exact. Future work may explore methods of reducing this error."}, {"title": "More details about the categorical braid group action", "content": "The aim of this appendix is to provide a few more details about the particular categorical braid group\naction that we use in our experiments."}, {"title": "Sketch of the construction of the category", "content": "The category $C_n$ we consider is the 2-Calabi-Yau triangulated category associated to the Dynkin\ngraph of type $A_n$. This is an undirected graph with n vertices and n 1 edges arranged in a line, as\nshown in Figure 4."}, {"title": "", "content": "Let $I_n$ be the Dynkin graph of type $A_n$. Let $I^{dbl}$ be its doubled quiver, which is a directed graph in\nwhich each undirected edge of I is replaced by a pair of oppositely oriented directed edges, as shown\nin Figure 5."}, {"title": "", "content": "Recall that the path algebra of a directed graph (or quiver) Q over some field k is generated as a\nfree vector space by all possible paths in Q, including the trivial paths at each vertex. The product in\nkQ is given as follows: let q: a \u2192 b be a path and p: c \u2192 d be a path. The product pq is equal to\nzero unless b = c. If b = c, then the product pq is simply the composite path a \u2192 b \u2192 d. The path\nalgebra of Q is denoted kQ.\nWe are interested in a quotient of the path algebra of $I^{dbl}$ called the zig-zag algebra, and denoted $Z_n$.\nTo obtain $Z_n$, we impose the following relations on $I^{dbl}$. In what follows, (i\u00b11) represents the\nunique arrow i \u2192 i \u00b1 1."}, {"title": "", "content": "\u2022 For each i, set\n$(i + 1|i)(i + 2|i + 1) = 0 and (i \u2013 1|i)(i \u2013 2|i \u2014 1) = 0$.\n\u2022 For each i, set\n$(i + 1|i)(i|i + 1) = (i \u2013 1|i)(i|i \u2212 1)$.\nConsequently, in the zig-zag algebra, any path of length at least 3 is automatically zero, and the only\nsurviving paths of length 2 are back-and-forth loops starting from any vertex (and all possible such\nloops are set to be equal). The paths of length 0 and 1 remain as-is.\nLet $Z_n - proj$ be the category of (graded) projective modules over $Z_n$. The category $C_n$ is constructed\nas a differential graded version of the bounded homotopy category of complexes of projective modules\nover $Z_n$, in which we identify the internal grading shift with the homological grading shift, and\nconsequently also the triangulated shift. For more details, see, e.g. [44, Section 6]."}, {"title": "Important properties of the category", "content": "In this section", "is\ntriangulated": "there is a a triangulated shift functor [1", "1": "is denoted [n", "B[n": "for any\ninteger n.\nThe category $"}]}