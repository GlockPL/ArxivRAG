{"title": "GraphMoRE: Mitigating Topological Heterogeneity via Mixture of Riemannian Experts", "authors": ["Zihao Guo", "Qingyun Sun", "Haonan Yuan", "Xingcheng Fu", "Min Zhou", "Yisen Gao", "Jianxin Li"], "abstract": "Real-world graphs have inherently complex and diverse topological patterns, known as topological heterogeneity. Most existing works learn graph representation in a single constant curvature space that is insufficient to match the complex geometric shapes, resulting in low-quality embeddings with high distortion. This also constitutes a critical challenge for graph foundation models, which are expected to uniformly handle a wide variety of diverse graph data. Recent studies have indicated that product manifold gains the possibility to address topological heterogeneity. However, the product manifold is still homogeneous, which is inadequate and inflexible for representing the mixed heterogeneous topology. In this paper, we propose a novel Graph Mixture of Riemannian Experts (GraphMoRE) framework to effectively tackle topological heterogeneity by personalized fine-grained topology geometry pattern preservation. Specifically, to minimize the embedding distortion, we propose a topology-aware gating mechanism to select the optimal embedding space for each node. By fusing the outputs of diverse Riemannian experts with learned gating weights, we construct personalized mixed curvature spaces for nodes, effectively embedding the graph into a heterogeneous manifold with varying curvatures at different points. Furthermore, to fairly measure pairwise distances between different embedding spaces, we present a concise and effective alignment strategy. Extensive experiments on real-world and synthetic datasets demonstrate that our method achieves superior performance with lower distortion, highlighting its potential for modeling complex graphs with topological heterogeneity, and providing a novel architectural perspective for graph foundation models.", "sections": [{"title": "Introduction", "content": "Capturing the characteristics of structures with low distortion embeddings is critical to graph representation learning, while how to effectively improve the expressive capacity to adapt to topological heterogeneity remains an unresolved challenge. Particularly with the development of graph foundational models, there is an increasing necessity to uniformly process graph data from a wide variety of types and domains, imposing higher demands on adaptive feature extraction of complex structures. Most existing graph representation learning methods (Kipf and Welling 2017; Veli\u010dkovi\u0107 et al. 2018; Ying et al. 2021; Hou et al. 2024), including existing works on graph foundation models(Liu et al. 2023b; Sun et al. 2023; Zhao et al. 2024; Liu et al. 2023a; Mao et al. 2024), embed graphs into Euclidean space, which are not conducive to capture the complex topological structures (Bronstein et al. 2017; Nickel and Kiela 2017).\nIn recent years, Riemannian representation learning has received wide attention due to the ability to naturally represent different topological structures (Yang et al. 2024; Fu et al. 2024). However, real-world graphs always exhibit topological heterogeneity, meaning they are hybrids of substructures with different topological characteristics, each suited to be represented in different curvature spaces. Most of previous works (Liu, Nickel, and Kiela 2019; Chami et al. 2019; Bachmann, B\u00e9cigneul, and Ganea 2020; Zhang et al. 2021) exclusively consider a single constant curvature space, ignoring the problem of topological heterogeneity. Although some recent studies (Wang et al. 2021; Sun et al. 2022; Cho et al. 2023) attempt to construct a mixed-curvature space (i.e., product manifold) that incorporates multiple Riemannian manifolds with different curvatures, they globally learn the representations of all nodes. In other words, the product manifold is homogeneous(Di Giovanni, Luise, and Bronstein 2022) with the globally uniform curvature for each point in space, which is inadequate and inflexible for representing the mixed heterogeneous topology.\nConsidering the heterogeneity of topological patterns across different substructures of the graph, we aim to adaptively embed structures with different topological patterns in corresponding optimal embedding space, rather than applying the same curvature globally to the whole graph. To achieve automatic adaptation to heterogeneous topologies, there are two main issues need to be addressed. The first issue is about topology pattern identification. The topological patterns of substructures in a graph are often not explicitly expressed and vary with resolution, making it difficult to directly divide the graph into subgraphs with different topological patterns and process them separately. The second issue is about optimal embedding space selection. The appropriate type of Riemannian space and the optimal curvature vary from different topological properties. Moreover, the substructure of a graph is still complex and difficult to classify as a single type such as tree-like or cycle-like. Therefore, selecting the appropriate embedding space for different topological patterns also presents challenges.\nTo address the first topology pattern identification issue, we approach it from the perspective of local topology characterization, transforming the problem of finding substructures with different topological patterns into estimating the geometric properties around each node, thus providing a foundation for heterogeneity processing of the graph. To address the second optimal embedding space selection issue, we introduce Mixture-of-Experts (MoE) to model the complex geometric properties. In the MoE architecture, the experts can naturally correspond to different curvature spaces and construct personalized mixed curvature spaces for different nodes, while the gating module can adaptively route different nodes to the appropriate embedding space.\nTo this end, we propose GraphMoRE, a Riemannian graph MoE to address the problem of topological heterogeneity. Specifically, we first propose a topology-aware gating mechanism, consisting of a multi-resolution local topology encoding module to estimate local geometric properties and a gating network which assigns Riemannian expert weights for nodes to route them to corresponding optimal embedding space. To achieve adaptive routing for nodes with different geometric properties, the gating network is guided by the goal of encouraging higher weights to be assigned to curvature spaces with lower distortion. Additionally, to adapt to complex geometric properties, we design diverse Riemannian experts considering the type of spaces and the degree of curving in space. The experts performs representation learning in differentiated curvature spaces to flexibly construct personalized mixed curvature spaces for nodes through expert weights. Note that, our method is equivalent to the graph is embedded into a heterogeneous manifold with different curvatures at each point. Therefore, we provide an alignment strategy to measure the pairwise distance between nodes in different embedding spaces. We compare GraphMORE with relevant methods in Table 1 and our method enjoys the advantage of learning fine-grained embeddings in heterogeneous manifold with diverse curvatures. Our contributions are as follows:\n\u2022 We propose analyzing topological heterogeneity from the perspective of local topology characterization, construct personalized embedding spaces for nodes, to minimize the distortion of heterogeneous topologies.\n\u2022 To the best of our knowledge, we are the first to introduce the MoE into Riemannian representation learning, providing a new perspective for the design of graph foundation models based on Riemannian geometry.\n\u2022 Extensive experiments demonstrate that our method outperforms advanced relevant methods, highlighting the potential of GraphMoRE on topological heterogeneity."}, {"title": "Related Work", "content": "Riemannian Graph Learning\nRiemannian space has been introduced into graph representation learning and received wide attention (Peng et al. 2021; Yang et al. 2022). To address the limitation of single constant curvature spaces in topological heterogeneity, Q-GCN (Xiong et al. 2022) introduces the pseudo-Riemannian manifold into GNNS. KHGCN (Yang et al. 2023) utilizes discrete curvature to improve message passing, alleviating inconsistency between local structure and global curvature. These works are orthogonal to ours.\nIn this paper, we mainly focus on the works related to mixed curvature space, which is mainly designed for non-uniform structured data (Gu et al. 2019; Skopek, Ganea, and B\u00e9cigneul 2020). DyERNIE (Han et al. 2020), M2GNN (Wang et al. 2021) and AMCAD (Xu et al. 2022) respectively introduce product manifold into knowledge graph and retrieval system. SELFMGNN (Sun et al. 2022) proposes a mixed curvature graph contrastive learning, designs a hierarchical attention mechanism to fuse the representations of different curvature subspaces. FPS-T (Cho et al. 2023) develops the graph transformer to mixed curvature space. MofitRGC (Sun et al. 2024) introduces a diversified factor in the product manifold to provide flexibility. However, the product manifold proposed in most of these methods is still homogeneous. Although MofitRGC extends the original product manifold, its improvement is not intuitive and fundamental enough and is still insufficient to represent mixed heterogeneous topologies. The method proposed in this paper directly constructs personalized heterogeneous manifold for different geometric properties.\nGraph Mixture of Experts\nMixture of Experts (Jacobs et al. 1991; Shazeer et al. 2017) aims at training multiple experts with different skills, widely used in fields such as large language models. However, the exploration of MoE on graphs is still in the early stage. GraphDIVE (Hu et al. 2022) and G-Fame++ (Liu et al. 2023c) utilize MoE to learn diverse feature representations of graph, applying to imbalanced graph classification and fair graph representation learning. Considering the complex mixture of homophily and heterophily, GMoE (Wang et al. 2023) and Node-MOE (Han et al. 2024) respectively propose using GNN with different hop numbers and different Laplacian filters as experts to enhance the ability to adapt to complex graph patterns. ToxExpert (Kim et al. 2023) points out that a single GNN model cannot learn molecules with different structure patterns well and proposes to train multiple topology-specific expert models. GraphMETRO (Wu et al. 2024) addresses out-of-distribution shifts by using a gating module to predict shift types and experts to generate shift-invariant representations. Note that, the diversity of experts and the design of the gating network are crucial. In this paper, we will mainly introduce how to design the components of MoE to mitigate the etopological heterogeneity."}, {"title": "Preliminary", "content": "Riemannian Manifold\nManifold. A manifold is a generalization of high-dimensional surface, possessing the properties of Euclidean space locally. A Riemannian manifold is a smooth manifold coupled with a Riemannian metric g. In the tangent space $T_xM^d$ of a point x on a d-dimensional Riemannian manifold $M^d$, the Riemannian metric $g_x$ defines a positive definite inner product $T_xM^d\\times T_xM^d \\rightarrow R$, which is used to define the geometric properties and operations of the Riemannian manifold. The transformation of vector v between manifold $M^d$ and tangent space $T_xM^d$ is performed through logarithmic mapping $log_x(v): M^d \\rightarrow T_xM^d$ and exponential mapping $exp_x(v): T_xM^d \\rightarrow M^d$ at point x. In Riemannian space, GNNs generally map nodes to the tangent plane through $log_0(\\cdot)$ for message passing and aggregation, and then map nodes back to Riemannian space through $exp_0(\\cdot)$, where 0 is the reference point.\nConstant Curvature Spaces. Curvature is a geometric quantity that describes the degree of curving in space. In the Riemannian manifold, the sectional curvature $\\kappa(x)$ at each point x is defined. For a manifold with equal sectional curvature at all points, it is defined as a constant curvature space $M^d$. According to the sign of curvature K, constant curvature spaces can be divided into three categories: spherical space ($\\kappa > 0$), hyperbolic space ($\\kappa < 0$), and Euclidean space ($\\kappa = 0$), which are suitable for modeling data with different topologies respectively.\nProblem Formulation\nNotations. A graph can be described as G = (A,X), where $A \\in {0,1}^{N\\times N}$ is the adjacency matrix, $X \\in R^{N\\times d}$ is the node feature matrix, N denotes the number of nodes and d denotes the dimension of node feature. Let V, E be the set of nodes and edges in the graph, respectively.\nProblem Definition. We primarily consider the problem of Riemannian representation learning on the graph with topological heterogeneity. Given a graph G = (A, X), in which various topological patterns exist. Our goal is to learn an encoding function $\\varphi : V \\rightarrow P$, which captures the geometric properties around node v and adaptively maps node v to the personalized embedding space P, where P is composed of diverse curvature spaces. Unlike existing work on mixed curvature, we embed different topological patterns into the corresponding optimal curvature spaces to maximize the preservation of the graph structure, rather than embedding all nodes into the same curvature space."}, {"title": "GraphMoRE", "content": "In this section, we present GraphMoRE, a novel Graph Mixture of Riemannian Experts framework to address the problem of topological heterogeneity. The key insight is that we decompose topological heterogeneity from the perspective of local topology characterization. In brief, we first introduce diverse Riemannian experts (Sec. 4.1) to provide a foundation for flexibly constructing diverse embedding spaces. Then, we design a topology-aware gating mechanism (Sec. 4.2) to capture geometric properties around nodes and construct personalized mixed curvature spaces for each node (Sec. 4.3). Overall framework is shown in Figure 1.\nDiverse Riemannian Experts\nTo perform heterogeneity processing on different topological patterns, we aim to provide personalized embedding spaces. However, the substructures of the graph are still complex, such that a single constant curvature space is not sufficient to match their geometric shapes well. Therefore, we consider multiple Riemannian spaces with different curvatures to better model local topology. In our method, we design each expert model as the Riemannian GNN in different curvature spaces. More importantly, we consider the diversity of Riemannian experts and provide personalized mixed curvature spaces through the gating network.\nRiemannian GNN. Before discussing further, we briefly introduce the operations of Riemannian expert with the backbone of the $\\kappa$-stereographic model. Benefit from the $\\kappa$-stereographic model provides a unified framework to describe manifolds with positive, negative, and zero curvature, our Riemannian experts learn embeddings in the unified space $U_K$. Specifically, $\\kappa$-stereographic model is a smooth manifold $M_{\\kappa} = {x \\in R^d | -\\kappa||x||^2 < 1}$. When $\\kappa > 0$, $M_{\\kappa}$ is the stereographic sphere model, while $M_{\\kappa}$ is the Poincar\u00e9 ball model of radius $1/\\sqrt{-\\kappa}$ when $\\kappa < 0$.\nThe message passing and aggregation of Riemannian GNN in the $\\kappa$-stereographic model can be formulated as:\n$h_u^{(l+1)} = Agg (log_0 (h_v^{(l)}, v \\in\\mathcal{N}(u)))$, (1)\n$h_u^{(l+1)} = exp_0 (Comb (h_u^{(l)}, h_u^{(l+1)}))$, (2)\nwhere $log_0$ and $exp_0$ are exponential and logarithmic maps (Bachmann, B\u00e9cigneul, and Ganea 2020), $\\mathcal{N}(\\cdot)$ denotes the neighbor nodes, $Agg(\\cdot)$ and $Comb(\\cdot)$ are neighbor aggregation and combination operators.\nDiversity of Experts. To enhance the ability to model complex topological patterns, we consider the diversity of Riemannian experts in terms of both the type of curvature spaces and the degree of curving in space. For the type of curvature spaces, we divide Riemannian experts into three categories, focusing on hyperbolic space H, spherical space S, and Euclidean space E respectively, where Euclidean space is a special case of the $\\kappa$-stereographic model with zero curvature. Considering that Riemannian GNN only slightly adjusts the curvature within a relatively limited range (Fu et al. 2021), the initial value of curvature is crucial, which means that even if the type of curvature space has been considered, it is still necessary to consider the degree of curving in space. Therefore, we assign differentiated values to each type of Riemannian experts to initialize their curvature. The diverse Riemannian experts are denoted as $E = {E_{H_1}, E_{H_2}, \u2026, E_E, ..., E_{S_K}}$, each corresponding to a Riemannian GNN in different curvature spaces. Thus, we can flexibly construct personalized embedding spaces by fusing the representations of diverse Riemannian experts with the outputs of the gating network, providing the ability to model different topological patterns.\nTopology-aware Gating Mechanism\nAs mentioned before, the topological patterns of substructures are often not explicitly expressed. We consider approaching it from the perspective of local topology characterization, transforming the problem into estimating the geometric properties around nodes. Specifically, we design a topology-aware gating mechanism, which includes multi-resolution local topology encoding and distortion guided gating network, to capture the geometric properties around nodes and route them to personalized embedding spaces.\nMulti-resolution Local Topology Encoding. To extract the characterization of the topology around nodes, we first consider local topology sampling centered on the nodes:\n$s_u = Sampler(v, r)$, (3)\nwhere $s_u$ represents the subgraph sampled around the node $v$, $Sampler(\\cdot,\\cdot)$ denotes the sampling strategy that can be flexibly chosen, r denotes the scale of sampling. Considering that the local geometric properties exhibit some variation with changes in resolution, to obtain more abundant local topological characterizations, we further perform multi-resolution local topology sampling by Eq. (3) as follows:\n$S_v = {Sampler(v, r), r \\in R}$, (4)\nwhere $S_v$ represents the sampled local topology subgraphs and R denotes the scale set of sampling.\nThen, we introduce a GNN $\\Phi(\\cdot)$ designed for encoding the local topology, which embeds and pools the sampled local topology subgraphs $S_v$, and concatenates them to obtain the local topology characterizations of node v:\n$T_v = || Pooling(\\Phi(s), s \\in S_v)$, (5)\nwhere $|\\mid$ denotes concatenation operation and $Pooling(\\cdot)$ denotes the pooling function.\nDistortion Guided Gating Network. After encoding the local topology of all nodes, we estimate the local geometric properties around the nodes and select the optimal embedding space for them. Specifically, the gating network $\\varphi(\\cdot)$ receives the local topology characterizations T and outputs the weights of different Riemannian experts for each node:\nW_v = Softmax(\\varphi(T_v)), (6)\nwhere $W_v = {W_{v,\\epsilon} | \\epsilon \\in E}$ is a weight vector, with each element representing the weight value of the corresponding Riemannian expert for node v. The topology characterizations of the sampled local topology subgraphs are similar for neighboring nodes, which ensures that the gating network assigns them nearly identical expert weights, enabling them to be embedded into similar embedding spaces.\nFor specific topological patterns, the gating network is expected to assign higher weights to the appropriate Riemannian experts. For the example in Figure 1, the topology around node u is tree-like, so the output of the gating network is more biased towards hyperbolic Riemannian experts with negative curvatures, and experts with more matching curvatures are assigned relatively higher weights. To enable the gating network to have such ability, we consider embedding distortion as it can measure the inconsistency between the embedding distance of Riemannian experts and the graph distance. Specifically, for nodes in substructures with specific geometric properties, inappropriate expert weights cause these nodes to be embedded in mismatched curvature spaces, resulting in higher distortion. The optimization objective of minimizing the embedding distortion can promote the gating network to assign appropriate expert weights for nodes, which can be formalized as:\n$L_D = \\frac{1}{2} \\sum_{i,j\\in V} (\\frac{d(i, j)}{g(i, j)} - 1)^2$, (7)\nwhere $g(i, j)$ is the shortest path distance between node i and j on the graph, $d(i,j)$ is the distance between them in the Riemannian embedding space constructed in our method, which is related to the mixture and alignment of experts and will be described in detail in the next section.\nMixture and Alignment of Experts\nTo construct a personalized embedding space for each node, we consider fusing the output of Riemannian experts with the expert weights which are assigned based on the local geometric properties, thus enabling heterogeneity processing of different nodes. The output of Riemannian experts is denoted as $Z = {Z_{\\epsilon},\\epsilon \\in E}$, and the embedding of node v can be formulated as:\n$Z_v = ||_{\\epsilon \\in E} W_{v,\\epsilon} \\& Z_{\\epsilon}$, (8)\nwhere $Z_{\\epsilon}$ represents the embedding representation of node v in the curvature space of expert $\\epsilon$, $\\& $ denotes the product operation defined on the manifold of expert $\\epsilon$, that is, $we \\& x = exp_0 (w \\cdot log_0(x))$. After the mixture of Riemannian experts, each node is embedded into a personalized mixed curvature space, which is equivalent to the graph is embedded into a heterogeneous manifold with varying curvatures at different points, thereby maximizing the preservation of mixed heterogeneous topological structures.\nMoreover, due to the different spaces in which nodes are embedded, directly calculating the pairwise distance between nodes results in deviation. To achieve concise and effective embedding alignment, we consider integrating the expert weights of nodes u and v for each node pair (u, v):\n$W_{(u,v)} = Softmax(W_u \\cdot W_v)$, (9)\nwhere $W_{(u,v)}$ is the aligned expert weight. We remap node u and v through $W_{(u,v)}$ to an aligned embedding space. The aligned embedding space is relatively advantageous for both when the original embedding spaces of two nodes are similar. For example, in Figure 1, the weights of nodes u and v are biased towards hyperbolic experts, and the aligned weight $W_{(u,v)}$ can highlight experts which are important for both while reducing the weights of other experts. Otherwise, the aligned embedding space is neutral between the original embedding spaces of both. The embedding distance between nodes can be formulated as:\n$d^2 (u, v) = \\sum_{\\epsilon \\in E} W_{(u,v)} d_{\\epsilon} (Z_u, Z_v)$. (10)"}, {"title": "Optimization Objective", "content": "The overall optimization objective consists of the downstream task objectives and minimization of embedding distortion, which can be formulated as:\n$L = L_{task} + \\lambda L_D$, (11)\nwhere $\\lambda$ is a trade-off hyperparameter. The overall process of our method is summarized in Algorithm 1."}, {"title": "Comlexity Analysis", "content": "The overall time complexity is O(|R||D| + |Ds| + K|D|). Separately, the complexity of local topology sampling is O(|R||D|), and the complexity of the gating mechanism is O(Ds), where |D| is the size of the input graph and |Ds| is the size of the sampled subgraphs. The encoding process involves K experts with the complexity of O(K|D|)."}, {"title": "Experiment", "content": "To evaluate GraphMoRE\u00b9 proposed in this paper, we conduct comprehensive experiments and further analyze the effectiveness of GraphMoRE on topological heterogeneity.\nExperimental setup\nDatasets. We conduct experiments on a variety of real-world datasets, including citation networks (Cora (Sen et al. 2008), Citeseer (Kipf and Welling 2017), PubMed (Namata et al. 2012)), airline networks (Airport (Chami et al. 2019)) and co-purchase networks (Photo (Shchur et al. 2018)). In addition, we generate graphs with topological heterogeneity for further comparison. The statistics and topological heterogeneity analysis are detailed in the Appendix A.1."}, {"title": "Performance Evaluation", "content": "Performance on Real-world Graphs. We evaluate our method on link prediction and node classification tasks, and the results are summarized in Table 2 and Table 3. For the link prediction task, we use Fermi-Dirac decoder (Nickel and Kiela 2017) where the distance between two nodes in our method is calculated by Eq. (9) and (10). As observed, GraphMoRE achieves the best results on all datasets among 9 baselines, and compared with the runner-up method, we achieve gains of 2.01%, 1.94%, and 1.92% respectively in AUC score on Cora, Citeseer, and PubMed. For the node classification task, we use GCN, GAT, and SAGE as backbone respectively. GraphMoRE achieves the best results on the vast majority of datasets.\nPerformance on Synthetic Graphs. To further verify the ability to model mixed heterogeneous topologies, we evaluate our method on three different scale synthetic graphs with topological heterogeneity. We select the Riemannian methods Q-GCN and MotifRGC which are designed for topological heterogeneity and the Euclidean method GCN, as baselines on the link prediction task. As shown in Figure 2, GraphMoRE significantly outperforms all baselines. In addition, as the scale of datasets increases, the performance of all baselines declines to varying degrees, while GraphMORE consistently maintains excellent performance. This phenomenon is attributed to we analyze topological heterogeneity from the perspective of local topology characterization, embedding each node into corresponding personalized embedding space rather than modeling all nodes globally, which decouples GraphMoRE from the scale of the dataset.\nAblation Study. We conduct ablation studies with four variants on two downstream tasks to verify the effectiveness of each component in GraphMoRE. Here, we use GCN as the classifier backbone for the node classification task."}, {"title": "Conclusion", "content": "In this paper, we propose a novel method GraphMoRE to address the problem of topological heterogeneity from the perspective of local topology characterization. We first propose a topology-aware gating mechanism that selects the optimal embedding space for nodes individually by estimating local geometric properties. Then we design diverse Riemannian experts to flexibly construct personalized mixed curvature spaces, effectively embedding the graph into a heterogeneous manifold with varying curvatures at different points. Finally, we present a concise and effective alignment strategy to fairly measure pairwise distances between different embedding spaces. Extensive experiments have shown the advantages of GraphMoRE on topological heterogeneity. Future work will further explore the potential and broader impact of Riemannian MoE in enhancing the ability of graph foundation models to uniformly handle diverse graph data from a wide variety of types and domains."}, {"title": "Experiment Details", "content": "Datasets Details\nReal-world Datasets. We use five real-world datasets, including citation networks, airline networks and co-purchase networks, to evaluate GraphMoRE on link prediction and node classification tasks. Statistics of the real-world datasets are concluded in Table A.1.\nSynthetic Datasets. We synthesize graphs with topological heterogeneity by generating substructures with different topological patterns (e.g., trees, cycles, etc.) and mixing them by adding random edges between them. Considering the extent of topological heterogeneity, we control it by the scale of the synthetic graph, which means that larger graphs contain more complex mixed topologies. For three synthetic graphs Syn.Small, Syn.Middle, and Syn.Large, containing 4027, 10014, and 20018 nodes respectively. We uniformly use the node2vec (Grover and Leskovec 2016) algorithm to initialize the node features of synthetic graphs, where the feature dimension is 128.\nGraph Sectional Curvature. We explore topological heterogeneity by analyzing the sectional curvature of the graph. Specifically, according to the parallelogram law (Gu et al. 2019), the local topology with positive sectional curvature is more closely matched with cycle-like positively curved space, while the local topology with negative sectional curvature is more closely matched with tree-like negatively curved space. Given a graph G with the node set V, we can calculate the sectional curvature of the geodesic triangle composed of node m and its neighboring nodes b and c as:\n$\\kappa(m; b, c) = \\frac{1}{N} \\sum_{a \\in V} \\frac{g(a,m)^2 + \\frac{g(b, c)^2}{4}}{\\frac{g(a, b)^2 + g(a, c)^2}{2}}$, (A.1)\nwhere g(a, b) is the shortest path distance between node a and b on the graph. Figure A.1 shows the sectional curvature statistics of datasets used in this paper, where the degree of near-uniform distribution of positive and negative sectional curvatures is positively correlated with the degree of topological heterogeneity.\nExperiment Setting\nDownstream Tasks. For the link prediction task, we follow the settings in (Chami et al. 2019), spliting the training set, validation set, and test set in proportions of 85%, 5%, and 10%. We sample negative links from any possible links outside the training set (i.e. unobservable links), and keep the number of positive and negative links consistent. We use the Area under the ROC Curve (AUC) and Average Precision (AP) as the evaluation metrics. For node classification tasks, we follow the settings in (Fu et al. 2021; Sun et al. 2024), and use the Weighted-F1 (W-F1) and Macro-F1 (M-F1) as the evaluation metrics.\nHyperparameters. Hyperparameters are obtained by grid search, the search range is as follows: the learning rate is {0.1, 0.01, 0.001}, the weight decay is {0.0, 0.0005, 0.001}, and the loss balance coefficient $\\lambda$ is {1.0, 0.5, 0.1, 0.05, 0.01}. For all experiments, we train the model for at least 200 epochs, the early stop is set to 100. The other settings are described in Sec 5.1. The parameters of Riemannian experts are optimized by Riemannian Adam (B\u00e9cigneul and Ganea 2019), while the parameters of other models are optimized by Adam (Kingma and Ba 2014)."}, {"title": "Implement Details", "content": "Details of GraphMoRE\nIn our method, considering the generality across all datasets, the number of Riemannian experts is set to 5, with initial curvatures of {-3, -1, 0, 1, 3}. For Cora and Citeseer datasets, the embedding dimension of the Riemannian expert is set to 32, while for other datasets it is set to 16. The local topology sampling employs a simple strategy that utilizes induced subgraphs of ego networks with different radius.\nRunning Environment\nWe conduct the experiments with:\n\u2022 Operating System: Ubuntu 20.04 LTS.\n\u2022 CPU: Intel(R) Xeon(R) Platinum 8358 CPU@2.60GHz with 1TB DDR4 of Memory.\n\u2022 GPU: NVIDIA Tesla A100 with 40GB of Memory.\n\u2022 Software: CUDA 11.7, Python 3.8.0, Pytorch 1.13.1, Py-Torch Geometric 2.3.1."}]}