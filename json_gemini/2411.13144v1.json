{"title": "COPYRIGHTMETER: Revisiting Copyright Protection in Text-to-image Models", "authors": ["Naen Xu", "Changjiang Li", "Tianyu Du", "Minxi Li", "Wenjie Luo", "Jiacheng Liang", "Yuyuan Li", "Xuhong Zhang", "Meng Han", "Jianwei Yin", "Ting Wang"], "abstract": "Text-to-image diffusion models have emerged as powerful tools for generating high-quality images from textual descriptions. However, their increasing popularity has raised significant copyright concerns, as these models can be mis-used to reproduce copyrighted content without authorization. In response, recent studies have proposed various copyright protection methods, including adversarial perturbation, con-cept erasure, and watermarking techniques. However, their effectiveness and robustness against advanced attacks remain largely unexplored. Moreover, the lack of unified evaluation frameworks has hindered systematic comparison and fair assessment of different approaches.\nTo bridge this gap, we systematize existing copyright protection methods and attacks, providing a unified taxonomy of their design spaces. We then develop COPYRIGHTMETER, a unified evaluation framework that incorporates 17 state-of-the-art protections and 16 representative attacks. Leveraging COPYRIGHTMETER, we comprehensively evaluate protection methods across multiple dimensions, thereby uncovering how different design choices impact fidelity, efficacy, and resilience under attacks. Our analysis reveals several key findings: (i) most protections (16/17) are not resilient against attacks; (ii) the \"best\" protection varies depending on the target priority; (iii) more advanced attacks significantly promote the upgrading of protections. These insights provide concrete guidance for developing more robust protection methods, while its unified evaluation protocol establishes a standard benchmark for fu-ture copyright protection research in text-to-image generation.", "sections": [{"title": "1. Introduction", "content": "Recent advances in text-to-image diffusion models (T2I DMs), such as Stable Diffusion (SD) [1], DALL\u00b7E 3 [2], and Imagen [3], have revolutionized digital content creation by generating high-quality images from textual descriptions. While these models foster creativity by producing art and realistic scenes, they also raise significant copyright concerns [4]. Fine-tuning pre-trained models on specialized datasets allows them to mimic specific themes such as distinct art styles, which can lead to unauthorized reproductions [5], [6]. Artists are increasingly worried that their unique styles could be copied without permission, resulting in potential copyright infringement [7]. Furthermore, models trained on extensive datasets may produce images that closely resemble the style or content of specific artists, even if the artist or their creations are not directly referenced in the prompt [8]. As these AI-driven technologies evolve, it is crucial to balance innovation with the protection of creators' rights, as many artists fear that their unique art styles could be easily copied, potentially drawing customers away [9].\nThe urgent need to safeguard digital intellectual property leads to the development of three main protection categories: (i) Obfuscation Processing, which preprocesses data before release online to prevent unauthorized use, often using adversarial perturbation to confuse AI models while preserving content for normal users [7], [10]. (ii) Model Sanitization, which modifies pre-trained DMs to remove or alter protected copyright elements before public deployment [11], [12]. (iii) Digital Watermarking, which embeds invisible identifiers in AI-generated content to assert copyright ownership and support effective content management [1], [13], [14].\nGiven the significance of these protection mechanisms, recent studies have raised concerns about their effectiveness and robustness [15], [16], [17], [18]. This has led to sev-eral critical questions: RQ1 \u2013 What are the strengths and limitations of different protection mechanisms, especially their robustness against attacks? RQ2 \u2013 What are the best practices for copyright protection even in adversarial and envolving environments? RQ3 \u2013 How can existing copyright protection methods be further improved?\nDespite their importance for understanding and improv-ing copyright protection, these questions are under-explored due to the following challenge.\nNon-holistic evaluations - Existing studies often lack comprehensive evaluation of protections and attacks [21], [22], focus narrowly on limited perspectives, such as [16] fo-cuses solely on model sanitization against textual inversion, without providing a holistic evaluation. Moreover, many rely on limited metrics, failing to fully capture the characteristics and impacts of the protections being evaluated.\nNon-unified framework - Inconsistent datasets and DM versions across studies lead to evaluations under varying conditions, making comparision challenging. For example, Glaze [7] and Mist [23] are evaluated with different SD"}, {"title": "2. Background", "content": "Diffusion models are a class of generative models that transform random noise into coherent data through a forward step that gradually adds noise to data and a reverse step that denoises it to recover the original data distribution. Our study focuses on the latent diffusion models (LDMs) for their strong performance and low computational costs.\nText-to-image diffusion models (T2I DMs) generate im-ages from textual descriptions by learning to reverse the noise addition process guided by text. A notable open-source T2I DM example is Stable Diffusion (SD). Given a text prompt, it generates an image that reflects the specified semantic features, involves two key components:\nConditioning on Textual Descriptions \u2013 The reverse diffusion process is guided by textual descriptions, which are embedded into a high-dimensional vector using transformer-based models or other deep learning architectures. This vector informs each step of the reverse diffusion to align the generated image with the text.\nTraining Objective \u2013 T2I DMs are trained to predict and remove noise at each step, guiding image generation to match text prompts. This is achieved by minimizing the difference between actual and predicted noise:\n$\\mathcal{L}_{D M}(\\theta)=\\mathbb{E}_{x_{0}, \\epsilon, t, y}\\left[\\left\\|\\epsilon-\\epsilon_{\\theta}\\left(x_{t}, y, t\\right)\\right\\|_{2}\\right]$\nwhere $\\epsilon$ is the noise vector, and $\\epsilon_{\\theta}(x_{t}, y, t)$ is the model's estimate of the noise, conditioned on the noisy image $x_{t}$, the textual description $y$, and the timestep $t$.\nBeyond these design aspects, T2I DMs have driven advancements in generative Al across content creation, de-sign, education, and entertainment, bridging the gap between textual descriptions and visual content. T2I can also be fine-tuned with tools like DreamBooth, which enables them to"}, {"title": "2.2. Copyright Protection in Text-to-Image Models", "content": "In T2I DMs, copyright protection is a critical concern. The central challenge is ensuring that images generated from the model do not resemble copyrighted images. Techniques like DreamBooth fine-tuning on DM allow models to mimic specific copyrighted content, while DMs may also inadver-tently produce similar works. Another significant challenge is ensuring generated images can be traced back to their copyrighted sources. Conversely, the associated attacks aim to exploit these models for unauthorized purposes. The two primary attack methods involve generating an image that matches a specific, potentially copyrighted image or manipulating the generated image to make it untraceable. We will formalize and discuss the prominent categories of protection and corresponding attack methods in Section 3."}, {"title": "3. Taxonomy", "content": "In this section, we provide a holistic overview of various copyright protection and attack methods. As depicted in Fig-ure 1, we divide copyright protection into three categories: Obfuscation Processing (OP), Model Sanitization (Ms), and Digital Watermarking (DW). Correspondingly, we identify three attack categories: Noise Purification (NP), Concept Recovery (CR), and Watermark Removal (WR). Table 2 presents the definitions and detailed methods of these pro-tections and their corresponding attacks. Figure 1 shows the overall system design of COPYRIGHTMETER, while Figure 2 provides specific examples of copyright protection and"}, {"title": "3.1. Protection Schemes", "content": "This subsection contains survey-style descriptions of the investigated copyright protection schemes. Table 4 shows the copyright protection methods and detailed characteristics."}, {"title": "3.1.1. Obfuscation Processing (OP)", "content": "This approach intro-duces protective perturbations into copyrighted images to prevent replication from T2I DMs. When these protected images are used as training or reference data (e.g., in image-to-image transformation), they mislead DMs that aim to replicate the originals, thereby protecting data owners from unauthorized replication and misuse of their data.\nFormalization \u2013 Given a copyrighted image $x$, the aim is to create a protected image $x_{pro}$ by adding a care-fully crafted perturbation $\\delta$, such that $x_{pro} = x + \\delta$. This perturbation $\\delta$ is designed to either maximize the latent space distance between $x_{pro}$ and $x$ (untargeted protection) or minimize the latent space similarity between $x_{pro}$ and a deliberately chosen dissimilar target image $x_{t}$ (targeted protection). Additionally, to ensure the perturbation remains inconspicuous, the pixel distance between $x$ and $x_{pro}$ should be constrained by an upper bound $\\epsilon$, maintaining the visual fidelity of the protected image. This can be formatted as:\n$\\underset{\\delta}{\\max } D_{z}\\left(x, x_{p r o}\\right) \\text { or } \\underset{\\delta}{\\min } D_{z}\\left(x_{p r o}, x_{t}\\right), \\text { s.t. } D_{p}\\left(x, x_{p r o}\\right)<\\epsilon$.\nApproaches \u2013 Since all methods maintain visual simi-larity by ensuring the perturbation $\\delta$ maintain a small pixel space distance between $x$ and $x_{pro}$, we omit this common-ality and focus solely on the unique protection concepts of each method. AdvDM [26] optimizes $\\delta$ to maximize the diffusion training loss and increase the latent noise vector's distance of $x_{pro}$ and $x$. Based on AdvDM, Mist [23] opti-mizes $\\delta$ to maximize distance both in the latent noise vector and latent encoded representation. Glaze [7] optimizes $\\delta$ by adjusting it to approach $x_{t}$ with a specific style, aiming to minimize $D_{z}\\left(x_{p r o}, x_{t}\\right)$. PhotoGuard (PGuard) [10] using two schemes using either the encoder or the entire diffusion process to optimize $\\delta$ to minimize $D_{z}(x_{pro}, x_{t})$ in the latent space of encoder and LDM, respectively. Anti-DreamBooth (AntiDB) [27] optimizes $\\delta$ to minimize DM's generation ability by making a difficult to reconstruct from $x_{pro}$."}, {"title": "3.1.2. Model Sanitization (Ms)", "content": "This approach is designed for model providers by guiding pre-trained DMs to remove copyright concepts before public deployment, ensuring that the models do not reproduce copyrighted content illegally.\nFormalization \u2013 Given a concept protected by copyright $C_{c r} \\in C$ (where $C$ is the set of all concepts) and a specific unrelated concept $c_{\\varnothing} \\in C \\backslash C_{c r}$. It shifts model's generation distribution conditioned on $c_{c r}$, denoted as $p_{\\phi}(x | C_{c r})$, toward the distribution conditioned on the unrelated concept $c_{\\varnothing}$, de-noted as $p(x | c_{\\varnothing})$. To measure the alignment, we minimize the KL divergence $D_{K L}$ between these distributions through the transformation $\\phi$, the model's output distribution is ad-justed to reduce its ability to generate images corresponding to $C_{c r}$. The objective can be formalized as:\n$\\arg \\min {\\Phi} D_{K L}\\left(P(x | c_{\\phi})|| P_{\\phi}(x | C_{c r})\\right)$.\nApproaches - Based on the difference in distribution alignment, the approaches can be categorized into two types: fine-tuning and inference guiding methods.\nFine-tuning methods adjust $p_{\\phi}(x | C_{c r})$ by modifying the DM's U-Net weights, targeting different components de-pending on the method [42]. For instance, Forget-Me-Not (FMN) [12] fine-tunes U-Net cross-attention layers' weights to minimize the Frobenius norm of attention maps between input feature and embedding of $C_{c r}$, aligning $p_{\\phi}(x | C_{c r})$ more closely with $p(x | c_{\\varnothing})$. Erased Stable Diffusion (ESD) [11] fine-tunes to both cross-attention and unconditional layers to diminish $C_{c r}$ 's influence in denoising prediction. Ablat-ing Concepts (AC) [32] further fine-tunes U-Net weights, including projection matrices in cross-attention layers, and text transformer embedding to minimize KL divergence for a tighter alignment. Unified Concept Editing (UCE) [19] strategically modifies U-Net's cross-attention keys and val-ues associated with text embeddings of $c_{c r}$ to align $p(x | C_{c r})$ with $p(x | c_{\\varnothing})$ while preserving unrelated concepts $c_{\\varnothing}$ .\nInference guiding methods adjust the sampling process without altering model weights. In SD, each sampling step involves conditional and unconditional denoising. The final noise prediction is derived by taking the difference between these two samplings. Negative Prompt (NP) [33] replaces unconditional noise prediction with noise conditioned on $C_{c r}$ , guiding diffusion away from the $C_{c r}$ . Safe Latent Diffusion (SLD) [20] adds a safety guidance term, further shifting the distribution away from $p(x | C_{c r})$ ."}, {"title": "3.1.3. Digital Watermarking (DW)", "content": "This approach embeds invisible messages in images to trace image origins and verify copyright. Unlike traditional post-hoc watermarks [43], [44] applied after image generation and do not involve DMs, we discussed watermarks in the generation process of DMs. This can be achieved by embedding watermarks directly in the training data and fine-tuning the DM, or by modifying latent vectors to impact the generation of images.\nFormalization \u2013 Embedding a watermark message $m$ into an image $x$ with a function $w$ results in a watermarked image $x_{w m}=w(x, m)$. An extraction function $e$ is decodes the message $m_{w m}=e(x_{w m})$. The watermarked image $x_{w m}$ should remain visually similar to the $x$, and the $m_{w m}$ should accurately reflect $m$. The goal is to find $w$ that minimizes either pixel distance $D_{p}$ or latent space distance $D_{z}$ between $x$ and $x_{w m}$, while optionally minimizing the text discrepancy $D_{t}$ between $m$ and $m_{w m}$, depending on the specific method. This can be formalized as:\n$\\min _{w}\\left[\\alpha D_{p}\\left(x, x_{w m}\\right)+\\beta D_{z}\\left(x, x_{w m}\\right)+\\lambda D_{t}\\left(m, m_{w m}\\right)\\right],$\nwhere $\\alpha, \\beta$, and $\\lambda$ are weights that balance image quality, latent space similarity, and message accuracy, respectively. Depending on the approach, either $D_{p}$ or $D_{z}$ (or both) may be used, and $D_{t}$ is included if relevant.\nApproaches - The following methods outline different watermark embedding processes, denoted by $w$. Diffusion-Shield (DShield) [13] encodes the watermark message $m$ as a binary sequence, embedding each bit into distinct regions of the image $x$, with a decoder optimized to minimize the discrepancy between $m_{w m}$ and $m$, while controlling the $l_{\\infty}$ -norm to reduce the pixel distance between $x$ and $x_{w m}$. Diagnosis (Diag) [37] applies a text trigger to a dataset subset, fine-tuning the model to generate $x_{w m}$, and trains a binary classifier for watermark detection. Stable Signature (StabSig) [1] fine-tunes the decoder of the image generator with a binary signature, producing $x_{w m}$ while minimizing perceptual distortion $D_{p}\\left(x, x_{w m}\\right)$ and message discrepancy $D_{t}\\left(m, m_{w m}\\right)$. Tree-Ring (TR) [14] embeds m in the Fourier space of initial noise latent vector, detectable through DDIM inversion [45], while minimizing $L_{1}$ distance between $m$ and $m_{w m}$ from the Fourier transform of the inverted noise vector. ZoDiac [38] is equipped for watermarking existing images by embedding m into the latent vector through DDIM inversion, incorporating Euclidean distance, SSIM loss, and Watson-VGG perceptual loss to minimize the pixel distance of $x_{w m}$ and $x$. Gaussian Shading (GShade) [39] maps m to latent representations following a standard Gaussian distribution, aiming to preserve the distribution between $x$ and $x_{w m}$ for fidelity."}, {"title": "3.2. Attack Schemes", "content": "This subsection outlines the copyright attack schemes evaluated. Table 5 summarizes attack methods and their detailed characteristics."}, {"title": "3.2.1. Noise Purification (NP)", "content": "This process employs spe-cific transformation as an attack to remove the protective perturbations added to images in OP, thereby evaluating the effectiveness of OP under attack and assessing its resilience.\nFormalization \u2013 Given a protected image $x_{p r o}=x+\\delta$, the adversary aims to apply a transformation $\\tau$ to remove the perturbation $\\delta$. These methods can be classified into two categories: (i) Experience-based methods, which use common transformations (e.g., JPEG compression) as $\\tau$ to remove perturbation $\\delta$ while having little impact on the pix-els difference between $x_{p r o}$ and $x_{p u r}$. (ii) Optimization-based methods eliminate the potential protection $\\delta$ more accurately by customizing transformations to align the latent and pixel spaces of $x_{p u r}$. Specifically, it minimizes the pixel distance between $x_{p u r}$ and reconstructed image $f_{\\theta}\\left(x_{p u r}\\right)$ generated from the latent representation. Besides, for purification fi-delity, it is crucial that $x_{p u r}$ remains visually similar to the original image $x$. However, as $x$ is typically unavailable during attacks. Therefore, $x_{p r o}$ is used to approximate $x$ due to the minor perturbation $\\delta$. Visual similarity is then achieved by constraining the pixel distance between $x_{p u r}$ and $x_{p r o}$. This overall process can be formatted as:\n$\\min _{\\tau} D_{p}\\left(x_{p u r}, f_{\\theta}\\left(x_{p u r}\\right)\\right), \\text { S.t. } D_{p}\\left(x_{p r o}, x_{p u r}\\right)<\\epsilon$.\nApproaches \u2013 Experience-based methods use the fol-lowing transformation as $\\tau$ : JPEG [28] is a lossy compres-sion algorithm that uses discrete cosine transform to remove high-frequency components from $x_{p r o}$; Quantization (Quant) [29] compresses pixel values to single discrete values. Optimization-based methods include: Total Variation Minimization (TVM) [30] reduces $\\delta$ by minimizing unnec-essary pixel intensity variations (i.e., gradient amplitude). IMPRESS [15] purifies $x_{p r o}$ by minimizing the consistency between $x_{p u r}$ and $f_{\\theta}\\left(x_{p u r}\\right)$ while limiting the LPIPS between $x_{p r o}$ and $x_{p u r}$ for visual similarity; DiffPure [31] adds noise to $x_{p r o}$ and then denoises to remove $\\delta$, limiting the upper bound of pixel distance between $x_{p r o}$ and $x_{p u r}$."}, {"title": "3.2.2. Concept Recovery (CR)", "content": "This process targets vul-nerabilities to recover sanitized concepts, enabling sanitized models to generate images with copyrighted concepts, thus posing a risk of illegal replication. This evaluation assesses the resilience of sanitized models to such recovery attempts.\nFormalization \u2013 For a sanitized model with output distribution $p_{\\theta}\\left(x | C_{c r}\\right)$ aligned with unrelated concept distri-bution $p(x | c_{\\varnothing})$, CR aims to realign $p_{\\theta}\\left(x | C_{c r}\\right)$ to a reference distribution $p\\left(x | C_{r e f}\\right)$. This reference distribution corresponds to images containing $C_{r e f}$, which are similar to the copyright content. The goal is to minimize the divergence between $p\\left(x | C_{r e f}\\right)$ and $p_{\\theta}\\left(x | C_{c r}\\right)$, enabling the sanitized model to regenerate images containing $C_{c r}$. This can be formatted as:\n$\\arg \\min D_{K L}\\left(P\\left(x | C_{r e f}\\right) || P_{\\theta}\\left(x | C_{c r}\\right)\\right)$.\nApproaches - These methods learn embeddings from reference images with $C_{r e f}$ and adjust the sanitized model to realign its output distribution. LoRA [34] modifies $\\theta$ using a low-rank decomposition of weight updates, efficiently fine-tuning the model to align $p_{\\theta}\\left(x | C_{c r}\\right)$ with $p\\left(x | C_{r e f}\\right)$. Similarly, DreamBooth (DB) [5] fine-tunes models on a set of images with $C_{r e f}$, embedding $C_{r e f}$ into the model's output domain to produce images with the distribution $p(x | C_{r e f})$. Textual In-version (TI) [35] optimizes embedding for $C_{r e f}$ by modifying the loss function to incorporate $C_{r e f}$ during noise prediction, minimizing the discrepancy between noise predictions for generated and reference images. Concept Inversion (CI) [16] learns specialized embeddings that can recover $C_{c r}$ for each Ms approach to further improve alignment with $C_{c r}$. Ring-A-Bell (RB) [36] is a model-agnostic method that extracts holistic representations of $C_{c r}$ to identify prompts that might trigger unauthorized generation of copyright content."}, {"title": "3.2.3. Watermark Removal (WR)", "content": "To assess the resilience of DW against watermark removal, this approach evaluates watermark robustness by attempting to remove them.\nFormalization \u2013 Given a watermarked image $x_{w m}$, an adversary applies typical image transformation attack $\\alpha$ to generate a watermark-removed image $x_{w r}=\\alpha(x_{w m})$. The goal is to make the watermark undetectable while keeping $x_{w r}$ visually similar to $x_{w m}$. Following [14], [46], the pixel-level distortion between $x_{w r}$ and $x_{w m}$ is constrained to stay below a threshold $\\epsilon$, ensuring visual similarity. Formally, this objective is expressed as:\n$D_{p}\\left(x_{w m}, x_{w r}\\right) \\leq \\epsilon$.\nApproaches - Brightness Adjustment (Bright) [39] ad-justs the brightness of $x_{w m}$ to produce $x_{w r}$. Image Rotation (Rotate) [39] rotates $x_{w m}$ to disrupt synchronization between the watermark embedder and detector. Random Crop (Crop) [39] removes portions of $x_{w m}$. Gaussian Blur (Blur) [39] convolves $x_{w m}$ with a Gaussian kernel to smooth the image and reduce watermark visibility. VAE-Cheng20 (VAE) [41] compresses $x_{w m}$ using discretized Gaussian mixture like-lihoods and attention modules to obscure the watermark. DiffPure [31] adds noise to $x_{w m}$, followed by DM-based denoising to remove the watermark."}, {"title": "3.3. Threat Model", "content": "We systematically categorize the security threats to copyright protection methods based on the adversary's ob-jective, knowledge, and capability.\nAdversary's objective. In the field of text-to-image (T2I) diffusion models, adversaries aim to generate specific style/concept images. They exploit system flaws and chal-lenge security measures to enable illegal copying and edit-ing of images. Their objectives are multifaceted, including emulating a specific artist's style, undeterred by existing ob-fuscation protections, the regeneration of sanitized concepts from purposefully sanitized models, and evading watermark detection. All these endeavors are pursued while maintaining a level of quality akin to the original copyrighted images.\nAdversary's knowledge. Considering the variations in different protection methods, we've tailored our model of the adversary's background knowledge to capture these nuances. For obfuscation protections and digital watermarking, the adversary is capable of accessing the safeguarded or water-marked artistic images. In model sanitization, the adversary can access the sanitized model and a small set of reference images embodying the target concepts.\nAdversary's capability. In a similar vein, we've ad-justed our model of the adversary's capability to reflect these nuances. For obfuscation processing and digital watermark-ing, the adversary can modify the protected or watermarked images. In the context of model sanitization, the adversary can draw upon their knowledge of sanitized methods to retrain sanitized models using example images, thereby recovering the sanitized concepts."}, {"title": "4. Experiments", "content": "Leveraging COPYRIGHTMETER, we conduct a system-atic evaluation of existing copyright protection and attack methods, uncovering their intricate design landscape."}, {"title": "4.1. Experimental Setup", "content": "Datasets. We evaluate on three datasets: WikiArt [47], CustomConcept101 [6] (referred to as Concept), and Per-son [16]. WikiArt contains over 42,000 artworks from 129 artists, categorized by genre (e.g., Impressionism). Concept consists of images of 101 specific concepts, each with 3 to 15 images. Person consists of photos of 10 distinct celebrities, with 15 images for each individual derived from the LAION dataset [48]. For OP, following [15], [38], we use WikiArt and Concept. For MS, following [12], [16], we use WikiArt and Person. For DW, we use all three datasets.\nModels. We evaluate the widely used and open-source DM implementation Stable Diffusion (SD). Previous studies used different versions of the SD, making it difficult to isolate the effects of copyright protection methods from model variations. We select a representative SD [49] version 1.5\u00b9, the most widely downloaded version on the Hugging Face platform with a resolution of 512\u00d7512 as the T2I DM in image generation experiments for a unified evaluation.\nMetrics. Following [10], [15], [26], COPYRIGHTMETER incorporates several key metrics: Peak Signal-to-Noise Ratio (PSNR) quantifies the ratio between maximum possible signal power and noise; Structural Similarity Index Mea-sure (SSIM) evaluates structural similarity, brightness, and contrast between two images; Visual Information Fidelity (VIFp) assesses image quality based on information fidelity; Learned Perceptual Image Patch Similarity (LPIPS) uses"}, {"title": "4.2. Obfuscation Processing Evaluation", "content": "In this subsection, we evaluate the performance of obfus-cation processing (OP) methods to understand how different design choices affect their effectiveness. Specifically, we begin by applying the OP methods to generate the protected images, and then employ DreamBooth [5] to mimic the style of these protected images. Our evaluation focuses on three aspects: fidelity, which measures the similarity between protected and original images; efficacy, which measures how effectively the protected images prevent mimicry; and resilience, which examines the robustness of protection when using noise purification (NP) attacks to remove the perturbation. The setup details are shown in Appendix B.1.\nFidelity - For a protected image, it is crucial that it appears visually identical to the original to maintain the image's utility. High fidelity indicates better preservation of artistic and semantic values. To evaluate the fidelity of various OP protections, we use several widely-used metrics, such as LPIPS, SSIM, PSNR, VIFp, and FID, as outlined in Table 7. Figure 3 and 5 quantitatively and qualitatively show the fidelity evaluation of these methods compared with the protected image with the originals, respectively.\nFigure 3 illustrates that fidelity varies among different OP protection methods. AntiDB shows the highest fidelity, with the lowest LPIPS averaging around 0.1 and FID around 80, along with the highest SSIM (exceeding 0.9), PSNR (above 36), and VIFp (around 4.4) across datasets. AdvDM and Mist also exhibit relatively low FID averaging around 110, suggesting better preservation of image quality. This is likely due to these methods incorporating DMs in perturba-\ntion optimization (cf. Table 4), enhancing fidelity compared to methods relying solely on image encoder.\nInconsistencies arise when comparing metrics like LPIPS and FID. For instance, PGuard's low LPIPS of around 0.095 suggests a high visual similarity to the original images, but its high FID of over 180 suggests poor overall fidelity across the dataset. This disparity may stem from the different focuses of these metrics: LPIPS emphasizes semantic and perceptual similarities and visual details, while FID assesses how well the generated images align with the distribution of the original dataset, considering broader structural and statistical properties. Therefore, relying on a single metric can be misleading, highlighting the need for diverse metrics to comprehensively evaluate fidelity.\nThese findings align with the visualizations in Figure 5, where the AntiDB-protected images closely resemble the originals, while those from AdvDM and Mist maintain high similarity with only slight noise. Notably, all three methods DM into their optimization, contributing to their fidelity ad-vantage. In contrast, images protected by Glaze and PGuard show more noticeable alterations. Specifically, Glaze intro-duces subtle and unique distortions, while PGuard results in a stretched appearance compared to the original images. Overall, while fidelity differs across OP methods, all suc-cessfully preserve key visual characteristics, ensuring utility without compromising the viewer's experience.\nRemark 1 OP methods that use diffusion models during the perturbation optimization process tend to achieve higher fidelity compared to those that rely solely on image encoders.\nEfficacy - Following [7], [26], we fine-tune pre-trained SD models using protected images to generate mimicked images. For comparison, we also generate mimicked images from original (unprotected) images as a baseline. We assess the similarity between mimicked images produced from protected images and original images using FID, CLIP-I, and text-image alignment with CLIP-T. Figure 4 presents the quantitative results, and Figure 5 displays visual examples."}, {"title": "4.3. Model Sanitization Evaluation", "content": "Similar to Sec 4.2, we assess model sanitization (Ms) across three key dimensions: fidelity, efficacy, and resilience. Fidelity measures the sanitized model's ability to maintain performance on unrelated content. Efficacy gauges how effectively the sanitized model prevents the generation of copyrighted content, evaluating the thoroughness of the san-itization process. Resilience examines the sanitized model's robustness against concept recovery (CR) attacks, assess-ing whether it consistently avoids reproducing copyright-protected concepts even under adversarial conditions. The detailed experimental setup is given in Appendix B.2.\nFidelity - For sanitized models, it is crucial that sani-tization preserves the ability to generate images for other concepts while excluding the copyright concept. Table 6 evaluates the fidelity of Ms methods on MS-COCO 2017 [51] 30K dataset prompts. We use FID to measure the dif-ferences between images generated by the sanitized models (with the original DM for reference) and real-world images from the dataset. Additionally, CLIP-T is used to assess the alignment between generated images and prompts.\nOur analysis reveals that model sanitization (Ms) meth-ods achieve a successful balance between copyright pro-tection and image generation capabilities, with only minor impacts on overall performance. In Table 6, sanitized models experience a marginal increase in FID scores compared to their original counterparts, with SLD showing the most notable change (16.95 vs. 16.21 for the original SD model). This subtle increase suggests a minor impact on image fidelity, likely due to the model inadvertently altering rep-resentations of unrelated but adjacent concepts or facing creative constraints when adjusted to exclude copyrighted content. Interestingly, CLIP-T scores remain remarkably consistent across all methods (0.30-0.31), indicating well-preserved textual alignment. These findings align with pre-vious research [11], [19], [20], confirming that while MS methods may slightly affect image fidelity, they successfully maintain text alignment for unrelated concepts.\nIn summary, the sanitization process achieves its primary goal of removing specific content without significantly com-promising overall performance, demonstrating an effective balance between protecting copyrighted material and main-taining generative capabilities.\nEfficacy - To evaluate the effectiveness of various model sanitization (Ms) methods in removing copyrighted con-cepts, we focus on two key metrics: image similarity and text-image alignment. We compare images generated by sanitized models to those from the original model using FID and measure the alignment between generated images and their prompts using CLIP-T scores. A higher FID or a lower CLIP-T implies a more effective Ms method."}, {"title": "4.4. Digital Watermarking Evaluation", "content": "Similarly, we evaluate digital watermarking (DW) based on three criteria: fidelity, assessing the visual consistency be-tween images before and after DW; efficacy, determined by the ACC of extracted watermark messages; and resilience, measuring the ACC of message extracted from image af-ter watermark removal (WR) attacks. Further experimental setup details are outlined in Appendix B.3.\nFidelity - Maintaining visual similarity to the original image and alignment with the corresponding prompt is es-"}, {"title": "5. Exploration", "content": "Next, we explore the generalizability, efficiency, and sensitivity of current protection methods. We further com-pare these methods with their contemporary versions and industry-leading online text-to-image applications. Further-more, we also conduct user studies to evaluate the alignment between evaluation metrics and human judgment."}, {"title": "5.1. Generalizability", "content": "While previous experiments use DreamBooth [5] for image mimicry, other fine-tuning methods can also achieve mimicry. To assess generalizability, following [13], [24], we employ a standard script from Diffusers\u00b2 for fine-tuning (details in Appendix B.4). In Figure 16, differences in texture patterns, artifacts, or deviations from protected im-ages reveal protection effectiveness against specific mimicry techniques. Notably, AdvDM and Mist exhibit the highest generalizability, providing strong protection under both fine-tuning methods. Glaze is less effective against DreamBooth but performs better with the standard script. Conversely, PGuard is robust with DreamBooth but is less effective with the standard script. AntiDB provides noticeable protection against both methods, particularly performing well against DreamBooth mimicry. These findings emphasize the need for protection methods that account for diverse mimicry techniques to enhance generalizability."}, {"title": "5.2. Efficiency", "content": "Computational cost is a key factor in copyright protec-tion applications. Op and Dw involve lightweight, image-level manipulations, while Ms and CR require deeper model-level changes, increasing time consumption. While prior studies often overlook time efficiency, we explore the time consumption of Ms and CR methods.\nAs shown in Figure 17, within Ms, inference-guiding methods are more efficient than model fine-tuning methods as they can sanitize a concept within"}]}