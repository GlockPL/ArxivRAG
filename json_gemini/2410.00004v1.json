{"title": "RETRO-LI: Small-Scale Retrieval Augmented Generation Supporting Noisy Similarity Searches and Domain Shift Generalization", "authors": ["Gentiana Rashiti", "Geethan Karunaratne", "Mrinmaya Sachan", "Abu Sebastian", "Abbas Rahimi"], "abstract": "The retrieval augmented generation (RAG) system such as RETRO has been shown to improve language modeling capa- bilities and reduce toxicity and hallucinations by retrieving from a database of non-parametric memory containing trillions of en- tries. We introduce RETRO-LI that shows retrieval can also help using a small scale database, but it demands more accurate and better neighbors when searching in a smaller hence sparser non- parametric memory. This can be met by using a proper semantic similarity search. We further propose adding a regularization to the non-parametric memory for the first time: it significantly reduces per- plexity when the neighbor search operations are noisy during infer- ence, and it improves generalization when a domain shift occurs. We also show that the RETRO-LI's non-parametric memory can poten- tially be implemented on analog in-memory computing hardware, exhibiting O(1) search time while causing noise in retrieving neigh- bors, with minimal (<1%) performance loss. Our code is available at: https://github.com/IBM/Retrieval-Enhanced-Transformer-Little", "sections": [{"title": "1 Introduction", "content": "Natural language processing is a rapidly growing field in machine learning. Advancements in large language models are mostly a prod- uct of the neural scaling law [20]. Not only are language models themselves becoming larger, but their training data also follows suit. While GPT-3 [3] was trained on 300 billion tokens, GPT-4 [36] was trained on around 13 trillion tokens. As a result, the information retained by a language model is difficult to update or fact-check. Many of these data sources contain discriminatory language or mis- information, which are difficult to remove given the sizes of these datasets [7].\nIn retrieval augmented generation (RAG) [26], a language model is enhanced with a non-parametric memory as depicted in Fig- ure 1. During training and inference, for each input sequence, RAG searches this memory for the k most similar sequences (the so- called nearest neighbors) and uses them as additional input. RAG has been shown to help with under-fitted language models as the non-parametric memory increases with the number of training to- kens [15]. Moreover, it has been shown to reduce toxic language and hallucinations [52] through its use of retrieval. Furthermore, unlike in purely parametric models, it is straightforward and does not re- quire extensive computing power to update the retrieval database of a RAG model to reflect updated information. Finally, in a question- answering context, the ability to provide sources for the answer helps combat misinformation. This provides users with the opportunity to double-check the responses themselves, aiding the models' inter- pretability.\nDespite all the aforementioned functional advantages of RAG, in many cases, the retrieval database of non-parametric memory con- tains trillions of tokens to be searched. In fact, in RETRO [2], the scale begins at a retrieval database size of billions of tokens, which raises the question of search speed and optimization. Although similarity search acceleration libraries such as FAISS [19] and ScaNN [10] can compare millions of vectors in milliseconds, they quickly become a bottleneck, especially for retrieval databases containing trillions of tokens (e.g., see Section 3.3.1). To address this critical bottleneck, we suggest three directions whose effectiveness is shown in our pro- posed RETRO-LI.\nThe contributions of this paper are as follows:\n1) We enable RAG to work with a small-scale database. In RETRO-LI, we change the embedding of the retrieval system, and for the first time add regularization in the form of Gaussian noise to the neighbor embeddings of the non-parametric memory. We employ a novel embedding model that excels in semantic similarity search allowing retrieval of higher quality neighbors despite searching in a sparse non-parametric memory, and the regularization consistently improves domain shift generalization. We show that the new retrieval helps with language modeling on small-sized retrieval databases over having no retrieval database."}, {"title": "2 RETRO-LI", "content": "Our proposed RETRO-LI is a medium-sized parametric model based on RETRO with a small-scale non-parametric database. In RETRO-LI, the embedding model used for the neighbor search is a BERT based sentence similarity model called SBERT. The architecture diagram of RETRO-LI is shown in Figure 2, where we enhanced GPT-2 atten- tion blocks similarly to what has been down with RETRO-fitting to improve our language modeling performance without extensive re- training. RETRO-fitting as introduced in the RETRO paper [2], refers to taking a language model not trained with retrieval and adding re- trieval in the form of a frozen retrieval database and chunked cross-attention (CCA) blocks in order to incorporate the information from neighbor embeddings in the training sequences.\nTo query the key-value retrieval database (DB), each input se- quence is split into chunks. An input sequence contains 1024 tokens, for a chunk length of 64 tokens this is 16 chunks per sequence.\nDB([Chunko, ..., Chunk15]) =\n[[No, Co]o, ..., [N9, C9]0\n:\n[[No, Co]15, ..., [N9, C9] 15\n(1)"}, {"title": "2.1 Embeddings for Neighbor Search", "content": "In RETRO-LI, we set out to work with small retrieval databases and almost no token overlap (see Appendix A.2). Thus, we have to be ju- dicious about which neighbors are returned. In this setup with smaller retrieval databases, the search space of sequences is not as densely populated, thus not finding the true closest sequence carries a big- ger penalty. Considering both ease of use and theoretical results, we choose to use SBERT [41], a BERT-based sentence-similarity model, in RETRO-LI.\nWe use the pre-trained model multi-qa-mpnet-base-dot-v1 of SBERT due to its superior performance in semantic textual similarity tasks in the massive text embedding benchmark (MTEB [32]). The embedding model outputs 768-dimensional vectors and has a model size of 420 MB. This vector dimensionality and model size are com- patible with the embedding size and the model size of the GPT-2 backbone used in RETRO-LI.\nSBERT adds a pooling operation to the output of Bert, resulting in a fixed-size sentence embedding. For our chosen embedding model, this pooling operation is CLS pooling, which involves using the CLS token embedding to represent the sentence. These embeddings were specifically trained for semantic similarity search. More details on SBERT can be found in Appendix B."}, {"title": "2.2 Regularization of Non-Parametric Memory", "content": "The role of noise in the retrieval has to the best of our knowledge never been analyzed. Here, a first foray into this topic is made.\nAdding noise to word embedding as a way of improving the gener- alization capabilities of language models has been done before [55], but not in combination with retrieval.\nTaking inspiration from NEFTune [18], we aim to improve gener- alization through a word-embedding regularizer. We introduce a new regularization method by adding noise to the word embeddings of the non-parametric memory, in contrast to NEFTune which regularizes the input sequence, in RETRO-LI their retrieved neighbors are regularized (see Figure 2).\nFor a matrix M of dimension n_emb \u00d7 seq_len describing the em- bedding matrix of a sequence, we compute:\n\u03c3 = \u03bb\u03b5\u00b7 MEAN(ABS(M))\n(2)\nThe relative standard deviation At describes the magnitude of the noise added. During training, we add a vector sampled from this N(0, 0) distribution to our neighbor embeddings.\nWe explore a variety of regularizers, some similar to NEFTune, some more similar to the intrinsic noise in the stochastic IMC hard- ware (see Section 3.2.3), ensuring that the signal-to-noise ratio from different approaches remains comparable."}, {"title": "3 Results", "content": "We base our work on an earlier RETRO implementation [50] in which the retrieval database and the similarity search are implemented with FAISS index. The training is accelerated with a single Tesla V100 GPU with a maximum 80 GB memory."}, {"title": "3.1 Experimental Setup", "content": "The task is language modeling on WikiText-103 and the numbers we report are the perplexities for WikiText-103-Validation. Just as in RETRO, we employ a sliding window approach, where we compute the perplexities for an overlapping proportion greater than or equal to 75% of the context, more on this in Appendix D.\nOur work is also inspired by the Chinchilla law [15] to determine the architecture and training strategy so that it fits a setup of any scale. For this particular task, we train on one GPU for 1'078'012 sample sequences resulting in a total of 1.104 \u00d7 10\u00ba tokens. The time to train one epoch grows with the number of neighbors, but sub-linearly. Training on 10 neighbors takes about twice as long as training on 2 neighbors but training on 2 neighbors takes about the same time as training on no neighbors.\nWe conduct an initial exploration of the experiment space with only a frozen GPT-2 component. So we train not only the CCA but also the feed-forward (FFW) blocks (see Appendix E for the cor- responding architecture diagram). A pattern emerges, where SBERT does better than BERT and 3 neighbors do best while not taking much more time to train on, compared to no neighbors."}, {"title": "3.2 Language Modeling", "content": "For our retrieval experiments we only freeze the GPT-2 layers as shown in Figure 2. We explored un-freezing these layers as well, see Appendix F. Both RETRO-LI-off and RETRO-LI-on are trained on the same data and, aside from the GPT-2 attention blocks and em- beddings, from scratch."}, {"title": "3.2.1 Retrieval", "content": "For the first batch of experiments, we gradually increase the num- ber of neighbors from 2 up to 10 and run the experiments for three random seeds. In Table 1 we report their average. BERT embed- dings with 5 neighbors already outperforms RETRO-LI-off. More- over, changing the embedding model to SBERT improves our perfor- mance significantly. This shows that not only does SBERT find more semantically similar neighbors, but our model also correctly attends to them.\nFurthermore, we see that for BERT embeddings all cases except the number of neighbors equal to 5 under-perform RETRO-LI-off. At this stage of the model training, where we only freeze GPT-2 blocks, it is too difficult for our model to find its way through the loss land- scape to get to the best possible performance. Adding sub-optimal neighbors at this point exacerbates the issue.\nThis makes the SBERT results even more remarkable. The neigh- bors found through the SBERT embeddings genuinely inform the model, helping it to converge. Still, not all numbers of neighbors im- prove the performance compared to no retrieval, confirming what [1] and [42] have already observed.\nFinally, we note that with this setup, for the BERT embeddings, and RETRO-LI-off the variance between the random seeds is very large. This is partially because we were unable to train all checkpoints to convergence, as some got stuck in local minima. Thus another aspect we show here is how SBERT embeddings help to avoid/escape these local minima.\nOverall, adding neighbors at a stage where the language model itself has not converged yet can help, but does not always. We con- clude that we must first address the language modeling before we can add neighbors and benefit from them."}, {"title": "3.2.2 Ablation of Embeddings for Neighbor Search", "content": "Similarly to NEFTune, we add uniform noise to the word embed- dings during training. The goal of it is to improve the generalization capabilities of our model. We add it to the training sequence itself, to the neighbors, and also both. We again average the result over three random seeds and ablate the noise type as well as the noise strength.\nPreliminary ablations performed to determine the best type of noise, both in terms of magnitude and in terms of where the noise is added (so whether to add on the input embeddings or to the neigh- bor embeddings), showed that regularization works best when mod- erately added only to the neighbor embeddings. The full results are in Appendix C. We also performed additional experiments to determine the best type of noise among the uniform and the Gaussian while ensuring a similar signal-to-noise ratio. Results are shown in Table 2. A Gaussian with zero mean and At = 0.2 (the variance as described by Equation 2) improves our generalization performance better than the rest of the regularizers we have tried.\nTable 3 shows the impact of this best-performing regularizer (the Gaussian regularizer with t = 0.2) in both language modeling and domain shift. In the WikiText-103 language modeling, the regular- izer improves or at least does not impair the validation perplexity in various inference settings: having no neighbors, ideal retrieval, or different amounts of noise during inference (i.e., \u03bb\u2081 \u2208 {0.2, 0.4, 1}). For the domain shift experiments, it exhibits more promising behav- ior and consistently shows benefits when different noisy scenarios are considered. More details are provided in the next subsection."}, {"title": "3.2.3 Ablation of Regularizations", "content": "We use three architectural settings for domain shift generalization:\n\u2022 Off (i.e., no retrieval). This model was never trained with retrieval.\n\u2022 RETRO-LI-on with no neighbors: This model was trained with re- trieval, but we do not give it any neighbors at inference time.\n\u2022 RETRO-LI-on with ideal (i.e., not noisy) retrieval: This model was trained with retrieval and we give it the ten nearest neighbors per chunk at inference time.\nThe setting RETRO-LI-on with no neighbors is meant to ascertain how much of the domain shift performance gain is simply due to im- proved language modeling capabilities, and not due to the retrieval it- self. In settings with RETRO-LI-on, we further have a model trained with a regularizer or without one. In Table 3, we only include the best-performing regularizer, a Gaussian regularizer with t = 0.2 (see Equation 2), which also reflects the signal-to-noise ratio ob- served in the modern IMC hardware (more on this in Section 3.3.1).\nFor domain shift experiments with RETRO-LI-on settings, we take the model and plug in a new retrieval database. In this case, creat- ing a new retrieval database consists of the following steps. First, we train the models on a base dataset A. For a new domain dataset B, C, D, or E (i.e., a different domain) we create a retrieval database with the training tokens of B, C, D, or E, respectively, then chunk the val- idation sequences of B, C, D, or E, respectively, and find the closest neighbors in the retrieval database for each validation chunk. Then, we plug this database, which includes chunk-to-neighbor mapping, into our model, feed it with the validation sequences as input, and measure the new perplexity. We do not fine-tune the models on any of the target domains whatsoever.\nTable 4 lists the datasets used for domain shift experiments, which are ordered by the size of the retrieval databases. The datasets are chosen such that they are sourced from a variety of domains and have sufficiently distinctive characteristics to identify them as unique domains. The datasets are further explained with their differences highlighted in Appendix A.\nIn the context of this work, a domain refers to the formality of the language. At one end, we have Wikipedia, which is highly for-"}, {"title": "3.3 Domain Shift Generalization: Plug-and-play", "content": "It is clear in Table 17 and Figures 6 that not only is Gaussian, At = 0.2 the best checkpoint overall it can handle large amounts of noise better than other regularizers or no regularizer. We see how for \u03bb\u2081 = 0.2 and \u2081 = 0.4 the perplexity barely increases for any of the regularizers (None, Uniform, Gaussian). For \u2081 = 1.0 which is the largest relative standard deviation in our setup this changes. Here clear patterns emerge. For instance, Gaussian, At = 0.4 does consistently worst because it has the lowest signal-to-noise ratio of the regularizers shown here. It is most affected by A\u2081 = 1.0. Moreover, Gaussian, At = 0.2 does best for every dataset and noise level at inference time. It is least affected by \u03bb\u2081 = 1.0. This is unsurprising, as we also observe that out of all the regularizers Gaussian, At = 0.2 is least affected by the inference mode no retrieval. Most of this model's generalization performance comes from improved language modeling and not from the retrieved neighbors themselves.\nThe uniform regularizer and no regularizer get similar results for almost all combinations of datasets and types of inference noise, but that changes for \u03bb\u2081 = 1.0. For all datasets except BBC-News and WikiText-103, no regularizer is less affected by i = 1.0 than the uniform regularizer. However, it has to be said that, even for these two datasets, the performance decrease is very similar."}, {"title": "3.3.1 Noisy retrieval with IMC hardware", "content": "Before delving into the potential benefits and challenges of retrieval with the IMC hardware, let us explain how it has been done in the state-of-the-art. Currently, we search for the nearest neighbors using FAISS which is an inverted vector file (IVF) index. An IVF index clusters the vectors to be searched into e centroids, where each vector is assigned to one centroid. Upon receiving a query, FAISS searches nprobe of those c cells to find the nearest neighbors. This reduces the search time by reducing the search space to the number of centroids to be probed while preserving good performance. However, an IVF index requires training to identify optimal centroids.\nIn Table 5, we measure the time it takes for the search function call to IndexIVF_search to complete. This function is called on 16 chunks concurrently on a Tesla V100 GPU.\nNext, as a natural extension of memory-augmented neural net- works [21], we assess whether the non-parametric memory of RETRO-LI can be moved to a specialized hardware that operates based on the IMC principles. This would make vector searches and thus retrieval much faster, as it foregoes the memory wall issues pre- vailing in GPUs caused by the high bandwidth data transfers. If the retrieval database is very large (such as in RETRO'S case where we have trillions of tokens and 28 billion database keys) the data trans- fer becomes the bottleneck, especially during inference. We estimate that on a larger scale IMC hardware platform inspired by the early-stage prototype [22], the similarity search time could be brought down to several hundred nano-seconds.\nOne drawback of the IMC hardware however is low-precision"}, {"title": "3.3.2 Qualitative Results", "content": "In order to better understand the quantitative results, we present here some generated samples from our worst-performing domain, SlimPa- jama. We evaluate the semantic similarity of the model-generated chunk to the real continuation chunk.\nFor RETRO-LI-on and RETRO-LI-off we pick their best- performing validation samples on SlimPajama-Validation, feed 75% of the context window into the model and generate the next chunk. In order to evaluate the similarity of the generated chunk to the real continuation, we embed it using SBERT and report the similarity measure. We employed several generation modes of which we only present the best-performing ones from retrieval on and off in Tables 7 and 8 respectively, full results for this experiment can be found in Appendix H.\nThe dot product results are in brackets for each chunk. We see that the RETRO-LI-on generated sample is not only more coherent but more similar semantically to the real continuation chunk than the RETRO-LI-off generated sample is to its own real continuation chunk."}, {"title": "3.4 Domain Shift Generalization: Fine-tuning", "content": "In addition to the plug-and-play style domain shift generalization experiments, we conduct experiments in which both RETRO-LI-on (with regularization of Gaussian with At = 0.2) and RETRO-LI-off models undergo a minimal amount of fine-tuning steps with partial parameter updates while the bulk of their parameters remain frozen. The main motivation for this exercise is to assess how far the per- plexity performance can go down with a minimalistic fine-tuning ef- fort (in terms of both the number of updated parameters and steps of updates) compared to the plug-and-play domain shift experiments which involved no fine-tuning whatsoever.\nFrom a selected dataset (e.g. B, C, D), we first randomly sample 15% of the training set data and reserve it as the input sequences to be used during the fine-tuning phase. The remaining 85% of the training set data are directed toward building the retrieval database to be used during the fine-tuning phase. Note that the training input sequences and the retrieval database are made mutually exclusive in order to avoid undesired leakage effects during retrieval.\nAs a starting point, we take the best checkpoints created by training both RETRO-LI-off and RETRO-LI-on models on the base WikiText-103 dataset as mentioned in Section 3.3. For RETRO-LI-on models we observe updating CCA layer parameters leads to slightly worse performance. Hence we update only the parameters in the feed-forward layers and the read-out layers for both model types dur- ing fine-tuning for a few epochs.\nAfter fine-tuning for a few epochs, the updated model checkpoints are used to run inference on the validation sets from the correspond- ing datasets. The validation training sequences and the validation re- trieval database are prepared exactly the same way as in plug-and- play experiments explained in Section 3.3 such that the performance can be compared under equal settings.\nThe results of fine-tuning experiments are presented in Table 9. We evaluate on a subset of datasets used for the plug-and-play do- main shift generalization experiments, namely, BBC-News, Reuters, CNN-DailyMail. At epoch=0, i.e., before any fine-tuning, the per- plexities reported on the validation show that RETRO-LI-on consis-"}, {"title": "4 Related Work", "content": "In RAG, there are various approaches to make use of retrieved neigh- bors. For instance, one approach [26] adds the neighbors to enhance the context of an input sequence, whereas RETRO [2] attends to the neighbors using the CCA mechanism. It is also possible to use both approaches by adding the nearest neighbor to the context window as well as attending to the other neighbors through CCA. RETRO out- performs the approaches without CCA slightly on Natural Questions tasks, but [52] outperforms RETRO by a wide margin by combining the two approaches.\nScalability is central for RETRO in particular, as it significantly improved results in language modeling once the retrieval database reached one trillion tokens. For the models closer to our model size adding retrieval barely made a difference. The figure on the second page of the RETRO paper reveals that although bits-per-byte drops upon adding retrieval, even making the retrieval database almost 10\u00d7 larger (up to 10 billion tokens) did not affect the performance.\nAll of this culminates in a state-of-the-art test perplexity of 2.4 on WikiText-103-Validation. It has to be said that, as the RETRO's authors noted themselves, it is evident that this perplexity is mainly due to validation set leakage. This is practically unavoidable with a retrieval database of this size no matter the validation set, as there is only so much data that can be web-scraped and is public domain.\nFurthermore, a retrieval database consisting of trillions of tokens is unrealistic for most setups. Although it is a one-time cost and adding new entries is straightforward, it is difficult to find open-source datasets of this size, let alone have the resources to clean up and process them appropriately.\nRecent works [17, 27, 31] apply principles of RAG for few-shot learning tasks where retrieval helps with the meta-learning process. However, these models are typically larger compared to RETRO- LI, operate on different tasks/datasets, and crucially fail to establish a retrieval-off baseline for their respective models. This makes the quantitative comparison of RETRO-LI against these models some- what unrealistic."}, {"title": "4.1 RAG and RETRO", "content": "Harnessing the inherent capabilities of language models for specific tasks can be done in many ways. One is through classical fine-tuning, where we add a task-specific head to a pre-trained model to obtain a task-specific model. We can also use pre-trained models via few-shot prompting for tasks such as generative question answering. Fi- nally, in [18] they settled on instruction tuning, where existing sam- ples were augmented with instructions in natural language. The au- thors observed that adding uniform noise, reminiscent of noise added in adversarial literature, to the word embeddings of the instructions improved the performance of such models significantly through a regularization effect. Given that one objective of these RAGs is to improve generalization through the non-parametric memory, we in-vestigated adding such a regularizer to our non-parametric memory."}, {"title": "4.2 NEFTune", "content": "With RETRO-LI, we have shown that by proper architectural en- hancements and training strategies, there is a role for retrieval in the medium-size parametric models using small-sized retrieval databases that are orders of magnitude smaller (570 K up to 2.89 B database tokens) compared to RETRO. RETRO-LI consistently improves lan- guage modeling and cross-domain generalization compared to the same architecture without retrieval. By applying regularization to the non-parametric memory, we improved the generalization abil- ity of RETRO-LI even further. Additionally, we have shown that the non-parametric memory can be made robust against noisy similarity searches, which makes it amenable for deployment on the efficient IMC hardware without performance loss. When needed, RETRO-LI can similarly benefit from fine-tuning.\nFuture work would evaluate task-specific performance, especially in the domain of question answering. RETRO has been shown to lower hallucinations and be less toxic when compared to its non- retrieval counterpart by retrieving from trillions of tokens, but not yet at a small scale. Moreover, there are embedding models that are bet- ter suited for semantic similarity search than SBERT, as measured by the MTEB benchmark. Changing the embedding model is straight- forward, although the retrieval database must be re-computed.\nIn RETRO-LI, we attend to all retrieved neighbors indiscrimi- nately, whereas future work could add a similar mechanism as Self- RAG [1] to decide when and if to retrieve neighbors, or select an op- timal subset of them [53]. It might also explore RETRO-fitting better foundational models, or different attention-based architectures alto- gether.\nFuture work would also include more accurate hardware-aware training, obtained by applying training techniques similar to the ones presented in [40]. These techniques so far are applied to paramet- ric weights. It will be interesting to see how these training dynamics play out in the non-parametric memories similar to the ones used in RETRO-LI."}, {"title": "5 Conclusion and Outlook", "content": "B(_,-,-)H_4 _s.4 +7 _6 _s._8 * -; _e97> ?07 _s0@ 6 _-6 ?09 _-;; _-91B _-177 _! _ 0177C -6 _-33 _"}, {"title": "A Appendices", "content": "We describe the dataset used in the language modeling experiments and the datasets used for the domain shift experiments. The goal was to use datasets with strong baselines to compare our model to, and which would need minimal pre-processing for our purposes."}, {"title": "A.1 WikiText", "content": "The WikiText dataset [30] was created to address the issue of there not being a text dataset with long-form content and original punctuation, capitalization, and numbers. As opposed to web-scraped content, the text is more structured and has fewer typos or colloquialisms. This is because the authors employed a measure of quality control by only considering Wikipedia articles, as well as restricting themselves to verified good or featured articles.\nA good article has been nominated by a reviewer as well as confirmed by an impartial editor to be \"well-written, contain factually accurate and verifiable information, are broad in coverage, neutral in point of view, stable, and illustrated, where possible, by relevant images with suitable copyright licenses.\"\u00b9 Only about 0.5% of Wikipedia articles make the cut.\nOn the other hand, a featured article refers to one of the best articles on Wikipedia as decided upon by the editors. The criteria for a featured article are stricter than for a good article, and more comprehensive\u00b2. Such articles make up around 0.09% of Wikipedia. More details on the processing steps can be found in Section 4.3 of their paper.\nWe use WikiText-103 which consists of around 103 million training and 254 thousand validation tokens. We use the same database for training and validation and generate it using WikiText-103-Train. Validation and training sequences of the WikiText dataset are disjoint by design. To avoid leakage during training we ensure that the neighbors of the training sequences are never direct continuations.\nFinally, we report the 1-gram Jaccard-Similarity [33] of the training sequences and their two nearest neighbors in Figure 4 and the ten nearest neighbors in Figure 5 as a way to determine leakage. As is evident in the histograms, for most neighbors we do not get a Jaccard-Similarity of more than 0.2, so limited leakage is assured.\nIn the RETRO paper they checked if the training sequences have eight or more contiguous tokens in common with one of their neighbors to determine the degree of leakage. In our case, for the training and validation set, 4.76% and 5.1% of the ten nearest neighbors respectively have at least eight contiguous tokens in common with the sequence used to query the index. Evaluating such samples qualitatively, in Figure 3 we can show that this is hardly leakage in a sense that our model could exploit, as the neighbors come from different articles."}, {"title": "A.2 Domain shift datasets", "content": "For our domain shift experiments, we choose datasets of varying textual structures and sources. If a train/test/validation split is given, we take the training data to create the retrieval database and validation data to evaluate on. If only a train/test split is given, we use the test data to evaluate on.\nSince for the inference experiments we created the retrieval database fully out of the training data and only predicted on the validation data, no leakage is possible."}, {"title": "A.3 SlimPajama", "content": "RETRO was trained on MassiveText [37] which is unfortunately proprietary. Furthermore, The Pile [6] which is an open-source alternative to MassiveText and on which RETRO was also trained for comparison purposes, was taken down in July of 2023 due to a DMCA (Digital Millenium Copyright Act) notice\u00b3. The NVIDIA RETRO implementation training data was mostly based on The Pile [45] as well. This means that we could not use the same dataset as RETRO, which would make it harder for us to compare our performance to theirs. As an alternative, looking for datasets similar in data sources and size to The Pile, we decided on SlimPajama [46] which is a cleaned and de-duplicated version of RedPajama [4]. De-duplication is especially important for textual training data, see [25]. In Table 10 we compare The Pile to SlimPajama, where Pile-CC is comparable to CommonCrawl and C4 [38]. We see that although the data sources themselves are similar, the proportions are only somewhat comparable.\nAs most text in the SlimPajama dataset comes from CommonCrawl, it is largely unstructured and conversational. Although for humans it is easy to read and understand, this type of content is significantly dissimilar to Wikipedia articles, in flow, grammar, as well as vocabulary.\nWe use a sub-sampled version of Slimpajama-627B, namely Slimpajama-6B4. Furthermore, Slimpajama-6B still needed some clean-up, as there were many instances of repeating characters used as filler or for code comments in the CommonCrawl portion of the dataset. This leads to a total of 5.5B tokens usable for training. Finally, to reduce memory usage and simplify the code structure, we truncate all samples longer than"}, {"title": "A.4 BBC-News", "content": "The BBC-News dataset [9] consists of 2'225 documents from the British Broadcast Corporation news website, across topics such as business, entertainment, politics, sports, and tech. News articles are highly structured in terms of flow, grammar, and vocabulary, so this dataset is similar to WikiText in that regard. Using their train/test split and our tokenization scheme, we end up with 589'677 training tokens and 468'831 validation tokens. The validation set size is unusually large with respect to the training set size. Since we used the training set as our retrieval database, this might have hindered performance somewhat."}, {"title": "A.5 Reuters", "content": "The Reuters-21578 dataset as created and used by Hayes in [11], consists of 19'043 documents and has 674 categories in total. This dataset is comprised of news stories from the Reuters financial news-wire service in 1987 and just like the BBC-News dataset, is very similar to WikiText in terms of structure and language. However, the differences in vocabulary might be larger here, as the articles are from decades ago. Using Hayes' train/test split and our tokenization scheme, we end up with 3'454'605 training tokens and 174'335 validation tokens."}, {"title": "A.6 Pile-of-Law", "content": "The Pile-of-Law dataset [12] was created in order to mitigate the effects of potentially harmful and biased training data such as can be found through web-crawling alone. The 34 data sources range from \"legal case opinions and filings\", mostly from the U.S.A., to study materials for law exams and were extensively analyzed for toxicity and biases. We use two of their data sources."}, {"title": "A.6.1 Atticus Contracts", "content": "The Atticus contracts subset contains contracts from the Atticus project, specifically the commercial contracts dataset [13]. It consists of 510 contracts and was created to train a model to highlight portions of the contract a human should review. Contract language is highly structured and repetitive, but not necessarily similar to Wikipedia or news articles. We now move away from article style text into something new entirely. Using their train/test split and our tokenization scheme, we end up with 456'681'888 training tokens and 152'337'169 validation tokens, which for performance reasons we had to sub-sample into 7'605'872 validation tokens."}, {"title": "A.6.2 Founding Documents", "content": "This subset consists of letters by U.S.A., founders, which were scraped from Founders Online [49"}]}