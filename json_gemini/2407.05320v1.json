{"title": "KAE: A Property-based Method for Knowledge Graph Alignment and Extension", "authors": ["Daqian Shi", "Xiaoyue Li", "Fausto Giunchiglia"], "abstract": "A common solution to the semantic heterogeneity problem is to perform knowledge graph (KG) extension exploiting the information encoded in one or more candidate KGs, where the alignment between the reference KG and candidate KGs is considered the critical procedure. However, existing KG alignment methods mainly rely on entity type (etype) label matching as a prerequisite, which is poorly performing in practice or not applicable in some cases. In this paper, we design a machine learning-based framework for KG extension, including an alternative novel property-based alignment approach that allows aligning etypes on the basis of the properties used to define them. The main intuition is that it is properties that intentionally define the etype, and this definition is independent of the specific label used to name an etype, and of the specific hierarchical schema of KGs. Compared with the state-of-the-art, the experimental results show the validity of the KG alignment approach and the superiority of the proposed KG extension framework, both quantitatively and qualitatively.", "sections": [{"title": "1 Introduction", "content": "The semantic heterogeneity problem arises whenever there is a need to exploit knowledge graphs (KGs) from heterogeneous resources [36]. Here we focus on KGs where nodes are entities decorated by data properties and relations are object properties. Furthermore, we talk of entity type (etype) meaning the class to which an entity belongs, e.g., Person and Event. A solution of semantic heterogeneity is to perform KG extension that extends the reference KG by knowledge encoded in one or more candidate KGs. Meanwhile, KG alignment is an attractive topic that involves various tasks, e.g., well-known ontology matching (OM), where two practical tasks are considered critical steps to achieving KG extension. Firstly, the alignments between etypes from reference and candidate KGs [2, 21, 32], exploiting only schema level information, as it is the most often case in OM. Secondly, the alignments between reference etypes and candidate entities, exploiting additional information of entities, also known as etype recognition [43,47]. These two kinds of alignments are prerequisites for extending the reference KG at both the schema level and instance level, respectively.\nEtype alignment is a prepositive task during KG extension since etypes define the schema of collected entities in a KG. The reference KG can directly integrate candidate"}, {"title": "2 Intuitive Discussion", "content": "In the knowledge integration area, KG alignment and extension can be organized as sequential tasks, where KG alignment aims to align the knowledge from candidate KG and reference KG, and KG extension aims to integrate target information into the reference KG following the aligned knowledge. To clearly define the above-mentioned tasks,\nwe define a KG as a hierarchy of concepts, where properties are used to describe them.\nSpecifically, we define the schema of a KG and its inner relations as KGS = (C, P, R), where C = {C1, ..., Cn} being the classes of entities (i.e. etypes), P = {p1,\u2026,Pm} being the set of properties, R = {{Ci,T(Ci))|Ci \u2208 C'} being the set of correspondences between etypes and properties, and function T(Ci) returns properties associated with Ci. As for the entities I, we define I = {I1, ..., I\u2081} being the set of entities in KG, where each entity I\u2081 can be identified by one or several etypes. t(I\u2081) refers to the set of associations between entities and properties. We consider that the property pi is used\nto describe an etype C\u2081 or an entity I\u2081 when the property belongs to set T(Ci) or t(Ii),\nrespectively. Thus, given a reference KGref and a candidate KGcand, considering the\ntwo cases in KG alignment, etype alignment task aims to align the candidate etypes"}, {"title": "2.2 Observations and Motivations", "content": "Property is one of the most basic and critical elements for intentionally defining KG concepts, which is independent of the specific label used to name concepts and of the specific hierarchical schema of KGs [31]. For each KG schema, etypes play the role of categorization, and properties aim to draw sharp lines so that each entity in the domain falls determinately either in or out of each etype [29]. Meanwhile, we also have the following observations when comparing properties across different KGs:\nIn a specific KG, each etype is described by a set of properties, whereas most of\nthe properties are distinguishable according to the belonging etypes and a small\nnumber of properties are shared across different etypes.\nSame or similar properties are shared across different KGs for describing same\nconcepts."}, {"title": "3 Knowledge Graph Formalization", "content": "We formalize the relation between properties and KG concepts as associations to introduce the property information into the target KG tasks. At the schema level, the KG schema will be flattened into a set of triples, where each triple encodes information about etype-property associations, e.g., triple \u201corganization-domain-LocatedIn\" encodes the \"organization-LocatedIn\" association. Instance-level cases generally define triples as \"entity-property-entity\", where two associations are encoded. For instance, instance-level triple \"EiffelTower-LocatedIn-Paris\" encodes two entity-property associations \"EiffelTower-LocatedIn\" and \"LocatedIn-Paris\". We introduce FCA [27] to encode such associations as a formalization of the KGs we process. Notice that both the schema-level and instance-level associations are included in the formalization. Specifically, we have two working assumptions:\nWe consider both etypes C and entities I. Similar to general formalization methods [18,51], we associate an entity with its set of properties t(I\u2081). Different from general methods, we also associate an etype with its set of properties T(Ci).\nEtype characterization exploits not only the properties associated with it but also the properties which are associated with other concepts. Thus, we introduce the notion of unassociated properties and exploit this distinction in the formalization process."}, {"title": "4 Property-based Similarity Metrics", "content": "One of the intuitions of our work is to identify etypes and entities by properties that are essential elements for defining KG concepts [30]. The reference etype tends to match with the candidate etype/entity when they have properties overlapped. Therefore, it is critical to measure the overlapped properties between etypes since etypes with completely overlapped properties rarely happen in real-world KGs [26]. Meanwhile, we also exploit the intuitions underlying the normalization of the \"get-specific\" heuristic provided in [33] to distinguish the weights of different overlapped properties. The key inspiration is that properties at different levels of specificity have different relevance in the etype recognition. In particular, a more specific property provides more information that allows for defining concepts. As a result, in this section, we introduce three notions of horizontal specificity, vertical specificity, and informational specificity and their corresponding similarity metrics for measuring the degree of overlapped properties."}, {"title": "4.1 Horizontal Specificity", "content": "For measuring the specificity of a property, a possible idea is to horizontally compare the number of etypes that are described by a specific property, namely the shareability of the property [30]. If a property is used to describe diverse etypes, it means that the property is not highly characterizing its associated etypes. Thus, for instance, in figure 1, the property name is used to describe Person, Place, Athlete and Artist, where name is a common property that appears in different contexts. Dually, settlement is a horizontally highly specific property since it is associated only with the etype Place. Based on this intuition, we consider the specificity of a property as related to its shareability. Therefore, we propose HS (Horizontal Specificity) for measuring property specificity. More precisely, HS aims to measure the number of etypes that are associated with the target property in a specific KG. We model HS as:\n\\(HSKG(E,p) = WE(P) * \u03b5\u03bb(1\u2212|\u039a\u03c1|)\\)\nwhere: p is the input property and Kp is the set of etypes described by the input property in a specific KG and |Kp| is the number of etypes in Kp, thus |Kp| \u2265 1; e denotes the natural mathematical constant [25]; \u5165 represents a constraint factor, the range of A is (0, 1]. The reason for using an exponential function to model the HS is that we aim to normalize the horizontal specificity. The motivation is that different properties may have a larger difference on |Kp| in large KGs. Need to note that the range of the HS is [-1,1], and the extremum points both indicate that the property is shared by one or a few etypes, signifying that the property is highly specific. The impact of such specificity is determined by the target etypes used for description, which is captured by the A, resulting in different outcomes."}, {"title": "4.2 Vertical Specificity", "content": "Etypes are organized into classification hierarchies such that the upper-layer etypes represent more abstract or more general concepts, whereas the lower-layer etypes represent"}, {"title": "4.3 Informational Specificity", "content": "Horizontal specificity allows measuring the shareability of properties, which is independent and does not change (increase/decrease) with the number of entities populating it. We take into account this fact by introducing the notion of informational specificity IS. The intuition is that IS will decrease when the entity counting increases. Thus, for instance, the IS of gold medalist decreases when there are increasing entities of athletes, as from the schema in Figure 2. Clearly, IS, differently from HS, can be used in the presence of entities. The definition of informational specificity is inspired by Kullback-Leibler divergence theory [54], which is introduced to measure the difference between two sample distributions Y and \u00dd. More specifically, given a known sample distribution Y, assume that a new coming attribute x changes Y to Y. Then, the Kullback-Leibler divergence theory demonstrates that the importance of x for defining Y is positively related to the difference between Y and Y. In the definition of informational specificity, we need to exploit some notions from information theory, where we apply informational entropy H(K) as:\n\\(H(K) =\\frac{- \u03a3F(E) log \\frac{F(E)}{F(K)}}\\)\nwhere K refers to any subset of K in a KG and |K| is the number of etypes included; H(\u00b7) represents the informational entropy of an etype set; Ei is a specific etype in set Ku, thus Kv \u2265 1; F(E\u00bf) refers to the number of samples of etype Ei, and F(K) refers to the number of all samples in K. Need to notice when we calculate the informational entropy for KGs without entities, F(E\u2081) = 1 since each KG includes one etype sample. For KGs with entities, F(E\u2081) and F(K) depend on the number of samples of the given etype and the subset of KG. After obtaining informational entropy, the"}, {"title": "4.4 Similarity Metrics", "content": "We have modeled the specificity of properties, which represent their weights for describing KGs from different measuring aspects. Then, we define three similarity metrics based on the corresponding specificity to measure the property overlapping between two concepts. Given two KGs, the reference KG A and candidate KG B, we define a function for calculating different similarities between etypes/entities from A and B based on their corresponding specificity:\n\\(Sim(Ea, Eb) =\\frac{1}{2}\u03a3( \\frac{( SPCA (Ea:Pi) + SPCB(Eb, Pi))}{prop(Ea)| + prop(Eb)|})\\)\nwhere we take Ea, Et as the candidate etypes from A and B respectively, thus Ea \u2208 A and Eb \u2208 B; prop(E) refers to the properties associated with the specific etype and prop(E)| is the number of properties in prop(E); SPCETG(\u00b7) represents the specificity measurements we defined above, SPC(\u00b7) = {HS(\u00b7),VS(\u00b7), IS(\u00b7)}, thus SPCA(Ea, pi) and SPCB(Eb, pi) refer to the specificity of the aligned property pi in A and B respectively; k is the number of aligned properties which are associated with both etype Ea and Eb. As a result, we obtain three similarity metrics which are horizontal similarity Sim\u00ed, vertical similarity Simy and informational similarity Sim1.\nNotice that each similarity metric is symmetric, more specifically, Sim(Ea, Eb) =\nSim(Eb, Ea). Note also that we apply z-score normalization [42] to all similarity metrics at during calculations to constrain the range of SimH, Simy, Sim\u2081 in [0, 1] for computational convenience purposes. The normalization following the function z =\n\u03c3 , where z is the normalized value, x is the original value, \u03bc is the mean of all\nvalues, and o is the standard deviation of all values."}, {"title": "5 The Proposed Method", "content": "In order to extend the reference KG by given candidate KGs, we propose a framework that exploits the FCA formalization and property-based similarity metrics defined above, shown in Figure 3. The framework mainly consists of six modules, namely:\nKG parser, KG formalization module, property matcher, similarity calculation module, etype matcher, and KG extension module. Notice that modules are marked in different colors for distinguishing their usage."}, {"title": "5.1 Similarity Calculation Algorithm", "content": "The property-based similarity calculation is one of the critical parts of this work.\nWe detail the calculation as SimilarityCalculation(\u00b7), as shown in Algorithm 1. After formalizing reference KGref and candidate KGcand, we assume that the two FCA contexts fa and fo are generated correspondingly. Then we obtain property matching pairs PM from the property matcher. To calculate the similarity of etypes, we need to generate candidate etypes pairs EM for further processing. For each candidate pair in EM, we check their correlated properties and update the specificity values to S\u0456\u0442\u043d, Simy and Sim, when their properties are aligned. After traversing all the candidate pairs, we obtain a complete etype similarity list L which will be used for training the ML\nmodel and aligning candidate etypes. Notice that we present the algorithm for calculating the horizontal similarity S\u0456\u0442\u0126 in algorithm 1, the metrics vertical similarity Simv"}, {"title": "5.2 Knowledge Graph Extension Algorithm", "content": "With the help of etype alignment results, we extend the reference KG by integrating\nthe entities from the candidate KG KGcand, details are shown in Algorithm 2. For each\netype Es that is aligned with etype Ea from KGcand, we directly add its properties and\nentities into Ea by addProperty(\u00b7) and addEntities(\u00b7), respectively. We also consider\nall subclasses of the aligned etype Et since the subclass inherit the properties of the\netype and will bring new entities. If the subclass Esub of Et is not aligned with any\nother etypes in KGref, Esub and its associated properties and entities will be merged\ninto Ea. Thus, we integrate candidate entities with KGref when their etypes are able\nto align with KGref. Then, the proposed property-based similarity metrics are applied\nto align the rest of the candidate entities with etype Eref from reference KG, namely\nfunction EtypeRecognizer(\u00b7). If we successfully match an etype En with the entity\nEnti, Enti will be merged into En, if not, Enti will be discarded. Need to notice that\ndepending on the real-world application scenario and topic, the KG engineer will decide"}, {"title": "5.3 Machine Learning-based Matchers", "content": "According to the Algorithm 2 and Figure 3, we can find there are three ML-based\nmatchers in our proposed framework. Here we present more details of training and\nsetting these matchers.\nEtype Matcher We develop an ML-based method that deals with etype matching as a\nbinary classification task. The main idea is to predict if two incoming etypes are aligned\nwith each other. For applying this method, a list of candidate pairs are generated by\npairing etypes from KGcand and KGref. We will record candidate etype pairs EMali\nwhen the result of classification is \u201caligned\u201d. The proposed property-based similarity\nmetrics SimH, Simy and Sim, are introduced to train the ML models for matching\netypes. For better performance, we also exploit label-based and language-based similar-\nity metrics, along with property-based similarity metrics for training the etype matcher.\nEtype Recognizer The strategy for developing an etype recognizer is very similar to\nthe etype matcher, where we predict if candidate entities are aligned with target etypes.\nThus, we will also generate candidate pairs that consist of entities from KGcand and\netypes from KGref. Etypes from KGref will be outputted as the final recognition re-\nsults. Compared to the etype matcher, the main difference is that the etype recognizer\nmainly uses property-based similarity metrics as features for model training since the\nlexical labels are not applicable to match entities and etypes. Thus, property-based sim-\nilarity metrics SimH, Simy, and Sim\u2081 are applied for the etype recognizer.\nProperty Matcher The property matcher aims to align properties between KGs, where\nlabel-based and language-based similarity metrics are used for modeling training. The\nmatching strategy is same as etype matcher. It is critical to obtain a powerful property\nmatcher since both etype matcher and etype recognizer are based on the result of prop-\nerty matcher. In this section, we discuss the following solutions to reduce the effect of\nmisaligned properties.\nUse of the formalization parameter WE(p). As we introduced in section 2, besides\n\"associated\" (positive) and \u201cunassociated\u201d (negative) properties, we also defined\n\"undefined\" properties (neutral). Since misaligned properties will not be used for\nsimilarity calculation, they are treated as \"undefined\" properties which will not af-\nfect the model training and reduce the additional interference. However, additional\ninterference from misaligned properties appears if \u201cunassociated\u201d and \u201cundefined\u201d\nproperties are not distinguished.\nUse of similarity metrics. Similar to lexical-based similarity metrics, our property-\nbased similarity metrics also allow to match etypes by soft aligning, even if there\nare few properties not aligned. This will increase the robustness of our etype recog-\nnition approach."}, {"title": "6 Evaluation", "content": "In this section, we aim to evaluate our proposed method by two crucial steps during KG integration, including (1) etype alignment, and (2) etype recognition of entities. Thus, we organize this section as follows. Section 6.1 introduces the experimental setups,\nincluding the datasets we used, feature selection, and evaluation strategy. Sections 6.2\nand 6.3 present the analysis and quantitative evaluation results, respectively. In section\n6.4, we also demonstrate the ablation study to explain the setting of parameters."}, {"title": "6.1 Experimental Setup", "content": "Dataset Selection. For evaluating the result of etype alignment, we exploit the Ontology Alignment Evaluation Initiative (OAEI) as the main reference for the selection of the etype recognition problems. As of today, this in fact the major source of KG alignment problems. Our proposed method for KG extension involves extending a reference KG through one or more candidate KGs. This implies that the reference KG typically possesses a more comprehensive and high-quality schema, serving as a foundation for KG extension. Our approach focuses on KGs that incorporate etypes associated with a substantial number of properties and complete schemas. As a result, we have selected the following cases: the bibliographic ontology dataset (BiblioTrack) [20] and conference track (ConfTrack) [59] (ra110 version). From the bibliographic ontology dataset, we select #101-103 and series #301-304, which present real-life ontologies for bibliographic references from the web. We select the alignment between #101 and #304 as the training set for training our ML-based etype matcher, and the rest of the ontology alignments as the testing set. The conference track contains 16 ontologies, dealing with conference organizations, and 21 reference alignments. We set all 21 reference alignments from the conference track as the testing set to validate our etype matcher. Notice that we select the training and testing set from different datasets since we aim to prove the adaptation of our approach, which also prevents our approach from overfitting.\nFor validating the algorithm of etype recognition, we build a dataset called EnType,\nsince there is no publicly released dataset for such etype recognition task between two\nKGs. We exploit DBpedia infobox dataset\u00b9\u00b9 as the reference KG for providing reference\netypes. Because DBpedia is a general-purpose KG that contains common etypes in the\nreal world, where sufficient properties are applied for describing these etypes. Then we\nselect candidate entities from DBpedia, SUMO and several domain-specific datasets\n[53]. The entities we selected mainly according to common etypes, more specifically,"}, {"title": "6.2 Etype Alignment", "content": "Qualitative analysis\nQuantitative Evaluation. We apply two ML models to evaluate the validity of our proposed similar metrics on the etype alignment task, including XGBoost [16] and artificial"}, {"title": "6.3 Etype Recognition of Entities", "content": "Qualitative Analysis\nQuantitative Evaluation As for the evaluation of the etype recognizer, we involve two subsets EnTypeself and EnTypeGen in this experiment. The subset EnType Self contains candidate entities and reference etypes from the same KG, i.e., self recognition, which should be a relatively easier task. In turn, general recognition denotes the recognition on subset EnTypeGen, where candidate entities are selected from other resources"}, {"title": "6.4 Ablation Study", "content": "We demonstrate ablation studies in this section for validating the effectiveness of some specific components introduced in our KG extension framework.\nEffect of similarity metrics The first ablation study is to evaluate if each of the pro-\nposed property-based similarity metrics is effective. In this experiment, we test the\nbackbones14 (B) which were used in the etype alignment and recognition tasks, respec-\ntively. Based on the backbones, we also design a controlled group that includes models\ntrained without one of the property-based similarity metrics (i.e. B-Simy, B-SimH and\nB-Sim\u2081) and models trained without all metrics (i.e. B-L). If the backbones perform\nbetter than the corresponding models in the controlled group, we can quantitatively\nconclude that each of the property-based similarity metrics (Simy, SimH, Sim\u2081) con-\ntributes to the etype alignment and recognition tasks. Table 7 demonstrates the F1-\nmeasure of each group. We apply ConfTrack for etype alignment and EnTypeGen for\netype recognition. Note that we select two models for both cases as Table 7 shows.\nWe find that backbones perform better than models in the controlled group, especially\nfor models trained without all metrics. Thus, we consider all property-based similar-\nity metrics contribute to better recognition performance. Particularly, Simy and SimH\nsignificantly affect the performance of etype alignment cases, and Sim, affects etype\nrecognition cases more.\nEffect of constraint factor In section 4.1, we defined a constraint factor A for calculat-\ning the metric S\u0456\u0442\u043d. This study aims to statistically identify the value of A. We apply\nthe dataset ConfTrack and its two best-performed models. The value of A is set evenly\nfrom 0.1 to 1 by discrete points. We evaluate if this pre-set factor affects the final recog-\nnition performance and obtain the best value of A for generic etype recognition. Table\n8 demonstrates the results, where we highlight both the best and second-best results.\nWe can find that different values of A do affect the final etype recognition performance.\nAnd two models show a similar trend that the best value of A is close to 0.5. As a result,\nwe assign x = 0.5 to calculate metric Sim in our experiments."}, {"title": "7 Case Study", "content": "This section aims to qualitatively analyze the KG extension performance of our pro-\nposed method by use cases. In the case of KG extension, we assume that there will be\na reference KG extended based on one or more candidate KGs. We aim to simulate a\nreal-world scenario, where people extend the general purpose KG by specific-domain\nKGs to enlarge its usability. We select the widely applied schema.org as the reference\nKG. For the candidate KGs, we introduce two specific-domain KGs Transportation15\nand educationtrentino16. These two KGs are created for presenting local transportation\nand education, respectively. We selected these KGs because they provide very different\nexamples in terms of the number of properties and etypes. Moreover, almost all their\netypes labels are human understandable, which helps qualitative analysis.\nWe introduce four ranking metrics in the case study, namely Class Match Measure\n(CMM) [1], Density Measure (DEM) [1], Focus [26] and TF-IDF [46]. CMM aims to\nevaluate the coverage of a KG for the given search etypes, by looking for etypes in"}, {"title": "8 Related Work", "content": "Ontology matching and schema alignment are attractive research topics in recent decades. In the early phases of the research, researchers mainly focused on string-based methods. String analysis techniques were defined including 1) string-based metrics (N-gram,\nLevenshtein, etc.), 2) syntactic operations (lemmatization, stop word removal, etc.) and\n3) semantic analysis (synonyms, antonyms, etc.) [14]. Sun et al, [52] review a wide\nrange of string similarity metrics and propose the ontology alignment method by se-\nlecting similarity metrics in different scales. Although string-based methods can lead\nto effective performance in many cases, selecting the right metric for matching specific\ndatasets is the most challenging part. To solve this issue, an ensemble matching strategy\nis introduced in some studies [12,38], which apply multiple matchers based on differ-\nent string-based metrics. The principle of these works is that the combined matchers\nare more powerful than individual ones. The structure of a KG has also been consid-\nered as important information for identifying etypes [5, 28]. Such studies suppose that\ntwo etypes are more likely to be aligned if they have the same super-class or sub-class.\nThe LogMap system [35] uses a two-step matching strategy, that is, matches two etypes\nEa and Eb by a lexical matcher, and then considers the etypes that are semantically\nclose to Ea are more likely to be semantically close to Eb. AML [24] introduces an\nontology matching system that consists of a string-based matcher and a structure-based\nmatcher, building internal correspondences by exploiting is-a and part-of relationships.\nMachine learning techniques have been widely applied to this topic. Some studies\nmodel the etype matching task as a binary classification task, trying to encode the in-\nformation like string and structure similarities as features for model training. Amrouch\net al, [3] develop a decision tree model by exploiting lexical and semantic similarities\nof the etype labels to match schemas. By encoding the lexical similarity of the su-\nperclass and subclass as structure similarity, Bulygin and Stupnikov [13] improve the\nformer method and achieve promising results. At the same time, formal concept analy-\nsis (FCA) lattices are applied in schema matching methods [15,51]. To refine the health\nrecords searching outputs, Cure et al, [18] exploit FCA and Semantic Query Expan-\nsion to assist the end-user in defining their queries and in refining the expanded search\nspace. Stumme et al, [51] propose a bottom-up ontology merging approach by using\nFCA lattices to keep the ontology hierarchy."}, {"title": "8.2 Entity Type Recognition", "content": "According to the different usage and motivation, studies on entity type recognition (also\ncalled entity typing) focus on three main directions: (1) recognizing the type of en-\ntity from text [40, 56]; (2) recognizing the type of entities from the single KG for KG"}, {"title": "9 Conclusions", "content": "In this paper, we have proposed an ML-based framework for KG alignment and exten-\nsion via a set of novel property-based similarity metrics. Firstly, we introduce a KG\nformalization method, which encodes etypes/entities and their corresponding proper-\nties into FCA contexts. We discuss that the corresponding properties are used to inten-\ntionally describe etypes, which provides us with a novel insight for identifying etypes.\nThen we propose three metrics for measuring the contextual similarity between refer-\nence etypes and candidate etypes/entities, namely the horizontal similarity Sim, the\nvertical similarity Simy, and the informational similarity Sim\u0131. Based on our pro-\nposed metrics, we introduce the framework with detailed algorithms and modules for"}]}