{"title": "Continual Learning of Nonlinear Independent Representations", "authors": ["Boyang Sun", "Ignaiver Ng", "Guangyi Chen", "Yifan Shen", "Qirong Ho", "Kun Zhang"], "abstract": "Identifying the causal relations between interested variables plays a pivotal role in\nrepresentation learning as it provides deep insights into the dataset. Identifiability, as the central theme of this approach, normally hinges on leveraging data from\nmultiple distributions (intervention, distribution shift, time series, etc). Despite\nthe exciting development in this field, a practical but often overlooked problem\nis, what if those distribution shifts happen sequentially? In contrast, any intelli-\ngence possesses the capacity to abstract and refine learned knowledge sequentially;\nlifelong learning. In this paper, with a particular focus on the nonlinear indepen-\ndent component analysis (ICA) framework, we move one step forward toward the\nquestion of enabling models to learn meaningful (identifiable) representations in a\nsequential manner, termed continual causal representation learning. We theoreti-\ncally demonstrate that model identifiability progresses from a subspace level to a\ncomponent-wise level as the number of distributions increases. Empirically, We\nshow that our method achieves performance comparable to nonlinear ICA methods\ntrained jointly on multiple offline distributions and, surprisingly the incoming new\ndistribution doesn't necessarily benefit the identification of all latent variables.", "sections": [{"title": "1 Introduction", "content": "In many data-driven problems, observations can be considered as the output of mathematical functions\napplied to the underlying representations. The process of learning those meaningful representations\nbased on the measured variables, known as representation learning, is crucial in modern machine\nlearning fields with applications spanning from visions [53, 1, 11, 26] to natural language processing\n[31, 34, 50, 8]. However, traditional approaches to representation learning often fall short by merely\ncapturing statistical correlation without considering the underlying causal structure, which is crucial\nfor achieving robust generalization across different tasks and datasets [40]. Therefore, identifying\ncausal relations between relevant variables is indispensable in representation learning to comprehend\nthe intricate relationships within the datasets.\nDiscovery of causal relations is never an easy problem. In many scenarios, we want to benefit the\nidentifiability of the causal structure by leveraging non-i.i.d data. In traditional causal discovery, it's\nwell known the causal structure can only be identified up to Markov equivalent class without assuming\nfunctional class [45] in the i.i.d case. However, interventions, seen as an \"active distribution shift\",\ncan break the asymmetry and reveal the causal direction. Besides that, a series of studies indicate that\ndistribution shifts and heterogeneous data can improve the identifiability of causal structure [18, 48].\nWhen the interested variables are not directly measured, causal representation learning (CRL)[40]\naims at recovering the latent causal variables and their causal structures from the observations.\nHowever, similar to causal discovery, learning identifiable representations from i.i.d data is highly\nchallenging. In fact, the identifiability of nonlinear independent component analysis (ICA), as\nthe simplest case of CRL where all latents are independent, is proven to be impossible without"}, {"title": "2 Related Work", "content": "Causal representation learning. Beyond conventional representation learning, causal representation\nlearning aims to identify the underlying causal generation process and recover the latent causal\nvariables. There are pieces of work aiming towards this goal. For example, it has been demonstrated\nin previous studies that latent variables can be identified in linear-Gaussian models by utilizing\nthe vanishing Tetrad conditions [43], as well as the more general concept of t-separation [42].\nAdditionally, the Generalized Independent Noise (GIN) condition tried to identify a linear non-\nGaussian causal graph [56]. However, all of these methods are constrained to the linear case while\nnonlinear ICA provides a promising framework that learns identifiable latent causal representations\nbased on their non-linear mixture. However, the identifiability of nonlinear ICA has proven to be\na challenging task [21], which always requires further assumptions as auxiliary information, such\nas temporal structures [44], non-stationarities [19, 20], or a general form as auxiliary variable [22]."}, {"title": "3 Identifiable Nonlinear ICA with Sequentially Arriving Distributions", "content": "In this section, we conduct a theoretical examination of the relationship between model identification\nand the number of distributions. Initially, we introduce the causal generation process of our model (in\nSection 3.1), which considers the dynamics of changing distributions. Subsequently, we demonstrate\nthat model identifiability improves with the inclusion of additional distributions. More specifically,\nwe can achieve component-wise identification with 2n + 1 distributions (in Section 3.2.1), and\nsubspace identification with n + 1 distributions (in Section 3.2.2). Building on these theoretical\ninsights, we introduce our method for learning independent causal representation in the context of\ncontinually emerging distributions (in Section 3.3)."}, {"title": "3.1 Problem Setting", "content": "As shown in Figure 1, we consider the data generation process as follows:\n$z_s \\sim p_{Z_s|u}, \\qquad z_c \\sim p_{Z_c}, \\qquad z_s = f_u(\\tilde{z}_s), \\qquad x = g(z_c, z_s)$,\nwhere $x \\in \\mathcal{X} \\subseteq \\mathbb{R}^d$ are the observations mixed by latent vari-\nables $z \\in \\mathcal{Z} \\subseteq \\mathbb{R}^n$ through an invertible and smooth nonlinear\nfunction $g: \\mathcal{Z} \\rightarrow \\mathcal{X}$ ($d > n$), which is also called a $C^2$ dif-\nfeomorphism. The latent variables $z$ can be partitioned into two\ngroups: changing variables $Z_s \\in \\mathcal{Z}_s \\subseteq \\mathbb{R}^{n_s}$ whose distribution\nchanges across the distribution indicator $u$, and invariant variables\n$Z_c \\in \\mathcal{Z}_c \\subseteq \\mathbb{R}^{n_c}$ whose distribution remains invariant. In this paper,\nwe refer $u$ as the domain. Given $T$ distributions in total, we have\n$p_{Z_s|u_k} \\neq p_{Z_s|u_l}, p_{Z_c|u_k} = p_{Z_c|u_l}$, for all $k,l \\in \\{1,...,T\\},k \\neq l$.\nWe parameterize the distribution change for changing variables $z_s$\nas the function of $u$ to its parent variables $\\tilde{z}_s$, i.e. $z_s = f_u(\\tilde{z}_s)$.\nOne can understand this setting with the following example: sup-\npose the higher level variables follow Gaussian distribution, i.e.,\n$\\tilde{z}_s \\sim \\mathcal{N}(0, I)$, and $u$ could be a vector denoting the variance of the distribution. The combination of\n$u$ with $\\tilde{z}_s$ will produce a Gaussian variable with different variances at different distributions.\nThe objective of nonlinear ICA is to recover the latent variables $z_s$ and $z_c$ given the observation $x$\nand domain variables $u$ by estimating the unmixing function $g^{-1}$. In this paper, we consider the case"}, {"title": "3.2 Identifiability Theory of Nonlinear ICA", "content": "The identifiability is the key to nonlinear ICA to guarantee meaningful recovery of the latent variables.\nMathematically, the identifiability of a model is defined as\n$\\forall(\\theta,\\theta'): p_{\\theta}(x) = p_{\\theta'}(x) \\implies \\theta = \\theta'$,\nwhere $\\theta$ represents the parameter generating the observation $x$. That is, if any two different choices\nof model parameter $\\theta$ and $\\theta'$ lead to the same distribution, then this implies that $\\theta$ and $\\theta'$ are equal\n[25]. For our data generation defined in (1), we have $\\theta = (g, z_c, z_s)$, and $\\theta' = (\\hat{g}, \\hat{z}_c, \\hat{z}_s)$ which\ndenotes the estimated mixing function, estimated invariant variables, and estimated changing variables\nrespectively. Thus, a fully identifiable nonlinear ICA needs to satisfy at least two requirements:\nthe ability to reconstruct the observation and the complete consistency with the true generating\nprocess. Unfortunately, current research cannot achieve this level of identifiability without further\nassumptions that are considerably restrictive. Therefore, existing works typically adopt a weaker\nnotion of identifiability. In the following, we discuss two types of identifiability for the changing\nvariable, and show that the identifiability progressively increases from subspace identifiability to\ncomponent-wise one by incorporating more distributions.\nIn this work, we follow [27] and assume our estimated latent process $(\\hat{g}, \\tilde{Z}_c, \\tilde{Z}_s)$ could generate\nobservation $x$ with identical distribution with observation $x$ generated by the true latent process\n$(g, z_c, z_s)$, i.e.,\n$p_{X|u}(x'|u') = p_{\\hat{X}|u}(\\hat{x}'|u'), \\qquad x' \\in \\mathcal{X}, u' \\in \\mathcal{U}$."}, {"title": "3.2.1 Component-wise Identifiability for Changing Variable", "content": "First, we show that the changing variable can be identified up to permutation and component-wise\ninvertible transformation with sufficient changing distributions. Specifically, for the true latent\nchanging variable $z_s$, there exists an invertible function $h = g^{-1} \\circ \\hat{g}: \\mathbb{R}^{n_s} \\rightarrow \\mathbb{R}^{n_s}$ such that\n$z_s = h(\\hat{z}_s)$, where $h$ is composed of a permutation transformation $\\pi$ and a component-wise nonlinear\ninvertible transformation $A$, i.e., $\\hat{g} = g \\circ \\pi \\circ A^{-1}$. That is, the estimated variable $\\hat{z}_i$ and the true variable\n$z_i$ have a one-to-one correspondence with an invertible transformation for $\\forall i, j \\in \\{1, ..., n_s\\}$. We\nhave the following lemma from [27]."}, {"title": "3.2.2 Subspace Identifiability for Changing Variable", "content": "Although component-wise identifiability is powerful and attractive, holding $2n_s + 1$ different distri-\nbutions with sufficient changes remains a rather strong condition and may be hard to meet in practice.\nIn this regard, we investigate the problem of what will happen if we have fewer distributions. We first\nintroduce a notion of identifiability that is weaker compared to the component-wise identifiability\ndiscussed in the previous section."}, {"title": "3.3 Method", "content": "In this section, we leverage the insight of the identifiability theory from previous section to develop\nour estimation method.\nAs shown in Lemma 1 and Theorem 1, we are aiming at estimating causal\nprocess $\\hat{g}, \\hat{z}_c, \\hat{z}_s$, to reconstruct the distribution of observation. As shown in Figure 3, we construct a\nVariational Autoencoder (VAE) with its encoder $q_{\\hat{g}^{-1},q_{\\tilde{z}_s|z}}(z|x)$ to simulate the mixing process and the\ndecoder $\\hat{g}$ to reconstruct a matched distribution $x = \\hat{g}(z)$. Besides, as introduced in data generation\nin Equation 1, the changing latent variable is generated as the function of high-level invariance $\\tilde{z}$\nwith a specific domain influence $u$. Assuming the function is invertible, we employ a flow model to\nobtain the high-level variable $\\tilde{z}$, by inverting the function, i.e., $z_s = f_u^{-1}(z_s)$. To train this model,\nwe apply an ELBO loss as:\n$\\mathcal{L}(\\hat{g}, q_{\\tilde{z}_s}, f_u^{-1}, q_{\\tilde{z}}) = \\mathbb{E}_{x \\sim q_X} \\mathbb{E}_{z \\sim q_{\\hat{g}^{-1},q_{\\tilde{z}_s|z}}}( \\frac{1}{2} ||x-\\hat{x}||^2 + \\alpha KL(q_{\\hat{g}^{-1},q_{\\tilde{z}_s|z}} (z_c|x) || p(Z_c))$\n$\\quad + \\beta KL(q_{\\hat{g}^{-1},q_{\\tilde{z}_s|z},f_u^{-1}} (\\tilde{z}_s|x || p(\\tilde{Z}_s)),$\nwhere $\\alpha$ and $\\beta$ are hyperparameters controlling the factor as introduced in [14]. To make the (5)\ntractable, we choose the prior distributions $p(\\tilde{z}_s)$ and $p(z_c)$ as standard Gaussian $\\mathcal{N}(0, I)$.\nThe subspace identifiability theory in Section 3.2.2\nimplies that the ground-truth solution lies on a manifold that can be further constrained with more\nside information, up to the solution with component-wise identifiability. Consequently, it is intuitive\nto expect that when we observe distributions sequentially, the solution space should progressively\nnarrow down in a reasonable manner.\nIt motivates us to first learn a local solution with existing distributions and further improve it to align\nwith the new arriving domain without destroying the original capacity. Specifically, to realize causal"}, {"title": "4 Experiments", "content": "In this section, we present the implementing details of our method, the experimental results, and the\ncorresponding analysis."}, {"title": "4.1 Experiment Setup", "content": "Data. We follow the standard practice from previous work [22, 27] and compare our method to the\nbaselines on synthetic data. We generate the latent variables $z_s$ for non-stationary and mixed Gaussian\ndistributions with domain-influenced variance and mean, while $z_c$ follows standard Gaussian and\nmixed Gaussian with constant mean and variance. The mixing function is a 2-layer MLP with\nLeaky-Relu activation. More details are in Appendix A6.\nEvaluation metrics. We use Mean Correlation Coefficient (MCC) to measure the identifiability of\nthe changing variable $z_s$. However, as the identifiability result can only guarantee component-wise\nidentifiability, it may not be fair to directly use MCC between $\\hat{z}_s$ and $z_s$ (e.g. if $\\hat{z} = z^2$, we will\nget a distorted MCC value). We thus separate the test data into the training part and test part, and\nfurther train separate MLP to learn a simple regression for each $\\hat{z}_s$ to $z_s$ to remove its nonlinearity on\nthe training part and compute the final MCC on the test part. We repeat our experiments over 5 or 3\nrandom seeds for different settings."}, {"title": "4.2 Experimental Results", "content": "Comparison to baseline and joint training. We evaluate the efficacy of our proposed approach\nby comparing it against the same model trained on sequentially arriving distributions and multiple\ndistributions simultaneously, referred to as the baseline and theoretical upper bound by the continual\nlearning community. We employ identical network architectures for all three models and examine\nfour distinct datasets, with respective parameters of $z$ being Gaussian and mixed Gaussian with\n$n_s = 4, n = 8$, as well as $n_s = 2, n = 4$. Increasing numbers of distributions are assessed for each\ndataset. Figure 4 shows our method reaches comparable performance with joint training. Further\nvisualization can be found in Appendix A4.\nIncreasing distributions. For dataset $n_s = 4, n = 8$ of Gaussian, we save every trained model after\neach domain and evaluate their MCC. Specifically, we evaluated the models on the original test dataset,\nwhich encompasses data from all 15 distributions. As shown in part (a) of Figure 5, remarkably,\nincreasing distributions lead to greater identifiability results, which align with our expectations that\nsequential learning uncovers the true underlying causal variables as more information is revealed.\nSpecifically, we observe that the MCC reaches a performance plateau at 9 distributions and the extra\ndistributions(from 9 to 15) don't provide further improvement. This appears to be consistent with the\nidentifiability theory that $2n_s + 1 = 9$ distributions are needed for identifiability.\nAs dis-\nDiscussed in Section 3.2.2, the new domain may impair the identifiability of partial variables. While joint\ntraining always shuffles the data and doesn't care about the order information, learning sequentially\nto some extent mitigates the impairment of identifiability.\nTo test our hypothesis, specifically, we conducted an experiment in which both $z_1$ and $z_2$ are Gaussian\nvariables. The variance and mean of $z_1$ change in the second domain, while the other variable\nchanges in the third domain. We then compare our method with joint training only for latent variable\n$z_1$. We repeat our experiments with 3 random seeds and the experiment shows that the MCC of\nour method for $z_1$ reaches up to 0.785 while joint training retains at 0.68 as shown in Figure 5(b).\nIn terms of visual contrast, the scatter plot obtained using our method on the left of Figure 5(b)"}, {"title": "Ablation study on prior knowledge of chang- ing variables.", "content": "A major limitation of our ap-\nproach is the requirement for prior knowledge\nof the number of changing variables. Devel-\noping a method to automatically determine the\nnumber of changing variables is nontrivial in the\ncontinual learning scenario. Therefore, we turn\nto conducting an ablation study to investigate\nthe sensitivity of this prior knowledge."}, {"title": "5 Conclusion", "content": "In this paper, we present a novel approach for the fundamental, but overlooked problem: as long as\nthe learning of identifiable representations relies on multiple distributions, how can we facilitate this\nlearning in a continual manner? We believe this approach is unaviodable in practical scenario and just\nakin to human learning. With a particular focus on nonlinear ICA framework, we examined the rela-\ntionship between model identification and the number of observed distributions. Our findings indicate\nthat as additional distributions are incorporated, the identifiability of changing variables escalates,\nwith subspace identification achievable with $n_s + 1$ distributions and component-wise identification\nrequiring $2n + 1$ distributions or more. Besides, we briefly show that the introduction of new\ndistributions does not necessarily contribute to all variables. Empirical evaluations have demonstrated\nthat our approach achieves performance on par with nonlinear ICA techniques trained jointly across\nmultiple offline distributions, exhibiting greater identifiability with increasing distributions observed.\nLimitation A obvious limitation of this approach is the requirement for prior knowledge of the\nnumber of changing latent variable. Besides that, the gradient based method pose a challenge to the\nscaling of the algorithm.\nFuture work We believe this paper is only a preliminary exploration of CCRL\u2014there are many\nissues that need to be addressed. Firstly, this paper focuses solely on the nonlinear Independent\nComponent Analysis (ICA) framework, which is the simplest form of CRL. Recovering the latent\nvariables and their causal structure by observing sequentially arriving distributions is a direct challenge.\nSecondly, even in causal discovery, how to leverage sequential distribution shifts to benefit the\nidentification of causal relations is worth investigating. Thirdly, it is important to note that humans\nlearn actively and selectively. Correspondingly, understanding how to \"smartly\" utilize important\ndomains also merits in-depth exploration."}, {"title": "A1 Proof and Discussion", "content": "We divide our proof into the following parts. First, we start from the matched distribution of\nthe estimated observation and the real observation, then we show the the true latent variables can\nbe expressed as invertible transformations of the estimated variables. We then use derivatives to\nconstruct component-wise relations between the estimated variables with the true latents. We finally\nshow, with enough domains, we can construct the matrix whose invertibility will force the changing\nvariables subspace identifiable with n + 1 domains and component-wise identifiable with 2n + 1\ndomains.\nWe start from the matched distribution as introduced in Equation 3: for u' \u2208 U\n$p_{X|u} \\approx p_{\\hat{X}|u}$\nwill imply\n$p_{g(z)|u} \\approx p_{\\hat{g}(\\hat{z})|u}$.\naccording to the function of transformation, we can get\n$p_{g^{-1}\\circ g(z)|u}|J_{g^{-1}}| \\approx p_{g^{-1}\\circ \\hat{g}(\\hat{z})|u}|J_{g^{-1}}|$.\nLet $h := g^{-1}\\circ \\hat{g}$ to express the transformation from estimated latent variables to real latent variables,\ni.e., $z = h(\\hat{z})$. As long as both $\\hat{g}$ and $g$ are invertible, the transformation $h$ should also be invertible.\nWe can then get the following\n$p_{z|u} \\approx p_{h(\\hat{z})|u}$"}, {"title": "A1.1 Subspace identifiability with ns + 1 domains", "content": "Take the derivative of Equation 16 with estimated invariant variable $\\hat{z}_j$ where $j \\in \\{n_s+1,..., n\\}$. We\ncan get\n$\\sum_{i=1}^{n} \\frac{\\partial q_i(z_i, u)}{\\partial z_i} \\frac{\\partial z_i}{\\partial \\hat{z}_j} + \\frac{\\partial \\log |J_h|}{\\partial \\hat{z}_j} = 0$.\nThe equation allows us to construct the component-wise relation between true latent variable $z$ with\nestimated invariant variables $\\hat{z}$ as expressed using $\\tilde{z}$. However, the Jacobian term $\\frac{\\partial \\log |J_h|}{\\partial \\hat{z}_j}$ is\nintractable as we have no knowledge about $h$(once we have, everything is solved). If we have multiple\ndomains $u = u_0,..., u_{n_s}$, we have $n_s + 1$ equations like equation above. We can remove the\nintractable Jacobian by taking the difference for every equation $u = u_1,..., u_{n_s}$ with the equation\nwhere $u = u_0$:\n$\\sum_{i=1}^{n} \\frac{\\partial q_i(z_i, u_k)}{\\partial z_i} - \\frac{\\partial q_i(z_i, u_0)}{\\partial z_i} \\frac{\\partial z_i}{\\partial \\hat{z}_j} = 0$.\nAs long as the $j \\in \\{n_s+1, ..., n\\}$, the distribution of estimated variable $\\hat{z}_j$ doesn't change across all\ndomains. The right-hand side of the equation above will be zero. Thus,\n$\\sum_{i=1}^{n} \\frac{\\partial q_i(z_i, u_k)}{\\partial z_i} - \\frac{\\partial q_i(z_i, u_0)}{\\partial z_i} \\frac{\\partial z_i}{\\partial \\hat{z}_j} = 0$"}, {"title": "A1.2 Component-wise identifiability for 2ns + 1 domains", "content": "Differentiating both sides of Equation 16 with respect to $\\hat{z}_j$, $j \\in \\{1, ..., n\\}$, we can get\n$\\frac{\\partial \\hat{q}_j(\\hat{z}_j, u)}{\\partial \\hat{z}_j} = \\sum_{i=1}^{n} \\frac{\\partial q_i(z_i, u)}{\\partial z_i} \\frac{\\partial z_i}{\\partial \\hat{z}_j} + \\frac{\\partial \\log |J_h|}{\\partial \\hat{z}_j}$.\nFurther differentiate with respect to $\\hat{z}_q$, $q \\in \\{1, . . ., n\\}$, $q \\neq j$, according to the chain rule,\n$0 = \\sum_{i=1}^{n} \\frac{\\partial^2 q_i(z_i, u)}{\\partial z_i \\partial z_j} \\frac{\\partial z_i}{\\partial \\hat{z}_j} \\frac{\\partial z_i}{\\partial \\hat{z}_q} + \\frac{\\partial q_i(z_i, u)}{\\partial z_i} \\frac{\\partial^2 z_i}{\\partial \\hat{z}_j \\partial \\hat{z}_q} + \\frac{\\partial^2 \\log |J_h|}{\\partial \\hat{z}_j \\partial \\hat{z}_q}$.\nThis equation allows us to have the component-wise relation between $\\hat{z}$ with $z$. Following the\nsame ideas, and introducing multiple domains come into play to remove the Jacobian term. Using\nassumption 4 in Lemmal, for $u = u_0,..., u_{2n_s}$, we have $2n_s + 1$ equations like Equation 30.\nTherefore, we can remove the effect of the Jacobian term by taking the difference for every equation\n$u = u_1,..., u_{2n_s}$ with the equation where u = u_0:\n$\\sum_{i=1}^{n} (\\frac{\\partial^2 q_i(z_i, u_k)}{\\partial \\hat{z}_j \\partial \\hat{z}_q} - \\frac{\\partial^2 q_i(z_i, u_0)}{\\partial \\hat{z}_j \\partial \\hat{z}_q})\\frac{\\partial z_i}{\\partial \\hat{z}_j} = 0$.\nFor invariant variables $z_e$, their log density doesn't change across different domains. Thus, we can\nget rid of invariant parts of the equation above and have\n$\\sum_{i=1}^{n_s} (\\frac{\\partial^2 q_i(z_i, u_k)}{\\partial \\hat{z}_j \\partial \\hat{z}_q} - \\frac{\\partial^2 q_i(z_i, u_0)}{\\partial \\hat{z}_j \\partial \\hat{z}_q})\\frac{\\partial z_i}{\\partial \\hat{z}_j} = 0$.\nThus, if the above matrix is invertible according to assumption 4 in Theorem 1, we will leave its null\nspace all zero. i.e.,$\\frac{\\partial z_i}{\\partial \\hat{z}_j} = 0$ and $\\frac{\\partial \\hat{z}_j \\partial \\hat{z}_q}{\\partial^2 z_i} = 0$ for all $i \\in \\{1, ..., n_s\\}, j, q \\in \\{1, . . ., n\\}, j \\neq q$. We"}, {"title": "A1.3 Discussion of component-wise identifiability of repeated distribution for partial changing variables", "content": "In this section, we start with an example to discuss the possible scenarios where there are repeated\ndistributions for partially changing variables among different domains. Based on this example, we\nproceed to provide an intuitive proof of Remark 1.\nLet's follow the proof of component-wise identifiability of changing variables. We directly look into\nthe equation\n$0 = \\sum_{i=1}^{n} \\frac{\\partial^2 q_i(z_i, u)}{\\partial \\hat{z}_j \\partial \\hat{z}_q} \\frac{\\partial z_i}{\\partial \\hat{z}_j} + \\frac{\\partial q_i(z_i, u)}{\\partial z_i} + \\frac{\\partial^2 \\log |J_h|}{\\partial \\hat{z}_j \\partial \\hat{z}_q}$.\nOur goal is to produce the matrix containing $\\frac{\\partial^2 q_i(z_i, u)}{\\partial \\hat{z}_j \\partial \\hat{z}_q}$ and $\\frac{\\partial q_i(z_i, u)}{\\partial z_i}$ whose null space only contains\nzero vector. However, we can't ensure every arrived domain will bring enough change. In this case,\ndistributions of the same variable on different domains may be the same. i.e., $q_i (z_i, U_l) = q_i(z_i, U_k)$\nwhere $l \\neq k$. Our discussion will mainly revolve around this situation.\nLet's start with the simplest case where there are only two changing variables $\\hat{z}_1$ and $\\hat{z}_2$ and no\ninvariant variables. We know from Theorem1 that we need 2n + 1 domains to reveal their component-\nwise identifiability. Therefore, for $u = u_0, ..., u_4$, we have the following linear system:"}, {"title": "A2 Discussion of those properties", "content": "A2.1 Possible impairment for partial\nchanging variables when new domains are involved\nBefore we dive into the details, we need to first clarify one basic concept: Incremental domains can't\naffect the overall identifiability of all changing variables theoretically. As long as both Theorem 1\nand Lemma 1 state that the overall identifiability is determined by the distribution of true latent\nchanging variables, the way of learning can't influence it theoretically. However, the identifiability\nof partial changing variables will be affected as shown in Section 3.2.2 and Experiment 4.2. We\ndemonstrate the influence of the new domain on the identifiability of partial variables through a\ncarefully designed example, as inferred below."}, {"title": "A3 Pseudo Code", "content": "Algorithm A1 Continual Nonlinear ICA\nRequire: Training data sequentially arriving {x|u1,...,x|uT}\nKaiming_init(0), M\u2081 \u2190 {} for all t = 1, . . ., T\nfor u = u1,..., u do:\nfor {x1,..., Xd}|u do\nMt Mt U random select x\nCalculate loss L(0) as (5)\nv \u2190 VoL(0,x)\nVk \u2190 VoL(0, Mk) for all k < t\nv' \u2190 Solve quadratic programming as (7)\n0 \u2013 0 \u2013 \u03b1\u03bd'\nReturn 0"}, {"title": "A4 Visualization", "content": "To provide a more intuitive demonstration of the identifiability of\nchanging variables and compare our method with joint training, we\nconducted an experiment in the following setting: with ns = 2, n = 4, and z values generated from a\nGaussian distribution across 15 domains. We stored models trained on subsets of the training data\ncontaining 3, 5, 7, and 9 domains, a part of the whole 15 domains respectively. The test set consisted\nof all 15 domains, and we used these models to sample corresponding 2 values. These generated 2\nvalues were then compared to the ground truth values of z for evaluation.\nSpecifically, we provide the scatter plot of true latent variables z with the estimated variables 2 in\nFigure A3,A4,A5,A6 for both our methods and joint training. Figure A3,A4,A5, A6 corresponds to a\ndifferent training set that includes 3, 5, 7, and 9 domains respectively. For each figure, 2s,i represents\nthe ith estimated changing variable, 2c,i represents ith estimated invariant variable, zs,i represents\nthe ith true changing variable and zc,i represents the ith true invariant variable.\nBased on the experiment results, we observe a stronger linear correlation that appears for estimated\nchanging variables with real changing ones as more domains are included in the training process\nfor both our method and joint training. That is, more domains will imply stronger identifiability,\naligned with our expectations. Beyond that, our approach shows slightly inferior or even comparable\nperformance compared to joint training, demonstrating its effectiveness."}, {"title": "A5 Broader Impacts", "content": "The primary goal of our proposed method is to draw the attention to the problem of enabling the\nmodel learn identifiable representations in a sequential manner. This task is essential and has broad\napplications. We are confident that our method will be beneficial and will not result in negative\nsocietal impacts."}, {"title": "A6 Experiment Details", "content": "A6.1 Data\nWe follow the data generation defined in Equation 1. Specifically, we discuss Gaussian cases where\n$z_c \\sim \\mathcal{N}(0,I), z_s \\sim \\mathcal{N}(\\mu_u, \\sigma^2 I)$ for both $n_s = 2, n = 4$ and $n_s = 4, n = 8$. For each domain u, the\n$\\mu_u \\sim Uniform(-4, 4)$ and $\\sigma^2 \\sim Uniform(0.01, 1)$.\nWe also discuss the mixed Gaussian case or both ns = 2, n = 4 and ns = 4, n = 8 where zc is the\nnormalization of mixing of two Gaussian variables $\\mathcal{N}(0, I)$ and $\\mathcal{N}(0.25, I)$ with domain-specific\nmodulation and translation. For ns = 2, n = 4, each domain contains 10000 samples for training and\n1000 samples for testing. For ns = 4, n = 8, each domain contains 5000 samples for training and\n1000 samples for testing. Specifically, we first mix those two Gaussian and do the normalization. After"}, {"title": "A6.2 Mean correlation coefficient", "content": "Mean correlation coefficient(MCC) is a standard metric for evaluating the recovery of latent factors\nin ICA literature. It averages the absolute value of the correlation coefficient between true changing\nvariables with the estimated ones. As stated"}]}