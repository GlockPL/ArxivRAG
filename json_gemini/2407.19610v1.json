{"title": "MIXTURE OF MODULAR EXPERTS: DISTILLING KNOWLEDGE\nFROM A MULTILINGUAL TEACHER INTO SPECIALIZED\nMODULAR LANGUAGE MODELS", "authors": ["Mohammed Al-Maamari", "Mehdi Ben Amor", "Michael Granitzer"], "abstract": "This research explores the integration of Knowledge Distillation (KD) and Mixture of Experts (MoE)\nto create modular, efficient, and specialized multilingual language models. The primary objectives\ninclude evaluating adaptive versus fixed alpha methods in KD, developing and comparing modular\nMoE architectures in handling multi-domain inputs and preventing catastrophic forgetting.\n\nWe address the computational challenges of large language models (LLMs) and the need for modular\nmodels. KD compresses LLMs into smaller models for efficiency, while MoE architectures enhance\nmodularity by combining multiple experts for specialized tasks. Experiments showed that adaptive\nand fixed alpha methods in KD yielded similar performance, with marginal improvements from\nadaptive alpha. The combined loss approach slightly outperformed alternating losses, providing more\nstable learning. The router, trained to classify input sequences into English, French, German, or\nPython, achieved high accuracy precision, recall, and F1 score of 99.95%, with Logistic Regression\nas the most effective classifier.\n\nEvaluation of modular MoE architectures revealed that the Pre-trained Language Experts (PLE)\nsetup and Joint Expert Embedding Training (JEET) demonstrated similar performance, while the\nMoE with Common Expert (MoE-CE) setup showed slightly lower performance. However, when\nincluding a common expert in MoE-CE, its performance approaches that of both PLE and JEET.\nThe study on catastrophic forgetting indicated that sequential training led to significant forgetting,\nwhile single-session training with balanced batches approach, and MoE approach mitigated this issue\neffectively. The MoE architecture preserved knowledge across multiple languages, demonstrating its", "sections": [{"title": "1 Introduction", "content": "Language models (LMs) are pivotal in Natural Language Processing (NLP), facilitating a variety of tasks such as\nmachine translation [1], sentiment analysis [2], and text generation [3]. Despite their potential, large-scale models\nencounter challenges like computational inefficiency, limited adaptability, and catastrophic forgetting. Our study\nexplores the amalgamation of Knowledge Distillation (KD) and Mixture of Experts (MoE) to mitigate these challenges,\naiming to improve efficiency, modularity, and specialization in language models.\n\nTransformers, the backbone of many large models, require substantial computational resources [4], which hampers\ntheir scalability and accessibility. The increasing complexity and size associated with supporting more languages and\ndomains adversely affect training durations and generalization abilities [5]. Additionally, fine-tuning for specific tasks\nconsumes significant resources and often falls short of achieving optimal outcomes [6]. Catastrophic forgetting is a\nmajor hurdle, particularly in models handling multiple languages and domains, as they tend to lose previously acquired\nknowledge when exposed to new data [7].\n\nSpecialized models, when trained on narrow domains such as programming languages, have demonstrated superiority in\nspecific tasks like code completion and bug detection over their general-purpose counterparts [8]. Introducing modularity\ninto neural network design enhances flexibility, scalability, and maintainability, enabling updates to individual network\nsegments without necessitating a complete retraining.\n\nThis research primarily focuses on exploring various integration strategies of KD and MoE to create specialized,\nefficient, and modular language models. While we employed straightforward knowledge distillation techniques,\nreaching state-of-the-art knowledge distillation was not our objective. Instead, our primary goal was to investigate the\nfeasibility of different integration methods of KD and MoE. KD is the process where smaller student models learn to\nmimic the behavior of a larger, more capable teacher model using their probabilistic outputs [9]. MoE architectures,\non the other hand, dynamically delegate tasks to specialized models, thereby enhancing performance across varied\ndomains and languages [10].\n\nOur research objectives include evaluating adaptive versus fixed alpha methods in KD, training a router to efficiently\ndirect inputs to the appropriate experts, and comparing various MoE architectures to determine their effectiveness in\nhandling multi-domain inputs and in averting catastrophic forgetting. This study contributes to the development of\nmore adept and effective NLP systems that can support a broad spectrum of applications."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Knowledge Distillation", "content": "\"DistilBERT\" by Sanh et al. [11] introduces a method to distill BERT into a smaller, faster model while retaining\nmost of its performance. By leveraging a triple loss function (language modeling, distillation, and cosine-distance\nlosses), DistilBERT reduces model size by 40% and maintains 97% of BERT's language understanding capabilities.\nThis approach makes the model suitable for deployment in environments with constrained computational resources.\nDistilBERT's training involves using every second layer from the teacher model to retain inductive biases and employs\ntechniques like gradient accumulation and dynamic masking."}, {"title": "2.2 Mixture of Experts", "content": "\"Mixtral of Experts\" by Jiang et al. [13] presents Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.\nThe model dynamically selects two out of eight feedforward blocks per token at each layer, optimizing computational\nresource usage. Mixtral's transformer model incorporates MoE layers with a routing mechanism to allocate tokens\nto experts. Evaluations show superior performance in various benchmarks, highlighting the model's efficiency and\neffectiveness.\n\n\"Branch-Train-MiX\" by Sukhbaatar et al. [14] investigates methods for training LLMs across multiple specialized\ndomains. The Branch-Train-MiX (BTX) method involves branching from a seed model, training domain-specific\nexperts, and integrating them into a unified MoE model. This approach improves training efficiency and model\nperformance by leveraging parallelism and specialization. BTX outperforms baselines like Llama-2 in accuracy and\ncomputational efficiency.\n\n\"Branch-Train-Merge\" by Li et al. [15] introduces the Branch-Train-Merge (BTM) algorithm, which enhances the\nefficiency of training large language models. BTM facilitates independent training of subparts of the model on different\ndata subsets, reducing communication overhead. The approach involves three steps: branching, training, and merging.\nBTM achieves improved perplexities and higher updates per second due to reduced communication overhead, making it\na scalable and efficient training paradigm.\n\nOur research integrates MoE with Knowledge Distillation (KD) to develop specialized multilingual models. Unlike\nMixtral and BTX, which focus on token-level routing and parallel training of domain-specific experts, our work\nemphasizes sequence-level routing and the integration of KD with MoE. This approach aims to address multi-domain\nadaptability and reduce catastrophic forgetting, contributing to the development of modular and efficient language\nmodels."}, {"title": "3 Methods", "content": "This section describes the methodologies, tools, and algorithms used in this research, focusing on dataset preparation,\nmodel training, knowledge distillation techniques, and MoE architecture design."}, {"title": "3.1 Dataset Preparation", "content": "The dataset comprises multilingual text data in English, German, French, and Python code. The primary sources are the\nWiki40B dataset [16] for natural languages and the CodeParrot GitHub \"codeparrot/github-code-clean\" dataset [17]\nfor Python code. The Wiki40B dataset includes 2,926,536 English, 1,227,206 French, and 1,554,908 German training\nsamples, ensuring balanced token counts across these languages. Python data contains 645K Python code files, filtered\nand balanced based on code length to ensure comprehensive coverage."}, {"title": "3.2 Tokenization", "content": "Byte Pair Encoding (BPE) [18] was used for tokenization, with a vocabulary size of 32,000 tokens. The tokenizer was\ntrained on a balanced dataset of the four languages, using special tokens such as <unk>, <s>, and </s>. This setup\nensured efficient vocabulary usage and compatibility with the models."}, {"title": "3.3 Teacher Model Training", "content": "The teacher model, a GPT-2 Medium with 340 million parameters, was trained on the multilingual dataset. Initial\nattempts with Mistral 1.6B and Phi 1.34B configurations faced stability issues, leading to the selection of GPT-2\nMedium for its balance of performance and computational efficiency. The training process involved a context length\nof 1024 tokens, a virtual batch size of 512, and optimization techniques such as gradient accumulation and clipping.\nTraining was conducted on 2 A100 GPUs using DeepSpeed and Accelerate libraries."}, {"title": "3.4 Knowledge Distillation", "content": "Knowledge distillation (KD) involved transferring knowledge from the GPT-2 Medium teacher model to smaller\nstudent models. The student models were trained to replicate the teacher's outputs using a combined loss function\nincorporating Cross-Entropy Loss and Reverse Kullback-Leibler (RKL) Divergence Loss [12]. The adaptive alpha\nmethod dynamically adjusted the weights of these losses based on training progress."}, {"title": "3.5 Mixture of Experts Architecture", "content": "The Mixture of Experts (MoE) architecture includes multiple specialized sub-models (experts) and a router mechanism.\nThree setups were evaluated:\n\nSetup 1: Pre-trained Language Experts (PLE) - Experts pre-trained independently via KD from the teacher model,\nhandling specific languages.\n\nSetup 2: Joint Expert Embedding Training (JEET) - Experts trained concurrently with a shared embedding layer,\nmaintaining modularity.\n\nSetup 3: MoE with Common Expert (MoE-CE) - Includes a common expert trained on all languages, sharing the\nembedding layer with specialized experts."}, {"title": "3.6 Router", "content": "The router, pre-trained to classify inputs into English, French, German, or Python, achieved high accuracy (99.95%)\nusing TF-IDF vectorization and Logistic Regression. It dynamically selects the appropriate expert during inference,\noptimizing the allocation of computational resources."}, {"title": "3.7 Training and Inference", "content": "During training, data was batched by language, and the router directed each batch to the corresponding expert. This\napproach ensured specialization without interference. During inference, the system handled mixed-language batches,\nmaintaining efficiency and specialization."}, {"title": "4 Results", "content": "The experiments focused on evaluating different aspects of the Mixture of Experts (MoE) architecture, including\nKnowledge Distillation (KD) methods, router training, and the impact of modular MoE architectures on system\nperformance."}, {"title": "4.1 Experimental Setup", "content": "The experimental setup involved two distinct KD approaches: independent KD and sequential KD. Independent KD\ntrained separate student models for each language (English, French, German, and Python) using the outputs of the\nteacher model. Sequential KD distilled a single student model sequentially with knowledge from each language. The\nevaluation metrics used were perplexity and cross-entropy loss. The training hyperparameters, including a context\nlength of 1024 tokens and a vocabulary size of 32,000, were consistent with those used for the teacher model. The\nMoE training utilized the same tokenizer and frameworks such as DeepSpeed and Accelerate to manage computational\ndemands."}, {"title": "4.2 Adaptive vs. Fixed Alpha", "content": "Research Question 1 (RQ1): What is the effectiveness of adaptive vs. fixed alpha methods in Knowledge Distillation?\n\nThe adaptive alpha method, which dynamically adjusts weights during training, was hypothesized to perform better\nthan the fixed alpha method. The results, shown in Figure 7, indicated that the adaptive alpha method outperformed\nthe fixed alpha method by a small margin (0.01 improvement in evaluation loss). Testing different fixed alpha values\nshowed that an alpha of 0.5 performed nearly as well as the adaptive method, suggesting minimal benefits of dynamic\nadjustment in this setup."}, {"title": "4.3 Alternating Losses (AL) VS Combined Losses (CL)", "content": "Research Question 2 (RQ2): What is the impact of alternating losses during training on model convergence and\nperformance?\n\nThe hypothesis was that alternating KD_loss and LM_loss would enhance model performance. However, the results,\nshown in Figure 8, indicated that the combined loss method performed slightly better, with an evaluation loss marginally\nlower than the alternating losses method (4.305 vs. 4.322). This suggests that consistently applying both losses at each\ntraining step provides a more effective learning signal."}, {"title": "4.4 The Router", "content": "Research Question 3 (RQ3): Can we train a router that accurately routes input sequences to one of the four experts,\nclassifying the input sequence into one of the four classes [En, Fr, De, Py]?\n\nVarious classifiers were evaluated, with the Logistic Regression classifier achieving the highest performance (99.95%\naccuracy). The confusion matrix in Figure 9 and Table 3 confirmed the router's effectiveness in accurately classifying\ninput sequences."}, {"title": "4.5 Modular MoE Model Design", "content": "Research Question 4 (RQ4): How do the three MoE architectures compare in terms of performance and robustness?\n\nThree MoE setups were compared: Pre-trained Language Experts (PLE), Joint Expert Embedding Training (JEET),\nand MoE with Common Expert (MoE-CE). The results, summarized in Table 4, showed that PLE achieved the best\nperplexities for English and German, while JEET performed best for French and Python. MoE-CE, when using the\ncommon expert, showed performance approaching that of PLE and JEET, highlighting the benefits of including a\ncommon expert."}, {"title": "4.6 Impact of Adding a Common Expert to the MoE System", "content": "Research Question 5 (RQ5): Does adding a common expert improve the overall performance of the MoE system and\nthe performance of each expert independently?"}, {"title": "4.7 Catastrophic Forgetting in Modular MoE Architecture VS Non-Modular Approaches", "content": "Research Question 6 (RQ6): How does the modular MoE architecture compare to non-modular approaches in terms\nof catastrophic forgetting?\n\nThree experiments were conducted to compare the impact of catastrophic forgetting: 1. Sequentially distilling knowledge\ninto a single student model. 2. Distilling knowledge into a single student model in one session. 3. Employing the MoE\narchitecture with four separate student models.\n\nThe results, shown in Table 6, indicated significant catastrophic forgetting in the sequential setup (up to 38% in German),\nwhereas both single session distillation and MoE showed no catastrophic forgetting. The evaluation loss comparison for\nsequential training and MoE architecture is shown in Figure 10.\n\nThe results confirmed that the modular MoE architecture effectively mitigates catastrophic forgetting, supporting the\nhypothesis that modularity allows for the retention of knowledge across multiple languages."}, {"title": "5 Discussion", "content": "This section interprets and contextualizes the findings from our experiments, providing insights into the efficacy of\ndifferent Knowledge Distillation (KD) methods and Mixture of Experts (MoE) architectures. It addresses the research\nquestions, compares our results with existing literature, and reflects on the challenges and limitations encountered\nduring the research."}, {"title": "5.1 Interpretation of Findings", "content": ""}, {"title": "5.1.1 Adaptive vs. Fixed Alpha", "content": "The comparison between adaptive alpha and fixed alpha methods, discussed in subsection 4.2, revealed that both\napproaches yielded similar performance levels. The adaptive alpha approach showed a marginally better performance,\nbut this advantage was minimal due to the consistency of the dataset used for training both the teacher and student\nmodels. The similarity in KD loss and cross-entropy loss changes likely contributed to the comparable performance of\nboth methods."}, {"title": "5.1.2 Alternating Losses vs. Combined Losses", "content": "The experiment comparing alternating losses (AL) and combined losses (CL), as described in subsection 4.3, indicated\nthat the combined loss approach slightly outperformed the alternating loss approach. The minimal difference suggests\nthat the simplicity of the combination method-weighted averaging of the losses\u2014may have masked potential benefits\nof alternating losses. More sophisticated methods of loss alternation may yield more pronounced differences and\nwarrant further exploration."}, {"title": "5.1.3 Router Performance", "content": "The router's performance, detailed in subsection 4.4, achieved high accuracy, precision, recall, and F1 scores. The\ndistinctiveness of the four classes (English, German, French, and Python) and the balanced dataset used for training\ncontributed to this success. The results confirm the router's capability to accurately classify input sequences and support\nthe MoE architecture's performance."}, {"title": "5.1.4 Modular MoE Model Design", "content": "The performance comparison of the three MoE architectures, discussed in subsection 4.5, highlighted the strengths and\nlimitations of each setup. Pre-trained Language Experts (PLE) and Joint Expert Embedding Training (JEET) performed\ncomparably, with PLE excelling in English and German, and JEET in French and Python. MoE with Common Expert\n(MoE-CE) improved significantly with the inclusion of a common expert during inference, suggesting that a shared\nknowledge base can enhance performance in multi-language tasks."}, {"title": "5.1.5 Catastrophic Forgetting in Modular vs. Non-Modular Approaches", "content": "The study on catastrophic forgetting, as outlined in subsection 4.7, demonstrated that sequential training led to significant\nforgetting of previously learned languages, while single-session training and MoE approaches effectively mitigated"}, {"title": "6 Conclusion", "content": ""}, {"title": "6.1 Summary of Contributions", "content": "This research integrates Knowledge Distillation (KD) and Mixture of Experts (MoE) to develop modular, efficient, and\nspecialized multilingual language models. The primary objectives were to evaluate adaptive versus fixed alpha methods\nin KD, compare modular MoE architectures, and address catastrophic forgetting."}, {"title": "6.1.1 Knowledge Distillation", "content": "The experiments comparing adaptive and fixed alpha methods in KD revealed similar performance, with the adaptive\nalpha method providing a slight improvement. The combined loss approach offered more stable learning dynamics\ncompared to alternating losses."}, {"title": "6.1.2 Mixture of Experts", "content": "Three MoE architectures were assessed: Pre-trained Language Experts (PLE), Joint Expert Embedding Training (JEET),\nand MoE with Common Expert (MoE-CE). PLE and JEET performed similarly, while MoE-CE, without utilizing\nthe common expert, lagged behind but demonstrated enhanced results with the inclusion of a common expert. This\nindicates the effectiveness of shared knowledge in improving performance across multiple languages."}, {"title": "6.1.3 Router Performance", "content": "The router, employing Logistic Regression for classification, achieved high accuracy and reliability in selecting the\nappropriate expert model for inputs, with accuracy, recall, precision, and F1-score all at 99.95%."}, {"title": "6.1.4 Catastrophic Forgetting", "content": "Sequential training resulted in significant catastrophic forgetting, whereas single-session training and the MoE approach\neffectively mitigated this issue. The modular MoE architecture preserved knowledge across multiple languages,\npreventing catastrophic forgetting."}, {"title": "6.2 Implications and Impact", "content": "The integration of KD with MoE facilitates the development of modular, specialized, and efficient models that perform\nwell across diverse tasks. The modularity of the MoE architecture enhances flexibility, allowing for the addition of\nnew experts without retraining the entire system and addressing catastrophic forgetting by enabling the model to retain\nknowledge across multiple languages and domains."}, {"title": "6.3 Challenges and Limitations", "content": "The research was constrained by computational resources and a relatively small dataset of 490 million tokens, limiting\nthe generalizability of the findings. Additionally, the focus on a limited number of languages and a single programming\nlanguage indicates that further experimentation is needed to extend the approach to other languages, domains, and\nmodalities."}, {"title": "6.4 Future Work", "content": "Future research should aim to scale the approach to larger datasets and more diverse languages and domains. Optimizing\nthe training and integration process for the MoE architecture and exploring the applicability of the methods to other\ncontexts are recommended. Further investigation into adaptive alpha methods and advanced loss functions for the\ncommon expert could provide deeper insights and enhance model performance."}], "equation_list": [{"index": 0, "text": "Ltotal \u03b1 \u00b7 LLM + \u03b2 \u00b7 LKD"}]}