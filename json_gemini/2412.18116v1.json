{"title": "AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation", "authors": ["Hao Wen", "Shizuo Tian", "Borislav Pavlov", "Wenjie Du", "Yixuan Li", "Ge Chang", "Shanhui Zhao", "Jiacheng Liu", "Yunxin Liu", "Ya-Qin Zhang", "Yuanchun Li"], "abstract": "Large language models (LLMs) have brought exciting new advances to mobile UI agents, a long-standing research field that aims to complete arbitrary natural language tasks through mobile UI interactions. However, existing UI agents usually demand high reasoning capabilities of powerful large models that are difficult to be deployed locally on end-users' devices, which raises huge concerns about user privacy and centralized serving cost. One way to reduce the required model size is to customize a smaller domain-specific model with high-quality training data, e.g. large-scale human demonstrations of diverse types of apps and tasks, while such datasets are extremely difficult to obtain. Inspired by the remarkable coding abilities of recent small language models (SLMs), we propose to convert the UI task automation problem to a code generation problem, which can be effectively solved by an on-device SLM and efficiently executed with an on-device code interpreter. Unlike normal coding tasks that can be extensively pretrained with public datasets, generating UI automation code is challenging due to the diversity, complexity, and variability of target apps. Therefore, we adopt a document-centered approach that automatically builds fine-grained API documentation for each app and generates diverse task samples based on this documentation. By guiding the agent with the synthetic documents and task samples, it learns to generate precise and efficient scripts to complete unseen tasks. Based on detailed comparisons with state-of-the-art mobile UI agents, our approach effectively improves the mobile task automation with significantly higher success rates and lower latency/token consumption. Code will be open-sourced.", "sections": [{"title": "1 INTRODUCTION", "content": "Automatically controlling mobile devices through natural language has long been a goal for mobile developers and researchers [1, 5, 23]. Agents powered by Large Language Models (LLMs) and Vision Language Models (VLMs, both LLM and VLM are refered as LLM for simplicity in this paper) have emerged as promising solutions for automating flexible and complex user tasks [19, 24, 34, 36, 37, 40-42]."}, {"title": "2 RELATED WORK AND MOTIVATION", "content": ""}, {"title": "2.1\nGUI-based Mobile Task Automation", "content": "The goal of mobile task automation is to automatically complete user tasks by interacting with mobile devices. Compared to previous template-based methods (e.g. Siri, Google Assistant, Cortana, etc.), where predefined templates are used to process and respond to user inputs, GUI-based mobile task automation can achieve more complex and flexible tasks because it doesn't depend on third-party APIs or non-trivial programming efforts from developers.\nThe input of a GUI-based task automation agent is an arbitrary task described in natural language related to a mobile app. The output is a sequence of GUI actions that can be executed on a smartphone. A task is a multi-step functionality request from the user intended for completion on a smartphone, often lacking explicit instructions. A GUI State represents the current status or condition of the user interface in a mobile app, often including an arrangement of controls depicted through images and text, typically organized as a GUI tree. A GUI element is a control in the GUI state that human users can interact with, such as a button, text box, input field, or slider. A GUI action, performable by the user or an agent on the device's screen, is defined by a tuple (GUI element, action type, value). Action type represents how the target GUI element is manipulated. We consider four main types of smartphone interactions, including \u201cclick\u201d, \u201clong click\", \"input\", and \"swipe\u201d.\nBefore the emergence of LLMs, researchers developed supervised learning techniques [6, 21, 30] and reinforcement learning methods [14, 22, 31] for GUI-based mobile task automation, which struggle with the flexibility and adaptability needed for dynamic, real-world applications. The advent of LLMs brings a transformative shift in GUI-based mobile\""}, {"title": "2.2 Current Practice and Limitations", "content": "Researchers have explored various techniques to customize domain-specific SLMs for GUI-based mobile task automation. One area of focus involves enhancing the grounding capability of SLMs, which refers to locating GUI elements based on user instructions within a single GUI screen [7, 11, 13]. These methods gather extensive GUI-domain training data and use it to fine-tune a vision language model, thereby improving their ability to follow GUI domain-specific instructions. Other studies aim to acquire domain-specific knowledge about particular apps through exploration and use it to enhance SLMs' task-solving capabilities [2, 36].\nAs shown in Table 1, on-device GUI agents still exhibit lower accuracy and longer latencies compared to agents based on more powerful LLMs in the cloud. The high latency arises because model inference is computationally intensive, leading to delays on mobile devices with less computational resources, especially for prompts with long contexts. GUI agents must query the model at each step to process the GUI state and generate corresponding actions. Each GUI state typically consumes a substantial number of tokens (over 500 on average), causing significant processing delays-even for on-device SLMs. A typical task requires between 5 and 20 steps, further increasing latency.\nThe root cause of the performance degradation is the weak reasoning ability of SLMs. We conducted an in-depth analysis and identified three detailed reasons: (1) Limited Domain-specific Knowledge of Apps: On-device SLM-based agents often struggle to accurately interpret the specific functionality of UI components upon first encounter. For example, given the user task: Modify a contact's phone number, agents"}, {"title": "2.3 Script-based GUI Agents: Opportunities and Challenges", "content": "The limitations of current methods highlight the need for a more accurate and efficient approach to automating mobile UI tasks beyond traditional step-wise UI agents. Recent studies have shown that SLMs can perform well on challenging coding tasks [29, 47], and synthesizing executable code are more advantageous than using Text/JSON actions in agents [35]. This motivates the development of a script-based on-device GUI agent to address the identified limitations, offering the following benefits:\n(1) Reduction of Query Overhead: Instead of querying the model for each individual step, script-based GUI agents can generate a multi-step action plan based on a single piece of code. This approach significantly reduces the overhead associated with multiple queries. (2) Enhanced Domain-specific Knowledge Utilization: By building an API document specifically for code generation, GUI agents can better leverage app-domain knowledge in a manner more familiar to language models. (3) Improved Action Tracking and Termination Control: Executing GUI actions based on code lines shifts the responsibility of action tracking from the model to a code status tracker. This approach is more controllable and avoids the issues of misalignment and hallucination, leading to more reliable task completion.\nAlthough there is great potential for script-based on-device GUI agents, it still faces significant challenges.\nDynamic nature of mobile apps. For mobile apps, many UI states and elements are dynamic, frequently changing in text, location, or size (e.g., titles, dates, names). This variability increases the difficulty of both extracting app-domain knowledge and executing auto-generated scripts. To generate multi-step scripts that can be directly executed within app UIs, the system needs to provide all UI information within the entire app for summarizing knowledge. These dynamic UI elements need to be identified and abstracted to give an overview of the app. When executing the script, the system needs to locate the dynamic UI elements and perform actions according to the code, even when the elements change.\nExtensive UI element space. A typical UI may contain over 100 elements with different XML descriptions, expanding the element space in the exploration history to tens of thousands. Describing all these elements for generating code is both redundant and potentially misleading, complicating the task of identifying the necessary UI elements for a given task. Additionally, UI elements often have relationships, such as inclusion (where some elements are nested within others, e.g. a song's title and artist within a single song item in Figure 3) or dependency (where one UI element is editable only after another has been clicked). It is still difficult for LLMs or SLMs to understand complex element relationships only based on a static UI document.\nLimited Context Length and Computational Resources\nDue to limited computational power, reduced memory capacity, and energy constraints, on-device SLMs often have shorter context lengths (e.g., 4096 for Llama 3.1 8B [9]). As apps grow more complex, including detailed app usage patterns in the prompt can easily exceed the context length. Without this usage pattern, script generation loses valuable guidance and insight into app usage. Moreover, for a given user task, only a small portion of GUI elements may be relevant, yet they can be buried within a long app usage context. In such cases, the on-device SLMs may struggle to identify and utilize the relevant information [17]."}, {"title": "3 OUR APPROACH: AUTODROID-V2", "content": "We introduce AutoDroid-V2 to address the aforementioned challenges of on-device GUI agents. The core idea behind AutoDroid-V2 is automatically constructing an informative app document and guiding the LLMs/SLMs to generate domain-specific device-control scripts.\nAs shown in Figure 2, AutoDroid-V2 operates in two stages. Offline Stage: AutoDroid-V2 first constructs an app document by analyzing the app exploration history (details in \u00a73.1). The document serves as the fundamental guidelines for flexible script generation, which is desired to be concise (easy to process by LLMs) and precise (helpful for generating complicated executable code). We introduce various techniques (AI-guided GUI state compression, element XPath auto-generation, GUI dependency analysis, etc.) to achieve these goals. The built document is then used to synthesize large-scale user tasks for fine-tuning the local LLM (details in \u00a73.2). Given the importance of training data in LLM customization, we adopt sandbox validation and tree-based search techniques to improve the data quality. Online Stage: Given a user's task request, AutoDroid-V2 calls the customized local LLM to generate a multi-step script and executes the script with our domain-specific interpreter (details in \u00a73.3). Running the script reliably (i.e. able to handle runtime dynamics) and efficiently are two main objectives of the interpreter, which are addressed with our dependency-enhanced execution and prompt compression/caching techniques."}, {"title": "3.1 Automated App Document Generation", "content": "The goal of automated app document generation is to capture and analyze essential GUI elements in an app, guiding the backbone language model to be aware of possible interaction use cases even without real-time observation. The input consists of random exploration traces, modeled as sequences of <GUI State, GUI Action> pairs. These sequences are often aimless and redundant, containing many repeated actions and GUI elements. The output is a concise app document containing typical GUI states, important elements, and GUI transition relationships, as shown in Figure 3. Two primary challenges arise in this process: reducing the GUI element space and modeling transition relationships between GUI elements."}, {"title": "3.1.1 Functionality-aware GUI States Grouping", "content": "To ensure a structured and meaningful representation of the app's interface and interaction logic, AutoDroid-V2 first organizes the GUI states by iterating over the exploration history. Specifically, AutoDroid-V2 merges similar GUIs into an abstract GUI state, where similarity is determined by both layout similarity and functionality similarity.\nLayout similarity refers to how the visual components (e.g., buttons, text fields, etc.) are arranged within the GUI. Let $T_1, T_2, ...T_n$ represent a set of GUI trees observed in the exploration traces. The layout of each tree, $T_i$, is extracted by removing detailed content and repeated sibling elements. If a group $T_1, T_2, ...T_n$ shares the same layout $l_i$, they are considered layout-similar.\nHowever, layout similarity alone is insufficient. Many different GUI states in an app may share the same layout due to common development patterns. For instance, a date selection screen and a background theme selection screen in a calendar app might both use a similar layout (e.g., a popup with a series of checkboxes). Despite having the same layout, these two screens serve different functions. To address this, we introduce functionality similarity, which considers the tasks or actions that users can perform within a particular GUI state. This ensures that even if the layouts are similar, the underlying functionality is accurately captured and differentiated. AutoDroid-V2 uses GPT-40 to assess and classify each GUI group based on its functionality after the layout-based grouping is performed.\nThen, we can represent each abstract GUI state $S(i)$ after grouping as $(state\\_name(i), layout(i))$, where $state\\_name(i)$ refers to the task or function associated with the GUI state (e.g., \"date selection\" or \"theme selection\") and $layout(i)$ represents the shared visual structure."}, {"title": "3.1.2 Abstract GUI Elements", "content": "To reduce the GUI element space, we introduce abstract GUI elements in AutoDroid-V2, each of which may encapsulate several real GUI elements encountered during exploration. A single abstract element is typically one visible node in the GUI tree with which the user can interact. These elements can be classified into two types: dynamic and static elements. Static elements are those that remain constant regardless of changes in the app's status, such as the 'Search button' in Figure 3. Dynamic elements contain content that can change based on context, such as the 'song title' in Figure 3.\nIn addition to a single abstract element, AutoDroid-V2 introduces another abstract GUI element type called the abstract element list. This represents a collection of single elements that can be indexed or filtered, such as the 'song list' in Figure 3.\nTo address the dynamic nature of mobile apps and further reduce the GUI element space, AutoDroid-V2 merges a group"}, {"title": "3.1.3 Forward and Backward Dependency", "content": "To provide necessary information for agents to plan solution steps, AutoDroid-V2 profiles the GUI element transition relationships by constructing an Element Transition Graph (ETG) and analyzing both the forward and backward dependency for each element. In the ETG, transitions are represented as triples $(e_i, a_i, e_{i+1})$. Here, $e_i$ and $e_{i+1}$ are nodes representing GUI elements, and $a_i$ is the directed edge representing the action taken to transition from $e_i$ to $e_{i+1}$.To build ETG, we define the root node $e_r$ as the elements in the first GUI of the app after it is opened. For each action $a_i$ observed during the exploration, we locate the existing element $e_i$ that $a_i$ is performed on, and identify all the GUI elements $(e_{i+1}, e_{i+2}...)$ that become available in the subsequent GUI state resulting from $a_i$. Edges are added to represent the transitions: $(e_i, a_i, e_{i+1}), (e_i, a_i, e_{i+2}), etc$.\nAfter constructing ETG, we analyze the transition relationships between elements by defining two types of dependencies for each element $e_i$: the backward dependency and the forward dependency. Backward dependency records all the paths from other elements $e_1, e_2, ...e_{i-1}$ leading to $e_i$. Forward dependency represents the subsequent GUI state $e_i$ leads to. At runtime, forward dependency is included in the prompt, providing the context by showing how one GUI element leads to the next sequence of interactions. If the script generated by the LLM fails to correctly trigger a transition between GUI elements, the backward dependency is then used to trace back through the possible previous interactions and identify an alternative path that can successfully reach the target GUI element."}, {"title": "3.2 Data Synthesis for SLM Customization", "content": "To improve the task automation capabilities of on-device SLMs, it is crucial to train them with large-scale data. We generate a large set of simulated user tasks of varying complexity based on the app document, along with their corresponding solutions. These synthetic task-solution pairs are used to fine-tune the SLMs, enabling them to create GUI-specific code with correct syntax and dependencies."}, {"title": "3.2.1 Large-scale Task & Solution Generation", "content": "AutoDroid-V2 generates tasks by sampling groups of elements from the app document, creating task-solution data of varying complexity levels. This approach enables on-device SLMs to learn to handle diverse tasks effectively. Task generation can be viewed as the inverse of task automation. In task automation, a user task T is mapped to a set of GUI elements and corresponding actions, forming a sub-graph $G_s$ in the Element Transition Graph (ETG). Conversely, generating tasks is to map $G_s$ to T. AutoDroid-V2 iteratively samples a $G_s$ from the ETG and queries LLMs to generate user tasks solvable with these elements. The complexity of a task is determined by the number and type of GUI elements involved and the number of actions required to complete the task. By varying the size of $G_s$, tasks of different complexities can be generated, denoted as {T1, T2, ..., Tn}.\nTo regulate the task solution format and execute them on real devices, we develop a Python library to interpret the scripts. The design principle of this library is being concise and intuitive, allowing LLMs/SLMs to effectively learn through simple instructions and in-context examples. The library provides two categories of functions: GUI action APIs and information retrieval APIs. The GUI action APIs (including tap, long_tap, set_text, scroll) are designed to manipulate an element or a specific child element within an element list. These actions are invoked using methods like: <element>.tap(), <element>.tap(child_element), <element>.set_text(<text>), or <element>.scroll(<direction>) Information retrieving APIs (including get_text, get_attributes, match, indexing) facilitate the accurate identification of task-related GUI elements by gathering necessary information about them. Examples include <element_list>.match(<text or attribute dict>) or <element_list>[<idx>].\nThen, we design a detailed prompt to guide LLMs in generating executable code of user tasks for training. The prompt includes the user task, a simplified App Document (which details the name, description, options, and effects of each element), a description of the domain-specific library, and the GUI elements in the current GUI (often the GUI elements in the original GUI screen of the app):"}, {"title": "3.2.2 Validation-based Script Revising", "content": "For each generated script, we validate it by executing it in a real environment before including it in the training dataset. Many of the generated scripts may not be executable due to LLM errors and app dynamics. Therefore, we need to handle encountered errors and revise the scripts accordingly during script validation. We introduce a domain-specific code status detector module that addresses errors by regenerating the script based on the error information. Specifically, for each task, we initially generate a code sample C = Co, which is executed on a mobile device or emulator based on the domain-specific library mentioned in Section 3.2.1. If execution fails with error information &, it indicates issues with C. The code status detector sends error information & as well as the GUI state where the error occurs to the LLM, which is responsible for regenerating the script based on this GUI state to continue execution.\nSpecifically, we categorize error & as sending an illegal action to a GUI element (such as inputting text into a button), matching or indexing a non-existent element in the screen (such as index out of range for an element list), or logic errors (the previous action does not result in the GUI state that contains the current element). This categorization makes it easier for LLMs to understand the error, rather than processing raw error information."}, {"title": "3.2.3 Tree-based Script Quality Improvement", "content": "Even if a script executes successfully, it may not align with the intended user tasks. Thus, AutoDroid-V2 uses a on-cloud LLM-based reward model to evaluate whether the code C successfully achieves the task T. This reward model takes as input the task T, the executed code C, and a sequence of environment observation states S = (s\u2081, . . ., sn), outputting a judgment I and feedback Fout. If the reward model deems the code C as complete, the task-code pair (T, C) is added to the dataset. Otherwise, the error information & and feedback Fout are recorded and used in a code search tree to regenerate a code sample.\nThe code search tree is a Depth-First Search (DFS) tree designed to generate code samples, where each node represents a possible attempt to achieve the task goal through code. The initial code Co resides at the root node of the tree. During the search process, new nodes are expanded in a depth-first manner. At each node, the LLM is queried to generate a new code C' based on the task T, the current node's code C, the error information &, and the feedback Fout from the reward model. The LLM generates new code sample C' that tries to address the error information & and the feedback Fout, while achieving the task T.\nThe new code sample C' is then validated again through the dynamic task execution validator. The search continues until a code sample C is accepted by the validator or the maximum number of search attempts is reached. The final task-solution pair (T, C) is then added to the final dataset, which will be used to fine-tune the SLM."}, {"title": "3.3 Script-based Runtime Task Execution", "content": "At runtime, AutoDroid-V2 queries the LLM to synthesize domain-specific script tailored to the user's task, which can be executed by the domain-specific code executor."}, {"title": "3.3.1 Runtime Dynamicity Handling", "content": "AutoDroid-V2 handles runtime dynamicity with two techniques. The first is the error handler, which is to re-generate the script after encountering runtime failures as described in Section 3.2.2. The second is the dependency-aware instruction execution.\nAt runtime, AutoDroid-V2 executes instructions from the script by grounding the target GUI element in the GUI screen to send proper GUI actions. AutoDroid-V2 introduces a dependency-enhanced instruction execution mechanism to handle incomplete scripts that may fail to account for all necessary navigation paths to a target GUI element. Specifically, let $e_c$ represent the GUI element invoked in the script, and let $T_c$ represent the current GUI tree, we match $e_c$ by checking: 1) Whether $T_c$ is the GUI state that $e_c$ belongs to, in case the current GUI screen is another state that contains different GUI elements that share the identical XPaths with $e_c$. 2) The identifier of $e_c$ can match one GUI element $e_i$ in $T_c$. If not matched, AutoDroid-V2 leverages backward dependency of $e_c$ to automatically navigate to it. The dependency represents potential navigation sequences that can guide the GUI testing process"}, {"title": "3.3.2 Enhancing Efficiency with Prompt Compression and Reuse", "content": "Due to the limited computing resources of mobile devices and constrained context lengths of on-device SLMs, using the detailed app documents for task script generation (like how we synthesize data in Section 3.2.1) can be costly or even infeasible at runtime. Therefore, we propose to shorten the runtime query prompt. Specifically, a runtime code query prompt consists only of the basic instruction, the user task, and all the element names in the app document, which reduces the prompt length from 15.4k to 2.8k on average (most of which can be cached and reused). The feasibility of such simplifications is from fine-tuning, which trained the intrinsic task-agnostic knowledge into the model parameters and left only the task-dependent in the prompt for the customized SLMs' reference for different tasks. As such, this reduction does not affect the performance of on-device SLMs.\nAutoDroid-V2 also employs prompt cache [10] techniques to further speed up on-device SLM inference, particularly in reducing the prefilling latency. The prompt cache stores the KV state of existing queries, enabling a new query to reuse the KV cache if it shares the same prefix with a previous query, which allows the new query to skip the computation of the shared part. The document portion of a prompt in AutoDroid-V2 is consistent across all queries and can be cached and reused for each user task. Additionally, the document statement accounts for most of the input prompt length (by 97.6%), so caching this part significantly reduces the prefilling latency."}, {"title": "4 EVALUATION", "content": "We implement AutoDroid-V2 using Python and Java. SLMs are deployed on mobile devices using llama.cpp, a framework to enable LLM/SLM inference in C/C++."}, {"title": "4.1 Experimental Setup", "content": "Dataset. We evaluate the effectiveness and efficiency of AutoDroid-V2 and baseline on two benchmarks: DroidTask [36] and AitW-subset [28]. DroidTask [36] is a mobile task automation dataset with 158 high-level tasks across 13 popular apps. AitW [43] is a large-scale dataset for Android device control. The original AitW dataset contains 417k tasks but lacks the exploration environment of the apps. So we choose the 'google apps' and 'general' subsets of the AitW dataset with 68 tasks using LlamaTouch [43] environment, which is executable and can be explored. The exploration trace is provided by the original datasets. Our document generation and task synthesis are independent of the test set.\nHardware. We evaluate the performance of AutoDroid-V2 on two devices: (1) A OnePlus ACE 2 Pro with 8 3.2 GHz ARM-based cores (Snapdragon 8 Gen2 CPU) and Adreno\u2122 740 GPU. (2) a MacBook Pro with an Apple M2 Pro chip, featuring a 10-core CPU, a 16-core GPU, and 16 GB of unified memory, selected to represent devices with higher computational capacity. The local LLM Llama-3.1 [9] is deployed on the smartphone based on the llama.cpp framework [10]. The local LLMs are fine-tuned on an 8x A100 80GB server for 1 epoch, taking about 2.5 GPU hours.\nBaselines. We mainly compare AutoDroid-V2 with 2 types of baselines. (1) VLM-based agents, including CogAgent [13], SeeClick [7]. CogAgent is an 18B visual language model specializing in GUI navigation tasks. SeeClick is a GUI agent built on Qwen-VL [3], with 9.6B parameters. (2) LLM-based agents, including AutoDroid [36], Mind2Web [8]. AutoDroid is a memory-augmented mobile task automation framework, which is similar to our framework but adopts a step-wise method for task automation. We evaluate AutoDroid using GPT-40 [15] and a fine-tuned Llama-3.1 [9] based on its automatically generated dataset. Mind2Web is a generalist agent for UI task automation, which we evaluate based on GPT-40 [15] and Llama-3.1 [9] respectively.\nMetrics. Given a sequence of UIs {U1, U2, ..., Un} and corresponding actions A = {A1, A2, ..., An} performed by human annotators to complete a task T, if an agent generates a sequence of decisions \u00c2 = {\u00c21, \u00c22, ..., \u00c2k } on {\u00db1, \u00db2, ..., \u00dbk}, we use the following metrics to evaluate its performance:\nSuccess Rate: In DroidTask [36], a task is considered completed if the ground-truth action sequence A is a subsequence of the agent-generated sequence \u00c2, i.e., $A \\in \u00c2$. This metric reflects the agent's ability to complete the"}, {"title": "4.2 Success Rate and Redundancy", "content": ""}, {"title": "4.2.1 DroidTask", "content": "We first evaluate the accuracy of AutoDroid-V2 and baselines in DroidTask, and the result is shown in Table 3. AutoDroid-V2 achieves an average task accuracy of 54.4%, significantly higher than the baselines, whose accuracy ranges from 10.5% to 43.9%. Among the baseline methods, AutoDroid [36] achieves the highest performance because it also adopts automatic task generation and fine-tuning techniques to enhance the performance of on-device LLMs. VLM on-device UI agents (CogAgent [13] and SeeClick [7]) exhibit lower accuracy because they rely solely on screenshot images, which are inadequate for text-heavy apps. Besides, the grounding capabilities of smaller on-device VLMs are limited, leading to frequent misplacement of UI elements.\nAutoDroid-V2 outperforms baselines mainly because of the following reasons: Compared to methods that do not fine-tune LLMs, AutoDroid-V2 generates fine-tuned data that covers every GUI element in the app document, offering detailed insights into how to use the app. While AutoDroid [36] also fine-tunes LLMs by generating tasks and solutions, it relies on a step-wise method. In contrast, AutoDroid-V2 uses script-based fine-tuning data, which is more suitable for smaller LLMs. For the script-based method, the responsibility of determining whether a task is complete is shifted from the LLMS to the code executor. If a script runs without errors, it is considered complete, thus eliminating the need for the LLM to make a task completion decision. And AutoDroid is less effective at determining task completion [36]. AutoDroid tries to address this limitation by incorporating a small amount of manually annotated data from [6], this solution is neither scalable nor fully effective. Our experiments found that AutoDroid repeatedly executes actions for 25.5% The success"}, {"title": "4.2.2 AitW", "content": "AutoDroid-V2 outperforms the baselines by an average of 14.3%, as shown in Table 4. LLM-based GUI agents (CogAgent [13] and SeeClick [7]) perform better on AitW [28] than on DroidTask [36] because they were fine-tuned on the dataset and have a comprehensive understanding of the apps. The results demonstrate that AutoDroid-V2 consistently outperforms the baselines in complex apps like Chrome, Google Maps, and YouTube. This is because these apps often feature dynamic or deeply nested UI elements (e.g. hierarchical menus in Google Maps or dynamic content in YouTube driven by recommendation systems). Step-by-step agents may struggle to locate and interact with such elements consistently due to the limited context provided by a single screen. In contrast, AutoDroid-V2's script-based approach, with access to a full application overview, allows it to consider the potential effects of each action and anticipate future elements, leading to more effective solutions."}, {"title": "4.3 Latency and Cost", "content": "Figure 4 illustrates the on-device LLM inference latencies of AutoDroid-V2 and the step-wise method (AutoDroid [36]) across different apps on Snapdragon 8 Gen 2 and Apple M2 Pro. We quantize the fine-tuned Llama3.1 8B models of AutoDroid-V2 and AutoDroid to 8-bit precision and deploy them on mobile devices using the Llama.cpp framework [10]. The average LLM inference latency for each task of AutoDroid-V2 is 46.3s, compared to 669.2s for the baseline on Snapdragon 8 Gen 2, resulting in a 93.1% reduction in inference latency.\nThis acceleration is mainly due to the script-based method, which reduces the number of LLM queries. For the stepwise method, the latency for each step remains stable, so the overall latency depends on the number of steps in a task. The Calendar app, for instance, has the highest average step count (13.4 steps per task), resulting in the highest latency."}, {"title": "4.4 Fine-grained Performance Analysis", "content": ""}, {"title": "4.4.1 Performance with Different LLMs", "content": "We use Auto-Droid, the best-performing baseline on the Llama-3.8B model, as a reference and compare it with AutoDroid-V2 across different LLMs. The results presented in Figure 5 show the"}, {"title": "4.4.2 Impact of Script Validation", "content": "We also compare the success rate of AutoDroid-V2 with and without the train data validator module mentioned in Section 3.2.2 and Section 3.2.3. The LLM is Llama3.1 8B, and the average success rate of methods with and without a solution validator are 54.4%, and 51.9% respectively, improved by 2.5%. We observe that 4 applications such as Contacts, Files, Notes, and SMS experience a minor decrease in success number (1-3 tasks) after the validation step. These apps are highly user-specific, containing personal data such as contact names, file names, and notes. Validation becomes problematic for these apps because some tasks cannot be completed unless certain user-specific data is present. For instance, the task \"Delete a contact named Alice\" cannot be completed if Alice is not in the user's contact list. Thus, some valid tasks and solutions are filtered out by the validator, even though the solutions themselves are correct. Consequently, these tasks are excluded from the training data, which diminishes the fine-tuned LLM's ability to complete such tasks. For other apps, tasks do not require specific user data (e.g. Camera, Calendar, Recorder). In these cases, the validator can filter out incorrect solutions, improving or maintaining the fine-tuned model's accuracy. One potential solution to this issue is to pre-set user data using a random explorer before validating the app. The task generator can then be provided with this user data, enabling it to generate tasks that can be validated successfully."}, {"title": "5 DISCUSSION", "content": "One potential concern is how AutoDroid-V2 handles situations where structured text representations of GUIs are unavailable. Typically, AutoDroid-V2 identifies and interacts with the target GUI elements at runtime by matching their"}, {"title": "6 CONCLUSION", "content": "We present a document-guided, script-based, end-to-end system named AutoDroid-V2 to support mobile task automation using on-device SLMs. Experiments show that the script-based method significantly improves the efficiency and performance of GUI agents. We believe this approach has the potential to enable the full deployment of GUI agents on devices, achieving accuracy comparable to that of cloud-based GUI agents."}]}