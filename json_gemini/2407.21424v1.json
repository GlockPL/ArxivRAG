{"title": "Cost-Effective Hallucination Detection for LLMs", "authors": ["Simon Valentin", "Jinmiao Fu", "Gianluca Detommaso", "Shaoyuan Xu", "Giovanni Zappella", "Bryan Wang"], "abstract": "Large language models (LLMs) can be prone to hallucinations generating unreliable outputs that\nare unfaithful to their inputs, external facts or internally inconsistent. In this work, we address several\nchallenges for post-hoc hallucination detection in production settings. Our pipeline for hallucination\ndetection entails: first, producing a confidence score representing the likelihood that a generated answer\nis a hallucination; second, calibrating the score conditional on attributes of the inputs and candidate\nresponse; finally, performing detection by thresholding the calibrated score. We benchmark a variety of\nstate-of-the-art scoring methods on different datasets, encompassing question answering, fact checking,\nand summarization tasks. We employ diverse LLMs to ensure a comprehensive assessment of perfor-\nmance. We show that calibrating individual scoring methods is critical for ensuring risk-aware down-\nstream decision making. Based on findings that no individual score performs best in all situations, we\npropose a multi-scoring framework, which combines different scores and achieves top performance across\nall datasets. We further introduce cost-effective multi-scoring, which can match or even outperform more\nexpensive detection methods, while significantly reducing computational overhead.", "sections": [{"title": "1 Introduction", "content": "Despite their impressive capabilities, large language models (LLMs) can be prone to generating hallucinations\nundesirable outputs that are incorrect, unfaithful, or inconsistent with respect to the inputs (or the output\nitself) [1]. These unreliable behaviors pose significant risks for adopting LLMs in real-world applications.\nChallenges in detecting hallucinations lie, among other things, in hallucinations taking different forms, being\ncontext-dependent and sometimes being in conflict with other desirable properties of generated text [2, 3].\nHallucinations may be harmless in some contexts, but can be undesired or potentially dangerous in other\napplications (e.g., erroneous medical advice). Detecting and quantifying hallucination risk is thus a critical\ncapability to enable safe applications of LLMs and improve generated outputs.\nPrior work has proposed various approaches for detecting and mitigating hallucinations in LLM-generated\noutputs, including verifying faithfulness to inputs [4], assessing internal coherence [5], consulting external\nknowledge sources [6], and quantifying model uncertainty [2, 3, 7, 8]. However, deploying these methods\nin production settings is far from trivial due to several challenges: First, there is limited comparative\nevaluation illuminating how different detection methods perform. Second, existing approaches for detecting\nhallucinations differ greatly in their computational demands, and guidelines are lacking on cost-effectiveness\ntrade-offs to inform method selection for real-world applications with constraints. Third, hallucination\ndetection in the real world often requires careful consideration of risks and false positive/negative trade-offs,\nrequiring methods to provide well-calibrated probability scores. Fourth, many applications of LLMs take"}, {"title": "2 Detecting LLM Hallucinations", "content": ""}, {"title": "2.1 Formalizing Hallucination Detection", "content": "We study the problem of quantifying the probability that a generated output from a language model contains\nhallucinations. More formally, let x represent an input token sequence to a language model G, and let z\nrepresent a generated output text sequence from the model. We define a binary random variable $y \\in \\{0,1\\}$\nthat indicates whether z is a \u201cpermissible\u201d output (y = 1) or contains a \u201challucination\u201d (y = 0).\nOur goal is to develop a scoring function to model the probability that a given output text contains a\nhallucination conditioned on the input. This is critical, as in real-world scenarios, we need to set risk-aware"}, {"title": "2.2 Scoring Methods", "content": "Many hallucination detection methods proposed in the literature make use of LLMs to \"judge\" the output of\nan LLM (either the same or a different one). For clarity, we thus distinguish between generator and detector\nLLMs. The generator is defined as the model used to generate the original response. The detector LLM is\nthe model used to score a generated text for the presence of hallucinations. In general, the generator and\ndetector LLMs may coincide. However, it may be more desirable to use different LLMs in some scenarios,\ne.g., when computational cost is a greater concern, where a smaller LLM may be used to judge outputs of\na more expensive LLM, or when using hallucination scoring methods that require white-/grey-box access,\nwhile the generator is black-box.\nToday, many interactions with LLMs take the form of API-calls. Typically, only the output tokens\nare returned with no access to the logits of the predicted tokens, treating the LLM effectively as a black\nbox. Sometimes, inference parameters may be accessible, allowing for setting different temperature (among\nothers) values, thereby providing some (grey-box) access to the model. Generally, hallucination detection\nmethods vary in their required model access, ranging from black-box APIs to full white-box access to the\nmodel weights. That is, while some methods only require token-level outputs, other methods may need\naccess to the logits of the generated tokens, or some control over inference parameters like temperature.\nIn our experiments, we evaluate a comprehensive set of hallucination scoring methods. Generally, we do\nnot make any assumptions about the generator LLM being white-, grey- or black-box. We divide methods\ninto single-generation methods, which require only one generated output, and multi-generation methods\nwhich are based on multiple alternative generations."}, {"title": "2.3 Single-generation", "content": "We first provide an overview over a set of hallucination detection methods that are based on scoring the\nhallucination from a single given generated output.\nInverse Perplexity This method provides a prominent instance of using a model's logits over output\ntokens to score the confidence in the output. Computed as the inverse of the exponentiated average negative\nlog-likelihood of the LLM's response [9],\n$\\text{Perplexity}^{-1}(W) = \\exp \\left( - \\frac{1}{N} \\sum_{i=1}^N \\log p(w_i | w_{i-1}, ..., w_1) \\right)$, (1)\ninverse perplexity thus provides a sequence-length normalized expression of the model's confidence. Here,\nthe model may either be the generator LLM or a different detector LLM. Using a different LLM from the"}, {"title": "2.4 Multi-generation", "content": "Multi-generation methods are based on quantifying the consistency across multiple generated outputs from\nthe generator LLM. This follows the notion of white-/grey-box uncertainty quantification via logits that if the\nLLM is confident in its response, multiple generated responses will probably be alike and contain compatible\nfacts. Conversely, for fabricated information, sampled responses are more likely to differ and contradict one\nanother. Crucially, this rests on the assumption that the model's confidence is calibrated, as we discuss and\nevaluate below."}, {"title": "2.5 Calibration", "content": "Initial hallucination scores may not be properly calibrated, which can lead to poor downstream decisions.\nSeminal work by Guo et al. [16] has demonstrated that neural models tend to be miscalibrated, particularly\nin the form of models being overconfident. Later work, focusing on language models, has confirmed this\nacross a wide range of tasks, models and datasets [15]. Notably, there is also research which demonstrates\nthat given particular prompts and in-distribution tasks, LLMs can be well-calibrated [10]. However, this has\nbeen shown to be brittle and dependent on context [17].\nFormally, our score outputs probabilities $p_y(\\hat{y} = 0|x, z)$ that z contains a hallucination, parameterized\nby . The score is calibrated if, for any probability level $p \\in [0,1]$, the average observed frequency of\nhallucinations matches the predicted probability:\n$E[y | p_y(\\hat{y} = 0|x, z) = p] = p$ (3)\nA naive approach for obtaining scores would be to compute the model's probabilities marginally, ignoring\nthe context/question \u00e6 and generated response z and directly estimating p(y = 1). However, this does not\nallow for conditioning on individual inputs to obtain calibrated probabilities $p(y = 1 | x, z)$ for specific x,\nand z, which is, however, impossible to guarantee [18]. Common calibration methods include temperature\nscaling (Platt scaling) of logit outputs [16, 19], isotonic regression [20] or histogram binning [21], which\noperate marginally, and can thereby not account for different confidence levels for different inputs (e.g., with\nan LLM being more confident on certain domains than others).\nAn alternative is to partition the inputs into G groups and compute calibration separately for each group\ng\u2208 G. However, this assumes the groups are disjoint and does not handle inputs belonging to multiple\ngroups, which is often necessary [18]. More advanced calibration methods, such as multicalibration, which\nwe use in this work, allow defining G potentially overlapping groups [18]. Prior work has shown the\neffectiveness of modern calibration techniques in scoring LLM confidence, albeit in a white-box setting [8].\nWe describe our calibration approach in the experimental section."}, {"title": "2.6 Multi-Scoring: Combining Scores", "content": "Different scoring methods capture different aspects of hallucinations, e.g., incorrect, non-factual, non-\ngrounded, irrelevant, inconsistent with other answers, etc. As a result, some scores may work better on\nsome data or for some specific models, and worse on others. Therefore, we design a multi-scoring method\nto combine the complementary information from individual scores into a single, strong predictor.\nDenote each available score by $s_n$, for n = 1, ..., \u039d. \u03a4\u03bf obtain an aggregated score, we run a logistic re-\ngression using as features the concatenations of the logit of each score, i.e. [logit($s_1(x, z)$), ..., logit($s_N(x,z)$)],\nand the labels of the calibration dataset as target variables."}, {"title": "2.7 Cost-Effective Multi-Scoring", "content": "Scoring methods based on multiple generations incur the cost of additional generations as well as the cost\nof checking their consistency. Similarly, multi-scoring can be a viable choice when there are only few LLM\ngenerations to check for hallucinations, but incurs considerable computational cost, especially when using\nmulti-generation methods. To avoid prohibitive computational costs in practice, we propose cost-effective\nmulti-scoring, where we set a fixed computational budget and compute the best performing combination of\nscores that stay within the specified budget.\nGiven an input text \u00e6 and generated text z, we have N scoring functions $s_1(x, z), ..., s_N(x,z)$ with\nassociated costs $c_1, ..., c_n$ (e.g., number of generations required). We are given a total computational budget\nB. Our goal is to find the optimal subset of scores S* C {1, ..., N} that maximizes detection performance\nwhile staying within budget B:\n$S^* = \\underset{S: S \\subseteq \\{1,...,N\\}}{\\text{arg min}} L(\\underset{i \\in S}{f(s_i(x,z))}) \\quad \\text{s.t.} \\sum_{i \\in S} c_i \\leq B$ (4)\nwhere L measures loss on a validation set. This is a constrained optimization problem over subsets of\nscoring functions. When $B = \\min_i c_i$, it reduces to selecting the single best score at the lowest cost. When\n$B = \\sum_i c_i$, it recovers full multi-scoring. In between, the optimal subset S* provides the best trade-off\nbetween performance and cost. We note that in general, this problem is computationally challenging, given\nthe exponential runtime complexity. However, given that there are only generally a small set of potential\nscores (N ~ 10) and the logistic regression is very fast to fit, computing all combinations takes only 1.8\nseconds on a single Intel Xeon processor (3.1 GHz), iterating over all candidate solutions sequentially. When\nthis approach is not feasible, some exemplary alternatives include classic greedy forward-selection methods\nor regularising the model via an L1 penalty while scaling the regularisation term to accommodate score\ncost. Overall, cost-effective multi-scoring allows for flexibly balancing multiple scores under computational\nconstraints.\nQuantifying Cost While the cost $c_i$ of scoring function $s_i$ is presented abstractly above, accurately\nquantifying computational cost can be challenging in practice. The actual runtime per method is a reasonable\nproxy, however the runtime depends on model architecture, hardware acceleration, batching, etc. One\napproach is to directly benchmark each scoring function's average runtime empirically on the target hardware.\nHowever, this overlooks nuances like caching effects and ignores runtime variability due to implementational\ndifferences. To simplify our analysis, and as LLM calls are generally much more expensive than calling\nsmaller NLI models based on parameter size, we leverage the number of LLM calls required per method as\na proxy. See Table 1 for an overview. If more precision is desired, we suggest empirically running different\nscores and computing their computational cost in the actual application, as the cost will depend on the\nprecise setup and a range of factors."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Experimental Setup", "content": ""}, {"title": "3.1.1 Datasets", "content": "TriviaQA TriviaQA [24] is a commonly used factual open-response QA dataset. Originally set up as a\nreading comprehension task, it is today often used without context as a closed-book (free-recall) task [25]. We\nuse the validation fold, containing 17944 question-answer pairs. We use mistralai/Mistral-7B-Instruct-\nv0.2 [26] to generate candidate answers. To decide whether a given response should be labelled as positive or\nnegative, we check whether the correct answer is contained in the generated answer after removing formatting,\nfollowing the original evaluation script [24].\nFEVER The closed-response Fact Extraction and VERification dataset [27] provides a comprehensive\nbenchmark of factual hallucination detection. We take the test fold, containing 14027 labelled examples of\nsource documents, claims and whether they are supported, refuted or contain not enough info. For the\npurpose of hallucination detection, we look at all claims that are either supported or refuted.\nHaluEval We include the hallucination detection dataset HaluEval [28] and use the summarization task,\nwhich contains 10000 labeled examples of source documents, summaries and labels for whether the summary\ncontains hallucinations.\nBIG-bench It provides an evaluation of LLMs on a diverse set of tasks [29]. We select 11 tasks suitable\nfor hallucination detection, leading a total of 8664 labelled examples from the validation fold."}, {"title": "3.1.2 Metrics", "content": "Practical applications of hallucination detection require calibrated scores, but often also binary decisions over\nwhether a given output is hallucinated or not. To this end we report the Brier score [30], binary decision\nmetrics F1 score and accuracy."}, {"title": "3.1.3 LLMS", "content": "We conduct experiments with Mistral-7B-Instructv0.2 [26], Mixtral-8x7B-Instruct-v0.1 [31], falcon-\n7b-instruct [32] and OpenLLaMA-13b [33-35]. All models are used with default configurations via Hugging-\nFace Transformers [36]."}, {"title": "3.1.4 Calibration", "content": "The calibration step is performed via the following multicalibration approach, using the Fortuna library [37]:\nTo obtain groups, we compute embeddings of the input text x and the generated response z, such that\nour embedding is e := [embed(x), embed(z)]. We obtain embeddings from Universal AnglE Embedding [38],\nwhich is the SOTA in the MTEB benchmark [39] at the time of writing. We subsequently reduce the\ndimension of a via UMAP [40] and perform soft-clustering via Gaussian Mixture Models, as a simple off-the-\nshelf algorithm (fitting the number of cluster components via BIC [41]). The calibration error is measured\nfor each group g\u2208 G separately. The model is then iteratively patched on the group with the largest error\nuntil the calibration error drops below a threshold for all groups. This provably converges to a calibrated\nmodel with theoretical guarantees [18]. We fit the calibrator to a random calibration fold of 80% of the data\nand report only held-out test results. For all binary predictions, we set the threshold to the 50th percentile\nto not impose preferences over false true/negative rates, but note that in practice any threshold could be\napplied."}, {"title": "3.2 Individual Scoring Methods", "content": "Table 2 presents the results of different hallucination detection methods on all datasets. Multi-generation\nmethods (SelfCheckGPT-NLI, HallucinationRail and SimilarityDegree) are employed only for TriviaQA,\nsince the latter provides the true answer to each question, which can be compared to the alternative generated\nanswers to assess their agreement. In contrast, for HaluEval, BIG-Bench and FEVER, the true answer is not\nprovided. Instead, we compare the given candidate answer from the dataset against the provided binary label\nof correctness. If we exclude Multi-Scoring methods, multi-generation methods such as SelfCheckGPT-NLI\nand SimilarityDegree (with 10 generations) achieve the best performance on TriviaQA, P(True) is the best\non HaluEval and BIG-Bench, and P(InputContradict) performs best on FEVER. Thus, we find that there\nis no single best scoring method across all datasets. This is likely because different scoring methods capture\ndifferent aspects of hallucination, supporting the notion that hallucination is a multi-faceted concept. Multi-\ngeneration methods measure hallucination based on the consistency of the generator LLM's responses and\non the ability of the comparison method to identify whether multiple alternative responses are in agreement.\nWe find that SelfCheckGPT-NLI performs best in our experiments in comparison to SimilarityScore and\nHallucinationRail. While all of these methods follow similar approaches, there are subtle differences in\nhow they assess the consistency between different responses. More generally, as we discuss below, multi-\ngeneration methods are appropriate only if one can assume that there is exactly one correct response, but\ncan fail otherwise.\nOther methods are based on different notions of hallucination, such as NLI (DeBERTa) measuring the\nentailment of the response given the input. Interestingly, variants of P(True) can be appropriate for detecting\ndifferent kinds of hallucinations. P(True) explicitly asks the evaluator LLM to check whether the response\nis correct. In some contexts, the evaluator LLM may have the ability to directly evaluate this. However, in\nother situations, as we see with the FEVER dataset, other methods such as P(InputContradict) can be more\nappropriate, if we are trying to directly target a specific form of hallucination, which may not be subsumed\nunder a general \"correct or not?\" prompt. Other applied scenarios may target even more different (or\nmore specific) kinds of hallucinations, though the variants we include in this work are designed to cover the\nspace in a reasonable manner. Similarly, while the datasets included in these experiments were selected to\ncover a wide range of different kinds of hallucinations, real-world applications may show even new kinds of\nhallucinations, for which no public datasets are available. Overall, these findings highlight the need for our\nmulti-scoring method which can absorb the strength of each individual method and can be easily applied to\na concrete hallucination detection setting while only requiring a relatively small amount of labeled data.\nOverall, neither the inverse perplexity score nor the NLI (DeBERTa) scores emerge as the best scores for\nany of the datasets we consider. While they may add information that can be exploited in (cost-effective)\nmulti-scoring, as reported below, individually they perform worse than some of the other scores."}, {"title": "3.3 Multi-Scoring", "content": "To evaluate the proposed multi-scoring method, we conduct experiments on all datasets combining the scores\nvia logistic regression. As presented in Table 2, the multi-scoring ensemble achieves an F1 score of 0.9106 on\nTriviaQA, outperforming the best individual F1 score of 0.8614 (achieved by SelfCheckGPT-NLI). Similarly,\nBrier and Accuracy also show the highest performance for multi-scoring. For HaluEval, we also find that\nmulti-scoring outperforms the best individual score (P(True)) in all metrics. BIG-Bench shows more nuance,\nas multi-scoring outperforms the best individual score (P(True)) on Brier and Accuracy, but not on F1.\nThis reflects the fact that the metrics capture different properties, and practical considerations may require\na decision as to which metric should be prioritized in a given application. For FEVER, multi-score again\noutperforms the best individual score (P(InputContradict)) in all metrics.\nThus we see that combining scores performs better than the top performing individual scores, showing\nthat combinations of different scoring signals complement each other and can boost performance. This\ndemonstrates that combining complementary signals enables more robust hallucination detection, while\nbalancing the strength of each scoring method. Crucially, while some settings may benefit from combinations\nof different signals (such as when trying to detect different kinds of hallucinations in a given generated\noutput), in other settings the data-driven selection of an informative signal may be sufficient (such as when\ntrying to detect a more narrowly defined notion of hallucination). These scenarios are covered by our use\nof multi-scoring, which allows for arriving at optimally combined hallucination scores (or binary decisions)"}, {"title": "3.4 Cost-effective Multi-Scoring", "content": "Shown in Table 2, cost-effective multi-scoring methods are also amongst the top performers, but at consid-\nerably lower cost compared to multi-score. We see that at cost C = 1, measured as the number of LLM\ncalls (but see our discussion about measuring cost above), we recover the best individual scores, which varies\nacross each dataset. As we increase the budget, the cost-effective multi-scores converge to the performance of\nthe multi-score itself. Notice that in several instances, cost-effective multi-score with C = 2 already performs\nas good as multi-score itself. For TriviaQA, we see that performance increases on all metrics as we increase\nthe computational budget, with cost-effective multi-score at C = 5 outperforming SelfCheckGPT-NLI as the\nmost performant individual scores at half the computational cost in all metrics but Brier. HaluEval shows\nno improvement beyond a budget of C = 2, which is likely due to the kind of hallucination to detect being\nmore narrowly defined and well-capture by a small number of signals, thereby not benefiting from additional\nscores. For BIG-Bench, as discussed for multi-score, we recover the best individual score at C = 1, and\nmatch multi-score at higher budgets. FEVER results indicate that, again, we recover the best individual\nscore at budget C = 1, and can already recover the full multi-score at a budget of C = 3.\nWe now take a closer look at cost-effect multi-scoring when including multi-generation scoring methods.\nThe overall cost budget is varied over the entire range. For each budget, we solve the constrained optimization\nin Eq. 4 to find the optimal subset S* of scores. We compare the hallucination detection F1 score achieved by\nthe cost-effective ensemble versus individual scoring functions and the full ensemble with all scores. Figure 2\nshows the results for TriviaQA for responses generated via Mixtral-8x7B-v0.1. With a minimal budget\nof B = 1, cost-effective selection recovers the best single method, as expected. As the budget increases, it\nselectively adds more expensive functions, gradually improving F1, though the gains are marginal at higher\nbudgets per unit cost. At the maximum budget, it recovers the unconstrained full ensemble performance. In\nbetween, cost-effective multi-scoring incorporates both less and more and expensive methods to maximize\ndetection within the computational constraints.\nThese results demonstrate that the proposed cost-effective multi-scoring approach can intelligently bal-\nance the trade-off between computational expense and hallucination detection effectiveness. It outperforms"}, {"title": "3.5 Exploration of Relationships between Scores", "content": "As discussed above, different scoring methods can target different kinds of hallucations. To explore this\nempirically, we here explore their relationships via Spearman rank correlations. As presented in Figure 4,\ncalibrated scores across all different individual scores are positively correlated. Meanwhile, the magnitude\nof their correlations is smaller than would perhaps be expected if one were to consider hallucinations as\na uniform phenomenon. At the same time, we can see certain \"clusters\" of more strongly inter-correlated\nscores emerge. For example, multi-generation methods including SelfCheckGPT, HallucinationRail and Sim-\nilarity Degree, which check the consistency across multiple generations, show comparably high correlations.\nP(True) and P(True) Verbalised and P(InputContradict) emerge as a similarly correlated cluster of stronger\ncorrelations. Overall, this supports the idea that different scores can capture distinct information, and the\nneed to empirically select appropriate (combinations of) scores for a given application, as we propose with\ncost-effective multi-scoring."}, {"title": "3.6 Experiments across Different LLMs", "content": "Most of the scoring methods considered in this work rely on detector LLMs to compute scores for hallu-\ncination detection. In Table 4 of Appendix A, thus present additional data for different LLMs, namely\nMixtral-8x7B-Instruct-v0.1, falcon-7b-instruct and OpenLLaMA-13b. Overall, we see that hallucina-\ntion detection performance is correlated with performance on general LLM benchmarks. Thus, more overall\ncapable LLMs are likely to perform better at hallucination detection than less capable ones. However, con-\ncrete applications may need to take the inference cost of LLMs into account, where one may not always be\nable to use the most expensive model. We note that cost-effective multi-scoring could here also make use of\nscores computed via LLMs with different computational costs to find cost-effective combinations."}, {"title": "3.7 The Importance of Calibration", "content": "To analyze the impact of calibration, we conduct an ablation study by evaluating model performance with\nand without calibrating the individual scores. Table 3 in Appendix A indicates that model performance"}, {"title": "3.8 Exploring Multi-Generation Assumptions", "content": "User: Generate a recipe for Lunch.\nLLM Response 1: Here is a recipe for a tomato and basil soup. First, bring 1L of veg-\netable broth to a boil...\nLLM Response 2: Here is a recipe for a quinoa and avocado salad. First, cook 150g of\nquinoa according to the package instructions and let it cool...\nOur results indicate that methods based on the uncertainty among multiple generations can provide\nstrong signals when such multiple generations are available. However, such methods can also suffer from\nproblems in practice. Methods based on multiple generations make use of the uncertainty of the generator,\nthat is the distribution over generations z given the input \u00e6, for model G, i.e., $p_G(z | x)$ and the consistency\nbetween actual sampled generations z. We are interested in classifying whether the candidate output contains\na hallucination or not, that is estimating and making use of p(y | x, z). The generating model's uncertainty\n$p_G(z|x)$ can be a useful proxy for p(y | x, z), but only under particular conditions.\nOn a technical level, sampling multiple answers requires access to the generator's temperature parameter,\nas temperatures of zero collapse the generations to a single response, as is mentioned, e.g., in [14]. There-\nfore, these methods are technically grey-box models, as they require some access to the model's inference\nparameters.\nMore conceptually, the generator's uncertainty (or confidence [15]) over $p_G(z | x)$ reflects uncertainty\nover generated tokens. All recently proposed multi-scoring methods we are aware of are based on the idea\nof scoring the consistency across multiple generations using model-based metrics. However, self-consistency\nacross multiple generations is neither a necessary nor sufficient criterion for a hallucination to be present.\nSufficiency is not given as a model may consistently provide an incorrect response. Self-consistency across\nmultiple generations is also not necessary, as many tasks allow for multiple hallucination-free answers that\nare contradictory to each other.\nAs a relatively harmless example, in tasking a model to generate cooking recipes for lunch, the model\nmay generate, among other things, a recipe for a salad and a recipe for a soup. Clearly, the steps in preparing\nthese dishes contain contradictory information, while otherwise being free of hallucinations in themselves.\nThus, if there is more than one correct response, the self-consistency assessment in multi-generation methods\nmay falsely score an output as likely to be hallucinated. As there are numerous situations where there exist\nmultiple correct responses, assuming that there is only one could lead to worse LLM responses, also via a\nloss of diversity.\nPractically, methods based on multiple generations can be costly due to added computational overhead,\nas we have seen above. Finally, these methods are based on the assumption that LLMs are calibrated. As\nsuch, multi-generation methods do not allow for detecting cases where the generator LLM is confident yet\nwrong."}, {"title": "4 Conclusion", "content": "In this work, we compared a comprehensive set of scoring methods to provide calibrated probability scores\nfor the presence of hallucinations in generated LLM outputs. Overall, we have observed that no single\nhallucination detection score performs best across all datasets. Our experiments showed that combinations\nof scores, as suggested with multi-scoring, can effectively combine complementary signals to yield higher"}]}