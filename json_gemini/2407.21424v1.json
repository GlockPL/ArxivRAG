{"title": "Cost-Effective Hallucination Detection for LLMs", "authors": ["Simon Valentin", "Jinmiao Fu", "Gianluca Detommaso", "Shaoyuan Xu", "Giovanni Zappella", "Bryan Wang"], "abstract": "Large language models (LLMs) can be prone to hallucinations generating unreliable outputs that are unfaithful to their inputs, external facts or internally inconsistent. In this work, we address several challenges for post-hoc hallucination detection in production settings. Our pipeline for hallucination detection entails: first, producing a confidence score representing the likelihood that a generated answer is a hallucination; second, calibrating the score conditional on attributes of the inputs and candidate response; finally, performing detection by thresholding the calibrated score. We benchmark a variety of state-of-the-art scoring methods on different datasets, encompassing question answering, fact checking, and summarization tasks. We employ diverse LLMs to ensure a comprehensive assessment of performance. We show that calibrating individual scoring methods is critical for ensuring risk-aware downstream decision making. Based on findings that no individual score performs best in all situations, we propose a multi-scoring framework, which combines different scores and achieves top performance across all datasets. We further introduce cost-effective multi-scoring, which can match or even outperform more expensive detection methods, while significantly reducing computational overhead.", "sections": [{"title": "Introduction", "content": "Despite their impressive capabilities, large language models (LLMs) can be prone to generating hallucinations undesirable outputs that are incorrect, unfaithful, or inconsistent with respect to the inputs (or the output itself) [1]. These unreliable behaviors pose significant risks for adopting LLMs in real-world applications. Challenges in detecting hallucinations lie, among other things, in hallucinations taking different forms, being context-dependent and sometimes being in conflict with other desirable properties of generated text [2, 3]. Hallucinations may be harmless in some contexts, but can be undesired or potentially dangerous in other applications (e.g., erroneous medical advice). Detecting and quantifying hallucination risk is thus a critical capability to enable safe applications of LLMs and improve generated outputs.\nPrior work has proposed various approaches for detecting and mitigating hallucinations in LLM-generated outputs, including verifying faithfulness to inputs [4], assessing internal coherence [5], consulting external knowledge sources [6], and quantifying model uncertainty [2, 3, 7, 8]. However, deploying these methods in production settings is far from trivial due to several challenges: First, there is limited comparative evaluation illuminating how different detection methods perform. Second, existing approaches for detecting hallucinations differ greatly in their computational demands, and guidelines are lacking on cost-effectiveness trade-offs to inform method selection for real-world applications with constraints. Third, hallucination detection in the real world often requires careful consideration of risks and false positive/negative trade-offs, requiring methods to provide well-calibrated probability scores. Fourth, many applications of LLMs take"}, {"title": "Detecting LLM Hallucinations", "content": "We study the problem of quantifying the probability that a generated output from a language model contains hallucinations. More formally, let x represent an input token sequence to a language model G, and let z represent a generated output text sequence from the model. We define a binary random variable y \u2208 {0,1} that indicates whether z is a \u201cpermissible\u201d output (y = 1) or contains a \u201challucination\u201d (y = 0).\nOur goal is to develop a scoring function to model the probability that a given output text contains a hallucination conditioned on the input. This is critical, as in real-world scenarios, we need to set risk-aware"}, {"title": "Formalizing Hallucination Detection", "content": "We study the problem of quantifying the probability that a generated output from a language model contains hallucinations. More formally, let x represent an input token sequence to a language model G, and let z represent a generated output text sequence from the model. We define a binary random variable y \u2208 {0,1} that indicates whether z is a \u201cpermissible\u201d output (y = 1) or contains a \u201challucination\u201d (y = 0).\nOur goal is to develop a scoring function to model the probability that a given output text contains a hallucination conditioned on the input. This is critical, as in real-world scenarios, we need to set risk-aware thresholds, balancing false positive/negative rates to accommodate the production requirement. Having access to scores allows us to flexibly set the threshold. Conceptually, the key reason to model this probabilistically is that there is inherent uncertainty in determining whether a given text contains a hallucination or not. Even human raters may disagree on the assessment, based on their own knowledge and definition. Some key contributors to this epistemic uncertainty are: First, that no system has complete world knowledge to perfectly assess factual correctness. Second, that there is ambiguity in whether something is a hallucination. Finally, any automatic scoring model may make occasional errors, so probabilistic scores reflect confidence. Therefore, while conditional on x and z the true hallucination label y is fixed, our estimate of y remains uncertain. The probabilistic score thus reflects this epistemic uncertainty the degree of belief that z contains a hallucination given the available knowledge: p(y = 0 | x, z; \u03c6), where \u03c6 denotes parameters of the scoring function. We refer to this conditional probability function as the hallucination score, denoted sy(x,z). The hallucination score can then be applied to downstream tasks, including making risk-aware binary decisions.\nWe discuss concrete instances of hallucination scores in the following subsection. Generally, the form of sy can vary between hallucination detection approaches. In particular, sy may depend on multiple candidate texts generated for the same input, as implemented by a number of hallucination detection methods proposed in the literature. We denote K candidate generated outputs as Z = [z1, ..., zk]. Then sy(x, Z) could quantify inconsistencies within texts in Z using different metrics \u03c6, as discussed below."}, {"title": "Scoring Methods", "content": "Many hallucination detection methods proposed in the literature make use of LLMs to \"judge\" the output of an LLM (either the same or a different one). For clarity, we thus distinguish between generator and detector LLMs. The generator is defined as the model used to generate the original response. The detector LLM is the model used to score a generated text for the presence of hallucinations. In general, the generator and detector LLMs may coincide. However, it may be more desirable to use different LLMs in some scenarios, e.g., when computational cost is a greater concern, where a smaller LLM may be used to judge outputs of a more expensive LLM, or when using hallucination scoring methods that require white-/grey-box access, while the generator is black-box.\nToday, many interactions with LLMs take the form of API-calls. Typically, only the output tokens are returned with no access to the logits of the predicted tokens, treating the LLM effectively as a black box. Sometimes, inference parameters may be accessible, allowing for setting different temperature (among others) values, thereby providing some (grey-box) access to the model. Generally, hallucination detection methods vary in their required model access, ranging from black-box APIs to full white-box access to the model weights. That is, while some methods only require token-level outputs, other methods may need access to the logits of the generated tokens, or some control over inference parameters like temperature.\nIn our experiments, we evaluate a comprehensive set of hallucination scoring methods. Generally, we do not make any assumptions about the generator LLM being white-, grey- or black-box. We divide methods into single-generation methods, which require only one generated output, and multi-generation methods which are based on multiple alternative generations."}, {"title": "Single-generation", "content": "We first provide an overview over a set of hallucination detection methods that are based on scoring the hallucination from a single given generated output.\nInverse Perplexity This method provides a prominent instance of using a model's logits over output tokens to score the confidence in the output. Computed as the inverse of the exponentiated average negative log-likelihood of the LLM's response [9],\nPerplexity\u207b\u00b9(W) = exp(\u2212\u00b9/N \u2211\u1d62\u208c\u2081\u1d3a log p(w\u1d62|w\u1d62\u208b\u2081, ..., w\u2081)),\ninverse perplexity thus provides a sequence-length normalized expression of the model's confidence. Here, the model may either be the generator LLM or a different detector LLM. Using a different LLM from the"}, {"title": "Multi-generation", "content": "Multi-generation methods are based on quantifying the consistency across multiple generated outputs from the generator LLM. This follows the notion of white-/grey-box uncertainty quantification via logits that if the LLM is confident in its response, multiple generated responses will probably be alike and contain compatible facts. Conversely, for fabricated information, sampled responses are more likely to differ and contradict one another. Crucially, this rests on the assumption that the model's confidence is calibrated, as we discuss and evaluate below."}, {"title": "Calibration", "content": "Initial hallucination scores may not be properly calibrated, which can lead to poor downstream decisions. Seminal work by Guo et al. [16] has demonstrated that neural models tend to be miscalibrated, particularly in the form of models being overconfident. Later work, focusing on language models, has confirmed this across a wide range of tasks, models and datasets [15]. Notably, there is also research which demonstrates that given particular prompts and in-distribution tasks, LLMs can be well-calibrated [10]. However, this has been shown to be brittle and dependent on context [17].\nFormally, our score outputs probabilities p\u1d67(\u0177 = 0|x, z) that z contains a hallucination, parameterized by \u03c6. The score is calibrated if, for any probability level p\u2208 [0,1], the average observed frequency of hallucinations matches the predicted probability:\nE[y | p\u1d67(\u0177 = 0|x, z) = p] = p\nA naive approach for obtaining scores would be to compute the model's probabilities marginally, ignoring the context/question ax and generated response z and directly estimating p(y = 1). However, this does not allow for conditioning on individual inputs to obtain calibrated probabilities p(y = 1 | x, z) for specific x, and z, which is, however, impossible to guarantee [18]. Common calibration methods include temperature scaling (Platt scaling) of logit outputs [16, 19], isotonic regression [20] or histogram binning [21], which operate marginally, and can thereby not account for different confidence levels for different inputs (e.g., with an LLM being more confident on certain domains than others).\nAn alternative is to partition the inputs into G groups and compute calibration separately for each group g\u2208 G. However, this assumes the groups are disjoint and does not handle inputs belonging to multiple groups, which is often necessary [18]. More advanced calibration methods, such as multicalibration, which we use in this work, allow defining G potentially overlapping groups [18]. Prior work has shown the effectiveness of modern calibration techniques in scoring LLM confidence, albeit in a white-box setting [8]. We describe our calibration approach in the experimental section."}, {"title": "Multi-Scoring: Combining Scores", "content": "Different scoring methods capture different aspects of hallucinations, e.g., incorrect, non-factual, non-grounded, irrelevant, inconsistent with other answers, etc. As a result, some scores may work better on some data or for some specific models, and worse on others. Therefore, we design a multi-scoring method to combine the complementary information from individual scores into a single, strong predictor.\nDenote each available score by sn, for n = 1, ..., N. To obtain an aggregated score, we run a logistic regression using as features the concatenations of the logit of each score, i.e. [logit(s\u2081(x, z)), ..., logit(s\u0274(x,z))], and the labels of the calibration dataset as target variables."}, {"title": "Cost-Effective Multi-Scoring", "content": "Scoring methods based on multiple generations incur the cost of additional generations as well as the cost of checking their consistency. Similarly, multi-scoring can be a viable choice when there are only few LLM generations to check for hallucinations, but incurs considerable computational cost, especially when using multi-generation methods. To avoid prohibitive computational costs in practice, we propose cost-effective multi-scoring, where we set a fixed computational budget and compute the best performing combination of scores that stay within the specified budget.\nGiven an input text \u00e6 and generated text z, we have N scoring functions s\u2081(x, z), ..., s\u0274(x,z) with associated costs c1, ..., c\u0274 (e.g., number of generations required). We are given a total computational budget B. Our goal is to find the optimal subset of scores S* \u2282 {1, ..., N} that maximizes detection performance while staying within budget B:\nS* = arg min L(f(s\u1d62(x,z))\u1d62\u2208S) s.t. \u2211\u1d62\u2208S c\u1d62 \u2264 B\nwhere L measures loss on a validation set. This is a constrained optimization problem over subsets of scoring functions. When B = mini ci, it reduces to selecting the single best score at the lowest cost. When B = \u2211ici, it recovers full multi-scoring. In between, the optimal subset S* provides the best trade-off between performance and cost. We note that in general, this problem is computationally challenging, given the exponential runtime complexity. However, given that there are only generally a small set of potential scores (N ~ 10) and the logistic regression is very fast to fit, computing all combinations takes only 1.8 seconds on a single Intel Xeon processor (3.1 GHz), iterating over all candidate solutions sequentially. When this approach is not feasible, some exemplary alternatives include classic greedy forward-selection methods or regularising the model via an L1 penalty while scaling the regularisation term to accommodate score cost. Overall, cost-effective multi-scoring allows for flexibly balancing multiple scores under computational constraints.\nQuantifying Cost While the cost ci of scoring function si is presented abstractly above, accurately quantifying computational cost can be challenging in practice. The actual runtime per method is a reasonable proxy, however the runtime depends on model architecture, hardware acceleration, batching, etc. One approach is to directly benchmark each scoring function's average runtime empirically on the target hardware. However, this overlooks nuances like caching effects and ignores runtime variability due to implementational differences. To simplify our analysis, and as LLM calls are generally much more expensive than calling smaller NLI models based on parameter size, we leverage the number of LLM calls required per method as a proxy. See Table 1 for an overview. If more precision is desired, we suggest empirically running different scores and computing their computational cost in the actual application, as the cost will depend on the precise setup and a range of factors."}, {"title": "Experiments", "content": ""}, {"title": "Experimental Setup", "content": ""}, {"title": "Datasets", "content": "TriviaQA TriviaQA [24] is a commonly used factual open-response QA dataset. Originally set up as a reading comprehension task, it is today often used without context as a closed-book (free-recall) task [25]. We use the validation fold, containing 17944 question-answer pairs. We use mistralai/Mistral-7B-Instruct-v0.2 [26] to generate candidate answers. To decide whether a given response should be labelled as positive or negative, we check whether the correct answer is contained in the generated answer after removing formatting, following the original evaluation script [24].\nFEVER The closed-response Fact Extraction and VERification dataset [27] provides a comprehensive benchmark of factual hallucination detection. We take the test fold, containing 14027 labelled examples of source documents, claims and whether they are supported, refuted or contain not enough info. For the purpose of hallucination detection, we look at all claims that are either supported or refuted.\nHaluEval We include the hallucination detection dataset HaluEval [28] and use the summarization task, which contains 10000 labeled examples of source documents, summaries and labels for whether the summary contains hallucinations.\nBIG-bench It provides an evaluation of LLMs on a diverse set of tasks [29]. We select 11 tasks suitable for hallucination detection, leading a total of 8664 labelled examples from the validation fold."}, {"title": "Metrics", "content": "Practical applications of hallucination detection require calibrated scores, but often also binary decisions over whether a given output is hallucinated or not. To this end we report the Brier score [30], binary decision metrics F1 score and accuracy."}, {"title": "LLMS", "content": "We conduct experiments with Mistral-7B-Instructv0.2 [26], Mixtral-8x7B-Instruct-v0.1 [31], falcon-7b-instruct [32] and OpenLLaMA-13b [33-35]. All models are used with default configurations via Hugging-Face Transformers [36]."}, {"title": "Calibration", "content": "The calibration step is performed via the following multicalibration approach, using the Fortuna library [37]: To obtain groups, we compute embeddings of the input text x and the generated response z, such that our embedding is e := [embed(x), embed(z)]. We obtain embeddings from Universal AnglE Embedding [38], which is the SOTA in the MTEB benchmark [39] at the time of writing. We subsequently reduce the dimension of e via UMAP [40] and perform soft-clustering via Gaussian Mixture Models, as a simple off-the-shelf algorithm (fitting the number of cluster components via BIC [41]). The calibration error is measured for each group g\u2208 G separately. The model is then iteratively patched on the group with the largest error until the calibration error drops below a threshold for all groups. This provably converges to a calibrated model with theoretical guarantees [18]. We fit the calibrator to a random calibration fold of 80% of the data and report only held-out test results. For all binary predictions, we set the threshold to the 50th percentile to not impose preferences over false true/negative rates, but note that in practice any threshold could be applied."}, {"title": "Individual Scoring Methods", "content": "Table 2 presents the results of different hallucination detection methods on all datasets. Multi-generation methods (SelfCheckGPT-NLI, HallucinationRail and SimilarityDegree) are employed only for TriviaQA, since the latter provides the true answer to each question, which can be compared to the alternative generated answers to assess their agreement. In contrast, for HaluEval, BIG-Bench and FEVER, the true answer is not provided. Instead, we compare the given candidate answer from the dataset against the provided binary label of correctness. If we exclude Multi-Scoring methods, multi-generation methods such as SelfCheckGPT-NLI and SimilarityDegree (with 10 generations) achieve the best performance on TriviaQA, P(True) is the best on HaluEval and BIG-Bench, and P(InputContradict) performs best on FEVER. Thus, we find that there is no single best scoring method across all datasets. This is likely because different scoring methods capture different aspects of hallucination, supporting the notion that hallucination is a multi-faceted concept. Multi-generation methods measure hallucination based on the consistency of the generator LLM's responses and on the ability of the comparison method to identify whether multiple alternative responses are in agreement. We find that SelfCheckGPT-NLI performs best in our experiments in comparison to SimilarityScore and HallucinationRail. While all of these methods follow similar approaches, there are subtle differences in how they assess the consistency between different responses. More generally, as we discuss below, multi-generation methods are appropriate only if one can assume that there is exactly one correct response, but can fail otherwise.\nOther methods are based on different notions of hallucination, such as NLI (DeBERTa) measuring the entailment of the response given the input. Interestingly, variants of P(True) can be appropriate for detecting different kinds of hallucinations. P(True) explicitly asks the evaluator LLM to check whether the response is correct. In some contexts, the evaluator LLM may have the ability to directly evaluate this. However, in other situations, as we see with the FEVER dataset, other methods such as P(InputContradict) can be more appropriate, if we are trying to directly target a specific form of hallucination, which may not be subsumed under a general \"correct or not?\" prompt. Other applied scenarios may target even more different (or more specific) kinds of hallucinations, though the variants we include in this work are designed to cover the space in a reasonable manner. Similarly, while the datasets included in these experiments were selected to cover a wide range of different kinds of hallucinations, real-world applications may show even new kinds of hallucinations, for which no public datasets are available. Overall, these findings highlight the need for our multi-scoring method which can absorb the strength of each individual method and can be easily applied to a concrete hallucination detection setting while only requiring a relatively small amount of labeled data.\nOverall, neither the inverse perplexity score nor the NLI (DeBERTa) scores emerge as the best scores for any of the datasets we consider. While they may add information that can be exploited in (cost-effective) multi-scoring, as reported below, individually they perform worse than some of the other scores."}, {"title": "Multi-Scoring", "content": "To evaluate the proposed multi-scoring method, we conduct experiments on all datasets combining the scores via logistic regression. As presented in Table 2, the multi-scoring ensemble achieves an F1 score of 0.9106 on TriviaQA, outperforming the best individual F1 score of 0.8614 (achieved by SelfCheckGPT-NLI). Similarly, Brier and Accuracy also show the highest performance for multi-scoring. For HaluEval, we also find that multi-scoring outperforms the best individual score (P(True)) in all metrics. BIG-Bench shows more nuance, as multi-scoring outperforms the best individual score (P(True)) on Brier and Accuracy, but not on F1. This reflects the fact that the metrics capture different properties, and practical considerations may require a decision as to which metric should be prioritized in a given application. For FEVER, multi-score again outperforms the best individual score (P(InputContradict)) in all metrics.\nThus we see that combining scores performs better than the top performing individual scores, showing that combinations of different scoring signals complement each other and can boost performance. This demonstrates that combining complementary signals enables more robust hallucination detection, while balancing the strength of each scoring method. Crucially, while some settings may benefit from combinations of different signals (such as when trying to detect different kinds of hallucinations in a given generated output), in other settings the data-driven selection of an informative signal may be sufficient (such as when trying to detect a more narrowly defined notion of hallucination). These scenarios are covered by our use of multi-scoring, which allows for arriving at optimally combined hallucination scores (or binary decisions)"}, {"title": "Cost-effective Multi-Scoring", "content": "Shown in Table 2, cost-effective multi-scoring methods are also amongst the top performers, but at considerably lower cost compared to multi-score. We see that at cost C = 1, measured as the number of LLM calls (but see our discussion about measuring cost above), we recover the best individual scores, which varies across each dataset. As we increase the budget, the cost-effective multi-scores converge to the performance of the multi-score itself. Notice that in several instances, cost-effective multi-score with C = 2 already performs as good as multi-score itself. For TriviaQA, we see that performance increases on all metrics as we increase the computational budget, with cost-effective multi-score at C = 5 outperforming SelfCheckGPT-NLI as the most performant individual scores at half the computational cost in all metrics but Brier. HaluEval shows no improvement beyond a budget of C = 2, which is likely due to the kind of hallucination to detect being more narrowly defined and well-capture by a small number of signals, thereby not benefiting from additional scores. For BIG-Bench, as discussed for multi-score, we recover the best individual score at C = 1, and match multi-score at higher budgets. FEVER results indicate that, again, we recover the best individual score at budget C = 1, and can already recover the full multi-score at a budget of C = 3.\nWe now take a closer look at cost-effect multi-scoring when including multi-generation scoring methods. The overall cost budget is varied over the entire range. For each budget, we solve the constrained optimization in Eq. 4 to find the optimal subset S* of scores. We compare the hallucination detection F1 score achieved by the cost-effective ensemble versus individual scoring functions and the full ensemble with all scores. Figure 2 shows the results for TriviaQA for responses generated via Mixtral-8x7B-v0.1. With a minimal budget of B = 1, cost-effective selection recovers the best single method, as expected. As the budget increases, it selectively adds more expensive functions, gradually improving F1, though the gains are marginal at higher budgets per unit cost. At the maximum budget, it recovers the unconstrained full ensemble performance. In between, cost-effective multi-scoring incorporates both less and more and expensive methods to maximize detection within the computational constraints.\nThese results demonstrate that the proposed cost-effective multi-scoring approach can intelligently balance the trade-off between computational expense and hallucination detection effectiveness. It outperforms"}, {"title": "Exploration of Relationships between Scores", "content": "As discussed above, different scoring methods can target different kinds of hallucations. To explore this empirically, we here explore their relationships via Spearman rank correlations. As presented in Figure 4, calibrated scores across all different individual scores are positively correlated. Meanwhile, the magnitude of their correlations is smaller than would perhaps be expected if one were to consider hallucinations as a uniform phenomenon. At the same time, we can see certain \"clusters\" of more strongly inter-correlated scores emerge. For example, multi-generation methods including SelfCheckGPT, HallucinationRail and SimilarityDegree, which check the consistency across multiple generations, show comparably high correlations. P(True) and P(True) Verbalised and P(InputContradict) emerge as a similarly correlated cluster of stronger correlations. Overall, this supports the idea that different scores can capture distinct information, and the need to empirically select appropriate (combinations of) scores for a given application, as we propose with cost-effective multi-scoring."}, {"title": "Experiments across Different LLMs", "content": "Most of the scoring methods considered in this work rely on detector LLMs to compute scores for hallucination detection. In Table 4 of Appendix A, thus present additional data for different LLMs, namely Mixtral-8x7B-Instruct-v0.1, falcon-7b-instruct and OpenLLaMA-13b. Overall, we see that hallucination detection performance is correlated with performance on general LLM benchmarks. Thus, more overall capable LLMs are likely to perform better at hallucination detection than less capable ones. However, concrete applications may need to take the inference cost of LLMs into account, where one may not always be able to use the most expensive model. We note that cost-effective multi-scoring could here also make use of scores computed via LLMs with different computational costs to find cost-effective combinations."}, {"title": "The Importance of Calibration", "content": "To analyze the impact of calibration, we conduct an ablation study by evaluating model performance with and without calibrating the individual scores. Table 3 in Appendix A indicates that model performance"}, {"title": "Exploring Multi-Generation Assumptions", "content": "Our results indicate that methods based on the uncertainty among multiple generations can provide strong signals when such multiple generations are available. However, such methods can also suffer from problems in practice. Methods based on multiple generations make use of the uncertainty of the generator, that is the distribution over generations z given the input \u00e6, for model G, i.e., pG(z | x) and the consistency between actual sampled generations z. We are interested in classifying whether the candidate output contains a hallucination or not, that is estimating and making use of p(y | x, z). The generating model's uncertainty pG(z | x) can be a useful proxy for p(y | x, z), but only under particular conditions.\nOn a technical level, sampling multiple answers requires access to the generator's temperature parameter, as temperatures of zero collapse the generations to a single response, as is mentioned, e.g., in [14]. Therefore, these methods are technically grey-box models, as they require some access to the model's inference parameters.\nMore conceptually, the generator's uncertainty (or confidence [15]) over pG(z | x) reflects uncertainty over generated tokens. All recently proposed multi-scoring methods we are aware of are based on the idea of scoring the consistency across multiple generations using model-based metrics. However, self-consistency across multiple generations is neither a necessary nor sufficient criterion for a hallucination to be present. Sufficiency is not given as a model may consistently provide an incorrect response. Self-consistency across multiple generations is also not necessary, as many tasks allow for multiple hallucination-free answers that are contradictory to each other.\nAs a relatively harmless example, in tasking a model to generate cooking recipes for lunch, the model may generate, among other things, a recipe for a salad and a recipe for a soup. Clearly, the steps in preparing these dishes contain contradictory information, while otherwise being free of hallucinations in themselves. Thus, if there is more than one correct response, the self-consistency assessment in multi-generation methods may falsely score an output as likely to be hallucinated. As there are numerous situations where there exist multiple correct responses, assuming that there is only one could lead to worse LLM responses, also via a loss of diversity.\nPractically, methods based on multiple generations can be costly due to added computational overhead, as we have seen above. Finally, these methods are based on the assumption that LLMs are calibrated. As such, multi-generation methods do not allow for detecting cases where the generator LLM is confident yet wrong."}, {"title": "Conclusion", "content": "In this work, we compared a comprehensive set of scoring methods to provide calibrated probability scores for the presence of hallucinations in generated LLM outputs. Overall, we have observed that no single hallucination detection score performs best across all datasets. Our experiments showed that combinations of scores, as suggested with multi-scoring, can effectively combine complementary signals to yield higher hallucination detection performance than any individual score. Further, we demonstrate that cost-effective multi-scoring can find the highest performing scores at a given computational budget.\nMore generally, our findings support the notion that hallucinations can be rather multi-faceted than present a uniform phenomenon. Thus, detecting hallucinations may require different methods. In concrete settings, one may be interested in even more fine-grained detection of particular types of hallucinations, such as in specific domains like code generation [42].\nThe approach outlined in this work requires only a small amount of labeled data to calibrate hallucination scores and combine scores via (cost-effective) multi-scoring. However, it is important to acknowledge the limitations of the current work. Future research could explore the effectiveness of this approach on more diverse datasets, investigate alternative scoring methods, or extend the methodology to multi-modal models. While improvements in LLM performance can be expected to also lower their rate of occurrence, hallucinations are unlikely to go away completely [43]. Thus, even as models improve further, detecting hallucinations is likely to remain relevant for applying LLMs in a reliable and trustworthy manner in future.\nIn summary, our work presents a promising approach for detecting hallucinations in LLM outputs by combining multiple scoring methods in a cost-effective way, offering a pathway towards more reliable and trustworthy language models that can be applied in real-world settings with more confidence."}]}