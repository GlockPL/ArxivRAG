{"title": "READING YOUR HEART: LEARNING ECG WORDS AND SENTENCES VIA PRE-TRAINING ECG LANGUAGE MODEL", "authors": ["Jiarui Jin", "Haoyu Wang", "Hongyan Li", "Jun Li", "Jiahui Pan", "Shenda Hong"], "abstract": "Electrocardiogram (ECG) is essential for the clinical diagnosis of arrhythmias and other heart diseases, but deep learning methods based on ECG often face limitations due to the need for high-quality annotations. Although previous ECG self-supervised learning (eSSL) methods have made significant progress in representation learning from unannotated ECG data, they typically treat ECG signals as ordinary time-series data, segmenting the signals using fixed-size and fixed-step time windows, which often ignore the form and rhythm characteristics and latent semantic relationships in ECG signals. In this work, we introduce a novel perspective on ECG signals, treating heartbeats as words and rhythms as sentences. Based on this perspective, we first designed the QRS-Tokenizer, which generates semantically meaningful ECG sentences from the raw ECG signals. Building on these, we then propose HeartLang, a novel self-supervised learning framework for ECG language processing, learning general representations at form and rhythm levels. Additionally, we construct the largest heartbeat-based ECG vocabulary to date, which will further advance the development of ECG language processing. We evaluated HeartLang across six public ECG datasets, where it demonstrated robust competitiveness against other eSSL methods. Our data and code are publicly available at https://github.com/PKUDigitalHealth/HeartLang.", "sections": [{"title": "INTRODUCTION", "content": "Electrocardiogram (ECG) is a common type of clinical data used to monitor cardiac activity, and is frequently employed in diagnosing cardiac diseases or conditions impairing myocardial function (Hong et al., 2020; Liu et al., 2021). A primary limitation of using supervised deep learning methods for ECG signal analysis is their dependency on large-scale, expert-reviewed, annotated high-quality data. Moreover, even with sufficient data, these methods are often designed to address specific tasks, which curtails the generalization ability of the model. To overcome these challenges, ECG self-supervised learning (eSSL) has demonstrated efficacy by training on vast amounts of unlabeled ECG recordings to learn generic ECG signal representations, which are then fine-tuned for specific downstream tasks (Pup & Atzori, 2023)."}, {"title": "RELATED WORK", "content": "Current eSSL methods can be primarily classified into two categories: contrastive-based methods and reconstruction-based methods. The core principle of contrastive-based methods involves creating positive and negative sample pairs, aiming to maximize the similarity of positive pairs and minimize the similarity of negative pairs (Zhang et al., 2023b). Reconstruction-based methods focus on training a model to reconstruct the original input from partial or transformed data, thus learning effective data representations (Zhang et al., 2023c). However, almost all methods treat ECG signals as ordinary time-series data, which have two significant drawbacks:\nIgnoring Form and Rhythm Characteristics of ECG. ECG diagnostics from multi-level characteristics are essential Hong et al. (2019). For example, myocardial infarction is diagnosed by observing ST segment elevation of a single heartbeat (Vogel et al., 2019). Likewise, cardiac rhythm characteristics are critical, as arrhythmias like atrial fibrillation (AF) are identified based on the overall cardiac rhythm (Carrington et al., 2022). However, existing eSSL methods typically employ fixed-size and fixed-step time windows to segment the signal (Song et al., 2024). This perspective treats ECG signals as ordinary time-series signals, thereby ignoring the unique form and rhythm characteristics inherent to ECG signals, ultimately leading to a decline in the effectiveness of self-supervised learning for both.\nIgnoring Latent Semantic Relationships of ECG. Due to significant differences in heart rate and other factors between different subjects, and even among different samples from the same subject (Lan et al., 2022), using fixed-size and fixed-step time windows to segment data leads to substantial discrepancies among samples. The differences between samples disrupt the potential semantic relationships between different heartbeats, which in turn negatively impacts the effectiveness of learning a generalized representation in self-supervised learning.\nTo address these challenges, we propose a self-supervised learning framework named HeartLang for ECG language processing (ELP). A distinguishing feature of ECG signals is the clear visibility of heart rate patterns, where individual heartbeats are easily identifiable. The core concept of our framework treats heartbeats as words and rhythms as sentences, enabling self-supervised learning at both form and rhythm levels to capture multi-level general representations. Our method consists of four key components: (1) the QRS-Tokenizer, which generates the ECG sentences from the raw ECG signals; (2) the ST-ECGFormer, which leverages spatio-temporal information to enhance latent semantic extraction from ECG sentences; (3) the construction of the largest ECG vocabulary to date, where heartbeat quantization and reconstruction enable form-level representation learning; and (4) masked ECG sentence pre-training, which facilitates rhythm-level general representation learning. Through these approaches, our method can learn both form-level and rhythm-level representations of ECG signals without labels, and extract latent semantic representations in ECG sentences. The main contributions of this work are summarized in below:\n\u2022 We propose HeartLang, a novel self-supervised learning framework for ECG language processing, designed to learn general representations at form and rhythm levels and extract latent semantic relationships from unlabeled ECG signals.\n\u2022 We present a paradigm-shifting perspective of ECG signals, treating them as a language with distinct words (heartbeats) and sentences (rhythms), and design a QRS-Tokenizer that generates the ECG sentences from the raw ECG signals based on this perspective.\n\u2022 We design ST-ECGFormer, a novel transformer-based backbone network for ECG signal analysis, which leverages the spatio-temporal features in ECG signals to enhance representation learning and optimize latent semantic relationships extraction for ECG sentences.\n\u2022 To the best of our knowledge, we have constructed the largest ECG vocabulary based on heartbeats to date. This ECG vocabulary includes a wide variety of heartbeat morphological representations across different cardiac conditions, which will further advance the development of ECG language processing."}, {"title": "2.1 SELF-SUPERVISED LEARNING FOR ECG SIGNALS", "content": "In recent years, ECG self-supervised learning (eSSL) has demonstrated its ability to learn general representations from unlabeled ECG signals, significantly improving the performance of down-"}, {"title": "2.2 ECG LANGUAGE PROCESSING", "content": "ECG language processing (ELP) is an emerging paradigm for handling ECG signals, first proposed by Mousavi et al. (2021). Since ECG signals inherently possess significant and clear semantic information in heartbeats, they can be processed using methods similar to natural language processing (NLP). Both Mousavi et al. (2021) and Choi et al. (2023) employ approaches that segment different waves within heartbeats to construct vocabularies for modeling. However, when dealing with ECG signals of varying quality, existing methods struggle to accurately segment fine-grained waveforms. Moreover, current ELP methods have relatively small vocabularies (no more than 70 clusters), which limits the richness of the semantic information. In addition, research on ELP remains sparse, highlighting it as a field in urgent need of further exploration. To address these limitations, we propose a new perspective that directly treats heartbeats as words for modeling and have built the largest ECG vocabulary to date, consisting of 5,394 words, which will significantly advance the development of the ELP research field."}, {"title": "3 METHOD", "content": "In this section, we provide a detailed explanation of the specific structure of the HeartLang framework. We first define multi-lead ECG data as $X \\in \\mathbb{R}^{C \\times T}$, where C represents the number of ECG leads (electrodes) and T represents the total timestamps. The configuration of ECG leads follows the standard 12-lead ECG setup. The overview of the framework is shown in Figure 2. The use of the framework can be divided into four steps. First, a QRS-Tokenizer is used to generate the ECG sentences from the raw ECG signals as described in the Section 3.1. Second, constructing the ECG vocabulary is achieved through the steps in the Section 3.3. Third, masked ECG sentence pre-training of the framework is performed as described in the Section 3.4. Finally, fine-tuning is performed for downstream tasks."}, {"title": "3.1 GENERATING ECG SENTENCES USING THE QRS-TOKENIZER", "content": "QRS Detection. A key concept of our method is to treat heartbeats as words, thus making the segmentation of the original ECG signal into semantic heartbeat patches essential. We introduce QRS-Tokenizer, a tokenizer that generates the ECG sentences from the raw ECG signals based on QRS waveforms. Initially, the I-lead signal is bandpass filtered between 5 and 20 Hz, followed by moving wave integration (MWI) using a Ricker wavelet on the filtered signal, and the squared integration signal is saved. The local maxima of the MWI signal are then traversed, with each maximum that occurs after the refractory period and exceeds the QRS detection threshold being classified as a QRS complex. Following detection, we obtain the indices of the detected QRS complexes $Q = \\{q_i|i = 1, ..., N\\}$, where N denotes the number of detected QRS indices per sample, which varies between ECG recordings.\nAssuming the time window size is t. For each lead, we center each index in Q, using the midpoint between every two indices as the interval boundaries, and independently segment the QRS complex"}, {"title": "Generating ECG Sentences.", "content": "After segmentation, we concatenate the individual ECG words of the 12 leads in sequence, forming the overall ECG sentence $x \\in \\mathbb{R}^{l \\times t}$, where l represents the sequence length and t denotes the time window size. Given the variability in heart rates across samples, the resulting sequence lengths are inconsistent. Similar to natural language processing, we set l to the maximum length of the ECG sentence. If the length of the ECG sentence is less than l, it will be padded to l through the zero-filled patches; if the length of the ECG sentence exceeds l, the interval length will be truncated to l. In this paper, l is set to 256, and t is set to 96."}, {"title": "3.2 ST-ECGFORMER BACKBONE NETWORK", "content": "To more effectively capture spatio-temporal features and latent semantic relationships within ECG sentences, we designed a backbone network called ST-ECGFormer. This backbone network is employed in various components of the HeartLang, including vector-quantized heartbeat reconstruction (VQ-HBR) training, masked ECG sentence pre-training, and downstream tasks fine-tuning.\nToken Embedding. ECG signals exhibit high temporal resolution, and the QRS complexes that form ECG sentences contain rich temporal features. These QRS complexes are mapped into a higher-dimensional token feature space, allowing their distinguishing features to be more effectively extracted and encoded. We apply a 1-D convolutional layer-based mapping function to transform each individual ECG word into a corresponding token. After this transformation, the ECG sentence can be represented as $x_t \\in \\mathbb{R}^{l \\times D}$, where D denotes the dimension of the token feature space.\nSpatio-temporal and Position Embedding. To enable the spatial and temporal information of the ECG sentence to be better captured by HeartLang, a temporal embedding set $TE = \\{te_0, te_1, te_2,...,te_{10}\\}$ and a spatial embedding set $SE = \\{se_0, se_1, se_2,...,se_{12}\\}$, both D-dimensional and learnable during the training process, were initialized. For the spatial embedding, we divide the original 12-lead ECG signals into 12 segments, with each lead corresponding to a spatial embedding. The spatial embedding of each individual ECG word is mapped back to the lead from which it originated. For the temporal embedding, the original 10-second ECG signal is divided into 10 segments, where each second corresponds to a temporal embedding. We assign the temporal embedding of each individual ECG word to the time interval of its QRS complex indices Q. Specifically, for zero-filled patches, their spatial and temporal embeddings are set to $te_0$ and $se_0$, respectively, to ensure feature consistency. Next, a class token is added at the beginning of the sequence to enhance the representation. Additionally, a position embedding list $PE = \\{pe_0, pe_1, pe_2,..., pe_l\\}$ is introduced to reinforce the sequential relationships between individual ECG words. Thus, the ECG sentence can be described by the following formula:\n$x' = x_t +TE+SE + PE,$\n$u \\in \\{te_0, te_1, te_2, ..., te_{10}\\}, v \\in \\{se_0, se_1, se_2,..., se_{12}\\}$.\nTransformer Encoder. Finally, the ECG sentence will be input into the transformer encoder (Vaswani et al., 2017). To ensure stability during the training process, we employ the pre-layer normalization strategy (Xiong et al., 2020), which applies layer normalization to the input of the attention mechanism:\n$Q = LN(x')w^Q, K = LN(x')w^K, V = LN(x')w^V,$\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_{head}}})V$\nwhere $d_{head}$ denotes the dimension of each head in the multi-head attention, and LN represents layer normalization."}, {"title": "3.3 VECTOR-QUANTIZED HEARTBEAT RECONSTRUCTION TRAINING", "content": "The individual ECG words segmented by the QRS-Tokenizer lack generalization properties, as each individual ECG word is independent. We aim for the HeartLang to learn general representations across subjects during the subsequent pre-training stage. To achieve this, we introduce an ECG vocabulary, a codebook containing collective ECG words that have cross-subject generalization properties. We believe that the same type of heartbeat across different subjects should be consistent in semantic level. Similar individual ECG words from different subjects are mapped to the same discrete and compact collective ECG word, allowing physiological differences between subjects to be overcome and form-level features of heartbeats to be learned. The construction of the ECG vocabulary is jointly optimized by quantization and reconstruction processes, as depicted in the upper half of Figure 2. This concept is inspired by VQ-NSP (Jiang et al., 2024), which encodes EEG signals into discrete latent representations and decodes them.\nVector Quantization. We first define an ECG vocabulary $V = \\{v_i|i = 1,...,k\\} \\in \\mathbb{R}^{k \\times d}$, where k is the number of collective ECG words in the vocabulary and d is the dimension of each collective ECG word. Given an ECG signal sample $X \\in \\mathbb{R}^{C \\times T}$, it is first generated by the QRS-Tokenizer into ECG sentence $x \\in \\mathbb{R}^{l \\times t}$. After the ECG sentence is input into the ST-ECGFormer, a set of collective ECG word embeddings $P = \\{p_i|i = 1, ..., l\\}$ is obtained. Then, a quantizer is used to convert them into collective ECG word embeddings. The ECG vocabulary looks up the nearest neighbor of each interval representation $p_i$ in V. We use cosine similarity to find the closest collective ECG word embedding. This procedure can be formulated as\n$z_i = arg \\underset{v_i}{\\min} ||l_2(P_i) - l_2(V_i)||_2,$\nwhere $v_i$ is the collective ECG word embedding, and $l_2$ represents $l_2$ normalization.\nHeartbeat Reconstruction. Due to the high signal-to-noise ratio of ECG signals, reconstructing the raw signals directly can efficiently train an ECG vocabulary and effectively learn form-level features of heartbeats. After being labeled by the quantizer, the normalized discrete collective ECG word embeddings $\\{l_2 (z_i)|i = 1, ...,l\\}$ are fed into the transformer decoder. This process can be represented as\n$\\hat{x} = U_{i=1}^{l} f_a(l_2(Uz_i)),$\nwhere $\\hat{x}$ is the reconstructed ECG sentence and $f_a$ is the decoder. To make the update of the ECG vocabulary more stable, we adopt an exponential moving average (EMA) strategy. The mean squared error (MSE) loss is utilized to guide the quantization and reconstruction processes. Finally, the loss function for training the VQ-HBR process is defined as\n$L_v = \\sum_{x \\in D} \\sum_{i=1}^{l} (||x_i - \\hat{x}|| + \\frac{\\pi}{\\tau}||sg(l_2(p_i)) - l_2 (Uz_i) ||^2 + \\frac{\\pi}{\\tau}||l_2(p_i) - sg(l_2(Uz_i)) ||^2)$"}, {"title": "3.4 MASKED ECG SENTENCE PRE-TRAINING", "content": "The pre-training process of the HeartLang framework is illustrated in the lower part of Figure 2. In this stage, HeartLang learns generalized rhythm-level representations by masking parts of individual ECG words within an ECG sentence and predicting the corresponding collective ECG words based on the unmasked context. The pre-training stage primarily includes individual ECG words masking and collective ECG words prediction. This process is inspired by the work of Peng et al. (2022).\nIndividual ECG Words Masking. To enable the HeartLang to learn the rhythm-level features of ECG sentences, we perform random masking on the individual ECG words. This allows the HeartLang to understand the content of the entire ECG sentence based on the unmasked context. For the ECG sentence $x \\in \\mathbb{R}^{l \\times t}$ obtained through the QRS-Tokenizer, where individual ECG words can be represented as: $e = \\{e_i|i = 1, . . .,l\\}$. We randomly generate a mask matrix $M = \\{m_i|i = 1,..., l\\}$, where $m_i \\in \\{0,1\\}$. Then, a learnable mask token $e_m \\in \\mathbb{R}^{t}$ is used to replace the masked heartbeats. Thus, the entire masked ECG sentence can be represented as $X_M = \\{e_i : m_i = 0|i = 1,..., N\\} \\cup \\{e_m : m_i = 1|i = 1, ..., N\\}$.\nCollective ECG Words Prediction. The task of this stage is to predict the collective ECG word indices of the masked parts based on the unmasked individual ECG words, by minimizing the discrepancy between the predicted word indices and the true word indices. We extract the indices of the collective ECG words corresponding to the masked segments of the ECG sentence. The output hidden vectors are then denoted as $h = \\{h_i | i = 1, . . .,l\\}$, which are subsequently used to predict the corresponding collective ECG words via a linear classifier:\n$p(v'_{e_M}) = softmax(Linear(h)).$\nThe target loss function for this stage is:\n$L_p = - \\sum_{x \\in D} \\sum_{m_i=1}logp(v_i|e_M).$"}, {"title": "4 EXPERIMENTS", "content": "We follow the benchmark configuration and results provided by MERL (Liu et al., 2024). It is worth noting that the baseline methods in the benchmark were pre-trained on MIMIC-IV-ECG (Gow et al., 2023) dataset. Since MERL is a multimodal method based on ECG-Text, with an additional clinical report text supervision modality, while HeartLang performs pre-training and downstream fine-tuning solely on ECG recordings, we do not directly compare the two methods in the result presentation but instead compare with other eSSL methods."}, {"title": "4.1 VQ-HBR TRAINING AND PRE-TRAINING CONFIGURATION", "content": "MIMIC-IV-ECG. This publicly accessible dataset (Gow et al., 2023) contains 800,035 12-lead ECG recordings from 161,352 subjects. Each ECG recording was sampled at 500 Hz and lasted for 10 seconds. To prepare the pretraining dataset, we replaced the \u201cNaN\u201d and \u201cInf\u201d values in the ECG recordings with the average of six neighboring points.\nImplementation. Before VQ-HBR training and pre-training stage, we first downsampled all records in the dataset to 100 Hz and used the QRS-Tokenizer to transform the raw ECG recordings into a unified ECG sentence. We split the training and validation sets into 9:1, with the validation set data used for VQ-HBR training. In the VQ-HBR training stage, we set the learning rate to 5 \u00d7 10-5 and trained for 100 epochs, with an ECG vocabulary size of 8,192 and a collective ECG word dimension of 128. In the pre-training stage, we set the learning rate to 5 \u00d7 10-4, trained for 200 epochs, and applied a random masking rate of 50%. For both stages, a randomly initialized ST-ECGFormer was used as the backbone network, the AdamW optimizer was selected, and cosine annealing was applied for learning rate scheduling. All experiments were conducted on 8 NVIDIA GeForce RTX 4090 GPUs, with a batch size of 64 per GPU. We set the random seed to 0 to ensure the reproducibility of all results. More experimental details are provided in the appendix."}, {"title": "4.2 DOWNSTREAM TASKS CONFIGURATION", "content": "We evaluated our method on the three widely used public datasets listed below, which cover over 100 types of cardiac conditions. Detailed information on the data split can be found in the appendix.\nPTB-XL. This publicly accessible dataset (Wagner et al., 2020) contains 21,837 12-lead ECG recordings collected from 18,885 patients. Each ECG recording was sampled at 500 Hz and lasted for 10 seconds. Based on the SCP-ECG protocol, the multi-class classification task has four subsets: Superclass (5 classes), Subclass (23 classes), Form (19 classes), and Rhythm (12 classes). We followed the official data split (Strodthoff et al., 2021) for training, validation, and testing.\nCPSC2018. This publicly accessible dataset (Liu et al., 2018) contains 6,877 12-lead ECG recordings. Each recording was sampled at 500 Hz, with durations ranging from 6 to 60 seconds. The dataset is annotated with 9 different labels. We split the dataset into 70%:10%:20% for training, validation, and testing.\nChapman-Shaoxing-Ningbo (CSN). This publicly accessible dataset (Zheng et al., 2020; 2022) contains 45,152 12-lead ECG recordings. Each ECG recording was sampled at 500 Hz and lasted for 10 seconds. Following the configuration provided by MERL, we removed ECG records with \"unknown\" annotations. The refined version of the dataset contains 23,026 ECG recordings with 38 distinct labels. We split the dataset into 70%, 10%, 20% for training, validation, and testing.\nImplementation. Before fine-tuning in downstream tasks, we first downsampled all records in the dataset to 100 Hz and used the QRS-Tokenizer to transform the raw ECG recordings into a unified ECG sentence. For linear probing, we kept the ST-ECGFormer backbone network frozen and only trained the randomly initialized parameters of the linear classifier. To explore the performance of our method under low-resource conditions, we conducted linear probing using 1%, 10%, and 100% of the training data for each task. We set the learning rate to 5 \u00d7 10-3 and trained for 100 epochs. For the CPSC2018 and CSN datasets, we scaled the ECG recordings to the range of [-3, 3] to enhance QRS detection. All test results were obtained from the best validation model, rather than testing the model on the test set after each epoch and reporting the highest result. For all downstream tasks, we used the macro AUC as the evaluation metric. We set the random seed to 0 to ensure the reproducibility of all results. More experimental details are provided in the appendix."}, {"title": "5 RESULTS AND DISCUSSIONS", "content": "Table 1 presents the linear probing results of HeartLang compared to existing eSSL methods. In the PTB-XL dataset, HeartLang consistently demonstrated significant advantages across 1% to 100% of the training data. Specifically, compared to the second-best eSSL method, our method achieved an average macro AUC improvement of 8.14. Notably, on the Form and Rhythm subsets in PTB-XL dataset, HeartLang outperformed the second-best eSSL methods by an average of 9.85 in macro"}, {"title": "5.1 EVALUATION ON LINEAR PROBING", "content": "AUC with 100% of the training data, further highlighting significant advantages of HeartLang in ECG heartbeat and rhythm representation learning. This validates the effectiveness of our proposed signal slicing perspective of heartbeats as words and rhythms as sentences. For the CPSC2018 and CSN datasets, our method only outperformed others on the CSN dataset with 100% training data. We speculate that this is due to the significant baseline drift in these datasets, which may have reduced the performance of the QRS-Tokenizer. Nevertheless, our method remains highly competitive compared to other eSSL methods.\nWe attribute the weaker performance of other eSSL methods to their disruption of the semantic information in ECG signals. For contrastive eSSL methods, data augmentation methods such as rotation, shifting, and adding noise introduce semantic distortion to the positive and negative pairs in ECG signals, which leads to a decline in representation learning performance. For generative eSSL methods, treating ECG signals as ordinary time-series data and applying the fixed-size and fixed-step time windows for slicing cannot accommodate the broad and complex dynamic characteristics of ECG signals. This results in patches without clear semantic information, ultimately leading to a decline in representation learning performance. In contrast, our method uses the QRS-Tokenizer to segment ECG patches with clear semantic meaning and enhances representation learning by reconstructing both heartbeat form and cardiac rhythms, ultimately achieving superior performance in downstream tasks."}, {"title": "5.2 EVALUATION ON SIGNAL SLICING PERSPECTIVE", "content": "To further validate the effectiveness of our proposed \u201cheartbeats as words, rhythms as sentences\u201d signal slicing perspective, we compared it with the traditional signal slicing perspective in this experiment, which utilizes the fixed window size and time step. For fixed-size and fixed-step time windows perspective, we created ECG sentences from the raw ECG recordings in the same manner but without the QRS detection process, instead slicing based on fixed window sizes and strides. We then performed VQ-HBR training, pre-training, and linear probing using the same configurations."}, {"title": "5.3 ECG VOCABULARY VISUALIZATION", "content": "In this section, we visualize the ECG vocabulary to reflect how the collective ECG words correspond to the original individual ECG words. We trained and validated VQ-HBR on MIMIC-IV-ECG, where the effective use of collective ECG words in the validation set amounted to 5,394 (discrete words)."}, {"title": "5.4 ABLATION STUDY", "content": "In this section, we present a comprehensive ablation study of HeartLang to illustrate the effectiveness of each component within the framework. For the experiment without the ECG vocabulary, we discarded it and applied mean squared error (MSE) loss to reconstruct the masked segments. This approach did not allow HeartLang to effectively learn general representations during pre-training, highlighting the role of ECG vocabulary in helping HeartLang capture latent semantic relationships in ECG sentences. For the experiment without pre-training, we fine-tuned a randomly initialized ST-ECGFormer on downstream tasks. The results indicated that the pretraining process effectively learned general representations in ECG sentences, leading to significant improvements in downstream performance. We also conducted an ablation study on the structure of ST-ECGFormer, completing all training and fine-tuning procedures. The results showed that, aside from a slight performance drop under low-resource conditions, HeartLang consistently enhanced performance, demonstrating that ST-ECGFormer effectively learns spatio-temporal representations of ECG signals and boosts the performance of HeartLang on downstream tasks."}, {"title": "6 CONCLUSION", "content": "In this paper, we propose a novel perspective on ECG signal processing, treating them as a language with distinct words (heartbeats) and sentences (rhythms). Based on this perspective, we introduce the QRS-Tokenizer, which generates the ECG sentences from the raw ECG signals. Building upon these, we propose HeartLang, a novel self-supervised learning framework for ECG language processing. HeartLang learns form-level general representations through vector-quantized heartbeat reconstruction training and rhythm-level general representations through masked ECG sentence pre-training. Additionally, we constructed the largest heartbeat-based ECG vocabulary to date. This ECG vocabulary includes a wide variety of heartbeat morphological representations across different cardiac condition, which will further advance the development of ECG language processing. We evaluated HeartLang across six public ECG datasets, where it demonstrated robust competitiveness against other eSSL methods. We hope that the ideas presented here can inspire the ECG research community, particularly in the emerging field of ECG language processing."}, {"title": "A. DETAIL IN EXPERIMENTAL SETTINGS", "content": "In this section, we provide detailed hyperparameter settings for HeartLang. Due to computational resource limitations, the VQ-HBR training and pre-training for the ablation experiments were conducted using four NVIDIA GeForce RTX 4090 GPUs, with a batch size of 64 per GPU."}, {"title": "A.1 HYPERPARAMETER SETTINGS", "content": null}, {"title": "A.2 DATASET SPLIT", "content": "We describe the dataset split in Table 7. For the MIMIC-IV-ECG (Gow et al., 2023) dataset, we split the training and validation sets with a ratio of 9:1. The validation set of this dataset is used during the VQ-HBR training stage, but not in the pretraining stage. For the four subsets of PTBXL, we adhere to the official split from the official work (Wagner et al., 2020). For CPSC2018 (Liu et al., 2018) and CSN (Zheng et al., 2020; 2022), We followed the division method provided by MERL (Liu et al., 2024) to ensure consistency."}, {"title": "B MORE RESULTS AND DISCUSSIONS", "content": null}, {"title": "B.1 EVALUATION ON DIFFERENT VOCABULARY SIZES", "content": "We present the performance of downstream tasks under different vocabulary sizes in Table 8. We reduced the vocabulary size to 64, similar to the vocabulary size used in previous ECG language processing studies, thereby limiting its semantic expressions. The results show that a larger vocabulary size leads to significant performance improvements. A larger vocabulary provides richer semantic representations and increases the complexity of pre-training, enabling the model to learn more generalized representations and improve the performance of downstream tasks."}, {"title": "B.2 EVALUATION ON FEWER LEAD CONFIGURATION", "content": "Based on the fewer lead configuration recommended by Oh et al. (2022), the configuration is shown in Table 9, and the downstream task validation results are presented in Table 10. In most cases, the downstream performance improves significantly with the increase in the number of leads, especially in the Superclass and Subclass subsets for disease diagnosis. Notably, even under the single-lead condition, the downstream task performance of HeartLang surpasses that of most baseline methods in Table 1. This demonstrates that HeartLang is well-adapted to the special case of single-lead configurations, highlighting its strong generalization capability."}, {"title": "B.3 LIMITATIONS AND FUTURE WORKS", "content": "Our proposed QRS-Tokenizer relies on the QRS complex features of heartbeats, which are a critical component of its functionality. However, for certain diseases or conditions characterized by irregular QRS complexes, the tokenizer may struggle to accurately represent these atypical patterns, leading to performance degradation. Additionally, when segmenting heartbeats, the QRS-Tokenizer pads intervals smaller than 96 with zeros. While this approach is simple and effective for most high-quality datasets, it can partially disrupt the characteristics of heartbeats in datasets with significant baseline drift, as their baselines may deviate substantially from zero. These challenges highlight the need for future improvements to the QRS-Tokenizer, with a focus on enhancing its robustness to handle both irregular QRS complexes and baseline drift effectively, paving the way for more reliable ECG language processing across diverse cardiac conditions."}, {"title": "C MORE VISUALIZATION RESULTS", "content": null}, {"title": "C.1 INDIVIDUAL ECG WORDS AND COLLECTIVE ECG WORDS VISUALIZATION", "content": "We further visualized additional individual ECG words and collective ECG words to demonstrate the semantic richness of our constructed ECG vocabulary, as shown in Figure 5."}, {"title": "C.2 ECG SENTENCE VISUALIZATION", "content": "We visualize the constructed ECG Sentences, as illustrated in Figures 6, 7, and 8. In these figures, the blue lines correspond to the original signals, while the red lines denote the reconstructed signals. As shown in Figure 6, the reconstructed signal demonstrates a notably smoother profile, suggesting that the collective ECG word has effectively captured the morphological characteristics of the heartbeat and exhibits strong generalization capabilities. One notable feature of the QRS-Tokenizer we designed is its ability to adaptively segment individual ECG words based on heart rate, as demonstrated in Figures 7 and 8. In Figure 7, due to the relatively fast heart rate, the individual ECG words within the ECG Sentence utilize the full sentence length of 256. In contrast, Figure 8 shows a relatively slower heart rate, resulting in the use of only a smaller portion of the ECG Sentence, with the remaining sections zero-filled."}]}