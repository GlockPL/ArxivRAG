{"title": "Spatial Transformers for Radio Map Estimation", "authors": ["Pham Q. Viet", "Daniel Romero"], "abstract": "Radio map estimation (RME) involves spatial in-terpolation of radio measurements to predict metrics such as the received signal strength at locations where no measurements were collected. The most popular estimators nowadays project the measurement locations to a regular grid and complete the resulting measurement tensor with a convolutional deep neural network. Unfortunately, these approaches suffer from poor spatial resolution and require a great number of pa-rameters. The first contribution of this paper addresses these limitations by means of an attention-based estimator named Spatial Transformer for Radio Map estimation (STORM). This scheme not only outperforms the existing estimators, but also ex-hibits lower computational complexity, translation equivariance, rotation equivariance, and full spatial resolution. The second contribution is an extended transformer architecture that allows STORM to perform active sensing, where the next measurement location is selected based on the previous measurements. This is particularly useful for minimization of drive tests (MDT) in cellular networks, where operators request user equipment to collect measurements. Finally, STORM is extensively validated by experiments with one ray-tracing and two real datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "Radio maps (cf. Fig. 1), also known as radio environment maps, provide radio frequency (RF) metrics such as the received signal strength across a geographical region [1], [2]. Radio maps find a large number of applications, including network planning, frequency planning, cellular communications, device-to-device communications, dynamic spectrum access, robot path planning, aerial traffic management in unmanned aerial systems, and fingerprinting localization to name a few; see e.g. [1], [3]\u2013[5] and references therein.\nRadio map estimation (RME) involves constructing a radio map by relying on measurements collected across the area of interest. Before the advent of deep learning, the most popular estimators were built upon kernel-based learning (see [6] and references therein), Kriging [7], [8], sparsity-based in-ference [9], matrix completion [10], dictionary learning [11], and graphical models [12]. The most recent estimators are based on deep neural network (DNNs); see e.g. [8], [13]\u2013[15]. Unfortunately, these schemes entail grid discretization and a very large number of trainable parameters, which renders them computationally expensive and drastically limits their spatial resolution. Besides, they lack important desirable properties in RME, such as translation and rotation equivariance.\nA relevant task in RME is also active sensing, where the next measurement location is to be decided given the previously collected measurements. The special case where the upcoming measurements must lie on the trajectory of a mobile robot such as a UAV has been considered in [8]. Other works have proposed extensions and improvements in different settings, but the approaches therein are reminiscent of the aforementioned estimation schemes.\nSome works related to RME have considered attention-based estimators, which is the topic of this paper. For example, vision transformers have been used to accommodate side information, such as building maps [16], [17] and satellite images [18]. In other works, transformers are fed with radio measurements. This is the case of [19], which uses a trans-former for predicting what a device would measure given the measurements collected by a device with different hardware characteristics, and of [20], where a transformer fills miss-ing RSS features for fingerprinting-based localization. Thus, transformers have been applied to problems that are related to RME or to modified versions of the RME problem where transformers are used to process images. However, the plain RME problem, where a radio map needs to be constructed relying on radio measurements and their locations, requires spatial interpolation of radio measurements and this has never been tackled using transformers.\nThe first contribution of this paper is an attention-based scheme referred to as Spatial Transformer for Radio Map estimation (STORM). As shown by numerical experiments in one ray-tracing and two real datasets, STORM outperforms the existing estimators, thereby setting the state of the art in RME. Besides, it offers key advantages over existing DNN"}, {"title": "II. THE RME PROBLEM", "content": "This section presents the most prevalent formulation of RME, both in the conventional and active sensing setups. For simplicity, the exposition assumes that the signal strength is quantified using the received signal power, but it carries over readily to other metrics.\nLet $R\\subset \\mathbb{R}^d$ encompass the Cartesian coordinates of all points within the region of interest, whose dimension d is typically 2 or 3. Very often, R is a rectangular area in a horizontal plane. A power map is a function that returns the signal power $\\varphi(r)$ that a sensor with an isotropic antenna at location $r \\in R$ would receive. This power is the result of the contribution of one or multiple transmitters as well as the propagation effects in the environment.\nThe received power is measured at N locations ${r_n}_{n=1}^N \\subset R$ by one or multiple receivers (or sensors). The n-th mea-surement can be written as $\\eta_n = \\varphi(r_n) + \\phi_n$, where $\\phi_n$ denotes measurement error. Given ${(r_n,\\eta_n)}_{n=1}^N$, the RME problem is to estimate $\\varphi(r)$, $r \\in R$. On the other hand, in the active sensing problem, one is given the measurements ${(r_n,\\eta_n)}_{n=1}^N$ as well as the set ${r_n}_{n=N+1}^{N+M}$ of candidate locations and selects one of these candidate locations, say $r_m$, to collect the next measurement $\\eta_m$. The goal is to choose m so that a target metric of the estimation error is minimized given the measurements ${(r_n, \\eta_n)}_{n=1}^{N+1} \\cup {(r_m, \\eta_m)}$."}, {"title": "III. BACKGROUND ON TRANSFORMERS", "content": "This section introduces notation and reviews the core con-cepts behind attention-based schemes in machine learning."}, {"title": "A. Attention Heads", "content": "The building blocks of transformers are attention heads. To simplify the exposition, single-head attention is explained here but, in practice and in our experiments, an extension called multi-head attention is used.\nThe cross-attention operator is, intuitively speaking, a func-tion that returns a vector encoding the information that a certain set of reference vectors ${z_1, ..., z_{Nz}} \\subset \\mathbb{R}^{D_z}$ provide about a vector $x \\in \\mathbb{R}^D$. In particular, this operator returns a convex combination of the value vectors $v(z_n)$:\n$\\mathcal{H}(Z, x) = \\frac{\\sum_{n=1}^{N_z} \\alpha(z_n, x)v(z_n)}{\\sum_{n=1}^{N_z} \\alpha(z_n, x)} \\in \\mathbb{R}^D, $\nwhere $Z \\triangleq [z_1, ..., z_{Nz}] \\in \\mathbb{R}^{D_z \\times N_z}$ and $\\alpha(z, x) \\geq 0$ are the so-called (unnormalized) attention weights. The value vectors are provided by the learnable function $v : \\mathbb{R}^{D_z} \\rightarrow \\mathbb{R}^D$, which is normally linear. Vector $v(z_n)$ encodes the informa-tion in $z_n$ that is relevant for the task at hand.\nThe attention weights are determined by the relation be-tween $x$ and the vectors in Z. The learnable function $\\alpha: \\mathbb{R}^{D_z} \\times \\mathbb{R}^D \\rightarrow \\mathbb{R_+}$ can be thought of as quantifying the similarity between z and x. In this way, if x is very similar to $z_{n_0}$ for some $n_0$ and dissimilar to the remaining reference vectors, then $\\alpha(z_{n_0}, x)$ will dominate and $\\mathcal{H}(Z, x) \\approx v(z_{n_0})$. Usually, $\\alpha$ is the so-called inner-product attention function$\\footnote{A factor of 1/$\\sqrt{D}$ is often explicitly included inside the exponential but it is absorbed here into either k or q to simplify notation.}$\n$\\alpha(z,x) = \\exp[k(z)^Tq(x)]$, where $k : \\mathbb{R}^{D_z} \\rightarrow \\mathbb{R}^D$ and $q : \\mathbb{R}^{D} \\rightarrow \\mathbb{R}^D$ are (typically linear) learnable functions that respectively return the so-called key and query vectors. One can think of $k(z)$ as a vector that encodes the information in z and of $q(x)$ as a vector that encodes the information relevant to x. Thereby, $\\alpha(z, x)$ captures how relevant z is to x.\nBy letting$\\footnote{Matrices in the literature on transformers are the result of transposing the matrices here. We adopt the common notation in our community.}$\n$V \\triangleq [v(z_1), ..., v(z_{Nz})]$ and $K \\triangleq [k(z_1), ..., k(z_{Nz})]$, expression (1) becomes\n$\\mathcal{H}(Z,x) = V \\,\\textrm{softmax}(K^Tq(x))$.\nThe cross-attention operator $\\mathcal{H}$ can be extended to matrices of input vectors $X \\triangleq [x_1, ..., x_{Nm}]$. In this case, $\\mathcal{H}(Z, X)$ is a matrix whose n-th column is given by $\\mathcal{H}(Z,x_n)$ or, equivalently $\\mathcal{H}(Z,X) = V \\,\\textrm{softmax}(K^TQ)$, where $Q \\triangleq [q(x_1), ..., q(x_{Nm})]$. The self-attention operator is the special case where $Z = X$. It will be denoted as $\\mathcal{H}(X) \\triangleq \\mathcal{H}(X, X)$."}, {"title": "B. Transformers", "content": "Transformers [23] are in essence feed-forward deep neural networks that involve attention heads. The original architec-ture, here presented with some simplifications, is the composi-tion of attention blocks. To introduce what an attention block is, define the layer normalization operator (see references in [24]) as $l(x) = a \\frac{x - x_{\\textrm{mean}}}{x_{\\textrm{std}}} + b$, where a and b are learnable parameters and $x_{\\textrm{mean}}$ and $x_{\\textrm{std}}$ are the sample mean and sample standard deviation of the entries of x. When applied to a matrix, l operates column-wise.\nWith this notation, an attention block B is described by:\n$X' \\triangleq X + \\mathcal{H}(l_1(X))$\n$\\mathcal{B}(X) = X' + f_{\\textrm{MLP}}(l_2(X'))$,\nwhere $f_{\\textrm{MLP}}$ is a multi-layer perceptron applied separately to each column, and functions $l_1$ and $l_2$ implement layer normalization without sharing their a and b parameters."}, {"title": "IV. ATTENTION BASED RME", "content": "Transformers were originally proposed in the context of natural language processing (NLP). Subsequently, they were adapted to image processing [24]. To the best of our knowl-edge, they have not been applied to the interpolation of measurements collected across space. However, this paper shows that this is not only possible but it also results in an elegant and effective radio map estimator. Sec. VI will show that it actually beats the state-of-the-art in RME.\nA simple possibility to design a transformer-based RME estimator would be to consider the existing DNN-based es-timators and replace the CNN therein with a vision trans-former [24]. However, this would suffer from the limitations of these estimators; cf. Sec. I. Instead, an alternative route is taken here, which proceeds by adopting a somehow abstract perspective. In particular, note from the problem formulation in Sec. II that any estimator of $\\varphi(r)$ given the data ${(r_n, \\eta_n)}_{n=1}^N$ is a function of the form $\\hat{\\varphi}(r; {(r_n,\\eta_n)}_{n=1}^N)$.\nNote that two desirable properties for any estimator like this are (i) that it is invariant to permutations of the measurements and (ii) that it accommodates an arbitrary N. Since the cross-attention operator $\\mathcal{H}$ satisfies these two properties, one could think of constructing the feature vectors $z_n = [\\eta_n,r_n^T]^T$ and estimate the map as $\\hat{\\varphi}(r) = \\mathcal{H}(Z,r)$, where $Z \\triangleq [z_1, ..., z_N]$. This could be composed with other layers to form a deep network. Unfortunately, such an approach does not satisfy desirable invariance properties, as discussed next."}, {"title": "A. Feature Design", "content": "Maxwell's equations dictate that RME exhibits certain in-variance properties, such as translation and rotation invariance. Note that if the estimation of a map as a whole is considered rather than the estimation of a map at a single location r, these invariances become equivariances. If the invariances of an problem are not imposed by the estimator architecture, they must be learned from data, which increases drastically the amount of training data required to attain a target performance. This motivates a feature design that enforces these invariances.\nTranslation invariance means that translating the coordinate system shall not change the estimate of $\\varphi(r)$. To impose this invariance, one can replace the feature vectors with translation-invariant features, for example $[\\eta_n, (r_n - r)^T]^T$. Since this would also translate the second input of $\\mathcal{H}$ to the origin (r - r = 0), it is more appropriate to use self-attention. This means that $\\hat{\\varphi}(r)$ can be taken to be a function of $\\mathcal{H}(X)$, or even $\\mathcal{B}(X)$, where $x_n = [\\eta_n, (r_n - r)^T]^T$.\nSimilarly, rotation invariance means that the estimate of $\\varphi(r)$ shall not change if the coordinate system is rotated. To accommodate this invariance, the centered measurement locations ${r_n - r}_{n=1}^N$ will be suitably rotated, which means that the feature vectors become $x_n = [\\eta_n, U(r_n - r)^T]^T$, where U is a rotation matrix. This rotation is defined by one angle if d = 2 and by two angles if d = 3. Note that rotating all locations by a certain angle amounts to rotating the coordinate system by the opposite angle. Thus, one can define a rotation by the direction in which the x-axis points after the rotation. One possibility is to choose the direction of a specific measurement location, for instance the one corresponding to the largest $\\eta_n$. However, this means that a small change in the measurements could result in a large change in the rotation angle if the index of the strongest measurement changes. Since this would render the estimator unstable, a more robust approach is adopted here, where the x-axis is rotated so that it points in the direction of\n$\\sum_{n=1}^N \\exp(\\eta_n) (r_n - r)$.\nFinally, besides imposing invariances, a suitable feature design can also facilitate learning. For this reason, other fea-tures can be appended to the aforementioned feature vectors. For example, one can concatenate the cylindrical or spherical coordinates of $U(r_n - r)$ and even the sines and cosines of the resulting angular coordinates."}, {"title": "B. Dataset Preparation", "content": "The data to train and test an RME estimator typically consists of a collection of measurements collected in multiple geographical areas along with their locations. Using this data, one needs to generate sets of training and testing examples. Since the proposed estimator is gridless, the procedure differs from the one used in existing DNN estimators.\nIn particular, one can proceed as follows to generate the t-th example. First, select one of the geographical areas. Among the measurements in this area, choose one as the target and $N[t]$ of them as the input. As discussed later, the value of $N[t]$ must be selected depending on the training or testing goals. With the $N[t]$ measurements, their locations, and the location of the target measurement, construct the feature matrix $X[t]$ as indicated in Sec. IV-A. By denoting the target measurement as $\\eta[t]$, the dataset can be expressed as ${(X[t], \\eta[t])}_{t=1}^T$"}, {"title": "C. Architecture", "content": "The proposed estimator adopts a transformer architecture and it comprises the blocks in the shaded area of Fig. 2. The rest of the blocks are used for active sensing and are described later. The feature vectors $x_n$ are first passed separately through a linear layer L that increases their dimension to D. Then, a composition of attention blocks is applied. The input and output of every block are vectors of dimension D. Finally, the output vectors of the last attention block are passed through a linear layer that reduces their dimension to 1. The returned N scalars can be collected in the vector $F(X) \\triangleq [F_1(X), ..., F_N(X)]$. To obtain an estimate $\\hat{\\varphi}(r)$, these N scalars can be reduced into a single one, e.g. by averaging. One can then train by minimizing the mean square error:\n$\\frac{1}{T} \\sum_{t=1}^T \\frac{1}{N[t]} (\\eta[t] - \\frac{1}{N[t]} \\sum_{n=1}^{N[t]} F_n(X[t]))^2$.\nThe limitation of this approach is that examples for all the necessary values of N[t] need to be included in the dataset. This may result in an unnecessarily very large dataset. To alleviate this issue, causal self-attention is commonly used in the context of transformers. As discussed later, it is not fully suitable for RME, but the training time reduction may pay off. A causal self-attention head $H_c$ is similar to the self-attention head introduced in Sec. III-A, but the n-th output vector is only allowed to depend on the input vectors ${x_{n'}}_{i=1}^n$. In other words, the n-th column of the matrix $H_c(X)$ is $H(X_n,x_n)$, where $X_n \\triangleq [x_1, ..., x_n]$. An attention block that uses a causal self-attention head will be denoted as $B_c$.\nThus, if one replaces all attention heads in the aforemen-tioned architecture with causal self-attention heads, one can train the network so that $F_n(X)$ is an estimate of $\\varphi(r)$ given the first n measurements. This can be achieved with the loss\n$\\frac{1}{TN} \\sum_{t=1}^T \\sum_{n=1}^N (\\eta[t] - F_n(X[t]))^2$,\nwhere now $N[t] = N$ for all t. The main limitation of this approach is the loss of the invariance to the permutation of the measurements. This is not a problem in NLP since the tokens in a word are ordered, but it is a problem in RME since the measurements are not ordered. However, this is the price to be paid for the reduction in training time."}, {"title": "V. ATTENTION-BASED ACTIVE SENSING", "content": "The estimator proposed in Sec. IV will be referred to as STORM and will be extended next to the active sensing set-ting. The idea is that, given the measurements ${(r_n,\\eta_n)}_{n=1}^N$, the candidate locations ${r_n}_{n=N+1}^{N+M}$, and a target location r, STORM must return not only an estimate of $\\varphi(r)$ but also quantify how informative a measurement at each of these candidate locations would be to improve this estimate.\nTo this end, process ${r_n}_{n=N+1}^{N+M}$ like the measurement locations, with the same translation and rotation as used to obtain X. This yields the matrix X, which has one row less than X since it does not contain the measurement values. Decomposing F into an encoder and a decoder part $F_D$ and $F_E$, the architecture of STORM is extended to obtain:\n$F(X) = F_D(F_E(X))$\n$C(X, \\mathcal{X}) = C_D(F_E(X), C_E(X))$.\nHere, $C(X, \\mathcal{X})$ is a vector with M entries, where the m-th entry is a scalar between 0 and 1 that quantifies how informative a measurement at $r_m$ would be to improve the estimate of $\\varphi(r)$. Its encoder and decoder parts are denoted as $C_E$ and $C_D$ and pictorially described by Fig. 2 (left).\nFor training, the measurements at the candidate locations are used. Thus, one can construct X and $\\mathcal{X}$ as above except that $\\mathcal{X}$ now contains M more columns with the candidate locations and measurements. Note that the candidate locations are not used in (4) to obtain the rotation angle. The causal self-attention operator used in $F_E$ and $F_D$ is modified as follows: The first N output columns coincide with the N output columns of $H_c$ from Sec. IV-C. In turn, for $m > N$, the m-th output column is given by $H([X_N,X_m], x_m)$. This allows one to use the m-th output column of F(X) as an estimate of $\\varphi(r)$ given the N measurements ${(r_n, \\eta_n)}_{n=1}^N$ as well as the measurement at $r_m$, but not the measurements at other candidate locations. This is therefore the estimate in the next step of the active sensing process if the m-th candidate location is selected. The outputs $F_{N+1}(X), ..., F_{N+M}(X)$ will be therefore referred to as the candidate estimates. The attention block that results from this modification will be denoted as $B_c$. The overall training architecture of STORM for active sensing is presented on the right side of Fig. 2.\nTo train STORM to predict the quality of each candidate estimate without using the measurements at the candidate locations, the idea here is to construct a combined estimate as a convex combination of the candidate estimates. The weights in this convex combination are the entries of $C(X, \\mathcal{X})$. The loss becomes:\n$\\frac{1}{T} \\sum_{t=1}^T \\frac{1}{2} (\\eta[t] - F_n(X[t]))^2 \\+\\frac{1}{2} \\left[ \\eta[t] - \\sum_{m=1}^M C_m(X_N[t], X[t])F_{N+m}(X[t])\\right]^2$.\nNote that this loss also promotes good estimation performance, since it is desirable that the same network can be used both for estimation and for selecting the next measurement location, rather than using a dedicated transformer for each task."}, {"title": "VI. EXPERIMENTS WITH SYNTHETIC AND REAL DATA", "content": "This section evaluates the performance of STORM on three datasets. When it comes to map estimation error, STORM is compared with three non-DNN and four DNN estimators. The non-DNN estimators include K-nearest neighbors (K-NN), Kriging [7], [8], and kernel ridge regression (KRR); see [6] and references therein. These estimators are trained as described in [22]. The compared DNN estimators include [DNN 1] the completion autoencoder in [15], [DNN 2] the U-Net from [14], [DNN 3] the U-net from [13], and [DNN 4] the autoencoder in [8]. Unless stated otherwise, STORM uses multi-head attention with 2 heads and embedding dimension D = 48, which results in around 100 k parameters.\nEach of the considered datasets consists of several measure-ment sets (MSs). Each one is a set of measurements collected in an $L_x \\times L_y$ rectangular environment. Each MS corresponds to a different function $\\gamma$. These MSs are split into training and testing MSs. The training measurements are obtained by selecting the measurements inside randomly drawn $L \\times L$ square patches inside a randomly selected MS. Likewise for the testing measurements. To favor the competing grid-aware DNN estimators, the aforementioned square patches are aligned with the grid in which the measurements are collected. Worse performance of these estimators is expected otherwise. The grid spacing is denoted as $\\Delta$.\nAt each Monte Carlo iteration, the measurements ${(r_n, \\eta_n)}_{n=1}^N$ inside a patch are split into two subsets by partitioning the index set $\\tilde{N} \\triangleq {1,2,..., \\tilde{N}}$ into $N_{\\textrm{obs}}$ and $N_{\\textrm{nobs}}$, that is, $N_{\\textrm{obs}} \\cup N_{\\textrm{nobs}} = \\tilde{N}$ and $N_{\\textrm{obs}} \\cap N_{\\textrm{nobs}} = \\emptyset$. The cardinality N = $|N_{\\textrm{obs}}|$ is fixed and presented on the horizontal axis of the figures. The measurements with indices in $N_{\\textrm{obs}}$ are passed to each estimator and the returned map estimate $\\hat{\\varphi}$ is evaluated at the locations ${r_n}_{n \\in N_{\\textrm{nobs}}}$. The root mean square error (RMSE) is then defined as\n$\\textrm{RMSE} = \\mathbb{E} \\sqrt{ \\frac{1}{N_{\\textrm{nobs}}} \\sum_{n \\in N_{\\textrm{nobs}}} [ \\hat{\\varphi}(r_n) - \\varphi(r_n) ]^2 }$,\nwhere the expectation is over patches and realizations of $N_{\\textrm{obs}}$.\nThe first dataset in this paper is generated by Remcom's Wireless InSite ray-tracing software using a 3D model of an area in downtown Rosslyn, Virginia, with $L_x \\approx L_y \\approx 700 m. Each MS corresponds to a different transmitter location. Fig. 3 shows the RMSE vs. N. It is observed that STORM outperforms all other benchmarks for all N despite the fact that the complexity of most competitors is significantly higher.\nThe second experiment uses the USRP dataset collected in [25]. Each MS has $L_x \\approx L_y \\approx 53 m and consists of approximately 12000 measurements. Fig. 4 shows the RMSE vs. N. It can be again observed that STORM outperforms all benchmarks. Kriging is the second best estimator, but recall that its complexity per point estimate is cubic in N, whereas the complexity of STORM is quadratic.\nThe third experiment relies on the 4G dataset from [22]. The transmitters are the base stations deployed by a cellular operator in a real-world 4G network. Each MS is collected in a different rectangular area with $L_x = 252 m$ and $L_y = 260 m. Fig. 5 shows the RMSE vs. N for this dataset. Once more, STORM offers the best estimation performance. Despite the fact that STORM is trained with N = 100, it still performs well at 120 measurements, which corroborates the value of the considered causal attention blocks.\nThe last experiment quantifies the performance of STORM when it comes to active sensing. For each patch, N mea-surements, one evaluation location r, and the remaining M = N-N measurements are passed to STORM. The measurement with the greatest value of the quality predictor $C_m$ is then also given to STORM, which provides a refined estimate of $\\gamma(r)$. This estimate is used to compute the RMSE and compared with the RMSE that results from selecting the additional measurement uniformly at random among the candidate locations. Fig. 6 shows the RMSE vs. N. It is observed that choosing the next measurement as dictated by STORM leads to a significant improvement in the RMSE in both the ray-tracing and USRP datasets. The case of the 4G dataset is also similar but omitted due to lack of space."}, {"title": "VII. CONCLUSIONS", "content": "This paper proposed STORM, a transformer network for radio map estimation. This estimator operates in a gridless fashion, which circumvents many of the limitations of existing DNN-based estimators. It is also seen to outperform the state-of-the-art estimators in three datasets. An extension of STORM to active sensing was also proposed and seen to yield satisfactory results."}]}