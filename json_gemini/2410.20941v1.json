{"title": "Instruction-Tuned LLMs Succeed in Document-Level MT Without Fine-Tuning\u2014But BLEU Turns a Blind Eye", "authors": ["Yirong Sun", "Dawei Zhu", "Yanjun Chen", "Erjia Xiao", "Xinghao Chen", "Xiaoyu Shen"], "abstract": "Large language models (LLMs) have excelled in various NLP tasks, including machine translation (MT), yet most studies focus on sentence-level translation. This work investigates the inherent capability of instruction-tuned LLMS for document-level translation (docMT). Unlike prior approaches that require specialized techniques, we evaluate LLMs by directly prompting them to translate entire documents in a single pass. Our results show that this method improves translation quality compared to translating sentences separately, even without document-level fine-tuning. However, this advantage is not reflected in BLEU scores, which often favor sentence-based translations. We propose using the LLM-as-a-judge paradigm for evaluation, where GPT-4 is used to assess document coherence, accuracy, and fluency in a more nuanced way than n-gram-based metrics. Overall, our work demonstrates that instruction-tuned LLMs can effectively leverage document context for translation. However, we caution against using BLEU scores for evaluating docMT, as they often provide misleading outcomes, failing to capture the quality of document-level translation.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of natural language processing tasks (Radford et al., 2019; Brown et al., 2020; Touvron et al., 2023; Dubey et al., 2024). In the realm of machine translation (MT), recent findings also suggest that LLM-based models rival dedicated commercial systems like Google Translate, particularly in translating high-resource languages (Hendy et al., 2023; Peng et al., 2023; Jiao et al., 2023; Zhu et al., 2024a,b). Nonetheless, most research has focused only on sentence-level translation. While some studies have begun to explore document-level translation (docMT) with LLMs, there is a prevailing belief that directly applying instruction-tuned LLMs to docMT performs poorly without specialized training and prompting techniques, largely due to the limited availability of document-level content in instruction-tuning datasets (Wu et al., 2024; Cui et al., 2024; Li et al., 2024). However, their conclusions are frequently drawn from n-gram-based metrics without thorough analysis to substantiate the models' true performance.\nIn this work, we conduct an in-depth investigation into the inherent capabilities of instruction-tuned LLMs in handling docMT tasks. Unlike previous studies that explore special tricks, such as multi-turn inference (Wang et al., 2023), we directly prompt LLMs to translate entire documents in a single pass. Comparing this method to a simpler baseline that translates individual sentences separately and then stitches them together, we can evaluate whether instruction-tuned LLMs can leverage their inherent ability to incorporate document-level context and improve translation quality.\nA key challenge in our research is the evaluation of document-level machine translation (docMT). Traditional metrics like BLEU, ChrF, and TER (Papineni et al., 2002; Popovi\u0107, 2015; Snover et al., 2006), though widely used, often poorly correlate with human judgment (Freitag et al., 2022), especially in docMT, where maintaining coherence and logical flow across a document is essential\u2014something n-gram overlap struggles to capture. Metrics like CTT, AZPT, and BLONDE (Jiang et al., 2021; Wang et al., 2023) address specific aspects such as terminology consistency and zero-pronoun accuracy, but still rely heavily on word matching and symbolic statistics. We argue that an ideal docMT metric should be (1) context-aware\u2014capturing document-level coherence and accuracy, (2) structured\u2014evaluating aspects such"}, {"title": "2 Problem Settings", "content": "Given a document containing $l$ source sentences $X = \\{x^1,\\ldots,x^l\\}$, the goal of docMT is to generate its translation $Y = \\{y^1,\\ldots, y^l\\}$ as a sequence of sentences in the target language. In this work, we explore two approaches for generating translations using instruction-tuned LLMs:\n\u2022 ST[k]: We concatenate $k$ source sentences into a chunk, input each chunk into the LLM for translation, and then concatenate the translated chunks together to form the full document translation.\n\u2022 DOC: We instruct the LLM to directly translate the entire documented in one pass.\nThe DOC approach is designed to capture inter-sentence dependencies by considering the full document context, potentially leading to more coherent and accurate translations. However, it also requires understanding and generating longer text tokens, which increases the risk of cumulative errors if LLMs are not specialized into document-level translation yet."}, {"title": "3 BLEU-based Evaluation", "content": "Document-level BLEU (d-BLEU, Liu et al., 2020) is widely used for evaluating translations in DocMT. However, we notice that it is sensitive to overly lengthy generation, which can be problematic as LLMs sometimes overgenerate. We find that even minor overgeneration can significantly affect the final d-BLEU score. We argue that documents are generally independent units, so they should be weighted equally in the evaluation. We, therefore, propose an alternative, AvgBLEU, defined as:\n$\\text{AvgBLEU} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{BLEU} \\left(Y_{\\text{ref}}^{i}, Y_{\\text{pred}}^{i}\\right)$\nHere, $N$ is the number of documents, and $Y_{\\text{ref}}$ and $Y_{\\text{pred}}$ represent the reference document translations and the predicted translations, respectively. This allows us to calculate the average BLEU score (AvgBLEU) for the entire dataset, providing a comprehensive measure of translation quality."}, {"title": "4 LLM-as-a-judge Evaluation", "content": "Maruf et al. (2021) outlines various discourse phenomena that should be considered when evaluating document-level translations, such as cohesion and the use of discourse connectives. In the past, automatic evaluation of these aspects was difficult due to the need for deep semantic understanding, and evaluations typically focused on one aspect at a time using specialized test sets (Hardmeier and Federico, 2010; Gong et al., 2015; Jwalapuram et al., 2019). Inspired by the \"LLM-as-a-judge\" approach (Zheng et al., 2023), we aim to assess multiple aspects simultaneously using a strong LLM.\nEvaluation Setup. We design four (sub) metrics: (1) Fluency, (2) Content Errors (CE), (3) Lexical Cohesion Errors (LE), and (4) Grammatical Cohesion Errors (GE). All metrics are measured using prompts provided to GPT-4. See Appendix C for details on prompt design.\nFluency is rated on a scale of 1 to 5, with higher being better. Since fluency can be evaluated solely based on the translated text, we present only the model's outputs to GPT-4 for this assessment, decoupling fluency from metrics that require consideration of source and reference texts.\nContent Errors refer to translation mistakes such as mistranslations, omissions, or additions. We instruct GPT-4 (gpt-4-0613) to output a list containing all identified mistakes. The CE score is determined by the length of this list, and report the average CE score over the test set.\nCohesion Errors are further divided into two subcategories: lexical (LE) and grammatical (GE), which affect text connection and the logic flow, respectively. LE includes incorrect vocabulary usage, missing synonyms, or overuse of certain terms that disrupt the flow. GE includes pronouns, conjunctions, and sentence-linking structure mistakes. Similar to CE, we prompt GPT-4 to generate a list of identified errors, with the score corresponding to the length of the list.\nOther settings, such as translation directions and the models of interest, remain consistent with Section 3. Due to the cost associated with using GPT-4, we sample 70 documents per translation direction from the WMT22 dataset for our evaluation.\nResults. The results with en-zh are shown in Although ST3 scores higher than DOC on AvgBLEU, DOC consistently outperforms ST3 in Fluency. Additionally, DOC generally exhibits fewer CE, also known as content errors. For cohesion errors, the results are mixed: DOC shows better LE with vicuna-7B and its -16K version, and Mistral-7B, while Vicuna-13B and its -16K version yield higher LE. As for GE, DOC performs better with -16K models and Mistral-7B while others are mostly comparable. We also observe that the -16K versions perform similarly to their original counterparts in fluency but demonstrate notable improvements in CE reduction. This pattern is consistent across all translation directions, with full results provided in Appendix D. Overall, our approach enables a more detailed evaluation of"}, {"title": "5 Conclusion", "content": "In this work, we investigate the performance of instruction-tuned LLMs in document-level machine translation (docMT), comparing the translation of entire documents in a single pass to translating individual sentences that are then concatenated. Our findings show that translating entire documents yields better results, as the model can capture inter-sentence dependencies and maintain discourse coherence, even without explicit fine-tuning for docMT tasks. However, evaluating these improvements is challenging. Traditional metrics like BLEU fail to consider discourse-level phenomena, often favoring sentence-level translations and producing misleading results. To address this limitation, we propose the LLM-as-a-judge approach, utilizing GPT-4 to assess specific aspects of discourse through tailored prompts. This method enhances interpretability and can be adapted for evaluating translation quality in other domains."}, {"title": "Limitations", "content": "Translation Directions. We evaluate only high-resource language pairs, which limits the generalizability of our findings for low-resource languages. Due to data availability constraints, our experiments focus on well-resourced translation directions. Future research should explore whether instruction-tuned LLMs translating entire documents yield better results than translating sentences independently in low-resource languages.\nModel Size and Diversity. We focus exclusively on small-scale LLMs. Future work should investigate larger models to observe whether instruction-tuned LLMs continue to perform better in docMT, and whether BLEU would work.\nMax Length. A small fraction (~ 2%) of documents in WMT22, including both their source texts and translations, exceed 2048 tokens. Thus, we focus solely on samples within the model's context length (2048 tokens), as these instruction-tuned LLMs are primarily trained on text within this limit. In future work, we will evaluate LLMs with longer context lengths, examine -16K models, and investigate whether long conversation instruction-tuned will help and whether those phenomena persist when translating text that exceeds the models' context length.\nEthical Considerations\nOur study aims to investigate the docMT reliability of instruction-tuned LLMs without fine-tuning for docMT, concerned by the potential for accumulating errors during decoding, which may lead to increased hallucinations. We expect minimal social risks associated with our efforts."}, {"title": "A Evaluation Metrics Shortcoming Analysis", "content": "While COMET has been shown to provide more reliable evaluations than BLEU in many cases, it is primarily trained on sentence-level translations and, as such, is not well-suited for docMT. Given that COMET lacks specific training to capture the complexities of inter-sentence dependencies and discourse-level phenomena, it is not an ideal metric for evaluating the true capabilities of LLMs in docMT tasks. Therefore, in this work, we opted to explore more appropriate evaluation methods tailored to document-level translation challenges.\nSimilarly, metrics like ChrF (Popovi\u0107, 2015), ChrF2, and TER have made incremental progress by incorporating word-level matching mechanisms that extend beyond simple token overlap, but they still fundamentally rely on surface-level statistics. Like BLEU, these metrics do not adequately account for deeper discourse relationships, cohesion, and the broader context required for accurate docMT assessment. As a result, their limitations become more apparent when evaluating LLMs on longer texts, where capturing the overall document structure is essential.\nWhile metrics such as CTT and AZPT are designed to address specific issues like terminology consistency and zero pronoun accuracy, they remain grounded in automatic identified lexical alignment. These metrics operate under the assumption that the presence of specific terminology or pronouns directly correlates with translation quality. However, in practice, meaning can be conveyed in multiple ways without strictly adhering to these surface-level features. This makes CTT and AZPT limited in scope, as they are unable to fully assess translation quality when alternative phrasing or omitted pronouns still preserve meaning accurately.\nBlonde represents a more sophisticated approach by categorizing and analyzing discourse coherence using linguistic features such as verb tense (e.g., VBD for past tense verbs). While this is a step toward capturing discourse-level phenomena, Blonde is still constrained by symbolic statistical methods. Its reliance on predefined linguistic categories means that it struggles to account for the full range of discourse phenomena that can arise in real-world documents. As a result, these metrics, despite their improvements, remain insufficient for capturing the nuances of document-level translation in its entirety."}, {"title": "B d-BLEU Performance", "content": "We observe that the trend in Table 5 remains consistent with Table 2, and the BLEU score shows an even stronger preference for translations that are processed separately and concatenated. It is worth to Notice that the red data point in Table 5 is influenced by the sensitivity of BLEU, where a certain generated translation contains a long-repeated incorrect token toward the end, thus lowering the overall score. When calculating the BLEU score for this sample, we find that the document receives a score near zero, despite the fact that the earlier part of the translation is mostly accurate. This sensitivity is one of the reasons why BLEU should not be used in docMT."}, {"title": "C GPT4-as-a-judge Evaluation Prompts", "content": "C.1 Fluency\nFluency refers to the naturalness and smoothness of a text in the target language, without awkward or unnatural phrasing. In machine translation evaluation, fluency is crucial for assessing the readability and linguistic quality of the output, which is often not fully captured by traditional metrics like BLEU. While BLEU focuses on n-gram overlap between the translation and reference text, it does not directly evaluate how natural the translation sounds or whether it adheres to syntactic rules. Fluency, in contrast, provides a more nuanced evaluation of the model's ability to produce human-like text.\nIn this task, we assess fluency on a scale of 1 to 5, with higher scores indicating more fluent translations. Evaluators are instructed to analyze the text and assign a score based solely on the naturalness and grammatical correctness of the model's output. Importantly, the fluency evaluation is conducted in isolation, decoupled from cohesion, with only inference text input, to ensure a clear focus on the text's immediate readability. Cohesion, which refers to the grammatical and lexical connectivity between text units (Halliday and Hasan, 2014), is considered separately to avoid confounding the two metrics, as fluency and cohesion could be correlated, as it is common sense that if a text is cohesive, its flow is naturally better. See the correlation heatmaps like Figure 1 which show that our prompt design successfully decouples these two metrics.\nThe evaluation is supported by specific examples and justifications for the assigned score. Below is the prompt used to guide the evaluation:\n\u2022 Task: Evaluate the fluency of the text.\n\u2022 Scoring: Provide a score from 1 to 5, where:\n5: The text is highly fluent, with no grammatical errors, unnatural wording, or stiff syntax.\n4: The text is mostly fluent, with minor errors that do not impede understanding.\n3: The text is moderately fluent, with noticeable errors that may slightly affect comprehension.\n2: The text has low fluency, with frequent errors that hinder understanding.\n1: The text is not fluent, with severe errors that make it difficult to understand.\n\u2022 Explanation: Support your score with specific examples to justify your evaluation."}, {"title": "C.2 Content Errors", "content": "Unlike fluency, which assesses the naturalness and grammatical correctness of the output, accuracy focuses on the semantic alignment between the translated text and the original reference. The evaluator's task is to identify and categorize errors that affect the translation's fidelity, such as mistranslations, omissions, or additions.\nRather than relying on simple n-gram matching, the evaluation emphasizes meaning preservation. The evaluator compares the translation with the reference text, identifying instances where the translation deviates in meaning. However, if the translated text conveys the same information as the reference but uses different words or phrasing, it is not considered an error, since we suspect that this phenomenon could happen in LLMs in document-level translation task. This approach ensures that the model's output is evaluated based on its ability to faithfully represent the source content, capturing specific issues like mistranslations or information loss, and ensuring semantic integrity. The accuracy evaluation prompt is structured as follows:\nPlease evaluate the accuracy of the following text by comparing it to the reference text provided.\n\u2022 Task: Compare the text to the reference text.\n\u2022 Identify Mistakes: List all mistakes related to accuracy.\nMistake Types:\n* Wrong Translation: Incorrect meaning or misinterpretation leading to wrong information.\n* Omission: Missing words, phrases, or information present in the reference text.\n* Addition: Extra words, phrases, or information not present in the reference text.\n* Others: Mistakes that are hard to define or categorize.\n\u2022 Note: If the text expresses the same information as the reference text but uses different words or phrasing, it is not considered a mistake.\n\u2022 Provide a List: Summarize all mistakes without repeating the exact sentences. Provide an empty list if there are no mistakes."}, {"title": "C.3 Cohesion Errors", "content": "Cohesion is a critical aspect of machine translation evaluation as it ensures that the various parts of the text are well-connected and that the overall flow is logical. Unlike metrics such as fluency or accuracy, cohesion specifically examines how sentences are linked together through lexical (lexical cohesion) and grammatical (grammatical cohesion) means (Maruf et al., 2021). This is particularly important in document-level translation, where the consistency of vocabulary and the logical connection of grammatical structures across a longer text are challenging for models to maintain.\nIn the context of translations produced using the ST3 and DOC paradigms, evaluating cohesion allows us to assess whether the model effectively leverages contextual information to maintain consistency across the text. By decoupling cohesion from fluency, our evaluation framework enables evaluators to focus specifically on identifying lexical cohesion mistakes such as incorrect vocabulary usage, missing synonyms, or overuse of certain terms that disrupt the flow and grammatical cohesion mistakes such as errors in pronouns, conjunctions, or sentence-linking structures.\nThe evaluator is asked to identify any mistakes related to cohesion and categorize them as either lexical or grammatical cohesion issues. The evaluation prompt is structured as follows:\nPlease evaluate the cohesion of the following text by comparing it to the reference text.\n\u2022 Task: Evaluate the cohesion of the text.\n\u2022 Definition: Cohesion refers to how different parts of a text are connected using language structures like grammar and vocabulary. It ensures that sentences flow smoothly and the text makes sense as a whole.\n\u2022 Identify Mistakes: List all mistakes related to cohesion.\nLexical Cohesion Mistakes: Issues with vocabulary usage, incorrect or missing synonyms, or overuse of certain words that disrupt the flow.\nGrammatical Cohesion Mistakes: Problems with pronouns, conjunctions, or grammatical structures that link sentences and clauses.\n\u2022 Provide Lists: Provide separate lists for lexical cohesion mistakes and grammatical cohesion mistakes. Provide empty lists if there are no mistakes."}]}