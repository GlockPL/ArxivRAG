{"title": "DriveGazen: Event-Based Driving Status Recognition using Conventional Camera", "authors": ["Xiaoyin Yang"], "abstract": "We introduce a wearable driving status recognition device and our open-source dataset, along with a new real-time method robust to changes in lighting conditions for identifying driving status from eye observations of drivers. The core of our method is generating event frames from conventional intensity frames, and the other is a newly designed Attention Driving State Network (ADSN). Compared to event cameras, conventional cameras offer complete information and lower hardware costs, enabling captured frames to encode rich spatial information. However, these textures lack temporal information, posing challenges in effectively identifying driving status. DriveGazen addresses this issue from three perspectives. First, we utilize video frames to generate realistic synthetic dynamic vision sensor (DVS) events. Second, we adopt a spiking neural network to decode pertinent temporal information. Lastly, ADSN extracts crucial spatial cues from corresponding intensity frames and conveys spatial attention to convolutional spiking layers during both training and inference through a novel guide attention module to guide the feature learning and feature enhancement of the event frame. We specifically collected the Driving Status (DriveGaze) dataset to demonstrate the effectiveness of our approach. Additionally, we validate the superiority of the DriveGazen on the Single-eye Event-based Emotion (SEE) dataset. To the best of our knowledge, our method is the first to utilize guide attention spiking neural networks and eye-based event frames generated from conventional cameras for driving status recognition.", "sections": [{"title": "Introduction", "content": "Driver state and behavior are crucial to traffic safety. Factors such as the driver's attention level, driving condition, and fatigue directly impact their perception and response to road situations. Developing effective technologies to identify and monitor driver states has become a significant research direction in traffic safety. However, predicting driver states from conventional RGB images is a challenging task; spatial and temporal cues from driving conditions can be adversely affected by head posture and partial occlusion. Existing facial recognition models for classifying driving states in RGB frames are built on complex CNN-based models, such as ResNet 50, Transformer, and Inception-based methods. Different lighting conditions and fast user movements make driver state recognition more complicated, and despite cumbersome large network enhancement modules, driver state recognition from RGB images remains difficult and fragile. We will introduce a novel wearable driver state recognition prototype where users only need to wear a pair of glasses (DG3). Mobile wearable devices can provide stronger feature capture under rapid head movements while offering high resolution for capturing more spatial features and higher temporal resolution for capturing more temporal features. Even though this device provides a stable fixed view of both eyes and traditional camera technology is mature and low-cost, estimating driver states from eye features still faces unique challenges. A key issue is that traditional cameras cannot effectively resolve temporal information under limited lighting conditions. These temporal features are not only crucial for driver state recognition but also important for inferring more informative spatial features. For example, while the eye sockets are major spatial cues, they provide less information for driving state classification. In contrast, subtle movements related to facial units, such as raising the outer eyebrows and squinting, provide stronger cues for eye-based driving state recognition. To address these challenges, we designed the DriveGazen method, which first generates realistic synthetic dynamic visual sensor (DVS) events from video frames and employs the Attention Driving State Network (ADSN) to combine the best features of events and intensity frames, guiding asynchronous event-based driver state recognition with spatial texture cues from the corresponding intensity frames. To train our lightweight eye-based driving state network (ADSN) and stimulate research on event-based eye driver state recognition, we collected a new eye-based event driving state (DriveGaze) dataset. We validated our method on the DriveGaze dataset and demonstrated state-of-the-art driver state recognition capabilities, achieving a significant improvement of 3% in both WAR and UAR compared to the second-best method.\nSpecifically, our work makes the following five contributions:\n\u2022 A novel real-time driver state recognition method based on low-cost conventional camera;"}, {"title": "Related Work", "content": "We focus on measuring driving status and recognition. Then, we explain the spiking neural network mainly used.\nDriving Status Sensing Methods. Researchers utilize physiological measurements such as electroencephalography (EEG), functional near-infrared spectroscopy (fNIRS), electrocardiography (ECG), electrodermal activity (EDA), and respiration (RESP) to identify driver states. For example, changes in states can lead to variations in facial temperature, hence there are studies employing facial infrared thermal imaging techniques for identification purposes. Additionally, some research examines driver states by collecting their hormone levels. Apart from physiological measurements, behavioral measurements are also employed to gauge driver states. Some studies utilize near-infrared (NIR) facial expression recognition methods to identify driver states more accurately. Simultaneously, by analyzing dialogues between drivers and in-vehicle information systems, researchers have found that using voice recognition can identify various states. Moreover, there are studies that identify driver states through posture movements, validating the feasibility of using radio frequency (RF) technology for state recognition. In addition to physiological and behavioral measurements, some studies propose collecting information about driver, vehicle status, and changes in the surrounding environment to infer the driver's state. For instance, based on the pressure characteristics of the throttle and brake pedals, classify the driver's happy and unhappy states. Furthermore, utilizing inertial measurement units (IMUs) to detect driver states is also a common method. Additionally, some studies use self-report scales to measure driver states, such as Positive and Negative Affect Schedule (PANAS), Self-Assessment Manikin (SAM), and Differential Emotions Scale (DES).\nDriving Status Recognition Algorithms. Researchers typically employ supervised machine learning for implementation. Lee et al. successfully classified three driving states based on PPG, EMG, and IMU signals using SVM, achieving an accuracy of 99.52%. Ooi et al. and Gao et al. utilized SVM to classify driver states based on EDA and FEA signals, achieving an accuracy of 85%. Other SVM-based development algorithms are also frequently used for state recognition. For example, Wan et al. used Least Squares Support Vector Machine (LS-SVM) to detect states based on multimodal signals. Another commonly used algorithm is k-Nearest Neighbors (kNN); Raja et al. employed this method to classify anger and neutral states. Nor and Wahab used Multi-Layer Perceptron (MLP) to recognize driver states based on velocity and accelerator pedal position. Other traditional machine learning algorithms (such as Bayesian networks) are also used for state recognition. Deep learning algorithms have also been successfully implemented in driver state recognition. Lee et al. collected near-infrared and thermal image data of driver's faces and used Convolutional Neural Networks (CNN) to classify driver's anger and neutral states, achieving a recognition accuracy of 99.96%. Although detection accuracy in various studies sometimes reaches 99.96%, most studies are conducted on different datasets. Different recognition tasks, data collection methods, and even different expressions of the same state category can all affect recognition accuracy in driver state detection.\nSpiking Neural Network Unlike artificial neural networks (ANN) that are purely digitally coded and whose input and output are numerical values, spiking neural networks (SNN) simulate biological processes, include the concept of time, and only exchange information (pulse), with input and output being pulse sequences. SNN describe the properties of units in the nervous system with varying degrees of detail. SNN simulate three states of biological neurons: resting, depolarized, and hyperpolarized. When a neuron is in a resting state, its membrane potential remains constant and is usually set to 0. An increase in membrane potential is called depolarization; conversely, a decrease in membrane potential is hyperpolarization. When the membrane potential is above the potential threshold, an action potential, or pulse, is triggered, and a binary-valued pulse signal is used as output to transmit information between neurons. SNN are low-energy biomimetic methods that work in continuous time using discrete signals such as pulses. They can accept the sparsity found in biology and are compatible with high temporal resolution. SNN balance accuracy and computational feasibility. Existing facial driver state recognition methods can usually only identify the peak emotional state or a single driving state in the entire sequence, and are therefore not suitable for applications that require robust estimation of intermediate states. We introduce a lightweight guide attention driving state mothod (DriveGazen) that can effectively recognize various states using SNN. DriveGazen"}, {"title": "DriveGazen", "content": "The DriveGazen method first generates realistic synthetic dynamic visual sensor (DVS) events from video frames and utilizes the Attention Driving State Network (ADSN) to combine the best features of events and intensity frames, guiding asynchronous event-based driver state recognition with spatial texture cues from the corresponding intensity frames. Next, I will provide a detailed description of each part of the method."}, {"title": "Video to Event(v2e)", "content": "We convert RGB video into grayscale frames, where pixel values are treated as luma intensity values.  represents the sampled frame. We denote Y as the pixel's luma intensity value in a luma frame Y. Similarly, L represents the pixel's log intensity values in a log intensity frame L.\nStandard digital video represents intensity linearly, while DVS pixels detect changes in log intensity. For luma intensity values $Y < 20$ digital numbers (DN), we use a linear mapping from exposure value (intensity) to log intensity to reduce quantization noise in the synthetic DVS output.\nSince real DVS pixels have finite analog bandwidth, an optional low-pass filter is used to filter the input L value. The v2e model simulates this effect by making the filter bandwidth (BW) increase monotonically with the intensity value. Although the photoreceptor and source follower form a 2nd-order low-pass filter, one pole usually dominates, so the filter is implemented as an infinite impulse response (IIR) first-order low-pass filter. The nominal cutoff frequency is \u0192\u0437\u0430\u0432\u0442\u0430\u0445 for full white pixels. The filter's bandwidth is proportional to the luma intensity values Y. We denote the filtered L value as $L_{ip}$. To avoid nearly zero bandwidth for small DN pixels, an additive constant limits the minimum bandwidth to about 10% of the maximum value.\nWe assume the pixel has a memorized brightness value $L_{mem}$ in log intensity, and the new low-pass filtered brightness value is $L_{ip}$. The model then generates a signed integer quantity $N_{e}$ of positive ON or negative OFF events from the change $\u2206L = L_{lp} - L_{mem}$, where $N_{e} = [\u2206L/0]$. If $\u2206L$ is a multiple of the ON and OFF thresholds, multiple DVS events are generated. The memorized brightness value is updated by $N_{e}$ multiples of the threshold.\nDVS pixels emit spontaneous ON events called leak events, with a typical rate of approximately 0.1 Hz. These events are caused by junction leakage and parasitic photocurrent in the change detector reset switch. The v2e model adds these leak events by continuously decreasing the memorized brightness value $L_{mem}$. The leak rate varies according to random fluctuations in the event threshold, decorrelating leak events across different pixels.\nThe quantal nature of photons leads to shot noise: if, on average, K photons are accumulated in each integration period, then the average variance will also be K. At low light intensities, the effect of shot noise on DVS output events increases significantly, resulting in balanced ON and OFF shot noise events at rates above 1 Hz per pixel. The v2e model simulates temporal noise using a Poisson process. It generates ON and OFF temporal noise events to match a noise event rate $R_{n}$ (default 1 Hz). To model the increase in temporal noise with reduced intensity, the noise rate $R_{n}$ is multiplied by a linear function of luma $0 < Y < 1$, which reduces noise in brighter areas by a factor $0 < c < 1$ (default $c = 0.25$). This modified rater is multiplied by the time step $\u2206t$ to obtain the probability $p = r \u00d7 \u25b3t < 1$, which is applied to the next sample. For each sample, a uniformly distributed number in the range 0-1 is compared against two thresholds $[p, 1 \u2013 p]$ to determine if an ON or OFF noise event is generated. These noise events are added to the output and reset the pixels."}, {"title": "Attention Driving State Network(ADSN)", "content": "As illustrated in Figure 2, Specifically, ADSN includes spatial and temporal feature extractors and a guiding attention module. The spatial feature extractor achieves spatial feature extraction by decoupling the sequence length, extracting spatial information only from the first and last frames of the grayscale sequence. ADSN aggregates asynchronous events captured between two grayscale frames into n synchronized event frames. The core of the temporal feature extractor is the spiking neural layer (SNN), which makes decisions based on membrane potential to remember temporal information from previous event frames. Unlike RNN, SNN can learn temporal dependencies of arbitrary length without special handling. The guide attention module uses spatial cues from the spatial feature extractor to guide feature learning and enhancement of event frames. It also uses the spiking neural layer to transform spatial features $F_{s}$ into spikes $J_{s}$, which are then fused with the event frames. Next, I will provide a detailed description of each module of the network."}, {"title": "Spatial Feature Extractor S", "content": "As illustrated in Figure2, to decouple spatial feature extraction from grayscale sequences in terms of sequence length, the extractor only extracts spatial information from the first frame $I_{1}$ and the last frame $I_{n}$ of the sequence. This not only reduces the reliance on grayscale frames, but also improves recognition performance compared to using all frames, as shown in experiments. First, the two grayscale frames are concatenated in the channel dimension, and then the channel dimension is restored to the original number of channels through a 1 \u00d7 1 convolution kernel, in order to keep consistent with the convolution operation of the temporal feature extractor(Equation 6). Next, multi-scale spatial features are extracted and fused through a multi-scale self-attention module. This allows the network to learn small-scale action unit information and also consider the joint information of larger-scale action units while using residuals to reduce information loss(Equation 2). Then, two 3 \u00d7 3 convolution kernels further extract high-level features in the spatial dimension.\nIn order to better retain the temporal features extracted by the temporal feature extractor, we also use a convolution-spiking neural layer to convert the spatial features $F_{s}$ into a pulse form J(Equation 1), and add it to the temporal features to enhance feature discrimination. Formally, the spatial feature extractor can be defined as:\n$J_{s} = \u0424^{1} (F_{s})$\n(1)\n$F_{s} = C_{3}(C_{3}(\u00a7_{(3,5,7)} (I_{s}) +I_{s}))$\n(2)\n$\u00a7_{(i_{1},..., i_{n})}(\u00b7) := C_{1} ([w\u00ceC_{i_{1}} (\u00b7), . . ., w_{n}C_{i_{n}} (\u00b7)])$\n(3)\n$\u03c9_{1}, ..., \u03c9_{n} = \u03c3 ([Y(C_{i_{1}} (I_{s})), ..., Y(C_{i_{n}} (I_{s}))])$\n(4)\n$Y() := C_{1} ((C_{1}(A(\u00b7))))$\n(5)\n$I_{s} = C_{1}([I^{1}, I^{n}])$\n(6)\nwhere (.) denotes channel-wise concatenation; $C_{i}$ and $o$ denote an i \u00d7 i convolutional layer and a softmax function, respectively; $i_{1},..., i_{n}$ denote the value of i in the convolutional layer $C_{1}$ of the multi-scale self-attention module. According to the equation 2, the values here are 3, 5, 7 respectively. A denotes the adaptive average pooling layer; is a serial operation of a batch normalization operation and a ReLU activation function; $I_{t}$ is a spiking layer that keeps membrane potential from the previous time step, t - 1. The initial membrane potential, t = 0 (see Equation 15).Which only realizes the conversion from floating-point features to 0-1 pulse features without maintaining any temporal information."}, {"title": "Temporal Feature Extractor T", "content": "The core architecture of the temporal feature extractor is a spiking neural network. Spiking neurons output spike signals based on the accumulation, decay, and reset mechanism of membrane potential to capture the temporal trend in the input sequence. When the membrane potential exceeds a threshold, an action potential (i.e., a spike) is triggered and the membrane potential is reset. The triggering process itself is non-differentiable and cannot be trained by traditional stochastic gradient descent optimization methods. Instead, this paper adopts spatio-temporal backpropagation (STBP) and a convolution-spiking neural layer to circumvent this problem. The convolution-spiking neural layer uses a convolution-based layer for signal aggregation and a LIF-based spiking neural layer to manage the potential decay and reset process. This modification makes it possible to learn different accumulation strategies by leveraging convolution-based methods and allows spiking neurons to operate effectively in the temporal domain. The temporal feature extractor receives a total of n event frames as input, denoted as $E^{1}$ to $E^{n}$, and processes each frame in chronological order. Formally, after receiving the spatial features F, and pulse features $J_{s}$ from the spatial feature extractor, the temporal feature extraction process of $E^{t}$ can be expressed by equations 7 to 15:\n$O^{t} = M (\u03c4' (\u03c4' (J_{t}))) $\n(7)\n$J_{t} = P(J_{t}) \u2295 J_{s}$\n(8)\n$J_{t} = H (A(F_{s}, F_{t}))$\n(9)\n$F_{t} = C_{3}(\u0424^{2} (C_{3}(\u0424(\u00a7'_{(3,5,7)} (E^{t})))))$\n(10)\n$\u03c4(\u00b7) := \u03a6 (\u03a8(\u00b7))$\n(11)\n$\u00a7'_{(i_{1}, ..., i_{n})} (\u00b7) := C_{1} ([\u03c9\u00ceC_{i_{1}} (\u00b7), . . ., \u03c9_{n}C_{i_{n}} (\u00b7)])$\n(12)\n\u00a7' represents the same structure as the multi-scale self-attention module \u00a7 in the spatial feature extractor. \u03a8 is a fully connected layer; M extracts the membrane potential from the spiking neural layer, and A is the guide attention module. H is multi-head self-attention. It(\u00b7) is a spiking neural layer that records the previous spiking state $P^{t-1}$ and accumulated membrane potential $V^{t-1}$. Upon receiving a new input stimulus $X^{t}$, the membrane potential adjusts based on the previous pulse emission and accumulates the new stimulus. The spiking neural layer emits updated pulses $P^{t}$ and updates the membrane potential $V^{t}$ as follows:\n$P^{t} = h(V^{t} \u2013 \u0398)$\n(13)\n$V^{t} = \u03b1V^{t-1}(1 \u2013 P^{t-1}) + X^{t}$\n(14)\n$h(x) = \\begin{cases}0  &  x < 0 \\\\1 & x >= 0\\end{cases}$\n(15)\n$\u0398$ is the membrane potential threshold, set to 0.3 in this experiment. The parameter a is the attenuation factor for hyperpolarization. The potential $V^{t}$ is updated such that, for a spike at t - 1, the membrane potential resets to 0 by scaling 1 \u2013 Pt\u22121, with Xt as the corresponding input. Finally, the driving state is predicted based on the average value of Ot for t \u2208 [1, n], as defined in equation 16:\n$R = \u03c3(\\frac{1}{n} \u03a3 O_{t})$\n(16)\nwhere o is a Softmax activation function."}, {"title": "Guide Attention Module A", "content": "Due to the lack of reliable texture information in the event domain, relying solely on event information cannot generate an effective solution. Therefore, we utilize spatial features extracted from grayscale frames to inject rich texture clues into the temporal feature extractor. To guide grayscale frames to guide event frames from the spatial domain to the temporal domain, firstly, the spatial cues $F_{s}$ learned by the spatial feature extractor are passed through a channel attention module. The learned temporal attention mechanism scores are then allocated to the event frames to enhance the temporal clues. Further spatial attention learning is conducted on the enhanced grayscale frames, and then the spatial attention scores are allocated to the event frames. Finally, the strengthened temporal clues are obtained by adding them to the event frames. The operation process is defined as equations 17 to 20, and the design diagram of the guided attention module is shown in Figure 3.\n$F_{cp} = F \u2297 F_{p} + F$\n(17)\n$F_{p} = \u03c3(\u03c3(C_{1}(F_{c}))) $\n(18)\n$F_{c} = CP(A)$\n(19)\n$\u2206 = \u03c4(F_{s}) \u00b7 \u03c3(F_{t})$\n(20)"}, {"title": "Dataset", "content": "To the best of our knowledge, there currently does not exist a dataset for driving state recognition based on eye events captured by conventional cameras. To address the lack of training data for event-based driving state recognition, we collected a new event-based driving state dataset (DriveGaze);"}, {"title": "Assessment", "content": "Our algorithm is not only to remember the individual's \"peak\" state, but also to use time clues to distinguish the states of different phases. Therefore, the main goal of this experimental part is to identify any phase of the driving state. When training and testing a state sequence, a uniformly distributed random starting point and the corresponding test time length are selected. The selection of the starting point ensures that the uniformly distributed random starting point keeps the same probability of being selected in any phase of the sequence within the closed interval from the first frame of the sequence to the sequence length minus the test time length. The test time length is defined by the number of event frames used x and the skip time y between two adjacent event frames, denoted as $E_{x} - S_{y}$. The skip time defines a window in the time domain where all events are ignored. Without loss of generality, the skip time is expressed as a multiple of 1/60 s, that is, one frame corresponds to an event frame and a grayscale frame. $E_{x} - S_{y}$ means that the test time length is equal to $(x + (x \u2212 1) \u00d7 y)/60$ seconds. Taking $E_{4} S_{3}$ as an example, $E_{4} S_{3}$ means that the number of event frames used in the end is 4; the skip time between adjacent event frames used is 3 \u00d7 1/60 s, that is, 3 frames are skipped; the test time length is 13/60 s. Correspondingly, $E_{8} \u2013 S_{7}$ means that the number of event frames used is 8 frames, 7 frames are skipped between frames, and the test time length is 57/60 s. If the test time length cannot be met due to the short sequence length, the sequence will be read cyclically until it is met. In order to reduce the impact of randomness on the test and ensure the fairness of the comparative experiment, this paper takes randomly selected starting points for all comparative methods for testing, and takes the average of 20 tests after 20 times.We use the same random starting points for single-frame competing methods, where only the random start frame is used."}, {"title": "Metrics", "content": "To evaluate the proposed approach and compare it to competing methods, we adopt two widely used metrics: Unweighted Average Recall (UAR) and Weighted Average Recall (WAR). UAR reflects the average accuracy of different driving status classes without considering instances per class, while WAR indicates the accuracy of overall driving status; please refer to the supplementary materials for formal definitions of both metrics."}, {"title": "Training Setup", "content": "ADSN is implemented in PyTorch . We used Spike-Timing-Dependent Plasticity (STDP) for local weight adjustment and Adam optimizer with the decay rate of the first-order moment estimate set to 0.9, the decay rate of the second-order moment estimate set to 0.999, and the weight decay set to 1 \u00d7 10\u20134 for global optimization of the network. We trained ADSN for 150 epochs using a batch size of 128 on an NVIDIA TITAN V GPU. For the SNN settings, we use a spiking threshold of 0.3 and a decay factor of 0.2 for all SNN neurons.For more details of the experiment please refer to the supplementary materials."}, {"title": "Loss Function", "content": "Because driving status recognition is a classification task, we use a regular cross-entropy loss for supervised training of ADSN:\n$l = - \\frac{1}{7} \u03a3 y_{i} log(\u0177_{i}),$\n(21)"}, {"title": "Evaluation", "content": "We compared the effectiveness of DriveGazen with existing recognition methods (including full-face, monocular, and binocular methods) on the collected driving state dataset DriveGaze and the third-party emotion recognition dataset SEE. In order to verify the design ideas of any stage of the recognition state, three eye-based recognition methods Eyemotion, EMO, and SEEN were selected for comparison. In terms of network design, five common face-based temporal information methods were selected to compare with ADSN. They are ResNet18+LSTM, Resnet50+GRU, 3D Resnet18, R(2+1)D, and Former DFER. For details on training or finetuning of each method, please refer to the supplementary materials. Among these previous methods, Eyemotion and EMO are single-frame methods for predicting emotions, while all other methods require full video sequences. We compare DriveGazen with SEEN during different sequence lengths.As shown in Table 1, DriveGazen of E8-S7 provides the best performance of 92.4% and good complexity in driving state recognition.As shown in Table 2, DriveGazen of E4-S3 outperforms the runner-up method SEEN by 1% in WAR and UAR on the emotion recognition dataset SEE. Our method with the same settings also outperforms SEEN by at least 4% in accuracy under overexposure and HDR lighting conditions. Eyemotion performs slightly better than DriveGazen of E4-S3 in low-light conditions. We believe that Eyemotion benefits from being pre-trained on ImageNet, otherwise Eyemotion's accuracy would be 1% lower than that provided by DriveGaze in the E4-S3 setting. In addition, Eyemotion requires a personalization pre-processing step, which requires subtracting an average neutral image for each person. Personalization significantly improves the accuracy of neutral emotion estimation regardless of whether Eyemotion is pre-trained on ImageNet."}, {"title": "Ablation Study", "content": "We perform a series of ablation studies on the DriveGaze dataset that investigate the impacts of input, the influence of each component of ADSN, and the impact of outputs.Table 3 summarizes the experimental results.For more detailed ablation experiments, please refer to the supplementary materials.\nImpacts of Input. ADSN leverages the first and last intensity frames. Experiments (A), (B) and (C) gauge the impact of the intensity frames: experiment (A) only uses the first intensity frame, experiment (B) replaces the last intensity frame with the second frame, and experiment (C) uses all the intensity frames corresponding to the included event frames."}, {"title": "Influence of ADSN components.", "content": "We investigate the effectiveness of the different components that comprise ADSN: 1) the effectiveness of the attention, residual and spatiotemporal features fusion(experiments (D) and (G)) and 2) the benefits of SNNs (experiments (H) to (I)).\nImpact of outputs. ADSN estimates status based on the average of n membrane potentials; see Equation 7 and Equation 16. Instead of using the average of n membrane potentials, we define the prediction score based on the potential generated by the last event frame only (experiment (J)); similar to the previous but using output spikes instead of potential (experiment (K))."}, {"title": "Conclusion", "content": "We introduce a novel wearable prototype for driver status recognition, which can effectively estimate the driver's status under challenging lighting conditions. We investigated recognition based on input from conventional cameras. Conventional cameras have high resolution, captured frames can robustly encode spatial information. However, parsing temporal cues is challenging. We introduce DriveGazen, a learning-based novel solution for extracting informative temporal cues for status recognition. DriveGazen introduces several novel design components: a method for generating DVS event frames from video frames,a spatial feature extractor based on multi-scale self-attention, a CNN-SNN-based temporal feature extractor and guide attention mechanism. Leveraging spatial awareness and the pulse mechanism of SNN to effectively provide discriminative features for classification. Spatial attention is injected into temporal feature extraction during both training and inference stages. Our extensive experimental results demonstrate that DriveGazen can effectively estimate driver status at any stage. To the best of our knowledge, DriveGazen is the first attempt to utilize conventional camera-generated events and guided attention SNN for driving status recognition tasks."}]}