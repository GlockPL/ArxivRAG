{"title": "Towards Visual Text Design Transfer Across Languages", "authors": ["Yejin Choi", "Jiwan Chung", "Sumin Shim", "Giyeong Oh", "Youngjae Yu"], "abstract": "The art of visual text design serves as a potent medium for conveying themes, emotions, and atmospheres within a multimodal context. From compelling film posters to evocative album covers, the fusion of typography and imagery transcends the communicative potential of mere words. Nevertheless, the translation of a visual style's essence across disparate writing systems presents a substantial challenge for computational models. Can generative models accurately comprehend the intricacies of design and effectively transfer the intended aesthetic across linguistic boundaries? In this study, we introduce Multimodal Style Translation (MuST-Bench), a pioneering task designed to evaluate the efficacy of visual text translation across diverse writing systems. Our studies with MuST-Bench reveal that current visual text generation models struggle with the proposed task due to the inadequacy of textual descriptions in conveying visual design. We introduce SIGIL, a framework for multimodal style translation that eliminates the need for style descriptions. SIGIL enhances image generation models through three innovations: glyph latent for multilingual settings, pretrained VAEs for stable style guidance, and an OCR model with reinforcement learning feedback for optimizing readable character generation. SIGIL surpasses baselines in style consistency and legibility while maintaining visual similarity, unlike description-based methods. We plan to release our benchmark and model to inspire further research in multilingual visual text understanding and generation.", "sections": [{"title": "Introduction", "content": "Recent advancements in image generation have greatly improved the creation of visual text, allowing for instruction-based control over text style. However, these text-to-image models often fall short of accurately conveying abstract concepts like \"design\" and \"style\" through text alone. In practical scenarios, hand-crafted artistic visual text typically lacks precise descriptions of these concepts. For instance, in film posters, album covers, or advertisements, human experts are essential to communicate the main theme and atmosphere. This raises an important question: can we evaluate a machine's ability to understand and transfer such diverse concepts in a multimodal context? To address this, we introduce a novel task called multimodal style translation, specifically designed for controlled evaluation of creative visual text style generation. We define multimodal style translation as a description-free, few-shot style transfer task across different writing systems, challenging models to capture the essence of style design beyond just the font's surface form.\nMuST-Bench, our proposed benchmark, is a carefully curated dataset that extends beyond simple multilingual data collection by comparing language pairs with similar stylistic features across diverse typographical styles. This ensures a robust evaluation of models' capabilities in capturing the nuances of visual text design across different linguistic and cultural contexts. Furthermore, each dataset entry is augmented with human-annotated bounding boxes at the character level, substantially enhancing the benchmark's quality and precision. MuST-Bench samples feature natural artistic typographies sourced from film posters, enabling the evaluation of transfer capabilities from English to five target languages with diverse writing systems: Chinese, Korean, Thai, Russian, and Arabic. However, to the best of our knowledge, no previous methods have been proposed to handle multilingual visual text generation conditioned on a few-shot image samples. Additionally, as we empirically demonstrate later (see section 5.2), state-of-the-art visual text generation models struggle with our benchmark, failing to capture the intricate designs present in artistic typography instances.\nTo address the challenges, we introduce the Style Integrity and Glyph Incentive Learning (SIGIL) framework. SIGIL introduces two technical novelties: first, SIGIL guides the character generation process using a distance metric defined in the pretrained glyph latent space. This space, based on a pretrained VAE, allows for the comparison of glyphs across different languages. Our empirical results demonstrate that guidance within the glyph latent space achieves superior style fidelity compared to textual conditioning. Second, we employ an OCR model to generate confidence scores for the images during the generation process. These scores serve as rewards in a reinforcement-learning-based optimization, enhancing the readability of the generated text.\nSpecifically, our contributions are threefold:\n1. MuST-Bench: We present MuST-Bench, a novel corpus designed for the task of multimodal style translation. MuST-Bench includes human-annotated character-level bounding boxes and encompasses a variety of languages (Chinese, Korean, Thai, Arabic, and Russian) along with diverse typographical styles, facilitating a thorough cross-linguistic evaluation of model performance.\n2. SIGIL framework: We propose Style Integrity and Glyph Incentive Learning (SIGIL) that enhances style transfer fidelity by using glyph latent guidance. By leveraging input style images for direct style guidance, SIGIL achieves superior consistency in generating styled text across different languages. It also incorporates a reinforcement learning approach that leverages rewards from an off-the-shelf OCR model, improving the letter accuracy of the generated images.\n3. Evaluation metrics: We introduce a comprehensive evaluation scheme for multimodal style transfer tasks, assessing the robust transfer of both characters and styles. Our proposed metrics include model-based evaluation with an OCR model, image-to-image similarity scoring, and"}, {"title": "Related Work", "content": "Text to Image Generation. Recent advancements in text-to-image generation tasks [3, 7, 19], notably with models like DALLE-3 [3], have demonstrated exceptional capabilities in image synthesis. Despite these advancements, generating precise and coherent text within images remains challenging. To improve text coherence and accuracy, models like Imagen [19] and DeepFloyd IF [6] employ pretrained large language models such as T5-XXL [16]. Stable Diffusion 3 [7] introduces a novel transformer-based architecture that uses separate weights for image and text tokens, enhancing text comprehension. However, these methods require an extensive image-caption pair dataset, making it difficult to extend them to languages other than English.\nVisual Text Generation. Delving in text to image generation model's visual text generation ability, TextDiffuser [5] incorporates a Layout Transformer capable of understanding text positions and layouts, which helps the Diffusion Model generate text more effectively. GlyphControl [26] advances text generation in diffusion models by aligning text based on its location, implicitly considering font size and text box position. AnyText [24] uses text glyph, position, and masked image inputs alongside OCR-encoded stroke data and image caption embeddings, enabling multilingual visual text generation with a dataset of large-scale multilingual text images. However, further research is required to advance the concept of 'text as images' focusing on generating textual content that is itself viewed as a creative and visually appealing image. While several works [20, 5, 24] datasets provide valuable visual text data, they may not fully meet the requirements of our proposed task, as indicated in table 1. This discrepancy highlights the need for a new benchmark that is specifically tailored to address the unique demands of our research in visual text analysis.\nSubject-driven Image Generation. In the domain of subject-driven image generation, models such as Textual Inversion[8], Dreambooth [18], and Custom Diffusion [11] have significantly advanced the ability to generate images consistent with specific artistic styles or visual prompts by utilizing images as inputs. However, they exhibit limitations in text generation. DS-Fusion [23] is a semantic typography generation method that integrates styles and glyphs through generative adversarial training with input style images. While it effectively achieves semantic typography generation, it can only train one glyph per subject. SIGIL expands this capability by allowing the training of two glyphs per subject and addresses issues in generating characters with diffusion models by employing OCR models in reinforcement learning for improvements.\nCross-language text generation. CLASTE [25] enabled cross-language scene text generation but requires a source-target language pair dataset. To create this dataset, synthetic data was generated by"}, {"title": "MuST-Bench", "content": "Corpus. To the best of our knowledge, MuST-Bench is the first dataset to feature instance-level multilingual typography pairs. Using English as the pivot language, we curated 432 film poster pairs for Korean, 427 pairs for Chinese, 100 pairs for Russian, 100 pairs for Thai, and 60 pairs for Arabic. Each film poster image is decomposed into 10 ~ 20 typography character images. MuST-Bench encompasses a diverse range of typography styles across approximately 1,120 film poster images, as illustrated in fig. 3.\nCuration. Our data curation process, depicted in fig. 2, involves sourcing multilingual film poster images from a movie database website\u00b9. By using natural sources, we obtain instance-level typography pairs that capture the artistic choices of human designers. We then apply a rigorous human filtering process to select only cross-language posters that exhibit similar styles, pairing them accordingly. Subsequently, each character and whole word is manually labeled with precise bounding boxes. We plan to make our dataset publicly available. Refer to Appendix C for further details.\nBenchmark. We define multimodal style translation as a sequence-to-sequence character generation problem. Given an indexed list of characters from a film poster, the task is to generate a corresponding sequence of characters in the target language that closely matches the ground truth film poster in that language. Both the input and output sequences are formatted as cropped patches of characters within the image. Additionally, we allow the textual form of the characters as inputs, as illustrated in fig. 4.\nEvaluation metrics. Assessing the accuracy of style translation to the target language is a complex task. MuST-Bench simplifies this evaluation by providing ground-truth targets for each language. Given the target images, we compare their similarity to the generated character images. To evaluate the quality of style transfer, we utilize semantic image-to-image similarity with the off-the-shelf CLIP model [15], commonly referred to as CLIP-I in the literature [18]. Another critical criterion is the shape correctness of the generated visual text, i.e., readability. Following previous studies [24], we use an off-the-shelf OCR model to assess this accuracy metric. The OCR model transcribes the generated characters into text, which is then compared with the ground-truth text. The generated images are resized to 25 by 25 pixels as inputs for the OCR model, as larger characters are not"}, {"title": "SIGIL Framework", "content": "In the training process, we use 3 to 5 specific style images, which is a common number used in personalized image generation [18, 8]. Along with these style images, we provide a prompt, e.g., \"a typography of \u2018K\u201d and a corresponding glyph image, e.g., a white background with a black \u2018K' glyph created using the Pillow library. However, the ground truth, e.g., the desired style image of the 'K', is not used during training. Since we assume there is no ground truth in real-world translation tasks, we introduce the SIGIL method, which allows us to generate the glyph without relying on direct supervision from ground truth.\nAt inference time, only the prompt, e.g., \"a typography of 'K',\u201d is used as input. Our model is trained on one style across multiple glyphs. As shown in Appendix G, while previous approaches [23] could only generate a single glyph per style, we have extended this capability to generate two glyphs. For instance, we can now generate both 'A' and 'B' in a vivid style. The training time for this process is detailed in Appendix D.\n4.1 Overview\nSIGIL utilizes three types of inputs to generate the sequence of character images in the target language. First, the sequence of character images in the source language is provided to guide typography styles. Second, the textual form of the target characters is rendered as glyphs to guide the target shape. Finally, the same textual forms are also provided as instructions using various prompt templates. These different prompts help control variations in the output characters. For further details, please refer to the experiments in Appendix B.\n4.2 Glyph Guidance on the Latent Space\nWe extend a conditional diffusion model architecture [18] to transform character images from the source language (style prior) into styled character images in the target language. Following the parameterization by Ho et al. [9], we train a U-Net architecture $M_{\\theta}$ to estimate the Gaussian noise $\\epsilon_t$ at step t using mean squared error (MSE) as the distance metric.\n$L_{diff} = ||\\epsilon - \\hat{\\epsilon}||^2 = E_t [||\\hat{\\epsilon}_t - \\epsilon_t ||^2], \\hat{\\epsilon}_t = M_{\\theta}(z_{t+1})$\nWe adopt a latent diffusion setting [17], where the diffusion process occurs in the latent space z defined by a pretrained variational autoencoder (VAE) [10] encoder, rather than in the pixel space.\nTo incorporate glyph shape constraints, we propose an efficient solution: using the same VAE encoder to minimize the difference between the generated image and the rendered glyphs in the latent space. Specifically, the glyphs are encoded as latent vectors $z_g$ to be compared with the model-generated diffusion noise $\\hat{\\epsilon}_t$. Our glyph loss is defined as follows:\n$L_{glyph} = ||\\epsilon - z_g||^2 = E_t [||\\hat{\\epsilon}_t - z_g||^2]$\nBy using the latent semantic space as our metric space, we provide a cost-effective way to compare characters across languages without training a separate classifier. As previous literature suggests [27,"}, {"title": "Enhancing Readability through OCR Rewards.", "content": "Whether two visual texts contain the same characters can be only poorly measured in the general image-based distance space. As a response, we propose to use off-the-shelf optical character recognition (OCR) models to provide goodness scores on how well the drawn character conforms to the original textual form. A hypothetical image $y_c$ corresponding to character c is obtained by decoding the generated latent $z_c$ of the U-Net module through the VAE decoder $f_{rv}$. Given the image $y_c$, the OCR model $f_{\\theta}$ outputs probabilities over the entire set of possible characters $c' \\in C$. The probability of the label c is used as a reward $R_{base}$ to optimize the U-Net parameters $M_{\\theta}$.\n$R_{base}(z_c): f_{\\theta}(c|y_c), y_c = f_{rv}(z_c)$\nWe identified a failure mode where the naive OCR rewards cause the generator to produce multiple small instances of the same letter to exploit the reward system. To counteract this issue, we propose a simple modification to the reward definition: introducing a penalty based on the number n of detected characters. The character count n is derived from the inference results of the same OCR model. The final reward function combines the base reward with a character number penalty $\\delta(n)$:\n$R(z_c) := R_{base}(z_c)\\delta(n), \\delta(n) = e^{-(n-1)}$\nWe optimize the above criterion using a reinforcement learning approach. Specifically, we adopt Proximal Policy Optimization (PPO) [21] enhanced with engineering choices of Black et al.,[4]. Denoting the log probability of the next timestep in the backward diffusion process as $log p(z_{t-1}|z_t)$, the final reinforcement learning loss $L_{RL}$ is defined as:"}, {"title": "Experiment", "content": "5.1 Qualitative Evaluation\nQualitative comparison We compared the latest models in the field of text-to-image generation, including DALL-E3, the multilingual text generation model AnyText, and the generation model DS-Fusion, focusing on their performance in generating English, Chinese, and Korean typography. To enable these models to accept input style images as references, we utilized GPT-4V as a bridge for DALL-E3, allowing it to receive style images (refer to Appendix E). AnyText facilitated this through its text editing capabilities. DALL-E3 produced visually impactful results, generating highly accurate text in English. However, it was limited to English and could only guide the image style through text prompts. Consequently, when comparing the input style image with the generated image, the visual similarity was not as strong. AnyText demonstrated high accuracy in generating text for the languages it was trained in, namely English, Chinese, and Korean. Despite this, the output's color, shape, and texture were relatively monotonous. DS-Fusion employed a method where style words and text to be generated were input as text prompts, creating images through Latent Diffusion Models (LDMs)[17] for model training. We compared the results from this method with those generated by directly inputting style images into the model for training. SIGIL excelled in creating textures and colors, faithfully replicating the style of the input images. For example, the first column of fig. 5, accurately reproduced the red border color and texture of the \"Metal\" style. In the second column, it captured the glossy texture and gradient colors of the \"Cyber\" style. In the third and fourth columns,"}, {"title": "Quantitative Evaluation", "content": "OCR and CLIP-I Evaluation. From the MuST-Bench, we derived 10 distinct style subjects and composed 26 prompts per language. Each prompt was sampled 4 times, generating a total of 1,040 styled characters per language. The evaluation utilized the benchmark and metrics discussed in section 3, and the results are shown in table 2. SIGIL significantly outperforms the others in OCR accuracy for English, Chinese, and Korean. For the CLIP-I score, AnyText achieved slightly higher scores for some languages. This is because AnyText uses image editing to generate results, which means that not only the text but also the background is included in the similarity calculation. Consequently, we remove the background for both human and MLLM-based evaluation.\nMLLM Evaluation. For the style fidelity evaluation using MLLMs, we employed the method mentioned in section 3. To ensure the reproducibility of the evaluation results, we used a consistent input judgment template. For GPT-4V, we set the seed to 100 and the temperature to 0. Although Claude-3-OPUS does not support seed fixation, setting the temperature to 0 minimizes variability. To further ensure reproducibility, we provided consistent input data and verified the results across multiple runs. For each evaluation, the images generated by each method were randomly shuffled before being presented to the MLLM for assessment. A total of 30 questions were involved in the evaluation process. The evaluation results are presented in table 3. SIGIL achieved the highest style fidelity scores on both GPT-4V and Claude. Interestingly, even though images were generated from DALL-E3 using prompts created by GPT-4V based on the input style image, SIGIL still received higher scores in the GPT-4V evaluation. During the evaluation, we identified a safety misclassification issue with GPT-4V for some typographic images, and four such samples are included in Appendix A.\nUser study. We had human workers rank the model outputs based on style fidelity and legibility. A total of 60 users participated, using the same set of samples as in the MLLM evaluation. For the style fidelity evaluation, participants were instructed to Rank the images in the order of similarity to the reference image. For the legibility evaluation, they were asked to Order the images by how well the visual text matches the prompt. table 4 shows that SIGIL significantly outperforms the baselines. DALL-E3 scored second-highest in terms of legibility for English, but the lowest for all other languages. Despite its high resolution and visual effects, its low style fidelity score indicates difficulties in visual transfer based on text-only descriptions. For example, see the rows labeled \"Style Images\" and \"DALL-E3\" in fig. 5."}, {"title": "Ablation Study", "content": "We examine the impact of hyperparameters on the glyph loss coefficient $f(t)$. As shown in fig. 6 (a), higher values of $\\lambda$ result in a rapid convergence of the glyph loss, but this often leads to a significant divergence from the intended style. Conversely, higher values of $\\tau$ promote stable convergence, with the variations in generated results between epochs diminishing, as illustrated in fig. 6 (b).\nTo further support our finding in fig. 6, we report quantitative results in table 5, which re-affirms our findings in the qualitative ablation studies.\nIn the generator component of SIGIL, if glyph images are used directly without glyph latent guidance, the generator training results are as shown in fig. 7 (a). In comparison, by using the glyph latent guidance we propose, the generation results can be obtained as shown in fig. 7 (b).\nAdditionally, we conduct more ablation studies on the objective part. Specifically, we test the effects of applying SIGIL without $L_{glyph}$, SIGIL without the Generator (which combines style and glyph), and SIGIL without the Corrector (the RL part) to assess the contribution of each component.\nThe results in the table 6 show that the generator has the greatest impact on style generation accuracy, while glyph generation accuracy is most significantly influenced by $L_{glyph}$. Additionally, the Corrector primarily proves effective when the generator already achieves a certain level of success in glyph generation. Also, the results presented in table 7 further demonstrate that the generator has the greatest influence on style fidelity."}, {"title": "Conclusions", "content": "In this paper, we introduce the SIGIL, a novel approach for generating typography in various languages while maintaining a consistent style. SIGIL can optimize generated images to match the input style image even with a small amount of data, without relying on extensive image-caption datasets. In this study, we have successfully combined two specific glyphs per style during training. In future work, we plan to explore methods that enable the combination of a greater number of glyphs.\nOur contributions include a comprehensive benchmark to support the training and evaluation of multimodal style translation. The key advantages of our approach are as follows: (i) Unlike traditional visual text generation methods, our style subject-driven approach allows for effective model training without the need for large volumes of visual text data for each language. (ii) We propose an innovative technique that integrates an OCR model into the reinforcement learning process for generative models, significantly enhancing text generation accuracy. (iii) By providing a dataset and benchmark of multilingual text pairs in the same style, we establish a solid foundation for future research in expanding visual text generation across multiple languages. These contributions collectively advance the field of multilingual visual text generation, paving the way for further developments and applications in this area."}, {"title": "MLMM Evaluation Details", "content": "A.1 Comparison of Judgement Templates.\nIn our study, we explored two distinct methodologies for evaluating the output of MLLMs. The first approach consolidates all generated images into a single composite image, facilitating an evaluative process from a human-centric perspective. This approach is delineated in the judgment template shown in fig. 8. Alternatively, the second method involves separately encoding each generated image, which is illustrated in the judgment template of fig. 9."}, {"title": "Safety Misclassification of GPT-4V", "content": "In fig. 12, we present examples that, despite containing content deemed safe, elicited a \"400 Bad Request\" error from GPT-4V."}, {"title": "More Ablation Studies", "content": "Visual Text Generation Prompts. The prompt for visual text generation can be expressed in various ways, as shown in fig. 13. In this ablation study, the seed was fixed at 100, and the prompt was changed for experimentation. While the initial images generated for each prompt vary greatly, the final generated images at the end of training maintain a variety of style fidelity without losing legibility."}, {"title": "Dataset Curation Details", "content": "Collecting Multilingual Film Poster Images. We collected multilingual posters for movie titles from a movie database website. Then, we kept only those titles that had pairs in different languages, creating pairs of multilingual posters.\nFiltering by Style Similarity. We hired three AI researchers to perform filtering tasks by determining the style similarity of the text in two poster images displayed on the screen. A screenshot of the full text of instructions for this task can be found in fig. 14, which also shows the annotation tool we developed specifically for this task.\nCharacter-level Bounding Box Annotation. The three human annotators who participated in the prior filtering task volunteered for this task as well. They also carried out bounding box labeling on"}, {"title": "Ethical Considerations", "content": "In this paper, we present the MuST-Bench, which incorporates copyrighted film posters designed by human experts. To address the issue of copyright, instead of distributing raw data, we provide download links for each poster image along with bounding box annotations. Furthermore, accompanying code is made available, enabling easy transformation of the posters into the format proposed in the paper. This approach ensures compliance with copyright laws while maintaining the utility of the dataset for research purposes."}, {"title": "Implementation and Training Details", "content": "SIGIL comprises two main components, the generator and the corrector. The implementation and training details for each are as follows:\nGenerator. For the pre-training configuration, we employed the runwayml/stable-diffusion-v1-5 model publicly available weights from the Huggingface Hub (https://huggingface.co/models). The training dataset utilized was derived from the MuST-Bench style subject, where each style subject allowed for fine-tuning on two glyph combinations. Training was conducted for approximately 1,000 epochs. The total epochs were adjusted based on the dataset volume; for instance, a dataset containing 15 training samples warranted an extension to 1,005 epochs to ensure thorough model training. The learning rate was maintained at 1e-4 throughout the training process. Regarding the framework and computational resources, we extended the LoRA-based implementation of Dreambooth to process multi-subject inputs, allowing for concurrent fine-tuning on two glyph subjects. Training was performed on a single NVIDIA A100 40GB GPU, with each session completing in about 20 minutes.\nCorrector. The corrector was initiated from the last checkpoint obtained after the preliminary fine-tuning phase (generator's last checkpoint). The training dataset comprised images sampled during the generator's operation, serving as the primary data for further training. Training was conducted with an emphasis on efficiency, incorporating an early stopping mechanism to curtail the training as soon as the model reached a satisfactory level of performance. On average, the training duration was approximately 30 minutes using the same GPU as the generator. The learning rate was set to 3e-4, which was determined to be optimal for achieving convergence while maintaining training stability. Additionally, the EasyOCR tool was employed to facilitate multi-language text recognition. While utilizing this off-the-shelf OCR model, we adapted its output mechanism to provide confidence scores for characters presented in the prompt instead of predicted characters."}, {"title": "Image Input to DALL-E3", "content": "SIGIL can accept style images as input. However, DALL-E 3 only accepts text inputs. Therefore, to ensure fairness in evaluation, GPT-4V is used as a bridge to enable DALL-E 3 to \"see\" images. As shown in fig. 15, GPT-4V views the style image and creates a description of that style. This description is then combined with the textual prompt and sent to DALL-E 3. The importance of using GPT-4V for style description can be seen by comparing fig. 16 (a) and (b)."}, {"title": "User Studies", "content": "In the user study, assessments were conducted on two distinct parameters, style fidelity and legibility. 60 participants were provided with detailed guidelines for each instruction, as outlined in fig. 17, before initiating the study. fig. 18 is a sample of a user study. fig. 19 exemplifies a scoring sheet used for ranking the outcomes of comparison methods based on style fidelity, and illustrates a scoring sheet designed for the evaluation of legibility."}, {"title": "Limitations", "content": "In the real world, there is abundant high-resolution typography data. However, for collecting multi-language pairs with the same style, movie poster data has proven to be the most effective. When extracting typography from movie posters, the resulting size is relatively small. If the input style image is low resolution, it can complicate VAE-based pixel-level encoding, potentially affecting the generation results. In future research, we aim to explore methods to generate typography in different languages from single-language style images, thereby utilizing more real-world data.\nUnlike English, some glyphs with complex strokes in Chinese and Korean present challenges in generation. Applying fine-grained image generation methods could enable the accurate creation of all glyphs.\nLastly, we observed that the EasyOCR model used as a reward model in the Reinforcement Learning process sometimes exhibits False Negative issues with generated images. Additionally, if the model"}]}