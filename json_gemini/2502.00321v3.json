{"title": "MIM: Multi-modal Content Interest Modeling Paradigm for User Behavior Modeling", "authors": ["Bencheng Yan", "Si Chen", "Shichang Jia", "Jianyu Liu", "Yueran Liu", "Chenghan Fu", "Wanxian Guan", "Hui Zhao", "Xiang Zhang", "Kai Zhang", "Wenbo Su", "Pengjie Wang", "Jian Xu", "Bo Zheng", "Baolin Liu"], "abstract": "Click-Through Rate (CTR) prediction is a crucial task in recommendation systems, online searches, and advertising platforms, where accurately capturing users' real interests in content is essential for performance. However, existing methods heavily rely on ID embeddings, which fail to reflect users' true preferences for content such as images and titles. This limitation becomes particularly evident in cold-start and long-tail scenarios, where traditional approaches struggle to deliver effective results. To address these challenges, we propose a novel Multi-modal Content Interest Modeling paradigm (MIM), which consists of three key stages: Pre-training, Content-Interest-Aware Supervised Fine-Tuning (C-SFT), and Content-Interest-Aware UBM (CiUBM). The pre-training stage adapts foundational models to domain-specific data, enabling the extraction of high-quality multi-modal embeddings. The C-SFT stage bridges the semantic gap between content and user interests by leveraging user behavior signals to guide the alignment of embeddings with user preferences. Finally, the CiUBM stage integrates multi-modal embeddings and ID-based collaborative filtering signals into a unified framework. Comprehensive offline experiments and online A/B tests conducted on the Taobao, one of the world's largest e-commerce platforms, demonstrated the effectiveness and efficiency of MIM method. The method has been successfully deployed online, achieving a significant increase of +14.14% in CTR and +4.12% in RPM, showcasing its industrial applicability and substantial impact on platform performance. To promote further research, we have publicly released the code and dataset at https://pan.quark.cn/s/8fc8ec3e74f3.", "sections": [{"title": "1 Introduction", "content": "Click-Through Rate (CTR) prediction plays a vital role in applications such as recommendation systems, web searches, and online advertising[8, 35], as it directly impacts user engagement and platform revenue. Among its key components, User Behavior Modeling (UBM) has emerged as a critical optimization direction. By leveraging users' historical interactions, UBM effectively captures their underlying preferences, enabling more accurate predictions and enhancing overall system performance [23, 41, 43].\nTraditional UBM methods predominantly rely on ID embeddings to represent items and user behaviors(see Figure1 blue part). While effective in some scenarios, ID-based approaches face inherent limitations. First, ID embeddings primarily encode collaborative filtering (CF) signals but fail to effectively capture user preferences for content, such as images and titles, resulting in a misalignment between representations and actual user interests. Second, these methods require abundant user-item interaction data, resulting in poor performance in cold-start and long-tail scenarios [37, 38].\nTo overcome these limitations, there is a growing need for content-based multi-modal UBMs(see Figure1 green part). This shift is motivated by two key factors. First, user interactions with recommendation systems are predominantly mediated through visual and textual content, such as product images and descriptions. Modeling these features is essential for accurately reflecting user preferences. Second, recent advancements in multi-modal foundation models (FoMs), such as Vision Transformers (ViT) [11, 16], LLaMA[28], Vicuna[6], BEIT-3[30], GPT-4[22], and so on, provide powerful tools for extracting rich semantic information from the visual and textual content.\nExisting multi-modal user behavior modeling methods can be broadly categorized into two classes: two-stage pretraining methods and end-to-end training methods. Two-stage pretraining methods leverage pre-trained foundation models to extract multi-modal features, which are then integrated into user behavior models. End-to-end methods, on the other hand, jointly optimize multi-modal models and user behavior modeling modules, demonstrating their potential in better aligning user interests with content semantics. Despite achieving considerable success, these approaches also have"}, {"title": "2 Related Works", "content": "2.1 User Behavior Modeling\nUser Behavior Modeling (UBM) has been widely studied as a critical component of Click-Through Rate (CTR) prediction due to its ability to capture users' historical preferences and predict their future interactions. Traditional UBM methods, such as DIN [43], use attention mechanisms to selectively focus on relevant behaviors, while extensions like DIEN [42] incorporate sequential modeling to capture the temporal evolution of user interests. Despite their success, these methods heavily rely on ID-based embeddings, which primarily encode collaborative filtering (CF) signals but fail to effectively model content-based preferences, particularly for items such as images and texts. This limitation becomes pronounced in cold-start and long-tail scenarios where interaction data is sparse [37, 38]. Recent efforts, such as the introduction of hybrid UBM approaches, have attempted to integrate content features but still lack a unified framework for aligning content semantics with user-specific interests.\n2.2 Multi-modal Recommendation\nMulti-modal recommendation integrates diverse content features such as images, texts, and videos to improve user understanding and prediction accuracy. Traditional methods often augment CTR models with multi-modal (MM) features as additional attributes, directly feeding them into the models to enhance representation power [21]. While effective to some extent, these approaches neglect the critical need to align MM features with user-specific interests, resulting in a limited understanding of user preferences.Recent advancements focus on bridging the domain gap between pre-trained foundation models and downstream recommendation tasks [15, 29, 32, 39]. However, these methods largely ignore the role of explicit user interest modeling in refining MM embeddings, limiting their ability to capture sophisticated user preferences. Moreover, the practicality of existing methods in industrial applications remains a challenge. Many end-to-end frameworks incur high computational and memory costs, making them inefficient for large-scale deployment [39]. To address these issues, our proposed MIM paradigm systematically bridges the semantic gap between MM features and user interests while maintaining scalability and efficiency, achieving significant performance gains in real-world applications.\n2.3 Foundation Models\nPre-trained Foundation Models have significantly advanced the fields of Computer Vision (CV) and Natural Language Processing (NLP) by learning transferable representations from large-scale data. In CV, models like iGPT[4] and ViT[11] have pioneered the application of transformer architectures for image recognition, leveraging self-supervised tasks such as masked patch prediction to capture rich visual semantics. Further advancements, such as Swin Transformer[16] and BEiT[2], improved efficiency and scalability, making transformers a dominant paradigm in vision tasks.In NLP,"}, {"title": "3 MIM", "content": "3.1 Preliminaries and Method Overview\n3.1.1 Preliminaries.\nCTR Prediction. Considering a typical item search scenario\u00b9, given a set of users \\(U\\), a set of items \\(I\\), and a set of queries \\(Q\\), the CTR prediction task is to predict whether a user \\(u \\in U\\) will click the item \\(i \\in I\\) when \\(u\\) searches a query \\(q \\in Q\\). It can be formulated as: \\(\\hat{y} = f(u, i, q)\\) where \\(\\hat{y}\\) is the predicted click-through rate score. Note that query \\(q\\) can be a text query and can also be an image query, which refers to a user wanting to search for an item with similar content to the image query.\nUser Behavior Modeling. Given user behavior \\(B = \\{b_1, b_2, ..., b_l \\}\\), where \\(l\\) is the behavior length, a general UBM is expressed as\n\\(h_B^{ID} = \\sum_{i=1}^{l} \\alpha_{t,i}^{ID} h_i^{ID}\\)\nwhere \\(h_i^{ID}\\) is the ID embedding of item \\(b_i\\), \\(\\alpha_{t,i}\\) is the relevant score between \\(b_i\\) and target item \\(t\\) via attention mechanism based on\n3.1.2 Method Overview. The overall framework is illustrated in Figure 2. The proposed MIM paradigm comprises three key stages designed to address the limitations of existing approaches and enhance user behavior modeling. In the pre-training stage, the primary focus is on adapting foundational models to domain-specific data, ensuring the extraction of high-quality multi-modal embeddings capable of understanding diverse item content. Subsequently, the Content-Interest-Aware Supervised Fine-Tuning (C-SFT) stage bridges the semantic gap between user interests and content representations by leveraging explicit user behavior signals, such as purchase actions, to guide embeddings in aligning with user preferences. Finally, the CiUBM stage integrates the refined multi-modal embeddings with ID-based collaborative filtering signals into a unified and flexible framework, enhancing the overall representation power. To address efficiency challenges in large-scale industrial applications, a representation center is introduced, which precomputes and stores embeddings for efficient retrieval, significantly reducing both training and inference costs.\n3.2 Pre-training\nThe pre-training stage focuses on equipping the multi-modal embeddings with the ability to understand item content effectively. This stage leverages foundational models (FoMs) pre-trained on public datasets (e.g., ImageNet and Wikipedia), which are further adapted to downstream domains. We define vision-based and language-based pre-trained FoMs as \\(F_V\\) and \\(F_L\\) respectively. To address the limitations of general pre-trained models, two aspects of adaptation need to be considered.:\nDownstream Data Adaptation (DDA). Knowledge learned from generic datasets may not align well with domain-specific item content. To overcome this, \\(F_V\\) and \\(F_L\\) are continually pre-trained on downstream datasets containing item images and textual descriptions. This process ensures that the resulting models are fine-tuned to the unique characteristics of domain-specific data, enabling a deeper understanding of visual and textual content.\nDifferent Modal Alignment (DMA). Effective multi-modal representation requires a seamless alignment between embeddings from different modalities. Following the contrastive learning framework"}, {"title": "3.3 Content-Interest-Aware Supervised Fine-tuning", "content": "After the pre-training stage, FoM can understand what content an item has. However, To enhance its effectiveness in user behavior modeling, the focus of embeddings must shift from representing \"what the content is\" to \"what users are interested in. Ideally, multimodal embeddings for items with similar user interests should be drawn closer together in the latent space, while embeddings for unrelated items should be pushed farther apart. Inspired by the recent advancements in contrastive learning [7, 18, 20, 33], we propose a content-interest-aware supervised contrastive finetuning (C-SFT) method to effectively model user interest similarity within the multi-modal (MM) embedding space.\nTo design such a contrastive fine-tuning, we need to answer the following three questions: Q1: How to define user interest pairs? Q2: How to encode multi-modal features? Q3: What is a proper learning objective? Next, we present the design strategies for C-SFT by correspondingly answering the aforementioned questions.\nQ1: User Interest Pairs Definition.\nThe success of the contrastive learning framework largely relies on the definition of the user interest pairs [24]. It drives us to seek a strong and direct user interest signal.\nIn this paper, we leveraged data from visual search scenario, we define the user interest pair as  , which refers to a user who has searched an image query \\(q\\) and purchased an item \\(i\\). Compared to textual queries, image queries carry richer semantic meanings and can more accurately express users' intentions and interests\nThe reason to use purchase behaviors rather than click behaviors is that purchase actions provide a clearer and more reliable indication of user interests. We also conduct experiments to show the impacts of different signals in Section 4.3.2.\nQ2: Multi-modal Encoder. Given the user interest pairs , a multi-modal encoder extracts embeddings for both the query and the item. The query image \\(q\\) and item image \\(img_i\\) are processed using \\(F_V\\), while the item title \\(txt_i\\) is encoded by \\(F_L\\):\n\\(h_{img}^q = F_V(q), h_{img}^i = F_V(img_i), h_{txt}^i = F_L(txt_i)\\)\nTo integrate the item's multi-modal content, we fuse the image and title embeddings using a tensor-based approach inspired by TFN [40], capturing both intra- and inter-modal interactions. The fused embedding is defined as:\n\\(\\Omega = [h_{img}^i; 1] \\otimes [h_{txt}^i; 1]\\)\nwhere \\(\\otimes\\) indicates the outer product. Finally, the fused representation is refined using an MLP:\n\\(H^{MM} = MLP(\\Omega)\\)"}, {"title": "3.4 CiUBM: Content-interest-aware UBM", "content": "After obtaining the multi-modal (MM) embedding \\(H^{MM}\\) for item \\(i\\), we design a content-interest-aware user behavior model (CiUBM) to enhance the existing UBMs by integrating ID and content-based signals. CiUBM consists of three components:\nID Interest Module. This module uses existing UBM techniques to model user preferences based on ID embeddings, capturing collaborative filtering (CF) signals. It provides a straightforward integration of prior UBM methods.\nContent Interest Module. This module leverages the high-quality MM embeddings to calculate user content interest relative to the"}, {"title": "4 Experiments", "content": "4.1 Experimental Settings\n4.1.1 Datasets. (1) Pre-training Datasets. As introduced in Section 3.2, a downstream-related dataset is applied to align the style and modal distribution. We collect images and titles of items from Taobao, one of the world's largest online retail platforms. There are a total of 1.42+B items. (2) C-SFT Datasets. We collect purchase logs in the last 6 months from the visual search scenario in Taobao and construct the C-SFT dataset as introduced in Section 3.3. There are a total of 0.14 B user interest pairs. (3) Downstream Datasets. The\n4.1.2 Baselines. Here, we compare our method with various UBM methods, including Avg Pooling\u00b3, DIN[43], DIEN[42], BST[5], and SIM[23], TWIN[3]. By default, the base backbone of UBM is set as SIM [23].\n4.1.3 Training Details. We use the Adam optimizer with a learning rate of 0.005 for all methods. The batch size N is 1024 for all methods. By default, we take EVA-2 [31] for images and BGE[34] for texts. We set hyper-parameters \\(\\alpha = 0.5, \\beta = 0.5\\) for loss weights and \\(k = 10\\) for negative samples by grid searching. Besides, we also evaluate the impact of different FoM, including ResNet50[14], SwinTransformer[16], ConvNext v1[17], ConvNext V2[31], ConvNext v2 large[31], EVA-2[12], Bert[9], GPT-2[26], BEiT-3[30] and BGE[34] in Section4.3.\nMIM is trained by 64 A100 GPUs in the pre-training stage, 40 A100 GPUs in the C-SFT stage, and 100 V100 GPUs in the CiUBM training stage. We run all experiments multiple times with different random seeds and report the average results.\n4.2 Performance on CTR Prediction Tasks\nWe evaluated the performance of MIM on CTR prediction tasks by applying it to various existing UBM methods. As a universal paradigm, MIM is compatible with most UBM frameworks. To assess its effectiveness, we compared the AUC scores of the original models (denoted as Base) with those enhanced by MIM. The AUC metric [13] is reported. As shown in Table 2, integrating MIM leads to consistent performance improvements across all baseline methods and datasets. For example, SIM achieves AUC gains of 0.23pt to 0.54pt, and similar gains are observed for other UBMs. While these improvements may appear modest, in large-scale scenarios such as Taobao, a 0.1pt improvement in AUC can result in several percentage points of uplift in online CTR. This uplift can bring billions in revenue to the platform, highlighting the economic value of our approach. These results validate the effectiveness and generalizability of MIM. Which can significantly enhances user behavior modeling by bridging the gap between content and user interest embeddings.\n4.3 Effectiveness Evaluation\nIn this section, we conduct various experiments to detailedly evaluate the effectiveness of MIM.\n4.3.1 Effectiveness of high-quality MM embeddings.\nThe impact of FoM. The experimental results in Figure 5 demonstrate the effectiveness of upgrading FoM. For \\(F_V\\), the shift from ResNet50 to EVA-2 yields constant performance improvements, demonstrating the ability of stronger visual FoMs to capture richer content semantics. Similarly, for \\(F_L\\), upgrading from smaller to larger models (e.g., BEiT-3 base to large) improves AUC, highlighting the benefits of scaling parameter sizes. These findings confirm that enhancing FoM, both in terms of model strength and parameter size, significantly improves the representation and alignment of user interests across modalities.\nThe impact of pre-training. We evaluate the impact of different strategies used in the pre-training stage. The results are presented in Table 3. Here, Plain FoM refers to the FoM that is only pretrained on general datasets. From Table 3, it shows (1) directly applying Plain FoM only achieves limited improvement due to the sub-effectiveness in item content understanding. (2) Both downstream data adaption and different modal alignment can help FoM better capture the item content and provide a significant improvement.\n4.3.2 Effectiveness of C-SFT. As presented in Section 3.3, there are two important factors that influence C-SFT, i.e., the user interest signal and loss function. Thus, we train different versions of C-SFT (see Table 4) and evaluate the performance of these versions in CTR prediction tasks. Specifically, for the user interest signal, we take the users' clicking and users' purchase as the expression of user interest. Furthermore, we also take an interest-irrelevant signal (item categories) with Cross Entropy loss as a comparison. For the loss function, the contrastive loss (i.e., Equ 5) and Cross Entropy loss are compared. The AUC results are reported in Table 4.\nThe impact of user interest signal. Compared with the performance of version v1, v2 and v3 achieves a better performance. It demonstrates the importance of introducing the user interest signal. Without such signals, MM features can hardly achieve a positive effect on CTR prediction. Furthermore, v3 (or v5) obtains a better performance than v2 (or v4). One of the possible reasons is that the purchase behavior is more strong and precise, resulting in a better expression of user interest. It also shows that understanding user interest plays an important role in MIM."}, {"title": "4.4 Ablation Study", "content": "In this section, we conduct an ablation study to analyze the impact of different components. The results are presented in Table 5. Some"}, {"title": "4.5 Efficiency Evaluation", "content": "In this section, we analyze the efficiency of the proposed paradigm, including time and GPU memory efficiency. Detailedly, we develop two versions: (1) MIM (w/o RC) refers to MIM without representation center, i.e., MM embedding is real-time inferred by FoM. (2) MIM (E2E) refers to jointly training FoM and CiUBM in an E2E manner. We report the GFLOPs (the number of giga floating-point operations) per sample to reflect the train and inference time cost"}, {"title": "4.6 Generalization in Cold Start Scenarios", "content": "Here we analyze the performance of the cold start items to show the generalization of our proposed paradigm. Detailedly, we divide different items into 10 sets (denoted as S1 to S10), where S1~S10 refers to the newest~oldest item sets. Then we evaluate the improvement of different sets respectively. The results are presented in Figure 6. It shows that MIM contributes more to the newest items (e.g., S1) and achieves more AUC gains. This demonstrates that MM features enable new items to better represent their features, leading to better performance."}, {"title": "4.7 Evaluation of Industrial Applications", "content": "Here, we first develop MIM on a real-world industrial application, i.e., the sponsored search system in Taobao, and have continually achieved three releases of MIM in our industrial application in the past year.\nOnline A/B Testing for CTR Prediction Tasks. Compared with the online model, these three releases achieved 0.81pt AUC gains in total, and 0.26pt, 0.25pt, and 0.30pt AUC gains for each. During the online A/B test, we observed 14.14% CTR gains in total, and 5.1%, 4.75%, and 4.29% CTR gains for each release. It also achieves 4.12% RPM (Revenue Per Mile) gain in total, and 1.5%, 1.19% and 1.43% RPM gains for each release.\nImpact on Content-Sensitive Categories. We also report AUC, CTR, and RPM gains in different item categories in one of the releases (others have similar results). Table 7 shows more improvement can be archived on the content-sensitive categories (e.g., Clothes), where users more care about the styles of items and can be easily attracted by item images or titles. On the contrary, lower improvement is obtained from content-insensitive categories (e.g., Cars). It demonstrates user interest can be well modeled by MIM and improve the performance of CTR prediction.\nUniversality in Recommendation Tasks. Furthermore, we also try to apply MIM in a recommendation system, i.e., the display advertising system in Taobao, and achieve similar improvement (0.4pt offline AUC gains, 3.5% CTR gain and 1.5% RPM gain online). From the success of different systems, we believe MIM is a universal and industrial welcomed multi-modal content interest modeling paradigm."}, {"title": "5 Conclusion", "content": "In this paper, we propose MIM, a novel and universal multi-modal content interest modeling paradigm for industrial-scale applications. By introducing a decomposed training paradigm and a representation center, MIM effectively integrates multi-modal features into User Behavior Modeling (UBM), addressing the limitations of traditional ID-based methods and existing multi-modal approaches. The proposed method aligns user interests with content embeddings, significantly enhancing prediction performance while maintaining efficiency in large-scale real-world applications. Extensive experiments validate the effectiveness and generalizability of MIM, showing consistent improvements across diverse baselines and datasets. These results demonstrate MIM's practical value and its potential to drive further innovation in multi-modal user modeling for industrial applications."}]}