{"title": "Towards Specification-Driven LLM-Based Generation of Embedded Automotive Software", "authors": ["Minal Suresh Patil", "Gustav Ung", "Mattias Nyberg"], "abstract": "The paper studies how code generation by LLMs can be combined with formal verification to produce critical embedded software. The first contribution is a general framework, spec2code, in which LLMs are combined with different types of critics that produce feedback for iterative backprompting and fine-tuning. The second contribution presents a first feasibility study, where a minimalistic instantiation of spec2code, without iterative backprompting and fine-tuning, is empirically evaluated using three industrial case studies from the heavy vehicle manufacturer Scania. The goal is to automatically generate industrial-quality code from specifications only. Different combinations of formal ACSL specifications and natural language specifications are explored. The results indicate that formally correct code can be generated even without the application of iterative backprompting and fine-tuning.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Models (LLMs) have shown promising, and sometimes astonishing, results in code generation [35,29]. However, from several studies [33,41], it is also clear that it is hard to guarantee code correctness and quality. In the area of automotive embedded systems, correctness and quality of the software are crucial. To be more specific, by correctness we here mean functional correctness with respect to functional specifications and also absence of errors that may cause safety and cybersecurity issues. By quality, we mean all other properties typically expected in embedded code, as defined in coding standards and guidelines such as MISRA-C [23] and \"the power of 10\" rules [14].\nIn the present paper, we consider the problem of using LLMs to generate source code for critical embedded software. We make the following two contributions:"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Deductive Verification of C Programs", "content": "FRAMA-C [7] is an open-source, extensively developed framework that verifies C programs annotated with ACSL [4] specifications. Its WP plug-in enables users to verify that C code meets ACSL specifications through deductive verification [1,13]. Specifically, WP utilizes an advanced form of weakest precondition calculus [3,20], which inspired its name. ACSL specifications are placed within special comments \"/*@ ...*/\" and primarily include function contracts and code annotations. Function contracts feature pre-conditions (requires clauses) and post-conditions (ensures clauses), which are pure logical formulas that need to be verified before and after any function call. The assigns clause specifies the frame conditions of the function, i.e. the set of memory locations, including global variables and pointers, that are allowed to be modified by the function. Code annotations, such as assert clauses, are pure logical formulas linked to specific points in the program that must be validated on every execution path that passes through these points. Ghost variables are variables that are evaluated in a static runtime environment disjoint from the normal heap and stack. These can be related to the normal program variables by ghost statements, but may not affect the program."}, {"title": "2.2 Large Language Models", "content": "In recent years, Large Language Models (LLMs) have shown impressive progress in a wide range of tasks. Notably, there has been a significant increase in the number of LLMs designed specifically for coding tasks, particularly in the area of code generation. LLMs utilize Transformers [36], which are the most advanced neural architecture in natural language processing. Additionally, Transformers have proven highly effective in addressing classic problems in verification [9,11], reasoning [21], and in the auto-formalization of mathematics and formal specifications [12,8]. OpenAI's gpt-3.5 and gpt-4-turbo build on the pre-trained GPT-3 model, with additional fine-tuning using Reinforcement Learning with Human Feedback (RLHF) [27]. Although they are not specifically optimized for code generation, both models have shown impressive performance on various related tasks [24,30]."}, {"title": "2.3 Prompting LLMs", "content": "Prompts are user-provided inputs, such as queries and instructions that guide LLMs and instruct their behavior for specific tasks. Previous work [38,37] have demonstrated that the quality of input prompts plays a crucial role in the performance of LLMs, significantly influencing the quality of the output. A prompt template is often used to generate prompts in a consistent and reproducible way. Chain-of-Thought (CoT) prompting [31] has significantly enhanced zero- and few-shot performance in various complex reasoning tasks, such as arithmetic, commonsense, symbolic, and logical reasoning. A crucial feature of CoT prompting is the use of rationales, which demonstrate the step-by-step reasoning process. In this work, we design our prompts based on Zero-shot-CoT [19], which involves merely introducing the rationale-triggering sentence \"Let's think step by step\" to the LLMs, which leads to significant improvements in zero-shot performance."}, {"title": "3 Spec2code Framework", "content": "Kambhampati et al. [17] examines the role of LLMs in planning tasks known as LLM-Modulo Framework. It concludes that while LLMs cannot independently plan or verify, they can assist within a hybrid framework that integrates LLMs with external critics.\nBuilding on this core principle, we introduce spec2code which aims to leverage the capabilities of LLMs to generate code from both informal and formal specifications via prompting. spec2code aims to make use of the generative power of LLMs while ensuring the functional correctness and quality of the code generated through software verifiers."}, {"title": "4 Case Studies", "content": "Our case study focuses on three automotive control modules obtained from Scania: the Steering Fluid Level Detection (SFLD) module, the Brake Light Activation (BRAK) module, and the Power Steering Backup (STEE) module. We assess the correctness and quality of the generated code. This work not only evaluate the feasibility of using LLMs for such tasks but also aims to identify potential limitations and areas for improvement in the context of automotive software engineering."}, {"title": "4.1 Code Style of Application Modules in Embedded/Automotive Software", "content": ""}, {"title": "4.2 Working with the case studies", "content": "All three case studies are taken from Scania's development repository. Each case study has a set of natural language specifications of varying quality associated with it. From these natural language specifications, the ACSL specifications were derived by hand.\nWe restrict all case studies to one single translation unit each, meaning that the interaction with library code has been abstracted away. For example, the interaction with RTDB has been removed, and instead the relevant signals are assumed to be globally defined static variables. Importantly, the actual code functionality has not been changed, and therefore the modified code follows the original code."}, {"title": "4.3 Case study 1: Oil Level Warning (SFLD)", "content": "The oil level warning module is a software responsible for emitting diagnostics warnings whenever the oil level has been low for a specific amount of time. The original module has 200 LoC, not including declarations and type definitions from header files. There are 1 high-level natural language specification, 11 low-level natural language specifications, and 10 derived ACSL specifications for this module."}, {"title": "4.4 Case study 2: Brake Light Activation (BRAK)", "content": "The brake light module is a software responsible for activation of brake lights. The original module has 400 LoC not including declarations and type definitions from header files. In total, there are 1 high-level natural language specification, 17 low-level natural language specifications, and 17 derived ACSL specifications for this module"}, {"title": "4.5 Case study 3: Power Steering Backup (STEE)", "content": "The power steering backup module is software responsible for activation of a backup system in case the primary power steering system fails. This module and its specifications has been thoroughly studied in a previous paper [34]. In the present paper, a modified version has been used. There are 1 high-level natural language specification, 5 low-level natural language specifications, and 5 derived ACSL specifications for this module. This version of the module has 150 LoC."}, {"title": "5 Experimental Setup", "content": ""}, {"title": "5.1 Specification Types", "content": "1. High-Level Natural Language (HLNL): Set of high-level specifications that captures the overall functionality and constraints of the system focusing on what the system needs to achieve. A high-level specification is shown below.\nHigh-Level Natural Language specification 1\nIf there is a brake light request, then the truck lights shall be activated."}, {"title": "5.2 LLM Selection", "content": "In our experiments, we evaluate our framework on current SotA LLMs namely gpt-4-turbo [26] and gpt-3.5 [25]. These models were chosen because they offer API access, which allows us to leverage their capabilities without the need for high memory requirements. For gpt-4-turbo and gpt-3.5 we configure the parameters of the LLMs as follows: max_token : 4096, temperature : 0.8, presence-penalty : 0.5, frequency-penalty : 1, and top-p: 1. A temperature of 0.8 is preferred for our experiments because it introduces controlled randomness, allowing for more diverse and creative programs while still maintaining coherence."}, {"title": "5.3 Prompt Design", "content": "We designed a prompt based on the Zero-shot-CoT prompting technique to guide the LLM in generating code from ACSL specifications, as well as high-level and low-level specifications provided in natural language. We first specify the system prompt to define the intended role, capabilities, and behavior of the LLM. Next, we specify the user prompt that instructs the LLM to complete the given task, i.e., completing the function definition from specifications. We provided the structure of the C program, including the header, global variables, and function header, as part of the prompt (See Appendix A for prompt template)."}, {"title": "5.4 Evaluation of Generated Code", "content": "In all case studies, each combination of model and specification produces a generated program which is then compiled. If the compilation is successful, then the program is evaluated in three different ways: functional correctness, program equivalence, and code quality.\nFunctional correctness. The functional correctness of the program is verified with respect to its ACSL specification by using the weakest-preconditions calculus plugin (WP) in Frama-C. We choose Frama-C as a verification framework because we deem it to be mature and of state-of-the-art. In our evaluation we use the Frama-C version 27.1 with the backend solvers Z3 [10] and Alt-Ergo [6]. The results can be seen in section 6.\nProgram equivalence. The functional correctness verification shows whether the implementation satisfies the specification, but it is possible that the specification itself might be incomplete. By incompleteness, we mean that the set of program behaviors specified by the specification might not cover the intended program behaviors of the code. We evaluate the relative correctness of the original code from the Scania repository and the code generated by the LLM using program equivalence based on program semantics.\nAll case studies include one handwritten program each, which is verified w.r.t. its formal specification, and is considered to be the reference program. The generated programs are compared to the relevant reference program, and any semantic deviation is captured. Concretely, the evaluation is done by using the tool diffkemp [22]. This tool can show program equivalence on instruction-level, for programs written in LLVM byte-code. A set of semantics preserving transformations is used to accomplish this task. Hence, the tool can fail to show equivalence either because the programs are not equivalent, or because the programs are too different structurally.\nCode Quality. The code quality is evaluated quantitatively by counting the lines of code (LoC), and qualitatively by evaluating its conformance with \"the power of 10\" rules [14]. Only the lines of code in the implemented function are counted, and all empty lines and comments are removed. Generally a longer program implementing the same functionality as a short program can be considered to be more complex, and of lower quality. However, analysis of LoC is surely not sufficient for evaluating the code quality. Hence, we also manually inspect the generated code and evaluate its conformance with \"the power of 10\" rules. A summary of the rules is the following:\n1. Avoid complex flow constructs, such as goto and recursion.\n2. All loops must have fixed bounds.\n3. Do not use dynamic memory allocation after initialization.\n4. Restrict size of function to around 60 LOC.\n5. (Use a minimum of two runtime assertions per function.) Not used.\n6. Restrict the scope of data to the smallest possible."}, {"title": "6 Results", "content": "In this section, we discuss the results of our research by placing them in the context of the three research questions:\n1. Capability of current SotA LLMs to generate code based on formal, high-level, and low-level specifications (RQ1).\n2. Effectiveness of input prompt design on the LLM's performance (RQ2).\n3. Evaluation of both functional correctness and overall quality of the LLM-generated code (RQ3).\nIt is worth noting that all compilable programs produced by the LLM adhered to the \"power of 10\" rules, and no warnings were issued by the GCC compiler. Consequently, our analysis tables focus solely on the Lines of Code (LoC) metric."}, {"title": "7.1 Threats to Validity", "content": "Internal Validity. A potential threat to the internal validity of our method is the natural non-deterministic behavior of LLMs, which leads to unpredictable and non-repeatable code generation. We experimented with various temperature settings and ultimately set the LLM's temperature to 0.8 to control the randomness in the code produced. Another, a potential threat to the internal validity, when evaluating performance of LLMs for code generation, is that the code examples used in the evaluation have been part of the training data. This threat is completely avoided in the experiments of the present paper since all case studies use proprietary, not published, code from the manufacturer Scania."}, {"title": "8 Conclusion", "content": "The paper has studied how code generation by LLMs can be combined with formal verification with the aim of producing critical embedded software. The first contribution is a general framework spec2code, where LLMs are combined with different types of critics. In the second contribution, as a first feasibility study, a minimalistic instantiation of spec2code, without iterative backprompting and fine-tuning, was applied to three industrial case studies from the heavy vehicle manufacturer Scania. The goal was to, from specifications only, automatically generate industrial quality code that could be hypothetically integrated in the Scania production code.\nDespite not using iterative backprompting and any fine-tuning, we were able to produce successfully compiled code in all three case studies. The code was also formally verified, for some combination of specifications, in two of the three case studies. We view this result as quite promising, although the case studies were all relatively small and of low complexity. However, we expect that with the addition of iterative backprompting and fine-tuning, which we plan as future work, the approach will also be much more powerful and become capable of handling more challenging coding problems. Additionally, we intend to include the pass@k_in future work as this will offer a more comprehensive understanding of the LLMs ability to generate code by accounting for the presence of a correct program among the top k candidate programs. Furthermore, while we acknowledge that closed-source models for scientific evaluation may be sub-optimal, we aim to address this concern in the future work by evaluating our framework using open-source LLMs.\nAn important conclusion of the work is that we note that, while a human programmer utilizes specifications and context knowledge, the LLMs in spec2code only have access to specifications. Thus, the correctness of the generated code is with respect to the specifications only. This implies that to have complete specifications becomes crucial. This we judge to be rare in real industrial software development. This raises the question if the extra efforts needed to make specifications complete, is more or less compared to the effort saved by delegating the programming to LLMs. This is also something we plan to investigate in future work."}]}