{"title": "Transfer Learning in Vocal Education: Technical Evaluation of Limited Samples Describing Mezzo-soprano", "authors": ["Zhenyi Hou", "Xu Zhao", "Kejie Ye", "Xinyu Sheng", "Shanggerile Jiang", "Jiajing Xia", "Yitao Zhang", "Chenxi Ban", "Daijun Luo", "Jiaxing Chen", "Yan Zou", "Yuchao Feng", "Guangyu Fan", "Xin Yuan"], "abstract": "Vocal education in the music field is difficult to quantify due to the individual differences in singers' voices and the different quantitative criteria of singing techniques. Deep learning has great potential to be applied in music education due to its efficiency to handle complex data and perform quantitative analysis. However, accurate evaluations with limited samples over rare vocal types, such as Mezzo-soprano, requires extensive well-annotated data support using deep learning models. In order to attain the objective, we perform transfer learning by employing deep learning models pre-trained on the ImageNet and Urbansound8k datasets for the improvement on the precision of vocal technique evaluation. Furthermore, we tackle the problem of the lack of samples by constructing a dedicated dataset, the Mezzo-soprano Vocal Set (MVS), for vocal technique assessment. Our experimental results indicate that transfer learning increases the overall accuracy (OAcc) of all models by an average of 8.3%, with the highest accuracy at 94.2%. We not only provide a novel approach to evaluating Mezzo-soprano vocal techniques but also introduce a new quantitative assessment method for music education.", "sections": [{"title": "1. Introduction", "content": "In vocal education, quantitative assessment of vocal techniques has always been an issue that remains unsolved. This issue mainly results in the diversity of educators with their teaching methods\u00b9 and learners' differences in voice and physical conditions\u00b2. These factors hinder the development of systematic quantitative assessment, especially in evaluating rare voice types such as Mezzo-soprano, Countertenor, and Bass\u00b3.\nFrom the physiological viewpoint, voice types are determined by several factors, including the length, thickness, and tension of the vocal folds, as well as the size and configuration of the oropharyngeal cavity. For example, mezzo-sopranos usually have slightly longer and thicker vocal cords, enabling them to produce a lower voice. Because of this, they own a wide range, deep timbre, and excellent performance in the lower-middle register. Few female singers have this unique kind of vocal condition. In addition, due to the particular structure of the vocal folds, the mezzo-sopranos(compared to the sopranos) require more support from the diaphragmatic and the core respiratory muscles in order to maintain a sustained vibration of the vocal folds at specific pitches+ 6. The difficulties of technical training further exacerbate the scarcity of mezzo-sopranos. The Mezzo-soprano has a high bias in chest voice (authentic voice), which makes the normalization of high and low registers and training over the voice change region more complex than other types.\nIn traditional vocal assessment, teaching and evaluation over-reliance on the educator's experience and subjective judgment, making it difficult to objectively and accurately reflect the real level of the learner's singing ability. In this context, deep learning techniques demonstrate significant potential for application, especially in processing audio data and quantitative analysis of metrics8,9.\nIn recent years, there has been tremendous progress in applying deep learning to audio classification10-15. Convolutional neural networks (CNNs) have demonstrated strong feature extraction capabilities in audio classification10. The convolutional recurrent network (CRNN) proposed by Choi et al. significantly improved classification accuracy\u00b9\u00b9. Zhang et al. demonstrated that CNNs have achieved comparable accuracy to human beings in music genre classification achieved accuracy comparable to humans12, and Stefanus et al. achieved accurate classification of choir vocal music by CRNN13, further extending the application of deep learning to audio tasks. Demir et al. applied CNN on an environmental sound dataset and achieved good classification results 14. Guzhov et al. proposed the ESResNet network15 to improve environmental sound classification performance further. All of the above works extend the application of deep learning on audio data. However, they all rely on many well-labeled datasets and still have limitations when dealing with scarce vocal data with few samples.\nExisting publicly available audio datasets are designed to cover a broad range of vocal types but remain under-represented for specific Bel canto voice parts, such as mezzo-sopranos14,16\u201320. Large-scale audio datasets like VGGSound and FSD50K provide valuable resources for audio classification studies18,20, but they lack sufficient data to represent rare vocal types in the field of Bel canto. Existing professional vocal datasets are also inconsistent in labeling important points such as pitch, timbre, and technique, making them inefficient to use for quantitative analysis21.\nThe lack of sufficient data has posed a significant challenge to apply deep learning in audio classification20. Transfer learning emerges as an effective solution 22\u201325.Transfer learning is a method of extending a training model for a specific task to another target task to extract valid features for the new task based on the prior knowledge of the source task25,26. In recent years, transfer learning has been widely applied in small-sample learning24,25,27, image processing28-31 and audio classification 22,23,27.\nTo address the above issues, we constructed the Mezzo-soprano Vocal Set (MVS) dataset firstly. Then, we used models pre-trained on publicly available datasets, such as UrbanSound8K and ImageNet, to initialize the weights of our deep learning model32\u201334. Finally, we transferred the model to the MVS dataset for fine-tuning, which enables efficient learning of mezzo-soprano vocal technique features. Our approach not only effectively mitigates the issue of data scarcity but also significantly improves the accuracy of vocal technique evaluation.\nIn summary, the main contributions of our work are as follows:\n1.  First, we construct a mezzo-soprano dataset for the audio evaluation of scarce vocal parts and carefully label ten professional vocal techniques according to a unified evaluation criterion.\n2.  We select three CNN-based deep learning models for audio evaluation35-37 and further improve their generalization performances through transfer learning of pre-trained models for large datasets. Our experimental results show that our method improves the overall accuracy (OAcc) of all models by an average of 8.3%, with a maximum accuracy of 94.2%.\n3.  Our work provides an effective solution for the technical evaluation of the mezzo-soprano voice type and a personalized and efficient teaching tool for voice educators. At the same time, learners can engage in self-reflection and active learning according to the objective scores predicted by the model and develop independent learning techniques."}, {"title": "2. Proposed method", "content": "In order to study the assessment of vocal technique for rare voices38, we construct the Mezzo-soprano Vocal Set (MVS) dataset focusing on Mezzo-soprano, which collects recordings of several professional singers in vocal singing, as well as singers majoring in vocal performance at the Conservatory of Music and obtains 1,212 high-quality audio segments of mezzo-sopranos. Each segment lasts for three to five minutes, sampled at 48,000 Hz, and saved as a WAV file. MVS aims to comprehensively characterize and evaluate the critical technical features of mezzo-soprano singing, especially in vocal training and education. The audio segments in MVS are labeled under 10 vocal techniques: Vibrato, Throat, Position, Open, Clean, Resonate, Unify, Falsetto, Chest, and Nasal. Each technique is rated on a scale of 1 (highest) to 5 (lowest) based on how well the technique is accomplished. The brief description of each technique is listed as follows:\nVibrato. Vibrato in vocal music refers to the vibrato of the voice. A relaxed and rounded state of muscles and an unobstructed breath produce a standard, beautiful vibrato. Unscientific vocalization, such as muscle tension and tongue pressure, can result in a fast or slow vibrato or a dull, straight sound without vibrato.\nThroat. The throat plays a key role in controlling the airflow, which means that a right position of the throat can help singers make full use of the airflow to produce enough energy for volume control and regulation. At the same time, a stable throat can help singers maintain the stability and durability of their voices, and keep the quality and expressiveness over a long period of time.\nPosition. In the field of Bel canto, position refers to the singer's adjustment of the position of the throat to control the flow of the air. Singers need to learn to put their voices into the right places, such as specific areas of the head, nose or mouth, to achieve optimal resonance and vocal clarity.\nOpen. Cavity opening refers to the state in which the singer's throat and respiratory system are adjusted to allow the voice to vibrate and resonate freely in the mouth and throat.\nClean. Clean in vocal music refers to the purity and clarity of the sound and how accurately it matches the pitch. This concept is closely related to the quality and accuracy of the sound.\nResonate. Resonance is the process of utilizing body cavities to enhance the timbre and volume of a sound. It includes the resonance of the oral, pharyngeal, nasal, and head and chest cavities. Right throat position and opening can help a singer achieve pharyngeal resonance, resulting in a fuller and more powerful voice.\nUnify. Experts pay attention to the unity of the sound in each voice area in the field of Bel canto. The principle of unity of the voice area of Bel canto requires the consistency in vocal method of the high, middle, and low voice areas, as well as the voice position and the timbre and volume of the voice.\nFalsetto. Falsetto is the sound effect of the vibration of the edges of the vocal cords in singing. In the field of Bel canto, falsetto usually refers to those areas of the voice which are higher than the average chest voice, with a lighter, thinner sound texture, thus giving the audience an ethereal, floating feeling.\nChest. Chest in vocal music refers to the sound effect of the overall vibration of the vocal cords in singing. It is also known as full voice or chest voice and is a singing technique based on chest resonance. In the field of Bel canto, control of the chest voice is very important because it affects the expressiveness of the musical work and the singer's vocal health.\nNasal. The nasal tone in the singing timbre is wrong, resulting in muffled, unclear singing timbre, poor resonance, poor penetration, and other problems. We often apply binary standard to classify the nasal tone timbre aiming at judging the singer's singing quality."}, {"title": "2.2 Mel-frequency Cepstral Coefficient", "content": "We preprocess the given audio file into Mel-frequency Cepstral Coefficient (MFCC) features 39 before further investigations. MFCC features are widely used in issues such as speech recognition, sentiment analysis, and voiceprint recognition. MFCC captures features containing sound information by simulating the working mode of the human ear, and its transformation process is defined by the following formula40:\n$mel(f) = 2595 * log [log_{10} (1+\\frac{f}{700})]$ (1)\nwhere f is a linear frequency. We first perform pre-emphasis filtering on the speech signal to enhance the energy of the high-frequency part and compensate for the high-frequency attenuation caused by the channel effect. Calculation is conducted according to the formula:"}, {"title": "2.3 Transfer learning", "content": "We pre-trained the model on a large-scale dataset, and after multiple rounds of training, we have found a better local optimal solution from the loss function26,41,making the loss function reach a minimum value:\n$ \\theta^* = \\underset{\\theta}{argmin} [L(\\theta)]$ (9)\nwhere the loss function of the pre-trained model is L(\u03b8).\u03b8 is the weight parameter of the model. During the fine-tuning process on the MVS dataset, we did not start with random initialization. Instead, we started training from the pre-trained model's \u03b8*, and the new objective loss function is:\n$L_{new}(\\theta^*) = \\sum_{i=1}^{n} l(f(x_i, \\theta^*), y_i)$ (10)\nwhere $L_{new}(\\theta^*)$ is the new objective loss function, $f(x_i, \\theta^*)$ is the model's prediction on the input data $x_i$, l(\u00b7) is the loss of a single sample, and $y_i$ is the target label. \u03b8* is the weight parameter obtained from the pre-trained model. At this point, the model can find out new local optima more easily with less training starting from \u03b8*.\nWe pre-trained the model on a larger dataset to learn universal features. After we fine tune on smaller datasets, the model tends to find an optimal solution that balances generalization ability, rather than overfitting the details of a small number of samples. We use transfer learning methods to effectively transfer extensive knowledge learned on larger datasets to smaller datasets. This not only improves the performance of the model, but also effectively prevents overfitting and exhibits faster convergence speed during the optimization process."}, {"title": "3. Experimental Setup", "content": "In this section, we perform deep learning methods on supervised vocal technique assessment through the MVS dataset. First, in Section 3.1, we propose the problem formulation. Then, in Section 3.2, we briefly introduce the baseline model. Finally, in Sections 3.3 and 3.4, we elaborate on the experimental setup as well as the evaluation criteria for training and testing."}, {"title": "3.1 Problem definition", "content": "We assume a set of n training vocal segments $V = \\{V_i\\}_{i=1}^{n}$ with corresponding labels $Y = \\{Y_i\\}_{i=1}^{n}$. $Y_i$ is a set of labels for the vocal techniques, where n = 10. We study the MFCC spectral characteristics of Bel canto singing to achieve scoring and appreciation of bel canto singing techniques."}, {"title": "3.2 Baseline", "content": "To provide a baseline model for comparison, we extracted MFCC features from audio files of the MVS dataset and trained three CNN models (CRNN35, MobileNet v236, and CAM++37) without using data augmentation strategies to benchmark their performance."}, {"title": "3.3 Pre-trained Models", "content": "We use the same architecture, hyperparameter settings, and training program as it had been used in the baseline model. Three CNN-based networks, CRNN, MobileNet v2, and CAM++, were pre-trained on ImageNet and Ubransound 8k respectively. During the testing process, the predicted results of the dataset refer to the average of the accuracies from three separate training sessions of these three models.\nThe ImageNet dataset contains images from hundreds of object categories, totaling over 14 million images. It contains images of diverse objects and scenes, ranging from images of animals and plants to images of everyday objects. All images are accurately labeled, and each image has a corresponding object category and position label. This allows the dataset to provide accurate reference standards for training and evaluating algorithms.\nThe Urbansound8k dataset is a classic audio dataset containing 8732 detailed annotated urban sound clips, covering 27 hours of audio recording, all stored in WAV format at a sampling rate of 44100 Hz. It is often used to test the performance of sound classification and recognition algorithms, especially in complex urban environments. This dataset covers 10 different categories of urban sounds, such as air-conditioning, car horn, and children playing."}, {"title": "3.4 Training and evaluation", "content": "All of our models are implemented and trained on PyTorch42. We use Adam as the optimizer, with an initial learning rate of 0.0001, and train our model using the cross-entropy loss function.\nDuring the training process, the batch size is 64. In addition, when OAcc(Overall Accuracy) did not improve after more than 10 verifications, we adopted an early stopping strategy to halt the training process. Finally, we evaluated it using the model that achieved the highest accuracy on the validation set. The random seeds for PyTorch and scikit learn are fixed to avoid biased results. We standardize the input data and convert it to a size of 224 \u00d7 224 to facilitate subsequent transfer learning. Finally, the MVS dataset was divided into training, validation, and testing subsets in an 8:1:1 ratio."}, {"title": "3.5 Metric", "content": "We use the highest accuracy on the validation set to evaluate, in which the formula for accuracy is defined as follows:\n$OAcc = \\frac{TP}{TP+FP}$ (11)\nwhere TP (True Positive) is the number of correctly predicted samples, and FP (False Positive) is the number of incorrectly predicted samples."}, {"title": "4. Results", "content": "Tab. 1 illustrates the audio evaluation results of the three models. Thanks to the efficient structure of MobileNet v2, it outperforms CRNN and CAM++ in all scenes. Comparing the baseline without loading the weights of the pre-trained models, all our models are upgraded and improved after pre-training with additional datasets (ImageNet or Urbansound8k). Among them, the increase in accuracy of the models after pre-training on the large-scale image dataset ImageNet is relatively small (0.7% to 1.9%). Meanwhile, pre-training on the Urbansound 8k audio dataset significantly improves the accuracy of the models, with an average increase of 4.3%. Specifically, CRNN shows an increase of 4.9%.\nWhat is interesting is that when we combined Urbansound8k and ImageNet for pre-training, the performances of all models gain significantly. This is especially remarkable for MobileNet v2, which achieved an accuracy of 94.2%, for a 9.0% improvement over the benchmark. Besides, CAM++ gained 9.0%. In conclusion, all models showed a considerable performance improvement after transfer learning the pre-training weights of the additional dataset. It demonstrates the simplicity and effectiveness of the transfer learning for audio evaluation on the MVS dataset."}, {"title": "5. Discussion", "content": "We consider the ImageNet pre-trained model to be an excellent texture detector that can be effectively extended to MFCC spectrograms, as can be seen from Grad-CAM (Fig. 6), which is sensitive to the energy waviness of the audio and adaptively detects the texture of the audio. It helps the model to quickly capture audio features, as seen in the test curve (Fig. 5).\nThe variety of environmental sounds in the UrbanSound8K dataset and the vocal data in our MVS makes it easier for the model to explore the common features of the sounds, even if they are differences in content. The background noise improves the robustness of the model to some extent as well.\nWe believe that joint transfer learning is not a simple \"1+1=2\" process, but rather a multimodal feature adaptation process. The model learns different dimensions of audio features. We will conduct extensive experiments and deeply explore the principles and applications in the field of multimodality in our future investigations.\nIn the future, we first aim to increase the size and volume of the dataset for robust training and exceptionally more extensive vocal material. Then, we will further improve the quality of the labels and refine the score distribution. We will continue to keep pace with advanced transfer learning methods and use more robust models to improve the quality of audio scoring in further investigations. At the same time, we would like to further expand the application scenarios of singing technique evaluation by adding musical emotions to the evaluation system, since music is rich in multidimensional information, and the Bel canto is an artistic crystallization combining music and emotion."}, {"title": "6. Conclusion", "content": "In this paper, we construct a Mezzo-soprano Vocal Set (MVS) dataset to quantify and evaluate vocal techniques by deep learning. At the same time, we pre-train models on ImageNet and Urbansound8k datasets to tackle the issues of low prediction accuracy and poor generalization performance of the evaluation models due to the scarcity of data samples. Our experimental results show that after transfer learning, the OAcc of all models improved by 8.3% on average, with the highest model reaching 94.2%. For the Mezzo-soprano, which is a rare voice type, our work not only provides a scientific method for vocal technique assessment, but also provides educators with a more efficient tool for personalized and effective vocal teaching."}, {"title": "Data Availability Statement", "content": "The publicly available datasets used for transfer learning in this study can be found at https://urbansounddataset.weebly.com/urbansound8k.html and https://www.image-net.org/. Our self-constructed mezzo-soprano dataset, MVS, can be obtained from the corresponding author upon reasonable request."}]}