{"title": "In the Picture: Medical Imaging Datasets, Artifacts, and their Living Review", "authors": ["AMELIA JIM\u00c9NEZ-S\u00c1NCHEZ", "NATALIA-ROZALIA AVLONA", "SARAH DE BOER", "V\u00cdCTOR M. CAMPELLO", "AASA FERAGEN", "ENZO FERRANTE", "MELANIE GANZ", "JUDY WAWIRA GICHOYA", "CAMILA GONZ\u00c1LEZ", "STEFF GROEFSEMA", "ALESSA HERING", "ADAM HULMAN", "LEO JOSKOWICZ", "DOVILE JUODELYTE", "MELIH KANDEMIR", "THIJS KOOI", "JORGE DEL POZO L\u00c9RIDA", "LIVIE YUMENG LI", "ANDRE PACHECO", "TIM R\u00c4DSCH", "MAURICIO REYES", "TH\u00c9O SOURGET", "BRAM VAN GINNEKEN", "DAVID WEN", "NINA WENG", "JACK JUNCHI XU", "HUBERT DARIUSZ ZAJ\u0104C", "MARIA A. ZULUAGA", "VERONIKA CHEPLYGINA"], "abstract": "Datasets play a critical role in medical imaging research, yet issues such as label quality, shortcuts, and metadata are often overlooked. This lack of attention may harm the generalizability of algorithms and, consequently, negatively impact patient outcomes. While existing medical imaging literature reviews mostly focus on machine learning (ML) methods, with only a few focusing on datasets for specific applications, these reviews remain static - they are published once and not updated thereafter. This fails to account for emerging evidence, such as biases, shortcuts, and additional annotations that other researchers may contribute after the dataset is published. We refer to these newly discovered findings of datasets as research artifacts. To address this gap, we propose a living review that continuously tracks public datasets and their associated research artifacts across multiple medical imaging applications. Our approach includes a framework for the living review to monitor data documentation artifacts, and an SQL database to visualize the citation relationships between research artifact and dataset. Lastly, we discuss key considerations for creating medical imaging datasets, review best practices for data annotation, discuss the significance of shortcuts and demographic diversity, and emphasize the importance of managing datasets throughout their entire lifecycle. Our demo is publicly available at http://130.226.140.142.", "sections": [{"title": "INTRODUCTION", "content": "High-quality datasets are a key element to the development of machine learning (ML) models for medical imaging applications and, more generally, for healthcare. Such datasets are characterized by having diverse representation of patients, sufficient sample sizes, accurate labels or annotations, and comprehensive documentation. Failing to meet these requirements has a direct impact on a model's robustness and reliability [10, 23, 149, 165, 196], thereby affecting the model's clinical utility. Inaccurate or incomplete annotations can lead to models that make incorrect predictions. Insufficient or lack of documentation may miss information such as demographics or hospital scanner, leading to biased and inaccurate models [17, 124]. A lack of diversity limits the generalizability of the model across heterogeneous patient populations, whereas a sufficient sample size is necessary to ensure that the model can learn meaningful patterns and avoid overfitting. Ultimately, dataset quality is as important as the choice of the method to build the ML model.\nDespite the critical importance of data, current efforts in medical imaging do not consider the evolving nature of datasets [73]. Such datasets often consist of two parts: images such as chest X-rays, and target labels such as lung diseases. However, additional evidence about these datasets, such as shortcuts, biases, or additional annotations, emerges over time, but is often not available in the original dataset documentation. We refer to these newly discovered aspects of datasets as research artifacts. Literature reviews, both in medical imaging and in ML more generally, primarily focus on ML models (see [11, 20, 105, 166, 193] for examples), with only a few addressing specific issues such as fairness in model predictions [18, 19, 149]. Some reviews summarize available datasets within a specific application such as dermatology [26, 184] or ophthalmology [91]. A recent work [45] assesses the documentation of publicly available magnetic resonance imaging (MRI), color fundus photography, and electrocardiogram datasets, yet carries out a static review that is reviewed"}, {"title": "PROPOSAL: A LIVING REVIEW OF MEDICAL IMAGING DATASETS", "content": "Development of novel algorithms with state-of-the-art results is considered the more prestigious activity within the ML field [9, 156], leading to datasets often being considered \u201cas-is\u201d benchmarks. However, medical imaging datasets not only are foundational to ML development but they also evolve over time, both explicitly and implicitly. A dataset evolves explicitly if the data itself is updated, for example, due to errors. A dataset evolves implicitly as new evidence emerges, such as erroneous target labels, additional annotations, and shortcuts [122, 124]. For example, the CheXpert dataset [76] was initially published without the recommended datasheet [49]. A datasheet co-authored by some of the CheXpert authors was later released [48], but this datasheet is not linked from the original dataset. By not incorporating such evidence into the original dataset, it is often not taken into consideration in subsequent research.\nTraditional systematic reviews of medical imaging datasets, such as [26, 91, 99, 184], do not capture this evolution. We argue that we need a living review to keep track of novel open datasets, as well as emerging connections and issues in existing datasets. Living systematic reviews are a more recent development in meta-research, but they are crucial for rapidly evolving topics, such as for example evidence around COVID in recent years [190]. Given the rapid developments in ML, it would be advantageous to adopt a similar framework for datasets. Various best practices for living reviews already exist in other fields, for example [161, 162].\nHere we outline our vision for the living review of open datasets, studying the relationships between the datasets and their research artifacts. We propose a framework of these relationships, as well as protocols for the maintenance of the living review.\nConceptualization. Our living review consists of three parts, as illustrated in Figure 2:\n(1) an overarching living review publication,\n(2) documentation of research artifacts via dataset-specific publications on Zenodo, which the overarching paper links to,\n(3) a SQL database for exploring the links between the datasets and the research artifacts.\nAn \"artifact\", like a Greek amphora, is a produced object \u2013 the output of a process. We define a research artifact as any additional evidence related to a dataset, for example derived datasets like PruneCXR [69] and LongTailCXR [70], additional annotations like spurious correlations [25], segmentation masks [43], or compressed information in the form of embeddings [163]. Such artifacts are crucial as they provide the necessary context for understanding, reproducing and validating research findings. Moreover, computer science papers with shared artifacts receive about 75% more citations than those without [41], underscoring the critical role of artifacts in the visibility and impact of research. We map the relation between the datasets and the research artifacts with the citation function (use, produce, extend, introduce, other). We exemplify this for the CheXpert dataset in Fig. 2, for more details and examples please see Tables C1-C2-C3.\nImplementation. The overarching, continuously updated, living review publication will link the dataset-specific publications (peer-reviewed or preprints) where smaller teams can document datasets by building on existing guidelines [47, 152], and extending them by documenting resources that provide additional evidence related to responsible use of these datasets, such as demographic biases (Section 5) or shortcuts (Section 6). These datasheets could be hosted on Zenodo [36] which allows each datasheet to have its own Digital Object Identifier (DOI). To enhance interoperability and findability, we could package the documentation in a structured format like Croissant [3]. At regular intervals, we can update the overarching living publication, linking to the newly created datasheets."}, {"title": "CONSIDERATIONS FOR CREATING A DATASET", "content": "Relevance to the living review. Researchers can access the recommended documentation to better understand decisions made throughout the dataset creation process, including the translation from the clinical problem, balancing metadata sharing with patient privacy, and considerations regarding dataset size.\nTranslation of clinical problem to ML problem. ML applications in healthcare are often related to specific parts of the treatment of a patient. Consequently, the development of a dataset will depend on translating these clinical challenges into ML problems that can be evaluated with ML metrics. This translation requires aligning expectations among various stakeholders in the multidisciplinary team while also establishing a utility criterion, for example, a reduction of workload of the radiology department [173, 194]. The right definition of the ML problem is essential for determining the necessary data (whether newly collected or already existing, such as from retrospective studies), designing an appropriate labeling process, assessing the usefulness of the proposed methods, and understanding the potential limitations of such evaluations from a clinical standpoint [148]. This step also helps to identify potential risks or pitfalls in the data creation process, where biases or shortcuts may be introduced. Importantly, we must remember that an ML dataset is always a proxy, and therefore achieving high performance on the ML task, such as detecting cancer, does not necessarily translate to desired outcomes, such as detecting cancer at earlier stages and reducing patient mortality.\nDesign of ground truth. Data is never truly raw or objective [38, 52, 196]. While we might hope for medical imaging data to be different, and only capture objective reality, this is not the case. Research shows that medical datasets are defined through the painstaking work of multidisciplinary teams within specific clinic, geographical, legal and socioeconomic contexts [115, 119, 196].\nThe processes shaping medical imaging datasets begin before any data is collected [196]. In particular, regulatory constraints determine what data can be collected and predetermine its purpose. However, issues arise during data acquisition, as data collection is often poorly defined, resulting in variable quality. In contrast, clinical trials follow more regulated protocols. Conducting such structured pilot studies can help refine and standardize consistent acquisition protocols. The context of creation and use direct the design of the datasets and can be accounted for through purposeful investigation of local assumptions and meanings embedded in the dataset. Similar to computer vision datasets [160], commercial and operational pressures hint at the fact that medical imaging datasets have their own politics and are created with a specific purpose, whether commercial or public. To this end, the development of a data management plan is increasingly required to streamline the data lifecycle process [73].\nImpact of labeling process on data quality. The consequences of labeling decisions are better understood than those of earlier stages and include, among others, the clinical relevance of ML models [124, 125], the proliferation of social inequality and exclusion [98], and the impact on the performance of trained ML models [17]. To address those challenges, we must look into the processes, guidelines, and incentives of labeling [40], as well as the interpersonal and organizational structures of entities responsible for that work [115, 119]. For example, epistemic differences within multidisciplinary teams creating medical imaging datasets - such as misunderstandings regarding clinical terminology like \"opacity\" in chest X-rays, with different meanings across countries or lacking direct translations - can affect the clinical outcomes of developed models [196]. Overlooking design choices in medical datasets can lead to misalignment between ML systems and the real-world needs, (see also Section 4). Recognizing these datasets as constructed [144], not objective, is essential to their fair and equitable use in medical practice.\nTrade-off between metadata and patient privacy. Metadata such as demographics is essential for evaluating the robustness and fairness of ML models. However, the need for detailed metadata often clashes with the imperative to protect patient privacy, as even anonymized data can carry risks of re-identification and misuse. We do not cover this in detail in this work, but various methods for federated and privacy-preserving ML have been developed, for example [86, 151].\nData size. ML systems are often expected to perform better with more data [87], as has been both observed in practice, and shown by statistical learning theory [181] wherein error tolerance, statistical dependency of the samples, data dimensionality, and model capacity all interact with the model performance. Datasets in medical imaging have grown from hundreds to thousands, with the largest public datasets like CheXpert [76], MIMIC-CXR [82], and Emory breast [78] with up to hundreds of thousands of patients. This contrasts with general computer vision datasets, which often contain millions of images and serve as the basis for many empirical findings. In industry, medical datasets are also scaling up to millions, offering advantages for model development.\nWith (smaller) public datasets, a common solution is to inject additional knowledge from another source into the dataset, such as domain knowledge provided by experts, or using data or representations from a different source. Transfer learning [20, 134] is therefore often used in medical imaging. A common approach is to fine-tune models pretrained on ImageNet [155], although recent results show this strategy is more sensitive to shortcuts [84] than if training on RadImageNet [114], a recent dataset with a million images from different radiological modalities.\nGiven the various factors contributing to the interaction between the data and the learning performance, there are no guarantees that larger datasets will necessarily result in better target models. For example, merging datasets from different sources can lead to shortcuts and biases [23, 167]. In this regard, dataset distillation [195] which aims to reduce the size of the data while maintaining or improving its representativeness, could lead to more robust algorithms, despite the \"bigger is better\" intuition."}, {"title": "DATA ANNOTATION PRACTICES AND QUALITY", "content": "Relevance to the living review. Linked documentation of the annotation process (annotation guidelines + annotations) reflects observer variability and trade-offs between annotator expertise and cost.\nLabeling tasks in medical imaging depend on the context of the disease, the task itself (classification, segmentation, etc) and how the target labels are acquired. For example, ground truth labels for lesions or nodules could be confirmed via biopsies, while other (gold standard) labels or annotations could be based on interpretation of the experts or other annotators. Here we discuss two important considerations which vary with the task: inter-observer variability and expertise-cost trade-offs.\nInter-observer variability. Establishing observer variability of the task at hand is crucial before data collection and annotation, as it helps set accuracy targets, determine the required number of annotators, estimate the needed data, and assess variability. Observer variability is measured by having multiple annotators curate a small set of representative examples and then calculating agreement for binary classification, or metrics like Dice score for segmentation [109]. Notably, observer variability varies greatly according to the task, anatomical structures and their sizes, image type, and expertise level [83]. For example, mostly healthy large organs, e.g., the lungs, the liver, and the brain, imaged on volumetric scans will have a low observer variability, while small structures, e.g., lung nodules, will show larger variability.\nObserver variability is also influenced by inconsistent training across institutions (e.g., [6]) and insufficient guidelines for annotations [143]. Comprehensive labeling instructions [143], task-specific training [27], and a structured feedback loop can help mitigate this, but are not always feasible if the annotation task is not part of the clinical workflow, and therefore might be more often used in industry datasets.\nExpertise and cost trade-offs. Selecting the right annotators for a task is another critical consideration [142], and given the growing demand for annotated data, careful thought must be given to matching the annotators' skills to the task's requirements, both in terms of accuracy and detail of the annotations needed, and domain-specific contexts, for example when diseases have different prevalence across countries.\nDomain expertise of annotators can vary from clinicians to laypersons. Expert-annotated data can be collected from (retrospective) studies at hospitals, although some types of annotations needed for ML (such as granular annotations like image contours) may not be created as part of the clinical workflow. In such cases only weakly-labeled data might be available, and/or additional annotations might be created by other experts or graduate students for example. In industry, the commonly cited phrase \"Garbage in - garbage out\" prompts companies to allocate significant resources to ensure high quality annotations and curation, possibly hiring domain-specific experts through their customer base. This can result in datasets with hundreds of subcategories rather than a limited set of high-level categories, more granular annotations, and annotation workflows with various quality control measures.\nMedical expertise is not always required for medically-related annotation tasks. For example, studies shown that laypersons are able to detect surgical instruments in laparoscopic images [108], and several other successful results with crowdsourcing in medical imaging have been reported [131], although often missing details about the annotation process. Additionally, annotations for training data typically do not require the same level of accuracy and detail as testing data used for evaluating the generalizability of the developed algorithms. This can allow using weakly-labeled training with scribbles [103, 106] or bounding boxes [24, 128], or in 3D data, leveraging sparse annotations only from a few slices within a volume [139]. Taken together, these strategies can allow employing novice annotators for annotating training data while reserving domain experts for curating the test set.\nIt is crucial to emphasize that despite the apparent advantages of crowdsourcing approaches, there is a lot of hidden data work, i.e., the labor involved in data collection and annotation is often invisible and undervalued [136, 156]. There are various reports of unethical practices, for example by tech companies, with regard to data workers who might be dealing with disturbing images or language. The motivations and conditions of workers annotating medical images might be different, but crowdsourcing studies often do not provide this information [131].\nExpertise vs. cost trade-offs: (semi-)automatic approaches. Given the cost of annotation and the growing need for large datasets, various automated methods for annotation have been proposed. Computer vision methods can be used to extract additional features from the image, for example asymmetry, border irregularity or Fitzpatrick skin type in skin lesions [60, 147], and used as additional labels, for example via multi-task learning. Natural language processing (NLP) techniques have been proposed to extract diagnostic labels from unstructured clinical reports. However, this strategy has been shown to introduce labeling errors, for example in chest X-rays [122, 183]. Large language models (LLMs) have shown groundbreaking performance in the general language domain and are expected to unlock new possibilities for analyzing medical texts [172]. However, due to domain-specific challenges (language, contextual nuances, and the ambiguity inherent in medical terminology) their effectiveness is still limited, see for example studies for radiography or free-text CT or MR reports [39, 97, 118].\nHybrid approaches such as active learning [11, 44], active label correction [8] and hard sample mining (collecting additional \"hard samples\" from specific device manufacturers or rare disease subtypes) offer opportunities to combine the advantages of both human annotation and automated methods. While such methods are popular in literature, in academic papers the human annotators are sometimes simulated by giving the algorithm access to the existing labels in the data, while in industry, the (re)-labeling process is more often done with domain experts. This also allows industry"}, {"title": "INSIDE A DATASET: DEMOGRAPHICS", "content": "Relevance to the living review. We present four case studies (chest x-rays, skin lesions, brain MRI and fetal ultrasound), each with unique properties. We propose addressing these factors in our living review, emphasizing the importance of the documentation of the study design and patient demographics.\nThe potential of overfitting to established benchmark datasets is a long-standing debate in the ML community [57, 71, 101]. While the past few years have seen a surge in public datasets, they typically lack demographic metadata about the subjects. This leads to a host of problems. If the data does not come with demographic information, researchers cannot assess whether the datasets and models trained on them have demographic biases. As a result, not only the resulting algorithms, but also what we learn about ML development, can be tainted by demographic biases without our knowledge. Equally important, research on algorithmic bias and fairness, which needs demographic metadata, has access to very few datasets. As a result, large amounts of research resources have been dedicated to a very small set of case studies, whose particularities - for better and for worse - drive the progress of the research field. Here we present four case studies, illustrated in Figure 3, further clinically relevant details in Section B.\nCase study 1: chest X-rays. Chest X-rays are the most commonly performed radiologic examinations worldwide [176], requiring significant expertise for accurate and meaningful interpretation [179]. ML revolutionized chest X-ray diagnosis, highlighted by the release of NIH-CXR14 dataset [183] and CheXNet's model claiming radiologist-level pneumonia detection [146]. However, these claims have been criticized for relying on shortcuts [81, 124], and for low inter-rater agreement [25]. Subsequently, additional datasets such as CheXpert [76], MIMIC-CXR [82], and PadChest [14] have become widely used in the research community.\nWith the primary purpose of diagnosing and/or monitoring pathologies, chest X-ray datasets often include demographic information like age, gender, sex, race and ethnicity to analyze potential biases\u00b9. While age and gender are typically included, (self-reported) race or ethnicity are only available in a few datasets, such as MIMIC-CXR and CheXpert [135]. Age is generally skewed toward older populations (PadChest median age 62, MIMIC-CXR largest group aged 60-80 [198]). Age can be predicted from chest X-rays [74], which can lead to favoring well-represented age groups. Even without significant gender imbalance, performance disparities between genders persist [96, 164, 165]. Balancing the data has proven ineffective, and studies have ruled out causes such as under-representation [96], physiological differences [186], and shortcut learning [81, 124, 129]. Less is known about the effect of label errors, which have different effects on diagnostic labels \u2013 in particular, the \"no finding\" label is known to often be associated with follow-up images of patients [123]. Performance disparities between racial groups favoring white individuals have been noted [164]. ML can infer protected attributes like race, despite this being a challenging task for human experts [51]. Recent research [53] suggests that fine-tuning on specialized datasets alone cannot reduce the influence of these protected attributes.\nCase study 2: skin lesions. In recent years there has been significant growth in ML for dermatology [120], including tasks such as classification [132], segmentation [116], and lesion localization [110]. This surge may be largely explained by the availability of public datasets, such as Fitzpatrick17k [60], PAD-UFES-20 [133], and HIBA [150] as well as the International Skin Imaging Collaboration (ISIC) [77], which aggregates over 490K images from datasets such as HAM10000 [175] and BCN 20000 [67].\nDespite the progress, the under-representation of demographic groups in these datasets limits the generalizability of ML [26]. Skin lesions manifest differently across populations, influenced by factors like skin tone, genetic background, age and UV light exposure [185]. Nonetheless, most public datasets are limited in terms of geographic and skin tone diversity, with a predominance of lighter-skinned patients (Fitzpatrick I to III) [59, 184], potentially resulting in models that underperform for under-represented groups. For example, melanoma - the deadliest type of skin cancer - is much more prevalent in lighter-skinned individuals but can also affect those with darker skin tones, who might then be misdiagnosed [58].\nRecent efforts to diversify skin lesion datasets, such as the inclusion of PAD-UFES-20 [133] and HIBA [150] in ISIC, have improved the representation of Latin American individuals, a region that was previously under-represented. However, there is still a strong lack of representation in terms of diversity of skin tone. Addressing these disparities is a global challenge that requires a collaborative effort from the research community, drawing on diverse perspectives and contributions from different backgrounds and regions worldwide.\nCase study 3: fetal ultrasound. Ultrasound is the fundamental imaging modality for antenatal care. The acquisition process consists of a physical examination with an ultrasound probe looking for specific 2D slices called standard planes.\nCase study 4: neuroimaging. MRI is the third most commonly performed imaging modality after CT and X-rays. Its superior soft tissue contrast enables detailed visualization of brain anatomy, making it ideal for detecting abnormalities, and most public MRI datasets for ML research focus on the brain [32]. Key application areas for brain MRI and notable datasets, include neurodegenerative diseases (OASIS [92, 94, 111, 112], brain cancer (BraTS [5], LUMIERE [171], Ocana [126], and TCGA-GBM [159]), and stroke, (ISLES 2022 [66], ATLAS v2.0 [102], among others).\nThe conversion from the standard clinical imaging format, DICOM, to NIFTI or other formats is complex and error-prone [100] and depends on how different formats implement DICOM standards. Moreover, many public neuroimaging datasets undergo extensive preprocessing prior to release, often to standarize datasets for open challenges, and ensure that model evaluations focus on algorithm performance rather than the effects of preprocessing. These factors may explain why the neuroimaging community -unlike other disciplines- has made progress in advancing data-sharing practices by adopting standards and tools like BIDS [56], DataLad [63], NITRC.org [90] and OpenNeuro [113].\nDemographic reporting in brain MRI is generally poor. For example, in US studies from 2010 to 2020, 77% report sex, but only 10% and 4% report race and ethnicity, respectively [170]. Studies show performance disparities in models by sex and race, with black females being most affected [31, 75]. Recent efforts to diversify brain MRI datasets include the addition of children (BraTS-PEDS [88]) and Sub-Saharan African populations (BraTS-Africa [2]). Future work should focus on preserving raw data authenticity, enhancing diversity, and more complete metadata."}, {"title": "INSIDE A DATASET: SHORTCUTS", "content": "Relevance to the living review. We suggest documenting and annotating new evidence related to errors or shortcuts in existing datasets.\nDifferent terminology in the literature refers to shortcuts: confounders, spurious correlations, hidden stratification, etc. Shortcuts are decision rules that perform well on benchmark data but fail to transfer to challenging test cases, often with out-of-distribution data [50]. When shortcuts fail, they result in biases and misdiagnosis, for example chest pain in women as anxiety or heart burn [177], misconception that Black patients have high pain threshold [68], or delayed referrals for minority patients who may be judged as malingering or drug seeking [62]. Many shortcuts in medical imaging have been shown, often when the ML model memorizes irrelevant clinical characteristics, like the hospital where the patient was scanned [23]. Hence, when the shortcut is missing, the model performance drops. For example,"}, {"title": "DATA LIFECYCLE", "content": "Relevance to the living review. Our framework incentivizes researchers to adopt better dataset management practices. We recommend using persistent identifiers and storing, and versioning of datasets for reproducibility, and licensing and proper tracking for author attribution.\nEffective research data management is crucial for reproducibility, re-usability, and efficiency. The data lifecycle consists not only of data acquisition and analysis, but rather of data acquisition (see Section 3), data organization and standardization, data and metadata annotation (see Section 4), data management and tracking during analysis, and ultimately, data maintenance. A comprehensive overview, with an example from neuroimaging, is provided in [121]."}, {"title": "STATEMENTS", "content": "9.1 Ethical considerations statement\nWe propose a living review framework to connect publicly available medical imaging datasets with associated research artifacts for the research community. Our proposal is based on publicly available data, and no additional private or sensitive data were collected. Researchers who want to contribute with new evidence to update our database should make sure that their annotation or documentation metadata is compliant with the General Data Protection Regulation (GDPR) in the European Union (EU), the EU AI Act, and other relevant national and international legislation governing data privacy. We briefly discuss considerations about the trade-off between metadata and patient privacy in Section 3, and considerations about FAIR principles and licensing in Section 7.\n9.2 Adverse impact statement\nThe goal of our living review framework is to benefit the research community by improving the usability and accountability of publicly available medical imaging datasets. By providing insights into annotations, errors, and additional findings, our framework helps reduce the risk of misinterpretation or reliance on flawed data. However, without proper stewardship, moderation and ongoing maintenance, there is a risk that insufficiently validated datasets or artifact could be included. It is important to note that our proposal is a data/literature exploration tool for researchers, and should not directly be used for clinical decisions."}, {"title": "ACKNOWLEDGMENTS", "content": "This project has received funding from the Independent Research Council Denmark (DFF) Inge Lehmann 1134-00017B. We extend our gratitude to the speakers and participants of the \"Datasets through the Looking-Glass\u201d webinar and the \"In the Picture: Medical Imaging Datasets\" workshop, which have helped to shape this research. The workshop received funding from the Danish Data Science Academy (DDSA). EF gratefully acknowledges the support of the Google Award for Inclusion Research (AIR) Program. AH is employed at Steno Diabetes Center Aarhus that is partly funded by a donation from the Novo Nordisk Foundation. AH is supported by a Data Science Emerging Investigator grant by the Novo Nordisk Foundation (NNF22OC0076725). TR was supported by a scholarship from the Hanns Seidel Foundation with funds from the Federal Ministry of Education and Research Germany (BMBF). DW received funding from the Jill and Herbert Hunt Scholarship, University of Oxford. MAZ is funded by TRAIN (ANR-22-FAI1-0003-02). We thank Freepick for the icons in Figure 2."}, {"title": "SHORTCUTS", "content": "Machine learning models are prone to rely on spurious correlations to make predictions, which are usually easier to detect than the genuine disease patterns, as explained in Section 6. Such shortcuts can be well-localized objects in the image - e.g., mechanical ventilation tubes or pacemakers, which are often present in patients with certain diseases- or more global ones, like specific noise patterns or intensity distributions associated to a certain acquisition setting or device brand. Demographic attributes like sex/gender, age or ethnicity can also lead to shortcut learning, disproportionately impacting historically underserved subgroups [7], especially when datasets are highly imbalanced. To categorize the different types of shortcuts, we adopt the Medical Imaging Contextualized Confounder Taxonomy (MICCAT) [84], see Fig. A1, which we believe can be useful to understand how spurious correlations arise and how they can be mitigated. MICCAT extends beyond traditional demographic attributes, such as sex/gender, age, and ethnicity, to include a broader set of confounders that are domain- and context-specific. These confounders encompass patient-level and environment-level factors.\nPatient-level confounders include both demographic attributes and anatomical confounders. Demographic attributes, such as gender [1, 96], age [1], and ethnicity [51], represent standard factors typically considered in bias analysis. Anatomical confounders, on the other hand, refer to physical or disease-related characteristics specific to organs or conditions. Examples include body mass index, tissue density, breast density, and bone density, as well as combinations of these factors. Such anatomical variations may define subgroups where models underperform, and their identification often requires analysis beyond standard demographic characteristics [182].\nEnvironment-level confounders include both external and imaging confounders. External confounders arise from physical or virtual elements within the image, such as chest drains [81, 124], pen marks near skin lesions [189], patient positioning [29], or text and measurement calipers [104]. These elements typically produce localized artifacts visible to the human eye. In contrast, imaging confounders result from variations in the imaging process, including differences in equipment brands, scanner types, acquisition parameters, noise, or motion artifacts. Such confounders generally create global artifacts that affect the entire image and may not be perceptible to the human eye. Systematic differences, such as variations in exposure settings for chest X-rays [95] or distinctive characteristics of imaging devices used across different medical centers [23], can unintentionally introduce shortcuts for machine learning models. Rather than learning clinically relevant features, models may instead rely on these acquisition-specific factors, which are associated with disease labels but do not represent true underlying medical conditions [130]."}, {"title": "CLINICAL CASE STUDIES", "content": "Case study 1: chest X-rays. Chest X-rays are the most commonly performed radiologic examinations worldwide [176", "179": ".", "183": ".", "146": ".", "124": "and for demonstrating low inter-rater agreement for pathologies like pneumonia [25", "14": ".", "76": "MIMIC-CXR [82"}, {"14": "have become widely used in the research community.\nWith the primary purpose of diagnosing and/or monitoring treatment of various pathologies", "135": ".", "198": ".", "74": "raising concerns about unintended information leakage favoring well-represented age groups. Widely used chest X-ray datasets show no significant gender distribution imbalance (e.g.", "NIH-CXR14": {"M": 56.5, "F": 43.5, "CheXpert": {"M": 58.7, "F": 41.3, "165": "with unclear causes. Balancing the data has proven ineffective. Studies have ruled out hypothesis like under-representation [96", "186": "and shortcut learning (e.g., support devices like chest drains [81, 124, 129", "135": "only MIMIC-CXR and CheXpert report self-reported race, with predominantly white populations accounting for 60.7% and 56.4% of samples, respectively. Performance disparities between racial groups favoring white individuals have been noted"}}}]}