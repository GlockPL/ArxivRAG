[{"title": "Learning to Explore with Lagrangians for Bandits under Unknown Linear Constraints", "authors": ["Udvas Das", "Debabrota Basu"], "abstract": "Pure exploration in bandits models multiple real-world problems, such as tuning hyper-parameters or conducting user studies, where different safety, resource, and fairness constraints on the decision space naturally appear. We study these problems as pure exploration in multi-armed bandits with unknown linear constraints, where the aim is to identify an r-good feasible policy. First, we propose a Lagrangian relaxation of the sample complexity lower bound for pure exploration under constraints. We show how this lower bound evolves with the sequential estimation of constraints. Second, we leverage the Lagrangian lower bound and the properties of convex optimisation to propose two computationally efficient extensions of Track-and-Stop and Gamified Explorer, namely LATS and LAGEX. To this end, we propose a constraint-adaptive stopping rule, and while tracking the lower bound, use pessimistic estimate of the feasible set at each step. We show that these algorithms achieve asymptotically optimal sample complexity upper bounds up to constraint-dependent constants. Finally, we conduct numerical experiments with different reward distributions and constraints that validate efficient performance of LAGEX and LATS with respect to baselines.", "sections": [{"title": "1 INTRODUCTION", "content": "Decision-making under uncertainty is a ubiquitous challenge encountered across various domains, including clinical trials (Villar et al., 2015), recommendation systems (Zhao and Yang, 2024), and more. Multi-Armed Bandit (MAB) serves as an archetypal framework for sequential decision-making under uncertainty and allows us to study the involved information-utility trade-offs (Lattimore and Szepesv\u00e1ri, 2020). In MAB, at each step, an agent interacts with an environment consisting of K decisions (also knows as arms) corresponding to K noisy feedback distributions (or reward distributions). At each step, the agent takes a decision, and obtains a reward from its unknown reward distribution. The goal of the agent is to compute a policy, i.e. a distribution over the decisions, that maximises a certain utility metric (e.g. accumulated rewards (Auer et al., 2002), probability of identifying the best arm (Kaufmann et al., 2016) etc.).\nIn this paper, we focus on the pure exploration problem of MABs, where the agent interacts by realising a sequence of policies (or experiments) with the goal of answering a query as correctly as possible. A well-studied pure exploration problem is Best-Arm Identification (BAI), where the agent aims to identify the arm with the highest expected reward (Even-Dar et al., 2002a; Bubeck et al., 2009; Jamieson and Nowak, 2014; Kaufmann et al., 2016). BAI has been applied in hyper-parameter tuning (Li et al., 2017), communication networks (Lindst\u00e5hl et al., 2022), influenza mitigation (Libin et al., 2019), finding the optimal dose of a drug (Aziz et al., 2021a) etc. However, real-world scenarios often impose constraints on the arms that must be satisfied (Carlsson et al., 2024). For example, Baudry et al. (2024) considers a recommendation problem with the aim to guarantee a fixed (known) minimum expected revenue per recommended content while identifying the best content"}, {"title": "Pure Exploration under Constraints.", "content": "In recent years, such aforementioned real-life problems naturally motivated the study of pure exploration under a set of known and unknown constraints (Katz-Samuels and Scott, 2018; Wang et al., 2021b; Li et al., 2023; Wu et al., 2023; Carlsson et al., 2024). Specifically, we aim to find the optimal policy that maximises the expected rewards over the set of arms and also satisfies the true constraints with confidence $1 - \\delta$. This is known as the fixed-confidence setting of pure exploration (Wang et al., 2021b; Carlsson et al., 2024), while there exists fixed-budget setting also which is of independent interest (Katz-Samuels and Scott, 2018; Li et al., 2023; Faizal and Nair, 2022). Existing literature has studied either the general linear constraints when they are known (Carlsson et al., 2024; Camilleri et al., 2022a), or very specific type of unknown constraints, e.g. safety (Wang et al., 2021b), knapsack (Li et al., 2023), fairness (Wu et al., 2023), preferences (Lindner et al., 2022) etc. Here, we study the pure exploration problem in the fixed-confidence setting subject to unknown linear constraints on the policy, which generalises all these settings (Section 2). Further discussions on related works is in Appendix B.1.\nRecently, Carlsson et al. (2024) shows that if the constraints are known, and so is the feasible policy set, a bandit instance may become harder or easier depending on the geometry of constraints. They state that studying similar phenomenon for unknown constraints is an open problem as the feasible policy set has to be estimated. Specifically, we have to simultaneously control concentrations of the mean rewards and the constraints to their 'true' values to reach the feasible policy set. Additionally, at any finite time, the estimated constraints might exhibit small but non-zero errors. This prevents from 'exactly' obtaining the feasible policy set, and consequently, the recommended optimal and feasible policy (though the error might be below numerical limits). Hence, for rigour, we relax the problem of finding the optimal feasible policy to finding an r-good feasible policy. For a given $r > 0$, an r-good policy has mean reward not more than $r$ away from that of the optimal policy (Mason et al., 2020; Jourdan and Degenne, 2022; Jourdan et al., 2023). In practice, one can consider $r$ to be the numerical limit of computation or a reasonably small quantity."}, {"title": "This leads us to two questions", "content": "\u2022 How does the hardness of pure exploration to find r-good feasible policy under unknown constraints change if the constraints are estimated sequentially?\n\u2022 How can we design a generic algorithmic scheme to track both the constraints and optimal policy with sample- and computational-efficiency?"}, {"title": "Our Contributions address these questions as follows:", "content": "1. Lagrangian relaxation of the lower bound. The minimum number of samples required to conduct r-good pure exploration with fixed confidence is expressed by a lower bound, which is an optimisation problem under known constraints. To efficiently handle unknown constraints, we propose a novel Lagrangian relaxation of this optimisation problem (Section 3). At every step, we construct pessimistic feasible policy set, and plug it in the lower bound. The Lagrangian multipliers balance the identifiability of an r-good policy and the feasibility under estimated constraints. We leverage results from convex analysis to show that the Lagrangian relaxation of the lower bound with pessimistic feasible set preserves all the desired properties of the lower bound under known constraints, and consequently, allow us to design lower bound tracking algorithms. Further, we characterise the Lagrangian lower bound for Gaussian rewards, which connects with the lower bound for known constraints.\n2. A generic algorithm design. We leverage this Lagrangian formulation of lower bound to propose two algorithms. First, incorporating the constraint estimates in the existing stopping rule for known constraints ensures concentration of both the means' and constraints' estimates to their true values before recommending an r-good policy with confidence level $1 - \\delta$. We show that this stopping rule also ensures that the recommended policy is feasible with the same confidence level. Then, we"}, {"title": "extend the Track-and-Stop (Garivier and Kaufmann, 2016) and gamified explorer (Degenne et al., 2019b) approaches for the Lagrangian lower bound to design LATS (Lagrangian Track and Stop) and LAGEX (Lagrangian Gamified EXplorer), respectively (Section 4).", "content": "3. Upper bound on sample complexities. We provide upper bounds on sample complexities of LATS and LAGEX (Section 4). This requires proving a novel concentration inequality for the constraints. As a consequence, LATS achieves an upper bound, which is $(1 + \\varsigma)$ times the upper bound of TS under known constraints. $\\varsigma$ is the shadow price of the true constraints and quantifies its stability under perturbation. In contrast, LAGEX leads to an upper bound that has only an additive $\\varsigma$ factor with the known constraint lower bound. This suggests that LAGEX should be more sample-efficient than LATS. Our experimental results across synthetic and real data validate that LAGEX requires the least samples among competing algorithms and exactly tracks the change in hardness due to constraints across environments (Section 5)."}, {"title": "2 PROBLEM FORMULATION", "content": "Notation. $x, \\mathbf{x}, \\mathbf{X},$ and $\\mathcal{X}$ denote a scalar, a vector, a matrix, and a set respectively. $x_i$ denotes $i$-th component of $\\mathbf{x}$. For a positive semi-definite matrix $\\mathbf{A}$ and vector $\\mathbf{z}, ||\\mathbf{z}||_{\\mathbf{A}} = \\sqrt{\\langle \\mathbf{z}, \\mathbf{Az} \\rangle}$. Also, in $\\mathbb{R}^d$, we include $0_d$. $[K]$ refers to $\\{1, . . . , K\\}$. $\\text{Supp}(P)$ denotes the support of a distribution $P$. $\\Delta_K$ is the simplex over $[K]$.\nProblem Formulation. We work with a MAB instance with $K \\in \\mathbb{N}$ arms. Each arm $a \\in [K]$ has a reward distribution $P_a$ with unknown mean $\\mu_a \\in \\mathbb{R}$. The agent, at each time step $t \\in \\mathbb{N}$, chooses an action $A_t \\in [K]$, and observes a stochastic reward $R_t \\sim P_{A_t}$. A feasible policy $\\pi \\in \\Delta_K$ satisfies $\\mathbf{A}\\pi \\le 0$ with respect to the set of $d$ linear constraints $\\mathbf{A} \\in \\mathbb{R}^{d \\times K}$.\nIf $\\mathbf{A}$ is known, the agent has access to the non-empty and compact set of feasible policies $\\mathcal{F} \\triangleq \\{\\pi \\in \\Delta_K \\mid \\mathbf{A}\\pi \\le 0\\}$. The agent aims to identify an $r$-good optimal feasible policy, i.e. any feasible policy which belongs to the set $\\Pi^*_r \\triangleq \\{\\pi \\in \\mathcal{F} \\mid \\mu^\\top \\pi + r \\ge \\mu^\\top \\pi^*\\}$, where\n$\\pi^* = \\arg \\max_{\\pi \\in \\mathcal{F}} \\mu^\\top \\pi.$\n(1)\nDefinition 1 (($1 - \\delta$)-correct and ($1 - \\delta$)-feasible $r$-good pure exploration). For $\\delta \\in [0,1)$, a policy recommended by a pure exploration algorithm is ($1 - \\delta$)-correct and ($1 - \\delta$)-feasible if $\\Pr[\\hat{\\pi} \\ne \\pi^*] \\le \\delta$ and $\\Pr[\\mathbf{A} \\hat{\\pi} \\ge 0] \\le \\delta$.\nIn our setting, we do not have access to the true set of constraints. Hence, using the observations, we construct $\\hat{\\mathbf{A}}$ as an estimate of $\\mathbf{A}$. Then, the agent builds an estimated feasible set $\\hat{\\mathcal{F}} \\triangleq \\{\\pi \\in \\Delta_K \\mid \\hat{\\mathbf{A}}\\pi \\le 0\\}$ to identify the optimal feasible policy as $\\hat{\\pi}^* \\triangleq \\arg \\max_{\\pi \\in \\hat{\\mathcal{F}}} \\mu^\\top \\pi$. In addition, the estimated r-good policy set is $\\hat{\\Pi}^*_r \\triangleq \\{\\pi \\in \\hat{\\mathcal{F}} \\mid \\mu^\\top \\pi + r \\ge \\mu^\\top \\hat{\\pi}^*\\}$. We know that obtaining accurate estimates of these quantities would require us to collect feedback of satisfying constraints over time. This poses an additional cost on top of using observations to estimate $\\mu$.\nGoal. In order to recommend a ($1 - \\delta$)-correct and ($1 - \\delta$)-feasible policy that is r-good with respect ot the true optimal policy $\\pi^*$, we aim to minimise the expected number of interactions $\\mathbb{E}[T_\\delta] \\in \\mathbb{N}$.\nMotivation: Extension of Prior Setups. Now, we clarify our motivation by showing that different existing problems are special cases of our setting.\na. Thresholding Bandits. Thresholding bandits (Aziz et al., 2021a) are motivated from the safe dose finding problem, where one wants to identify the most effective dose of a drug below a known safety level. This has also motivated the safe arm identification problem (Wang et al., 2021b). Our setting generalises it further and detects the optimal combination of doses of available drugs yielding highest efficacy while staying below the safety threshold. Formally, we identify $\\pi^* = \\arg \\max_{\\pi \\in \\Delta_K} \\mu^\\top \\pi$, such that $\\mathbf{I}^\\top \\pi \\le \\mathbf{\\theta}$ for thresholds $\\mathbf{\\theta} > 0$.\nb. Optimal Policy under Knapsack. Bandits under knapsack constraints are studied in both BAI (Li et al., 2023; Tran-Thanh et al., 2012; Li et al., 2021) and regret minimisation literature (Badanidiyuru et al., 2018; Agrawal and Devanur, 2016; Immorlica et al., 2022; Agrawal et al., 2016; Sankararaman and Slivkins, 2018). Detecting an optimal arm might have additional resource constraints than the"}, {"title": "3 LAGRANGIAN RELAXATION OF THE LOWER BOUND", "content": "In this section, we discuss the Lagrangian relaxation of the lower bound and its properties, necessary to design a correct and feasible r-good pure exploration algorithm under unknown linear constraints. We require two structural assumptions.\nAssumption 1 (Assumptions on means, policy, and constraints). (a) The mean vector $\\mu$ belongs to a bounded subset $\\mathcal{D}$ of $\\mathbb{R}^K$. (b) There exists a unique optimal feasibly policy (Equation (1)). (c) For the true constraint $\\mathbf{A}$, there exists a non-zero slack vector $\\Gamma$, such that $\\max_{\\pi \\in \\Delta_K}(-\\mathbf{A}\\pi) = \\Gamma$.\nWe impose the unique optimal and feasible policy assumption following Carlsson et al. (2024) to ensure that the solution of Equation (1) is a unique extreme point of the polytope $\\mathcal{F}$. The assumption on slack is analogous to assuming the existence of a safe-arm (Pacchiano et al., 2020), or that of Slater's condition for the constraint optimisation problem (Liu et al., 2021). Standing on these assumptions, we prove that $\\pi^*$ is unique, i.e $\\pi^*$ is an extreme point in the polytope $\\mathcal{F}$."}, {"title": "3.1 Information Acquisition: Estimating Constraints", "content": "The agent acquires information at every step $t \\in \\mathbb{N}$ by sampling an action $a_t \\sim \\omega_t$. $\\omega_t \\in \\Delta_K$ is called the allocation policy. This yields a noisy reward $r_t \\in \\mathbb{R}$ and cost vector $\\mathbf{A}_{a_t} \\in \\mathbb{R}^d$. As the arms are independent, we represent the a-th arm as the a-th basis of $\\mathbb{R}^K$. Thus, using the observations obtained till $t$, we estimate the mean vector as $\\hat{\\mu}_t \\triangleq (\\mathbf{\\Sigma}_t^{-1} \\sum_{s=1}^{t-1} r_s \\omega_s)$. Here, $\\mathbf{\\Sigma}_t \\triangleq \\nu \\mathbf{I} + \\sum_{s=1}^{t-1} \\omega_s \\omega_s^\\top$ for any $\\nu > 0$, is the Gram matrix or the design matrix. Similarly, the estimate of the $i$-th row of the constraint matrix is $\\hat{\\mathbf{A}}_i \\triangleq (\\mathbf{\\Sigma}_t^{-1} \\sum_{s=1}^{t-1} \\mathbf{A}_{a_s})_i$. But naively using $\\hat{\\mathbf{A}}_t$ to define the feasible policy set does not ensure that for any $t$, the estimated feasible set $\\hat{\\mathcal{F}}$ is a superset of $\\mathcal{F}$. Hence, we define a confidence ellipsoid around $\\hat{\\mathbf{A}}_t$ that includes $\\mathbf{A}$ with probability at least $1 - \\delta$, and construct a pessimistic estimate of $\\mathbf{A}$. Formally, the confidence ellipsoid is\n$\\mathcal{C}_t \\triangleq \\{\\mathbf{A}' \\in \\mathbb{R}^{d \\times K} \\mid ||\\mathbf{A}'_i - \\hat{\\mathbf{A}}_i||_{\\mathbf{\\Sigma}_t} \\le f(t, \\delta) \\forall i \\in [d]\\},$ (2)\nwhere $f(d, t) \\triangleq 1 + \\sqrt{\\log(\\frac{d}{\\delta}) + \\log \\det(\\mathbf{\\Sigma}_t)}$ is a monotonically non-decreasing function of $t$.\nLemma 1 (Pessimistic feasible sets). If we construct the pessimistic feasible policy set at any time $t \\in \\mathbb{N}$ as\n$\\hat{\\mathcal{F}}_t \\triangleq \\{\\pi \\in \\Delta_K\\colon \\min_{\\mathbf{A}' \\in \\mathcal{C}_t} \\mathbf{A}'\\pi \\le 0\\},$ (3)\nwe observe that $\\mathcal{F} \\subseteq \\hat{\\mathcal{F}}_t$ with probability $1 - \\delta$.\nWe denote the $\\mathbf{A}'$ achieving the above minimum as $\\tilde{\\mathbf{A}}_t$. In Figure 1, we visualise this result using the numerical values obtained from our algorithms. We observe that as we acquire more samples, our estimated feasible policy set $\\hat{\\mathcal{F}}_t \\to \\mathcal{F}$."}, {"title": "3.2 Lagrangian Relaxation with Estimated Constraints", "content": "Search for the optimal policy is essentially a linear programming problem when we know the mean vector $\\mu$ and a constraint matrix $\\mathbf{A}$. The challenge in bandit is to identify them from sequential feedback, i.e. to differentiate $\\mu$ from the other confusing instances in the same family of distributions. These are called the alternative instances. The strategy is to gather enough statistical evidence to rule out all such confusing instances, specifically the one that has minimum KL-divergence from $\\mu$ as observed under the allocation policy $\\omega$ (Garivier and Kaufmann, 2016). This intuition has led to the lower bound of Carlsson et al. (2024) stating that the expected stopping time of any ($1 - \\delta$)-correct and always-feasible algorithm satisfies\n$\\mathbb{E}[T_\\delta] \\ge T_{\\mathcal{F},r}(\\mu) \\ln(\\frac{1}{2.4\\delta}),$ (4)\nif $\\mathbf{A}$ is known. $T_{\\mathcal{F},r}(\\mu)$ is called the characteristics time. Its reciprocal is a max-min optimisation problem over the set of alternative instances $\\mathcal{A}_\\mathcal{F}(\\mu, \\pi) \\in \\{\\lambda \\in \\mathcal{D} \\mid \\max_{\\pi \\in \\Pi^*_r} \\lambda^\\top \\pi - r > \\max_{\\pi \\in \\Pi^*} \\lambda^\\top \\pi^*\\}$, i.e.\n$T_{\\mathcal{F},r}^{-1}(\\mu) \\triangleq \\sup_{\\omega \\in \\Delta_K} \\max_{\\pi \\in \\Pi^*_r} \\inf_{\\lambda \\in \\mathcal{A}_\\mathcal{F}(\\mu, \\pi)} \\sum_{a=1}^K \\omega_a d(\\mu_a, \\lambda_a) = \\sup_{\\omega \\in \\Delta_K} \\max_{\\pi \\in \\Pi^*_r} \\inf_{\\lambda \\in \\mathcal{A}_\\mathcal{F}(\\mu, \\pi)} \\omega^\\top d(\\mu, \\lambda).$ (5)\n$\\Lambda_{\\mathcal{F}}(\\mu, \\pi)$, referred as the Alt-set, is the set of all bandit instances whose mean vectors are in a bounded subset $\\mathcal{D} \\in \\mathbb{R}^K$ but the optimal policy is different than that of $\\mu \\in \\mathcal{D}$. Now, we inspect the change in this lower bound at any step $t > 0$, when we only have access to a pessimistic estimate $\\hat{\\mathcal{F}}_t$ and the confidence ellipsoid $\\mathcal{C}_t$ but do not know $\\mathcal{F}$. For brevity, we exclude $t$ from the subscripts for where it is clear from the context. Now, we observe that the Alt-set given $\\hat{\\mathcal{F}}$ is\n$\\Lambda_{\\hat{\\mathcal{F}}}(\\mu, \\pi) \\in \\{\\lambda \\in \\mathcal{D} \\mid \\max_{\\pi \\in \\Pi^*_r} \\lambda^\\top \\pi - r > \\lambda^\\top \\pi^*\\}.$ (6)\nSince $\\mathcal{F} \\subseteq \\hat{\\mathcal{F}}$, we observe that $\\Lambda_{\\hat{\\mathcal{F}}}(\\mu, \\pi) \\subseteq \\Lambda_{\\mathcal{F}}(\\mu, \\pi)$. Now, we are ready define to Lagrangian relaxation of the lower bound, i.e.\n$\\begin{aligned}\nT_{\\hat{\\mathcal{F}},r}^{-1}(\\mu) &= \\sup_{\\omega \\in \\Delta_K} \\max_{\\pi \\in \\Pi^*_r} \\inf_{\\lambda \\in \\Lambda_{\\hat{\\mathcal{F}}}(\\mu, \\pi)} \\omega^\\top d(\\mu, \\lambda) \\\\\n&\\le \\inf_{\\mathbf{l} \\in \\mathbb{R}^d} \\min_{\\mathbf{A}' \\in \\mathcal{C}_t} \\sup_{\\omega \\in \\Delta_K} \\max_{\\pi \\in \\Pi^*_r} \\inf_{\\lambda \\in \\Lambda_{\\mathcal{F}}(\\mu, \\pi)} \\omega^\\top d(\\mu, \\lambda) - \\mathbf{l}^\\top \\mathbf{A}'\\omega.\\\n\\end{aligned}$\n(7)\nWe denote this Lagrangian relaxation of the characteristic time with $\\hat{\\mathcal{F}}$ as $T_{\\hat{\\mathcal{F}},r}^{-1}(\\mu)$. For non-negative Lagrange multipliers $\\mathbf{l} \\in \\mathbb{R}^d$, the first inequality is true due to the existence of a slack for the true"}, {"title": "constraints $\\mathbf{A}$.", "content": "The second inequality is due to the pessimistic choice of the estimated constraint. Equation (7) shows that the reciprocal of the Lagrangian relaxation, $T_{\\hat{\\mathcal{F}},r}^{-1}(\\mu)$, serves as a upper bound on the characteristic time $T_{\\mathcal{F},r}(\\mu)$ for known constraints (Carlsson et al., 2024).\nThe Lagrangian relaxation leads to a natural question:\nDoes the dual of the optimization problem for $T_{\\hat{\\mathcal{F}},r}^{-1}(\\mu)$ yield the same solution as the primal?\nTheorem 1 (Strong Duality and Range of Lagrange Multipliers). The optimisation problem in Equation (7) satisfies\n$\\begin{aligned}\n\\inf_{\\mathbf{l} \\in \\mathbb{R}^d} \\min_{\\mathbf{A}' \\in \\mathcal{C}_t} \\sup_{\\omega \\in \\Delta_K} \\max_{\\pi \\in \\Pi^*_r} \\inf_{\\lambda \\in \\Lambda_{\\mathcal{F}}(\\mu, \\pi)} \\omega^\\top d(\\mu, \\lambda) - \\mathbf{l}^\\top \\mathbf{A}'\\omega = \\sup_{\\omega \\in \\Delta_K} \\min_{\\mathbf{l} \\in \\mathcal{L}} \\max_{\\pi \\in \\Pi^*_r} \\inf_{\\lambda \\in \\Lambda_{\\mathcal{F}}(\\mu, \\pi)} \\omega^\\top d(\\mu, \\lambda) - \\mathbf{l}^\\top \\tilde{\\mathbf{A}}\\omega.\\\n\\end{aligned}$\n(8)\nHere, $\\mathcal{L} \\triangleq {\\mathbf{l} \\in \\mathbb{R}^d \\mid 0 \\le ||\\mathbf{l}||_1 \\le D(\\omega, \\mu, \\hat{\\mathcal{F}})}$, where $\\gamma \\equiv \\min_{i \\in [1,d]}\\{-\\,\\tilde{\\Gamma}_i^\\top \\omega^* \\}$, i.e. the minimum slack w.r.t. the optimal allocation. (Proof is in Appendix C.)\nHereafter, we use the RHS of Eq. (8) as $T_{\\hat{\\mathcal{F}},r}^{-1}(\\mu)$. Theorem 1 provides a hypercube to search for the Lagrangian multipliers, which is a linear programming problem.\nRemark: Connections with Lagrangian-based Methods in Bandits. Regret minimisation literature leverages Lagrangian-based optimistic-pessimistic methods (Tirinzoni et al., 2020; Slivkins et al., 2023) to obtain both the sub-linear regret and constraint violation guarantees (Liu et al., 2021; Bernasconi et al., 2024). Our proposed algorithm LAGEX (Algorithm 2) is a prime example where the \"self-boundedness\" of the dual variables results in tighter constraint violation guarantees (Figure 7 and 6).\nInner Optimisation Problem. Now, we peel the layers of the optimisation problem in Eq. (8) and focus on obtaining\n$D(\\omega, \\mu, \\hat{\\mathcal{F}}) \\triangleq \\min_{\\mathbf{l} \\in \\mathcal{L}} \\max_{\\pi \\in \\Pi^*_r} \\inf_{\\lambda \\in \\Lambda_{\\mathcal{F}}(\\mu, \\pi)} \\omega^\\top d(\\mu, \\lambda) - \\mathbf{l}^\\top \\tilde{\\mathbf{A}}\\omega.$\nFor known constraints and $r = 0$, Carlsson et al. (2024) has leveraged results from convex anal-ysis (Boyd and Vandenberghe, 2004) to show that the most confusing instance for $\\mu$ lie in the boundary of the normal cone $\\Lambda_{\\mathcal{F}}(\\mu, \\pi)^C$ spanned by the active constraints $\\mathbf{A}\\pi$ for $\\pi \\in \\Pi^*$. $\\mathbf{A}_\\pi$ is a sub-matrix of $\\mathbf{A}$ consisting at least $K$ linearly independent rows. This is called the projection lemma. Specifically, $D(\\omega, \\mu, \\mathcal{F} \\mid r = 0) = \\max_{\\pi \\in \\Pi^*} \\min_{\\pi' \\in \\nu(\\pi^*)} \\min_{\\lambda: \\lambda^\\top (\\pi - \\pi') = 0} \\omega^\\top d(\\mu, \\lambda)$. In our setting, we are sequentially estimating both the mean vectors and the constraints, and thus, the normal cone. Now, we derive the projection lemma for our pessimistic feasible set and $r \\ne 0$.\nProposition 1 (Projection Lemma for Unknown Constraints). For any $\\omega \\in \\mathcal{F}$ and $\\mu \\in \\mathcal{D}$, the following projection lemma holds for the Lagrangian relaxation,\n$D(\\omega, \\mu, \\hat{\\mathcal{F}}) = \\min_{\\mathbf{l} \\in \\mathcal{L}} \\max_{\\pi \\in \\Pi^*_r} \\min_{\\pi' \\in \\nu(\\pi^*)} \\min_{\\lambda: \\lambda^\\top (\\pi - \\pi') = r} \\omega^\\top d(\\mu, \\lambda) - \\mathbf{l}^\\top \\tilde{\\mathbf{A}}\\omega.$ (9)\nThis reduces the inner minimisation problem to a less intensive discrete optimisation, where we only have to search over the neighbouring vertices of the optimal policy in $\\mathcal{F}$ for a solution. Now, a natural question arises around this formulation:\nCan we track $D(\\omega, \\mu, \\hat{\\mathcal{F}})$ in the projection lemma over time as we sequentially estimate the constraints?\nTheorem 2. For a sequence {$\\hat{\\mathcal{F}}_t$}$_{t \\in \\mathbb{N}}$ and {$\\hat{\\mathbf{A}}_t$}$_{t \\in \\mathbb{N}}$, we first show that (a) $\\lim_{t \\to \\infty} \\hat{\\mathcal{F}}_t \\to \\mathcal{F}$, (b) $\\lambda^*$ is unique, and (c) $\\lim_{t \\to \\infty} \\hat{\\mathbf{A}}_t \\to \\lambda^*$.\nThus, for any $\\omega \\in \\mathcal{F}$ and $\\mu$,\n$\\lim_{t \\to \\infty} D(\\omega, \\mu, \\hat{\\mathcal{F}}_t) \\to D(\\omega, \\mu, \\mathcal{F}),$\nwhere $\\lambda^*$ is such that for any $\\lambda \\in \\arg \\min_{\\lambda \\in \\Lambda_{\\hat{\\mathcal{F}}}(\\mu, \\pi)} \\omega^\\top d(\\mu, \\lambda)$. (Proof is in Appendix D.)"}, {"title": "Outer Optimisation Problem.", "content": "As we guarantee the convergence of $D(\\omega", "1": "Convexity of feasible space and optimal set function. Let us first analyse the properties of $\\mathcal{F"}, ".", "For any two member of $\\omega_1,\\omega_2 \\in \\mathcal{F}$ satisfying $\\mathbf{A}\\omega_1 \\le 0$ and $\\mathbf{A}\\omega_2 \\le 0$, their convex combination for any $\\alpha \\in [0, 1"], "Delta_K": "mathbf{A"}, {"2": "Continuity of limit. We have already proven in Section D that $\\lim_{t \\to \\infty"}, {"3": "Continuity of limit of inverse sampling complexity. This statement directly follows from the statement 2. Due to convexity of KL-divergence and convergence of $\\hat{\\mathcal{F"}]