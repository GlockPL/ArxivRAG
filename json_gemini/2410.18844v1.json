{"title": "Learning to Explore with Lagrangians for Bandits under Unknown Linear Constraints", "authors": ["Udvas Das", "Debabrota Basu"], "abstract": "Pure exploration in bandits models multiple real-world problems, such as tuning hyper-parameters or conducting user studies, where different safety, resource, and fairness constraints on the decision space naturally appear. We study these problems as pure exploration in multi-armed bandits with unknown linear constraints, where the aim is to identify an r-good feasible policy. First, we propose a Lagrangian relaxation of the sample complexity lower bound for pure exploration under constraints. We show how this lower bound evolves with the sequential estimation of constraints. Second, we leverage the Lagrangian lower bound and the properties of convex optimisation to propose two computationally efficient extensions of Track-and-Stop and Gamified Explorer, namely LATS and LAGEX. To this end, we propose a constraint-adaptive stopping rule, and while tracking the lower bound, use pessimistic estimate of the feasible set at each step. We show that these algorithms achieve asymptotically optimal sample complexity upper bounds up to constraint-dependent constants. Finally, we conduct numerical experiments with different reward distributions and constraints that validate efficient performance of LAGEX and LATS with respect to baselines.", "sections": [{"title": "1 INTRODUCTION", "content": "Decision-making under uncertainty is a ubiquitous challenge encountered across various domains, including clinical trials (Villar et al., 2015), recommendation systems (Zhao and Yang, 2024), and more. Multi-Armed Bandit (MAB) serves as an archetypal framework for sequential decision-making under uncertainty and allows us to study the involved information-utility trade-offs (Lattimore and Szepesv\u00e1ri, 2020). In MAB, at each step, an agent interacts with an environment consisting of $K$ decisions (also knows as arms) corresponding to $K$ noisy feedback distributions (or reward distributions). At each step, the agent takes a decision, and obtains a reward from its unknown reward distribution. The goal of the agent is to compute a policy, i.e. a distribution over the decisions, that maximises a certain utility metric (e.g. accumulated rewards (Auer et al., 2002), probability of identifying the best arm (Kaufmann et al., 2016) etc.).\nIn this paper, we focus on the pure exploration problem of MABs, where the agent interacts by realising a sequence of policies (or experiments) with the goal of answering a query as correctly as possible. A well-studied pure exploration problem is Best-Arm Identification (BAI), where the agent aims to identify the arm with the highest expected reward (Even-Dar et al., 2002a; Bubeck et al., 2009; Jamieson and Nowak, 2014; Kaufmann et al., 2016). BAI has been applied in hyper-parameter tuning (Li et al., 2017), communication networks (Lindst\u00e5hl et al., 2022), influenza mitigation (Libin et al., 2019), finding the optimal dose of a drug (Aziz et al., 2021a) etc. However, real-world scenarios often impose constraints on the arms that must be satisfied (Carlsson et al., 2024). For example, Baudry et al. (2024) considers a recommendation problem with the aim to guarantee a fixed (known) minimum expected revenue per recommended content while identifying the best content"}, {"title": "2 PROBLEM FORMULATION", "content": "Notation. $x, \\mathbf{x}, X$, and $\\mathbf{X}$ denote a scalar, a vector, a matrix, and a set respectively. $x_i$ denotes $i$-th component of $\\mathbf{x}$. For a positive semi-definite matrix $\\mathbf{A}$ and vector $\\mathbf{z}, ||\\mathbf{z}||_{\\mathbf{A}} = (\\mathbf{z}^T\\mathbf{Az})^{\\frac{1}{2}}$. Also, in $\\mathbb{R}^d$, we include $\\mathbf{0}_d$. $[K]$ refers to $\\{1, ..., K\\}$. $Supp(P)$ denotes the support of a distribution $P$. $\\Delta_K$ is the simplex over $[K]$.\nProblem Formulation. We work with a MAB instance with $K \\in \\mathbb{N}$ arms. Each arm $a \\in [K]$ has a reward distribution $P_a$ with unknown mean $\\mu_a \\in \\mathbb{R}$. The agent, at each time step $t \\in \\mathbb{N}$, chooses an action $A_t \\in [K]$, and observes a stochastic reward $R_t \\sim P_{A_t}$. A feasible policy $\\pi \\in \\Delta_K$ satisfies $\\mathbf{A}\\pi < 0$ with respect to the set of $d$ linear constraints $\\mathbf{A} \\in \\mathbb{R}^{d \\times K}$.\nIf $\\mathbf{A}$ is known, the agent has access to the non-empty and compact set of feasible policies $\\mathcal{F} \\triangleq \\{\\pi \\in \\Delta_K | \\mathbf{A}\\pi \\leq 0\\}$. The agent aims to identify an r-good optimal feasible policy, i.e. any feasible policy which belongs to the set $\\Pi_r \\triangleq {\\pi \\in \\mathcal{F} | \\boldsymbol{\\mu}^T\\pi + r \\geq \\boldsymbol{\\mu}^T\\pi^*}$, where\n$\\pi^* \\triangleq \\underset{\\pi \\in \\mathcal{F}}{\\arg \\max} \\boldsymbol{\\mu}^T\\pi.$\nDefinition 1 (($1 - \\delta$)-correct and ($1 - \\delta$)-feasible r-good pure exploration). For $\\delta \\in [0,1)$, a policy recommended by a pure exploration algorithm is ($1 - \\delta$)-correct and ($1 - \\delta$)-feasible if $\\Pr[\\widehat{\\pi} \\neq \\pi^*] \\leq \\delta$ and $\\Pr[\\mathbf{A}\\widehat{\\pi} \\geq 0] \\leq \\delta$.\nIn our setting, we do not have access to the true set of constraints. Hence, using the observations, we construct $\\widehat{\\mathbf{A}}$ as an estimate of $\\mathbf{A}$. Then, the agent builds an estimated feasible set $\\widehat{\\mathcal{F}} \\triangleq \\{\\pi \\in \\Delta_K | \\widehat{\\mathbf{A}}\\pi \\leq 0\\}$ to identify the optimal feasible policy as $\\widehat{\\pi} \\triangleq \\underset{\\pi \\in \\widehat{\\mathcal{F}}}{\\arg \\max} \\boldsymbol{\\mu}^T\\pi$. In addition, the estimated r-good policy set is $\\widehat{\\Pi}_r \\triangleq {\\pi \\in \\widehat{\\mathcal{F}} | \\boldsymbol{\\mu}^T\\pi + r \\geq \\boldsymbol{\\mu}^T\\widehat{\\pi}}$. We know that obtaining accurate estimates of these quantities would require us to collect feedback of satisfying constraints over time. This poses an additional cost on top of using observations to estimate $\\mu$.\nGoal. In order to recommend a ($1 - \\delta$)-correct and ($1 - \\delta$)-feasible policy that is r-good with respect ot the true optimal policy $\\pi^*$, we aim to minimise the expected number of interactions $\\mathbb{E}[T_{\\delta}] \\in \\mathbb{N}$.\nMotivation: Extension of Prior Setups. Now, we clarify our motivation by showing that different existing problems are special cases of our setting.\na. Thresholding Bandits. Thresholding bandits (Aziz et al., 2021a) are motivated from the safe dose finding problem, where one wants to identify the most effective dose of a drug below a known safety level. This has also motivated the safe arm identification problem (Wang et al., 2021b). Our setting generalises it further and detects the optimal combination of doses of available drugs yielding highest efficacy while staying below the safety threshold. Formally, we identify $\\pi^* = \\underset{\\pi \\in \\mathcal{K}}{\\arg \\max} \\mu^T \\pi$, such that $\\mathbf{I}^T\\pi < \\theta$ for thresholds $\\mathbf{\\theta} > 0$.\nb. Optimal Policy under Knapsack. Bandits under knapsack constraints are studied in both BAI (Li et al., 2023; Tran-Thanh et al., 2012; Li et al., 2021) and regret minimisation literature (Badanidiyuru et al., 2018; Agrawal and Devanur, 2016; Immorlica et al., 2022; Agrawal et al., 2016; Sankararaman and Slivkins, 2018). Detecting an optimal arm might have additional resource constraints than the"}, {"title": "3 LAGRANGIAN RELAXATION OF THE LOWER BOUND", "content": "In this section, we discuss the Lagrangian relaxation of the lower bound and its properties, necessary to design a correct and feasible r-good pure exploration algorithm under unknown linear constraints. We require two structural assumptions.\nAssumption 1 (Assumptions on means, policy, and constraints). (a) The mean vector $\\mu$ belongs to a bounded subset $\\mathcal{D}$ of $\\mathbb{R}^K$. (b) There exists a unique optimal feasibly policy (Equation (1)). (c) For the true constraint $\\mathbf{A}$, there exists a non-zero slack vector $\\Gamma$, such that $\\underset{\\pi \\in \\Delta_K}{\\max}(-\\mathbf{A}\\pi) = \\Gamma$.\nWe impose the unique optimal and feasible policy assumption following Carlsson et al. (2024) to ensure that the solution of Equation (1) is a unique extreme point of the polytope $\\mathcal{F}$. The assumption on slack is analogous to assuming the existence of a safe-arm (Pacchiano et al., 2020), or that of Slater's condition for the constraint optimisation problem (Liu et al., 2021). Standing on these assumptions, we prove that $\\pi^*$ is unique, i.e $\\pi^*$ is an extreme point in the polytope $\\mathcal{F}$."}, {"title": "3.1 Information Acquisition: Estimating Constraints", "content": "The agent acquires information at every step $t \\in \\mathbb{N}$ by sampling an action $a_t \\sim \\omega_t$. $\\omega_t \\in \\Delta_K$ is called the allocation policy. This yields a noisy reward $r_t \\in \\mathbb{R}$ and cost vector $\\mathbf{A}_{a_t} \\in \\mathbb{R}^d$. As the arms are independent, we represent the $a$-th arm as the $a$-th basis of $\\mathbb{R}^K$. Thus, using the observations obtained till $t$, we estimate the mean vector as $\\widehat{\\boldsymbol{\\mu}}_t \\triangleq \\Sigma_t^{-1} (v\\boldsymbol{\\mu}_0 + \\sum_{s=1}^{t-1} r_s \\mathbf{a}_s)$. Here, $\\Sigma_t \\triangleq v\\mathbf{I} + \\sum_{s=1}^{t-1} \\mathbf{a}_s \\mathbf{a}_s^T$ for any $v > 0$, is the Gram matrix or the design matrix. Similarly, the estimate of the $i$-th row of the constraint matrix is $\\widehat{\\mathbf{A}}_i \\triangleq \\left(\\mathbf{A}^T\\Sigma_t\\right)_i (\\Sigma_t^2)^{-1}$. But naively using $\\widehat{\\mathbf{A}}_t$ to define the feasible policy set does not ensure that for any $t$, the estimated feasible set $\\widehat{\\mathcal{F}}$ is a superset of $\\mathcal{F}$. Hence, we define a confidence ellipsoid around $\\widehat{\\mathbf{A}}_t$ that includes $\\mathbf{A}$ with probability at least $1 - \\delta$, and construct a pessimistic estimate of $\\mathbf{A}$. Formally, the confidence ellipsoid is\n$\\mathcal{C}_t \\triangleq {\\mathbf{A}' \\in \\mathbb{R}^{d \\times K} | ||\\mathbf{A}'_i - \\widehat{\\mathbf{A}}_i||_{\\Sigma_t} \\leq f(t, \\delta) \\forall i \\in [d]}$,\nwhere $f(d,t) \\triangleq 1 + \\sqrt{\\log \\frac{1}{\\delta} + \\log \\det\\Sigma_t}$ is a monotonically non-decreasing function of $t$.\nLemma 1 (Pessimistic feasible sets). If we construct the pessimistic feasible policy set at any time $t \\in \\mathbb{N}$ as\n$\\widehat{\\mathcal{F}}_t \\triangleq {\\pi \\in \\Delta_K: \\underset{\\mathbf{A}' \\in \\mathcal{C}_t}{\\min} \\mathbf{A}'\\pi \\leq 0}$,\nwe observe that $\\mathcal{F} \\subset \\widehat{\\mathcal{F}}_t$ with probability $1 - \\delta$.\nWe denote the $\\mathbf{A}'$ achieving the above minimum as $\\underline{\\mathbf{A}}_t$. In Figure 1, we visualise this result using the numerical values obtained from our algorithms. We observe that as we acquire more samples, our estimated feasible policy set $\\widehat{\\mathcal{F}}_t \\rightarrow \\mathcal{F}$."}, {"title": "3.2 Lagrangian Relaxation with Estimated Constraints", "content": "Search for the optimal policy is essentially a linear programming problem when we know the mean vector $\\mu$ and a constraint matrix $\\mathbf{A}$. The challenge in bandit is to identify them from sequential feedback, i.e. to differentiate $\\mu$ from the other confusing instances in the same family of distributions. These are called the alternative instances. The strategy is to gather enough statistical evidence to rule out all such confusing instances, specifically the one that has minimum KL-divergence from $\\mu$ as observed under the allocation policy $\\omega$ (Garivier and Kaufmann, 2016). This intuition has led to the lower bound of Carlsson et al. (2024) stating that the expected stopping time of any ($1 - \\delta$)-correct and always-feasible algorithm satisfies\n$\\mathbb{E}[T_{\\delta}] \\geq T_{\\mathcal{F}, r}(\\mu) \\ln \\frac{1}{2.4\\delta}$,\nif $\\mathbf{A}$ is known. $T_{\\mathcal{F}, r}(\\mu)$ is called the characteristics time. Its reciprocal is a max-min optimisation problem over the set of alternative instances $\\Lambda_{\\mathcal{F}}(\\mu, \\pi) \\in {\\lambda \\in \\mathcal{D} | \\underset{\\pi \\in \\Pi_r}{\\max} \\lambda^T\\pi - r > \\underset{\\pi \\in \\Pi}{\\max} \\lambda^T\\pi^*}$, i.e.\n$T_{\\mathcal{F},r}^{-1}(\\mu) \\triangleq \\underset{\\boldsymbol{\\omega} \\in \\Delta_K}{\\sup} \\underset{\\pi \\in \\Pi_r}{\\max} \\underset{\\boldsymbol{\\lambda} \\in \\Lambda_{\\mathcal{F}}(\\mu, \\pi)}{\\inf} \\sum_{a=1}^K \\omega_a d(\\mu_a, \\lambda_a)$\n$ \\triangleq \\underset{\\boldsymbol{\\omega} \\in \\Delta_K}{\\sup} \\underset{\\pi \\in \\Pi_r}{\\max} \\underset{\\boldsymbol{\\lambda} \\in \\Lambda_{\\mathcal{F}}(\\mu, \\pi)}{\\inf} \\boldsymbol{\\omega}^T d(\\boldsymbol{\\mu}, \\boldsymbol{\\lambda}).$\n$\\Lambda_{\\mathcal{F}}(\\mu, \\pi)$, referred as the Alt-set, is the set of all bandit instances whose mean vectors are in a bounded subset $\\mathcal{D} \\in \\mathbb{R}^K$ but the optimal policy is different than that of $\\mu \\in \\mathcal{D}$. Now, we inspect the change in this lower bound at any step $t > 0$, when we only have access to a pessimistic estimate $\\widehat{\\mathcal{F}}_t$ and the confidence ellipsoid $\\mathcal{C}_t$ but do not know $\\mathcal{F}$. For brevity, we exclude $t$ from the subscripts for where it is clear from the context. Now, we observe that the Alt-set given $\\widehat{\\mathcal{F}}$ is\n$\\Lambda_{\\widehat{\\mathcal{F}}}(\\mu, \\pi) \\in {\\lambda \\in \\mathcal{D} | \\underset{\\pi \\in \\widehat{\\Pi}_r}{\\max} \\lambda^T \\pi - r > \\lambda^T\\pi^*}$.\nSince $\\mathcal{F} \\subset \\widehat{\\mathcal{F}}$, we observe that $\\Lambda_{\\widehat{\\mathcal{F}}}(\\mu, \\pi) \\subseteq \\Lambda_{\\mathcal{F}}(\\mu, \\pi)$. Now, we are ready define to Lagrangian relaxation of the lower bound, i.e.\n$T_{\\widehat{\\mathcal{F}},r}^{-1}(\\mu) \\triangleq \\underset{\\boldsymbol{\\omega} \\in \\Delta_K}{\\sup} \\underset{\\pi \\in \\Pi_r}{\\max} \\underset{\\boldsymbol{\\lambda} \\in \\Lambda_{\\widehat{\\mathcal{F}}}(\\mu, \\pi)}{\\inf} \\boldsymbol{\\omega}^T d(\\boldsymbol{\\mu}, \\boldsymbol{\\lambda})$\n$\\leq \\underset{\\ell \\in \\mathbb{R}^d}{\\inf} \\underset{\\mathbf{A}' \\in \\mathcal{C}}{\\min} \\underset{\\boldsymbol{\\omega} \\in \\Delta_K}{\\sup} \\underset{\\pi \\in \\Pi_r}{\\max} \\underset{\\boldsymbol{\\lambda} \\in \\Lambda_{\\mathcal{F}}(\\mu, \\pi)}{\\inf} \\boldsymbol{\\omega}^T d(\\boldsymbol{\\mu}, \\boldsymbol{\\lambda}) - \\ell^T \\mathbf{A}' \\boldsymbol{\\omega}$.\nWe denote this Lagrangian relaxation of the characteristic time with $\\widehat{\\mathcal{F}}$ as $T_{\\widehat{\\mathcal{F}},r}^{-1}(\\mu)$. For non-negative Lagrange multipliers $\\ell \\in \\mathbb{R}^d$, the first inequality is true due to the existence of a slack for the true"}, {"title": "4 LATS AND LAGEX: ALGORITHM DESIGN AND ANALYSIS", "content": "Now, we propose two algorithms to conduct pure exploration with Lagrangian relaxation of lower bound, and derive upper bounds on their sample complexities.\nAssumption 2 (Distributional assumptions on rewards and constraints). We require two distributional assumptions on rewards and constraints. (i) Reward distributions ${P_a}_{K=1}$ are sub-Gaussian one parameter exponential family with mean vector $\\mu \\in \\mathcal{D}$. (ii) Each constraint follows a sub-Gaussian K-parameter exponential family parameterised by $A^2$ for $i \\in [d]$."}]}