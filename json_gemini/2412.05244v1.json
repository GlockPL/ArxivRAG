{"title": "ENHANCING FOUNDATION MODELS FOR TIME SERIES\nFORECASTING VIA WAVELET-BASED TOKENIZATION", "authors": ["Luca Masserano", "Abdul Fatir Ansari", "Boran Han", "Xiyuan Zhang", "Christos Faloutsos", "Michael W. Mahoney", "Andrew Gordon Wilson", "Youngsuk Park", "Syama Rangapuram", "Danielle C. Maddix", "Yuyang Wang"], "abstract": "How to best develop foundational models for time series forecasting remains an\nimportant open question. Tokenization is a crucial consideration in this effort:\nwhat is an effective discrete vocabulary for a real-valued sequential input? To ad-\ndress this question, we develop WaveToken, a wavelet-based tokenizer that allows\nmodels to learn complex representations directly in the space of time-localized\nfrequencies. Our method first scales and decomposes the input time series, then\nthresholds and quantizes the wavelet coefficients, and finally pre-trains an autore-\ngressive model to forecast coefficients for the forecast horizon. By decomposing\ncoarse and fine structures in the inputs, wavelets provide an eloquent and compact\nlanguage for time series forecasting that simplifies learning. Empirical results on\na comprehensive benchmark, including 42 datasets for both in-domain and zero-\nshot settings, show that WaveToken: i) provides better accuracy than recently pro-\nposed foundation models for forecasting while using a much smaller vocabulary\n(1024 tokens), and performs on par or better than modern deep learning mod-\nels trained specifically on each dataset; and ii) exhibits superior generalization\ncapabilities, achieving the best average rank across all datasets for three comple-\nmentary metrics. In addition, we show that our method can easily capture com-\nplex temporal patterns of practical relevance that are challenging for other recent\npre-trained models, including trends, sparse spikes, and non-stationary time series\nwith varying frequencies evolving over time.", "sections": [{"title": "INTRODUCTION", "content": "Time series forecasting is integral to decision-making processes in many domains, including fi-\nnance, healthcare, supply chain optimization, and climate science. Over the last decade, the field\nhas seen a gradual but steady adoption of \u201cglobal\" deep learning models in lieu of traditional \u201clocal\"\nstatistical models (Lara-Ben\u00edtez et al., 2021; Benidis et al., 2022). Recently, the success of large\nlanguage models (LLMs) on natural language and vision applications has spurred an increasing in-\nterest for developing similar \u201cfoundation models\" in other fields (for example, Subramanian et al.\n(2024); Golling et al. (2024); Ansari et al. (2024); Das et al. (2023)). These efforts aim at building\ngeneral-purpose machines able to learn complex representations from vast amounts of data and to\ngeneralize to a wide variety of tasks, based on the premise that LLMs are general pattern recogniz-\ners (Mirchandani et al., 2023). In other words, if a problem can be reduced to that of modeling an\narbitrary sequence of tokens defined on a discrete vocabulary, then an autoregressive transformer\nmight be capable of learning non-trivial relationships via next-token prediction, regardless of the\ninputs representing text or not. The sequential nature of time series forecasting aligns seamlessly\nwith this perspective, which is why several recent works have proposed adapting transformer-based\narchitectures into foundation models for time series (see Section 2 for a review).\nTokenization is a crucial albeit still understudied (Dagan et al., 2024) component of LLMs, as it pro-\nvides the vocabulary on which token streams are defined and the autoregressive structure is learned."}, {"title": "RELATED WORK", "content": "Tokenization in LLMs. Most tokenizers originate from the natural language processing (NLP) lit-\nerature and have been developed and studied for various domains such as math (Singh & Strouse,\n2024), code (Zheng et al., 2023), and several languages (e.g., Tolmachev et al. (2018); Alyafeai\net al. (2023)). In modern LLMs, the most popular tokenizers learn a dictionary directly on data,\nsuch as variants of Byte-Pair Encoding (BPE; Gage (1994)). Tokenization of real-valued numbers\nhas received particular attention due to the difficulty of finding what is the most appropriate dis-\ncrete vocabulary for a continuous input. One of the most popular methods is training a Vector\nQuantized-Variational AutoEncoder (VQ-VAE) (Van Den Oord et al., 2017), which learns a dic-\ntionary of k-dimensional codewords to capture latent representations. Instead of learning a token\nfor each numerical value, Golkar et al. (2023) proposes a fixed scheme that allocates a dedicated\nembedding vector for numerics and scales it by the number value, thereby improving efficiency\nand generalization. In this work, we similarly adopt a fixed scheme, but employ a tailored wavelet\ndecomposition that enhances the crucial spectral properties of time series signals.\nWavelet-based forecasters. Several approaches have tried to embed wavelets in forecasting\npipelines. An early example is that of Papadimitriou et al. (2003), who integrate the discrete wavelet\ntransform with ARIMA modeling to capture complex patterns over long time periods. More recently,\nZhou et al. (2022) proposed to substitute attention blocks with Fourier- and wavelet-enhanced blocks\nin the transformer architecture. Within this line of work, Zhang et al. (2022) proposed to model trend\nand seasonality separately with an MLP and Fourier attention, respectively. In addition, they showed\nthat under linear transformations, attention models in time domain, Fourier domain and wavelet do-\nmain have the same representation power. Sasal et al. (2022) leverage a redundant wavelet transform\nthat yields J series of coefficients (one per decomposition level), each having the same length as the\noriginal time series. They then learn a separate transformer model for each scale. To the best of our\nknowledge, WaveToken is the first application of a maximally decimated wavelet transform to build\na tokenizer tailored for time series forecasting with LLMs.\nModeling and generation of signals. Different methods in the signal processing literature propose\nto integrate spectral or wavelet decomposition with deep learning architectures to enhance the ca-\npabilities of these models on a variety of tasks. Apart from the already-mentioned VQ-VAE (Van\nDen Oord et al., 2017), several works exploit spectrograms and log-mel spectra, for example, as\npre-processing steps on medical or audio data (Choi et al., 2023; Purwins et al., 2019). Similarly,\naudio codecs (Zeghidour et al., 2021) are emerging as a critical technique to bridge the gap be-\ntween continuous waveforms and token-based language models. As for image generation, Guth\net al. (2022) proposed to speed up denoising by learning diffusion models in the wavelets domain.\nRecent concurrent works by Tian et al. (2024), Mattar et al. (2024) and Zhu & Soricut (2024) learn\na transformer model on a multi-scale sequence or in the space of 2D wavelet coefficients.\nFoundation models for time series forecasting. Several approaches are being developed to adapt\nLLMs to other domains, such as time series forecasting. Recent efforts in this direction include,\ne.g., Xue & Salim (2023), which converts time series into text and re-frames the task as a question-\nanswering problem; Gruver et al. (2024), which tokenizes real-valued data as strings of digits and\nleverages models such as GPT-3 (Brown, 2020) and Llama 2 (Touvron et al., 2023); and Jin et al.\n(2023), which prompts a frozen LLM with a prefix describing the task and patch embeddings of time\nseries aligned with text prototypes. Recently, Rasul et al. (2023); Goswami et al. (2024); Das et al.\n(2023); Ansari et al. (2024); Woo et al. (2024); Talukder et al. (2024) proposed different paradigms\nto pre-train transformer-based architectures on a large corpus of time series. See also Zhang et al.\n(2024) for a recent survey. While these works adopt domain-specific designs such as patching, lags\nand time features, none of them tackles the problem by learning an autoregressive model in the\nexpressive space of time-localized frequencies."}, {"title": "LANGUAGE MODELING OF TIME-LOCALIZED FREQUENCIES", "content": "We introduce a wavelet-based tokenizer that allows the model to learn complex representations\ndirectly in the space of time-localized frequencies. Our method first scales and decomposes the\ninput time series, then thresholds and quantizes the wavelet coefficients, and finally it pre-trains an\nautoregressive model to forecast wavelet coefficients for the horizon window."}, {"title": "A BRIEF TOUR OF WAVELETS", "content": "Here we provide a very brief introduction to the main concepts and terminology regarding wavelets.\nSee Appendix A and references therein for more details. The Wavelet transform (WT) was intro-\nduced to address some limitations in existing mathematical tools namely the Fourier transform,\nwhich implicitly assumes a signal is stationary \u2014 by employing a quickly decaying zero-mean oscil-\nlatory function, known as the mother wavelet, which inherently adapts its time-frequency resolution\nto the signal's characteristics. This dual localization property, achieved through the modulation of\nthe wavelet's scale parameter, enables the WT to efficiently analyze non-stationary signals achieving\nlocalization in both time and frequency domain (Daubechies, 1992; Mallat, 2009).\nWe can divide wavelet families into two sets of basis functions: the father wavelet, which captures\nlow frequencies (the approximation), and the mother wavelet, which focuses on high frequencies\n(the detail). Both sets can be modulated via scaling and translation to achieve a multi-resolution\ndecomposition of an arbitrary signal $f(x)$ into the $J$-th lowest approximation component combined\nwith the $J-1$ successive details. In what follows, we obtain approximation $\\{a_k\\}_J$ and detail\ncoefficients $\\{d_k\\}_j, \\forall k, j$ via the maximally decimated Discrete Wavelet Transform (DWT), which\ndecomposes a signal and preserves its length $N$ by applying a cascade of high-pass and low-pass\nconjugate mirror filters via successive convolutions and down-sampling operations in $O(N)$ time."}, {"title": "TOKENIZATION VIA WAVELET DECOMPOSITION", "content": "We ask the following question: given a real-valued univariate time series $X_{1:C} = [x_1,...,x_C]$,\nwhere $C$ is the context length, can we find an optimal map $T: \\mathbb{R} \\rightarrow V$ that encodes the input with a\ncompact but expressive discrete vocabulary $V$? To this end, we develop WaveToken, a tokenization\npipeline divided in scaling, wavelet decomposition, thresholding and quantization. See Section 4.4\nfor an extensive analysis of the chosen hyper-parameters."}, {"title": "MODEL TRAINING, OBJECTIVE FUNCTION AND FORECASTING", "content": "Given a time series context $x_{1:C}$, we apply the steps detailed in Section 3.2 and then concate-\nnate the resulting discrete tokens for approximation and detail coefficients into a vector $Z_{1:C} =\n[a_J, d_J,..., d_1]$, so that the model can learn and forecast coarse-to-fine structures in a multi-scale\nfashion. To minimize ad-hoc modifications to the language model, we opt for an encoder-decoder\narchitecture based on the T5 family (Raffel et al., 2020), which has recently been shown to achieve\nexcellent zero-shot performance on a comprehensive benchmark (Ansari et al., 2024). Other pro-\nposed LLM-based forecasters either reformulate the problem as a question-answering task or use\npatches of the input as tokens, and would therefore not be immediately compatible with our wavelet-\nbased tokenizer (see section 2 for an exhaustive review). For brevity, in what follows we refer to\nWaveToken as being both the tokenizer and the model paired together.\nWe train the model via next-token prediction by minimizing the cross-entropy between the predicted\ndistribution and the categorical output distribution over the corresponding label tokens obtained from\nthe horizon window $Z_{C:C+H}$. In symbols, the loss function for a time series (including EOS) is\n$\\ell_o = - \\sum_{h=1}^{H+1} \\sum_{i=1}^{|V|} \\mathbb{1}\\{z_{C+h+1} = i\\} \\log p_o(z_{C+h+1} = i | Z_{1:C+h})$\nWhile the cross-entropy loss does not induce a metric onto the underlying vocabulary, it offers\ngreater flexibility to learn output distributions of arbitrary shapes. This property lends itself well\nto forecasting applications, where it is often important to capture the correct shape or pattern of the\ntime series without imposing structural limitations such as those inherent in traditional loss functions\nlike MSE and MAE (Le Guen & Thome, 2019).\nLearning an autoregressive model on concatenated groups of wavelet coefficients might seem\ncounter-intuitive: the temporal structure is only preserved within each coefficient group and is bro-\nken as the input transitions from, e.g., approximations to details. In practice, this turns out to be\nsurprisingly helpful as it offers the model a natural way to break down complex sub-structures in\ninputs and outputs by dividing them into a hierarchy of time-localized frequency bands. As we will\nsee in Section 4.3, the model easily learns to exploit these partitions and is able to attend to the right\ncoefficients to improve the overall forecasting accuracy.\nAt inference time, the model produces sample paths over the vocabulary via autoregressive sampling\nfrom the predicted distribution $P_o(Z_{C+h+1} | Z_{1:C+h}), h = 1,..., H$. To obtain a time series\nforecast, we first de-quantize the tokens by mapping them to the corresponding bin center: $\\bar{q}^{-1}(i) =\nC_i$. We then apply the inverse discrete wavelet transform (IDWT) and un-scale the reconstructed\nseries by multiplying it by $\\sigma_{1:C}$ and adding $\\mu_{1:C}$, as depicted in Figure 2."}, {"title": "EXPERIMENTS", "content": "Setup of empirical evaluation\nModels and baselines. We pre-train WaveToken with T5 models of four sizes\u00b2 - Mini (19.2M),\nSmall (44.5M), Base (199M) and Large (705.8M) \u2014 for 200K steps on 8 A100 GPUs, and we com-\npare their performance against i) popular task-specific models trained for each dataset separately,"}, {"title": "IN-DOMAIN & ZERO-SHOT BENCHMARKS", "content": "Figure 3 summarizes the forecasting performance of all models on the in-domain datasets of Bench-\nmark I. WaveToken outperforms all other baselines with the exception of Chronos-Large on WQL\nand MASE, and is superior with respect to VRSE. Considering each model size for each metric,\nWaveToken achieves lower (i.e., better) scores than Chronos 75% of the times and largely improves\non other recent LLM-based forecasters. Similarly, our method is considerably better even than task-\nspecific models trained separately on each dataset. Note again that WaveToken uses a vocabulary"}, {"title": "QUALITATIVE ANALYSIS", "content": "So far we have seen how wavelets allow a transformer-based architecture trained on a large corpus\nto achieve excellent forecasting performance especially on previously unseen datasets, which hints\nat their superior generalization capabilities. It is then worth analyzing more deeply some edge cases\nof practical relevance to showcase how wavelets can efficiently capture a wide variety of complex\npatterns, frequencies and local structures within a compressed representation. Figure 1 shows ex-\namples of synthetically-generated time series which exhibit strong trends, sharp spikes and several\nfrequencies evolving over time. We evaluate WaveToken against popular recent foundation models\nfor time series forecasting. Both Chronos, TimesFM and Moirai clearly underestimate trends, strug-\ngle at isolating sudden spikes and are not able to capture non-stationary behaviours. On the other\nhand, our model is able to leverage the different concatenated coefficient groups which represent the\ntime-localized frequency bands, thereby providing accurate forecasts with very low uncertainty.\nThis phenomenon can be further explained by looking at patterns in the cross-attention layers of\nChronos and our model, as they use the same T5 encoder-decoder architectures. Figure 5 shows\nheat-maps of the cross-attention weights in the eighth decoder layer of Chronos-Base (left) and\nWaveToken-Base (right), when forecasting the sparse spikes in the second row of Figure 1. Two\nmain things are worth noticing: first of all the attention map for our model is clearly divided in four\nquadrants, of which the upper-left and lower-right ones exhibit generally larger attention values.\nThese clearly show that, to forecast approximation coefficients in the horizon (time-steps 0-34 on\nthe x-axis), the model is learning to attend more to approximation coefficients in the context (time-\nsteps 0-258 on the y-axis), and to forecast detail coefficients in the future (time-steps 35-68 on the"}, {"title": "ABLATION STUDY", "content": "Figure 6 shows the effect of different vocabulary sizes, wavelet families and decomposition levels on\nthe forecasting accuracy of WaveToken-Small trained for 200K steps on one A100 GPU. Regarding\nvocabulary size, which determines the precision with which tokens encode wavelet coefficients, we\nobserve a gradual but consistent improvement until $|V| = 1024$, which we select as the optimal\nvalue. For higher vocabulary sizes, WQL and MASE remain flat or worsen on both in-domain and\nzero-shot benchmarks. This phenomenon can be ascribed to the intrinsic compression properties of\nwavelets (Mallat, 2009): by concentrating most of the signal energy onto a few coefficients, we can\neffectively capture more information with a smaller codebook, while a larger one would only reserve\nmore tokens to spurious coefficients. Note that our vocabulary is much smaller than comparable\nmodels: Chronos for example, that leverages the same architectures, uses $|V| = 4096$. As to the\nmany wavelet families available, the central panel of Figure 6 shows that the Biorthogonal-2.2 ba-\nsis achieves optimal performance. This family uses two separate filters with two vanishing moments\nfor analysis and synthesis, a dual structure that allows for symmetric or near-symmetric wavelet\nfunctions that help preventing distortions near the signal edges. This is known to be advantageous\nin many applications, including image compression\u00b3. Families with higher vanishing moments ca-\npable of capturing higher order polynomial did not prove effective. As to the decomposition level,\nwe observe that first-level coefficients are sufficient to achieve good forecasting accuracy. Albeit\ndeeper levels provide more granular time-localized frequencies, the higher number of groups in the\nconcatenated coefficients make it harder for the attention mechanism to identify the relevant ones"}, {"title": "CONCLUSION", "content": "In this work, we develop WaveToken, a tokenization pipeline tailored to a specific goal: constructing\na general-purpose forecasting model capable of capturing a wide variety of complex patterns while\nconsuming as little information as possible, thereby leading to excellent generalization performance\non unseen datasets. We leverage a recently-proposed framework to pre-train an encoder-decoder\narchitecture in the context of time series (Ansari et al., 2024) and re-purpose it to learn an autore-\ngressive model in the space of time-localized frequencies. The resulting wavelet-based vocabulary\nis both compact \u2014 using 1024 tokens, i.e. one quarter of Chronos \u2014 and very expressive, and leads\nto\ni) excellent forecasting accuracy with respect to all other baselines in terms of three complementary\nmetrics: weighted quantile loss for probabilistic forecasts, mean absolute scaled error for point\nforecasts, and visual relative squared error to measure discrepancies in the frequency content relative\nto the ground truth.\nii) superior generalization capabilities, with WaveToken being the best model across all datasets\nand metrics in terms of average rank. In addition, our method can easily capture complex temporal\npatterns in several edge cases relevant for practical applications, as shown in Figure 1.\nAs potential future directions, we foresee that exploring techniques to automatically handle context\nlengths larger than 512 in the tokenizer would allow models to capture longer-range dependencies\nif needed. The DWT is in fact a natural tool to leverage in these settings, as it can encode long\ncontexts without increasing the input length through convolutions and down-sampling. Furthermore,\nWaveToken exploits an autoregressive model on wavelet coefficients. As such, it suffers from slower\ndecoding and inference times relative to recently proposed alternatives, such as patch-based models.\nApplying WaveToken to patch-based architectures represents an interesting area for future research."}, {"title": "EVALUATION METRICS", "content": "Consider a collection of $N$ time series $\\{x_i = [x_{i,1},..., x_{i,C+H}]\\}_{i=1}^N$ that include both context\nand horizon. In Section 4, we evaluated WaveToken and all the other baselines with respect to three\nmetrics, which we now describe more in detail: weighted quantile loss (WQL), mean absolute scaled\nerror (MASE), and visual relative squared error (VRSE)\nWeighted quantile loss (WQL). We use this metric to evaluate probabilistic forecasts $\\hat{q}_{i,C+t}^{\\alpha}$\nobtained by generating $N=20$ samples from the model for an input $i$ at nine quantile levels\n$\\alpha\\in \\{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\\}$. WQL aggregates the standard quantile loss at level\n$\\alpha$ $QL_{\\alpha}(\\hat{q}, x)$ (Koenker & Hallock, 2001) over multiple horizon steps $t=1,...,H$ and series $i$ by\ntaking a weighted average:\n$WQL = 1/9 \\sum_{\\alpha} \\frac{\\sum_{i,t} QL_{\\alpha} (\\hat{q}_{i,C+t}^{\\alpha}, x_{i,t})}{\\sum_{i,t} |x_{i,t}|}$,\nwhere 9 is the number of quantiles used.\nMean absolute scaled error (MASE). We use this metric to evaluate point forecasts $\\hat{x}_i =\n[x_{i,1},..., x_{i,C+H}]$, which we take to be the median quantile $\\hat{q}_{i,C+t}^{0.5}$ across $N=20$ samples for\nprobabilistic models. The MASE (Hyndman & Koehler, 2006) scales the mean absolute error by the\nempirical error of the seasonal na\u00efve model:\n$MASE(\\hat{x}_i, x_i) = \\frac{C}{H} \\frac{\\sum_{t=1}^{C+H} |\\hat{x}_{i,t} - x_{i,t}|}{\\sum_{t=1}^{C-S} |x_{i,t} - x_{i,t+S}|}$,\nwhere $S$ is a seasonality parameter.\nVisual relative squared error (VRSE). We use this metric to measure the frequency content of\n(point) forecasts relative to the ground truth. As for MASE, we take the median forecast $\\hat{q}_{i,C+t}^{0.5}$ as\ninput to the metric for probabilistic models. VRSE (Posam et al., 2024) is defined as follows:\n$VRSE(\\hat{x}_i, x_i) = \\frac{\\sum_{f} (A_{\\hat{x}}(f) - A_{x}(f))^2}{\\sum_{f} (A_{x}(f))^2}$,"}, {"title": "ADDITIONAL RESULTS: BENCHMARKS I & II", "content": "Figures 10 and 11 report the average rank across all datasets in the corresponding benchmark (in-\ndomain and zero-shot, respectively) achieved by all the evaluated models. Tables 1-3 report the\nraw per-dataset values of all metrics for all models on the in-domain benchmark, along with the\ncorresponding Aggregate Relative Score and Average Rank. Similarly, Tables 4-6 report the same\nvalues on the zero-shot benchmark. Results for WaveToken, Chronos, PatchTST, DeepAR and TFT\nare averaged over three seeds."}, {"title": "ADDITIONAL RESULTS: LONG-HORIZON FORECASTING", "content": "Figure 12 shows results on a long-horizon benchmark constructed by increasing the forecast length\n$H$ of each dataset in Benchmark II (Zero-Shot) by a factor of 2 and 3, implying maximum horizons\nof 112 and 168 (for NN5 Daily). Benchmark I (In-Domain) was not used as increasing the forecast\nlength over the prescribed ones (see Table 9) would imply mixing the test and training portions of\nthe datasets. The datasets in Benchmark II without sufficient history in all time series to allow for\nlonger horizons have been skipped. WaveToken-Base outperforms other foundation models across\nall three metrics in the $H \\times 2$ setting. When $H \\times 3$, TimesFM performs better with respect to WQL\nonly. All results for Chronos and WaveToken are averaged across three different seeds."}, {"title": "ADDITIONAL RESULTS: QUALITATIVE ANALYSIS", "content": "Figures 13, 14 and 15 show all the cross-attention maps for the 12 decoder layers of Chronos-Base\n(left) and WaveToken-Base (right) when forecasting the spiky data of the second row in Figure 1."}]}