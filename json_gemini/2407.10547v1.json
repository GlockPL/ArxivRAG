{"title": "LEARNING SOCIAL COST FUNCTIONS FOR HUMAN-AWARE PATH PLANNING", "authors": ["Andrea Eirale", "Matteo Leonetti", "Marcello Chiaberge"], "abstract": "Achieving social acceptance is one of the main goals of Social Robotic Navigation. Despite this topic has received increasing interest in recent years, most of the research has focused on driving the robotic agent along obstacle-free trajectories, planning around estimates of future human motion to respect personal distances and optimize navigation. However, social interactions in everyday life are also dictated by norms that do not strictly depend on movement, such as when standing at the end of a queue rather than cutting it. In this paper, we propose a novel method to recognize common social scenarios and modify a traditional planner's cost function to adapt to them. This solution enables the robot to carry out different social navigation behaviors that would not arise otherwise, maintaining the robustness of traditional navigation. Our approach allows the robot to learn different social norms with a single learned model, rather than having different modules for each task. As a proof of concept, we consider the tasks of queuing and respect interaction spaces of groups of people talking to one another, but the method can be extended to other human activities that do not involve motion.", "sections": [{"title": "1 Introduction", "content": "Robot navigation, especially indoor, has made tremendous progress, enabling the first commercial robots to enter a large number of homes as well as public spaces, such as shopping centers, airports, and restaurants. Most such robots are based on geometric navigation, treating people as dynamic obstacles. On the other hand, social navigation aims to achieve socially acceptable robot behavior\u2014a central goal of sharing spaces with service robots.\nMost of the work in social navigation is focused on the direction and velocity of the people surrounding the robot, to respect personal space and act naturally in scenarios that require coordination, such as narrow corridors. Our work is orthogonal to such efforts and considers how robot path planning should be affected not by people's velocity but by their activity. Indeed, even when people do not move at all, such as in a queue, the robot should not just consider them as static obstacles and cut in front of them.\nWe propose a learning-based approach that leverages existing widespread planners. We focus the learning effort on the social aspect, rather than on areas of navigation in which current planners are very effective, such as obstacle avoidance. For this reason, contrary to end-to-end learning, we propose to learn an additional term of the cost function to use in planning, while retaining the default behavior everywhere else. Planners, to be efficient, work on a cost map, while social navigation depends on many other aspects, such as whether two obstacles are, in fact, people, and whether they are talking to each other or not. Our model takes in input a representation of the position of the people in the vicinity of the robot and the position of the goal, and outputs a social cost map in addition to the usual obstacle one. We present an architecture and a methodology to train such a neural network, which effectively projects the social navigation problem into classical 2D navigation.\nAs a proof of concept, we use two common situations that appear frequently in public spaces: people queuing and people talking in small groups. Our methodology is not tied to these particular scenarios and can be extended to any other setting in which practitioners can provide a dataset. Both scenarios have been extensively studied separately. We show how the same method can effectively plan for both scenarios at the same time, with the potential to incorporate many more. We evaluate the generalization abilities of the proposed architecture; we analyze the generated plans in a simulated environment in Gazebo [1], and we demonstrate the resulting behaviors in the real world on a Pal Robotics TIAGO."}, {"title": "2 Related Work", "content": "The first examples of designing a behavioral policy able to consider social factors date back more than twenty years, with the deployment of RHINO [2] and MINERVA [3] as tour guides in museums. In those cases, the robotic system perceived people as dynamic, non-responsive obstacles. In the following years, researchers focused on systems able to distinguish humans from inanimate objects."}, {"title": "2.1 Motion-based Human-Aware Navigation", "content": "Many works focused on proxemics, inflating and reshaping the cost function of people in the scenery, depending on the velocity and the facing direction of each person [4, 5, 6, 7]. One of the most well-known solutions in this category is the Robot Operating System (ROS) social navigation layers 1, which alter the cost map with a Gaussian distribution around people, with an increased cost in the direction of motion. Mateus et al. [8] exploit deep learning with hybrid asymmetric Gaussian functions to represent proxemics for modeling comfort distance with pedestrians. Truong and Ngo [9] took states (position, orientation, motion, and hand poses) and social interaction information relative to the robot into account to model extended personal space and social interaction space using two-dimensional Gaussian functions.\nSocial Force Model (SFM) [10] is another popular technique to model human and robot motions. Ferrer et al. [11] extend the SFM to present a proactive navigation approach, where the robot can produce actions with a minimum impact on surrounding pedestrians. Yang et al. [12] employ the SFM on an omnidirectional robot to achieve natural and human-like movements. More recent works consider autonomous agents able to plan an optimal trajectory, avoiding collisions with people and obstacles, by planning around estimates of future human motion. These predictions are often achieved with deep neural networks, like generative adversarial networks [13], convolutional neural networks [14, 15], and attention transformers [16].\nIntention-aware navigation is the direct evolution of the previous methods. In this case, by exploiting behavior prediction models, it is possible not only to predict the future movements of humans but also to estimate their final goals. Bai et al. [17] model the uncertainty of human intent in the Partially Observable Markov Decision Process (POMDP) framework. Navigation in dense human crowds is another interesting task, where solutions focused on avoiding blockage due to human activities [18] and cooperating with people through interacting Gaussian processes [19]. Mavrogiannis et al. employ geometric [20] and topological invariant [21] representations to model the coupling among trajectories of multiple navigating agents.\nDeep Reinforcement Learning has been used in many works for prediction in crowd navigation domains. Chen et al. [22] apply CADRL, a deep reinforcement learning framework for socially aware multi-agent collision avoidance. Everett et al. [23] exploit an actor-critic variant to relax prior assumptions and learn policies and agent motion models simultaneously. Furthermore, Tai et al. [24] train a generative adversarial imitation learning model on a dataset generated using the social force model [10]. Finally, Chen et al. [25] use attention-based reinforcement learning to produce interaction-aware collision avoidance behaviors.\nThe works cited above, as well as the majority of scientific literature on social navigation, focus on people's movements: their trajectory, intention, and destination. However, several social norms are independent of movement but dependent on people's activity. For instance, walking between two people talking to each other is considered rude, even if they are just standing still. Such scenarios are currently under-represented in the scientific literature and form the basis of this work."}, {"title": "2.2 Task-specific Social Navigation", "content": "Other works have been more focused on solving specific social tasks during navigation. Xiao et al. [26] train a Performer architecture [27] in an imitation-learning fashion to learn a cost function and improve an MPC controller. This system is then used to solve different social tasks, such as moving around corners and respecting comfort distance in a human-aware manner. However, unlike our approach, Performer-MPC requires the cost function to be learned individually for each navigation scenario.\nWithin the queue following, one of the social navigation scenarios we consider in this paper, Nakauchi et al. [28] designed a dedicated pipeline. Their work focuses on how a line of people can be defined and then perceived by the autonomous agent. The navigation system generates a series of goals (depending on the number of people in the queue) until the back of the line is reached, and does not provide full path planning. In [29, 30], Banisetty et al. exploit simple geometric reasoning to compute a social goal at the beginning of the line. The true goal of the navigation, placed at the end of the line, is then replaced with the social goal. However, this system only allows the robot to join the queue and is unable to follow the line until the robot reaches the final goal.\nMany works focused on detecting groups of people, based on the definition of F-formations provided by Kendon [31]. Cristiani et al. [32] and Setti et al. [33] proposed Hough Voting with the people's positions and head orientations to detect F-formations, while Hedayati et al. [34, 35] treated the problem as a binary classification. A few works focused on unsupervised group detection in dynamic, egocentric views [36, 37], while Schmuck et al. [38, 39] proposed GROup detection With Link prediction (GROWL), demonstrating the effectiveness of Graph Neural Network in the detection of people interaction. While our work does not strictly focus on recognizing the complex connections within groups of people, our experimentation shows that, for social navigation purposes, implicit knowledge of the interaction between people arising from simple spatial information is often sufficient to recognize and avoid sparse groups of people. However, group detection could be integrated within our methodology to further improve the navigation, especially through dense crowds.\nDifferently from all the work described above, our approach is targeted at all scenarios in which people are static, and yet their position or activity dictates socially acceptable robot paths."}, {"title": "3 Background", "content": "Grid-based path planners, such as Dijkstra's or A*, work with a discrete representation of the environment, which can be formalized as a two-dimensional, undirected graph $M = (V, E)$, usually referred to as a grid map. In this graph, $V$ is a vertex set of size $n \\times m$, where each vertex corresponds to a cell in the map, and $E$ is an edge set of size $k$. Edges connect adjacent map cells. A path is a sequence of vertices $P = (v_1, v_2, ..., v_n)$ such that there exists an edge $e_{i,i+1} \\in E$ connecting $v_i$ and $v_{i+1}$, with an associated distance $d_{i,i+1}$ for $1 < i < n - 1$.\nGiven a starting vertex $v_s \\in V$ and a goal $v_g \\in V$, a shortest feasible path from $v_s$ to $v_g$ is defined as $P^* = (v_1, v_2, ..., v_n)$, where $v_1 = v_s$ and $v_n = v_g$. This path is a minimizer of:\n$P^* = \\arg \\min_{v_1,...,v_n \\in V} \\sum_{i=1}^{n-1} f_c(v_i, v_{i+1}),$ (1)\nwhere $f_c: V \\times V \\rightarrow \\mathbb{R}^+$ is a cost function.\nCost functions can be defined with several terms, encoding both the distance between two nodes and the desirability of each node. A cost map $C$ corresponding to a grid map $M$ is an $n \\times m$ matrix, with a cost $c_i \\in \\mathbb{R}^+$ for each element $v_i \\in V$, representing the cost of entering the node. Usually, a priori information about the environment, like walls and other static obstacles, is gathered in a global cost map $C_G$. Sensor data, containing the detection of obstacles around the agent, are instead included in a local cost map $C_L$ centered on the robot's reference frame. The total cost of a transition is, therefore:\n$f_c(v_i, v_{i+1}) = d_{i,i+1} + C_{G,i+1} + C_{L,i+1},$ (2)\nwhere $C_{G,i+1}$ and $C_{L,i+1}$ are the costs of $v_{i+1}$ respectively in $C_G$ and $C_L$."}, {"title": "4 Methodology", "content": "We want to retain all the desirable properties of grid-based navigation systems, such as path optimality and obstacle avoidance, while introducing socially acceptable behaviors. All social aspects that require the robot to avoid a particular node in the map can be encoded in an additional cost map. We learn the social cost function $f_s: M_s \\rightarrow C_s$ through a deep neural network, able to generate the social cost map $C_s$ encoding social obstacles."}, {"title": "4.1 Network Architecture", "content": "The network is an encoder-decoder consisting of convolutional, maxpooling, and upscaling layers with 342,049 (learnable) parameters. The network structure is shown in Figure 2. The input discretizes a 24x24m area with a resolution of 0.2m, represented in a grid map with 120x120 cells. These measures are compatible with a typical indoor Lidar sensor, usually in the 8\u201318m range."}, {"title": "4.2 Dataset creation", "content": "We generate the dataset with a series of RGB images for each human activity we want the system to react to. The dataset consists of pairs $(M_s^{(i)}, C_s^{(i)}))$ of grid map and corresponding cost map.\nThe maps for a particular human activity only have elements pertaining to that activity, excluding other elements, such as obstacles. The label image, that is, the intended output of the network, is a grey-scale cost map with the same dimensions as the input image, representing only the costs corresponding to the input map.\nAs a proof of concept, in this work, we consider the social scenarios of a queue of people and that of small groups of people talking. In principle, any scenario that only requires the robot to avoid certain areas can be added."}, {"title": "4.2.1 Queuing", "content": "In this scenario, we expect the robot to follow the queue to the goal without cutting the line. We create grid maps with a line of people behind a goal, represented with two different colors, so that the network can distinguish the people from the goal. Each sample we generate is randomized through a series of varying parameters: the number of people in the queue (which also influences the total length of the queue), the distance between two people, the distance between the first person and the goal, and the deviation of each person from the center axis of the queue. The label image represents a U-shaped obstacle surrounding the queue. The thickness of the U-shaped obstacle walls should be tuned according to the robot's footprint, in order for the planner to plan around the queue and leave the only available path toward the goal behind the queue (cf. Figure 1). This \u201csocial corridor\" towards the goal should be narrow enough to force the robot to wait behind the last person."}, {"title": "4.2.2 Groups of People", "content": "In this scenario, we expect the robot to go around groups of people rather than through them. The input grid map contains the small groups of people. In this case, the randomized parameters are: the number of people for each group, their position in the map, and the number of groups. The label image consists of a variable number of virtual obstacles (depending on the position of the people), filling the gap between them."}, {"title": "4.2.3 Further refinements", "content": "Each training image contains at least one instance for each considered social scenario, positioned randomly within the grid map space. To improve generalization, we also diversify the dataset according to the following criteria. In the queuing scenario, with a small probability, the training image does not include the queue. Instead, it has the goal in a random position, with the label image not containing any virtual obstacle. This variation prevents the network from associating the social obstacle with the goal alone. Similarly, to prevent the network from associating the obstacle with each person instead of a group of people, a certain number of isolated people have a probability of spawning in the image. Each generated sample pair is oriented differently to obtain rotation invariance in the prediction.\nEvery sample pair of grid map and label is created programmatically with an automatic generator, which may raise the question: Can such a generator be used online-do we need the neural network at all? The generator for each scenario can produce pairs $(M_s^{(i)}, C_s^{(i)}))$, but it is not equivalent to the function $f_s$ that translates a general social grid map into a cost map. The generator first creates the social obstacles in randomized locations and then places people and the goal. This is easier than the inverse operation, and works on a single scenario. For this reason, generators are used only to produce samples for the function $f_s$, which is then learned to classify a given grid map into its corresponding cost map and to do so with possibly different scenarios simultaneously."}, {"title": "4.3 Deployment", "content": "We deployed the method in simulation and on a real robot by integrating it into a ROS2 [40] node. Periodically, this node retrieves the position of the goal and people and converts this information into the input grid map image representation $M_s$ used by the network. The generated image is then provided to the neural network, and the output prediction of the social cost map $C_s$ is published on an OccupancyGrid ROS2 topic. This map is used as a local cost map by the Nav2 [41] planner, in addition to all the other traditional cost maps provided to the navigation system. From Nav2 parameters, we select a wavefront Dijkstra expanded holonomic planner in combination with a Model Predictive Path Integral Controller (MPPI). In order to wait in line behind each person, we use a simple Nav2 behavior tree with Wait nodes in combination with a short obstacle maximum detection range, which enables the robot to reach the queue and then wait for the following person to proceed, freeing the computed path, before advancing."}, {"title": "5 Experimental Validation", "content": "In this section, we first evaluate the generalization of the learned social cost function, and then demonstrate the resulting behaviors in a number of scenarios both in simulation and on a real robot."}, {"title": "5.1 Learning Evaluation", "content": "The network was trained with a generated dataset containing 5 \u00d7 105 samples for 5 epochs on a machine with an Intel Core i7-9700K CPU and an Nvidia GeForce RTX 2080 Ti, taking approximately 30 minutes. Generated cost maps may differ from the expected test label in a few cells but still produce a socially correct path. For this reason, instead of the accuracy of the cost map on the test set, we measure the success rate of the generated cost maps. We consider a cost map successful if the path induced by the map is correct; that is, it follows the queue appropriately and does not navigate across groups.\nWe produced three different test sets, containing respectively images only with queues, only with groups, and with queues and groups mixed. In every dataset we generated for training and testing, where the queue appears, the distance between two people in the line and from the first person and the goal is in the range 0.5-1.5 meters, while the deviation of each person from the center axis of the queue is in the range 0-0.5 meters from each side. In all cases the success rate is above 95%, which we consider sufficiently reliable."}, {"title": "5.2 Simulated Scenarios", "content": "We now demonstrate the system in several practical scenarios in simulation, comparing the generated plan with and without the learned social costs. We built Gazebo [1] simulation environments containing a queue of people, a variable number of groups of people interacting, and other people passing by. The robot mounts a lidar sensor to detect physical obstacles around it, while the information regarding the position of people is obtained directly from the simulator.\nIn Figure 3, the robot is given a goal in front of a queue. The network can recognize and delimit the queue as well as the area between each group of people talking. Given the social obstacles, the planner computes a path around the groups and the line of people, queuing correctly behind the last person in line. In comparison, without the social layer, the robot plans between the people talking and directly to the goal, cutting the line.\nWe further demonstrate the queue task by approaching the line of people from different angles. The robot is positioned in front, on the side, and behind different queues containing a variable number of people. As seen from Figure 4, the network can correctly recognize the queue regardless of where the robot comes from. Furthermore, the provided social cost map allows the planner to compute an appropriate path for the robot to queue correctly, position behind the last person, and follow the line of people until the goal is reached. Although the maximum number of people appearing in training set queues is 5, the network can correctly recognize and delimit a queue with any number of people, as long as the whole line is sufficiently aligned and contained within the input image.\nWe further demonstrate the talking groups scenario in Figure 5(a). The network can correctly identify different groups formed by a variable number of people. The network's virtual obstacles often have irregular shapes, but they are sufficient to prevent the robot from crossing the interaction zone.\nA series of limitations of the network are dictated by the dataset used for its training. For example, the network cannot correctly recognize the queue if it is not settled along a straight line, since our dataset only contained straight queues. Our tests demonstrated that as long as each person's deviation from the center of the image does not exceed 1m (from both sides), the network can classify the queue correctly, but it often fails otherwise. Varying the maximum distance between each person in the queue could also lead to instability of the recognition. We noted that recognition starts failing when the mutual distance between people in the line is around 3m. Furthermore, while groups of people not in a queue but near one do not usually interfere, they may be interpreted as a second queue if placed near the goal. Similarly, if a goal is given too close to a particularly aligned group, it could be misinterpreted as a line of people.\nOther incorrect results could arise in the presence of a crowd of people. The network can correctly recognize multiple groups of people and set the appropriate costs for each interaction space, even when such groups are close to each other. However, if the distances between people of different groups are smaller than the distances of people within a group, the network can produce incorrect virtual obstacles. Further refinement of the dataset can overcome some of these limitations, but if the environment is extremely crowded, other strategies that require the robot to ask people to move become necessary."}, {"title": "5.3 Real-robot deployment", "content": "The same system used in the simulation environment was deployed on a PAL Robotics TIAGO robot. Similarly to the simulation, the Lidar sensor detects obstacles around the robot. The information regarding the number and position of people in the scene are extracted from YoloV8 [42], using an RGB-D camera mounted on TIAGo's head. We recreated the tests made in simulation in the real environment (one example is shown in Figure 6).\nFor the queue task, once the people in line were correctly recognized and localized, the robot was given different starting points: in front, on the side, and behind the queue. Also in the real-life setting, the network recognized and delimited the queue correctly. The robot entered the line of people and followed it until the goal was reached, independently from the direction it came from.\nWe recreated a scenario with multiple groups with different numbers of people. A goal was provided at the end of the map. The network delimited the interaction areas, and the robot reached the goal without crossing any group of people.\nThe reliability of the perception and tracking system may limit the correctness of the computed social cost. In the queuing task, when the robot enters the line of people, the last person in the queue can obstruct the vision of the people behind. For this reason, the social grid map cannot be just generated from perceptions each time. To mitigate this problem, we keep track of every person seen in the past time instants, similarly to how the navigation stack remembers obstacles that are not presently in view. We place a marker in the last position where a person was detected and remove it when the area is in view and the laser sees no obstacle."}, {"title": "6 Conclusions", "content": "This paper proposes a novel method to enhance Social Robotic Navigation by modifying the planner cost function. Differently from past literature, we do not focus on proxemics or estimates of people's movement based on kinematic information. Instead, we consider areas the robot should plan to avoid based on the position of people in the environment, in accordance to specific social norms. We develop a proof of concept based on two common social tasks: queuing in a line of people and respecting interaction spaces of people talking with each other. We demonstrate the effectiveness of our solution both in simulation and in a real-world scenario, comparing it with traditional goal-based navigation. We focused on learning and navigation, and used off-the-shelf libraries for computer vision. The social maps could be further refined if the vision module was able to detect more than people's positions, for instance, whether or not they were talking, and with whom.\nAdapting the robot's behavior to what people are doing in the environment has not received as much attention as how people are moving. Some social norms affect navigation despite people not moving at all. We expect this line of research to receive more attention within social navigation in the near future."}]}