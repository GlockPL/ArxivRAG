{"title": "LightWeather: Harnessing Absolute Positional Encoding for Efficient and Scalable Global Weather Forecasting", "authors": ["Yisong Fu", "Fei Wang", "Zezhi Shao", "Chengqing Yu", "Yujie Li", "Zhao Chen", "Zhulin An", "Yongjun Xu"], "abstract": "Recently, Transformers have gained traction in weather forecasting for their capability to capture long-term spatial-temporal correlations. However, their complex architectures result in large parameter counts and extended training times, limiting their practical application and scalability to global-scale forecasting. This paper aims to explore the key factor for accurate weather forecasting and design more efficient solutions. Interestingly, our empirical findings reveal that absolute positional encoding is what really works in Transformer-based weather forecasting models, which can explicitly model the spatial-temporal correlations even without attention mechanisms. We theoretically prove that its effectiveness stems from the integration of geographical coordinates and real-world time features, which are intrinsically related to the dynamics of weather. Based on this, we propose LightWeather, a lightweight and effective model for station-based global weather forecasting. We employ absolute positional encoding and a simple MLP in place of other components of Transformer. With under 30k parameters and less than one hour of training time, LightWeather achieves state-of-the-art performance on global weather datasets compared to other advanced DL methods. The results underscore the superiority of integrating spatial-temporal knowledge over complex architectures, providing novel insights for DL in weather forecasting.", "sections": [{"title": "Introduction", "content": "Accurate weather forecasting is of great significance in a wide variety of domains such as agriculture, transportation, energy, and economics. In the past decades, there was an exponential growth in the number of automatic weather stations, which play a pivotal role in modern meteorology (Sose and Sayyad 2016). They are cost-effective for applications (Bernardes et al. 2023; Tenzin et al. 2017) and can be flexibly deployed to almost anywhere around the world, collecting meteorological data at any desired resolution.\nWith the development of deep learning (DL), studies have embarked on exploring DL approaches for weather forecasting. The goal of data-driven DL methods is to fully leverage the historical data to enhance the accuracy of forecasting (Schultz et al. 2021). The weather stations around the world are ideally positioned to provide a substantial amount of data for DL methods. However, the observations of worldwide stations exhibit intricate spatial-temporal patterns that vary across regions and periods, posing challenge for global-scale weather forecasting (Wu et al. 2023b).\nRecently, Transformers have become increasingly popular in weather forecasting due to their capability to capture long-term spatial-temporal correlations. When confronting the challenge of global-scale forecasting, Transformer-based methods employ more sophisticated architectures, leading to hundreds of millions of parameters and multiple days of training time. In the era of large model (LM), this phenomenon become particularly evident. Such expenses limit their scalability to large-scale stations and restrict their application in practical scenarios (Deng et al. 2024).\nDespite the complexity of these architectures, we observe that the resulting improvements in performance are, in fact, quite limited. This motivates us to rethink the bottleneck of station-based weather forecasting and further design a model as effective as Transformer-based methods but more efficient and scalable. For this purpose, we delve deeper into the architecture of Transformer-based weather forecasting models and obtain an interesting conclusion: absolute positional encoding is what really works in Transformer-based weather"}, {"title": "Related Works", "content": "DL Methods for Station-based Weather Prediction\nAlthough there has been a great success of radar- or reanalysis-based DL methods (Bi et al. 2023; Lam et al. 2023; Chen et al. 2023), they can only process gridded data and are incompatible with station-based forecasting.\nFor station-based forecasting, spatial-temporal graph neural networks (STGNNs) are proved to be effective in modeling spatial-temporal patterns of weather data (Lin et al. 2022; Ni, Wang, and Fang 2022), but most of them only provide short-term forecasting (i.e., 6 or 12 steps), which limits their applicability.\nRecently, Transformer-based approaches have gained more popularity for their capability of capturing long-term spatial-temporal correlations. For instance, MGSFformer (Yu et al. 2024a) and MRIformer (Yu et al. 2024b) employ attention mechanisms to capture correlations from multi-resolution data obtained through down sampling. However, attention mechanisms take quadratic computational complexity for both spatial and temporal correlation modeling, which is unaffordable in global-scale forecasting.\nSeveral studies attempted to enhance the efficiency of attention mechanisms. Typically, AirFormer (Liang et al. 2023) restricts attention to focusing only on local information. Corrformer (Wu et al. 2023b) employs a more efficient multi-correlation mechanism to supplant attention mechanisms. Nevertheless, these optimizations came with a greater amount of computation and parameter, resulting in limited improvements in efficiency.\nStudies of the effectiveness of Transformers\nThe effectiveness of Transformers has been thoroughly discussed in the fields of computer vision (CV) (Yu et al. 2022; Lin et al. 2024) and natural language processing (NLP) (Bian et al. 2021). In time series forecasting (TSF), LSTF-Linear (Zeng et al. 2023) pioneered the exploration and outperformed a variety of Transformer-based methods with a linear model. Shao et al. (2023) posited that Transformer-based models face over-fitting problem on specific datasets. MTS-Mixers (Li et al. 2023) and MEAformer (Huang et al. 2024) further questioned the necessity of attention mechanisms in Transformers for TSF and replaced them with MLP-based information aggregations. These studies consider positional encoding as supplementary to attention mechanisms and consequently remove it along with attention mechanisms, yet none have recognized the importance of positional encoding."}, {"title": "Preliminaries", "content": "Methodology\nProblem Formulation. We consider N weather stations and each station collects C meteorological variables (e.g., temperature). Then the observed data at time t can be denoted as $X_t \\in \\mathbb{R}^{N\\times C}$. The 3D geographical coordinates of stations are organized as a matrix $\\Theta \\in \\mathbb{R}^{3\\times N}$, which is naturally accessible in station-based forecasting. Given the historical observation of all stations from the past $T_h$ time steps and optional spatial and temporal information, we aim to learn a function $F(\\cdot)$ to forecast the values of future $T_f$ time steps :\n$Y_{t:t+T_f} = F(X_{t-T_h:t}; \\Theta, t),$   (1)\nwhere $X_{t-T_h:t} \\in \\mathbb{R}^{T_h\\times N \\times C}$ is the historical data, and $Y_{t:t+T_f} \\in \\mathbb{R}^{T_f \\times N \\times C}$ is the future data."}, {"title": "Overview of LightWeather", "content": "As illustrated in Figure 2, LightWeather consists of a data embedding layer, an absolute positional encoding layer, an MLP as encoder, and a regression layer. LightWeather replaces the redundant structures in Transformer-based models with a simple MLP, which greatly enhances the efficiency without compromising performance.\nData Embedding\nLet $X_{i,j} \\in \\mathbb{R}^{T_h}$ be the historical time series of station i and variable j. The data embedding layer maps $X_{i,j}$ to the embedding $E_{i,j} \\in \\mathbb{R}^d$ in latent space:\n$E_{i,j} = FC_{embed}(X_{i,j}),$   (2)\nwhere $FC(\\cdot)$ denotes a fully connected layer.\nAbsolute Positional Encoding\nAbsolute positional encoding injects information about the absolute position of the tokens in sequence, which is widely regarded as an adjunct to permutation-invariant attention mechanisms. However, we find it helpful to capture spatial-temporal correlations by introducing additional geographical and temporal knowledge into the model.\nIn our model, absolute positional encoding includes two parts: spatial encoding and temporal encoding.\nSpatial Encoding. Spatial encoding provides the geographical knowledge of stations to the model, which can explicitly model the spatial correlations among worldwide stations. Specifically, we encode the geographical coordinates of the station into latent space by a simple fully connected layer, thus spatial encoding $S^i \\in \\mathbb{R}^d$ can be denoted as:\n$S^i = FC_s(\\Theta^i),$   (3)\nwhere $\\Theta^i \\in \\mathbb{R}^3$ represents the coordinates of the station i.\nTemporal Encoding. Temporal encoding provides real-world temporal knowledge to the model. We utilize three learnable embedding matrices $T \\in \\mathbb{R}^{24\\times d}$, $D \\in \\mathbb{R}^{31\\times d}$ and $M \\in \\mathbb{R}^{12\\times d}$ to save the temporal encodings of all time steps (Shao et al. 2022). They represent the patterns of weather in three scales (T denotes hours in a day, D denotes days in a month and M denotes the months in a year), contributing to model the multi-scale temporal correlations of weather. We add them together with data embedding to obtain $H^{i,j}$:\n$H^{i,j} = E_{i,j} + S^{i} + T_t + D_t + M_t.$   (4)\nEncoder\nWe utilize a L-layer MLP as encoder to learn the representation Z from the embedded data $H^{i,j}$. The l-th MLP layer with residual connect can be denoted as:\n$(Z^{i,j})^{l+1} = FC(\\sigma(FC((Z^{i,j})^l))) + (Z^{i,j})^l, $   (5)\nwhere $\\sigma(\\cdot)$ is the activation function and $(Z^{i,j})^0 = H^{i,j}$.\nRegression Layer\nWe employ a linear layer to map the representation $Z \\in \\mathbb{R}^{d\\times N\\times C}$ to the specified dimension, yielding the prediction $Y \\in \\mathbb{R}^{T_f \\times N \\times C}$.\nLoss Function\nWe adopt Mean Absolute Error (MAE) as the loss function for LightWeather. MAE measures the discrepancy between the prediction $\\hat{Y}$ and the ground truth Y by:\n$\\mathcal{L}(\\hat{Y}, Y) = \\frac{1}{NCT_f} \\sum_{i=1}^{N} \\sum_{j=1}^{C} \\sum_{t=1}^{T_f} | \\hat{Y}_{t,i,j} - Y_{t,i,j} |.$   (6)"}, {"title": "Theoretical Analysis", "content": "In this part, we provide a theoretical analysis of LightWeather, focusing on its effectiveness and efficiency of spatial-temporal embedding.\nEffectiveness of LightWeather. The effectiveness of LightWeather lies in the fact that absolute positional encoding integrates geographical coordinates and real-world time features into the model, which are intrinsically linked to the evolution of atmospheric states in global weather system. Here we theoretically demonstrate this relationship.\nTheorem 1. Let {\u03bb, \u03c6, z} be the longitude, latitude and elevation of a weather station and v is a meteorological variable collected by the station, then the time evolution of v is a function of \u03bb, \u03c6, z and time t:\n$\\frac{\\partial v}{\\partial t} = F(\\lambda, \\varphi, z,t)$.   (7)\nProof. We provide the proof with zonal wind speed as an example; analogous methods can be applied to other meteorological variables.\nAccording to the basic equations of atmospheric dynamics in spherical coordinates (Marchuk 2012), the zonal wind speed u obeys the equation:\n$\\frac{du}{dt} = -\\frac{1}{\\rho a cos \\varphi} \\frac{\\partial p}{\\partial \\lambda} + fv + \\frac{uv tan \\varphi}{r} + F_x,$   (8)\nwhere $\\frac{d}{dt} = \\frac{\\partial}{\\partial t} + u \\frac{\\partial}{a cos \\varphi \\partial \\lambda} + v \\frac{\\partial}{a \\partial \\varphi} + w \\frac{\\partial}{\\partial z}$, p is pressure, \u03c1 is atmospheric density, $F_x$ is zonal friction force, and r is geocentric distance.\nThe geocentric distance can be further denoted as r = a+z, where a is the radius of the earth and z is the elevation. Since a is a constant and a >> z, we have $\\frac{\\partial r}{\\partial z} = 1$ and we can approximate r with a.\nIt is possible to render the left side of the equation spatial-independent by rearranging terms:\n$\\frac{\\partial u}{\\partial t} = -\\frac{\\partial u}{a cos \\varphi \\partial \\lambda} + v \\frac{\\partial u}{a \\partial \\varphi} + w \\frac{\\partial u}{\\partial z} - \\frac{1}{\\rho a cos \\varphi} \\frac{\\partial p}{\\partial \\lambda} + fv + \\frac{uv tan \\varphi}{a} + F_x.$   (9)\nTherefore, we have\n$\\frac{\\partial u}{\\partial t} = F(\\lambda, \\varphi, z, t).$   (10)\nConsidering the use of historical data spanning $T_h$ steps for prediction, it is not difficult to draw the corollary:\nCorollary 1.1.\n$V_t = \\alpha V_{t-1:t-T_h} + G(\\lambda, \\varphi, z, T),$   (11)\nwhere $V_{t-1:t-T_h}$ is the historical data, and $a \\in \\mathbb{R}^{T_h}$.\nAccording to Eq. (11), we conclude that predictions for the future are bifurcated into two components: the fitting to historical observations and the modeling of the function $G(\\lambda, \\varphi, z, \\tau)$, which represents the spatial-temporal correlations. When the scale is small, e.g., in single-station forecasting, even a simple linear model can achieve great performances (Zeng et al. 2023). However, as the scale expands to global level, modeling G becomes the key bottleneck of forecasting.\nThe majority of prior models are designed to fit historical observations more accurately by employing increasingly complex structures. In comparison, LightWeather can explicitly model G with \u03bb, \u03c6, z, \u03c4 introduced by absolute positional encoding, thereby enhancing the predictive performance."}, {"title": "Experimental Setup", "content": "Experiments\nDatasets. We conduct extensive experiments on 5 datasets including worldwide and nationwide:\n\u2022 GlobalWind and GlobalTemp (Wu et al. 2023b) contains the hourly averaged wind speed and temperature of 3,850 stations around the world, spanning 2 years with 17,544 time steps.\n\u2022 Wind_CN and Temp_CN contains the daily averaged wind speed and temperature of 396 stations in China, spanning 10 years with 3,652 time steps.\n\u2022 Wind_US (Wang et al. 2019) contains the hourly averaged wind speed of 27 stations in US, spanning 62 months with 45,252 time steps.\nWe partition all datasets into training, validation and test sets in a ratio of 7:1:2.\nBaselines. We compare our LightWeather with the following three categories of baselines:\n\u2022 Classic methods: HI (Cui, Xie, and Zheng 2021), ARIMA (Shumway, Stoffer, and Stoffer 2000).\n\u2022 Universal DL methods: Informer (Zhou et al. 2021), FEDformer (Zhou et al. 2022), DSformer (Yu et al. 2023), PatchTST (Nie et al. 2023), TimesNet (Wu et al. 2023a), GPT4TS (Zhou et al. 2023), Time-LLM (Jin et al. 2024), DLinear (Zeng et al. 2023).\n\u2022 Weather forecasting specialized DL methods: AirFormer (Liang et al. 2023), Corrformer (Wu et al. 2023b), MRIformer (Yu et al. 2024b).\nEvaluation Metrics. We evaluate the performances of all baselines by two commonly used metrics: Mean Absolute Error (MAE) and Mean Squared Error (MSE).\nImplementation Details. Consistent with the prior studies (Wu et al. 2023b), we set the input length to 48 and the predicted length to 24. Our model can support larger input and output lengths, whereas numerous models would encounter out-of-memory errors, making comparison infeasible. We adopt the Adam optimizer (Kingma and Ba 2014) to train our model. The number of layers in MLP is 2, and the hidden dimensions are contingent upon datasets, ranging from 64 to 2048. The batch size is set to 32 and the learning rate to 5e-4. All models are implemented with PyTorch 1.10.0 and tested on a single NVIDIA RTX 3090 24GB GPU.\nMain Results\nTable 1 presents the results of performance comparison between LightWeather and other baselines on all datasets. The results of LightWeather are averaged over 5 runs with standard deviation included. It can be found that most Transformer-based models presents limited performance,"}, {"title": "Efficiency Analysis", "content": "Earlier in the paper, we illustrated the performance-effciency comparison with other mainstream Transformer-based methods in Figure 1. In this part, we further conduct a comprehensive comparison between our model and other baselines in terms of parameter counts, epoch time, and GPU memory usage. Table 2 shows the results of the comparison. Benefiting from the simple architecture, LightWeather surpasses other DL methods both in performance and efficiency. Compared with the weather forecasting specialized methods, LightWeather demonstrates an order-of-magnitude improvement across three efficiency metrics, being about 6 to 6,000 times smaller, 100 to 300 times faster, and 10 times memory-efficient respectively."}, {"title": "Ablation Study", "content": "Effects of Absolute Positional Encoding. Absolute positional encoding is the key component of LightWeather. To study the effects of absolute positional encoding, we first conduct experiments on models that are removed the spatial encoding and the temporal encoding respectively. The results are shown in Exp. 1 and 2 depicted in Table 3, where we find that the removal of each encoding component leads a decrease on MSE. This indicates that both spatial and temporal encodings are beneficial.\nThen we make a comparison between relative and absolute positional encoding. Specifically, relative spatial encoding embeds the indices of stations instead of the geographical coordinates.\nHyperparameter Study. We investigate the effects of two important hyperparameters: the number of layers L in MLP and the hidden dimension d. As illustrated in Figure 4 (a), LightWeather achieves the best performance when L = 2, whereas an increase in L beyond 2 results in over-fitting and a consequent decline in model performance. Figure 4 (b) shows that the metrics decrease with the increment of hidden dimension and begin to converge when d exceeds 1024. For reasons of efficiency, we chose d = 64 in our previous experiments, but this selection did not yield the peak performance of our model. Moreover, it should be emphasized that LightWeather can outperform other Transformer-based models even when d = 32 and the parameters is less than 10k. This further substantiates that absolute positional encoding is more effective than the complex architectures of Transformer-based models.\nIn this section, we further evaluate the effects of absolute positional encoding by applying it to Transformer-based models with the results reported in Table 4. Only channel-independent (CI) Transformers (Nie et al. 2023) are selected due to our encoding strategy, i.e., we generate spatial embeddings for each station respectively. It is evident that absolute positional encoding can significantly enhance the performance of Transformer-based models, enabling them to achieve nearly state-of-the-art results."}, {"title": "Visualization", "content": "Visualization of Forecasting Results. To intuitively comprehend the collaborative forecasting capabilities of LightWeather for worldwide stations, we present a visualization of the forecasting results here. Kriging is employed to interpolate discrete points into a continuous surface, facilitating a clearer observation of the variations."}, {"title": "Conclusion", "content": "This work innovatively highlights the importance of absolute positional encoding in Transformer-based weather forecasting models. Even in the absence of attention mechanisms, absolute positional encoding can explicitly capture spatial-temporal correlations by integrating geographical coordinates and real-world time features, which are closely related to the evolution of atmospheric states in global weather system. Subsequently, we present LightWeather, a lightweight and effective weather forecasting model. We utilize the absolute positional encoding and replace the main components of Transformer with an MLP. Extensive experiments demonstrate that LightWeather can achieve satisfactory performance on global weather datasets, and the simple structure endows it with high efficiency and scalability to fine-grained data. This work posits that the incorporation of geographical and temporal knowledge is more effective than relying on intricate model architectures. This approach is anticipated to have a substantial impact beyond the realm of weather forecasting, extending its relevance to the predictions that involve geographical information (e,g., air quality and marine hydrology) and illuminating a new direction for DL approaches in these domains. In future work, we plan to integrate the model with physical principles more closely to enhance its interpretability."}, {"title": "A Thereotical Proofs", "content": "A.1 Proof of Corollary 1.1\nProof. According to Theorem 1, we have\n$V_t = V_{t-1} + \\int_{(t-1)\\Delta t}^{t\\Delta t} F(\\lambda, \\varphi, z, t) dt,$   (12)\nwhere $\\Delta t$ is the interval between time steps.\nFor brevity, $\\int_{(t-1)\\Delta t}^{t\\Delta t} F(\\lambda, \\varphi, z, t) dt$ is denoted as $I_t = \\int_{(t-1)\\Delta t}^{t\\Delta t} I(\\lambda, \\varphi, z, \\tau)$. We can split $V_{t-1}$ into two parts, then we have\n$V_t = \\alpha_1 V_{t-1} + (1 - \\alpha_1) V_{t-1} + I_t,$   (13)\n$= \\alpha_1 V_{t-1} + (1 - \\alpha_1) (V_{t-2} + I_{t-1}) + I_t$.\nBy repeating this procedure for the subsequent values of v, we have\n$V_t = \\alpha_1 V_{t-1} + \\alpha_2 V_{t-2} + \\cdots + \\alpha_{T_h-1} V_{t-T_h+1}$  (14)\n$+ (1 - \\alpha_1 - \\alpha_2 - \\cdots - \\alpha_{T_h-1}) V_{t-T_h}$\n$+ I_t + (1 - \\alpha_1) I_{t-1} + (1 - \\alpha_1 - \\alpha_2) I_{t-2}$ \n$+ \\cdots + (1 - \\alpha_1 - \\alpha_2 - \\cdots - \\alpha_{T_h-1}) I_{t-T_h}$.\nLet $\\alpha_{T_h} = (1 - \\alpha_1 - \\cdots - \\alpha_{T_h-1})$ and $a$ denotes $\\{\\alpha_1, \\alpha_2, \\cdots, \\alpha_{T_h} \\}$, then Eq. (14) can be expressed as\n$V_t = a V_{t-T_h:t-1} + I_t + (1 - \\alpha_1) I_{t-1}$  (15)\n$+ (1 - \\alpha_1 - \\alpha_2) I_{t-2} + \\cdots$\n$+ (1 - \\alpha_1 - \\alpha_2 - \\cdots - \\alpha_{T_h-1}) I_{t-T_h}$.\nThe weighted sum of I is a function of \u03bb, \u03c6, z, T, thus we have\n$V_t = a V_{t-T_h:t-1} + G(\\lambda, \\varphi, z, t).$   (16)\nA.2 Proof of Theorem 2\nProof. The data embedding layer maps the input data into latent space with dimension d, thereby introducing $T_h(d + 1)$ parameters. Analogously, the regression layer introduces $T_f(d + 1)$ parameters. For the positional encoding, spatial encoding costs 3(d + 1) parameters, and temporal encoding costs (24 +31 + 12)(d + 1) parameters. The parameter count of a L-layer MLP with residual connect is $2Ld(d+1)$. Thus, the total number of parameters is The total number of parameters required for the LightWeather is $(2Ld + T_h + T_f + 70)(d + 1)$."}, {"title": "C Overall Workflow", "content": "Algorithm 1: Overall workflow of LightWeather.\nRequire: historical data $X \\in \\mathbb{R}^{T_h\\times N \\times C}$, geographical coordinates $\\Theta \\in \\mathbb{R}^{N\\times 3}$, the first time step t\nEnsure: forecasting result $Y \\in \\mathbb{R}^{T_f \\times N \\times C}$\n1: X = X.transpose (1, -1) /* X \u2208 $\\mathbb{R}^{C\\times N \\times T_h}$ */\n2: E = $FC_{embed}$ (X)/* Data embedding, E \u2208 $\\mathbb{R}^{C\\times N \\times d}$ */\n3: S = $FC_s$ ($\\Theta$) /* S\u2208 $\\mathbb{R}^{N\\times d}$ */\n4: S = S.repeat(C, 1, 1) /* S\u2208 $\\mathbb{R}^{C\\times N \\times d}$ */\n5: hour, day, mon = time_feature(t) /* Obtain hour, month and day from t */\n6: T = T[hour].repeat(C, N, 1)\n7: D = D[day].repeat(C, N, 1)\n8: T = M[mon].repeat(C, N, 1)\n9: $Z_0$ = E+ S+T+D+M /* $Z_0$ \u2208 $\\mathbb{R}^{C\\times N \\times d}$ */\n/* MLP encoder */\n10: for l in {0, 1, \u2026\u2026\u2026, L \u2212 1} do\n11:  $Z_{l+1}$ = $FC_2$ ($\\sigma$ ($FC_1$ ($Z_l$))) + $Z_l$\n12: end for\n13: Y = $FC_r$ ($Z_L$) /* Regression layer, Y \u2208 $\\mathbb{R}^{C\\times N \\times T_f}$ */\n14: Y = Y.transpose (1, -1)\n15: return Y"}]}