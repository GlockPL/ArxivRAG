{"title": "LightWeather: Harnessing Absolute Positional Encoding for Efficient and Scalable Global Weather Forecasting", "authors": ["Yisong Fu", "Fei Wang", "Zezhi Shao", "Chengqing Yu", "Yujie Li", "Zhao Chen", "Zhulin An", "Yongjun Xu"], "abstract": "Recently, Transformers have gained traction in weather forecasting for their capability to capture long-term spatial-temporal correlations. However, their complex architectures result in large parameter counts and extended training times, limiting their practical application and scalability to global-scale forecasting. This paper aims to explore the key factor for accurate weather forecasting and design more efficient solutions. Interestingly, our empirical findings reveal that absolute positional encoding is what really works in Transformer-based weather forecasting models, which can explicitly model the spatial-temporal correlations even without attention mechanisms. We theoretically prove that its effectiveness stems from the integration of geographical coordinates and real-world time features, which are intrinsically related to the dynamics of weather. Based on this, we propose LightWeather, a lightweight and effective model for station-based global weather forecasting. We employ absolute positional encoding and a simple MLP in place of other components of Transformer. With under 30k parameters and less than one hour of training time, LightWeather achieves state-of-the-art performance on global weather datasets compared to other advanced DL methods. The results underscore the superiority of integrating spatial-temporal knowledge over complex architectures, providing novel insights for DL in weather forecasting.", "sections": [{"title": "Introduction", "content": "Accurate weather forecasting is of great significance in a wide variety of domains such as agriculture, transportation, energy, and economics. In the past decades, there was an exponential growth in the number of automatic weather stations, which play a pivotal role in modern meteorology (Sose and Sayyad 2016). They are cost-effective for applications (Bernardes et al. 2023; Tenzin et al. 2017) and can be flexibly deployed to almost anywhere around the world, collecting meteorological data at any desired resolution.\nWith the development of deep learning (DL), studies have embarked on exploring DL approaches for weather forecasting. The goal of data-driven DL methods is to fully leverage the historical data to enhance the accuracy of forecasting (Schultz et al. 2021). The weather stations around the world are ideally positioned to provide a substantial amount of data for DL methods. However, the observations of worldwide stations exhibit intricate spatial-temporal patterns that vary across regions and periods, posing challenge for global-scale weather forecasting (Wu et al. 2023b).\nRecently, Transformers have become increasingly popular in weather forecasting due to their capability to capture long-term spatial-temporal correlations. When confronting the challenge of global-scale forecasting, Transformer-based methods employ more sophisticated architectures, leading to hundreds of millions of parameters and multiple days of training time. In the era of large model (LM), this phenomenon become particularly evident. Such expenses limit their scalability to large-scale stations and restrict their application in practical scenarios (Deng et al. 2024).\nDespite the complexity of these architectures, we observe that the resulting improvements in performance are, in fact, quite limited. This motivates us to rethink the bottleneck of station-based weather forecasting and further design a model as effective as Transformer-based methods but more efficient and scalable. For this purpose, we delve deeper into the architecture of Transformer-based weather forecasting models and obtain an interesting conclusion: absolute positional encoding is what really works in Transformer-based weather"}, {"title": "Methodology", "content": "Problem Formulation. We consider N weather stations and each station collects C meteorological variables (e.g., temperature). Then the observed data at time t can be denoted as \\(X_t \\in \\mathbb{R}^{N\\times C}\\). The 3D geographical coordinates of stations are organized as a matrix \\(\\Theta \\in \\mathbb{R}^{3\\times N}\\), which is naturally accessible in station-based forecasting. Given the historical observation of all stations from the past \\(T_h\\) time steps and optional spatial and temporal information, we aim to learn a function \\(F(\\cdot)\\) to forecast the values of future \\(T_f\\) time steps :\n\\[Y_{t:t+T_f} = F(X_{t-T_h:t}; \\Theta, t),\\]\nwhere \\(X_{t-T_h:t} \\in \\mathbb{R}^{T_h\\times N\\times C}\\) is the historical data, and \\(Y_{t:t+T_f} \\in \\mathbb{R}^{T_f\\times N\\times C}\\) is the future data."}, {"title": "Overview of LightWeather", "content": "As illustrated in Figure 2, LightWeather consists of a data embedding layer, an absolute positional encoding layer, an MLP as encoder, and a regression layer. LightWeather replaces the redundant structures in Transformer-based models with a simple MLP, which greatly enhances the efficiency without compromising performance.\nLet \\(X_{i,j} \\in \\mathbb{R}^{T_h}\\) be the historical time series of station i and variable j. The data embedding layer maps \\(X_{i,j}\\) to the embedding \\(E_{i,j}\\) in latent space:\n\\[E_{i,j} = FC_{embed}(X_{i,j}),\\]\nwhere \\(FC(\\cdot)\\) denotes a fully connected layer.\nAbsolute positional encoding injects information about the absolute position of the tokens in sequence, which is widely regarded as an adjunct to permutation-invariant attention mechanisms. However, we find it helpful to capture spatial-temporal correlations by introducing additional geographical and temporal knowledge into the model.\nIn our model, absolute positional encoding includes two parts: spatial encoding and temporal encoding.\nSpatial Encoding. Spatial encoding provides the geographical knowledge of stations to the model, which can explicitly model the spatial correlations among worldwide stations. Specifically, we encode the geographical coordinates of the station into latent space by a simple fully connected layer, thus spatial encoding \\(S^{i} \\in \\mathbb{R}^d\\) can be denoted as:\n\\[S^{i} = FC_s(\\Theta^{i}),\\]\nwhere \\(\\Theta^{i} \\in \\mathbb{R}^{3}\\) represents the coordinates of the station i.\nTemporal Encoding. Temporal encoding provides real-world temporal knowledge to the model. We utilize three learnable embedding matrices \\(T \\in \\mathbb{R}^{24\\times d}\\), \\(D \\in \\mathbb{R}^{31\\times d}\\) and \\(M \\in \\mathbb{R}^{12\\times d}\\) to save the temporal encodings of all time steps (Shao et al. 2022). They represent the patterns of weather in three scales (T denotes hours in a day, D denotes days in a month and M denotes the months in a year), contributing to model the multi-scale temporal correlations of weather. We add them together with data embedding to obtain \\(H_{i,j}\\):\n\\[H_{i,j} = E_{i,j} + S^{i} + T_t + D_t + M_t.\\]\nWe utilize a L-layer MLP as encoder to learn the representation Z from the embedded data \\(H_{i,j}\\). The l-th MLP layer with residual connect can be denoted as:\n\\[(Z^{i,j})_{l+1} = FC_2(\\sigma(FC_1((Z^{i,j})_l))) + (Z^{i,j})_l,\\]\nwhere \\(\\sigma(\\cdot)\\) is the activation function and \\((Z^{i,j})_0 = H_{i,j}\\).\nWe employ a linear layer to map the representation \\(Z \\in \\mathbb{R}^{d\\times N\\times C}\\) to the specified dimension, yielding the prediction \\(Y \\in \\mathbb{R}^{T_f\\times N\\times C}\\).\nWe adopt Mean Absolute Error (MAE) as the loss function for LightWeather. MAE measures the discrepancy between the prediction Y and the ground truth Y by:\n\\[\\mathcal{L}(Y, \\hat{Y}) = \\frac{1}{NCT_f} \\sum_{i=1}^{N} \\sum_{j=1}^{C} \\sum_{t=1}^{T_f} |\\hat{Y}^{i,j}_t - Y^{i,j}_t|.\\]"}, {"title": "Theoretical Analysis", "content": "In this part, we provide a theoretical analysis of LightWeather, focusing on its effectiveness and efficiency of spatial-temporal embedding.\nThe effectiveness of LightWeather lies in the fact that absolute positional encoding integrates geographical coordinates and real-world time features into the model, which are intrinsically linked to the evolution of atmospheric states in global weather system. Here we theoretically demonstrate this relationship.\nLet \\{\\lambda, \\varphi, z\\} be the longitude, latitude and elevation of a weather station and \\(v\\) is a meteorological variable collected by the station, then the time evolution of \\(v\\) is a function of \\(\\lambda, \\varphi, z\\) and time \\(t\\):\n\\[\\frac{\\partial v}{\\partial t} = F(\\lambda, \\varphi, z, t).\\]\nWe provide the proof with zonal wind speed as an example; analogous methods can be applied to other meteorological variables.\nAccording to the basic equations of atmospheric dynamics in spherical coordinates (Marchuk 2012), the zonal wind speed \\(u\\) obeys the equation:\n\\[\\frac{du}{dt} = -\\frac{1}{\\rho r cos \\varphi} \\frac{\\partial p}{\\partial \\lambda} + fv + \\frac{uv tan \\varphi}{r} + F_\\lambda,\\]\nwhere \\(\\frac{d}{dt} = \\frac{\\partial}{\\partial t} + u \\frac{\\partial}{a cos \\varphi \\partial \\lambda} + v \\frac{\\partial}{a \\partial \\varphi} + w\\frac{\\partial}{\\partial z}\\), \\(p\\) is pressure, \\(\\rho\\) is atmospheric density, \\(F_\\lambda\\) is zonal friction force, and \\(r\\) is geocentric distance.\nThe geocentric distance can be further denoted as \\(r = a+z\\), where \\(a\\) is the radius of the earth and \\(z\\) is the elevation. Since \\(a\\) is a constant and \\(a >> z\\), we have \\(\\frac{\\partial}{\\partial r} = \\frac{\\partial}{\\partial z}\\) and we can approximate \\(r\\) with \\(a\\).\nIt is possible to render the left side of the equation spatial-independent by rearranging terms:\n\\[\\frac{\\partial u}{\\partial t} = -(u \\frac{\\partial}{a cos \\varphi \\partial \\lambda} + v \\frac{\\partial}{a \\partial \\varphi} + w\\frac{\\partial}{\\partial z}) + \\frac{1}{\\rho a cos \\varphi} \\frac{\\partial p}{\\partial \\lambda} + fv + \\frac{uv tan \\varphi}{a} + F_\\lambda.\\]\nTherefore, we have\n\\[\\frac{\\partial u}{\\partial t} = F(\\lambda, \\varphi, z, t).\\]\nConsidering the use of historical data spanning \\(T_h\\) steps for prediction, it is not difficult to draw the corollary:\n\\[v_\\tau = \\alpha v_{\\tau-1:\\tau-T_h} + G(\\lambda, \\varphi, z, \\tau),\\]\nwhere \\(v_{\\tau-1:\\tau-T_h}\\) is the historical data, and \\(\\alpha \\in \\mathbb{R}^{T_h}\\).\nAccording to Eq. (11), we conclude that predictions for the future are bifurcated into two components: the fitting to historical observations and the modeling of the function \\(G(\\lambda, \\varphi, z, \\tau)\\), which represents the spatial-temporal correlations. When the scale is small, e.g., in single-station forecasting, even a simple linear model can achieve great performances (Zeng et al. 2023). However, as the scale expands to global level, modeling G becomes the key bottleneck of forecasting.\nThe majority of prior models are designed to fit historical observations more accurately by employing increasingly complex structures. As shown in Figure 3 (a) (b), they simplistically regard G as a function of historical values, and the complex structures may lead to over-fitting of it. In comparison, LightWeather can explicitly model G with \\(\\lambda, \\varphi, z, \\tau\\) introduced by absolute positional encoding, as shown in Figure 3 (c), thereby enhancing the predictive performance."}, {"title": "Experimental Setup", "content": "We conduct extensive experiments on 5 datasets including worldwide and nationwide:\nGlobalWind and GlobalTemp contains the hourly averaged wind speed and temperature of 3,850 stations around the world, spanning 2 years with 17,544 time steps.\nWind_CN and Temp_CN contains the daily averaged wind speed and temperature of 396 stations in China, spanning 10 years with 3,652 time steps.\nWind_US contains the hourly averaged wind speed of 27 stations in US, spanning 62 months with 45,252 time steps.\nWe partition all datasets into training, validation and test sets in a ratio of 7:1:2."}, {"title": "Experiments", "content": "We compare our LightWeather with the following three categories of baselines:\nInformer is a Transformer with a sparse self-attention mechanism.\nWe evaluate the performances of all baselines by two commonly used metrics: Mean Absolute Error (MAE) and Mean Squared Error (MSE).\nConsistent with the prior studies (Wu et al. 2023b), we set the input length to 48 and the predicted length to 24. Our model can support larger input and output lengths, whereas numerous models would encounter out-of-memory errors, making comparison infeasible. We adopt the Adam optimizer (Kingma and Ba 2014) to train our model. The number of layers in MLP is 2, and the hidden dimensions are contingent upon datasets, ranging from 64 to 2048. The batch size is set to 32 and the learning rate to 5e-4. All models are implemented with PyTorch 1.10.0 and tested on a single NVIDIA RTX 3090 24GB GPU.\nTable 1 presents the results of performance comparison between LightWeather and other baselines on all datasets. The results of LightWeather are averaged over 5 runs with standard deviation included. It can be found that most Transformer-based models presents limited performance,"}, {"title": "Efficiency Analysis", "content": "Earlier in the paper, we illustrated the performance-effciency comparison with other mainstream Transformer-based methods in Figure 1. In this part, we further conduct a comprehensive comparison between our model and other baselines in terms of parameter counts, epoch time, and GPU memory usage. Table 2 shows the results of the comparison. Benefiting from the simple architecture, LightWeather surpasses other DL methods both in performance and efficiency. Compared with the weather forecasting specialized methods, LightWeather demonstrates an order-of-magnitude improvement across three efficiency metrics, being about 6 to 6,000 times smaller, 100 to 300 times faster, and 10 times memory-efficient respectively."}, {"title": "Ablation Study", "content": "Absolute positional encoding is the key component of LightWeather. To study the effects of absolute positional encoding, we first conduct experiments on models that are removed the spatial encoding and the temporal encoding respectively. The results are shown in Exp. 1 and 2 depicted in Table 3, where we find that the removal of each encoding component leads a decrease on MSE. This indicates that both spatial and temporal encodings are beneficial.\nThen we make a comparison between relative and absolute positional encoding. Specifically, relative spatial encoding embeds the indices of stations instead of the geographical coordinates. Kindly note that we project the temporal di-"}, {"title": "Generalization of Absolute Positional Encoding", "content": "In this section, we further evaluate the effects of absolute positional encoding by applying it to Transformer-based models with the results reported in Table 4. Only channel-independent (CI) Transformers (Nie et al. 2023) are selected due to our encoding strategy, i.e., we generate spatial embeddings for each station respectively. It is evident that ab-"}, {"title": "Visualization", "content": "To intuitively comprehend the collaborative forecasting capabilities of LightWeather for worldwide stations, we present a visualization of the forecasting results here. Kriging is employed to interpolate discrete points into a continuous surface, facilitating a clearer observation of the variations.\nThe forecasting results are shown in the right column of Figure 5, while the ground-truth values are in the left. Overall, the forecasting results closely align with the ground-truth values, indicating that LightWeather can effectively capture the spatial-temporal patterns of global weather data and make accurate predictions.\nTo better interpret the effectiveness of our model, we visualize the learned embeddings of absolute positional encoding. Due to the high dimension of the embeddings, t-SNE (Van der Maaten"}, {"title": "Conclusion", "content": "This work innovatively highlights the importance of absolute positional encoding in Transformer-based weather forecasting models. Even in the absence of attention mechanisms, absolute positional encoding can explicitly capture spatial-temporal correlations by integrating geographical coordinates and real-world time features, which are closely related to the evolution of atmospheric states in global weather system. Subsequently, we present LightWeather, a lightweight and effective weather forecasting model. We utilize the absolute positional encoding and replace the main components of Transformer with an MLP. Extensive experiments demonstrate that LightWeather can achieve satisfactory performance on global weather datasets, and the simple structure endows it with high efficiency and scalability to fine-grained data. This work posits that the incorporation of geographical and temporal knowledge is more effective than relying on intricate model architectures. This approach is anticipated to have a substantial impact beyond the realm of weather forecasting, extending its relevance to the predictions that involve geographical information (e,g., air quality and marine hydrology) and illuminating a new direction for DL approaches in these domains. In future work, we plan to integrate the model with physical principles more closely to enhance its interpretability."}, {"title": "A Theoretical Proofs", "content": "According to Theorem 1, we have\n\\[v_\\tau = v_{\\tau-1} + \\int_{(\\tau-1)\\Delta t}^{\\tau \\Delta t} F(\\lambda, \\varphi, z, t)dt,\\]\nwhere \\(\\Delta t\\) is the interval between time steps.\nFor brevity, \\(\\int_{(\\tau-1)\\Delta t}^{\\tau \\Delta t} F(\\lambda, \\varphi, z, t)dt\\) is denoted as \\(I_\\tau = \\int_{(\\tau-1)\\Delta t}^{\\tau \\Delta t} I(\\lambda, \\varphi, z, t)\\). We can split \\(v_{\\tau-1}\\) into two parts, then we have\n\\[v_\\tau = \\alpha_1 v_{\\tau-1} + (1 - \\alpha_1)v_{\\tau-1} + I_\\tau,\\]\n\\[= \\alpha_1 v_{\\tau-1} + (1 - \\alpha_1) (v_{\\tau-2} + I_{\\tau-1}) + I_\\tau.\\]\nBy repeating this procedure for the subsequent values of \\(v\\), we have\n\\[v_\\tau = \\alpha_1 v_{\\tau-1} + \\alpha_2 v_{\\tau-2} + \\dots + \\alpha_{T_h-1} v_{\\tau-T_h+1}\n+ (1 - \\alpha_1 - \\alpha_2 - \\dots - \\alpha_{T_h-1}) v_{\\tau-T_h}\n+ I_\\tau + (1 - \\alpha_1) I_{\\tau-1} + (1 - \\alpha_1 - \\alpha_2) I_{\\tau-2}\n+ \\dots + (1 - \\alpha_1 - \\alpha_2 - \\dots - \\alpha_{T_h-1}) I_{\\tau-T_h}.\\]\nLet \\(\\alpha_{T_h} = (1 - \\alpha_1 - \\dots - \\alpha_{T_h-1})\\) and \\(\\mathbf{\\alpha}\\) denotes \\{\\alpha_1, \\alpha_2, \\dots, \\alpha_{T_h}\\}, then Eq. (14) can be expressed as\n\\[v_\\tau = \\mathbf{\\alpha} v_{\\tau-T_h:\\tau-1} + I_\\tau + (1 - \\alpha_1) I_{\\tau-1}\n+ (1 - \\alpha_1 - \\alpha_2) I_{\\tau-2} + \\dots\n+ (1 - \\alpha_1 - \\alpha_2 - \\dots - \\alpha_{T_h-1}) I_{\\tau-T_h}.\\]\nThe weighted sum of I is a function of \\(\\lambda, \\varphi, z, \\tau\\), thus we have\n\\[v_\\tau = \\mathbf{\\alpha} v_{\\tau-T_h:\\tau-1} + G(\\lambda, \\varphi, z, \\tau).\\]\nThe data embedding layer maps the input data into latent space with dimension d, thereby introducing \\(T_h(d+1)\\) parameters. Analogously, the regression layer introduces \\(T_f(d + 1)\\) parameters. For the positional encoding, spatial encoding costs \\(3(d + 1)\\) parameters, and temporal encoding costs \\((24 +31 + 12)(d + 1)\\) parameters. The pa- rameter count of a L-layer MLP with residual connect is \\(2Ld(d+1)\\). Thus, the total number of parameters is The to- tal number of parameters required for the LightWeather is \\((2Ld+T_h +T_f +70)(d+1)\\)."}, {"title": "C Overall Workflow", "content": "The overall workflow of LightWeather is shown in Algo- rithm 1."}]}