{"title": "GenSim2:\nScaling Robot Data Generation with\nMulti-modal and Reasoning LLMs", "authors": ["Pu Hua", "Minghuan Liu", "Annabella Macaluso", "Yunfeng Lin", "Weinan Zhang", "Huazhe Xu", "Lirui Wang"], "abstract": "Robotic simulation today remains challenging to scale up due to the\nhuman efforts required to create diverse simulation tasks and scenes. Simulation-\ntrained policies also face scalability issues as many sim-to-real methods focus\non a single task. To address these challenges, this work proposes GenSim2, a\nscalable framework that leverages coding LLMs with multi-modal and reasoning\ncapabilities for complex and realistic simulation task creation, including long-\nhorizon tasks with articulated objects. To automatically generate demonstration\ndata for these tasks at scale, we propose planning and RL solvers that generalize\nwithin object categories. The pipeline can generate data for up to 100 articulated\ntasks with 200 objects and reduce the required human efforts. To utilize such\ndata, we propose an effective multi-task language-conditioned policy architecture,\ndubbed proprioceptive point-cloud transformer (PPT), that learns from the gener-\nated demonstrations and exhibits strong sim-to-real zero-shot transfer. Combining\nthe proposed pipeline and the policy architecture, we show a promising usage of\nGenSim2 that the generated data can be used for zero-shot transfer or co-train\nwith real-world collected data, which enhances the policy performance by 20%\ncompared with training exclusively on limited real data.", "sections": [{"title": "1 Introduction", "content": "Robot learning requires large amounts of interaction data and evaluation, which are expensive to\nacquire at scale in the real world. Robot simulation holds the promise of providing such data and\nverification in high diversity and efficiency across objects, tasks, and scenes. While the ability to\nsimulate has led to many successes in AI across Gaming, Go, and Mathematical Proofs [2, 3, 4],\nthere are two requirements for such a path to be successful in robotics: The data needs to scale in\ncomplexity without significant human efforts and the data needs to be realistic enough to transfer\nto the real world. Previous works [5, 6, 7, 8, 9, 10, 11] have made significant progress in scalable\nsimulation benchmarks in robotics and training policies on the simulation data.\nFoundation models [12], particularly generative models pre-trained on internet-scale data [13, 14,\n15], have demonstrated impressive capabilities required for generating robot simulation tasks, such\nas coding [16], spatial reasoning [17], task semantics [9], planning [18, 19], video prediction[20, 21],\nand cost and reward understanding [22, 23]. While foundation models have shown impressive capa-\nbilities to output actions to solve robotic tasks directly in the real world [24], simulation provides a\nlow-cost and scalable platform to learn robust end-to-end policies. In addition to generating numer-\nous tasks, automatically solving them to produce demonstration data and transferring the learned"}, {"title": "2 Related Work", "content": "Task Generation in Robotic Simulation. Training and evaluation for robotics have greatly bene-\nfited from developments in robotic simulation such as Mujoco, NVIDIA Isaac, Drake, and Sapien\n[25, 26, 27, 1]. With these tools, researchers and engineers have been able to develop benchmarks\nwith tens to hundreds of unique, hand-designed robotic tasks [28, 29, 30, 31, 32, 33, 28]. However,\nthe requirements of high-quality assets, scenes, and task design along with required verifications\nfor task solvability and real-world transfer demand a significant amount of human skill and effort.\nRecent works have explored methods such as domain randomizations [34, 7, 35, 36], procedural\nasset generation [8, 37] and text-to-3D synthesis [38, 39, 40, 41] to mitigate some of these efforts.\nResearchers also investigated how generative models can generate suites of robotic or agent tasks\n[23, 42, 43, 44, 45, 9, 10, 11] or generate interactive simulations directly from videos [20, 21]. In\nparticular, GenSim [9] develops a novel pipeline that generates over 100 simulation tasks utilizing\nLLMs, verifies these tasks, uses them to generate data, and trains multi-task policies on top of this\ndata. Similarly, RoboGen [10] uses RL to solve complicated locomotion tasks and deformable ma-\nnipulation and RoboCasa [11] generates massive high-fidelity tasks for multiple embodiments in\nmobile manipulation tasks. In our work, we leverage multi-modal LLMs to generate 6-DOF robotic\nmanipulation and long-horizon tasks at scale. Furthermore, we focus on complex yet realistic tasks\nthat can solved with different solvers and transferred to the real world.\nMulti-Task Policy Learning with 3D Information. With the surge of robotic data, recent works\nhave explored multi-task policy learning, usually conditioned on language inputs. In particular,\npolicy learning often benefits from access to explicit 3D information when doing generalized 6-\nDOF tasks [46, 47]. [48] uses RGB-D inputs and output pixel-level affordance map. [46] uses voxel\nas policy input information and output keyframe actions. [47, 49] uses point cloud as inputs to the\npolicies. [50] uses keypoints at the corners to manipulate boxes with dexterous hands. In our work,\nwe focus on point-cloud transformers as multi-task policies for articulated object manipulation.\nSim-to-Real Transfer. While simulation provides scalable training data and evaluation runs, sim-\nto-real transfer [51, 52] is a challenge and an active research area. RL-based controllers [53] leverage\nsimulation environments for large-scale interactions to learn robust behaviors. Previous works have\nalso explored techniques such as domain randomizations [54, 34], domain adaptations [55], and\npolicy composition [56] to improve sim-to-real transfer. Our work can be viewed as a distillation\nprocess from the knowledge in foundation models such as MLLMs[14, 13], into policies, which can\nhave a smaller sim-to-real \u201csemantic\" gap compared to rule-based simulation data.\""}, {"title": "3 Generating Tasks and Training Policies at Scale", "content": "The proposed GenSim2 framework executes a series of processes including task proposal (3.1),\ndemonstration generation (3.2), and policy learning and transfer (3.3). We illustrate the pipeline in\nFig. 2. Our work enhances GenSim [9] at the level of task complexity beyond top-down pick-and-"}, {"title": "3.1 Task Proposal", "content": "Primitive Task. Our pipeline begins with proposing primitive tasks, namely tasks with a single\nsimple motion (e.g. opening a box), by prompting a LLM to generate a novel task, defined by its\ntask description (a short phrase or sentence), used assets and task code. We start from a fixed asset\nlibrary and a small task library initialized with hand-designed example tasks. We parse them into the\nprompts for in-context learning. We query the LLM to discover applicable assets for creating a new\ntask. The LLM then takes in as input the generated task definition and outputs the corresponding\ntask code. This code is finally compiled and ready for demonstration generation in Section 3.2.\nLong-horizon Task. Moreover, we extend our pipeline to generate long-horizon tasks that consist\nof multiple steps of primitive tasks and involve manipulating articulated and rigid-body objects. Ex-\namples include opening a box, placing a ball inside, and then closing the box. When generating\na long-horizon task, we execute a task decomposition process between task proposal and code im-\nplementation, to decompose the long-horizon task into several sub-tasks (primitive tasks). We use\ntwo distinct methods for generating long-horizon tasks: (1) Top-down: We directly generate a task\nin a curriculum, then decompose the task into several sub-tasks, each with a dedicated solver. (2)\nBottom-up: We first generate primitive tasks and build up a task library. Then the LLM will be\nprompted to select tasks from the pre-built task library to compose a new task. We have observed\nreasoning LLM to improve task proposal performance in this stage. After we have finished the task\nplanning, we continue to generate solvers and demonstrations for the subtasks in succession. Fig. 9\ndemonstrates visualizations of some generated long-horizon tasks."}, {"title": "3.2 Demonstration Generation", "content": "In this section, our goal is to create a solver to generate demonstrations given a task (or sub-task)\ncode. The objective of our generation pipeline is to collect large-scale, high-quality data for gen-\neral 6-DOF manipulation task learning in the real-world setting, our task solver should meet the\nfollowing requirements: 1) Universal for 6-DOF tasks without task-specific designs; 2) Robust to\ndifferent scene configurations; 3) Fast to execute; 4) Deterministic with high success rates. While\nsuch a solver is easy to design for top-down pick-and-place, in which we only need to specify some\n2D position on the table as waypoints and execute primitive \"pick\" and \"place\" actions, it becomes\nchallenging for more complex tasks such as general articulated object manipulation."}, {"title": "Motion Planner.", "content": "kPAM[57] is a keypoint-based planner proposed for category-level manipulation\ntasks. It defines a robotic task by an actuation pose, namely the homogeneous transformation re-\nquired to manipulate the target object, and it addresses an optimization problem based on several\nkeypoint-based constraints to get this pose. Following Wang et al. [33], we improve and extend\nKPAM to produce an object-centric trajectory of end-effector poses, termed actuation motions, to\nsolve a generated task. We parse the constraints and actuation motions into parameterized config-\nurations that are easily interpretable and codable. We provide an illustration in Fig. 7 to show how\nthe KPAM planner solves a task."}, {"title": "Multi-modal Task Solver Generation.", "content": "Compared to previous works that primarily utilize language\nprompts, our approach incorporates visual information and key points [58, 59, 60], serving as an\nexplicit representation of scene details (e.g., object structures, affordance and spatial information).\nThe pipeline of solver generation is depicted in Fig. 7. First, we execute the generated code and\ncapture scene images. This visual data, together with object key points, are then used to prompt an\nMLLM to generate the planner constraints. Subsequently, we visualize the actuation pose defined\nby these constraints. Finally, we query the MLLM to generate the actuation motions. Optionally,\nwe also introduce reject sampling to guide the MLLM in refining its previous outputs."}, {"title": "3.3 Policy Learning and Tranfer", "content": "Multi-Task Training. We design a multi-task policy structure, denoted Proprioception Point cloud\nTransformer (PPT), as illustrated in Fig. 4. Specifically, we handle three kinds of observations: point\ncloud, proprioceptive states, and language task description, all of which can be obtained from the\nreal world. Each observation is separately tokenized via respective encoders and cross-attention,\nfused together in the shared latent space through transformer blocks, and post-processed into global\ncondition tokens [61]. Given the global condition tokens, the policy predicts a sequence of actions\nthrough the policy head. In our implementation, we test against various policy heads such as MLPs,\nthe transformer decoder [62], and the diffusion model [63]. See Appendix B.2 for more details."}, {"title": "4 Experiment: Task and Data Generation", "content": "In this section, we aim to verify the feasibility of the task generation framework and investigate the\neffectiveness of the task and data generation pipeline.\nEvaluation Criteria. We evaluate the task generation procedure mainly on two types of success\nrates: 1) execution rate, the success rate of completing the whole pipeline without any error such as\nsyntax or runtime error; 2) solution rate, the success rate of solving the generated task.\nAblation Study. In this section, we conduct a careful ablation study on each component of\nGenSim2. The results, depicted in Fig. 13, are summarized as follows:\nTypes of LLMs: We test multi-modal LLM, reasoning LLM, and vanilla LLM on solution rate in\nFig. 5 left. Without visual data as input, the performance of the pipeline deteriorates with signifi-\ncantly lower success rates in task generation under both criteria. The performance drops because the\nconstraints for actuation pose can not be correctly generated as it requires details for object structure."}, {"title": "5 Experiment: Multi-Task Policy Training and Transfer", "content": "In this section, we show how the generated data by GenSim2 can be used by multi-task imitation\nlearning and sim-to-real transfer."}, {"title": "5.1 Multi-task Training in Simulation", "content": "Training. With the tasks and data generated by our pipeline, we train a multi-task policy (with 382M\nparameters) across different numbers of tasks and test its generalization to new scenarios. In Fig. 6\nleft, we jointly train a different number of LLM-generated tasks and test on 4 original tasks under\nthe low data regime such as 10 demos per task. Interestingly, adding more tasks will first drop the\nperformance and then increase it by virtue of scaling. Please refer to Appendix B for more details.\nGeneralization. After training, we test the generalization ability of our policy to unseen object\ninstances, compared with a policy with RGB inputs. We split the instances into train/test sets. Eval-\nuated results are shown in Fig. 6 right, where we find that the success rates of the PPT architectures\nonly drop by less than 3% on unseen instances, whereas the RGB policies reduce by quite a bit.\nThese results indicate that by generating data with object-level and spatial-level variance as domain\nrandomization, along with the pointcloud-based policy architecture design, the trained policy can\nacquire generalization in both aspects, which lays the foundation for further sim-to-real transfer."}, {"title": "5.2 Real-World Experiments", "content": "To evaluate the quality of the data collected in simulation and how GenSim2 helps in real-robot\ntasks, we test sim-to-real and co-training on the generated data over several tasks. As for the setup,\nwe use the Franka Research 3 robot arm with a modified, deformable TPU parallel gripper for easier\ngrasping. The robot work cell is equipped with three RealSense D435 cameras: one wrist-mounted\nand two externally facing the scene. Each captures an RGB-D observation which is combined and\nprocessed into a point cloud to be used as observations. More details are listed in Appendix C.\nWe perform experiments on 8 real-world tasks, collecting 100 demonstrations for each task in sim-\nulation, along with an additional 10 real-world demonstrations via teleoperation. We assess the\nquality and utility of the generated data across three distinct training setups: (1) using only simula-\ntion data, (2) using only real-world data, and (3) using a combination of both. The results of these\nevaluations are presented in Tab. 2. Our findings can be summarized as follows: (a) data generated\nby GenSim2 enables effective zero-shot sim-to-real transfer, with the resulting policy outperform-\ning one trained solely on limited real-world data; and (b) when co-trained with real-world data, the\ndata generated by GenSim2 significantly enhances the policy performance even by 20% in absolute\nvalues and 50% in relative scale. These results underscore the potential of large-scale, high-quality\ndata generation, like GenSim2, to reduce the burden of extensive real-world data collection while\nimproving policy effectiveness for real-world tasks."}, {"title": "6 Limitations and Discussions", "content": "Our proposed method has several limitations. Due to the lack of \"robotic centric\" knowledge, such as\n3D spatial understanding, foundation models like GPT-4V still face hallucination issues in creating\nmeaningful tasks and successfully coding them. Additionally, human involvement, though minimal,\nis still required to generate these complex manipulation tasks. Finally, we have only considered\n6-dof tasks in zero-shot sim-to-real transfer with limited point cloud observations."}, {"title": "7 Conclusion and Future Work", "content": "In this work, we propose GenSim2, a task and demonstration generation framework that utilizes\nmulti-modal foundation models to generate up to 100 robotic simulation tasks at scale, such as long-\nhorizon manipulation with articulated objects. We ablate on different task solvers and generation\ncomponents, as well as propose a multi-task pointcloud-based policy architecture that distills gen-\nerated demonstrations from simulation transfer to the real world. Future works include expanding\ntask complexity and diversity through advanced multi-modal agents and 3D asset generation. Future\nworks can explore advanced sim-to-real methods for complex tasks with multiple embodiments."}, {"title": "Appendix", "content": "A Task Generation Details\nA.1 Asset Statistics\nWe summarize the information of the articulated asset library we used in this project in Table 4. To\nease the workload of the LLM in understanding the manipulatable parts of an Articulation, we only\nreserve one unfixed (prismatic or revolute) joint in each object and fix the others. For rigid body\nobjects, we build a dataset adapted from the YCB object dataset in Maniskill2. We have modified\nthe scale and joint limits of the objects to make the scene proper."}, {"title": "A.2 Task Generation Statistics", "content": "In this section, we investigate the statistics of the tasks in the generation process. We have generated\n100 tasks based on the asset library described in Appendix A.1, including 50 primitive tasks and 50\nlong-horizon tasks. Notice that we consider tasks with the same object class but different instances\nas the same tasks. Despite the tasks we have generated, these tasks do not include all assets in the\nasset library and the asset library can also be expanded to contain more object classes and instances."}, {"title": "A.3 Task Solver Details", "content": "KPAM Planner. kPAM[57] is a keypoint-based planner proposed for category-level manipulation\ntasks, and we follow the roadmap proposed in Wang et al. [33] to improve kPAM and apply it in our\narticulation tasks. It generally defines a robotic task by the homogeneous transformation required\nto manipulate the target object. kPAM discovers such transformation by solving an optimization\nproblem built on several keypoint-based constraints. After kPAM implicitly defines the end-effector\npose to get contact with the object, which we call the actuation pose, we need to build the trajectory\nto approach the actuation pose and subsequently complete the task. To this end, pre-actuation and\npost-actuation motions are introduced, both of which include a series of waypoints to guide the end-\neffector motions (such as making a turn or pushing by moving forward). Together with the actuation\npose, an object-centric trajectory of key poses is designed to solve an articulation manipulation task.\nWe provide an illustration of leveraging KPAM planner to solve a task in Fig. 8.\nRL Learner. In addition to planners, we also leverage a reinforcement learning (RL) learner [10] as\nan alternative solver for the generated manipulation tasks, in which we prompt an LLM to produce\na reward function for a target task. After the task proposal stage, we're provided a task definition\nand task code generated by the LLM which is used as context for a LLM to generate an executable\nreward function. The LLM is supplemented with a small library of curated reward examples as well\nas an API consisting of reward functions corresponding to various predefined components that it can\nrefer to, such as the distance between the end-effector and the object, or the joint positions of certain\narticulated objects. If successful generation is achieved, we append the reward into the task code\nand execute the final learning code using Proximal Policy Optimization (PPO)[66] and fix a default\nset of hyperparameters for all tasks.\nIn summary, we propose the kPAM motion planner and RL policy learner as complementary meth-\nods for solving generated tasks. For kPAM, since it is defined by a well-formulated optimization\nproblem regarding fine-grained constraints and costs, the output motions are much smoother and\nmore natural than motions from other learning-based methods. Moreover, kPAM is inherently gen-\neralizable to different scene configurations and object instances because the constraints and actuation\nmotions are all \"object-centric\" elements, which will be further exploited for domain randomization\nin Section 3.3. Furthermore, kPAM is fast to execute (~ 2 seconds planning time given a configura-\ntion) and robust to multiple runs. Despite the strength of KPAM, RL provides advantages in creative\nsolution proposals for tasks that are too complex or ambiguous for kPAM to define proper con-\nstraints, especially for those involving thin objects or contact-rich manipulation. In this project, we\ngive priority to the kPAM planner in most cases because it produces more realistic and generalizable\nmotions, and leaves those that can not be solved for the RL learner."}, {"title": "A.4 Human Verification", "content": "We measure the human efforts required in GenSim2, including labeling keypoints for the motion\nplanner and multi-shot rejection sampling in task solver generation.\nKeypoint Labeling. We set up a protocol for keypoint labeling and annotated 1-3 keypoints for each\nasset. We measured the time spent on 10 objects and the average time is 8.2s per task.\nRejection Sampling. While task solver generation inevitably involves some degree of human in-\ntervention, we have made significant efforts to minimize this effort. To evaluate the efficiency of\nour approach, we conducted a user study measuring the time required for a human user to generate\na novel task and corresponding solver using GenSim2, and compared it with the time needed for\nmanual task design. The results are presented in Tab. 3. We consider the performance of one of\nthe project's authors (User 1) as expert proficiency and additionally involve two other users who\nare unfamiliar with our pipeline. The findings indicate that our method significantly reduces task\ncreation time by 50%, and over 90% of the required time involves waiting for LLM to response.\nFurthermore, the reduced performance gap between experts and beginners suggests that our pipeline\nrequires less technical expertise, making it accessible to users with varying levels of familiarity."}, {"title": "A.5 Demonstration Generation Statistics", "content": "In this section, we assess the efficiency and quality of demonstration generation using our KPAM-\nbased motion planner. Recalling our goal to use our generation pipeline as a data amplifier to\nenhance the generalization and sim-to-real transfer ability of a simulation policy, the generated data\nshould be diverse and of high quality for data augmentation or domain randomization.\nTo this end, we evaluate the robustness of our kPAM solver to random scene configurations and\nobject instances. For primitive tasks, we add uniform noise in [-0.1m, 0.1m] to the x and y positions\nof the object, as well as a random rotation in [-30\u00b0, 30\u00b0] to the yaw axis. For some hard long-horizon\ntasks, in which the articulation is so small that it may be struggling to place something inside, we fail\nto generate successful demonstrations with normal randomization, so we reduce the scale of noise\nby half. For articulations, we randomly select an instance adapted from PartNet-Mobility and add\nnoise to its default joint position.\nWe test the success rates of generating demonstrations for 24 single-step and 15 long-horizon tasks,\nand report them task by task in Tab. 5. For each task, we run the demo collection process 5 times, and\neach time the kPAM solver is executed for 50 episodes with both spatial and object randomization.\nWe compute the mean and standard deviation of the 5 runs on the success rate."}, {"title": "B Multi-Task Training Details", "content": "B.1 Task Settings\nWe gradually add the number of training tasks from 4 to 24 in Section 5.1 and test the scaling ability\nof our framework on the original 4 tasks. We provide a detailed task list here to better clarify our\ntask settings:\n\u2022 4 tasks: OpenBox, CloseBox, OpenLaptop, CloseLaptop;\n\u2022 10 tasks: 4 tasks + OpenDrawer, PushDrawerClose, SwingBucketHandle, LiftBucketUpright,\nPressToasterLever, PushToasterForward;\n\u2022 15 tasks: 10 tasks + MoveBagForward, OpenSafe, CloseSafe, RotateMicrowaveDoor, CloseMi-\ncrowave;\n\u2022 20 tasks: 15 tasks + CloseSuitcaseLid, SwingSuitcaseLidOpen, RelocateSuitcase, TurnOnFaucet,\nTurnOffFaucet;\n\u2022 24 tasks: 20 tasks + SwingDoorOpen, ToggleDoorClose, CloseRefrigeratorDoor, OpenRefriger-\natorDoor.\nB.2 Architecture Implementation\nWe implemented a multi-modal policy architecture, as illustrated in Fig. 4, which includes a trans-\nformer policy stem, an action head, and different encoders for modeling various types of observa-\ntions as tokens.\nSpecifically, we take the CLIP [67] tokenizer to encode the task instruction, which is frozen during\ntraining; a pre-trained PointNext [68] to encode the point cloud, which is fine-tuned during training;\nan MLP for encoding the proprioception states, which is trained from scratch. The transformer\nconducts self-attention over all tokens, and then we post-process the tokens for different action\nheads. For example, we compute the mean pooling of all the tokens as the global condition of\nthe diffusion and MLP head; for the transformer, we compute cross-attention between the modeled\ntoken and a set of position embeddings to get the final action sequence. Regarding the PointNext\nencoder, we utilize the pre-trained model on ScanObjectNN Classification\u00b9, as it does not include\ncolor information, making it easier for downstream sim-to-real transfer. Note that the diffusion head\nand the transformer head both model and predict action sequences, but the MLP head only works\nfor single-step action.\nB.3 Policy Architecture Comparison against Baselines.\nWe first evaluate the Proprioception Pointcloud Transformer (PPT) in solving multiple tasks. We\ntrain one policy with demonstrations of 10 tasks on the commonly used manipulation benchmark,\nRLBench [32]. In particular, we train PPT for 250 epochs on data collected using the same setup as\nin Yan et al. [69]. Results shown in Tab. 6 performance best performances over 6/10 tasks improved\nperformance over recent competitive baseline methods such as PerAct [46] and GNFactor [70] with\na significant reduction in required camera views and pre-trained feature representations. Note that\nGNFactor [70] requires rather hassles to set up for real-world experiments, as it requires getting\nfeatures from the neural radiance field [71]. On the contrary, our proposed PPT architecture only\nrequires point cloud, languages, and robot sensor states, which are much easier to obtain with real-world robots.\nB.4 Additional Experiments for Object-Level Generalization\nAfter multi-task training, we test the generalization ability of our policy on unseen object instances.\nWe chose tasks using assets with more than 20 instances and 10 instances to respectively construct"}, {"title": "E.1 Prompt Templates", "content": "In this section, we demonstrate some examples of the prompt templates we have used to query LLM.\nThe prompt templates are shown as follows:\nPYou are an expert in creating robotic simulation environments and tasks. You are given some artic-ulated assets for example. Please come up with a creative use of the gripper to manipulate a singleArticulation. Note that the simulation engine does not support deformable objects or accurate colli-sion models for contacts. Moreover, the robot can only execute a simple trajectory of one motion.\nHere are all the assets. Please try to come up with tasks using only these assets.\nHere are some examples of good tasks. Try to learn from these structures but avoid overlappingwith them.\nHere are some bad example task instances with reasons. Try to avoid generating such tasks.reasons:\nPlease describe a NEW task in natural languages and explain its novelty and challenges.Note:Do not use assets that are not in the list above.Do not repeat the tasks similar to the good examples or the already generated tasks.The task needs to obey physics and remain feasible.Do not create similar tasks with the same \"assets-used\" set.All the assets are on the table when initialized.Do not place objects on small objects.Only one Articulation can be loaded.The task contains a simple trajectory of one motion.The task should have a clear goal, e.g. use \"open/close\" instead of \"adjust position\".Before the next step, please check if the generated task is a bad task shown in the above examplesand meets all the criteria as stated above. Specifically, if the task **only** contains a simple tra-jectory of one motion, and should have a **clear** goal. Explain in detail, and get a conclusion.If the task is a bad task, regenerate a new one.\nThen, format the answer in a Python dictionary with keys \"task-name\" and value type string, \u201ctask-description\" (one short phrase), and value type string with lower-case and separated by hyphens,\"assets-used\" and value type list of strings, and \"success-criteria\" (choose from \"articulated_open\",\"articulated_closed\", \"distance_articulated_rigidbody\", \"distance_gripper_rigidbody\", and \"dis-tance_gripper_articulated\") and value type list of strings.Try to be as creative as possible.\nPlease remember not to add any extra comments to the Python dictionary.Let's think step by step.Prompt: Task Decomposition\nYou are an expert in creating robotic simulation environments and tasks. A robot arm with a 2-finger gripper is used in all the robotic simulation environments and tasks. In each task, there isexactly one Articulation and one rigid body object that you can manipulate. You will be givenlong-horizon tasks with each task including at least 2 sub-tasks. Each sub-task can only includeone simple motion such as moving the gripper to some object, opening or closing the gripper fin-gers, or interacting with certain Articulations by its prismatic/revolute joints.\nPlease come up with a decomposition of the given long-horizon task to get several sub-tasks.Some rules of such decomposition are listed here:"}, {"title": "E.2 Example for Multi-modal LLM Response", "content": "We provide an example conversation when generating a solver config for the task \"close-drawer\"\nwith a multi-modal LLM, and compare it with the response generated by a vanilla LLM. Some of\nthe prompt details are omitted, and please refer to Appendix E for the complete prompts.\nExample for Multi-modal LLM Response\nYou are an expert in solving robotic tasks by coding task solution configs. Now please solve the newlygenerated task by generating the task solution config.\nHere is the task description.\n{\n\"task-name\": \"close-drawer\",\n\"task-description\": \"push a drawer to a closed position\",\n\"assets-used\": [\"drawer\"],\n\"success-criteria\": [\"articulated_closed\"]\n}\nThe initial scene of this task is visualized in the first uploaded image. The frame axes of the gripperand the object are respectively visulized in the second and third images, where red, green and blueaxes represent X, Y and Z axes respectively.\nNow please first generate the constraint part for task \"close-drawer\" in the same config format as theabove.\ntask_name: CloseDrawercategory_name: Articulatedtool_keypoint_name_list: [tool_head, tool_tail, tool_side]object_keypoint_name_list: [articulated_object_head]constraint_list:keypoint_name: tool_headtarget_keypoint_name: articulated_object_headtolerance: 0.0001type: point2point_constraintaxis_from_keypoint_name: tool_headaxis_to_keypoint_name: tool_sidetarget_axis: [1.0, 0, 0]target_axis_frame: objecttolerance: 0.01target_inner_product: 1type: frame_axis_orthogonalVisualization of Actuation PoseNow please generate 3 different pre/post-actuation motions for task \"close-drawer\" following thesame config format shown above based on the constraint part that was generated previously. Thepre/post-actuation motions of different solutions can be diverse, but their task names should be thesame.Output:task_name: CloseDrawercategory_name: Articulated# Pre-actuation motions for the manipulator.pre_actuation_motions:[\"translate_x\", -0.1][\"translate_z\", -0.15]# Post-actuation motions for the manipulator.post_actuation_motions:[\"translate_x\", 0.1]Success!"}, {"title": "Example for Vanilla LLM Response", "content": "You are an expert in solving robotic tasks by coding task solution configs. Now please solve the newlygenerated task by generating the task solution config.\nHere is the task description.\n{\n\"task-name\": \"close-drawer\",\n\"task-description\": \"push a drawer to a closed position\",\n\"assets-used\": [\"drawer\"],\n\"success-criteria\": [\"articulated_closed\"]\n}\nNow please first generate the constraint part for task \"close-drawer\" in the same config format asthe above.\nOutput:\ntask_name: CloseDrawercategory_name: Articulatedtool_keypoint_name_list: [tool_head, tool_tail, tool_side]object_keypoint_name_list: [articulated_object_head]constraint_list:keypoint_name: tool_headtarget_keypoint_name: articulated_object_headtolerance: 0.0001type: point2point_constraintaxis_from_keypoint_name: tool_sideaxis_to_keypoint_name: tool_tailtarget_axis: [0, 1.0, 0]target_axis_frame: worldtolerance: 0.01target_inner_product: 1type: frame_axis_parallelaxis_from_keypoint_name: tool_headaxis_to_keypoint_name: tool_tailtarget_axis: [0, 0, 1.0]target_axis_frame: worldtolerance: 0.01target_inner_product: 0type: frame_axis_orthogonalFail to generate correct constraints!Visualization of Actuation Pose"}, {"title": "E.3 Example for Reasoning LLM Response"}]}