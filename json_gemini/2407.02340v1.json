{"title": "RVISA: Reasoning and Verification for Implicit Sentiment Analysis", "authors": ["Wenna Lai", "Haoran Xie", "Guandong Xu", "Qing Li"], "abstract": "With an increasing social demand for fine-grained sentiment analysis (SA), implicit sentiment analysis (ISA) poses a significant challenge with the absence of salient cue words in expressions. It necessitates reliable reasoning to understand how the sentiment is aroused and thus determine implicit sentiments. In the era of Large Language Models (LLMs), Encoder-Decoder (ED) LLMs have gained popularity to serve as backbone models for SA applications, considering impressive text comprehension and reasoning ability among diverse tasks. On the other hand, Decoder-only (DO) LLMs exhibit superior natural language generation and in-context learning capabilities. However, their responses may contain misleading or inaccurate information. To identify implicit sentiment with reliable reasoning, this study proposes RVISA, a two-stage reasoning framework that harnesses the generation ability of DO LLMs and the reasoning ability of ED LLMs to train an enhanced reasoner. Specifically, we adopt three-hop reasoning prompting to explicitly furnish sentiment elements as cues. The generated rationales are utilized to fine-tune an ED LLM into a skilled reasoner. Additionally, we develop a straightforward yet effective verification mechanism to ensure the reliability of the reasoning learning. We evaluated the proposed method on two benchmark datasets and achieved state-of-the-art results in ISA performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Sentiment analysis (SA) aims to evoke opinions, sen- timents, and emotions through different computational methods [1]. Nowadays, people have demonstrated a stronger willingness to express and share their ideas online about day- to-day activities and global issues. With the increasing demand on social media, SA has gained significant interest considering great commercial value in exploring customer opinions or sentiments from user reviews or other sources of information. Meanwhile, sentiments can assist learning, communication, decision-making, and situation awareness in human-centric environments [2]. Traditionally, SA is classified into three levels, which are document-level, sentence-level, and aspect- level [3]. While the document and sentence level analyze the sentiment towards the overview of a document or sentence, aspect-based sentiment analysis (ABSA) is more fine-grained to extract the opinion towards a given aspect or entity. In many cases, there may be multiple aspects in one sentence, making it challenging to pinpoint a specific target and identify the corresponding sentiment. Considering context information, sentiment analysis can be further classified into implicit sentiment analysis (ISA) and explicit sentiment analysis (ESA), where expressions in ISA contain no explicit polarity markers but still deliver human- aware sentiment polarity [4]. In 2021, [5] split the SemEval- 2014 Restaurant and Laptop benchmarks into Explicit Senti- ment Expression slice and Implicit Sentiment Expression slice based on the presence of opinion words, drawing attention to ISA in ABSA tasks. [6] conducted pre-experiments on 20 existing sentiment classifiers and investigated that traditional methods performed ineffectively towards the same implicit case. They suggested that majority of traditional classifiers tend to overlook ISA problem and address ISA superficially. Although humans can easily grasp real intent and perceive changes in mood with common sense and reasoning ability, it is more difficult for models to tackle ISA than ESA, due to limited context information and insufficient reasoning skills. As recent great triumph of large language models (LLMs) has demonstrated impressive complex reasoning with chain-of- thought (CoT) prompting [7], [8] and in-context learning abil- ity [9], more scholars tend to embrace LLMs for downstream applications [10]\u2013[12]. [13] investigated the performance of LLMs in prompt-based inference and observed that for tasks requiring structured sentiment output, like ABSA tasks, both DO LLMs (e,g., GPT-3.5-turbo [14]) and ED LLMs (e.g., Flan-T5-XXL [15]) tend to lag behind ED backbone models (e.g., T5-Large [16]) trained with domain-specific data in automatic and human evaluations. The performance can vary significantly with different prompt designs. These indicate that deploying LLMs for ISA directly without training may not fully unleash their reasoning capacity for achieving satisfac- tory results. [6] first employed CoT fine-tuning on Flan-T5 for ISA and gained improved performance. However, intermediate steps generated by Flan-T5 were most likely to be untrust- worthy, with insufficient or duplicate content constrained by weak generation capacity. As illustrated in Figure 1, different LLMs performed diversely in analyzing implicit sentiment towards the aspect term 'price', given the text \u2018a cheaper price should not equal a \u201ccheap\u201d product'. Inferior models, like Flan-T5 in the group of Encoder-Decoder (ED) LLMs, displayed excellent comprehension and reasoning in solving tasks with diverse input information, but limited generation and prompt-based inference capabilities on open-text [17]. They were predisposed to inaccurately predict implicit senti- ment in the absence of explicit cues. Conversely, Decoder-only (DO) LLMs with more advanced generation ability, such as Vicuna-13B [18] and GPT-3.5-turbo, demonstrated enhanced proficiency in explicitly deducing sentiment elements pertinent to the context under reasoning prompts, while reliability in achieving accurate or correct responses was not guaranteed. Moreover, LLMs often showcase superior performance with emergent abilities when scaling up at a certain level [19], the direct deployment or fine-tuning of large-scale models (e.g., GPT-3.5-turbo) might be hindered by considerable computa- tional costs. To effectively discern implicit sentiment polarities towards a specified aspect, it is essential to exploit reliable reasoning methods for applicable backbone models. With this motivation, we attempt to equip ED backbone models with enhanced reasoning ability by explicitly learning from convincing rationales provided by DO LLMs through synchronous verification. Specifically, we follow the sentiment element construction and design corresponding three-hop rea- soning prompting to guide DO LLMs in explicitly inferring sentiment elements before determining the final sentiment. Then an ED model is served as the backbone model and fine-tuned based on the generated rationales and golden labels in datasets. To ensure the quality of reasoning learning, we further introduce an answer-based verification mechanism as an additional signal to assess the reliability of the rationale, which promotes dialectical learning to identify and rectify potential inaccuracies. In summary, the contributions of this work are as follows:"}, {"title": "II. RELATED WORK", "content": "In this work, we train a skilled reasoner with the cooperation of LLMs to conduct implicit sentiment analysis, learning fruitful information from rationales generated by reasoning prompting. We draw attention to the existing research on implicit sentiment analysis and methods that learn from rea- soning prompting making use of emergent abilities showcased in LLMs.\nImplicit sentiment analysis has gained considerable at- tention in the field of sentiment analysis [4], [20]. In the beginning, great efforts have been taken into solving the implicit sentiment detected in sentence level [21], [22]. With the increasing social demand, recent scholars attempted to de- velop effective paradigms tackling the unique characteristics of implicit sentiment analysis at a more fine-grained level towards the aspect target [5], [6], [23]. To capture the implicit sen- timent expression, some research exploited extra knowledge to further improve the learning performance. [5] pre-trained on large-scale sentiment annotated corpora with supervised contrastive learning objectives to align the representation of explicit and implicit sentiment expressions. Instead of making use of external knowledge, [24] generated explicit sentiment augmentation based on the language model itself to enhance implicit classification tendencies. Considering the difficulties of obtaining the full knowledge through additional means, [23] proposed reasoning learning under causal intervention to capture the correlation within the expressions. The relationship within fine-grained sentiment analysis can be summarized into four key sentiment elements involving target, aspect, opinion, and sentiment polarity, which are highly close to each other in understanding the underlying sentiment [25]. With the impressive performance of chain-of-thought (CoT) and in- context learning abilities showcased in LLMs, [6] introduced CoT fine-tuning to guide the ED backbone model inferring sentiment elements including implicit sentiment polarities step-by-step in an easy-to-hard manner. Similar to that, our approach makes use of fine-grained sentiment elements as cues for chain-of-thought prompting. But considering the limited generation capabilities of ED LLMs (e.g., Flan-T5 [15]), rather than inferring the sentiment elements from backbone models themselves, we train ED backbone models to become proficient reasoners by leveraging the informative rationale generated from DO LLMs (e.g., GPT-3.5-turbo [14]).\nLLMs have demonstrated impressive complex reasoning abilities with Chain-of-Thought (CoT) prompting [8], [26]. The use of reasoning prompting aims to guide the model in thinking step-by-step and leveraging most of the inference power for task solving. It is discovered effective in boosting the zero-shot or few-shot performance of LLMs [27]\u2013[30]."}, {"title": "III. TWO-STAGE REASONING FRAMEWORK", "content": "We propose a novel two-stage framework, RVISA (as shown in Figure 3), aiming to empower ED models with enhanced reasoning ability and incorporate the answer-based verification mechanism for reasoning refinement during model learning. In the initial stage, we leverage DO LLMs to generate insightful rationales and predict labels through our three-hop reasoning prompting approach. Then the verification signals are curated according to the correctness of LLM prediction labels. In the second stage, the generated rationales are employed for multi- task fine-tuning on an ED backbone model. To further ensure the reliability of the generated rationales, we implement a straightforward yet effective verification mechanism with an additional task supervised by the verification signals to guide self-revision during the reasoning learning process. Different tasks are distinguished by task-specific prefixes. It is thought that the model can learn to understand the underlying logic and relationships among sentiment elements that govern implicit sentiment prediction, by training on reasoning rationales and self-verification signals at the same time under the supervision of gold labels that have been annotated.\nIn sentiment analysis tasks including explicit sentiment analysis and implicit sentiment analysis, given the dataset $D = {(x_i, y_i)}_1^N$, where $1 \\leq i \\leq N$, $x_i$ represents an input sentence serving as a data example. Within each sentence $x_i$, an aspect term $t_i$ is identified, denoted as $t_i \\subset x_i$. The relevant sentiment elements consist of aspect $a_i$, opinion $o_i$, and sentiment polarity $y_i$. The objective of the task is to infer the sentiment polarity $y$ towards the aspect term $t_i$, given the in- put sentence $x_i$ and the specified aspect term $t_i$. In the standard prompting approach for direct fine-tuning, the LLM predicts the sentiment polarity $\\hat{y}_i$ solely via $\\hat{y}_i = argmax p(y_i | x_i, t_i)$. Without considering the intermediate sentiment elements, this approach potentially limits the ability of models to capture the sentiment nuances present in the text.\nTo improve the generation of informative rationales, we prompt DO LLMs to generate intermediate steps during the inference of implicit sentiments. To understand how the senti- ment is aroused, sentiment elements are essential in directing inference process since they contribute to constructing the complete picture of sentiment analysis. Therefore, we further design the three-hop prompting as illustrated in Figure 2, deviating from conventional prompting modes. The objective of this design is to dominate reasoning process by extracting closely associated sentiment elements. Simultaneously, this design is conducive to standardizing the generative structure, facilitating improved learning of patterns and interconnections among rationales. The details of the three-hop reasoning prompting are explained as follows."}, {"title": "IV. EXPERIMENTS", "content": "In the experiments, we evaluate the results on Restaurant and Laptop datasets in SemEval-2014 [38]. To test the perfor- mance for ISA, we follow the prior works utilizing datasets that further labeled with explicit and implicit tags [5]. \u03a4\u03bf generate effective rationales conducive to reasoning learning, we make use of DO LLMs, Vicuna-13B [18] and GPT-3.5- turbo [14] in stage 1 for rationale preparation. Considering the impressive performance of ED style models in understanding input information and comprehension among different tasks, Flan-T5 [15] serves as the backbone LLM during the multi- task fine-tuning stage. We test with different sizes of Flan- T5, scaling from the base model (250M) to the XXL model (13B). For the baseline methods, we compared with the re- cently reported best results, including seven baseline methods, which are BERT+SPC [39], BERT+ADA [40], BERT+RGAT [41], BERTAsp+CEPT [5], BERTAsp+SCAPT [5], THOR [6] and ABSA-ESA [24]. Among them, THOR [6] stimulates performance based on CoT prompting with three-step gen- eration. Compared to their method, we utilize a multi-task learning framework during training while directly inferring the final prediction during inference time. To identify the optimal hyperparameters in the training loss, a greedy search is undertaken using the validation set to determine the final values of $\\alpha$ and $\\gamma$. Without verification supervision, we get the best result when $\\alpha = 0.5$ with explanation and prediction tasks only. With the verification supervision, we get the greatest performance when $\\alpha = \\gamma = 0.3$. The following experiments will follow this hyperparameter setting."}, {"title": "V. DISCUSSION", "content": "We propose a two-stage reasoning framework, RVISA, to learn effectively and reliably from the rationales generated by DO LLMs for implicit sentiment analysis. We show that RVISA holds the potential to promote the reasoning and learn- ing ability of ED model under the supervision of verification through extensive experiments. In this section, we discuss the error scenario after fine-tuning using our proposed method and the limitations for further improvements.\nOur proposed method demonstrates superior performance in implicit sentiment analysis. To further explore the error scene, we calculate the error ratio considering sentiment types, including explicit and implicit, and with the relationship to the corresponding sentiment labels. The result is shown in Figure 5 with the rationales generated from GPT- 3.5-turbo. It can be observed that for the Laptop dataset, errors in neutral predictions within the explicit dataset surpass those in the implicit dataset, resulting in the F1 score performance in the implicit dataset exceeding that in all data. Conversely, in the Restaurant dataset, the error ratio associated with neutral predictions in the implicit dataset exceeds that of the original dataset. This observation underscores the significant influence of neutral sentiment distribution on error distribution patterns. Moreover, the ratio of incorrect predictions pertaining to neutral polarity exceeds 60%. This suggests the nuanced challenges associated with accurately discerning neutral sen- timents within sentiment analysis tasks, highlighting the need for further refinement and optimization in model training and inference processes.\nIn this study, we propose a straight- forward yet effective verification mechanism to enhance the overall performance in sentiment analysis. The answer-based verification plays a key role in the RVISA framework, demon- strating its significance in reasoning learning. It is worth noting that while the current answer-based verification sig- nal is effective, there is potential for further enhancement through the exploration of alternative verification modes or the incorporation of additional pertinent factors. This avenue for future research paves the way for more nuanced and reliable sentiment analysis. On the other hand, the three-hop prompt- ing proves instrumental in generating effective rationales by deducing sentiment elements. It is manually designed with the format drawing on prior works, which poses challenges in fur- ther optimization. Given the evolving landscape of advanced techniques focused on optimizing prompts for LLMs, it is unclear whether the prompt can be generated automatically or optimized through the utilization of soft prompts in this study. This raises a feasible direction for further exploration."}, {"title": "VI. CONCLUSIONS", "content": "In conclusion, this study sheds light on implicit sentiment analysis in the era of LLMs and proposes a novel two- stage learning framework, RVISA, designed to incorporate reasoning and verification for implicit sentiment analysis. By leveraging the generative prowess of DO LLMs, we empower ED backbone models with enhanced reasoning capabilities. The utilization of three-hop reasoning prompting facilitates the explicit generation of cues guided by sentiment ele- ment construction, which is conducive to reasoning learning. Through a straightforward and effective answer-based veri- fication mechanism, we ensure robust and reliable reasoning learning to further improve the proficiency of our ED backbone model in inferring implicit sentiment. The experimental results demonstrate superior performance and achieve state-of-the-art results in ISA on two benchmark datasets."}, {"title": "APPENDIX", "content": "To delve into the quality of rationales generated from Vicuna-13B and GPT- 3.5-turbo, the analysis for wrong and ambiguous prediction is conducted as illustrated in Figure 6. In both Restaurant and Laptop datasets, Vicuna-13B exhibited a slightly higher count of incorrect predictions compared to GPT-3.5-turbo. This suggests that stronger models such as GPT-3.5-turbo demonstrate a superior capability to generate higher-quality rationales, leading to more accurate final predictions. However, the percentage of ambiguous predictions originating from GPT-3.5-turbo surpassed that of Vicuna-13B, which indicates that the more powerful model exhibits a greater tendency to generate uncertain expressions rather than provide definitive judgments when deciphering the nuanced sentiment. It also underscores the inherent challenge of capturing subtle nuances in sentiment within constrained contextual information.\nTable IV shows the results when generating the rationale with GPT-3.5-turbo using diverse prompting. Under the reasoning prompt, the LLM tends to break down the problem into four fine-grained steps, leveraging its own pre-trained knowledge. However, even with more steps of inference, the prediction is the complete opposite of the gold label. On the other hand, with the guidance of three-hop prompting, the rationale follows the generation format as shown in the prompting template, which is more concise and structured to infer the sentiment elements leading to the final correct prediction."}]}