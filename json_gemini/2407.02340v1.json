{"title": "RVISA: Reasoning and Verification for Implicit Sentiment Analysis", "authors": ["Wenna Lai", "Haoran Xie", "Guandong Xu", "Qing Li"], "abstract": "With an increasing social demand for fine-grained sentiment analysis (SA), implicit sentiment analysis (ISA) poses a significant challenge with the absence of salient cue words in expressions. It necessitates reliable reasoning to understand how the sentiment is aroused and thus determine implicit sentiments. In the era of Large Language Models (LLMs), Encoder-Decoder (ED) LLMs have gained popularity to serve as backbone models for SA applications, considering impressive text comprehension and reasoning ability among diverse tasks. On the other hand, Decoder-only (DO) LLMs exhibit superior natural language generation and in-context learning capabilities. However, their responses may contain misleading or inaccurate information. To identify implicit sentiment with reliable reasoning, this study proposes RVISA, a two-stage reasoning framework that harnesses the generation ability of DO LLMs and the reasoning ability of ED LLMs to train an enhanced reasoner. Specifically, we adopt three-hop reasoning prompting to explicitly furnish sentiment elements as cues. The generated rationales are utilized to fine-tune an ED LLM into a skilled reasoner. Additionally, we develop a straightforward yet effective verification mechanism to ensure the reliability of the reasoning learning. We evaluated the proposed method on two benchmark datasets and achieved state-of-the-art results in ISA performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Sentiment analysis (SA) aims to evoke opinions, sentiments, and emotions through different computational methods [1]. Nowadays, people have demonstrated a stronger willingness to express and share their ideas online about day-to-day activities and global issues. With the increasing demand on social media, SA has gained significant interest considering great commercial value in exploring customer opinions or sentiments from user reviews or other sources of information. Meanwhile, sentiments can assist learning, communication, decision-making, and situation awareness in human-centric environments [2]. Traditionally, SA is classified into three levels, which are document-level, sentence-level, and aspect-level [3]. While the document and sentence level analyze the sentiment towards the overview of a document or sentence, aspect-based sentiment analysis (ABSA) is more fine-grained to extract the opinion towards a given aspect or entity. In many cases, there may be multiple aspects in one sentence, making it challenging to pinpoint a specific target and identify the corresponding sentiment.\nConsidering context information, sentiment analysis can be further classified into implicit sentiment analysis (ISA) and explicit sentiment analysis (ESA), where expressions in ISA contain no explicit polarity markers but still deliver human-aware sentiment polarity [4]. In 2021, [5] split the SemEval-2014 Restaurant and Laptop benchmarks into Explicit Sentiment Expression slice and Implicit Sentiment Expression slice based on the presence of opinion words, drawing attention to ISA in ABSA tasks. [6] conducted pre-experiments on 20 existing sentiment classifiers and investigated that traditional methods performed ineffectively towards the same implicit case. They suggested that majority of traditional classifiers tend to overlook ISA problem and address ISA superficially. Although humans can easily grasp real intent and perceive changes in mood with common sense and reasoning ability, it is more difficult for models to tackle ISA than ESA, due to limited context information and insufficient reasoning skills.\nAs recent great triumph of large language models (LLMs) has demonstrated impressive complex reasoning with chain-of-thought (CoT) prompting [7], [8] and in-context learning ability [9], more scholars tend to embrace LLMs for downstream applications [10]\u2013[12]. [13] investigated the performance of LLMs in prompt-based inference and observed that for tasks requiring structured sentiment output, like ABSA tasks, both DO LLMs (e,g., GPT-3.5-turbo [14]) and ED LLMs (e.g., Flan-T5-XXL [15]) tend to lag behind ED backbone models (e.g., T5-Large [16]) trained with domain-specific data in automatic and human evaluations. The performance can vary significantly with different prompt designs. These indicate that deploying LLMs for ISA directly without training may not fully unleash their reasoning capacity for achieving satisfactory results. [6] first employed CoT fine-tuning on Flan-T5 for ISA and gained improved performance. However, intermediate steps generated by Flan-T5 were most likely to be untrustworthy, with insufficient or duplicate content constrained by weak generation capacity. As illustrated in Figure 1, different LLMs performed diversely in analyzing implicit sentiment towards the aspect term 'price', given the text \u2018a cheaper price should not equal a \u201ccheap\u201d product'. Inferior models, like Flan-T5 in the group of Encoder-Decoder (ED) LLMs, displayed excellent comprehension and reasoning in solving tasks with diverse input information, but limited generation and prompt-based inference capabilities on open-text [17]. They were predisposed to inaccurately predict implicit sentiment in the absence of explicit cues. Conversely, Decoder-only (DO) LLMs with more advanced generation ability, such as Vicuna-13B [18] and GPT-3.5-turbo, demonstrated enhanced proficiency in explicitly deducing sentiment elements pertinent to the context under reasoning prompts, while reliability in achieving accurate or correct responses was not guaranteed. Moreover, LLMs often showcase superior performance with emergent abilities when scaling up at a certain level [19], the direct deployment or fine-tuning of large-scale models (e.g., GPT-3.5-turbo) might be hindered by considerable computational costs. To effectively discern implicit sentiment polarities towards a specified aspect, it is essential to exploit reliable reasoning methods for applicable backbone models.\nWith this motivation, we attempt to equip ED backbone models with enhanced reasoning ability by explicitly learning from convincing rationales provided by DO LLMs through synchronous verification. Specifically, we follow the sentiment element construction and design corresponding three-hop reasoning prompting to guide DO LLMs in explicitly inferring sentiment elements before determining the final sentiment. Then an ED model is served as the backbone model and fine-tuned based on the generated rationales and golden labels in datasets. To ensure the quality of reasoning learning, we further introduce an answer-based verification mechanism as an additional signal to assess the reliability of the rationale, which promotes dialectical learning to identify and rectify potential inaccuracies.\nIn summary, the contributions of this work are as follows:\n\u2022 We propose a novel two-stage learning framework, Reasoning and Verification for Implicit Sentiment Analysis (RVISA), marking the endeavor to improve the proficiency of ED backbone models as adept reasoners in ISA, complemented by the generative strengths of DO LLMs.\n\u2022 We introduce a straightforward yet efficacious verification mechanism to provide reliable supervision for reasoning learning and improve overall performance.\n\u2022 The evaluation outcomes on two benchmark datasets underscore the efficacy of our method in achieving state-of-the-art results in ISA performance."}, {"title": "II. RELATED WORK", "content": "In this work, we train a skilled reasoner with the cooperation of LLMs to conduct implicit sentiment analysis, learning fruitful information from rationales generated by reasoning prompting. We draw attention to the existing research on implicit sentiment analysis and methods that learn from reasoning prompting making use of emergent abilities showcased in LLMs.\nA. Implicit Sentiment Analysis\nImplicit sentiment analysis has gained considerable attention in the field of sentiment analysis [4], [20]. In the beginning, great efforts have been taken into solving the implicit sentiment detected in sentence level [21], [22]. With the increasing social demand, recent scholars attempted to develop effective paradigms tackling the unique characteristics of implicit sentiment analysis at a more fine-grained level towards the aspect target [5], [6], [23]. To capture the implicit sentiment expression, some research exploited extra knowledge to further improve the learning performance. [5] pre-trained on large-scale sentiment annotated corpora with supervised contrastive learning objectives to align the representation of explicit and implicit sentiment expressions. Instead of making use of external knowledge, [24] generated explicit sentiment augmentation based on the language model itself to enhance implicit classification tendencies. Considering the difficulties of obtaining the full knowledge through additional means, [23] proposed reasoning learning under causal intervention to capture the correlation within the expressions. The relationship within fine-grained sentiment analysis can be summarized into four key sentiment elements involving target, aspect, opinion, and sentiment polarity, which are highly close to each other in understanding the underlying sentiment [25]. With the impressive performance of chain-of-thought (CoT) and in-context learning abilities showcased in LLMs, [6] introduced CoT fine-tuning to guide the ED backbone model inferring sentiment elements including implicit sentiment polarities step-by-step in an easy-to-hard manner. Similar to that, our approach makes use of fine-grained sentiment elements as cues for chain-of-thought prompting. But considering the limited generation capabilities of ED LLMs (e.g., Flan-T5 [15]), rather than inferring the sentiment elements from backbone models themselves, we train ED backbone models to become proficient reasoners by leveraging the informative rationale generated from DO LLMs (e.g., GPT-3.5-turbo [14]).\nB. Reasoning Prompting\nLLMs have demonstrated impressive complex reasoning abilities with Chain-of-Thought (CoT) prompting [8], [26]. The use of reasoning prompting aims to guide the model in thinking step-by-step and leveraging most of the inference power for task solving. It is discovered effective in boosting the zero-shot or few-shot performance of LLMs [27]\u2013[30]."}, {"title": "C. Learning from Rationale", "content": "Learning from explanations and empowering the training model with reasoning abilities have been explored in various fields [10]\u2013[12], [34]. LLMs are capable of validating their responses with reasonable intermediate steps [7], [8], rationales can be used as demonstrations [27] or extra fine-tuning data [10], [35], [36] to improve the learning performance. Considering the training cost for LLMs, rationales can also serve as valuable supervised signals for smaller task-specific models, which can be more easily deployed [11], [12], [34], [37]. However, [12] directly kept the answer generated by LLMs as supervision signals, which neglected the potential erroneous occurrence. [34] reorganized the rationale set based on answer-based filtering to mitigate the possibility of error learning, but potential misleading information may still exist even with the guidance of a correct answer. In contrast, our approach augments the overall performance by incorporating an answer-based verification mechanism as an additional layer of supervision for multi-task learning. This innovative strategy not only preserves valuable insights contained within the rationales but also leverages them to refine the learning process with both positive and negative signals.\nWe compare various prompting methodologies and substantiate the superior performance of our three-hop prompting in the nuanced domain of implicit sentiment analysis through comprehensive experiments. Beyond that, the introduction of the verification mechanism further improves the performance. The integration of three-hop prompting with the verification mechanism effectively navigates complexities inherent in LLM reasoning process."}, {"title": "III. TWO-STAGE REASONING FRAMEWORK", "content": "We propose a novel two-stage framework, RVISA (as shown in Figure 3), aiming to empower ED models with enhanced reasoning ability and incorporate the answer-based verification mechanism for reasoning refinement during model learning. In the initial stage, we leverage DO LLMs to generate insightful rationales and predict labels through our three-hop reasoning prompting approach. Then the verification signals are curated according to the correctness of LLM prediction labels. In the second stage, the generated rationales are employed for multi-task fine-tuning on an ED backbone model. To further ensure the reliability of the generated rationales, we implement a straightforward yet effective verification mechanism with an additional task supervised by the verification signals to guide self-revision during the reasoning learning process. Different tasks are distinguished by task-specific prefixes. It is thought that the model can learn to understand the underlying logic and relationships among sentiment elements that govern implicit sentiment prediction, by training on reasoning rationales and self-verification signals at the same time under the supervision of gold labels that have been annotated.\nA. Problem Definition\nIn sentiment analysis tasks including explicit sentiment analysis and implicit sentiment analysis, given the dataset D = {(x\u1d62, y\u1d62)}\u1d3a, where 1 \u2264 i \u2264 N, x\u1d62 represents an input sentence serving as a data example. Within each sentence x\u1d62, an aspect term t\u1d62 is identified, denoted as t\u1d62 \u2282 x\u1d62. The relevant sentiment elements consist of aspect a\u1d62, opinion o\u1d62, and sentiment polarity y\u1d62. The objective of the task is to infer the sentiment polarity \u0177\u1d62 towards the aspect term t\u1d62, given the input sentence x\u1d62 and the specified aspect term t\u1d62. In the standard prompting approach for direct fine-tuning, the LLM predicts the sentiment polarity \u0177\u1d62 solely via \u0177\u1d62 = argmax p(y\u1d62|x\u1d62, t\u1d62). Without considering the intermediate sentiment elements, this approach potentially limits the ability of models to capture the sentiment nuances present in the text.\nB. Three-hop Reasoning Generation\nTo improve the generation of informative rationales, we prompt DO LLMs to generate intermediate steps during the inference of implicit sentiments. To understand how the sentiment is aroused, sentiment elements are essential in directing inference process since they contribute to constructing the complete picture of sentiment analysis. Therefore, we further design the three-hop prompting as illustrated in Figure 2, deviating from conventional prompting modes. The objective of this design is to dominate reasoning process by extracting closely associated sentiment elements. Simultaneously, this design is conducive to standardizing the generative structure, facilitating improved learning of patterns and interconnections among rationales. The details of the three-hop reasoning prompting are explained as follows.\na) Three-hop Reasoning (TH-RE): Fine-grained sentiment analysis involves dissecting key sentiment elements involving the target, aspect, opinion, and sentiment polarity [25]. Various approaches exist for solving these individual subtasks or their combinations, collectively contributing to a comprehensive sentiment analysis picture. To address the complexity of the task holistically, it is essential to consider the components systematically and tackle them incrementally. [6] first design the prompting based on the CoT strategy by explicitly inferring the sentiment elements and then employ the prompting for three-step generation during model fine-tuning. However, the prompting for each step is inferred separately for a single sentiment element at a time, with the results concatenated as context information for the subsequent step.\nIn our design, we adopt a structured approach by explicitly presenting sentiment elements in a natural language sequence to construct a three-hop reasoning prompt. This approach underscores the causal relationships among sentiment elements and the final sentiment polarity prediction in a single iteration.\nAs shown in the template below, we incorporate sentiment elements as cues at the end of \"let's think step-by-step\", guiding the language model to generate reasoning steps in alignment with the sentiment elements' understanding and finally infer the sentiment polarity. We expect DO LLMs to predict the explanation via \u00ea\u1d62 = argmax p(e\u1d62|x\u1d62, t\u1d62), where a\u1d62, o\u1d62, y\u1d62 \u2282 e\u1d62.\nb) Three-hop Rationalization (TH-RE): Diverse from TH-RE, we integrate three-hop reasoning with rationalization to establish the Three-hop rationalization (TH-RA) prompting. Specifically, the gold label will be given as the reference, which prompts the LLMs to elucidate the annotated sentiment label through a systematic and step-by-step inference process guided by sentiment elements. We expect DO LLMs to predict the explanation via \u00ea\u1d62 = argmax p(e\u1d62|y\u1d62, x\u1d62, t\u1d62), where a\u1d62, o\u1d62, y\u1d62 \u2282 \u00ea\u1d62.\nC. Multi-task Fine-tuning\nWe employ a multi-task fine-tuning approach to simultaneously learn the rationales generated by the LLM and the annotated labels. Given the dataset D = {(x\u1d62, y\u1d62)}\u1d3a, where 1 \u2264 i \u2264 N, we generate an explanation e\u1d62 to serve as a rationale for each input x\u1d62 as detailed in Section III-B. Each explanation e\u1d62 encompasses a generated label \u0177 from the LLM, denoted as \u0177\u1d62 \u2282 e\u1d62. Subsequently, we construct a new dataset D\u2091\u2093\u209a = {(x\u1d62, e\u1d62)}\u1d3a, where 1 \u2264 i \u2264 N. The objective during the training phase is to effectively utilize the generated content and learn from two distinct tasks: the explanation task utilizing data from D\u2091\u2093\u209a and the prediction task utilizing the data from the original dataset D\u209a\u1d63\u2091 = D = {(x\u1d62, y\u1d62)}\u1d3a where 1 < i < N. To further enhance reasoning performance, we introduce the reasoning verification mechanism within the existing multi-task learning framework. This mechanism enhances the learning process by providing verification signals for additional-task learning. The details will be elaborated in the subsequent sections.\n1) Learning with Rationale: To train the proficient reason-ers, we employ the multi-task learning framework and divide the learning task into explanation and prediction, where explanation tends to furnish the rationale based on the input sample and the task objective, while the prediction task focuses solely on inferring sentiment polarity. Through the implementation of multi-task learning, the training phase incorporates the losses associated with both explanation and prediction tasks. The loss function is delineated as follows, where \\(L_{exp}\\) is the loss for explanation task and \\(L_{pre}\\) is the loss for prediction task:\n\\[L_{loss} = \\alpha L_{exp} + (1 - \\alpha) L_{pre}\\]\nwhere the prediction \\(L_{pred}\\) aims to minimize the cross-entropy loss for label prediction:\n\\[L_{pre} = \\frac{1}{N} \\sum_{i=1}^{N} l_{CE}(y_i, \\hat{y}_i)\\]\nwhile the explanation loss \\(L_{exp}\\) tends to minimize the generation loss for the rationale, and there exists a subtle distinction between reasoning (RE) and rationalization (RA) scenarios.\n\\[RE: L_{exp} = \\frac{1}{N} \\sum_{i=1}^{N} l_{CE}(f(x_i, t_i), \\hat{e}_i)\\]\n\\[RA: L_{exp} = \\frac{1}{N} \\sum_{i=1}^{N} l_{CE}(f(x_i, t_i, y_i), \\hat{e}_i)\\]\nThe objective is to equip the model with proficiency in both explanation and prediction, thereby enhancing its reasoning capabilities. However, during the inference phase, only the prediction task is required for evaluation to optimize the inference efficiency and mitigate computational costs.\n2) Reasoning with Verification: Considering the rationale generated by LLM is directly employed without any post-filtering processes, it might introduce some error patterns that can negatively influence the performance of multi-task fine-tuning. Some research works perform answer-based filtering to improve the rationale quality. [34] directly removed the incorrect rationale given by reasoning prompting based on the final prediction and supplemented it with the rationale generated under rationalization prompting to complete the final rationale set for training. [10] demonstrated that answer-based filtering can also be compensated by a diversity of reasoning paths using diverse reasoning and retaining the rationales leading to the correct answer. In our approach, we preserve the sets of rationales generated by the LLM by introducing a verification signal to facilitate further analysis of rationale quality within the multi-task learning framework. This is achieved by incorporating an additional task for verification. Specifically, we leverage the rationale set generated by our TH-RE prompting and adopt answer-based verification according to the prediction label \u0177\u1d62 provided by the LLM and the ground truth annotation y\u1d62. Rationales that lead to the correct answer label are deemed to possess higher quality and utility compared to those inferring an incorrect answer label. Based on this premise, we complete the prompting using the following template:"}, {"title": "IV. EXPERIMENTS", "content": "In the experiments, we evaluate the results on Restaurant and Laptop datasets in SemEval-2014 [38]. To test the performance for ISA, we follow the prior works utilizing datasets that further labeled with explicit and implicit tags [5]. To generate effective rationales conducive to reasoning learning, we make use of DO LLMs, Vicuna-13B [18] and GPT-3.5-turbo [14] in stage 1 for rationale preparation. Considering the impressive performance of ED style models in understanding input information and comprehension among different tasks, Flan-T5 [15] serves as the backbone LLM during the multi-task fine-tuning stage. We test with different sizes of Flan-T5, scaling from the base model (250M) to the XXL model (13B). For the baseline methods, we compared with the recently reported best results, including seven baseline methods, which are BERT+SPC [39], BERT+ADA [40], BERT+RGAT [41], BERTAsp+CEPT [5], BERTAsp+SCAPT [5], THOR [6] and ABSA-ESA [24]. Among them, THOR [6] stimulates performance based on CoT prompting with three-step generation. Compared to their method, we utilize a multi-task learning framework during training while directly inferring the final prediction during inference time. To identify the optimal hyperparameters in the training loss, a greedy search is undertaken using the validation set to determine the final values of \u03b1 and \u03b3. Without verification supervision, we get the best result when \u03b1 = 0.5 with explanation and prediction tasks only. With the verification supervision, we get the greatest performance when \u03b1 = \u03b3 = 0.3. The following experiments will follow this hyperparameter setting.\nB. Main Results\na) Multi-task learning outperforms the baselines: The main results of baselines and our method, RVISA, are demonstrated in Table I. The evaluation metrics include Accuracy and Macro-F1 score. Notably, as THOR [6] does not provide the accuracy outcome for implicit sentiment, we rerun the results based on the provided source code. It can be seen that RVISA significantly outperforms the baseline methods, irrespective of whether learning is from Vicuna-13B or GPT-3.5-turbo, underscoring the efficacy of learning within the proposed multi-task learning framework.\nb) Strong teachers lead to higher quality learning: The performance of RVISA, training under the assistance of GPT-3.5-turbo exhibits enhanced reasoning capabilities in implicit sentiment inference compared to RVISA trained by using the rationales generated by Vicuna-13B. This disparity can be attributed to the superior common sense knowledge and reasoning prowess exhibited by GPT-3.5-turbo in producing high-quality rationales, which play a pivotal role in transferring reasoning abilities to the Flan-T5 backbone model. However, the smaller backbone model with a base size (250M) lags behind some of the baseline methods due to its limited generation capacity to derive advantage from rich knowledge through in-context learning.\nc) Explicit rationales learning helps implicit reasoning: We further compare our method with THOR, which is built upon a chain-of-thought strategy. Instead of eliciting the reasoning ability of the language model through sequential three-step prompting, our method stands out by explicitly giving the rationales as informative resources equipped with a verification mechanism to ensure the learning quality. The comparative results are depicted in Table II. RVISA demonstrates superior performance over THOR in terms of F1 score for implicit sentiment analysis while maintaining competitive results in overall F1 score. This underscores the effectiveness of our method in learning implicit sentiment through reasoning tasks and adeptly capturing implicit relationships among instances. Although THOR claimed the three-step generation during fine-tuning can unleash the reasoning power of the backbone model, the re-run result demonstrated limited improvement in F1 scores. This suggests the vulnerability of THOR to enhance prompt-based inference depending on the backbone model (i.e., Flan-T5 [15]) itself. In contrast, our method prioritizes effective learning from high-quality sentiment information and closely related tasks, offering a more coherent and justifiable approach to achieving high performance in implicit sentiment analysis.\nC. Ablation Study\nWe conducted an ablation study on the three-hop prompting (TH) and verification mechanism (VE) components, the results of which are summarized in Table III. Our analysis compares the F1 scores in both overall and implicit sentiment scenarios. The findings indicate that the absence of the verification mechanism leads to performance degradation in both cases, with a more significant decline of over one point observed in the implicit sentiment results. This highlights the critical role of verification signals in the context of reasoning learning from LLMs, as the answer-based mechanism aids the backbone model in identifying potential errors or unreasonable attributes during multi-task learning processes."}, {"title": "V. DISCUSSION", "content": "We propose a two-stage reasoning framework, RVISA, to learn effectively and reliably from the rationales generated by DO LLMs for implicit sentiment analysis. We show that RVISA holds the potential to promote the reasoning and learning ability of ED model under the supervision of verification through extensive experiments. In this section, we discuss the error scenario after fine-tuning using our proposed method and the limitations for further improvements.\na) Error Analysis: Our proposed method demonstrates superior performance in implicit sentiment analysis. To further explore the error scene, we calculate the error ratio considering sentiment types, including explicit and implicit, and with the relationship to the corresponding sentiment labels. The result is shown in Figure 5 with the rationales generated from GPT-3.5-turbo. It can be observed that for the Laptop dataset, errors in neutral predictions within the explicit dataset surpass those in the implicit dataset, resulting in the F1 score performance in the implicit dataset exceeding that in all data. Conversely, in the Restaurant dataset, the error ratio associated with neutral predictions in the implicit dataset exceeds that of the original dataset. This observation underscores the significant influence of neutral sentiment distribution on error distribution patterns. Moreover, the ratio of incorrect predictions pertaining to neutral polarity exceeds 60%. This suggests the nuanced challenges associated with accurately discerning neutral sentiments within sentiment analysis tasks, highlighting the need for further refinement and optimization in model training and inference processes.\nb) Limitations: In this study, we propose a straightforward yet effective verification mechanism to enhance the overall performance in sentiment analysis. The answer-based verification plays a key role in the RVISA framework, demonstrating its significance in reasoning learning. It is worth noting that while the current answer-based verification signal is effective, there is potential for further enhancement through the exploration of alternative verification modes or the incorporation of additional pertinent factors. This avenue for future research paves the way for more nuanced and reliable sentiment analysis. On the other hand, the three-hop prompting proves instrumental in generating effective rationales by deducing sentiment elements. It is manually designed with the format drawing on prior works, which poses challenges in further optimization. Given the evolving landscape of advanced techniques focused on optimizing prompts for LLMs, it is unclear whether the prompt can be generated automatically or optimized through the utilization of soft prompts in this study. This raises a feasible direction for further exploration."}, {"title": "VI. CONCLUSIONS", "content": "In conclusion, this study sheds light on implicit sentiment analysis in the era of LLMs and proposes a novel two-stage learning framework, RVISA, designed to incorporate reasoning and verification for implicit sentiment analysis. By leveraging the generative prowess of DO LLMs, we empower ED backbone models with enhanced reasoning capabilities. The utilization of three-hop reasoning prompting facilitates the explicit generation of cues guided by sentiment element construction, which is conducive to reasoning learning. Through a straightforward and effective answer-based verification mechanism, we ensure robust and reliable reasoning learning to further improve the proficiency of our ED backbone model in inferring implicit sentiment. The experimental results demonstrate superior performance and achieve state-of-the-art results in ISA on two benchmark datasets."}]}