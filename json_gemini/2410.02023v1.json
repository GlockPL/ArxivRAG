{"title": "DeepProtein: Deep Learning Library and Benchmark for Protein Sequence Learning", "authors": ["Jiaqing Xie", "Yue Zhao", "Tianfan Fu"], "abstract": "In recent years, deep learning has revolutionized the field of protein science, enabling advancements in predicting protein properties, structural folding and interactions. This paper presents DeepProtein, a comprehensive and user-friendly deep learning library specifically designed for protein-related tasks. DeepProtein integrates a couple of state-of-the-art neural network architectures, which include convolutional neural network (CNN), recurrent neural network (RNN), transformer, graph neural network (GNN), and graph transformer (GT). It provides user-friendly interfaces, facilitating domain researchers in applying deep learning techniques to protein data. Also, we curate a benchmark that evaluates these neural architectures on a variety of protein tasks, including protein function prediction, protein localization prediction, and protein-protein interaction prediction, showcasing its superior performance and scalability. Additionally, we provide detailed documentation and tutorials to promote accessibility and encourage reproducible research. This library is extended from a well-known drug discovery library, DeepPurpose and publicly available at https://github.com/jiaqingxie/DeepProtein/tree/main.", "sections": [{"title": "1 Introduction", "content": "Understanding the representation of proteomics is vital in developing traditional biological and medical progress [Wu et al., 2022b, Fu et al., 2024], multi-omics genomics [Wu et al., 2022a, Chen et al., 2021], and curing human diseases [Chen et al., 2024c,b]. Being the working house of the cell, it provides lots of functions that support human's daily life, such as catalyzing biochemical reactions that happen in the body as a role of enzymes and providing helpful immune responses against detrimental substance that acts as immunoglobulin [Wu et al., 2024]. Under the necessity of analyzing those useful proteins, several related protein databases are available to researchers [Berman et al., 2000, Bairoch and Apweiler, 2000, Consortium, 2015, Pont\u00e9n et al., 2008]. Apart from the 2D database, some recent 3D Protein Database used AlphaFold 2.0 [Jumper et al., 2021] is important to better assist in learning those representations in 3d-dimensional space. The success of AlphaFold 2.0 has sparked a significant increase in interest in using machine learning techniques for protein learning tasks, of which the goal is to improve our understanding of proteins' biochemical mechanisms.\nDeep learning has shown its power in protein learning tasks, including protein-protein interaction [Gainza et al., 2020], protein folding [Jumper et al., 2021, Lu, 2022, Panou and Reczko, 2020, Chen et al., 2016], protein-ligand interaction [Li et al., 2021b, Xia et al., 2023], and protein function and property prediction [Gligorijevi\u0107 et al., 2021, Sevgen et al., 2023]. Convolutional neural network (CNN) [Shanehsazzadeh et al., 2020] and TAPE Transformer [Rao et al., 2019] on proteins are models with regard to protein sequence-to-sequence (Seq2Seq) learning, including the pretrained transformer"}, {"title": "2 Related Works", "content": "Benchmarks and libraries are crucial in AI-based therapeutic science, e.g., multi-omics data [Lu, 2018], protein learning [Xu et al., 2022], small-molecule drug discovery [Gao et al., 2022, Zheng et al., 2024, Xu et al., 2024], and drug development (clinical trial) [Chen et al., 2024a, Wang et al., 2024]. They provide standardized metrics for evaluating the performance of various algorithms and models. These benchmarks enable researchers to compare different approaches systematically, ensuring reproducibility and reliability of results.\nIn this section, we briefly discuss the benchmark studies in this area. Proteins are vital in drug discovery because they often serve as the primary targets for therapeutic agents, influencing disease mechanisms and biological pathways. Additionally, proteins play key roles in various cellular processes, making them essential for identifying potential drug candidates and biomarkers in the drug development pipeline. A couple of protein learning benchmarks are developed, including PEER [Xu"}, {"title": "3 DeepProtein Library and Benchmark", "content": "In this section, we elaborate on a couple of AI-solvable protein problems and the related datasets."}, {"title": "3.1 AI-solvable Protein Problems", "content": "Protein Function Prediction. Protein function prediction involves determining the biological roles and activities of proteins based on their sequences or structures. This process is crucial for understanding cellular mechanisms and interactions, as a protein's function is often linked to its sequence composition and the context of its cellular environment. Machine learning algorithms are employed to analyze known protein databases, identifying patterns and features that correlate with specific functions. Accurate predictions can facilitate drug discovery, help elucidate disease mechanisms, and support advancements in synthetic biology by providing insights into how proteins can be engineered for desired activities [Zhang et al., 2021]. We consider the following datasets."}, {"title": "3.2 Cutting-edge Deep Learning Methods", "content": "At the core of deep learning lies the artificial neural network, a machine learning technique inspired by the architecture and functionality of the human brain. What distinguishes deep learning from other machine learning approaches is its exceptional capacity to recognize and analyze complex, nonlinear patterns in data, leading to enhanced performance and accuracy. Concretely, we incorporate several cutting-edge neural network architectures into two groups: 1) sequential-based learning and 2) structural-based learning. Detailed model architectures are described as follows:\nSequential based learning It generally takes a sequence as an input, uses one-hot encoding to pre-encode the input characters. Such learning methods include convolutional neural network, recurrent neural network and transformers."}, {"title": "3.2.1 Convolutional Neural Network (CNN) (One-dimensional)", "content": "captures the local patterns in the data features, commonly used to analyze images and text. (One-dimensional) Convolutional neural network (CNN) takes amino acid sequences as the input. CNN has four layers; the number of filters for the four layers is 32, 64, and 96 respectively. The kernel sizes are 4, 8, and 12, respectively. The convolutional layer is followed by a one-layer MLP (multi-layer perceptron) to produce the prediction, which is a scalar."}, {"title": "3.2.2 Recurrent Neural Network (RNN)", "content": "models sequence data and captures the long-term dependencies in the sequence data. RNN has two well-known variants: long short-term memory networks (LSTMs) [Hochreiter and Schmidhuber, 1996] and gated recurrent units (GRU) [Cho et al., 2014]. The difference between GRU and LSTM is that GRU simplifies LSTM by removing the cell state and reducing the number of gates. We use a two-layer bi-directional GRU following three-layer CNN as the neural network architecture. The dimension of the hidden state in GRU is set to 64. ReLU function is applied after each GRU or CNN layer."}, {"title": "3.2.3 Transformer", "content": "[Vaswani et al., 2017] architecture leverages the power of self-attention mechanisms and parallel computation to enhance the neural network's capability and efficiency in handling sequence data. We use the transformer encoder to represent the amino acid sequence. Two layers of transformer architectures are stacked. The dimension of embedding in the transformer is set to 64. The number of attention heads is set to 4. The ReLU function is applied after each self-attention layer. LayerNorm is applied after MLP layers."}, {"title": "Structural-based learning", "content": "It generally transforms the input sequence into a valid SMILES string, then transforms the chemical substance into a graph. Then, graph filters are learned toward the input graph signal. Such learning methods are widely called Graph Neural Networks. Recently, graph transformers have shown their power in protein function prediction, and we included them as a part of structural-based learning."}, {"title": "3.2.4 Graph Neural Network (GNN)", "content": "is a neural network architecture designed to process graph-structured data that takes input from nodes and edges, facilitating the flow of information between connected components to capture their interactions. It learns vector representations for both individual graph nodes and the overall graph structure. We consider the following GNN variants:"}, {"title": "3.2.4.1 Graph Convolutional Network (GCN)", "content": "[Kipf and Welling, 2016]. GCN is a GNN variant that iteratively updates the node representation by aggregating the information from its neighbors. GCN has three layers, and the node embedding dimension is set to 64. After GCN, all the node embeddings are aggregated with a readout function (Weighted Sum and Max) to get graph-level embedding, followed by a one-layer MLP to get the final prediction. BatchNorm is applied after MLP layers."}, {"title": "3.2.4.2 Graph Attention Network (GAT)", "content": "[Velickovic et al., 2018]. GAT employs an attention mechanism to introduce anisotropy into the neighborhood aggregation function. This network features a multi-headed architecture that enhances its learning capacity. The node embedding dimension is 64. Readout function is the same as the one deployed in GCN model."}, {"title": "3.2.4.3 Message Passing Neural Network (MPNN)", "content": "[Gilmer et al., 2017]. MPNN is a GNN variant that considers passing messages (and modeling interactions) between both edges and nodes based on their neighbors. Edge features are included necessarily compared with GCN and GAT. Readout function is Sum And Max. Node and edge embedding dimension is 64."}, {"title": "3.2.4.4 Neural Fingerprint (NeuralFP)", "content": "[Duvenaud et al., 2015]. NeuralFP uses Graph convolutional network (GCN) [Kipf and Welling, 2016] to learn a neural network-based molecular embedding (also known as molecular neural fingerprint, or NeuralFP) from a large amount of molecule data without labels. The neural fingerprint is essentially a real-valued vector, also known as embedding. Then, the neural fingerprint is fixed and fed into a three-layer MLP to make the prediction. Node embedding dimension is 64. BatchNorm is applied after MLP layers."}, {"title": "3.2.4.5 Attentive Fingerprint (AttentiveFP)", "content": "[Xiong et al., 2019]. AttentiveFP is a variant of graph neural networks that is enhanced by the attention mechanism when evaluating node and edge embedding. The model consists of three AttentiveFP layers with individual readout function: AttentiveFP readout. Node and edge embedding dimension is 64."}, {"title": "3.2.5 Graph Transformer", "content": "[Yun et al., 2019] is a type of neural network architecture designed to process graph-structured data by leveraging self-attention mechanisms. They extend the principles of traditional transformers, enabling them to capture the relationships and interactions between nodes in a graph effectively."}, {"title": "3.2.6 Path-Augmented Graph Transformer (PAGTN)", "content": "[Chen et al., 2019]. It used augmented path features to capture long-range (>1 hop) graph properties. The model consists of 5 PAGTN layers with LeakyReLU activation. Node embedding dimension is 64."}, {"title": "3.2.7 Graphormer", "content": "[Ying et al., 2021]. It utilized transformer on graphs with spatial, centrality, and edge encoding. For simplicity and scalability on large graphs, we only deployed one Graphormer layer with ReLU activation. Node embedding dimension is 64. LayerNorm is applied after MLP layers."}, {"title": "Training setup.", "content": "For all the models, the maximal training epoch number is set to 100. We employed the Adam optimizer [Kingma and Ba, 2014] for training, with a default learning rate of 0.0001 for sequence-based learning and 0.00001 for structural-based learning. The batch size is equal to 32. More detailed hyper-parameter setups are listed in Table 7 in the appendix."}, {"title": "3.3 Experimental Setup and Implementation Details", "content": "Code Base. This library is an extension of the well-established drug discovery library, DeepPurpose [Huang et al., 2020], building upon its foundational capabilities to offer enhanced features for protein-related tasks. By leveraging the strengths of DeepPurpose, this new library provides additional tools and functionalities tailored specifically for protein science. The library is publicly available at https://github.com/jiaqingxie/DeepProtein/tree/main.\nHardware Configuration. All experiments that are mentioned in this paper were trained on a 40GB NVIDIA A40 and a 24GB NVIDIA RTX 3090. The parameters we provide have ensured the scalable training on these two types of GPUs. When running GNNs on protein localization tasks, we observed a large portion of GPU memory occupied irregularly, so we recommend cutting down the size of the number of workers from 8 to 4 or batch size from 32 to 8 or even smaller to potentially avoid GPU out-of-memory (OOM) problems.\nSoftware Configuration. The library is implemented in Python 3.9, PyTorch 2.3.0, PyTDC 0.4.1 [Huang et al., 2021], DeepPurpose 0.1.5 [Huang et al., 2020], and RDKit 2023.9.6 [Landrum et al., 2006], scikit-learn 1.2.2 [Pedregosa et al., 2011], and DGLlife 0.3.2 [Li et al., 2021a]. Besides, wandb is included in DeepProtein so that researchers can observe the visualization of training curves and test results easily."}, {"title": "3.4 Results & Analysis", "content": "For each method, we used five different random seeds to conduct independent runs and reported the average results and their standard deviations. The results of protein function prediction are reported in Table 2.\nStatistical Test. We also conduct statistical tests to confirm the superiority of the best-performed method compared with the second-best baseline method. The hypothesis is that the accuracies of the best method are the same as those of the baseline method. Student's T-test is used with significance level alpha as 1% to calculate the p-values. When the p-values are below the 0.05 threshold, we reject the hypothesis and accept the alternative hypothesis, i.e., the best method is statistically significant compared with the second-best method. We use \u201c**\u201d to denote the method that achieves statistically better results than all the other methods (pass statistical tests)."}, {"title": "4 Conclusion", "content": "In this paper, we have developed DeepProtein, which marks a significant advancement in the application of deep learning to protein science, providing researchers with a powerful and flexible tool to tackle various protein-related tasks. By integrating multiple state-of-the-art neural network architectures and offering a comprehensive benchmarking suite, DeepProtein empowers users to explore and optimize their models effectively. The detailed documentation and tutorials further enhance accessibility, promoting widespread adoption and reproducibility in research. As the field of proteomics continues to evolve, DeepProtein stands to contribute substantially to our understanding of protein functions, localization, and interactions, ultimately driving forward discoveries that can impact biotechnology and medicine."}, {"title": ".1 Evaluation Metrics", "content": "In this section, we describe the basic evaluation metrics for both classification and regression tasks.\nClassification metrics. Most classification tasks are binary classification, except subcellular pre-diction in protein localization prediction, which is a 10-category classification problem, where we use accuracy (acc) (the fraction of correctly predicted/classified samples) as the evaluation metric. In binary classification, there are four kinds of test data points based on their ground truth and the model's prediction,\n1. positive sample and is correctly predicted as positive, known as True Positive (TP);\n2. negative samples and is wrongly predicted as positive samples, known as False Positive (FP);\n3. negative samples and is correctly predicted as negative samples, known as True Negative (TN);\n4. positive samples and is wrongly predicted as negative samples, known as False Negative (FN)."}, {"title": "PR-AUC (Precision-Recall Area Under Curve).", "content": "The area under the Precision-Recall curve summa-rizes the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds."}, {"title": "ROC-AUC Area Under the Receiver Operating Characteristic Curve", "content": "summarizes the trade-off between the true positive rate and the false positive rate for a predictive model using different probability thresholds. ROC-AUC is also known as the Area Under the Receiver Operating Characteristic curve (AUROC) in some literature."}, {"title": "Regression metrics.", "content": "In the regression task, both ground truth and prediction are continuous values."}, {"title": "Mean Squared Error (MSE)", "content": "measures the average of the squares of the difference between the forecasted value and the actual value. It is defined as MSE = $\\frac{\\sum_{i=1}^{N}(Y_i - \\hat{Y_i})^2}{N}$, where N is the size of the test set; $y_i$ and $\\hat{y_i}$ denote the ground truth and predicted score of the i-th data sample in the test set, respectively. MSE value ranges from 0 to positive infinity. A lower MSE value indicates better performance."}, {"title": "Mean Absolute Error (MAE)", "content": "measures the absolute value of the difference between the predicted value and the actual value. It is defined as MAE = $\\frac{1}{N}\\sum_{i=1}^{N}|Y_i - \\hat{Y_i}|$, where N is the size of the test set; $y_i$ and $\\hat{y_i}$ denote the ground truth and predicted score of the i-th data sample in the test set, respectively. MAE value ranges from 0 to positive infinity. It emphasizes the ranking order of the prediction instead of the absolute value. A lower MAE value indicates better performance."}, {"title": "Spearman rank correlation ($\\rho$)", "content": "also known as Spearman's $\\rho$, is a nonparametric statistical test that measures the association between two ranked variables. A higher $\\rho$ value indicates better performance."}, {"title": "R-squared (R2) score", "content": "is defined as the proportion of the variation in the dependent variable that is predictable from the independent variable(s). It is also known as the coefficient of determination in statistics. Higher R\u00b2 scores indicate better performance."}, {"title": "A Hyperparameter Settings", "content": "In table 7, we have listed a common settings of hyperparameter used in this library. In terms of learning rate (lr), a higher learning rate which is equal to 0.0001 for graph neural networks would lead to failure in training. For Subcellular and its binary version, a training epoch of 60 is enough for convergence. For small-scale protein datasets such as IEDB [Yi et al., 2018], PDB-Jespersen, and SAbDab-Liberis, a larger learning rate of 0.001 also leads to convergence and the same performance when using CNN, CNN-RNN, and Transformer. For TAP, SAbDab-Chen and CRISPR-Leenay, larger learning rate of 0.0001 is suggested when training graph neural networks."}, {"title": "B More results of performance on protein sequence learning", "content": "In this section, we show three more model performance on Fluorescence, Beta-lactamase and Human PPI dataset."}, {"title": "C More results of empirical memory and runtime complexity", "content": "In this section, we show four more empirical complexity results on Fluorescence, Beta-lactamase, SubCellular Binary and Yeast PPI. In Figure 5, the same pattern is found as Figure 3: Transformer took up a large amount of memory to train while the training speed is reasonably fast. Training GNN is slow since the DGL framework computes the message passing on the CPU. CNNs are both fast and memory efficient to train. A different case is found in Figure 6 where for SubCellular Binary and Yeast PPI, GCN is the slowest to train with occupying a large amount of GPU memory space."}, {"title": "D Training Curves of DeepProtein", "content": "In this section, we plot the metrics (Accuracy or R2) and training loss for SubCellular, PPI Affinity, SAbDab-Chen and TAP."}, {"title": "E Few Lines of Codes", "content": "Protein Function Prediction We show an example of CNN on Beta_lactamase with DeepProtein. More case studies can be found in the GitHub repository of DeepProtein."}, {"title": "F Tutorial", "content": "Demos can be found under DEMOS folder in DeepProtein."}]}