{"title": "Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification", "authors": ["FATEMA-E- JANNAT", "SINA GHOLAMI", "JENNIFER I. LIM", "THEODORE LENG", "MINHAJ NUR ALAM", "HAMED TABKHI"], "abstract": "In the medical domain, acquiring large datasets poses significant challenges due to privacy concerns. Nonetheless, the development of a robust deep-learning model for retinal disease diagnosis necessitates a substantial dataset for training. The capacity to generalize effectively on smaller datasets remains a persistent challenge. The scarcity of data presents a significant barrier to the practical implementation of scalable medical AI solutions. To address this issue, we've combined a wide range of data sources to improve performance and generalization to new data by giving it a deeper understanding of the data representation from multi-modal datasets and developed a self-supervised framework based on large language models (LLMs), SwinV2 to gain a deeper understanding of multi-modal dataset representations, enhancing the model's ability to extrapolate to new data for the detection of eye diseases using optical coherence tomography (OCT) images. We adopt a two-phase training methodology, self-supervised pre-training, and fine-tuning on a downstream supervised classifier. An ablation study conducted across three datasets employing various encoder backbones, without data fusion, with low data availability setting, and without self-supervised pre-training scenarios, highlights the robustness of our method. Our findings demonstrate consistent performance across these diverse conditions, showcasing superior generalization capabilities compared to the baseline model, ResNet-50.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent days, the artificial intelligence domain has witnessed a revolutionary breakthrough. However, in the medical field, a significant gap persists due to the scarcity of data. Machine learning models require extensive datasets for effective training, yet the medical domain faces constraints in this regard, primarily due to privacy concerns surrounding patient data. This scarcity poses a substantial challenge, hindering the progress and application of scalable medical AI solutions in healthcare.\nTo bridge this gap, our research addresses two key challenges. First, we focus on developing a robust machine learning classifier based on Large Language Models (LLM) to detect eye diseases from optical coherence tomography (OCT) images for AI-based eye care management. Second, we address the challenge of creating a machine learning model capable of learning from varied unlabeled data, making it useful in real-world situations with new and unseen data.\nAge-related macular degeneration (AMD), along with other sight-threatening conditions such as diabetic macular edema (DME), choroidal neovascularization (CNV), and diabetic retinopathy (DR), ranks among the leading causes of irreversible blindness and vision impairment (VI) globally. VI affects nearly 2.2 billion people worldwide, with almost 1 billion cases potentially preventable through early diagnosis and intervention [48]. Therefore, it is critical to identify those who are at risk of developing the disease or seeing it progress, especially from the early stages to the more advanced stages, as prompt intervention can stop the disease's progression or slow it down, ultimately preventing"}, {"title": "2 RELATED WORKS", "content": "In recent years, computer vision has emerged as an important tool in medical imaging, facilitating advanced diagnostics, treatment planning, and disease monitoring. The integration of deep learning (DL) techniques has particularly revolutionized this field, by automating tasks such as image classification, segmentation, and disease diagnosis, thereby revolutionizing medical image analysis. Concurrently, transformer networks, originally designed for natural language processing, have shown promising potential, broadening the scope of applications within medical imaging. This section explores the latest developments and research efforts in leveraging DL and transformer networks for medical image analysis, with an emphasis on the contributions and advances made by them.\nThe field of computer vision has undergone a significant transformation due to the evolution of deep learning (DL), which began with the introduction of AlexNet [22] in 2012. Serving as one of the first deep convolutional neural network (CNN) models, AlexNet's success in the ImageNet was a major turning point in computer vision methodologies, transitioning from traditional methods to automated DL techniques. Subsequently, numerous backbone models like VGG [37], ResNet [15], and Inception [40] have developed, further advancing image analysis capabilities. These advancements\nextended beyond traditional computer vision, impacting medical domains by demonstrating effectiveness in tasks such as medical image classification [1, 6, 8, 21, 23, 36, 38, 51]. Notably, DL techniques have found significant application in the classification of optical coherence tomography (OCT) images, particularly in diagnosing conditions like age-related macular degeneration (AMD), choroidal neovascularization (CNV), and diabetic macular edema (DME). Studies such as those referenced by [43] and [25] have underscored the remarkable accuracy and effectiveness of DL in distinguishing abnormal OCT images from normal ones, indicating the potential for automated screening and the development of computer-aided diagnostic tools. However, DL algorithms, particularly CNNs, need substantial amounts of training data, which can be challenging to obtain in medical imaging where data scarcity is common. Consequently, techniques such as transfer learning and domain adaptation have become essential for leveraging knowledge from source tasks to enhance performance in target tasks, addressing the data scarcity issue.\nIn recent advancements within medical image analysis, transformer models have emerged as a promising avenue for enhancing diagnostic accuracy and efficiency. Originally developed for natural language processing tasks, transformers have been adapted to handle the complex spatial relationships present in medical images. In 2017 [44], the transformer model was first introduced for the NLP task which leverages the self-attention mechanism to learn the contextual relationship among words within a sentence. Drawing inspiration from this concept, the vision transformer (ViT) [10] utilizes multi-head attention mechanisms to understand the contextual relationships among image pixels. This design excels in learning long-range dependencies in data, enhancing its ability to interpret spatial and temporal aspects of images. By treating images as sequences of patches, ViT effectively comprehends the overall context, a crucial factor in image classification, object detection, and pose estimation. The introduction of ViT marks a significant milestone, setting new benchmarks in image classification and showcasing its immense potential in various computer vision applications including medical image analysis. The application of transformer models in medical imaging, particularly in ophthalmology, has been extensively studied, as evidenced by works such as Ayana et al. [5], Okolo et al. [32], Alshammari et al. [3], Wang et al. [47], and Kihara et al. [20]. Notably, Wu et al. [49] proposed a transformer-based approach tailored for fundus image analysis, wherein images are segmented into patches for sequential classification. This method has demonstrated remarkable performance in contrast to traditional convolutional neural networks (CNNs) across various metrics such as accuracy, specificity, precision, sensitivity, and quadratic weighted kappa score. The success of this method underscores the effectiveness of the applicability of attention mechanisms in diagnosing diabetic retinopathy. However, the adoption of a Vision Transformer (ViT) architecture poses challenges due to its heavy computational requirements, as highlighted by Islam et al. [16].\nThe development of a large language model, named BERT (Bidirectional Encoder Representations from Transformers) [9], has revolutionized natural language processing tasks, demonstrating the power of self-supervised learning techniques. Inspired by their success in language understanding, researchers have begun exploring the application of these models in the computer vision domain. By leveraging the pre-trained representations learned from vast amounts of data, these models provide a novel way to address problems in medical image analysis, like limited labeled data scenarios. The subsequent introduction of Masked Image Modeling (MIM) marked a significant advancement in the field of self-supervised learning (SSL). MIM techniques, such as the masked autoencoder (MAE) introduced by He et al. [14], focus on reconstructing masked portions of input data, allowing models to learn robust feature representations by capturing the underlying structure of visual data. Building upon this foundation, SimMIM [50] introduced a straightforward but successful method that directly predicts the pixel values of masked patches in images. These advancements highlight the potential of MIM-based approaches to address challenges in medical imaging, including data scarcity and the need for robust feature extraction, ultimately advancing diagnostic capabilities."}, {"title": "3 METHODOLOGY", "content": "In this work, we have taken a holistic approach by combining datasets sourced from three distinct origins. This process adds a wider range of information to the overall representation learning because each dataset contributes distinct modalities. Combining a wide range of data sources improves performance and the model's capacity to generalize to new data by giving it a deeper understanding of the data representation. We trained an SSL MAE network with SwinV2 as its backbone architecture by utilizing this combined dataset. This pre-trained weight makes a robust foundation for classifying retinal diseases effectively in the downstream tasks.\nThere are four essential stages in our proposed framework.\n(1) Data Fusion: Our study used three OCT image datasets, combining their training and validation sets for Self-Supervised Pre-training, followed by individual fine-tuning on each dataset to evaluate classification performance and generalization.\n(2) Self-Supervised Pre-training: In this initial stage of training, a self-supervised pre-training is conducted on a combined collection of unlabeled Optical Coherence Tomography (OCT) images, employing a transformer-based Masked Autoencoder(MAE) approach to extract detailed visual representations. Through this self-supervised learning process, the model gains an understanding of the structure and features within the multimodal OCT images. Subsequently, this learned weight is transferred to a supervised classifier model, leveraging the learned representations to improve the classification task.\n(3) Supervised Fine-tuning: Following the self-supervised pre-training phase, a supervised fine-tuning is conducted. This fine-tuning process aims to refine the model's classification capabilities by leveraging the weights transferred from the pre-trained model. By exposing the model to labeled data and adjusting its parameters based on the specific task requirements, the fine-tuning stage further optimizes the model's performance, enhancing its ability to accurately classify retinal diseases from OCT images.\n(4) Baseline Training: In our evaluation study, ResNet50 was used as the baseline model against which we compared the performance of our proposed model. By employing ResNet50 as a benchmark, we were able to assess the efficacy of our proposed approaches in improving our task."}, {"title": "3.1 Data Fusion", "content": "For our study, we utilized three distinct datasets comprising Optical Coherence Tomography (OCT) images depicting various retinal diseases. Each dataset was partitioned into training, validation, and test sets. During the Self-Supervised Pre-training phase, we combined the training and validation sets from all three datasets into a unified training and validation set. This combination of data modalities aims to enhance the diversity and richness of the training data,"}, {"title": "3.2 Self-Supervised Pre-training", "content": "Self-supervised learning (SSL) is a method where models learn from unlabeled data by understanding its structure. In this study, we used a technique called Masked Autoencoder (MAE), which masks parts of input data randomly and trains the model to recreate the original by learning the representation of the input data. The MAE consists of two parts: an encoder and a decoder. We resized images to (224x224) and fed them through the encoder, which randomly masks"}, {"title": "3.2.1 Swin-based MAE", "content": "The encoder used in the Swin transformer-based Masked Autoencoder (MAE) is built with a Swin transformer backbone and has an embedding size of 96. The architecture of the Swin transformer has different numbers of layers at each stage (2, 2, 18, 2), which corresponds to the distribution of layers at each stage. The model employs shifted window attention mechanisms in each step to concentrate on local information within 4x4 patches, gradually constructing a global understanding through the connecting of shifted windows. With each step, the number of attention heads-which is set to (6, 12, 24, 48)-doubles, allowing the model to attend to progressively finer details and identify hierarchical features in the input image. In the meantime, a more expressive representation is made possible during the decoding process by the decoder, which is constructed with an embedding size of 768.\nThe decoder network has a similar number of attention heads and layers as the encoder, along with Swin transformer layers that are set up to restore the spatial dimensions of the encoded features and a patch-expanding mechanism. This layer-wise design ensures a gradual reconstruction of the original image dimensions, facilitating effective decoding of the encoded representation acquired by the encoder."}, {"title": "3.2.2 SwinV2-based MAE", "content": "For this task, we utilize a SwinV2-based Masked Autoencoder (MAE), capitalizing on the superior performance of the SwinV2 network. While retaining the Swin-based decoder, we used the SwinV2 for the encoder component to address challenges related to training stability, high-resolution processing, and data efficiency. The enhanced capabilities of SwinV2 align with our requirements, creating a balance between detail-oriented feature extraction and computational efficiency. Leveraging an embedding dimension of 96, depths configured as (2, 2, 6, 2), and attention heads ranging from (3, 6, 12, 24), this customized approach allowed the SwinV2-based MAEs to excel in capturing intricate details essential for our task."}, {"title": "3.3 Supervised Fine-Tuning", "content": "We added a classification head in place of the decoder in the classifier network. The classification head employed a linear layer to process the encoder's features and produce class logits, which were then used for classification.\nThe linear layer consisted of three consecutive dense layers, each incorporating Rectified Linear Unit (ReLU) activation functions. The input image was gradually transformed into highly encoded features by these layers. The initial layer had an input size equal to the dimension of the positional embedding from the encoder, with an output size of 512. The subsequent layer refined these features, mapping them to a 256-dimensional space, followed by a final layer compressing them into a 128-dimensional feature vector.\nThe purpose of this hierarchical transformation was to prime the model for successful classification tasks by highlighting and reducing the amount of intrinsic discriminative features in the input. During training, the linear layer learns weights and then class logits are transformed into class probabilities using the softmax function, allowing the model to predict classes accurately.\nThis methodology included fine-tuning one dataset, followed by assessing the model's classification performance on the corresponding test set. Additionally, two separate test sets from distinct datasets were utilized to assess the model's generalization and robustness. This iterative cross-data evaluation process was replicated across all three datasets, providing a thorough examination of the model's adaptability to varying data sources.\nThe development of Multi-OCT-SelfNet, which combines fusion data with supervised fine-tuning and self-supervised pre-training techniques, offers a strong framework for the classification of retinal diseases in Optical Coherence Tomography (OCT) images. This framework attempts to enhance the model's ability to generalize to new, unseen data by leveraging learned representations by combining the Masked Autoencoder (MAE) architecture with a subsequent classifier model. This holistic approach underscores the importance of utilizing fusion data with both self-supervised and supervised techniques to attain comprehensive and effective disease classification in OCT imaging.\nThe overall framework is given in Figure 2 where it is shown the two-phase training approach, Firstly, it utilizes a masked image autoencoder for self-supervised learning from unlabeled images. Then, in the second phase, the pre-trained encoder is combined with a linear classifier for classification tasks, transferring the learned weights from the initial phase. This strategy optimizes the model's efficiency and effectiveness in handling classification tasks."}, {"title": "3.4 Baseline Model", "content": "ResNet50 stands as a versatile solution for handling intricate tasks like classifying age-related macular degeneration (AMD) and diabetic retinopathy using Optical Coherence Tomography (OCT) images, showcasing its efficacy in medical imaging analysis. To benchmark our proposed method's performance, we employed ResNet50 as our baseline model.\nThe ResNet50 architecture comprises a 7\u00d77 kernel convolution and a max pooling layer, succeeded by a series of convolutional layers with varying sizes and numbers of kernels. With 50 convolutional layers, the network is then followed by average pooling and fully connected layers, with the number of nodes matching the classes for multi-class classification, employing softmax activation."}, {"title": "3.5 Loss Function", "content": "For the pre-training stage, we have used a loss function, which only takes into account the pixels where the mask is active, and uses the mean squared error (MSE) between the predicted image and the original image. In Equation 1, the loss function is provided.\n$M_{ratio} = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\hat{x_i})^2 \\times m_i $\nHere xi is the predicted image, xi is the original input image, mi is the mask, mratio is the mask ratio and N is the number of total sample.\nThe MSE is multiplied by the mask to calculate the loss on the pixels where the mask is active. The mask ratio indicates the percentage of the image that is masked. Since the mask is being used to focus only on specific areas of the image, the loss is calculated by dividing the mean square error (MSE) by the mask ratio. This allows us to properly normalize the loss to the proportion of the image that is masked and scale the loss accordingly."}, {"title": "3.6 Datasets", "content": "3.6.1 DS1. This dataset comprises a total of 109,559 Optical Coherence Tomography (OCT) retinal images acquired using Spectralis OCT from Heidelberg Engineering, Germany. These images are categorized into four classes: Normal, Choroidal Neovascularization (CNV), Diabetic Macular Edema (DME), and Drusen. Upon identifying identical images, as documented previously [13], we followed their methodology to cleanse the dataset, resulting in 101,565 images. Subsequently, we reclassified Drusen images as Age-related Macular Degeneration (AMD). DS1 was then partitioned into training, testing, and validation sets using an 80%, 10%, and 10% ratio, respectively. The distribution of samples across each category within each set is illustrated in Figure 4.\n3.6.2 DS2. The DS2 dataset is acquired using Heidelberg Engineering Spectralis SD-OCT protocols approved by the Institutional Review Board (IRB) [39]. It consists of retinal images sourced from 45 subjects, distributed as follows: 15 normal subjects, 15 patients diagnosed with dry age-related macular degeneration (AMD), and 15 patients with diabetic macular edema (DME). To organize the dataset for training, validation, and testing purposes, we split the subjects accordingly: the first 10 subjects from each category were allocated to the training set, subjects 11 to 12 were assigned to the validation set, and subjects 13 to 15 were designated to the test set. Figure 4 shows the distribution of samples within each set, across each category.\n3.6.3 DS3. This dataset comprising OCT images from 500 subjects, was obtained using two different fields of view: 3-mm and 6-mm. Each 3-mm file contains 304 scans per patient, while a 6-mm file contains 400 scans. Our analysis focused on the slice images of the fovea (image numbers 160-240 from the 6-mm scans), capturing the most prominent retinal features, while peripheral retinal sections were deemed of limited significance for classification. This dataset comprises categories such as NORMAL, AMD, CNV, DR, OTHERS, RVO, and CSC. Given the relatively low number of samples in the RVO and CSC categories, we excluded them. The \"OTHERS\" category comprises diseases, including retinal detachment (RD), retinal hemorrhage (RH), and retinitis pigmentosa (RP), among others, which were also discarded due to their lack of particularity. All OCT images were captured using a spectral-domain OCT system with a center wavelength of 840 nm (RTVue-XR, Optovue, CA) []. Figure 4 provides an overview of the sample distribution across each category within each set."}, {"title": "4 EXPERIMENTS", "content": "4.1 Implementation Details\n4.1.1 Self-Supervised Pre-training Implementation Details. For this stage of self-supervised pre-training, all experiments were conducted using the computational power of the NVIDIA Tesla V100 graphical processing unit (GPU). Specific hyperparameters were selected to optimize model performance. The learning rate was set to 1.5 \u00d7 10\u22124, and the Adam optimizer was employed with a weight decay of 0.05 [30]. Additionally, the optimizer utilized \u1e9e\u2081 and \u1e9e2 values of 0.9 and 0.95, respectively. Input data consisted of batches comprising 32 normalized images. Training proceeded for a total of 100 epochs. To ensure the most robust configuration, the model with the lowest validation loss was saved for subsequent fine-tuning iterations.\n4.1.2 Supervised Fine-Tuning Implementation Details. During the fine-tuning stage, similar to the training phase, experiments were conducted using the NVIDIA Tesla V100 GPU, and the hyperparameters were selected as follows, the learning rate was adjusted to 3 \u00d7 10-4, and the Adam optimizer was employed with a weight decay of 1 \u00d7 10-6. The optimizer utilized \u1e9e\u2081 and \u1e9e2 values of 0.9 and 0.99, respectively. As the loss function, the categorical cross-entropy loss is employed. The training process involved the application of multiple data augmentation techniques, such as random resized crop, random horizontal flip, color jitter, random grayscale, and ImageNet normalization, to improve the robustness of the model. Training extended over 100 epochs, with early stopping criteria implemented using patience of 10 epochs. The model with the highest validation accuracy was saved for subsequent testing.\nDuring training and fine-tuning, all images were resized to 224 x 224. Python 3.10.9, Pytorch 1.12.1, and CUDA 11.2 were used to implement the codes.\n4.1.3 Baseline Model: ResNet-50 Implementation Details. During the experiments with the baseline model, an NVIDIA GeForce RTX 3060 Ti GPU was used. The learning rate was set to 3 \u00d7 10-4, accompanied by a weight decay of 10-6, a batch size of 24, and the utilization of the Adam optimizer with decoupled weight decay [30]. Momentum and adaptive learning rate scaling were set by \u1e9e\u2081 and \u1e9e2 values of 0.9 and 0.999, respectively. Training proceeded for a maximum of 100 epochs, with early stopping criteria based on validation loss, incorporating a patience of 10 epochs. To enhance the model's robustness and generalization capabilities, various data augmentation techniques were employed, including random rotation, horizontal flip, color jittering, Gaussian blurring, and elastic transform."}, {"title": "4.2 Evaluation Metrics", "content": "As illustrated in Figure 4, all datasets exhibit a substantial class imbalance, with a predominance of 'NORMAL' cases. Given this, relying solely on accuracy can be misleading. To mitigate this issue, we employ AUC-ROC as the primary evaluation measure, as it effectively provides a more robust evaluation by considering the trade-off between true positive and false positive rates across different thresholds. Additionally, we incorporate accuracy, AUC-PR, and F1-score to offer a comprehensive evaluation.\n4.2.1 Accuracy. The number of accurate predictions made by the model is known as accuracy, and it is determined by dividing the total number of predictions by the number of correct predictions. The accuracy formula is provided by Equation 2.\n$Accuracy = \\frac{TP + TN}{TP+TN + FP + FN}$\nHere, TP = True Positives, TN = True Negatives, FP = False Positives, and FN = False Negatives.\n4.2.2 AUC-ROC. One important metric for assessing the performance of the classifier is the Area Under the Receiver Operating Characteristic curve (AUC-ROC). Plotting each class's true positive rate against its false positive rate across a range of threshold values creates the ROC curve. The area under this curve, or AUC-ROC, gives a thorough overview of the model's performance in all classes and threshold settings. A higher AUC-ROC signifies better discrimination ability among the different classes, indicating superior classifier performance.\n4.2.3 AUC-PR. The Area Under the Precision-Recall curve (AUC-PR), a performance metric for classifier evaluation, is comparable to AUC-ROC. By plotting precision against recall across different threshold values, the PR curve emphasizes the trade-off between recall and precision. The AUC-PR quantifies the overall performance of the model across various threshold settings by measuring the area under this curve. A higher AUC-PR score indicates better classifier performance.\n4.2.4 F1-Score. Another performance evaluation metric that accounts for both recall and precision is the F1-Score. This metric is particularly helpful when dealing with data imbalances. By incorporating both precision and recall, the F1-Score provides a comprehensive evaluation of a classifier's effectiveness. This metric is especially suitable in situations where accurately capturing both positive and negative instances is critical for decision-making and model evaluation.\nEquation 3 is used to calculate the F1-Score.\n$F1Score = \\frac{2 * Precision * Recall}{Precision + Recall}$\n4.2.5 Penalty-Based Performance Index. To provide a quantitative assessment of the model's generalization performance, we created the Penalty-Based Performance Index to evaluate the models' performance across all test sets. This method calculates a penalty for each score by subtracting it from 1, representing the error rate. The scores considered for this calculation include accuracy, AUC-ROC, F1-score, and AUC-PR, encompassing various aspects of model performance. The average penalty for each model is then computed, indicating the overall error tendency which represents, on average, how much the model's score deviates from perfect score across all test sets. Finally, these average penalties are transformed into a scale of 1 to 100 to obtain a performance score for each model. A lower score indicates better performance, which means the model's accuracy is closer to a perfect score with minimal variance across the test sets."}, {"title": "4.3 Self-Supervised Pre-training Result", "content": "In our ablation study, we conducted pre-training for 100 epochs on three transformer-based networks. One network utilized the Swin architecture as its backbone, while the other two employed distinct variations of SwinV2 as back-bones-referred to as SwinV2 and SwinV2-large, respectively. The objective was to assess the efficacy of these models and their performance in subsequent tasks. In particular, after 100 training epochs, the SwinV2-based MAE demonstrated remarkable proficiency, obtaining a mean squared error (MSE) loss of 0.007 as shown in Fig 6. Moreover, Fig 5 provides a detailed visualization of the masked image input and the corresponding image reconstruction from the SwinV2-based MAE at various epochs, offering insights into the model's learning dynamics throughout the training process. While the reconstructed images may display imperfections, our project prioritized the capture of intricate image structures and patterns over flawless reconstructions. Consequently, in subsequent tasks, we employed these pre-trained weights, capitalizing on their learned representations, rather than initializing the classifiers with random weights. By applying the learned knowledge encoded in the pre-trained weights, this method allowed us to take advantage of the benefits of the transfer learning approach, facilitating enhanced performance and accelerated convergence in downstream tasks."}, {"title": "4.4 Supervised Fine-tuning Result", "content": "4.4.1 Performance Comparison with Different Encoder Network. We evaluated different encoder networks against the baseline ResNet-50 model in our extensive experiment. To ensure consistency, we utilized the same encoder from the SSL networks and employed transfer learning to transfer the learned weights. For downstream classification tasks, we also integrated a classifier network. Each supervised network underwent fine-tuning individually for every dataset, followed by evaluations on both the respective test set and the test sets from other datasets. This comprehensive evaluation allowed us to observe the network's performance on previously unseen test data and to observe its generalization capability. The performance of each model was then compared with that of the baseline model ResNet-50, which underwent training on each dataset and subsequent evaluation on all three test sets. Throughout this experimental process, we applied data augmentation techniques to enhance robustness. Performance metrics including Accuracy, AUC-ROC, AUC-PR, and F1-Score were employed to measure effectiveness. Analysis of Table 1 revealed the consistent performance of the self-supervised fine-tuning approach over the baseline model, with the SwinV2-based classifier demonstrating the most reliable performance across all test sets, particularly in scenarios with smaller datasets such as Dataset-2 and Dataset-3. While Dataset-1, with its larger sample size, yielded similar performance between the proposed framework and the baseline model, it's noteworthy that our method showcased superior performance on smaller datasets as well. The generalization capabilities during off-domain evaluation, when the model was fine-tuned on one dataset and evaluated on other test sets, significant performance boosts were observed in our proposed method.\n4.4.2 Performance Evaluation without Data Fusion during Pre-training Phase. In this experiment, we assessed the performance without the data fusion during the SSL pre-training phase. We opted to pre-train the SSL model with each dataset individually and subsequently evaluated the classifier's performance solely on the respective test sets. This"}, {"title": "4.4.4 Performance Comparison in Limited Data Settings", "content": "In this experiment, we conducted an in-depth analysis to assess the effectiveness of our proposed framework in scenarios characterized by limited data availability, where acquiring a larger labeled dataset is not feasible. We compared the result to the baseline model. We fine-tuned the model during the fine-tuning stage using only 50% of the available training data for each dataset, as opposed to utilizing the entire dataset. Subsequently, we evaluated the performance of our proposed framework on the respective test sets. This experimental setup enabled us to gain valuable insights into the practical applicability of our model in clinical contexts, where"}, {"title": "4.4.5 Performance Evaluation on the Effect of Correcting Data Imbalance", "content": "In this experiment, we have addressed the data imbalance issue by assigning different weights to each class in the loss function. This strategy ensures that the model focuses more on the minority class during training, reducing the bias towards the majority class. We compared the results of our model, using this weight-adjusting method, to the baseline model. The goal was to observe how our"}, {"title": "6 CONCLUSION", "content": "The proposed work aims to leverage innovative techniques such as multi-source data fusion, self-supervised learning, and transformer networks to overcome the challenges posed by limited data availability in retinal disease diagnosis. During the self-supervised phase, the model is pre-trained using publicly available datasets, ensuring that sensitive clinical data is not exposed. In the downstream classification task specific to individual clinical settings, the model is fine-tuned solely on the local dataset, eliminating the need to share any private data. This approach preserves data privacy while allowing the model to adapt effectively to diverse clinical environments. This framework will be beneficial in situations where access to extensive datasets is limited offering a scalable and practical approach to implementing medical AI solutions. The results of our study showcase the effectiveness of our proposed framework, Multi-OCT-SelfNet, which consistently surpasses the baseline performance. Our approach demonstrates superior performance scores across all datasets, particularly excelling with smaller datasets. Through meticulous experimentation, we've validated the efficacy of our methodology. The incorporation of data fusion and self-supervised pre-training significantly enhances performance, as evidenced by our ablation study. This highlights the significance of these components in improving both the resilience and accuracy of our model, thereby facilitating robust generalization."}, {"title": "5 FUTURE WORK", "content": "While our framework has shown significant potential in the automated classification of retinal diseases using OCT images, its complexity poses challenges in terms of interpretability. As we move forward, enhancing the transparency of our model is a key priority. By improving interpretability, we aim to build AI-based diagnostic tools that clinicians can"}]}