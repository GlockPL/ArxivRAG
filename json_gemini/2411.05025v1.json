{"title": "LLMs as Research Tools: A Large Scale Survey of Researchers' Usage and Perceptions", "authors": ["ZHEHUI LIAO", "MARIA ANTONIAK", "INYOUNG CHEONG", "EVIE YU-YEN CHENG", "AI-HENG LEE", "KYLE LO", "JOSEPH CHEE CHANG", "AMY X. ZHANG"], "abstract": "The rise of large language models (LLMs) has led many researchers to consider their usage for scientific work. Some have found benefits using LLMs to augment or automate aspects of their research pipeline, while others have urged caution due to risks and ethical concerns. Yet little work has sought to quantify and characterize how researchers use LLMs and why. We present the first large-scale survey of 816 verified research article authors to understand how the research community leverages and perceives LLMs as research tools. We examine participants' self-reported LLM usage, finding that 81% of researchers have already incorporated LLMs into different aspects of their research workflow. We also find that traditionally disadvantaged groups in academia (non-White, junior, and non-native English speaking researchers) report higher LLM usage and perceived benefits, suggesting potential for improved research equity. However, women, non-binary, and senior researchers have greater ethical concerns, potentially hindering adoption.", "sections": [{"title": "1 Introduction", "content": "From Vannevar Bush's hypothesized Memex in 1945 [13] to Apple's vision of the Knowledge Navigator in 1987 [73], many have long envisioned tools that organize and interact with the sum of our knowledge to enable us to better conduct complex knowledge work and push the boundaries of what we know. Recent advancements in generative Al technologies that are trained on terabytes of text scraped from the internet [84] and fine-tuned to provide a chat interface [76] bring us one step closer to making this dream a reality. More directly, this technology has also sparked a surge of interest across research, industry funding, and startups to develop a new generation of research support tools [20, 25, 28, 59, 87, 88, 91].\nIn fact, the recent burst in popularity of widely available generative AI tools, such as ChatGPT,\u00b9 and findings from small-scale interview and survey studies with researchers [26, 72] suggest that many in the research community have already found benefits in incorporating current generative Al models into their research workflows. Adopting this new generation of knowledge tools has opened up many possibilities, such as improved efficiency, greater research equity, and inspiring novel ideas. At the same time, new tools both breathe new life into familiar research risks and ethical concerns like transparency, reproducibility, plagiarism, and data fabrication - while introducing new dangers to the research process. It is possible that these tools could result in researchers losing essential skills [5, 52], the development of emerging social norms and associated reputational costs [37], and a decrease in research creativity, among other possibilities. Differences in perceptions about risks, ethics, and social acceptability across demographic groups and researcher backgrounds could also drive differences in adoption, so that any benefits accrue unevenly. This could potentially exacerbate existing structural barriers in academia due to biases and other factors [29].\nWhile prior work has mostly focused on research domain-specific investigations [44, 47, 62, 78, 97] or small-scaled surveys or qualitative interviews [26, 72], we conducted a large scale survey with verified published authors. These authors were sourced from Semantic Scholar, a platform that maintains an open repository of published researchers from a wide range of research domains, demographic backgrounds, and research experiences. Our survey of these researchers was designed to explore the following research questions.\n\u2022 RQ1: What are the different ways researchers use\u00b2 LLMs in their research process today?\n\u2022 RQ2: How does the background\u00b3 of a researcher relate to the way they use LLMs?\n\u2022 RQ3: What are researchers' perceptions of LLMs for usage in different parts of the research process?\n\u2022 RQ4: How does the way a researcher uses LLMs relate to their perceptions?\n\u2022 RQ5: How does the background of a researcher relate to their perceptions?\n\u2022 RQ6: How does the source of an LLM affect researchers' perceptions and usage?\nOur survey focused on better understanding how researchers are actually using LLM-based researcher tools in their own work today, and how they perceive the risks and benefits of leveraging LLMs for different research tasks. In particular, we were also interested in researchers' perceptions not only of LLM usage but also about the acceptability of using these tools, and the possible differences in perception across demographic groups, leading us to recruit researchers across nationalities, languages, career stages, discipline, gender, age, etc. The differences we uncover between these groups reveal rapidly changing social norms around the usage of AI tools in research, highlighting important considerations around research equity and broader adoption.\nIn particular, we find around 81% of researchers we surveyed have used LLMs in one or more places in their research pipeline, with the tasks of Information Seeking and Editing reported most frequently and Data Analysis and Generation reported least frequently. We also find surprising differences between researchers of different demographic groups. Based"}, {"title": "2 Related Work", "content": "Recent work has suggested that researchers have already begun to adopt LLMs into their research workflows across various disciplines. In early 2023, Morris [72] conducted in-depth interviews with 20 researchers of diverse backgrounds,7 focusing on the opportunities and potential concerns around the use of LLMs as research tools in their respective fields. This study, with many others, reveals that researchers across numerous disciplines are incorporating LLMs into different stages of research, including ideation, literature review, data creation, cleaning and analysis, programming, and, most commonly, writing or drafting research papers [10, 31, 70, 89, 93, 101].\nMoreover, LLMs have been explored as a potential solution to challenges in the academic publishing process. Alvarez [7] suggests that LLMs could address the \"overtaxed peer-review system\", a sentiment supported by Liang et al. [61], who found that over 80% of researchers considered ChatGPT-generated feedback more beneficial than feedback from at least some human reviewers. Separately, Koller et al. [53] advocated for the use of LLMs in conference submissions, arguing that these tools help researchers \"contextualize their work, democratize knowledge, enhance data analysis, and produce better scientific output.\" As a result, the increasing adoption of LLM assistance is evident in both scientific research articles [62] and peer reviews [78].\nBeyond evidence of the research community adopting widely available LLM-based tools into their workflow, the AI and HCI research community have also devoted many resources in recent years to exploring the next generation of"}, {"title": "2.2 Risks and Ethical Implications of LLMs in Research", "content": "While LLMs have shown great promise for a future where novel AI capabilities can have significant positive impacts on science, the current implementations have faced many critiques from the community, including claims that LLMs have a popularity bias, contain a reductive view on how researchers learn their knowledge, and generate made-up articles completely [10, 34]."}, {"title": "2.2.1 Lack of precision or \u201challucination\"", "content": "One of the primary concerns of the applications of LLMs to scientific research is their insufficient level of precision and accuracy [7]. LLMs have been observed to generate plausible-sounding but entirely fictional content, a phenomenon popularly referred to as \"hallucination\" [6]. For example, Galactica, an LLM trained on scientific papers [88], was taken down after producing convincing but false scientific articles [34]. Current LLMs struggle with tasks requiring precise calculations and logical reasoning [85]. In software development, studies indicate that developers often reject LLMs' initial code suggestions [100] and face difficulties in understanding and debugging the generated code [60, 67, 92]. Without proper vetting, inaccurate content could contribute to the spread of misinformation and erode trust in research [31, 52]. In fields like medicine or engineering, where precision is crucial, LLM inaccuracies could have serious real-world consequences [8, 90]. Our results showed a similar phenomenon in using LLMs for science, where hallucination and misinformation were among the most frequently mentioned risks by our participants based on qualitative responses."}, {"title": "2.2.2 Undermined research integrity", "content": "The adoption of LLMs in academic research raises fundamental questions about diminished research integrity and originality. Researchers have expressed worry about the potential \"proliferation of low-quality research\" [10], as LLMs may facilitate the mass production of superficial or derivative work. This concern is corroborated by Kobiella et al. [52], who found that knowledge workers experienced a decreased sense of achievement when using LLMs, driven by reduced ownership, lack of challenge, and concerns about output quality. The"}, {"title": "2.2.3 Unexplainability and obscurity", "content": "The complexity of LLMs makes it difficult to understand or explain why they come to certain output, which in turn affects the reliability and interpretation of research results assisted by LLMs [83]. The obscurity issue restricts access to model's internal workings and training data, thereby hindering research transparency and verifiability [98]. This issue is particularly concerning because many researchers, particularly those lacking technical expertise or computational resources predominantly rely on commercial closed models [90, 98]. Sallou et al. [79] asserts that software engineering research faces threats to validity from the prevalent use of closed-source models, potential data leakage and reproducibility issues due to output variability and time-based drift, all of which can compromise the reliability and generalizability of research findings. While open models offer greater transparency, compared to closed models, by offering access to code and weights, many still fall short of full disclosure, commonly withholding elements like training datasets or fine-tuning processes [45, 63]. In this work, we ask survey participants to discuss their preferences for the predominantly closed models offered by industry versus open source and non-profit alternatives."}, {"title": "2.3 Demographic Influences on LLM Perception and Adoption", "content": "In addition to the high-level benefits and risks of LLMs, individual perceptions and usage patterns of LLMs are shaped by various factors, including personality traits, age, gender, and educational background [41]. For instance, in the realm of personality and age, research has shown that people with a high level of agreeability and younger people tend to have more positive views of AI, while those susceptible to conspiracy theories often have more negative perceptions [86]. A notable gender gap has been observed in LLM adoption, with male users outnumbering female users, which could be mitigated through technology-related education [24]. In the field of scientific research, structural biases have long led to disparities in academic publishing, citations, and career advancement along the lines of gender, race, economic status, and more [29, 36]. Despite some progress in recent decades, projections indicate that gender gaps in STEMM (Science, Technology, Engineering, Mathematics, and Medicine) fields may persist for generations without significant systemic reform [35]. Interestingly, LLMs present an opportunity to reduce certain inequities in research and publishing. They can lower barriers for non-native English speakers [72] and provide high-quality reviews to novice researchers who may struggle to obtain feedback from peers [16].\nWhile LLMs show promises and challenges in academic settings, a significant gap exists in our understanding of their impact across diverse demographic groups. Quantitative research examining how LLM usage patterns and perceptions vary among different populations within academia is notably scarce. To address this gap, we conducted a large-scale survey of researchers from a wide range of backgrounds. Our aim was to explore the nuanced and potentially differential impacts of LLMs on various demographic groups within the academic community. This approach recognizes that understanding the risks and opportunities presented by LLMs is important not only for the research community as a whole but also for specific demographic subgroups who may experience unique challenges or benefits. Our findings indicate that LLMs have a disproportionate effect on researchers with different identities, suggesting both challenges and opportunities for using these tools to address longstanding inequities in the research ecosystem."}, {"title": "3 Methods", "content": "Drawing insights from prior literature, we designed a questionnaire to study researchers' usage and perception of LLMs, and recruited participants among verified published authors. We initially collected 1,226 responses and ended up with 816 responses after filtering to ensure completeness and quality. Different from prior work that only reported participants' fields of study in small-scale surveys and interviews [26, 72], we additionally collected fine-grained demographic information in our survey. We transformed the dataset to generate the final demographic groups in which some of the response options were grouped to form coarser buckets (e.g., years of research experience), and free responses (e.g., fields of study) were manually coded and discretized for analysis. The survey collected both multiple-choice responses and free-text responses. We used linear mixed-effects models to test the relationships between researchers' LLM usage and perception, as well as between researchers' background and usage and perception. For free-text responses, we conducted an iterative open thematic analysis to gain a deeper qualitative understanding of participants' perceptions of LLMs [11, 19]."}, {"title": "3.1 Survey Design, Participant Recruitment, and Data Collection", "content": "When designing the questionnaire in the survey, we used the four following approaches: First, for inspiration, we looked to recent literature on using LLMs as a productivity tool for research [10, 71, 72, 78, 95] and other scenarios [60, 66], which included qualitative interviews and survey results. Second, we reviewed historic papers on how the research community had adopted new tools in the past, specifically around the use of crowdsourcing for data collection, user studies, and other productivity tasks [49, 50, 57]. Third, we publicized an anonymous formative survey on X/Twitter targeted towards researchers with open-ended questions about whether and how they use LLMs for research in order to help define initial categories of usage that we later refined. Finally, we shared early drafts of the questionnaire with other researchers in our own institutions for feedback and iteration. In the end, we classified LLM usage for research into six broad categories, each with more specific use cases under them: information seeking, editing, ideation & framing, directing writing, data cleaning & analysis, and data generation. We provide the full set of final survey questions in the Supplementary Materials."}, {"title": "3.1.2 Data collection", "content": "We collected survey responses from participants who have published at least one research paper in the past. To ensure participants were published authors, we partnered with Semantic Scholar for targeted recruitment of researchers who are listed as an author of at least one published paper on the platform. Semantic Scholar maintains a large-scale academic knowledge graph of researchers (i.e., authors) and papers, and provides a freely available web service to browse them as author profile pages. Researchers can claim their author profile pages and send corrections to Semantic Scholar, which in turn employs a quality assurance team for verifying the claims and corrections. A survey recruitment email was sent to 107,346 verified claimed authors, and the click-through rate was around 1.6%. After click-through, 71.6% of the participants signed the consent form to start the survey, of which 60.6% completed the survey. We collected 1,226 unfiltered survey responses, which we subsequently filtered to exclude those from participants who did not progress past the first page or spent fewer than 2 seconds on each question. In total, this resulted in n = 816 survey responses that we used for our analysis. The survey contained a mix of optional and required questions. For example, participants could choose not to disclose their demographic information, such as gender or race. The study was reviewed and exempted by the University of Washington IRB."}, {"title": "3.1.3 Limitations", "content": "Since we recruited from verified authors listed on Semantic Scholar, we could have tied the survey responses to participants\u2019 author metadata from Semantic Scholar to obtain high-precision demographic information (such as a list of publications, years of experiences, institutions, pronouns, etc.). However, for privacy concerns, we only used their email addresses for targeted recruitment of verified published authors. We instead relied on self-reporting using optional survey questions for demographic information, and did not tie survey responses to their author metadata. While Semantic Scholar covers a wide range of fields of study, we did find more participants to be in the field of computer science (40%), but other fields such as social sciences, biology, medicine, and natural sciences were also represented. There were also more men who responded to the survey (79%), which may partly reflect the existing imbalances in these fields of study. The detailed distributions are reported in \u00a74. Finally, the survey responses were collected in batches of recruitment emails over a six-month period from November 2023 to April 2024, with the bulk of the responses received in January 2024. The uses and perceptions of researchers may change over time as LLM tools continue to evolve, but we hope this survey can give the readers a snapshot of the current state of the community and support informed decisions as we continue to build consensus and norms around the use of LLMs for research."}, {"title": "3.2 Quantitative Analysis of Survey Responses", "content": "Out of the 816 responses, 644 provided demographic information. We focused on five demographic categories collected in the survey for later analysis: gender, race, years of research experience, native language, and field of study. The first four were collected as answers to multiple-choice questions and further consolidated into broader categories during analysis. For example, to balance our analysis given a large proportion of men participants, we collapsed all responses from women, non-binary, and other participants into a single category. Field of study was collected as free response and manually classified into four categories by the authors (more details on this process can be found in Appendix B). Answers that did not fit into any categories, such as \u201cPrefer Not to Answer\u201d or \u201cPrefer to Self-Describe\u201d were filtered out from demographic-specific analysis. We ended up with 611 responses with gender identity, 527 with racial identity, 644 with years of research experience and native language information, and 635 with field of study information. The final distributions of demographic groups are:\n\u2022 Gender: Man (79%); Woman, Non-Binary, Other (21%)\n\u2022 Race: White (61%); Non-White (39%)\n\u2022 Years of Research Experience: 11+ (57%); 4-10 (32%); 0-3 (11%)\n\u2022 Native Language: Native English (62%); Non-Native English (38%)\n\u2022 Field of Study: Computer Science (40%); Social Science & Humanities (24%); Natural Science & Engineering (21%); Biology & Medicine (15%)\nFinally, we inspected our demographic data for correlation between certain demographic groups. For example, are a majority of the male participants also white? Given the demographic variables are categorical, we conduct a series of Chi-square tests of independence in R (chisq.test) between all pairs of the five demographic groups, with multiple comparisons   -value correction using Holm-Bonferroni (p.adjust)."}, {"title": "3.2.2 Statistical methods", "content": "To address potentially correlated measurements arising from the same participants contributing multiple ratings, known as repeated measures, we employ linear mixed effects models to test the association between participant ratings (e.g., LLM usage frequency or perceptions) and participant demographic fixed-effects, while controlling for participant-specific effects. Such models are widely used in medicine [18] and behavioral sciences [21] for regression analysis with repeated measures and have also seen adoption in HCI research [9, 17, 32, 33]. In this work, we primarily use linear models of the form:\nRating ~ Demographic + UsageType + (1 ParticipantID)   (1)\nFor example, to measure the association between race and LLM usage, we regress participant usage (Rating) onto the race binary variable (Demographic), including additional control terms for the type of LLM usage (UsageType) as well as a random intercept term (1 ParticipantID) to capture participant-specific effects, such as when an individual has a tendency to give systematically higher or lower ratings. We can repeat this process for other demographic variables, for example swapping out race for gender or years of experience, to obtain different linear model fits. 10 We fit"}, {"title": "3.3 Qualitative Analysis of Free-Text Responses", "content": "To collect deeper insights beyond pre-defined multiple choices, several of the questions in our survey (Q64, 65, and 66) were paired with an optional free-text response question in which participants could elaborate and provide the reasoning behind their multiple-choice answers. To analyze these responses, we followed an iterative open thematic analysis approach [11, 19]. Specifically, three of the paper authors read through and coded the same subset of the responses into thematic categories independently. Then, the three authors meet with the entire research team to compare their codes and discuss their findings to settle on a final set of themes. Using the final set of themes, the three authors coded"}, {"title": "4 Results", "content": "We asked participants to mark how frequently they used LLMs for each of six broad categories of research tasks. When considering their answers across all six categories, we find that LLMs are now a common tool for researchers, with a total of 80.88% (660 out of 816) of respondents adopting the use of LLMs somewhere in their research process. However, most usage today is still concentrated around tasks that have some connection to manuscript preparation,"}, {"title": "4.2 RQ2: How does the background of a researcher relate to the way they use LLMs?", "content": "Our second research question asks how usage varies according to the demographics, background, and other contextual factors related to the researcher. In Figure 4, the first column of heatmaps shows LLM usage frequency for each high-level category of LLM usage broken down by the five background characteristics that we surveyed.\nAcross all the LLM usage categories, we find that researchers' racial identity significantly influenced the usage of LLMs, with Non-White researchers (\u03bc = 2.68, \u03c3 = 1.73) reporting more frequent usage of LLMs than White researchers (\u03bc = 2.06, \u03c3 = 1.52; Estimate = 0.616, p < .0001). Finally, we note that for the specific category of using LLMs for editing, we see significantly greater usage by NNES (Non-native English-speaking) researchers (Estimate = 0.5069, p < .0001), though this difference was not found for other categories of LLM usage, including using LLMs for direct writing."}, {"title": "4.3 RQ3: What are researchers' perceptions of LLMs for usage in different parts of the research process?", "content": "We ask participants their perceptions of the benefits and risks of LLM usage, how acceptable they perceive the use LLMs to be (i.e., ethics), and their comfort with disclosing LLM usage to peers and reviewers for each category of LLM usage using Likert rating questions. We also asked participants to elaborate on their answers via open-ended free-response questions. In the final row of columns 2-6 of Figure 4, we present the average Likert rating given by participants to different questions of perceptions broken down by category of LLM usage."}, {"title": "4.3.1 Perceptions of LLM benefits", "content": "Overall, as seen in Figure 4, participants found greater benefits from LLM usage categories of Information Seeking (\u00b5 = 3.2) and Editing (\u03bc = 3.4) compared to the remaining four categories (\u03bc \u2264 2.6). When we asked participants to elaborate on what specific benefits and usefulness they found, our analysis unveiled the following themes: language equity, other equity, efficiency, routine task assistance, search, literature review, editing, overcoming writer's block, broadening perspectives, programming, and brainstorming. Most frequently mentioned"}, {"title": "4.3.2 Perceptions of LLM risks and ethical concerns", "content": "While LLMs have risks related to the quality of research conducted, researchers could also have ethical concerns separate from issues of quality. In order to distinguish risks from ethics, we first asked participants to rate on a Likert scale their perception of risks (1: not risky at all \u2013 5: extremely risky), given known issues with LLMs today. Next, we asked their perception of the acceptability (1: Unacceptable \u2013 5: Acceptable) of using LLMs given a future where LLMs can prevent hallucinations and can always attribute any copyrighted text (if generated) to the original sources (i.e., their perception of the ethics of using LLMs).\nOverall, as seen in Figure 4, we find that researchers perceive using LLMs for Editing as not risky (\u03bc = 2.5), Direct Writing as moderately risky (\u03bc = 3), and the remaining categories as very to extremely risky (\u00b5 \u2265 3.2). In contrast when it came to ethics, researchers find Ideation & Framing (\u03bc = 2.9) and Data Cleaning & Analysis (\u03bc = 2.96) to be more on the unacceptable side, while the remaining categories were found to be more acceptable (\u03bc \u2265 3.4).\nWe asked one free-response question which gave participants the option to elaborate on risks and ethics together. Our thematic analysis of responses unveiled the following themes: hallucination and misinformation, inaccuracy, biases, lack of disclosure, plagiarism, disrespecting authorship, fabrication, decreasing creativity, pollution of the research ecosystem, decreasing diligence, and deskilling. Most frequently mentioned were hallucination and misinformation, plagiarism, fabrication, and decreasing diligence."}, {"title": "4.3.3 Comfort with disclosure", "content": "Finally, we asked participants to rate on a Likert scale their comfort with disclosing the use of LLMs along different LLM usage categories (1: uncomfortable - 5: comfortable). Overall, as seen in Figure 4,"}, {"title": "4.4 RQ4: How does the way a researcher uses LLMs relate to their perceptions?", "content": "We find that people's perceptions of risks, benefits, and ethics and their willingness to disclose usage of LLMs to the community all had a significant impact on their stated usage of LLMs (p < .0001). Figure 5 presents heatmaps showing the number of responses (log scaled) for each level of usage frequency and perception level. These values are summed across all six categories of LLM usage, i.e., each respondent is represented six times in each heatmap if they answered all questions about usage and perceptions. In addition, Table 4 in Appendix A presents the correlation between perceptions and frequency of LLM usage broken down by type of LLM usage.\nOverall, as expected, we see that greater perceived risks and greater perceived ethical concerns are associated with lower usage, and greater perceived benefits are associated with higher usage. However, perceived benefits has the strongest correlation to usage (0.62, p < .0001). As shown in Figure 5's heatmaps, some who find few risks or ethical concerns with certain categories of usage still choose to use LLMs for them infrequently or not at all, potentially because they find little benefit. Indeed, when looking at Table 4 in the Appendix, we can see overall a weaker relationship between risk and frequency (-0.401, p < .0001) and ethics and frequency (0.389, p < .0001) compared to benefits. While this is also true for every category of LLM usage, in certain categories, risks and ethics have a stronger relationship with (non-)usage, such as Direct Writing."}, {"title": "4.5 RQ5: How does the background of a researcher relate to their perceptions?", "content": "In Figure 4, columns 2-6 show heat maps related to respondents' perceptions of using LLMs across all six usage categories and broken down by respondents' background.\nAcross all LLM usage categories, a researcher's race and years of experience have a significant effect on their perception of the benefits of LLMs. Non-White researchers (\u03bc = 3.14, \u03c3 = 1.31) perceive more benefits in using LLMs for research than White researchers (\u03bc = 2.67, \u03c3 = 1.35), while junior researchers (\u03bc = 3.20, \u03c3 = 1.29) with 0-3 years of experience perceive more benefits than senior researchers (\u03bc = 2.76, \u03c3 = 1.38) with 11+ years of experience. Given our results regarding usage frequency across backgrounds (RQ2) and correlation between perceptions and usage (RQ3), this suggests that perceived utility is a primary driver of greater usage for non-White researchers and junior researchers. Similarly, we find that NNES researchers perceive greater benefits in using LLMs for Editing than native English-speaking researchers (Estimate = 0.4187, p = 0.0004), in addition to actually using LLMs for Editing significantly more as well.\nIn general, the groups who reported perceiving more benefits are some of the groups who are traditionally less advantaged in the research community: non-White researchers, non-native English speakers, and researchers with the least experience. These findings support arguments that LLM usage can potentially play a role in improving research equity, echoing our qualitative results."}, {"title": "4.5.2 Differences in perceptions of LLM risks", "content": "Overall, there were few significant differences in how people of different backgrounds perceived the risks of LLMs across all the LLM usage categories. One exception was between White and non-White researchers, where non-White researchers perceive fewer risks in using LLMs for Data Cleaning & Analysis (Estimate = -0.573, p < .0001), and Data Generation (Estimate = -0.274, p = 0.0352) than White researchers."}, {"title": "4.5.3 Differences in perceptions of the ethics of using LLMs", "content": "We find that across all LLM usage categories, a researcher's gender has a significant effect on their perception of the ethics of using LLMs. Overall, researchers who identify as women, non-binary, and other genders (\u03bc = 3.24, \u03c3 = 1.58) perceive LLM usage in research as less acceptable than those who identify as men (\u03bc = 3.6, \u03c3 = 1.47) as shown in Table 2. This suggests that ethical concerns are a major driver for lower LLM usage for women and non-binary researchers compared to men, though the differences there in terms of usage were not significant.\nWe also find differences in ethical concerns across backgrounds according to specific LLM usage categories. Similarly to perceptions of risk, we find that non-White researchers perceive fewer ethical concerns compared to White researchers for the two LLM usage categories related to data (Data Cleaning & Analysis: Estimate = 0.4059, p = 0.0027, Data Generation: Estimate = 0.3066, p = 0.0230) as well as for Direct Writing (Estimate = 0.3391, p = 0.0098). This suggests that differences in ethical concerns along with perceptions of risk and benefits may contribute to differences in White and non-White researchers' LLM usage.\nWe also see differences in ethical concerns between more junior (4-10 years of experience) and more senior researchers (11+ years of experience) for the categories of Editing (Estimate = 0.3385, p = 0.0215) and Data Cleaning & Analysis (Estimate = 0.4578, p = 0.0013), where senior researchers have greater ethical concerns. This suggests that in addition to differences in perceptions of benefits, differences in ethical concerns may drive what differences there are in LLM usage between more junior and senior researchers, though the usage differences we find are not significant.\nFinally, we find significant differences in perception of ethics across research fields for some LLM categories of usage. Computer science researchers on the whole have fewer ethical concerns than researchers in other fields. In particular, computer science researchers perceive using LLMs for Editing as more acceptable than social science & humanities researchers (Estimate = 0.5706, p = 0.0008), and they also perceive Data Generation as more acceptable than natural science & engineering researchers (Estimate = 0.5279, p = 0.0075). Despite not seeing greater usage of LLMs by computer scientists than researchers in other fields, it is possible that usage may yet be more normalized in computer science due to LLMs being a major topic of active research."}, {"title": "4.5.4 Differences in comfort with disclosure", "content": "We find that a researcher's field of research has a significant effect on their comfort with disclosing usage of LLMs to peers and reviewers. In particular, computer scientists (To peers: \u03bc = 4.07, \u03c3 = 1.39; To reviewers: \u03bc = 3.91, \u03c3 = 1.47) are more comfortable disclosing LLM usage than social science & humanities (To peers: \u03bc = 3.52, \u03c3 = 1.57; \u03a4\u03bf reviewers: \u03bc = 3.42, \u03c3 = 1.58) or biology & medicine researchers (To peers: \u03bc = 3.37, \u03c3 = 1.61; To reviewers: \u03bc = 3.21, \u03c3 = 1.65). This finding echoes our findings related to ethical concerns, which taken together further suggests greater acceptance in the computer science research community around the use of LLMs compared to other fields. Interestingly, we see no significant differences across other aspects of researcher background with regards to comfort with disclosure; indeed as mentioned earlier, people are mostly comfortable with disclosure to both peers and reviewers across all LLM categories of usage."}, {"title": "4.6 RQ6: How does the source of the LLM affect researchers' perception and usage?", "content": "Participants were split on whether the source of an LLM, (i.e., non-profit versus pro-profit entities), impacted their perception of benefits and risks. 54.81% of participants (359) reported that their perception would change depending on the source of LLM while 45.19% (296) reported no difference."}, {"title": "5 Discussion", "content": "Results from our survey showed widespread adoption of LLMs in the research community and provided detailed insights into the different ways researchers leverage LLMs in their workflows. We found that while LLMs offer the potential for enhancing equity and productivity, particularly benefiting those less advantaged in the research community, they also raise concerns about research integrity, quality, and potential homogenization of scholarly output. The varying levels of LLM usage and comfort levels with disclosure across disciplines and career stages highlight the ongoing negotiation of new social norms in academia."}, {"title": "5.1 Deep and Pervasive Integration of LLMs in Research", "content": "Our work revealed that many researchers have already found benefits in incorporating widely available LLM-based tools into their current workflow, from literature review to data analysis to writing assistance. This confirms and expands upon prior assumptions about the prevalence of LLM usage in academia [10, 31, 51, 53, 72]. While we did not set out to explore how researchers describe their relations with LLM-based research support tools, many in the free-form responses explicitly describe these tools with varying levels of autonomy and agency. More specifically, the ways participants described LLMs ranged from direct manipulation [81] (\"just another tool in the toolbox\") to data sources (\"a custom Wikipedia page\") to human-AI teaming [99] (\u201ca useful research collaborator or assistant,\u201d) to fully autonomous"}, {"title": "5.2 \"A Game-changer Leveling the Field\u201d: Equity Benefits of LLMs", "content": "An unexpected finding was the frequent mention of equity as one of the main benefits of"}]}