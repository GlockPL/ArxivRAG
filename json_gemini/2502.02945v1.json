{"title": "LLM-KT: Aligning Large Language Models with Knowledge Tracing using a Plug-and-Play Instruction", "authors": ["Ziwei Wang", "Jie Zhou", "Qin Chen", "Min Zhang", "Bo Jiang", "Aimin Zhou", "Qinchun Bai", "Liang He"], "abstract": "The knowledge tracing (KT) problem is an extremely important topic in personalized education, which aims to predict whether students can correctly answer the next question based on their past question-answer records. Prior work on this task mainly focused on learning the sequence of behaviors based on the IDs or textual information. However, these studies usually fail to capture students' sufficient behavioral patterns without reasoning with rich world knowledge about questions. In this paper, we propose a large language models (LLMs)-based framework for KT, named LLM-KT, to integrate the strengths of LLMs and traditional sequence interaction models. For task-level alignment, we design Plug-and-Play instruction to align LLMs with KT, leveraging LLMs' rich knowledge and powerful reasoning capacity. For modality-level alignment, we design the plug-in context and sequence to integrate multiple modalities learned by traditional methods. To capture the long context of history records, we present a plug-in context to flexibly insert the compressed context embedding into LLMs using question-specific and concept-specific tokens. Furthermore, we introduce a plug-in sequence to enhance LLMs with sequence interaction behavior representation learned by traditional sequence models using a sequence adapter. Extensive experiments show that LLM-KT obtains state-of-the-art performance on four typical datasets by comparing it with approximately 20 strong baselines.", "sections": [{"title": "1 Introduction", "content": "Knowledge tracing [1, 25, 38] aims to infer students' performance based on their historical question-answer records for personalized education. This technique can help teachers and education systems understand the knowledge status of students, such as their skills and forgetting behavior. By doing so, it provides more accurate teaching plans and resources. Effectively solving the knowledge tracing problem can significantly enhance the efficiency of computer-aided education.\nTraditional deep learning-based models mainly focus on modeling the interaction between questions using IDs (e.g., Question IDs or Concept IDs) to learn the sequence behavior information (See Figure 1). Sequence learning models (e.g., LSTM [11] and Transformer [34]) are utilized to capture the representation of problem-solving records [21, 23]. Nagatani et al. [19], Wang et al. [35] integrate the time factor into the model to further learn the sequence information. Additionally, to learn the relationships among questions and concepts, graph neural networks are adopted [18, 27]. These models enhance the interaction representations of an ID sequence using extra knowledge, such as time features and graph structure. However, the questions' textual information that contains rich semantic knowledge is not well explored.\nRecently, several studies have incorporated pre-trained language models (PLMs, such as BERT [7]) into knowledge tracing to model the textual information of the question [30, 32]. For instance, BiDKT"}, {"title": "2 Related Work", "content": "2.1 Deep learning-based Knowledge Tracing\nTraditional knowledge tracking algorithms are mainly based on machine learning algorithms, such as Bayesian Knowledge Tracing (BKT) [4] and Item Response Theory (IRT) [10]. With the continuous development and progress of neural networks, deep learning-based knowledge tracing algorithms have emerged to model the sequence interaction [3, 6, 23, 24]. DKT [23], or Deep Knowledge Tracing, is the first model to apply deep learning to the field of knowledge tracing, which learns the features of students' historical problem-solving records using Long Short-Term Memory (LSTM). SAKT [21] utilized the self-attention mechanism to address the problem of insufficient generalization ability existing in the processing of sparse data. AKT [9] further introduced a new monotonic attention mechanism and the classic Rasch-model in psychometrics to better understand students' knowledge mastery status and learning processes. BEKT [31] proposed a multi-layer bidirectional transformer encoder with a self-attention mechanism and bidirectional analysis, to understand the student's past learning logs. Zhang et al. [39] proposed a new structure called Dynamic Key-Value Memory Networks (DKVMN), which can utilize the relationships between underlying concepts and directly output the mastery level of each concept by students.\nTo further evaluate the time aspect, DKT-Forget [19] enhances DKT by translating the time interval into a numerical value. This value, along with learning interaction data like answering questions, is fed into the neural network. In contrast, HawkesKT [35] leverages the intensity function and mechanisms of the Hawkes process to measure the triggering effects of events across different time points. This approach clarifies how learning events temporally influence the probability of subsequent occurrences and the knowledge state. Addressing limitations in the learning process, which is vital for KT tasks, LPKT [24] assesses students' knowledge states by modeling their learning journey, capturing knowledge gains while also considering the phenomenon of forgetting. Simultaneously, Sun et al. [29] offers a novel perspective in the KT field by developing the Progressive Knowledge Tracing model. This model emphasizes the learning journey through students' sequential thought processes and divides it into three relatively independent, yet progressively advanced stages: concept mastery, question-solving, and answering behavior, effectively modeling the transition from abstract reasoning to concrete responses."}, {"title": "2.2 PLMs-Enhanced Knowledge Tracing", "content": "In the field of knowledge tracing, Pre-trained Language Models (PLMs), such as BERT [7], ROBERT [17], are used to enhance the semantic representation for knowledge tracing [13, 14, 30, 32]. For instance, BiDKT [30] adapts BERT to trace knowledge by predicting the correctness of randomly masked responses within sequences. MLFBK [14] leverages the power of BERT to mine latent relations among multiple explicit features, such as individual skill mastery, students' ability profiles, and problem difficulty. Furthermore, Tong et al. [32] proposes a hierarchical exercise feature enhanced knowledge tracing framework that utilizes BERT to generate exercise text embeddings and then feeds them into three systems (KDES, DFES, and SFES) to extract knowledge distribution, semantic features, and difficulty features. These hierarchical features are concatenated with student responses and input into a sequence model, aiming to improve knowledge tracing by comprehensively considering the diverse attributes of exercises. Moreover, LBKT [15] combines the strengths of BERT for capturing complex data relations and LSTM for handling long sequences, enhancing performance on data with over 400 interactions. However, the integration of LLMs with knowledge tracing has not been explored well."}, {"title": "2.3 Context-aware Knowledge Tracing", "content": "The context information (such as textual features in the questions and concepts) contains a wealth of semantic knowledge, which can help reduce the cold-start phenomenon of knowledge tracing. Several studies utilized the context information to enhance traditional deep learning models [22, 28]. For example, RKT [22] used the textual information of the questions to capture relations between exercises. EERNN [28] and EKT [16] considered the text of the questions to learn a good question representation for knowledge tracing. Additionally, Liu et al. [18] proposed a pre-training method called PEBG, which learns question embeddings with rich relational"}, {"title": "3 Our Proposed Method", "content": "In this section, we propose our LLM-KT framework, a novel LLM-based framework specifically designed to handle knowledge tracing tasks (See Figure 2). To align the LLMs with knowledge tracing from the task level, we design a plug-and-play instruction with specific tokens to flexibly integrate multiple modalities (e.g., long context and IDs) into LLMs. In this way, we combine the advantages of LLMs, which contain rich knowledge and strong reasoning capacities, with traditional sequence models that excel at learning sequence interaction behavior. For the modality level, we first propose a plug-in context to capture the long context of the sequence of questions using a context encoder and align the vector representations with LLMs via a context adapter. Then, we introduce a plug-in sequence to enhance the LLMs with the sequence interactive representation learned from traditional sequence models for knowledge tracing.\nKnowledge tracing involves predicting whether a student will answer a new question correctly based on their historical question-answer records. Formally, given a student's exercise history as $H = (e_1, e_2, ..., e_i, ..., e_n)$, where N is the number of historical exercises. Here, $e_i = (q_i, a_i)$, where $q_i$ represents the information of the i-th question the student answered and $a_i$ indicates the student's response to this question ($a_i = 1$ means the student answered correctly, and $a_i = 0$ means the student answered incorrectly). The goal of the task is to predict the value of $a_{N+1}$ (also defined as y) when the student answers $q_{N+1}$."}, {"title": "3.1 Plug-and-Play Instruction", "content": "To utilize the rich world knowledge and reasoning capacity, we design a Plug-and-Play Instruction for LLM-KT to guide LLMs in helping the model accurately capture changes in student learning behaviors and knowledge states. Specifically, we develop instructions with question-specific and concept-specific slots to seamlessly integrate knowledge from various modalities, including IDs and contextual information. We insert specific tokens and replace the token embeddings with representations of long context and IDs that contain rich semantic and interaction information. The Plug-and-Play Instruction organizes data into (input, output) pairings using designated tokens. The input consists of a student's prior question-answer records along with information about the current question requiring an answer. The output indicates the LLM's predicted evaluation of the student's expected answer's accuracy. Specifically, the instruction with input prompt x and output y is structured as follows:"}, {"title": "3.2 Plug-in Context", "content": "Due to the challenge of processing excessively long texts with large models, it is impractical to model the complete history of all questions. Thus, we propose a Plug-in Context $f_{cont}$ to model the textual information of question and concept. Particularly, we adopt a context encoder to model the text, and then a context adapter is used to align the representation with LLMs.\n3.2.1 Context Encoder. We leverage a context encoder to encode question and knowledge concept texts into vector representations. Here, we use a pre-trained language model (e.g., LLaMA2 [33], BERT [7], and all-mpnet-base-v2 [26]) as the context encoder to obtain the textual question representation $r_{QText} \u2208 R^{d^t}$ and concept representation $r_{QText} \u2208 R^{d^t}$, where $d^t$ is the hidden dimension of context encoder. These representations help knowledge tracing models capture the semantic relationships between questions and knowledge concepts.\n$r_{QText} = ContextEncoder(QText)$\n$r_{CText} = ContextEncoder(CText)$"}, {"title": "3.2.2 Context Adapter.", "content": "Then, we align these representations with the semantic space of the large model to ensure compatibility and effective knowledge tracing. To achieve this alignment, we design a context adapter to map the text representation learned by the context encoder to LLMs.\n$h_{QText} = ContAdapter(r_{QText})$\n$h_{CText} = ContAdapter(r_{CText})$\nwhere the $h_{QText} \u2208 R^{d_e}$ and $h_{CText} \u2208 R^{d_e}$ are the representations of question and concept after alignment. Particularly, we used a simple multi-layer perceptron (MLP) layer as a context adapter to translate the space."}, {"title": "3.3 Plug-in Sequence", "content": "Large language models have great abilities in natural language understanding and generation by training on large-scale text data. To integrate new modalities like ID sequence into LLMs, we propose a Plug-in Sequence $f_{seq}$ to better capture the interactive information. In this way, we can utilize the strengths of LLMs and traditional models to learn both semantic and interaction behavior simultaneously. Similarly, we adopt traditional models as a sequence encoder to learn the sequence representation. Then, a sequence adapter is used to convert the sequence representation into the space of LLMs."}, {"title": "3.3.1 Sequence Encoder.", "content": "We treat the sequence of IDs as a new modality to be injected into LLMs. We adapt existing traditional sequence learning models (e.g., DKT [23], AKT [9]) for knowledge tracing to learn question ID embeddings $r_{QID} \u2208 R^{d_s}$ and concept ID embeddings $r_{CID} \u2208 R^{d_s}$, where $d_s$ means the dimension of sequence encoder. These models are good at learning the sequence interaction based on the question IDs and concept IDs.\n$r_{QID} = SeqEncoder(QID)$\n$r_{CID} = SeqEncoder(CID)$"}, {"title": "3.3.2 Sequence Adapter.", "content": "However, due to the semantic space of the traditional model being quite different from that of the large model, we introduce a sequence adapter to align the ID representations from the traditional knowledge tracer with LLMs. This approach ensures better integration and enhances the performance of the knowledge tracing task.\n$h_{OID} = SeqAdapter(r_{QID})$\n$h_{CID} = SeqAdapter(r_{CID})$\nwhere $h_{QID} \u2208 R^{d_e}$ and $h_{CID} \u2208 R^{d_e}$ are the aligned representations of question ids and concept ids."}, {"title": "4 Experimental Settings", "content": "In this section, we first introduce the datasets and evaluation in Section 4.1. Then, we list the baselines in Section 4.2 and provide the implementation details in Section 4.3."}, {"title": "4.1 Datasets and Evaluation", "content": "4.1.1 Datasets. To evaluate the effectiveness of our LLM-KT, we conduct experiments on four commonly used benchmark datasets for knowledge tracing. The statistical information of these datasets is listed in Table 1.\n\u2022 ASSISTments2009 (Assist2009) [9] collects the exercises of 4151 students during the 2009 to 2010 school year. The same as Ghosh et al. [9], we use the skill builder data version of this dataset. To ensure the validity of the data, we only retain those records where both the skill_name and skill_id fields are not empty.\n\u2022 ASSISTments2015 (Assist2015) [9] comprises responses from students on 100 distinct questions. Different from Assist2009, this dataset does not provide metadata of questions.\n\u2022 Junyi Academy (Junyi) [2] is provided by Junyi Academy - the premier online learning platform in Taiwan, consisting of over 16 million exercise attempt logs. These logs are contributed by more than 72,000 students, spanning a year, specifically from August 2018 to July 2019. We use the dataset provided by bigdata ustc [2], which is processed specifically for knowledge tracing.\n\u2022 NeurIPS 2020 Education Challenge (Nips2020) [36] is released by the NeurIPS 2020 Education Challenge. In this paper, we use the datasets from Challenge Task 3 & 4 and extract the records of the top 150 most frequently appearing questions. Note that, to obtain the textual information of the questions, we convert the figures into text manually.\n4.1.2 Evaluation. Following [6, 9, 24], we use two widely-used metrics: Area Under the Curve (AUC) and Accuracy (ACC) to evaluate the effectiveness of our model."}, {"title": "4.2 Baselines", "content": "In our research, we compare our proposed approach with several strong baseline methodologies to assess its efficacy and performance. We split these baselines into four parts: deep-learning (DL)-based, pre-trained language models (PLMs)-based, context-aware and LLMs-based methods.\n4.2.1 DL-based Methods. DL-based methods learn the interactions among students' records effectively by taking the relationships and times into account. Here, we select 7 typical baselines as follows:\n\u2022 DKT [23] uses RNNs([8] to model temporal dependencies in student learning, capturing the evolution of knowledge states.\n\u2022 DKVMN [39] implements a dynamic key-value memory network, where static matrices store knowledge concepts and dynamic matrices update mastery levels, enhancing the modeling of concept relationships.\n\u2022 SAKT [21] employs a self-attention mechanism([34]) to identify key knowledge concepts (KCs) from past interactions.\n\u2022 AKT [9] utilizes a monotonic attention mechanism to build context-aware representations of student interactions, capturing performance over appropriate time scales.\n\u2022 LPKT [24] models the learning process by formalizing learning cells and incorporating gates for managing retention and forgetting over time.\n\u2022 LBKT [37] analyzes the interplay of learning behaviors (e.g., speed, attempts, hints) and uses a forgetting factor to update learners' knowledge states.\n\u2022 MRT-KT [6] employs a multi-relational transformer with a novel relation encoding scheme to model fine-grained interactions between question-answer pairs in knowledge tracing.\n4.2.2 PLMs-based Methods. PLMs-based methods improve the performance of knowledge tracing via the rich knowledge and powerful natural language understanding of PLMs. Here, we adopt the following baselines:\n\u2022 LBKT+ [15] addresses long-sequence data in knowledge tracing by integrating a BERT-based architecture with Rasch model embeddings for difficulty levels and an LSTM for sequential processing.\n\u2022 MLFBK [14] utilizes BERT to incorporate explicit features and latent relations, enhancing prediction efficiency in knowledge tracing.\n\u2022 BiDKT [30] adapts BERT for knowledge tracing by leveraging bidirectional context in interaction histories, unlike traditional RNN-based models."}, {"title": "4.2.3 Context-Aware Methods.", "content": "For context-aware methods, they utilize the context of questions to learn semantic knowledge. Particularly, we select the following four algorithms:\n\u2022 EERNN [16] combines student records and exercise content into a single vector, processed by a bidirectional LSTM, with two variants: EERNNM (Markov property) and EERNNA (Attention mechanism).\n\u2022 EKT [16] extends EERNN by using a knowledge state matrix, which captures the impact of exercises on multiple concepts, while a memory network tracks concept mastery.\n\u2022 RKT [22] uses relation-aware self-attention to integrate contextual information from exercises and performance data. It also includes a forgetting model with an exponentially decaying kernel to address interactions and forgetfulness.\n\u2022 DCL4KT-A [13] introduces a difficulty-centered contrastive learning method and leverages LLMs to optimize and predict difficulty from unseen data.\n4.2.4 LLMs-based Methods. Furthermore, LLM-based methods have good reasoning abilities with rich commonsense knowledge. We conduct four versions of LLMs based on both fine-tuning and prompting:\n\u2022 LLM-FTID finetunes the LLaMA model with instructions using QIDs and/or CIDs depending on the dataset.\n\u2022 LLM-FTTokenID finetunes the LLaMA model by treating QID and CID as specific tokens, where their embeddings are updated during training.\n\u2022 LLM-FTText finetunes the LLaMA model using textual information of questions and concepts.\n\u2022 GPT-40 inputs the same textual information as LLM-FTText directly into the GPT-40 framework.\nFor DL-based, PLMs-based, and Context-aware Methods, we only report the results from the original papers or other relevant experimental papers to ensure the reliability of the experimental results. For LLMs-based Methods, since Assist2015 and Junyi have no textual information of the question and concept, we don't provide the results of LLM-FTText and GPT-40. Note that we remove the information missed in the corresponding dataset (such as QID in Assist2015) from the prompt template in our experiments."}, {"title": "4.3 Implementation Details", "content": "In our experiments, we use the deep learning framework PyTorch Lightning for its ease of use and efficient management of training processes. Following MRT-KT [6], we divide the student dataset in a ratio of 8:1:1 for training, validation, and test sets. Then, we train on the training set, select the best model based on the validation set, and evaluate it on the test set. Our LLM-KT model is based"}, {"title": "5 Experimental Analysis", "content": "In this section, we conduct extensive experiments to evaluate the effectiveness of our LLM-KT. To be specific, we compare our model with the strong baselines in Section 5.1. Then, we explore the performance of the main components in Section 5.2 and investigate the influence of sequence length in Section 5.3. Finally, we analyze the influence of context encoder, sequence encoder, and ensemble function g in Section 5.4."}, {"title": "5.1 Main Results", "content": "In this section, we report the results of our LLM-KT and the selected baselines across four benchmark datasets in terms of AUC and ACC (Table 2). To evaluate the effectiveness of our model, we compare it with four categories: DL-based methods, PLMs-based methods, context-aware methods, and LLMs-based methods."}, {"title": "5.2 Ablation Studies", "content": "To investigate the performance of the main components contained in our proposed LLM-KT (Table 3). From the data source, we remove the ID and text of the question (- Question), and the ID and text of the concepts (- Concept) from our model. From the model structure, we remove the Plug-in Sequence (- Sequence) and Plug-in Context (- Context). Due to missing questions or concepts, we mainly conduct the experiments on the Assist2009 and Nips2020.\nFrom the results, we observe that both question and concept information can help the model understand the student's state from the history record to improve the performance of knowledge tracing. For instance, removing questions from inputs will reduce 3.08 points in terms of AUC (0.8291 vs 0.7983). Additionally, our plug-in sequence and plug-in context effectively capture the sequence interaction behaviors and long textual context. Removing any one of them from our LLM-KT will reduce the performance. The textual context helps the model learn the complex semantic relationships between the questions and concepts, and the sequence information helps the model capture the interaction behaviors based on a sequence of IDs."}, {"title": "5.3 Influence of Sequence Length", "content": "In this section, we examine how sequence length affects knowledge tracing. We present our model's results alongside several robust baselines, measured by AUC and ACC (Figure 3 and 4). We define the sequence lengths as 20, 50, or 100."}, {"title": "5.4 Further Analysis", "content": "5.4.1 Influence of Context Encoder. We investigate the influence of different context encoders and provide their average performance for both AUC and ACC (Table 5). For the context encoder, we select three typical sentence encoders: LLaMA2, BERT, and all-mpnet-base-v2 (MPNET). We input the entire question and concepts into the Context-Encoder to generate a vector roText and rCText. Consequently, we derive a single vector for each question and all associated concepts, which remains unaffected by increases in question length or the number of concepts. Specifically, the vectors are derived by averaging the hidden states from the last layer along the sequence length for the BERT and LLaMA2 models, respectively. For the MPNET model, we calculate the the vector using Sentence Transformer. The dimensions d' are 768, 768, and 4096 for BERT, MPNET, and the LLaMA2-based Context-Encoder, respectively. We develop a Context-Adapter to convert d' into the embedding layer dimension de of LLaMA2 (e.g., 4096).\nAmong the models, the LLaMA2-enhanced model achieves the highest average AUC (0.8581) and ACC (0.7865), demonstrating its superiority in capturing long context and making accurate predictions. In contrast, the corresponding model with BERT shows the lowest overall performance (AUC: 0.8443, ACC: 0.7659), particularly struggling with the Nips2020 dataset where it achieves the lowest AUC (0.8116) and ACC (0.7331). Particularly, LLaMA2 achieves a 2-point increase in ACC on average when compared to BERT (0.7865 vs 0.7659). By training on a large-scale corpus with huge parameters, LLaMA2 learns powerful text embeddings, which help LLMs acquire the long textual knowledge of the questions. For MPNET, it is fine-tuned on 1B sentence pairs using a contrastive learning objective to improve the sentence representation. Based on the experiments, we recommend using MPNET, which is the same size as BERT and achieves competitive performance to LLaMA2.\n5.4.2 Influence of Sequence Encoder. We investigate the influence of different sequence encoders and compare them with LLM-FTTokenID over four datasets (Table 4). Particularly, we employ two sequence encoders: DKT [23] and AKT [9] to model the sequence of IDs. For LLM-FTTokenID, it initiates the embedding of the question- and concept-specific token randomly and updates the embeddings by fine-tuning on the training dataset. Unlike LLM-FTTokenID, We use representations of question IDs and concept IDs learned by DKT and AKT to initiate the tokens' embeddings.\nThe findings indicate that models utilizing DKT and AKT perform better than LLM-FTTokenID, showcasing their robust ability to model knowledge tracing tasks effectively. The AKT-based model achieves over a 15-point enhancement in terms of ACC over Nips2020 when compared to LLM-FTTokenID. It suggests that the embeddings derived from traditional sequence models capture extensive semantic and interaction knowledge. Also, LLMs cannot capture the knowledge well by simply fine-tuning on sequence of IDs. Additionally, DKT and AKT obtain comparable results over these four datasets (0.8295 vs 0.8302 for average ACC).\n5.4.3 Influence of Function g. To combine the representations of context and sequence, we use a function g to merge them, as mentioned in Equation 1. In our experiment, we explore the influence of different methods on two datasets, Assist2009 and Nips2020, using AUC and ACC as performance metrics (Table 6). To be specific, we evaluate the performance of three strategies, including concatenation (Concat), average (Avg), and addition (Add). For the Assist2009 dataset, the Add strategy achieves the highest AUC (0.8870) and ties for the best ACC (0.8168) with the Concat strategy. In contrast, for the Nips2020 dataset, the Add strategy again outperforms others in both AUC (0.8291) and ACC (0.7561). When averaging the performance across both datasets, the Add strategy consistently demonstrates superior results, achieving the highest overall AUC (0.8581) and ACC (0.7865). These findings indicate that the Add strategy is the most effective method for combining features across the evaluated scenarios."}, {"title": "6 Conclusions and Further Work", "content": "In this paper, we propose a large language models-based framework for knowledge tracing (LLM-KT), leveraging the Plug-and-Play Instruction to elegantly translate the sequential tracing task into a language modeling problem. This approach incorporates multiple modalities, such as the textual information of the questions and the interaction behavior information from traditional sequence models, into the large model. Extensive experiments demonstrate that our method achieves superior performance, surpassing previous state-of-the-art results. Ablation studies confirm the effectiveness of injecting both textual information and sequence behavior information. Additionally, we analyze the impact of sequence length and the roles of the context or sequence encoders. In the future, we would like to explore alternative integration methods and alignment strategies. It is also interesting to design a more effective algorithm to capture the long-term sequence and reduce the impact of unrelated information."}]}