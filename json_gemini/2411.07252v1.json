{"title": "High quality ECG dataset based on MIT-BIH recordings for improved heartbeats classification", "authors": ["Ahmed.S Benmessaoud", "Farida Medjani", "Yahia Bousseloub", "Khalid Bouaita", "Dhia Benrahem", "Tahar Kezai"], "abstract": "Electrocardiogram (ECG) is a reliable tool for medical professionals to detect and diagnose abnormal heart waves that may cause cardiovascular diseases. This paper proposes a methodology to create a new high-quality heartbeat dataset from all 48 of the MIT-BIH recordings. The proposed approach computes an optimal heartbeat size, by eliminating outliers and calculating the mean value over 10-second windows. This results in independent QRS-centered heartbeats avoiding the mixing of successive heartbeats problem. The quality of the newly constructed dataset has been evaluated and compared with existing datasets. To this end, we built and trained a PyTorch 1-D Resnet architecture model that achieved 99.24% accuracy with a 5.7% improvement compared to other methods. Additionally, downsampling the dataset has improved the model's execution time by 33% and reduced 3x memory usage.", "sections": [{"title": "I. INTRODUCTION", "content": "Heart disease is a major global cause of death, with heart arrhythmia accounting for 16% of deaths in the last two decades (World Health Organization, 2019) [1]. ECG signals are widely used for diagnosing heart disease due to their affordability, convenience, ease of use, and precision. Support Vector Machine (SVM), Multilayer Perceptron (MLP), and Decision Tree (DT) algorithms have been commonly used in machine learning to classify and detect anomalies in ECG signals [2], [3], [4]. Recent research in machine learning (ML) has found that Convolutional Neural Network (CNN)-based automated feature extraction techniques are scalable and highly accurate in ECG signal classification. Deep learning ap-proaches, including those in [5], [6], [7], have achieved state-of-the-art results, even competing with human cardiologists in signal analysis.\nHowever, deep learning requires large amounts of data and computation and the usage of GPUs. Moreover, the performance of deep learning models is highly impacted by the quality of data. Thus, effective preprocessing techniques are crucial to enhance data quality, reduce data dimensionality, and minimize both model size and training and inference time. One of the challenges in ECG heartbeat classification is the limited number of public datasets available. The two largest publicly accessible datasets are the MIT [8] and PTB [9] datasets, which provide only continuous ECG recordings on the time domain and their corresponding labels."}, {"title": "II. MOTIVATIONS", "content": "We have found only two references that have proposed methodologies to improve the quality of the MIT recordings by creating new heartbeat datasets (Acharaya et al. [5] and Kachuee et al. [6]). Unfortunately, only [6] has made their dataset publicly available. The goal of this work is to create and publish a new high-quality heartbeat dataset based on MIT recordings. We selected the MIT dataset, due to its large size, diversity, and comprehensive annotations, compared to the PTB dataset.\nRecently, Acharya et al. [5] have segmented all the MIT dataset ECG recordings into segments of 260 samples and centered them around their R-peaks. This approach ensures that important information in each heartbeat is preserved and not mixed with those of surrounding heartbeats. However, the authors' choice of 260 samples as heartbeat size, based on a 360Hz sampling rate, is problematic. First, they have not given a method that permits estimating this value. Secondly, we found that in numerous ECG recordings from the MIT dataset, over 50% of heartbeats have a length greater than 260 samples, as illustrated in Fig. 2. This highlights the need for a more comprehensive analysis to estimate an appropriate heartbeat size that must include the maximum heartbeat signal information."}, {"title": "A. Outlier removal", "content": "From the set of RR time intervals lengths a box plot is created to visualize their distribution and identify outliers (Fig. 2).\nAs shown in Fig. 2, we have defined outliers as heartbeats with sizes that deviate significantly from the majority of the data.\nLower outliers are very short RR distances due to Analog-to-Digital Converter noise [11] when recording the signals. Most of upper outliers are included in the normal class and represent only 1.8% of total amount of data.\nThe equations to calculate upper and lower outliers are as follows:\n$UpperOutliers > Q3 + 1.5 \\times (Q3- Q1)$ (1)\n$LowerOutliers < Q1 - 1.5 \\times (Q3- Q1)$ (2)\n$Q1$ and $Q3$ are respectfully the 25th and 75th percentile.\nFig. 3 shows a sample from both lower and upper outliers.\nAbout 5200 upper outliers and 1200 lower outliers were eliminated."}, {"title": "B. Heartbeats and dataset creation", "content": "To extract heartbeats from the recordings, we first use a slope-sensitive QRS detector [12] to create a set of RR time intervals lengths from all the 48 (30-min) recordings. After that, we apply the following steps to generate the independent centered heartbeats:\n1) We retrieved the set of RR time intervals within each recording and eliminated any outlier intervals using the IQR method (see Section III-A).\n2) We split each of the cleaned recordings (after outlier removal) into 10-seconds windows."}, {"title": "C. Downsampling and Normalization", "content": "To enhance the quality of the new dataset for heart-beat classification, we used downsampling and normalization. Heartbeats are mainly classified based on the signal's shape instead of bit-by-bit details. Hence, it is better to downsample heartbeats from 360 Hz to a sampling rate of 120 Hz. This results in 150 samples per heartbeat reducing memory usage by 3 times, as well as, impacting computational time, and complexity. Additionally, the dataset have been normalized using the z-score method [13]."}, {"title": "IV. MODEL ARCHITECTURE", "content": "We train a 1-D residual convolutional neural network (ResNet) [14] for ECG heartbeat classification, as shown in Fig. 5. The network comprises 34 layers, including 3 residual blocks. Each block contains 3 ConvBlocks, a skip connection, and a max pooling layer. Each ConvBlock includes a 1-D convolutional layer, a batch normalization layer, and a swish activation function. The number of convolution channels is 128, 64, and 32 for each residual block, respectively. After the last ResBlock, an AvgPooling layer computes an adaptive average over each channel, followed by a 32-neuron dense layer that outputs 5 classes normalized by a Softmax layer. This architecture, with only 269061 trainable parameters, is efficient and suitable for deployment on resource-constrained devices.\nThe proposed architecture is relatively deep and uses residual blocks, skip connections, and batch normalization to overcome vanishing gradients [15] and improve gradient propagation, information flow, and training stability [14], [16]. Additionally, the resulting model size of only 269061 trainable parameters suggests that the proposed architecture is efficient and may be suitable for deployment on resource-constrained devices such as real-time inference on an edge platform."}, {"title": "B. Model Training", "content": "We used the newly created dataset that consits of 109494 heartbeat signals, split into 80% for training and 20% for testing. The model was trained using categorical cross-entropy loss and the ADAM optimizer [17], with a batch size of 512, accelerated by 2 Nvidia GTX 1070 GPUs. Training was performed using a custom batch generator with the Torch deep learning library [18], with no data augmentation applied."}, {"title": "V. RESULTS AND DISCUSSION", "content": "Table II shows the confusion matrix obtained by our train-ing, the total number of samples in each class is indicated as support.\nTable III represents the impact of downsampling on the model's performance and training time. Since the ECG waves of the different classes are distinguishable by their general shapes as explained in III-C. This makes downsampling a promising preprocessing technique for this task. The results show that we achieved a 1.5\u00d7 speedup in training compared to the training of the raw data and a 2% accuracy increase and reduced 3x memory usage."}, {"title": "VI. CONCLUSION", "content": "In this study, we presented a methodology for creating a high-quality heartbeat dataset based on the MIT-BIH record-ings. We eliminated outlier heartbeats using the IQR method and created an optimal heartbeat size to overcome the prob-lem of mixed heartbeats. The resulting dataset have been downsampled to accelerate training and reduce memory usage. We developed a 1-D Resnet architecture model to classify five different heartbeat types. The proposed method achieved 99.24% accuracy with 5.7% improvement compared to state-of-the-art methods [5], [6] due to the dataset. The use of outlier elimination and optimal window size selection highly contributed the quality of the dataset. The downsampling allowed for more efficient training accelerated the process by 33% and 3x less memory usage. We will publish the dataset to facilitate further research in this area. In conclusion, this study highlights the effectiveness of our deep learning approach for ECG heartbeat classification and the importance of high-quality datasets for achieving accurate results."}]}