{"title": "FedTLU: Federated Learning with Targeted Layer Updates", "authors": ["Jong-Ik Park", "Carlee Joe-Wong"], "abstract": "Federated learning (FL) addresses privacy concerns in language modeling by enabling multiple clients to contribute to training language models. However, non-IID (identically and independently distributed) data across clients often limits FL's performance. This issue is especially challenging during model fine-tuning, as noise due to variations in clients' data distributions can harm model convergence near the optimum. This paper proposes a targeted layer update strategy for fine-tuning in FL. Instead of randomly updating layers of the language model, as often done in practice, we use a scoring mechanism to identify and update the most critical layers, avoiding excessively noisy or even poisoned updates by freezing the parameters in other layers. We show in extensive experiments that our method improves convergence and performance in non-IID settings, offering a more efficient approach to fine-tuning federated language models.", "sections": [{"title": "I. INTRODUCTION", "content": "Language modeling has driven major application advancements in natural language processing like conversational agents and machine translation [1]\u2013[4]. However, the widespread use of language data raises privacy concerns, as language data from individuals is personal and prone to exploitation for malicious purposes [4]\u2013[6].\nFederated Learning (FL) addresses these concerns by training a global model without centralizing user data [7]\u2013[9]. However, one of the core challenges in FL is developing a global model that performs well across clients, whose data can vary significantly in size and distribution [10], [11]. This non-IID nature of client data often causes federated models to underperform compared to centralized models [10], [12], [13]. Language modeling has these issues because clients may exhibit distinct linguistic patterns, content, and topics, further complicating model convergence and generalization [14]\u2013[16].\nToward the end of the training, as the model nears a stationary point, stochastic local updates-amplified by data heterogeneity-create variability that makes it difficult to maintain global model consistency [17], [18]. This variability slows convergence and prevents the global model from reaching optimal performance [17]\u2013[20].\nWhile local fine-tuning helps models adapt to client-specific data [21], [22], fine-tuning only local models can lead to divergence, as each client's model becomes too specialized to its local data and tends to overfit, reducing the global model's overall effectiveness across all clients [23]. Moreover, individual clients often lack enough data for effective independent fine-tuning [24], [25]. A well-optimized global model, updated across clients, provides a stronger starting point [26]. Once aligned, local fine-tuning can enhance specific client tasks without diverging from the global objective. Therefore, FL requires a well-generalized global model to meet the collective needs of all participants. Therefore, the primary focus should be on fine-tuning the global model to ensure consistency across clients, with local tuning as an optional step for personalization.\nThis work aims to improve global model performance in FL. Uniformly updating all layers during aggregation, especially in the final stages, can amplify noise from certain clients, degrading performance and hindering convergence [27], [28]. Toward the end of training, as the model nears a stationary point and gradients shrink, noisy or non-representative updates from certain clients can disproportionately affect the global model [8].\nTherefore, we can mitigate the impact of such noisy updates by selectively updating only the most important parameter sets, such as layers, which are likely to significantly reduce global loss. However, the key challenge in this approach is determining which layers to update. Clients do not know the updates made by other clients and have limited information about the criticality of each layer. Therefore, server-side coordination is needed to aggregate client information and determine the most critical layers to update while considering the heterogeneous data distributions across clients.\nIn this context, we propose Federated Learning with Targeted Layer Updates (FedTLU). FedTLU selectively identifies and updates only the most critical layers to reduce global loss on the server side. By focusing on these key layers, FedTLU enables more efficient training, faster convergence, and improved global model performance across diverse clients. In this work, we make three primary contributions:\n\u2022 We propose a novel server-side scoring mechanism that identifies critical layers to update without relying on client-side knowledge.\n\u2022 FedTLU is designed to be robust against noisy updates, even in environments with non-representative or noisy client data.\n\u2022 FedTLU consistently outperforms random and last-layer updates, achieving better test performance up to 7.86% globally and up to 8.27% locally, particularly during the noisy fine-tuning phase.\nWe review related work in the 'Related Work' section, present the FedTLU algorithm in \u2018Methodology,\u2019 and offer theoretical support in 'Theoretical Analysis.' The 'Experimental Evaluation' validates our approach, and we summarize our findings in the \u2018Conclusion.\u2019"}, {"title": "II. RELATED WORK", "content": "In both centralized learning (CL) and FL, fine-tuning is widely recognized for improving model performance, particularly during the final stages of training. In CL, where all data is stored on a central server, fine-tuning often involves updating only a small subset of model parameters, such as layers near the output nodes [29], or even selecting layers randomly for updating [30]\u2013[32]. This selective approach improves convergence and reduces computational costs [33], [34]. Refining a subset of layers allows the model to adjust task-specific representations while leveraging previously learned features [9], [35]. These layers also can be selected based on their importance, determined through explainable neural network techniques like sensitivity or criticality scoring that assess how vital certain parameters are for generalization [36], [37]. Such methods prioritize updates where they will have the most significant impact.\nSimilarly, fine-tuning a subset of layers in FL can benefit convergence. FL can theoretically mirror CL under certain conditions, par-"}, {"title": "III. METHODOLOGY", "content": "This section introduces FedTLU, a method for selecting layers to update during global aggregation in FL, particularly for fine-tuning language models with non-IID data. The goal is to efficiently update the most critical layers or blocks to improve model convergence and generalization, especially in the final training stages.\nWe assume the usual FL training framework, in which training proceeds iteratively and in each round t, clients first compute local model updates based on their local data. These updated model parameters are then sent to the server, which aggregates them to form the new global model with parameters $W^{(t+1)}$ in each layer i.\nLayer scoring: To quantify the significance of each layer, we define a score for the weights $W_i$ in each layer i as follows:\n$Score(W_i) = \\frac{||\\Delta W||_{aggre, i}}{\\sqrt{n_i} \\cdot std(\\Delta W)}$,\nwhere $|\\Delta W||$ is the L2 norm of the difference between the parameters before round t's update ($W^{(t)}$) and the aggregated parameters after the update $W^{(t+1)}$, $n_{aggre, i}$ is the number of parameters in layer $W_i$, $std(\\Delta W_i)$ is the standard deviation of the differences in parameters in the layer.\na) Large Numerator $(||\\Delta W||)$: A large numerator indicates substantial changes between the current and aggregated parameters, suggesting that the layer is making a significant contribution to reducing the global loss [46].\nb) Small Denominator $(\\sqrt{n_i} \\cdot std(W_i))$: A small standard deviation $std(\\Delta W_i)$ reflects consistent parameter changes, indicating an effective gradient update. The factor $n_i$ normalizes by layer size, allowing for fair comparison across layers [44], [46].\nTraining algorithm: Modern deep neural networks, particularly language models, often consist of repeated blocks of layers with the same sequence of channel sizes [1], [3]. While individual layers may differ in parameters, repeated blocks typically have the same total parameter count. To leverage this structure, we group layers into blocks for comparison and updating, so that we can fairly compare blocks with similar roles. By using aggregated scores for these blocks, we account for gradient magnitude and consistency.\nAlgorithm 1 outlines the FedTLU process. In each global round (line 2), the server randomly selects a subset of clients based on the participation rate C (line 3). Selected clients receive the global model $W^{(t)}$ and perform local updates (line 6), then return their updated models to the server (line 7). The server aggregates these models (line 9) and computes a score for each layer based on the difference between current and aggregated parameters (line 10). Aggregation methods can vary, including FedAvg [7] or FedProx [8].\nBlock scores are computed by summing the scores of all layers within each block (line 11), and blocks are grouped by their parameter sequence for fair comparison (line 13). The top S blocks from each group are selected based on their scores (line 14), and the global model is updated with these blocks (line 16). This process repeats for T global rounds, focusing updates on the most important layers or blocks to improve convergence across heterogeneous clients."}, {"title": "IV. THEORETICAL ANALYSIS", "content": "In this section, we theoretically analyze why updating only a few layers of the global model during aggregations can benefit FL.\nA. Assumptions\nWe make the following assumptions in our analysis:\na) Gradient Smoothness: The gradients of the global loss function are Lipschitz continuous, implying that the global loss function"}, {"title": "C. Theoretical Comparison: Full Model vs. Subset Updates", "content": "Assuming that the gradient over the subset $W^{(t)}_S$ is not aligned with the full gradient, i.e., large \u03b4. The loss reduction is defined as:\n$\\Delta = L(W^{(t)}) \u2013 L(W^{(t+1)})$.\nFor the subset update can be expressed as:\n$\\Delta_s \\geq \\eta (||\\nabla L(W^{(t)})||^2 - \\delta) (1 - \\frac{\\eta L}{2})$.\nFor the full model update, the loss reduction is:\n$\\Delta_{All} \\geq \\eta ||\\nabla L(W^{(t)})||^2 (1 - \\frac{\\eta L}{2})$.\nGiven the loss reduction inequalities for subset and full model updates, consider the scenario where the subset update yields a greater loss reduction than the full model update. This implies that the subset update provides a tighter boundary. The inequality describing the difference between the loss reduction boundaries for the subset update and the full model update is given by:\n$-\\eta \\delta (1-\\frac{\\eta L}{2}) \\geq 0$.\nFor this inequality to hold, we require $1 - \\frac{\\eta L}{2} < 0$, which implies $\u03b7L > 2$. This condition indicates that when the Lipschitz constant L is large, even a relatively small learning rate \u03b7 causes the loss function to behave less smoothly, meaning the updates may become noisy. In such cases, deviating from the full model with a large \u03b4 update can be beneficial for a more efficient loss reduction."}, {"title": "V. Experimental Setup and Evaluation Metrics", "content": "We trained two language models for next-word prediction: a standard Transformer [1] on UDPOS data and GPT-2 [3] on Penn Treebank (Penn TB) for 150 communication rounds. The sequence length was set to 128. Both models were optimized with Stochastic Gradient Descent (SGD) at a learning rate of 0.0001 and batch size of 16, with each client performing 5 local epochs across 100 clients. We used FedAvg [7] and FedProx [8] (proximal term \u03bc = 0.1) for aggregation. Experiments were conducted on a single NVIDIA GeForce RTX 2080 Ti.\nThe network architectures consist of the repeated (main body) and non-repeated parts (input and output layers). Four update strategies were evaluated: Full, FedTLU, Random, and Last updates. In the Full update, all layers were updated in each round. For Random and FedTLU, updates were applied to the non-repeated part and a portion of the main body layers, with FedTLU selecting layers based on scores and Random selecting layers randomly. The Last update case involved updating only the last few layers of the non-repeated part. Each case was evaluated three times, and the average perplexity, which measures prediction accuracy (lower is better), was reported. Global testing perplexity was calculated after each aggregation using test sets from the datasets, while local testing performance was measured by averaging perplexity scores from participating clients after the local training using the same test sets."}, {"title": "B. Experimental Results", "content": "We compared FedTLU's performance under three conditions: 1) training the global model from scratch, 2) fine-tuning pre-trained models, and 3) scenarios with noisy or potentially malicious clients. For training from scratch, each communication round varied the portion of updated layers (75%, 50%, and 25%). Each local client was allocated up to a maximum number of tokens, calculated by dividing the total dataset size by the number of clients. A minimum number of tokens is half of this maximum number. Local datasets were non-overlapping, and 10% of clients participated per round."}, {"title": "VI. CONCLUSION", "content": "We introduced FedTLU, a targeted layer update strategy for federated learning (FL). FedTLU improves convergence and reduces the impact of noisy client updates. Experimental results show that FedTLU outperforms random and last-layer updates, providing a more efficient and stable solution for FL in language modeling."}, {"title": "VII. ACKNOWLEDGMENT", "content": "The authors are partially supported by the National Science Foundation under grants CNS-2409138 and CNS-2106891 and by the Department of Energy under grant DESC0025652."}]}