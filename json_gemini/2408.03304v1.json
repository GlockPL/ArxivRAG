{"title": "Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks", "authors": ["Rafael Sterzinger", "Christian Stippel", "Robert Sablatnig"], "abstract": "Etruscan mirrors constitute a significant category in Etruscan art, characterized by elaborate figurative illustrations featured on their backside. A laborious and costly aspect of their analysis and documentation is the task of manually tracing these illustrations. In previous work, a methodology has been proposed to automate this process, involving photometric-stereo scanning in combination with deep neural networks. While achieving quantitative performance akin to an expert annotator, some results still lack qualitative precision and, thus, require annotators for inspection and potential correction, maintaining resource intensity. In response, we propose a deep neural network trained to interactively refine existing annotations based on human guidance. Our human-in-the-loop approach streamlines annotation, achieving equal quality with up to 75% less manual input required. Moreover, during the refinement process, the relative improvement of our methodology over pure manual labeling reaches peak values of up to 26%, attaining drastically better quality quicker. By being tailored to the complex task of segmenting intricate lines, specifically distinguishing it from previous methods, our approach offers drastic improvements in efficacy, transferable to a broad spectrum of applications beyond Etruscan mirrors.", "sections": [{"title": "1 Introduction", "content": "With more than 3,000 identified specimens, Etruscan hand mirrors represent one of the biggest categories within Etruscan art. On the front, these ancient artworks feature a highly polished surface, whereas, on the back, they typically depict engraved and/or chased figurative illustrations of Greek mythology [5]. A primary component of their examination involves the labor- and cost-intensive task of manually tracing the artworks; an exemplary mirror is illustrated in Fig. 2 together with the sought-after tracing."}, {"title": "2 Related Work", "content": "Segmentation: In the field of image segmentation, techniques are led by advanced deep learning architectures such as the UNet [12], DeepLabV3++ [2], Pyramid Attention Network [7], etc. These advancements are particularly propelled by industries where precise segmentation is paramount: For example, in medical imaging, intricate segmentations are crucial for identifying vascular structures within the retina, a crucial aspect for diagnosing retinal diseases [8].\nPhotometric Stereo: When considering historical artifacts where the content of interest is engraved or chased into the object, as is the case with Etruscan mir-"}, {"title": "3 Methodology", "content": "In the following we will detail our methodology comprised of:\n- the dataset; general information, splitting the data into training, validation, and testing, as well as, the preprocessing of depth maps\n- the simulation of human interaction; details on the statistics of engravings, acquiring individual line segments, and the procedure for adding and erasing\n- the architecture; describing the overall deep neural network used for refining the initial segmentation"}, {"title": "3.1 Dataset", "content": "Our dataset includes a diverse array of Etruscan mirrors from public collections in Austria. It consists of PS-scans of 59 mirrors, with 53 located at the Kunsthistorischen Museum (KHM) Wien and the remaining 6 scattered throughout Austria. Annotations were acquired for 19 mirrors, encompassing 19 backsides and 10 fronts, resulting in a total of 29 annotated examples. Notably, engravings predominantly adorn the backside to avoid interference with reflectance, however, they are also occasionally found on the front, albeit with less density, near the handle or around the border. For information on the acquisition process, we refer the reader to Sterzinger et al. [13].\nDividing these annotations into training, validation, and test sets is chal- lenging due to three factors: limited sample size, strong variations in the density"}, {"title": "3.2 Simulation of Human Interaction", "content": "In order to simulate realistic human interactions, we first look into the statistics of the annotations included in the dataset; necessary to quantitatively capture human-stroke width. Next, to refine initial predictions, we describe the process of filtering and correcting false positives and negatives. Within this, we motivate and denote the algorithm used to extract line segments. Tying all components together, we finally describe simulating human interaction: Starting from either false positives or negatives, we extract the largest error segment and provide a hint in the form of a line with width taken from the acquired statistics of the ground truth annotations.\nStatistics With the goal of simulating realistic interaction, one crucial compo- nent to consider is the stroke width. For this, we look into the statistics of the annotations included in our dataset by extracting individual thickness, using Algorithm 1: Starting from a binary mask, the ground truth $Y^*$ in our case, we obtain distance information via the euclidean_distance_transform which, for each pixel, returns the Euclidean distance in pixels to the closest non-mask"}, {"title": "3.3 Architecture", "content": "With regards to our architecture, we employ a UNet [12] with an EfficientNet- B6 [15] following the proposal by Sterzinger et al. [13] but expand upon the input to condition the network on the (simulated) human input \u2206. For clarification, the input is now comprised of a 3 \u00d7 H \u00d7 W tensor, including the depth map X, the human input A, as well as the initial prediction Y with all three quantities concatenated. Given that our data resources are limited, we train on a per- patch-level employing augmentations among which are rotations, flips, and shifts, optimizing the Dice loss. For the initial prediction Y, we employ the exact same methodology as proposed by Sterzinger et al. [13]."}, {"title": "4 Evaluation", "content": "In this section, we evaluate our design choices: During this process, we report the Intersection-over-Union (IoU) as well as the pseudo-F-Measure (pFM), a metric commonly used for evaluating the binarization quality of handwritten documents. It is thus well-suited for our binarization task, i.e., a task where shifting the mask by a single pixel will have a significant impact on per-pixel metrics. Compared to the standard F-Measure, the pFM relies on the pseudo- Recall (p-Recall) which is calculated based on the skeleton of $Y^*$ [11]:\n$$pFM(Y', Y^*) = \\frac{2 \\times p-\\text{Recall}(Y', Y^*) \\times \\text{Precision}(Y', Y^*)}{p-\\text{Recall}(Y', Y^*) + \\text{Precision}(Y', Y^*)}$$\nGiven that we work within an interactive paradigm, we are required to also provide a metric that excludes the human input A from the evaluation and hence report the relative pFM improvement over $Y^\\Delta$, i.e.:\n$$PFM_A(Y', Y^\\Delta, Y^*) =  \\frac{pFM(Y', Y^*) \u2013 pFM(Y^\\Delta, Y^*)}{pFM(Y^\\Delta, Y^*)}$$\nIn addition, based on the fact that during training we introduce random- ness, i.e., by chance, missing parts can be added ($A^+$) or superfluous ones erased ($A^-$), and that sub-segments are sampled and dilated at random, we evaluate on the test/validation set five times and report the average."}, {"title": "4.1 Training", "content": "Our model $f_{\\text{iter}}$ is trained on an NVIDIA RTX A5000 until convergence, i.e., no improvement > 1e - 3 w.r.t. the pFM, (see Equation 3) for ten consecutive epochs, using a batch size of 32 and a learning rate of 3e - 4. As a loss function, we employ a generalized Dice overlap (Dice loss) that is well suited for highly un- balanced segmentation masks [14] and optimize it using Adam [3]. Additionally, we incorporate a learning rate scheduler that also monitors the $pFMA$ on our validation set: If there is no improvement for three consecutive training epochs, the learning rate is halved."}, {"title": "4.2 Ablation Study", "content": "In the following, we present our ablation study, focusing on input options, dif- ferent stroke widths (widths kept fixed and sampled randomly), as well as the necessity of our two operations (add and erase).\nInput Options: Starting with the evaluation of different input options and their impact on the predictive patch-wise performance of $f_{\\text{iter}}$ (fixed stroke width, one interaction; results are denoted in Table 1): As expected, simply iterat- ing over the initial prediction Y (stemming from network $f_{\\text{init}}$) results in no improvement, rendering the human an essential part of the refinement process."}, {"title": "5 Results", "content": "After verifying the effectiveness of our methodology, we pick the three mirrors from our validation/test set, namely ANSA-1700, ANSA-1701, and Wels-11944, and evaluate our human-in-the-loop approach on whole mirrors, performing mul- tiple interactions (limited to 3,000; typically requiring much less). Again, due to the introduced randomness, we repeat this process ten times and report the av- erage result, skipping the variation as it is negligible. As described in Section 4.2, for this, we start greedily by selecting the patch with the lowest pFM, simulate adding missing/erasing superfluous parts, selecting the operation which yields a"}, {"title": "6 Limitations and Future Work", "content": "While our proposed method shows promising results, it is important to acknowl- edge its limitations: At the moment human guidance aids refinement only lo- cally, i.e., modifications happen just in the vicinity of the provided annotation. Moving forward, one could focus on further refining our methodology by explor- ing additional techniques to enhance efficiency. For instance, it would be mean- ingful to investigate the integration of quickly trainable learning algorithms, such as Gaussian processes which can immediately be adapted to newly provided an- notation and thus allow for global adjustments, potentially further reducing the amount of human input required. Additionally, leveraging Gaussian processes is accompanied by the option of active learning strategies, which could allow the identification and annotation of patches where the model is most uncertain with the chance of expediting refinement further."}, {"title": "7 Conclusion", "content": "In summary, our research addresses the labor-intensive process of manually trac- ing intricate figurative illustrations found, for instance, on ancient Etruscan mir- rors. In an attempt to automate this process, previous work has proposed the use of photometric-stereo scanning in conjunction with deep neural networks. By doing so, quantitative performance comparable to expert annotators has been achieved; however, in some instances, they still lack precision, necessitating cor- rection through human labor. In response to the remaining resource intensity, we proposed a human-in-the-loop approach that streamlines the annotation process by training a deep neural network to interactively refine existing annotations based on human guidance. For this, we first developed a methodology to mimic human annotation behavior: We began by analyzing annotation statistics to capture stroke widths accurately and proceeded by introducing algorithms to select erroneous patches, identify false positives and negatives, as well as cor- rect them by erasing superfluous or adding missing parts. Next, we verified our design choices by conducting an ablation study; its results showed that provid- ing human guidance will yield improvements exceeding pure manual annotation, utilizing both operations is beneficial, and augmenting stroke widths by ran- dom sampling performs worse than leaving it constant. Finally, we evaluated our method by considering mirrors from our test and validation set. Here, we achieved equal quality annotations with up to 75% less manual input required. Moreover, the relative improvement over pure manual labeling reached peak val- ues of up to 26%, highlighting the efficacy of our approach in reaching drastically better results earlier."}]}