{"title": "Open-set object detection: towards unified problem formulation and benchmarking", "authors": ["Hejer Ammar", "Nikita Kiselov", "Guillaume Lapouge", "Romaric Audigier"], "abstract": "In real-world applications where confidence is key, like autonomous driving, the accurate detection and appropriate handling of classes differing from those used during training are crucial. Despite the proposal of various unknown object detection approaches, we have observed widespread inconsistencies among them regarding the datasets, metrics, and scenarios used, alongside a notable absence of a clear definition for unknown objects, which hampers meaningful evaluation. To counter these issues, we introduce two benchmarks: a unified VOC-COCO evaluation, and the new OpenImages Road benchmark which provides clear hierarchical object definition besides new evaluation metrics. Complementing the benchmark, we exploit recent self-supervised Vision Transformers [22] performance, to improve pseudo-labeling-based OpenSet Object Detection (OSOD), through OW-DETR++. State-of-the-art methods are extensively evaluated on the proposed benchmarks. This study provides a clear problem definition, ensures consistent evaluations, and draws new conclusions about effectiveness of OSOD strategies.", "sections": [{"title": "Introduction", "content": "Object Detection (OD) is a fundamental task in several applications of computer vision. Usually, corresponding models are trained to detect a closed set of object classes. However, in real life, they may encounter objects from new classes not seen or not labelled in the training dataset. We call these objects unknown or Out Of Distribution (OOD). In some applications, where safety and confidence are key, not confusing these objects with the known classes or even being able to detect and recognise them as unknowns is crucial. For example, an undetected or a misclassified unknown object can be dangerous for self-driving cars.\nWhile OOD-OD approaches [4,5] aim to correctly classify already detected objects as a known class or unknown, two other related topics focus also on the ability of the model to localise the unknowns. First, OpenSet Object Detection (OSOD) approaches [10,32] seek to actively localise all unknown objects without falsely categorising them as known classes. Second, Open-World Object Detection (OWOD) methods [8, 14, 20, 27] try to handle a continuously expanding world of object classes. At different time steps, the model receives new sets of known"}, {"title": "Related Work", "content": null}, {"title": "Background", "content": "Acquiring a confident object detector that is aware of the unknown is tackled by multiple methods under different settings and different names.\nOOD | Out-Of-Distribution detection was firstly associated with image classifi-cation. Latest approaches include post-hoc methods such as KNN with threshold-based criterion [26] or rectifying output activations at an upper limit [25]. Other methods are ad-hoc and require specific re-training of the model. It includes modifying the loss function [28], ensembling and scoring rules [17] or even us-ing external unlabelled datasets [29], GAN-generated images [15,18] or mixup augmentations [12] to represent the unknown. Naturally, these methods can be applied to OD, during the classification process. Detected objects are simply reclassified as known or unknown. Some approaches propose combining both custom loss and data generation in the feature space [5], while others [4] add a distance-based loss in the feature domain. However, those methods do not seek to improve the model's ability to localise unknown objects.\nOSOD | Open-Set Object Detection approaches aim to both localise and cor-rectly classify the knowns and the unknowns. In this setting, the questions \"what do we consider as unknown objects?\" is fundamental. In the literature, this prob-lem is often ill-posed as some works define knowns and unknowns as instances from the same dataset [30], whereas others [24] argue that this is the matter of class definition and not differences in the datasets. On the other hand, [31] groups unknowns into sub-groups of related unknown objects. Finally, [13] de-fines the unknowns as all classes under a known super-class. However, it only considers one super-class per dataset, which is usually not the case in real-world applications. Whatever the problem definition, OSOD approaches [10,32] pro-pose custom loss functions and contrastive clustering for better known-unknown separability. Specifically, OpenDet [10] identifies unknowns by differentiating be-tween high and low-density regions in the latent space. It promotes compact fea-tures for known classes, thereby expanding low-density regions for unknowns. It also optimises the unknown probability based on the uncertainty of predictions.\nOWOD | Open-World Object Detection is a setting introduced in ORE [14], where the goal is to deal with expanding sets of classes through time. Instead of just localising and identifying known and unknown objects, OWOD systems aim also to continuously learn and incorporate new classes as they encounter them, without forgetting the old ones. First, ORE [14] adapts Faster-RCNN [23] to the OWOD setting by integrating contrastive clustering and employing an automatic pseudo-labeling mechanism, leveraging RPN proposals with the high-est objectness scores for potential unknown objects. Yet, this approach requires some labeled unknown samples for proper calibration, which violates the OOD hypothesis. OW-DETR [8] develops the idea of pseudo-labeling by using the most activated feature map regions as unknown objects. MAVL [20] exploits the capacities of multi-model ViTs trained with aligned image-text pairs to create a class agnostic object detector which pseudo-labels the unknowns resulting from"}, {"title": "SOTA inconsistencies and problem ambiguities", "content": "Methods inconsistencies: Studying the aforementioned methods, we found a lot of nuances which can effect the models performances and usability in real-world conditions. First, the settings and goals of OOD-OD, OSOD and OWOD approaches are different such as explained in sec. 2.1. Moreover, some methods use an external supervision on other datasets, which can violate the open-set setting making the comparison unfair with other fully unsupervised methods on the unknowns. In addition, ORE [14] requires labeled unknown data for calibration (Unk. label), while several methods need a calibration on a validation set to function optimally (Pre-calibration). Furthermore, to detect the unknown, some methods are based on pseudo-labeling, where others rely on re-estimating detections using a learned distribution. Finally, note that there is no comparison along all previous SOTA approaches using the same benchmarks.\nThe definition of training scenarios: The use of different benchmarks results in the non-separation between the unlabeled vs. the unseen scenarios. In fact, while for some settings unknown objects can be present in the training dataset but not labeled, they are completely absent for others. [3] noticed that the VOC-COCO benchmark is part of the unlabeled scenario which they called mixed unknowns but they do not propose nor benchmark on a totally unseen scenario.\nThe definition of unknown objects: Another big ambiguity lies in the def-inition of unknown objects. In fact, for supervised OD, the goal is to detect a well defined set of classes both present and labeled in the training dataset. How-ever, the term unknown can encompass all entities that fall outside the realm of the recognized or the known. In this case, the distinction between foreground and background (e.g. are trees and buildings considered as unknown objects or background) and between objects and their parts (e.g. do we want to de-tect a person as one object or to detect the different body parts and clothes as different objects) is unclear. The task of deciding what to detect and what"}, {"title": "New Problem Definition and Unified Benchmarks", "content": "In this section, we propose new unified benchmarks, provide scenarios separation and adapt the [13] problem definition to richer contexts to consistently evaluate SOTA OSOD and OWOD approaches in the same OSOD framework, as OWOD methods can be also considered as OSOD approaches with a supplementary option i.e. dealing with an expanding world of new classes through time.\nSimilarly to usual supervised OD methods, we define the OSOD training dataset as $D_{train} = \\{(X_i, Y_i), x_i \\in X, Y_i \\in Y\\}$, where $x_i$ is an input image and"}, {"title": "New unified OSOD benchmark", "content": "While a significant number of OWOD and OSOD approaches use a combination of Pascal-VOC [6] and MS-COCO [19], they each introduce unique splits and protocols, preventing methods comparison. Recognizing this issue, we first pro-pose a consolidated split that introduces minimal deviations from the original VOC and COCO configurations. To our knowledge, we are the first to benchmark latest OWOD and OSOD methods on a singular platform. We advocate for our approach as a foundational step towards unifying methodologies and ensuring consistent evaluation. To evaluate the known and unknown detection capacities separately and jointly, our evaluation is segmented into three distinct splits:\n$D_{test,ID} = \\{x_i,\\forall C_j \\in Y_i, C_j \\in K\\}$ is the set of test images containing only known objects (pure ID);\n$D_{test,OOD} = \\{x_i, \\forall C_j \\in Y_i, C_j \\in U\\}$ is the set of test images containing only unknown objects (pure OOD);\n$D_{test,all} = D_{test}$ is the set of all test images, containing known, unknown, or both kinds of objects.\nFor training, we used the usual VOC trainval split such as [3], which has 20 classes in 14K images. For testing, we used both VOC and COCO test sets while adopting the aforementioned splits for finer analysis. In fact, COCO dataset encompasses the same 20 classes from VOC along with additional 60 non-VOC classes. These are aggregated under a single unknown label. For this benchmark, $D_{test,ID}$ is the standard VOC test split, $D_{test,all}$ is the combination of VOC and COCO test splits, and $D_{test,OOD}$ contains only COCO images where no object of the 20 known classes is present."}, {"title": "Training scenarios", "content": "While studying unknown detection settings, two different scenarios can be de-fined, such as presented in tab. 2:"}, {"title": "OpenImagesRoad: A richer unseen hierarchical benchmark", "content": "Hierarchical object definition: We think OSOD-III [13] is a step in the right direction as it defines an object as all classes belonging to a same single known super-class. Similarly, we employ a hierarchical approach to define what an ob-ject is, but extend the idea to multiple super-classes that can be present in a single dataset. We believe that such hypothesis is practical, realistic and holis-tic for contemporary applications such as autonomous driving where a wide range of super-classes are encountered and where unknown objects should be handled appropriately. In this case, we consider that the dataset is hierarchi-cally annotated; each class can be categorised under one super-class. We define $SC = \\{SC_i,0 < i < L\\}$ as the set of super-classes in the dataset. We divide each super-class $SC_i$ into a subset of known and unknown classes. In this case, the goal is to accurately detect all known instances, and all novel objects within predetermined super-classes. Every other detection that falls outside the defined $SC$ will be considered as a false detection. This rich and clear definition of what we want to detect allows a consistent evaluation of the unknown detection abil-ities. In practice, we have chosen the OpenImages dataset as a base for our own evaluation protocol thanks to its annotation and class hierarchy definition.\nOpenImages [16]: is a large dataset with 1.9M images. The classes are struc-tured in a hierarchical manner, resembling a tree, with secondary classes branch-ing out of primary super-classes. However, such as demonstrated by BigDetec-tion [1], the initial OpenImages dataset presents several problems such as over-"}, {"title": "Metrics", "content": "While $AP_{unk}$ offers a great insight into the model performance to accurately detect the unknown in a well defined benchmark such as OpenImages Road, it can hide some model characteristics by combining the localisation and classification tasks. For example, if a model localises well all unknown objects but fails to classify them as unknown $AP_{unk} = 0$. To provide a finer analysis, we propose two new metrics.\nFirst, we adopt a class agnostic average precision as a localization-focused metric which we call $AP_{all}$. It evaluates the localisation performances of the"}, {"title": "Can We Improve Pseudo-labeling OSOD?", "content": "Pseudo-labeling approaches are an efficient way to approach OSOD and OWOD. We find them particularly useful for the simple knowledge distillation they allow from self-supervised models. However, many pseudo-labeling techniques such as ORE [14] or MAVL [20], are based on supervised learning paradigms, requir-ing additional external annotated data which is out of the scope of the work presented here. Moreover, these methods are constrained by the narrow defini-tion of what constitutes an object, derived primarily from the labeled data they have been trained on. On the other hand, in recent years, self-supervised Vi-sion Transformers (ViT) demonstrated tremendous capacities in representation learning without any prior annotated knowledge.\nIn this section, we present our efforts to push the capacities of the SOTA OW-DETR [8] method by leveraging DINOv2 [22] self-supervised features for better pseudo-labeling. Because we consider that our contribution is a refinement of OW-DETR, we call the resulting methods OW-DETR+ and OW-DETR++."}, {"title": "Background: OW-DETR [8]", "content": "OW-DETR | Open-World Detection Transformer [8] is an adaptation of the object detector D-DETR (Deformable DETR) [33] tailored for OWOD. At the core of the D-DETR framework, the multi-scale features of an image are"}, {"title": "Pseudo-labeling with Self-supervised ViT", "content": "OW-DETR+ | In order to improve OW-DETR, by leveraging pre-trained ViT features, a first step is to replace Resnet activation maps used for pseudo-labeling by DINOv2 [22] activation maps. We call this variant OW-DETR+. Note that DINOv2 is only used for the activation map extraction for the unknown pseudo-labeling while ResNet continues to be the backbone of the detection method.\nOW-DETR++ | To further improve OW-DETR+, we propose the novel pseudo-labeling pipeline illustrated in fig. 1. We first propose to filter out the back-ground. Given an input image passed through the DINOv2 backbone, we obtain a feature map $F \\in R^{H\\times W\\times d}$, where each pixel is represented by a d-dimensional feature vector. Subsequently, we apply DBSCAN [9] with predefined eps and min_samples on these features since it can automatically assign the number of clusters. This algorithm provides a great ability to separate between the fore-ground and the background regions. Hence, we filter out the biggest cluster, likely to be the background, especially for OD benchmarks where usually sin-gle objects are not omnipresent in the image. Second, we aim to find different objects in the image by localising the regions having similar and homogeneous semantics. However, DBSCAN offers very coarse semantic clusters. Thus, we ap-ply agglomerative clustering on the same features, refine the clusters boundaries and minimize noisy representations using morphological operations such as ero-"}, {"title": "Experiments", "content": null}, {"title": "Datasets and evaluation metrics", "content": "We deliver results on the two proposed benchmarks VOC-COCO and OpenIm-agesRoad, for the three test splits such as described in sections 3.1 and 3.3.\nFor known classes, we report the standard mean average precision (mAPk) on both benchmarks. For unknown classes, we only report A-OSE and U-Recall on the VOC-COCO benchmark, since unknown objects are not clearly defined. We then report U-Recall, $AP_u$, $AP_{all}$ and $AP_{sc}$ (when applicable) on the Open-ImagesRoad benchmark, thanks to the unknown object definition."}, {"title": "Implementation Details", "content": "To deliver a consistent and fair evaluation, we use the official implementations of all tested methods. To maintain fairness, for ORE [14], we calibrated the distribution exclusively on $D_{train}$ with only labeled known objects. For MAVL [20], we use the public pre-trained model for pseudo-labeling our dataset, and then use these pseudo-labels to train ORE."}, {"title": "VOC-COCO benchmark", "content": "We provide a performance evaluation of previous SOTA OSOD and OWOD methods, alongside the different contributions that we brought to OW-DETR on the proposed VOC-COCO benchmark in tab. 3.\nFor the known objects, OpenDet outperforms all methods with the highest mAPk. Note that, for all benchmarked methods, mAPk is higher for $D_{test,ID}$ than $D_{test,all}$, where the presence of both known and unknown objects may lead to more confusion.\nHowever, the VOC-COCO benchmark makes it difficult to extract interesting conclusions regarding the unknown detection. Indeed, a high U-Recall only indi-cates that unknown predictions have been successfully placed over unknown ob-jects. However, it does not indicate proper detection performance, which should be measured as a recall-precision trade-off for different confidence thresholds.\nA-OSE is a first step to measure this trade-off, but only covers classification performance. Since both metrics are computed at fixed confidence thresholds,"}, {"title": "OpenImagesRoad benchmark", "content": "We compare in tab. 4 performances of OpenDet [10], RandBox [27] and OW-DETR [8] alongside its different improvements. ORE is unreported as it shows poor performance in unknown detection in tab. 3. For fairness, ORE (MAVL [20]) is also unreported as it uses external supervision to extract pseudo-labels.\nInteresting performance trends emerge depending on the nature of each method. Indeed, pseudo-labeling methods such as RandBox, OW-DETR, OW-DETR, OW-DETR+ and OW-DETR++ all have lower $AP_u$ than the contrastive method OpenDet which indicates that contrastive methods seem to learn a broader unknown representation which is good for classification. On the other hand, OW-DETR++ have higher $AP_{all}$ and $AP_{sc}$ than OpenDet, demonstrating that this pseudo-labeling based method seems more apt for object localization. The higher $AP_{sc}$ also indicates a good ability of pseudo-labeling methods to understand high granularity classification but at the same time, an inability to understand smaller grained differences between known and unknown as shown by the lower $AP_u$. The surprising drop between $AP_u$ and $AP_{all}$ for OpenDet can be explained by frequent double detections of unknown objects: as unknown and known objects. Furthermore samples of all methods are included in the appendix.\nOur proposed pseudo-labeling improvements on OW-DETR, namely OW-DETR+ and OW-DETR++ bring impressive performance gains. Note that no knowledge on unknown objects is transferred from DINOv2 since it is only used"}, {"title": "Conclusions", "content": "In this work, we propose a new unified OSOD problem formulation and bench-marking. We introduced the OpenImagesRoad hierarchical dataset which per-mits OSOD performance evaluation in realistic, well defined conditions. We also proposed the unified VOC-COCO benchmark for fast, although limited, benchmarking of OSOD and OWOD methods. We brought improvements to the pseudo-labeling baseline OW-DETR, namely OW-DETR+ and OW-DETR++, and compared these methods alongside main SOTA approaches. Both new bench-marks and methods evaluation allowed us to shine new light on OSOD perfor-mance. First, pseudo-labeling and contrastive methods each have advantages and drawbacks, therefore the choice of the method depends on the chosen use-case. Second, methods are sensitive to the learning scenario with pseudo-labeling methods thriving in Sunlabeled scenarios when compared to Sunseen."}]}