{"title": "Declare and Justify: Explicit assumptions in AI evaluations are necessary for effective regulation", "authors": ["Peter Barnett", "Lisa Thiergart"], "abstract": "As AI systems advance, AI evaluations are becoming an important pillar of regulations for ensuring safety. We argue that such regulation should require developers to explicitly identify and justify key underlying assumptions about evaluations as part of their case for safety. We identify core assumptions in AI evaluations (both for evaluating existing models and forecasting future models), such as comprehensive threat modeling, proxy task validity, and adequate capability elicitation. Many of these assumptions cannot currently be well justified. If regulation is to be based on evaluations, it should require that AI development be halted if evaluations demonstrate unacceptable danger or if these assumptions are inadequately justified. Our presented approach aims to enhance transparency in AI development, offering a practical path towards more effective governance of advanced AI systems.", "sections": [{"title": "1 Introduction", "content": "The rapid pace of AI development has prompted demands for regulation to help safeguard against novel risks, including catastrophic risks [1]. This regulation should aim to prevent harms caused by malicious actors misusing AI systems, as well as large-scale accident risks caused by autonomous Al systems acting in misaligned ways [2-4].\nToday's frontier AI systems are not created by understanding and implementing specific capabilities; they are instead iteratively shaped through a training process that encourages instrumental capabili-ties to emerge. Consequently, AI developers do not know what their systems will be capable of until they test them \u2014 and sometimes not even then. As OpenAI CEO Sam Altman said about predicting capabilities [5], \u201cUntil we go train that model, it's like a fun guessing game for us.\"\nMajor AI developers [6\u20138] have put forward plans for safety based on AI evaluations [9, 10] that attempt to assess a model's capacity to facilitate dangerous activities such as hacking [11, 12], bioweapons design [13, 14], and human manipulation [15, 16]. Governments are requiring that AI developers provide them access to models for testing [17, 18]. Clearly, much depends on these evaluation efforts, especially for avoiding potentially catastrophic risks.\nBut safety cases based on Al evaluations rest on many underlying assumptions about the scope and limitations of testing that may not have been adequately interrogated. Previous work discusses various limitations to AI evaluations [19\u201322]. In this paper, we identify key assumptions we argue should be stated and justified by developers as part of any safety plan or regulatory effort."}, {"title": "2 Current AI evaluations workflow", "content": "AI evaluations form a large component of AI developer safety plans, such as the Anthropic Re-sponsible Scaling Policy [6], the OpenAI Preparedness Framework [7], and the Google Deepmind Frontier Safety Framework [8]. These seek to estimate whether current models have dangerous ca-pabilities that could lead to catastrophic harm, and to predict if future models will. In this paper, we distinguish between existing and future models because the capabilities of future models can only be inferred from those of existing models which can be directly interacted with. These capability evaluations can include \u201chuman uplift\" studies, where the capability being measured is the ability to assist humans at harmful tasks [23, 24].\nFor evaluating existing models, the process is:\n1. Assess threat vectors via which the AI system could cause harm.\n2. Design proxy tasks which estimate the system's ability to exploit these threat vectors.\n3. Attempt to get the model to do these proxy tasks.\n4. If a model can do these proxy tasks, trigger an action such as don't release the model or don't continue training the model without first resolving the risk.\nAn implicit assumption is made that if evaluators are unable to make a model perform well on the proxy tasks, then it is unlikely to have dangerous capabilities, and therefore will be safe to deploy.\nDevelopers today acknowledge that some AI systems may not be safe to even create; for example, an inadequately secured model could be used to cause harm if stolen, or a model that has capabilities allowing it to break out of its containment could act autonomously. For forecasting and preventing risks from future models, it appears from the developer safety plans that the standard process is:\n1. Assess threat vectors via which future AI systems could cause harm.\n2. Determine precursor capabilities that would appear before an AI system develops the actu-ally dangerous capabilities.\n3. Design proxy tasks for these precursor capabilities.\n4. Attempt to get the model to do these proxy tasks.\n5. If a model ever displays these precursor abilities, stop development or deployment until sufficient precautions [25] are implemented.\nThis approach makes the implicit assumption that there exists enough of a time and compute [26] gap between reaching precursor capability and full required capability for evaluators to catch the precursors and stop further development."}, {"title": "3 Key assumptions in AI evaluations", "content": "For AI evaluations to provide justified confidence in a model's lack of dangerous capabilities, several key assumptions must hold. Below, we explore a non-exhaustive listing of relevant assumptions."}, {"title": "3.1 Evaluating existing models", "content": "1. Comprehensive Threat Modeling: Have all the relevant threat vectors been considered?\nEvaluators must adequately cover the space of dangerous capabilities the AI system could have that would allow it to cause harm. This requires threat modeling which covers all exploitable threat vectors, including vectors evaluators didn't consider but which the AI system might be capable of finding without detection.\nSufficient justification for this assumption may be obtainable when the goal is to prevent harm via misuse by malicious actors. But this would require evaluators (potentially working with threat asses-sors and domain experts) to be correctly confident that they can find all threat vectors that malicious actors would be able to find. Al developers have committed to working with domain experts as part of safety assessments [6, 27]. This could prove challenging, especially when considering well-resourced (potentially nation-state level) malicious actors."}, {"title": "3.2 Forecasting future models", "content": "1. Comprehensive Coverage of Future Threat Vectors: Have all the threat vectors relevant to the next iteration of models been considered? This is similar to the assumption of comprehen-sive threat modeling when evaluating existing models, but with greater inherent uncertainty. Jus-tifying this assumption requires either a comprehensive mapping of possible future threat vectors or a robust argument that newly created models will not be capable of taking advantage of novel, unanticipated threat vectors.\nThis assumption may be justifiable for misuse risks, although not at present given the nascent state of AI threat modeling. This would require that future models do not have unexpected or unconsidered capabilities that malicious actors could learn about and use. This assumption is not feasible to justify when considering risks from autonomous AI systems. Methods do not exist to predict if or when models will gain the ability to exploit threat vectors which evaluators may fail to consider."}, {"title": "4 Implications for regulation", "content": "Al regulation aimed at preventing catastrophic harm may amongst other components heavily rely on AI evaluations. However, as discussed, gaining assurance of safety using AI evaluations relies on many underlying assumptions. We propose that regulation based on AI evaluations should require AI developers to publish a list of the assumptions being made (e.g. the assumptions listed in this paper) and justify them, and these justifications should be subject to review by third party experts. Justifying these assumptions is essential as part of a case for safety, and if the assumptions are not justified it is not appropriate to make inferences about a model's capabilities beyond the specific tests. This proposal is intended as a practical measure to enhance transparency and assist regulators in determining whether AI development is safe.\nEvaluations can provide useful information about model capabilities, and should likely be performed even if the assumptions cannot be justified. But evaluations should not be used to argue that AI systems are safe in the absence of such justifications. AI evaluations should not provide a false sense of security, and rigorously listing assumptions may help alleviate this.\nWe do not know exactly what AI regulation will look like, however regulation may be based on the capabilities of AI models; for example, models with certain capabilities may only be deployed with certain precautions, or AI developers may be required to argue that their systems are safe because they lack certain capabilities.\nAl developers should explicitly state and justify the assumptions being made as part of an evaluations-based case for safety. These should be released for public scrutiny, as long as this it-self would be safe (for example, it should not alert malicious actors to novel threat vectors). These assumptions and justifications should then be assessed by third-party experts. For example, if AI developers are required to publish safety and security protocols [35] which rely on evaluations, they"}, {"title": "5 Conclusion", "content": "In this paper, we have discussed the role of AI evaluations for avoiding catastrophic risks. We have identified key assumptions in safety cases based on AI evaluations, both for evaluating existing models and for forecasting future models. If AI regulation is to be based on such evaluations, it should require AI developers to comprehensively identify and justify these assumptions. This can offer regulators much needed transparency into determining whether AI development is safe. Al development should then be halted if evaluations reveal unacceptable dangers, or if the underlying assumptions cannot be adequately justified."}]}