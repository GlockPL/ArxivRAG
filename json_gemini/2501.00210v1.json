{"title": "Debunking the CUDA Myth Towards GPU-based AI Systems: Evaluation of the Performance and Programmability of Intel's Gaudi NPU for AI Model Serving", "authors": ["Yunjae Lee", "Juntaek Lim", "Jehyeon Bang", "Eunyeong Cho", "Huijong Jeong", "Taesu Kim", "Hyungjun Kim", "Joonhyung Lee", "Jinseop Im", "Ranggi Hwang", "Se Jung Kwon", "Dongsoo Lee", "Minsoo Rhu"], "abstract": "With the rise of AI, NVIDIA GPUs have become the de facto standard for Al system design. This paper presents a comprehensive evaluation of Intel Gaudi NPUs as an alternative to NVIDIA GPUs for Al model serving. First, we create a suite of microbenchmarks to compare Intel Gaudi-2 with NVIDIA A100, showing that Gaudi-2 achieves competitive performance not only in primitive AI compute, memory, and communication operations but also in executing several important Al workloads end-to-end. We then assess Gaudi NPU's programmability by discussing several software-level optimization strategies to employ for implementing critical FBGEMM operators and vLLM, evaluating their efficiency against GPU-optimized counterparts. Results indicate that Gaudi-2 achieves energy efficiency comparable to A100, though there are notable areas for improvement in terms of software maturity. Overall, we conclude that, with effective integration into high-level AI frameworks, Gaudi NPUs could challenge NVIDIA GPU's dominance in the Al server market, though further improvements are necessary to fully compete with NVIDIA's robust software ecosystem.", "sections": [{"title": "1 Introduction", "content": "\"In the past few years there have been many studies claiming GPUs deliver substantial speedups (10\u00d7-1,000\u00d7) over multi-core CPUs on throughput computing kernels. After applying optimizations appropriate for both CPUs and GPUs the performance gap between NVIDIA GTX280 and Intel Core i7 960 narrows to only 2.5x\" \"Debunking the 100X GPU vs. CPU Myth: An Evaluation of Throughput Computing on CPU and GPU\", Intel, ISCA, 2010 [39]\nIntel's latency-optimized processor architectures have dominated the computing industry for decades, serving as the foundation for executing a wide range of applications. However, as GPU computing gained prominence in the late 2000s, NVIDIA, then the underdog, began to challenge Intel's long-standing dominance in the server market. With the rise of AI, throughput-optimized processor architectures spearheaded by NVIDIA GPUs dethroned Intel, establishing NVIDIA's CUDA software ecosystem as the de facto standard for training and deploying AI models.\nOne might argue that domain-specific architectures for AI, also known as Neural Processing Units (NPUs), present competitive alternatives to GPUs. However, cases of successfully utilizing NPUs for deploying AI services are limited to a handful of hyperscalers that can amortize the enormous development costs of NPUs by serving millions to billions of customers with their Al offerings (e.g., Google's TPU [31], Meta's MTIA [12], Amazon's Inferentia [2]). As a result, most Al services deployed today are built using NVIDIA GPUs. The primary advantage of NVIDIA GPUs over commercially available NPUs is their ease of programming with CUDA. The flexible programming interface of CUDA, along with the rich software ecosystem built around GPU-accelerated backend libraries (e.g., cuBLAS, cuDNN, cuSPARSE, cuSOLVER, cuDF, cuVS [48-51, 61, 62]), allows developers to easily implement and optimize new Al models created by AI practitioners (e.g., state-space models like Mamba [1]). While some cloud service providers like Google and Amazon do offer NPUs for developers in the form of \"AI-as-a-Service\" [3, 15], these platforms provide only limited access and programmability for the backend NPUs, such as Google TPU [14] and Amazon Inferentia [2]. This limitation makes it challenging to perform low-level kernel implementation and performance optimization specific to the target backend NPU architecture.\nGiven this landscape, Intel's Gaudi NPU [19] is noteworthy for several reasons. First, Gaudi NPUs come with a native programming language called TPC-C (the CUDA equivalent for Gaudi) [27] as well as low-level compute primitives that ease the implementation of compute kernels targeting the NPU's compute engines. Second, the performance of end-to-end AI applications utilizing these compute kernels is (according to Intel's claims) comparable to, and in some cases better than, that of NVIDIA GPUs. Third, Gaudi NPUs are currently widely available for purchase, allowing researchers to thoroughly characterize this new NPU device vs. NVIDIA GPUs.\nTo this end, this paper presents a detailed characterization of Intel's Gaudi NPU for AI model serving, assessing whether Intel, now the underdog, can pose a tangible threat to NVIDIA's seemingly unassailable dominance in the Al computing market. A thorough understanding of this emerging NPU architecture and its applicability to various Al workloads can offer valuable insights for programmers, Al service providers, and computer architects working on next-generation NPU designs. As such, we conduct a comprehensive analysis of the Gaudi NPU from multiple dimensions, evaluating not just its raw performance but also its programmability for facilitating performance optimization and AI model development."}, {"title": "2 Background", "content": "2.1 Intel Gaudi Hardware Architecture\n(Compute) The Gaudi processor architecture is designed based on a heterogeneous compute paradigm, integrating two key components (Figure 1): Matrix Multiplication Engines (MMEs) and fully programmable Tensor Processing Cores (TPCs). Gaudi-2 features two MMEs and 24 TPCs, which together provide high throughput by pipelining computations between the MMEs and TPCs.\nThe MME is a large, output-stationary systolic array with a 256\u00d7256 MAC (Multiply-Accumulate) structure [34], designed to handle general matrix multiplication (GEMM) workloads, such as fully connected, convolutional, and batched GEMM layers. The MME is designed to be highly configurable in order to maximize the utilization of its MAC array. Specifically, the two MMEs in Gaudi-2, originally composed of two separate 256\u00d7256 MAC units, can be dynamically reconfigured at runtime as a single 512\u00d7256 MAC unit, a single 1024\u00d7128 MAC unit, and others, depending on the shape of the input and output matrices of the GEMM operation. The optimal MME configuration for each target GEMM is determined by the Gaudi graph compiler, which we discuss in Section 3.2. Similar to NVIDIA's Tensor Cores [54], the MME is a co-processor purposefully designed to accelerate matrix multiplications. As such, the MME is not directly programmable, meaning users cannot alter its functionality and can only utilize it for matrix multiplications.\nUnlike the MME, the TPC is a highly programmable, VLIW (Very Long Instruction Word)-based processor designed to execute multiple types of instructions in parallel. Each instruction type is processed by dedicated units that handle load/store operations and scalar/vector operations (Figure 1), enabling efficient parallel execution. The SIMD (Single Instruction, Multiple Data) vector unit can handle 2048-bit wide vector operations. This makes the TPC highly effective for various data-parallel tasks in AI, particularly for nonlinear and non-matrix-based computations, such as vector gather-scatter operations or activation functions.\nIn terms of performance, the MME in Gaudi-2 delivers up to 432 TFLOPS of throughput for BF16 (brain floating point 16-bit [70]) operations. The TPCs provide an additional 11 TFLOPS for BF16. In comparison, NVIDIA's A100 offers 312 TFLOPS for matrix operations (using Tensor Cores) and 39 TFLOPS for vector operations (using SIMD Cores). In total, Gaudi-2 delivers approximately 1.26 times in aggregate higher compute throughput than A100 (Table 1).\n(Memory) Gaudi-2 features 96 GB of HBM2E, delivering a bandwidth of 2.45 TB/sec (Table 1). High memory bandwidth plays a crucial role in AI workloads, particularly in memory-bound tasks such as the embedding vector gathers in RecSys [46, 69] and the decoding stages of LLMs [7, 57, 67]. The A100 offers 2 TB/sec of bandwidth, making Gaudi-2 approximately 20% higher in terms of maximum memory throughput. Regarding on-chip storage, Gaudi-2 includes 48 MB of on-chip SRAM, referred to as shared memory, which serves as a scratchpad for the Gaudi graph compiler. This shared memory acts as temporary storage, facilitating data movement between the MMEs, TPCs, and DMA engines to maximize both on-chip data reuse and hardware utilization. Each TPC has its own local memory (used as a scratchpad within the TPC), divided into scalar and vector memory banks. The scalar memory in Gaudi-2 TPC is 1 KB in size and is accessed in 4-byte aligned chunks, while the vector memory is 80 KB and is accessed in 128- or 256-byte chunks. These local memories are private to each TPC, ensuring fast, dedicated memory operations without interference from other TPCs. In contrast, global memory (including the on-chip shared memory and off-chip HBM), is accessible to the entire system with a minimum access granularity of 256-byte chunks.\n(Communication) Intel's HLS-Gaudi-2 server [20] is integrated with eight Gaudi-2 chips. Each Gaudi-2 is equipped with 24\u00d7100 GbE RoCEv2 [18] ports, providing a maximum bandwidth of 2.4 Tbps when all eight chips participate in collective communication. Of the 24 ROCE ports, 21 are dedicated to direct, point-to-point (P2P) inter-chip communication, with each pair of Gaudi-2 chips connected by three 100 GbE links. Since any given pair of Gaudi-2 is connected via P2P links, the effective bandwidth depends on the number of Gaudi-2 chips involved in the collective communication. For example, when two Gaudi-2 chips communicate, only 300 Gbps of communication bandwidth is available (3\u00d7100 GbE links), which is just 1/8 of the maximum 2.4 Tbps bandwidth. In contrast, NVIDIA's DGX A100 server is integrated with a network switch (NVSwitch [47]) that enables all GPUs within the node to communicate simultaneously at the total NVLink bandwidth. Unlike P2P connections where multiple processors must split bandwidth, NVSwitch ensures that each GPU can transfer data at maximum speed, regardless of how many GPUs are involved in communication. Consequently, for AI model serving that do not fully utilize all eight Gaudi-2 chips, such dynamic scaling can result in limited communication bandwidth, affecting system-wide performance."}, {"title": "2.2 Intel Gaudi Software Architecture", "content": "(Programming model) The TPC programming model is based on the Single Program Multiple Data (SPMD) parallel programming paradigm. In this model, the target workload is partitioned across different TPCs, all of which execute the same TPC program. The workload distribution is performed by partitioning the index space (equivalent to CUDA grid), enabling each TPC to process different data independently. The index space can be divided up to five dimensions, and each member of the index space is allocated with an indivisible unit of work processed by a single TPC.\nA TPC program is typically structured using a for-loop, which iteratively executes vector-wide \"Load Compute Store\" operations. For optimal performance, the TPC programmers are advised to carefully partition the index space across the TPCs while maximizing the performance of individual TPCs. Two important best practices are recommended for TPC programmers. First, to maximize memory bandwidth utilization, the TPC's data access granularity should be aligned to 256 bytes as this is the minimum access granularity for global memory. Second, to fully utilize the TPC processor, it is recommended that programmers manually unroll the for-loop to maximize both instruction-level and memory-level parallelism . This recommendation stems from the fact that TPC instructions have an average architectural latency of 4 processor cycles (i.e., the effect of executing a TPC instruction is reflected in the architectural state 4 cycles later) [22]. By unrolling four TPC instructions within a given iteration of a for-loop, the processor pipeline can be better utilized and memory access latency can be better hidden.\n(Graph compiler) Intel's Gaudi NPU comes with a software suite called Intel Gaudi SDK [25], which is tightly integrated with PyTorch and TensorFlow. This integration allows developers to utilize familiar tools while taking advantage of Gaudi NPU's hardware acceleration. Gaudi SDK includes a graph compiler that converts Al models into a format optimized for execution on Gaudi NPUs. The graph compiler not only applies high-level model optimizations, such as operator and kernel fusion, but it also performs hardware-specific optimizations. These include MME configuration adjustments for high MME utilization and operator pipelining between the MME and TPC. When an MME operation is followed by a TPC operation (e.g., GEMM followed by an activation function), the graph compiler breaks them into smaller, independent sub-operations to enable pipelined execution. This approach helps hide latency and reduce overall execution time. The graph compiler fully controls the process of orchestrating data transfers between the MME and TPC for pipelined execution, using on-chip shared memory.\nWhile lowering the target model graph into Gaudi NPU executable operations is crucial for performance optimization, the programmer, unfortunately, has no control over the graph compiler's optimization process. In other words, users cannot modify the behavior of the graph compiler nor dictate when a particular graph compiler optimization pass should be activated or not."}, {"title": "3 Characterizing Gaudi NPU Performance", "content": "3.1 Motivation and Evaluation Methodology\n(Motivation) AI practitioners utilize high-level Al software frameworks like PyTorch for model development. Consequently, a competitive Al software ecosystem should provide not only highly optimized, low-level backend libraries that accelerate performance-critical primitive AI operations (e.g., GEMM, vector gather-scatter, collective communication), but, more critically, it should also deliver the performance benefits of hardware acceleration at the end-to-end AI application level. To this end, the primary objective of our characterization is twofold. First, we develop microbenchmarks using Intel Gaudi SDK and our custom-designed TPC-C kernels to evaluate Gaudi's ability to achieve high-performance in key primitive AI operations (Table 2). Second, we set out to explore whether Gaudi can deliver competitive performance at the end-to-end AI application level. In our end-to-end performance characterization, we use recommendation systems (RecSys) and large language models (LLMs), as they represent the two most widely deployed Al models in today's datacenters while exhibiting very different compute and memory characteristics (Table 3).\n(Methodology) All experiments discussed in the rest of this paper are conducted using an HLS-Gaudi-2 server (which contains eight Gaudi-2 chips connected via RoCE) and a DGX A100 server (which contains eight A100 GPUs connected via NVSwitch and NVLink) (Table 1). On the software side, we use Intel Gaudi Software v1.18.0, which is based on PyTorch 2.4, along with the TPC-C SDK for custom Gaudi kernel development. For the GPU system evaluation, we use PyTorch 2.4 and CUDA 12.4 for GPU kernel development. All experimental results presented in this paper assume the BF16 data type, except when we evaluate end-to-end RecSys models which utilize FP32. The non-RecSys evaluation results for FP32 and their key takeaways were practically identical to those for BF16, so we omit presenting those results for brevity.\nFor our microbenchmark analysis targeting primitive AI operations (Section 3.2 to Section 3.4), we focus on absolute performance and its resource utilization. For end-to-end Al workload analysis (Section 3.5), we evaluate both performance and energy-efficiency. When evaluating the energy-efficiency of end-to-end AI workloads, each system's power consumption is measured using nvidia-smi [53] for A100 and hl-smi [26] for Gaudi-2."}, {"title": "3.2 Primivite \u201cCompute\u201d Operations", "content": "(MME for GEMM operations) We first analyze the efficiency of Gaudi MMEs in conducting GEMM operations. GEMM involves multiplying two matrices, matrix A of size (M \u00d7 K) and B of size (K \u00d7 N), to produce a result matrix C of size (M \u00d7 N). In Figure 4, we compare the performance of various (M,K,N) GEMM shapes on Gaudi-2 and A100 by plotting the achieved TFLOPS using a roofline model. To simplify our discussion, we classify GEMM operations into two types: (1) square-shaped GEMM (represented by square markers), where the dimensions M, K, and N are all equal; and (2) irregularly-shaped GEMM (represented by triangle markers), where dimension N is set to a relatively small value compared to M and K, resulting in input matrices A and B that are tall and skinny, exhibiting the properties of memory-bound GEMV operations. As shown in Figure 4, Gaudi-2 consistently outperforms A100 across all (M,K,N) GEMM shapes we explore in this study. Notably, Gaudi-2 achieves 429 TFLOPS when M=K=N=8192, reaching 99.3% of its peak compute throughput (Table 1). Part of Gaudi-2's higher absolute GEMM performance is due to its superior hardware specifications; its MME provides a maximum of 432 TFLOPS, which is 40% higher than the A100's 312 TFLOPS offered by its Tensor Cores. Therefore, we also compare how efficiently these two processors are able to utilize its hardware resources by measuring their compute \"utilization\" during GEMM executions.\nIn Figure 5, we measure the ratio of achieved TFLOPS to peak TFLOPS to quantify the compute utilization of GEMM operations. The results indicate that Gaudi-2 not only achieves higher absolute TFLOPS (Figure 4) but also outperforms A100 in terms of compute utilization. Across all evaluated data points, Gaudi-2 achieves an average 4.5% higher compute utilization (maximum of 32% when M=K=N=2,048) than A100. These results were counterintuitive to our initial expectations because large systolic arrays, as employed in Gaudi MMEs, are known to suffer from low MAC utilization when the GEMM operation is irregularly shaped and, therefore, not optimally aligned with the geometry of the systolic array [13, 32, 58]. For example, in a typical output-stationary systolic array, when the GEMM's M and N dimensions are smaller than the height and width of the systolic array, the MAC units can experience significant underutilization . As discussed in Section 2, however, recall that Gaudi's MME can dynamically reconfigure the geometry of its systolic array (i.e., height and width dimensions) to better align with the target GEMM's (M,K,N) shape, significantly enhancing the utility of their MAC units . To better understand this behavior, we use the Intel Gaudi Profiler to reverse-engineer how the graph compiler and runtime system manages MME's GEMM execution, which provide hints on how the MME geometry is dynamically configured in relation to the target (M,K,N) GEMM shape. In Figure 7, we summarize the results of our reverse-engineering, showing how the geometry of the MME systolic array is configured as a function of the input GEMM's M and N dimension sizes while fixing K=16, 384 and how this configuration translates into the MME's compute utilization . Compared to a typical output-stationary systolic array design without reconfigurability , the configurable MME architecture provides up to 15% improvement in compute utilization vs. non-configurable, output-stationary systolic array.\nKey takeaway #1: When performing GEMM, Gaudi-2 achieved both higher absolute performance and greater compute utilization than A100. This superior GEMM performance and efficiency can be attributed not only to Gaudi-2's higher max compute throughput but, more importantly, to the configurability of the Gaudi-2 MME, which enables its systolic array to flexibly adapt its geometry to be most optimal for the target GEMM's (M,K,N) shape.\n(TPC for non-GEMM operations) We now evaluate the performance of Gaudi-2's TPC in conducting vector operations, which is critical in performing AI operations like activation functions. Our microbenchmarks for non-GEMM operations are designed based on the STREAM [43] benchmark suite, which measures sustainable compute throughput and memory bandwidth for element-wise vector operations. Algorithm 1 summarizes these three microbenchmarks: ADD, SCALE, and TRIAD. These microbenchmarks access two (SCALE) or three (ADD, TRIAD) arrays in a streaming fashion. The number of floating-point operations involved is 1 for both ADD and SCALE (addition and multiplication, respectively) and 2 for TRIAD (multiplication followed by addition). We implement these microbenchmarks using TPC-C, where each TPC executes the ADD, SCALE, or TRIAD operation over a dedicated set of array elements (a total of 24 million elements) assigned to its specific index space (Figure 2). We utilize these microbenchmarks to demonstrate how performance can be optimized by applying the two TPC programming best practices discussed in Section 2.2, namely (1) the need to align data access granularity in 256 bytes, and (2) the importance of unrolling loops to maximize parallelism.\nWe first show how a single TPC's performance is improved by applying the two aforementioned best practices. In Figure 8(a), we measure the compute throughput by varying our microbenchmarks' data access granularity from 2 to 2,048 bytes, without loop unrolling. The results clearly show the significant performance drop when the data access granularity is set lower than 256 bytes, which is Gaudi's minimum memory access granularity. At data access granularities higher than 256 bytes, the overall throughput saturates at around 55 GFLOPS for TRIAD and around 30 GFLOPS for SCALE and ADD. From this point, we explore how much further performance improvements can be achieved by unrolling the for-loop in Algorithm 1 (discussed in Section 2.2). As shown in Figure 8(b), the compute throughput of SCALE improves remarkably, while ADD and TRIAD achieve only slight improvements as the loop unrolling factor increases. Overall, these results highlight the importance of loop unrolling in TPC-C kernels to exploit instruction-level and memory-level parallelism for high-performance."}, {"title": "3.3 Primitive \u201cMemory\u201d Operations", "content": "Our characterization of non-GEMM primitive compute operations, using the STREAM benchmark suite, confirmed the competitiveness of the Gaudi-2 memory system in handling streaming memory access patterns. Another key aspect of memory system performance is its ability to manage random memory accesses. Inspired by the design philosophy behind the GUPS (Giga Updates Per Second [41]) benchmark suite, we developed our microbenchmarks to measure the memory system's performance in handling vector gather-scatter operations. These operations involve reading (vector gather) and writing (vector scatter) large amounts of data at random memory locations, which is highly memory-intensive and exhibits low data locality. The performance of these vector gather-scatter operations is particularly relevant for Al workloads like RecSys and LLMs, which require frequent embedding table lookups [5, 33, 35, 38, 40]. To this end, our microbenchmarks perform vector gathers from and, similarly, vector scatters to random locations within a 2D vector array. This 2D vector array consists of 4 million vectors, with vector sizes ranging from 16 bytes to 2,048 bytes. In Figure 9, we show the memory bandwidth utilization of Gaudi-2 and A100 during vector gather-scatter operations. In general, Gaudi-2 achieves competitive memory bandwidth utilization when the vector size is \u2265256 bytes, which is its minimum memory access granularity. For instance, Gaudi-2 achieves on average 64% memory bandwidth utilization for \u2265256 bytes vector gather operations, which is only slightly lower than A100's 72% average memory bandwidth utilization. However, for vector sizes smaller than 256 bytes, Gaudi-2 exhibits a significant drop in memory bandwidth utility, achieving only an average 15% memory throughput for \u2264128 bytes vector gather vs. A100's average 36% memory bandwidth utilization, a 2.4x drop in memory performance.\nWe speculate that the primary reason for A100's superior vector gather-scatter performance is as follows. Several prior studies focusing on reverse-engineering NVIDIA GPU microarchitecture [30, 44] observed that its last-level cache either uses a cache line size of 32 bytes or a 32-byte sectored cache, suggesting that the minimum data access granularity for off-chip memory is also optimized for 32-byte data transfers [6, 63, 65]. This design enables NVIDIA GPUs to fetch 32, 64, and 128 bytes from off-chip memory with minimal memory bandwidth waste, unlike Gaudi-2, which inevitably wastes bandwidth for data transfer sizes smaller than 256 bytes.\nKey takeaway #3: Gaudi-2 provides competitive memory performance for regular data transfers with streaming access patterns. However, for random memory accesses like vector gather-scatter, Gaudi-2's performance falls short of A100's when the data transfer size is smaller than its 256-byte minimum access granularity."}, {"title": "3.4 Primitive \u201cCommunication\u201d Operations", "content": "Recent large-scale AI models, like RecSys and LLMs, require multiple GPU or NPU devices for model serving, which necessitates frequent collective communications, such as AllReduce and Reduce-Scatter. In this subsection, we use the collective communication libraries developed by Intel and NVIDIA (HCCL [23] and NCCL [52], respectively) to characterize the performance of six representative collective communication operations. Both Intel's HLS-Gaudi-2 and NVIDIA's DGX A100 server nodes provide an aggregate of 300 GB/sec of intra-node communication bandwidth. In Figure 10, we use the bus bandwidth utilization suggested by NCCL [55] to compare the communication performance of both systems as the number of participating devices varies from 2 to 8 devices. When all eight devices participate in communication, Gaudi-2 shows higher bus bandwidth utilization than A100 for 5 of the 6 collective communication patterns evaluated. However, as the number of communicating devices decreases, Gaudi-2 experiences an almost linear decline in bus bandwidth utilization, unlike A100, whose bus bandwidth utilization remains relatively stable regardless of the number of communicating devices. As discussed in Section 2.1, NVIDIA's DGX A100 server is equipped with an all-to-all network switch (NVSwitch) that enables all GPUs within the server node to communicate simultaneously, leveraging the full aggregate intra-node NVLink bandwidth. In contrast, Intel's HLS-Gaudi-2 server directly connects each pair of Gaudi-2 devices using P2P links, so the effective collective communication bandwidth scales proportionally with the number of devices involved. This setup explains the gradual decrease in Gaudi-2's bus bandwidth utilization as the number of devices used for collective communication decreases.\nKey takeaway #4: The system-level collective communication performance of Gaudi-2 based system falls short of A100, not because of the limitations of the Gaudi-2 processor architecture itself, but because of its lack of an all-to-all network switch provisioned in NVIDIA's DGX A100 system. This switch enables A100 GPUs to more flexibly exploit intra-node network bandwidth, regardless of the number of devices involved in communication, a feature that is currently missing in Intel Gaudi-2 systems."}, {"title": "3.5 End-to-End Application-level Analysis", "content": "(Models and backend software) We now evaluate Gaudi-2 and A100 at the end-to-end AI application level, focusing on RecSys and LLMs. RecSys incorporates a heterogeneous mix of sparse and dense layers, including frontend embedding layers (which perform embedding lookups where multiple embedding vectors are \"gathered\" from embedding tables) and backend MLP layers. Consequently, we evaluate two RecSys model configurations based on DLRM-DCNv2 [69] from the latest MLPerf benchmark suite [45] (Table 3): the compute-intensive RM1, where feature interaction and bottom/top MLP layers are dominant, and the memory-intensive RM2, where embedding layers are dominant. Because Intel Gaudi SDK currently lacks support for multi-device RecSys serving (a feature that is natively supported in TorchRec [29] for serving RecSys over multi-GPUs), we focus on single-device RecSys serving for Gaudi-2.\nAs for LLMs, we evaluate Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct for single- and multi-device serving [9]. Both systems can employ KV caches and FlashAttention [8] by using NVIDIA's TensorRT-LLM [56] for A100 and Intel's optimum-habana [17] for Gaudi-2 as their backend engines. Finally, a synthetic dataset with an input token length fixed at 100 and output token lengths swept from 25 to 400 were used to examine the effect of fixed input-output lengths. Dynamic LLM model serving scenarios with variable input-output token lengths [37, 71] are evaluated in Section 4.2. NVIDIA's CUDA Graphs [16] and Intel's HPU Graphs [24] were used as a performance tuning knob whenever appropriate and we report the highest performance achieved."}, {"title": "4 Characterizing Gaudi NPU Programmability", "content": "In this section, we present case studies on utilizing the Gaudi NPU's programming model and its software stack to conduct performance optimizations for RecSys and LLM serving systems.\n4.1 Performance Optimization at the Low-level TPC-C: A DLRM Case Study\nMeta's TorchRec library [29] is built on FBGEMM's GPU-optimized embedding lookup operator, which reduces CUDA kernel launch overhead by batching multiple embedding tables' vector gather operations into a single CUDA kernel execution (referred to as BatchedTable). Currently, Intel's Gaudi SDK does not support TorchRec, so its embedding lookup implementation does not batch vector gather operations across multiple tables. Instead, each TPC kernel launch processes only a single table's embedding vector gathers (henceforth referred to as SingleTable), resulting in N separate TPC kernel launches for N embedding table lookups.\nTo evaluate Gaudi's programmability for low-level performance optimizations, we implemented both the SingleTable and the BatchedTable embedding lookup operators for Gaudi-2 using TPC-C. Our approach incorporates several optimizations tailored for embedding lookups as follows. The SingleTable operator performs embedding lookups individually for each table. The TPC-C kernel's for-loop is unrolled by a factor of 4 over embedding table lookup indices to maximize memory-level parallelism (i.e., four embedding vector gathers per each TPC are concurrently initiated for each for-loop iteration). The gathered embedding vectors are stored inside TPC's local memory to minimize data movement. Additionally, we distribute workloads (i.e., offsetsPerTable in Figure 13(a)) across multiple TPC units to maximize chip-wide memory-level parallelism.\nDespite these optimizations in our SingleTable operator, a challenge remains: with low batch sizes, a single TPC unit cannot fully utilize memory bandwidth because the workload per TPC is limited to embedding vector lookups within a single embedding table. Even when multiple embedding tables are subject to embedding lookups, memory bandwidth remains underutilized because embedding lookups across multiple tables are performed sequentially through separate TPC-C kernel launches (i.e., memory bandwidth utilization does not increase with a larger number of tables, Figure 14(a)). To address this issue, our BatchedTable operator fuses embedding lookups from multiple tables into a single TPC-C kernel. Similar to FBGEMM's CUDA-optimized BatchedTable, our TPC-C BatchedTable implementation treats multiple tables as one large table, using a separate offset to indicate the starting index location of each table (tableOffsets in Figure 13(b)). This approach requires passing indices and offsets for all tables to the TPC-C kernel in a single call. Consequently, our BatchedTable achieves significantly higher memory bandwidth utilization compared to SingleTable as the number of tables increases, as shown in Figure 14(a). It is worth noting that, with larger batch sizes, the performance gap between SingleTable and BatchedTable diminishes, as SingleTable can exploit more parallelism across different batches to improve memory bandwidth utilization\nOverall, our Gaudi-2 BatchedTable achieves an average memory bandwidth utilization of 34.2% and a peak utilization of 70.5%, representing a 1.52\u00d7 improvement over SingleTable. In comparison, A100 demonstrates an average memory bandwidth utilization of 38.7% with a peak of 81.8% (Figure 14(d)). As shown in our vector gather-scatter microbenchmark experiments (Figure 9), Gaudi-2 shows sub-optimal performance in fine-grained vector gathers so BatchedTable (Gaudi-2) experiences a noticeable performance drop for vector sizes below 256 bytes, with an average utilization of 12.0%. In contrast, the A100 sustains much higher performance at these lower vector sizes, with an average utilization of 25.3%.\nKey takeaway #6: This case study confirmed that the TPC-C programming system provides a sufficient level of flexibility for low-level performance optimizations. Compared to state-of-the-art FBGEMM-based A100 executions, our Gaudi-2 optimized kernel for embedding layers achieved, on average, 95% of the throughput of A100 for large embedding vector sizes (\u2265256 bytes) but only 47% for small vectors (<256 bytes). The noticeable performance degradation for small vectors primarily stems from A100's superior hardware architecture (which better supports fine-grained memory accesses) rather than from the differences in the programming models."}, {"title": "4.2 Performance Optimization at the High-level PyTorch: A vLLM Case Study", "content": "Serving LLMs over batched requests poses unique challenges due to the dynamic nature of input-output sequences across different requests. These variations can result in GPU memory fragmentation, which reduces the maximum batch size that the serving system can support, lowering throughput. To address this issue, VLLM [37", "68": "experience a significant increase in latency as batch size grows, so supporting high-performance PagedAttention is critical for vLLM.\nWhile VLLM natively supports a CUDA-optimized PagedAttention kernel [37", "42": "to directly utilize GPU's Tensor Cores (alongside the normal CUDA Cores) for computation within the low-level CUDA kernel. However, Gaudi programmers can only access the MME units at the PyTorch level, whose functionality is limited to the built-in, pre-compiled MME-optimized kernels provided with the Intel Gaudi SDK. Consequently"}]}