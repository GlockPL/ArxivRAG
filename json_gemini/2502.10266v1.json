{"title": "ARE LARGE LANGUAGE MODELS THE FUTURE CROWD WORKERS OF LINGUISTICS?", "authors": ["Iris Ferrazzo"], "abstract": "Data elicitation from human participants is one of the core data collection strategies used in empirical linguistic research. The amount of participants in such studies may vary considerably, ranging from a handful to crowdsourcing dimensions. Even if they provide resourceful extensive data, both of these settings come alongside many disadvantages, such as low control of participants' attention during task completion, precarious working conditions in crowdsourcing environments, and time-consuming experimental designs.\nFor these reasons, this research aims to answer the question of whether Large Language Models (LLMs) may overcome those obstacles if included in empirical linguistic pipelines. Two reproduction case studies are conducted to gain clarity into this matter: Cruz and Lombard et al.. The two forced elicitation tasks, originally designed for human participants, are reproduced in the proposed framework with the help of OpenAI's GPT-40-mini model. Its performance with our zero-shot prompting baseline shows the effectiveness and high versatility of LLMs, that tend to outperform human informants in linguistic tasks. The findings of the second replication further highlight the need to explore additional prompting techniques, such as Chain-of-Thought (CoT) prompting, which, in a second follow-up experiment, demonstrates higher alignment to human performance on both critical and filler items. Given the limited scale of this study, it is worthwhile to further explore the performance of LLMs in empirical Linguistics and in other future applications in the humanities.", "sections": [{"title": "Introduction", "content": "Empirical linguistic studies are often based on evidence gathered through tasks completed by humans. This data collection strategy allows for a richer view on how people perceive, judge, and use language by relying on the multitude- perspective of the crowd [Ahmadabadi et al.]. This applies to studies involving groups of informants of varying sizes, including those recruited via crowdsourcing platforms such as Prolific\u00b9 and Amazon Mechanical Turk\u00b2. Research suggests that crowd workers are a viable resource for exploring a variety of empirical linguistic phenomena, including syntactic properties of minority languages [Sheehan et al.], language proficiency judgments [Thwaites et al.], lexical diversity [Khalilia et al.], and pragmatic implicatures of conversational negation [Capuano et al.], among others.\nHowever, this comes with a cost: humans are demanding, expensive, and difficult to engage in reproducible research settings. Although quick and extensive data collection is made possible through crowdsourcing by reaching matching subscribers with newly published studies, it is still questionable how researchers can actively control the quality and the attention of their participants' performance."}, {"title": null, "content": "Concerns over the precarity of the working conditions in crowdsourcing have also been broadly voiced [Van Zoonen et al.]. Furthermore, with the widespread of computational models, especially Large Language Models (LLMs), that can perform human-like tasks and even outperform the accuracy of crowd workers [Koco\u0144 et al., Gilardi et al.] and experts [T\u00f6rnberg, OpenAI et al.], illicit evidence of their usage in crowdsourcing studies is currently under observation [Veselovsky et al.]. Questions both in Linguistics and in Natural Language Processing (NLP) and Generation (NLG) arise around textual data collected via crowdsourcing that may be already produced by machines, rather than by the hired human crowd workers. In order to overcome the shortcomings and costs of managing human participants in such empirical studies, the present paper investigates to what extent researchers of humanities, in this case of empirical Linguistics, may use LLMs in their research workflows as a replacement of, or in addition to, human informants and crowd workers.\nWhile research on the analytic abilities of LLMs as task solvers, especially using ChatGPT, GPT-4, LLama, Gemini, and Claude, is a current NLP matter and shows the potential and limitations of these models across different tasks and disciplines [Koco\u0144 et al.], less attention is given to the possible operationalization of these models in humanities and social sciences research. Furthermore, this line of research lies in the controversy of the replacement of humans through machines, which is perceived as critical [Ziems et al.]. Filling these gaps, this paper aims to:"}, {"title": null, "content": "\u2022 address the question of whether LLMs can become the next generation of crowd workers for empirical linguistic studies. Linguistic research conducted with human participants usually goes beyond mere annotation tasks (see common NLP-tasks such as sentiment analysis, Part-of-Speech-tagging, syntactic parsing, text classification, among others) and grounds in language processing, judgement, and generation under very specific analytical conditions,\n\u2022 extend the range of action of LLMs in the humanities by testing their behaviour on task outlines that were designed traditionally for humans,\n\u2022 develop an approachable basic prompt engineering framework that can be easily reused also by non- programmers who want to explore LLMs' potential in their research,\n\u2022 support the line of work that intends to overcome the English-centric training and usage of LLMs by studying their performance on Romance languages."}, {"title": null, "content": "These objectives are addressed in the present paper by replicating two empirical linguistic studies that have been conducted on human participants. In the proposed pipeline, their work is replaced by LLMs, specifically OpenAI's GPT-40-mini, here used to reach crowd workers-like performance levels. Although this study cannot cover the entire spectrum of areas of interest in linguistic research, the results point in the direction of an alignment between LLMs and human participants that may support a needed interdisciplinary opening between NLP and the humanities. In summary, we report the following findings:"}, {"title": null, "content": "1. The proposed pipeline demonstrates adaptability and applicability to a wide range of instruction-based tasks, underscoring the broader significance of this research.\n2. In the first replicated study, GPT-40-mini's performance aligns well with human participants, highlighting LLMs' potential in empirical linguistic research.\n3. The second task reproduced introduces a more nuanced perspective on LLMs-to-humans alignments and offers an opportunity to explore strategies for managing outcomes, especially through the use of specific prompting techniques (see Chain-of-Thought prompting).\n4. GPT-40-mini outperforms human informants in all experimental conditions tested."}, {"title": null, "content": "The paper is structured as follows: first, it briefly reviews the state of the art of LLMs' performance in instruction-based tasks in NLP and in empirical Linguistics (Section 2), second, it outlines the replication methodology designed for our two case studies, including details about the model and prompt engineering (Section 3). Third, the results of the two replications are presented (Section 4 and 5) and discussed in light of the aforementioned research questions (Section 6). Finally, we conclude the paper with a brief summary of newly gained insights, research limitations, and future work directions (Section 7)."}, {"title": "Related Work", "content": null}, {"title": "Large Language Models as crowd workers in NLP", "content": "LLMs are remarkable data annotators. Their pre-training and supervised instruction fine-tuning build intelligent assistants that mimic human behaviour [OpenAI et al.]. Consequently, LLMs exhibit exceptional capabilities in adhering to instructions designed to elicit specific model responses, while offering a user-friendly interaction experience. Within the field of NLP, ongoing research investigates these abilities through the development of frameworks that either incorporate human informants, or that operate independently of them [Koco\u0144 et al.]. Even if LLMs do not consistently outperform humans across all tasks, they are well-positioned to make meaningful contributions to social science analysis in collaboration with human researchers [Ziems et al.]. These pipelines often task computational models with performing direct or indirect annotation [Van Dalfsen et al.], using either supervised or unsupervised methods [Horych et al.], or by establishing semi-automated annotation workflows [Ostyakova et al., Xu et al.]. Recent findings suggest that LLMs match or surpass human crowd workers in numerous NLP tasks [T\u00f6rnberg, Ziems et al., He et al., Koco\u0144 et al., Gilardi et al.]. However, these studies predominantly focus on crowdsourcing scenarios where participants are required to assign labels to data, rather than engage in language generation or exercise complex judgment. Wu et al. use LLMs in the replication of previous crowdsourcing pipelines in a course assignment and describe high variance in the results according to task complexity. They suggest that analysing LLMs within established pipelines or workflows provides a clearer insight into their strengths and limitations; accordingly, this paper aligns with that objective."}, {"title": "Large Language Models as crowd workers in empirical Linguistics", "content": "Although disciplines outside NLP, such as cognitive research [Dillion et al.] and economics [Gui and Toubia], have already explored whether LLMs may replace or at least productively simulate human participants in empirical studies, this matter remains understudied in Linguistics. [Argyle et al., 2023] provided initial evidence suggesting that language models can serve as proxies for specific human sub-populations, offering new avenues for research in the social sciences. This has been applied to linguistic tasks such as sentiment analysis [Borst et al., Rebora et al.], topic detection [Kosar et al.], semantic annotation [Gilardi et al.], paraphrase generation [Cegin et al.], and text classification [De Lange et al.], but has not been extended systematically outside NLP-related annotation tasks, apart from few exceptions (see [Karjus, 2024] on a set of case studies such as annotation of semantic change detection, and [Karjus and Cuskley, 2024] on measurements of linguistic divergence in US American English across political spectra). Even with a linguistic core interest, these annotation tasks display a strong connection to NLP-based pipelines rather than to actual ongoing linguistic research. Linguistics researchers continue to focus mostly on experimental designs involving human participants, or they engage with LLMs primarily through the ChatGPT interface [Dynel, Han, Duncan], rather than exploring a broader range of models within actual coding environments. Empirical linguistic studies vary in design, employing diverse data collection methods, analytical goals, and participant criteria, offering a valuable opportunity to evaluate LLMs as crowd workers."}, {"title": "Methods", "content": "The overall approach to test LLMs as linguistic crowd workers is illustrated in Figure 1 and is described in detail below, including chosen models, replication procedure, and materials used."}, {"title": "GPT-4-mini as crowd worker", "content": "A closed-source model of the OpenAI family, GPT-40-mini, is chosen to assess LLMs in the role of crowd workers in the proposed study replications. Although closed-source LLMs can be challenging in matters of limited transparency of their training data and of additional costs of their API usage, these models deliver state-of-the-art results and offer a successful user experience. This facilitates a beginner-friendly introduction to these models, making them accessible to humanities researchers who wish to experiment with the models' API using a lightweight code base without requiring access to high-performance computing infrastructures. GPT-4o-mini is selected for the present investigation as OpenAI's most cost-efficient small model to the present moment. Our baseline approach involves zero-shot prompting of GPT-40-mini. The model is presented with a prompt that includes information about the participants' profile to be impersonated by the model, along with the task instructions from the studies being replicated. A recurrent meaningful piece of information about the informants is their mother tongue. Other prompt engineering techniques were discarded to first establish a baseline that evaluates the model's responses in a way that is comparable with the original human-based study. In this paper, we build this initial baseline by prompting the model directly without fine-tuning. Although fine-tuning has been shown to enhance LLMs' performance in text annotation [Alizadeh"}, {"title": "Replicating empirical linguistic studies with LLMs", "content": "The goal of the paper is to replicate the workflow of two studies that were conducted in the field of empirical Linguistics, this time by replacing human participants with LLMs as crowd workers. We test the performance of LLMs in linguistic tasks that go beyond mere annotation and address the question of model-to-human judgment alignment. The studies we choose to replicate with LLMs match a set of fixed parameters in order to fill in the previously mentioned research gaps:\n\u2022 The analytical focus must be set on Romance languages,\n\u2022 The task designed for their human participants is based on language generation and/or understanding, not just labelling,\n\u2022 The research question is rooted in linguistics rather than originating from the field of NLP.\nTwo forced-choice binary elicitation tasks that match the aforementioned criteria were selected. They involve Spanish and English code-switching environments [Cruz] and neological intuition in French native speakers [Lombard et al.]. Both studies prompt responses from human participants on a computer screen, but they elicit them in two different forms: a discourse completion [Cruz] conducted in person and a linguistic judgment survey (by pressing \"yes\" or \"no\" buttons; Lombard, Huyghe, and Gygax [Lombard et al.]) completed on the Qualtrics online platform. For the sake of clarity and consistency throughout the paper, the two studies will be referred to as Cruz_23 and Lombard_21.\nIn our replications we keep the experimental design as close as possible to the original. The main contribution of our research is to adapt their pipeline to LLMs as crowd workers. For this reason, it does not intend to offer a better or different version of the original experiments and no judgement is passed about the quality of their empirical pipeline."}, {"title": "Replication pipeline", "content": "We replicate Cruz_23 and Lombard_21 through the same straightforward coding framework, which can be accessed online (see Appendix A). This shows the versatility of the proposed methodology that can be used to replicate several new crowdsourcing studies and to approach prompting techniques to test LLMs in different research settings. The code pipeline is divided into four main parts: (1) first, the critical stimuli and the distractor sentences are read and pre-processed for further analysis. (2) Then, different prompting strategies are operationalised on only one \"LLM-informant\" at the time, i.e. in one query using GPT-40-mini. This keeps the API usage costs under control and gives users the possibility to try different prompt wordings until the expected model's output is matched. As"}, {"title": "Study 1: Modulating gender assignment in Spanish\u2013English bilingual speech (Cruz 2023)", "content": null}, {"title": "Study overview", "content": "We replicate the task originally devised by Cruz_23, whose goal was to examine linguistic factors (i.e., semantic a.k.a. biological gender, analogical gender, and other-language phonemic cues, see Table 1) that may affect gender assignment (masculine or feminine) to English nouns occurring in code-switches. As part of their experiment, crowd workers completed first a forced choice elicitation task involving a code-switching environment: switches between Spanish determiners and English nouns (e.g., Ya estamos en plane, in English: We are already in plane, see Cruz_23: 586), where participants had the binary choice between the two possible Spanish determiners: el (masculine) and la (feminine). This is linguistically relevant since Spanish and English exhibit different distributional patterns in terms of gender assignment: Spanish employs a binary gender system, allocating every noun to either the masculine or feminine category, whereas the grammatical gender of English is absolute. Accordingly, Cruz_23 investigates whether the gender of the Spanish translation equivalent (analogical gender) influences the assignment mechanism used in code-switched speech or whether this mechanism functions independently of the gender system of the other language.\nThe experimental setting of task 1 comprehended 34 participants, early bilinguals, who reacted to 80 sentences from the CESA Corpus [Carvalho] containing critical stimuli (English nouns equally divided into human-denoting nouns and inanimate nouns) and 75 distractor nouns (with binary answer options that do not display any marks for gender assignment in Spanish, i.e. mi and su, in English: my and his/her/its). The experimental stimuli can be found in Cruz_23's supplementary materials. Table 1 gives an overview of the eight tested conditions according to semantic and/or grammatical gender, animacy of the referent, and phonemic cues about the grammatical gender of the Spanish translation."}, {"title": "Results and error analysis", "content": "Cruz_23 describes their findings in terms of gender congruency, defined as the proportion of Spanish determiners chosen by their study participants whose grammatical gender aligns (gender congruent) or does not align (gender incongruent) with the Spanish translation of the English target noun. Their descriptives and our corresponding findings are shown in Figure 3 for direct comparison. Across the eight tested conditions, GPT-40-mini shows comparable performance tendencies to human participants and reveals gender congruency patterns.\nAs a general trend, GPT-40-mini exhibits higher gender congruency choices in all experimental conditions compared to human participants. Strong alignments with human choices are found with human denoting nouns (Conditions 1, 2, 5, 6) that are assigned by the model a determiner of the grammatical gender in line with the referent's semantic (a.k.a. biological) gender almost without doubt (Conditions 1, 2, 5, and 6 combined; M=0.99). As in the original study, proportions of gender-congruent selections for inanimate nouns were higher for masculine gender (Conditions 3 and 4 combined; M-human=0.89, M-GPT=0.96) compared to feminine gender (Conditions 7 and 8 combined; M- human=0.62, M-GPT=0.80). English target nouns whose Spanish translation exhibits morphological cues of feminine grammatical gender (e.g., border - in Spanish: frontera. Condition 7; M-human=0.55, M-GPT=0.88) are assigned gender congruent determiners more often than without cues for grammatical gender (e.g., honey - in Spanish: miel. Condition 8; M-human=0.70, M-GPT=0.73). In Condition 7, the model significantly outperforms human participants,"}, {"title": null, "content": "whereas in Condition 8, both humans and LLMs achieve comparably lower gender congruency levels. The gender incongruent model's decisions and their corresponding scores can be found in Appendix B."}, {"title": null, "content": "The task duration of our reproduction with GPT-40-mini was approximately 72 seconds for each iteration through all the critical and distractors items, which stands for one informant's performance in the experimental design. The study duration reported with human participants is 50 minutes per person, which includes a first verbal exchange between the researcher and the participant, followed by the completion of tasks 1 and 2. Since we replicate only task 1, we can expect it to take originally approximately 25 minutes per participant."}, {"title": "Study 2: Neological intuition in French (Lombard et al. 2021)", "content": null}, {"title": "Study overview", "content": "As second approach to our purpose, we replicate Lombard_21 metalinguistic task of novel word identification with French (non-linguist) native speakers. To assess what classes of linguistic information about neologisms may help native speakers to recognise their occurrence in a given speech sample, two variables are analysed: formal novelty and lexical regularity. The main hypothesis is that informants may tend to spot neologisms easier when they display high levels of formal novelty (i.e., they are newly created words rather than existing words used with new meanings) and of lexical irregularity (i.e., they are created through irregular rather than regular word formation processes). Morphological neologisms are new words created through modifications to the form and, typically, the meaning of a lexical base (1), whereas semantic neologisms are existing words that acquire new, additional meanings (2):\n1. e.g., Certaines c\u00e9l\u00e9brit\u00e9s vont \u00e0 contre-courant, en se faisant d\u00e9tatouer (Lombard_21: 4, in English: Some celebrities go against the tide by having their tattoos removed)\n2. e.g., Dans une relation toxique, les tensions et les critiques sont omnipr\u00e9sentes (Lombard_21: 4, in English: In a toxic relationship, tension and criticism are omnipresent)\nParticipants (n=68) were instructed to first read a set of French sentences and then judge whether they contain neologisms. In a positive case, they should identify novel words by clicking on them and their reaction times were logged. The stimuli comprehended 80 experimental sentences including neologisms that were created ad hoc for the study purpose, and 40 filler sentences without neologisms. They can be accessed in Lombard_21's supplementary materials. Each experimental condition included 20 stimuli (see Table 2). The critical sentences had neologisms (nouns and verbs) placed in different syntactic positions to prevent the formation of repeated decision patterns, displayed a standardized length, and presented syntactic simplicity. Filler sentences were constructed analogously, they only differ in the absence of neologisms."}, {"title": "Results and error analysis", "content": "The original study reports the average of 20 minutes registered by the human participants to complete the survey online. Our reproduction shows an average of 68 seconds per \"LLM-informant\".\nLombard_21's discussion of their results is twofold: first, accuracy of neologism detection is measured, and second, participants' reaction times in correct detections are compared across the four experimental conditions. The first approach is illustrated in Figure 4. LLM-informants of our replication outperform humans in the task of neologism detection. Accuracy is above ninety-seven percent (M-GPT-40-mini=0.985) in all four conditions tested. This aligns with the overall findings of the original paper, as human participants also demonstrated high accuracy, albeit slightly lower than that of our LLM (M-human = 0.91). The analysis of reaction times recorded by GPT-40-mini in all iterations through both critical and filler items indicates that this measurement does not serve as a reliable parameter to gain insights into the model's reasoning process, as it does for human participants. Given that computation time is influenced by various external factors, such as hardware, model architecture, input complexity, and optimization techniques, it cannot be considered a meaningful evaluation parameter of the model's performance. Consequently, our results assessment focuses exclusively on neologisms detection rates."}, {"title": null, "content": "A closer analysis of GPT-40-mini's results in our replication shows one small drop in performance registered in Condition 4 (M-GPT-4o-mini=0.97), signalling that if the model experiences any difficulties in detecting neologisms, it is mostly when these words are not morphologically new, but are used with irregular new, extended meanings. The error analysis of the model's mistakes in detecting neologisms suggests that these are concentrated on a small number of individual words, with one word in particular (velours, in English \"velvet\"/\"velvet clothes\", Condition 4) accounting for fifty percent of the errors in all conditions. This potentially indicates the word as an outlier. When this word is excluded from the total error count, the distribution of error scores across the tested conditions shifts, neutralising differences between conditions. GPT-40-mini excels at the task and its performance is not influenced by linguistic parameters as it is the case with human participants.\nThe most surprising aspect of the error analysis is not the error rates observed in the experimental conditions, but rather those associated with the fillers. In the original study, filler sentences correspond to items that do not contain neologisms,"}, {"title": "Follow-up 1: Modification of the zero-shot prompting function", "content": "First, we conduct an experiment and modify only one parameter of the replication pipeline: the zero-shot prompting function. In this function, the role parameter (i.e., \"role\": \"system\") is used to define the role designed for the language model during task completion. OpenAI's chat models, including GPT-40-mini, follow a structured message format where each message has a role. The three main roles are:\n1. System: It provides high-level instructions that define the assistant's behaviour.\n2. User: It represents the user input or prompt.\n3. Assistant: It stands for previous model's response.\nIn the first follow-up experiment, we explicitly instruct the model to prioritize careful execution of its role and to focus on being the best-performing informant in the given task.3 The accuracy scores across the four conditions tested and on filler sentences are shown in Appendix B. The model keeps outperforming human participants and reaches high accuracy rates (M-GPT-40-mini-role= 0.98). Condition 4 registers the lowest score (M-GPT-40-mini-role= 0.94), which is in line with our baseline. The performance of GPT-40-mini on filler sentences improves of 0.06 points."}, {"title": "Follow-up 2: Chain-of-Thought (CoT) prompting", "content": "Second, we introduce a different modification to the original pipeline, independent of the first follow-up experiment, and we implement Chain-of-Thought (CoT) prompting. The modification of the prompt aims to encourage the model to reason through its decisions more explicitly. CoT elicits a series of intermediate reasoning steps from the model to improve the ability of LLMs to perform complex reasoning [Wei et al.]. Two examples, one from the critical and one from the filler stimuli, are shown in the adapted prompt to exemplify how the thinking process should take place in order to solve the task at best. Both example sentences are excluded from the test dataset. A summary of the versions of used prompts across the different study replication designs can be found in Appendix C.\nCoT results present a general small drop in accuracy (Conditions 2, 3, and 4) in respect to our baseline and the first follow-up experiment with the modified zero-shot prompting function. However, it maintains high accuracy scores (M-GPT-40-mini-CoT= 0.94). In Condition 4, human participants outperform the model for 0,03 points (M-human= 0.92, M-GPT-40-mini-CoT= 0.91), but the two scores are close. The gain of this second follow-up experiments is GPT-40-mini's performance on filler sentences, reaching 0.99 accuracy."}, {"title": "Discussion", "content": "This study examined the performance of LLMs in replicating empirical linguistic studies originally conducted with human participants. While our analysis focuses on two specific cases, we demonstrate that this approach is suitable across a broad range of disciplines both within and beyond the humanities. Our findings highlight both the potential and the limitations of using LLMs as the future crowd workers of empirical Linguistics. The major takeaway from this paper is a general tendency to alignments between human and LLM judgments in linguistic tasks, suggesting that these models serve as a solid starting point for further research on synthetic crowdsourcing studies. This can be carried out also by non-experts as our accessible code base exhibits and can be extended to a wide range of applications, as demonstrated in our follow-up experiments.\nBreaking down the findings from the two replications, the main constant is a very accurate task completion carried out by GPT-40-mini. The model outperforms human participants in both case studies across all conditions tested. The reproduction of Cruz_23 supports the notion that the gender of the Spanish translation equivalent (analogical"}, {"title": null, "content": "gender) of the English target noun mediates the assignment mechanism in code-switched speech also for LLMs. The tendencies observed in GPT-40-mini's performance are numerically close to those registered with human participants. First, human-denoting nouns appear to evoke the semantic (biological) gender of their referents, reaching the highest gender congruency rates in the replication. While for humans this effect is linked to real-world referential properties, LLMs lack direct access to the real world, implying that such properties must be encoded in their vast training corpus. Second, Spanish-English bilinguals typically default to a masculine assignment strategy for inanimate nouns, a trend that is replicated in our study. Third, phonemic cues in the morphological endings of Spanish translations of English target nouns play a facilitating role, especially for LLMs, in assigning gender, particularly in the case of feminine inanimate nouns.\nIn contrast, the second replication (Lombard_21) presents a different scenario in three key aspects. First, after removing outliers, no significant differences between experimental conditions are observed, leading to the rejection of the original study's hypothesis that neologism detection may be influenced by neologism formation and linguistic usage patterns. Second, the measurement of reaction times in GPT-40-mini raises concerns regarding using this parameter in future replications since it may be influenced by external factors that may affect computation. Especially its consistency as a variable of meaningful comparability across iterations is questioned, highlighting the need for further research into alternative evaluation methods for human vs. LLMs performance. Third, while the model excels in detecting neologisms, its accuracy drops significantly in identifying fillers. This finding proposes two considerations: first, zero-shot prompting may not be the optimal strategy for all experimental scenarios [Van Dalfsen et al.], reinforcing the importance of exploring alternative prompting techniques. The first follow-up experiment reveals that the role parameter in OpenAI chat models' zero-shot prompting function does not significantly impact task outcomes. The second experiment demonstrates that Chain-of-Thought (CoT) prompting is a viable alternative to zero-shot prompting for more structured tasks, as it enhances complex reasoning [Wei et al.]. The model's performance in the CoT experiment aligns more closely with that of human participants, whose results are high but not as consistently high as the model's baseline performance.\nThis generally high performance of LLMs can be interpreted in two ways. On the one hand, it serves as an encouragement to integrate these models into research pipelines beyond the NLP community. On the other hand, it raises concerns about potential \"over-performance\" of those models. LLMs are trained to be highly efficient assistants, often avoiding expressions of uncertainty or negative responses. This may explain also why \"yes\" answers may be preferred to \"no\" answers since they may convey a better user-experience. The second replication and its follow-up experiments highlight that LLMs do not consistently serve as suitable substitutes for human participants, as the differences observed in human ratings are often neutralized or even reversed. While GPT-40-mini exhibits superior performance in detecting neologisms, this advantage becomes a limitation in the context of the original study, which seeks to examine human strategies for identifying new words. The model's high accuracy, particularly in Conditions (2) and (3), where human participants demonstrated lower detection rates, indicates that it may rely on different underlying mechanisms than those employed by humans. However, when the task is structured as a sequence of reasoning steps (CoT), as implemented in our second follow-up experiment, the model's responses improve in accuracy on filler items. This underscores the potential necessity of a greater adaptation of linguistic research methodologies in their replication with LLMs, as approaches effective for human participants may not be directly transferable to LLM-based pipelines."}, {"title": "Conclusion", "content": "This work introduces a framework to replicate empirical linguistic tasks, which are carried out with human participants, with the help of LLMs. Under this framework, OpenAI's GPT-40-mini is implied to reproduce two forced tasks with different linguistic foci. Two general tendencies arise in the analysis of reproductions outcomes. First, the empirical results demonstrate the effectiveness of the proposed framework in achieving great task accuracies. GPT-4o-mini outperforms human participants in all experimental conditions tested. Second, alignment patterns in LLMs' decision- making processes with human judgments are outlined especially in the first replication. The second task reproduced shows that GPT-40-mini may overperfom and not be always an adequate substitute of human participants. However, the two follow-up experiments demonstrate the adaptation and extension potential of our research design, focusing on ways to refine model's outputs. Overall, our framework has some limitations. The number of replications, linguistic subfields, and model's architectures tested in the present paper is restricted and should constitute a starting point for further research. Future studies could also test the replication design with open-source LLMs to have better control over the availability of data for the model. In conclusion, our replications use only one model, GPT-40-mini, which, despite being one of the newest and most powerful of the OpenAI family, does not provide the opportunity for further comparisons."}, {"title": "Appendix A: Supplementary materials", "content": "The Python notebook that contains the code used for in present paper can be accessed at this link. The supplementary materials of Cruz and Lombard et al. were reorganized in a new format that can be found here."}, {"title": "Appendix B: Results from the conducted replications.", "content": null}, {"title": "Appendix C: Prompts versions across all experiments.", "content": null}]}