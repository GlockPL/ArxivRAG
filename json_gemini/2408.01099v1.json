{"title": "Contribution-based Low-Rank Adaptation with\nPre-training Model for Real Image Restoration", "authors": ["Dongwon Park", "Hayeon Kim", "Se Young Chun"], "abstract": "Recently, pre-trained model and efficient parameter tuning\nhave achieved remarkable success in natural language processing and\nhigh-level computer vision with the aid of masked modeling and prompt\ntuning. In low-level computer vision, however, there have been limited in-\nvestigations on pre-trained models and even efficient fine-tuning strategy\nhas not yet been explored despite its importance and benefit in various\nreal-world tasks such as alleviating memory inflation issue when inte-\ngrating new tasks on AI edge devices. Here, we propose a novel efficient\nparameter tuning approach dubbed contribution-based low-rank adap-\ntation (CoLoRA) for multiple image restorations along with effective\npre-training method with random order degradations (PROD). Unlike\nprior arts that tune all network parameters, our COLORA effectively fine-\ntunes small amount of parameters by leveraging LoRA (low-rank adap-\ntation) for each new vision task with our contribution-based method\nto adaptively determine layer by layer capacity for that task to yield\ncomparable performance to full tuning. Furthermore, our PROD strat-\negy allows to extend the capability of pre-trained models with improved\nperformance as well as robustness to bridge synthetic pre-training and\nreal-world fine-tuning. Our CoLoRA with PROD has demonstrated its\nsuperior performance in various image restoration tasks across diverse\ndegradation types on both synthetic and real-world datasets for known\nand novel tasks.", "sections": [{"title": "1 Introduction", "content": "Image restoration (IR) is a fundamental low-level computer vision task that\naims to recover the original clean image from the input data that was degraded\nby noise [29, 39, 92, 101], blur [36, 37, 55, 70, 80], and / or bad weather condi-\ntions [40,81,88,95]. It does not only enhance the visual quality of images, but\nalso improves the performance of mid to high-level vision downstream tasks such\nas classification [26,35], object detection [67,69], and autonomous driving [3,51]."}, {"title": "2 Related works", "content": "2.1 NLP and High-level Vision Tasks\nPre-training. In NLP, self-supervised pre-training utilizing large models and\nbillions of data [4, 17,66] is a fundamental option. These methods train the\nmodel to predict missing content by hiding part of the input sequence. Vari-\nous self-supervised pre-training methodologies [19,21] have also been proposed\nin high-level computer vision fields. Recently, contrastive learning [9,24] and\ntransformer-based masked autoencoder methods [23,83] have emerged, enhanc-\ning semantic information learning. These pre-trained networks provide better"}, {"title": "2.2 Low-Level Vision Tasks", "content": "Image restoration for multiple degradations. Recently, methodologies [8,\n14,54,75,78,86,87] have been proposed that use a single network architecture to\nbuild multiple independent restoration models, each trained on different degra-\ndation datasets, demonstrating high performance in various degradation tasks.\nHowever, these methods require numerous network parameters as it necessitates\nan independent network trained for each degradation tasks. To address this,\nan all-in-one IR methods [12, 41, 42, 42, 46, 53, 57, 57, 76, 90,99] have been pro-\nposed. Firstly, there was a method [12] of learning a unified network for various\ndegradations through knowledge distillation techniques. Secondly, there were\nmethods [41, 46, 53, 76,90] of using an adaptor to enable the unified network to\nadapt to various degradations. Lastly, there were methods [42,57,99] of using\nan additional module corresponding to a specific degradation in a unified model,\nincluding a classifier that selects the additional module. All-in-one IR methods\nachieved high performance in various degradation tasks. However, these meth-\nods have the limitation of requiring to re-train the unified model and adaptor\nor classifier to extend to new tasks. Furthermore, all-in-one methods train from\nscratch without any pre-trained model.\nPre-training with synthetic data. Constructing a pre-trained model for IR\ntasks requires a significant quantity of low-quality images paired with their high-\nquality counterparts [38, 93]. However, obtaining such image pairs in the real\nworld presents considerable cost and difficulty [48]. To deal with such problems,\npre-training methods using synthetic degradation functions have been proposed\nas illustrated in Fig. 1. IPT [7] and EDT [43] proposed methods that select"}, {"title": "3 Method", "content": "We propose a Contribution based efficient LoRA (COLORA) with Pre-training\nwith Random Order Degradation (PROD) for IR, as illustrated in Fig. 2. Sec-\ntion 3.1 introduces the pre-training method PROD, and Section 3.2 investigates\nthe quantified contribution to each layer. In Section 3.3, we propose COLORA\nthat adjusts the ratio of learnable network parameter based on contributions."}, {"title": "3.1 Pre-training with Random Order Degradation (PROD)", "content": "We propose a PROD that randomly applies synthetic degradation to clean im-\nages during the pre-training, as illustrated in Fig. 2. Detailed information on\ndistortion functions and magnitudes are in the supplementary materials. Apply-\ning 1 to N synthetic degradations to a clean image, PROD can represent a total\nof (HN+1 \u2212 1)/(H \u2212 1) different types of degraded images where N is set to 6 and\nH(= 5) represents the number of degradation functions. This PROD method en-\nables about 137K kinds of degradation representations, which is about 4K times\nmore than the previous single [7,43] and fixed order [48] synthetic degradation\nmethods. Note that a similar random degradation idea was proposed in [91]\nand [15], but it was designed for super resolution and classification, not for pre-\ntraining for multiple tasks. The experimental results are in the supplementary\nmaterial. The PROD significantly expands the scalability and generalization of\nthe pre-train model, bringing excellent performance in IR with real data."}, {"title": "3.2 The Key Components of Fine-Tuning for a New Task", "content": "We utilized Filter Attribution method based on Integral Gradient (FAIG) [82]\nscores to quantify the major contributing network parts for new IR tasks. FAIG\nis measured through Integrated Gradients (IG) calculations using pre-trained\nand fine-tuned models. The FAIG calculation for each layer involves the baseline\nmodel (Oba) and the target model (@ta), defined as follows:\n$FAIG(C_t(\\Theta_{ba}, \\Theta_{ta}), x) = \\sum_{i=1}^{I} \\sum_{j=1}^{M} |p^{ta}_{i,j} - p^{ba}_{i,j}|$\nwhere $p = \\int_{0}^{1} \\frac{\\partial L[p(\\beta_{t}),x]}{\\partial \\rho(\\beta)} d\\beta$\n                                                    (1),\nwhere M represents the total number of steps in the integral approximation,\nsetting to 100 as in FAIG. \u1e9et, j, i and pare t/M, the kernel index, layer index"}, {"title": "3.3 Contribution-based Low-Rank Adaptation (CoLoRA)", "content": "Hu et al. [28] proposed a method known as LoRA aimed at fine-tuning only small\nnetwork parameters. The training weight matrix W is represented as follows:\n$Wo + W = Wo + BA$,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (2),\nwhere Wo \u2208 Rd\u00d7k, B \u2208 Rd\u00d7r, A \u2208 Rr\u00d7k, and the r < min(d, k) is pre-trained\nweight matrix, projection weight matrices, and rank, respectively. In the fine-\ntuning phase, A and B consist of learnable parameters while Wo remains frozen\nand does not undergo gradient updates. The output vectors are coordinate-wise\nsum of Wo and AW = BA, which have the same input. The conventional LORA\nemploys a fixed rank value of r for all layers of the network because optimizing\nthe low-rank parameter for each layer is infeasible, thus imposes constraints on\nthe efficient utilization of parameters and limits its overall performance.\nTo address these limitations, we propose CoLoRA, a method that flexibly\nadjusts the ratio of learnable network parameter based on contributions. In Sec-\ntion 3.2, we adjust the ratio of learnable parameters 6 by applying different\nvalues of r for each layer based on the quantified FAIG score. The d in COLORA,\naccording to the size of r, is defined as di = ri(di+ki)/(di\u00d7ki), where i denotes\nthe network layer index, and d and k represent the size of Wo. For the FAIG\nscores measured for each block as in Fig. 3 (a), excluding the intro and end lay-\ners, we normalize to ensure the maximum value = 1. Using the normalized FAIG\nscore, we propose a threshold-based scaling to determine & without increasing"}, {"title": "4 Experiments", "content": "Experiment setups. We evaluated the proposed CoLoRA with PROD on 6 real\nIR tasks using CNN-based NAFNet [8] and transformer-based Restormer [86].\nTo evaluate our method, we compare it with the previously pre-trained method-\nology, DegAE [48]. In Restormer, the DegAE [48] uses publicly released pre-\ntrained weights, and in NAFNet, the DegAE [48] was reproduced based on the\npublicly released code. In CoLoRA, we experimentally used a = 1 and \u03b2 = 0.2\nfor NAFNet, and a = 0.75 and \u1e9e = 0.1 for Restormer. The learning iteration for\nthe pre-training model and fine-tuning were set to 200K and 10K, respectively.\nThe learning rate starts at 1e-3 for NAFNet and 3e-4 for Restormer and gradu-\nally decreases to le-6 according to the cosine annealing schedule. The evaluation\nof experiment results was performed using PSNR. In pre-training, PSNR loss\nwas used, and in the fine-tuning process, NAFNet and Restormer used PSNR\nloss and L1, respecitvely. Detailed information is in the supplementary material.\nSix real-world image restoration task datasets: To evaluate our proposed\nmethod, we conducted experiments using 6 real-world Image Restoration (IR)\ndatasets: Real Rain, Raindrop, Rain&Raindrop (RainDS [65]), Noise (SIDD [1]),\nHaze (SMOKE [33]), and Blur (BSD [96]). RainDS [65] consists of 120 training\ndata and 100 test data for each task, Rain and Raindrop, Rain and Raindrop.\nSMOKE [33] includes of 120 training images and 12 test images. BSD [96] com-\nprises of 18,000 training images and 3,000 test images (2ms-16ms). SIDD [96]\ndata comprises of 160 training images and 1,280 validation samples."}, {"title": "4.1 Benchmark Results for Real Various IR Tasks", "content": "In Table 1, we summarized the benchmark results on the real Rain, Raindrop,\nRain&Raindrop, Haze and Blur datasets to evaluate our proposed CoLoRA with\nPROD. To make a fair comparison with previous methods, we trained the model\nusing the entire training dataset from the real various IR task datasets. For\neach task, all methods except DegAE and Our PROD are initialized randomly\nwithout a pre-trained model. All methods except Our CoLoRA perform full\nfine-tuning on new tasks. In full fine-tuning, NAFNet and Restormer have 29 M\nand 26 M (100%) trainable parameters, respectively. The CoLoRA method have\n2M and 2.9 M (7% and 11% of the total) trainable parameter in NAFNet and\nRetormer, respectively. Our PROD achieved state-of-the-art PSNR on datasets\ncontaining Real Rain, Raindrop, Rain&Raindrop, Haze and Blur. Our CoLoRA"}, {"title": "4.2 Comparing Proposed Methods in Limited Real-world Scenario", "content": "In Fig. 4, we conducted a comparative study to demonstrate the performance\nand efficiency of the parameter-efficient fine-tuning method, CoLoRA, and the\nproposed pre-training method PROD, depending on the number of training data.\nTo summarize each experiment, (a) \u201cPre-training Methods\" presents the exper-\nimental results based on different pre-training methods, (b) \"Tuning Strategies\"\npresents the results based on efficient parameter tuning methods, and (c)&(d)\n\"Different Network Architectures\" provides the experimental results based on\npre-training and tuning methods on CNN and vision transformer architectures.\""}, {"title": "5 Discussion", "content": "Empirical analysis on pre-train methods without any fine-tuning: To\nobserve the scalability and generalization capabilities of our pre-training method,\nPROD, we conduct an empirical analysis on 6 real IR tasks without any fine-\ntuning. The investigated pre-training methods consist of untrained Fixed, Single,\nDeGAE, and PROD. Table 3 summarizes the PSNR for 6 real IR tasks based\non pre-training methods without any fine-tuning. Our proposed PROD achieves\nbetter results even when not trained for degradation, surpassing previous ap-\nproaches. Fig. 6 (a) illustrates the degradation representations of various IR\ntasks using t-SNE, based on pre-training methods without fine-tuning. Features\nare extracted from the last layer of the middle block of the pre-trained NAFNet\nfor t-SNE plotting, followed by global average pooling, and t-SNE was plotted\nfor 100 samples. We believe that the embedded scalability and generalization\nability of PROD shows promising performance on the above IR results.\nWhy CoLoRA, practical deployment: Considering that IR can be executed\nimmediately after image acquisition and used for on-device AI, the efficiency,\nrobustness, and scalability of CoLoRA are very important. Compared to full-\ntuning, CoLoRA excels in efficiency, robustness, and scalability. For six tasks,\nCOLORA has fewer parameters than Full and LORA (41M for COLORA, 41M"}, {"title": "S1 Details on the proposed CoLoRA with PROD", "content": "In Figure S1, we compare our COLORA - PROD method with existing meth-\nods [7, 43, 48]. Figure S1 illustrates the differences between PROD, which in-\ntroduces degradation during pre-training, and other prior works, as well as the\ndistinctions between CoLoRA, a efficient tuning approach, and simple full fine-\ntuning. Specifically, in simple full fine-tuning, the entire network is tuned. In\ncontrast, our CoLoRA approach is a parameter-efficient tuning methodology\nthat flexibly adjusts the ratio of learnable network parameters based on layer\npositions. While prior works must have a separate full fine-tuned network for\neach novel IR task, our proposed COLORA only need to store the small tunable\nadaptor for each task (approximately 7% of the entire network parameters for\ncomparable performance to full-size tuning), so that it will be advantageous in\nterms of memory for multiple target tasks."}, {"title": "S2 Details on synthetic degradation functions of PROD", "content": "The environmental settings for synthetic blur, noise and JPEG functions were\nsimilar to DegAE [48]. In the synthetic blur function (fBlur), we employ Gaussian"}, {"title": "S3 Ablation studies for CoLoRA", "content": "In Table. S1, we performed an additional ablation study on bias and normal-\nization for deblurring. \"Bias & Norm\" improves performance by 0.08 dB with\nadditionally tuned parameter (0.1M), which is consistent with our FAIG analy-\nsis. The level of importance could be different for various tasks."}, {"title": "S4 Details on the d of the proposed CoLoRA.", "content": "The LORA [28] method uses a fixed value of r for all layers. On the other hand,\nour proposed CoLoRA adjusts the ratio of learnable parameters (8) by using\ndifferent values of r depending on the layer location, as shown in Figure S3.\nThe d in COLORA, according to the size of r, is defined as follows: di = (ri(di+\nki))/(di\u00d7ki), where i denotes the network layer index, and d and k represent the\nsize of Wo. The proposed method has designed the size of r to use a small ratio\nof learnable network parameters (approximately 7%) in the entire network for\neach task. Thanks to CoLoRA, additional IR operations use only small network\nparameters, significantly reducing memory usage. Table S2 shows Norm(FAIG)\nvalues according to location in NAFNet and Restormer. For real 6 IR tasks,\nNorm(FAIG) values show the average value and standard deviation. The middle\npart has a very small value compared to the encoder part and decoder part."}, {"title": "S5 Details of experiment setups", "content": "For a fair comparison, we evaluate the proposed method under the same condi-\ntions using PyTorch [58] on an NVIDIA A100 GPU. All our experiments were\nbased on the code published by NAFNet [8] and Restormer [86]. In all our exper-\niments, the channel dimension of NAFNet [8] is set to 32, the channel dimension\nof Restormer [86] is configured to 48. In NAFNet, the Encoder block is con-\nfigured as [2,2,4,8], the Middle block has 12 layers, and the Decoder block is\nstructured as [2,2,2,2]. In Restormer, the Encoder block is composed of [4,6,6], with 8 Middle blocks, and the Decoder block is structured as [6,6,4]. We trained\nthe model with the AdamW [50] optimizer (\u03b21 = 0.9, \u03b22 = 0.9, weight decay\n1e-3), and the training patch size is set to 256 \u00d7 256. In NAFNet, the batch size\nis set to 32, and in Restormer, the batch size is set to 4. The initial learning\nrate starts at 3e - 4 and gradually decreases to le - 6 according to the cosine\nannealing schedule. Data augmentation for training such as random crop, hori-\nzontal flip, and 90-degree rotation were used. In the case of DegAE [48], we used"}, {"title": "S6 Details of evaluation metrics", "content": "To ensure a fair comparison of our proposed method, we evaluated its perfor-\nmance using PSNR and SSIM evaluation metrics. Since evaluation metrics on the\nY component in the YUV domain or in the RGB domain tends to exhibit similar\ntrends, we exclusively perform certain measurements, such as Rain, Raindrop,\nand Rain&Raindrop, on the Y component. Specifically in the case of bench-\nmark, Rain, Raindrop, and Rain&Raindrop, we applied evaluation metrics only\nto the Y component in the YUV domain. In the benchmark results presented in"}, {"title": "S7 Comparison of pre-train methods and tuning\nstrategies for extremely limited training data", "content": "In this section, we evaluated the proposed CoLoRA on 6 IR datasets with DegAE\nand a training iteration is 10K. Table S3 summarizes the PSNR results for six real\nIR tasks with extremely small number of training data during fine-tuning in\nNAFNet. The extremely small number of training data for each tasks were set to\n[2, 4, 8]. The full fine-tuning method (Full) has 29 M trainable parameters in\nNAFNet [8]. In NAFNet, the full fine-tuning (Full) method requires 6 times\nmore network parameters 174M (29M\u00d76) than the original network parameters\n29M due to explicitly separated structures for multiple degradations, and our\nproposed CoLoRA uses an additional 2M parameters for a single task, resulting\nin a total of 41M (29M+2M\u00d76) parameter for the network required in the 6\nIR tasks. These results demonstrate that the our proposed PROD + Full and\nPROD + CoLoRA method effectively and efficiently improves various IR per-\nformances, with extremely limited data."}, {"title": "S7.1 Ablation study on scales (a and \u03b2) of CoLoRA (Restormer)", "content": "In Table S4, we investigated the PSNR results based on the scaling factors (\u03b1,\u03b2)\nof COLORA to further optimize the tuned network parameters. All experiments\nare conducted using the transformer-based Restormer with full training data.\nAs the values of a and \u1e9e increase, the performance improves, and accordingly,\nthe tuned network parameters also increase. We have experimentally selected\n(\u03b1 = 0.75, \u03b2 = 0.1) that efficient and brings the highest performance. Through\nscale adjustment, it is possible to use the optimal parameters for each task."}, {"title": "S8 Details on tuning strategies in NAFNet", "content": "Table S5 provides detailed tuned parameters for experiments based on tuning\nstrategies. The tuned parameter are in millions (M). The Full method has 29\nM trainable parameters. The Only Decoder, LoRA, and CoLoRA(Ours) have\napproximately 2 M trainable parameters. The LoRA [28] is constructed based\non open-source code, with the value of r set to 16 for all layers."}, {"title": "S9 Our proposed PROD vs BSRGAN", "content": "Indeed, BSRGAN developed another pre-trained method with multiple degrada-\ntions only for SR, while our PROD is a method for multiple IR tasks. Neverthe-\nless, we performed experiments using 64 fine-tuning datasets on rain/drop and\nblur, demonstrating the superiority of our work over BSRGAN (PROD 31.4dB vs\nBSRGAN 30.8dB for deblur and PROD 22.7dB vs BSRGAN 22.1dB for derain)."}, {"title": "S10 Benchmark results", "content": "We evaluated our proposed CoLoRA with PROD on the mixed Snow&Haze [11]\nbenchmark dataset in Table S6. The mixed Snow&Haze [11] benchmark dataset\ncontains 10,000 training images and 3,000 test images. \"All-in-One image restora-\ntion\" may experience a decrease in performance as the number of tasks increases,\nand there are inherent limitations in extending it to new tasks. On the other\nhand, \"Efficient Fine-tuning\" maintains consistent results even as the number of\ntasks increases, and it is easily scalable to accommodate new tasks. In Park [57],\nthe results are presented for All-in-One image restoration methods when han-\ndling only three tasks. Thanks to COLORA with PROD, the proposed method\nin NAFNet architecture provides the highest PSNR compared to existing meth-\nods, even though only a small ratio of tuned parameters are learned in the entire\nnetwork."}, {"title": "S11 Comparison of tuning strategies in DeGAE", "content": "In this section, we evaluated the proposed CoLoRA on 6 IR datasets with DegAE\nand a training iteration is 10K. Figure S4 summarizes the experimental results\nbased on DegAE with CoLoRA. All experiments are conducted using the CNN-\nbased NAFNet architecture. We investigated the following tuning methods: Full\nfine-tuning (Full), and the proposed CoLoRA method. The Full method has\n29 M trainable parameters. CoLoRA(Ours) have approximately 2 M trainable\nparameters. Our proposed \"CoLoRA + DegAE\" method produces results similar\nto \"Full + DeGAE\" while tuning only very small network parameters."}, {"title": "S12 Details on comparison of pre-train methods and\ntuning strategies", "content": "We conducted a detailed comparative study to demonstrate the performance\nand efficiency of the proposed pre-training method PROD, and the parameter-\nefficient fine-tuning method, CoLoRA. To summarize each experiment, \"pre-\ntraining methods\" presents the experimental results based on different pre-\ntraining methods in Figure S5 (top), \"tuning strategies\" presents the results\nbased on efficient parameter tuning methods in Figure S5 (bottom), and \u201cdif-\nferent network architectures\u201d provides the experimental results based on pre-\ntraining and tuning methods on CNN and vision transformer architectures in\nFigure S6. Figure S5 and S6 illustrate a detailed summary of the PSNR (dB)\nresults for 6 IR tasks with real data in relation to the number of training data\nduring fine-tuning. The number of training data for each tasks were set to [16,\n32, 64, 128]. These results demonstrate that the our proposed PROD + Full\nand PROD + CoLoRA method effectively and efficiently improves various IR\nperformances, even with limited training data and a sparse number of learnable\nparameters, underscoring its effectiveness.\nFigure S7, Figure S8 and S9 illustrates the results for 6 IR task with test\ndata tasks using test data, comparing outcomes between previous methods and\nour proposed CoLoRA with PROD. Our methods (PROD + Full and PROD +\nCoLoRA) appear to produce superior restoration results compared to Random\n+ Full and DegAE + Full [48]."}]}