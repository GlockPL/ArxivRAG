{"title": "Contribution-based Low-Rank Adaptation with Pre-training Model for Real Image Restoration", "authors": ["Dongwon Park", "Hayeon Kim", "Se Young Chun"], "abstract": "Recently, pre-trained model and efficient parameter tuning have achieved remarkable success in natural language processing and high-level computer vision with the aid of masked modeling and prompt tuning. In low-level computer vision, however, there have been limited investigations on pre-trained models and even efficient fine-tuning strategy has not yet been explored despite its importance and benefit in various real-world tasks such as alleviating memory inflation issue when integrating new tasks on AI edge devices. Here, we propose a novel efficient parameter tuning approach dubbed contribution-based low-rank adaptation (CoLoRA) for multiple image restorations along with effective pre-training method with random order degradations (PROD). Unlike prior arts that tune all network parameters, our COLORA effectively fine-tunes small amount of parameters by leveraging LoRA (low-rank adaptation) for each new vision task with our contribution-based method to adaptively determine layer by layer capacity for that task to yield comparable performance to full tuning. Furthermore, our PROD strategy allows to extend the capability of pre-trained models with improved performance as well as robustness to bridge synthetic pre-training and real-world fine-tuning. Our CoLoRA with PROD has demonstrated its superior performance in various image restoration tasks across diverse degradation types on both synthetic and real-world datasets for known and novel tasks.", "sections": [{"title": "1 Introduction", "content": "Image restoration (IR) is a fundamental low-level computer vision task that aims to recover the original clean image from the input data that was degraded by noise [29, 39, 92, 101], blur [36, 37, 55, 70, 80], and / or bad weather conditions [40,81,88,95]. It does not only enhance the visual quality of images, but also improves the performance of mid to high-level vision downstream tasks such as classification [26,35], object detection [67,69], and autonomous driving [3,51]."}, {"title": "2 Related works", "content": "2.1 NLP and High-level Vision Tasks\nPre-training. In NLP, self-supervised pre-training utilizing large models and billions of data [4, 17,66] is a fundamental option. These methods train the model to predict missing content by hiding part of the input sequence. Various self-supervised pre-training methodologies [19,21] have also been proposed in high-level computer vision fields. Recently, contrastive learning [9,24] and transformer-based masked autoencoder methods [23,83] have emerged, enhancing semantic information learning. These pre-trained networks provide better"}, {"title": "2.2 Low-Level Vision Tasks", "content": "Image restoration for multiple degradations. Recently, methodologies [8, 14,54,75,78,86,87] have been proposed that use a single network architecture to build multiple independent restoration models, each trained on different degradation datasets, demonstrating high performance in various degradation tasks. However, these methods require numerous network parameters as it necessitates an independent network trained for each degradation tasks. To address this, an all-in-one IR methods [12, 41, 42, 42, 46, 53, 57, 57, 76, 90,99] have been proposed. Firstly, there was a method [12] of learning a unified network for various degradations through knowledge distillation techniques. Secondly, there were methods [41, 46, 53, 76,90] of using an adaptor to enable the unified network to adapt to various degradations. Lastly, there were methods [42,57,99] of using an additional module corresponding to a specific degradation in a unified model, including a classifier that selects the additional module. All-in-one IR methods achieved high performance in various degradation tasks. However, these methods have the limitation of requiring to re-train the unified model and adaptor or classifier to extend to new tasks. Furthermore, all-in-one methods train from scratch without any pre-trained model.\nPre-training with synthetic data. Constructing a pre-trained model for IR tasks requires a significant quantity of low-quality images paired with their high-quality counterparts [38, 93]. However, obtaining such image pairs in the real world presents considerable cost and difficulty [48]. To deal with such problems, pre-training methods using synthetic degradation functions have been proposed as illustrated in Fig. 1. IPT [7] and EDT [43] proposed methods that select"}, {"title": "3 Method", "content": "We propose a Contribution based efficient LoRA (COLORA) with Pre-training with Random Order Degradation (PROD) for IR, as illustrated in Fig. 2. Section 3.1 introduces the pre-training method PROD, and Section 3.2 investigates the quantified contribution to each layer. In Section 3.3, we propose COLORA that adjusts the ratio of learnable network parameter based on contributions."}, {"title": "3.1 Pre-training with Random Order Degradation (PROD)", "content": "We propose a PROD that randomly applies synthetic degradation to clean images during the pre-training, as illustrated in Fig. 2. Detailed information on distortion functions and magnitudes are in the supplementary materials. Applying 1 to N synthetic degradations to a clean image, PROD can represent a total of $(\\mathcal{H}^{N+1} - 1)/(\\mathcal{H} - 1)$ different types of degraded images where N is set to 6 and $\\mathcal{H}(= 5)$ represents the number of degradation functions. This PROD method enables about 137K kinds of degradation representations, which is about 4K times more than the previous single [7,43] and fixed order [48] synthetic degradation methods. Note that a similar random degradation idea was proposed in [91] and [15], but it was designed for super resolution and classification, not for pre-training for multiple tasks. The experimental results are in the supplementary material. The PROD significantly expands the scalability and generalization of the pre-train model, bringing excellent performance in IR with real data."}, {"title": "3.2 The Key Components of Fine-Tuning for a New Task", "content": "We utilized Filter Attribution method based on Integral Gradient (FAIG) [82] scores to quantify the major contributing network parts for new IR tasks. FAIG is measured through Integrated Gradients (IG) calculations using pre-trained and fine-tuned models. The FAIG calculation for each layer involves the baseline model $(\\Theta_{ba})$ and the target model $(\\Theta_{ta})$, defined as follows:\n$\\FAIG_i(\\Theta_{ba}, \\Theta_{ta}, x) \\approx \\sum_{j=1}^{M-1} \\frac{1}{M}  [\\frac{\\partial L(p(\\beta_{t, j}),x)]}{\\partial p(\\beta_{t,j})}]_{t=0}^{\\beta_{i,j,i}};$"}, {"title": "3.3 Contribution-based Low-Rank Adaptation (CoLoRA)", "content": "Hu et al. [28] proposed a method known as LoRA aimed at fine-tuning only small network parameters. The training weight matrix $W$ is represented as follows:\n$W_o + \\Delta W = W_o + BA,$"}, {"title": "4 Experiments", "content": "Experiment setups. We evaluated the proposed CoLoRA with PROD on 6 real IR tasks using CNN-based NAFNet [8] and transformer-based Restormer [86]. To evaluate our method, we compare it with the previously pre-trained methodology, DegAE [48]. In Restormer, the DegAE [48] uses publicly released pre-trained weights, and in NAFNet, the DegAE [48] was reproduced based on the publicly released code. In CoLoRA, we experimentally used \u03b1 = 1 and \u03b2 = 0.2 for NAFNet, and \u03b1 = 0.75 and \u03b2 = 0.1 for Restormer. The learning iteration for the pre-training model and fine-tuning were set to 200K and 10K, respectively. The learning rate starts at 1e-3 for NAFNet and 3e-4 for Restormer and gradually decreases to 1e-6 according to the cosine annealing schedule. The evaluation of experiment results was performed using PSNR. In pre-training, PSNR loss was used, and in the fine-tuning process, NAFNet and Restormer used PSNR loss and L1, respecitvely. Detailed information is in the supplementary material.\nSix real-world image restoration task datasets: To evaluate our proposed method, we conducted experiments using 6 real-world Image Restoration (IR) datasets: Real Rain, Raindrop, Rain&Raindrop (RainDS [65]), Noise (SIDD [1]), Haze (SMOKE [33]), and Blur (BSD [96]). RainDS [65] consists of 120 training data and 100 test data for each task, Rain and Raindrop, Rain and Raindrop. SMOKE [33] includes of 120 training images and 12 test images. BSD [96] comprises of 18,000 training images and 3,000 test images (2ms-16ms). SIDD [96] data comprises of 160 training images and 1,280 validation samples."}, {"title": "4.1 Benchmark Results for Real Various IR Tasks", "content": "In Table 1, we summarized the benchmark results on the real Rain, Raindrop, Rain&Raindrop, Haze and Blur datasets to evaluate our proposed CoLoRA with PROD. To make a fair comparison with previous methods, we trained the model using the entire training dataset from the real various IR task datasets. For each task, all methods except DegAE and Our PROD are initialized randomly without a pre-trained model. All methods except Our CoLoRA perform full fine-tuning on new tasks. In full fine-tuning, NAFNet and Restormer have 29 M and 26 M (100%) trainable parameters, respectively. The CoLoRA method have 2M and 2.9 M (7% and 11% of the total) trainable parameter in NAFNet and Retormer, respectively. Our PROD achieved state-of-the-art PSNR on datasets containing Real Rain, Raindrop, Rain&Raindrop, Haze and Blur. Our CoLoRA"}, {"title": "4.2 Comparing Proposed Methods in Limited Real-world Scenario", "content": "In Fig. 4, we conducted a comparative study to demonstrate the performance and efficiency of the parameter-efficient fine-tuning method, CoLoRA, and the proposed pre-training method PROD, depending on the number of training data. To summarize each experiment, (a) \u201cPre-training Methods\" presents the experimental results based on different pre-training methods, (b) \"Tuning Strategies\" presents the results based on efficient parameter tuning methods, and (c)&(d) \"Different Network Architectures\" provides the experimental results based on pre-training and tuning methods on CNN and vision transformer architectures."}, {"title": "4.3 Ablation Study On Scale Values (a and \u03b2) of CoLoRA", "content": "In Table 2, we investigated the PSNR results based on COLORA's scaling factors (\u03b1,\u03b2) for NAFNet and Restormer, to optimize network parameters. Selecting a and \u03b2 depends on the trade-off between performance and memory. We found that using small values for \u03b2 (0.1-0.2) was efficient while using large values for \u03b1 (0.7-1.0) was effective. We selected \u03b1 = 1, \u03b2 = 0.2 for NAFNet and \u03b1 = 0.75, \u03b2 = 0.1 for Restormer for the highest performance efficiently. With these criteria, \u03b1 and \u03b2 can be practically chosen depending on the target device and the applied task."}, {"title": "5 Discussion", "content": "Empirical analysis on pre-train methods without any fine-tuning: To observe the scalability and generalization capabilities of our pre-training method, PROD, we conduct an empirical analysis on 6 real IR tasks without any fine-tuning. The investigated pre-training methods consist of untrained Fixed, Single, DeGAE, and PROD. Table 3 summarizes the PSNR for 6 real IR tasks based on pre-training methods without any fine-tuning. Our proposed PROD achieves better results even when not trained for degradation, surpassing previous approaches. Fig. 6 (a) illustrates the degradation representations of various IR tasks using t-SNE, based on pre-training methods without fine-tuning. Features are extracted from the last layer of the middle block of the pre-trained NAFNet for t-SNE plotting, followed by global average pooling, and t-SNE was plotted for 100 samples. We believe that the embedded scalability and generalization ability of PROD shows promising performance on the above IR results.\nWhy CoLoRA, practical deployment: Considering that IR can be executed immediately after image acquisition and used for on-device AI, the efficiency, robustness, and scalability of CoLoRA are very important. Compared to full-tuning, CoLoRA excels in efficiency, robustness, and scalability. For six tasks, COLORA has fewer parameters than Full and LORA (41M for COLORA, 41M"}, {"title": "6 Conclusion", "content": "We proposed CoLoRA with PROD, pre-training with synthetic data and efficient parameter tuning in both abundant and limited real-data scenarios for image restoration. Our PROD yielded excellent pre-trained model as compared to prior arts and our CoLoRA enabled efficient fine-tuning only about 7% learnable parameters. We demonstrate that our proposed method is flexible enough to work with diverse network architectures and achieves state-of-the-art performance on various IR tasks with real-world datasets."}]}