{"title": "NEURON PLATONIC INTRINSIC REPRESENTATION FROM DYNAMICS USING CONTRASTIVE LEARNING", "authors": ["Wei Wu", "Can Liao", "Zizhen Deng", "Zhengrui Guo", "Jinzhuo Wang"], "abstract": "The Platonic Representation Hypothesis posits that behind different modalities of data (what we sense or detect), there exists a universal, modality-independent representation of reality. Inspired by this hypothesis, we treat each neuron as a system, where we can detect the neuron's multi-segment activity data under different peripheral conditions. We believe that, similar to the Platonic idea, there exists a time-invariant representation behind different segments of the same neuron, which reflects the intrinsic properties of the neuron's system. Intrinsic properties include the molecular profiles, location within brain regions and morphological structure, etc. The optimization objective for obtaining intrinsic neuronal representations should meet two criteria: (I) the representations of recording segments from the same neuron must exhibit higher similarity compared to those from different neurons; (II) the representations should generalize effectively to out-of-domain data. To this end, we propose the NeurPIR (Neuron Platonic Intrinsic Representation) framework, which leverages contrastive learning by treating segments from the same neuron as positive pairs and those from different neurons as negative pairs. In the implementation, we adopt VICReg, which only uses positive pairs while indirectly separating dissimilar samples through regularization terms. To validate the efficacy of our method, we first conducted tests on simulated neuronal population dynamics data generated by the Izhikevich model. The results confirmed that our approach accurately captured the neuron types as defined by the preset hyperparameters. Subsequently, we applied our method to two real - world neuron dynamics datasets, which included neuron type annotations derived from spatial transcriptomics and the location of each neuron within brain regions. The representations learned from our model not only accurately predicted neuron types and locations but also demonstrated robustness when tested on out-of-domain data (data from unseen animals). This finding underscores the potential of our approach in furthering the understanding of neuronal systems and offers valuable insights for future neuroscience research. Code is available at https://github.com/ww20hust/NeurPIR.", "sections": [{"title": "1 INTRODUCTION", "content": "Unraveling the intricacies of neuronal activity and the information encoded within neural dynamics stands as a monumental challenge in the field of neuroscience. Plato's cave allegory and Platonic Representation Hypothesis Huh et al. (2024) suggests the existence of a universal, modality-independent representation of world that transcends the modalities through which we perceive it. Drawing inspiration from this philosophical concept, we propose a novel perspective on neuronal"}, {"title": "2 RELATED WORK", "content": "Single Neuron Models: Single neuron models are essential in understanding the fundamental properties of neuronal dynamics and behavior. The Leaky Integrate-and-Fire model Liu & Wang (2001), developed in the 1950s, is a minimalistic model that simulates the integration of synaptic inputs and the leakage of membrane potential over time. The Hodgkin-Huxley model Nelson & Rinzel (1995), introduced in 1952, provides a detailed description of action potential generation using complex equations to represent ionic currents across the neuronal membrane, offering deep insights into neu-"}, {"title": "3 METHOD", "content": ""}, {"title": "3.1 GOAL", "content": "Our goal is to develop a method for learning intrinsic neuron representations from neuron population data. These representations should meet three key criteria: (i) neurons with similar functional roles should exhibit greater similarity in their representations compared to those with different roles; (ii) the learned representations should be robust to variations in neuronal activity patterns caused by different stimuli or environmental conditions; and (iii) the representations should be adaptable and generalizable to new and unseen neuronal activity patterns."}, {"title": "3.2 ARCHITECHTURE", "content": "The ideal embedding space for neuron representations should cluster recording segments of the same neuron while also ensuring semantic consistency by placing similar neurons close to each other within the space. In line with the criteria outlined in Section 3.1, We conducted advanced contrastive learning loss functions, VICReg Bardes et al. (2021). We also carefully designed the data sampling methods to generate multi-segments activity data for each neuron for training purposes.\nData sampling: Without loss of generality, we take bi-segment data as an example. Consider two scenarios: 1) If a neuron has recordings from multiple sessions, we randomly extract two segments $(X_{seg}, X_{seg})$ from different sessions. 2) If a neuron has recordings from only a single session, we randomly extract two non-overlapping segments $(X_{seg}, X'_{seg})$ from that session. We repeat this process B times for a batch size of B, obtaining a positive pair batch $(X_2, X_2)$."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 DATA", "content": "Simulated Data: Since the neuron intrinsic property is hardly available in vivo neuronal recordings, we applied to synthetic data where we can access the ground-truth intrinsic property. To make the synthetic data exhibit dynamics similar to that of real neurons, we simulated the data following the Izhikevich model. The Izhikevich model is a spiking neuron model that combines biological realism with computational efficiency. It is designed to capture the rich dynamics of real neurons while remaining computationally simple. The model is defined by the following differential equations:\n$\\frac{dV}{dt} = 0.04V^2 + 5V + 140 \u2013 u + I,$"}, {"title": "4.2 EVALUATION", "content": "Evaluation on Simulated Data: The process is divided into two steps: (i) perform self-supervised contrastive learning on the dynamic data of all neurons to obtain a representation for each neuron; (ii) Employ a 5-fold cross-validation approach to use these neuron representations as input to a classifier for predicting the pre-defined neuron type labels from the simulation.\nEvaluation on Real Data - Bugeon: The process is divided into three steps: (i) perform self-supervised contrastive learning on the dynamic data of all neurons from four mouse in the real dataset to obtain a representation for each neuron; (ii) Based on neurobiological prior knowledge and spatial transcriptomic gene expression information, assign cell type labels to each neuron. The labels fall into two categories: (a) excitatory and inhibitory, and (b) Lamp5, Pvalb, Vip, Sncg, and Sst; (iii) Implement a 4-fold (with the folds based on the identity of the mice) cross-validation approach where the neuron representations are used as input to a classifier to predict the neuron's type labels for both categories (a) and (b).\nOut-of-Domain Evaluation - Steinmetz dataset: The process is divided into three steps: (i) Perform self-supervised contrastive learning on the dynamic data of all neurons from all mice in the real dataset to obtain a representation for each neuron; (ii) As before, assign location within brain regions labels to the neurons; (iii) Implement a 10-fold (with the folds based on the identity of the mice) cross-validation approach where the neuron representations are used as input to a classifier to predict the neuron's location. During neurodevelopment, where the position of a neuron is crucial for its differentiation, maturation, and connectivity. The location can influence the neuron's gene expression, synaptic connections, and ultimately its function Patel & Poo (1982). In this sense, the location is an intrinsic property because it defines role within the nervous system."}, {"title": "4.3 COMPARISON OF METHODS", "content": "LOLCAT: This method Schneider et al. (2023a) follows a supervised learning paradigm. It directly uses activity data from a subset of neurons to train a classifier to predict neuron labels, and then validates on the remaining neurons. Consequently, the representations learned in the intermediate"}, {"title": "4.4 REAL DATA - NEURON PLATONIC INTRINSIC REPRESENTATION CONTAINS\nMOLECULAR INFORMATION", "content": "In this section, we utilized a public multimodal dataset, referred to as Bugeon, to train and evaluate our model. We compared the performance metrics of neuron type classification for three methodologies: NeuPRINT, LOLCAT, and the proposed NeurPIR (PCA and UMAP were excluded as they cannot process behavioral information). The evaluation metrics included precision (Prec.), recall (Rec.), and F1 score, which are essential for assessing the classification effectiveness across various neuron types, categorized into subclasses and classes."}, {"title": "4.5 REAL DATA - SHOWS ROBUSTNESS ON OUT-OF-DOMAIN (UNSEEN ANIMAL) DATA", "content": "In this section, we focus on validating the consistency of the intrinsic representations generated by the model across different animals. The Steinmetz dataset, which includes neural activity data from ten rats, provides neuron labels based on their respective brain regions. It can be intuitively seen from Figure 3 that the response patterns of neurons in the same brain area are significantly different in different mice, but we hope that the representation obtained from the model still contains consistent information, like consistent brain area information. This corresponds to the evaluation of the generalizability of the representations obtained by the model on cross-modal (here, cross-animal) data in deep learning. We use a task of classifying location within brain regions to validate the intrinsic representations. We use NeurPIR to perform self-supervised training on all the neurons from the all mice to obtain the intrinsic representations. For the downstream task of brain region classification, we used 10-fold cross-validation (with folds based on the identity of the mice).\nAs shown in Figure 4.4, the model's generalizability is evaluated through brain region classification across different neural representations. In-domain validation results demonstrate that consistently outperforms other methods like LOLCAT and NeuPRINT across all location within brain regions, including visual cortex (ViS), thalamus (Thal), hippocampus (Hipp), and midbrain (Mid). Specifically, achieves validation Precision close to 0.80 in most regions, with NeuPRINT slightly trailing behind.\nWhen examining out-of-domain performance (cross-animal), we notice a general drop in accuracy for all methods. However, NeurPIR still retains a higher degree of accuracy across all location within brain regions compared to NeuPRINT and LOLCAT, showing the model's ability to capture more robust intrinsic representations across different animals. This consistent outperformance across both in-domain and out-of-domain tests suggests that the representations obtained by better generalize across animals while still preserving critical brain region information."}, {"title": "5 CONCLUSION AND DISCUSSION", "content": "In this paper, we present a novel and scalable approach for extracting and leveraging the intrinsic properties of neurons. This approach holds significant potential for re-evaluating existing neuroscience data and enhancing our understanding of neural computation. Future research may focus on further enhancing its ability to handle an even more diverse range of datasets and applying it to other domains where extracting intrinsic properties from complex systems is of great importance. Limitations: (i) The representation learned by our method can only distinguish neurons with substantial differences in essential attributes. For instance, when neurons are associated with more refined brain-area labels, it becomes challenging to differentiate them, necessitating more data for training support. (ii) The learned neuron representations only support data collected from the same technology. The generalization of cross - platform data, such as two-photon data and neurpixel data, remains to be explored. (iii) Over very long timescales, some of the short-term invariant properties of neurons may change. This can be exploited to study the changes in neuronal properties during the progression of diseases like Alzheimer's disease."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "To enhance the reproducibility of this study, we provide an Appendix section comprising 4 subsections that offer detailed supplementary information. Appendix A.3 presents the pseudo-code of Synthetic Data. Appendix A.4 presents python code for downloading and organizing the steinmetz dataset. Appendix A.5 presents the pseudo-code of sownstream task. Appendix A.6 presents Description and Function across firing types / neuron types / brain regions in this paper."}]}