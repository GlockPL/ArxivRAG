{"title": "Do Robot Snakes Dream like Electric Sheep?\nInvestigating the Effects of Architectural Inductive Biases on Hallucination", "authors": ["Jerry Huang", "Prasanna Parthasarathi", "Mehdi Rezagholizadeh", "Boxing Chen", "Sarath Chandar"], "abstract": "The growth in prominence of large language\nmodels (LLMs) in everyday life can be largely\nattributed to their generative abilities, yet some\nof this is also owed to the risks and costs as-\nsociated with their use. On one front is their\ntendency to hallucinate false or misleading in-\nformation, limiting their reliability. On an-\nother is the increasing focus on the compu-\ntational limitations associated with traditional\nself-attention based LLMs, which has brought\nabout new alternatives, in particular recurrent\nmodels, meant to overcome them. Yet it re-\nmains uncommon to consider these two con-\ncerns simultaneously. Do changes in archi-\ntecture exacerbate/alleviate existing concerns\nabout hallucinations? Do they affect how and\nwhere they occur? Through an extensive eval-\nuation, we study how these architecture-based\ninductive biases affect the propensity to hallu-\ncinate. While hallucination remains a general\nphenomenon not limited to specific architec-\ntures, the situations in which they occur and the\nease with which specific types of hallucinations\ncan be induced can significantly differ based\non the model architecture. These findings high-\nlight the need for better understanding both\nthese problems in conjunction with each other,\nas well as consider how to design more univer-\nsal techniques for handling hallucinations.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have rapidly\nemerged as a every-day tool in modern life (Ope-\nAI, 2024), with many relying on their abilities to\naccomplish a variety of specific tasks. However,\nthis opened up concerns relating to their propensity\nto hallucinate (Huang et al., 2023), with no con-\ncrete reasons for this behaviour (Dziri et al., 2022b;\nRawte et al., 2023; Chen et al., 2024), hindering the\nability to directly train LLMs that are consistently\nfactual or able to explain themselves through their\nfactual knowledge (Madsen et al., 2024a).\nIn parallel, as LLMs evolve and existing limita-\ntions are discovered, alternative architectures have\nbecome increasingly common and grow in popu-\nlarity. In particular, Transformer LLMs (Vaswani\net al., 2017) and linear sequence models (Gu et al.,\n2022; Gu and Dao, 2024) present contrasting meth-\nods of encoding sequences, with the Transformer\nusing attention (Bahdanau et al., 2015) to form\nlossless representations of the context, while linear\nsequence models follow the recurrent neural net-\nwork (Rumelhart et al., 1986; Jordan, 1986) in their\nuse of a compressed state representation.\nWith the intensifying focus in both directions, a\nnotable void exists in verifying how each can affect\nthe others in conjunction. For example, existing\nworks in hallucination detection and mitigation fo-\ncus almost exclusively on Transformer-based mod-\nels (Maynez et al., 2020; Longpre et al., 2021a;\nGuerreiro et al., 2023; Shi et al., 2023; Ji et al.,\n2023b; Farquhar et al., 2024; Wei et al., 2024), with-\nout extension to recurrent-style models, despite the\nuse of a unified hidden representation potentially\nacting as an information bottleneck that can induce\nmore common hallucinations. This lack of unified\nunderstanding on both topics prompts the need for\na more explicit investigation.\nIn this work, we comprehensively explore the\ndifferences between pure attention LLMs and recur-\nrent LLMs, specifically with respect to the propen-\nsity to hallucinate. Using a set of 20 different hallu-\ncination tasks, categorized into 6 groups that evalu-\nate both faithfulness and factuality hallucinations,\nwe evaluate across numerous open-source LLMs\nthat range in scale from under 1B parameters to\n70B parameters all the while covering a variety of\ndifferent architecture choices such as self-attention,\nrecurrent and hybrid models. We further evalu-\nate across factors such as instruction-tuning, all to\nbuild a more comprehensive picture of architecture-"}, {"title": "Related Work", "content": "Hallucinations in LLMs. Though various specific\ndefinitions exist, hallucination can broadly refer to\n\"the generation of nonsensical information unfaith-\nful to the provided source content\" (Ji et al., 2023a).\nWhile this topic has become particularly important\nin order to ensure the safe and responsible use of\nlanguage models, both classifying and quantifying\nthe hallucinations is challenging. In particular, it\nis difficult to ascertain if the divergence occurs be-\ncause of specific data heuristics (Lebret et al., 2016;\nWiseman et al., 2017) or because of the innate lack\nof similarity between pre-training and downstream\ntasks (Rashkin et al., 2021), while measuring for\nhallucinations automatically introduces various bi-\nases (Reiter, 2018; Tian et al., 2020; Ganguli et al.,\n2022) that may not fully capture the scope of the\nerrors. However, a variety of task-specific bench-\nmarks (Li et al., 2020; Pagnoni et al., 2021; Zhou\net al., 2021; Santhanam et al., 2022) have shown\nvarious LLMs to struggle with factual inconsisten-\ncies, highlighting a need to render them safer for\nevery-day use.\nLLM Architectures. General large language mod-\nels architecture can be broadly thought to be com-\nposed of two components: token mixers which\nserves to model transformations between time steps\n(such as attention and recurrent layers) and chan-\nnel mixers, such as multi-layer perceptrons and\nmixtures-of-experts (MoE) (Jacobs et al., 1991;\nShazeer et al., 2017), which allow communication\nbetween different channels within a single time\nstep. Accordingly, token mixing often forms a\ncomputational bottleneck in terms of time com-\nplexity while channel mixers consist of a memory\nbottleneck. Though the contemporary standard for\ntoken mixing remains self-attention, alternatives\nare becoming increasingly common as they begin\nto display more promise. These include the use of\nlinear attention (Katharopoulos et al., 2020) to com-\npensate for the quadratic memory complexity of\nvanilla self-attention, to new recurrent models (Gu\nand Dao, 2024; Team, 2024b) that function as a\nlinear recurrent neural network but can process all\nelements of a sequence in parallel, and well as\nhybrid mixtures of recurrent mechanisms and at-\ntention (AI21, 2024; Dao and Gu, 2024) that have\nemerged as a meaningful competitor.\nArchitecture and Hallucination. Despite the\ngrowing research in new architectural components,\nwhether or not how they affect hallucination has\nyet to be studied. While some works (Madsen et al.,\n2024b; Hu et al., 2024; Schimanski et al., 2024)\nhave proposed modifications to either the learning/-\ngeneration pipeline as a way of reducing hallucina-\ntions, explicit proposals for hallucination reduction\nthrough structural modification have yet to be sug-\ngested. Additionally, though some works (Elhage\net al., 2021; Fu et al., 2023; Lutati et al., 2023;\nPoli et al., 2024) have demonstrated self-attention\nto aptly solve synthetic tasks that form an essen-\ntial component of language modelling, these fail to\nremain faithful on more realistic datasets, punctu-\nating a major limitation of existing LLMs. Finally,\nthough tangential work demonstrates that recurrent\nmodels may suffer from issues with information\nretention (Vardasbi et al., 2023), formally defining"}, {"title": "Background", "content": "Attention for Sequences. Vanilla self-attention as\nused in Transformers is powerful but costly. When\nprovided an embedded text representation as a se-\nquence of tokens $x \\in \\mathbb{R}^{L \\times d}$, each Transformer\nlayer in the network applies a function\n$T_\\ell(x) = FF_\\ell(A_\\ell(x) + x) + A_\\ell(x)$                                              (1)\nwhere $A_\\ell$ is the self-attention of the $\\ell$-th layer and\n$FF_\\ell$ is the following feed-forward network. Self-\nattention computes, for a token at position i in a\nsequence, a weighted average of the feature rep-\nresentations of all tokens (the values $V_\\ell$) in the\nsequence with a weight proportional to a similarity\nscore between i (the query $Q_\\ell$ at position i) and the\nrest of the sequence (the keys $K_\\ell$). In particular,\nthese can be computed for all positions in parallel\n$Q_\\ell = xW_\\ell^Q \\quad K_\\ell = xW_\\ell^K \\quad V_\\ell = xW_\\ell^V$\n$A_\\ell(x) = V = \\text{softmax}(Q_\\ell K_\\ell^T / \\sqrt{d}) V_\\ell$                                                      (2)\nproviding the model a lossless representation of\nthe complete past context. This can be seen as\nequivalent to search and retrieval within a database,\nwhere search is defined using query-key parame-\nterizations and retrieval with value parameteriza-\ntion. In this setting, the database from which the\ninformation is being retrieved is equivalent to the\nmodel parameters, which store information from a\ntraining corpus that the weights $W_\\ell^K, W_\\ell^Q, W_\\ell^V$\nparameterizing the model attempt to mimic.\nMulti-head attention, a variant of self-attention\npopularized by the Transformer (Vaswani et al.,\n2017), has become the dominant variant used in\nLLMs such as LLaMA (Touvron et al., 2023) and\nGemma (Team, 2024a). Additional variants, such\nas multi-query attention from Falcon (Almazrouei\net al., 2023) and grouped-query attention from Mis-\ntral (Jiang et al., 2023), adapt multi-head attention\nbut remain build upon the same underlying prin-\nciple of self-attention. Consequently, such meth-\nods are considered to fall under the family of self-\nattention token mixers.\nFrom Transformers to Recurrent LLMs. One\nof the original concerns of Transformers was the\nquadratic complexity of attention, leading to large\namounts of work focused on improving this bound\nas it could directly affect the ability to learn from\nlong sequences. Instead of directly modifying at-\ntention (Katharopoulos et al., 2020; Kitaev et al.,\n2020; Choromanski et al., 2021), Gu et al. (2020)\nmotivated a novel paradigm using state-space mod-\nels (SSMs) from control theory. SSMs map an\ninput $x(t) \\in \\mathbb{R}^d$ to an intermediate state $h(t) \\in \\mathbb{R}^n$\nthat is then projected to an output $y(t) \\in \\mathbb{R}^d$:\n$\\bar{h}'(t) = A h(t) + B x(t), \\quad y(t) = C h(t) + D x(t)$\nwhere A, B, C and D are all trainable parameters.\nGu et al. (2021) use this paradigm to define a re-\ncurrent model to work on discrete signals, in which\ncase the input can be regarded as discretized data\nsampled from a continuous signal with a step size\n$\\Delta$, for which the corresponding SSM is defined by:\n$h_t = \\bar{A} h_{t-1} + \\bar{B} x_t \\quad \\text{Y}_t = \\bar{C} h_t + \\bar{D} x_t$\n$\\bar{A} = \\frac{(I + \\Delta A/2)}{(I - \\Delta A/2)} \\quad \\bar{B} = \\frac{\\Delta B}{(I - \\Delta A/2)}$\nand $\\bar{C} = C$ (D is equivalent to a residual connec-\ntion and set to 0.) Thus\n$K = (CB, CAB, ..., CA^{L-1}B) \\quad y = K * x$\nwhere K is the SSM kernel. As y can be computed\nin O(Llog L) with a Fast Fourier Transform (Cor-\nmen et al., 2009), the entire output can be computed\nin tandem based on the input, given the matrices\nthat parameterize the system. Furthermore, setting\nA as a Hurwitz matrix, SSMs can preserve long-\nterm dependency information and enable them to\ncapture long-dependence information, overcoming\na long-standing issue (Bengio et al., 1994) with\nprior recurrent models (Hochreiter and Schmidhu-\nber, 1997; Cho et al., 2014).\nFuture works then modify this general structure\nfor additional properties; Mamba (Gu and Dao,\n2024) use input-dependent B and C, enabling for"}, {"title": "Experiments and Results", "content": "Models. For our comparison, we use various mod-\nels (Table 2) that vary in size and architecture,\nwith a particular focus on different time-mixing\nmethods. These include self-attention models\nsuch as Pythia, LLaMA2/3 (Touvron et al., 2023),\nFalcon-7B (Almazrouei et al., 2023), Mistral-\n7B (Jiang et al., 2023), Gemma1/2 (Team, 2024a)\nand Mixtral-8x7B (Jiang et al., 2024). We also\ntest recurrent models including Mamba (Gu and\nDao, 2024; Dao and Gu, 2024), FalconMamba-7B\nand RWKV (Finch) (Peng et al., 2023), as well\nas hybrid models like RecurrentGemma (Team,\n2024b) and Jamba (AI21, 2024). For models with\ninstruction-tuned versions, we use both the base\nand instruction-tuned variant. Additional details\nare available in Appendix A.\nDatasets. We evaluate on the Hallucination Leader-\nboard (Hong et al., 2024), consisting of tasks\n1) Closed-book Open-domain QA: NQ-\nOPEN (Kwiatkowski et al., 2019), TRIVI-\nAQA (Joshi et al., 2017), TRUTHFULQA\n(MC1, MC2, Generative) (Lin et al., 2022),\nPOPQA (Joshi et al., 2017)\n2) Summarization: XSUM (Narayan et al.,\n2018), CNN/DM (See et al., 2017)\n3) Reading Comprehension\u2660: RACE (Lai et al.,\n2017), SQUADv2 (Rajpurkar et al., 2018), NQ-\nSWAP (Longpre et al., 2021b)\n4) Instruction Following: MEMOTRAP (Liu\nand Liu, 2023), IFEVAL (Zhou et al., 2023)\n5) Hallucination Detection: FAITHDIAL\u2660 (Dziri\net al., 2022a), HALUEVAL\u2660 (QA, Summa-\nrization, Dialogue) (Li et al., 2023a), TRUE-\nFALSE (Azaria and Mitchell, 2023)\n6) Fact Checking: FEVER (Thorne et al., 2018)\nFor tasks, a higher score (ranging from 0 to 100)\nindicates better performance. Tasks are further\ndivided into two broader categories: faithfulness\n(\u2191), i.e. it evaluates whether an LLM generation\nadheres to the given source of information, and\nfactuality (+), i.e. it evaluates whether LLMs gen-\nerate factually correct content according to world\nknowledge acquired during training.\nInvestigating Task Biases\nAt a broader scope that architecture does not appear\nto influence hallucination, both for task categories"}, {"title": "Investigating Task Biases", "content": "and factuality/faithfulness. For example, with like-\nsized models falling within different categories of\ntheir time mixers (self-attention, recurrent layers,\nhybrid), mean performance on tasks within each\ngrouping is not significantly different nor do they\nshow a particular ordering\u00b9. However, with specific\ntasks, differences start to present themselves, which\nwe describe in our following observations.\n01: Recurrent models struggle with rare\nknowledge.\nInterestingly, recurrent and hybrid models all\nsignificantly underwhelm on POPQA, a task that\ntests for uncommon (thin-tailed) factual knowledge\n(left plot in Figure 1). For example, FalconMamba\nachieves an exact-match (EM) score of 0.7 com-\npared to 17.5 by a similar Falcon. Similarly, 2B\nand 9B RecurrentGemma achieve 7.6 and 16.0 EM\nrespectively, compared to 14.6/21.3 EM for 2B/7B\nGemma and 13.6/18.2 by 2B/9B Gemma2, while\nJamba (0.4 EM) performs worse than a similarly\nsized Mixtral-8x7B (31.9 EM).\nPrevious work (Vardasbi et al., 2023) suggests\nthat hidden states can become dense and difficult\nto extract information from in the future. Although\nthis is relevant for hallucinations, these concerns\nare unlikely to apply in this setting given that the\nprompt consists of only a question and the amount\nof information to store within the hidden representa-\ntion is unlikely to lead to it becoming highly dense.\nAdditional works (Dao and Gu, 2024) have instead\nposited that using recurrent models with additional\nmechanisms enabling input filtering can enhance\nperformance on information-dense data such as lan-\nguage; however, this is again unlikely to be the case\ndue to the general simplicity of the task. Yet this\nresult is significant as it indicates recurrent models,\ndespite the task's simplicity, struggle when com-\npared to attention-based equivalents, suggesting a\ndirect link to the token mixing that is utilized. The\nreason may instead be that recurrent/hybrid models\ndo not learn by explicitly memorizing and acting\nas a retrieval system (as discussed in Section 3). In\nthis sense, despite potentially observing low-tail\ninformation, the learning mechanism may not have\nled to said information being stored within parame-\nters, making recurrent/hybrid models more likely\nto produce incorrect information in such settings\nwith higher degrees of uncertainty.\n02: Recurrent models can better follow\ncontexts by relying less on memory.\nThere are also cases where recurrent models con-\nsistently outperform attention equivalents, namely\non MEMOTRAP (right plot in Figure 1). Here,\nthe LLM is prompted to complete a well-known\nproverb with an ending that deviates from the\ncommonly used ending, testing for over-reliance\non knowledge memorized from a training corpus.\nThis suggests that although recurrent models might"}, {"title": "On the role of Instruction-Tuning.", "content": "struggle with long-tail knowledge, they are less\nprone to ignoring contextual cues not stored within\nparametric memory, or they more model the dynam-\nics based on the context as opposed to acting as a\nretrieval system. Noticeable drops exist from Fal-\nconMamba (58.8) to Falcon (38.9) and from Recur-\nrentGemma 2B/9B to Gemma2 2B/9B (53.7/43.4\nvs 42.7/34.4), suggesting that attention can lead to\nmore hallucinations. It further reveals a benefit of\nrecurrent layers; by not memorizing information\ndirectly within parameters, there may be a reduced\ntendency to repeat previous observations (Jelassi\net al., 2024) and greater focus on the context.\n03: Scale is required for emergent qualities.\nHowever, some scale to the data and model re-\nmain important. Note that Mamba, Finch and\nPythia fail to draw the same distinctions as the\nother models despite sharing the same vocabulary\nand training data. However, given the scale of train-\ning for these (300B tokens compared to >1T tokens\nfor other models) and the fact that the discussed\ntasks relate directly to information memorization,\nit is possible that these phenomena fail to emerge\nat this data scale. This is particularly evidenced\nby how these models perform better with size on\nMEMOTRAP, which is designed in a way such that\nlarger models should normally perform worse.\n4.2\nThe use of instruction-tuning appears to show\ninconsistent improvements across categories (Ta-"}, {"title": "Impact of Model Size on Hallucinations.", "content": "ble 1) and tasks\u00b2. There exists no strict pattern for\noverall hallucination; on some tasks, its use is ef-\nfective for all (ex. TRUTHFULQA) or no (ex. TRIV-\nIAQA) models. Smaller trends also exist, such as\ninstruction-following observing the strongest gains.\nHowever, changes in faithfulness are generally\npositive for pure attention models (shown in teal)\nand negative for models with recurrent layers\n(shown in magenta), whereas changes in factual-\nity show no consistent trends. This suggests that\ninstruction-tuning for LLMs, as initially proposed\nwith self-attention based models, enables particular\nproperties which are no longer relevant after the\naddition of recurrent layers. Their inductive biases\ntherefore can have a particular effect on specific\nforms of hallucinations emerging, with instruction-\ntuning being a manner to mitigate these.\nRecalling the links between attention and re-\ntrieval from Section 3, the process of instruction-\ntuning can be understood as shifting the projection\nweights towards a new space through the additional\nfine-tuning (which is of itself simply an additional\nstep of language modeling). This leads to the over-\nwriting of some of the pre-training corpus with that\nused for fine-tuning, effectively replacing the keys\nand values from the underlying retrieval storage.\nWhile effects are ambiguous on factuality, as some\npotentially relevant facts can be removed, faithful-\nness often improves from this process, as the fine-\ntuning corpus is more likely to contain information\nrelevant to context following and thereby make it\neasier for the attention mechanism to retrieve rel-"}, {"title": "On the score.", "content": "evant information for such a purpose. In addition\nto highlighting how enhancing the ability to follow\ninstructions does not necessarily lead to factually\ncorrect results, another conclusion stemming from\nthe fact that recurrent models generally observe\nno benefits in faithfulness after instruction-tuning\nis that such models' inductive biases render them\ninherently more faithful or the process of making\nthem more faithful is distinct from self-attention.\n4.3\nFigure 3 depicts the performance to\nscale changes for factuality and faithfulness, where\nit becomes apparent that increasing the number of\nparameters in hybrid and recurrent models yields\na significantly lower increase in faithfulness com-\npared to pure attention models.\nThis suggests that recurrent layers enable an in-\nherent ability to follow contexts not improved by\nmodel size. Their inductive biases can be perceived\nas encouraging context following, supported by\nhow the hidden state incorporates information from\nthe context with the current input to control the gen-\neration. This perspective aligns these results with\nour instruction-tuning observations, where such\nmodels do not exhibit a clear benefit compared to\ntheir attention-based counterparts. In particular,\nwe can interpret this as indicating that the base\nmodel learns to perform this output control even\nwithout direct instruction-tuning; hence applying\nthese techniques is ineffective as we previously\nobserved."}, {"title": "Discussion", "content": "Hallucination Mitigation for Different Architec-\ntures. Hallucinations motivate the need to both\ndetect and mitigate them, particularly when they\ncan escalate to have adverse effects. Existing tech-\nniques may be insufficient, as they contain potential\nimplicit biases that can fundamentally conflict with\nthe manners in which hallucinations may differ\nbetween models. Facts such as sequence models\nsaturating in faithfulness in a manner not resolved\nthrough instruction-tuning indicate that existing\nmitigation techniques can differ in their effective-\nness due to a bias towards specific architectures.\nThis means that current techniques also further\ndeserve a look, in particular in terms of their ef-\nfectiveness across the different inductive biases of\nmodels. While some methods evidently might be\nspecifically catered to specific models, e.g. us-\ning attention (Li et al., 2023b; Zhang et al., 2024;\nChuang et al., 2024), other methods may implicitly\nalso possess biases due to the underlying assump-\ntions being made or specific requirements that may\nbe unevenly addressed through different architec-\ntures, such as the reliance on data-refinement or\nadditional context (Shi et al., 2024).\nInherent Faithfulness and Factuality. Designing\ninherently faithful (Herman, 2019; Wiegreffe and\nPinter, 2019) or factual models remains an impor-\ntant issue. While much work suggests attention-"}, {"title": "Conclusion", "content": "based LLMs lack an inherent tendency to be faith-\nful (Jacovi and Goldberg, 2020; Wiegreffe and\nMarasovic, 2021), methods have been designed\nto render models more interpretable (Madsen et al.,\n2024b) for specifically evaluating this. Similarly,\nsome of these methods have been shown to also\nadapt to Mamba models (Sharma et al., 2024).\nHowever, while interpretability is useful for evalu-\nating models as being faithful or factual, it does not\nensure an inherent tendency towards being either.\nNevertheless, these results hint that architectures\ncan provide an avenue through which desiderata\ncan be attained. SSMs present an option which\nin particular appears suited for settings which re-\nquire utilizing information provided within a con-\ntext, given their use within control theory to model\nsystem dynamics. With language, this can be anal-\nogous to using the fixed-size representation of the\nprevious context for control, which may mean that\nsuch models can remain more faithful as the con-\ntext remains the source that guides the generation.\nIn comparison, attention-based models have been\nshown to act as retrieval systems that retrieve to-\nkens based on high similarity with memorized in-\nformation; hence, when a large difference exists\nbetween the context and what has been stored from\ntraining, breakdowns in faithfulness can occur.\nAre hallucinations the direct result of how token-\nmixing takes place across time? We study whether\ndifferent inductive biases of LLM architectures can\nincrease this propensity by observing how hallu-\ncinations change across numerous tasks. Patterns\nemerge demonstrating that some architectural bi-\nases can lead to either improvements or degrada-\ntion in performance compared to others, both for\nindividual tasks and categorizations. Additionally,\nsome model types lead to inherent behaviour when\ncomparing the effects of instruction-tuning and\nscaling model size. In sum, model-specific induc-\ntive biases can have a direct effect on the type of\nhallucinations they are faced with. However, the\ndata used and the specific model construction can\nalso affect learning. We hope that future work can\nbuild on our findings by exploring both the types of\nhallucinations that can occur and techniques meant\nto mitigate them and how to design more universal\ntechniques to make models more robust or reliable."}, {"title": "Limitations", "content": "Evaluating faithfulness and factuality. A major\nlimitation existing in hallucination detection and\nmitigation research is the lack of metrics that pro-\nvide explicit information regarding hallucination.\nWhile the development of such metrics is an active\narea of research, limitations still exist, such as bi-\nases when involving additional models within the\nevaluation process.\nLimits on tasks. Another limitation of this work\nis the non-exhaustive set of tasks and domains in\nwhich we measure hallucinations. Due to the ex-\nhaustive ways in which hallucinations can be mea-\nsured, we limit ourselves to tasks that are well\nmotivated and frequently used in practice."}, {"title": "Ethical Concerns", "content": "This paper provides an analysis on the effects of dif-\nferent architectural inductive biases on the propen-\nsity to hallucinate within large language models.\nAs such, mistakes in methodology can lead to un-\nsupported confidence or skepticism regarding their\nperformance with the explored task or related ones.\nWhile skepticism may not be an ethical issue, un-\nsupported confidence can be problematic. How-\never, the overall message is that all LLMs have\nspecific tasks and settings in which they will dis-\nplay a greater propensity to hallucinate, hence we\ndo not believe that the ideas expressed in this work\nwill explicitly lead to unsupported confidence."}]}