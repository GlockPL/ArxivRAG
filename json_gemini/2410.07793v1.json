{"title": "Do Current Language Models Support Code Intelligence for R Programming Language?", "authors": ["ZIXIAO ZHAO", "FATEMEH H. FARD"], "abstract": "Recent advancements in developing Pre-trained Language Models for Code (Code-PLMs) have urged many areas of Software Engineering (SE) and brought breakthrough results for many SE tasks. Though these models have achieved the state-of-the-art performance for SE tasks for many popular programming languages, such as Java and Python, the Scientific Software and its related languages like R programming language have rarely benefited or even been evaluated with the Code-PLMs. Research has shown that R has many differences with other programming languages and requires specific techniques. In this study, we provide the first insights for code intelligence for R. For this purpose, we collect and open source an R dataset, and evaluate Code-PLMs for the two tasks of code summarization and method name prediction using several settings and strategies, including the differences in two R styles, Tidy-verse and Base R. Our results demonstrate that the studied models have experienced varying degrees of performance degradation when processing R programming language code, which is supported by human evaluation. Additionally, not all models show performance improvement in R-specific tasks even after multi-language fine-tuning. The dual syntax paradigms in R significantly impact the models' performance, particularly in code summarization tasks. Furthermore, the project-specific context inherent in R codebases significantly impacts the performance when attempting cross-project training. Interestingly, even when Large Language Models like CodeLlama and StarCoder2 are used for code generation, the Pass@K (K = 1, 5, 10) results lags signigicantly behind Python scores. Our research shows that R as a low resource language requires different techniques to collect a high quality data. Specifically separating the two R styles has a great impact on the results and the separate dataset could increase the performance of the models. Our research sheds light on the capabilities of Code-PLMs and opens new research directions for researchers and practitioners for developing code intelligence tools and techniques for R. With R's widespread use and popularity, the results of our study can potentially benefit a large community of R developers, both in research and industry.", "sections": [{"title": "1 INTRODUCTION", "content": "The R programming language was first developed by statisticians Ross Ihaka and Robert Jetman at the University of Auckland, New Zealand, in the early 1990s as a personal project [35]. R was originally designed as a statistical programming language and a statistical teaching tool by combining the syntax of S, which is widely used among statisticians, with the semantics of Scheme [35]. The use and impact of the R language are growing every year as the number of users and application scenarios increase [27]. Version 1.0.0 of R appeared on the CRAN (The Comprehensive R Archive Network)\u00b9 in 2000, and by the time this paper is written (2024), the current version of R is 4.4.1 and there are ~ 21, 423 available widely used packages on CRAN.\nWith the growing size of the community and applications of R, it is ranked 6th in the PYPL Popularity of Programming Language \u00b2 and 17th in the popularity of programming languages \u00b3. R's extensive library of packages, ranging from data manipulation to advanced statistical modeling, makes it one of the best choices in teaching [27, 79], IT Sector [53], scientific fields [2, 47, 50, 51], and Finance [69]. Though R is studied from various Software Engineering perspectives [33, 57, 73, 84], developing automation tools to support R developers, such as comment generation, is still in its infancy, compared to other languages like Java or Python.\nThe development of pre-trained language models has revolutionized the field of software en-gineering and development [52]. These models, trained on massive datasets using deep learning techniques, possess the ability to generate human-like text. The significance of pre-trained language models in software engineering and development cannot be overstated [90]. They facilitate code autocompletion and generation, enhancing developers' productivity and accuracy.\nHowever, even though the R language has unparalleled importance in related fields, very limited research on code intelligence in R has been shown. A conventional GPT-2 [59] model is used for the code completion task in R [58]. Though a large number of pre-trained models have been successfully developed in recent years and have proven their effectiveness in various software engineering (SE) tasks [24, 26, 31, 46, 78], to the best of our knowledge, no model has attempted to extend their effectiveness to the R language. While progress has been made in code intelligence for R [58], there exists a gap in the literature, limiting the realization of its full potential.\nMore recently, Large Language Models (LLMs) have demonstrated remarkable capabilities across various software engineering tasks [12, 43, 59, 62]. However, recent studies indicate that these models exhibit inconsistent performance across different programming languages [55]. The per-formance disparity in different language is particularly evident in the R programming language, where the unique characteristics of the language pose specific challenges for LLMs. The coexis-tence of multiple syntax paradigms in R, i.e., Tidy-verse and Base, along with its heavy reliance on project-specific contexts and specialized libraries, could create additional complexities that current LLMs struggle to handle effectively.\nIn this study, we intend to explore to what extent the current techniques are applicable to R. However, as there is no current dataset, we first collect a dataset of high-quality R packages from GitHub repositories (inclusion/exclusion criteria described in Section 3.2.2). Then, we consider four pre-trained models, CodeBERT [26], GraphCodeBERT [31], UniXcoder [30] and CodeT5 [78] to empirically study their performance for two tasks of code summarization and method name prediction, with various programming styles and settings. The results of the study are compared across models and with other programming languages. We then investigate an R-specific approach based on different programming styles to evaluate its effect on the models' performance. In this study we also investigate the performance of LLMs for R language on code generation. This paper is the second Stage of our accepted TOSEM Registered Paper\u2074, in which we are reporting the results along with the discussions.\nOur results reveal several key findings regarding the models and their capabilities for R pro-gramming language. First, studied models consistently experience varying degrees of performance degradation when processing R code compared to other programming languages. Second, our experiments demonstrate that multi-language fine-tuning does not uniformly improve perfor-mance across all PLMs for R-specific tasks, contrary to expectations. Third, we find that different syntax of the R styles in Tidy-verse and Base code significantly impact the performance, which is affecting the results more for code summarization task. The challenges extend beyond syntax to project-specific contexts inherent in R codebases, which poses a substantial challenge when attempting cross-project training. This is further complicated by the observation that combined syntax approaches generally underperform compared to either individual syntax paradigms. Our experiments with LLMs also show the low Pass@K scores for code generation, falling much behind the scores obtained for Python. These insights emphasize the need for more nuanced approaches to improving model performance for R programming, beyond simply increasing training data or applying multi-language fine-tuning strategies.\nThe contributions are as follows:\n\u2022 We open source an R dataset to support other research related to code intelligence in R5.\n\u2022 We empirically evaluate the performance of different pre-trained models for R, for the specified tasks.\n\u2022 We provide an R-specific approach and study its effect on the models' performance.\nIt is worth noting that though the foundation models (i.e., Large Language Models) in the past few months have found their way into SE, there is rarely a study or application available to use them for R. Additionally, their usage is costly if one wants to call their APIs. Therefore, their usage is not available for all, and fine-tuning them is not possible due to being large, besides their hidden ability for this programming language. LLMs have some shortcomings [48] and still studying the smaller Code-PLMs can benefit the community. Therefore, we have studied the Code-PLMs and also added results for code generation using LLMs.\nSignificance: This study provides valuable insights into the effectiveness of using pre-trained code models for R and highlights the need for further research in this field. The results of our work can be used as a reference for future research and provide a foundation for the development of intelligent tools for the R community. Applying software engineering practices such as version control, documentation, testing, and modularization can lead to more reliable, maintainable, and scalable R code [72]. Additionally, scientific software developers consider functional correctness, performance, portability, and maintainability as the top important characteristics [39]. This is of specific importance as research shows that many scientists develop/use scientific software and \u201cbetter software leads to better research\u201d [29]. Our work, thus, can provide insights to support the R developers, superficially in the context of program comprehension and updating comments, which could lead to better maintainability. This is one of the two important aspects for scientific software developers, and R is considered as a language used for the scientific software development [10]. We provide discussions and implication at the end, that helps shaping the future research for code intelligence in R, including the need to curate R datasets and developing separate techniques to increase the performance of the models, including LLMs, for the two R styles."}, {"title": "2 RELATED WORKS", "content": "In this section, we provide some background information and related work to our study."}, {"title": "2.1 Pre-trained Language Model for Code", "content": "In recent years, Pre-trained Language Models (PLMs) that are pre-trained on code \u2013we refer to these models as Code-PLMs\u2013 have achieved great performance in different software engineering tasks [26, 30, 31, 78]. PLMs are self-supervised deep learning models that have been previously trained on large amounts of text data to perform various natural language processing tasks. These models capture knowledge during the pre-training phase, which is then transferred to different downstream tasks, mainly by fine-tuning the model on specific downstream tasks.\nPre-trained models, specifically transformer-based models [71], have found significant appli-cations in various software engineering tasks including code summarization, bug detection, and code translation [26, 30]. Examples of these models are CodeBERT [26], GraphCodeBERT [31], CodeT5 [78], and UniXCoder [30]. PLMs outperform many of the state-of-the-art models on several datasets and tasks and have been used extensively in multiple software engineering tasks [26, 34, 52].\nThe use of pre-trained language models in software engineering offers several advantages, such as reducing the amount of labeled data required for training and increasing the efficiency of training [32]. They also provide a strong baseline for evaluating the performance of models and can be used to develop novel techniques in this field.\nDue to the advantages of the PLMs for source code, there are works that investigate the usage and capabilities of Code-PLMs. Wan et al. [75] studied the features captured by CodeBERT and GraphBERT through attention analysis, probing, and syntax tree induction, on Python, Java, and PHP. Troshin and Chirkova [70] introduce probing tasks to discover the syntactic and semantic information of various Code-PLMs. Karmakar and Robbes [40] uses probing tasks to study the Code-PLMs but does not include R language. Ahmed et al. [6] studies the Code-PLMs specifically by evaluating the models' ability to learn semantics through objective and straightforward evaluation.\nWith all those studies on different areas and tasks in software engineering, to the best of our knowledge, no study has attempted to apply PLMs to the R programming language."}, {"title": "2.2 Scientific Programming Languages", "content": "Scientific programming languages play a pivotal role in the realm of data analysis, statistical modeling, and visualization, catering to the needs of researchers, statisticians, and data scien-tists [66]. Among the diverse array of programming languages utilized in scientific domains, R stands out as a powerful and popular choice due to its robust statistical capabilities and extensive ecosystem [28]. One of the key contributors to R's widespread adoption is its vibrant and dedi-cated community, which has actively expanded the language's functionality. Over the past decade, there has been a lot of research devoted to expanding the applicability and functionality of the R language. Notable contributions include the development of ggplot2 [74], a sophisticated and flexible data visualization package that enables the creation of high-quality graphics with ease. The Tidy-verse [80], a collection of R packages designed to enhance data manipulation and analysis, has further streamlined workflows, promoting code readability and efficiency. RStudio, an integrated development environment (IDE) specifically tailored for R, has played a pivotal role in facilitating a user-friendly programming experience. Offering features like syntax highlighting, code completion,"}, {"title": "2.3 Scientific Software, Jupyter Notebooks, and R Studies", "content": "In software engineering, several attempts and perspectives have been aimed at advancing code intelligence in R. However, to the best of our knowledge, no research has been conducted explicitly exploring the potential of pre-trained language models in this context.\nPopov et al. [58] develop a deep learning model for code completion in R. Sharma et al. [65] studied Self-Admitted Technical Debt (SATD) in R software packages and Chandramouli et al. [15] presented analyzeR, a SonarQube plugin for analyzing object-oriented R packages. Zanella and Liu [84] study the R packages on CRAN through a social network perspective. They report that the performance of R packages can be explained as a flow of information in the obtained network. Huang and Yang [33] conducts an empirical study on the R ecosystem and found that the R development has been done by also software engineers and from other disciplines such as biology, environmental and medical science, other than statistics.\nIn another work, Plakidas et al. [57] investigates the evolution of R and Ooms [54] discusses the dependency versioning of R and demonstrates the issue and the solution through three use cases. Morandat et al. [49] applies static and dynamic analysis to assess the design of R. Korkmaz et al. [41] studies how the impact of R packages could be modeled through their dependencies and their contributors' network.\nPackage dependency in R and CRAN packages [23], distribution of R packages [22], characterizing the bugs in R and Python [3], and Studying the API breaking changes in a specific field of R packages [17] are among other research on R. Babu [11] develops a question answering system for R and [81] conducts the dependently typing for R vectors, arrays, and matrices.\nAnother closely researched area is scientific software and scientific computing. Ivie and Thain [36] discussed the reproducibility in scientific computing and Goble [29] reports that 91% of scientists surveyed online use scientific software, and at least 38% spend one fifth of their time developing scientific software. The author discusses that better software leads to better research.\nOther works also study the data science practices in Jupyter notebooks. Liu et al. [45] develop a model to generate code documentation for Jupyter Notebooks. Zhang et al. [87] collect a dataset and build a model to classify the cells of Jupyter notebooks in data science projects. Yang et al. [82] develop a Jupyter notebook extension based on program synthesis and and test selection to generate documentations that help users find bugs for data wrangling. Dong et al. [25] study the data cleaning practices in 30 Jupyter notebooks. Robinson et al. [61] examine Jupyter notebooks to define error identification strategies of Python notebooks. Their work is a replication of a similar work with user study to find these strategies for R Markdown notebooks Abdel-Ghani et al. [1]. Maintenance [37], bug analysis [21], structure improvement [67], and understanding documentation practices Wang et al. [76] are among other research explored in Jupyter notebooks.\nDifferences among our work and the current literature. Among the related works, the closest ones are the approaches that use deep learning to detect SATD [65] or complete code in R [58]. The work multiple_E [13] purposed a translater that is able to translate HumanEval [16] benchmark dataset to other programming language including R. Still, none of these or other research studies the ability of Code-PLMs for different tasks in R or considers the two R styles. Though the Jupyter notebook supports R programming language, the current studies mainly focus on"}, {"title": "3 METHODOLOGY", "content": "In this section, we will review the research questions, data collection process, studied tasks and Code-PLMs, along with our approach to answer each of the research questions."}, {"title": "3.1 Research Questions", "content": "The research questions we investigate and the answers in this research are as follows.\nRQ1: What is the performance of the existing Code-PLMs on R language?\nMany studies show that Code-PLMs obtain state-of-the-art performance for programming languages on the CodeSearchNet dataset. However, to what extent these models can accomplish the tasks for R language is not defined. Therefore, in this RQ, we evaluate the performance of the Code-PLMs for R. The answer to this RQ reveals the capability of the code-PLMs for R language, which is not explored in the existing research.\nRQ2: Does the multilingual training in the fine-tuning phase increase the performance of the models compared to monolingual training for R?\nPrevious research [4] has shown that multilingual training in the fine-tuning phase increases the performance of the Code-PLMs for code summarization and method name prediction. In this RQ, we investigate whether such an effect is observed in the case of R. Answering RQ2 helps identify the effect of multilingual training for other languages beyond the six languages covered in the CodeSearchNet dataset (R in our study). This finding adds value to the current literature to assess the effectiveness of different approaches for low-resource languages, in this case, R.\nRQ3: What is the effect of intra- or cross-project training?\nPrevious research [5, 42] studied the effect of different dataset designs on the performance of the code summarization models. In this RQ, we investigate the outcome of models based on choosing the training and test functions from the same or different R repositories, also known as intra- or cross-project data selection. Although intra- or cross-project effect has been explored for other languages, it has not been explored for R. The results can shed light on designing experiments and the dataset to increase the performance of the models.\nRQ4: Can we improve the performance of the models based on different programming styles in R?\nTwo of the popular programming styles in R are Tidy-verse and base R. The Tidy-verse is a collection of R packages designed for data science \u2076. Though both are used frequently, the syntax of each one is different from the other [79]. Here, we investigate the effect of different programming styles on the performance of Code-PLMs. The results can benefit developing models for code intelligence tasks for the R developers, for both base and Tidy-verse.\nRQ5: What is the performance of Large Language Models on the selected tasks for R?\nRecent advances in foundation models or Large Language Models have shown promising results for some of the software engineering tasks [12, 62, 78] Accordingly, in this RQ, we will explore the performance of LLMs for the studied tasks for R. Currently, there is limited research on assessing the capabilities of LLMs for low-resource languages, including R. With the interest in using LLMs, the"}, {"title": "3.2 Data Collection", "content": "3.2.1 Overview. Figure 1 presents an overview of our methodology for preparing the data. First, we collect GitHub repositories that include R files, considering the inclusion/exclusion criteria that we discuss next. Then, the eligible code snippets are parsed into code tokens using the tree-sitter parser. In the end, we followed the Roxygen2 documentation structure to match the code snippets and their corresponding natural language description to generate a dataset where each line of the data contains a pair of Programming Language (PL) and its correspondence Natural Language (NL).\n3.2.2 Scraping GitHub Repositories. The API we used to collect the data is the GitHub REST API\u2077, which allows users to search for repositories based on specific criteria. Based on previous work [19, 34], we devised a similar selection strategy for the R language to ensure that the included code repositories are active, original, and relevant. To ensure the objectivity of the selection, we list the following Inclusion/Exclusion criteria adopted from previous works [19, 34].\nInclusion criteria:\n\u2022 The project should be a public project with an open-source license on GitHub.\n\u2022 The project needs to be an R package and have a proper R package structure. For example, it needs to contain a DESCRIPTION file.\n\u2022 The project needs to be an active and up-to-date project. We flirted the repositories where the last commit was made within a year.\nExclusion criteria:\n\u2022 The project should be a non-personal project. So all personal projects are excluded.\n\u2022 The project should not be archived. When a repository is archived, its issues, pull requests, code, labels and other components of the project become read-only. The owner and collabo-rators also can not update the project. So all archived repositories are excluded.\n3.2.3 Data. To create the dataset for software engineering tasks, we formatted our data following the CodeSearchNet dataset [34] structure, where each line of the JSON file contains one data example, including the function name, the code tokens, the natural language description present in their respective documentation and the doc Strings. Each data point also contains metadata on the function, such as the repository it was extracted from. In order to do so, we performed a series of preprocessing and filtering on the raw data. The following discusses preprocessing and filtering in detail."}, {"title": "3.2.4 Parser", "content": "Roxygen is a documentation generator for R code that uses a parser to analyze R code and extract information that can be used to create documentation [73]. Roxygen can use the information in the function to generate documentation for the function. For example, it can extract the names and default values of the function's arguments, as well as information about the return value of the function. Roxygen can also use ASTs to identify any errors or potential problems in the code, such as syntax errors or unused variables.\nIn this study, we utilize Roxygen to achieve two primary objectives. First, we employ Roxygen to match code snippets from R language packages with their corresponding natural language descriptions. Second, we leverage Roxygen and the dependency information present in R language packages to classify them into either the base R or the Tidy-verse version. The process of using Roxygen to match code snippets with their corresponding natural language descriptions can be outlined as follows. Within R language packages that adhere to the Roxygen framework, there"}, {"title": "3.3 Tasks", "content": "In the following, we explain the selected tasks for our experiments.\nCode Summarization is a technique in software engineering that aims to automatically generate a concise and informative summary of the code [26, 31]. Code summarization has the advantage of being fast, consistent, and reproducible, and can help developers easily understand the semantics of the source code [90]. This can be particularly useful in large and complex software projects where it can be difficult to keep track of the different components and understand how they interact with each other. In recent years, code summarization has gained significant attention in the software engineering community, with several research studies exploring the effectiveness of various summarization techniques and models [24, 34]. However, previous research in code summarization has mainly focused on programming languages that are widely used in firms such as Java and Python [26, 31, 46], leaving a gap in our understanding of code summarization for other programming languages, such as R. Therefore, we study the code summarization as one of the tasks. The dataset we collected includes both the methods and their natural language comments, written by developers. Therefore, we can use the collected dataset and it is ready to train the models for code summarization.\nMethod Name Prediction or extreme code summarization refers to the task of predicting or generating appropriate names for methods (functions or procedures) in programming code [7]. Given a code snippet, the model aims to generate a suitable name that accurately represents the intended behavior or functionality of the method. This task is important in software development as well as code comprehension because well-named methods can improve code readability, main-tainability, and understandability [7]. Method Name Prediction often involves training models"}, {"title": "3.4 Studied Code-PLMs", "content": "In this section, we will briefly discuss the chosen benchmarking pre-trained models. We choose these models based on their architecture, pre-training tasks, and performance, and we follow the recommendation of a recent empirical study for selecting them [52]. Niu et al. [52] conducted empirical studies on various code intelligence tasks and code-PLMs and provided recommendations for the model choices for each task. Accordingly, we choose CodeBERT, GraphCodeBERT, UniXcoder, and CodeT5 in our study. Note that at the time of this study, to the best of our knowledge, there is no research on evaluating the models for R language. Therefore, we follow the recommendations of [52] and choose the following models, in addition to conducting experiments with large language models, GPT-3.5-Turbo and CodeLlama.\nCodeBERT [26] is a language model specifically designed for programming languages. Code-BERT's architecture is based on the Transformer model, similar to the original GPT model, but with modifications to accommodate programming code. CodeBERT is pre-trained using two primary tasks: masked language modeling (MLM) and replaced token detection (RTD). For MLM, which has been shown to be effective in literature [24], random tokens in the input code are masked, and the model is trained to predict the masked tokens. For RTD [18], the objective is to use both bimodal and unimodal data for training. CodeBERT has shown promising results in various programming language-related tasks, such as natural language code retrieval, and code summarization. It has achieved state-of-the-art performance at the time of publication in code-related benchmarks and competitions, demonstrating its effectiveness in understanding and generating code.\nGraphCodeBERT [31] extends CodeBERT [26] by incorporating the graph-based representation of code. It represents code snippets as abstract syntax trees or control flow graphs (CFGs) to"}, {"title": "3.5 Approach", "content": "In this section, we will provide a detailed experimental setup of each of the studied RQs.\nRQ1: This RQ is motivated by the lack of research and understanding of the performance of Code-PLMs for the R programming language. To address this challenge, we compared the four Code-PLMs for each of the languages on the CodeXGLUE benchmark (which is the same data as CodeSearchNet for these tasks), and the R dataset. To achieve the comparison, we trained each of the models on each of the programming languages shown in the first six rows of Table 1 and also on the RCombine datasets separately, and then tested on the test split of each one. Therefore, we can evaluate them when trained and tested on each of the languages in a mono-lingual setting.\nRQ2: For this RQ, we intend to evaluate the effect of multi-lingual fine-tuning for R. To achieve this, we combined the training splits of all the seven languages (i.e., Ruby, JavaScript, Java, Go, PHP, Python, and RCombine) and then train each of the Code-PLMs on this multi-lingual dataset. We tested the performance of each model on the test set of each of the programming languages separately. The training and testing were done for code summarization and method name prediction, and the results are reported accordingly.\nRQ3: For this RQ, we use the RCombine dataset and we split the functions of the training and test set based on the repository they belong to. For the intra-project setting, we spited the training and test sets such that we have some functions from every repository in the training set and some of its functions in the test set. Note that we take care of not having duplicate functions in the training and test sets. For the cross-project setting, the repositories are split such that their functions only belong to the training or test set. Therefore, the model does not have functions from the same repository in the training and test set.\nRQ4: As mentioned earlier in section 3, there are two different opinionated programming styles in R, the Base version, and Tidy-verse. The Base R is the fundamental R programming language, and it is the only dialect for a long time it can be more stable when compared with Tidy-verse [79]. Tidy-verse, on the other hand, is a collection of packages that work as an add-on to base R that provides extra functionality and makes the workflow and syntax more user-friendly. Despite the benefits and user groups of each, the syntax can be very different. Listing 1 provides an example of Tidy-verse and a Base R example is shown in Listing 2. Based on these differences, we hypothesize that separating the datasets might affect the obtained scores in a positive way.\nTherefore, for this RQ, we repeat the experiments of RQ1-RQ3, but with RBase and RTidy datasets separately. To have a fair comparison, we decrease the size of the RTidy to include the same number of records as the Rbase. This will be in addition to having the experiments on the RTidy dataset with its original number of records.\nRQ5: To answer this research question, we run experiments using CodeT5 [78] and UniX-Coder [30], as well as ChatGPT [12], CodeLlama [62] and StarCoder2 [43]. We exclude other models as they are not developed for code generation. For code generation, as stated earlier, the models will be prompted to generate code. We prompted ChatGPT [12] and CodeLlama in a zero-shot setting and compare its results with the full fine-tuned models. For code summarization and method name prediction tasks, we selected 100 samples from the test dataset (same as the ones used for human evaluation, described in Section 3.7)."}, {"title": "3.6 Evaluation Metrics", "content": "For code summarization, following previous works [26, 78], we evaluate the results using smoothed BLEU-4 score [44]. The smoothed BLEU-4 score is a technique used in the field of machine translation to assess the quality of translations. It is an extension of the BLEU (Bilingual Evaluation Understudy) metric, which measures the similarity between a machine-generated translation and one or more reference translations. The BLEU score can be calculated using the following formula1:\n$BLEU = BP * exp(\\sum_{n=1}^{N} W_nlogP_n)$ (1)\nwhere BP is the brevity penalty and Pn is the modified precision score, let c be the length of the candidate and r be the effective reference corpus length, BP can be calculated using:\n$BP = \\begin{cases}\n1 & \\text{if } c > r \\\\\ne^{1-r/c} & \\text{if } c < r\n\\end{cases}$ (2)\nWhen reporting the smoothed BLEU score, the model with the higher score is considered a better model for this task. A higher score is considered better.\nFor method name prediction, we follow the previous work [4], and report the F1-score to evaluate the classification models, as defined below.\n$F1 = 2 * \\frac{precision * recall}{precision + recall}$ (3)\nWhere $precision = \\frac{Truepositive}{Truepositive+Falsepositive}$ and $recall = \\frac{Truepositive}{Truepositive+Falsenegative}$. Here, Truepositive indicates that the token that is part of the name is been predicted as part of the function name. When reporting the F1 score, the model with the higher F1 score is considered a better model for this task.\nFor code generation, following the metrics used in code generation studies using HumanEval [62], we report the pass@K. Pass@K is an evaluation metric used to measure the probability that at least one correct answer is included among the top k responses generated by a model, calculated as follows [16]:\n$pass @k := E_{problems} 1[\\text{at least one correct answer is within the top k}]\n(4)\nFor instance, if k is set to 5, pass@5 signifies the probability that at least one correct answer is present within the first five responses generated by the model. This metric helps assess whether the model's output contains the correct result among multiple possibilities, providing a measure of the reliability of the model's output. We will apply the metric for k \u2208 {1, 5, 10}.\nStatistical tests. Following the previous works [4], we apply the Wilcoxon signed-rank test to compare the results of the models for all the tasks."}, {"title": "3.7 Human Evaluation", "content": "Other than automatic evaluation metrics, we also conduct a human evaluation, following the protocol used in previous studies [63, 64]. We randomly select 100 samples from the test dataset for each task. Each sample will include code, ground truth values, and the generated output (code comments or method names) for all models. Two Computer Science graduate students, who are both familiar with R, conducted the evaluation. Each record was ranked by two reviewers and the average value were reported. Each sample was evaluated based on the following criteria, on a Likert scale of 1 to 5, where 1 shows the lowest and 5 shows the highest.\nFor code summarization, we considered the following aspects:\n\u2022 Informativeness (I), which considers how well the generated comment covers the key points of the code.\n\u2022 Relevance (R), which evaluates the consistency of the provided details in the comments and compares it with the details in the code.\n\u2022 Fluency (F), which assesses English fluency in terms of the comments being well-written and grammatically correct.\nFor method name prediction, we considered the semantic similarity of the ground truth and the generated names. To avoid bias, we hided whether the text (comment or method name) is generated by a model or is written by a human. Moreover, the evaluators were educated before scoring. We provided examples and explanations for each criterion before they start evaluation."}, {"title": "3.8 Experimental Setup", "content": "All the experiments were run on the UBC Advanced Research Computing Sockeye and Digital Research Alliance of Canada. Each job requires the allocation of a compute node with 1 CPU core, 48GB of memory, and 2 NVIDIA Tesla V100 32GB GPUs with 32GB of memory for execution.\nFor hyper-parameters used during the training phase, we followed the setting introduced in the original papers. We trained the models for 10 epochs with a learning rate of 0.00005. The source_length is set to 256 and the target_length is to 128 for code summarization. For method name prediction, the target length is adjusted to 10. The batch size is set to 32 for CodeBERT [26] and GraphCodeBERT [31], 48 for UnixCoder [30] and CodeT5 [78] to perform a fair comparison to the original paper and the between R language and other programming languages. All experiments are run on Dell EMC R440 GPU compute nodes."}, {"title": "4 RESULTS", "content": "The obtained results are presented in this section."}, {"title": "4.1 RQ1: Performance of Code-PLMs in Mono-Lingual Fine-tuning Setting on R", "content": "4.1.1 Code Summarization. Table 2 presents the BLEU-4 scores for code summarization, for all languages. The Combinedsix represents the results when we combined all six languages from CodeSearchNet. Row Rcombine shows the dataset that we combine both Tidy-verse and Base R datasets.\nWe first replicated the summarization task by fine-tuning the pre-trained model for six languages, following the reported set of instructions. This step is done to ensure the reliability of our results and avoid errors that might have been introduced by the hardware or the system environment. For"}, {}]}