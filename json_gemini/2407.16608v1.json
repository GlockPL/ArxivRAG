{"title": "Deep Bayesian segmentation for colon polyps: Well-calibrated predictions in medical imaging", "authors": ["Daniela L. Ramos", "Hector J. Hortua"], "abstract": "Colorectal polyps are generally benign alterations that, if not identified promptly and managed successfully, can progress to cancer and cause affectations on the colon mucosa, known as adenocarcinoma. Today advances in Deep Learning have demonstrated the ability to achieve significant performance in image classification and detection in medical diagnosis applications. Nevertheless, these models are prone to overfitting, and making decisions based only on point estimations may provide incorrect predictions. Thus, to obtain a more informed decision, we must consider point estimations along with their reliable uncertainty quantification. In this paper, we built different Bayesian neural network approaches based on the flexibility of posterior distribution to develop semantic segmentation of colorectal polyp images. We found that these models not only provide state-of-the-art performance on the segmentation of this medical dataset but also, yield accurate uncertainty estimates. We applied multiplicative normalized flows(MNF) and reparameterization trick on the UNET, FPN, and LINKNET architectures tested with multiple backbones in deterministic and Bayesian versions. We report that the FPN + EfficientnetB7 architecture with MNF is the most promising option given its IOU of 0.94 and Expected Calibration Error (ECE) of 0.004, combined with its superiority in identifying difficult-to-detect colorectal polyps, which is effective in clinical areas where early detection prevents the development of colon cancer.", "sections": [{"title": "1. Introduction", "content": "Colorectal cancer is the second leading cause of cancer deaths worldwide, both in terms of prevalence and mortality for both genders. In 2020, it caused 935,000 deaths, accounting for 10% of all cancer-related deaths. This highlights the importance of its study and early detection (Globocan, 2020). The 5-year survival rate for this type of cancer is around 65% for all stages of the disease combined (NCI, 2020), but if detected early, this survival rate increases to 90% (ACS, 2022). Colorectal polyps are known as direct pre-cursors of this disease, if they are not treated adequately, effectively, and in time. The main tool to detect them is visually during the colonoscopy procedure, but this can lead to human errors during the diagnostic process, as studies have reported a rate of undetected polyps during the process, ranging from 6-28% (Lee et al., 2017). Given the aforementioned reports, the importance of the development of automatic detection systems (ADS) for the accurate identification of colon polyps is evident. Recently, there have been several studies and approaches to automatic systems for polyps. One of the first proposals included morphology as WM-DOVA, where the"}, {"title": "2. Bayesian Neural Networks", "content": "In the following, we introduce theoretical foundations of variational inference for Bayesian neural networks. It also covers measurement of model calibration, the association of uncertainties to predictions, and definition of recommended loss functions for binary segmentation cases."}, {"title": "2.1. Variational Inference in Bayesian Neural Networks", "content": "In the following, we introduce theoretical foundations of variational inference for Bayesian neural networks. It also covers measurement of model calibration, the association of uncertainties to predictions, and definition of recommended loss functions for binary segmentation cases."}, {"title": "2.2. Variational Inference in Bayesian Neural Networks", "content": "Within DNN framework, let $D = \\{(x_i, y_i)\\}_{i=1}^N$ where N is size of the sample and $x_i \\in \\mathbb{R}^d$, $y_i = (y_i^{(1)}, ..., y_i^{(K)}) \\in \\{(0,1)\\}^K$, d is dimension of input variables, K is the number of different classes (output), $\\omega \\in \\Omega$ the vector of parameters for the network and $p(\\omega)$ a prior on weights w. Posterior distribution is given by:\n$$p(\\omega|D) = \\frac{p(D|\\omega) p(\\omega)}{p(D)} = \\frac{\\prod_{i=1}^{N}p(y_i|x_i, \\omega) p(\\omega)}{p(D)}$$ (1)\nPredictive distribution (for a new pair $x_*, y_*$) is written as:"}, {"title": "2.3. Monte Carlo estimator", "content": "Considering that the integration to compute the predicted distribution must be done over the entire \u03a9 space, we consider a Monte Carlo estimator as follows:\n$$p_\\theta(y_*|x_*) = \\frac{1}{T} \\sum_{i=1}^{T} p(y_*|x_*, \\omega_t)$$(7)\nHere $\\{\\omega_t\\}_{t=1}^T$ is a set of weight vectors randomly drawn from optimized variational distribution $q_\\theta(\\omega)$ with T number of samples. For a high value of T, it converges to the probability of $q_\\theta(y_*|x_*)$ shown in Eq.(5) for all $\\omega \\in \\Omega$. (Kwon et al., 2020)"}, {"title": "2.4. Reparameterization Trick", "content": "Part of the strategies for generating inference about posterior distribution and variance reduction is a sampling process during optimization, called reparameterization trick. Being w the weights of the network, they can be written in terms of an auxiliary variable \u0454:\n$$\\omega \\sim q(\\omega|\\theta) = g(\\epsilon, \\theta)$$(8)\nFor e ~ p(e) where p is an independent distribution of parameter 0 that we want to optimize in network training process. We get an estimation of $q_\\theta$ with:"}, {"title": "2.5. Multiplicative Normalizing Flows", "content": "In the analysis of limit in Eq.(6), ideal variational distribution is when $KL\\{q||p\\}$ equals zero. However, achieving this with mean field approximation introduced in Eq.(4) is not feasible. For this purpose, we consider a more complex and flexible family of distributions that allows the true posterior distribution to be one of the possible solutions. By increasing the complexity, we expect significant performance enhancements because we can draw samples from a more reliable distribution that is closer to the true posterior. Multiplicative normalized flows (MNF), are a way to obtain mentioned distributions through a combination of auxiliary random variables with normalization flows Louizos and Welling (2017). By associating the parameter 0 with a family of distributions to be compared over the posterior, and introducing an auxiliary latent variable in the form of a vector z ~ $q_\\theta(z) = q(z)^2$,"}, {"title": "3. Observing calibration", "content": "A perfectly calibrated model is defined as one where prediction P is a real probability in frequentist terms, i.e., it represents real probability that prediction is correct. This applies to a scenario with variables X and Y, where X \u2208 X, Y in Y = {0,1}. The joint distribution of X and Y is given by p(X, Y) = p(Y|X)p(X). Otherwise, we have a neural network with input h(X) and prediction (\u0176, P), being \u0176 inference about the class and its"}, {"title": "3.1. Expected calibration error (ECE)", "content": "Several metrics are available to measure a model calibration, one of the most common and recognized is the so-called Expected Calibration Error (ECE). This metric, naturally derived from Eq.(17) represents the difference between prediction confidence and accuracy (Wang et al., 2022a)\n$$ECE = E_\\wp [|E [\\hat{Y} = Y|\\hat{P} = p] - p|],$$ (18)\nwhich is obtained by computing the weighted average of accurac\u0443 \u0430\u0441\u0441(\u0412\u043c) by partitioning p-space of predictions into M bins, where confidences are denoted by conf (Bm), value n, and |BM| the number of pixels that fall into a bin. In semantic segmentation scheme n represents the number of pixels\n$$ECE = \\frac{1}{n} \\sum_{m=1}^M |B_M||acc(B_M) - conf(B_M)|.$$(19)"}, {"title": "3.1.1. ECE for semantic segmentation", "content": "For ECE estimation, we adopt the approach followed in (Wang et al., 2022a), where each pixel in an image is considered as a single sample, result-"}, {"title": "3.2. Reliability diagrams", "content": "Reliability diagrams are a visual representation of ECE, or equivalently, how well a model is calibrated. These graphs illustrate correlation between the expected accuracy of a sample and model confidences, using a partitioning of the prediction space into M bins. If model is perfectly calibrated, i.e. if the condition Eq. (17) is satisfied then, the relationship should be represented by an identity function. Any deviation from a perfect diagonal indicates a lack of calibration, implying that uncertainties are either under- or over-estimated (Guo et al., 2017)."}, {"title": "4. Metrics and loss functions", "content": "To evaluate the performance of the models, we considered IOU (Intersection over Union) since it measures the exact spatial similarity between areas segmented by the model and the masks. This metric, based on F-score, is particularly useful for evaluating accuracy of segmentation models in scenarios where high accuracy at edges of region of interest is critical (M\u00fcller et al., 2022).\n$$IoU_c = \\frac{\\sum_i (y_i(c) \\land \\hat{y}_i(c))}{\\sum_i (y_i(c) \\lor \\hat{y}_i(c))},$$ (21)"}, {"title": "4.2. Loss functions", "content": "The loss functions are crucial in training stage to produce accurate predictions, especially in semantic segmentation domain. Our work will employ Jaccard loss, Dice loss, binary cross entropy, and total loss from the python library Segmentation Models (Iakubovskii, 2019)."}, {"title": "4.2.1. Region based", "content": "\u2022 Jaccard Loss: This loss function calculates intersection over union between region of interest (ROI) and region predicted by the model, to optimize the overlap between them\n$$J(A, B) = 1 - \\frac{A \\cap B}{A \\cup B},$$ (23)\nwhere A is region of interest in ground truth, and B is the region which is predicted by the model.\n\u2022 Dice Loss: Similar to Jaccard Loss, this loss function is also focused on calculating the intersection over union"}, {"title": "4.2.2. Distribution based", "content": "\u2022 Binary cross-entropy: This is computed as the difference between actual distribution and the predicted distribution (Ma et al., 2021).\n$$BCE(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^N y_i log(\\hat{y_i}) + (1 - y_i) log(1 - \\hat{y_i}),$$(25)\nhere, y is real value, \u0177 is the prediction and N is number of samples.\n\u2022 Total loss: This function takes into account both the similarity beetween regions of interest and the focus on the minority class in cases of class imbalance in semantic segmentation task.\n$$TL(y, \\hat{y}) = D(A, B) + (0.5 * BFL).$$(26)\nDice loss is denoted by D, and BFL is Binary Focal Loss function"}, {"title": "4.3. Neg-Log Likelihood", "content": "The Negative Log Likelihood (NLL) is a function used to measure how closely a model fits the actual data. It is calculated based on number of samples n and the distribution p(Y|X)\n$$NLL = -\\frac{1}{n} \\sum_{i=1}^n log p((Y|x_i)).$$(28)\nIn our case we will use it as a loss function for BNN models, any loss that includes an NLL is equivalent to minimizing the divergence Kullback-Leibler in Eq.(3), or alternatively, it is a binary cross entropy computed between the distribution defined by training set and the probability distribution defined by model Goodfellow et al. (2016)."}, {"title": "5. Dataset", "content": "The CVC-CLINICDB database, which is a free and public database, will be used for this work. It was developed by (Baena J., 2015), and comprises 612 images extracted from colonoscopy videos and created for the study"}, {"title": "6. Experimental Setup", "content": "Using the database referenced in Fig.(1), a binary segmentation task was conducted, class 0 represents the background and class 1 represents the polyp. The dataset was divided into three parts: training, validation, and test, with 70%, 20%, and 10% respectively. All images were resized to 256x256 to eliminate black borders and facilitate network input. For the training images, preprocessing was performed in the following order\n1. Adjust brightness, saturation, and contrast of the image randomly."}, {"title": "6.2. Deterministic models", "content": "At training phase, models were optimized using Adam optimizer with a batch size equals to eight. Early stopping was implemented by monitoring loss value on the validation set with a patience of 3. Four loss functions were used, as defined in Sec.(4.2). The pipeline was built using Tensorflow v:2.15\u00b9 and Tensorflow-probability v:0.222. furthermore, we selected three architectures: Unet, Linknet, and FPN, using python library Segmentation Models\u00b3.\nThis module offers several advantages, including ease of implementation, a choice of four model architectures have been proven to be effective for binary segmentation and 25 backbones with pre-trained weights to achieve efficient convergence (Iakubovskii, 2019). These architectures were tested with four loss functions mentioned in Sec. (4.2) and three backbones that have been suggested for use in medical image segmentation: Seresnet101, Densenet169, EfficienNetB7 (Abedalla et al., 2021). A total of 36 iterations of deterministic models were performed for all possible combinations, as shown in the tables Tab.B.3, Tab.B.4, Tab.B.5."}, {"title": "6.3. Bayesian models", "content": "We adapted deterministic architectures to a Bayesian approach using the module models from segmentation models. To carry out this task, we utilized MNFConv2D class from tf-mnf module, replacing some strategic Conv2D Tensorflow layers in this code. Moreover, we have modified output layer of these architectures by adding a layer Independent Bernoulli from Tensorflow-probability module. Three architectures with highest IOU metric in test Appendix B, Unet + EB7, FPN + EB7, and Linknet + EB7, were evaluated with three different configurations each, resulting in a total of nine models. The MNFConv2D layers were strategically placed in the networks. All models were trained using the defined NLL loss Eq.(28) function.\nThe nine modified configurations are as follows:\n1. UNET: Backbone output - Fig.(2), all layers of the final block of the backbone, last layer of each decoder.\n2. FPN: Backbone output, all layers of the final block of the backbone - Fig.(3), output concatenate + output last pyramidal block.\n3. Linknet: Backbone output, all layers of the final block of the backbone, last layer of each decoder Fig.(4)."}, {"title": "7. Results", "content": "Considering results achieved in all architectures Unet, FPN and Linknet, and using IOU as the main metric, and secondly, recall, it was determined that Efficient NetB7 is best backbone in terms of performance in iterations of Tab.B.3, Tab.B.4, Tab.B.5. In particular, binary cross entropy loss function was found to be the most efficient for Unet and Linknet architectures. Conversely, for FPN, total loss function was the best alternative. The top-performing model in iterations was Linknet+EB7+BinaryCE, achieving an IOU of 0.941 in test. Otherwise, the model with worst performance was FPN+ Densenet169+ Total loss, with an IOU of 0.78 and recall 0.72 in test. Upon analyzing the tables in Appendix B, it is found that the best configuration for all iterations performed with Densenet169 backbone was FPN - BinaryCE, with IOU = 0.92. The results of the combinations performed with different architectures and loss functions in Densenet169 show the presence of many false negatives. This is evidenced by the recall, which is consistently below 0.8 in most combinations and lower on average than other iterated"}, {"title": "7.4. Feature Importance", "content": "Following the methodology in Wickstr\u00f8m et al. (2020), We compute the feature importance of the segmentation images via image-gradient approach Simonyan et al. (2014) to interpret the results generated by the networks. Fig. 8 illustrates the crucial pixels for the segmentation process, particularly in areas containing polyps. Most notable features are observed near the borders of polyps. Bayesian techniques are not affected by changes far from the regions of interest, demonstrating robustness and interpretability. However, the influence zone surpasses the polyps borders, suggesting that the prediction also takes into account the global setting. Deterministic networks are inadequate in detecting atypical regions in situations with extremely small polyps, resulting in unsatisfactory segmentation outcomes. This is supported by the feature visualization shown at the top of Fig. 6. In contrast, uncertainty estimates can identify areas where polyps may be present and offer crucial insights into the unreliability of neural networks in making predictions in certain pixel locations."}, {"title": "8. Conclusions", "content": "The result of this study shows that Bayesian models evaluated stand out for their good performance, since they have an IOU in test set consistently above 0.9, which shows efficiency of architectures tested for semantic segmentation of medical images. The architecture based on Linknet + EfficientnetB7 demonstrated good results in both, deterministic and its Bayesian configuration (MNF layers). It presented a good calibration as well as a balanced option in visual terms and with adequate sensitivity for detecting colorectal"}]}