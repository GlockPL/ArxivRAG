{"title": "Using Code Generation to Solve Open Instances of Combinatorial Design Problems", "authors": ["Christopher D. Rosin"], "abstract": "The Handbook of Combinatorial Designs catalogs many types of combinatorial designs, together with lists of open instances for which existence has not yet been determined. We develop a constructive protocol CProl, which uses Large Language Models (LLMs) to generate code that constructs combinatorial designs and resolves some of these open instances. The protocol starts from a definition of a particular type of design, and a verifier that reliably confirms whether a proposed design is valid. The LLM selects strategies and implements them in code, and scaffolding provides automated hyperparameter tuning and execution feedback using the verifier. Most generated code fails, but by generating many candidates, the protocol automates exploration of a variety of standard methods (e.g. simulated annealing, genetic algorithms) and experimentation with variations (e.g. cost functions) to find successful approaches. Testing on 16 different types of designs, CPro1 constructs solutions to open instances for 6 of them: Symmetric and Skew Weighing Matrices, Equidistant Permutation Arrays, Packing Arrays, Balanced Ternary Designs, and Florentine Rectangles.", "sections": [{"title": "1 Introduction", "content": "A Packing Array is one of many types of combinatorial designs cataloged in the Handbook of Combinatorial Designs [Colbourn and Dinitz, 2006] (henceforth Handbook). A Packing Array is an N by k array of elements from {0,1,..., \u03c5 \u2013 1}, such that every pair of columns contains each ordered pair of elements at most once (Fig. 2). Given k and v, we want to know the largest N for which a Packing Array exists (since we can always construct Packing Arrays with smaller N by removing rows). For k = 14 v = 9, the Handbook lists N = 18 as the best known. Later results using SAT solvers raised this to N = 20 [Noritake et al., 2014]. The upper bound is N < 21 [Colbourn and Dinitz, 2006], so it is an open question whether N = 21 is possible.\nOne approach involves trying various heuristic methods, experimentally tuning each to determine whether they can construct the desired design. In this paper, we develop a protocol CProl (Fig. 1) that uses LLMs to generate code for diverse candidate methods. The protocol automates an experimental process to identify and optimize heuristic construction strategies. It succeeds in constructing a Packing Array with N = 21 for k = 14 v = 9, fully resolving this open question. The protocol also resolves open instances of other existence problems from the Handbook. Tables 1 and 2 show the main results."}, {"title": "2 Related Work", "content": "Code generation is one of the primary applications of LLMs [Hou et al., 2024; Jiang et al., 2024]. LLMs have the potential to translate natural language requirements to working programs, even without being given the ability to fully test the code.\nFor difficult problems, it can help generate multiple candidate programs and test whether any succeed [Chen et al., 2021]. Protocols that prompt the LLM for a natural-language description before generating code can generate more diverse and successful candidates [Wang et al., 2024], and we use such an approach here.\nIn code generation benchmarks, LLM-generated code is commonly tested on a small number of test cases. Particularly when generating multiple candidate solutions, this can lead to a situation where generated code is correct on the limited set of test cases, but fails to generalize to other inputs [Liu et al., 2024b]. Only a restricted set of problems, for which an oracle verifier can fully verify solutions, benefit from generating arbitrarily large numbers of candidates [Stroebl et al., 2024]. In this paper, our problems have such oracle verifiers, so we can fully benefit from generating large numbers of candidate solutions.\nLLMs have been applied to rewrite existing code to improve its performance [Gong et al., 2025], including by sampling and testing multiple optimization candidates from the LLM [Qiu et al., 2024]. Our protocol includes a step that attempts to improve performance by generating multiple optimization candidates and measuring their performance.\nCode generation with LLMs has been used to develop and improve heuristics for combinatorial optimization problems [Liu et al., 2024a], including generating improved search operators for genetic algorithms [Ye et al., 2024]. In this paper, we use LLMs to propose and implement heuristic strategies in an open-ended way, and the successful strategies that emerge include genetic algorithms [Mitchell, 1998] and simulated annealing [Kirkpatrick et al., 1983].\nLLaMEA-HOP [van Stein et al., 2024] uses automated hyperparameter tuning together with LLM-generated heuristics. The authors motivate it by noting it is potentially very costly to use the LLM for hyperparameter tuning, so they ask the LLM to expose a hyperparameter configuration space, and then offload hyperparameter tuning to a separate specialized system. We share this motivation, and use automated hyperparameter tuning for generated code that exposes hyperparameters.\nEfforts to apply LLMs to mathematics have focused on benchmarks with known solutions [Hendrycks et al., 2021; He et al., 2024] and generation of step-by-step proofs that could be verified with systems like Lean [Yang et al., 2024; Stroebl et al., 2024]. Here, we focus on problems that can be resolved by constructing a combinatorial object that can be easily verified, rather than requiring a step-by-step proof."}, {"title": "3 Method", "content": "Combinatorial designs are systems of finite sets that satisfy a set of constraints. The specific finite sets and constraints involved define the type of combinatorial design (e.g. Packing Arrays, or Balanced Incomplete Block Designs). The existence problem (or combinatorial design problem) for a particular type of combinatorial design has a small number of input parameters (e.g. size), and asks whether it is possible to construct a design that satisfies the constraints for these parameters. An instance of the combinatorial design problem specifies particular numerical values for the parameters, and a solution to the instance would exhibit a combinatorial object (the design) that meets the constraints instantiated with these parameters (or a proof that none exists, but in this paper we are limited to seeking solutions that construct the design). For example, one instance of the existence problem for Packing Arrays would ask whether a design with N = 21 k = 14 \u03c5 = 9 exists, and a solution could give an N by k array that meets the constraints of the definition given in Section 1. Note there may be more than one solution to a particular instance of the existence problem.\nA Large Language Model (LLM) takes a textual prompt as input and returns a textual response, which may include natural language and/or programming language code. LLMs are trained via machine learning, but in this paper we are only using off-the-shelf pretrained LLMs. We use the term protocol to refer to an algorithm or system which includes calls to an LLM, and the scaffolding consists of the elements of this protocol other than the LLM."}, {"title": "3.2 Selection of Combinatorial Designs", "content": "From the Handbook, we select 16 types of combinatorial designs (Table 3) that have clearly defined open instances with relatively small parameters that might be amenable to heuristic search. For each of these, we review the literature to identify which instances have remained open.\nFor each selected type of design, we provide:\nTextual Definition Defines the design and mandates a specific representation as an array of small integers.\nVerifier in Python Determines whether a proposed design in this representation is correct.\nOpen Instances Parameters for which existence of this type of design is not yet known. The ultimate goal is to construct designs with these parameters.\nDevelopment Instances (Dev Instances) Parameters for which designs of this type are known to exist, including some of the smallest ones, as well as ones just slightly smaller than the open instances. Candidate approaches have their generated code executed on these, with results checked by the Verifier, to identify the most promising candidates."}, {"title": "3.3 Prototyping Set", "content": "Our goal is to develop an LLM-based protocol which could resolve open instances of combinatorial design existence problems. Since the existence of open instances is unknown, it could be that they don't exist and constructing them is impossible - this makes it difficult to prototype protocols for constructing these designs. To aid in prototyping, we first establish a small prototyping set of open instances that actually do exist and can be constructed. We manually experimented with local search methods for 5 of of the selected combinatorial designs: Bhaskar Rao Designs, Difference Triangle Sets, Equidistant Permutation Arrays (EPA), Supersimple Designs, and Symmetric Weighing Matrices (SymmW). The local search methods select changes which minimize a cost function, while sometimes accepting worsening moves to escape local optima. Local search succeeded in resolving one open instance for EPA and two for SymmW; these form our prototyping set for use while developing a protocol that can also resolve these. The manually-developed local search algorithm for EPA is shown in Algorithm 1."}, {"title": "3.4 Protocol Development", "content": "In our protocol, an LLM takes a combinatorial design's definition as input, selects diverse strategies, and generates code for them. We provide scaffolding that compiles and executes the generated code on development instances, checks results with the verifier, and selects the most promising candidates for use on open instances. We generate code in C to take advantage of its speed.\nExperiments on the prototyping set used GPT-40 [Hurst et al., 2024] and Claude 3.5 Sonnet [Anthropic, 2024]. For GPT-40, the 2024-05-13 version gave better initial results than a later version, so we continued using the 2024-05-13 version. Through experimentation on the prototyping set, we establish these observations and elements of the protocol:\nPrompting for Diversity Prompting for 20 strategies yields more diverse approaches than asking for 10, or repeatedly asking for 1.\nPrompting for Details, then Code Prompting for a detailed textual description of the approach, before prompting for the code, improves reliability of the generated code.\nPrompting to Prevent Early Termination We expect solutions to run for minutes or hours before succeeding. Generated code often terminates too quickly (milliseconds), which we partially mitigate by prompting for code that should \"not terminate until a valid solution is found.\"\nMultiple Random Seeds Testing development instances on multiple seeds encourages randomized strategies, which have an increased chance of success when running multiple seeds in parallel on open instances. We run a candidate program on all of the development instances, each with multiple seeds, in parallel (the rest of the protocol is serial).\nLLMs Fail to Generate Viable Local Search Our manually developed solutions to the prototyping instances relied on local search. While the LLMs often propose strategies that the LLM describes as \"Local Search\", their actual implementations are consistently naive: simple hill climbing which quickly gets stuck in local optima, or pure random search that makes no progress. This is concerning, particularly since local search methods which avoid local optima are well-established [Hoos and St\u00fctzle, 2004; Cai et al., 2013].\nAutomated Hyperparameter Tuning The LLMs sometimes propose simulated annealing, which could potentially serve as a viable alternative to local search. But the LLMs usually propose naive annealing schedules that very quickly ramp temperature down to the point where it becomes simple hill climbing stuck in local optima. We partially mitigate this by asking the LLM to expose hyperparameters and provide ranges and defaults for them, and we then have the scaffolding provide automated hyperparameter tuning. We find extreme ends of the range are often important (e.g. simulated annealing cooling rate of 1.0, which yields constant-temperature simulated annealing that resembles local search), and we find that an adaptive hyperparameter tuning method [Akiba et al., 2019] struggles to explore these extremes. We therefore provide grid based hyperparameter tuning, that uses a coarse-grained linear grid across the middle of the range of each hyperparameter, and logarithmic/fine-grained near the end points. Hypertuning begins with up to 1000 settings to the hyperparameters, and tests each of these settings for 0.5 seconds on the development instances and seeds in parallel (for a total of 500 seconds elapsed wall clock time, excluding overhead). Then the next round takes up to 100 of the best settings and executes for 5 seconds, and a final round takes up to 10 of the best settings and executes for 50 seconds, returning the best-scoring setting to the hyperparameters. This is able to find successful hyperparameter settings for simulated annealing.\nPrompting for Code Optimization Prompting for iterative refinement to generated code has limited success. Open-ended refinements are likely to abandon what was already successful. But more limited refinements that prompt only for optimizing speed are more successful - e.g. converting an expensive cost function implementation into a faster incremental one. Our protocol samples 50 optimization candidates from the LLM, and then if there is substantial improvement it repeats the process (up to 5 rounds).\nSandboxing Running the generated code in a protected sandbox is essential. Otherwise, buggy C code can allocate excessive memory that results in crashing the whole protocol. We use firejail on Linux, which also blocks file system and network access, and can block problematic programs without crashing the rest of the protocol.\nGenerate Many Candidates Most generated code fails, often for simplistic reasons (see Section 4.3). But repeating the process eventually obtains code that succeeds on some development instances. Running the protocol at smaller scale with a total of 100 or 300 generated programs failed to solve the full set of 3 prototyping instances (see Table 4), but 1000 candidates (50 repetitions of 20 strategies each) succeeded."}, {"title": "3.5 Experiment", "content": "For each of the 16 types of combinatorial design, we run the protocol once with GPT-40 and once with Mistral Large. We also test ablated versions of the protocol on each type of combinatorial design. Each ablation uses the same candidate programs generated by the original run.\nWhen we succeed in solving least one open instance of a combinatorial design problem, we extend by testing the generated code on adjacent open instances, including all of the smallest open instances from the Handbook shown in Tables 1 and 2.\nThe experiments are run on a Linux machine with AMD Ryzen 9 7950X3D CPU and 128GB of memory, and the machine is fully dedicated to one experiment at a time. With the chosen parameters shown in Algorithm 3, a full run on one type of combinatorial design takes approximately 7 days of runtime. The majority of the runtime is used running candidate programs on development instances and open instances."}, {"title": "4 Results", "content": "Tables 1 and 2 show the main results. The protocol CPro1 succeeds on open instances of the existence problem for 6 types of combinatorial designs, including 4 that were not in the prototyping set. The tables show that there has been some progress elsewhere in the literature on these open instances from the Handbook, and the results from CProl add significantly to this progress.\nFor the other types of combinatorial designs in Table 3, code generated by CProl was able to solve instances in the development set, but this code was unable to solve any of the open instances. Neither GPT-40 nor Mistral Large solved any open instances for these, and none of the ablation runs solved any open instances for these.\nThe successful results are constructed by programs that range from 120-270 lines of C code.\nResults for both GPT-40 and Mistral Large are shown in Table 5. CProl using Mistral Large solves a subset of the open instances that are solved using GPT-40. The full results in Tables 1 and 2 and Appendix B were generated using GPT-40."}, {"title": "4.1 Ablation", "content": "Table 5 also shows ablation results. All of the considered elements of the protocol, including automated hyperparameter tuning and optimization of the initial code, are needed to obtain the full results. Automated hyperparameter tuning was developed for simulated annealing during prototyping (Sec. 3.4), but it was also needed to obtain positive results from the genetic algorithm code generated for Balanced Ternary Designs."}, {"title": "4.2 Strategies Implemented by Generated Code", "content": "Each run has a total of 1000 candidates, starting from 50 lists of 20 proposed strategies each. With both GPT-40 and Mistral Large, simulated annealing and genetic algorithms are frequently proposed \u2013 often close to 50 times (nearly every list of proposed strategies) or even more than 50 if alternate names are counted. Other frequently proposed strategies include depth-first search (under various names e.g. \"backtracking\"), greedy algorithms, tabu search, recursive constructions, branch and bound, and more.\nAll of the successes in Table 5 were due to simulated annealing or genetic algorithms, except for Florentine Rectangles which used depth-first search.\nAs discussed in Sec. 3.4, simulated annealing runs the risk of rapid cooling schedules getting permanently stuck in local optima, which destroys the productivity of the extended runs that are needed for many of these results. The code generated here mitigated this by using a constant temperature (which is fine-tuned by automated hyperparameter tuning), or a periodic reset, or (in one ablation run) an extremely slow cooling schedule. The constant temperature implementations were achieved by a mix of hyperparameter tuning setting cooling rate to 1.0, and generated code which hardwired a constant temperature \u2013 however in both cases the LLM-generated strategy details do not explicitly address the value of a constant temperature; even when hardwired it seems more like an accidental feature that ends up performing well."}, {"title": "Algorithm 1 Manually created local search for EPA. It can escape local optima by randomly selecting a row r in which all column swaps are worsening moves; it is forced to choose one of these.", "content": "Input: N,k,d\nOutput: Valid EPA with parameters (N,k,d)\nState: E has N rows, each a permutation of {0, 1, ..., k \u2212 1}\ndef d(i, j): return #columns differing in rows i,j of E\ndef cost = $\\sum_{rows i<j} |d(i, j) \u2013 d|$\nInitialize E with random permutation in each row\nwhile cost > 0 do\nr = random choice from {rows i : \u2203j with d(i, j) > 0}\nc, d = distinct col. in r that, if swapped, minimize cost\nswap columns c and d in r\nend while\nreturn E"}, {"title": "Algorithm 2 CPro1-generated simulated annealing for EPA. It can escape local optima by accepting a worsening move, with a probability controlled by a constant temperature selected by automated hyperparameter tuning.", "content": "Input: N,k,d\nOutput: Valid EPA with parameters (N,k,d)\nState: E has N rows, each a permutation of {0, 1, ..., k \u2212 1}\ndef d(i, j): return #columns differing in rows i,j of E\ndef cost = $\\sum_{rows i<j} |d(i, j) \u2013 d|$\nInitialize E with random permutation in each row\nwhile cost > 0 do\nr = random choice from {rows i}\nc, d = randomly selected distinct columns in r\n\u2206 = (cost if c and d are swapped) - (current cost)\nif \u2206 < 0, or otherwise with probability $e^{-\u2206/0.444444}$:\nswap columns c and d in r\nend while\nreturn E"}, {"title": null, "content": "different methods of escaping local optima. CProl's solution performs somewhat better: on the hardware used for these experiments, for open instance n=12 d=8 m=21, CPro1's simulated annealing has a median solution time of 10 hours, compared to 21 hours for manually developed local search.\nThe genetic algorithm for Balanced Ternary Designs uses a rather small population of only 100, running for over a billion generations to construct some of the designs. This genetic algorithm has no elitism, and a high mutation rate, which could maintain diversity during such extended runs.\nBroadly speaking, the application of genetic algorithms and simulated annealing to generating combinatorial designs is well established (Handbook chapter VII.6). However, from reviewing the literature, the application of genetic algorithms and simulated annealing (or local search) specifically to Symmetric and Skew Weighing Matrices, Balanced Ternary Designs, and Equidistant Permutation Arrays appears to be novel. Genetic algorithms have been proposed for equidistant permutation arrays, but with no results reported yet [Mariot et al., 2021].\nFor Packing Arrays, some of the previous results from the literature were obtained in 2001 via genetic algorithms and simulated annealing [Stardom, 2001]. Unlike the constant-temperature simulated annealing generated here, this earlier simulated annealing used a traditional cooling schedule that ramps temperature down towards 0 rather quickly. Given increases in computing power since 2001, simulated annealing can now run for orders of magnitude more iterations, making constant temperature potentially more valuable.\nFor Florentine Rectangles, the successful generated code implements a depth-first search that randomly shuffles the choices to be tried at each position in the array. This randomization enables effective use of the multiple random seeds that run in parallel under CPro1. The code also achieves efficiency by incrementally tracking the status of constraints. Even so,"}, {"title": null, "content": "the code does not appear to be fully optimized (for example, it does not leverage bit parallelism). A recent application of Florentine Rectangles to coding theory [Adhikary et al., 2022] presented new systematic constructions for Florentine Rectangles that scale up to arbitrarily large n, but noted that these constructions did not improve upon results from the Handbook for small n.\nAll of the successful strategies are randomized. This gives the possibility of running them further with different random seeds, to generate additional examples of the generated designs with the same parameters."}, {"title": "4.3 Modes of Failure and Success", "content": "A majority of generated candidates fail completely and score 0 on the development instances. Even for candidates which score more than 0, many are naive and have no hope of solving instances beyond the very smallest development instances. These failures happen for a variety of reasons depending on the model. For example, considering simulated annealing candidates for Equidistant Permutation Arrays (see Table 4):\nClaude 3.5 Sonnet's failed run Here, 37 of the 50 simulated annealing candidates have an off-by-one bug when checking argc (the number of command-line arguments), stopping execution and scoring 0. The generated code is required to take the instance-defining parameters, a random seed, and any hyperparameters as command-line arguments. In C, the number of command-line arguments includes an extra one for the command itself; Claude 3.5 Sonnet's off-by-one error fails to account for this.\nMistral Large's failed run A majority of simulated annealing candidates either (a) have a bug that swaps between rows, destroying the property that each row is a permutation, or (b) drive temperature rapidly to 0, becoming stuck in local optima with no possibility of escape - these candidates can score more than 0, but never solve more than a quarter of the development instances. Only 3 of Mistral Large's simulated annealing candidates avoid problems like these and solve over a quarter of the development instances, but 3 candidates didn't provide enough experiments to optimize simulated annealing for the problem.\nGPT-40's successful run A majority of its candidates which solve less than a quarter of the development instances either (a) drive temperature rapidly to 0 as with Mistral Large, or else (b) have a syntax error that prevents the C code from compiling. But problems occur less frequently than with Mistral Large, and GPT-4o has 12 simulated annealing candidates that are viable experiments solving over a quarter of the development instances; enough experimentation that GPT-40 is able to succeed with a candidate that solves an open instance. An example of such experimentation is with variations in the cost function. Comparing to the eventual successful approach in Algorithm 2, we see variants which square the $|d(i, j) \u2013 d|$, or which only count the rows in which $|d(i, j) \u2013 d| > 0$ instead of using the actual value of $|d(i, j) \u2013 d|$. Such variants are reasonable experiments, but ultimately they have mediocre scores and likely would not be viable for solving open instances. The cost function in Algorithm 2, when combined with other appropriate details (e.g. neighborhood) is able to succeed in solving an open instance."}, {"title": "5 Limitations", "content": "The positive results reported here arise from an automation of computational experimentation that could have been done manually. The research community has only undertaken limited effort on such computational experimentation for the designs with positive results here; this may have left low hanging fruit for CProl. Some of the designs with no positive results here have received greater attention from the research community. For example, for Coverings, an online repository notes results from various contributors, who have used simulated annealing, local search, and other methods [Gordon, 2025]. In CProl's run on Coverings, generated code was successful on development instances, but had no success on open instances.\nThe combinatorial design research community has greater focus on systematic mathematical constructions (e.g. using algebraic methods), rather than direct computational search for small designs. Systematic constructions may be scaled up to much larger instances than the ones within reach of direct search by heuristics. Even for small instances, some of the literature results in Tables 1 and 2 come from systematic mathematical constructions [Greig, 2002; Georgiou et al., 2023]. CPro1 doesn't necessarily exclude systematic constructions \u2013 for example, for Costas Arrays, some of the generated code implements the Welch method of systematic construction [Golomb and Taylor, 1984], and this successfully solves larger development instances than are solved by direct search. However, CProl is never able to extend these methods in a way that could succeed on open instances.\nThe positive results result from applying standard methods, not inventing new methods. The protocol helps automate substantial experimentation in trying various standard methods, and testing variants (e.g. cost function variants as noted in Section 4.3) to optimize the method to the problem at hand but it is not inventing new techniques.\nThe results presented here are from two full runs of CPro1 on each type of combinatorial design \u2013 one with GPT40 and one with Mistral Large. Repeated runs would yield different results. Since the LLMs are inherently nondeterministic in their responses, it isn't possible to exactly the replicate the results of any one run.\nWe did not test CProl at larger scale (e.g. 10,000 candidates per run rather than 1000), and it is possible this could give better results.\nA test run of CProl on the Cap Set problem failed to reproduce FunSearch's result [Romera-Paredes et al., 2024] of a size-512 cap set for dimension n = 8."}, {"title": "6 Code Availability", "content": "The Python code for CPro1, along with the solutions to open instances and the generated C code that constructed them, are available: https://github.com/Constructive-Codes/CPro1"}, {"title": "7 Conclusion", "content": "The protocol CPro1 uses LLMs to generate code, and has successfully solved open instances of the existence problem for 6 types of combinatorial designs. The protocol can be run on additional types of combinatorial designs, by supplying a textual definition, a Python verifier, and small collections of development instances and open instances. The protocol could likely be applied in other domains that allow automated full verification of solutions. The protocol can be readily used with new LLMs as they become available (e.g. \"reasoning models\" like OpenAI's o1 [OpenAI, 2024]), which may be able to reduce failure modes and improve overall capabilities."}]}