{"title": "Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis", "authors": ["Zhen Ye", "Xinfa Zhu", "Chi-Min Chan", "Xinsheng Wang", "Xu Tan", "Jiahe Lei", "Yi Peng", "Haohe Liu", "Yizhu Jin", "Zheqi DAI", "Hongzhan Lin", "Jianyi Chen", "Xingjian Du", "Liumeng Xue", "Yunlin Chen", "Zhifei Li", "Lei Xie", "Qiuqiang Kong", "Yike Guo", "Wei Xue"], "abstract": "Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing. This work makes the following contributions: First, we explore the scaling of train-time and inference-time compute for speech synthesis. Second, we propose a simple framework Llasa for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard LLMs such as Llama. Our experiments reveal that scaling train-time compute for Llasa consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. Furthermore, from the perspective of scaling inference-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available.", "sections": [{"title": "1. Introduction", "content": "Recent years have witnessed the remarkable success of large language models (LLMs) in the text domain, represented by the GPT series(Brown et al., 2020; Achiam et al., 2023; Radford et al., 2019). These advances have demonstrated that increasing model size and training data consistently yields better performance across a wide array of natural language understanding and generation tasks. However, as the text domain approaches data saturation, new directions are emerging, such as \u201col\u201d models (Jaech et al., 2024) that emphasize extensive computational effort at test time-thereby exhibiting an inference-time scaling effect. By investing more resources during inference, these models are able to produce higher-quality outputs and handle more complex tasks, offering a flexible avenue for performance improvement after the training phase.\nMeanwhile, text-to-speech (TTS) research has also made impressive strides. Many existing TTS systems focus on devising better model architectures-leveraging well-designed modules, larger datasets, and increased model size-to push synthesis quality ever higher. While these efforts have propelled the field forward, they also tend to narrow the community's perspective: the focus on better architectures can overshadow investigations into broader, potentially transformative research questions. In contrast, the text LLM community has converged on a relatively standard framework-a simple Transformer model with a tokenizer-which allows researchers to concentrate on fundamental issues such as training-time scaling laws (Kaplan et al., 2020), inference-time scaling behaviors (Snell et al., 2024), and downstream adaptations (e.g., fine-tuning (Hu et al., 2021), pruning, and quantization(Zhu et al., 2024a)). Such a common design philosophy has catalyzed rapid progress and deeper insights into the text domain.\nMotivated by these observations, we propose aligning TTS with the minimalist yet powerful paradigm of text LLMs. We introduce a single Transformer-based TTS model that relies on a well-designed speech tokenizer. More specifically, our TTS system, named Llasa, is initialized from the Llama (Touvron et al., 2023) model with an expanded vocabulary that incorporates speech tokens and is trained using the next-token prediction paradigm. Although this model may not always match the performance of highly customized TTS systems, its streamlined design creates a unified foundation for exploring a wider range of research questions-beyond architecture exploration.\nIn particular, we systematically investigate both training-time and inference-time scaling effects under this unified TTS framework. Our experiments show that scaling training-time compute (e.g., increasing model size or training data) not only improves naturalness but also enhances expressive prosody, effectively capturing the meaning conveyed in text without explicit labels. Additionally, we examine the utility of inference-time scaling by incorporating speech understanding models as verifiers in the search process. We find that spending more computational effort during inference aligns generation outputs more closely with specific verifier biases, yielding better emotional expressiveness, timbre consistency, and content accuracy. Evaluations on LibriSpeech test sets (Panayotov et al., 2015), seed-tts-eval (Anastassiou et al., 2024) and ESD datasets (zho, 2022) demonstrate state-of-the-art results, and further highlight how in-context learning capabilities can be combined with search-based refinements to control factors such as speaker identity or emotion.\nIn summary, our paper makes several key contributions. We design a TTS model named Llasa that is fully aligned with standard LLM architectures by utilizing a single Transformer and a well-designed speech tokenizer, thereby creating a simple, flexible, and scalable system. Additionally, we find that increasing training-time compute for Llasa leads to significant improvements in speech naturalness and prosody accuracy, which reflects a deeper semantic understanding of the input text. We further demonstrate that scaling inference-time compute\u2014achieved by incorporating speech understanding verifiers\u2014enhances emotional expressiveness, timbre consistency, and content accuracy in synthesized speech. Furthermore, by providing open access to our models and frameworks, we aim to foster further research and development in the TTS community."}, {"title": "2. Methods", "content": "Our TTS framework is designed to fully align with the standard text LLM paradigm, keeping only two main components: (1) a tokenizer and (2) a single Transformer-based LLM. We initialize the Transformer $\\&$ from an existing LLM (e.g., Llama), and inherit its tokenizer for the text portion. Hence, the core new challenge is to convert raw speech waveforms into sequences of discrete tokens such that the Transformer can model them in an autoregressive manner.\nTo achieve this, we introduce our speech tokenizer, X-codec2, which encodes waveforms into speech tokens and can decode them back to high-quality audio. Unlike some prior tokenizers for TTS, ours requires no additional information during decoding, ensuring that all aspects of the speech signal, such as content, prosody, and timbre, are captured by the LLM.\nFormally, let: 1. $Tokenizetext(X) = {x_1,...,x_T}$ be the text tokenizer, which converts input text X into T text tokens. 2. $Tokenizespeech(Y) = {y_1,...,y_S}$ be the speech tokenizer, which converts a speech waveform Y into S discrete tokens. 3. $Detokenizespeech({Y_1,...,y_S}) = \\hat{Y}$ be the speech decoder, which reconstructs the waveform $\\hat{Y}$ from its token representation.\nGiven a dataset $D = {(X_i, Y_i)}_1^N$, where $X_i$ is the text transcription and $Y_i$ is the corresponding audio, we represent each pair $(X_i, Y_i)$ as a token sequence $(x_1,..., x_T, y_1,...,y_S)$. Our Transformer, with parameters $\\&$, learns the joint distribution of text and speech tokens via\n$P(x_1,..., x_T, y_1,\u2026\u2026\u2026,y_S) =P(x_1,...,x_T)\u00b7P(y_1,\u2026\u2026\u2026,y_S|x_1,...,x_T)$\nSince the text tokens {x1,...,xT} are always given as input during training and inference, the model focuses on learning the conditional probability:\n$P(y_1,..., y_S | x_1,...,x_T) = \\prod_{s=1}^S P(y_s | x_1,..., x_T, y_1,\uff65\uff65\uff65, y_{s-1})$\nTherefore, the loss is calculated over the speech tokens {y1,...,ys}. The objective is to minimize the negative log-likelihood:\n$L = -\\sum_{s=1}^S log P(y_s | x_1,..., x_T, y_1,..., y_{s-1})$"}, {"title": "2.2. Speech Tokenizer", "content": "As highlighted by AudioLM (Borsos et al., 2023), discrete speech representations can be categorized into both semantic tokens and acoustic tokens. Language modeling on semantic tokens excels at capturing high-level information such as content and emotion, while modeling with acoustic tokens focuses on low-level details, including timbre and other acoustic characteristics. Our X-codec2 tokenizer builds on the concepts from prior work X-codec (Ye et al., 2024). We fuse these semantic and acoustic features into a unified codebook but introduce a crucial modification: replacing residual vector quantization with a single vector quantizer to ensure 1D causal dependency. This design naturally aligns with the left-to-right autoregressive mechanism of LLMs and also reflects the inherently left-to-right temporal structure of audio signals.\nOur X-codec2 consists of three main components: the Encoder, the VQ module, and the Decoder.\nEncoder Given a raw speech waveform Y, we employ two separate encoders to derive its semantic and acoustic representations:\n\u2022 Semantic Encoder $Enc_s$: We adopt a pre-trained Wav2Vec2-BERT (Barrault et al., 2023) to obtain multilingual speech semantic features that capture content and emotional cues.\n\u2022 Acoustic Encoder $Enc_a$: Following the design in Xin et al. (2024) and Kumar et al. (2024), this module uses multiple residual convolutional blocks with Snake activation functions to encode low-level acoustic details.\nWe then concatenate the two outputs to form a fused feature embedding,\n$H = [Enc_s(Y), Enc_a(Y)]$,\nwhich serves as the input to our vector quantizer.\nVector Quantization To obtain discrete tokens, we apply FSQ(\u00b7) (Mentzer et al., 2024) to H:\n$H_q = FSQ(H)$,\nwhere $H_q$ is the quantized feature. We adopt FSQ due to its stability in training and high codebook usage efficiency. Notably, FSQ does not require an explicit VQ objective term (e.g., codebook commitment loss), simplifying optimization during training."}, {"title": "Decoder", "content": "From the quantized representation $H_q$, we aim to reconstruct both semantic and acoustic information:\n\u2022 Semantic Reconstruction: We follow Ye et al. (2024) and employ a semantic decoder to predict semantic features, using an l2 loss for reconstruction. It is worth noting that during inference, predicting semantic features is unnecessary; this component is designed to provide a supervisory signal to ensure the codebook retains sufficient semantic information.\n\u2022 Acoustic Reconstruction: Following Vocos (Siuzdak), we replace the ConvNeXt backbone with a Transformer-based decoder that predicts the short-time Fourier transform (STFT) magnitude and phase; an inverse STFT (iSTFT) head then converts the predicted spectra back to time-domain waveforms.\nCodec Training The training process closely follows that of X-Codec, simultaneously optimizing both semantic and acoustic reconstruction. We incorporate a multi-period discriminator (MPD) (Kong et al., 2020), a multi-scale STFT (MS-STFT) discriminator, and a spectral discriminator, with FFT sizes {78, 126, 206, 334, 542, 876, 1418, 2296} (Anonymous, 2025), for adversarial training. Additionally, following (Anonymous, 2025), we incorporate a perceptual loss during the final steps of the training process to further enhance intelligibility."}, {"title": "2.3. Scaling Train-time Compute", "content": "Our primary goal in this section is not to locate a compute-optimal configuration for TTS, but rather to show that, akin to text-based LLMs, increasing train-time resources (either by enlarging the model or expanding the training dataset) consistently improves performance. Specifically, we investigate two scaling strategies:\n\u2022 Fix Training Data, Vary Model Size. We fix the training data at 250k hours and scale the size of the Transformer $\\&$. Concretely, we adopt Llama 3.2 with 1B and 3B parameters, as well as Llama 3.1 with 8B parameters, which we denote as Llasa-1B-250k, Llasa-3B-250k, and Llasa-8B-250k, respectively, to observe how increased model capacity influences TTS quality.\n\u2022 Fix Model Size, Vary Training Data. We choose an LLM initialized from Llama 3.2 1B and train on three progressively larger subsets of our dataset D, containing 80k, 160k, and 250k hours of speech, respectively. Notably, the 80k and 160k subsets are randomly sampled as 1/3 and 2/3 partitions of the full 250k dataset, which we denote as Llasa-1B-80k, Llasa-1B-160k, and Llasa-1B-250k (identical to the previously mentioned Llasa-1B-250k model)."}, {"title": "We evaluate these on two aspects:", "content": "Text Understanding Ability A longstanding challenge in Text-to-Speech (TTS) technology is that TTS systems often fail to fully comprehend the meaning of text as humans do, which leads to mechanical pronunciation, lack of emotion, unnatural pauses, and difficulties in distinguishing homographs. Following BASE TTS (\u0141ajszczak et al., 2024), we use seven categories of texts-Questions, Emotions, Compound Nouns, Complex Syntax, Foreign Words, Paralinguistics, and Punctuation for English evaluation. Additionally, we propose seven categories tailored for Chinese: Questions, Emotions, Paralinguistics, Chinese Poetry, Rare Characters, Polyphonic Words, and Tongue Twisters (details are provided in Appendix A). In each case, the TTS system must exhibit a deeper textual understanding to produce natural and context-appropriate speech (e.g., correct pronunciation for polyphonic words and more expressive speech for emotional content). By examining the synthesized audio for each category, we measure how increased training data or parameter count benefits the system's text understanding ability.\nIn-context Learning Ability We also evaluate the model's zero-shot TTS capabilities (Wang et al., 2023)-whether it can produce intelligible, high-quality speech for speakers unseen during training. This aligns with prior zero-shot TTS protocols, which typically assess how well a model generalizes to new speaker identities, timbres, and emotional expressions with no additional fine-tuning."}, {"title": "2.4. Scaling Inference-time Compute", "content": "Recent research has begun exploring the scaling behavior of LLMs during inference, showing that additional computational resources often in the form of sophisticated search strategies can further enhance performance. Concretely, such approaches adjust the model's output distribution at test time by generating multiple candidate outputs from a baseline model and then applying post-hoc filtering and refinement via verifiers or scoring mechanisms, thereby elevating the quality of the generated content. When extending this concept to text-to-speech (TTS), we hypothesize that generating multiple speech candidates and performing a targeted search among them can yield outputs that more closely match the task requirements. In line with prior work (Snell et al., 2024; Ma et al., 2025), our search framework centers on two fundamental design choices:\nVerifier Selection For TTS, many off-the-shelf speech understanding models can serve as verifiers (or reward models) to evaluate synthesized audio from multiple perspectives. These include speaker verification models for measuring timbre similarity, emotion representation models (Ma et al.,"}, {"title": "Algorithms", "content": "We categorize the two different reward methods as follows:\n\u2022 Output Reward Models (ORMs):These models assess the speech segment only after it has been fully generated, evaluating it holistically. ORMs typically follow a simpler design but may be less efficient due to the absence of intermediate guidance signals. A common search strategy based on ORMs is the Best-of-N approach, where multiple candidate outputs are generated, scored using a reward model, and the highest-scoring output is selected.\n\u2022 Process Reward Models (PRMs): These models evaluate the generation process step by step, optimizing at each incremental stage (e.g., every second in a 10-second clip). Unlike conventional reward models that produce a single score for the final output, PRMS provide a sequence of scores, allowing for more fine-grained feedback throughout the generation process. While this approach enables detailed control, it also increases the risk of overfitting to intermediate rewards and of converging to local optima. The most common search algorithm leveraging PRMs is beam search, which systematically explores the solution space while optimizing both the sampling and evaluation of intermediate steps.\nIn our experiments, we explore both PRMs and ORMs to analyze how different search strategies impact the final quality of synthesized speech. A more detailed discussion of these methods and their outcomes is provided in the next experiments section."}, {"title": "3. Experiments", "content": "In this section, first, we compare our proposed speech tokenizer with existing codecs to assess its effectiveness. Second, we evaluate the performance of our TTS systems. We explore the effect of scaling both train-time and inference-time compute and compare our models against other baseline TTS systems. Lastly, we evaluate the extensibility of our framework in Appendix 4, particularly its applicability to speech understanding tasks, highlighting its versatility and potential for broader applications"}, {"title": "3.1. Codec Experiments", "content": "We train our codec model on a corpus of approximately 150k hours of multilingual speech, drawn from the Emilia dataset (En/Zh/De/Fr/Ja/Ko) (He et al., 2024) and MLS (En/Fr/De/Nl/Es/It/Pt/Pl)(Pratap et al., 2020a). All audio is sampled at 16kHz. We set the total downsampling ratio R to 320, use a codebook size of 65536, and employ a projection dimension of 8 in our VQ module. During training, we randomly crop 6-second segments from the audio. The learning rate is 1 \u00d7 10\u20134, preceded by a 3000-step warmup. In total, we train for 1.4 million steps, we activate perceptual loss at the final 0.2 million steps.\nFor evaluation, we use the test-clean subset of LibriSpeech (Panayotov et al., 2015), which contains 2620 utterances at 16 kHz.\nWe evaluate our system using several metrics. a HuBERT-based ASR system for Word Error Rate (WER) 1. Short-Time Objective Intelligibility (STOI). Perceptual Evaluation of Speech Quality (PESQ). A WavLM-based speaker verification model for speaker similarity (SPK SIM) 2, and UTMOS 3.\nWe compare our codec against multiple baselines, including DAC (Kumar et al., 2024), SpeechTokenizer (Zhang et al., 2024), BigCodec (Xin et al., 2024), StableCodec (Parker et al., 2024), SemantiCodec (Liu et al., 2024), X-codec (Ye et al., 2024), Mini (D\u00e9fossez et al., 2024), EnCodec (D\u00e9fossez et al., 2022), and WavTokenizer (Ji et al., 2024). All baseline results are obtained using their official checkpoints.\nAs shown in Table 1, X-codec2 achieves the best performance at a token rate of 50 for most metrics. Moreover, its UTMOS score closely matches that of the ground truth, indicating that the reconstructed audio faithfully preserves the original speech quality. We also observe that certain models exceed the ground truth in UTMOS when operating at low token rates. We suspect this occurs because, under limited token constraints, the decoder behaves partly as a generative model-yielding plausible speech output but the alignment with the input was less precise. Additionally, we found that metrics such as WER at a low token rate can achieve good results by integrating speech semantic information, as demonstrated by models like Mini, X-codec, and SpeechTokenizer. Another important observation is that the acoustic reconstruction capability of codecs at low token rates remains relatively limited. For instance, DAC operating at 600 tokens achieves a SPK SIM of 0.95 and a PESQ score exceeding 4. In contrast, current codecs at lower token rates attain SPK SIM values below 0.85 and PESQ scores around 3. However, compared to earlier models like DAC"}, {"title": "3.2. TTS experiments", "content": "All TTS models are trained for 3 epochs with a batch size of 2 million tokens and a maximum learning rate of 5e-5. We employ a cosine learning rate schedule with a warmup phase covering 3% of an epoch, and the final learning rate is set to 10% of the peak learning rate. During training, text sequences are tokenized and placed on the left, followed by tokenized speech sequences on the right, forming a concatenated sequence that is then cropped to a maximum length of 2048 tokens.\nOur training dataset integrates multiple high-quality speech datasets, including Libriheavy (Kang et al., 2024), the Chinese-English subset from the Emilia corpus (He et al., 2024), WenetSpeech4TTS (Ma et al., 2024), and our internal data. This diverse collection constitutes a large-scale training corpus of 250,000 hours, comprising mixed Mandarin Chinese and English speech data. All textual content maintains its original punctuation.\nOur TTS systems are evaluated through a series of comprehensive experiments designed to assess various aspects of performance.\nSimilar to (\u0141ajszczak et al., 2024), we evaluated the models' text understanding capabilities by focusing on their ability to accurately comprehend and synthesize speech from complex textual inputs. This assessment aimed to evaluate the text understanding abilities of TTS systems. The full testset is in Appendix A, each sentence was synthesized five times by each model. Due to the absence of speech prompts, timbre and style were generated randomly. The evaluation criteria are also in Appendix A; a linguistic expert rated the TTS outputs using a discrete 3-point scale, and we calculated the average score for each category.\nTo assess the in-context learning capability of our model, we conducted experiments on three test sets: Seed-TTS-Eval, LibriSpeech test-clean, and the ESD dataset.\nThe first two datasets primarily evaluate the model's ability to clone the voice of an unseen speaker given a short speech clip, focusing on speaker similarity. For both, speaker similarity (SIM) and Word Error Rate (WER) are used as key evaluation metrics:\nconsists of three subsets: test-zh, test-en, and test-hard. These experiments focus on cross-sentence speaker similarity and the generation of intelligible speech. LibriSpeech test-clean (Panayotov et al., 2015) is a widely used benchmark for English zero-shot TTS evaluation (Wang et al., 2023), providing a standardized setting to assess the model's ability to generate natural and intelligible speech from unseen speakers.\nThe third test set, ESD (Emotional Speech Dataset)(Zhou et al., 2021; zho, 2022), evaluates the model's ability to clone emotions in speech. This dataset consists of 10 native English speakers and 10 native Chinese speakers, covering five emotion categories: neutral, happy, angry, sad and surprised. For reproducibility, we selected the longest utterance from each speaker as the prompt and the second longest as the ground truth, resulting in a total of 100 evaluation samples (50 English, 50 Chinese). We used  to measure emotional similarity.\nThrough these evaluations, we aimed to provide a comprehensive assessment of our TTS models, ensuring their effectiveness in speaker identity preservation, intelligibility, and emotional expressiveness across diverse linguistic and contextual challenges."}, {"title": "3.2.2. SCALING TRAIN-TIME COMPUTE", "content": "Figure 1 presents expert scores for Chinese TTS tasks, including Emotion, Paralinguistics, Poetry, Polyphonic Characters, Tongue Twisters, Questions, and Rare Characters. Increasing both model size (1B \u2192 3B \u2192 8B) and training data (80k \u2192 160k \u2192 250k hours) generally improves performance. Specifically, simpler tasks like Questions already achieve strong results, with only marginal gains from further scaling. In contrast, larger models (e.g., 8B parameters) yield significant improvements in emotional speech, Chinese poetry, and tongue twisters, where deeper semantic comprehension is essential. Meanwhile, rare characters benefit most from broader data coverage, as increasing model size alone has little effect. We also evaluate our models on English TTS tasks-Compound Nouns, Emotions, Foreign Words, Paralinguistics, Punctuations, Questions, and Syntactic Complexity. As with Chinese, scaling up both the model size (1B \u2192 3B \u2192 8B) and training data (80k \u2192 160k \u2192 250k hours) generally yields higher expert scores. We observe that both Compound Nouns and Foreign Words primarily benefit from increased training data rather than model scaling, suggesting that a wider variety of examples is necessary for correct pronunciation and lexical coverage."}, {"title": "In-context Learning Ability", "content": "Based on Table 2, Table 4, and Table 3, we observe that as the model size and training data increase, the metrics for speaker similarity, word error rate, and emotional similarity consistently improve, reflecting the enhancement of in-context learning ability."}, {"title": "3.2.3. SCALING INFERENCE-TIME COMPUTE", "content": "In this section, we conduct a series of experiments to explore how increasing inference-time compute affects the performance of different strategies, such as PRM and ORM. All experiments are conducted using the Llasa-1B-250K model and evaluated on seed-tts-eval test-hard testset. We begin by evaluating zero-shot TTS performance using SIM-O and WER metrics, with direct inference serving as the baseline. Our initial goal is to improve SIM, utilizing two key strategies: PRM with beam search and ORM with best-of-N. The similarity verifier is a WavLM-finetuned speaker verification model.\nFor beam search, we generate B candidate beams conditioned on the speech prompt, where each beam is expanded by M tokens per step. We set M = 25, corresponding to 0.5 seconds in our experiments. Each beam then expands into N = 16 new candidate sequences. From the resulting pool of B \u00d7 N sequences, we select the top B based on their similarity scores. This process repeats until an end-of-sequence (EOS) token is generated or the sequence length reaches 2048.\nFor best-of-N, we generate multiple independent responses and select the one with the highest similarity score as the final output. To match the compute budget of beam search, we also produce B \u00d7 N candidates.\nAs shown in Figure 2, our simplest method for improving SIM is best-of-N. The orange line indicates that as inference-time compute grows, SIM improves markedly. Next, the blue line shows that PRM beam search outperforms ORM under the same compute budget.\nHowever, when we also aim to optimize WER (using Whisper Large v3 as a verifier), we select the lowest-WER candidate at the final PRM step from the B \u00d7 N sequences. The red line reveals that WER remains poor, especially for larger beam widths, sometimes lagging behind the baseline. We suspect that maximizing SIM through PRM leads to locally optimal speech with inadequate diversity. To address this, we propose a partial PRM strategy, applying PRM only during the first n seconds, then switching to ORM. Here, n = 2 in our experiments. This hybrid approach (the green line) achieves higher SIM than best-of-N while maintaining WER near ground truth, indicating sufficient diversity. Finally, substituting the later ORM step with a WER-based verifier (the purple line) simultaneously boosts both SIM and WER as inference-time compute increases, demonstrating that this mixed strategy strikes an effective balance between speaker similarity and transcription accuracy.\nFrom another perspective, we also compare the impact of inference-time scaling across different model sizes and test sets. In these evaluations, we fix the beam width at 16. As shown in Tables 3, 4, and 2, our results show that inference-time scaling not only improves speaker similarity but also enhances emotion similarity. Additionally, we generally observe that larger models benefit more from inference-time scaling. However, for certain relatively simple tasks and metrics, the performance gap between large and small models"}, {"title": "3.2.4. COMPARE WITH BASELINE MODEL", "content": "In the previous sections, we analyzed our codec and explored the impact of scaling train-time and inference-time compute on the performance of TTS systems. In this section, we directly compare our model against other TTS baselines.\nFor Seed-TTS-Eval, we select the following baseline models: Seed-TTS (Anastassiou et al., 2024), MaskGCT (Wang et al., 2024), E2-TTS (32 NFE)\u2020 (Eskimez et al., 2024), F5-TTS (32 NFE) (Chen et al., 2024b), CosyVoice (Du et al., 2024a), CosyVoice 2 (Du et al., 2024b), and FireRedTTS (Guo et al., 2024). Our results are taken from the original papers whenever available; otherwise, they are sourced from CosyVoice 2.\nThe results, as shown in Table 2, indicate that for direct inference, our model achieves WER performance comparable to these state-of-the-art (SOTA) models. However, one notable limitation of our approach is SIM-O. As discussed earlier, the reconstruction capability of single-token codecs remains constrained compared to mel-based vocoder reconstructions or RVQ codec-based reconstructions used in the baselines.\nFor LibriSpeech test-clean, we compare against the following baselines: ELLA-V (Song et al., 2024), VALL-E R (Han et al., 2024), CLaM-TTS (Kim et al.), VALL-E (Wang et al., 2023), VALL-E 2 (Chen et al., 2024a), Voicebox (Le et al., 2024), and MELLE (Meng et al., 2024b).\nAs shown in Table 3, we observe similar trends in continuous TTS for LibriSpeech test-clean. However, an interesting finding is that our model achieves a high SIM-r score, particularly on LibriSpeech test-clean, where our best SIM-R (0.626) is already very close to the ground truth (GT) codec resynthesis (0.638). Given that the continuous generation task is fully aligned with the autoregressive training paradigm, this suggests that, from a generative modeling perspective, a single Transformer-based architecture does not inherently suffer disadvantages from metric Sim-r and WER compared to carefully designed AR+NAR hybrid architectures. The only drawback of Sim-o may arise at the system level, particularly in the final step of codec acoustic reconstruction, where converting the intermediate representation back into a waveform may introduce limitations due to single VQ as mentioned in Sec 3.1.2.\nFrom an inference-time scaling perspective, our approach outperforms all baselines. While this comparison might not be entirely fair-as our inference process utilizes more computational resources\u2014it presents an alternative viewpoint: if the goal is to achieve the best possible quality, disregarding computational constraints, inference-time scaling provides a viable solution. Notably, as shown in table 2 our model achieves a WER of 3.12 on the test-hard of Seed-TTS-Eval, demonstrating that allocating more compute at inference time is particularly beneficial for synthesizing challenging speech, effectively addressing cases where previous models have struggled."}, {"title": "4. Extending to Speech Understanding Tasks", "content": "While we have primarily demonstrated the viability of our single Transformer + tokenizer approach for TTS, we also explored its performance on speech understanding task, in particular, ASR. The only modification is to swap the position of speech and text tokens: speech tokens come first, followed by text tokens, and during training we apply the cross-entropy loss solely to the text tokens. We use the same tokenizer X-codec2. ASR models are trained on Libriheavy (Kang et al., 2024), MLS English (Pratap et al.,"}, {"title": "6. Conclusion", "content": "This paper presents Llasa, a scalable TTS system that aligns with text LLM architectures, using a single Transformer and a tokenizer. We systematically explore train-time and inference-time compute scaling, showing that larger models and datasets improve speech naturalness, prosody, and text comprehension. Additionally, inference-time scaling, leveraging speech understanding models as verifiers, enhances speaker similarity, emotional expressiveness, and content accuracy. Our experiments confirm state-of-the-art performance with strong zero-shot TTS capabilities. We release our models publicly to drive further research."}]}