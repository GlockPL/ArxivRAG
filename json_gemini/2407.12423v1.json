{"title": "StuGPTViz: A Visual Analytics Approach to Understand Student-ChatGPT Interactions", "authors": ["Zixin Chen", "Jiachen Wang", "Meng Xia", "Kento Shigyo", "Dingdong Liu", "Rong Zhang", "Huamin Qu"], "abstract": "The integration of Large Language Models (LLMs), especially ChatGPT, into education is poised to revolutionize students' learning experiences by introducing innovative conversational learning methodologies. To empower students to fully leverage the capabilities of ChatGPT in educational scenarios, understanding students' interaction patterns with ChatGPT is crucial for instructors. However, this endeavor is challenging due to the absence of datasets focused on student-ChatGPT conversations and the complexities in identifying and analyzing the evolutional interaction patterns within conversations. To address these challenges, we collected conversational data from 48 students interacting with ChatGPT in a master's level data visualization course over one semester. We then developed a coding scheme, grounded in the literature on cognitive levels and thematic analysis, to categorize students' interaction patterns with ChatGPT. Furthermore, we present a visual analytics system, StuGPTViz, that tracks and compares temporal patterns in student prompts and the quality of ChatGPT's responses at multiple scales, revealing significant pedagogical insights for instructors. We validated the system's effectiveness through expert interviews with six data visualization instructors and three case studies. The results confirmed StuGPTViz's capacity to enhance educators' insights into the pedagogical value of ChatGPT. We also discussed the potential research opportunities of applying visual analytics in education and developing AI-driven personalized learning solutions.", "sections": [{"title": "1 INTRODUCTION", "content": "Groundbreaking developments in generative AI, particularly through Large Language Models (LLMs) applications such as ChatGPT, have introduced unprecedented opportunities in educational methodologies. These tools not only expedite students' information searches but also assist instructors in refining classroom activities and delivering personalized guidance. However, as the integration of LLMs into educational scenarios is still nascent, it is imperative for instructors to carefully plan and assess how students utilize LLMs in their learning activities to harness the full potential of LLMs and enhance the student learning experience. A fundamental step in this process is to gain a comprehensive understanding of student interactions with LLMs, thereby acquiring key pedagogical insights such as students' cognitive levels, learning attitudes, and mastery of knowledge. Nevertheless, efforts to provide instructors with these pedagogical insights are still in the initial stages. Existing research on LLMs in education primarily focuses on educational ethics and potential application scenarios, such as using LLMs as automated tools for evaluating student essays. To our knowledge, in-depth studies on students' interactions with LLMs for learning tasks are scarce, facing two main challenges. First, there are no publicly available datasets dedicated to capturing students' conversations with LLMs. The existing datasets are primarily composed of everyday conversations with general users. While some include conversations about learning tasks like programming or solving math problems, they are few and lack guaranteed quality because the learning scenarios involving LLM use are not carefully crafted and evaluated by instructors. Moreover, these conversations typically follow a \"one question, one answer\" format, as their primary purpose is to assess LLMs' problem-solving capabilities. These deficiencies highlight the urgent need for data collection of learning-centered conversations with well-structured tasks. Second, understanding how students interact with LLMs for pedagogical insights through conversation data presents significant challenges. One major difficulty is that instructors are eager to understand the extent of higher-order thinking (e.g., independent thinking) students engage in when using these advanced AI tools. To comprehend this higher-order thinking, it is essential to measure students' cognitive levels. However, accurately interpreting these cognitive levels based on students' inquiries to LLMs has never been explored. Additionally, assessing students' proficiency in utilizing LLMs poses another challenge, which involves evaluating the various LLMs' responses and observing how students adjust their prompts in response. Moreover, tracking the progression of these interactions introduces an added layer of complexity. Visual analysis is a potential way. However, current research has often overlooked these challenges. While there are many visualizations works on conversation analysis studies focusing on topic progression or sentiment analysis, such studies do not adequately capture and visualize the cognitive levels reflected in the evolving interactions, falling short of meeting instructors' needs. To address these challenges, we selected ChatGPT, notable for being one of the most prevalent LLM applications, to gather conversation data. In addition, we see a potential to introduce LLMs for visualization education to address diverse student backgrounds and manage varied learning activities such as concept comprehension, visualization literacy, and design. In collaboration with experienced course instructors, We devised and integrated an in-class exercise module into a graduate-level data visualization course at the local university, allowing students to interact with ChatGPT freely. Our approach yielded a significant collection of high-quality student-ChatGPT conversation data from well-crafted learning tasks. Through comprehensive thematic analysis and an extensive literature review, we developed a coding scheme that categorizes the diverse cognitive levels and several metrics to evaluate the quality of ChatGPT responses. To capture evolving strategies students use when interacting with ChatGPT, we analyzed various sequences and sets of the codes, defining these as \"interaction patterns\" which became the focal point of our analysis. Building on this foundation and design requirements derived from course instructors and experts, we introduce a pioneering visual analytics system for instructors to explore intricate interaction patterns and derive actionable pedagogical insights from student-ChatGPT conversation data. In particular, a customized tree visualization is designed to present the evolution and compare the characteristics of students' interaction patterns. To summarize, our key contributions are as follows:\n\u2022 We introduced ChatGPT to a real data visualization course, collected student-ChatGPT conversation data and developed a coding scheme for an in-depth analysis of interaction patterns.\n\u2022 We designed a visual analytics system, StuGPTViz, to help instructors discover insights into students' cognitive levels and proficiency when using ChatGPT.\n\u2022 Through three case studies and expert interviews, we demonstrated the effectiveness of our coding scheme and system in enhancing educational activities such as problem-solving guidance, personalized feedback, and exercise design.\nOverall, we present a design study that constitutes an initial yet crucial step toward analyzing student interaction patterns with ChatGPT, advancing the application of visual analytics in AI-driven education."}, {"title": "2 RELATED WORK", "content": "In this section, we discuss the relevant research, including LLMs in education and visualization education, visualization for AI-enhanced education, and visual analytics for conversational data."}, {"title": "2.1 LLMs in Education and Visualization Education", "content": "The integration of LLMs such as ChatGPT into educational settings has sparked a diverse range of discussions, with plenty of initial works centered on ethical considerations regarding their use in learning environments. Increasingly, the academic community recognizes the transformative potential LLMs hold for education, advocating for their adoption to revolutionize learning and teaching methodologies. Despite potential resistance from some educators, students inevitably turn to LLMs for assistance with coursework. Therefore, researchers and instructors have explored LLMs for various applications, including serving as teaching assistants for writing and coding, generating adaptive exercises, supporting personalized question-answering sessions, and facilitating innovative learning modes like \"learn by teaching\", where LLM plays the role as \"learner\" and students assume the role of the teacher to teach the AI. However, there is a notable gap in understanding how students strategize their use of ChatGPT for educational purposes. Existing literature predominantly focuses on the capabilities and applications of LLMs without delving into student interaction strategies, leaving educators without the necessary insights to fully leverage these tools in enhancing learning experiences. Simultaneously, the field of visualization education is gaining traction, not only within the visualization community but also more broadly. The challenges of teaching data visualization range from addressing diverse student backgrounds to managing varied learning activities such as concept comprehension, visualization literacy, and design evaluation-are substantial. ChatGPT's potential to support these educational challenges opens avenues to investigate how students use ChatGPT across different visualization learning tasks, particularly relevant to our project's focus. A comprehensive analysis of this research direction still remains unexplored. This gap presents a unique opportunity for our work to contribute to the field by offering insights into student strategies in employing ChatGPT within visualization learning contexts, thereby advancing the understanding and application of LLMs in educational settings."}, {"title": "2.2 Visualization for AI-Enhanced Education", "content": "The integration of visualization in AI-enhanced education is dedicated to leveraging visual analytics for learning analysis, such as interpreting complex data and AI algorithms to improve educational outcomes. While learning analysis harnesses data to refine and enhance learning processes, visualization techniques render these insights accessible and actionable for educators and students. Despite notable advancements in each domain, the integration of visual analytics specifically tailored to learning analysis within AI-enhanced educational environments remains underexplored.\nExisting research underscores the value of visual analytics in presenting student performance metrics, engagement levels, and learning behaviors, thus enriching our understanding of educational dynamics. Within this context, the subfield of open learner models exemplifies the potential of visual explanations, akin to Explainable AI (XAI), in demystifying AI-generated outputs, offering learners and educators transparent and trustworthy insights. Additionally, other works have employed visual analytics to examine students' interaction data with intelligent agents, using students' log data such as hint requests to delve into their problem-solving processes. Recently, with the advent of potent LLM-based conversational agents, the intricacy of interactions between students and AI has reached a new height. Goals once considered unrealistic, such as in-depth analysis of students' cognitive levels and thought processes, are now achievable. To our knowledge, the dedicated exploration of visual analytics to analyze and elucidate student interactions with advanced AI tools, such as ChatGPT, is just beginning. In response to this gap, our work proposes a novel visual analytics system based on the students-ChatGPT conversation data we collected. The system is designed to identify and unravel the intricate nuances of student-ChatGPT interactions. It equips educators with profound insights into how LLMs can be utilized to customize and elevate students' learning experiences."}, {"title": "2.3 Visual Analytics for Conversational Data", "content": "The exploration of visual analytics for conversational text data within the visualization community has encompassed a wide range of applications, from sentiment analysis and topic modeling to mapping conversation flows and interactions within user groups. For instance, T-Cal and IneqDetect focus on analyzing group conversations and estimating members' sentiments to assess collaboration effectiveness. Meanwhile, efforts such as ThreadReconstructor, MultiConVis, and VisOHC probe into the structure and core topics of online forum discussions. However, these initiatives mainly focus on summarizing the dynamics of multi-party conversations without delving into the intricacies of one-on-one dialogues.\nAnother significant gap in current methodologies is their constrained ability to uncover the depth of evolving cognitive levels in educational dialogues between students and AI tools like ChatGPT. Visualization tools for one-on-one medical conversations, like ConVIScope and Discursis, focus on charting patient-doctor dialogues but only reflect changes in sentiment and topic over time. They fall short in showcasing how one party (e.g., students) adjusts their responses to another party (e.g., LLMs' replies). Similarly, research aimed at analyzing educational dialogues often emphasizes engagement and comprehension, lacking a detailed visual analysis of the depth of thinking, learning strategies, or intentions revealed through these interactions.\nThese shortcomings highlight the necessity for a novel visual analytics framework designed to tackle the specific challenges posed by educational dialogues with LLMs. Consequently, our work introduces multiple visualizations, such as the Interaction Tree, to meticulously analyze students' interaction patterns with ChatGPT."}, {"title": "3 BACKGROUND AND DATA COLLECTION", "content": "This section outlines the background of our data visualization course, the collaborative efforts with our expert team, the in-class exercises, and the data collection considerations and procedures we adopted."}, {"title": "3.1 Data Collection Background and Considerations", "content": "At the invitation of our experts (two course instructors, E1&E2), we set out to incorporate ChatGPT into the curriculum of a postgraduate data visualization course for computer science majors in the first semester of 2024. This course, meeting once a week for three hours, attracted 55 registrants. Over the past four months, we have collaborated closely with E1, E2, and three teaching assistants (TA1-TA3) at our university. E1 is a professor with over 15 years of experience designing and teaching data visualization courses. E2 is a lecturer with three years of teaching experience and served as the primary instructor for this course. TA1, TA2, and TA3, are senior teaching assistants who have supported the professors in designing the in-class exercises, homework, and exams for the data visualization course for at least two semesters. All experts have used ChatGPT extensively over the last two years. In preparation, we conducted a three-hour meeting to outline how ChatGPT would be integrated into the course and how data would be collected. To maximize the benefits of ChatGPT for students, we decided to introduce an in-class exercise section. This would allow students to apply what they learned by interacting with ChatGPT and create an ideal setting for collecting diverse data on their learning interactions. During our discussion, two key considerations emerged:\nC1: Diverse Learning Tasks for Comprehensive Data Collection. To capture the full spectrum of student interactions with ChatGPT, exercises should be designed across various course aspects using diverse learning tasks (e.g., visualization understanding & design). These learning tasks should encompass different cognitive levels as outlined in Bloom's Taxonomy to ensure that the collected data fully reflects students' diverse engagement with ChatGPT. While these tasks are exemplified using a data visualization course, learning tasks for any course can be similarly designed to align with Bloom's Taxonomy.\nC2: Natural Learning Scenarios for Unbiased Data Collection. To accurately reflect genuine students' behaviors, it was vital to integrate ChatGPT into the learning process as an optional tool rather than a course mandate. This approach was intended to gather authentic interaction data, showcasing real student needs and preferences in using ChatGPT for learning.\nInformed by these considerations, we included a 40-minute open ChatGPT session at the end of each class (9 classes in total) to promote active student engagement with ChatGPT through various tasks. Additionally, we carried out a pilot study with about 30 HCI and data visualization Ph.D. students to evaluate the exercise section's design and procedures. Their conversation data helped us ascertain the appropriateness of our data collection approach. The data collection process was also approved by the university's ethics committee (IRB approval). The subsequent sections will provide an overview of our task designs for the in-class exercises and the data collection procedures."}, {"title": "3.2 Tasks Summary and In-class Exercise Procedure", "content": "Collaborating with instructors E1 and E2, we developed 27 exercise tasks spread across seven distinct types according to the levels of cognitive learning in the Revised Bloom's Taxonomy (C1, Fig. 2):\nTo begin data collection, we distributed a questionnaire to the 55 enrolled students to gauge their willingness to participate and gather background information. Forty-eight students consented, providing details on their undergraduate majors, data visualization expertise, programming skills, and prior ChatGPT experience. We also hosted a 40-minute introductory session on ChatGPT, including instructions on exporting and uploading conversation files to Canvas system\u00b9, the web-based learning management system used by our university.\nEach in-class exercise session, conducted during the last 40 minutes of the lecture, included a 10-minute self-learning segment with ChatGPT, a 25-minute task completion segment, and a 5-minute conversation log upload phase. Students first spent 10 minutes asking ChatGPT questions to learn key lecture terms. This was followed by 25 minutes of task completion (Fig. 2) with ChatGPT's assistance and a five-minute interval for uploading conversations to Canvas. Students were encouraged to interact naturally with ChatGPT (C2), and those confident with the course material were free skip the self-exploration."}, {"title": "3.3 Dataset Brief", "content": "The dataset we collected involved 48 students' conversation data with ChatGPT during the in-class exercise session of data visualization course over the entire spring 2024 semester. It consists of 744 unique conversations with 2507 turns after filtering out the empty conversations and those unrelated to the learning tasks. To the best of our knowledge, it is the only existing dataset specifically for student-ChatGPT conversations under strictly-defined learning activities. The collected data is in a structured format, including metadata such as student names (anonymized), task ID and task types, and conversation content. Each conversation is logged in sequential order, capturing both student prompts and ChatGPT responses. Additionally, we collected students demographic information, including their background in computer science, data visualization, and ChatGPT usage experience."}, {"title": "4 DESIGN REQUIREMENT AND DATA PROCESSING", "content": "This section presents the design requirements for the visual analytics system identified through discussions with our experts (E1 and E2), the procedure for coding student prompts, and the methodology for processing ChatGPT responses."}, {"title": "4.1 Visualization Design Requirements", "content": "We summarized experts' requirements for analyzing student-ChatGPT conversation data as follows:\nR1: Overview of students and tasks data. Experts highlighted the necessity of an overview of both students and the tasks, including the distribution of students' background information (e.g., knowledge in visualization) and tasks' characteristics (e.g., types). Instructors can then select specific students or tasks for deeper analysis of student-ChatGPT conversations.\nR2: Summarizing macro-level conversation characteristics. Before detailed analysis, experts require a comprehensive summary of the student-ChatGPT conversation, especially the cognitive levels of students' prompts and the qualities of ChatGPT responses based on selected tasks and students. This summary should span multiple user-selected viewpoints, covering broad categories such as various student groups and types of tasks, since instructors are always interested in the differences in behavior and performance among different student groups, such as those with and without a CS background. Additionally, instructors often have limited time and want to quickly see the differences among groups to prioritize their focus.\nR3: Identifying micro-level interaction patterns. Experts require a structured method to detect and summarize students' micro-level interaction patterns (i.e., recurring methods and strategies students employ when interacting with ChatGPT), along with essential metrics such as learning outcomes and pattern frequency. Emphasizing this information is crucial for instructors to identify which patterns are more effective for learning and merit deep exploration. They can further develop actionable insights on how students engage with ChatGPT throughout their learning journey.\nR4: Tracing interaction pattern evolution. Educators necessitate a method to trace the development of students' interaction patterns, emphasizing both the shared and unique sequences within the context of task-solving. This requirement involves visualizing how students' interaction patterns evolve in response to tasks, reflecting variations in cognitive engagement and problem-solving approaches. Such visualization should facilitate a deeper understanding of varied student approaches to learning tasks.\nR5: Evaluating interaction pattern performance. Experts wanted to evaluate interaction patterns of interest effectively. This involves identifying students who utilize the pattern and assessing relevant metrics such as their learning outcomes and ChatGPT's response quality. Such comprehensive analysis helps gauge the effectiveness of different interaction patterns, enabling instructors to provide targeted feedback and identify exemplary patterns as recommended learning strategies.\nR6: Examining detailed raw data. Experts expressed a desire to access the raw data, which includes original in-class activities, students' answers or responses, and students' conversation logs with ChatGPT.\nThese details can be used to justify their analysis results and provide straightforward examples for students to master effective interaction with ChatGPT."}, {"title": "4.2 Students' Prompts Coding", "content": "The open coding process for students' prompts is based on thematic analysis methodology and enriched by a literature review to identify prompt patterns. Following expert requirements (R2), we categorized codes into two types: learning-related codes reflecting students' cognitive levels regarding course materials, and ChatGPT-related codes denoting students' comprehension and proficiency in using ChatGPT as a supplementary tool.\nFirstly, we reviewed literature analyzing general user prompt patterns and intents, identifying 16 general prompt patterns as the initial ChatGPT-related codes. While these codes could represent students' proficiency with ChatGPT, they did not capture their cognitive levels. Therefore, we invited three visualization researchers, each with over four years of experience in data visualization education and thematic analysis, to independently conduct open coding of students' prompts. They refined the ChatGPT-related codes and developed learning-related codes that encapsulate the intent and thought processes behind each student's prompt. Our researchers engaged in repeated cross-checking and refinement until a consensus was reached, ultimately establishing 27 codes, including 15 learning-related and 12 ChatGPT-related codes.\nAfter finalizing the code space, we consulted with our experts (TA1-TA3) to further refine our codes. At TA3's suggestion, we used the revised Bloom's taxonomy to categorize the 15 learning-related codes, enhancing our understanding of students' learning intentions and mapping their cognitive processes. This taxonomy describes six stages of cognitive learning: remember, understand, apply, analyze, evaluate, and create, each representing an advancement in cognitive level. The authors independently categorized the learning-related codes, then discussed and refined any inconsistencies until agreement was reached.\nThe final code schema is summarized in Fig. 3. To ensure coding quality and consistency, the authors coded 30 conversations from the pilot experiment with the finalized labels and calculated the Inter-Rater Reliability (IRR) scores, resulting in an IRR of 0.84, validating the coding process's reliability. Finally, the authors coded all students' prompts. For prompts embodying multiple learning intents and strategies, we applied multiple codes. Additionally, we used the ordered sequence of codes and unordered sets of codes from each student's conversation data to illustrate the \"interaction patterns\" identified via expert requirements (R3). For each coded conversation, we mined all possible ordered code sequences (e.g., [Definition Inquiry, Follow up Question, ...]) and unordered code sets (e.g., {Definition Inquiry, Application Inquiry,...}) for further analysis."}, {"title": "4.3 Processing ChatGPT's Responses", "content": "To enhance our understanding of students' interactions with ChatGPT, we analyze and evaluate ChatGPT's response quality based on expert suggestions (TA1). Through our literature review, common metrics for evaluating ChatGPT's responses include response relevance, length, and correctness, all recognized as important by our experts. We employed the Ragas response relevance package, a renowned framework for evaluating large language models, to assign a numerical score to the relevance of each \"user prompt - LLM response\" pair. Additionally, we measured response length and, in collaboration with experts E2 and TA2, assessed response correctness by categorizing them into \"basically correct\" (score 1), \"partially correct\" (score 0.5), and \"basically wrong\" (score 0), respectively. Through further discussion, experts (E1 & E2) expressed interest in evaluating the amount of accurate information a student can acquire from each turn of interaction with ChatGPT. Consequently, we combine the two metrics, response relevance score and response correctness, to develop the new metric \"information gain\u201d inspired by the literature. The metric's formula is shown below, which primarily leverages the KL-divergence principle, calculates the amount of new information provided by the latest ChatGPT response compared to the existing set of responses:\n\\(IG(P,Q) = \\sum_{i} P(i) \\log(\\frac{R \\times C}{Q(i) / P(i)})\\)\nIn this formula, IG represents the information gain of the incoming response P under the cumulative response set Q. P(i) is calculated as the frequency of word i in the current response divided by the total number of words in that response. Q(i), on the other hand, is the cumulative frequency of word i up to the current response, divided by the total number of words in all responses up to that point. The variables R and C represent the numeric relevance score and correctness score, respectively. This computation quantifies the new information provided by ChatGPT's latest response compared to the existing knowledge base."}, {"title": "5 VISUALIZATION", "content": "Based on the design requirements identified in Sec. 4.1 and the data collected in Sec. 3, we developed a visual analytics system, StuGPTViz (Fig. 1), aimed at enabling instructors to analyze student interactions with ChatGPT effectively."}, {"title": "5.1 System Overview", "content": "StuGPTViz is intricately designed to facilitate a multi-level analysis of students' interactions with ChatGPT from the perspective of both tasks and students. It supports instructors in selecting specific tasks and students as focal points of analysis, thereby accommodating diverse analytical interests. Moreover, it enables a \"gradually deepening\" analysis process, allowing users to acquire both a broad overview and detailed insights into the interactions between students and ChatGPT. Specifically, StuGPTViz is structured into three main components:\nInitial selection: Instructors begin by selecting particular tasks or students of interest through the Task Overview and Student Overview in the Filter View (Fig. 1-a1, a2). This selection offers an overview of students and tasks data (R1), which also triggers updates in the Pattern Summary of the Pattern View (Fig. 1-b1).\nGradually deepening exploration: Starting from the Pattern Summary in the Pattern View (Fig. 1-b1), instructors can gain a summary of macro-level conversation characteristics of the selected students and tasks, covering both groups and individuals (R2). To delve deeper, instructors can further analyze the micro-level student-ChatGPT interaction patterns via Pattern Nuance. While the Pattern Mining Table (Fig. 1-b4) enable instructors to identify the pattern summary together with significant metrics like learning outcome (R3), the Interaction Tree traced the pattern evolution of each student (R4). Moreover, the interplay between these two visualizations enables instructors to explore and assess patterns of interest effectively (R5).\nDetailed inspection: Finally, the Task Description and Raw Conversation from the Detailed View (Fig. 1-c1, c2) provides access to the original tasks, students' responses, and their raw conversation logs with ChatGPT. This component allows instructors to examine task specifications, student prompts, and ChatGPT's replies, leveraging their expertise to interpret the data comprehensively (R6)."}, {"title": "5.2 Filter View", "content": "The Filter View (Fig. 1-A) serves as the gateway to analysis, presenting the distribution of various background metrics and enabling instructors to filter the students and tasks of interest (R1). The Task Overview (Fig. 1-a1) displays information about learning tasks and features a search box for quickly finding specific tasks by ID. The distribution of task difficulties and types, determined by experts (E1 & E2), is depicted through two bar charts, while an area chart illustrates the distribution of students' normalized average scores (x-axis) across tasks. Instructors can select tasks by clicking on the bars or adjusting sliders, with each metric chart dynamically updating in response to real-time user selections. Moreover, the Student Overview (Fig. 1-a2) provides insights into students' backgrounds. A search box allows quick location of students by aliases, and an area chart at the bottom visualizes the distribution of students' average scores. Positioned between them, three-segmented bar charts represent the distribution of students' prior experience in data visualization and computer science and their familiarity with ChatGPT, derived from a background survey conducted during the first class. By freely filtering each metric, instructors can effortlessly isolate students and tasks of interest (R1), then proceed to the Pattern View (Fig. 1-B)."}, {"title": "5.3 Pattern View", "content": "After identifying tasks and students of interest, instructors can delve into detailed analysis using the Pattern View (Fig. 1-B). Comprising the Pattern Summary and Pattern Nuance, the Pattern View enables a gradually deepening examination of interaction patterns, offering insights from macro-level summaries to micro-level details (R2, R3). Throughout the Pattern View, we consistently apply color scheme (Fig. 1, top-right corner) to represent the code categories defined in Fig. 3. Specifically, we use a sequential color scheme, transitioning from light yellow to dark brown, to denote increasing cognitive levels (from \"remember\" to \"create\") as demonstrated in students' prompts. Concurrently, a distinct color scheme (blue for effective prompt strategies from literature and green for others) signifies students' proficiency in using ChatGPT."}, {"title": "5.3.1 Pattern Summary", "content": "The Pattern Summary is introduced with a control button at the top-left corner (Fig. 4) for selecting the grouping mode. A comprehensive macro-level pattern summary (R2) under the chosen mode is displayed below. The grey card component presents a between-group level summary (Fig. 4-A), adjacent to which are the within-group level summary cards (Fig. 4-B). To begin, instructors can view the summary donuts chart (Fig. 4-a1), which shows the distribution of all students' prompt categories as defined in Sec. 4.2. For example, here, a predominantly light yellow section suggests that a majority of student prompts are at the \"Remember\" cognitive stage, while minor blue and major green segments indicate a limited use of literature-supported effective prompt strategies. This donuts chart design aims to clearly display the percentage of each cognitive level in students' prompts while optimizing space usage. Additionally, to effectively display and compare the overall quality of ChatGPT responses and students' learning outcomes, we selected two light grey bars and a dark grey bar (Fig. 4-a1) to represent the three metrics due to their simplicity and effectiveness. The details of metrics are introduced in Sec. 4.3.\nInstructors can easily check each group's background information by hovering over the summary donut chart (Fig. 5-A) and perform between-group comparisons of cognitive stage distribution and ChatGPT response quality using stacked bar charts and accompanying grey bar charts (Fig. 4-a2). Moreover, by switching to \"TaskG\" mode (Fig. 5-B), instructors can change the grouping from student background to task types, shifting the analysis towards task-specific student performance and interaction summaries. This flexible approach enables an efficient multi-viewpoint summary of macro-level students-ChatGPT conversations (R2). After identifying a specific group of interest, instructors can click on a stacked bar in the summary card (Fig. 4-a2), which highlights the selected group card (Fig. 4-b1) for deeper, within-group analysis. Instructors can sort students by various metrics by clicking on the metric summary bars in the first row, as shown in the example where students are sorted by their scores in ascending order (Fig. 4-b1). To move on, instructors can click on any stacked bar or the donuts chart (Fig. 4-b1) to investigate micro-level details in Pattern Mining Table (Fig. 6-A) and the Interaction Tree (Fig. 7-A)."}, {"title": "5.3.2 Pattern Mining Table", "content": "The Pattern Mining Table (Fig. 1-b4, Fig. 6-A) catalogs micro-level nuanced interaction patterns mined from the tasks and students selected by users (R3). Each interaction pattern is associated with specific metrics: length (\"L\"), frequency (\"C\"), and average score (\"Avg.\"). Aligned with the previous definition Sec. 4.2, interaction patterns are delineated as either a set of codes (denoted by curly braces {}, Fig. 6-B) or an ordered list of codes (indicated by an arrow \u2192 in the \"Pattern\" column, Fig. 6-C), with each code's background color reflecting its category. Instructors can sort these patterns by any metric, aiding in the identification of prevalent or effective patterns."}, {"title": "5.3.3 Interaction Tree", "content": "The Interaction Tree employs a decision-tree format design to trace the evolution of interaction patterns (R4) for individual students under each task (Fig. 1, Fig. 7-A). Each path within the Interaction Tree represents a student's detailed interaction process with ChatGPT under a specific task, beginning from a common root where nodes symbolize students' prompts, and adjacent solid links depict ChatGPT's responses. This format enables the aggregation of similar interaction paths, highlighting variations and commonalities in student interaction patterns. Consistent with the above settings, colors in the node coding for the category of the prompt and abbreviations inside or beside the node detailing the code content. When prompts encompass multiple codes, nodes incorporate pie-chart coloring to represent each code visually (Fig. 7-a1). To aggregate prompts with the same code contents, we use node size to indicate the number of students at the same round in their conversation. On the other hand, the solid links between nodes convey ChatGPT's response characteristics, including \"Response Token Length\" (RL) and \"Information Gain\" (IG). Each ChatGPT response's \"Information Gain\" is represented through variations in the horizontal length of the link, as it is the major information instructors (E1 & E2) want to notice, while link width and opacity double encode \"Response Token Length\". This visual encoding offers insights into the value and quality of ChatGPT's replies. Furthermore, the end of each path features one grey tag representing the student's alias and task's ID (Fig. 7-a2), together with another tag below encoding the student's performance on this task. Here, we utilized numerical values and color intensity to denote scores, thereby linking interaction patterns directly to learning outcomes. For instance, the highlighted student gained the full mark (Fig. 7-a2) after various interactions with ChatGPT. Through these meticulously designed components, instructors are not only equipped to perform a granular analysis of student interactions but also able to correlate them with educational outcomes, facilitating a comprehensive understanding of students' learning behaviors and the effectiveness of their interactions with ChatGPT (R5).\nTo further facilitate in-depth evaluation of each identified interaction pattern, we introduced an interplay between the Pattern Mining Table and the Interaction Tree. By clicking on each row, which corresponds to a specific interaction pattern (Fig. 6-B), the corresponding students who engage in this pattern will be highlighted in the Interaction Tree (Fig. 7-A). This enables instructors to assess relevant metrics such as learning outcomes and the quality of ChatGPT's responses for each identified pattern or individual student (R5).\nDesign alternative. During the design process, some alternatives were raised and discussed with our experts. For instance, we proposed to use a Sankey diagram together with a stacked bar chart design to represent the students' different prompt choices at each step. Specifically, each stacked bar represent the distribution of different categories of codes in each conversation turn (Fig. 7-b1). The Sankey flow represents the selected student's interaction pattern, ending with a grey bar with numerical values representing the student's score in this task. Although this design was clear to show the percentage of students' diverse interaction choices with ChatGPT at each conversation turn, the experts (E1 &E2) prioritized our Interaction Tree visualization since the quality of each ChatGPT's response was lacking and could be hard to add to the Sankey stacked bar chart easily. Meanwhile, they preferred a nuanced comparison between different students' interaction patterns, which is also a limitation of the Sankey-form diagram."}, {"title": "5.4 Detail View", "content": "Instructors can select the student's alias and task's ID tag at the end of each path in the Interaction Tree (Fig. 7-a2) to examine the specifics of each task and students' responses in the Task Description (Fig. 1-c1), and explore students' raw conversation with ChatGPT in the Raw Conversation (Fig. 1-c2). This functionality not only validates the analytical findings but also equips instructors with concrete examples and references for crafting feedback to students or refining task designs, serving as the end of our whole analysis workflow (R6)."}, {"title": "6 EVALUATION", "content": "This section delves into the assessment of StuGPTViz. We present the result of a student questionnaire to validate the settings of our open-ChatGPT in-class exercises. Then, we demonstrate the effectiveness of the Information Gain (IG) metric we introduced in 4.3. Subsequently, we showcase the system's capacity to facilitate the identification and analysis of student interactions with ChatGPT through three case studies. Additionally, we discuss the collective feedback from interviews with six domain experts (E1-E6). The insights from these experts evaluated the StuGPTViz's effectiveness and impact."}, {"title": "6.1 Questionnaire Feedback", "content": "We administered a mid-semester voluntary questionnaire consisting of multiple-choice and short-answer questions. The multiple-choice questions were designed to gauge students' experiences and attitudes towards using ChatGPT to learn data visualization and complete the designed in-class exercises. The short-answer questions aimed to collect students' general feedback, including their level of trust in and feelings about using ChatGPT for learning data visualization and other subjects. The detailed statistics of the questionnaire are provided in the supplementary (B). To summarize, the results indicated a strong positive reception: more than 90% students reported enjoying using ChatGPT in their learning process and expressed a willingness to utilize it extensively in our data visualization course. These findings affirmed the rationality behind our course material design and ensured the quality of the data collected for our study. Some other insights from the short-answer questions are discussed in the following Sec. 7."}, {"title": "6.2 Metric Evaluation", "content": "We evaluated the effectiveness of our Information Gain (IG) metric by sampling 10% of student-ChatGPT conversations. Two experts (E1 & E2) manually labeled the data into three categories: \"low information gain\" (score 0), \"average information gain\" (score 0.5), and \"high information gain\" (score 1). A score"}]}