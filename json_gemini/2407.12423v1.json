{"title": "StuGPTViz: A Visual Analytics Approach to Understand Student-ChatGPT Interactions", "authors": ["Zixin Chen", "Jiachen Wang", "Meng Xia", "Kento Shigyo", "Dingdong Liu", "Rong Zhang", "Huamin Qu"], "abstract": "The integration of Large Language Models (LLMs), especially ChatGPT, into education is poised to revolutionize students' learning experiences by introducing innovative conversational learning methodologies. To empower students to fully leverage the capabilities of ChatGPT in educational scenarios, understanding students' interaction patterns with ChatGPT is crucial for instructors. However, this endeavor is challenging due to the absence of datasets focused on student-ChatGPT conversations and the complexities in identifying and analyzing the evolutional interaction patterns within conversations. To address these challenges, we collected conversational data from 48 students interacting with ChatGPT in a master's level data visualization course over one semester. We then developed a coding scheme, grounded in the literature on cognitive levels and thematic analysis, to categorize students' interaction patterns with ChatGPT. Furthermore, we present a visual analytics system, StuGPTViz, that tracks and compares temporal patterns in student prompts and the quality of ChatGPT's responses at multiple scales, revealing significant pedagogical insights for instructors. We validated the system's effectiveness through expert interviews with six data visualization instructors and three case studies. The results confirmed StuGPTViz's capacity to enhance educators' insights into the pedagogical value of ChatGPT. We also discussed the potential research opportunities of applying visual analytics in education and developing AI-driven personalized learning solutions.", "sections": [{"title": "1 INTRODUCTION", "content": "Groundbreaking developments in generative AI, particularly through Large Language Models (LLMs) applications such as ChatGPT, have introduced unprecedented opportunities in educational methodologies [12,35]. These tools not only expedite students' information searches but also assist instructors in refining classroom activities and delivering personalized guidance [13,47,57]. However, as the integration of LLMs into educational scenarios is still nascent, it is imperative for instructors to carefully plan and assess how students utilize LLMs in their learning activities [53] to harness the full potential of LLMs and enhance the student learning experience [34,67]. A fundamental step in this process is to gain a comprehensive understanding of student interactions with LLMs, thereby acquiring key pedagogical insights such as students' cognitive levels, learning attitudes, and mastery of knowledge [41,45]. Nevertheless, efforts to provide instructors with these pedagogical insights are still in the initial stages. Existing research on LLMs in education primarily focuses on educational ethics and potential application scenarios, such as using LLMs as automated tools for evaluating student essays [10,38,39,55,59]. To our knowledge, in-depth studies on students' interactions with LLMs for learning tasks are scarce, facing two main challenges. First, there are no publicly available datasets dedicated to capturing students' conversations with LLMs. The existing datasets are primarily composed of everyday conversations with general users. While some include conversations about learning tasks like programming or solving math problems, they are few and lack guaranteed quality because the learning scenarios involving LLM use are not carefully crafted and evaluated by instructors. Moreover, these conversations typically follow a \"one question, one answer\" format, as their primary purpose is to assess LLMs' problem-solving capabilities. These deficiencies highlight the urgent need for data collection of learning-centered conversations with well-structured tasks [16,17,78]. Second, understanding how students interact with LLMs for pedagogical insights through conversation data presents significant challenges. One major difficulty is that instructors are eager to understand the extent of higher-order thinking (e.g., independent thinking) students engage in when using these advanced AI tools [21,71]. To comprehend this higher-order thinking, it is essential to measure students' cognitive levels [11]. However, accurately interpreting these cognitive levels based on students' inquiries to LLMs has never been explored. Additionally, assessing students' proficiency in utilizing LLMs poses another challenge, which involves evaluating the various LLMs' responses and observing how students adjust their prompts in response. Moreover, tracking the progression of these interactions introduces an added layer of complexity [27,68]. Visual analysis is a potential way. However, current research has often overlooked these challenges. While there are many visualizations works on conversation analysis studies focusing on topic progression or sentiment analysis [22,36,46], such studies do not adequately capture and visualize the cognitive levels reflected in the evolving interactions, falling short of meeting instructors' needs. To address these challenges, we selected ChatGPT, notable for being one of the most prevalent LLM applications [77], to gather conversation data. In addition, we see a potential to introduce LLMs for visualization education to address diverse student backgrounds and manage varied learning activities such as concept comprehension, visualization literacy, and design [52]. In collaboration with experienced course instructors, We devised and integrated an in-class exercise module into a graduate-level data visualization course at the local university, allowing students to interact with ChatGPT freely. Our approach yielded a significant collection of high-quality student-ChatGPT conversation data from well-crafted learning tasks. Through comprehensive thematic analysis and an extensive literature review [14,65,73,74], we developed a coding scheme that categorizes the diverse cognitive levels [44] and several metrics to evaluate the quality of ChatGPT responses [26,48]. To capture evolving strategies students use when interacting with ChatGPT, we analyzed various sequences and sets of the codes, defining these as \"interaction patterns\" which became the focal point of our analysis. Building on this foundation and design requirements derived from course instructors and experts, we introduce a pioneering visual analytics system for instructors to explore intricate interaction patterns and derive actionable pedagogical insights from student-ChatGPT conversation data. In particular, a customized tree visualization is designed to present the evolution and compare the characteristics of students' interaction patterns. To summarize, our key contributions are as follows:\n\u2022 We introduced ChatGPT to a real data visualization course, collected student-ChatGPT conversation data and developed a coding scheme for an in-depth analysis of interaction patterns.\n\u2022 We designed a visual analytics system, StuGPTViz, to help instructors discover insights into students' cognitive levels and proficiency when using ChatGPT.\n\u2022 Through three case studies and expert interviews, we demonstrated the effectiveness of our coding scheme and system in enhancing educational activities such as problem-solving guidance, personalized feedback, and exercise design.\nOverall, we present a design study that constitutes an initial yet crucial step toward analyzing student interaction patterns with ChatGPT, advancing the application of visual analytics in AI-driven education."}, {"title": "2 RELATED WORK", "content": "In this section, we discuss the relevant research, including LLMs in education and visualization education, visualization for AI-enhanced education, and visual analytics for conversational data."}, {"title": "2.1 LLMs in Education and Visualization Education", "content": "The integration of LLMs such as ChatGPT into educational settings has sparked a diverse range of discussions, with plenty of initial works centered on ethical considerations regarding their use in learning environments [38,55]. Increasingly, the academic community recognizes the transformative potential LLMs hold for education, advocating for their adoption to revolutionize learning and teaching methodologies [12]. Despite potential resistance from some educators, students inevitably turn to LLMs for assistance with coursework [35]. Therefore, researchers and instructors have explored LLMs for various applications, including serving as teaching assistants for writing and coding, generating adaptive exercises [66], supporting personalized question-answering sessions [10], and facilitating innovative learning modes like \"learn by teaching\", where LLM plays the role as \"learner\" and students assume the role of the teacher to teach the AI [62]. However, there is a notable gap in understanding how students strategize their use of ChatGPT for educational purposes. Existing literature predominantly focuses on the capabilities and applications of LLMs without delving into student interaction strategies, leaving educators without the necessary insights to fully leverage these tools in enhancing learning experiences [1,29]. Simultaneously, the field of visualization education is gaining traction, not only within the visualization community but also more broadly [8]. The challenges of teaching data visualization range from addressing diverse student backgrounds to managing varied learning activities such as concept comprehension, visualization literacy, and design evaluation-are substantial [52]. ChatGPT's potential to support these educational challenges opens avenues to investigate how students use ChatGPT across different visualization learning tasks, particularly relevant to our project's focus [3]. A comprehensive analysis of this research direction still remains unexplored [20]. This gap presents a unique opportunity for our work to contribute to the field by offering insights into student strategies in employing ChatGPT within visualization learning contexts, thereby advancing the understanding and application of LLMs in educational settings."}, {"title": "2.2 Visualization for Al-Enhanced Education", "content": "The integration of visualization in AI-enhanced education is dedicated to leveraging visual analytics for learning analysis, such as interpreting complex data and AI algorithms to improve educational outcomes [23]. While learning analysis harnesses data to refine and enhance learning processes, visualization techniques render these insights accessible and actionable for educators and students [18,58]. Despite notable advancements in each domain, the integration of visual analytics specifically tailored to learning analysis within AI-enhanced educational environments remains underexplored. Existing research underscores the value of visual analytics in presenting student performance metrics, engagement levels, and learning behaviors, thus enriching our understanding of educational dynamics [4,5]. Within this context, the subfield of open learner models exemplifies the potential of visual explanations, akin to Explainable AI (XAI), in demystifying AI-generated outputs, offering learners and educators transparent and trustworthy insights [15,25]. Additionally, other works have employed visual analytics to examine students' interaction data with intelligent agents, using students' log data such as hint requests to delve into their problem-solving processes [75]. Recently, with the advent of potent LLM-based conversational agents, the intricacy of interactions between students and AI has reached a new height. Goals once considered unrealistic, such as in-depth analysis of students' cognitive levels and thought processes, are now achievable [9,40,72]. To our knowledge, the dedicated exploration of visual analytics to analyze and elucidate student interactions with advanced AI tools, such as ChatGPT, is just beginning. In response to this gap, our work proposes a novel visual analytics system based on the students-ChatGPT conversation data we collected. The system is designed to identify and unravel the intricate nuances of student-ChatGPT interactions. It equips educators with profound insights into how LLMs can be utilized to customize and elevate students' learning experiences."}, {"title": "2.3 Visual Analytics for Conversational Data", "content": "The exploration of visual analytics for conversational text data within the visualization community has encompassed a wide range of applications, from sentiment analysis and topic modeling to mapping conversation flows and interactions within user groups [30,32,36,46]. For instance, T-Cal and IneqDetect [31,54] focus on analyzing group conversations and estimating members' sentiments to assess collaboration effectiveness. Meanwhile, efforts such as ThreadReconstructor, MultiConVis, and VisOHC [6,24,37] probe into the structure and core topics of online forum discussions. However, these initiatives mainly focus on summarizing the dynamics of multi-party conversations without delving into the intricacies of one-on-one dialogues. Another significant gap in current methodologies is their constrained ability to uncover the depth of evolving cognitive levels in educational dialogues between students and AI tools like ChatGPT. Visualization tools for one-on-one medical conversations, like ConVIScope and Discursis [7,49], focus on charting patient-doctor dialogues but only reflect changes in sentiment and topic over time. They fall short in showcasing how one party (e.g., students) adjusts their responses to another party (e.g., LLMs' replies). Similarly, research aimed at analyzing educational dialogues often emphasizes engagement and comprehension, lacking a detailed visual analysis of the depth of thinking, learning strategies, or intentions revealed through these interactions [8,50,76]. These shortcomings highlight the necessity for a novel visual analytics framework designed to tackle the specific challenges posed by educational dialogues with LLMs [3,33]. Consequently, our work introduces multiple visualizations, such as the Interaction Tree, to meticulously analyze students' interaction patterns with ChatGPT."}, {"title": "3 BACKGROUND AND DATA COLLECTION", "content": "This section outlines the background of our data visualization course, the collaborative efforts with our expert team, the in-class exercises, and the data collection considerations and procedures we adopted."}, {"title": "3.1 Data Collection Background and Considerations", "content": "At the invitation of our experts (two course instructors, E1&E2), we set out to incorporate ChatGPT into the curriculum of a postgraduate data visualization course for computer science majors in the first semester of 2024. This course, meeting once a week for three hours, attracted 55 registrants. Over the past four months, we have collaborated closely with E1, E2, and three teaching assistants (TA1-TA3) at our university. E1 is a professor with over 15 years of experience designing and teaching data visualization courses. E2 is a lecturer with three years of teaching experience and served as the primary instructor for this course. TA1, TA2, and TA3, are senior teaching assistants who have supported the professors in designing the in-class exercises, homework, and exams for the data visualization course for at least two semesters. All experts have used ChatGPT extensively over the last two years. In preparation, we conducted a three-hour meeting to outline how ChatGPT would be integrated into the course and how data would be collected. To maximize the benefits of ChatGPT for students, we decided to introduce an in-class exercise section. This would allow students to apply what they learned by interacting with ChatGPT and create an ideal setting for collecting diverse data on their learning interactions. During our discussion, two key considerations emerged:\nC1: Diverse Learning Tasks for Comprehensive Data Collection. To capture the full spectrum of student interactions with ChatGPT, exercises should be designed across various course aspects using diverse learning tasks (e.g., visualization understanding & design). These learning tasks should encompass different cognitive levels as outlined in Bloom's Taxonomy [44] to ensure that the collected data fully reflects students' diverse engagement with ChatGPT. While these tasks are exemplified using a data visualization course, learning tasks for any course can be similarly designed to align with Bloom's Taxonomy [19].\nC2: Natural Learning Scenarios for Unbiased Data Collection. To accurately reflect genuine students' behaviors, it was vital to integrate ChatGPT into the learning process as an optional tool rather than a course mandate. This approach was intended to gather authentic interaction data, showcasing real student needs and preferences in using ChatGPT for learning. Informed by these considerations, we included a 40-minute open ChatGPT session at the end of each class (9 classes in total) to promote active student engagement with ChatGPT through various tasks. Additionally, we carried out a pilot study with about 30 HCI and data visualization Ph.D. students to evaluate the exercise section's design and procedures. Their conversation data helped us ascertain the appropriateness of our data collection approach. The data collection process was also approved by the university's ethics committee (IRB approval). The subsequent sections will provide an overview of our task designs for the in-class exercises and the data collection procedures."}, {"title": "3.2 Tasks Summary and In-class Exercise Procedure", "content": "Collaborating with instructors E1 and E2, we developed 27 exercise tasks spread across seven distinct types according to the levels of cognitive learning in the Revised Bloom's Taxonomy (C1, Fig. 2):\nTo begin data collection, we distributed a questionnaire to the 55 enrolled students to gauge their willingness to participate and gather background information. Forty-eight students consented, providing details on their undergraduate majors, data visualization expertise, programming skills, and prior ChatGPT experience. We also hosted a 40-minute introductory session on ChatGPT, including instructions on exporting and uploading conversation files to Canvas system\u00b9, the web-based learning management system used by our university. Each in-class exercise session, conducted during the last 40 minutes of the lecture, included a 10-minute self-learning segment with ChatGPT, a 25-minute task completion segment, and a 5-minute conversation log upload phase. Students first spent 10 minutes asking ChatGPT questions to learn key lecture terms. This was followed by 25 minutes of task completion (Fig. 2) with ChatGPT's assistance and a five-minute interval for uploading conversations to Canvas. Students were encouraged to interact naturally with ChatGPT (C2), and those confident with the course material were free skip the self-exploration."}, {"title": "3.3 Dataset Brief", "content": "The dataset we collected involved 48 students' conversation data with ChatGPT during the in-class exercise session of data visualization course over the entire spring 2024 semester. It consists of 744 unique conversations with 2507 turns after filtering out the empty conversations and those unrelated to the learning tasks. To the best of our knowledge, it is the only existing dataset specifically for student-ChatGPT conversations under strictly-defined learning activities. The collected data is in a structured format, including metadata such as student names (anonymized), task ID and task types, and conversation content. Each conversation is logged in sequential order, capturing both student prompts and ChatGPT responses. Sample data are provided in the supplementary materials. Additionally, we collected students demographic information, including their background in computer science, data visualization, and ChatGPT usage experience."}, {"title": "4 DESIGN REQUIREMENT AND DATA PROCESSING", "content": "This section presents the design requirements for the visual analytics system identified through discussions with our experts (E1 and E2), the procedure for coding student prompts, and the methodology for processing ChatGPT responses."}, {"title": "4.1 Visualization Design Requirements", "content": "We summarized experts' requirements for analyzing student-ChatGPT conversation data as follows:\nR1: Overview of students and tasks data. Experts highlighted the necessity of an overview of both students and the tasks, including the distribution of students' background information (e.g., knowledge in visualization) and tasks' characteristics (e.g., types). Instructors can then select specific students or tasks for deeper analysis of student-ChatGPT conversations.\nR2: Summarizing macro-level conversation characteristics. Before detailed analysis, experts require a comprehensive summary of the student-ChatGPT conversation, especially the cognitive levels of students' prompts and the qualities of ChatGPT responses based on selected tasks and students. This summary should span multiple user-selected viewpoints, covering broad categories such as various student groups and types of tasks, since instructors are always interested in the differences in behavior and performance among different student groups, such as those with and without a CS background. Additionally, instructors often have limited time and want to quickly see the differences among groups to prioritize their focus.\nR3: Identifying micro-level interaction patterns. Experts require a structured method to detect and summarize students' micro-level interaction patterns (i.e., recurring methods and strategies students employ when interacting with ChatGPT), along with essential metrics such as learning outcomes and pattern frequency. Emphasizing this information is crucial for instructors to identify which patterns are more effective for learning and merit deep exploration. They can further develop actionable insights on how students engage with ChatGPT throughout their learning journey.\nR4: Tracing interaction pattern evolution. Educators necessitate a method to trace the development of students' interaction patterns, emphasizing both the shared and unique sequences within the context of task-solving. This requirement involves visualizing how students' interaction patterns evolve in response to tasks, reflecting variations in cognitive engagement and problem-solving approaches. Such visualization should facilitate a deeper understanding of varied student approaches to learning tasks.\nR5: Evaluating interaction pattern performance. Experts wanted to evaluate interaction patterns of interest effectively. This involves identifying students who utilize the pattern and assessing relevant metrics such as their learning outcomes and ChatGPT's response quality. Such comprehensive analysis helps gauge the effectiveness of different interaction patterns, enabling instructors to provide targeted feedback and identify exemplary patterns as recommended learning strategies.\nR6: Examining detailed raw data. Experts expressed a desire to access the raw data, which includes original in-class activities, students' answers or responses, and students' conversation logs with ChatGPT."}, {"title": "4.2 Students' Prompts Coding", "content": "The open coding process for students' prompts is based on thematic analysis methodology [69] and enriched by a literature review to identify prompt patterns [65,73,74]. Following expert requirements (R2), we categorized codes into two types: learning-related codes reflecting students' cognitive levels regarding course materials, and ChatGPT-related codes denoting students' comprehension and proficiency in using ChatGPT as a supplementary tool. Firstly, we reviewed literature analyzing general user prompt patterns and intents [65,73,74], identifying 16 general prompt patterns as the initial ChatGPT-related codes. While these codes could represent students' proficiency with ChatGPT, they did not capture their cognitive levels. Therefore, we invited three visualization researchers, each with over four years of experience in data visualization education and thematic analysis, to independently conduct open coding of students' prompts. They refined the ChatGPT-related codes and developed learning-related codes that encapsulate the intent and thought processes behind each student's prompt. Our researchers engaged in repeated cross-checking and refinement until a consensus was reached, ultimately establishing 27 codes, including 15 learning-related and 12 ChatGPT-related codes. After finalizing the code space, we consulted with our experts (TA1-TA3) to further refine our codes. At TA3's suggestion, we used the revised Bloom's taxonomy to categorize the 15 learning-related codes, enhancing our understanding of students' learning intentions and mapping their cognitive processes [44]. This taxonomy describes six stages of cognitive learning: remember, understand, apply, analyze, evaluate, and create, each representing an advancement in cognitive level. The authors independently categorized the learning-related codes, then discussed and refined any inconsistencies until agreement was reached. The final code schema is summarized in Fig. 3. To ensure coding quality and consistency, the authors coded 30 conversations from the pilot experiment with the finalized labels and calculated the Inter-Rater Reliability (IRR) scores, resulting in an IRR of 0.84, validating the coding process's reliability. Finally, the authors coded all students' prompts. For prompts embodying multiple learning intents and strategies, we applied multiple codes. Additionally, we used the ordered sequence of codes and unordered sets of codes from each student's conversation data to illustrate the \"interaction patterns\" identified via expert requirements (R3). For each coded conversation, we mined all possible ordered code"}, {"title": "4.3 Processing ChatGPT's Responses", "content": "To enhance our understanding of students' interactions with ChatGPT, we analyze and evaluate ChatGPT's response quality based on expert suggestions (TA1). Through our literature review, common metrics for evaluating ChatGPT's responses include response relevance, length, and correctness [28,42,43], all recognized as important by our experts. We employed the Ragas response relevance package [26], a renowned framework for evaluating large language models, to assign a numerical score to the relevance of each \"user prompt - LLM response\" pair. Additionally, we measured response length and, in collaboration with experts E2 and TA2, assessed response correctness by categorizing them into \"basically correct\" (score 1), \"partially correct\" (score 0.5), and \"basically wrong\" (score 0), respectively. Through further discussion, experts (E1 & E2) expressed interest in evaluating the amount of accurate information a student can acquire from each turn of interaction with ChatGPT. Consequently, we combine the two metrics, response relevance score and response correctness, to develop the new metric \"information gain\u201d inspired by the literature [51]. The metric's formula is shown below, which primarily leverages the KL-divergence principle [70], calculates the amount of new information provided by the latest ChatGPT response compared to the existing set of responses:\n$IG(P,Q) = \\sum_i P(i) \\log(\\frac{P(i)}{Q(i)}) \\times R \\times C,$ \nIn this formula, $IG$ represents the information gain of the incoming response $P$ under the cumulative response set $Q$. $P(i)$ is calculated as the frequency of word $i$ in the current response divided by the total number of words in that response. $Q(i)$, on the other hand, is the cumulative frequency of word $i$ up to the current response, divided by the total number of words in all responses up to that point. The variables $R$ and $C$ represent the numeric relevance score and correctness score, respectively. This computation quantifies the new information provided by ChatGPT's latest response compared to the existing knowledge base."}, {"title": "5 VISUALIZATION", "content": "Based on the design requirements identified in Sec. 4.1 and the data collected in Sec. 3, we developed a visual analytics system, StuGPTViz (Fig. 1), aimed at enabling instructors to analyze student interactions with ChatGPT effectively."}, {"title": "5.1 System Overview", "content": "StuGPTViz is intricately designed to facilitate a multi-level analysis of students' interactions with ChatGPT from the perspective of both tasks and students. It supports instructors in selecting specific tasks and students as focal points of analysis, thereby accommodating diverse analytical interests. Moreover, it enables a \"gradually deepening\" analysis process, allowing users to acquire both a broad overview and detailed insights into the interactions between students and ChatGPT. Specifically, StuGPTViz is structured into three main components:\nInitial selection: Instructors begin by selecting particular tasks or students of interest through the Task Overview and Student Overview in the Filter View (Fig. 1-a1, a2). This selection offers an overview of students and tasks data (R1), which also triggers updates in the Pattern Summary of the Pattern View (Fig. 1-b1).\nGradually deepening exploration: Starting from the Pattern Summary in the Pattern View (Fig. 1-b1), instructors can gain a summary of macro-level conversation characteristics of the selected students and tasks, covering both groups and individuals (R2). To delve deeper, instructors can further analyze the micro-level student-ChatGPT interaction patterns via Pattern Nuance. While the Pattern Mining Table (Fig. 1-b4) enable instructors to identify the pattern summary together with significant metrics like learning outcome (R3), the Interaction Tree traced the pattern evolution of each student (R4). Moreover, the interplay between these two visualizations enables instructors to explore and assess patterns of interest effectively (R5).\nDetailed inspection: Finally, the Task Description and Raw Conversation from the Detailed View (Fig. 1-c1, c2) provides access to the original tasks, students' responses, and their raw conversation logs with ChatGPT. This component allows instructors to examine task specifications, student prompts, and ChatGPT's replies, leveraging their expertise to interpret the data comprehensively (R6)."}, {"title": "5.2 Filter View", "content": "The Filter View (Fig. 1-A) serves as the gateway to analysis, presenting the distribution of various background metrics and enabling instructors to filter the students and tasks of interest (R1). The Task Overview (Fig. 1-a1) displays information about learning tasks and features a search box for quickly finding specific tasks by ID. The distribution of task difficulties and types, determined by experts (E1 & E2), is depicted through two bar charts, while an area chart illustrates the distribution of students' normalized average scores (x-axis) across tasks. Instructors can select tasks by clicking on the bars or adjusting sliders, with each metric chart dynamically updating in response to real-time user selections. Moreover, the Student Overview (Fig. 1-a2) provides insights into students' backgrounds. A search box allows quick location of students by aliases, and an area chart at the bottom visualizes the distribution of students' average scores. Positioned between them, three-segmented bar charts represent the distribution of students' prior experience in data visualization and computer science and their familiarity with ChatGPT, derived from a background survey conducted during the first class. By freely filtering each metric, instructors can effortlessly isolate students and tasks of interest (R1), then proceed to the Pattern View (Fig. 1-B)."}, {"title": "5.3 Pattern View", "content": "After identifying tasks and students of interest, instructors can delve into detailed analysis using the Pattern View (Fig. 1-B). Comprising the Pattern Summary and Pattern Nuance, the Pattern View enables a gradually deepening examination of interaction patterns, offering insights from macro-level summaries to micro-level details (R2, R3). Throughout the Pattern View, we consistently apply color scheme (Fig. 1, top-right corner) to represent the code categories defined in Fig. 3. Specifically, we use a sequential color scheme, transitioning from light yellow to dark brown, to denote increasing cognitive levels (from \"remember\" to \"create\") as demonstrated in students' prompts. Concurrently, a distinct color scheme (blue for effective prompt strategies from literature and green for others) signifies students' proficiency in using ChatGPT."}, {"title": "5.3.1 Pattern Summary", "content": "The Pattern Summary is introduced with a control button at the top-left corner (Fig. 4) for selecting the grouping mode. A comprehensive macro-level pattern summary (R2) under the chosen mode is displayed below. The grey card component presents a between-group level summary (Fig. 4-A), adjacent to which are the within-group level summary cards (Fig. 4-B). To begin, instructors can view the summary donuts chart (Fig. 4-a1), which shows the distribution of all students' prompt categories as defined in Sec. 4.2. For example, here, a predominantly light yellow section suggests that a majority of student prompts are at the \"Remember\" cognitive stage, while minor blue and major green segments indicate a limited use of literature-supported effective prompt strategies. This donuts chart design aims to clearly display the percentage of each cognitive level in students' prompts while optimizing space usage. Additionally, to effectively display and compare the overall quality of ChatGPT responses and students' learning outcomes, we selected two light grey bars and a dark grey bar (Fig. 4-a1) to represent the three metrics due to their simplicity and effectiveness [56]. The details of metrics are introduced in Sec. 4.3. Instructors can easily check each group's background information by hovering over the summary donut chart (Fig. 5-A) and perform between-group comparisons of cognitive stage distribution and ChatGPT response quality using stacked bar charts and accompanying grey bar charts (Fig. 4-a2). Moreover, by switching to \"TaskG\" mode (Fig. 5-B), instructors can change the grouping from student background to task types, shifting the analysis towards task-specific student performance and interaction summaries. This flexible approach enables an efficient multi-viewpoint summary of macro-level students-ChatGPT conversations (R2). After identifying a specific group of interest, instructors can click on a stacked bar in the summary card (Fig. 4-a2), which highlights the selected group card (Fig. 4-b1) for deeper, within-group analysis. Instructors can sort students by various metrics by clicking on the metric summary bars in the first row, as shown in the example where students are sorted by their scores in ascending order (Fig. 4-b1). To move on, instructors can click on any stacked bar or the donuts chart (Fig. 4-b1) to investigate micro-level details in Pattern Mining Table (Fig. 6-A) and the Interaction Tree (Fig. 7-A)."}, {"title": "5.3.2 Pattern Mining Table", "content": "The Pattern Mining Table (Fig. 1-b4, Fig. 6-A) catalogs micro-level nuanced interaction patterns mined from the tasks and students selected by users (R3). Each interaction pattern is associated with specific metrics: length (\"L\"), frequency (\"C\"), and average score (\"Avg.\"). Aligned with the previous definition Sec. 4.2, interaction patterns are delineated as either a set of codes (denoted by curly braces {}, Fig. 6-B) or an ordered list of codes (indicated by an arrow \u2192 in the \"Pattern\" column, Fig. 6-C), with each code's background color reflecting its category. Instructors can sort these patterns by any metric, aiding in the identification of prevalent or effective patterns."}, {"title": "5.3.3 Interaction Tree", "content": "The Interaction Tree employs a decision-tree format design to trace the evolution of interaction patterns (R4) for individual students under each task (Fig. 1, Fig. 7-A). Each path within the Interaction Tree represents a student's detailed interaction process with ChatGPT under a specific task, beginning from a common root where nodes symbolize students' prompts, and adjacent solid links depict ChatGPT's responses. This format enables the aggregation of similar interaction paths, highlighting variations and commonalities in student interaction patterns. Consistent with the above settings, colors in the node coding for the category of the prompt and abbreviations inside or beside the node detailing the code content. When prompts encompass multiple codes, nodes incorporate pie-chart coloring to represent each code visually (Fig. 7-a1). To aggregate prompts with the same code contents, we use node size to indicate the number of students at the same round in their conversation. On the other hand, the solid links between nodes convey ChatGPT's response characteristics, including \"Response Token Length\" (RL) and \"Information Gain\" (IG). Each ChatGPT response's \"Information Gain\" is represented through variations in the horizontal length of the link, as it is the major information instructors (E1 & E2) want to notice, while link width and opacity double encode \"Response Token Length\". This visual encoding offers insights into the value and quality of ChatGPT's replies. Furthermore, the end of each path features one grey tag representing the student's alias and task's ID (Fig. 7-a2), together with another tag below encoding the student's performance on this task. Here, we utilized numerical values and color intensity to denote scores, thereby linking interaction patterns directly to learning outcomes. For instance, the highlighted student gained the full mark (Fig. 7-a2) after various interactions with ChatGPT. Through these meticulously designed components, instructors are not only equipped to perform a granular analysis of student interactions but also able to correlate them with educational outcomes, facilitating a comprehensive understanding of students' learning behaviors and the effectiveness of their interactions with ChatGPT (R5). To further facilitate in-depth evaluation of each identified interaction pattern, we introduced an interplay between the Pattern Mining Table and the Interaction Tree. By clicking on each row, which corresponds to a specific interaction pattern (Fig. 6-B), the corresponding students who engage in this pattern will be highlighted in the Interaction Tree (Fig. 7-A). This enables instructors to assess relevant metrics such as learning outcomes and the quality of ChatGPT's responses for each identified pattern or individual student (R5). Design alternative. During the design process, some alternatives were raised and discussed with our experts. For instance, we proposed to use a Sankey diagram [60] together with a stacked bar chart design to represent the students' different prompt choices at each step. Specifically, each stacked bar represent the distribution of different categories of codes in each conversation turn (Fig. 7-b1). The Sankey flow represents the selected student's interaction pattern, ending with a"}, {"title": "5.4 Detail View", "content": "Instructors can select the student's alias and task's ID tag at the end of each path in the Interaction Tree (Fig. 7-a2) to examine the specifics of each task and students' responses in the Task Description (Fig. 1-c1), and explore students' raw conversation with ChatGPT in the Raw Conversation (Fig. 1-c2). This functionality not only validates the analytical findings but also equips instructors with concrete examples and references for crafting feedback to students or refining task designs, serving as the end of our whole analysis workflow (R6)."}, {"title": "6 EVALUATION", "content": "This section delves into the assessment of StuGPTViz. We present the result of a student questionnaire to validate the settings of our open-ChatGPT in-class exercises. Then, we demonstrate the effectiveness of the Information Gain (IG) metric we introduced in 4.3. Subsequently, we showcase the system's capacity to facilitate the identification and analysis of student interactions with ChatGPT through three case studies. Additionally, we discuss the collective feedback from interviews with six domain experts (E1-E6). The insights from these experts evaluated the StuGPTViz's effectiveness and impact."}, {"title": "6.1 Questionnaire Feedback", "content": "We administered a mid-semester voluntary questionnaire consisting of multiple-choice and short-answer questions. The multiple-choice questions were designed to gauge students' experiences and attitudes towards using ChatGPT to learn data visualization and complete the designed in-class exercises. The short-answer questions aimed to collect students' general feedback, including their level of trust in and feelings about using ChatGPT for learning data visualization and other subjects. The detailed statistics of the questionnaire are provided in the supplementary (B). To summarize, the results indicated a strong positive reception: more than 90% students reported enjoying using ChatGPT in their learning process and expressed a willingness to utilize it extensively in our data visualization course. These findings affirmed the rationality behind our course material design and ensured the quality ofthe data collected for our study. Some other insights from the short-answer questions are discussed in the following Sec. 7."}, {"title": "6.2 Metric Evaluation", "content": "We evaluated the effectiveness of our Information Gain (IG) metric by sampling 10% of student-ChatGPT conversations. Two experts (E1 & E2) manually labeled the data into three categories: \"low information gain\" (score 0), \"average information gain\" (score 0.5), and \"high information gain\" (score 1). A score of 0 was assigned to responses containing mostly incorrect information, a score of 1 to responses providing rich and accurate new information, and a score of 0.5 to responses that were partially inaccurate or partially redundant with previous. To measure the correlation between IG metric and experts' labeling, we calculated Pearson correlation [63], Spearman correlation [64] and Kendall Rank correlation [2] between them. The results are 0.609 (p = 0.00), 0.621 (p = 0.00) and 0.497 (p = 0.00), respectively. All the results show there is a moderate to strong and significant positive correlation between the IG metric and experts' judgment of ChatGPT's response quality. The sample of labeled data and metric results is provided in the supplementary materials. Although effective, the IG metric is a simplified measure that primarily considers word frequency, designed to provide an initial assessment. In the future, we plan to use more advanced third-party ChatGPT response quality evaluation methods to enhance the metric accuracy."}, {"title": "6.3 Case Study", "content": "We engaged experts (E1-E6) to evaluate StuGPTViz independently. We introduced the background, visual designs, and a brief workflow demo to them and yielded three case studies that underscore the system's utility in analyzing student interactions with ChatGPT. Experts E1 and E2 are the course instructors we collaborated with, and E3-E6 are newly invited experts, including three assistant professors and one lecturer from three different universities, all with expertise in data visualization."}, {"title": "6.3.1 Case 1: Enhancing Students' ChatGPT Utilization", "content": "In the first case study, E1 and E3 leveraged StuGPTViz to derive instructional strategies to optimize students' use of ChatGPT for learning and addressing challenging tasks.\nInitial Overview and Task Filtering: The investigation began with an overview of tasks and student backgrounds. To focus on challenging tasks, the experts used the Task Overview to filter out tasks with difficulty scores below 3, those categorized under \"self-learning\" and \"remember,\" and those with average scores exceeding 0.8 (Fig. 1(a1)). In the Student Overview, all students were retained to ensure a comprehensive analysis of diverse interaction patterns (Fig. 1(a2)).\nIdentifying Challenges and Specific Tasks: To review task summary patterns, the experts switched to the \"Task-Grouping\" mode in the Pattern View (Fig. 1-b1) and identified \u201canalyze\u201d tasks as particularly challenging, evidenced by longer stacked bars indicating higher cognitive engagement (Fig. 1-b2). Although the thick grey bar representing \u201cInformation Gain suggested students acquired significant information from ChatGPT, the overall learning outcomes for these tasks were suboptimal (Fig. 1-b2). Consequently, the experts focused on the \u201canalyze\u201d task group, identifying \u201cTask 3\u201d as notably difficult due to the extensive cognitive processing it required(Fig. 1-b3).\nAnalysis of Interaction Patterns: The examination of \u201cTask 3\u201d through the pattern mining table and Interaction Tree (Fig. 1-b4, b5) revealed nuanced dynamics of student-ChatGPT interactions. Initially, sorting by average score revealed infrequent and overly specific patterns. To uncover broadly applicable patterns, experts shifted to sorting by frequency (\"Count\"), identifying a recurring and effective pattern: a combination of \u201cDefinition Inquiry\u201d and \u201cFollow up Questions (Fig. 1-b6), notable for its prevalence and high average score exceeding 0.8."}, {"title": "6.3.2 Case 2: Provide Personalized Feedback", "content": "Experts E3 and E4 aimed to provide personalized feedback to students lagging behind, focusing on those with average scores below 0.5 in the Student Overview. They identified \"Group 1\" from the Pattern View donuts chart as the weakest, with student \"cx\" showing extensive ChatGPT interaction but the lowest scores (Fig. 8-a1). To better understand \"cx's\" interactions, the experts analyzed the Pattern Table (Fig. 8-B) sorted by usage frequency (\u201cCount\u201d), revealing \"cx\" predominantly engaged in basic \"Question Inquiry\" and \"Definition Inquiry\" and rarely modified default settings (Fig. 8-b1). This pattern indicated that \"cx\" primarily operated at initial cognitive levels, showing a dependency on ChatGPT's capabilities rather than engaging in self-driven learning or critical thinking."}, {"title": "6.3.3 Case 3: Refine Course Material Design", "content": "Experts E2 and E4 employed StuGPTViz to assess the alignment of existing in-class exercises with educational objectives, especially considering unrestricted access to ChatGPT. Their initial step involved excluding \"self-learning\" tasks from the analysis within the Task Overview. Progressing to the Pattern View, they aimed to compare cognitive engagement levels across various task types. The experts found that \"evaluate\" and \"create\" tasks elicited notably lower levels of advanced cognitive engagement compared to \"analyze\" and \"remember\" tasks (Fig. 9-A). They further examined the \u201cevaluate\u201d tasks and identified a predominant pattern of \"Question Inquiry\" (Fig. 9-B), indicating students often relayed task descriptions to ChatGPT for answers. A detailed check of the \"evaluate\" task descriptions in the Detailed View revealed this was particularly evident in tasks where students were explicitly instructed to analyze specific visual properties. The direct provision of analysis criteria encouraged a straightforward query-response dynamic with ChatGPT, bypassing deeper cognitive processing. Based on these insights, E4 recommended a shift in exercise design towards more open-ended questions that prompt students to independently determine relevant metrics or principles before engaging in analysis. This approach aims to ensure alignment with educational objectives considering unrestricted access to ChatGPT. On the other hand, the experts investigated \"create\" type tasks and found that the analysis results and task description indicated a lack of clear direction for students in formulating queries to ChatGPT, often resulting in incomplete (\"Empty\") or superficial (\u201cQuestion Inquiry\u201d, mainly refer to question copy & paste) interactions (Fig. 9-C). To address this, it was suggested that instructors could guide students toward requesting alternative design options from ChatGPT, including the rationale behind each. The experts concurred that prompting students to assess these options and incorporate their critical reasoning can greatly enhance the learning experience."}, {"title": "6.4 Expert Interview", "content": "Following the case studies, one-on-one interviews were conducted with six experts, each lasting around 80 minutes with a $80 compensation. System Workflow. The workflow of StuGPTViz was praised by all experts for its clarity and functionality. Experts noted its ease of use, allowing for a streamlined narrowing of analysis scope to achieve insights across multiple levels. Whether focusing on groups of students, individuals, or tasks, the system's design facilitated seamless navigation without added complexity. E5, an assistant professor specializing in teaching visualization to business students, highlighted, \"The workflow's logical progression and the interconnection of each view were particularly impressive, enabling a diverse analytical focus through a unified procedure.\" E3, E4, and E6 emphasized the importance of understanding students' learning outcomes. They appreciated the system's capability to filter, and highlight scores at various analytical stages. Visual Design and Interactions. Experts agreed that the visual design and interactive elements of StuGPTViz are clear and user-friendly, significantly enhancing the analytical process. The use of stacked bar charts was particularly commended for facilitating easy understanding and comparison of cognitive levels across students. The color coding, distinguishing between learning-related and ChatGPT usage-related codes, was found intuitive. E2 highlighted the clarity provided by the visual design: \"The ability to discern students' overall cognitive level at a glance is highly appreciated.\" The Interaction Tree visualization emerged as a favorite for its detailed representation of different patterns and the entire interaction journey, including ChatGPT responses and learning outcomes. E4, an assistant professor, praised the decision-tree format for showcasing diverse student strategies and ChatGPT's varied response quality. However, concerns were raised about the scalability of this visualization, especially for large classes over 100 students, suggesting a need for refining the summary of popular interaction patterns and detailed comparison of individual paths. Suggestions. Experts provided several actionable suggestions for enhancing StuGPTViz. E5 proposed adding a summary report panel to capture screenshots and annotate findings directly within the system, facilitating a comprehensive and customizable analysis experience. E6 recommended more flexible options for grouping tasks and students, suggesting a user-defined grouping mechanism to enable richer cross-correlations between different cohorts and tasks, visualized through a matrix-form panel. Despite the potential for increased complexity, E6 believed this feature could unveil deeper insights. Additionally, E1 and E4 suggested integrating the coding of students' conversations with ChatGPT directly into the workflow, similar to a \"grading the assignment\" process. This would streamline the evaluation process and enrich instructors' understanding of student interactions with ChatGPT."}, {"title": "7 DISCUSSION", "content": "This section discusses the significance and insights towards ChatGPT for data visualization education, and the generalizability and scalability of the proposed visual analytics system. ChatGPT for Data Visualization Education The introduction of ChatGPT into educational ecosystems marks a pivotal moment and necessitates a nuanced understanding of technological integration in learning. Our investigation into the patterns and strategies of student engagement with ChatGPT is critical, providing insights that guide instructors in facilitating the effective use of AI. Our study revealed several key findings from student questionnaires, student-ChatGPT conversation data, and discussions with course instructors. First, students reported that ChatGPT excels in summarizing key concepts, providing quick access to vast information, and offering tailored Q&A sessions. Instructors agreed that these functionalities are particularly beneficial for students with limited backgrounds, such as those who changed their major in graduate school, allowing them to keep pace with coursework without hindering class progress. Additionally, over 90% of students expressed satisfaction with ChatGPT's ability to handle data visualization queries, indicating a strong positive perception of its utility in data visualization education. However, instructors pointed out that recognizing ChatGPT's limitations in interpreting and processing visual data compared to textual information is crucial. For instance, through the collected student-ChatGPT conversations, instructors identified that some students resorted to asking ChatGPT for URLs of existing sample visualizations (e.g., demos of parallel coordinates) to obtain better figure quality after receiving a low-quality response. These findings inspire instructors to develop innovative approaches, such as guiding ChatGPT to provide descriptions or links to reference visualizations, for better educational content. Instructors also emphasized that the demand-driven nature of data visualization requires students to employ precise and strategic questioning techniques. This underscores the importance of teaching students how to prompt effectively to maximize ChatGPT's capabilities. Furthermore, as we navigate the ChatGPT-enhanced educational paradigm, our work highlighted the need to focus on cultivating students' higher-order cognitive skills, such as critical thinking and evaluative judgment. For instance, instructors suggested encouraging students to ask ChatGPT for alternative designs with underlying rationales, enabling them to critically assess options and fostering a collaborative learning dynamic. Approaches like this redefine the role of ChatGPT in education-from a universal solver to a pedagogical partner\u2014and emphasize the importance of critical engagement and decision-making skills in the data visualization domain. Generalizability & Scalability The application of StuGPTViz, while rooted in the context of data visualization education, unveils broader implications for ChatGPT-assisted learning environments. It offers a robust framework for analyzing student interactions with ChatGPT across a variety of courses. StuGPTViz's core workflow, which includes filtering tasks and students of interest, analyzing cognitive levels through prompts, assessing prompt engineering skills, evaluating ChatGPT's response quality, and identifying interaction patterns, encapsulates the universal aspects of leveraging ChatGPT in education. This approach enriches our understanding of effective AI integration into pedagogy and opens avenues for examining ethical considerations regarding ChatGPT's involvement in teaching practices. Regarding scalability, StuGPTViz demonstrates competence in managing classes of 48 students through the interaction tree visualization techniques equipped with simple pruning for clarity. This capability ensures the StuGPTViz's efficacy in distilling actionable insights from complex datasets, catering to the needs of regular-sized classes. However, scalability challenges arise as class sizes expand beyond this scope. For larger cohorts (e.g., exceeding 100 students) the incorporation of edge bundling techniques emerges as a potential refinement to enhance pattern visualization and comparison. Additionally, expanding the pattern mining table to include interaction patterns from different tasks or courses would further enhance its scalability and provide additional benefits. This adaptation will form a component of our further efforts, aiming to ensure that StuGPTViz remains effective in in diverse educational settings, advancing the goal of inclusive AI-enhanced education."}, {"title": "8 CONCLUSION", "content": "This study introduces StuGPTViz, a visual analytics system for instructors to analyze student-ChatGPT interactions. In particular, we collected student-ChatGPT conversations in a graduate-level data visualization course and developed a comprehensive coding scheme to categorize students' prompts from cognitive levels and ChatGPT's response qualities. We then build StuGPTViz to visualize student-ChatGPT interaction patterns and support multi-level, multi-perspective analysis. StuGPTViz empowers instructors with deep insights into students' cognitive processes, their reliance on ChatGPT, and their ability to use it effectively, highlighting areas for pedagogical intervention to promote higher-order thinking. The system's effectiveness, validated through expert interviews and case studies, confirms its potential to impact student-ChatGPT conversation analysis and visualization education. As we look to the future, StuGPTViz sets the stage for broader research into the application of visual analytics in education and the development of AI-enhanced personalized learning experiences."}]}