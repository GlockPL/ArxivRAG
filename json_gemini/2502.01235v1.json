{"title": "One-step full gradient suffices for low-rank fine-tuning, provably and efficiently", "authors": ["Yuanhe Zhang", "Fanghui Liu", "Yudong Chen"], "abstract": "This paper studies how to improve the performance of Low-Rank Adaption (LoRA) (Hu et al., 2022) as guided by our theoretical analysis. Our first set of theoretical results show that for random initialization and linear models, i) LORA will align to the certain singular subspace of one-step gradient of full fine-tuning; ii) preconditioners improve convergence in the high-rank case. These insights motivate us to focus on preconditioned LoRA using a specific spectral initialization strategy for aligning with certain subspaces. For both linear and nonlinear models, we prove that alignment and generalization guarantees can be directly achieved at initialization, and the subsequent linear convergence can be also built. Our analysis leads to the LORA-One algorithm (using One-step gradient and preconditioning), a theoretically grounded algorithm that achieves significant empirical improvement over vanilla LoRA and its variants on several benchmarks. Our theoretical analysis, based on decoupling the learning dynamics and characterizing how spectral initialization contributes to feature learning, may be of independent interest for understanding matrix sensing and deep learning theory. The source code can be found in the https://github.com/YuanheZ/LoRA-One.", "sections": [{"title": "1 Introduction", "content": "How to efficiently learn nonlinear models has been the recurring theme in machine learning (Alpaydin, 2020), especially in the era of large language models (LLMs) (Brown et al., 2020; Thoppilan et al., 2022). Parameter-efficient fine-tuning of LLMs (Achiam et al., 2023) aims to design scalable strategies to approximate/learn an unknown feature shift A such that LLMs perform well on new tasks while retain the knowledge from pre-trained models.\nOne typical parameter-efficient strategy is Low-Rank Adaptation (LoRA) (Hu et al., 2022), which learns a low-rank approximation of the unknown feature shift, \u2206 \u2248 AB, using two low-rank matrices A and B. LORA uses random Gaussian initialization for A and zero initialization for B:\n[Ao]ij ~N(0, a\u00b2) and [Bo]ij = 0, a>0. (LORA-init)"}, {"title": "1.1 Contributions and Algorithm Design Principles", "content": "In this work, we theoretically investigate the behavior of gradient descent (GD) update of LoRA parameters (At, Bt) and identify the subspaces they align with. Our analysis covers both linear and nonlinear models, with an overview of our results given in Table 1. Building on design principles distilled from these insights, we develop theoretically grounded algorithms that achieve enhanced performance in practical applications.\nIn Section 3, we start by analyzing LoRA for fine-tuning a multi-output linear model. Denoting one-step gradient of full fine-tuning as G\u00b9, we prove that the gradient update aligns At with the singular subspace of Gt while Bt always stays in a certain singular subspace w.r.t. Gt; see Section 3.1 for details.\nConsequently, by computing the singular value decomposition (SVD) of G\u0127 = UGSGV and we can directly achieve the above alignment if we use its certain singular subspaces for initialization:\nAo=VUG[:,1:r]SG[1:r,1:r]UG[1:r,] (Spectral-init)\nBo=UG[:,1:r]SG[1:r,1:r]VG[1:r,]"}, {"title": "2 Problem Settings", "content": "In this section, we introduce the problem setting of fine-tuning pre-trained linear and nonlinear models with the following assumptions."}, {"title": "2.1 Basic Assumptions", "content": "We consider both linear and nonlinear pre-trained models with multiple outputs and thus matrix parameters (instead of vectors), which is consistent with LoRA in practice.\nAssumption 2.1 (Pre-trained model). For the input x \u2208 Rd, we denote by W\u266e \u2208 Rd\u00d7k the known pre-trained parameter matrix. We assume that the pre-trained model can be linear or nonlinear with \u03c3(\u00b7) = max{0, \u00b7 } is the (entry-wise) ReLU activation function.\nfpre (x) :=\n(xW4) \u2208 R* linear\n[[(xW)], \u2208 R nonlinear\nNote that our results can handle large dimension d and k. Next, we assume there exists an unknown low-rank feature shift A on Wt that we aim to estimate.\nAssumption 2.2. The downstream feature matrix W\u00a3 := W\u00a2 + \u2206 admits an unknown low-rank feature shift \u0394 \u2208 Rd\u00d7k, where Rank (\u2206) = r* < min{d, k}.\nThis assumption is widely used in the literature on LoRA analysis and matrix factorization (Zhang et al., 2021; St\u00f6ger and Soltanolkotabi, 2021; Soltanolkotabi et al., 2023; Xiong et al., 2023). Besides, we also assume that the data is well-behaved, e.g., (Gaussian/sub-Gaussian) concentration.\nAssumption 2.3 (Downstream data for fine-tuning). We consider the label-noiseless setting for fine-tuning linear and nonlinear models. Given the unknown W\u00b9, the N downstream data points {(xi, Yi)}1 are i.i.d. and satisfy the following data generation process:\nN\n\u1ef9:= \u222b(xW4)\u2208Rk, \u222biN=1 i.i.d.~SG, linear\n[(xW)], i.i.d. N(0, Id) nonlinear"}, {"title": "2.2 Full Fine-tuning and LoRA", "content": "Our goal is to efficiently recover A by fine-tuning on the downstream data. Let the complete SVD of A\u2208 Rdxk be\n\u2206=U S\u2217VT := U[S\u2217 00 0]VT,\nwhere \u016a \u2208 Rdxd and \u2207 \u2208 Rk\u00d7k are the left and right singular matrices, and \u00a7* \u2208 Rd\u00d7k is a rank-r* diagonal matrix with nonzero singular values {\u03bbi}i=1r\u2217. It admits the compact SVD \u2206 = US*VT with U \u2208 Rd\u00d7r*, VT \u2208 Rr*\u00d7k, and S* \u2208 Rr**r*. The left/right singular subspaces spanned by U and V play an important role in our analysis.\nWe write the downstream data in a compact form X = [x1,\u2026 ,\u1fb6\u014a] \u2208 RN\u00d7d and the label matrix Y = [Y1\u00a8\u00a8\u1ef9N] \u2208 R\u00d1\u00d7k is generated by either linear or nonlinear target functions in Assumption 2.3. We introduce the training based on full fine-tuning and LoRA below.\nFull Fine-tuning: We consider the following empirical risk minimization with a squared loss\nL(W) := 1/2N ||XW\u2212Y ||F^2 linear,\n1/2N ||\u03c3(XW)\u2212Y ||F^2 nonlinear,\nwhere the parameter W can be learned by gradient descent (GD) initialized at W\u00b9, i.e., W\u2030 := W\u00a3.\nLORA: LORA updates two low-rank matrices A \u2208 Rd\u00d7r, B\u2208Rr\u00d7k for efficiency with the following empirical risk\n\n\u0128 (A, B):= 1/2N||X(W+AB)\u2212\u1ef8||F^2 linear,\n1/2N|| (X(W+AB)) -\u1ef8||F^2 nonlinear\nwhich can be minimized using GD\nAt+1 = At \u2212 \u03b71\u2207AL (At, Bt), \nBt+1 = Bt \u2212 72\u2207BL (At, Bt),\nwith stepsizes 71, 72 > 0. Notice that our results are able to handle imbalanced step-sizes, i.e., 71 \u2260 72 in Hayou et al. (2024)."}, {"title": "3 Analysis of LoRA under Linear Model", "content": "In this section, we establish the alignment between LoRA and one gradient of full fine-tuning. This result guides us to design new strategies for speeding up practical LoRA-based algorithms, which achieve this alignment at initialization.\nWe formally define the negative gradient of full fine-tuning in Eq. (2.2) for the linear setting after the first step as\nGt:= -VwL(W1) = 1/N X (Y-XW1).\nNote that XX is a non-singular square matrix by Zeng and Lee (2023, Lemma 6). Since left multiplication by a non-singular square matrix does not change the rank by Horn and Johnson (2012, 0.4.6 (b)), we have Rank(G) = Rank(\u25b3) = r*. Denote by {\u00bf(Gt)}i=1d the singular values of Gh in non-increasing order."}, {"title": "3.1 Alignment under LoRA Initialization", "content": "We first present the results for the alignment of Bt by recalling the notations V*(\u00b7) and V,*,(\u00b7).\nTheorem 3.1 (Alignment between G\u0127 and Bt). Under assumptions in Section 2.1 for the linear setting, consider the LoRA updates (2.3) with (LoRA-init). We have\nVT r*,1 (Gt) V (Bt)||op = 0, \u2200t\u2208N+.\nOne can see that, due to the zero initialization of Bo in (LoRA-init), after the first GD step, it holds that B\u2081 = 71 AG\u00b9, which has rank < r* and lies in the right top-r* singular subspace of Gh. The subsequent GD dynamics of Bt is always restricted to this invariant subspace.\nNext we build the alignment for At with the notations Ur*(\u00b7), Ur*,1(\u00b7) and k\u00b9 as the condition number of G.\nTheorem 3.2 (Alignment between G and At. Simplified version of Theorem B.9). For the r > 2r* case, under assumptions in Section 2.1 for the linear setting, we consider the LORA updates (2.3) with [Ao]ij ~ N(0, a\u00b2) in (LoRA-init). Then for any constant \u03b8 \u2208 (0,1), by taking\n\u03b1= O(\u03bbr\u2217+1dr\u2217||G+||F4\u03ba) , and running gradient descent for t* steps with\nt\u2217 <ln(1/0)/ln(1+\u221a\u03b71\u03b72\u03bbr\u2217(G))"}, {"title": "3.2 Spectral Initialization for Global Convergence", "content": "Theorem 3.2 has demonstrated the alignment on the rank-r* singular space of Gh and (At, Bt). In other words, if we take the SVD of Gh and choose the certain singular subspace for initialization in (Spectral-init), we can directly achieve the alignment at the initialization (without training) and recover A to some extent, which is the main target of this work.\nBy the following standard concentration result for (sub)-Gaussian data: with probability at least 1 \u2013 2C exp(-\u20ac\u00b2N) for some constants C > 0, we have\n-Idop\u2264\u03f5:=min{c/\u03ba,1/2\u043a}=c/\u043a.\nRecall is the condition number of \u2206 and X** is the r*-th singular value of \u2206, we have the following result at the spectral initialization.\nProposition 3.3. [One-step gradient suffices] Under assumptions in Section 2.1 for the linear setting via (Spectral-init), taking e in Eq. (3.4), then with probability at least 1 2C exp(-62N) for constant C > 0, we have\n||AoBo - A||op \u2264 \u20ac||A||op \u2264\u03bb\u2217r/2.\nProposition 3.3 demonstrates that, after one-step full gradient, i.e., using spectral initialization (Spectral-init), AoBo is able to recover A with small error. Besides, under (Spectral-init), the alignment between \u2206 and Bt in Theorem 3.1 via (LoRA-init) can be simplified as below.\nLemma 3.4. Under assumptions in Section 2.1 for the linear setting, and spectral initialization (Spectral-init), we always have B\u0142V\u2081 = 0d\u00d7(d\u2212r*) for any t \u2208 N+, where V\u2081 comes from the complete SVD of A in Eq. (2.1).\nLemma 3.4 shows that Bt's dynamics always stays in the low-dimensional target (right singular) subspace under the spectral initialization, which contributes to track the behavior of ||AtBt - A||op In this regime, there is no significant difference on setting different step-size 71 and 12. For ease of description, we set n\u2081 = n2 := \u03b7 for the later analysis.\nTheorem 3.5 (Global convergence. Simplified version of Theorem B.17). Under assumptions in Section 2.1 for the linear setting, suppose we use the initialization scheme (Spectral-init), and take e in Eq. (3.4) and r > r*, \u03b7 = O(1/\u03ba\u03bb\u2081). Then with probability at least 1 \u2013 2C exp(-\u20ac\u00b2N) for a universal constant C > 0, we have\n||AtBt - A||F < 1/64K(1-\u03b7/4)t \u03bb\u2217r/\u221a2"}, {"title": "3.3 Preconditioned GD under Spectral Initialization", "content": "By Theorem 3.5, the linear convergence rate heavily depends on \u03ba. The convergence will be slow if the downstream feature shift A is ill-conditioned (i.\u0435., \u043a is extremely large). This motivates us to add preconditioners, which is a key technique to accelerate convergence in matrix factorization/sensing (Tong et al., 2021; Zhang et al., 2021, 2023; Jia et al., 2024). We apply to analysis of LoRA as well as algorithm design. In the over-ranked setting (r > r*), B+B+ and AT At are not necessarily invertible. Hence we add the following preconditioners to vanilla GD (2.3)\nAt+1 = At - \u03b7\u03a3 (\u0391\u03b9\u0392\u03b9 \u2013 \u0394) (B) (BB), \nBt+1= \u0392\u03b9\u2212\u03b7 (\u0391\u03b9+\u0391\u03b9) A\u2020\u03a3(A+B+ \u2212 \u0394),\nwhere Mt denotes the pseudo-inverse of a matrix M. Such modified preconditioners are also considered in Li et al. (2024). Under (Spectral-init), similar to Lemma 3.4, the dynamics of Bt under precondition GD are still limited to the r*-dimensional singular subspace V of \u2206, i.e., BtV\u2081 = 0rx(k-r*); see the proof in Lemma B.18 in the appendix. We also have the following linear convergence under preconditioners.\nTheorem 3.6. Under assumptions in Section 2.1 for the linear setting, using precondition GD in Eq. (3.5) under spectral initialization (Spectral-init), we choose \u20ac<min{0.5\u22122\u03f5/(1+\u03f5)2,1/2\u221ar\u2217\u03ba4} and set\u03b7\u2208 (0,0.5\u22122\u03f5/(1+\u03f5)2), then with probability at least 1-2Cexp(\u2212\u03f52N) for a universal constant C>0, we have\n||AtBt - A||F \u2264 (1 \u2212\u03f5/2 (\u03ba\u03bb) \u03b7t/4 )\u03bb\u2217r/\u221a2, \u2200t\u22650."}, {"title": "4 Analysis of LoRA under Nonlinear Models", "content": "Now we focus on the nonlinear setting described in Section 2, where we consider the exact-rank case r = r* for delivery. We will demonstrate the linear convergence rate in the linear setting can still hold for the nonlinear setting.\nFollowing Section 3.3, we continue to consider preconditioned GD on (At, Bt) with the same"}, {"title": "5 Algorithm and Experiments", "content": "In this section, we firstly present our algorithm, LoRA-One, and compare with previous gradient alignment based algorithm for fine-tuning. Then we evaluate LoRA-One against typical LoRA based algorithms across multiple NLP benchmarks and conduct an ablation study.\nAlgorithm and Discussion: We formally present our LoRA-One algorithm in Algorithm 1, which is driven by spectral initialization (Spectral-init) (shown in line 4-6) and the pre-condition strategy (shown in line 10-11). To ensure numerical stability, we slightly modify (Spectral-init) in our Algorithm 1 (shown in line 5-6) inspired by the trick in Wang et al. (2024a). This is because Sc is highly ill-conditioned and numerically unstable, and has some difficulty for hyperparameter search in practice, see the Ablation Study part.\nWe remark that our initialization strategy in Algorithm 1 (line 4-6) shares some similarity with gradient alignment work, e.g., LoRA-GA (Wang et al., 2024a), LoRA-pro (Wang et al., 2024b), but the mechanisms for gradient alignment differ significantly.\nMore importantly, the spirit of LoRA-GA might not help recover \u2206, as verified by our theory as well as the empirical results in Fig. 2, where LoRA-GA fails to generalize and remain at a high-risk level throughout training. We provide more discussion in Appendix E. Furthermore, we notice that Zhang and Pilanci (2024) propose to add preconditioners to AdamW (Loshchilov, 2017) in the view of stability. In contrast, our focus is on addressing the potential issue of ill-conditioning in the downstream tasks, which is theoretically proven to accelerate convergence, as demonstrated in Theorems 3.6, 4.3 and 4.4.\nExperiments on NLU tasks: We evaluate Algorithm 1 on multiple natural language understanding (NLU) benchmarks, e.g., MNLI, SST2, COLA, QNLI, and MRPC via a comprehensive comparison"}, {"title": "6 Conclusion", "content": "This paper theoretically demonstrates how LoRA can be improved from our theoretical analysis in both linear and nonlinear models: the alignment between LoRA's gradient update (At, Bt) and the singular subspace of G\u00b9, and adding preconditioners. Our theory clarifies some potential issues behind gradient alignment work and the theory-grounded algorithm, LoRA-One, obtains promising performance in practical fine-tuning benchmarks."}]}