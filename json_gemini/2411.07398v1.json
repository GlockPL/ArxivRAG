{"title": "Beyond Keywords: A Context-based Hybrid Approach to Mining Ethical Concern-related App Reviews", "authors": ["Aakash Sorathiya", "Gouri Ginde"], "abstract": "With the increasing proliferation of mobile applications in our everyday experiences, the concerns surrounding ethics have surged significantly. Users generally communicate their feedback, report issues, and suggest new functionalities in application (app) reviews, frequently emphasizing safety, privacy, and accountability concerns. Incorporating these reviews is essential to developing successful products. However, app reviews related to ethical concerns generally use domain-specific language and are expressed using a more varied vocabulary. Thus making automated ethical concern-related app review extraction a challenging and time-consuming effort.\nThis study proposes a novel Natural Language Processing (NLP) based approach that combines Natural Language Inference (NLI), which provides a deep comprehension of language nuances, and a decoder-only (LLaMA-like) Large Language Model (LLM) to extract ethical concern-related app reviews at scale. Utilizing 43,647 app reviews from the mental health domain, the proposed methodology 1) Evaluates four NLI models to extract potential privacy reviews and compares the results of domain-specific privacy hypotheses with generic privacy hypotheses; 2) Evaluates four LLMs for classifying app reviews to privacy concerns; and 3) Uses the best NLI and LLM models further to extract new privacy reviews from the dataset. Results show that the DeBERTa-v3-base-mnli-fever-anli NLI model with domain-specific hypotheses yields the best performance, and Llama3.1-8B-Instruct LLM performs best in the classification of app reviews. Then, using NLI+LLM, an additional 1,008 new privacy-related reviews were extracted that were not identified through the keyword-based approach in previous research, thus demonstrating the effectiveness of the proposed approach.", "sections": [{"title": "I. INTRODUCTION", "content": "Mobile applications are created with specific user goals in focus [1]. A user goal can be defined as any conceptual aim the given system should fulfill [2]. For instance, Sharing Economy applications (like Uber and Airbnb) aim to enhance social capital and stimulate economic development in resource-limited areas [3]. In contrast, the goal of Health&Fitness applications is to encourage healthy habits among both children and adults [4]. However, due to intense market rivalry, the app development cycle often aims to produce functional applications within brief intervals (such as days or weeks), leading developers to stray from their initial objectives frequently. These divergences frequently bring forth ethical concerns such as declining mental health, bias, privacy violations, and manipulation [5]-[8]. Applications that fail to sufficiently consider their users' ethical concerns are often labeled as untrustworthy or even deserted by their users [9]. Thus, for applications to endure the market's scrutiny, developers continuously keep track of user feedback through ratings and reviews found in app marketplaces (like Google Play Store). They typically analyze user feedback to gather insights on bug reports, feature suggestions, connectivity issues, resource consumption challenges (e.g., battery life), and interface problems [10]-[13].\nNumerous studies have investigated user perspectives on ethical concerns within software applications. Research conducted by Besmer et al. [14] and Nema et al. [15] underscores users' concerns regarding privacy breaches and data security measures in mobile applications. The emergence of discriminatory algorithms and the potential for bias in software functionalities are also significant areas of concern, as highlighted by the findings of Tushev et al. [16] and Olson et al. [17]. Furthermore, manipulative design tactics that coerce users or take advantage of psychological weaknesses are increasingly worrisome, as noted by Olson et al. [18]. However, these investigations largely depend on keyword-based sampling from app reviews, which limits the ethical issues users address to a predetermined set of terms.\nTo overcome this limitation, Harkous et al. [19] suggest using the NLI method. However, they rely on a set of generic privacy hypotheses (derived from generic privacy concepts) overlooking the fact that users' ethical concerns are domain-dependent [1]. For instance, individuals using ridesharing services (e.g., Uber and Lyft) may raise concerns about the constant tracking of their location, while those utilizing financial platforms (e.g., Robinhood and Coinbase) might express concerns regarding the sharing of their social security or banking details with the application. Additionally, NLI with generic hypotheses identifies a high number of false positives (FP) that require further manual analysis to identify ethical concern-related reviews [19].\nTo address these challenges, in this paper, we propose"}, {"title": "II. RELATED WORK", "content": "App Reviews: Numerous scholarly studies have assessed the significance of user feedback within app reviews [10], [12], [21], [22]. Noteworthy contributions from researchers such as Pagano et al. [21] and Khalid et al. [22] have explored app review classification comprehensively. However, these classifications are rather abstract, encompassing categories such as \u201ccommendation", "utility\u201d, \u201cissue reporting": "and \"feature suggestion\" [21], and \"operational failure\u201d, \u201ccompatibility", "user interface design\" [22].\nFurthermore, an investigation conducted by Lu and Liang [23], utilizing the categorizations established by the International Organization for Standardization (ISO), delineated six distinct review types based on their thematic focus": "Usability, Reliability, Portability, Performance, Feature Request (denoting \u201cCapabilities that a system/product ought to possess", "study": "Harkous et al. [19] leveraged NLI for this task and showed the limitations of using a keyword-based approach with a pre-defined set of keywords. However, their approach is based on a set of hand-crafted generic privacy hypotheses, which once again is a limitation since users' ethical concerns are domain-dependent [1]. Additionally, NLI flags a high number of FP which requires a large amount of manual work to extract relevant reviews [19]. To overcome these limitations, in this paper, we utilize domain-specific privacy hypotheses to create a set of privacy hypotheses for NLI to identify potential ethical concern-related app reviews.\nLarge Language Models (LLMs): LLMs are categorized into three groups based on their architecture structure: 1) encoder-only LLMs, (Eg: BERT) 2) encoder-decoder LLMs (Eg: ROBERTA), and 3) decoder-only LLMs (Eg: LLaMA) [32]. Encoder-only LLMs only use the encoder to encode the sentence and understand the relationships between words. The common training paradigm for these models is to predict the mask words in an input sentence [32]. Encoder-decoder LLMs adopt both the encoder and decoder module. The encoder module is responsible for encoding the input sentence into a hidden space, and the decoder is used to generate the target output text [32]. Decoder-only LLMs only adopt the decoder module to generate target output text. The training paradigm for these models is to predict the next word in the sentence [32].\nRecently, LLMs have been widely utilized, due to their ability to solve various problems in the domain of software engineering (SE), where they are currently employed in a multitude of applications, such as testing, code generation, and code summarization [32]. Historically, conventional SE"}, {"title": "III. MOTIVATION", "content": "Extracting ethical concerns-related reviews through manual inspection is a laborious task since app reviews for any mobile app appear in large numbers. Conversely, recent advances in automated requirements extraction rely solely on keyword matching techniques utilizing ML machine learning (ML) and deep learning (DL) methodologies [18]. The drawback of keyword matching techniques is, that the set of keywords associated with ethical concerns is curated based on a set of pre-identified generic (context-independent) keywords associated with ethical concerns. Although this technique appears to be more efficient than the manual alternative, there are several limitations to this method: the keyword-matching technique fails to account for the fact that keywords designated for specific ethical concerns may not align with the terminology utilized by users in their reviews [31]. Such discrepancies may arise, for instance, from typographical errors made by users. Additionally, the mere occurrence of certain keywords within a review does not inherently imply that the review addresses any ethical concern. For instance, consider the following review extracted from the dataset compiled by Ebrahimi et al. [1]:\n\"...I paid 189$ for 1 month of couples therapy. she then provided me a link for my husband to join us in the consult private room. the first link did not work at all. the second one she provided took him to a different consultant...\"\nThis review contains the term \u201cprivate\u201d, which was considered in the original compilation of keywords to delineate reviews pertinent to privacy concerns [1]. However, in this context, the term \u201cprivate\u201d pertains to the private consultation room and is not associated with privacy-related concerns. Consequently, the identification of reviews concerning privacy issues is significantly dependent on contextual interpretation; thus, merely conducting searches for related keywords within the review text might not be an effective approach.\nHarkous et al. [19] employed the NLI task [47] to mitigate the constraints associated with keyword-based search methodologies. They performed an extensive investigation into the privacy concerns articulated by users in app reviews, leveraging the concepts defined in the established privacy taxonomies [48], [49]. Utilizing these concepts, they formulated 31 privacy hypotheses, which were subsequently applied to the NLI task, aiming for comprehensive coverage of various dimensions within the privacy domain, irrespective of the linguistic variations present in app reviews. Despite addressing the limitations of keyword-based search, this methodology solely relies on generic privacy concepts rather than domain-specific privacy frameworks. Thus, overlooks the fact that users tend to articulate their ethical concerns using a more varied language, unlike specific terminologies, generally used while mentioning concerns related to technical aspects of the application [50].\nFor instance, consider the following three reviews selected from the domains of MH, finance, and food delivery applications derived from the dataset [1]. The term Facebook signifies a privacy-related concern within the MH domain. Conversely, in the food delivery context, the same term denotes a customer support issue, while in the finance sector, it pertains to a user registration concern.\nMental Health: \"Won't even let me sign up after collecting all of my Facebook data, just stole my identity.\"\nFinance: \"I got zero response back. I even blasted their Facebook but got nothing.\"\nFood Delivery: \"It doesn't recognize my facebook account so I can't even register for this.\"\nIn addition, there are a variety of app domains, each with a set of particular requirements [51] that collect different types of data. For example, data in the MH domain involves sensitive personal information such as emotional states, therapy progress, and medical history [52], while the finance domain handles financial/investment-related data and transactions [53]. Consider the following two reviews selected from the domains of MH and finance applications from the data set [1]. The MH app review expresses concern regarding private medical data being linked with Facebook whereas the investing app review highlights the concern regarding confidential banking details being collected.\nMental Health: \"You have to have a facebook account that steals all of our information including our medical registries\"\nFinance: \"App asked for my bank login to verify the account. did not offer any other solution. i'm not giving my login info to a third party, so i'll just put my money in webull.\""}, {"title": "IV. RESEARCH QUESTIONS (RQS)", "content": "Our three RQs are as follows:\nRQ1. To what extent can NLI accurately identify potential ethical concern-related app reviews?\nWe aim to investigate whether we can use NLI with domain-specific privacy hypotheses to flag the potential concern-related app reviews. These reviews can contain FP, but leveraging NLI filters the large set of unrelated app reviews. NLI has already been utilized by Harkous et al. [19] for identifying potential reviews but our purpose is to show that domain-specific hypotheses yield better results than generic hypotheses used by [19].\nRQ2. To what extent we can leverage LLaMA-like LLMS to classify ethical concern-related app reviews?\nNLI identifies a high number of FP which necessitates further manual analysis to identify relevant ethical concern-related reviews [19]. To reduce this manual effort we aim to investigate the efficiency of leveraging LLaMA-like LLMs for identifying relevant reviews. LLaMA-like LLMs have been employed in the SE domain for a variety of tasks and have shown encouraging results [32].\nRQ3. How effective is our approach in identifying ethical concern-related reviews as compared to the keyword-based approach?\nAfter evaluating NLI and LLM individually, we select the best-performing models and compare our hybrid approach with the keyword-based approach. We aim to evaluate NLI+LLM for extracting ethical concern-related reviews that do not contain predefined wordings used in the keyword-based method. We utilize the dataset from the previous study [1] and extract new concern-related reviews that were missed by that study based on the keyword approach."}, {"title": "V. PRELIMINARIES", "content": "Natural Language Inference (NLI): NLI pertains to the problem of ascertaining whether a natural language hypothesis can logically be derived from a specified premise [47]. An NLI model is required to evaluate whether a hypothesis is true (i.e. entailment), false (i.e., contradiction), or undetermined (i.e., neutral) in relation to a given premise. For instance, consider a premise stating, \"...collecting all of my Facebook data, just stole my identity...\". A hypothesis asserting, \u201ctoo much personal data is collected\" would be assigned an entailment label. Conversely, a hypothesis claiming \"user likes that data privacy is provided\" would be designated a contradiction label, and a hypothesis positing \"app has a good interface\" would be assigned a neutral label.\nMoreover, this methodology mitigates the dependency on specific keywords due to the extensive linguistic variability present in the premises associated with the hypotheses. For instance, both of the following reviews receive an entailment label for the hypothesis \"The user is not aware of how and why their data is being collected, processed, stored, and shared.\":\n\u2022 \"Don t bait people in to take their information and sell it and add them to your mailing list\" (P(entailment)=0.76)\n\u2022 \"This app has data trackers don t trust any app with your wellbeing that is sending your behavior data to multiple third parties\" (P(entailment)=0.87)\nNote that no review has any words in common with hypotheses, but both of them discuss the concern related to data collection and sharing. Here, P(entailment) denotes the probability of the entailment label and is referred to as entailment_score. We use these scores to filter out the potential reviews based on the defined heuristics.\nLarge Language Models (LLMs): LLMs based on the transformer architecture [54] have introduced a significant advancement in the field of NLP [33], [55]. LLaMA-like LLMs have demonstrated the power and versatility of the transformer architecture when scaling up the number of parameters [56]. In particular, they exhibit emergent abilities that arise suddenly at large scales and cannot be extrapolated from smaller models. The mechanisms behind emergence are not fully understood, but hypothesized factors include model capacity, depth, and ability to leverage huge amounts of pre-training data [57].\nMany of those models, after their pre-training phase, are further trained to follow instructions through Reinforcement Learning for Human Feedback (RLHF) [58], a technique for training models to align with human goals by providing feedback in the form of rewards [59]. This additional fine-tuning makes them a better choice for many NLP tasks because pre-trained models are excellent at completing the text when given an initial prompt, however, they are not ideal for NLP tasks where they need to follow instructions [58]. Due to these advantages, we decided to utilize a fine-tuned (instruct) version of LLMs to reduce the manual effort of identifying concern-related app reviews."}, {"title": "VI. DATASET", "content": "We utilize the ground truth data (manually validated), consisting of 1,376 privacy reviews from Ebrahimi et al. [1] in this study. This particular dataset was developed through the application of keyword-matching filtering alongside manual inspection of over 204K reviews mined from the most widely used Mental Health (MH) applications available on the Google Play Store and Apple App Store. Although the raw dataset consisted of reviews from three application domains: MH, finance (investment), and food delivery; for this study, we exclusively focused on reviews pertinent to MH applications. Mainly because of a notable increase in the number of active users of MH applications as a consequence of the COVID-19 pandemic [1] in the recent past. Individuals increasingly turn to these applications as a safer and more cost-effective means of addressing the psychological ramifications of social isolation, unemployment, and economic distress [60].\nTo collect the app reviews from the MH domain, the authors identified the top 100 apps in the Health&Fittness (MH) category on Google Play and the Apple App Store. Only the apps with 5,000 or more reviews were considered to include only popular and well-established apps. Additionally, physical health apps that did not explicitly support mental health were excluded. After examining the top 100 apps, five MH apps were selected for the analysis of reviews. For each of these apps, they collected all textual reviews available on the Apple App Store and Google Play using Python web scrapers."}, {"title": "VII. METHODOLOGY", "content": "Algorithm 1 and Figure 1 provide an overview of our research methodology. Our approach consists of three parts: (1) NLI-inference: we identify the best NLI model and the best set of hypotheses to extract potential privacy-related app reviews; (Algorithm 1) (2) LLM-inference: we then compare the performance of various LLaMA-like LLMs to classify potential reviews to privacy concerns and identify the best-performing LLM; (3) Finally, we combine NLI from RQ1 (NLI-inference) and LLM from RQ2 (LLM inference) to extract new privacy-related reviews (Figure 1).\nWe detailed the methods employed in each component in the following subsections.\n1) NLI Inference: Algorithm 1 describes the proposed NLI inference process. Using the existing 31 generic hypotheses (our hypotheses baseline) and the corresponding heuristics from Harkous et al. [19], and ground truth dataset from Ebrahim et al. [1] (lines 1-7), we first determine the best NLI model (lines 8-16) of the four chosen NLI models namely: Roberta-large-mnli, Nli-roberta-base, DeBERTa-v3-base-mnli-fever-anli and T5-base using Precision (P), Recall (R), and F1-score as measures. We performed 1,376 (number of app reviews) * 31 (generic hypotheses) * 4 (number of models) = 170,624 inference operations at this stage.\nNext, to determine the best hypotheses, we compare the performance of the generic hypotheses (baseline) with the newly defined domain-specific hypothesis and respective corresponding heuristics (lines 17-27), which were determined manually. We performed 1,376 (number of app reviews) * 21 (domain-specific hypotheses) = 28,896 inference operations. In the end (line 28), we use the best NLI model and best hypotheses with their corresponding heuristics to create a pseudo-labeled corpus containing 'maybe-privacy' and 'maybe-not-privacy' labels. This pseudo-labeled corpus is further used for the evaluation of LLMs in RQ2.\nGeneric hypotheses and corresponding heuristics (Baseline for RQ1): We use the generic privacy hypotheses and corresponding heuristics provided by Harkous et al. [19] as a baseline for RQ1. Harkous et al. defined 31 generic hypotheses based on Solove's [48] taxonomy of privacy violations and the taxonomy of privacy-enhancing technologies proposed by Wang and Kobsa [49]. Further, they define the following heuristics where $NE(i, t)$ is the number of hypotheses receiving an entailment score above a threshold $t$ for review $i$:"}, {"title": "VIII. RESULTS", "content": "This section presents and discusses the results of our investigation. For each RQ, we present the results of the analysis, and we discuss the findings.\nRQ1 NLI (Domain-specific hypotheses vs Generic hypotheses (Baseline)): First, we evaluate four NLI models using the baseline generic privacy hypotheses and select the best NLI model based on the highest F1-score.  shows the inference results for the generic hypotheses. DeBERTa-v3-base-mnli-fever-anli is the best-performing model with the highest F1-score of 0.5. It can be observed that all the models have low P values as NLI identifies the high number of FP. DeBERTa-v3-base-mnli-fever-anli annotated 1130/1376 reviews as \u2018maybe-privacy' along with achieving the goal of minimizing FP and FN. It annotated only 25 1-labeled reviews as maybe-not-privacy' and 741 0-labeled reviews as 'maybe-privacy'. Additionally, it can be noted that the resultant metrics of the T5-base model are 0 as it has 0 TP and FP.\nNext, we compare the inference results of the domain-specific hypotheses with generic hypotheses using the best-performing DeBERTa-v3-base-mnli-fever-anli model. shows the findings indicating that domain-specific hypotheses yield better results as compared to generic hypotheses. We achieved an F1-score of 0.54 with domain-specific hypotheses which shows an improvement of 1.08 times as compared to the generic hypotheses. This improvement is promising in terms of FP as we identified only 568 FP in the case of domain-specific hypotheses whereas this count was comparatively higher (741) for generic hypotheses.\nSummary of RQ1: The DeBERTa-v3-base-mnli-fever-anli NLI model with domain-specific hypotheses and corresponding heuristics performs best in extracting potential concern-related app reviews. It achieves an F1-score of 0.54 and 1.08 times improvement compared to the baseline generic hypotheses.\nRQ2 - LLAMA-like LLM vs RC (Baseline): In LLM inference, we use the 926 'maybe-privacy' reviews from the pseudo-labeled corpus of the DeBERTa-v3-base-mnli-fever-anli model with domain-specific hypotheses. For our baseline, we use the statistics of our dataset to calculate the metrics. The precision of the baseline RC is computed by dividing the number of privacy reviews by the total number of reviews (i.e., $358/926 = 0.38$). Regarding recall, there is only a 50% probability for a review to be classified as a privacy review since there are two possible classifications available. Finally, the F1-measure of baseline RC is calculated as $2 * (0.38 * 0.5)/(0.38 + 0.5) = 0.43$.\n shows the P, R, and F1-score of all the LLMs and the baseline RC along with the improvement in the F1-score of LLMs as compared to RC. These results highlight that the Llama3.1-8B-Instruct model achieved the best performance with an F1-score of 0.81 and an improvement of 1.86 times as compared to the RC. While the Llama-3-8B-Instruct model is the second best performing model with an F1-score of 0.69, the falcon-7b-instruct model achieved the highest recall of 0.95.\nSummary of RQ2: Llama3.1-8B-Instruct LLM shows the best performance for extracting concern-related reviews from the potential set of reviews with an F1=0.81 and 1.86 times improvement as compared to the baseline RC.\nRQ3 NLI+LLM vs Keyword-matching (Baseline): To extract the new set of privacy-related reviews from the dataset of 42,271 unlabeled reviews, we use the DeBERTa-v3-base-mnli-fever-anli model (best-performing NLI model) with the domain-specific hypotheses from RQ1 and Llama3.1-8B-Instruct LLM (best-performing LLM) with the prompt from RQ2. After data preprocessing, we executed the NLI inference and identified 6,591 'maybe-privacy' reviews. These reviews were then used in LLM inference operation, and 1,654 reviews were further labeled as 'yes' by the LLM indicating the privacy-related reviews. After this, we performed the manual inspection and created a dataset of 1,008 privacy-related reviews that were not extracted by the previous study [1] using keyword-based filtering. We show a few of the reviews below and make the whole dataset publicly available.\nReview 1: \"Don't bait people in to take their information and sell it and add them to your mailing list then force a paywall to use the app\"\nReview 2: \"How are you different from any other app now that is interested in our user patterns over our mental health?\"\nReview 3: \"This app has data trackers don t trust any app with your wellbeing that is sending your behavior data to multiple third parties\"\nAll these reviews mention privacy concerns but they do not contain any predefined set of keywords and are also specifically related to MH domain. This shows the importance of using NLI (with domain-specific hypotheses) and LLM to extract the ethical concern-related app reviews.\nSummary of RQ3: 1,008 new privacy-related reviews were extracted using the best-performing NLI model with domain-specific hypotheses and the best LLM."}, {"title": "IX. THREATS TO VALIDITY", "content": "The study presented in this paper has several limitations that could potentially limit the validity of the results.\nConstruct threats: The potential threat to the construct validity of our study is related to the appropriateness of the study dataset and our manually created dataset. Developing a dataset is a tedious job and also subject to reader bias. We mitigated this risk by choosing a dataset of privacy reviews that were previously identified and validated through manual inspection by Ebrahimi et al. [1]. Additionally, for curating a new dataset we employed a methodological approach for manual inspection, including four annotators to mitigate the risk of an individual bias.\nFurther, we utilize four NLI models and four LLMs with three evaluation metrics. Hence, we accept that applying other models to our dataset may lead to different results. The metrics P, R, and F1-score used in this study are widely applied and suggested to evaluate such models in SE.\nInternal threats: The process of defining domain-specific privacy hypotheses and corresponding heuristics, and designing the prompt for LLMs may introduce some threats to"}, {"title": "X. CONCLUSION AND FUTURE WORK", "content": "We present an NLI+LLM-based approach that enables developers to proficiently discern ethical concerns associated with their applications and enhance them towards being more trustworthy and responsible by leveraging user feedback. Our objective is to foster sustainable change by integrating a model within the software development lifecycle of developers, while simultaneously elevating awareness regarding pre-existing ethical issues that obstruct the usability of mobile applications, which represents an urgent necessity [80].\nWhile formulating our methodology, we undertook a comprehensive evaluation encompassing three distinct phases to address our three RQs and demonstrate the effectiveness of our approach. In the first phase, we evaluated four different NLI models to extract potential privacy reviews and compared the results of domain-specific privacy hypotheses with the generic privacy hypotheses. Our results showed that the DeBERTa-v3-base-mnli-fever-anli NLI model with domain-specific privacy hypotheses offered the best performance in extracting potential concern-related app reviews.\nIn the second phase of our analysis, we evaluated four LLaMA-like LLMs to classify concern-related reviews from the set of potential reviews. Our analysis showed that the Llama-3.1-8B-Instruct LLM was the best-performing model with an F1-score of 0.81. In the final phase of our analysis, we used NLI+LLM to extract new 1,008 privacy-related reviews from the dataset that were not extracted by the previous study using a keyword-based approach.\nIn future work, we intend to (i) leverage topic modeling to automatically identify the main topics addressed by users in concern-related reviews; (ii) create a user-friendly and interactive tool for developers to extract concern-related reviews and summarize them easily; (iii) automatically extract requirements from the concern-related reviews which can be directly addressed and implemented in the development phase; Moreover (iv) devise an interactive guide in which practitioners can explore concern-related topics and navigate through relevant reviews to understand the evidence for each recommendation."}]}