{"title": "Embedding Style Beyond Topics:\nAnalyzing Dispersion Effects Across Different Language Models", "authors": ["Benjamin Icard", "Evangelia Zve", "Lila Sainero", "Alice Breton", "Jean-Gabriel Ganascia"], "abstract": "This paper analyzes how writing style affects\nthe dispersion of embedding vectors across mul-\ntiple, state-of-the-art language models. While\nearly transformer models primarily aligned\nwith topic modeling, this study examines the\nrole of writing style in shaping embedding\nspaces. Using a literary corpus that alternates\nbetween topics and styles, we compare the sen-\nsitivity of language models across French and\nEnglish. By analyzing the particular impact\nof style on embedding dispersion, we aim to\nbetter understand how language models pro-\ncess stylistic information, contributing to their\noverall interpretability.", "sections": [{"title": "1 Introduction", "content": "In recent years, large language models (LLMs)\nhave shown advanced natural language process-\ning capabilities across diverse tasks, making their\nexplainability an important area of research (Zhao\net al., 2024). A key aspect of these models is their\nability to generate meaningful text representations\nthrough vector embeddings, that encode semantic\ninformation. Although topic modeling along em-\nbedding representations has been widely studied\n(Peinelt et al., 2020), the influence of writing style\non these representations has received less attention\n(Terreau et al., 2021; Chen et al., 2023). By lever-\naging sophisticated neural architectures (Achiam\net al., 2023; Jiang et al., 2023), current large-scale\nmodels, such as those developed by OpenAI and\nMistral, provide new investigative paths in that re-\nspect.\nThis paper aims to provide deeper insights into\nhow different language models encode writing\nstyle and study their sensitivity to stylistic features.\nSpecifically, we seek to examine the relative im-\npact of style versus topic on the spatial dispersion\nof embedding vectors, across different language\nmodels in both French and English. Our primary\nfocus is on the particular influence of writing style,\nwith an emphasis on explainability.\nTo conduct this analysis, we designed an ex-\nperimental study that systematically interchanged\ntopic and style dimensions using text generation\ntechniques. We selected two established literary\nworks as raw material: Raymond Queneau's Exer-\ncices de Style (Queneau, 1947) and F\u00e9lix F\u00e9n\u00e9on's\nNouvelles en trois lignes (F\u00e9n\u00e9on and Halperin,\n1970). Exercices de Style is a highly original piece\nof experimental literature in which Queneau writes\nnumerous stylistic variations of a single narrative (a\nbrief confrontation between bus passengers) while\nmaintaining the same topic across all versions. By\ncontrast, Nouvelles en trois lignes covers a wide\nrange of topics (e.g., political events, crime, na-\nture) while keeping a consistent style marked by\na vivid and ironic tone. To enrich this material,\nwe employed text generation techniques. We cre-\nated a corpus where Queneau's style aligns with\nF\u00e9n\u00e9on's unique style, and another corpus where\nF\u00e9n\u00e9on's style varies in line with Queneau's plu-\nrality of styles. This design aimed to effectively\nassess the impact of topic and style on embedding\ndispersion.\nSection 2 reviews existing work on the com-\nputational analysis of writing style, contrasting it\nwith topic modeling, with an emphasis on vector\nembedding techniques. Section 3 describes the\nQUENEAU-FENEON dataset, a collection of tex-\ntual documents compiled for topic and style exper-\nimentation, employing a specific text generation\nmethodology. Section 4 describes the experimental\ntasks and results obtained on this dataset, includ-\ning clustering to assess alignment with predefined\nclasses, analysis of how style and topic influence\nembedding dispersion, and the identification of key\nlinguistic features that may explain this dispersion.\nFinally, Section 5 concludes our investigation and\noutlines directions for future work."}, {"title": "2 Related Work", "content": "Embedding vector representations have been pri-\nmarily studied in the context of topic modeling,\nbuilding on BERT studies (Devlin et al., 2018).\nTechniques using word embeddings or sentence\nvectors, such as SBERT (Reimers and Gurevych,\n2019), were developed to extract topics from tex-\ntual documents, outperforming traditional statisti-\ncal topic modeling methods like Latent Dirichlet\nAllocation (LDA) (Blei et al., 2003). A notable\nadvancement in that respect is BERTopic, which re-\nfines these different methods (Grootendorst, 2022).\nRecent progress has focused on combining tradi-\ntional methods like LDA with word embeddings,\nresulting in improved topic quality metrics and in-\nterpretability (Dieng et al., 2020).\nCurrently, research on embedding vector rep-\nresentations of writing style remains relatively\nunderexplored compared to topic modeling (Dai\net al., 2019). Existing computational and statisti-\ncal approaches to writing style (Herrmann et al.,\n2021), including stylometry, focus on features such\nas word frequency, part-of-speech tags, N-grams\n(R\u00edos-Toledo et al., 2022), specific lexical entries\nand punctuation (Faye et al., 2024; Icard et al.,\n2024), TF-IDF (Bui et al., 2011), and vector em-\nbeddings (Chen et al., 2023). From a literary per-\nspective, these approaches consider writing style\nas a manifestation of an author's unique voice and\naesthetic choices (e.g. Verma and Srinivasan, 2019;\nMani, 2022).\nIn continuation of computational stylometry, and\nenabled by transformer architecture (Hao et al.,\n2021), recent studies have examined stylistic fea-\ntures using embedding techniques (Liu et al., 2024),\nwith particular focus on literary texts (Maharjan\net al., 2019), but also in other domains, like news\nmedia and Generative AI (Bevendorff et al., 2024).\nHowever, a comprehensive approach that fully cap-\ntures the entire spectrum of writing style remains\nunderdeveloped. Terreau et al. (2021) proposed a\nnovel evaluation framework for author verification\nembedding methods based on writing style, quan-\ntifying whether the embedding space effectively\ncaptures a set of stylistic features as the best proxy\nof an author's writing style. In addition to enhanc-\ning explainability, their work revealed that recent\nmodels are mostly driven by the inner semantics of\nauthors' production and are outperformed by sim-\nple baselines on several linguistic axes. Chen et al.\n(2023) proposed a writing style embedding method\nbased on contrastive learning for multi-author writ-\ning style analysis, achieving promising results in\ndetecting style changes in multi-author documents.\nAddressing the challenge of content-independent\nstyle representations, Wegmann et al. (2022) in-\ntroduced a variation of the authorship verification\ntraining task that controls for content using conver-\nsation or domain labels, finding that representations\ntrained by controlling for conversation are better at\nrepresenting style independent from content.\nWhile a handful of studies have explored style\nembedding representations from a comprehensive\nperspective, most existing research has primarily\nfocused on analyzing how specific models encode\ntargeted stylistic features, such as syntactic and lex-\nical embeddings. This paper proposes a structured\nmethodology to compare how style versus topic\nvariation influences the embedding dispersion of\nvarious language models in both French and En-\nglish."}, {"title": "3 Dataset", "content": "To conduct this study, we compiled a corpus named\nQUENEAU-FENEON, consisting of 584 textual doc-\numents, with 292 texts in French and 292 texts in\nEnglish. The corpus was developed in two stages:\nfirst, we created a reference corpus using extant\nliterary works by Raymond Queneau and F\u00e9lix\nF\u00e9n\u00e9on; second, we created a generated corpus\nby tranforming these original texts. The generated\ncorpus was created using GPT-40,\u00b9 with a prompt-\ning methodology described below."}, {"title": "3.1 Reference corpus", "content": "We began by compiling a reference corpus of 146\ntexts in each language by uniting two symmetric\nclasses with respect to topic and style variation.\nThe first class, named QUENEAU_REF, contains 73\ntexts extracted from Raymond Queneau's Exerci-\nces de style. These texts are written in 73 different\nstyles but all deal with the same topic of a bus jour-\nney. The second class, named FENEON_REF, also\ncontains 73 texts, this time extracted from F\u00e9lix\nF\u00e9n\u00e9on's Nouvelles en trois lignes. The specificity\nhere is that these texts include numerous topics but\nall share the consistent style of Feneon.\nWe used the original French versions of the\nQueneau and F\u00e9n\u00e9on's corpus (Queneau, 1947;\nF\u00e9n\u00e9on and Halperin, 1970) to form the French\nQUENEAU_REF and FENEON_REF classes. For"}, {"title": "3.2 Generated corpus", "content": "We obtained a generated corpus by applying\nGPT-40 text generation on QUENEAU_REF and\nFENEON_REF, respectively, in both French and\nEnglish. We began by generating a class named\nQUENEAU_GEN, prompting GPT-40 to rewrite\nall 73 stories of QUENEAU_REF in the uniform\nstyle of FENEON_REF. Then, we generated a com-\nplementary class named FENEON_GEN by prompt-\ning GPT-40 to rewrite each of the 73 stories of\nFENEON_REF into one of the 73 different writ-\ning styles of QUENEAU_REF. As in the reference\nclass, the generated class contains exactly 146 texts\nequally divided into 73 texts for QUENEAU_GEN\nand 73 texts for FENEON_GEN."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Corpus validation", "content": "We performed k-means clustering on the\nembeddings of the French and English\nQUENEAU-FENEON corpus. Twelve embed-\nding models were selected in that respect, based\non the following criteria: diversity, representa-\ntiveness, dimensionality, multilingual capability,\ncomputational efficiency, explainability, and\nhigh performance according to the Massive Text\nEmbedding Benchmark (MTEB) (Muennighoff\net al., 2022) at time of the paper submission\n(September 16, 2024).\u00b2 The full list of tested\nmodels is presented in Table 2 (see section of\nsupplementary materials indicating dimensionality\nper model and URLs).\nThe clustering task aimed to assess the method-\nology used to build the QUENEAU-FENEON dataset.\nHere k-means measures the ability of the embed-\nding models to effectively capture, and distinguish,\nthe different topics and styles of our corpus. No-\ntice that our goal was not to identify the optimal\nnumber of clusters for this dataset but, this number\nk being set to 4, to determine whether the texts in\nthe four clusters align with the four classes of the\nQUENEAU-FENEON corpus.\nWe combined two popular evaluation metrics to\nassess the clustering task: Purity (Manning, 2008)\nand NMI (Normalized Mutual Information) (Danon\net al., 2005). Ranging from 0 to 1, Purity and NMI\nare external cluster evaluation metrics based on\nthe a priori knowledge of our dataset (Soni and\nDwivedi, 2024). To facilitate model comparisons\nacross languages and dimensions, we define a qual-\nitative score $S_D(m)$ which, for a given model m,\ngiven a specific dimensionality D, averages the Pu-\nrity and NMI scores of the model m for D:\n$S_D(m) = \\frac{Purity_D(m) + NMI_D(m)}{2}$"}, {"title": "4.2 Dispersion within classes", "content": "To gain a deeper understanding of how topic\nand style impact embedding representations,\nwe analyzed embeddings dispersion within the\nQUENEAU-FENEON corpus. Specifically, we\naimed to determine whether variations in topic and\nin style lead to increase or decrease dispersion. Ad-\nditionally, we aimed to evaluate the relative contri-\nbution of style versus topic to this effect.\nTo conduct this analysis, we employed the\nUniform Manifold Approximation and Projection\n(UMAP) (McInnes et al., 2018) technique for di-\nmensionality reduction of our embedding vector\nspace. UMAP was chosen over other methods,\nlike PCA previously used for clustering, due to its\nsuperior ability to preserve both local and global\nstructures within the embedding space. Addition-\nally, t-distributed Stochastic Neighbor Embedding\n(t-SNE) is effective at preserving local structures,\nbut it often distorts the global structure (Anowar\net al., 2021), making it less suitable for our specific\nanalysis. Since we aimed to ensure that local infor-\nmation was primarily preserved while also main-\ntaining an accurate global structure, UMAP was the\nmost appropriate choice, as particularly well-suited\nfor distance-based analysis of high-dimensional\ntext embedding spaces (Cox et al., 2021).\nTo account for the non-deterministic nature of\nUMAP (McInnes et al., 2018), we performed 30\napplications of the model (iterations) with different\nrandom seeds, for dimensionality reductions of 2D,\n3D, 5D, and 10D. The adoption of different random\nseeds ensures that the results are stable and not\nsensitive to specific initial conditions, providing a\nmore robust estimate of embedding dispersion.\nFor the j-th iteration, we define $d_X^{i,j}$ as the\nEuclidean distance of the i-th embedding vector\nfrom the centroid $c_X^j$ of class X as follows:\n$d_X^{(i,j)} = ||v_X^{(i)} - c_X^{(j)} ||$\nwhere $v_X^{(i)}$ is the i-th embedding vector of class\nX in the j-th iteration, $c_X^{(j)}$ is the centroid vector\nfor class X in the j-th iteration, and $|| \\cdot ||$ is the\nEuclidean norm.\nTo capture the spatial distribution of high-\ndimensional embeddings, we calculate the mean\nEuclidean distance from the centroid of each class\nacross all iterations, written $d_X(i)$:\n$d_X(i) = \\frac{1}{30}\\sum_{j=1}^{30} d_X^{(i,j)}$\nFinally, the overall mean distance dx for class\nX across all embeddings is:\n$d_X = \\frac{1}{N}\\sum_{i=1}^{N} d_X(i)$\nwhere $d_X(i)$ is the averaged Euclidean distance\nof the i-th embedding vector for class X and N is\nthe total number of embedding vectors in the class.\nIn order to analyze the influence of topic vari-\nation, we compared the difference in embedding\ndispersion between classes presenting topic homo-\ngeneity versus topic heterogeneity, i.e. by compar-\ning QUENEAU_REF with FENEON_GEN, and also\nQUENEAU_GEN with FENEON_REF. To analyze\nthe influence of style, we compared the difference\nin embedding dispersion between classes showing\nstyle homogeneity versus style heterogeneity, i.e.\nby comparing FENEON_REF with FENEON_GEN,\nand also QUENEAU_GEN with QUENEAU_REF.\nUsing the metric defined in (4), we predict that\nboth topic and writing style influence embeddings\ndispersion, as in the local hypotheses (T) and (S):\n$\\begin{array}{ll} \\text { Topic } & d_{\\text {FENEON_GEN}}>d_{\\text {QUENEAU_REF }}  (T') \\\\ & d_{\\text {FENEON_REF}}>d_{\\text {QUENEAU_GEN }}  (T'') \\\\ \\text { Style } & d_{\\text {QUENEAU_REF}}>d_{\\text {QUENEAU_GEN }} (S') \\\\ & d_{\\text {FENEON_GEN}}>d_{\\text {FENEON_REF}}  (S'') \\end{array}$  (T)\n(S)\nBesides hypotheses (T) and (S), we expect the\ntopic to have a greater impact on embeddings dis-\npersion than style, as defined in the global hypoth-\nesis (T-S):\n$d_{\\text {FENEON_REF}}>d_{\\text {QUENEAU_REF}}$    (T-S)"}, {"title": "4.3 Style embedding interpretability", "content": "In the previous section, we observed that style vari-\nation, in addition to the more common topic varia-\ntion, also influences embedding dispersion. Here\nwe attempt to identify the key stylistic features re-\nlated to style hypothesis (S), that may drive embed-\nding vectors to exhibit greater or lesser dispersion.\nTo conduct this analysis, we used a framework\ndeveloped by Terreau et al. (2021) to evaluate how\nembedding vectors represent writing style.4 This\nframework generates stylometric reports to assess\nhow well embedding models recognize writing\nstyles in alignment with stylistic features identi-\nfied by well-known Python modules (e.g. spaCy,\nNLTK, Counter). Terreau et al. (2021) select eight\ngroups of stylistic features as predictive targets\nfor French and English regression models. These\nfeatures include: the relative frequency of func-\ntion words (e.g., prepositions, conjunctions, aux-\niliary verbs) compared to the total word count in\nthe text, the average values of structural features\n(e.g., word length, word frequency, syllables per\nword), indexes of lexical complexity (e.g., Yule's K\nconstancy measure (Yule, 2014), Shannon Entropy\n(Shannon, 1948)) and text readability metrics (e.g.,\nFlesch-Kincaid Grade Level (Kincaid, 1975)), the\nrelative frequency of punctuation marks (e.g., peri-\nods, commas) compared to total text length, num-\nbers (i.e., numerical digits), the average frequency\nof named entities (i.e., NER: persons, locations, or-\nganizations) per sentence, and part-of-speech tags\n(i.e., TAG: nouns, verbs, adjectives).\nOur main focus in this section is to analyze\nhow embedding dispersion responds to variations\nin these stylistic features when style varies across\nclasses, while the topic remains constant. In more\ndetail, we focus on examining the interaction be-\ntween differences in the frequencies of the eight\nstylistic features and the difference in dispersion be-\ntween QUENEAU_GEN and QUENEAU_REF. This\ncomparison is the only case where styles are\nexpected to differ significantly between classes,\nwhile the topic remains exactly the same. As a\ncontrol, we also compared QUENEAU_GEN and\nFENEON_REF, where the topics are expected to dif-\nfer significantly, while the style remains the same.\nWe first measured mean ground frequencies\nof the eight stylistic features, written $f_s$ with\ns a stylistic feature, by applying Terreau et al.\n(2021)'s extraction module on the three classes\nof interest: QUENEAU_GEN, QUENEAU_REF and\nFENEON_REF. For each of the two targeted com-\nparisons, a pairwise t-test was conducted for each\nfeature variation to assess statistical significance."}, {"title": "5 Conclusion and perspectives", "content": "This paper provides evidence that writing style in-\nfluences embedding dispersion, though topic varia-\ntion has a stronger effect. This result is supported\nacross different language models in both French\nand English. Attempt at interpretability suggests\nthat specific linguistic features, particularly read-\nability and complexity indexes, function words and\npunctuation (to a lesser extent), partially explain\nembedding representations.\nIn the short run, two steps of investi-\ngation emerge. Firstly, some models we\ntested (e.g., sentence-camembert-base,\nall-MiniLM-L12-v2) showed greater respon-\nsiveness to stylistic variations leading to increased\ndispersion.  Other models (e.g., voyage-2,\nsolon-embeddings-large-0.1) were less\nor not affected. This observation, including cases\nwhere dispersion decreased contrary to expecta-\ntions (e.g., mistral-embed), suggests that dif-\nferent architectures process stylistic features in\nunique ways. Secondly, models generally re-\nsponded more to stylistic variation in French com-\npared to English. This was also reflected in\nthe weaker stylistic correlations observed for En-\nglish, suggesting that factors such as translation or\nlanguage-specific characteristics could play a role.\nReplicating the study on a larger French-English\ncorpus would provide further insight into these dif-\nferences and the detailed stylistic features control-\nling them.\nIn the long run, we aim for generalizability\nacross other models and genres, also hoping to\ninspire further stylistic studies on languages that\nare more typologically diverse than French and En-\nglish. Regarding models, we aim to compare the\nsensitivity of the architectures considered, with a\nfocus on open-weight models to enhance explain-\nability. Concerning genres, we aim to apply our\nmethodology to news articles, as a genre respond-\ning to stylistic conventions other than literary con-\nventions, associated to a great variety of topics and\na potential for high scalability. We leave these in-\nvestigations for future work."}, {"title": "Limitations", "content": "Our corpus methodology is validated by cluster-\ning. However, the corpus is limited to 292 textual\ndocuments in each language (a total of 584 doc-\numents), with 73 documents per class. A larger\ndataset should be used for replication in order to\nmitigate biases due to sample size and verify our\nhypotheses further.\nWhile UMAP dimensionality reduction to 2D\nproduced significant results in French, comparable\nresults were also observed with higher UMAP di-\nmensions. We omitted these in the paper for brevity\nbut they are available on our GitHub repository (see\nsupplementary materials below).\nAdditionally, our study is focused on eight main\nstylistic features, but these features are driven by\nsubfeatures that should be considered to gain a\nclearer understanding of their impact on embed-\nding dispersion. Other areas like journalism and\nscientific writing, which follow different stylistic\nconventions, are not explored either. Testing our\nhypotheses on other types of textual documents and\nassessing relevance of domain-specific features will\nbe essential for assessing the generalizability of our\nresults.\nLastly, open-weight LLMs that we tested, like\nDistilBERT and RoBERTa, offer potential for\ndeeper explainability and customization, in con-\ntrast to proprietary models, like OpenAI embed-\ndings, that lack transparency due to their closed-\nweights. Large open-weight models like LLaMA-2\nand Mistral-7B do exist, but their use requires sig-\nnificant computational resources."}, {"title": "Ethical considerations", "content": "Our research adheres to the following ethical princi-\nples: open science, transparency, inclusiveness, and\nsustainability. As academic researchers, we adhere\nto open science guidelines, with a concern for the\nreproducibility of experiments and the accessibility\nof our results. The dataset we provide contains\nno textual documents from the original sources,\nonly vector embeddings derived from those docu-\nments, aligning with the concept of \"transforma-\ntive fair use\". This approach ensures compliance\nwith intellectual property and data protection regu-\nlations. Transparency is upheld through raw data\navailability, code and prompts sharing on a dedi-\ncated GitHub repository, and a comprehensive doc-\numentation to ensure reproducibility by others. We\nare also guided by inclusiveness, as we expect our\nresearch project to contribute to advancing edu-\ncational and cultural AI literacy on the interplay\nbetween writing style and embedding representa-\ntions. To promote sustainability, the GitHub of\nthe study actually supports net-zero carbon initia-\ntives by others based on our framework. We also\nprioritize using smaller, open-source pre-trained\nlanguage models alongside larger ones, to reach\na balance between carbon footprint and resource\nconsumption."}, {"title": "Supplementary materials", "content": "Large language models used in the experi-\nments included the 1536-dimensional model text-\nembedding-3-small by OpenAI, and the 1024-\ndimensional models mistral-embed by Mistral,\nvoyage-27 by Voyage, and the RoBERTa-based\nmodels xlm-roberta-large,8 all-roberta-large-v1,9\nand multilingual-e5-large.10 Smaller models\nincluded the 768-dimensional models e5-base-\nv2,11 distilbert-base-uncased,12 all-MiniLM-L12-\nv2,13 the SBERT model sentence-camembert-\nbase14 and the multilingual model paraphrase-\nmultilingual-mpnet-base-v2.15 We also included\nsolon-embeddings-large-0.116 (1024D) by Solon\nas one of the best performing French embedding\nmodels (see MTEB leaderboard on HuggingFace,17\nat time of the submission: September 16, 2024).\nAll the code (including prompts for generating\nthe extended corpus in French and English),\ndata, and analytical results are available at\nthe following URL link: https://github.\ncom/evangeliazve/topic_style_\nembeddings_dispersion/tree/main"}]}