{"title": "State-Novelty Guided Action Persistence in Deep Reinforcement Learning", "authors": ["Jianshu Hu", "Paul Weng", "Yutong Ban"], "abstract": "While a powerful and promising approach, deep reinforcement learning (DRL) still suffers from sample inefficiency, which can be notably improved by resort-ing to more sophisticated techniques to address the exploration-exploitation dilemma. One such technique relies on action persistence (i.e., repeating an action over multiple steps). However, previous work exploiting action persistence either applies a fixed strategy or learns additional value functions (or policy) for selecting the repetition number. In this paper, we propose a novel method to dynamically adjust the action persistence based on the current exploration sta-tus of the state space. In such a way, our method does not require training of additional value functions or policy. Moreover, the use of a smooth scheduling of the repeat probability allows a more effective balance between exploration and exploitation. Furthermore, our method can be seamlessly integrated into various basic exploration strategies to incorporate temporal persistence. Finally, exten-sive experiments on different DMControl tasks demonstrate that our state-novelty guided action persistence method significantly improves the sample efficiency.", "sections": [{"title": "1 Introduction", "content": "Deep Reinforcement Learning (DRL) has emerged as a powerful tool for solving sequential decision-making problems, by leveraging the impressive approximation"}, {"title": "2 Related Work", "content": "We discuss the most related work to ours and organize it into three clusters: Sample Efficient DRL, Action Persistence, and Exploration in DRL."}, {"title": "Sample Efficient DRL", "content": "Sample efficiency is crucial in DRL which motivates extensive research efforts. Key approaches to enhancing sample efficiency include training world models [5], exploiting data augmentation [6], adding self-supervised loss [7] and increasing the replay ratio [8]. Here, we give a brief introduction to Data-regularized Q (DrQ) [9] and DrQv2 [10], which serve as our base algorithm due to its sample efficiency. DrQ leverages data augmentation, specifically by applying random shift on the training samples in image-based RL tasks. Yarats et al. [10] improve the performance in image-based DMControl [11] tasks upon DrQ by introducing several changes. One of the key changes is to switch the base DRL algorithm from SAC [12] to DDPG [13]."}, {"title": "Action Persistence", "content": "Action persistence or action repeat [4] is commonly used in DRL to reduce the computational burden of intensively choosing action. This technique also encourages exploration and helps with credit assignment for long-horizon tasks. Instead of simply treating it as a hyperparameter, research work has been conducted to decide action persistence dynamically and leverage it for better exploration. Lakshminarayanan et al. [14] introduce a method called Dynamic Action Repetition, which extends the Q-function to include Q-values for repeated actions, allowing dynamic selection of action persistences. However, this approach is limited to discrete action spaces and struggles when dealing with extensive action persistences. To address these limitations, sub-sequent work by Sharma et al. [15] and Biedenkapp et al. [16] propose training an additional repetition policy specifically for action persistence.\nInstead of providing an action persistence for each action, alternative methods have been explored. For instance, Yu et al. [17] propose a binary policy to decide whether to repeat the last action or not, while Park et al. [18] propose a safe region where the agent repeats actions until leaving this region. The Option [19] framework"}, {"title": "Exploration in DRL", "content": "Recent advancements in exploration strategies for DRL have introduced a variety of innovative methods [23]. One prominent approach to address the dilemma between exploration and exploitation is the use of intrinsic reward, by which the agent is guided to explore. Curiosity-driven exploration [24\u201326] and novelty-based rewards [27\u201330] fall into this category. Curiosity-driven exploration encourages agents to visit states that maximize the error in their learned predictive models, while novelty-based rewards guide the agent to explore novel states according to some novelty measures. Another significant advancement is the integration of uncertainty estimation into exploration strategies [31\u201335]. Uncertainty based exploration methods quantify the uncertainty, helping agents to more effectively explore areas where the model is less certain.\nMoreover, recent work has also focused on improving exploration through the use of hierarchical structures [36] and temporal abstraction [19]. Techniques such as options and macro-actions enable agents to operate at multiple levels of abstraction, allowing for more structured and efficient exploration. \u0141yskawa and Wawrzy\u0144ski [37] train agents with progressively finer time discretization. Notably, Dabney et al. [38] propose temporally-extended \\u03b5-greedy which is the most relevant one to our method. On each step, besides choosing a random action with probability \\u03b5, a zeta distribution (z(n) \u221d n\u207b\u03bc) is used to decide the action persistence of a random action. This allows the agent to explore persistently without modifying the greedy policy."}, {"title": "3 Background", "content": "In this section, we introduce the notations and recall the basic formulation of the actor-critic framework in deep reinforcement learning."}, {"title": "3.1 Problem Formulation", "content": "For any set X, \u2206(X) denotes the set of probability distributions over X. For a random variable X, E[X] (resp. V[X]) denotes its expectation (resp. variance). For any function \\u03d5 and i.i.d. samples x\u2081, ..., x\u2099 of X, \u00ca[\u03c6(X)] = \u03a3\u1d62\u208c\u2081\u207f \u03c6(x\u1d62) is an empirical mean estimation of E[\u03c6(X)]. Meanwhile, a Markov Decision Process (MDP) M = (S, A, r, T, \u03c1\u2080) is composed of a set of state S, a set of action A, a reward function r: S \u00d7 A \u2192 \u211d, a transition function T : S \u00d7 A \u2192 \u2206(S), and a probability distribution over initial states \u03c1\u2080 \u2208 \u2206(S). In reinforcement learning, the agent is trained by inter-acting with the environment to learn a policy \u03c0(\u00b7 | s) \u2208 \u2206(A) such that the expected return E\u03c0[\u03a3\u209c\u208c\u2080\u1d40 \u03b3\u1d57r\u209c | s\u2080 ~ \u03c1\u2080] is maximized."}, {"title": "3.2 Actor-Critic Framework", "content": "A typical off-policy actor-critic framework in DRL parameterizes the actor \u03c0\u03b8 and critic Q\u03c6 with neural networks. To solve the MDP, specific actor and critic losses are calculated using samples collected from a replay buffer D for updating the parame-ters. We introduce two typical actor-critic algorithms below, which are related to the base algorithms used in the experiments.\nDeep Deterministic Policy Gradient Incorporating a parameterized determinis-tic actor function \u03bc\u03b8(s), Deep Deterministic Policy Gradient (DDPG) [13] uses the following actor and critic loss to train the agent:\n$J_\\mu(\\theta) = E_{s_t \\sim D}[-Q_\\phi(s_t, a_t)|a_t=\\mu_\\theta(s_t)],$ \n$J_Q(\\phi) = E_{s_t,a_t,s_{t+1},r_t \\sim D}[(Q_\\phi(s_t, a_t) - (r_t +\\gamma Q_{\\phi'}(s_{t+1}, a_{t+1})))^2|a_{t+1}=\\mu_{\\theta'}(s_{t+1})],$ (1)\nwhere Q\u03c6'(s\u209c\u208a\u2081, a\u209c\u208a\u2081) is the Q-value defined from a target network, \u03b8 and \u03c6 represent the parameters of the actor and the critic respectively, \u03b8' and \u03c6' represent the parame-ters of the target actor and the target critic respectively, and D represents the replay buffer. The weights of a target network are the exponentially moving average of the online network's weights. Note that noise sampled from a noise process N is added to the actor policy for exploration in DDPG:\n$\\mu'(s_t) = \\mu_\\theta(s_t) + \\epsilon \\text{ where } \\epsilon\\sim N$ (2)\nSoft Actor-Critic Maximum entropy RL tackles an RL problem with an alternative objective function, which favors more random policies: $J = \u00ca_\u03c0[\u2211\u209c\u208c\u2080\u1d40 \u03b3\u1d57r\u209c + \u03b1H(\u03c0(\u00b7 | s\u209c))], where \u03b3 is the discount factor, \u03b1 is a trainable coefficient of the entropy term and H(\u03c0(\u00b7 | s\u209c)) is the entropy of action distribution \u03c0(\u00b7 | s\u209c). The Soft Actor-Critic (SAC) algorithm [12] optimizes it by training the actor \u03c0\u03b8 and critic Q\u03c6 with the following losses:\n$J_\\pi(\\theta) = E_{s_t\\sim D, a \\sim \\pi_\\theta(s_t)}[\\alpha\\text{log }\\pi_\\theta(a|s_t) \u2013 Q_\\phi(s_t, a)],$\n$J_Q(\\phi) = E_{s_t,a_t,s_{t+1},r_t \\sim D}[(Q_\\phi(s_t, a_t)-\n(r_t+\\gamma Q_{\\phi'}(s_{t+1}, a_{t+1}) \u2013 \\alpha \\text{ log } \\pi_\\theta(a_{t+1}|s_{t+1})))^2|a_{t+1}\\sim \\pi_\\theta(s_{t+1})].$ (3)\nThe same notations used in DDPG above, such as the parameters for the actor and critic are used here as well."}, {"title": "4 Effect of Action Persistence", "content": "In this section, we analyze the effect of both large and small persistence in DRL algorithms. Intuitively, larger action persistences mean fewer decisions required in a trajectory. Also, larger action persistences contribute to broader exploration by stick-ing to temporally persistent action sequences. Accordingly, smaller action persistences correspond to fine-grained policies which usually yield superior optimal performance."}, {"title": "4.1 Large Action Persistence", "content": "As observed in previous work, using large action persistence accelerate the exploration of the state space. We provide some theoretical formalization to explain this observa-tion. When using a large action persistence, the number of action decisions made in a finite horizon is smaller, leading to fewer possible action sequences. For an MDP with a finite horizon H, with a larger action persistence, the size of the set of all possible action sequences of length H is smaller:\n$||A_{k_1} || < ||A_{k_2} ||, \\text{ if } k_1 > k_2.$ (4)\nwhere Ak is the set of all possible action sequences of length H with an action persistence k \u2208 K:\n$A_k = \\{(a_0, ..., a_0, ..., a_{h-1}, ..., a_{h-1}, a_h,..., a_h) | a_i \\in A, \\forall i = 0, ..., h\\}$ (5)\n$ \\qquad \\qquad \\qquad \\underbrace{\\qquad \\qquad}_{k} \\qquad \\underbrace{\\qquad \\qquad}_{k} \\qquad \\qquad \\qquad \\qquad \\underbrace{\\qquad \\qquad}_{H-k*(h-1)}$\nwith h = [H/k] is the number of decisions made in an action sequences from Ak."}, {"title": "4.2 Small Action Persistence", "content": "Given two sets $|| A_{k_1} ||$ and $|| A_{k_2} ||$, which are both sufficient to cover the whole state space, exploration by trying random actions will be more efficient by considering the larger action persistence. However, using a large action persistence may prevent to cover the whole state space. In such case, a stochastic action persistence may circumvent this issue, while still preserving some benefits from large action persistence [38]. In our work, we further demonstrate that sampling the action persistence using a distribution that depends on both states and past exploration can improve further the exploration efficiency.\nTo further illustrate that incorporating repeated actions promotes temporally per-sistent exploration, we evaluate different behavior policies with or without repeated actions in a 51 \u00d7 51 mini grid environment. In this setting, the state space S = {(i, j) | 0 \u2264 i \u2264 50, 0 \u2264 j \u2264 50} contains all grid coordinates, and the agent can select actions from the action space A = {up, down, left, right} to move one step in the correspond-ing direction until reaching the boundaries. When a boundary is reached, any action attempting to move the agent outside the grid does not change its state, while other actions proceed normally. The agent is initialized at the center of the grid and is also reset to the center at the end of each episode. The policies tested in this mini-grid are as follows:\n1.  Random: Actions are uniformly sampled from the action space.\n2.  Random-zeta: As proposed in temporally-extended \\u03b5-greedy [38], a fixed zeta-distribution (z(n) \u221d n\u207b\u03bc) is used for deciding action persistence of a random action. We adopt the hyperparameter \u03bc = 2 from \\u03b5-greedy [38].\n3.  Count-repeat: Our method is instantiated for this environment. To specify, our method counts the visiting times of each state and decides the probability of repeat-ing the last action. Higher probabilities are assigned to novel states. A detailed description of our method is presented in Section 5.\nEquation: max\u03c0\u2208\u03a0k\u2081 \u00ca[\u03a3\u209c\u208c\u2080\u1d34 \u03b3\u1d57r\u209c | s\u2080 ~ \u03c1\u2080] \u2265 max\u03c0\u2208\u03a0k\u2082 \u00ca[\u03a3\u209c\u208c\u2080\u1d34 \u03b3\u1d57r\u209c | s\u2080 ~ \u03c1\u2080]"}, {"title": "5 Methodology", "content": "According to our analysis, incorporating large action persistences is an intuitive and effective way to achieve temporally persistent exploration. Meanwhile, smaller action persistences usually result in preferable fine-grained final optimal policies. To effec-tively balance the exploration and exploitation, we propose State-Novelty guided Action Persistence (SNAP). SNAP allows us to dynamically bridge the gap between large persistence strategies and small persistence strategies. In this section, we present (i) SNAP, which is an off-policy framework of adapting dynamic action persistences in the behavior policy, (ii) a state-novelty guided persistence adaptor to decide action persistence, (iii) and an instantiation of our method in an actor-critic algorithm."}, {"title": "5.1 SNAP Framework", "content": "This framework exploits the strong exploration capability of policies with large action persistences at the initial training stages and achieves superior performance of the fine-grained policies as training progresses. To achieve this, a state-novelty guided adaptor to decide action persistence is introduced in the behavior policy of an off-policy framework. By decoupling the behavior policy from the target policy, the off-policy framework allows for diverse exploration strategies without compromising the stability of the training process. Consequently, it enables leveraging different action persistences for varying levels of exploration. Meanwhile, it does not require to train additional value functions or policies specifically for action persistence decisions anymore.\nIdeally, action persistence should gradually transition from large to small. At the beginning of the training, large action persistences facilitate temporally persistent exploration, which leads to a larger coverage of the state space. As training progresses, action persistence is expected to gradually decrease for obtaining a fine-grained policy. Ultimately, the action persistence in the behavior policy should closely align with that of the target policy. Therefore, we choose to use state-novelty, a measure to"}, {"title": "5.2 Persistence Adaptor", "content": "In this part, we introduce the persistence adaptor module which dynamically makes the action persistence decision in behavior policy. Our method employs a count-based measure [27] for state-novelty. An illustration of the persistence adaptor is shown in Figure 3 (b). In this module, we have the following components. Firstly, to mitigate the dimensional explosion caused by using image inputs as states, an image encoder is applied to convert the images into feature vectors. In an image-based DRL architec-ture, there is an image encoder, trained by the critic loss, to map images to meaningful representations. It is reasonable to leverage this representation by sharing the encoder with the adaptor. Next, we use a hash function [39] as a quantization encoder to map the feature space to binary code space. The hashed representation has three main advantages: (i) The hashed space allows us to measure the state-novelty by count-ing; (ii) Close feature vectors are usually mapped to the same binary code; and (iii) The hash function is computationally efficient. The hash function \u03c6(s) used can be formally written as:\n$\\phi(s) = \\text{sgn}(Ag(s)) \\in \\{1, -1\\}^K,$ (7)\nwhere g is the encoder g: S\u2192 \u211d\u1d30, A is a K \u00d7 D matrix with elements drawn from a standard Gaussian distribution N(0, 1) and sgn corresponds to a sign function. To adaptively choose action persistence, the adaptor decides the probability of repeating last action for each state. The probability of repeating the previous action a\u209c\u208b\u2081 for state s\u209c in the behavior policy \u03c0' is given by\n$P(\\pi'(s_t) = a_{t-1}) = \\frac{\\alpha}{\\text{max}(1,\\sqrt{N(s_t)})},$ (8)\nwhere \u03b1 is a hyperparameter controlling exploration and \u00d1(s\u209c) is a pseudo-count, counted with the binary codes of the state s\u209c. The proposed formula for calculating the probabilities of action repetition allows for dynamic adjustment of action persistence, adapting progressively throughout the training process."}, {"title": "5.3 Algorithm", "content": "Algorithm 1 integrates the proposed SNAP into an off-policy actor-critic framework. The difference between the SNAP algorithm and the traditional actor-critic algorithm is highlighted in orange, which contains two key modifications:\n1.  It adopts the persistence adaptor to decide the action persistence in behavior policy, while interacting with the environments.\n2.  It dynamically updates the state-novelty measure by using samples from the replay buffer.\nNote that the hash table is not updated immediately upon encountering new states. Instead, we update the hash table using the same mini-batch used for training the actor"}, {"title": "6 Experimental Results", "content": "To validate our proposition and show the effectiveness of our proposed algorithm, we conduct the following experiments:\n\u2022   We experimentally demonstrate the effect of using different action persistences;\n\u2022   We benchmark different methods of exploiting repeated actions;\n\u2022   We evaluate the performance of combining the proposed SNAP with different basic exploration strategies: Epsilon-greedy, Entropy-regularization and Noisy net.\n\u2022   We conduct an ablation study to assess the design choice of the probability scheduler and the measure of state novelty."}, {"title": "6.1 Different Action Persistences", "content": "We start with experiments of using different action persistences k = 1, 2, 4 in Reacher-hard, Acrobot-swingup and Quadruped-run, as shown in Figure 4. The additional results of training the agents until convergence are shown in Section B. The dimensions of state space and action space of these three environments are respectively (4,1), (4,2) and (56, 12). Quadruped-run is a hard locomotion task due to the high dimensions and rich contact.\nFrom the results, it is observed that the choice of action persistence influences the sample efficiency of the original algorithm (DrQv2). There is not a fixed action persistence which can be optimal across all the environments. Compared to using a fixed action persistence, state-dependent action persistences in our method lead to a significant gain in sample efficiency. This can not be achieved by tuning the action per-sistence as a hyperparameter specifically for each environment. To concretely present how our method adjusts the action persistence during the training, the probabilities of repeating actions for all steps are saved. Based on our count-based method, this smooth scheduler for the repeat probabilities enables both the efficiency in exploration and the enhancement in performance."}, {"title": "6.2 Exploiting Repeated Actions", "content": "The experiments above provide some insights of how different action persistences influence the exploration and the sample efficiency. They also partially demonstrate the benefits of incorporating temporally persistent exploration into the policy. Here, we further compare ours with another method of exploiting repeated actions, called temporally-extended \\u03b5-greedy. The comprehensive evaluations across tasks with varying complexities are shown in Figure 5. Following the recommendations from Agarwal et al. [40], the aggregated performance is also plotted in Figure 6. As expected, employing a fixed distribution to decide the action persistence proves inef-fective in complex tasks, especially in those contact-rich tasks. In contrast, our method"}, {"title": "6.3 Improving Exploration Strategies with SNAP", "content": "As mentioned in Section 3, the base algorithm of DrQv2 is DDPG, which adds Gaus-sian noise to the actions from the deterministic policy to explore. Actually, other basic exploration strategies can also be temporally extended by integrating our method with them. We test combining our method with\n1.  Epsilon greedy: There is a probability of \\u03b5 for using random actions.\n2.  Entropy regularization: Replacing DDPG with SAC, which uses a stochas-tic policy and includes an entropy term to regularize the policy for encouraging exploration."}, {"title": "6.4 Abalation study", "content": "In this section, we analyze the design choice of leveraging state novelty to guide the action persistence and compare different measures of state novelty. Firstly, a linear scheduler and a sigmoid scheduler to decide the probability of repeating actions are tested. Both the evaluation curves and corresponding probabilities along the train-ing are shown in Figure 8. Our method outperforms the baselines of using a simple scheduler for deciding the probability. Notably, our method has a smaller variance as well. Additionally, different measures of the state novelty are compared: 1) count with quantized states 2) count with clustered states using k-means. For quantized states, each dimension in the state vectors is first discretized and then used for counting. As for k-means clustered states, the center of each cluster is first initialized by batches of states from the replay buffer and then continues to be updated during the training. The results shown in Figure 9 indicate the robustness of our method with respect to the choice of measure for state novelty."}, {"title": "7 Conclusion", "content": "In this paper, we introduce a novel method for dynamically deciding action per-sistence in DRL, guided by a state-novelty measure. By incorporating temporal persistent exploration into the behavior policy of off-policy DRL algorithms, our approach balances exploration and exploitation without incurring significant compu-tational overhead. Experimental results on DMControl tasks demonstrated significant improvements in sample efficiency over baseline methods. Meanwhile, our method can be integrated into other basic exploration strategies for incorporating temporal persistence. Future work could explore the application of our method to other DRL algorithms and more environments. Additionally, investigating alternative measures of state-novelty and their impact on action persistence strategies could provide further insights and enhance the effectiveness of our approach."}, {"title": "Appendix A Hyperparameters", "content": "The commonly used hyperparameters across all experiments are shown in Table A1. We just adopt the same hyperparameters from the DrQv2 [10]. For a fair comparison of using different action persistences, some hyperparameters are adjusted, as shown in Table A2, to make sure the agent is trained with same number of updates under different settings. Moreover, humanoid tasks in DMControl are extremely hard. So following the setting in original DrQv2, the hyperparameters such as total training frames are tuned in these tasks, shown in Table A3."}, {"title": "Appendix B Additional results", "content": "We also run experiments of using different action persistences k = 1,2,4 in Reacher-hard, Acrobot-swingup and Quadruped-run until convergence, as shown in Figure A1."}]}