{"title": "MBQ: Modality-Balanced Quantization for Large Vision-Language Models", "authors": ["Shiyao Li", "Yingchun Hu", "Xuefei Ning", "Xihui Liu", "Ke Hong", "Xiaotao Jia", "Xiuhong Li", "Yaqi Yan", "Pei Ran", "Guohao Dai", "Shengen Yan", "Huazhong Yang", "Yu Wang"], "abstract": "Vision-Language Models (VLMs) have enabled a variety of real-world applications. The large parameter size of VLMs brings large memory and computation overhead which poses significant challenges for deployment. Post-Training Quantization (PTQ) is an effective technique to reduce the memory and computation overhead. Existing PTQ methods mainly focus on large language models (LLMs), without considering the differences across other modalities. In this paper, we discover that there is a significant difference in sensitivity between language and vision tokens in large VLMs. Therefore, treating tokens from different modalities equally, as in existing PTQ methods, may over-emphasize the insensitive modalities, leading to significant accuracy loss. To deal with the above issue, we propose a simple yet effective method, Modality-Balanced Quantization (MBQ), for large VLMs. Specifically, MBQ incorporates the different sensitivities across modalities during the calibration process to minimize the reconstruction loss for better quantization parameters. Extensive experiments show that MBQ can significantly improve task accuracy by up to 4.4% and 11.6% under W3 and W4A8 quantization for 7B to 70B VLMs, compared to SOTA baselines. Additionally, we implement a W3 GPU kernel that fuses the dequantization and GEMV operators, achieving a 1.4\u00d7 speedup on LLaVA-onevision-7B on the RTX 4090. The code is available at https://github.com/thu-nics/MBQ.", "sections": [{"title": "1. Introduction", "content": "Large Vision-Language Models (VLMs) have made significant progress and enabled various real-world tasks, such as image captioning [10], visual question answering (VQA) [2], and so on. However, due to the large memory and computation overhead, existing large VLMs, such as LLaVA [29], InternVL [11], and QwenVL [4], are hard to deploy on commonly used accelerators, such as GPUs. For example, the largest LLaVA-onevision VLM, with 72B parameters, requires 144GB of memory, which exceeds the 80GB memory capacity of the A100 GPU.\nPost-Training Quantization (PTQ) methods are effective in reducing the memory and computation overhead of neural networks. Various PTQ methods have been developed to accelerate LLM inference. To reduce the memory access and storage overhead, weight-only quantization methods have been developed, such as AWQ [25], GPTQ [15], QuIP [7], and so on. To reduce the computation overhead, weight-activation quantization methods, such as SmoothQuant [39], SpinQuant [31], Atom [47], and so on, have been applied, enabling the use of the faster low-precision tensor cores on GPUs. To retain the task accuracy, during the calibration process, these methods search for optimal scaling factors [15, 36], channel-wise equalization factors [25, 39], rotation matrices [31], etc., by minimizing the feature reconstruction error of each block.\nWhile PTQ methods are well-studied for LLMs, their suitability for VLMs has not been fully explored, where tokens from multiple modalities need to be handled and cross-modality tasks need to be addressed. Our experiments reveal that directly applying SOTA PTQ methods for LLMS to VLMs results in substantial accuracy degradation. We speculate that the main reason is that existing methods treat vision and language tokens equally, overlooking their significantly different sensitivities.\nTo verify this, as depicted in Fig. 1, using an image-caption pair from the COCO dataset as the input [10], we visualize the loss gradient w.r.t. the output feature of the 13th layer in LLaVA-onevision-7B. We can see that the average absolute gradient value of the language token features is over 10x larger than that of vision token features. This 1st-order approximation analysis suggests that with the same size of perturbation on the features, language tokens might impact the loss function more than 10\u00d7 as much as vision tokens. Consequently, treating vision tokens and language tokens equally during calibration may over-emphasize the insensitive vision tokens, resulting in a notable accuracy loss.\nTaking the sensitivity differences into consideration, we propose an extremely simple but effective method, Modality-Balanced Quantization (MBQ), for quantizing large VLMs. Specifically, MBQ uses the gradients of the loss function [29] w.r.t. vision and language token features as the sensitivity indicators. These sensitivity indicators are then incorporated into the reconstruction loss as the objective for searching optimal channel-wise equalization factors [25, 39] in both weight-only (W3 and W4) and weight-activation quantization (W4A8 and W8A8). By balancing the effect of different modalities, MBQ can significantly increase the accuracy of the quantized VLMs.\nWe conduct extensive experiments across 7B-70B VLMs on challenging vision-language benchmarks. The results show MBQ significantly improves the task accuracy by up to 4% and 11% under W3 and W4A8 quantization compared with other SOTA methods. We also analyze the shortcomings of baseline methods and perform comprehensive ablation studies on various factors, including the choice of calibration datasets, alternative formulations of modality-balancing losses, and the quantization of visual encoders.\nFor W3 quantization, we design a GPU kernel that fuses 3-bit dequantization with general matrix-vector multiplication (GEMV). For W4, W4A8, and W8A8 quantization, we adopt existing open-source GPU kernels [25, 27] to accelerate the inference process. Experiments on different workloads show that we can achieve up to 1.4\u00d7 end-to-end speedup on LLaVA-onevision-7B on RTX 4090."}, {"title": "2. Preliminaries", "content": "2.1. The Inference Process of VLMS\nThe inference process of VLMs is shown in Fig. 2. The whole inference system consists of three key components:\n\u2022 Language Tokenizer: Transform natural language sentences into a series of language tokens.\n\u2022 ViT Encoder: Transform images into a series of vision tokens.\n\u2022 Large VLM: Take the language and vision tokens as input, and generate language tokens one by one.\nSpecifically, the transformer-based [38] VLMs have two distinctive stages, including the prefill and the decode stages. Take batch size = 1 as an example.\nDuring the prefill stage, VLMs can take both vision tokens and language tokens as the input prompt. In this stage, VLMs aim to understand both the vision and language information and the connections across each modality. The Key and Value tensors of each attention block in VLMs are stored as the KV Cache to save the computation overhead in the decode stage. The input activation of each layer is typically a large 2D matrix, making the primary computation operation in the prefill stage the General Matrix Multiply (GEMM), which is compute-bound.\nDuring the decode stage, VLMs take one generated token from step t as the input and use the KV Cache to generate the next token of step t+1. The generation of the current token depends on one previously generated token and the KV Cache. In this case, the input activation of each layer is typically a large 1D vector, and the main computation operator in the decode stage is the General Matrix-Vector Multiply (GEMV), which is memory-bound."}, {"title": "2.2. Quantization Formats", "content": "In this paper, we focus on applying the uniform integer quantization, which is a commonly used quantization format, to the weight (W) and input activation (X) matrices of each linear layers in VLMs.\nFor weight-only quantization, existing methods typically apply asymmetric uniform quantization for the weight matrices of linear layers, as shown below:\n$W_{asym} = \\frac{W_{FP16} - Z}{S_{asym}}$\n$S_{asym} = \\frac{max(W_{FP16}) - Z}{2^{N-1}}$\nwhere $W_{FP16}$ denotes the 16-bit floating-point (FP16) value, $W_{asym}$ denotes the low-precision integer value. N is bit-width. $S_{asym}$ and $Z = min(W_{FP16})$ denote the scaling factor and zero-point of the asymmetric quantization.\nFor weight-activation quantization, symmetric uniform quantization is commonly used for both weight and input activation matrices of linear layers:\n$W_{sym} = \\frac{W_{FP16}}{S_{sym}}$\n$S_{sym} = \\frac{absmax(W_{FP16})}{2^{N-1}-1}$\nwhere $S_{sym}$ denotes the scaling factor of the symmetric quantization.\nFor simplicity, we use W and A followed by a positive integer to indicate the quantization to a specific bit-width for Weight and Activation. For example, W4A8 denotes quantizing weights to 4-bit and activations to 8-bit."}, {"title": "2.3. Channel-Wise Equalization", "content": "Existing PTQ methods automatically search for optimal hyperparameters of quantization by minimizing the reconstruction error of each transformer block during a calibration process. A series of popular methods [25, 39] aim to equalize outliers in weight and activation matrices by channel-wise equalization (CWE). Specifically, they search the CWE factors E by minimizing the Mean Square Error (MSE) loss in each transformer block. Taking weight-only quantization as an example, the objective of CWE is shown in the following equation:\n$E^* = argmin_{E} ||Q(W * E)(E^{-1} * X) \u2013 WX||^2$,\nwhere Q means the quantization function."}, {"title": "3. Method", "content": "3.1. Sensitivity Varies Across Modalities\nAs introduced in Sec. 2.3, when using visual-language datasets for calibration, CWE treats visual and language activations equally during the calibration process.\nIntuitively, we speculate that the significant performance degradation when applying SOTA LLM quantization methods to VLMs stems from treating different modalities equally. This is because errors in vision tokens might have a smaller impact on the output context compared to introducing the same errors in language tokens, due to the following two reasons: (1) From the data perspective, visual data generally contains a high degree of redundancy and might be more fault tolerant for small perturbations. (2) From the model perspective, Zhang et al. [45] discover that the generated content of current VLMs is primarily biased by the pre-trained LLMs rather than the input image.\nTo verify the above intuition, we evaluate the sensitivity of the output tokens w.r.t the input vision and language tokens on the COCO caption dataset [10]. Specifically, we take the image-caption pairs as inputs of VLMs and calculate gradients of the SFT loss function w.r.t. language and vision tokens. The gradients can reflect the impacts on the output language tokens (caption) when small perturbations are applied to language (caption) and vision (image) token features. Note that due to the attention mechanism, the gradient of the SFT loss can still backpropagate to vision tokens in each transformer block, even though we only account for the loss of the output language tokens.\nAs shown in Fig. 1, the average absolute gradient of language tokens is an order of magnitude larger than that of vision tokens. This means that, for a similar perturbation, a vision token's impact on the SFT loss is only 0.1\u00d7 that of a language token. Therefore, if we still treat language and vision tokens equally, we will miss the opportunity to leverage the VLM's lower sensitivity to vision tokens to achieve higher performance.\nTo demonstrate the impact of accounting for modality-specific sensitivities during calibration, we conduct an oracle experiment by applying a modality-balancing factor of 0.1 to the vision tokens' reconstruction loss during CWE calibration. The optimization objective, referred to as balanced CWE, is shown below:\n$E^* = argmin[||Q(W * E) (E^{-1} * X_l) \u2013 WX_l||^2$\n$+0.1||Q(W *E)(E^{-1} * X_v) \u2013 WX_v||^2]$,\nwhere the $X_l$ and $X_v$ mean the language and vision tokens. The results of our ablation study are shown in Tab. 1, with a heuristic selected modality-balancing factor, the balanced CWE easily surpasses the performance of CWE by 1.55% ~ 3.66% under W3 quantization. The significant improvements indicate the importance of balancing the different sensitivities across different modalities."}, {"title": "3.2. Modality-Balanced Quantization (MBQ)", "content": "Given that the sensitivity differences between vision and language tokens may vary across layers and VLM families, exploring an automatic modality-balancing approach could further enhance the performance of the quantized model.\nIn this section, we aim to derive an approach for allocating the optimal Modality-Balanced factors to each layer by directly minimizing the change in the SFT loss function. Specifically, we employ the first-order Taylor approximation in Eq. (8) to determine how the SFT loss $L$ changes in response to a small perturbation \u0394 in output activation Y of each linear layer:\n$L(Y + \u2206) ~L(Y) + g^T * \u2206$,\nwhere $g^T$ represents the gradient of the output activation Y. The change in SFT loss caused by quantization can be expressed as the following equation:\n$||L(\\hat{Y})|| ~||g^T * \u0394||$\n$~<|g| * ||\u0394||$\n$~|g_v| * ||\u0394_v|| + |g_l| * ||\u0394_l||$\n$=|g_v| * ||Y_v - \\hat{Y_v}|| + |g_l| * ||Y_l \u2013 \\hat{Y_l}||$,\nwhere $Y_v$ and $\\hat{Y_v}$ represent the output vision tokens of the FP16 linear layer and the quantized linear, respectively. $Y_l$ and $\\hat{Y_l}$ represent the output language tokens of the FP16 linear layer and the quantized linear, respectively. The $|g_v|$ and $|g_l|$ represent the average absolute gradient of each linear layer's output vision and language tokens, respectively. In this paper, we combine MBQ with channel-wise equalization to search optimal equalization factors for better performance.\nFirstly, in order to accelerate the prefill stage of VLMs, we quantize both the weights and input activations of each linear layer to leverage fast low-precision tensor cores. The objective is shown in the following:\n$min_{E}[|g_v| * ||WX_v - Q(W *E)Q(E^{-1} * X_v)||(13)$\n$+|g_l| * ||WX_l - Q(W * E)Q(E^{-1} * X_l)||],$\nwhere $X_v$ and $X_l$ represent the input token and language activations of each linear layer, respectively.\nSecondly, to accelerate the decode stage of VLMs, we only quantize weights to reduce the memory overhead, aiming to minimize the balanced reconstruction error by the following objective:\n$min_{E}[|g_v| * ||WX_v \u2013 Q(W *E)(E^{-1} * X_v) ||$ (15)\n$+|g_l| * ||WX_l - Q(W * E)(E^{-1} * X_l)||],$\nNotably, unlike directly using the heuristically selected MSE-based balanced CWE loss in Sec. 3.1, our derived reconstruction error loss function relies on Mean Absolute Error (MAE). Our ablation study in Sec. 5.3 demonstrates that minimizing MAE-based reconstruction loss in MBQ yields better results than using an MSE-based one."}, {"title": "4. End-to-End Acceleration Implementation", "content": "As illustrated in Sec. 2.1, we need to quantize not only the VLMs with large parameter counts but also the ViT encoders for efficient deployment, which have substantial computational overhead when transforming high-resolution images into vision tokens.\nSpecifically, different from VLMs, the ViT encoders do not have two distinct stages, they take a series of image patches as input and generate a set of vision tokens as output. The input activation of each linear layer in ViT encoders are all 2D matrices, so the main computation operator is GEMM, which is compute-bound.\nTo this end, we apply weight-activation quantization to both the ViT encoders and VLMs to accelerate the prefill stage of VLMs in long input scenarios. For short input scenarios, we use weight-activation quantization for ViT encoders, and weight-only quantization for VLMs to accelerate the decode stage.\nTo achieve practical hardware acceleration, we designed a custom fused W3 quantization GPU kernel by fusing the dequantization process with the GEMV operator. To further reduce storage overhead, we pack eight 3-bit weights into three bytes. Specifically, the fused W3 kernel first loads the W3 weights instead of FP16 weights to reduce memory access overhead. It then dequantizes the W3 weights to FP16. Finally, the fused W3 kernel performs computations using the FP16 tensor core.\nWith the fused W3 kernel and open-source GPU kernel libraries [25, 27] for quantization, we can accelerate the inference speed of both the quantized ViT encoders and VLMs. Detailed experiments in Sec. 5.4 demonstrate the hardware performance of the proposed W3 kernel and end-to-end speedups across various scenarios."}, {"title": "5. Experiments", "content": "5.1. Experimental Setups\n5.1.1. Calibration Datasets\nThe calibration task requires leveraging both the VLM's ability to understand image details and its language modeling capabilities. \u201cImage captioning\u201d is one of the tasks that addresses both aspects. Specifically, we choose the improved COCO Caption dataset proposed by ShareGPT4V [8] and sample 128 image-caption pairs as the calibration dataset. Chen et al. [8] use GPT4-V to generate a high-quality caption for each image. For each VLM, we apply the corresponding conversation template to each image-caption pair to create an instructional format.\n5.1.2. Evaluation Datasets\nTo evaluate the performance of the quantized model, we conduct experiments on various vision-language benchmarks based on the LMMs-Eval [44]. Specifically, we use OCRBench [30] and TextVQA [37] for text recognition and comprehension, VizWiz [17] and SEED-Bench [18] to test visual perception, and ScienceQA [32] and MMMU [42] to evaluate visual reasoning. Additionally, to demonstrate the practical conversational performance of the quantized VLM, we present several cases on the LLaVA-bench-in-the-wild [29] and LLaVA-bench-wilder [19] datasets in the supplementary Sec. 8.2.\n5.1.3. Models\nWe benchmark various quantization methods on LLaVA-onevision [20], InternVL2 [11], and Qwen2-VL [4] families. For each model family, we select both a smaller and a larger parameter version to showcase the capability of MBQ across different model sizes. For LLaVA-onevision, we select models with 7B and 72B parameters, which utilize Qwen2-7B/-72B for the VLM and SigLIP-400M [43] for the ViT encoder. For InternVL2, we evaluate the 8B and 26B models, which incorporate InternLM2-8B/-20B as the VLM component and use InternViT-300M/-6B as the vision encoder. For Qwen2-VL, we use models with 7B and 72B parameters in our evaluation, which use Qwen2-7B/-72B for the VLM component and a 675M ViT encoder.\n5.1.4. Baselines\nFor weight-only quantization, we compare MBQ with the vanilla round-to-nearest quantization (RTN), GPTQ [15] and AWQ [25] under W3, which is based on channel-wise equalization. We apply group-wise asymmetric quantization for each method and keep the group size at 128. For weight-activation quantization, we compare with both the RTN and SmoothQuant [39] under W4A8, which also relies on channel-wise equalization. As discussed in SmoothQuant, we apply per-token symmetric quantization for activations and per-channel (output) symmetric quantization for weights to take advantage of low-precision tensor cores. The evaluation results of W4 and W8A8 are shown in Supplementary Sec. 8.\n5.2. Main Results\nFor Weight-only Quantization, as shown in Tab. 2, the results of RTN quantization indicate that smaller VLMs are more sensitive to weight-only quantization. The average accuracy loss for RTN-quantized 7B and 8B VLMs is 9.6%, significantly higher than the 1.5% loss seen in VLMs over 26B. This trend also aligns with observations in LLMs [23, 41].\nThe proposed MBQ can significantly outperform the AWQ baseline across different families. Especially within the LLaVA-onevision family, MBQ achieves an average accuracy improvement of 4.2% on the 7B VLM and 18.5% on the 72B VLM compared to AWQ. It is worth noting that for the LLaVA-onevision-72B VLM, AWQ even shows a 17% performance drop compared to RTN. Instead, the proposed MBQ can significantly improve the average accuracy and surpass RTN quantization. For the InternVL2 and Qwen2-VL families, MBQ can also outperform the RTN and AWQ baselines by around 1%.\nFor Weight-Activation Quantization, similar to weight-only quantization, MBQ can significantly outperform SmoothQuant and RTN baselines, with improvements of up to 11.6%. In many cases, the average performance of SmoothQuant is lower than that of RTN quantization, especially in InternVL2-26B under W4A8. The results indicate that with activation quantization, our insight into modality-balancing becomes more crucial for improving the performance of quantized VLMs, as the core idea of modality-balancing mainly addresses the sensitivity differences among various modalities within the activations.\n5.3. Ablation Study\n5.3.1. The Effect of Calibration Datasets\nAs AWQ and SmoothQuant are designed for LLM quantization, they use the Pile [16] validation dataset during calibration, which only contains language data.\nDirectly apply the vision-language dataset as calibration does not consistently improve the performance of the quantized VLMs. As shown in Tab. 3, when we use the COCO caption dataset as the calibration dataset, the performance of AWQ W3 can significantly increase by 2.1% and 10.3% on MMMU and SEED datasets, respectively. However, the performance of SmoothQuant W4A8 with the COCO caption calibration dataset significantly decreases by 1.7% and 31.4% on MMMU and SEED datasets. We speculate that this is because weight-activation quantization requires considering both weight and activation quantization errors. Ignoring the sensitivity differences between vision and language tokens in activations leads to a significant performance drop in SmoothQuant, even falling behind RTN quantization.\n5.3.2. The Effect of Modality-Balance\nModality-Balancing plays a crucial role in weight-activation quantization and can also improve the performance of weight-only quantization. As shown in Tab. 3, with the help of Modality-Balancing (MAE), the SmoothQuant with COCO calibration can significantly improve 13.4% and 54.2% on MMMU and SEED, respectively. For the weight-only quantization, Modality-Balancing (MAE) can increase the accuracy by 3.3% and 4.6% on MMMU and SEED, respectively. This significant performance improvement confirms the importance of accounting for the varying sensitivities of different modalities during the calibration process.\nIn addition, we find that Modality-Balancing (MAE) can consistently outperform Modality-Balancing (MSE) in both weight-only and weight-activation quantization. For both W3 and W4A8, Modality-Balancing with MAE reconstruction loss can achieve over 1% accuracy improvement on both MMMU and SEED datasets. Therefore, we recommend using the MAE reconstruction loss form, derived directly from the gradient of the activation with respect to the SFT loss, rather than the MSE reconstruction loss.\n5.3.3. Quantize Both Visual Encoder and VLM\nIn Sec. 4, we analyze the different efficiency bottlenecks of the ViT encoder and VLM. In real-world applications, we need to quantize both components for higher acceleration ratios. As shown in Tab. 4, we quantize the ViT encoder with SmoothQuant and VLM with MBQ in LLaVA-onevision-7B and evaluate the accuracy on MMMU and VizWiz benchmarks.\nExperimental results show that applying W4A8 quantization to the ViT encoder does not lead to a significant performance drop; instead, it even improves performance on some benchmarks. This suggests that the ViT encoder is not particularly sensitive to quantization, possibly due to the redundancy in vision tokens discussed in Sec. 1. Therefore, quantizing the ViT encoder alongside the VLM quantization is feasible from an algorithmic performance perspective.\n5.3.4. The Performance on Language-only Benchmark\nThe main idea of the proposed MBQ is to consider the sensitivity across different modalities during quantization, aiming to enhance performance in both vision-language and language-only tasks. Accordingly, we evaluated the performance of the quantized LLaVA-onevision-7B VLM on the MMLU benchmark with different quantization methods.\nAs shown in Tab. 5, with W3 and W4A8 quantization, MBQ achieves a performance improvement of 0.9% and 2%, compared to AWQ and SmoothQuant. This result demonstrates that for quantized VLMs, considering the sensitivity differences across modalities not only significantly reduces the performance loss on vision-language tasks, but also helps maintain performance on language-only tasks."}, {"title": "5.4. Efficiency Evaluation", "content": "5.4.1. Single Kernel Performance\nWe evaluate the speed of the proposed fused W3 GEMV kernel on the RTX 4090 GPU, testing linear layers with different weight matrix shapes in LLaVA-onevision-7B. Specifically, as shown in Tab. 6, we evaluate the kernel on 4 different shapes of weight matrices, including 3584 \u00d7 3584 (the out_proj layers), 3584\u00d710752 (the qkv_proj layers), 3584\u00d718944 (the up_proj and gate_proj layers), and 18944\u00d73584 (the down_proj layers). For each shape, we compare the proposed fused W3 GEMV kernel with the original FP16 GEMV kernel.\nThe evaluation results show that the fused W3 GEMV kernel achieves a speedup of 1.2x to 5.0\u00d7 across 4 different shapes, compared to the FP16 GEMV baseline. As the matrix size increases, the fused W3 kernel achieves a more significant speedup. This is because GEMV performance is memory-bound, and the impact of memory access speed becomes more pronounced with larger weight matrices. By applying W3 quantization, memory access latency is significantly reduced in large matrix GEMV operations, leading to a greater speedup.\n5.4.2. End-to-End Performance\nWith the help of the proposed W3 GPU kernel and the W4A8 GPU kernel from Qserve [27], we evaluate the latency of both the ViT encoder and the VLM in LLaVA-onevision-7B on RTX 4090. We run both the ViT encoder and VLM with FlashAttention-2 [13]. The evaluation results are shown in Tab. 7.\nFor the ViT encoder, the embedding layer will convert each input image into 729 (27 \u00d7 27) tokens as the input. As discussed in Sec. 4, the ViT encoder only has the prefill stage, the main operators are GEMMs, which is compute-bound. In this case, we apply the W4A8 kernel to the ViT encoder and achieve 1.15\u00d7 speedup.\nFor the VLM, as discussed in Sec. 2.1, the VLM has two distinct stages, and each stage requires different quantization schemes for inference acceleration.\nSpecifically, in order to accelerate the prefill stage, we need to apply weight-activation quantization to use the low-precision tensor cores. With W4A8 quantization, we evaluate the inference latency of the quantized VLM with the open-source GPU kernel. As shown in Tab. 7, when the input token length is 512 and 1024 tokens, W4A8 can achieve 1.16\u00d7 and 1.18\u00d7, respectively. As the token length increases, the speedup gradually becomes greater.\nIn order to accelerate the decode stage, we apply W3 quantization to the VLM. Since the latency of the decode stage remains similar across different input token lengths, we measure the average latency of the decode stage at input token lengths of 128, 256, 512, and 1024. As shown in Tab. 7, both W4A8 and W3 quantization can accelerate the decode stage. However, the W3-quantized VLM is 1.23\u00d7 faster than the W4A8 model. This result primarily stems from two factors: (1) the W3-quantized model has lower memory access overhead for large weight matrices compared to W4A8, and (2) the W4A8 operator requires additional dynamic quantization of activations, which incurs additional computational time.\nFor practical acceleration, we use weight-activation quantization to accelerate the ViT encoder and weight-only quantization to speed up the decode stage of VLMs. The main reason is that the FP16 VLM's prefill stage takes as long to process 1,024 tokens as the decode stage takes to generate just 4 tokens, as shown in Tab. 7. In real-world applications, the number of tokens to be decoded is large, necessitating multiple forward passes during the decode stage, which is the most time-consuming part of generation tasks. Thus, weight-only quantization is more suitable for VLMs."}, {"title": "6. Conclusion", "content": "In this paper, we identify a key phenomenon: the sensitivities of vision and language tokens to quantization differ significantly. Based on this finding, we propose Modality-Balanced Quantization (MBQ), a simple but effective quantization method to improve the accuracy of quantized VLMs for both weight-only and weight-activation quantization. Specifically, we use the gradients of the SFT loss function w.r.t. vision and language tokens as sensitivity indicators to balance the reconstruction loss during calibration.\nExperiments show that MBQ can outperform existing SOTA PTQ methods by 4.4% and 11.6% on both W4A8 and W3 settings across various VLMs. With the custom-designed W3 quantization kernel, we can achieve 1.4\u00d7 decoding speedup compared with the Pytorch baseline."}, {"title": "7. Related Work", "content": "7.1. LLM Quantization\nPost-Training Quantization (PTQ) techniques are widely used in LLMs to accelerate the inference process. They employ the low-precision data format and computation to reduce the memory and computation overhead.\nTo accelerate the memory-bound decode stage of LLMs, existing methods apply weight-only quantization to reduce the memory access overhead. GPTQ [15] quantizes one weight channel at each step and iteratively adjusts the un-quantized weights to mitigate reconstruction errors of each transformer block. AWQ [25] searches for proper channel-wise equalization factors by minimizing the block-wise reconstruction loss. SpQR and LLM-MQ [14, 22] propose mixed-precision quantization to allocate higher precision for weight outliers, while the rest of the weights are quantized to low-precision. QuIP [7] introduces LDLQ, an optimal adaptive method for a quadratic proxy objective. It reveals that ensuring incoherence between weight and Hessian matrices can enhance the effectiveness of LDLQ. QuIP utilizes LDLQ and achieves incoherence by employing random orthogonal matrix multiplication.\nTo accelerate the compute-bound prefill stage of LLMs, existing methods propose to use the weight-activation quantization to leverage faster low-precision tensor cores. SmoothQuant [39] employs a channel-wise equalization technique to address the challenges of quantizing activation values. This method expands the data range of weight channels while shrinking the data range of corresponding activation channels to achieve better data equalization. Omniquant [36] optimizes the boundaries for weight clipping and the scaling factor for equivalent transformation to minimize reconstruction errors block-by-block. Atom [47] employs a strategy involving mixed-precision and dynamic quantization for activations. Recently, many studies [3, 31] follow the computational invariance idea, by multiplying rotation matrices to the weight matrices and activation matrices.\nHowever, these methods focus solely on a single language modality without considering the differences between tokens from different modalities in multimodal scenarios, which is the core distinction between MBQ and existing quantization approaches. It is also worth noting that many existing studies search for various hyperparameters by minimizing reconstruction error, where MBQ can be used to achieve performance improvements with these methods on VLMs.\n7.2. Efficient VLM\nTo improve the efficiency of Large Vision-Language Models, existing work primarily focuses on designing lightweight modules, compressing vision tokens, and employing efficient model architectures.\nFirstly, for the lightweight model design, an effective approach is to incorporate efficient components within the VLMs. Some research [5, 48] directly utilizes pre-trained small language models with fewer than 3B parameters as their backbone, while others [12] train a small language model from scratch. For modality alignment, [1, 21] utilizes a lightweight transformer, while subsequent work [11, 20, 26, 29] directly adopts Linear or MLP layers to align the visual modality with the language model's latent space.\nSecondly, the number of vision tokens increases with image resolution, imposing a substantial computational burden on VLMs. To address this issue, [6, 35, 40] propose vision token reduction techniques to significantly lower the number of vision tokens, while some approaches [9, 28] remove redundant vision tokens to reduce computational overhead.\nFinally, in terms of efficient architectures, some work [24, 33] leverages the Mixture of Experts (MoE) architecture to enhance model performance without increasing active parameter counts, while others [34, 46] adopt efficient Mamba language models as the language backbone."}, {"title": "8. Additional Experiments", "content": "8.1. W4 and W8A8 Results on Large VLMS\nAs shown in Tab. 8, we present the evaluation results for W4 and W8A8 quantized VLMs from the LLaVA-onevision, InternVL2, and Qwen2-VL families. In most cases, the proposed MBQ achieves accuracy comparable to the AWQ and SmoothQuant baselines under W4 and W8A8 quantization. Furthermore, the average accuracy of the quantized VLMs is very close to that of the original FP16 VLMs, indicating that quantization under W4 and W8A8 is nearly lossless.\nA notable different case arises during the W4 quantization of LLaVA-onevision-72B, where AWQ significantly degrades the VLM's accuracy, with the average accuracy falling more than 10% below that of MBQ and RTN. A similar phenomenon also occurs during W3 quantization of LLaVA-onevision-72B in Tab. 2, demonstrating that the modality-balancing concept proposed by MBQ can more consistently maintain high model performance compared to SOTA quantization baselines, whether in high"}]}