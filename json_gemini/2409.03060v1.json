{"title": "Better Verified Explanations with Applications to Incorrectness and Out-of-Distribution Detection", "authors": ["Min Wu", "Xiaofu Li", "Haoze Wu", "Clark Barrett"], "abstract": "Building on VERIX (VERIfied explainability) (Wu, Wu, and Barrett 2023), a system for producing optimal verified explanations for machine learning model outputs, we present VERIX+, which significantly improves both the size and the generation time of verified explanations. We introduce a bound propagation-based sensitivity technique to improve the size, and a binary search-based traversal with confidence ranking for improving time-the two techniques are orthogonal and can be used independently or together. We also show how to adapt the QuickXplain (Junker 2004) algorithm to our setting to provide a trade-off between size and time. Experimental evaluations on standard benchmarks demonstrate significant improvements on both metrics, e.g., a size reduction of 38% on the GTSRB dataset and a time reduction of 90% on MNIST. We also explore applications of our verified explanations and show that explanation size is a useful proxy for both incorrectness detection and out-of-distribution detection.", "sections": [{"title": "1 Introduction", "content": "Explainable AI aims to extract a set of reasoning steps from the decision-making processes of otherwise opaque AI systems, thus making them more understandable and trustworthy to humans. Well-known work on explainable AI includes model-agnostic explainers, such as LIME (Ribeiro, Singh, and Guestrin 2016), SHAP (Lundberg and Lee 2017), and Anchors (Ribeiro, Singh, and Guestrin 2018), which provide explanations for neural networks by constructing a local model around a given input or identifying a subset of input features as \"anchors\u201d that (ideally) ensure a model's decision. While such methods can produce explanations efficiently, they do not provide formal guarantees and thus may be inadequate in high-stakes scenarios, e.g., when transparency and fairness are paramount.\nFormal explainable AI (Marques-Silva and Ignatiev 2022) aims to compute explanations that verifiably ensure the invariance of a model's decision. One such explanation is a minimal set of input features with the property that regardless of how the remaining features are perturbed, the prediction remains unchanged. The intuition is that these features capture an amount of (explicit or implicit) information in the input that is sufficient to preserve the current decision. The simplest approaches allow unbounded perturbations (Ignatiev, Narodytska, and Marques-Silva 2019; Shih, Choi, and Darwiche 2018; Darwiche and Hirth 2020), which may be overly lenient in some cases, potentially leading to explanations that are too course-grained to be useful. As an alternative, (La Malfa et al. 2021) and (Wu, Wu, and Barrett 2023) generalize these approaches by allowing both bounded and unbounded perturbations, computing explanations with respect to such perturbations for natural language processing and perception models, respectively. The former primarily perturbs each word in a text with a finite set of its k closest neighbors and thus has a discrete perturbation space; the latter considers \u03b5-ball perturbations over continuous and dense input spaces. The algorithm presented in (Wu, Wu, and Barrett 2023) is a naive and well-known approach: it simply traverses the features one by one and checks formally whether any of them can be discarded. This approach sometimes yields overly conservative explanations that are inefficient to compute.\nIn this paper, we explore methods for computing better explanations by significantly improving both the size and the generation time. We also demonstrate two applications that illustrate the usefulness of such explanations in practice. Our contributions can be summarized as follows.\n\u2022 We utilize bound propagation-based techniques to obtain more fine-grained feature-level sensitivity information, leading to better traversal orders which in turn produce smaller explanation sizes.\n\u2022 We propose a binary search-inspired traversal approach to enable processing features in a batch manner, thus significantly reducing the time required to generate explanations. We also incorporate a simple but efficient confidence ranking strategy to further reduce time.\n\u2022 We adapt the QuickXplain algorithm (Junker 2004) to provide a trade-off between explanation size and generation time, and notice that our adaptation is an optimization of (Huang and Marques-Silva 2023).\n\u2022 We demonstrate the usefulness of our explanations with practical applications to detecting incorrect predictions and out-of-distribution samples."}, {"title": "2 VERIX+: Verified eXplainability plus", "content": "Let f be a neural network and x an input consisting of m-dimensional features (x1,...,xm). The set of feature indices {1, ..., m} is written as \u0398(x), or simply O, when the context is clear. To denote a subset of indices, we use A \u2208 \u0398(x), and XA denotes the features that are indexed by the indices in A.\nWe write f(x) = c for both regression models (c is a single quantity) and classification models (c \u2208 C is one of a set of possible labels). For the latter case, we use y = (y1,..., yn) to denote confidence values for each label in C, e.g., the predicted class c = arg max(y). For image classification tasks, x is an image of m pixels, the values in y represent the confidence values for each of the n labels in C, and yc is the maximum value in y, where c is the predicted label."}, {"title": "2.1 Optimal verified explanations", "content": "We adopt the definition of optimal robust explanations from (La Malfa et al. 2021; Wu, Wu, and Barrett 2023) (see also (Shih, Choi, and Darwiche 2018; Ignatiev, Narodytska, and Marques-Silva 2019; Darwiche and Hirth 2020)). For a neural network f and an input x, we compute a minimal subset of x, denoted by XA, such that any e-perturbations imposed on the remaining features do not change the model's prediction.\nDefinition 2.1 (Optimal Verified Explanation (Wu, Wu, and Barrett 2023)). Given a neural network f, an input x, a manipulation magnitude e, and a discrepancy d, a verified explanation with respect to norm p \u2208 {1,2,\u221e} is a set of input features X\u0104 such that if B = \u0398(x) \\ A, then\n\u2200XB. ||XB - XB||p \u2264 \u03b5 \u21d2 |f(x) - f(x')| \u2264 \u03b4, (1)\nwhere x is some perturbation on the irrelevant features XB and x' is the input variant combining XA and xp. We say that the verified explanation XA is optimal if\n\u2200 x \u2208 XA. \u2203x', XB. || (xUxB) - (x' UXB)||p\n<\u03b5\u2227 |f(x) - f(x')| > \u03b4, (2)\nwhere x' is some perturbation of x \u2208 XA and U denotes concatenation of features.\nSuch explanations are both sound and optimal by Equations (1) and (2), respectively (Wu, Wu, and Barrett 2023). Note that the optimality we define here is local as it computes a minimal (not minimum) subset. A minimum subset is called a global optimum, also known as the cardinality-minimal explanation (Ignatiev, Narodytska, and Marques-Silva 2019). However, finding the global optimum is generally too computationally expensive to be practically useful, as it requires searching over an exponential number of local optima."}, {"title": "2.2 Workflow of VERIX+ in a nutshell", "content": "We present the overall workflow of our VERIX+ framework in Figure 1. Starting from the left, the inputs are a network f and an input x. The first step is to obtain a sensitivity map of all the input features and, by ranking their individual sensitivity, produce a traversal order. We introduce a new bound propagation-based technique (Algorithm 1) for obtaining more meaningful sensitivity maps and thus better traversal orders, which, in turn, reduce explanation sizes.\nThe traversal order is then passed to the main traversal algorithm, which computes optimal verified explanations. We propose two new optimizations, one based on binary search (Algorithm 2) and one adapted from the well-known QuickXplain algorithm (Junker 2004) (Algorithm 4). These can significantly reduce computation time compared to the existing sequential method, which processes features one by one. The key difference between these two is that the former reduces the time but not the size, as it does not change the traversal order, whereas the latter improves both size and time. Compared to the binary search-based technique, the QuickXplain technique computes comparatively smaller-sized explanations but takes more time, thus providing an alternative point in the size-time trade-off.\nThe CHECK procedure (Algorithm 3) is used by the traversal methods to formally check the soundness of a candidate explanation. We also add a simple but efficient confidence ranking algorithm which further reduces generation time. The confidence ranking is orthogonal to the other optimizations and benefits all three traversal approaches mentioned above.\nOnce the optimal verified explanation XA has been computed for network f and input x, it can be used in various ways. We highlight two applications: detecting incorrect predictions and detecting out-of-distribution examples. We show that explanation size is a useful proxy for both tasks. These applications are described in detail in Sections 4.3 and 4.4."}, {"title": "3 Methodological advances for explanation size and generation time", "content": "In this section, we discuss in detail several optimizations for improving both the size and the generation time of optimal verified explanations. The bound propagation-based sensitivity ranking discussed in Section 3.1 improves size, and the binary search-based traversal discussed in Section 3.2 improves time, so does the confidence ranking discussed in Section 3.3. Section 3.4 discusses how an adaptation of the QuickXplain algorithm (Junker 2004) enables an additional size-time trade-off."}, {"title": "3.1 Improving size: a bound propagation-based traversal order", "content": "In (Wu, Wu, and Barrett 2023), a traversal order is computed based on a heuristic that measures how sensitive a model's confidence is to each individual feature xi by imposing simple feature deletion (xi = 0) or reversal (1 \u2013 xi). Although this produces a reasonable ranking of the input indices, it ignores the fact that explanations are based on \u03b5-perturbations. We show how utilizing the perturbation information at the sensitivity phase can produce better traversal orders.\nProcedure TRAVERSALORDER in Algorithm 1 computes \u03b5-bounds for each feature of the input and then ranks these bounds to obtain an order of feature indices. We introduce variables \\hat{x} which represent symbolic inputs to f (Line 3).\n\u03c0\u2192 arg sort(e-bounds, descending)"}, {"title": "3.2 Improving time: a binary search-based traversal", "content": "Given a traversal order, the algorithm of (Wu, Wu, and Barrett 2023) simply processes the features one by one in a sequential order. Here, we propose an improvement that processes the features using an alternative approach inspired by binary search. The new algorithm searches for batches of consecutive irrelevant features. It simultaneously checks the whole batch to see whether it is irrelevant. If so, there is no need to process the features in the batch one by one. We note that this approach does not change the traversal order, so the computed explanation is the same as that computed by the original sequential algorithm.\nAs a further optimization (not shown), if the input data are known to be bounded as part of the problem definition, we intersect the interval [xi \u2013, xi + \u03b5] with the known bound."}, {"title": "3.3 Improving time: introducing a confidence ranking", "content": "The CHECK procedure checks whether a model's decision is invariant under \u03b5-perturbations of a specified subset of input features. That is, it must check whether the output c is in an allowable range for regression, i.e., |c \u2013 c'| < \u03b4, or whether the confidence of the predicted class yc is always the greatest among all classes, i.e., yc = max(y). In the latter case, this means that in the worst case, |y - 1 separate checks are needed to ensure that yc is the largest. In previous work (Wu, Wu, and Barrett 2023), these checks are done naively without any insight into what order should be used. In this work, we propose ranking these checks based on the confidence values of the corresponding outputs. We then proceed to do the checks from the most to the least likely classes. If a check fails, i.e., invariance of the model prediction is not guaranteed, this is typically because we can perturb the inputs to produce one of the next most likely classes. Thus, by checking the most likely classes first, we avoid the need for additional checks if one of those checks already fails.\nAlgorithm 3 shows the CHECK procedure which checks whether imposing \u03b5-perturbations on certain features xe of input x maintains the decision of model f: if yes, it returns True, meaning that these features are irrelevant. To start with, we use variables x and y to represent the inputs and outputs of the model and set y to be the logits and c to be the predicted class. In Line 4, we rank the confidence values of all the classes in a descending order. In other words, we prioritize the classes that are most likely after c. We allow \u03b5-perturbation on features in xe while fixing the others (Lines 5\u20138). For each class j in the sorted ranking (excluding c as this is the predicted class to be compared with), we call SOLVE to examine whether the specification \u03c6 \u21d2 \u0177c < \u0177j holds, i.e., whether the input constraints \u03c6 allow a prediction change with \u0177e smaller than \u0177j (Line 10). If the exitCode of SOLVE is UNSAT, then it means the specification is unsatisfiable in the sense that \u0177e will always be greater than or equal to \u0177j, i.e., the prediction cannot be manipulated into class j. The for loop (Lines 9\u201314) examines each class in ranking \\ c, and if all checks are UNSAT (algorithm returns True in Line 15), then \u0177 is ensured to be the greatest among \u0177, i.e., prediction invariance is guaranteed. Otherwise, if SOLVE returns SAT or Unknown for any \u0177j, the algorithm returns False. The key insight is that SOLVE is more likely to return SAT for classes with higher confidence, and once it does, the algorithm terminates. In practice, SOLVE can be instantiated with off-the-shelf neural network verification tools (Singh et al. 2019; M\u00fcller et al. 2022; Katz et al. 2019; Wang et al. 2021; Henriksen and Lomuscio 2020; Wu et al. 2024)."}, {"title": "3.4 A trade-off between size and time: QUICKXPLAIN", "content": "In previous sections, we propose approaches to orthogonally improve explanation size and generation time; in this section, we adapt the QuickXplain algorithm (Junker 2004) and optimize such adaptation (Huang and Marques-Silva 2023) to provide an additional trade-off between these two metrics. We remark that the QuickXplain-based approach works as an alternative to the binary search-based method, i.e., given a traversal order from the first step of our workflow, we can either use binary search or QuickXplain. The former improves time but does not affect size, whereas the latter affects both. In practice QuickXplain tends to produce smaller explanations but requires more time to do so. Confidence ranking benefits both techniques as it accelerates CHECK.\nWe present our QUICKXPLAIN adaptation in Algorithm 4. The function QXP(\u03a7\u03b1, \u03a7\u03b2, Xe) itself is recursive with three arguments: (1) the current explanation xa; (2) the current irrelevant set x\u03b2; and (3) the current (sub)set of input features that need to be analyzed, xe. These three sets always form a partition of the full set of features. To start with, xa and xB are initialized to \u00d8, and when QXP proceeds, irrelevant features are added into x\u03b2 in a monotonically increasing way; finally, after all features are done, xa is returned as the optimal explanation XA and x\u03b2 as the irrelevant set XB. Now we walk through the algorithm. Lines 3\u20137 considers the base case when xe has a single feature as in Algorithm 2. When there are more than one feature in xe, it is split into two subsets x4 and x (Line 8). In Lines 9-12, we check if the subset x4 or x\u0173 belongs to the irrelevant set: if True, then we add it into x\u03b2 when calling QXP to process the other subset. If neither x4 nor x is irrelevant, we take turns to process x4 and x\u0173 in Lines 13\u201322: the first if condition analyzes x and the second if processes x. If either of them has a single feature, then they are regarded as an explanation feature x (Line 15) or x\u2081 (Line 19) directly. This is to avoid the unnecessary execution of the very first if condition"}, {"title": "4 Experimental results", "content": "We have implemented the VERIX+ framework in Python. To realize the COMPUTEBOUND analysis in Algorithm 1, we utilize the bound-propagation\u00b3 package for fully-connected models and the auto_LiRPA\u2074 library for convolutional mod-els. While the latter also supports dense layers, the former computes tighter IBP bounds which, in our case, lead to smaller explanations. We use Marabous, a neural network ver-2This enables Algorithm 4 calling fewer numbers of the CHECK procedure (i.e., fewer oracle calls) than the similar adaptation of QuickXplain in (Huang and Marques-Silva 2023).\n3https://pypi.org/project/bound-propagation/\n4https://github.com/Verified-Intelligence/auto_LiRPA\n5https://github.com/NeuralNetworkVerification/Marabou"}, {"title": "4.1 Improvements for explanation size and generation time", "content": "We report on improvements to explanation size and generation time in Table 1, accompanied with example explanations in Figure 2. For each data point in the table, we collect \"valid\" explanations (i.e., excluding examples that are robust to \u03b5-perturbation) for the first 100 test images (to avoid selection bias) and take the average time and size. Experiments were performed on a workstation equipped with 16 AMD RyzenTM 7 5700G CPUs and 32GB memory running Fedora 37.\nFrom the MNIST-FC data points highlighted in yellow, we observe that probability ranking decreases time from 92.3 to 81.4, and binary search-based traversal further reduces it to 29.6. Similarly, for MNIST-CNN, time is significantly reduced from the original 439.2 to 46.3. Moreover, applying bound propagation-based traversal ultimately reduces time to 28.4 and 42.0 (~10 times faster) accordingly, as it adjusts the traversal order during computation and thus affects time and size simultaneously. That said, on MNIST-CNN, VERIX+ achieves 90.43% reduction in time over VERIX. We notice that size reduction on MNIST is not significant. We speculate that this is because the explanations obtained using the original approach (Wu, Wu, and Barrett 2023) are already close to the global optima, so there is little room for improvement. We show an example explanation for the MNIST-CNN model in Figure 2a. For the digit \"2\", we can see that both explanations focus on the central region as intuitively turning these pixels from black into white may change the prediction into \u201c8\u201d. In comparison, VERIX+ reduces both size and time.\nOn the other hand, size reduction is more substantial on GTSRB and CIFAR10, as shown by the data points in blue. For instance, using heuristic sensitivity on GTSRB-FC, binary search-based traversal has size 529.4 with a reduction of time from 614.9 to 233.8. When deploying Quick-Xplain, size is reduced to 485.0 with a slight overhead, i.e., a trade-off between size and time. Meanwhile, applying bound propagation-based traversal further reduces size to 333.4. Similarly, for GTSRB-CNN, size decreases from 569.0 to 355.3, i.e., an overall reduction of 37.56%. As for CIFAR10-FC, size is reduced from 588.9 to 465.8 when applying bound"}, {"title": "4.2 Explanations for normally and adversarially trained models", "content": "In Table 2, we compare our explanations for normally and adversarially trained MNIST-FC models on original and ma-licious samples. We describe the adversarial training for the MNIST-FC-ADV model in Appendix C.2. We produce explanations for both correct and incorrect samples. For each data point, we collected 300 samples and reported their average explanation size (excluding \u03b5-robust samples). Overall, we observe that both models have smaller explanations for original samples over malicious ones, and for correct predictions over incorrect ones. In comparison, the adversarially trained MNIST-FC-ADV model produces smaller explanations than MNIST-FC under all conditions. For instance, for the original and correct samples, MNIST-FC-ADV produces 27.64% smaller explanations than its normal counterpart (128.22 vs. 177.20). This is expected since with adversarial training, the model learns more implicit information about the samples, e.g., which pixels likely contain the key information for pre-diction and which are trivial perturbations, and thus only needs to pay attention to fewer yet indispensable pixels to make a correct decision. In Figure 10 of Appendix C.2, we show examples when MNIST-FC-ADV produces smaller explanations for both original and malicious inputs. This is also reflected in the \u03b5-robust rate, which increases from 4% to 52.3% after adversarial training, since now the model has learned to capture these principal features in the input and become less sensitive to perturbations."}, {"title": "4.3 Using explanation size to detect incorrect predictions", "content": "We demonstrate that explanation size is a useful proxy for detecting incorrect examples. Specifically, we collected 1000 samples - 500 correctly and 500 incorrectly classified \u2013 for both MNIST and GTSRB, and present our analysis in Figures 3 and 6 of Appendix B.1. For MNIST, Figure 5 of Appendix B.1 shows that there exists a significant separation between the correct and incorrect clusters. Previously, (Hendrycks and Gimpel 2017) proposed using the maximum softmax probabilities to detect erroneously classified examples. Following this, in Figures 3a and 6c, we plot the receiver operating characteristic (ROC) curves and compute the AUROC (area under the ROC curve) values when using maximum confidence (gray) and explanation size (blue) independently. We observe that our explanation size has a competitive effect on both datasets. Furthermore, utilizing both confidence and size, we trained classifiers to detect whether the prediction of an unseen sample is correct or erroneous."}, {"title": "4.4 Using explanation size to detect out-of-distribution examples", "content": "We also show that explanation size can help detect out-of-distribution (OOD) samples. Consider the scenario in which CIFAR10 and GTSRB images are fed into the MNIST-CNN model as OOD samples (we crop the former two to let them have the same size as MNIST). We use the OpenCV library to convert color images into gray-scale. The goal is to preserve the primary semantic meanings at the center of these images. We collected 900 samples in total \u2013 300 each from MNIST, CIFAR10, and GTSRB, and plotted their maximum softmax probability and explanation size, as shown in Figures 4a and 8a of Appendix B.2. We observe a significant separation between the in- and out-of-distribution samples. The existing work (Hendrycks and Gimpel 2017) mentioned above used the maximum softmax probabilities for OOD detection also. Following this, we compare the effect of using maximum confidence and explanation size to detect OOD samples from CIFAR10 and GTSRB. From Figures 4b and 4c, we can see that explanation size yields better ROC curves for both datasets and also achieves higher AUROC values, i.e., 93% on CIFAR10 and 98% on GTSRB. We perform similar OOD detection on the MNIST-FC model, and show the advantages of using explanation size in Figure 9 of Appendix B.2."}, {"title": "5 Related work", "content": "Several approaches to formal explanations (Marques-Silva and Ignatiev 2022) have been explored in the past. (Ignatiev, Narodytska, and Marques-Silva 2019) first proposed using ab-ductive reasoning to compute formal explanations for neural networks by encoding them into a set of constraints and then deploying automated reasoning systems such as Satisfiabil-ity Modulo Theories (SMT) solvers to solve the constraints. (La Malfa et al. 2021) brings in bounded perturbations, k-nearest neighbors and \u03b5-balls to produce distance-restricted explanations for natural language models. Adapting the \u03b5-perturbations to perception models for which inputs tend to be naturally bounded, (Wu, Wu, and Barrett 2023) proposes a feature-level sensitivity heuristic to obtain a ranking of the input features and thus produce empirically small explanations. In this paper, we utilize bound propagation-based techniques to obtain more fine-grained feature-level sensitivity. Compared to the existing heuristic from (Wu, Wu, and Barrett 2023) and later adopted by (Izza et al. 2024), we obtain \u03b5-perturbation dependent traversal orders that lead to even smaller explanation sizes. To reduce generation time, (Huang and Marques-Silva 2023) mimics the QuickXplain al-gorithm (Junker 2004) to avoid traversing all the features in a linear way (experiments only include the linear case though). Our optimization of this QuickXplain adaptation further reduces the number of oracle calls; we also perform a complete experimental comparison between the linear and non-linear approaches. Additionally, we introduce binary search-based traversals to further improve time. The Dichotomic search method from (Izza et al. 2024) is also inspired by binary search, but it includes unnecessary oracle calls, as it searches from the beginning every time after it finds a \"transition feature.\" Therefore, as the authors say in the paper, in their experiments, the number of oracle calls needed by their Di-chotomic search is \"always larger than\" the number of input features. In contrast, our recursive binary search-based algorithm does not have such redundant oracle calls and thus achieves much reduced generation time. Finally, our confidence ranking strategy accelerates the CHECK procedure, which benefits all such oracle calls. To the best of our knowledge, this is the first time that this strategy is proposed."}, {"title": "6 Conclusion and future work", "content": "We have presented the VERIX+ framework for computing optimal verified explanations with improved size and generation time. We also demonstrated the usefulness of the generated explanations in detecting incorrect and out-of-distribution examples. Future work could explore further techniques for improving the performance of verified explanation generation, perhaps by adapting parallel techniques from (Izza et al. 2024) or by finding ways to introduce approximations in order to gain scalability. It would also be interesting to evaluate explanations on other metrics such as usefulness to humans rather than simply size. Finally, we would like to explore additional applications, especially in areas where safety or fairness is crucial."}, {"title": "A.1 Proof for Theorem 3.1", "content": "Theorem 3.1 (Time Complexity). Given a network f and an input x = (x1,...,xm) where m \u2265 2, the time complexity of BINARYSEQUENTIAL(f, x) is 2 calls of CHECK for the best case (all features are irrelevant) and k2m = 2km+1 or k2m+1 = km+1 + km + 1 calls of CHECK for the worst case (all features are in explanation). When m = 1, it is obvious that it needs 1 CHECK call.\nProof. It is straightforward that, when x has a single feature, i.e., m = 1 and thus |xe| = 1, only the first if condition of Algorithm 2 will be executed. That said, only 1 call of the CHECK procedure in Line 5 is needed.\nFor the non-base case, when there are more than one input features, i.e., m \u2265 2, we analyze the time complexity of BINARYSEQUENTIAL(f, x) for the best case and the worst case separately.\n\u2022 In the best case, all the features are irrelevant so the input is essentially \u03b5-robust. The original input x is split into two subsets x4 and x\u0173 in Line 11. Algorithm 2 then terminates after two calls of CHECK in Lines 12 and 14 to examine whether x4 and x\u0173 are irrelevant, respectively. When CHECK returns True, they are put into the irrelevant set XB and the algorithm terminates. Therefore, for the best case, only 2 CHECK calls are needed.\n\u2022 In the worst case, all the input features are relevant. That is, the whole input x is an explanation. For this, we use km to denote the number of CHECK calls needed for an input that comprises m features, and use k2m and k2m+1 to denote that of an input comprising even and odd features, respectively.\nFor an input x that has 2m features (i.e., even features), when calling BINARYSEQUENTIAL, Line 11 splits it into two subsets x4 and x\u0173, each of which has m features. Then, the algorithm runs the CHECK procedure in Line 12 to check if the first subset x4 is irrelevant. It returns False as all features are relevant. Thus, the algorithm proceeds to the else condition in Line 21. Since x4 has m features and m \u2265 2, BINARYSEQUENTIAL(f, x4) in Line 25 is invoked. Afterwards, BINARYSEQUENTIAL(f, x\u0173) is invoked to check the second subset x\u0173. Each such instantiation of BINARYSEQUENTIAL takes the worst case as the whole input is an explanation. Therefore, for an input comprising 2m features, it will need km CHECK calls for subset x and km CHECK calls for subset x\u0173, as well as the extra 1 CHECK call in Line 21 at the beginning. That is, k2m = 2km + 1. This is the case when the input has even number of features.\nNow if it has odd features, e.g., 2m + 1, it will be split into two subsets with different sizes. In our case, when x is odd, we set |X4| = |xy| + 1. That is, the first subset x has m + 1 features and the second subset x has m features. Similarly, the algorithm needs to run the 1 CHECK in Line 12, and then proceeds to work with x4 and xy accordingly. Therefore, for an input that consists of 2m +1 features, we have k2m+1 = km+1 + km + 1."}, {"title": "A.2 Proof for Theorem 3.2", "content": "Theorem 3.2 (Time Complexity). Give a neural network f and an input x = (x1,...,xm) where m \u2265 2, the time complexity of QXP(\u00d8, \u00d8, x) is [log2 m] + 1 calls of CHECK for the best case (all features are irrelevant) and (m-1) \u00d7 2 calls of CHECK for the worst case (all features are in explanation). When m = 1, it is obvious that it needs 1 CHECK call.\nProof. It is straightforward that, when x has a single feature, i.e., m = 1 and thus |xe| = 1, only the first if condition of Algorithm 4 will be executed. That said, only 1 call of the CHECK procedure in Line 3 is needed.\nFor the non-base case, when there are more than one input features, i.e., m \u2265 2, we analyze the time complexity of QXP(0, 0, x) for the best case and the worst case separately.\n\u2022 In the best case, all features are irrelevant so the input is essentially \u03b5-robust. The original input x is split into two subsets x4 and x\u0173 in Line 8. Then in Line 9 the CHECK procedure examines if the first subset x is irrelevant. Since this is the best case when all features are irrelevant, it will return True. Then Line 10 will be executed to recursively call the QXP function to process the second subset x\u0173. In the new instantiation, similarly Lines 8, 9, and 10 will be executed so the QXP function is revoked again. This recursion continues until there is only one feature in the candidate feature set xe. Then in Line 4, the CHECK procedure examines the single feature, and the algorithm returns. That said, the number of CHECK calls is how many times the original input x can be divided into two subsets until there is only one feature left, plus the final CHECK call to process the last single feature. In other words, it is the number of times we can divide m by 2 until we get 1, which is [log2 m], plus the extra 1 call, hence [log2 m + 1. Here, the floor function [] is to accommodate the situation when log2 m is not an integer, since we split x as |x| = |x4| + 1 when x has odd features.\n\u2022 In the worst case, all the input features are relevant. That is, the whole input x is an explanation. After splitting x into two subsets x4 and x\u0173 in Line 8, the CHECK procedure examines whether x4 and x are irrelevant features in the if conditions in Lines 9 and 11, respectively. Both will return False as all input features are relevant for this case. Then the QXP function is recursively called to process the first subset x in Line 17 and the second subset x in Line 21. These new instantiations will have similar executions as above, until eventually each subset contains only a single feature. Then, in Lines 15 and 19, subsets x and x will be directly regarded as explanation features. Note that here we do not need another round of calling the QXP function, as we already know from Lines 9 and 11 that x4 and x\u0173 are not irrelevant thus in the explanation. That said, every time the candidate set xe is split into two subsets, two CHECK calls are needed for both subsets. Such division terminates when each input feature is in an individual subset. Since for an input x that has m features, 1 divisions are needed, therefore the total number of CHECK calls is (m - 1) \u00b7 2."}, {"title": "B.1 Detecting incorrect examples", "content": "Modern neural networks have the tendency of being overly confident and making incorrect predictions with higher confidence compared to actual prediction accuracies (Guo et al. 2017; Hein, Andriushchenko, and Bitterwolf 2019). However, for safety-critical applications, we hope to ensure networks flag potentially incorrect predictions for human intervention. In addition to MNIST results presented in Figure 3 of Section 4.3, we additionally present GTSRB results in Figure 6 to illustrate generalization."}, {"title": "B.2 Detecting out-of-distribution examples", "content": "Neural networks sometimes have high-confidence predictions when the test distribution is different from the training distribution, which poses a safety concern (Hein, Andriushchenko, and Bitterwolf 2019).We experimented using VERIX+ explanations as a method of detecting out-of-distribution samples. We trained models on the MNIST dataset and used GTSRB and CIFAR-10 images as out-of-distribution samples. We treated out-of-distribution as positive and evaluated the AUROC value using the softmax probability baseline as proposed by (Hend"}]}