{"title": "Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey", "authors": ["QI DONG", "RUBING HUANG", "CHENHUI CUI", "DAVE TOWEY", "LING ZHOU", "JINYU TIAN", "JIANZHOU WANG"], "abstract": "Short-Term Electricity-Load Forecasting (STELF) refers to the prediction of the immediate demand (in the next few hours to several days) for the power system. Various external factors, such as weather changes and the emergence of new electricity consumption scenarios, can impact electricity demand, causing load data to fluctuate and become non-linear, which increases the complexity and difficulty of STELF. In the past decade, deep learning has been applied to STELF, modeling and predicting electricity demand with high accuracy, and contributing significantly to the development of STELF. This paper provides a comprehensive survey on deep-learning-based STELF over the past ten years. It examines the entire forecasting process, including data pre-processing, feature extraction, deep-learning modeling and optimization, and results evaluation. This paper also identifies some research challenges and potential research directions to be further investigated in future work.", "sections": [{"title": "1 INTRODUCTION", "content": "Electricity-Load Forecasting (ELF) aims to meet power systems' daily operational, management, and planning needs. This can provide essential guidance and reference points for system operators and planners. The absence of large-scale energy storage technologies means that power systems must ensure a constant power supply to meet current demands [134]. This means that ELF has become an essential component in the planning, scheduling, and operational management of power systems. Short-Term Electricity-Load Forecasting (STELF) uses historical load data to predict future loads, over a period ranging from several hours to a few days. STELF is a time-series forecasting task primarily used for the short-term scheduling of smart grids (including equipment maintenance, load distribution, and unit startup and shutdown), and for determining electricity prices [176]. Data from a British electricity company in 1984 showed that a 1% reduction in forecasting error could save \u00a310 million annually [18]. Increasing the STELF accuracy could improve planning and scheduling, and reduce operational costs for power systems [102]. This practical impact of STELF has led to an increasing amount of attention from researchers. Our review of the literature from the past decade highlights that most load forecasting articles have focused on STELF [30, 57, 61, 76, 83, 138].\nMany factors can impact the electricity load, including climate, weather, economic conditions, seasonality, and electricity prices. Furthermore, advances in smart-grid technologies, and the wide- spread adoption of smart meters and other sensors have significantly increased both the complexity and the volume of electricity-load data. The data exhibits strong non-linearity, randomness, volatil- ity, and complexity. This is a challenge for STELF. An accurate, robust, and fast STELF model is essential for the reliable daily operations of power systems [170], and the development of efficient forecasting models has become an important STELF research goal [41].\nELF has been extensively studied since the 1970s, with various methods having been proposed [45]. STELF methods can be broadly categorized into three types: statistical methods, machine learning methods, and deep learning methods [42]. Statistical methods perform well when dealing with linear relationships, but are often inadequate for handling the nonlinear patterns commonly found in electricity-load data. Machine learning methods, such as support vector machines and decision trees, typically perform well with simple or moderately complex data patterns. Deep learning methods can capture and model complex nonlinear relationships through their multi-layered structures, which is particularly important for predicting dynamic changes in electricity loads. In summary, while traditional statistical and machine learning methods have their strengths, deep learning techniques are more suited to the dynamic and nonlinear characteristics of electricity-load data.\nDeep learning approaches are among the most revolutionary breakthroughs in the fields of computer science and artificial intelligence in recent years [105]. The concept of deep learning builds on earlier work on Artificial Neural Networks (ANNs) [71], which were a type of shallow learning model [143], consisting of an input layer, a hidden layer, and an output layer [6]. ANNs were used in early STELF studies [67, 133]. Deep Neural Networks (DNNs), a type of ANN with multiple hidden layers, can also be used for STELF [5, 77, 103]. The multiple hidden layers in DNNs enable a complex computational framework that uses features as inputs to represent different levels of data abstraction. Through a cascading network structure, each layer in the DNN is capable of extracting and recognizing different features of the data, forming a hierarchy from basic to advanced features, thereby significantly enhancing both the models flexibility and its ability to\nhandle complex issues. Recurrent Neural Networks (RNNs) [139] are DNNs that were designed specifically to process sequential data, making them highly suitable for time-series prediction tasks. Although RNNs are theoretically ideal for time-series data, they may face challenges, like vanishing or exploding gradients in practical applications [154]. Long Short-Term Memory (LSTM) [72] and Gated Recurrent Units (GRUs) [169] have addressed gradient vanishing, making them more effective for practical applications. These advanced technologies, capable of handling large-scale, high-dimensional, and nonlinear data, provide more accurate and flexible solutions for STELF, making deep learning the preferred technique for STELF [2, 32, 106].\nPrevious STELF reviews have compiled and examined the research achievements from various perspectives, examining all types of models, or concentrating on certain steps of the forecasting process [3, 4, 81]. Akhtar et al. [3], for example, reviewed various STELF models (including time series and regression models), rather than focusing only on Artificial Intelligence (AI) models. Although Hou et al. [81] reviewed load forecasting based on AI models, focusing on data processing and prediction models. Al Mamun et al. [4] reviewed load forecasting, but only focused on hybrid models based on machine learning algorithms. To date, there has been no comprehensive and exhaustive review based on deep learning for STELF that covers the entire forecasting process. This paper aims to fill this gap in the literature.\nThis article explores the application of deep learning in STELF, providing a comprehensive review of current relevant research. The entire STELF process is examined through a comprehensive review of the literature from 2014 to 2023. The paper is guided by eight research questions (RQs), each of which addresses a key aspect of the STELF process. This survey paper addresses the following key points: (1) a summary and analysis of the literature search results; (2) a classification and description of electricity load datasets; (3) an introduction to STELF data preprocessing methods; (4) an analysis of feature extraction; (5) a description, classification, and summary of STELF models based on deep learning; (6) a review of the optimization process; (7) a summary of evaluation metrics; and (8) a discussion of the challenges and trends for the future of STELF.\nThe rest of this paper is organized as follows: Section 2 introduces some background information about the formal description of STELF and the basic deep learning models. Section 3 explains the methodology of this review, including the eight RQs related to STELF, the literature retrieval methods, and the statistical results of the retrieval. Sections 4 to 11 answer the eight RQs from Section 3, respectively. Finally, Section 12 concludes the paper."}, {"title": "2 BACKGROUND", "content": "In this section, we introduce the task definition of STELF and the basic deep learning methods. The primary objective is to quickly familiarize readers with STELF and provide them with an initial understanding of deep learning methods."}, {"title": "2.1 STELF Task Definition", "content": "STELF aims to predict the electricity load over a future period ranging from a few hours to several days. The model's input consists of historical load data and some influencing factors, with the task being to learn a set of mapping functions from input to output. If $y_t$ represents the load demand at time t, the STELF goal is to predict the load demand within the next h hours, denoted as $\\hat{y}_{t+h}$. The prediction model can generally be expressed as:\n$\\hat{y}_{t+h} = f(y_t, y_{t-1}, ..., y_{t-n+1}, X_t),$ (1)\nwhere $\\hat{y}_{t+h}$ is the predicted load at time $t + h$; f is the prediction model (which can be statistical, a machine learning model, or a deep learning model); $y_t, y_{t-1}, \\cdot\\cdot\\cdot, y_{t-n+1}$ are the historical load data\nfor the previous n time periods; and $X_t$ are the external variables at time t (such as weather data, calendar information, etc.)."}, {"title": "2.2 Basic Deep-Learning Models", "content": "In this section, we introduce traditional deep learning models and explore their innovations and variations. We also provide an introduction to the basic definitions and structures of these models, establishing a basis for a more detailed exploration of the application of deep learning methods in STELF."}, {"title": "2.2.1 Deep Neural Networks (DNNs).", "content": "DNNs are a complex and highly non-linear method for representation learning, typically consisting of an input layer, multiple hidden layers, and an output layer [35]. Each neuron in the hidden layers functions as a unit that performs mapping within a multi-dimensional data space. Together, these neurons extract complex abstract features and patterns from the input data. Both the width and the depth of DNN networks can be modified [80]. Shallow neural networks, which have only a single hidden layer, offer only the number of neurons as an adjustable parameter. The strength of DNNs lies not only in their deep structure but also in their non-linear activation functions. Non-linear activation functions, such as ReLU, Sigmoid, or Tanh, enable the network to capture non-linear relationships and complex patterns in the input data. Considering the non-linear nature of ELF load curves (which are influenced by various external factors), the use of DNN as a predictive model is well justified [79].\nThe Deep Belief Network (DBN) is a DNN variant that uses a layered unsupervised learning method for initial weight pre-training [71]. This layer-by-layer unsupervised training process ensures that each layer effectively captures the features of the preceding layer, allowing the most fundamental features to be extracted from the training set. Typically, a DBN is composed of multiple stacked Restricted Boltzmann Machines (RBMs). An RBM is a type of ANN consisting of a visible layer and a hidden layer, where there are no connections between nodes within the same layer, but nodes between layers are fully connected [63]. Stacked RBMs are used for model pre-training and unsupervised learning, with the top layer fine-tuned using a backpropagation neural network. Fundamentally, an RBM learns a feature representation of a probability distribution over the original input data while also extracting feature information [101]."}, {"title": "2.2.2 Recurrent Neural Networks (RNNs).", "content": "RNNs are particularly effective at processing sequential data, such as time-series data [62]. The RNN internal loops allow for the continuous transmission of information, which is the use of previous information to influence the current output, a capability also known as the memory function [100].However, RNNs often encounter vanishing or exploding gradient issues when processing long sequences, which hinders their ability to learn long-term dependencies [12]. LSTM and GRU were designed to overcome the RNN gradient issues when handling long sequences. They use different memory mechanisms to retain input information over extended periods [108, 157]. Faced with the distinct temporal sequences and cyclic patterns in ELF, LSTM, and GRU can utilize historical information for load forecasting and avoid gradient-related issues.\nLSTM is a special kind of RNN capable of learning long-term dependencies, specifically designed to address the issue of vanishing gradients. The key to LSTM is its internal structure, the memory cell, which includes four main components: an input gate, a forget gate, an output gate, and a cell state that can maintain information over time [66].\nGRU is an improved model based on LSTM, but with a simpler structure and shorter training times, which helps it to better capture long-term dependencies within sequential data. GRU integrates the forget and input gates of LSTM into a single update gate, combining the cell state and hidden\nstate. Compared to LSTM, GRU has fewer parameters, due to having one less gating unit. This significantly improves the computational efficiency.\nBoth LSTM and GRU improve the information flow control through their gating mechanisms. While LSTM provides precise control mechanisms, GRU improves the computational efficiency of these controls by simplifying them. A choice between these two models typically depends on the specific demands of the task, the nature of the data, and the computational resources available."}, {"title": "2.2.3 Convolutional Neural Networks (CNNs).", "content": "In recent years, CNNs have become one of the most popular and widely utilized deep-learning models [119]. Although initially designed for processing image data, CNNs are also very effective at handling time-series data. For such data, One-Dimensional (1-D) convolutional layers can capture local patterns and features through a sliding window mechanism. Additionally, CNNs can handle data with grid-like topologies, allowing sequence data to be converted into graph-structured data for processing. A CNN model consists of four main parts [98, 112]: (i) convolutional layers (which create feature maps from the input data); (ii) pooling layers (which reduce the dimensionality of the convolutional features); (iii) flattening layers (which reshape the data into a column vector); and (iv) fully connected layers (which link the features extracted by the convolutional and pooling layers to other layers)."}, {"title": "3 METHODOLOGY", "content": "In this section, we present the eight RQs related to STELF that guided our study. This section also provides a detailed introduction to the literature retrieval methods, and the filtering methods used to screen search results. The research methodology of this paper was guided by previous work [90, 200], and represents a systematic, comprehensive, and rationality approach. For ease of description, we use the term \u201cSTELF\u201d to represent the term \u201cdeep-learning-based STELF\" in this paper, unless explicitly stated."}, {"title": "3.1 Research Questions", "content": "This paper provides a comprehensive review of the application of deep learning in STELF. It examines the entire STELF process, structured around the following RQs:\n\u2022 RQ1: What is the distribution and analysis of the literature search results?\n\u2022 RQ2: What are the electricity load datasets?\n\u2022 RQ3: How can a dataset be preprocessed for STELF?\n\u2022 RQ4: What are the methods for feature extraction?\n\u2022 RQ5: What are the deep-learning-based modeling methods for STELF?\n\u2022 RQ6: How can the training processes be optimized?\n\u2022 RQ7: How have the STELF research results been evaluated?\n\u2022 RQ8: What are the challenges and the future development trends of STELF?\nRQ1 explores the distribution of literature related to the use of deep learning for STELF over the past decade, leading to a detailed analysis of this data. RQ2 leads to an overview of electricity load datasets. An answer to RQ3 includes the steps and methods used in data preprocessing. RQ4 leads to a discussion of the deep-learning feature-extraction techniques employed in the prediction process. Answering RQ5 classifies, describes, and analyzes the current state of deep-learning models in STELF. RQ6 explores the methods for optimizing the model-training process. The answer to RQ7 is an organized summary of the evaluation methods used for the forecasting results. Finally, the answer to RQ8 lists the challenges and future development trends of STELF. In the following sections, we provide detailed responses to each RQ, as illustrated in Fig. 1."}, {"title": "3.2 Literature Search", "content": "The literature search method employed in this paper follows the approach used by Huang et al. [90], involving the following mainstream databases to ensure a comprehensive data collection:\n\u2022 ACM Digital Library;\n\u2022 Elsevier Science Direct;\n\u2022 IEEE Xplore Digital Library;\n\u2022 Springer Online Library;\n\u2022 Wiley Online Library;\n\u2022 MDPI.\nThe search time range was set to the period 2014 to 2023. An initial attempt using certain keywords for the search [200] in each database revealed that the ELF titles were not uniformly represented. Some papers, for example, did not include words such as \u201celectricity\u201d or \u201cpower\u201d in their titles, even though their actual contents were related to ELF. Furthermore, not all STELF papers had the phrase \u201cshort-term\u201d in their titles or keywords list. In addition, because of the diverse terminology used in deep-learning methods, the inclusion of such terminology in the keyword search could result in the omission of relevant papers. To broaden the scope, and avoid missing publications from a particular category, \u201celectricity\u201d, \u201cpower\u201d, \u201cshort-term\", and \u201cdeep learning\u201d were not included in the keywords. As a result, the final set of keywords used in the search was limited to four phrases: \u201cload forecasting\u201d, \u201cload forecast\u201d, \u201cload prediction\u201d, and \u201cload predicting\u201d.\""}, {"title": "3.3 Literature Selection and Statistics", "content": "A total of 2,823 papers were retrieved from the six databases, according to the search parameters. These papers were further filtered according to the following selection criteria:\n(1) Not written in English.\n(2) Not discussing ELF.\n(3) Not using deep-learning methods.\n(4) Not focused on \u201cshort-term\u201d forecasting research.\n(5) The paper was a review article.\nBased on these criteria, 628 papers were selected from the initial 2,823. Additionally, the references of these papers were examined according to the snowballing approach [90], yielding an additional 22 papers. In total, 650 papers were included in the preliminary review and statistical analysis. The details of this search and filtering are shown in Table 1.\nWe manually processed each of the 650 papers. This process involved an initial checking of all papers, followed by the extraction and recording of key information (including the deep-learning\nmodels, datasets, data-preprocessing methods, prediction intervals, model block diagrams, and evaluation metrics). Finally, the content was structured according to Fig. 1. Although the statistics and analysis results are based on all 650 publications, not all 650 are cited in this paper. Instead, we selectively cited papers with similar content based on the extracted information. Ultimately, this paper thoroughly reviews and cites approximately 200 articles."}, {"title": "4 ANSWER TO RQ1: THE DISTRIBUTION AND ANALYSIS OF THE SEARCH RESULTS", "content": "This section provides the answers to RQ1. Our approach involved an analysis of the publication year trends and their distribution across various literature sources, providing a framework for understanding the evolution and scope of the field."}, {"title": "4.1 Publication Trends", "content": "We gathered the publication year data of the 650 papers (shown in Fig. 2) to show the trends of STELF papers between 2014 and 2023. Fig. 2a shows the number of publications per year, and Fig. 2b shows the cumulative number of publications.\nFig. 2a shows that there were fewer than 10 publications per year during the first three years (2014 to 2016). There has been a rapid growth since 2017, with the number of publications exceeding 100 per year by 2021. Furthermore, an examination of the cumulative numbers of publications (Fig. 2b) reveals an exponential growth in the research output for STELF. Fig. 2b also shows a linear function with an exceptionally high coefficient of determination ($R^2$ = 0.9996). The trend shown in Fig. 2a is directly related to the rapid development of deep-learning technologies in recent years. It also highlights the significance of research in this field."}, {"title": "4.2 Types of Publication Venues", "content": "The papers were sourced from multiple journals and conferences, with their proportion and distribution plotted in Fig. 3. Fig. 3a shows that the number of publications published in journals (55%) exceeds those in conferences (45%). Fig. 3b shows that, apart from 2017, the number of journal publications consistently outpaces the number of conference publications. Fig. 3b also shows a trend, for both journals and conferences, of increasing publication volume."}, {"title": "5 ANSWER TO RQ2: THE ELECTRICITY LOAD DATASETS", "content": "This section provides the answers to RQ2, which discusses the classification of datasets and examines some common public datasets. When selecting electricity load datasets, researchers need to choose the appropriate type of dataset based on the specific requirements of the forecasting scenario, which may include loads for residential households, commercial buildings, industry, and entire cities' system-level load.\nElectricity load datasets play a crucial role in STELF. ELF typically relies on historical data to predict future electricity demand over specific periods. With the advancement of deep learning methods, these models require substantial amounts of data to train for accurate predictions. Elec- tricity load datasets provide comprehensive historical electricity usage information, including load changes during different periods, consumer usage patterns, and the impact of seasonal and weather factors on electricity demand [202]. This information is vital for a deep understanding and accurate prediction of electricity demand patterns."}, {"title": "5.1 Classification of Datasets", "content": "Electricity load datasets can be broadly categorized based on their accessibility as either public or not. Public datasets are typically available online, and are usually provided by government agencies, power market operators, or research institutions [10, 58, 97, 110, 178]. Non-public datasets, in contrast, are usually not available due to considerations such as protecting competitive advantage, ensuring national security, complying with legal regulations, or protecting personal privacy."}, {"title": "5.2 Common Public Datasets", "content": "The review of the 650 papers revealed a wide variety of datasets. Some papers (such as [26, 34, 152]) did not mention the name or source of the dataset, while others (such as [8, 171, 206]) used multiple datasets, making it difficult to compile statistics. Table 2 lists some of the most frequently used public datasets, which are:\n\u2022 Independent System Operator of New England (ISO-NE)\u00b9: The dataset includes hourly elec- trical load, temperature, day type, and other information for the New England area of North America, from March 2003 to December 2014.\n\u2022 Australian Energy Market Operator (AEMO)\u00b2: The dataset contains 30-minute time-series data on electrical loads for five regions in Australia (South Australia, Queensland, New South Wales, Western Australia, and Victoria).\n\u2022 Global Energy Forecasting Competition (GEFCom)\u00b3: Each competition provides different datasets that cover historical data and related influencing factors for various regions and periods. The design of the datasets reflects the real-world conditions of energy markets and system operations.\n\u2022 University of California, Irvine (UCI) Machine Learning Repository (MLR)4: This dataset is a widely used public database covering simple datasets to complex multivariate time-series datasets. The household load data uses a sampling frequency of one minute.\n\u2022 European Network of Transmission System Operators for Electricity (ENTSO-E)5: This dataset is from an organization of electricity transmission system operators covering 35 European countries. The dataset includes real-world hourly electricity load time series from across Europe.\n\u2022 PJM Interconnection (PJM)6: This dataset is from a regional transmission organization in the United States responsible for operating the Eastern Interconnection grid. PJM transmits electricity to 14 regions in the United States, with data recorded at hourly MW intervals.\n\u2022 Commission for Energy Regulation (CER)7: This dataset records the half-hourly load data of residential households and small to medium-sized enterprises in Ireland.\n\u2022 2016 China Electrical Mathematical Modeling Competition: This dataset originates from the China Electrical Engineering Mathematical Modeling Competition and includes electrical load data and weather data from 2009 to 2015.\nAll the datasets listed in Table 2 have been used at least 10 times. The frequent use of ISO-NE and AEMO especially, both of which exceed 30 times, highlights their significant role and the high level of activity these datasets sustain in ELF research. The sampling frequency of the datasets varies from every minute to every hour, highlighting the diversity of real-time granularity, which supports a wide range of research and practical applications. Many important power datasets are covered, and distributed across multiple regions (including North America, Australia, Europe, and China).\nThe majority of the public datasets can be accessed online through platforms such as Kaggle, official websites, and specialized data repositories. This facilitates their access and use by researchers worldwide. Open access and faster updates are two major trends in the development of power datasets."}, {"title": "6 ANSWER TO RQ3: STELF DATASET PREPROCESSING", "content": "In this section, we address RQ3, introducing the main data-preprocessing methods (including data cleaning, selection of external variables, and data reconstruction).\nData preprocessing is a crucial step that directly impacts the accuracy and reliability of the prediction models. Raw electricity-load data frequently contains noise, anomalies, and inconsistent records. There are also often missing values. Unaddressed, these issues can significantly disrupt the learning process of models, leading to inaccurate or ineffective predictions [124]. Data preprocessing uses a series of methods (such as filling in missing values, and anomaly detection, correction, and normalization) to enhance the data quality [150]. The data also often exhibits strong temporal characteristics and seasonal variability. Appropriate preprocessing can help models capture these complex patterns, enhancing their understanding and responsiveness to temporal dynamics [122].\nThis section examines the various data-preprocessing steps and methods. It should be noted that published papers generally only mention one or several data preprocessing steps, not all. For example, Dong et al. [39] only discussed data normalization; Gao et al. [55] focused only on handling missing data; and Huang et al. [36] introduced data standardization and data reconstruction. Specific preprocessing measures are usually applied based on the design requirements of the model."}, {"title": "6.1 Data Cleaning", "content": "The data-cleaning process involves handling missing values, outliers, erroneous records, duplicate data, and performing standardization.\nVarious data-imputation techniques can be used to fill in the gaps of missing data [53, 136]. Some simple approaches include using the value from a previous time point or calculating the average of data from before and after the missing value [24, 86, 107]. Other methods include data clustering [111], and using values from the same time on adjacent dates [198].\nOutlier detection is often addressed using the three-sigma method [20, 97]. Sharma et al. [146] also used the interquartile range for this purpose, while Qin et al. [135] used box plots. Typically, when encountering outliers, erroneous records, or duplicated data, the main remedial strategies involve deletion or replacement (using established methods for handling missing values).\nData standardization relates to eliminating scale differences in the original data, allowing for comparison and calculation on the same scale. The use of raw data for analysis may lead to biases towards features with larger numerical ranges [153]. Two methods for data standardization are Min-Max Scaling and Z-Score Normalization [87, 173]."}, {"title": "6.2 External Variable Selection", "content": "The accuracy of STELF is determined not only by the operational conditions within the power system, but also by carefully considering a series of important external variables [168]. The next task after completing the data cleaning is to identify the external variables that significantly impact the forecasting results [19], such as the variability of climatic conditions, periodic fluctuations in temperature, holiday status, and dynamic changes in industrial activities. Appropriate consideration of these external factors can lead to a more comprehensive load forecasting model. This usually involves correlation analysis and variable-importance evaluation.\nTable 3 lists six commonly used methods for external variable selection, and some of the papers that use them. The Pearson Correlation Coefficient, for example, calculates the Pearson correlation between two continuous variables, with values ranging from -1 to 1. By determining a threshold for the correlation coefficient, only variables whose correlation with the target variable exceeds this threshold are considered to be correlated.\nZheng et al. [207] used the Least Absolute Shrinkage and Selection Operator (LASSO) method to perform variable selection. Subbiah et al. [150] introduced the Robust ReliefF Mutual Information Recursive Feature Elimination Hybrid Feature Selection (RMR-HFS), which uses a combination of filter and wrapper methods for variable selection. These selection methods cover a wide range of data analysis needs, from linear to nonlinear correlations, from time series analysis to dimensionality- reduction techniques. Each method has its unique advantages, helping to better understand and uncover correlations within the data, and enabling the construction of more accurate and efficient predictive models."}, {"title": "6.3 Data Reconstruction", "content": "Data reconstruction plays a key role in dealing with the randomness, volatility, periodicity, and diversity [99] of raw load data, and has become one of the key focus points in many STELF studies. A deep exploration of historical load data combined with advanced analytical methods (such as decomposition and clustering techniques) can reveal the key recurring patterns and trends in the data. These things are crucial for predictive models, as they help the models better understand and capture the dependencies within time-series data.\nSeveral common methods are used for data reconstruction. The Variational Mode Decomposition (VMD) technique decomposes load data into a series of Intrinsic Mode Functions (IMFs), which are then used to reconstruct the data for training [2]. Zang et al. [195] also used VMD technology to decompose the load data into modalities of different frequencies, employing LSTM with self- attention mechanism for forecasting. This multi-frequency analysis method allows for a more nuanced handling of the complexity within load data. Similarly, Mathew et al. [123] used Empirical Mode Decomposition (EMD) to decompose the raw data into a series of IMFs, and then used the same model to train each mode. Based on EMD, Ensemble Empirical Mode Decomposition (EEMD) [193] and Improved Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (ICEEMDAN) [205] are also used for data reconstruction.\nAdvanced clustering methods have also been used for data reconstruction [167, 177, 185]. Wu et al. [177] applied K-shape time-series clustering to categorize users with similar electricity usage habits and characteristics into multiple types. Yang et al. [185] used the K-means clustering algorithm to identify customer groups with similar electricity usage behaviors. Wang et al. [167] employed the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm to deal with datasets that contained noise. Chaturvedi et al. [22] used wavelet transform technology to decompose load data into four wavelet components and then trained a neural network for each component, achieving precise predictions of different frequency characteristics.\nWhether using decomposition or clustering techniques, the goal is to reconstruct the overall data to capture the distribution characteristics and underlying patterns of the data. Due to the complexity of load data, adopting a divide-and-conquer approach (where each part is trained using the same or different models) can enhance the efficiency and accuracy of the model. Reconstruction techniques not only provide a solid data foundation for the models but also directly influence the model's design, the algorithm selection, and the precision of the forecasting outcomes."}, {"title": "7 ANSWER TO RQ4: METHODS FOR FEATURE EXTRACTION", "content": "This section provides the answers to RQ4, examining how deep learning can effectively extract deep non-linear features from the data, enhancing the model's predictive capabilities. The purpose of feature extraction is to mine complex relationships from the raw load data that aid the model in understanding load changes. In STELF, feature extraction is a critical step, as it directly impacts the performance of the predictive model.\nDeep learning can extract features in an unsupervised learning manner, automatically learning useful feature representations from the data [164]. At different levels of the ANN, features at various levels of abstraction can be learned, providing the model with rich information. In STELF, in addition to considering the feature relationships within the time series itself, spatial feature relationships must also be considered [28], such as for large-scale ELF involving regional or urban power grids with distinct spatial relationships. The spatial characteristics are also crucial for fully understanding load change patterns and enhancing the accuracy of forecasts."}, {"title": "7.1 Temporal Feature Relationship Extraction", "content": "Because electricity-load data are time-series data, RNNs can be used to capture the temporal dependencies. LSTMs and GRUs are often used in temporal feature relationship extraction. Xu et al. [182] used an LSTM to extract deep features of electricity loads and employed an Extreme Learning Machine (ELM) to model shallow patterns. Abdel et al. [1] proposed STLF-Net, using a GRU to get the long-term temporal representations of data.\nThe success of feature extraction models based on the Multi-Channel One-Dimensional Convo- lutional Neural Network (MCNN) [37] suggests that convolution operations can also be used to extract feature relationships in time-series data. This allows for the direct capture of inter-feature relationships on sequence data through One-Dimensional Convolution (Conv1D) operations. Al- though One-Dimensional CNNs (1-D CNNs) are functionally similar to RNNs (such as LSTM and GRU), they also have unique network structural designs to accommodate the varying characteristics and requirements of time-series data. These structural designs make it possible for 1-D CNNs to be optimized for specific types of sequence data, making them better at extracting useful feature relationships. Dai et al. [31] used CNNs with Conv1D and pooling layers, where the Conv1D layer extracts pivotal features from the input data, and a pooling layer reduces the dimensionality and spatial complexity of the features.\nTemporal Convolutional Networks (TCNs) are another type of ANN designed for sequence modeling tasks, especially those involving time-series data. Because the TCN is based on CNN [11], it has also been widely used to extract feature vectors and long-term temporal dependencies [15, 171, 206]. Zhang et al. [206] proposed a hybrid network architecture combining TCN and LSTM to address the issue of model complexity. Their model leveraged the TCN's ability to capture the receptive field in time series and effectively model temporal dependencies, while also incorporating the LSTM's ability to handle long-term dependency problems."}, {"title": "7.2 Spatial Feature Relationship Extraction", "content": "Extraction of spatial feature relationships involves the construction of an adjacency matrix to represent the connections between nodes in the power network. Multi-dimensional convolution is used to obtain the spatial features. Hua et al. [86", "163": "used CNNs to extract spatial information from the load data, with the resulting features then input into the RNN for training.\nAnother approach involves the construction of a spatio-temporal graph model that can simul- taneously capture the spatial and temporal dependencies of electricity-load data. Yu et al. [191", "88": "."}]}