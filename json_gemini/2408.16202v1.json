{"title": "Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey", "authors": ["QI DONG", "RUBING HUANG", "CHENHUI CUI", "DAVE TOWEY", "LING ZHOU", "JINYU TIAN", "JIANZHOU WANG"], "abstract": "Short-Term Electricity-Load Forecasting (STELF) refers to the prediction of the immediate demand (in the\nnext few hours to several days) for the power system. Various external factors, such as weather changes and\nthe emergence of new electricity consumption scenarios, can impact electricity demand, causing load data to\nfluctuate and become non-linear, which increases the complexity and difficulty of STELF. In the past decade,\ndeep learning has been applied to STELF, modeling and predicting electricity demand with high accuracy,\nand contributing significantly to the development of STELF. This paper provides a comprehensive survey\non deep-learning-based STELF over the past ten years. It examines the entire forecasting process, including\ndata pre-processing, feature extraction, deep-learning modeling and optimization, and results evaluation. This\npaper also identifies some research challenges and potential research directions to be further investigated in\nfuture work.", "sections": [{"title": "1 INTRODUCTION", "content": "Electricity-Load Forecasting (ELF) aims to meet power systems' daily operational, management,\nand planning needs. This can provide essential guidance and reference points for system operators\nand planners. The absence of large-scale energy storage technologies means that power systems\nmust ensure a constant power supply to meet current demands [134]. This means that ELF has\nbecome an essential component in the planning, scheduling, and operational management of power\nsystems. Short-Term Electricity-Load Forecasting (STELF) uses historical load data to predict future\nloads, over a period ranging from several hours to a few days. STELF is a time-series forecasting task\nprimarily used for the short-term scheduling of smart grids (including equipment maintenance, load\ndistribution, and unit startup and shutdown), and for determining electricity prices [176]. Data from\na British electricity company in 1984 showed that a 1% reduction in forecasting error could save\n\u00a310 million annually [18]. Increasing the STELF accuracy could improve planning and scheduling,\nand reduce operational costs for power systems [102]. This practical impact of STELF has led to an\nincreasing amount of attention from researchers. Our review of the literature from the past decade\nhighlights that most load forecasting articles have focused on STELF [30, 57, 61, 76, 83, 138].\nMany factors can impact the electricity load, including climate, weather, economic conditions,\nseasonality, and electricity prices. Furthermore, advances in smart-grid technologies, and the wide-\nspread adoption of smart meters and other sensors have significantly increased both the complexity\nand the volume of electricity-load data. The data exhibits strong non-linearity, randomness, volatil-\nity, and complexity. This is a challenge for STELF. An accurate, robust, and fast STELF model is\nessential for the reliable daily operations of power systems [170], and the development of efficient\nforecasting models has become an important STELF research goal [41].\nELF has been extensively studied since the 1970s, with various methods having been proposed [45].\nSTELF methods can be broadly categorized into three types: statistical methods, machine learning\nmethods, and deep learning methods [42]. Statistical methods perform well when dealing with\nlinear relationships, but are often inadequate for handling the nonlinear patterns commonly found\nin electricity-load data. Machine learning methods, such as support vector machines and decision\ntrees, typically perform well with simple or moderately complex data patterns. Deep learning\nmethods can capture and model complex nonlinear relationships through their multi-layered\nstructures, which is particularly important for predicting dynamic changes in electricity loads. In\nsummary, while traditional statistical and machine learning methods have their strengths, deep\nlearning techniques are more suited to the dynamic and nonlinear characteristics of electricity-load\ndata.\nDeep learning approaches are among the most revolutionary breakthroughs in the fields of\ncomputer science and artificial intelligence in recent years [105]. The concept of deep learning\nbuilds on earlier work on Artificial Neural Networks (ANNs) [71], which were a type of shallow\nlearning model [143], consisting of an input layer, a hidden layer, and an output layer [6]. ANNs\nwere used in early STELF studies [67, 133]. Deep Neural Networks (DNNs), a type of ANN with\nmultiple hidden layers, can also be used for STELF [5, 77, 103]. The multiple hidden layers in DNNs\nenable a complex computational framework that uses features as inputs to represent different\nlevels of data abstraction. Through a cascading network structure, each layer in the DNN is capable\nof extracting and recognizing different features of the data, forming a hierarchy from basic to\nadvanced features, thereby significantly enhancing both the models flexibility and its ability to"}, {"title": "2 BACKGROUND", "content": "In this section, we introduce the task definition of STELF and the basic deep learning methods. The\nprimary objective is to quickly familiarize readers with STELF and provide them with an initial\nunderstanding of deep learning methods."}, {"title": "2.1 STELF Task Definition", "content": "STELF aims to predict the electricity load over a future period ranging from a few hours to several\ndays. The model's input consists of historical load data and some influencing factors, with the task\nbeing to learn a set of mapping functions from input to output. If yt represents the load demand at\ntime t, the STELF goal is to predict the load demand within the next h hours, denoted as \u0177t+h. The\nprediction model can generally be expressed as:\n\u0177t+h = f(yt, Yt\u22121, . . ., Yt-n+1, Xt),   (1)\nwhere \u0177t+h is the predicted load at time t + h; f is the prediction model (which can be statistical, a\nmachine learning model, or a deep learning model); yt, Yt\u22121, \u00b7 \u00b7 \u00b7, Yt\u2212n+1 are the historical load data"}, {"title": "2.2 Basic Deep-Learning Models", "content": "In this section, we introduce traditional deep learning models and explore their innovations and\nvariations. We also provide an introduction to the basic definitions and structures of these models,\nestablishing a basis for a more detailed exploration of the application of deep learning methods in\nSTELF."}, {"title": "2.2.1 Deep Neural Networks (DNNs).", "content": "DNNs are a complex and highly non-linear method for\nrepresentation learning, typically consisting of an input layer, multiple hidden layers, and an output\nlayer [35]. Each neuron in the hidden layers functions as a unit that performs mapping within\na multi-dimensional data space. Together, these neurons extract complex abstract features and\npatterns from the input data. Both the width and the depth of DNN networks can be modified [80].\nShallow neural networks, which have only a single hidden layer, offer only the number of neurons\nas an adjustable parameter. The strength of DNNs lies not only in their deep structure but also in\ntheir non-linear activation functions. Non-linear activation functions, such as ReLU, Sigmoid, or\nTanh, enable the network to capture non-linear relationships and complex patterns in the input data.\nConsidering the non-linear nature of ELF load curves (which are influenced by various external\nfactors), the use of DNN as a predictive model is well justified [79].\nThe Deep Belief Network (DBN) is a DNN variant that uses a layered unsupervised learning\nmethod for initial weight pre-training [71]. This layer-by-layer unsupervised training process\nensures that each layer effectively captures the features of the preceding layer, allowing the most\nfundamental features to be extracted from the training set. Typically, a DBN is composed of multiple\nstacked Restricted Boltzmann Machines (RBMs). An RBM is a type of ANN consisting of a visible\nlayer and a hidden layer, where there are no connections between nodes within the same layer,\nbut nodes between layers are fully connected [63]. Stacked RBMs are used for model pre-training\nand unsupervised learning, with the top layer fine-tuned using a backpropagation neural network.\nFundamentally, an RBM learns a feature representation of a probability distribution over the original\ninput data while also extracting feature information [101]."}, {"title": "2.2.2 Recurrent Neural Networks (RNNs).", "content": "RNNs are particularly effective at processing sequential\ndata, such as time-series data [62]. The RNN internal loops allow for the continuous transmission of\ninformation, which is the use of previous information to influence the current output, a capability\nalso known as the memory function [100].However, RNNs often encounter vanishing or exploding\ngradient issues when processing long sequences, which hinders their ability to learn long-term\ndependencies [12]. LSTM and GRU were designed to overcome the RNN gradient issues when\nhandling long sequences. They use different memory mechanisms to retain input information over\nextended periods [108, 157]. Faced with the distinct temporal sequences and cyclic patterns in ELF,\nLSTM, and GRU can utilize historical information for load forecasting and avoid gradient-related\nissues.\nLSTM is a special kind of RNN capable of learning long-term dependencies, specifically designed\nto address the issue of vanishing gradients. The key to LSTM is its internal structure, the memory\ncell, which includes four main components: an input gate, a forget gate, an output gate, and a cell\nstate that can maintain information over time [66].\nGRU is an improved model based on LSTM, but with a simpler structure and shorter training times,\nwhich helps it to better capture long-term dependencies within sequential data. GRU integrates\nthe forget and input gates of LSTM into a single update gate, combining the cell state and hidden"}, {"title": "2.2.3 Convolutional Neural Networks (CNNs).", "content": "In recent years, CNNs have become one of the\nmost popular and widely utilized deep-learning models [119]. Although initially designed for\nprocessing image data, CNNs are also very effective at handling time-series data. For such data,\nOne-Dimensional (1-D) convolutional layers can capture local patterns and features through a\nsliding window mechanism. Additionally, CNNs can handle data with grid-like topologies, allowing\nsequence data to be converted into graph-structured data for processing. A CNN model consists of\nfour main parts [98, 112]: (i) convolutional layers (which create feature maps from the input data);\n(ii) pooling layers (which reduce the dimensionality of the convolutional features); (iii) flattening\nlayers (which reshape the data into a column vector); and (iv) fully connected layers (which link\nthe features extracted by the convolutional and pooling layers to other layers)."}, {"title": "3 METHODOLOGY", "content": "In this section, we present the eight RQs related to STELF that guided our study. This section\nalso provides a detailed introduction to the literature retrieval methods, and the filtering methods\nused to screen search results. The research methodology of this paper was guided by previous\nwork [90, 200], and represents a systematic, comprehensive, and rationality approach. For ease of\ndescription, we use the term \u201cSTELF\u201d to represent the term \u201cdeep-learning-based STELF\"in this paper, unless explicitly stated."}, {"title": "3.1 Research Questions", "content": "This paper provides a comprehensive review of the application of deep learning in STELF. It\nexamines the entire STELF process, structured around the following RQs:\n\u2022 RQ1: What is the distribution and analysis of the literature search results?\n\u2022 RQ2: What are the electricity load datasets?\n\u2022 RQ3: How can a dataset be preprocessed for STELF?\n\u2022 RQ4: What are the methods for feature extraction?\n\u2022 RQ5: What are the deep-learning-based modeling methods for STELF?\n\u2022 RQ6: How can the training processes be optimized?\n\u2022 RQ7: How have the STELF research results been evaluated?\n\u2022 RQ8: What are the challenges and the future development trends of STELF?\nRQ1 explores the distribution of literature related to the use of deep learning for STELF over\nthe past decade, leading to a detailed analysis of this data. RQ2 leads to an overview of electricity\nload datasets. An answer to RQ3 includes the steps and methods used in data preprocessing. RQ4\nleads to a discussion of the deep-learning feature-extraction techniques employed in the prediction\nprocess. Answering RQ5 classifies, describes, and analyzes the current state of deep-learning models\nin STELF. RQ6 explores the methods for optimizing the model-training process. The answer to\nRQ7 is an organized summary of the evaluation methods used for the forecasting results. Finally,\nthe answer to RQ8 lists the challenges and future development trends of STELF. In the following\nsections, we provide detailed responses to each RQ, as illustrated in Fig. 1."}, {"title": "3.2 Literature Search", "content": "The literature search method employed in this paper follows the approach used by Huang et al. [90],\ninvolving the following mainstream databases to ensure a comprehensive data collection:\n\u2022 ACM Digital Library;\n\u2022 Elsevier Science Direct;\n\u2022 IEEE Xplore Digital Library;\n\u2022 Springer Online Library;\n\u2022 Wiley Online Library;\n\u2022 MDPI.\nThe search time range was set to the period 2014 to 2023. An initial attempt using certain keywords\nfor the search [200] in each database revealed that the ELF titles were not uniformly represented.\nSome papers, for example, did not include words such as \u201celectricity\u201d or \u201cpower\u201d in their titles, even\nthough their actual contents were related to ELF. Furthermore, not all STELF papers had the phrase\n\u201cshort-term\u201d in their titles or keywords list. In addition, because of the diverse terminology used\nin deep-learning methods, the inclusion of such terminology in the keyword search could result\nin the omission of relevant papers. To broaden the scope, and avoid missing publications from a\nparticular category, \u201celectricity\u201d, \u201cpower\u201d, \u201cshort-term\", and \u201cdeep learning\u201d were not included in\nthe keywords. As a result, the final set of keywords used in the search was limited to four phrases:\n\u201cload forecasting\u201d, \u201cload forecast\u201d, \u201cload prediction\u201d, and \u201cload predicting\u201d.\""}, {"title": "3.3 Literature Selection and Statistics", "content": "A total of 2,823 papers were retrieved from the six databases, according to the search parameters.\nThese papers were further filtered according to the following selection criteria:\n(1) Not written in English.\n(2) Not discussing ELF.\n(3) Not using deep-learning methods.\n(4) Not focused on \u201cshort-term\u201d forecasting research.\n(5) The paper was a review article.\nBased on these criteria, 628 papers were selected from the initial 2,823. Additionally, the references\nof these papers were examined according to the snowballing approach [90], yielding an additional\n22 papers. In total, 650 papers were included in the preliminary review and statistical analysis. The\ndetails of this search and filtering are shown in Table 1.\nWe manually processed each of the 650 papers. This process involved an initial checking of all\npapers, followed by the extraction and recording of key information (including the deep-learning"}, {"title": "4 ANSWER TO RQ1: THE DISTRIBUTION AND ANALYSIS OF THE SEARCH RESULTS", "content": "This section provides the answers to RQ1. Our approach involved an analysis of the publication\nyear trends and their distribution across various literature sources, providing a framework for\nunderstanding the evolution and scope of the field."}, {"title": "4.1 Publication Trends", "content": "We gathered the publication year data of the 650 papers (shown in Fig. 2) to show the trends of\nSTELF papers between 2014 and 2023. Fig. 2a shows the number of publications per year, and\nFig. 2b shows the cumulative number of publications.\nFig. 2a shows that there were fewer than 10 publications per year during the first three years\n(2014 to 2016). There has been a rapid growth since 2017, with the number of publications exceeding\n100 per year by 2021. Furthermore, an examination of the cumulative numbers of publications\n(Fig. 2b) reveals an exponential growth in the research output for STELF. Fig. 2b also shows a linear\nfunction with an exceptionally high coefficient of determination (R2 = 0.9996). The trend shown in\nFig. 2a is directly related to the rapid development of deep-learning technologies in recent years. It\nalso highlights the significance of research in this field."}, {"title": "4.2 Types of Publication Venues", "content": "The papers were sourced from multiple journals and conferences, with their proportion and\ndistribution plotted in Fig. 3. Fig. 3a shows that the number of publications published in journals\n(55%) exceeds those in conferences (45%). Fig. 3b shows that, apart from 2017, the number of journal\npublications consistently outpaces the number of conference publications. Fig. 3b also shows a\ntrend, for both journals and conferences, of increasing publication volume."}, {"title": "5 ANSWER TO RQ2: THE ELECTRICITY LOAD DATASETS", "content": "This section provides the answers to RQ2, which discusses the classification of datasets and examines\nsome common public datasets. When selecting electricity load datasets, researchers need to choose\nthe appropriate type of dataset based on the specific requirements of the forecasting scenario,"}, {"title": "5.1 Classification of Datasets", "content": "Electricity load datasets can be broadly categorized based on their accessibility as either public or\nnot. Public datasets are typically available online, and are usually provided by government agencies,\npower market operators, or research institutions [10, 58, 97, 110, 178]. Non-public datasets, in\ncontrast, are usually not available due to considerations such as protecting competitive advantage,\nensuring national security, complying with legal regulations, or protecting personal privacy."}, {"title": "5.2 Common Public Datasets", "content": "The review of the 650 papers revealed a wide variety of datasets. Some papers (such as [26, 34, 152])\ndid not mention the name or source of the dataset, while others (such as [8, 171, 206]) used multiple"}, {"title": "6 ANSWER TO RQ3: STELF DATASET PREPROCESSING", "content": "In this section, we address RQ3, introducing the main data-preprocessing methods (including data\ncleaning, selection of external variables, and data reconstruction).\nData preprocessing is a crucial step that directly impacts the accuracy and reliability of the\nprediction models. Raw electricity-load data frequently contains noise, anomalies, and inconsistent\nrecords. There are also often missing values. Unaddressed, these issues can significantly disrupt the\nlearning process of models, leading to inaccurate or ineffective predictions [124]. Data preprocessing\nuses a series of methods (such as filling in missing values, and anomaly detection, correction, and\nnormalization) to enhance the data quality [150]. The data also often exhibits strong temporal\ncharacteristics and seasonal variability. Appropriate preprocessing can help models capture these\ncomplex patterns, enhancing their understanding and responsiveness to temporal dynamics [122].\nThis section examines the various data-preprocessing steps and methods. It should be noted\nthat published papers generally only mention one or several data preprocessing steps, not all. For\nexample, Dong et al. [39] only discussed data normalization; Gao et al. [55] focused only on handling\nmissing data; and Huang et al. [36] introduced data standardization and data reconstruction. Specific\npreprocessing measures are usually applied based on the design requirements of the model."}, {"title": "6.1 Data Cleaning", "content": "The data-cleaning process involves handling missing values, outliers, erroneous records, duplicate\ndata, and performing standardization.\nVarious data-imputation techniques can be used to fill in the gaps of missing data [53, 136].\nSome simple approaches include using the value from a previous time point or calculating the\naverage of data from before and after the missing value [24, 86, 107]. Other methods include data\nclustering [111], and using values from the same time on adjacent dates [198].\nOutlier detection is often addressed using the three-sigma method [20, 97]. Sharma et al. [146]\nalso used the interquartile range for this purpose, while Qin et al. [135] used box plots. Typically,\nwhen encountering outliers, erroneous records, or duplicated data, the main remedial strategies\ninvolve deletion or replacement (using established methods for handling missing values).\nData standardization relates to eliminating scale differences in the original data, allowing for\ncomparison and calculation on the same scale. The use of raw data for analysis may lead to biases\ntowards features with larger numerical ranges [153]. Two methods for data standardization are\nMin-Max Scaling and Z-Score Normalization [87, 173]."}, {"title": "6.2 External Variable Selection", "content": "The accuracy of STELF is determined not only by the operational conditions within the power\nsystem, but also by carefully considering a series of important external variables [168]. The next\ntask after completing the data cleaning is to identify the external variables that significantly impact\nthe forecasting results [19], such as the variability of climatic conditions, periodic fluctuations in\ntemperature, holiday status, and dynamic changes in industrial activities. Appropriate consideration\nof these external factors can lead to a more comprehensive load forecasting model. This usually\ninvolves correlation analysis and variable-importance evaluation."}, {"title": "6.3 Data Reconstruction", "content": "Data reconstruction plays a key role in dealing with the randomness, volatility, periodicity, and\ndiversity [99] of raw load data, and has become one of the key focus points in many STELF studies.\nA deep exploration of historical load data combined with advanced analytical methods (such as\ndecomposition and clustering techniques) can reveal the key recurring patterns and trends in the\ndata. These things are crucial for predictive models, as they help the models better understand and\ncapture the dependencies within time-series data.\nSeveral common methods are used for data reconstruction. The Variational Mode Decomposition\n(VMD) technique decomposes load data into a series of Intrinsic Mode Functions (IMFs), which\nare then used to reconstruct the data for training [2]. Zang et al. [195] also used VMD technology\nto decompose the load data into modalities of different frequencies, employing LSTM with self-\nattention mechanism for forecasting. This multi-frequency analysis method allows for a more\nnuanced handling of the complexity within load data. Similarly, Mathew et al. [123] used Empirical\nMode Decomposition (EMD) to decompose the raw data into a series of IMFs, and then used\nthe same model to train each mode. Based on EMD, Ensemble Empirical Mode Decomposition\n(EEMD) [193] and Improved Complete Ensemble Empirical Mode Decomposition with Adaptive\nNoise (ICEEMDAN) [205] are also used for data reconstruction.\nAdvanced clustering methods have also been used for data reconstruction [167, 177, 185]. Wu\net al. [177] applied K-shape time-series clustering to categorize users with similar electricity\nusage habits and characteristics into multiple types. Yang et al. [185] used the K-means clustering\nalgorithm to identify customer groups with similar electricity usage behaviors. Wang et al. [167]\nemployed the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm to\ndeal with datasets that contained noise. Chaturvedi et al. [22] used wavelet transform technology\nto decompose load data into four wavelet components and then trained a neural network for each\ncomponent, achieving precise predictions of different frequency characteristics."}, {"title": "7 ANSWER TO RQ4: METHODS FOR FEATURE EXTRACTION", "content": "This section provides the answers to RQ4, examining how deep learning can effectively extract\ndeep non-linear features from the data, enhancing the model's predictive capabilities. The purpose\nof feature extraction is to mine complex relationships from the raw load data that aid the model in\nunderstanding load changes. In STELF, feature extraction is a critical step, as it directly impacts the\nperformance of the predictive model.\nDeep learning can extract features in an unsupervised learning manner, automatically learning\nuseful feature representations from the data [164]. At different levels of the ANN, features at\nvarious levels of abstraction can be learned, providing the model with rich information. In STELF,\nin addition to considering the feature relationships within the time series itself, spatial feature\nrelationships must also be considered [28], such as for large-scale ELF involving regional or urban\npower grids with distinct spatial relationships. The spatial characteristics are also crucial for fully\nunderstanding load change patterns and enhancing the accuracy of forecasts."}, {"title": "7.1 Temporal Feature Relationship Extraction", "content": "Because electricity-load data are time-series data, RNNs can be used to capture the temporal\ndependencies. LSTMs and GRUs are often used in temporal feature relationship extraction. Xu\net al. [182] used an LSTM to extract deep features of electricity loads and employed an Extreme\nLearning Machine (ELM) to model shallow patterns. Abdel et al. [1] proposed STLF-Net, using a\nGRU to get the long-term temporal representations of data.\nThe success of feature extraction models based on the Multi-Channel One-Dimensional Convo-\nlutional Neural Network (MCNN) [37] suggests that convolution operations can also be used to\nextract feature relationships in time-series data. This allows for the direct capture of inter-feature\nrelationships on sequence data through One-Dimensional Convolution (Conv1D) operations. Al-\nthough One-Dimensional CNNs (1-D CNNs) are functionally similar to RNNs (such as LSTM and\nGRU), they also have unique network structural designs to accommodate the varying characteristics\nand requirements of time-series data. These structural designs make it possible for 1-D CNNs to\nbe optimized for specific types of sequence data, making them better at extracting useful feature\nrelationships. Dai et al. [31] used CNNs with Conv1D and pooling layers, where the Conv1D layer\nextracts pivotal features from the input data, and a pooling layer reduces the dimensionality and\nspatial complexity of the features.\nTemporal Convolutional Networks (TCNs) are another type of ANN designed for sequence\nmodeling tasks, especially those involving time-series data. Because the TCN is based on CNN\n[11], it has also been widely used to extract feature vectors and long-term temporal dependencies\n[15, 171, 206]. Zhang et al. [206] proposed a hybrid network architecture combining TCN and LSTM\nto address the issue of model complexity. Their model leveraged the TCN's ability to capture the\nreceptive field in time series and effectively model temporal dependencies, while also incorporating\nthe LSTM's ability to handle long-term dependency problems."}, {"title": "7.2 Spatial Feature Relationship Extraction", "content": "Extraction of spatial feature relationships involves the construction of an adjacency matrix to\nrepresent the connections between nodes in the power network. Multi-dimensional convolution is\nused to obtain the spatial features. Hua et al. [86] proposed a predictive model that combines CNN\nand GRU, extracting spatial features through the CNN and temporal features through the GRU."}, {"title": "8 ANSWER TO RQ5: DEEP-LEARNING-BASED MODELING METHODS FOR STELF", "content": "This section provides the answers to RQ5, offering an extensive review of the literature on deep-\nlearning-based predictive models. From the perspective of forecasting outcomes, predictive models\ncan be categorized as either deterministic or probabilistic [14]."}, {"title": "8.1 Deterministic Forecasting Models", "content": "In STELF, deterministic forecasting provides an exact numerical prediction, offering precise pre-\ndictions of the load level at a specific point in time or over a certain period in the future. This\ntype of forecasting focuses on delivering a concrete value rather than a range or distribution. Due\nto the extensive literature and methods involved, we examine this type of forecasting from the\nperspectives of single and hybrid models."}, {"title": "8.1.1 Single Models.", "content": "Simplicity is the main advantage of the single model, which is easy to under-\nstand and construct. Deep learning algorithms make use of deep networks, consisting of a series of\ncomplex hidden layers [43]. Early STELF achieved predictive results by stacking ANNs [35, 148].\nChen et al. [23] and Hossen et al. [80] explored ELF using DNNs. Chen et al. proposed a method\nbased on two-terminal sparse coding and deep neural network fusion, while Hossen et al. examined\nthe impact of single-layer versus double-layer DNN architectures. A DBN, similar in structure to\nDNN, combines multiple RBMs for ELF and uses a layer-by-layer unsupervised learning method to\npre-train the initial weights [33].\nDNNs or DBNs formed by stacking multiple layers typically lack memory capability, which\nmeans that they may not be able to use previous information effectively when processing time-\nseries data. RNNs use recurrent connections, allowing the network to retain previous information\nwhile processing sequence data, potentially adjusting the output based on earlier elements in\nthe sequence. This makes RNNs (including LSTM and GRU, as introduced in Section 2) very\npopular for time-series forecasting. LSTMs and GRUs are advanced RNNs that have been used\nin STELF [100, 125]. Zhu et al. [208] proposed a dual-attention encoder-decoder structure using\nattention mechanisms, using an LSTM as a specific encoder and decoder for nonlinear dynamic\ntime modeling. This attention mechanism dynamically focused on the importance of different parts\nof the input sequence, significantly enhancing the performance of RNNs used in conjunction. Aseeri\net al. [9] also used a GRU structure to focus on key variables, improving performance, particularly\nwith longer sequences."}, {"title": "8.1.2 Hybrid Models.", "content": "In STELF, hybrid deep-learning models are becoming a key technology\nfor solving complex forecasting problems. Hybrid models integrate a variety of deep-learning\nmodels, with advantages including their diversity and flexibility. Complementing each other's\nstrengths, they enhance the accuracy and robustness of predictions [115]. The hybrid model\nprimarily employs two strategies for combination: stage-wise training and joint training. Stage-\nwise training involves focusing on specific learning tasks at each stage, while joint training involves\ntraining all components of the hybrid model simultaneously.\n(1) Stage-wise Training: Stage-wise training strategy addresses some challenges for training\ncomplex models by breaking down the training process into a series of orderly stages. In each stage,\na part of the model is independently trained and optimized to learn specific patterns.\nA hybrid model integrating CNN with RNN has become a widely adopted solution, due to its\nexceptional performance [7, 52, 60, 144, 147, 188]. Zhang et al. [204] proposed a hybrid model\nbased on CNN and LSTM, using CNN layers for feature extraction from the input dataset and an\nLSTM model for sequence prediction, supporting multi-step forecasting of time-series data. Their\napproach leverages the efficient CNN capability to extract local features, using their output feature"}, {"title": "8.2 Probabilistic Forecasting Models", "content": "Probabilistic ELF predicts future demand using the uncertainty and randomness of the load. Unlike\ntraditional deterministic methods, probabilistic models provide a quantified expression of predic-\ntive uncertainty, which has significant advantages for power grid planning and operation [186].\nProbabilistic models express the uncertainty of prediction results by generating a distribution of\noutcomes, rather than a single value [92].\nThere are two main approaches for probabilistic ELF: parametric and non-parametric methods[81].\nParametric methods are based on assumptions about the distribution of the load data, typically\nassuming that the data follows a known probability distribution (such as the normal distribution\nor the Poisson distribution). Due to their use of fewer assumptions, non-parametric methods\nhave much broader applicability [159]. Non-parametric methods do not require that the data"}, {"title": "9 ANSWER TO RQ6: OPTIMIZING THE TRAINING PROCESS", "content": "This section provides the answers to RQ6, examining ways to optimize deep-learning training\nprocesses [200].\nOptimization mainly involves two aspects: network-structure optimization and error optimization.\nOptimization of the network structure relates to the architectural design of the model, and involves\nadjusting the number of layers, configuring the neurons, and modifying the connection methods.\nThis all aims at constructing a robust model capable of capturing the complex features of the data.\nError optimization involves thorough analysis and fine-tuning of the prediction errors, aiming to\nminimize the discrepancy between the model's predictions and the actual observed values, thereby\nenhancing the accuracy and reliability."}, {"title": "9.1 Network-Structure Optimization", "content": "Network architecture design is a very important task, requiring careful selection of the number of\nneurons in each layer and the number of hidden layers in the network, etc [94]. The selection is not\ncompleted in a single step, but rather needs to be determined based on the nature of the problem,\nthe characteristics of the data, and the expected performance of the model. Selecting the optimal\nnetwork structure and model parameters is a complex process, with a number of trial-and-error\nmethods and heuristic optimization algorithms having been proposed [164].\nAs a fundamental problem-solving method, the trial-and-error approach involves gradually\napproaching a solution through continuous attempts and adjustments. This is not limited to\nonly simple experiments and adjustments, but has been combined with heuristic optimization\ntechniques to form efficient and systematic optimization strategies. The heuristic algorithms draw\ninspiration from optimization mechanisms found in nature and social phenomena, including Particle\nSwarm Optimization (PSO) [74], Genetic Algorithms (GAs) [40], the Whale Optimization Algorithm"}, {"title": "9.2 Error Optimization", "content": "Once the loss function is defined, the network weights need to be updated by calculating the\nloss-function gradient using optimization algorithms, such as gradient descent and its variants\n(Adam, RMSprop, etc. [54, 174", "73": "."}]}