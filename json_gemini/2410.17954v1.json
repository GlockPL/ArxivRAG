{"title": "ExpertFlow: Optimized Expert Activation and Token Allocation for Efficient Mixture-of-Experts Inference", "authors": ["Xin He", "Shunkang Zhang", "Yuxin Wang", "Haiyan Yin", "Zihao Zeng", "Shaohuai Shi", "Zhenheng Tang", "Xiaowen Chu", "Ivor Tsang", "Ong Yew Soon"], "abstract": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large Language Models (LLMs) in terms of performance, face significant deployment challenges during inference due to their high memory demands. Existing offloading techniques, which involve swapping activated and idle experts between the GPU and CPU, often suffer from rigid expert caching mechanisms. These mechanisms fail to adapt to dynamic routing, leading to inefficient cache utilization, or incur prohibitive costs for prediction training. To tackle these inference-specific challenges, we introduce ExpertFlow, a comprehensive system specifically designed to enhance inference efficiency by accommodating flexible routing and enabling efficient expert scheduling between CPU and GPU. This reduces overhead and boosts system performance. Central to our approach is a predictive routing path-based offloading mechanism that utilizes a lightweight predictor to accurately forecast routing paths before computation begins. This proactive strategy allows for real-time error correction in expert caching, significantly increasing cache hit ratios and reducing the frequency of expert transfers, thereby minimizing I/O overhead. Additionally, we implement a dynamic token scheduling strategy that optimizes MoE inference by rearranging input tokens across different batches. This method not only reduces the number of activated experts per batch but also improves computational efficiency. Our extensive experiments demonstrate that ExpertFlow achieves up to 93.72% GPU memory savings and enhances inference speed by 2 to 10 times compared to baseline methods, highlighting its effectiveness and utility as a robust solution for resource-constrained inference scenarios.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have exhibited remarkable emergent capabilities [42], setting new benchmarks in diverse domains such as chatbots, image generation, and programming tasks [4, 12, 25, 32, 33, 50]. Scaling laws suggest that increasing model size leads to significant performance improvements [18]. However, this scalability comes at the cost of exponentially higher computational requirements. Mixture of Experts (MoE) models have emerged as a promising approach to address these challenges by employing a dynamic routing mechanism that directs input tokens to specific experts, thereby enhancing model capacity while maintaining comparable computational costs (FLOPs per token) during inference [7, 10, 16, 35, 39, 40].\nDespite their advantages, MoE models struggle in resource-limited environments due to high memory demands. For instance, the Mixtral-8\u00d77B MoE model requires about 96GB of memory, surpassing even high-end GPUs like the A100-80GB. A significant portion of this-45.1B out of 46.7B parameters-is dedicated to the experts, yet much of it remains underutilized since only a subset of experts is activated at any time. This inefficiency is problematic in systems with limited GPU resources, where adding more GPUs isn't feasible. Thus, developing memory-efficient strategies for MoE deployment is crucial.\nTo mitigate memory constraints, various offloading methods have been proposed [8, 9, 14, 17, 36, 39]. These methods typically involve transferring idle experts to CPU memory to free up GPU memory for active computations. However, the efficiency of offloading heavily depends on sophisticated arrangement strategy depending on the routing path, which exhibits complex and dynamic patterns. Since the routing path for each layer is typically unknown before computation, conventional offloading methods often handle expert transfer in a layer-wise manner, leading to substantial Expert IO transfers that can block processing [9, 36]. To address this, several studies have attempted to predict the expert selection patterns [8, 14, 19], yet the accuracy and training cost of these predictions remain significant bottlenecks. For instance, Lina [19] identifies spatial locality of experts between layers and proposes using the routing result from the previous layer to predict the next layer's routing. However, our experiments (\u00a7 5.2) reveal that this approach becomes ineffective in MoE models with a larger number of experts per layer. To enhance accuracy, some attempts, such as Pregated-MoE [14] employed simple neural networks, i.e., Multi-layer Perceptrons (MLP), as predictors to replace original MoE routers. However, this requires extensive fine-tuning of MoE"}, {"title": "2 Background and Motivation", "content": "The MoE architecture, introduced by [15], addresses scalability and adaptability challenges in traditional neural networks by leveraging specialized expert networks managed by a gating mechanism. This design enables the model to efficiently handle complex tasks by dynamically assigning input tokens to the most appropriate experts, thereby enhancing performance while maintaining computational efficiency. Recent advancements in computational power and algorithmic strategies have further refined MoE architectures, leading to deeper integration and more sophisticated gating mechanisms [7, 10, 35, 41]. Notably, large-scale transformer models such as Switch Transformer [10] have demonstrated the effectiveness of MoE in managing extensive and diverse datasets by dynamically allocating computational resources based on input characteristics. At the core of MoE is the routing mechanism that directs tokens to experts based on a softmax gating function. This function dynamically computes the probability of expert selection, ensuring tokens are routed to the most suitable experts. The resulting sequence of expert selections for each token across all layers forms the MoE routing path, which can be represented by an L \u00d7 E matrix, where L denotes the number of layers and E indicates the number of experts per layer. Given the input token x, the"}, {"title": "2.1 A Primer on Mixture-of-Experts (MoE)", "content": null}, {"title": "2.2 Challenge 1: Inaccurate and High-cost MoE Routing Prediction", "content": "The expert selection process remains hidden until computation reaches the current layer, which hinders proactive scheduling of experts, leading to frequent I/O operations for expert transfer between GPU and CPU. While some prediction-based methods have been explored to anticipate expert selections ahead of time, they come with notable limitations. For instance, approaches like Lina [19] rely on historical routing data to establish fixed patterns for expert selection, essentially using the current layer's routing results to prefetch experts for the next layer. However, this method is only effective for MoEs with a limited number of experts per layer. Our experiments (\u00a7 5.2) with MoEs having more than 32 experts per layer reveal that these fixed patterns fail to perform effectively. More recent methods, such as SiDA-MOE [8] and Pregated-MoE [14], integrate predictive branches directly into the MoE structure to replace original routers, allowing for proactive resource scheduling for the next layer during the current layer's execution. However, these methods require their predictors to accurately estimate expert selection probabilities. Without this precision, MoE performance can suffer, necessitating fine-tuning to mitigate performance loss, which reduces their practicality."}, {"title": "Opportunity 1: Simplifying MoE Routing Predictions to Expert Status Classification", "content": "During MoE inference, only the experts with the top-k highest weights are selected for computation. Therefore, it is not necessary to predict the exact selection probabilities; rather, determining the status of each expert is sufficient. Knowing in advance which experts will be activated allows for proactive resource scheduling. Besides, the layer-by-layer prediction approach in Pregated-MoE leads to delays in resource scheduling. Thus, a more effective strategy is to determine the status of experts across all layers in a single pass. This requires the predictor to have strong contextual modeling capabilities, enabling it to anticipate the internal routing of the MoE based on input tokens. Transformer architectures, known for their proficiency in modeling contextual relationships, are an ideal choice for our predictor structure. Furthermore, previous research in the Neural Architecture Search (NAS) domain [13, 23] has successfully demonstrated the effectiveness of using transformers as proxy models for predicting overall model performance. This suggests that employing transformers can provide the necessary contextual awareness while allowing us to decouple the predictor from the MoE architecture. Such an approach enables us to quickly evaluate the state of all experts without invasive modifications, offering a more practical solution for optimizing MoE routing. Our solution is detailed in \u00a7 3.1."}, {"title": "2.3 Challenge 2: Generalizing Expert Caching for Large-Scale MoE Models", "content": "Existing MoE inference systems employ caching strategies based on temporal and spatial locality, which often struggle with the dynamic nature of expert selection in large-scale models. Temporal locality strategies like LRU and LIFO [1, 9, 14, 44\u201346] exploit recent expert usage patterns. However, our experiments (\u00a7 5.2) reveal that these strategies often fail to adapt to sudden changes or capture long-term trends, leading to suboptimal cache utilization, especially as model size increases. Spatial locality approaches, such as those identified by Lina et al. [19] in small MoE models and the ring-based prefetching strategy used by SE-MoE [36], attempt to leverage structural relationships within the MoE model. However, these methods become inefficient for large-scale MoEs, requiring substantial GPU memory and incurring significant I/O overhead. Our findings indicate that while spatial locality is more effective in smaller models like Switch-32, its effectiveness dramatically decreases in larger models."}, {"title": "Opportunity 2: Predictive Expert Caching", "content": "Building on the insights from our experiments, there is a significant opportunity to develop a more sophisticated predictive caching strategy that can accurately model and anticipate diverse expert selection patterns. The empirical results from our study comparing temporal-locality-based and spatial-locality-based prediction methods (\u00a7 5.2) show that temporal locality has some effectiveness for larger batch sizes, aligning with the intuition that increased batch sizes activate a broader range of experts. However, both empirical prediction methods achieve relatively low accuracy, especially as model size increases. This situation underscores the potential for a predictive caching strategy capable of dynamically optimizing cache management in large-scale MoE models, adapting to the complex and evolving nature of expert utilization. Our proposed solution, detailed in \u00a7 3.2, addresses the challenges of efficient caching for MoE inference by leveraging a novel approach that integrates both temporal and spatial"}, {"title": "2.4 Challenge 3: I/O Bottlenecks and Expert Inefficiencies", "content": "Previous studies [26, 27, 48] have shown that standard LLM inference has distinct characteristics-prefilling is compute-bound while decoding is memory-bound-but MoE inference introduces additional unique challenges on top of these. During the prefilling phase, the processing of a large number of tokens can lead to the activation of a significant proportion of experts within each layer. This extensive activation surpasses the capacity of expert caching mechanisms, leading to frequent expert switching and elevated I/O overhead due to constant data transfers between CPU and GPU memory. Such overhead is particularly pronounced in models with a limited number of experts per layer and large batch sizes, such as Mixtral-8\u00d77B, which has only eight experts per layer. Conversely, in the decoding phase, the tokens distribution across experts is typically uneven, causing some experts to handle very few tokens. According to the roofline model [43], the computational cost of processing a single token is nearly equivalent to that of multiple tokens. This imbalance results in underutilization of computational resources, as many experts are not fully leveraged. These dual-dimensional inefficiencies underscore the inherent challenges in optimizing MoE systems for efficient inference."}, {"title": "Opportunity 3: Cost-Effective Token Distribution Strategy", "content": "In MoE training, token load balancing and advanced expert routing mechanisms [10, 35, 49] ensure even token distribution across experts, increasing computational load by requiring dynamic adjustments for optimal token allocation. While beneficial for training by promoting uniform learning and maximizing resource utilization, this approach introduces significant computational overhead. Direct application of these methods to MoE inference is impractical due to the additional computational burden, which can delay response times and degrade system performance. MoE inference thus requires a token distribution strategy that aligns with expert specialization and current computational loads without unnecessary costs. To overcome this, we propose a cost-effective token distribution strategy for MoE inference that optimizes expert utilization and minimizes computational overhead, detailed in Section 3.3. This method effectively balances the workload across experts, ensuring efficient processing and swift inference without compromising responsiveness."}, {"title": "3 ExpertFlow", "content": "This section provides a comprehensive overview of our system ExpertFlow, designed to enhance the inference efficiency of MoE models under resource-constrained environments."}, {"title": "3.1 MoE Routing Path Predictor", "content": "Our predictor aims to overcome these two key limitations: 1) Decoupling the predictor from MoE computation: We develop an encoder-decoder transformer architecture that enables the predictor to learn the contextual relationships between input tokens independently, without relying on MoE's intermediate results; 2) Classification-based training: We reformulate the routing path prediction task as a classification problem, reducing the complexity of training the predictor and improving stability."}, {"title": "3.1.1 Predictor Architecture", "content": "To achieve independence from MoE computation, we develop a transformer-based predictor known for its exceptional contextual modeling capabilities. We selected the T5 encoder-decoder architecture [30] over a decoder-only model due to its advanced attention mechanisms. Decoder-only models, like GPT [3, 28, 29], which use causal attention, restrict tokens to attending only to previous ones, thereby limiting the scope of contextual understanding. In contrast, the T5 encoder employs full self-attention, enabling each token to interact with all other tokens in the input sequence. Furthermore, the T5 decoder utilizes cross-attention to incorporate context from the encoder, greatly enhancing the predictor's capacity to capture"}, {"title": "3.1.2 Predictor Training", "content": "To train the predictor, we first construct the routing path dataset, which captures the relationship between input tokens and their corresponding routing paths through the MoE model. Given a tokenized input sequence X = {x\u2081, x\u2082,...}, the MoE model autoregressively generates an output sequence Y = {Y\u2081, Y\u2082,..., Yn}, where n \u2264 N and N is the predefined maximum output length.\nFor each token in the input sequence X and its corresponding output in Y, we track the MoE routing paths. These paths are stored in a routing path set R = {r\u2081, r\u2082,...}, where each element r\u1d62 \u2208 R^(L\u00d7E) is a binary matrix and records the expert selection at every layer for token i. Consequently, each data sample in routing path dataset is a triplet (X, Y, R) for each input sequence.\nThe training of our predictor is formulated as a classification task, with the objective of accurately predicting expert selection across all layers within the MoE model. We employ the binary cross-entropy (BCE) loss \u2112 to measure the discrepancy between the predicted probabilities and the actual expert selections, calculated for each layer. To clarify the formulation and avoid misuse of subscripts, we present the loss function for a single token with a ground truth routing path r and the predictor output logits p:"}, {"title": "3.2 Expert Cache Engine (ECE)", "content": "We introduce the Expert Cache Engine (ECE), a comprehensive system designed to adaptively manage experts in Mixture of Experts (MoE) models, addressing the challenges posed by dynamic expert routing. As illustrated in Fig. 3, ECE orchestrates the efficient movement and utilization of experts between CPU and GPU memory through below key modules:"}, {"title": "3.2.1 Predictive Locality-aware Expert Caching (PLEC)", "content": "Traditional caching strategies like LRU [1, 9, 44, 46] often struggle in large-scale MoE models, leading to cache thrashing due to frequent data swaps. This inefficiency stems from LRU's inability to account for the limitation of the maximum number of active experts per layer or their locality. For instance, Cache-MoE [9] sets a fixed cache size for each layer, resulting in increased latency and computational overhead when the number of active experts exceeds this limit.\nTo this end, we propose Predictive Locality-aware Expert Caching (PLEC), which leverages routing path information predicted by the predictor to anticipate active experts and prefetch them efficiently into the cache. This approach prioritizes expert locality, minimizing I/O disruptions during computation. For example, in an MoE model with 4 layers and 4 experts per layer, a traditional LRU might assign 2 experts per layer for a total cache size of 8. In a worst-case scenario where all experts in each layer become active, LRU would result in constant expert swapping. However, PLEC can use the predicted routing path to prefetch all 8 experts from the first two layers, fully utilizing the cache and avoiding unnecessary data transfers. As the computation of the first layer completes, PLEC asynchronously prefetches experts for the third layer, reducing I/O overhead and improving throughput."}, {"title": "3.2.2 Real-time Correction Mechanism", "content": "Although the predictor enables efficient cache scheduling, its inaccuracies can result in two types of experts: 1) mis-prefetched experts, which are idle experts incorrectly predicted as active and thus unnecessarily loaded into the cache, and 2) missed experts, which are active experts mistakenly predicted as idle, causing them to be absent from the cache. To address these cases, we introduce a real-time correction mechanism based on the MoE gate's output during computation, with a focus on handling the missed experts. These experts are labeled as on-demand experts and assigned high priority for immediate loading into the cache.\nThe correction mechanism first attempts to replace any mis-prefetched experts with on-demand experts, since the former are idle and do not contribute to ongoing computations. If no mis-prefetched experts are available, the mechanism then replaces experts that have already completed their computations. This approach ensures that the cache is dynamically adjusted without disrupting the ongoing processing of correctly predicted experts, optimizing resource usage and minimizing the impact of prediction inaccuracies."}, {"title": "3.3 Token Scheduler", "content": "we propose the Token Scheduler to address the challenges of excessive expert activation during prefilling and uneven token distribution during decoding. By utilizing predictions from a routing path predictor, it efficiently reorganizes tokens sharing similar routing paths into the same batch. This minimizes unnecessary expert activations and significantly improves the overall efficiency of inferencing MoE models. \nFig. 4 illustrates the workflow of our Token Scheduler. Consider two input batches, each containing four tokens, processed through a single MoE layer with four experts for simplicity. In a standard MoE setup, as illustrated in Fig. 4"}, {"title": "3.3.1 Adaptive KV-Cache Management for Consistent MoE Performance", "content": "The Key-Value (KV) cache is crucial for storing intermediate representations linked to specific input tokens, thereby enhancing computational efficiency during inference. Given the autoregressive nature of Mixture of Experts (MoE) computations, it is vital to maintain consistency across time steps, as each output relies on the preceding KV cache. However, the Token Scheduler's rebatching of input tokens presents challenges in aligning the KV cache with the new token distribution. Such misalignment can result in inconsistencies in MoE outputs, highlighting the need for an adaptive approach to manage the KV cache effectively. To tackle this challenge, we introduce two key operations, Merge and Reindex:\n1.  Merge consolidates the KV cache from different batches based on a global indexing scheme. This ensures that despite the reorganization of input tokens, all necessary representations remain unified and accessible, maintaining coherence throughout the computational process.\n2.  Reindex dynamically adjusts the indices of KV cache entries to align with the positions of tokens in their new batches. This precise alignment guarantees that each token accesses the correct cache entries, preserving the accuracy of subsequent MoE computations.\nThese adaptive strategies, integrated with the Token Scheduler, ensure that MoE models maintain consistent and reliable performance, even as input token distributions shift dynamically during inference. They not only preserve the integrity of MoE computations but also enhance the model's robustness and efficiency in handling real-world data."}, {"title": "3.3.2 Token Scheduling Problem Definition", "content": "As illustrated in Fig. 4, we consider two batches, each containing T tokens. Each token i in a batch is associated with a binary routing path matrix r\u1d62 of size (L, E). We assign a global index to each token across both batches, forming the set T = {1, 2, ..., 2T}. The goal of the Token Scheduler is to reorganize these tokens into two new batches, producing two sets of indices, T\u2081 and T\u2082, such that |T\u2081| = |T\u2082| = T and T\u2081\u222a T\u2082 = T. The batch-level routing path matrices R\u2081 and R\u2082 for the new batches are obtained by performing an element-wise AND (/\\) operation on the individual token-level routing path matrices:"}, {"title": "3.3.3 K-Means Clustering for Fast Token Rebatching", "content": "Given the complexity of the original optimization problem, finding an exact solution within limited time impractical, as the token scheduler must operate efficiently. We propose a fast heuristic solution based on K-means clustering, which leverages the similarity between token routing path matrices. This approach approximates a near-optimal solution and significantly improves inference speed, as demonstrated by our experiments.\nWe first calculate the similarity matrix S between all pairs of token routing path matrices using the Hamming distance. Formally, for tokens i and j, the similarity S\u1d62\u2c7c is defined as:\nK-means clustering begins with random centroid selection, followed by assigning each token to the cluster with the highest similarity to its centroid. To maintain balanced clusters, adjustments are made when a cluster exceeds the target size T, ensuring each cluster contains exactly T tokens. Once the tokens are assigned, the centroids are updated by selecting the token within each cluster that has the highest average similarity to the others. This iterative process continues until convergence or a preset iteration limit is reached. The output is two balanced sets of tokens, T\u2081 and T\u2082, each with T tokens. By clustering tokens based on their routing path similarity, our scheduler efficiently minimizes the number of activated experts, improving computational efficiency and reducing resource usage."}, {"title": "4 Inference Pipeline Optimization", "content": "In this section, we outline two pipeline optimization strategies designed to enhance the efficiency of our three key components. This approach ensures that any potential overhead introduced by these components is effectively managed and minimized."}, {"title": "4.1 Multi-Stream Overlapping Pipeline", "content": "The Multi-Stream Overlapping Pipeline, illustrated in Fig. 5, addresses I/O bottlenecks inherent in MoE layer computations. Unlike traditional sequential offload pipelines that struggle with dynamic expert selection, our approach combines efficient stream management with predictive routing to optimize resource utilization.\nOur system leverages predictive routing information for proactive expert management. As shown in Fig. 5, when transitioning from experts [5, 6, 7, 8] in the previous step to [2, 3, 4, 5] in the current step, our pipeline prefetches required experts during the previous step's computation. This strategy ensures that necessary experts are available in the GPU cache, reducing I/O-induced latencies and smoothing inter-step transitions. Furthermore, we exploit PCIe's bidirectional bandwidth to enable concurrent fetch and offload operations. This multi-stream approach, guided by predictive routing, significantly mitigates the impact of dynamic expert scheduling on system performance, offering a more efficient solution for MoE computations."}, {"title": "4.2 Dual-batch Inference Pipeline", "content": "To address the computational overhead introduced by the Routing Path Predictor and Token Scheduler, we propose an innovative Dual-Batch Inference Pipeline, as illustrated in Fig. 6. This pipeline architecture treats every two batches as a fundamental scheduling unit, necessitated by the Token Scheduler's requirement to reorganize input tokens between batches. The primary innovation of our tailored pipeline lies in its ability to overlap routing path prediction, token scheduling, and primary decoding tasks.\nBy interleaving routing path prediction and token scheduling within each dual-batch unit, these operations are executed concurrently with the main decoding process. This parallelization strategy effectively minimizes the additional overhead incurred from prediction and scheduling tasks, resulting in enhanced communication overlap and optimized computational efficiency. Consequently, the routing path prediction and token scheduling processes are seamlessly integrated into the overall inference workflow, facilitating high throughput and efficient resource utilization across all batches."}, {"title": "5 Evaluation", "content": "In this section, we first present the experimental setup (\u00a7 5.1) and the performance of routing path predictor (\u00a7 5.2). We then quantitatively demonstrate ExpertFlow's effectiveness in improving inference speed (\u00a7 5.3) and memory saving (\u00a7 5.4) on various large-scale MoE models. Finally, we conduct extensive ablation studies to evaluate the impact of different crucial system components."}, {"title": "5.1 Setup", "content": "Hardware Settings: Our experiments were conducted on a single NVIDIA A40 GPU with 48 GB of memory and Intel(R) Xeon(R) Gold 6338 CPU @ 2.00GHz.\nTasks and MoEs: We evaluate on two tasks: translation (WMT16 [2]) and summarization (XSUM [24]), using four MoE models: Mixtral-8\u00d77B [16] and Switch-32/64/128 with 32, 64, and 128 experts per layer, respectively [10].\nRouting Path Datasets: We created eight datasets by pairing above tasks and MoEs. For each dataset, we sampled 10,000 input sequences, each processed three times through"}, {"title": "5.2 Predictor Evaluation", "content": "Evaluation Metrics. For an MoE with L layers and E experts per layer, we introduce batch-level accuracy (Bacc) to evaluate prediction performance:\nTable 1 reports batch-level accuracy for various batch sizes, where our predictor consistently outperforms both TLP and SLP across all settings, demonstrating superior generalization and adaptability across diverse applications."}, {"title": "5.3 Inference Speed Evaluation", "content": "Results on Switch MoE Series. In Fig.7, our ExpertFlow system exhibits significant speedup across the Switch MoE series, with a peak of 9.99\u00d7 on the WMT16 dataset and 9.54\u00d7 on XSUM, compared to SE-MoE. Notably, as the model scales from Switch-32 to Switch-128, the average speedup increases from 2.53x to 7.94\u00d7. This trend highlights that our system becomes more effective as the number of experts increases,"}, {"title": "5.4 Memory Cost Evaluation", "content": "This section compares the efficiency of GPU memory savings achieved by various methods applied to Switch series and Mixtral-8\u00d77B MoE models. The assessment of GPU memory costs includes model parameters, input/output data, and intermediate activations such as key-value (KV) caches.\nResults on Switch MoE Series. Table 2 presents the memory savings achieved under various configurations of Switch models, tasks, and settings, including cache size (CS) and batch size (BS). The cache size indicates the proportion of experts that can be simultaneously resident on the GPU. For instance, a cache size of 16 for Switch-32 means that half of the experts can be accommodated on the GPU, though the effectiveness of this caching varies among different methods. Our ExpertFlow w/o predictor demonstrates substantial memory savings across a range of MoE models and configurations, achieving an average GPU memory saving of 74.31% (with a peak of 93.35%) on XSUM and 76.49% (with a peak of 93.72%) on WMT16. Although integrating a routing path predictor slightly increases memory usage compared to, the performance improvements (as detailed in Fig. 7) justify this trade-off, highlighting the advantages of intelligent routing mechanisms.\nAs MoE models scale from Switch-32 to Switch-128, all evaluated methods can increase the memory savings due to more experts being offloaded to the CPU, highlighting the need for scalable memory management strategies. However, in smaller models like Switch-32, increasing cache and batch sizes can drastically reduce memory savings, with Pregated-MoE experiencing a drop from 63.76% to 13.80%. This demonstrates the sensitivity of smaller models to caching mechanisms and batch configurations, indicating a need for customized strategies. To delve deeper into these effects, we conducted an ablation study (\u00a7 5.5) on how cache and batch size variations impact memory efficiency in smaller MoE models, aiming to gain further insights into the factors affecting memory savings in MoE systems.\nResults on Mixtral-8\u00d77B MoE. Due to limited compatibility with most baseline methods, our evaluation of the Mixtral-8\u00d77B model, which requires 96 GB of GPU memory for storage and causes out-of-memory (OOM) errors for the All-In-GPU (AIG) method, focuses on comparisons with Cache-MoE for memory savings. We tested different cache sizes while keeping the batch size constant at 16. Our results mirrored those from Switch models, with both Cache-MoE and our system using around 16 GB and 37 GB of GPU memory for cache sizes of 1 and 3, respectively. These findings highlight our ExpertFlow system's effective memory management, even in high-demand scenarios like the Mixtral-8\u00d77B model."}, {"title": "5.5 Ablation Study on System Components", "content": "Impact of Expert Cache Engine on Cache Hit Ratio. We conducted an ablation study to evaluate the effectiveness of our Expert Cache Engine, comparing it with the conventional LRU caching strategy. This study focused on the WMT16 dataset using the Switch-32 model, with results illustrated in Fig. 9. The results reveal that increasing cache size consistently improves the hit ratio, while larger batch sizes under a fixed cache size lead to decreased hit ratios. This aligns with the expectation that larger batch sizes tend to activate more experts concurrently, leading to a higher likelihood of exceeding cache capacity."}, {"title": "6 Discussion", "content": "Adaptive Cache and Batch Size Management. Our results in Fig. 9 demonstrate an inverse relationship between cache size and batch size on hit ratio, with larger batch sizes decreasing hit ratio but increasing throughput. This suggests the potential for an adaptive system that adjusts these parameters in real-time, balancing hit ratio optimization with throughput maximization to adapt to varying workload conditions and optimize resource utilization.\nBackup All Experts on CPU. To further enhance inference efficiency, future versions of our system could relocate the predictor to the CPU, freeing up valuable GPU memory for MoE model computations. Additionally, we have explored backing up all experts on the CPU, which eliminates the need for offloading and simplifies the system architecture. This approach reduces GPU memory overhead, requiring only an additional allocation equivalent to one expert's size for the multi-stream overlapping pipeline. In environments with abundant CPU resources, this strategy could simplify implementation while maintaining efficiency and reducing GPU memory demands.\nBoosting MoE Offloading Efficiency with CPU Computational Power. Our efforts to enhance MoE models by using the CPU's memory storage have been promising. Inspired by the Fiddler method [17], we plan to further utilize CPU computational power. This strategy involves shifting expert layer computations to the CPU while keeping activations on the GPU, potentially boosting system performance. By integrating this with predictive routing, we aim to better distribute workloads between the CPU and GPU, reducing GPU load and improving memory efficiency. This approach prepares us for future improvements in MoE deployment in resource-limited settings."}, {"title": "7 Related Work", "content": "The MoE-based LLM inference process presents significant GPU memory challenges, prompting prior research to explore a range of algorithmic and system-level solutions that can be broadly categorized into two major strands:\nMemory Compression. Distillation techniques [7, 36] reduce the number of experts by compressing the teacher network into a smaller student network. Model pruning methods have also been explored, such as pruning non-essential experts during fine-tuning based on usage frequency [6] and merging similar experts followed by low-rank decomposition [21]. Post-training quantization [9, 11, 20, 22] further reduces memory consumption by converting pre-trained models from high-precision (e.g., Float32) formats to lower-precision ones (e.g., Int4), without requiring extensive retraining. The contribution of our proposed ExpertFlow is orthogonal to this direction, and ExpertFlow can be seamlessly integrated with these techniques to further reduce GPU memory cost during MoE inference.\nMemory Offloading. Memory offloading involves transferring model components or computations to less expensive storage or processing units, optimizing GPU usage. Early efforts, such as the ZeRO series [31, 34] reduced memory by offloading optimizer states, gradients, and weights to CPUs or SSDs. Offloading strategies have since been extended to inference tasks [5, 37, 38]. For instance, FlexGen [37] implements a \u201czig-zag block schedule\u201d to offload activations and key-value (KV) caches, enabling models like OPT-175B [47] to run efficiently on a single 16GB GPU. Lamina [5] further improves efficiency by shifting attention computations from GPU to CPU. Despite these advancements, existing offloading techniques are primarily designed for dense LLMs and fail to fully address the flexible, dynamic expert routing in MoE models. Even there have emerged offloading methods tailored for MoE, they struggle with insufficient accuracy in routing paths prediction [9, 19] or high training costs for predictors [8, 14], undermining their practicality. Our work presents a unified system that tackles the critical challenges of improving the accuracy of routing path predictions and reducing the training overhead of predictors simultaneously."}, {"title": "8 Conclusion", "content": "This paper introduces ExpertFlow, a novel system that significantly improves MoE model deployment by addressing critical memory and computational challenges. Our approach integrates three key components: the Routing Path Predictor, Expert Cache Engine, and Token Scheduler, which synergistically optimize MoE inference. By accurately predicting expert selection patterns, ExpertFlow enables proactive expert scheduling and efficient token re-batching, substantially reducing GPU memory usage while maintaining competitive inference speeds. Comprehensive evaluations across various datasets and MoE models demonstrate the effectiveness of our ExpertFlow system and showcase the potential of predictive modeling and intelligent resource management in deploying large-scale neural networks, opening new avenues for research in adaptive inference strategies and specialized hardware for MoE models. This work contributes to making advanced AI models more accessible and practical for real-world scenarios."}]}