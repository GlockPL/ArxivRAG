{"title": "DEPICT: Diffusion-Enabled Permutation\nImportance for Image Classification Tasks", "authors": ["Sarah Jabbour", "Gregory Kondas", "Ella Kazerooni", "Michael Sjoding", "David Fouhey", "Jenna Wiens"], "abstract": "We propose a permutation-based explanation method for im-\nage classifiers. Current image-model explanations like activation maps\nare limited to instance-based explanations in the pixel space, making it\ndifficult to understand global model behavior. In contrast, permutation\nbased explanations for tabular data classifiers measure feature impor-\ntance by comparing model performance on data before and after permut-\ning a feature. We propose an explanation method for image-based mod-\nels that permutes interpretable concepts across dataset images. Given a\ndataset of images labeled with specific concepts like captions, we permute\na concept across examples in the text space and then generate images via\na text-conditioned diffusion model. Feature importance is then reflected\nby the change in model performance relative to unpermuted data. When\napplied to a set of concepts, the method generates a ranking of feature\nimportance. We show this approach recovers underlying model feature\nimportance on synthetic and real-world image classification tasks.", "sections": [{"title": "1 Introduction", "content": "Understanding AI model predictions is often important for safe deployment.\nHowever, explanation methods for image-based models are instance-based and\nrely on heatmaps or masks in the pixel space [21,28,32,42], and recent work has\ncalled into question their utility [1,2,13]. We hypothesize that these methods fall\nshort in part because they are in the pixel space rather than in the concept space\n(e.g., presence of an object), leading to an increase in the cognitive load placed\non a user. Furthermore, while useful for model debugging, it is often intractable\nto look at instance-based explanations for every single image in a large test set.\nWe propose an approach for explaining image-based models that uses per-\nmutation importance to produce dataset-level explanations in the concept space.\nIn contrast to instance-based explanations, our method generates a ranking of"}, {"title": "2 Related Works", "content": "We introduce DEPICT, a diffusion-enabled permutation importance approach\nto understand image-based classifiers. DEPICT lies at the intersection of ex-\nplainable AI, generative models, and human-computer interaction.\nExplainable AI. Explainable AI allows us to understand model behavior [35].\nGlobal explanations allow us to do so as a whole. E.g., linear models that oper-\nate directly on the input space are explainable via their weights, which reflect\nthe importance of each input feature with respect to the model's output [34].\nThe usefulness of these explanations depends in part on the interpretability of\nthe input space. If the input space is just pixels, such explanations are unlikely"}, {"title": "3 Method", "content": "Overview. In our setting, we have a set of test images and a black-box model\n$f: I \\rightarrow Y$ that maps images in $\\mathcal{I}$ to predictions in $\\mathcal{Y}$. In standard permutation\nimportance, one permutes a single feature across instances while holding the oth-\ners constant and examines the drop in model performance relative to baseline.\nThis does not yield meaningful explanations when permuting in pixel space. In-\nstead, we assume there is a relevant concept-based text space $\\mathcal{T}$ where permuting\nconcepts is easy (e.g., image captions). Given a text-conditioned diffusion model\n$g: \\mathcal{T} \\rightarrow \\mathcal{I}$, we permute concepts in text space $\\mathcal{T}$, transform captions to image\nspace $\\mathcal{I}$ with $g$, and use the generated images as a proxy for permutations in\nimage space.\nAccurately estimating model feature importance via this approach requires\nthree testable assumptions: (1) Permutable concepts: we can permute a set of rel-\nevant concepts in $\\mathcal{T}$; (2) Effective generation: we can obtain a mapping $g : \\mathcal{T} \\rightarrow \\mathcal{I}$\nsuch that $f$ can accurately classify generated instances; (3) Independent Permu-\ntation: while changing a concept for a set of instances, the other concepts in the\ninstances do not change. These assumptions require some algorithmic decisions\nand data considerations that we discuss below and verify in our experiments."}, {"title": "3.1 Permutation Importance on Tabular Data", "content": "We begin by recounting how permutation importance is performed in tabular\ndata [5] to aid in describing our approach. For simplicity we focus on binary\nclassification, although permutation importance generalizes to multi-class clas-\nsification and even regression. We assume: an input space $\\mathcal{T}$ (e.g., $\\mathbb{R}^d$ for $d$-\ndimensional numerical tabular data); a classifier $f : \\mathcal{T} \\rightarrow {0,1}$ that maps from\nthe input space to binary decisions; $N$ labeled examples ${x_i, y_i}_{i=1}^N$; and a loss $\\mathcal{L}$\nevaluating performance (e.g., error). The reference performance of the classifier\non the unpermuted data is given by $a = \\sum_{i=1}^N \\mathcal{L}(y_i, f(x_i))$ (Fig. 2).\nIn permutation importance, one permutes a single coordinate of the data\n$j$ for $j = 1,..., d$ while holding the others fixed and measures the change in\nperformance relative to the original model performance $a$. Let ${x_i^j}_{i=1}^N$ be the $N$\nexamples with the $j$th coordinate permuted among the samples. One calculates\nthe performance of $f$ on the permuted test set, as $a_j = \\sum_{i=1}^N \\mathcal{L}(y_i, f(x_i^j))$. The\npermutation importance of the $j$th coordinate for $f$ is the difference between\nthe original accuracy and the accuracy while permuting $j$, or $a - a_j$. Given"}, {"title": "3.2 Permutation Importance on Image Data", "content": "We now extend permutation importance to images. We assume: a space of images\n$\\mathcal{I}$; a classifier $f : \\mathcal{I} \\rightarrow {0,1}$ mapping images to predictions; $N$ labeled images\n${x_i, y_i}_{i=1}^N$; and a performance metric $\\mathcal{L}$.\nThe crux of the method is a parallel concept text space $\\mathcal{T}$ and functions for\nmoving between $\\mathcal{T}$ and $\\mathcal{I}$. In particular, we assume there is a concept text space\nlike scene image captions with $D$ concepts (such as the presence of a chair) that\ncan be permuted like tabular data and turned into text easily. For simplicity, we\nalso assume that we have corresponding concept labels ${c_i}_{i=1}^N$ for each input\nwith each $c_i \\in \\mathcal{T}$, where we can represent $c_i \\in {0,1}^{d}$, a $d$-dimensional binary\nvector indicating the presence of each concept. To move between the spaces,\nwe assume a generative model $g : \\mathcal{T} \\rightarrow \\mathcal{I}$ that maps a concept vector to a\nsample image matching the concepts (Fig. 2); we also assume a concept classifier\n$h : \\mathcal{I} \\rightarrow \\mathcal{T}$ that can accurately detect whether a concept appears in an image.\nFor instance, $g$ might be a diffusion model trained to map from a caption to an"}, {"title": "4 Experiments & Results", "content": "To validate DEPICT, we first consider a synthetic setting where generation is\neasy, followed by two real-world datasets: COCO [19] and MIMIC-CXR [14,16]."}, {"title": "4.1 Synthetic Dataset", "content": "In our synthetic dataset, images can contain any combination of six concepts\nthat each consist of a distinct colored geometric shape: {red, green blue} \u00d7\n{circle, rectangle}. Each image is generated according to an indicator variable\n$s\\in {0,1}^6$ indicating whether each shape is present. $s$ is drawn per-component\nfrom a Bernoulli distribution with $p = 0.5$. We generate the image $X_i$ from $s$\nby placing shapes randomly, such that no two shapes overlap. We construct a\ncaption for each image by with descriptions of each shape joined by a comma\n(e.g., a c-colored circle at (x, y) with radius r is described as \"c circle (x, y) r\")\n(full details are in supplementary 8).\nGiven images, we generate tasks and corresponding labels. Each task is de-\nfined by a weight vector $w \\in \\mathbb{R}^6$ over the six indicator variables where each\ncomponent is drawn uniformly over [0, 1]. Given the weight vector, the score of\nan image with indicator vector s is given by $w\\cdot s$. We define a binary classifica-\ntion task by thresholding image scores at the median of the dataset."}, {"title": "4.2 Real Dataset", "content": "We evaluate DEPICT's ability to generate concept-based explanations of image\nclassifiers on COCO [19]. We consider two settings reflecting different levels of\ndifficulty in ranking concepts, showing that DEPICT generates better rankings\ncompared to baselines.\nTarget models. We consider two sets of scene classifiers. For all target models,\nwe learn a concept bottleneck $g(x) \\in \\mathbb{R}^c$ where $c$ is 15 concepts that the classifier\nmay rely on (see supplementary 9 for full list). Then, we learn a linear classifier\n$f(g(x))$ parameterized by $w$ to map concepts to a final prediction. We train two\nsets of target classifiers:\nPrimary feature models. We first train binary tasks to classify images as\n{home or hotel} or {not}. By design, these models each rely heavily on one of\n15 concepts in the image: we resampled the training data such that there was a\n1:1 correlation between a concept in the image (e.g., person or couch) and the\noutcome, totalling 15 classifiers (full list in supplementary 9).\nMixed feature models. We also trained six scene classification tasks, where\na model classifies if an image is one of six scenes: (1) shopping and dining,\n(2) workplace, (3) home or hotel, (4) transportation, (5) cultural, and (6)\nsports and leisure. We did not resample the training data to encourage the\nmodel to rely on specific concepts, but instead used the entire training set to let\nthe model rely on any set of concepts (see supplementary 9 for details).\nDiffusion model. We fine-tune Stable Diffusion [30] on COCO [19] to generate\nimages for our task (examples in Fig. 5). We use COCO concept annotations as\ncaptions. E.g., if an image contains 2 persons and 1 couch, the corresponding\ncaption is \"2 person, 1 couch.\" We generate a scene label for each image using a\nnetwork trained on the Places 365 dataset [43] (full details in supplementary 9).\nUsing DEPICT. To generate model feature importances with DEPICT, we\npermute each concept in the text-space 25 times. For each permutation, we gen-\nerate a dataset with the diffusion model and pass these images through the target\nmodel. The AUROC drop compared to the dataset generated with non-permuted\ntext yields a distribution of model feature importance values per concept.\nOracle model feature ranking. We again calculate standardized regression\ncoefficients using the learned weight vector $w$. We also calculate an additional\noracle by permuting concepts at the bottleneck in the supplementary.\nBaselines. We compare DEPICT to GradCAM [32] and LIME [28]. We measure\nthe IOU between the GradCAM and LIME masks using each object annotation\nmask for each image in COCO (full details are in supplementary 9)."}, {"title": "4.3 DEPICT in Practice: A Case Study in Healthcare", "content": "Until now, we have applied DEPICT to datasets in which all concepts that\na model might rely on can be permuted. However, depending on the diffusion\nmodel and/or our knowledge of important concepts, we may only have the ability\nto permute on a subset of concepts on which the model relies. Here, we discuss"}, {"title": "5 Limitations", "content": "DEPICT's success relies on the diffusion model's ability to permute concepts ef-\nfectively and independently. In the experiments involving the synthetic dataset,\nDEPICT's ranking was highly correlated with the ranking generated by directly\npermuting concepts at the bottleneck (supplementary Fig. 11). Subsequently,\nDEPICT's ranking was also highly correlated with the ranking of the standard-\nized regression weights (Fig. 3). On the other hand, as DEPICT's ranking's cor-\nrelation with the ranking generated by permuting at the bottleneck decreased\n(supplementary Fig. 13, 14), so did its correlation with the logistic regression\nweights (Fig. 6, 8).\nFurthermore, when the diffusion model is conditioned on both permutable\nand non-permutable text (e.g., as in Section 4.3), the diffusion model could strug-\ngle to permute concepts in the image space if there are mentions of permutable\nconcepts in the non-permutable text space (e.g., if one is trying to permute\nthe patient age, and the radiology report mentions the original age of the pa-\ntient). While the concept classifier is used to ensure that the concept of interest\nhas been indeed permuted, this still limits the applicability of DEPICT. Mov-\ning forward, DEPICT's success relies on good generative models that can map\npermuted concepts in the text space to the image space effectively."}, {"title": "6 Conclusion", "content": "Understanding the reason behind AI model predictions can aid the safe de-\nployment of AI. To date, image-based model explanations have been limited to\ninstance-based explanations the pixel space [28, 32], which are difficult to in-\nterpret [1, 2, 12]. Instead, DEPICT generates image-based explanations at the\ndataset-level in the concept space. While directly permuting concepts in pixel\nspace is difficult, DEPICT permutes concepts in the text space and then gen-\nerates new images reflecting the permutations via text-conditioned diffusion.\nDEPICT relies on a text-conditioned diffusion model that effectively generates\nimages and independently permutes concepts across images. While we have in-\ncluded checks to verify these assumptions, we cannot guarantee that such a\ndiffusion model is available. However, given the rapid progress of the field, we\nexpect that the availability or the ability to train such models will improve, in-\ncreasing the feasibility of DEPICT."}, {"title": "7 Supplementary Materials Overview", "content": "This supplementary material provides additional details of the paper along with\nsupplementary results that were omitted from the main paper due to space con-\nstraints. In Section 8 we present details and additional supplementary results of\nthe synthetic dataset experiments. In Section 9, we present details and additional\nsupplementary results of the real dataset (COCO [19]) experiments. Finally, in\nSection 10, we present details and additional supplementary results of the case\nstudy in healthcare (MIMIC-CXR [14, 16])."}, {"title": "8 Synthetic Validation", "content": ""}, {"title": "8.1 Experiments", "content": "Dataset. Each image in the dataset is described by a set of concepts describing\ndistinct colored geometric shapes: {red, green blue} \u00d7 {circle, rectangle}. Given\nthe vector of indicator variables $s_i \\in \\mathbb{R}^6$, we construct the image $X_i$ by randomly\nplacing each of the shapes in the image such that no two shapes overlap. The\ncaption for the image is then a string describing each of the shapes in the image,\nseparated by a comma. For instance, an image containing a red-colored circle\nof radius 4 centered at (5,10) and a blue-colored rectangle with the top-left\ncorner at (20,30) and bottom-right corner at (50,60) would have the caption\n\"red circle 4 (5, 10), blue rectangle ((20,30) (50, 60))\". We show examples of real\nand generated images of the synthetic shapes dataset in Fig. 10. We note that,\nwhile the diffusion model does not generate the correct locations for the shapes,\nthis does not affect downstream classification results which do not rely on shape\nlocations.\nDiffusion Model. A diffusion model initialized on Stable Diffusion [30"}]}