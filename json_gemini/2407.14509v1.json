{"title": "DEPICT: Diffusion-Enabled Permutation\nImportance for Image Classification Tasks", "authors": ["Sarah Jabbour", "Gregory Kondas", "Ella Kazerooni", "Michael Sjoding", "David Fouhey", "Jenna Wiens"], "abstract": "We propose a permutation-based explanation method for im-\nage classifiers. Current image-model explanations like activation maps\nare limited to instance-based explanations in the pixel space, making it\ndifficult to understand global model behavior. In contrast, permutation\nbased explanations for tabular data classifiers measure feature impor-\ntance by comparing model performance on data before and after permut-\ning a feature. We propose an explanation method for image-based mod-\nels that permutes interpretable concepts across dataset images. Given a\ndataset of images labeled with specific concepts like captions, we permute\na concept across examples in the text space and then generate images via\na text-conditioned diffusion model. Feature importance is then reflected\nby the change in model performance relative to unpermuted data. When\napplied to a set of concepts, the method generates a ranking of feature\nimportance. We show this approach recovers underlying model feature\nimportance on synthetic and real-world image classification tasks.", "sections": [{"title": "1 Introduction", "content": "Understanding AI model predictions is often important for safe deployment.\nHowever, explanation methods for image-based models are instance-based and\nrely on heatmaps or masks in the pixel space [21,28,32,42], and recent work has\ncalled into question their utility [1,2,13]. We hypothesize that these methods fall\nshort in part because they are in the pixel space rather than in the concept space\n(e.g., presence of an object), leading to an increase in the cognitive load placed\non a user. Furthermore, while useful for model debugging, it is often intractable\nto look at instance-based explanations for every single image in a large test set.\nWe propose an approach for explaining image-based models that uses per-\nmutation importance to produce dataset-level explanations in the concept space.\nIn contrast to instance-based explanations, our method generates a ranking of"}, {"title": "2 Related Works", "content": "We introduce DEPICT, a diffusion-enabled permutation importance approach\nto understand image-based classifiers. DEPICT lies at the intersection of ex-\nplainable AI, generative models, and human-computer interaction.\nExplainable AI. Explainable AI allows us to understand model behavior [35].\nGlobal explanations allow us to do so as a whole. E.g., linear models that oper-\nate directly on the input space are explainable via their weights, which reflect\nthe importance of each input feature with respect to the model's output [34].\nThe usefulness of these explanations depends in part on the interpretability of\nthe input space. If the input space is just pixels, such explanations are unlikely"}, {"title": "3 Method", "content": "Overview. In our setting, we have a set of test images and a black-box model\nf:IY that maps images in I to predictions in Y. In standard permutation\nimportance, one permutes a single feature across instances while holding the oth-\ners constant and examines the drop in model performance relative to baseline.\nThis does not yield meaningful explanations when permuting in pixel space. In-\nstead, we assume there is a relevant concept-based text space T where permuting\nconcepts is easy (e.g., image captions). Given a text-conditioned diffusion model\ng: T\u2192 I, we permute concepts in text space T, transform captions to image\nspace I with g, and use the generated images as a proxy for permutations in\nimage space.\nAccurately estimating model feature importance via this approach requires\nthree testable assumptions: (1) Permutable concepts: we can permute a set of rel-\nevant concepts in T; (2) Effective generation: we can obtain a mapping g : T\u2192I\nsuch that f can accurately classify generated instances; (3) Independent Permu-\ntation: while changing a concept for a set of instances, the other concepts in the\ninstances do not change. These assumptions require some algorithmic decisions\nand data considerations that we discuss below and verify in our experiments."}, {"title": "3.1 Permutation Importance on Tabular Data", "content": "We begin by recounting how permutation importance is performed in tabular\ndata [5] to aid in describing our approach. For simplicity we focus on binary\nclassification, although permutation importance generalizes to multi-class clas-\nsification and even regression. We assume: an input space T (e.g., Rd for d-\ndimensional numerical tabular data); a classifier f : T \u2192 {0,1} that maps from\nthe input space to binary decisions; N labeled examples {xi, Yi}=1; and a loss L\nevaluating performance (e.g., error). The reference performance of the classifier\non the unpermuted data is given by a = \u2211i=1L(Yi, f(xi))\n\nIn permutation importance, one permutes a single coordinate of the data\nj for j = 1,..., d while holding the others fixed and measures the change in\nperformance relative to the original model performance a. Let {x}1 be the N\nexamples with the jth coordinate permuted among the samples. One calculates\nthe performance of f on the permuted test set, as aj = \u2211i=1L(yi, f(x)). The\npermutation importance of the jth coordinate for f is the difference between\nthe original accuracy and the accuracy while permuting j, or a aj. Given"}, {"title": "3.2 Permutation Importance on Image Data", "content": "We now extend permutation importance to images. We assume: a space of images\nI; a classifier f: I \u2192 {0,1} mapping images to predictions; N labeled images\n{xi, Yi}1; and a performance metric L.\nThe crux of the method is a parallel concept text space T and functions for\nmoving between T and I. In particular, we assume there is a concept text space\nlike scene image captions with D concepts (such as the presence of a chair) that\ncan be permuted like tabular data and turned into text easily. For simplicity, we\nalso assume that we have corresponding concept labels {c}1 for each input\nwith each ci \u2208 T, where we can represent c\u00bf \u2208 {0,1}d, d, a d-dimensional binary\nvector indicating the presence of each concept. To move between the spaces,\nwe assume a generative model g: T\u2192 I that maps a concept vector to a\nsample image matching the concepts (Fig. 2); we also assume a concept classifier\nh:IT that can accurately detect whether a concept appears in an image.\nFor instance, g might be a diffusion model trained to map from a caption to an"}, {"title": "4 Experiments & Results", "content": "To validate DEPICT, we first consider a synthetic setting where generation is\neasy, followed by two real-world datasets: COCO [19] and MIMIC-CXR [14,16]."}, {"title": "4.1 Synthetic Dataset", "content": "In our synthetic dataset, images can contain any combination of six concepts\nthat each consist of a distinct colored geometric shape: {red, green blue} \u00d7\n{circle, rectangle}. Each image is generated according to an indicator variable\ns\u2208 {0,1}6 indicating whether each shape is present. s is drawn per-component\nfrom a Bernoulli distribution with p = 0.5. We generate the image X\u2081 from s\nby placing shapes randomly, such that no two shapes overlap. We construct a\ncaption for each image by with descriptions of each shape joined by a comma\n(e.g., a c-colored circle at (x, y) with radius r is described as \"c circle (x, y) r\")\n(full details are in supplementary 8).\nGiven images, we generate tasks and corresponding labels. Each task is de-\nfined by a weight vector w \u2208 R6 over the six indicator variables where each\ncomponent is drawn uniformly over [0, 1]. Given the weight vector, the score of\nan image with indicator vector s is given by ws. We define a binary classifica-\ntion task by thresholding image scores at the median of the dataset."}, {"title": "4.2 Real Dataset", "content": "We evaluate DEPICT's ability to generate concept-based explanations of image\nclassifiers on COCO [19]. We consider two settings reflecting different levels of\ndifficulty in ranking concepts, showing that DEPICT generates better rankings\ncompared to baselines.\nTarget models. We consider two sets of scene classifiers. For all target models,\nwe learn a concept bottleneck g(x) \u2208 R where c is 15 concepts that the classifier\nmay rely on (see supplementary 9 for full list). Then, we learn a linear classifier\nf(g(x)) parameterized by w to map concepts to a final prediction. We train two\nsets of target classifiers:\nPrimary feature models. We first train binary tasks to classify images as\n{home or hotel} or {not}. By design, these models each rely heavily on one of\n15 concepts in the image: we resampled the training data such that there was a\n1:1 correlation between a concept in the image (e.g., person or couch) and the\noutcome, totalling 15 classifiers (full list in supplementary 9).\nMixed feature models. We also trained six scene classification tasks, where\na model classifies if an image is one of six scenes: (1) shopping and dining,\n(2) workplace, (3) home or hotel, (4) transportation, (5) cultural, and (6)\nsports and leisure. We did not resample the training data to encourage the\nmodel to rely on specific concepts, but instead used the entire training set to let\nthe model rely on any set of concepts (see supplementary 9 for details).\nDiffusion model. We fine-tune Stable Diffusion [30] on COCO [19] to generate\nimages for our task (examples in Fig. 5). We use COCO concept annotations as\ncaptions. E.g., if an image contains 2 persons and 1 couch, the corresponding\ncaption is \"2 person, 1 couch.\" We generate a scene label for each image using a\nnetwork trained on the Places 365 dataset [43] (full details in supplementary 9).\nUsing DEPICT. To generate model feature importances with DEPICT, we\npermute each concept in the text-space 25 times. For each permutation, we gen-\nerate a dataset with the diffusion model and pass these images through the target\nmodel. The AUROC drop compared to the dataset generated with non-permuted\ntext yields a distribution of model feature importance values per concept.\nOracle model feature ranking. We again calculate standardized regression\ncoefficients using the learned weight vector w. We also calculate an additional\noracle by permuting concepts at the bottleneck in the supplementary.\nBaselines. We compare DEPICT to GradCAM [32] and LIME [28]. We measure\nthe IOU between the GradCAM and LIME masks using each object annotation\nmask for each image in COCO (full details are in supplementary 9).\nEvaluation & Results. We quantitatively and qualitatively evaluate DEPICT\non COCO just as we did in the synthetic setting, as well as validate the as-\nsumptions of effective generation and independent permutation using a concept\nclassifier trained to predict the concepts in COCO (full details in supplementary\n9). Furthermore, for quantitative evaluation, we consider k \u2208 [1,15], as there are\n15 concepts to threshold over in the COCO models."}, {"title": "4.3 DEPICT in Practice: A Case Study in Healthcare", "content": "Until now, we have applied DEPICT to datasets in which all concepts that\na model might rely on can be permuted. However, depending on the diffusion\nmodel and/or our knowledge of important concepts, we may only have the ability\nto permute on a subset of concepts on which the model relies. Here, we discuss"}, {"title": "5 Limitations", "content": "DEPICT's success relies on the diffusion model's ability to permute concepts ef-\nfectively and independently. In the experiments involving the synthetic dataset,\nDEPICT's ranking was highly correlated with the ranking generated by directly\npermuting concepts at the bottleneck (supplementary Fig. 11). Subsequently,\nDEPICT's ranking was also highly correlated with the ranking of the standard-\nized regression weights (Fig. 3). On the other hand, as DEPICT's ranking's cor-\nrelation with the ranking generated by permuting at the bottleneck decreased\n(supplementary Fig. 13, 14), so did its correlation with the logistic regression\nweights (Fig. 6, 8).\nFurthermore, when the diffusion model is conditioned on both permutable\nand non-permutable text (e.g., as in Section 4.3), the diffusion model could strug-\ngle to permute concepts in the image space if there are mentions of permutable\nconcepts in the non-permutable text space (e.g., if one is trying to permute\nthe patient age, and the radiology report mentions the original age of the pa-\ntient). While the concept classifier is used to ensure that the concept of interest\nhas been indeed permuted, this still limits the applicability of DEPICT. Mov-\ning forward, DEPICT's success relies on good generative models that can map\npermuted concepts in the text space to the image space effectively."}, {"title": "6 Conclusion", "content": "Understanding the reason behind AI model predictions can aid the safe de-\nployment of AI. To date, image-based model explanations have been limited to\ninstance-based explanations the pixel space [28, 32], which are difficult to in-\nterpret [1, 2, 12]. Instead, DEPICT generates image-based explanations at the\ndataset-level in the concept space. While directly permuting concepts in pixel\nspace is difficult, DEPICT permutes concepts in the text space and then gen-\nerates new images reflecting the permutations via text-conditioned diffusion.\nDEPICT relies on a text-conditioned diffusion model that effectively generates\nimages and independently permutes concepts across images. While we have in-\ncluded checks to verify these assumptions, we cannot guarantee that such a\ndiffusion model is available. However, given the rapid progress of the field, we\nexpect that the availability or the ability to train such models will improve, in-\ncreasing the feasibility of DEPICT."}, {"title": "7 Supplementary Materials Overview", "content": "This supplementary material provides additional details of the paper along with\nsupplementary results that were omitted from the main paper due to space con-\nstraints. In Section 8 we present details and additional supplementary results of\nthe synthetic dataset experiments. In Section 9, we present details and additional\nsupplementary results of the real dataset (COCO [19]) experiments. Finally, in\nSection 10, we present details and additional supplementary results of the case\nstudy in healthcare (MIMIC-CXR [14, 16])."}, {"title": "8 Synthetic Validation", "content": "8.1 Experiments\nDataset. Each image in the dataset is described by a set of concepts describing\ndistinct colored geometric shapes: {red, green blue} \u00d7 {circle, rectangle}. Given\nthe vector of indicator variables s\u00bf \u2208 R6, we construct the image X\u2081 by randomly\nplacing each of the shapes in the image such that no two shapes overlap. The\ncaption for the image is then a string describing each of the shapes in the image,\nseparated by a comma. For instance, an image containing a red-colored circle\nof radius 4 centered at (5,10) and a blue-colored rectangle with the top-left\ncorner at (20,30) and bottom-right corner at (50,60) would have the caption\n\"red circle 4 (5, 10), blue rectangle ((20,30) (50, 60))\". We show examples of real\nand generated images of the synthetic shapes dataset in Fig. 10. We note that,\nwhile the diffusion model does not generate the correct locations for the shapes,\nthis does not affect downstream classification results which do not rely on shape\nlocations.\nDiffusion Model. A diffusion model initialized on Stable Diffusion [30] was\nfine tuned for 105000 iterations on 107,000 images with a batch size of 16 at a\n256x256 resolution and a learning rate of 1.0e-4. We fine-tuned only the U-Net\nand text-encoder of the model.\nConcept Classifier. The concept classifier g was a CNN with 5 layers, each\nconsisting of a convolution, batch norm, ReLU, and max pooling followed by a\n3-layer multilayer perceptron that made six predictions for the presence of the\nsix shapes. The model was trained on 50,000 images for 15 epochs.\nBaselines. We generated Grad-CAM [32] and LIME [28] explanations for the\npredicted class of each image. The class prediction was determined by threshold-\ning model predictions that maximized the true positive rate while minimizing\nthe false positive rate across the test set. Each GradCAM heatmap was first\nconverted to a binary mask by thresholding at the lowest non-zero value of the\nGrad-CAM heatmap. 5 features were used to generate each LIME mask. For ev-\nery shape in the image, we calculated the intersection over union (IOU) between\nthe shape and the explanation. Finally, we ranked shapes by their mean IOU\nacross the entire test set."}, {"title": "9 COCO", "content": "9.1 Experiments\nDataset. COCO [23] contains 117k training and 4.5k validation images anno-\ntated with 80 object categories, which we consider to be concepts in the images.\nCOCO also has 20k test images that are not labelled with object categories.\nInstead, we randomly sampled 10k images from the training set to use for test\nsets in downstream classification tasks, resulting in a final training set of 107k\nimages. To caption each image, we disregarded the natural language captions\ncorresponding to the images, and instead constructed new captions consisting of\nall the concepts in the images. E.g., if an image contained 2 persons and 1 couch,\nthe corresponding caption is \"2 person, 1 couch.\" The 15 concepts used were: per-\nson, bottle, cup, bowl, chair, couch, bed, dining table, tv, laptop, remote, cell\nphone, oven, sink, and book. For downstream scene classification, we labelled\neach of the images using a ResNet trained on Places365 [43]. We mapped the\nscene label to one of six indoor labels from the MIT SUN Database [39]: shop-\nping and dining, workplace (office building, factory, lab, etc.), home or hotel,\ntransportation (vehicle interiors, stations, etc.), sports and leisure, and cultural\n(art, education, religion, military, law, politics, etc.).\nDiffusion Model. We fine-tuned a Stable Diffusion [30] model for 1.34 million\niterations with a batch size of 64 on COCO image-caption pairs at a 256x256\nresolution and a learning rate of 1.0e-4. We fine-tuned only the U-Net and text-\nencoder of the model.\nConcept Classifier. We fine-tuned a DenseNet-121 [11] pretrained on Ima-\ngeNet [9] to predict the presence of the 80 objects in each image. The model\nwas trained using stochastic gradient descent with momentum minimizing bi-\nnary cross-entropy loss with a learning rate of 1.0e-1, momentum of 0.8, weight\ndecay of 1.0e-4 and a batch size of 128. Early stopping based on validation loss\nwith a patience of 5 was used after at least 8 training epochs. During training,\nimages were reshaped such that their smaller axis was 256 pixels, and then center\ncropped along their longer axis to 256x256. Images were also randomly rotated\nup to 45 degrees, and vertically flipped with probability 0.3. We used ImageNet\nnormalization across all experiments.\nPrimary feature models. We trained target classifiers on a binary task: home\nor hotel or not. We only considered images labelled with one of these two\nscene-level labels. Furthermore, for each of the target classifiers, we subsampled\nthe data such that there was a 1:1 correlation between the presence of a primary\nconcept (e.g., person) and the outcome. We trained 15 models using 15 concepts\nthat were present in more than 5% of the data: person, bottle, cup, bowl, chair,\ncouch, bed, dining table, tv, laptop, remote, cell, phone, oven, sink, and book.\nModels were trained with momentum 0.8, weight decay 1.0e-4, and learning rate\nof 1.0e-1. The best model was chosen as the epoch with the lowest validation\nloss. During training, images were reshaped such that their smaller axis was 256\npixels, and then center cropped along their longer axis to 256x256. Images were"}, {"title": "10 MIMIC-CXR", "content": "10.1 Experiments\nDataset. MIMIC-CXR [14, 16] consists of 242,479 frontal chest X-rays with\ncorresponding radiology reports. We split the data into 193706/24549/24224\nimages for training, validation, and test sets. To construct a final caption for each\nimage, we extracted demographic information corresponding to the patients'\nbody mass index (BMI), age, and sex at the time the chest X-ray was taken,\nand prepended these information to the radiology report corresponding to the\nchest X-ray. We subsampled the data for downstream tasks where we injected a\n1:1 correlation between pneumonia and each primary features: bmi, age, or sex.\nDiffusion Model. A diffusion model initialized on Stable Diffusion [30] with\nthe text encoder replaced with publicly available clinical BERT embeddings [3]\nwas fine tuned on chest X-ray/radiology report pairs for 295569 iterations on\na batch size of 16 at a 256x256 resolution with a learning rate of 1.0e-4. We\nfine-tuned only the U-Net and text-encoder of the model.\nTarget Models. We trained target classifiers to predict the presence of pneu-\nmonia. We trained the classifier on top of the concept classifier. During training,\nimages were reshaped such that their smaller axis was 256 pixels, and then ran-\ndomly cropped along their longer axis to 256x256. Images were also randomly\nrotated up to 15 degrees. We used ImageNet normalization across all experi-\nments.\nConcept Classifier. We fine-tuned a DenseNet-121 [11] pretrained on Ima-\ngeNet [9] to learn the presence of radiological findings and the three permutable\nconcepts: bmi, age, sex, enlarged cardiomediastinum, cardiomegaly, lung opacity,\nlung lesion, edema, consolidation, atelectasis, pneumothorax, pleural effusion,\npleural other, fracture, and support devices. The model was trained for three\nepochs using stochastic gradient descent with momentum minimizing binary\ncross-entropy loss with a learning rate of 1.0e-4, momentum of 0.8 and a batch\nsize of 32. During training, images were reshaped such that their smaller axis\nwas 256 pixels, and then randomly cropped along their longer axis to 256x256.\nImages were also randomly rotated up to 15 degrees. We used ImageNet nor-\nmalization across all experiments."}]}