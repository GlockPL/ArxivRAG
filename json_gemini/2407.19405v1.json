{"title": "Logic Distillation: Learning from Code Function by Function for Planning and Decision-making", "authors": ["Dong Chen", "Shilin Zhang", "Fei Gao", "Yueting Zhuang", "Siliang Tang", "Qidong Liu", "Mingliang Xu"], "abstract": "Large language models (LLMs) have garnered increasing attention owing to their powerful logical reasoning capabilities. Generally, larger LLMs (L-LLMs) that require paid interfaces exhibit significantly superior performance compared to smaller LLMs (S-LLMs) that can be deployed on a variety of devices. Knowledge distillation (KD) aims to empower S-LLMs with the capabilities of L-LLMs, while S-LLMs merely mimic the outputs of L-LLMs, failing to get the powerful logical reasoning capabilities. Consequently, S-LLMs are helpless when it comes to planning and decision-making tasks that require logical reasoning capabilities. To tackle the identified challenges, we propose a novel framework called Logic Distillation (LD). Initially, LD employs L-LLMs to instantiate complex instructions into discrete functions and illustrates their usage to establish a function base. Subsequently, based on the function base, LD fine-tunes S-LLMs to learn the logic employed by L-LLMs in planning and decision-making. During testing, LD utilizes a retriever to identify the top-K relevant functions based on instructions and current states, which will be selected and invoked by S-LLMs. Ultimately, S-LLMs yield planning and decision-making outcomes, function by function. Relevant experiments demonstrate that with the assistance of LD, S-LLMs can achieve outstanding results in planning and decision-making tasks, comparable to, or even surpassing, those of L-LLMs.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) like GPT-3.5 (Ouyang et al. 2022) and GLM-4 (GLM et al. 2024) have been extensively applied owing to their robust capabilities in logical reasoning. Particularly, LLMs demonstrate superior performance in autonomous embodied agents, showcasing advanced planning and decision-making capabilities grounded in logical reasoning (Xi et al. 2023).\nDespite the remarkable capabilities of LLMs such as GPT-3.5 and GLM-4, their substantial computational requirements render them impractical for deployment on most devices (Chen et al. 2024b,a). On the other hand, numerous companies have attempted to develop relatively smaller open-source LLMs, including GLM-4-9B (GLM et al. 2024) and LLaMA-7B (Touvron et al. 2023), which are compatible with consumer-grade GPUs like the RTX 4090 Ti. In this paper, we refer to LLMs that require invocation through a paid interface as Larger-LLMs (L-LLMs), in contrast to Smaller-LLMs (S-LLMs) deployable on consumer-grade GPUs. Generally, L-LLMs exhibit significantly superior performance across various domains, particularly in logical reasoning. Nonetheless, S-LLMs have garnered extensive attention owing to their convenient deployment and cost-free nature. Consequently, an increasing number of researchers are focusing on Knowledge Distillation (KD) of LLMs (Gou et al. 2021), where L-LLMs act as teachers imparting knowledge, while S-LLMs serve as students, mimicking the outputs of teachers (Xu et al. 2024).\nWhile KD has been demonstrated to effectively enhance the capabilities of S-LLMs in numerous tasks, such as Natural Language Understanding (Dai et al. 2023; Gilardi, Al-izadeh, and Kubli 2023), endowing S-LLMs with the planning and decision-making capabilities of L-LLMs through KD remains a challenge. We present the planning and decision-making outcome of a single step in a pursuit game, where LLMs control three blue dots in pursuit of an orange dot. Based on the aforementioned analysis, we summarize the limitations of S-LLMs with KD as follows: 1. Ineffectiveness in following complex instructions: Despite extensive fine-tuning, S-LLMs continue to struggle with understanding intricate rules in planning and decision-making tasks. 2. Failure to comprehend the logic of L-LLMs: Fine-tuned S-LLMs simply mimic the outputs of L-LLMs and lose their planning and decision-making capabilities when encountering unknown scenarios.\nRecently, there has been considerable work aimed at establishing the translation between natural language and code, where a sentence is a logical code line (Feng et al. 2020; Chen et al. 2021). Drawing inspiration from this, we suggest breaking down the planning and decision-making logic of L-LLMs into multiple stages, with each stage being represented by a specific code function. Subsequently, S-LLMs engage in planning and decision-making by learning and applying the pertinent functions. More specifically, we propose Logic Distillation (LD). First, we leverage the powerful comprehension and logical reasoning capabilities of L-LLMs to decompose the rules governing planning and decision-making tasks into multiple stages, forming different functions. Subsequently, we integrate these functions along with their comments, usage examples, etc., into a function base, and use it to fine-tune S-LLMs to enhance their ability to invoke relevant functions. Besides, to alleviate the issue of S-LLMs struggling to follow complex instructions, we propose transforming generation into selection. During each stage of planning and decision-making, LD employs a retriever to provide top-K relevant functions based on the current state and instructions, enabling S-LLMS to select and execute functions from this set.\nAs depicted in Figure 2, both KD and LD require guidance from a teacher with superior capabilities. KD emphasizes the imparting of content, which involves students mimicking the teacher's output. In contrast, the proposed LD concentrates on the underlying logic of task execution. The teacher decomposes a complex task into multiple basic logics, represented by functions, enabling the student to plan and make decision function by function.\nThe main contributions of this paper can be summarized as follows:\n\u2022 We analyzed the issues of Knowledge Distillation in the context of planning and decision-making in the LLMs era.\n\u2022 We propose Logic Distillation to enable S-LLMs to accomplish complex planning and decision-making tasks akin to L-LLMs.\n\u2022 We conducted experiments in different simulation scenarios to validate the effectiveness of the proposed method."}, {"title": "Methodology", "content": "This paper investigates interactive planning and decision-making tasks, where interactions can be divided into multiple steps (each interaction counts as one step), and each step can be further divided into multiple stages (several stages collectively complete one planning and decision-making process).\nAs illustrated in Figure 3, we explore the Logic Distillation (LD) to empower S-LLMs to engage in planning and decision-making akin to L-LLMs. LD first decomposes rules (instructions) x into multiple stages with L-LLMs and converts the execution logic into the corresponding code functions (such as calculating the distance between points). Then, LD will construct a function base containing functions and user manual of these functions. Besides, LD employs a retriever to find the top-K functions that most relevant to the current stage and x. Subsequently, S-LLMs will select functions f one by one to plan and make decisions. Overall, in this context, LD consists of three components: (i) L-LLMs, (ii) retriever, (iii) S-LLMs.\nL-LLMS\nL-LLMs exhibit exceptional capabilities in planning and decision-making. To distill these capabilities into the S-LLMs, we propose instantiating the reasoning logic of L-LLMs through functions:\n$P_{\\theta_L}(f, u|x) = [P_{\\theta_L}(y_i|x, y_{1:i-1}), f, u = \\prod_i y_i] $  (1)\nwhere L-LLMs $P_{\\theta_L}$ parametrized by $\u03b8_L$ that generates a current token yi based on a context of the previous i \u2013 1 tokens, multiple y constitute a function f and user manual (includes rule explanations, code comments, corresponding invocation stages, and so on). In addition, a collection of f and u forms the function base $D_f$.\nRetriever\nFor the retriever $P_{\\theta_R}(f|x)$, we have proposed two solutions tailored for function base of different scales. When the scale of the function base is large, we follow prior work (Lewis et al. 2020) to implement retrieval component based on DPR (Karpukhin et al. 2020), and retriever $P_{\\theta_R}(f|x)$ follows:\n$P_{\\theta_R}(f|x) \\propto exp(d(f), q(x)) \\rightarrow f_{candidates} = [f_1, ..., f_k]$ (2)\nwhere d(f) is a dense representation of functions (including code comments and rule descriptions), and q(x) is a query representation. Retriever $P_{\\theta_R}(f|x)$ will return a top-K list, where the K functions with highest prior probability. As for small-scale function base, we will fine-tune S-LLMs so that S-LLMs can directly select and utilize the appropriate functions from base $D_f$ to plan and make decisions.\nS-LLMS\nWe fine-tune S-LLMs to enable them to comprehend the functionality and the appropriate invocation timing of different functions. S-LLMs first select a function $f_j$ from $D_f$ or $f_{candidates}$ for stage j:\n$P_{\\theta_s}(f_j|x, u) = max([P_{\\theta_s}(f_1|x, u), ....., P_{\\theta_s}(f_k|x, u)])$ (3)\nThen, function $f_j$ will be executed to obtain the intermediate result of the j-th stage.\n$o_j = f_j(o_{j-1}), o_0 = x$ (4)\nIf there are J stages in a step of the task, then the S-LLMs will select J functions, and the planning and decision-making outputs for that step will be:\n$o = o_J = f(x, o_{J-1})$ (5)\nIf O meets the requirements of the task, the planning and decision-making process will be halted. Otherwise, the O will be regarded as input for the next step. The proposed LD is summarized in Algorithm 1.\nEmergency handling of S-LLMs in LD\nAn advantage of LLMs is their ability to respond to various situations. When using LLMs to control embodied agents, they often can respond to unforeseen circumstances. For instance, when LLMs control unmanned vessels for maritime exploration, they might navigate from the open sea to archipelagos, and LLMs can analyze the specific terrain, enabling swift traversal of the archipelago.\nTypically, emergency situations (such as avoiding whirlpools when controlling unmanned vessels) are simpler compared to the initial various instructions and rules. Therefore, in the LD framework, S-LLMs can be used to transform emergency $X_E$ into functions and add them to the function candidate list $f_{candidates}$:\n$P_{\\theta_s}(f_e, u|x) = [P_{\\theta_s}(y_i|X_E, y_{1:i-1}), f_e, u = [ y_i$ (6)\nThrough Equation 6, S-LLMs will possess stronger general capabilities.\nIt should be noted that, in contrast to KD, which necessitates S-LLMs to memorize massive L-LLMs' outputs, LD merely requires S-LLMs to remember the usage rules of functions. Consequently, LD preserves more general capabilities of LLMs, including function generation."}, {"title": "Why Selection Is Better", "content": "For the aforementioned limitation of S-LLMs, ineffectiveness in following complex instructions, we propose change the function of S-LLMs from generation to selection. Specifically, S-LLMs are required to select the appropriate functions from a provided set, which are to be employed at various stages when confronting a particular problem. This section theoretically analyzes the advantages of selection over generation.\nAssuming that the token list of LLMs contains a total of M types of tokens, the retriever provides K types of functions. For generation, the entropy of the prediction is:\n$H_{generation} = - \\sum_{i=1}^M P_{\\theta_s}(t_i) log P_{\\theta_s}(t_i),$ (7)\n$\\sum_{i=1}^M P_{\\theta_s}(t_i) = 1$\nwhere $t_i$ is a token in the token list.\nWith Lagrange multiplier method, we get:\n$Q(P_{\\theta_s}(t_1), P_{\\theta_s}(t_2), ..., P_{\\theta_s}(), \\lambda) =- \\sum_{i=1}^M P_{\\theta_s}(t_i) log P_{\\theta_s}(t_i) + \\lambda (\\sum_{i=1}^M P_{\\theta_s}(t_i) - 1)$ (8)\nthen partially differentiating Q in Equation 8 with respect to $P_{\\theta_s}(t_i)$ and \u03bb,\n$\\frac{\\partial Q}{\\partial P_{\\theta_s}(t_i)} = - log P_{\\theta_s}(t_i) - 1 + \\lambda$ (9)\n{\\frac{\\partial \\lambda}{\\partial Q} = \\sum_{i=1}^M P_{\\theta_s}(t_i) - 1\nLet Equation 9 be 0, we can get:\n$P_{\\theta_s}(t_1) = P_{\\theta_s}(t_2) = ... = P_{\\theta_s}(t_m) = \\frac{1}{M},$\n$H_{generation} = log M$ (10)\nwhich is the maximum value of $H_{generation}$.\nWhen we perform selection, the number of candidates will be K, and the maximum value of $H_{selection}$ will be log K. As K << M, log K <<log M, the maximum value of $H_{selection}$ will be much smaller than that of $H_{generation}$, and the lower bound of selection will be much higher. Therefore, compared to generation, selection is more effective in maintaining stable outputs for S-LLMs."}, {"title": "Experiments", "content": "Our experiments aim to: (1) verify the effectiveness of LD in planning and decision-making task, (2) compare LD with baselines from both global and local perspectives, and discuss the reasons for the effectiveness of LD (3) verify that LD has a stronger ability to respond to emergencies.\nWe conducted experiments based on the pursuit game to demonstrate that the proposed method can effectively enhance the planning and decision-making capabilities of S-LLMs. More specifically, the pursuit game involves two sides, each controlled by a different LLM. One LLM manages three blue dots, while the other LLM controls one orange dot. Each interaction between the two sides constitutes a step. In each iteration, the blue dots are constrained to move by two units, while the orange dot is restricted to a single unit of movement. The game concludes when the Manhattan distance between all three blue dots and the orange dot is less than 2 units.\nIn our experiments, the L-LLM is GLM-4, and the S-LLM is GLM4-9B-chat. The orange dot is consistently controlled by the L-LLM, while the blue dots are managed by the L-LLM, S-LLM, S-LLM with KD, and S-LLM with LD, respectively. Additional selection and judgment rules have been introduced to improve the success rate of the baselines. For selection, LLMs are provided with the next decision coordinates to choose from. Regarding judgment, if LLMs make more than seven illegal choices, the game is considered a failure. The upper limit for the number of moves in the game is capped at 100. We focus on utilizing LLMs to efficiently manage the blue dots to capture the orange dot.\nTo perform KD, we initialize 221 sets of starting positions randomly and produce 103,355 sets of outputs with the L-LLM. Subsequently, we fine-tune the S-LLM with LoRA (Hu et al. 2021) based on these outputs. For a more details of the pursuit game, please refer to the Appendix.\nFor LD, as depicted in Figure 4, we initially establish a function base with the L-LLM, where L-LLM decomposes the rules and instantiating the planning and decision-making logic into multiple functions. Moreover, to assist the S-LLM in learning how to utilize various functions and decide when to invoke them, L-LLM creates a user manual for these functions. By fine-tuning the S-LLM with the user manual, it can comprehend the logic of the L-LLM and execute planning and decision-making processes function by function."}, {"title": "Better Performance with LD", "content": "The results of the pursuit game are presented in Table 1. It is evident that S-LLM struggles to comprehend complex instructions, as its failures consistently stem from rule violations. Conversely, the \"Failure with Violation\" rate of L-LLM is 0%, demonstrating its superior comprehension and caability to follow instructions. By employing KD to mimic the L-LLM's outputs, the S-LLM's planning and decision-making capabilities significantly improve. Nonetheless, its success rate is notably lower than that of the L-LLM, and the number of steps in successful instances is higher. As for LD, the game's success rate has reached 100%, and the S-LLM completed the game with an average of 0.96 fewer steps than the L-LLM. These results comprehensively demonstrate the effectiveness of LD in enhancing the S-LLM's planning and decision-making capabilities."}, {"title": "Better Performance from a Local Perspective", "content": "We present the one-step planning and decision-making outcomes of different LLMs in Figure 5 (for the purpose of illustration, we have selected four scenarios where both the horizontal and vertical coordinates are within five units).\nFigure 5(a) illustrates that while KD improves the alignment of S-LLM's outputs with the rules, S-LLM still faces challenges in grasping the planning and decision-making logic of L-LLM. Consequently, the blue dots, controlled by S-LLM with KD, moves aimlessly, surpassing the maximum number of allowed movement steps, ultimately leading to the game's failure. Figure 5(b) depicts that while S-LLM successfully completes the game, it still makes irrational decisions for the blue dots during specific moves, leading to a significantly higher number of steps compared to L-LLM. Regarding L-LLM and S-LLM with LD, as depicted in Figures 5(c) and 5(d), all LLMs demonstrate proficient control over the various blue dots, even as the blue dots approach the orange dots from different positions. These results indicate that, compared to KD, S-LLM with LD more effectively grasps the planning and decision-making logic of L-LLM."}, {"title": "Better Performance from a Global Perspective", "content": "In Figure 6, we initialize different LLMs from identical starting positions: blue dots at (3,8), (14, 19),, (17, 2), and the orange dot at (20, 18) to enable a global comparison of the overall planning and decision-making capabilities among different LLMs.\nIn Figure 6(a), as S-LLM with KD merely mimics the outputs of L-LLM, blue dots may represent outputs from the L-LLM in different scenarios, resulting in behaviors such as repetitive circling. For instance, the point along the grey trajectory continuously shuttle back and forth, making it impossible to catch up with the orange point. In Figure 6(b), the point controlled by L-LLM appears to backtrack, mainly because of the orange point's continuous back-and-forth movements in an attempt to escape encirclement. Besides, from Figure 6(c), it can be observed that S-LLM with LD enables the blue dots to approach the orange dot in a more direct manner. Contrasting with L-LLM, S-LLM with LD requires fewer steps to successfully capture the target."}, {"title": "Pursuit Game with Emergencies", "content": "In order to assess the capacity of different LLMs to handle emergencies, we introduced a 5 \u00d7 5 restricted area within the game plane.\nIn the more intricate scenario, L-LLM can still grasp the rules through simple textual descriptions and implement effective planning and decision-making, as the success rate achieve 90%. Conversely, S-LLM with KD achieves a success rate of 52%, with failures resulting from rule violations (accounting for 45.5%), indicating an inability of S-LLM with KD to comprehend the new rules. As for S-LLM with LD, the process involves S-LLM initially abstracting the restricted area as a coordinate filtering function. Then, this function will handle the output of filter_valid_moves from Figure 4, producing a list that excludes the coordinates of the restricted area. Subsequently, this list is utilized by select_best_move to generate a suitable coordinate. The success rate of S-LLM with LD exceeds that of L-LLM by 10%, and its average number of steps is approximately 4 steps fewer than the baselines, demonstrating the powerful general capabilities of LD. It should be noted that, compared to KD, LD only uses few function usage examples to fine-tune S-LLM, which enable S-LLM to retain more general capabilities.\nIn Figure 7, we illustrate the comprehensive planning and decision-making processes of L-LLM and S-LLM with LD, which further validates the effectiveness of converting logic into functions."}, {"title": "Conclusion", "content": "LLMs have been widely applied across many different fields. However, larger LLMs (L-LLMs) with powerful capabilities in comprehension, planning, and decision-making are difficult to deploy on the vast majority of devices due to their parameter scale. In contrast to L-LLMs, smaller open-source LLMs (S-LLMs) are easier to deploy but fall significantly short in performance compared to their larger counterparts. To improve the performance of S-LLMs, researchers have proposed various Knowledge Distillation (KD) methods. Nevertheless, KD merely enables S-LLMs to mimic the outputs of L-LLMs, which is insufficient for addressing complex planning and decision-making problems. Thus, we propose Logic Distillation (LD), a method that instantiates the logic of L-LLMs by converting human-provided rules into functions, thereby establishing a function base. Subsequently, through fine-tuning, S-LLMs will comprehend the usage of each function, enabling S-LLMs to plan and make decisions function by function."}]}