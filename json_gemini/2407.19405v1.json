{"title": "Logic Distillation: Learning from Code Function by Function\nfor Planning and Decision-making", "authors": ["Dong Chen", "Shilin Zhang", "Fei Gao", "Yueting Zhuang", "Siliang Tang", "Qidong Liu", "Mingliang Xu"], "abstract": "Large language models (LLMs) have garnered increasing at-\ntention owing to their powerful logical reasoning capabili-\nties. Generally, larger LLMs (L-LLMs) that require paid in-\nterfaces exhibit significantly superior performance compared\nto smaller LLMs (S-LLMs) that can be deployed on a va-\nriety of devices. Knowledge distillation (KD) aims to em-\npower S-LLMs with the capabilities of L-LLMs, while S-\nLLMs merely mimic the outputs of L-LLMs, failing to get\nthe powerful logical reasoning capabilities. Consequently, S-\nLLMs are helpless when it comes to planning and decision-\nmaking tasks that require logical reasoning capabilities. To\ntackle the identified challenges, we propose a novel frame-\nwork called Logic Distillation (LD). Initially, LD employs L-\nLLMs to instantiate complex instructions into discrete func-\ntions and illustrates their usage to establish a function base.\nSubsequently, based on the function base, LD fine-tunes S-\nLLMs to learn the logic employed by L-LLMs in planning\nand decision-making. During testing, LD utilizes a retriever\nto identify the top-K relevant functions based on instructions\nand current states, which will be selected and invoked by\nS-LLMs. Ultimately, S-LLMs yield planning and decision-\nmaking outcomes, function by function. Relevant experi-\nments demonstrate that with the assistance of LD, S-LLMs\ncan achieve outstanding results in planning and decision-\nmaking tasks, comparable to, or even surpassing, those of L-\nLLMs.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) like GPT-3.5 (Ouyang et al.\n2022) and GLM-4 (GLM et al. 2024) have been exten-\nsively applied owing to their robust capabilities in logi-\ncal reasoning. Particularly, LLMs demonstrate superior per-\nformance in autonomous embodied agents, showcasing ad-\nvanced planning and decision-making capabilities grounded\nin logical reasoning (Xi et al. 2023).\nDespite the remarkable capabilities of LLMs such as\nGPT-3.5 and GLM-4, their substantial computational re-\nquirements render them impractical for deployment on most\ndevices (Chen et al. 2024b,a). On the other hand, numer-\nous companies have attempted to develop relatively smaller\nopen-source LLMs, including GLM-4-9B (GLM et al. 2024)\nand LLaMA-7B (Touvron et al. 2023), which are compati-\nble with consumer-grade GPUs like the RTX 4090 Ti. In\nthis paper, we refer to LLMs that require invocation through\na paid interface as Larger-LLMs (L-LLMs), in contrast\nto Smaller-LLMs (S-LLMs) deployable on consumer-grade\nGPUs. Generally, L-LLMs exhibit significantly superior per-\nformance across various domains, particularly in logical rea-\nsoning. Nonetheless, S-LLMs have garnered extensive at-\ntention owing to their convenient deployment and cost-free\nnature. Consequently, an increasing number of researchers\nare focusing on Knowledge Distillation (KD) of LLMs (Gou\net al. 2021), where L-LLMs act as teachers imparting knowl-\nedge, while S-LLMs serve as students, mimicking the out-\nputs of teachers (Xu et al. 2024).\nWhile KD has been demonstrated to effectively enhance\nthe capabilities of S-LLMs in numerous tasks, such as Nat-"}, {"title": "Related Work", "content": "Large language models (LLMs) (Chowdhery et al. 2022;\nThoppilan et al. 2022; Zhao et al. 2023; Koundinya Gun-\ndavarapu et al. 2024) are trained on broad data and can be\neasily adapted to a wide range of tasks (Bommasani et al.\n2021), which have been applied to education (Biswas 2023;\nKasneci et al. 2023), healthcare (Thirunavukarasu et al.\n2023; Peng et al. 2023; Guo et al. 2024), finance (Wu et al.\n2023), etc. However, LLMs with impressive capabilities of-\nten suffer from size limitations, making them impractical to\nrun on various devices and costly for invoking their inter-\nfaces (Chen et al. 2024b). In this paper, we refer to such\nLLMs as larger LLMs (L-LLMs). In contrast, we refer to\nLLMs with a smaller parameter scale that can be deployed\non various devices as smaller LLMs (S-LLMs). Generally,"}, {"title": "Methodology", "content": "This paper investigates interactive planning and decision-\nmaking tasks, where interactions can be divided into multi-\nple steps (each interaction counts as one step), and each step\ncan be further divided into multiple stages (several stages\ncollectively complete one planning and decision-making\nprocess).\nAs illustrated in Figure 3, we explore the Logic Dis-"}, {"title": "L-LLMS", "content": "L-LLMs exhibit exceptional capabilities in planning and\ndecision-making. To distill these capabilities into the S-\nLLMs, we propose instantiating the reasoning logic of L-\nLLMs through functions:\n$\\P_{\\theta_{1}}(f, u|x) = [P_{\\theta_{L}}(y_{i}|x, y_{1:i-1}), f, u = \\prod_{i} y_{i}$   (1)\nwhere L-LLMs $P_{\\theta_{1}}$ parametrized by $\\theta_{1}$ that generates a cur-\nrent token $y_{i}$ based on a context of the previous i \u2013 1 tokens,\nmultiple y constitute a function f and user manual (includes\nrule explanations, code comments, corresponding invocation\nstages, and so on). In addition, a collection of f and u forms\nthe function base $D_{f}$."}, {"title": "Retriever", "content": "For the retriever $P_{\\theta_{R}}(f|x)$, we have proposed two solutions\ntailored for function base of different scales. When the scale"}, {"title": "S-LLMS", "content": "We fine-tune S-LLMs to enable them to comprehend the\nfunctionality and the appropriate invocation timing of dif-\nferent functions. S-LLMs first select a function $f_{j}$ from $D_{f}$\nor $f_{candidates}$ for stage j:\n$P_{\\theta_{S}}(f_{j}|x, u) = max([P_{\\theta_{S}} (f_{1}|x, u),\u2026\u2026,P_{\\theta_{S}}(f_{k}|x,u)])$    (3)\nThen, function $f_{j}$ will be executed to obtain the interme-\ndiate result of the j-th stage.\n$\\o_{j} = f_{j}(o_{j-1}), o_{0} = x$  (4)\nIf there are J stages in a step of the task, then the S-LLMs\nwill select J functions, and the planning and decision-\nmaking outputs for that step will be:\n$O = o_{J} = f(x, O_{J-1})$   (5)\nIf O meets the requirements of the task, the planning and\ndecision-making process will be halted. Otherwise, the O\nwill be regarded as input for the next step. The proposed LD\nis summarized in Algorithm 1."}, {"title": "Emergency handling of S-LLMs in LD", "content": "An advantage of LLMs is their ability to respond to various\nsituations. When using LLMs to control embodied agents,\nthey often can respond to unforeseen circumstances. For\ninstance, when LLMs control unmanned vessels for mar-\nitime exploration, they might navigate from the open sea to\narchipelagos, and LLMs can analyze the specific terrain, en-\nabling swift traversal of the archipelago.\nTypically, emergency situations (such as avoiding\nwhirlpools when controlling unmanned vessels) are simpler\ncompared to the initial various instructions and rules. There-\nfore, in the LD framework, S-LLMs can be used to transform\nemergency $x_{E}$ into functions and add them to the function\ncandidate list $f_{candidates}$:\n$P_{\\theta_{S}} (f_{E}, u|x) = [P_{\\theta_{S}} (y_{i}|x_{E}, y_{1:i-1}), f_{E}, u = [ y_{i}$   (6)\nThrough Equation 6, S-LLMs will possess stronger general\ncapabilities.\nIt should be noted that, in contrast to KD, which necessi-\ntates S-LLMs to memorize massive L-LLMs' outputs, LD\nmerely requires S-LLMs to remember the usage rules of\nfunctions. Consequently, LD preserves more general capa-\nbilities of LLMs, including function generation."}, {"title": "Why Selection Is Better", "content": "For the aforementioned limitation of S-LLMs, ineffective-\nness in following complex instructions, we propose change\nthe function of S-LLMs from generation to selection. Specif-\nically, S-LLMs are required to select the appropriate func-\ntions from a provided set, which are to be employed at vari-\nous stages when confronting a particular problem. This sec-\ntion theoretically analyzes the advantages of selection over\ngeneration.\nAssuming that the token list of LLMs contains a total of\nM types of tokens, the retriever provides K types of func-\ntions. For generation, the entropy of the prediction is:\n$H_{generation} = \\sum_{i=1}^{M} P_{\\theta_{S}} (t_{i}) log P_{\\theta_{S}} (t_{i})$  (7)\n$\\sum_{i=1}^{M} P_{\\theta_{S}} (t_{i}) = 1$\nwhere $t_{i}$ is a token in the token list.\nWith Lagrange multiplier method, we get:\n$Q(P_{\\theta_{S}} (t_{1}), P_{\\theta_{S}} (t_{2}),..., P_{\\theta_{S}} (t_{M}), \\lambda)$  =\n$\\sum_{i=1}^{M} P_{\\theta_{S}} (t_{i}) log P_{\\theta_{S}} (t_{i}) + \\lambda (\\sum_{i=1}^{M} P_{\\theta_{S}} (t_{i}) - 1)$  (8)\nthen partially differentiating Q in Equation 8 with respect to"}, {"title": "Experiments", "content": "Our experiments aim to: (1) verify the effectiveness of LD\nin planning and decision-making task, (2) compare LD with\nbaselines from both global and local perspectives, and dis-\ncuss the reasons for the effectiveness of LD (3) verify that\nLD has a stronger ability to respond to emergencies\u00b9."}, {"title": "Better Performance with LD", "content": "The results of the pursuit game are presented in Table 1.\nIt is evident that S-LLM struggles to comprehend complex\ninstructions, as its failures consistently stem from rule vio-\nlations. Conversely, the \"Failure with Violation\" rate of L-\nLLM is 0%, demonstrating its superior comprehension and\ncaability to follow instructions. By employing KD to mimic\nthe L-LLM's outputs, the S-LLM's planning and decision-\nmaking capabilities significantly improve. Nonetheless, its\nsuccess rate is notably lower than that of the L-LLM, and the\nnumber of steps in successful instances is higher. As for LD,\nthe game's success rate has reached 100%, and the S-LLM\ncompleted the game with an average of 0.96 fewer steps than\nthe L-LLM. These results comprehensively demonstrate the\neffectiveness of LD in enhancing the S-LLM's planning and\ndecision-making capabilities."}, {"title": "Better Performance from a Local Perspective", "content": "We present the one-step planning and decision-making out-\ncomes of different LLMs in Figure 5 (for the purpose of il-\nlustration, we have selected four scenarios where both the\nhorizontal and vertical coordinates are within five units).\nFigure 5(a) illustrates that while KD improves the align-\nment of S-LLM's outputs with the rules, S-LLM still faces\nchallenges in grasping the planning and decision-making"}, {"title": "Better Performance from a Global Perspective", "content": "In Figure 6, we initialize different LLMs from identical start-\n        ing positions: blue dots at (3,8), (14, 19),, (17, 2), and the\n        orange dot at (20, 18) to enable a global comparison of the\n        overall planning and decision-making capabilities among\n        different LLMs.\nIn Figure 6(a), as S-LLM with KD merely mimics the\n        outputs of L-LLM, blue dots may represent outputs from\n        the L-LLM in different scenarios, resulting in behaviors\n        such as repetitive circling. For instance, the point along the\n        grey trajectory continuously shuttle back and forth, making\n        it impossible to catch up with the orange point. In Figure\n        6(b), the point controlled by L-LLM appears to backtrack,\n        mainly because of the orange point's continuous back-and-\n        forth movements in an attempt to escape encirclement. Be-\n        sides, from Figure 6(c), it can be observed that S-LLM with\n        LD enables the blue dots to approach the orange dot in a\n        more direct manner. Contrasting with L-LLM, S-LLM with\n        LD requires fewer steps to successfully capture the target."}, {"title": "Pursuit Game with Emergencies", "content": "In order to assess the capacity of different LLMs to handle\nemergencies, we introduced a 5 \u00d7 5 restricted area within the\ngame plane. Related results are presented in Table 2.\nIn the more intricate scenario, L-LLM can still grasp the\nrules through simple textual descriptions and implement ef-\nfective planning and decision-making, as the success rate\nachieve 90%. Conversely, S-LLM with KD achieves a suc-\ncess rate of 52%, with failures resulting from rule viola-\ntions (accounting for 45.5%), indicating an inability of S-\nLLM with KD to comprehend the new rules. As for S-LLM\nwith LD, the process involves S-LLM initially abstracting\nthe restricted area as a coordinate filtering function. Then,\nthis function will handle the output of filter_valid_moves\nfrom Figure 4, producing a list that excludes the coordi-\nnates of the restricted area. Subsequently, this list is utilized\nby select_best_move to generate a suitable coordinate. The\nsuccess rate of S-LLM with LD exceeds that of L-LLM by\n10%, and its average number of steps is approximately 4\nsteps fewer than the baselines, demonstrating the powerful\ngeneral capabilities of LD. It should be noted that, compared\nto KD, LD only uses few function usage examples to fine-\ntune S-LLM, which enable S-LLM to retain more general\ncapabilities. More discussion please refer to the Appendix.\nIn Figure 7, we illustrate the comprehensive planning and\ndecision-making processes of L-LLM and S-LLM with LD,\nwhich further validates the effectiveness of converting logic\ninto functions."}, {"title": "Conclusion", "content": "LLMs have been widely applied across many different\nfields. However, larger LLMs (L-LLMs) with powerful ca-\npabilities in comprehension, planning, and decision-making\nare difficult to deploy on the vast majority of devices due\nto their parameter scale. In contrast to L-LLMs, smaller\nopen-source LLMs (S-LLMs) are easier to deploy but fall\nsignificantly short in performance compared to their larger\ncounterparts. To improve the performance of S-LLMs, re-\nsearchers have proposed various Knowledge Distillation\n(KD) methods. Nevertheless, KD merely enables S-LLMs\nto mimic the outputs of L-LLMs, which is insufficient for\naddressing complex planning and decision-making prob-\nlems. Thus, we propose Logic Distillation (LD), a method\nthat instantiates the logic of L-LLMs by converting human-\nprovided rules into functions, thereby establishing a func-\ntion base. Subsequently, through fine-tuning, S-LLMs will\ncomprehend the usage of each function, enabling S-LLMs\nto plan and make decisions function by function."}]}