{"title": "Meta-Learn Unimodal Signals with Weak\nSupervision for Multimodal Sentiment Analysis", "authors": ["Sijie Mai", "Yu Zhao", "Ying Zeng", "Jianhua Yao", "Haifeng Hu"], "abstract": "Multimodal sentiment analysis aims to effectively integrate information from various sources to infer sentiment, where in\nmany cases there are no annotations for unimodal labels. Therefore, most works rely on multimodal labels for training. However, there\nexists the noisy label problem for the learning of unimodal signals as multimodal annotations are not always the ideal substitutes for the\nunimodal ones, failing to achieve finer optimization for individual modalities. In this paper, we explore the learning of unimodal labels\nunder the weak supervision from the annotated multimodal labels. Specifically, we propose a novel meta uni-label generation (MUG)\nframework to address the above problem, which leverages the available multimodal labels to learn the corresponding unimodal labels\nby the meta uni-label correction network (MUCN). We first design a contrastive-based projection module to bridge the gap between\nunimodal and multimodal representations, so as to use multimodal annotations to guide the learning of MUCN. Afterwards, we propose\nunimodal and multimodal denoising tasks to train MUCN with explicit supervision via a bi-level optimization strategy. We then jointly\ntrain unimodal and multimodal learning tasks to extract discriminative unimodal features for multimodal inference. Experimental results\nsuggest that MUG outperforms competitive baselines and can learn accurate unimodal labels.", "sections": [{"title": "1 INTRODUCTION", "content": "WITH the swift advancement of technology and the\nproliferation of social media platforms, multimodal\ndata have become increasingly prevalent for the execu-\ntion of a multitude of downstream applications, including\nmultimodal language analysis [1], multi-omics integrative\nanalysis [2], human action recognition [3], etc. Multimodal\nsentiment analysis (MSA) [4], [5], which seeks to extract\nhuman sentiments and viewpoints from language, acoustic,\nand visual data streams, has recently garnered considerable\ninterest. Researchers endeavor to devise effective models en-\ncompassing variations of recurrent neural networks (RNNs)\n[6], [7], [8], [9], attention-based networks [10], [11], [12],\n[13], [14], BERT-based models [1], [15] and so on to capture\nsufficient interactions between modalities and learn joint\nembedding in a common manifold. Predominantly, these\nmethodologies concentrate on learning rich multimodal\nrepresentations to uncover sophisticated cross-modal dy-\nnamics, and they demonstrate promising results in MSA.\nHowever, due to the subtle and complex expression of\nhuman beings, individual modalities may convey distinct\naspects of sentiment, leading to the label inconsistency\nproblem between multiple modalities and hindering the\nperformance of the model [16], [17]. In typical multimodal\nrepresentation learning tasks, annotations of unimodal sig-\nnals are often unavailable. As shown in Fig. 1, existing\nalgorithms mostly rely on the multimodal learning loss\nto update the whole model, which might not be able to\nextract discriminative and meaningful unimodal features\nfor better learning of downstream tasks. A few works try\nto define unimodal labels as the corresponding global label\nof the multimodal sample, and conduct various learning\ntasks based on the defined unimodal labels [18]. Neverthe-\nless, the global multimodal label might not be a suitable\nsubstitute of unimodal labels under some circumstances\nbecause unimodal representations fail to convey the global\ninformation of a multimodal sample and only reveal a\nspecific perspective of the sample. Some prior works [19],\n[20] try to calculate unimodal labels based on the distance\nbetween the corresponding unimodal representations and\nthe positive/negative centres of multimodal representations\nin a non-parametric way. However, these methods are\nnot learnable and have limited expressive power, and the\nquality of the calculated unimodal labels is hard to estimate.\nIn this paper, we present a novel multimodal learning"}, {"title": "2 RELATED WORK", "content": "MSA has garnered considerable interest due to its ability\nto decipher human language and extract sentiments from\nspoken words, acoustic signals, and visual cues. A plethora\nof existing studies has concentrated on devising fusion tech-\nniques aimed at learning rich and informative multimodal\nembeddings [4], [25], [26]. Specifically, methods that are\nbased on tensor fusion [27], [28], [29] have drawn consider-\nable attention, as they are capable of learning a unified mul-\ntimodal representation endowed with high expressiveness.\nTo highlight important interactions, numerous scholars\npropose cross-modal attention mechanisms [8], [11], [12],\n[30], [31]. Following the groundbreaking success of BERT\n[32], there has emerged a trend toward fine-tuning large pre-\ntrained Transformer models using multimodal datasets [1],\n[15]. For instance, All-modalities-in-One BERT (AOBERT)\n[33] introduces multimodal masked language modeling and\nalignment prediction tasks, which serve to further pre-\ntrain BERT, thereby deeply mining the correlations within\nand between modalities. Moreover, certain methodologies\nemploy tools such as Kullback-Leibler divergence (KL-\ndivergence) and canonical correlation analysis to regularize\nthe learning process of unimodal distributions, thereby\nmitigating the distributional disparities among different\nmodalities [34], [35], [36]. For instance, Sun et al. [37] apply\nKL-divergence and propose a meta-learning algorithm to\nlearn favorable unimodal representations for multimodal\ntasks. But they still use multimodal loss to meta-learn\nunimodal representations, whose objective and algorithm\nare totally different from our meta-learning strategy.\nRecently, self-supervised and weakly-supervised learn-\ning on multimodal data has attracted significant atten-\ntion [38], [39], [40], [41]. For instance, Chen et al. [42]\npropose a weakly-supervised algorithm to learn convolu-\ntional neural networks using cheaply available emotion\nlabels that contain noise. They use a probabilistic graphical\nmodel to simultaneously learn discriminative multimodal\ndescriptors and infer the confidence of label noise, such\nthat the label noise can be filtered out. Different from\nthem, we focus on learning unavailable unimodal labels\nin a weakly-supervised manner. Furthermore, modality\ncorrelation learning, which uses a predictor to predict\nthe correlation score of two or multiple modalities, is an\neffective strategy of weakly-supervised learning [18], [43].\nHowever, this line of research simply defines each unimodal\nlabel as the corresponding multimodal label to conduct\nweakly-supervised learning, while our method leverages\nweakly-supervised learning to learn accurate unimodal\nlabels based on the available multimodal labels. Another"}, {"title": "2.1 Multimodal Sentiment Analysis", "content": "MSA has garnered considerable interest due to its ability\nto decipher human language and extract sentiments from\nspoken words, acoustic signals, and visual cues. A plethora\nof existing studies has concentrated on devising fusion tech-\nniques aimed at learning rich and informative multimodal\nembeddings [4], [25], [26]. Specifically, methods that are\nbased on tensor fusion [27], [28], [29] have drawn consider-\nable attention, as they are capable of learning a unified mul-\ntimodal representation endowed with high expressiveness.\nTo highlight important interactions, numerous scholars\npropose cross-modal attention mechanisms [8], [11], [12],\n[30], [31]. Following the groundbreaking success of BERT\n[32], there has emerged a trend toward fine-tuning large pre-\ntrained Transformer models using multimodal datasets [1],\n[15]. For instance, All-modalities-in-One BERT (AOBERT)\n[33] introduces multimodal masked language modeling and\nalignment prediction tasks, which serve to further pre-\ntrain BERT, thereby deeply mining the correlations within\nand between modalities. Moreover, certain methodologies\nemploy tools such as Kullback-Leibler divergence (KL-\ndivergence) and canonical correlation analysis to regularize\nthe learning process of unimodal distributions, thereby\nmitigating the distributional disparities among different\nmodalities [34], [35], [36]. For instance, Sun et al. [37] apply\nKL-divergence and propose a meta-learning algorithm to\nlearn favorable unimodal representations for multimodal\ntasks. But they still use multimodal loss to meta-learn\nunimodal representations, whose objective and algorithm\nare totally different from our meta-learning strategy.\nRecently, self-supervised and weakly-supervised learn-\ning on multimodal data has attracted significant atten-\ntion [38], [39], [40], [41]. For instance, Chen et al. [42]\npropose a weakly-supervised algorithm to learn convolu-\ntional neural networks using cheaply available emotion\nlabels that contain noise. They use a probabilistic graphical\nmodel to simultaneously learn discriminative multimodal\ndescriptors and infer the confidence of label noise, such\nthat the label noise can be filtered out. Different from\nthem, we focus on learning unavailable unimodal labels\nin a weakly-supervised manner. Furthermore, modality\ncorrelation learning, which uses a predictor to predict\nthe correlation score of two or multiple modalities, is an\neffective strategy of weakly-supervised learning [18], [43].\nHowever, this line of research simply defines each unimodal\nlabel as the corresponding multimodal label to conduct\nweakly-supervised learning, while our method leverages\nweakly-supervised learning to learn accurate unimodal\nlabels based on the available multimodal labels. Another"}, {"title": "2.2 Label Ambiguity and Inconsistency", "content": "In MSA, the ground-truth labels given by multiple anno-\ntators often disagree for the reason that the annotation of\nsentiments and emotions is a subjective and ambiguous\ntask [17], [49]. Therefore, the annotated labels of some\nsamples might be inaccurate, which is referred to as the\nlabel ambiguity problem. Yannakakis et al. [49] reveal that\nassigning absolute values to emotions and sentiments is\nnot only a noisy task, but also is unsuitable due to their\nsubjective nature. They argue that ordinal labels are more\nsuitable to represent emotions and sentiments, suggesting\nthat assigning relative values to subjective concepts is\nbetter aligned with the underlying representations than\nassigning absolute values. Based on this view, they design\nthe preference learning paradigm for training affective\nmodels using ordinal data to verify the benefits of relative\nannotation in affective computing [50]. In addition, to\ndeal with the situation that the annotated labels are not\nnecessarily accurate, Hirano et al. [17] introduce a weakly-\nsupervised learning algorithm called tri-teaching to identify\nand remove the noisy labels. In contrast, Lotfian et al. [51] re-\nsolve the label ambiguity issue from a different perspective,\nusing curriculum learning to train the model where training\nsamples are gradually presented in increasing level of diffi-\nculty. They assume that the ambiguous samples for humans\nare also ambiguous for machines, and the performance can\nbe improved by prioritizing less ambiguous samples (i.e.,\neasier samples) at the beginning of the training process of\nthe deep neural network. While these methods and our\nalgorithm both aim to handle labelling issues, their focuses\nare different. Different from these methods that handle the\nlabel ambiguity issue of sentiments and emotions from\nvarious perspectives, our method mainly deals with the\nlabel inconsistency problem between different modalities\nwithin each multimodal sample. In other words, our MUG\nfocuses on learning accurate unimodal labels for each\nmodality instead of removing the label noise of multimodal\nsamples. The handling of the label ambiguity problem in\nMSA is left for our future work.\nThe works that address the label inconsistency problem\nbetween various modalities are closely related to our algo-\nrithm. MSA infers sentiment from language, acoustic, and\nvisual modalities. However, due to subtle and nuanced ex-\npression of human beings, different modalities may convey\ndistinct aspects of sentiment, leading to the inconsistency\nbetween various modalities and hindering the performance\nof the model [16], [20]. Wang et al. [16] reveal that significant\nperformance degradation of traditional models and mul-\ntimodal large language models occurs when semantically\nconflicting multimodal data are selected for evaluation.\nWhen handling the missing modality problem, Zeng et al.\n[52] also discover the label inconsistency issue between\nmodalities where the sentiment of a sample may change\nwhen a key modality is absent, resulting in the inaccurate\nprediction results. To resolve this issue, they propose an\nensemble-based missing modality reconstruction network to\nrecover semantic features of the key missing modality, and\nthen check the semantic consistency to determine whether\nthe absent modality is crucial to the overall sentiment polar-\nity [52]. Compared to these works, our algorithm addresses\nthe label inconsistency issue from a different perspective.\nInstead of avoiding the negative effect of label inconsistency,\nwe aim to learn accurate unimodal labels for individual\nmodalities, which can extract more discriminative and\nexpressive unimodal features for multimodal inference."}, {"title": "2.3 Unimodal Label Learning", "content": "Different modalities belonging to the same multimodal sam-\nple might express distinct sentiments. In typical multimodal\nrepresentation learning tasks, annotations of unimodal sig-\nnals are often unavailable. To obtain accurate unimodal la-\nbels for learning more favorable unimodal representations,\nSelf-MM [19] proposes a self-supervised non-parametric\napproach to define unimodal labels and extract expressive\nunimodal features. It first calculates the positive/negative\ncentres of multimodal representations according to multi-\nmodal annotations, and then defines unimodal labels based\non the distance between the corresponding unimodal rep-\nresentations and the positive/negative centres. Moreover,\nSUGRM [20] improves Self-MM [19] by projecting features\nof each modality into a common semantic feature space,\nwhich allows simpler calculation of the unimodal labels and\navoids unstable training. Different from these works, we\npropose a learnable meta-learning framework to generate\nunimodal labels in a weakly-supervised way, which is\nmore expressive and can estimate the quality of learned\nunimodal labels. The proposed framework trains MUCN\nin an effective and stable way via the elaborately de-\nsigned unimodal and multimodal denoising tasks, enabling\nMUCN to generate high-quality unimodal labels. To our\nbest knowledge, we are the first to develop meta-learning\nbased unimodal label learning algorithm in MSA.\nThe learning of unimodal labels can be regraded as\na noisy label learning problem. Actually, the learning of\nnoisy labels has been developed in various research areas\nand shows promising results [22], [23], [24], [53], [54], [55].\nFor example, Zheng et al. [22] propose a classic meta-\nlearning method to learn the label of noisy samples given\nclean samples. However, the training of its label correction\nnetwork (LCN) does not have explicit supervision signals\nbut merely relies on the main task loss of clean samples.\nMoreover, it needs to first update the main net using the\ncorrected labels generated by LCN and then computes\nthe meta-gradients of LCN, where the long bi-level back-\npropagation might weaken the effect of meta-gradients\nfor updating LCN and lead to gradient vanishing. The\nsimultaneous optimization over both main net and label\ncorrection network is difficult to achieve an optimal routine,\nlimiting the representation ability of the network and\naccuracy of corrected labels [23]. In contrast, we elaborately\ndesign unimodal and multimodal denoising tasks to learn\nMUCN with explicit supervision. Moreover, our bi-level\noptimization does not involve the update of other modules,\nwhich is more stable in training."}, {"title": "3 ALGORITHM", "content": "In this section, we describe the proposed MUG in detail,\nwhich aims to learn accurate unimodal labels and then\nextract more discriminative unimodal features for multi-\nmodal fusion. MUG is divided into three stages, namely\npre-training, meta-learning and joint training. The three\nstages and their interrelationships can be summarized as\nfollows: (1) Pre-training Stage: This initial phase serves\ntwo purposes. Firstly, it narrows down the distribution\ngap between unimodal representations and the projected\nmultimodal representations through a contrastive-based\nprojection module. Secondly, it initializes the parameters\nof the framework and cultivates efficient multimodal and\nunimodal representations. The primary objective here is to\nprepare the framework for subsequent meta-learning stage.\n(2) Meta-learning Stage: In the second stage, we leverage\nthe initialized framework and unimodal/multimodal rep-\nresentations to execute meta-learning of unimodal labels.\nThis stage aims to yield proficient unimodal labels for\nthe ensuing multi-task training stage. (3) Multi-task Train-\ning Stage: The third stage involves two facets: unimodal\nlearning tasks and multimodal learning task. Leveraging\nunimodal labels generated in the second stage, we conduct\nunimodal learning tasks. Simultaneously, the framework\naddresses multimodal learning task using the available\nmultimodal labels. This conjoined effort refines unimodal\nand multimodal representations to enhance inferential out-\ncomes. Importantly, the first two stages can be decoupled\nand trained independently, which significantly reduces the\ntraining time. By performing the first two stages only once,\nwe attain the necessary unimodal labels for the joint training\nstage. The diagram of MUG is shown in Fig. 2.\nOur downstream task is MSA, where the model is fed\nan utterance [56] (a segment of video demarcated by a\nsentence). Each utterance encompasses three modalities:\nacoustic (denoted as a), visual (denoted as v), and lan-\nguage (denoted as l). The objective of MSA is to infer a\nsentiment score based on the feature sequences of these\nthree modalities. A conventional multimodal framework\nencompasses the extraction of unimodal representations,\nthe amalgamation of these unimodal representations into a\nmultimodal representation, and the execution of inference\npredicated on the integrated multimodal representation.\nThe summary of notations and acronyms is shown in\nTable 1. In the following subsections, we illustrate the\nprocedures of the proposed MUG in detail."}, {"title": "3.1 Unimodal Networks", "content": "First of all, we illustrate the structure of unimodal networks\nand the procedures to obtain the unimodal representations\nfor the later fusion and the generation of unimodal labels.\nTo make a fair comparison with our closest baseline [19], we\nuse the same structure of unimodal networks as that in Self-\nMM [19]. Following the state-of-the-art algorithms [1], [19],\n[20], [47], [57], BERT [32] is used to extract the high-level\nlanguage representation. Specifically, the procedures of the\nBERT network are shown as below:\n$x_l = BERT(U_l; \\theta_{BERT})$\n$x_l = f(W_{down}x_l + b_{down}) \\in R^{d_l \\times 1}$       (1)\nwhere $U_l$ is the sequence of the input tokens, $x_l$ is the repre-\nsentation of the first token of the BERT output sequence. We\nuse a fully connected layer to further process to project\nthe language representation into a low-dimensional feature\nspace. Notably, for a fair comparison with state-of-the-art\nmethod [58], we also present the results of MUG with T5\n[59] as the language network. For the acoustic and visual\nmodalities, following our baseline [19], we use the LSTM\nnetwork as the unimodal network:\n$x_m = LSTM(U_m; \\theta_m) \\in R^{d_m \\times 1}$        (2)\nwhere $U_m \\in R^{T_m \\times d'_m}$ is the input feature sequence ($T_m$\ndenotes the sequence length, $d'_m$ is the input feature di-\nmensionality, and $m\\in \\{a,v\\}$). The generated unimodal\nrepresentation $x_m$ is used for fusion and learning the\nunimodal labels."}, {"title": "3.2 Pre-training of Multimodal Framework", "content": "In this stage, we train the framework to initialize parameters\nand propose the contrastive-based projection module (CPM)\nto prepare the framework for the meta-learning stage.\nFormally, given three unimodal sequences\n$U_m \\in R^{T_m \\times d'_m} (m \\in \\{l,a,v\\})$, a traditional multimodal\nlearning system can be formulated as:\n$x_m = F_m(U_m; \\theta_m) \\in R^{d_m \\times 1}, m\\in \\{l, a, v\\}$       (3)\n$x = F_M(X_l, X_a, X_v; \\theta_M) \\in R^{d \\times 1}$    (4)\n$\\hat{y} = P_M (x; \\theta_{PM})$(5)\nwhere $\\hat{y}$ is the prediction based on multimodal representa-\ntion $x$, $P_M$ is the multimodal predictor parameterized by\n$\\theta_{PM}$, $F_m$ is the unimodal network parameterized by $\\theta_m$,\nand $F_M$ is the multimodal fusion network parameterized\nby $\\theta_M$. $y^j$ is the ground truth label for multimodal sample j\n(the superscribe j is omitted in most equations for brevity),\n$\\theta \\in \\{\\theta_a, \\theta_v, \\theta_l,\\theta_M \\}$, $\\alpha$ is the learning rate, n is the batch size,\nand $L_M$ is the mean absolute error (MAE). For conciseness,\nthe structures of $F_m$, $F_M$ and $P_M$ are illustrated in Fig. 3.\nIn our framework, to prepare the model for the proposed\nmeta-learning strategy, we first need to train a unimodal\npredictor for each modality that is able to process both\nthe corresponding unimodal representation and multimodal\nembedding, such that we can use the multimodal anno-\ntations to help learn the unimodal labels for unimodal\nrepresentations (a more detailed illustration is provided\nin the next section). To achieve this goal, we design a\nlearnable contrastive-based projection module (CPM) for\neach modality to project the multimodal representation\ninto unimodal embedding space, and use the projected\nmultimodal embedding to train the unimodal predictor.\nThereby, we have the following equations:\n$x'_m = f(W_m x + b_m), m \\in \\{l, a, v\\}$  (7)\n$\\hat{y}_m' = P_m (x'_m; \\theta_{p_m})$ (8)\n$L_m' = \\frac{1}{n} \\sum_{j=1}^{n} | \\hat{y}_{m'}^j - y^j |$  (9)\nwhere $W_m$ and $b_m$ are the learnable parameters of the\nprojection layer in the CPM for modality m, and f is\nthe activation function. In Eq. 8, we use the projected\nmultimodal embedding $x_m'$ to train the unimodal predictor\n$P_m$, such that the unimodal predictor can process multi-\nmodal embedding to guide the learning of meta uni-label\ncorrection networks for individual modalities at the meta-\nlearning stage.\nThe projection layer maps the input multimodal rep-\nresentation into a metric space where it can be directly\ncompared with the corresponding modality in terms of sim-\nilarity. Nevertheless, projection layer alone is not expressive\nenough to reduce the distributional gap between unimodal\nand multimodal embeddings, and contrastive learning [60],\n[61], [62] is thereby used to further achieve this goal. Firstly,\nwe perform L2 normalization on the representations:\n$x \\leftarrow \\frac{x_i}{\\sqrt{\\sum_o (x^i_o)^2}}$    i$\\in \\{1, 2, ...\\}$      (10)\nwhere * represents any one of the possible subscribes or\nsuperscribes, o is the index at the feature dimension, and L2\nnormalization is used to constrain the values of similarity\nbetween representations to be -1 to +1. Then, we use dot\nproduct to measure the similarity between anchor and\npositives/negatives, and calculate the contrastive learning\nloss as follows:\n$L_{om} = - \\frac{1}{n} \\sum_{j=1}^{n} log(\\frac{e^{<x_j, x_m> /\\tau}}{\\sum_{g=1}^{n} e^{<x_j, x_g>/ \\tau}})$   (11)\nwhere $\\tau$ is the temperature parameter. The contrastive\nlearning is used to improve the mutual information between\nunimodal and multimodal representations and reduce the\ngap between them. In this way, we can directly use projected\nmultimodal representations and multimodal labels to guide"}, {"title": "3.3 Meta-Learning of MUCN", "content": "In this section, we learn clean labels of individual modalities\nbased on the weak supervision from multimodal labels via\nthe designed meta-learning strategy. In the meta-training\nstage, we propose the unimodal denoising task and design\nthe meta uni-label correction network (MUCN) to generate\nthe learned unimodal labels, where MUCN is trained\nbased on the loss of unimodal denoising task. In the\nmeta-testing phrase, we design multimodal denoising task\nand use multimodal representations (which have sentiment\nannotations) to evaluate the effectiveness of the updated\nMUCN. If MUCN becomes less discriminative after meta-\ntraining, we compute the meta-gradients of MUCN with\nrespect to the loss of multimodal denoising task, and then\nmeta-update MUCN. In this way, MUCN can be trained\nmore accurately and generate more reasonable unimodal\nlabels for unimodal representations. In contrast to the\nnon-parametric methods [19], [20], the proposed unimodal\nlearning strategy is more expressive and can estimate the\nquality of the defined unimodal labels. And compared to\nother meta-learning strategies [22], [23], [24], our method\nprovides explicit supervision signal for MUCN via the\nproposed unimodal and multimodal denoising tasks, which\nis more stable in training and ensures the effective learning\nof MUCN."}, {"title": "3.3.1 Unimodal Denoising Task", "content": "Firstly, in the meta-training stage, we train MUCN (the\nstructure of MUCN is shown in Fig. 3) by guiding it to learn\nto denoise the manually corrupted multimodal label and\nrecover the original multimodal label y using the unimodal\nrepresentation:\n$y_{mc}=MUCN_m(X_m, y + \\epsilon_m;\\theta_{mc}), \\epsilon_m \\sim N(0, I)$         (13)\n$L_{MUCN} (\\theta_{mc}) = |y - y_{mc} |, j\\in \\{1, 2, ..., n\\}$     (14)\n$\\theta'_{mc} = \\theta_{ma} - \\alpha  \\nabla_{\\theta_{mc}} L_{MUCN} (\\theta_{mc})$         (15)\nwhere the MUCN for modality m ($MUCN_m$) takes as\ninput the unimodal representation and the corrupted mul-\ntimodal label, and outputs a corrected label $y_{mc}$ that is\nexpected to match the original multimodal label y by the\ntraining using gradient descent. Following MAML [21],\nthe updated parameters $\\theta'_{m}$ are computed using one or\nmore gradient descent updates on the unimodal denoising\ntasks. S denotes the task set that contains a batch of\ninput. The Gaussian noise $\\epsilon_m$ prevents $MUCN_m$ to learn\nidentity mapping and provides the model the capability to\nlearn (at least) a sub-optimal unimodal label for the input\nunimodal representation. This training strategy enables a\nmore stable and effective learning of MUCN compared to\ntraining MUCN without explicit supervision [22]. To be\nmore specific, meta label correction (MLC) [22] imposes\nno additional constraints on the label correction of noisy\nsamples during the meta-training stage, but merely relies\non the meta-gradients computed using the clean samples to\nguide the learning of noisy labels via a bi-level optimization\nstrategy. In contrast to MLC, the proposed strategy can at"}, {"title": "3.3.2 Additional Design for Unimodal Denoising Task", "content": "After the halfway of the meta-learning where $MUCN_m$\nbecomes discriminative and can generate meaningful labels,\nwe change the objective of the unimodal denoising task\n(Eq. 14) as:\n$L_{MUCN} (\\theta_{mc}) = |y_{mi} - y_{mc}|$            (16)\nwhere $y_{mi}$ is computed as:\n$y_{mi} = \\lambda  y_m + (1 - \\lambda). y $  (17)\nwhere $y_{m}$ is the corrected unimodal label of sample j\ngenerated by $MUCN_m$ at the previous epoch, and $\\lambda$ is a co-\nefficient which is less than 1. In this way, we can leverage the\ndiscriminative power of $MUCN_m$ to improve the unimodal\ndenoising task and further prevent the $MUCN_m$ from\ndefining unimodal label as the corresponding multimodal\nlabel. Notably, $\\lambda$ decays as the training deepens:\n$\\lambda \\leftarrow (\\lambda_{init})^{\\frac{1}{E+1}}$(18)\nwhere $\\lambda_{init}$ is the initialized value of $\\lambda$, E denotes the\ncurrent epoch number of meta-learning."}, {"title": "3.3.3 Multimodal Denoising Task", "content": "Nevertheless, the training strategy of unimodal denoising\ntask still tends to define unimodal label as the corre-\nsponding multimodal label. Although multimodal label\nprovides abundant prior information to the unimodal label\nand they are equal/similar in some cases, this assumption\ninevitably introduces noise to the training of MUCN as\nelaborated in the Introduction section. To address this issue,\nwe innovatively design the multimodal denoising task to\nleverage clean multimodal labels and representations to\nguide the learning of MUCN. Specifically, we first estimate\nthe effectiveness of $MUCN_m$ by estimating whether it\ncan recover clean multimodal label given the projected\nmultimodal representation and corrupted label:\n$\\hat{y_c}=MUCN_m(X_m', \\tilde{y}; \\theta_{mc})$      (19)\n$L_{MUCN}(\\theta_{mc}) = \\frac{1}{ |D| } \\sum_{j\\in D}| \\tilde{y}^j - \\hat{y}_c^j | $      (20)\nwhere $y$ is the corrupted noise label, and D denotes the\nmultimodal denoising task set. To obtain a more accurate\nestimation of $MUCN_m$ and avoid the bias caused by the\nsame labels of unimodal and multimodal denoising tasks,\nwe additionally randomly sample more data to constitute\nD, i.e., D = SUS' where |S'| = b \u00b7 |S| (b is set to 10 in our\nexperiments). Notably, thanks to the CPM at the first stage,\nthe project multimodal representation and corresponding\nunimodal representation have similar distributions, and\ntherefore it is appropriate to feed them into the same\nMUCN."}, {"title": "3.3.4 Optimization Strategy", "content": "After the training on unimodal denoising task (see Eq. 15),\n$\\theta_{mc}$ is updated to $\\theta'_{mc}$, and we feed the same multimodal\ninput to go through Eqs. 19 and 20 again, which generates\nthe post correction loss $L_{MUCN}(\\theta'_{mc})$. Then, we estimate the\nvalues of $L'_{MUCN}(\\theta_{mc})$ and $L_{MUCN}(\\theta'_{mc})$. If $L'_{MUCN}(\\theta_{mc})$\n< $L_{MUCN}(\\theta_{mc})$ which suggests that $MUCN_m$ becomes\nmore discriminative after meta-training and multimodal\nlabel y is likely to be an excellent estimation of unimodal\nlabel, we simply skip the meta-testing stage and define the\nupdated parameters of $MUCN_m$ as:\n$\\theta_{mc}^{'} \\leftarrow \\theta_{mc}$   (22)\nAnd if $L'_{MUCN}(\\theta_{mc})$ > $L_{MUCN}(\\theta_{mc})$ which suggests that\nthe corresponding multimodal label y is not an excellent\nestimation of unimodal label and the corrected label $\\hat{y_{mc}}$\nis actually a more suitable unimodal label, we use a bi-\nlevel optimization strategy to correct the 'false' update of\n$MUCN_m$ via meta-updating:\n$\\theta_{mc} \\leftarrow \\theta_{mc} - \\alpha'. \\nabla_{\\theta'_{mc}} L_{MUCN}(\\theta_{mc}^{'})$     (23)\nwhere $\\alpha'$ is the learning rate at the meta-testing stage. Note\nthat here we directly take the derivative of the original $\\theta_{mc}$\nwith respect to $L_{MUCN}(\\theta'_{mc})$ (that is why it is a bi-level\noptimization strategy). The bi-level optimization strategy\nis based on the assumption that if the current $MUCN_m$\nis discriminative enough, then it is able to perform well\non multimodal denoising task whose annotated label is\navailable and features are of similar distribution. The meta-\nupdate of $\\theta_{mc}$ encourages $MUCN_m$ to learn to perform well\non label denoising task. Compared to other meta-learning\nbased methods [22], [23], [24], the proposed denoising\ntasks provide explicit supervision for MUCN and our bi-\nlevel optimization does not involve the update of other\nmodules, which enables a more stable and effective training\nof MUCN.\nSince only MUCN is updated at this stage, we do not\ngo through the network again but simply use the saved\nunimodal and multimodal representations generated by the\nnetwork at the first stage, which can greatly accelerate the\ntraining of MUCN."}, {"title": "3.4 Multi-Task Training", "content": "In this stage, we conduct joint training of unimodal and\nmultimodal learning tasks as done in [19"}]}