{"title": "Meta-Learn Unimodal Signals with Weak Supervision for Multimodal Sentiment Analysis", "authors": ["Sijie Mai", "Yu Zhao", "Ying Zeng", "Jianhua Yao", "Haifeng Hu"], "abstract": "Multimodal sentiment analysis aims to effectively integrate information from various sources to infer sentiment, where in many cases there are no annotations for unimodal labels. Therefore, most works rely on multimodal labels for training. However, there exists the noisy label problem for the learning of unimodal signals as multimodal annotations are not always the ideal substitutes for the unimodal ones, failing to achieve finer optimization for individual modalities. In this paper, we explore the learning of unimodal labels under the weak supervision from the annotated multimodal labels. Specifically, we propose a novel meta uni-label generation (MUG) framework to address the above problem, which leverages the available multimodal labels to learn the corresponding unimodal labels by the meta uni-label correction network (MUCN). We first design a contrastive-based projection module to bridge the gap between unimodal and multimodal representations, so as to use multimodal annotations to guide the learning of MUCN. Afterwards, we propose unimodal and multimodal denoising tasks to train MUCN with explicit supervision via a bi-level optimization strategy. We then jointly train unimodal and multimodal learning tasks to extract discriminative unimodal features for multimodal inference. Experimental results suggest that MUG outperforms competitive baselines and can learn accurate unimodal labels.", "sections": [{"title": "1 INTRODUCTION", "content": "WITH the swift advancement of technology and the proliferation of social media platforms, multimodal data have become increasingly prevalent for the execution of a multitude of downstream applications, including multimodal language analysis [1], multi-omics integrative analysis [2], human action recognition [3], etc. Multimodal sentiment analysis (MSA) [4], [5], which seeks to extract human sentiments and viewpoints from language, acoustic, and visual data streams, has recently garnered considerable interest. Researchers endeavor to devise effective models encompassing variations of recurrent neural networks (RNNs) [6], [7], [8], [9], attention-based networks [10], [11], [12], [13], [14], BERT-based models [1], [15] and so on to capture sufficient interactions between modalities and learn joint embedding in a common manifold. Predominantly, these methodologies concentrate on learning rich multimodal representations to uncover sophisticated cross-modal dynamics, and they demonstrate promising results in MSA.\nHowever, due to the subtle and complex expression of human beings, individual modalities may convey distinct aspects of sentiment, leading to the label inconsistency problem between multiple modalities and hindering the performance of the model [16], [17]. In typical multimodal representation learning tasks, annotations of unimodal signals are often unavailable. As shown in Fig. 1, existing algorithms mostly rely on the multimodal learning loss to update the whole model, which might not be able to extract discriminative and meaningful unimodal features for better learning of downstream tasks. A few works try to define unimodal labels as the corresponding global label of the multimodal sample, and conduct various learning tasks based on the defined unimodal labels [18]. Nevertheless, the global multimodal label might not be a suitable substitute of unimodal labels under some circumstances because unimodal representations fail to convey the global information of a multimodal sample and only reveal a specific perspective of the sample. Some prior works [19], [20] try to calculate unimodal labels based on the distance between the corresponding unimodal representations and the positive/negative centres of multimodal representations in a non-parametric way. However, these methods are not learnable and have limited expressive power, and the quality of the calculated unimodal labels is hard to estimate.\nIn this paper, we present a novel multimodal learning framework to excavate the rich information stored in the unimodal signals for multimodal learning. The proposed framework aims to learn unimodal labels under the weak supervision from multimodal annotations, and then jointly trains unimodal and multimodal learning tasks to leverage the available information to the utmost extent. Specifically, we propose a novel meta-learning framework to leverage the available multimodal labels to help learn unimodal labels, which is more expressive and discriminative than using non-parametric approaches to calculate unimodal labels as in previous algorithms of MSA [19], [20]. To the best of our knowledge, we are the first to introduce meta-learning [21] into MSA for learning accurate unimodal labels. Our framework is divided into three stages. In the first stage, we train the whole network with the human-annotated multimodal labels, and design a contrastive-based projection module to maximize the mutual information and narrow down the gap between unimodal and multimodal representations. The unimodal predictors are also trained using the projected multimodal representations to provide discriminative power at this stage. In the second stage, a novel meta-learning strategy is proposed to use multimodal labels to guide the learning of unimodal labels for individual modalities, which can estimate the quality of learned unimodal labels and help to meta-update the meta uni-label correction network (MUCN) via a bi-level optimization strategy. Different from previous works on noisy label learning that merely rely on the loss of clean samples to meta-update the label correction network [22], [23], [24], we elaborately design unimodal and multimodal denoising tasks to train MUCN in a successive manner with explicit supervision signal, enabling the effective learning of MUCN. The proposed bi-level optimization strategy does not involve the update of other modules (e.g., the main net) as in other noisy label learning algorithms based on meta-learning [22], [23], [24], which is stable in training. In the final stage, we jointly train multimodal learning task and unimodal learning tasks using the learned unimodal labels to extract more expressive and discriminative unimodal features for further fusion.\nIn brief, the contributions of this paper can be summarized as:\n\u2022\tWe elaborately devise a three-stage multimodal framework, named meta uni-label generation (MUG), to learn unimodal labels and then jointly train unimodal and multimodal learning tasks. In this way, we can leverage the available multimodal information to the utmost extent and extract more discriminative and expressive unimodal features for the better learning of multimodal tasks.\n\u2022\tWe propose a novel meta-learning algorithm to leverage multimodal labels for learning accurate unimodal labels. In the proposed meta-learning algorithm, we design the contrastive-based projection module to narrow down the gap between unimodal representations and the projected multimodal representations at the first stage. In this way, we can use the available multimodal labels to estimate the quality of learned unimodal labels, and then meta-update MUCN. Furthermore, compared to other meta-learning algorithms [22], [23], [24], we elaborately design unimodal and multimodal denoising tasks to train MUCN with explicit supervision at the meta-learning stage, which enables a more stable learning of MUCN.\n\u2022\tThe proposed MUG outperforms other algorithms on the task of MSA across three datasets. In addition, MUG outperforms other meta-learning based noisy label learning algorithm [22]."}, {"title": "2 RELATED WORK", "content": "2.1 Multimodal Sentiment Analysis\nMSA has garnered considerable interest due to its ability to decipher human language and extract sentiments from spoken words, acoustic signals, and visual cues. A plethora of existing studies has concentrated on devising fusion techniques aimed at learning rich and informative multimodal embeddings [4], [25], [26]. Specifically, methods that are based on tensor fusion [27], [28], [29] have drawn considerable attention, as they are capable of learning a unified multimodal representation endowed with high expressiveness. To highlight important interactions, numerous scholars propose cross-modal attention mechanisms [8], [11], [12], [30], [31]. Following the groundbreaking success of BERT [32], there has emerged a trend toward fine-tuning large pre-trained Transformer models using multimodal datasets [1], [15]. For instance, All-modalities-in-One BERT (AOBERT) [33] introduces multimodal masked language modeling and alignment prediction tasks, which serve to further pre-train BERT, thereby deeply mining the correlations within and between modalities. Moreover, certain methodologies employ tools such as Kullback-Leibler divergence (KL-divergence) and canonical correlation analysis to regularize the learning process of unimodal distributions, thereby mitigating the distributional disparities among different modalities [34], [35], [36]. For instance, Sun et al. [37] apply KL-divergence and propose a meta-learning algorithm to learn favorable unimodal representations for multimodal tasks. But they still use multimodal loss to meta-learn unimodal representations, whose objective and algorithm are totally different from our meta-learning strategy.\nRecently, self-supervised and weakly-supervised learning on multimodal data has attracted significant attention [38], [39], [40], [41]. For instance, Chen et al. [42] propose a weakly-supervised algorithm to learn convolutional neural networks using cheaply available emotion labels that contain noise. They use a probabilistic graphical model to simultaneously learn discriminative multimodal descriptors and infer the confidence of label noise, such that the label noise can be filtered out. Different from them, we focus on learning unavailable unimodal labels in a weakly-supervised manner. Furthermore, modality correlation learning, which uses a predictor to predict the correlation score of two or multiple modalities, is an effective strategy of weakly-supervised learning [18], [43]. However, this line of research simply defines each unimodal label as the corresponding multimodal label to conduct weakly-supervised learning, while our method leverages weakly-supervised learning to learn accurate unimodal labels based on the available multimodal labels. Another popular strategy of self-/weakly-supervised learning is to apply contrastive learning to learn representations from multimodal data [39], [40], [41], [44], [45], [46]. For example, Hybrid Contrastive learning (HyCon) [47] introduces intra-modal and inter-modal contrastive learning strategies that pushes unimodal representations from the same category closer and pushes those from different categories apart, and Contrastive FEature DEcomposition (ConFEDE) [45] simultaneously performs contrastive representation learning and feature decomposition of unimodal representations to enhance the generated multimodal representations. Moreover, Anand et al. [48] propose a label consistency calibration loss to prevent label bias by enabling calibration of the peer-ensembled fusion branches with respect to the difficulty of different emotion labels, and design a multimodal distillation loss to calibrate the fusion network by minimizing the KL-divergence with the modality-specific peer networks. They also conduct self-supervised unimodal-unimodal and unimodal-multimodal contrastive learning to improve the generalization of the model across diverse speaker backgrounds. In contrast, we perform unimodal-multimodal contrastive learning for a different purpose, aiming to narrow down the distribution gap between unimodal features and the projected multimodal representations.\nAlthough achieving promising performance, these methods do not consider the learning of unimodal labels, leading to the sub-optimal performance of the model due to the insufficient learning of unimodal features. Different from them, we propose a novel meta-learning strategy to learn unimodal labels based on the available multimodal labels, and conduct multi-task training of unimodal and multimodal learning tasks to extract favorable unimodal features for a more robust multimodal framework."}, {"title": "2.2 Label Ambiguity and Inconsistency", "content": "In MSA, the ground-truth labels given by multiple annotators often disagree for the reason that the annotation of sentiments and emotions is a subjective and ambiguous task [17], [49]. Therefore, the annotated labels of some samples might be inaccurate, which is referred to as the label ambiguity problem. Yannakakis et al. [49] reveal that assigning absolute values to emotions and sentiments is not only a noisy task, but also is unsuitable due to their subjective nature. They argue that ordinal labels are more suitable to represent emotions and sentiments, suggesting that assigning relative values to subjective concepts is better aligned with the underlying representations than assigning absolute values. Based on this view, they design the preference learning paradigm for training affective models using ordinal data to verify the benefits of relative annotation in affective computing [50]. In addition, to deal with the situation that the annotated labels are not necessarily accurate, Hirano et al. [17] introduce a weakly-supervised learning algorithm called tri-teaching to identify and remove the noisy labels. In contrast, Lotfian et al. [51] resolve the label ambiguity issue from a different perspective, using curriculum learning to train the model where training samples are gradually presented in increasing level of difficulty. They assume that the ambiguous samples for humans are also ambiguous for machines, and the performance can be improved by prioritizing less ambiguous samples (i.e., easier samples) at the beginning of the training process of the deep neural network. While these methods and our algorithm both aim to handle labelling issues, their focuses are different. Different from these methods that handle the label ambiguity issue of sentiments and emotions from various perspectives, our method mainly deals with the label inconsistency problem between different modalities within each multimodal sample. In other words, our MUG focuses on learning accurate unimodal labels for each modality instead of removing the label noise of multimodal samples. The handling of the label ambiguity problem in MSA is left for our future work.\nThe works that address the label inconsistency problem between various modalities are closely related to our algorithm. MSA infers sentiment from language, acoustic, and visual modalities. However, due to subtle and nuanced expression of human beings, different modalities may convey distinct aspects of sentiment, leading to the inconsistency between various modalities and hindering the performance of the model [16], [20]. Wang et al. [16] reveal that significant performance degradation of traditional models and multimodal large language models occurs when semantically conflicting multimodal data are selected for evaluation. When handling the missing modality problem, Zeng et al. [52] also discover the label inconsistency issue between modalities where the sentiment of a sample may change when a key modality is absent, resulting in the inaccurate prediction results. To resolve this issue, they propose an ensemble-based missing modality reconstruction network to recover semantic features of the key missing modality, and then check the semantic consistency to determine whether the absent modality is crucial to the overall sentiment polarity [52]. Compared to these works, our algorithm addresses the label inconsistency issue from a different perspective. Instead of avoiding the negative effect of label inconsistency, we aim to learn accurate unimodal labels for individual modalities, which can extract more discriminative and expressive unimodal features for multimodal inference."}, {"title": "2.3 Unimodal Label Learning", "content": "Different modalities belonging to the same multimodal sample might express distinct sentiments. In typical multimodal representation learning tasks, annotations of unimodal signals are often unavailable. To obtain accurate unimodal labels for learning more favorable unimodal representations, Self-MM [19] proposes a self-supervised non-parametric approach to define unimodal labels and extract expressive unimodal features. It first calculates the positive/negative centres of multimodal representations according to multimodal annotations, and then defines unimodal labels based on the distance between the corresponding unimodal representations and the positive/negative centres. Moreover, SUGRM [20] improves Self-MM [19] by projecting features of each modality into a common semantic feature space, which allows simpler calculation of the unimodal labels and avoids unstable training. Different from these works, we propose a learnable meta-learning framework to generate unimodal labels in a weakly-supervised way, which is more expressive and can estimate the quality of learned unimodal labels. The proposed framework trains MUCN in an effective and stable way via the elaborately designed unimodal and multimodal denoising tasks, enabling MUCN to generate high-quality unimodal labels. To our best knowledge, we are the first to develop meta-learning based unimodal label learning algorithm in MSA.\nThe learning of unimodal labels can be regraded as a noisy label learning problem. Actually, the learning of noisy labels has been developed in various research areas and shows promising results [22], [23], [24], [53], [54], [55]. For example, Zheng et al. [22] propose a classic meta-learning method to learn the label of noisy samples given clean samples. However, the training of its label correction network (LCN) does not have explicit supervision signals but merely relies on the main task loss of clean samples. Moreover, it needs to first update the main net using the corrected labels generated by LCN and then computes the meta-gradients of LCN, where the long bi-level back-propagation might weaken the effect of meta-gradients for updating LCN and lead to gradient vanishing. The simultaneous optimization over both main net and label correction network is difficult to achieve an optimal routine, limiting the representation ability of the network and accuracy of corrected labels [23]. In contrast, we elaborately design unimodal and multimodal denoising tasks to learn MUCN with explicit supervision. Moreover, our bi-level optimization does not involve the update of other modules, which is more stable in training."}, {"title": "3 ALGORITHM", "content": "In this section, we describe the proposed MUG in detail, which aims to learn accurate unimodal labels and then extract more discriminative unimodal features for multimodal fusion. MUG is divided into three stages, namely pre-training, meta-learning and joint training. The three stages and their interrelationships can be summarized as follows: (1) Pre-training Stage: This initial phase serves two purposes. Firstly, it narrows down the distribution gap between unimodal representations and the projected multimodal representations through a contrastive-based projection module. Secondly, it initializes the parameters of the framework and cultivates efficient multimodal and unimodal representations. The primary objective here is to prepare the framework for subsequent meta-learning stage. (2) Meta-learning Stage: In the second stage, we leverage the initialized framework and unimodal/multimodal representations to execute meta-learning of unimodal labels. This stage aims to yield proficient unimodal labels for the ensuing multi-task training stage. (3) Multi-task Training Stage: The third stage involves two facets: unimodal learning tasks and multimodal learning task. Leveraging unimodal labels generated in the second stage, we conduct unimodal learning tasks. Simultaneously, the framework addresses multimodal learning task using the available multimodal labels. This conjoined effort refines unimodal and multimodal representations to enhance inferential outcomes. Importantly, the first two stages can be decoupled and trained independently, which significantly reduces the training time. By performing the first two stages only once, we attain the necessary unimodal labels for the joint training stage. The diagram of MUG is shown in Fig. 2.\nOur downstream task is MSA, where the model is fed an utterance [56] (a segment of video demarcated by a sentence). Each utterance encompasses three modalities: acoustic (denoted as a), visual (denoted as v), and language (denoted as l). The objective of MSA is to infer a sentiment score based on the feature sequences of these three modalities. A conventional multimodal framework encompasses the extraction of unimodal representations, the amalgamation of these unimodal representations into a multimodal representation, and the execution of inference predicated on the integrated multimodal representation.\nThe summary of notations and acronyms is shown in Table 1. In the following subsections, we illustrate the procedures of the proposed MUG in detail."}, {"title": "3.1 Unimodal Networks", "content": "First of all, we illustrate the structure of unimodal networks and the procedures to obtain the unimodal representations for the later fusion and the generation of unimodal labels. To make a fair comparison with our closest baseline [19], we use the same structure of unimodal networks as that in Self-MM [19]. Following the state-of-the-art algorithms [1], [19], [20], [47], [57], BERT [32] is used to extract the high-level language representation. Specifically, the procedures of the BERT network are shown as below:\n$x_l = BERT(U_l;\\theta_{BERT})$\n$x_l = f(W_{down}x_l + b_{down}) \\in R^{d_l\\times1}$    (1)\nwhere $U_l$ is the sequence of the input tokens, $x_l$ is the representation of the first token of the BERT output sequence. We use a fully connected layer to further process to project the language representation into a low-dimensional feature space. Notably, for a fair comparison with state-of-the-art method [58], we also present the results of MUG with T5 [59] as the language network. For the acoustic and visual modalities, following our baseline [19], we use the LSTM network as the unimodal network:\n$x_m = LSTM(U_m;\\theta_m) \\in R^{d_m\\times1}$   (2)\nwhere $U_m \\in R^{T_m\\times d'_m}$ is the input feature sequence ($T_m$ denotes the sequence length, $d'_m$ is the input feature dimensionality, and $m\\in \\{a,v\\}$). The generated unimodal representation $x_m$ is used for fusion and learning the unimodal labels."}, {"title": "3.2 Pre-training of Multimodal Framework", "content": "In this stage, we train the framework to initialize parameters and propose the contrastive-based projection module (CPM) to prepare the framework for the meta-learning stage.\nFormally, given three unimodal sequences $U_m \\in R^{T_m\\times d'_m}$ ($m \\in \\{l,a,v\\}$), a traditional multimodal learning system can be formulated as:\n$x_m = F_m(U_m;\\theta_m) \\in R^{d_m\\times1}, m\\in \\{l, a, v\\}$    (3)\n$x = F_M(x_l, x_a, x_v;\\theta_M) \\in R^{d\\times1}$   (4)\n$\\hat{y} = P_M (x; \\Theta_{P_M})$  (5)\nwhere $\\hat{y}$ is the prediction based on multimodal representation $x$, $P_M$ is the multimodal predictor parameterized by $\\Theta_{P_M}$, $F_m$ is the unimodal network parameterized by $\\theta_m$, and $F_M$ is the multimodal fusion network parameterized by $\\theta_M$. $y^j$ is the ground truth label for multimodal sample j (the superscribe j is omitted in most equations for brevity), $\\theta \\in \\{\\theta_a, \\theta_v, \\theta_l,\\theta_M \\}$, $\\alpha$ is the learning rate, n is the batch size, and $L_M$ is the mean absolute error (MAE). For conciseness, the structures of $F_m$, $F_M$ and $P_M$ are illustrated in Fig. 3.\nIn our framework, to prepare the model for the proposed meta-learning strategy, we first need to train a unimodal predictor for each modality that is able to process both the corresponding unimodal representation and multimodal embedding, such that we can use the multimodal annotations to help learn the unimodal labels for unimodal representations (a more detailed illustration is provided in the next section). To achieve this goal, we design a learnable contrastive-based projection module (CPM) for each modality to project the multimodal representation into unimodal embedding space, and use the projected multimodal embedding to train the unimodal predictor. Thereby, we have the following equations:\n$x'_m = f(W_mx + b_m), m \\in \\{l, a, v\\}$    (7)\n$\\hat{y}'_m = P_m (x'_m; \\theta_{p_m})$    (8)\n$L_{m'} = \\frac{1}{n}\\sum_{j=1}^n | y^j - \\hat{y}'_m |$   (9)\nwhere $W_m$ and $b_m$ are the learnable parameters of the projection layer in the CPM for modality m, and f is the activation function. In Eq. 8, we use the projected multimodal embedding $x'_m$ to train the unimodal predictor $P_m$, such that the unimodal predictor can process multimodal embedding to guide the learning of meta uni-label correction networks for individual modalities at the meta-learning stage.\nThe projection layer maps the input multimodal representation into a metric space where it can be directly compared with the corresponding modality in terms of similarity. Nevertheless, projection layer alone is not expressive enough to reduce the distributional gap between unimodal and multimodal embeddings, and contrastive learning [60], [61], [62] is thereby used to further achieve this goal. Firstly, we perform L2 normalization on the representations:\n$x_i \\leftarrow \\frac{x_i}{\\sqrt{\\sum_o (x_i)^2}}, i \\in \\{1, 2, ...\\}$   (10)\nwhere * represents any one of the possible subscribes or superscribes, o is the index at the feature dimension, and L2 normalization is used to constrain the values of similarity between representations to be -1 to +1. Then, we use dot product to measure the similarity between anchor and positives/negatives, and calculate the contrastive learning loss as follows:\n$L_{om} = - \\frac{1}{n} \\sum_{j=1}^n log\\frac{e^{(x_j)T x'_m/\\tau}}{\\sum_{g=1}^n e^{(x_j)T x'_g/\\tau}}$   (11)\nwhere $\\tau$ is the temperature parameter. The contrastive learning is used to improve the mutual information between unimodal and multimodal representations and reduce the gap between them. In this way, we can directly use projected multimodal representations and multimodal labels to guide the learning of unimodal labels and estimate the quality of learned unimodal labels. And the unimodal modules can process both unimodal and projected multimodal signals given that they are close in terms of embeddings. Notably, in the unimodal-multimodal contrastive learning, we stop the gradients from the unimodal representations and only change the distributions of the projected multimodal representations to force the projected multimodal representations to have the same distribution as that of the unimodal representations.\nThe total loss for the first stage is defined as:\n$L = L_M + \\sum_m (\\eta \\cdot L_{m'} + \\gamma \\cdot L_{om})$   (12)\nwhere \u03b7 and \u03b3 are the weights of training losses."}, {"title": "3.3 Meta-Learning of MUCN", "content": "In this section, we learn clean labels of individual modalities based on the weak supervision from multimodal labels via the designed meta-learning strategy. In the meta-training stage, we propose the unimodal denoising task and design the meta uni-label correction network (MUCN) to generate the learned unimodal labels, where MUCN is trained based on the loss of unimodal denoising task. In the meta-testing phrase, we design multimodal denoising task and use multimodal representations (which have sentiment annotations) to evaluate the effectiveness of the updated MUCN. If MUCN becomes less discriminative after meta-training, we compute the meta-gradients of MUCN with respect to the loss of multimodal denoising task, and then meta-update MUCN. In this way, MUCN can be trained more accurately and generate more reasonable unimodal labels for unimodal representations. In contrast to the non-parametric methods [19], [20], the proposed unimodal learning strategy is more expressive and can estimate the quality of the defined unimodal labels. And compared to other meta-learning strategies [22], [23], [24], our method provides explicit supervision signal for MUCN via the proposed unimodal and multimodal denoising tasks, which is more stable in training and ensures the effective learning of MUCN."}, {"title": "3.3.1 Unimodal Denoising Task", "content": "Firstly, in the meta-training stage, we train MUCN (the structure of MUCN is shown in Fig. 3) by guiding it to learn to denoise the manually corrupted multimodal label and recover the original multimodal label y using the unimodal representation:\n$Y_{mc}=MUCN_m(x_m, y + \\epsilon_m;\\theta_{mc}), \\epsilon_m\\sim N(0, I)$   (13)\n$L_{MUCN} (\\Theta_{mc}) = |y - Y_{me} |$ , j\u2208 {1, 2, ..., n}    (14)\n$\\Theta'_{mc} = \\Theta_{mc} - \\alpha \\cdot \\bigtriangledown _{\\Theta_{mc}}L_{MUCN} (\\Theta_{mc})$     (15)\nwhere the MUCN for modality m ($MUCN_m$) takes as input the unimodal representation and the corrupted multimodal label, and outputs a corrected label $Y_{me}$ that is expected to match the original multimodal label y by the training using gradient descent. Following MAML [21], the updated parameters $\\Theta'_{m}$ are computed using one or more gradient descent updates on the unimodal denoising tasks. S denotes the task set that contains a batch of input. The Gaussian noise $\\epsilon_m$ prevents $MUCN_m$ to learn identity mapping and provides the model the capability to learn (at least) a sub-optimal unimodal label for the input unimodal representation. This training strategy enables a more stable and effective learning of MUCN compared to training MUCN without explicit supervision [22]. To be more specific, meta label correction (MLC) [22] imposes no additional constraints on the label correction of noisy samples during the meta-training stage, but merely relies on the meta-gradients computed using the clean samples to guide the learning of noisy labels via a bi-level optimization strategy. In contrast to MLC, the proposed strategy can at least learn the corresponding sample-specific global label for each modality via the unimodal denoising task during the meta-training stage. Thereby, our training strategy provides a good initialization for the parameters of MUCN and prevents MUCN from falling into a worse local optimum after meta-training, enabling a stable learning. Our experiment also verifies that the proposed meta-learning strategy outperforms MLC with a considerable margin (see Section 4.8)."}, {"title": "3.3.2 Additional Design for Unimodal Denoising Task", "content": "After the halfway of the meta-learning where $MUCN_m$ becomes discriminative and can generate meaningful labels, we change the objective of the unimodal denoising task (Eq. 14) as:\n$L_{MUCN} (\\Theta_{mc}) = |Y_{mi} - Y_{me}|$   (16)\nwhere $y_{mi}$ is computed as:\n$y_{mi} = \\lambda \\cdot y_m + (1 - \\lambda) \\cdot y$   (17)\nwhere $y_m$ is the corrected unimodal label of sample j generated by $MUCN_m$ at the previous epoch, and \u03bb is a coefficient which is less than 1. In this way, we can leverage the discriminative power of $MUCN_m$ to improve the unimodal denoising task and further prevent the $MUCN_m$ from defining unimodal label as the corresponding multimodal label. Notably, \u03bb decays as the training deepens:\n$\\lambda \\leftarrow \\frac{\\lambda_{(init)}}{E+1}$  (18)\nwhere $\\lambda_{init}$ is the initialized value of \u03bb, E denotes the current epoch number of meta-learning."}, {"title": "3.3.3 Multimodal Denoising Task", "content": "Nevertheless, the training strategy of unimodal denoising task still tends to define unimodal label as the corresponding multimodal label. Although multimodal label provides abundant prior information to the unimodal label and they are equal/similar in some cases, this assumption inevitably introduces noise to the training of MUCN as elaborated in the Introduction section. To address this issue, we innovatively design the multimodal denoising task to leverage clean multimodal labels and representations to guide the learning of MUCN. Specifically, we first estimate the effectiveness of $MUCN_m$ by estimating whether it can recover clean multimodal label given the projected multimodal representation and corrupted label:\n$Y_c=MUCN_m(x'_m, \\tilde{y}; \\Theta_{mc})$    (19)\n$L_{MUCN}(\\Theta_{me}) = \\frac{1}{|D|}\\sum_{j\\sim D} |y - Y_c|$  (20)\nwhere $\\tilde{y}$ is the corrupted noise label, and D denotes the multimodal denoising task set. To obtain a more accurate estimation of $MUCN_m$ and avoid the bias caused by the same labels of unimodal and multimodal denoising tasks, we additionally randomly sample more data to constitute D, i.e., D = S\u222aS' where |S'| = b \u00b7 |S| (b is set to 10 in our experiments). Notably, thanks to the CPM at the first stage, the project multimodal representation and corresponding unimodal representation have similar distributions, and therefore it is appropriate to feed them into the same MUCN. There is still one question remaining: How to construct the noisy multimodal label \u1ef9? In our setting, we feed the projected multimodal representation $x'_m$ into the unimodal predictor to obtain the prediction $\\hat{y}'_m$ (see Eq. 8) and add a Gaussian noise to the generated prediction, which is used as the noisy multimodal label \u1ef9. This is because the generated prediction $\\hat{y}'_m$ is semantically related to the true label y, and the Gaussian noise is used to increase the difficulty of learning and prevent $MUCN_m$ to learn an inverse mapping of the unimodal predictor. The equation is shown as below:\n$\\tilde{y} = \\hat{y}'_m + \\epsilon_{m'}, \\epsilon_{m'} \\sim N(0, I)$  (21)"}, {"title": "3.3.4 Optimization Strategy", "content": "After the training on unimodal denoising task (see Eq. 15), $\\Theta_{me}$ is updated to $\\Theta'_{m}$, and we feed the same multimodal input to go through Eqs. 19 and 20 again, which generates the post correction loss $L_{MUCN} (\\Theta'_{me})$. Then, we estimate the values of $L^D_{MUCN} (\\Theta_{me})$ and $L_{MUCN} (\\Theta'_{me})$. If $L^D_{MUCN} (\\Theta_{me})$ < $L_{MUCN} (\\Theta'_{me})$ which suggests that $MUCN_m$ becomes more discriminative after meta-training and multimodal label y is likely to be an excellent estimation of unimodal label, we simply skip the meta-testing stage and define the updated parameters of $MUCN_m$ as:\n$\\Theta'_{mc} \\leftarrow \\Theta_{mc}$    (22)\nAnd if $L^D_{MUCN} (\\Theta_{me})$ \u2265 $L_{MUCN} (\\Theta'_{me})$ which suggests that the corresponding multimodal label y is not an excellent estimation of unimodal label and the corrected label $Y_{me}$ is actually a more suitable unimodal label, we use a bi-level optimization strategy to correct the 'false' update of $MUCN_m$ via meta-updating:\n$\\Theta_{mc} \\leftarrow \\Theta_{me} - \\alpha \\cdot \\bigtriangledown _{\\Theta_{me}} L^D_{MU} (\\Theta'_{mc})$      (23)\nwhere \u03b1' is the learning rate at the meta-testing stage. Note that here we directly take the derivative of the original $\\Theta_{me}$ with respect to $L_{MUCN} (\\Theta'_{mc})$ (that is why it is a bi-level optimization strategy). The bi-level optimization strategy is based on the assumption that if the current $MUCN_m$ is discriminative enough, then it is able to perform well on multimodal denoising task whose annotated label is available and features are of similar distribution. The meta-update of $\\Theta_{me}$ encourages $MUCN_m$ to learn to perform well on label denoising task. Compared to other meta-learning based methods [22], [23], [24], the proposed denoising tasks provide explicit supervision for MUCN and our bi-level optimization does not involve the update of other modules, which enables a more stable and effective training of MUCN.\nSince only MUCN is updated at this stage, we do not go through the network again but simply use the saved unimodal and multimodal representations generated by the network at the first stage, which can greatly accelerate the training of MUCN."}, {"title": "3.4 Multi-Task Training", "content": "In this stage", "19": ".", "are": "n$\\hat{Y}_m = P_m (x_m;\\theta_{p_m})$"}]}