{"title": "Force-Based Robotic Imitation Learning: A Two-Phase Approach for Construction Assembly Tasks", "authors": ["Hengxu You", "Yang Ye", "Tianyu Zhou", "Eric Jing Du"], "abstract": "The drive for increased efficiency, safety, and cost-reductions in the construction industry has intensified interest in robotics and automation. While robots can mitigate labor-intensive tasks with basic programming, tasks demanding intricate manipulation, such as welding and pipe insertion, present considerable challenges. These tasks require refined adaptive force control, making them intricate to learn and often leading to slow or non-convergence in robotic training. To navigate these challenges, potential solutions include leveraging deep imitation learning, i.e., revised reinforcement learning from human demonstrations. However, conventional imitation learning methods, predominantly reliant on visual or positional cues, often fall short in construction assembly tasks such like pipe insertion, where visual inputs can be obstructed, and force information contains more features needed for the completion. Addressing this, the paper introduces a two-phase system framework that integrates human-derived force feedback into robot learning. The first phase encompasses real-time data capture from human operators using a robot arm synchronized with a virtual physical simulator simulation, facilitated by ROS-Sharp as the data-bridge module. In the second phase, the force feedback is translated into robot motion instructions. A generative method is then employed to embed force feedback within the reward function, steering the learning trajectory. The effectiveness of the proposed method in terms of robotic motor learning was gauged through metrics such as task completion time steps and success rates. The paper's contribution is a two-phase framework that provides a similar-to-real force-based collision simulation, allowing human experts to experience real-time force interactions. This enables the capture of realistic force-responsive demonstrations, ensuring that the collected data effectively guides robot learning for adaptive and precise manipulation in construction assembly tasks.", "sections": [{"title": "1. INTRODUCTION", "content": "The construction industry, in its pursuit of enhanced efficiency, reduced costs, and increased safety, is increasingly seeking automation and robotic assistance [1-3]. Robots have shown enormous potential to alleviate repetitive, and dangerous tasks from human workers, such as assembly, infrastructure inspection, material handling and heavy rigging [4-6]. Integrating the artificial intelligence (AI) agent with a physical robotic system could further improve the precision, reliability, and consistency of operations with competent training [7, 8]. While AI-enabled robots excel in performing repetitive and predefined tasks, dexterous and complex tasks still pose a significant difficulty such as welding and pipe insertion [9, 10]. Training a robot to perform these dexterous tasks demands delicate manipulation and adaptive force control, which induces diversity and several potential actions leading to a substantial increase in the complexity of the learning process and resulting in slow convergence or lack of convergence [11]\nTo tackle the challenges of learning in high-dimensional action spaces, Imitation Learning (IL) based methods are applied to leverage demonstrations from human experts or proficient use of human demonstrations as a form of instruction and reduce the size of action spaces that need to be explored [12-14]. Generative Adversarial Imitation Learning (GAIL)[15] could further address some key limitations of traditional IL by mitigating distributional shifts, thus enabling better exploration and performance in unseen states and generalizing better to new tasks [15]. The current formats of these instructions are largely reliant on visual or positional cues [16, 17]. In a task like a pipe installation, the vision sight is often obscured, and positional information might not be sufficient due to the irregular and rough interior surface of the pipe, which makes it evident that the conventional forms of instruction are inadequate for such tasks.\nThe limitations inherent in capturing vision-based demonstrations underscore the need for a more sophisticated form of human demonstration in robotic learning. In this context, we argue that force-based feedback represents a more effective modality for imitation learning, particularly in tasks requiring fine motor control. Human operators heavily depend on force feedback for tasks that necessitate dexterous manipulation, highlighting its significance [18, 19]. Integrating force feedback into the learning process of the robot could be instrumental in enhancing its ability to perform complex tasks [20, 21]. Besides, despite imitation learning's ability to reduce early-stage errors by following expert demonstrations, the robot agent may still deviate from optimal behavior due to data noise, algorithmic constraints, or unforeseen environmental variations [22, 23]. \u03a4\u03bf ensure that the training agents can be effectively instructed, the demonstrations are required to be high-quality and generalizable [24]. Ensuring that human operators experience realistic force interactions allows the collected demonstration data to be more representative of real-world conditions, making it more effective in guiding robotic. Without an environment that accurately simulates real-world force interactions, human demonstrations may not fully capture the nuanced responses required for effective robot learning. Thus, the primary challenge lies in how to provide realistic force-based responses during human data collection so that human expertise can naturally react to collisions and adjust actions based on force feedback.\nTo address these challenges, this paper introduces a groundbreaking two-phase system framework that capitalizes on human-derived force feedback, integrating this data into the robot's learning protocol. The first phase involves real-time data collection from human operators using a high-precision robotic arm equipped with force sensors, which is synchronized with a virtual robot in a Unity simulation environment. This setup allows for the detailed capture of force-feedback data during task execution, facilitated by the data-bridge module ROS# for seamless data transmission between the physical robot and the simulation. The second phase concentrates on learning with force-based guidance, where the force feedback data is processed and transformed into motion instructions for the robot, ensuring accurate feature extraction. The use of Generative Adversarial Imitation Learning (GAIL) to integrate this force data into the reward function is a novel approach that shapes the learning trajectory. The effectiveness of the proposed force-based imitation learning approach is validated through a range of performance metrics, including task completion time steps and success rates. The discussion also covers the benefits and potential limitations of applying this methodology in real-world scenarios, emphasizing its applicability and scalability."}, {"title": "2. LITERATURE REVIEW", "content": ""}, {"title": "2.1. Deep Learning in Robot Training", "content": "Over the past decade, Deep Learning (DL) has emerged as a compelling approach for training robotic systems to accomplish a variety of complex tasks, such as navigation in unseen environments [25], manipulation with high-cognitive complexity tasks [26, 27] or high-dexterity tasks [28] and humanoid locomotion [29]. In the specific field of intelligent robot learning, the methods could be classified into two major groups: Deep Reinforcement learning (DRL) and Deep Imitation Learning (DIL) [30].\nThe basic idea of DRL-related algorithms is to induce the deep neuron network (DNN) as the decision kernel of the intelligent robot and interpolate a high-dimensional optimal projection as the policy to guide the robot to make actions based on current observations to achieve the pre-defined goals [23, 31]. During the training process, the agent simultaneously undergoes exploration, where it tests new actions to gather information, and exploitation, where it leverages existing knowledge for optimal decision-making. These intertwined processes, comprising the exploration-exploitation trade-off, are intrinsic to the agent's learning throughout the reinforcement learning process [32]. Previous studies have already shown the effectiveness of the exploration-exploitation trade-off in reinforcement learning in scenarios involving tasks with limited action spaces, such as obstacle avoidance [33, 34], navigation [35, 36] and object operation [37, 38]. With the appropriate design of rewards, observations and actions, the robot trained with DRL can handle various tasks under different scenarios, such as under-water robot operation [39], emergency structural inspection [40] and rescue searching [41]. However, when it comes to dexterous operations, the inherent complexities give rise to an expansive action space and the vastness of possible actions leads the DRL models to invest a significant portion of exploration time. The process can be inefficient as the model sifts through numerous sub-optimal actions before finding an effective strategy [42].\nTo reduce the difficulty of exploring in large action space, the DIL-based methods were proposed that could leverage the human or expert demonstration for robot agent training [25], including behavioral cloning (BC), inverse reinforcement learning (IRL) and generative adversarial imitation learning (GAIL) [43]. The agent is forced to mimic expert behaviors by observing the demonstration so that it can rapidly find the correct solution to complete the task [26]. The versatility of DIL methods has been recognized, finding applications in various robot-learning domains, such as skill transfer [44], human-robot collaboration [45], etc. Liang et al. propose the learning-from-demonstration method to conduct quasi-repetitive construction tasks with demonstrations from humans [46]. The method also verifies the effectiveness of demonstration data generated in the virtual environment. Since the DIL methods have the overfitting problem that the agent would totally learn from the limited knowledge in a single demonstration, the combined framework is proposed that uses DIL to first initialize the training and reduce the action space then apply DRL to further explore within the limited scope to ensure generalizability [47]. [48-51] have theoretically shown the improvement of combining DIL and DRL compared with single DRL or DIL methods. From the perspective of real-word application, Huang et al. develop a novel training framework to teach the robot to learn long-horizon tasks with a more efficient exploration strategy. The results show that the combination of DIL-DRL training could effectively increase the robot's learning speed with high mission completion rate [52]. The DIL-DRL methods leverage the benefits of small action space and adversarial exploration proficiency which could largely increase the training efficiency."}, {"title": "2.2. Imitation Learning for Construction Robots", "content": "Construction robots have significantly advanced over recent years, enhancing efficiency, safety, and precision on construction sites. These robots are designed to handle various tasks, from material handling to intricate assembly processes. The development of these systems involves integrating advanced technologies such as autonomous navigation, scene understanding, and real-time monitoring. For instance, in [53], the importance of advanced scene understanding and adaptive manipulation in construction robots is discussed. Techniques like Clustering and Iterative Closest Point (CICP) and Generalized Resolution Correlative Scan Matching (GRCSM) enhance the robot's ability to accurately perceive and model construction components, enabling precise manipulation and adaptation to dynamic environments. Similarly, [54] highlights the integration of GRCSM for accurate geometry modeling and adaptive work plan formulation, allowing robots to handle tasks such as joint filling with high precision. Another critical aspect of construction robotics is ensuring effective human-robot collaboration. [55] introduces a digital twin system that enhances collaboration by synchronizing real-time data between the physical robot and its virtual model. This system facilitates real-time monitoring and control, improving efficiency and safety. Furthermore, [56, 57] offers a significant advancement in human-robot collaboration by enabling real-time data exchange and synchronization. Path planning is another essential component, as demonstrated in [58]. This paper proposes a method that integrates predicted movements of construction workers to achieve safe and efficient collaboration. [59] focuses on improving safety and collaboration by accurately estimating robot poses in real-time using a deep convolutional network. Additionally, [60] showcases the development of a robotic assembly system designed to improve the safety and efficiency of steel beam assembly processes. This system utilizes innovative methods for rotation, alignment, bolting, and unloading, significantly enhancing the assembly process. While the innovations in construction robotics have significantly improved various aspects of construction tasks, the integration of imitation learning (IL) offers a unique and powerful approach to further enhance these capabilities. By enabling robots to learn directly from human demonstrations, IL bridges the gap between human expertise and robotic execution, facilitating the handling of complex and dynamic construction environments with greater precision and adaptability.\nImitation learning (IL) has emerged as a vital approach in enhancing the capabilities of construction robots. By leveraging human expertise, IL enables robots to learn tasks through demonstration, which is particularly useful in the complex and dynamic environments of construction sites. This method focuses on achieving high accuracy, efficiency, and adaptability in various construction-related tasks. The framework introduced in [14] exemplifies this approach. It leverages a digital training environment and a federated construction skill cloud database to transfer construction skills from human workers to robots. This system enables robots to perform tasks such as ceiling installation with high precision by learning from human demonstrations in a virtual reality setting. Another significant contribution is found in [61], which discusses a method for teaching robots to perform tasks like ceiling tile installation by representing trajectories using generalized cylinders with orientation. This approach allows the robot to adapt its actions based on the geometric and orientation data of human demonstrations, enhancing its ability to handle complex tasks. [62, 63] explore the use of visual demonstrations and reinforcement learning to train robots for tasks such as ceiling tile installation. The context translation model and reinforcement learning components enable robots to learn and execute construction tasks by observing human demonstrations, improving their performance and adaptability. [46] highlights the feasibility and effectiveness of using IL to train robots for construction tasks. By combining context translation with reinforcement learning, this approach allows robots to learn from human demonstrations and adapt to the varying conditions of construction sites.\nThe previous work has made advanced explorations on how to use imitation learning to train robots with visual demonstrations. The success of these approach shows the potential of using other types of demonstrations, such as tactile or kinesthetic feedback, to guide the learning process. By expanding the types of demonstrations used, construction robots can achieve even greater versatility and performance in a broader range of tasks. This could lead to further innovations in how robots interact with their environments and adapt to new challenges, ultimately pushing the boundaries of what is possible in the construction industry."}, {"title": "2.3. Demonstration for Imitation Learning", "content": "In the realm of robot training within imitation learning paradigms, demonstrations serve as foundational blueprints that guide the robot's learning process [13]. Demonstrations have been instrumental in bridging the gap between human intuition and robotic execution, effectively translating human expertise into actions that a robot can mimic [64]. Demonstrations not only reduce the need for extensive training but also provide a structured pathway for robots to learn complex tasks that might be challenging to learn from scratch using traditional reinforcement learning methods. The major demonstration types to be used in DRL could be roughly divided into visual demonstrations [65], force/tactile demonstrations [66] and other types such as kinesthetic demonstrations [67] or language demonstrations [68].\nVisual demonstration primarily leverages images or videos to represent instructions on certain tasks [69]. Taking advantage of the detailed texture information captured from the environment, the visual data intuitively capture the spatial and contextual details of an environment, aiding in tasks that robotic tasks like object manipulation [70], navigation [71], and interaction [72]. [73] proposed a GAIL-based framework for long-horizon construction tasks, relying primarily on visual feedback to guide robots in object manipulation and placement in large-scale environments, such as installing window panels. While visual data have proven invaluable for a broad range of robotic tasks, this type of demonstration has inherent limitations when guiding dexterous operations, particularly those that involve fine-scale manipulations such as pipe insertion [74]. Predominantly, visual sensors capture macro-level information and might struggle to discern the nuanced movements and adjustments that are often crucial in delicate operations [75]. For tasks like pipe insertion, the tactile feedback, including subtle resistances or minute texture variations, provides vital cues that visual data might overlook since the changes could lead to small or even no detectable object shiftiness or deformation. Furthermore, minor surface irregularities, which can significantly influence the precision of dexterous tasks, can easily elude visual detection, especially in non-controlled environments such as lighting changes or camera view obscuring [76]. Moreover, the very nature of certain tasks can result in occlusions, where essential aspects of the operation become visually obscured [77]. While embedding a moveable visual sensor with the operation object, such as inside a pipe, could theoretically offer a vantage visual perspective, it introduces logistical hurdles, from potential interference with the operation to the difficulty in its subsequent removal. In summary, the visual demonstration is not qualified to provide instructive demonstration if the task is performed in a dynamically changing and high-interactable environment.\nForce demonstration, on the other hand, offers a uniquely apt perspective for dexterous operations. Despite the loss of sufficient texture information, force feedback captures the tangible distinctions intrinsic to delicate tasks [78]. [79] utilized tactile force feedback, processed through an SVM, to classify different interaction states in human-robot interaction tasks, enhancing the robot's ability to respond based on categorized tactile inputs. For intricate maneuvers such as pipe insertion, the precise force interactions, provide critical information about alignment, orientation, and fit with gentle guidance or resistance. The tactile nature of force demonstration ensures that even in scenarios where visual cues are compromised or unavailable, the agent can still receive a clear and direct signal of its interactions with the environment [80]. Consequently, for dexterous tasks demanding high precision and sensitivity, force demonstration becomes an invaluable source of guidance, ensuring accuracy and adaptability."}, {"title": "3. SYSTEM DESIGN", "content": ""}, {"title": "3.1. Overall Architecture", "content": "This study develops a motion skill transfer system to infer human adjustment strategies during task execution, enhancing a robot's learning for similar tasks. To parameterize the knowledge from human experience and speed up the robot arm training process, we propose an integration workflow including two steps: human data collection and fused deep reinforcement learning as shown in Fig.1. The Franka Emika Panda robot is used for data collection.\nIn the data collection session, a human expert is requested to hold the robot arm and attempt to insert the inner pipe into the outer one. Initially, we configure the robot arm to employ gravity compensation, enabling the expert to manipulate the end effector with minimal effort. The force data exerted directly on the end effector by the human expert is captured in Cartesian coordinates as $F_{demo} = (f_x, f_y, f_z)$. This primary action data is crucial as it directly influences the interaction forces relevant to the pipe insertion task.\nSimultaneously, the human expert, equipped with virtual reality (VR) goggles, observes a simulated pipe insertion scene within a Unity environment. Actions performed by the expert are mirrored by the virtual robot arm in the simulation, where interactions such as collisions are simulated, and feedback is provided to the real robot arm. This bi-directional data flow allows the expert to adjust actions based on both real-time physical and simulated feedback.\nBoth the expert's direct actions on the end effector and the environmental interactions observed within the simulation are recorded. These human data collection patterns inform the deep reinforcement learning phase, which utilizes the proximal policy optimization (PPO) strategy to train agents for tasks like insertion. The training phase is represented by red lines in Fig.1."}, {"title": "3.2. System Structure", "content": "The system comprises two phases: Data Collection through Interaction (DCI) and Learning with Guidance (LG). In DCI, a human operates the FEP robot arm to perform tasks like pipe insertion, with actions and system states recorded for demonstration. The real arm and its Unity environment digital twin synchronize, transmitting virtual collision forces back as haptic feedback, aiding humans to refine their actions for quality data. In LG, these demonstrations guide a reinforcement learning agent in Unity, initially mimicking human actions, then evolving its policy through environmental rewards and penalties. This process, illustrated in Fig.2, shows DCI (black lines) and LG (orange lines) structures.\nThe system is divided into three functional layers: the interface, module, and algorithm layers. The interface layer establishes a real-time interactive environment for data collection and physical interaction in both DCI and LG phases. The module layer handles haptic feedback computation, data recording, and DIL simulation in these phases. Lastly, the algorithm layer extracts general features from observations and identifies aligned actions to guide the training process."}, {"title": "3.3. Robot Control for Collision and Non-Collision", "content": "In the DCI phase, the control module operates the real robot arm's background algorithm for human data collection. Experts control a virtual inner pipe using the robot arm's end effector in 3D cartesian space, with joint torques calculated via inverse kinematics (IK). This is visualized in Fig.3, which shows the panda robot arm's joint configuration. The real arm, primarily for data collection, is modified by removing the end effector and maintaining 6 Degree-of-Freedom (DoF), as depicted in Fig.3(b) with a vertical pipe attached to joint 6 as the user handle. Note that the handle used is a small, rigid metal pipe, chosen for its negligible deformation under operational loads. This ensures accurate force measurements and reliable manipulations. To integrate this handler with our robotic system, we designed a custom adapter to mount the pipe onto the Franka Emika Panda robot arm, replacing the original end effector. This secure connection minimizes mechanical delay and ensures precise operations. Details of the mounting mechanism and its impact on experimental integrity are illustrated in Fig.3(c).\nTwo control modules, Motion and Force module, were developed to simulate the real operation condition corresponding to the to the collision and non-collision condition respectively. Let $x_o^t$ be the desired position of the handler at time stamp tand $x_{dis}^t = x^t \u2013 x_o^t$ denotes the displacement between the handler's current and desired position at time stamp t, then the control algorithm could be expressed as:\n$F = \\delta_{mov}(x^{t-1}, x^t x_o^t) \\cdot \\delta_{coll}(x) \\cdot (M_c\\cdot \\ddot{x}_{dis}^t + B_c\\cdot \\dot{x}_{dis}^t + K_c \\cdot x_{dis}^t) + g_c$, (1)\nwhere $F$ denotes the desired resistance force in cartesian space, $M_c$ denotes the inertia coefficient for robot's self-resistance, $B_c$ denotes the damping coefficient related to the speed of motion, $K_c$ denotes the damping coefficient for motion speed and $g_c$ refers to the constant force compensating for gravity. $\\delta_{mov}(.)$ and $\\delta_{coll}(.)$ adjust control modes based on simulation results. $\\delta_{mov} (x^{t-1}, x^t x_o^t)$ predicts whether the inner pipe is moving towards or away from the last"}, {"title": "3.4. Collision Simulation", "content": "The pipe insertion task poses significant computational challenges for Unity's PhysX engine due to the continuous contact between the inner and outer pipes. While PhysX is effective for basic physical interactions, it struggles with prolonged collisions, leading to resource-intensive processes, delays, and inaccuracies like object penetration. These issues disrupt simulation realism and compromise the quality of demonstration data, rendering it unsuitable for training. To address this issue, we developed a simplified force simulation system tailored specifically for contact-rich tasks like pipe insertion. Instead of relying entirely on PhysX for resolving collisions, we leveraged PhysX's OnTrigger function, which allows for precise detection of collision points without invoking the computationally expensive physics solver. In addition, we employed impedance control to simulate the collision forces. Even when objects are colliding, slight translations occur if the objects are pushed together by a large force. These small movements are used to calculate the velocity of the objects, which, combined with their mass, allows us to infer force feedback through a custom mathematical model. This method efficiently replicates the physical dynamics of the task without the need for constant collision recalculations. The process involves collision detection and impact resolution, yielding realistic output force on colliding objects. Detailed explanations follow in subsequent sections."}, {"title": "3.4.1 Collision Simulation", "content": "During collision detection, the simulator identifies contact points between object colliders in two stages: broad-phase and narrow-phase detection. Broad-phase is a preliminary step identifying potential colliding objects. If a possible collision is detected, narrow-phase detection accurately simulates the collision. The simulator checks for intersections between mesh colliders; if none are found, $\\delta_{coll}(x) = 0$. Otherwise, $\\delta_{coll}(x) = 1$. The value will be transferred to the control module to control the real robot arm as mentioned before. Upon intersection detection, contact points and their normal vectors (pointing outwards from contact points) are calculated. Fig.4 illustrates these points (red dots) and vectors (white lines). Contact points are denoted as $p_t^i \\in \\mathbb{R}^3$ with corresponding normal vector $n_i$ and the results are represented as $PT = {(p_t^i, n_i)|i \\in 1,...,N}$ where N is the number of total detected points."}, {"title": "3.4.2 Collision Resolution", "content": "To support real-time interaction response, we applied a customized trigger method to resolve the collision interaction that only provides the detection results with simplified physical interaction rules. Specifically, we split the total collision process into two phases: pre-collision during frame t-1 tot and on-collision during frame t tot + 1. For the pre-collision phase, we applied Newton's second law to calculate the impulse that only lasts for a single frame. Let $x^{t-1}$ and $x^t$ be the locations of the handler at time stamps t-1 and t, respectively. The moving speed of the handler is calculated by:where $T^{t-1,t}$ is the time difference between the two adjacent frames. Let m be the pre-defined mass of the inner pipe in the simulation environment, then the impulse magnitude on the inner pipe at frame t is calculated by:\n$F_{pre} = \\frac{mv^t}{T^{t-1,t}}$ (3)\nTo estimate the normal of the closest contact point, we attach a ray-caster on the inner pipe at the exact location of the closest contact point. The ray-caster will emit $n_{ray}$ rays that return $n_{ray}$ points on the outer pipe surface centered around $pt_{closest}$ distributed in a spherical shape. To ensure that all the ray-caster points are on the outer pipe surface, the angles between rays and the vector $x^t. x^t. pt_{closest}$ is set to be smaller than 45 degrees.\nGiven the set of ray-caster points as {$pt_{ray_i}|i \\in [1, ..., N_{ray}]$}, the center is calculated by:\n$pt_{raycen} = \\frac{\\sum_{i=1}^{n_{ray}} p^t_{rayi}}{N_{ray}}$, (4)\nand the normalized point sets are calculated as:\n$pt_{ray_i} = pt^t_{ray_i} - pt_{raycen}$ for i in $range(n_{ray})$. (5)\nThe normalized points are used to construct the matrix A as:\n$A =\\begin{bmatrix}\np^t_{ray_1}. x & p^t_{ray_1}. y& p^t_{ray_1}. z\\\\\n... & ...& ...\\\\\np^t_{ray_{nray}}. x & p^t_{ray_{nray}}. y& p^t_{ray_{nray}}. z\n\\end{bmatrix}$\nand derived the singular value decomposition (SVD) results of A as:\n$A = U\\cdot S\\cdot V^T$. (6)\nThen the surface normal is the last column of V which corresponds to the least singular vector of A, denoted as $n_{closest}$, and the direction of the impulse is calculated by:\n$n^t_{closest} = n^t_{closest} - 2 \\cdot (v^t * n_{closest}) * n^t_{closest}$ (7)\nand the impulse force vector is calculated by:\n$F^t_{pre} = F^t_{pre} F^t_{closest}$ (8)\nIn the on-collision phase with constant force, both inner and outer pipes being rigid bodies, the contact force equals the external force on the inner pipe, calculated using Eq (8). Here, not just the colliding force, but also friction, is crucial for guiding the expert. The variation in friction provides haptic feedback, indicating the inner pipe's position: increasing friction suggests an improper position. The normal supporting force and friction are derived using specific formulas as:\n$F_{normal}^t = \\frac{F_{on}^t * n_{closest}}{\nF_{friction} = F_{on}^t - F_{normal}}$ (9)\nTo ensure ease of separation between rigid bodies during expert manipulation, a shifting prediction module is implemented. This prevents experts from feeling unrealistic resistance when moving the inner pipe away from a contact point, as would be suggested by Eq (8). To address this, a motion prediction indicator $\\delta_{mov}(.,.)$ is used. If the collision happens at time stamp t and the current location is denoted as $x_t$, then at the time stamp $x^{t+1}$ we calculate the indicator's value as:\n$Ind = (x^{t+1} \u2212 x_o) * (x^t \u2212 x_o)$. (10)\nIf $Ind \\geq 0$, $\\delta_{mov}$ is set as 0. On the other hand, $\\delta_{mov}$ is set as 1, indicating that the expert is still trying to push the inner pipe to the collision point."}, {"title": "3.4.2 Deep Learning Process", "content": "Similar to the traditional reinforcement learning problem, the operation sequence of the pipe insertion task is formed as a Markov decision process (MDP), denoted as M = (S, A, Pa, Ra), where S is the set of possible states collected from the current environment, A is the set of potential actions that the agent could take, Pa is the probability that the agent would take action a with the current observation and Ra is the expected immediate reward set. Two types of observations are used for comparison: the traditional visual data and the simulated force data proposed in this study which will be discussed in the following section. The specific optimization target for pipe insertion is then formed as inserting the inner pipe into the outer pipe with a certain depth with the minimum number of steps.\nThen, during the data collection process, the human expert is required to hold the handler and try to insert the inner pipe into the outer pipe at a certain depth. The observations and collision feedback are recorded as a demonstration using the ML-Agent imitation learning package. Two types of demonstration data are used with the corresponding training setup for comparison, including visual demonstration and force demonstration.\nFor the force demonstration, the depth and distance information are both provided. The distance information used in this context refers to the distance between the inner pipe's tip (edge) and the outer pipe's center. The depth information refers to the moving distance along the insertion direction, which is critical for tracking the progression of the insertion and ensuring proper positioning throughout the task. Additionally, the force observation $F_{ob} = [F_{normal}, F_{friction}]^T$ is included as calculated in Eq (8). The human expert could also monitor the insertion process through a global camera outside as shown in Fig.5(a)."}, {"title": "4. EXPEERIMENTS", "content": "The physical interaction simulation system is designed in Unity using the physical engine.\nIn our experiments, the outer pipe exists only in Unity and its position is directly obtained, while the digital twin is built for the robot arm to collect human expert action data through VR. The training process is built with ML-Agents which is a well-designed reinforcement learning framework that could be directly plugged into the physical environment and collected data through the interaction. To assess the success of the pipe assembly task, the success criterion is set as the inner pipe being inserted into the outer pipe to a depth greater than 0.5m. A thin pad is placed inside the outer pipe at the 0.5m depth mark to detect collisions. If the inner pipe contacts the pad, it indicates that the task has been successfully completed.\nThe policy network to be trained is structured using the configuration file provided by ML-Agents (Juliani et al. 2018). For the GAIL model, the generator consists of an input layer, a single hidden layer and an output layer. The generator and discriminator network architecture employed in this work is adapted from established models commonly used in adversarial learning frameworks.\nThe input layer for the force demonstration group includes eight neurons: six for the observation vector $F_{ob}$ and two for depth and distance. The visual demonstration group's generator has an input of 258 neurons, formed from a 4\u00d764 dimensions vector plus two for depth and distance. Despite different input configurations, both groups use a common hidden layer with 256 neurons.\nThe output layers in both are identical, consisting of three neurons for Cartesian space force exertion. This design balances modality-specific inputs with a unified structure in hidden and output layers, capturing each feedback type's specifics while ensuring consistency.\nThe discriminator is formed by an input layer, two hidden layers and an output layer. The force group's discriminator has an 11-neuron input layer for the state-action pair (s, a), with 8 for observations and 3 for actions. It features two 256-neuron hidden layers and a single-neuron output for confidence scoring. Similarly, the visual group's generator and discriminator mirror this structure. The visual group's discriminator has 261 input neurons with 258 for observations from 64 Raycasters (with each vector's first three elements one-hot encoding colliding object types and the last element recording collision distance) and 3 for actions. The final two neurons track depth measurements.\nThe deep reinforcement learning policy networks for force and visual groups are the same as the generators, correspondingly. The reward mechanism for the training is implemented as follows: Let $F_{normal_t}, F_{friction_t}$ denote the normal force and friction force at timestep t. Then the reward $r_t$ is defined as:\n$r_t = \\begin{cases}\n\\frac{1}{MaxStep},if ||F_{normal_t}|| < ||F_{normal_{t-1}}|| and ||F_{friction_t}|| < ||F_{friction_{t-1}}||\n\\\\0, if ||F_{normal_t}|| = ||F_{normal_{t-1}}|| and ||F_{friction_t}|| = ||F_{friction_{t-1}}||\n\\\\-\\frac{1}{MaxStep}, else\n\\end{cases}$ (11)\nAdditionally, if the pipe is inserted to the desired depth, a value of 1 score will be added to the total score of the current episode. For every step before reaching the target depth, a negative score with $\u22121/MaxStep$ will be added to the total score. The reward mechanism is designed with two main objectives: task completion and collision avoidance. The robot is rewarded with 1 upon successfully inserting the pipe to the desired depth as the task is completed. To prevent damage from collisions, the robot is given positive feedback when both the normal and friction forces decrease over time, indicating it is avoiding collisions. Conversely, if the robot moves toward the collision point, a negative score is assigned. The $^1/_MaxStep$ term is used for normalization, ensuring the reward remains within a reasonable scale and promotes stable learning.\nFor training, we chose a batch size of 128 and a buffer size of 2,048, with a learning rate of 0.0003 decreasing linearly. Inputs are normalized, and a simple visual encoding is used. The extrinsic reward has a strength of 1.0 and a discount factor of 0.99. Behavioral cloning occurs in the first 10,000 steps with a strength of 1.0, not updating regularly (samples per update set to zero). The GAIL reward signal is at 0.01 strength with a 0.99 discount factor. Training retains the last five checkpoints over a maximum of 5 million steps, with each episode lasting 10,000 steps and summaries generated every 12,000 steps."}, {"title": "5. LEARNING RESULTS", "content": ""}, {"title": "5.1. Overall Performance", "content": "We collected 20 human demonstrations for each group and trained the agent with each data sample respectively. Fig.6 showed the reward plots of all 40 training trials with the green lines denoting the force demonstrations and the red lines denoting the visual demonstrations. The dotted black line is the reinforcement learning (RL) baseline without force or Raycaster visual observation, only the distance and depth information are provided. The solid purple line is the reward plot for force group baseline and the dashed purple line is for the visual group. The maximum cumulated reward for a single episode is approximately 1.4 and we set the total training step to be 10,000,000. The detailed training hyperparameters are listed in Table 1."}, {"title": "5.2. Condition 1: Randomized Inner Pipe Location", "content": "To ensure the generalization of the learned policy, we randomly initialized the outer pipe and inner pipe positions during the inferencing process with the target position to be fixed. The distance between the target center and the initial position varies from 0.6m to 0.8m along the x-axis"}]}