{"title": "Sparse Autoencoders for Hypothesis Generation", "authors": ["Rajiv Movva", "Kenny Peng", "Nikhil Garg", "Jon Kleinberg", "Emma Pierson"], "abstract": "We describe HYPOTHESAES, a general method\nto hypothesize interpretable relationships between\ntext data (e.g., headlines) and a target variable\n(e.g., clicks). HYPOTHESAES has three steps:\n(1) train a sparse autoencoder on text embeddings\nto produce interpretable features describing the\ndata distribution, (2) select features that predict\nthe target variable, and (3) generate a natural lan-\nguage interpretation of each feature (e.g., men-\ntions being surprised or shocked) using an LLM.\nEach interpretation serves as a hypothesis about\nwhat predicts the target variable. Compared to\nbaselines, our method better identifies reference\nhypotheses on synthetic datasets (at least +0.06\nin F1) and produces more predictive hypotheses\non real datasets (~twice as many significant find-\nings), despite requiring 1-2 orders of magnitude\nless compute than recent LLM-based methods.\nHYPOTHESAES also produces novel discoveries\non two well-studied tasks: explaining partisan dif-\nferences in Congressional speeches and identify-\ning drivers of engagement with online headlines.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) show promise as a tool for\nhypothesis generation. Discovering relationships between\ntext data and a target variable is an important and funda-\nmental task with diverse applications in economics, political\nscience, sociology, medicine, and business (Grimmer, 2010;\nRathje et al., 2021; Sun et al., 2022; Gentzkow & Shapiro,\n2010; Monroe et al., 2009; Nelson, 2020; Ranard et al.,\n2016; Ting et al., 2017). What features of a restaurant re-\nview predict a low rating? What features of a social media\npost predict whether it will go viral? What features of a\npatient's clinical notes predict if they will develop cancer?\nAutomated approaches to answering these questions have\nthe potential to significantly expand and accelerate scientific\ndiscovery (Ludwig & Mullainathan, 2023). Indeed, multiple\nlines of work-spanning decades of research-have sought\nto extract text features that can predict a target variable (Blei\net al., 2003; Monroe et al., 2009). Recent LLM-based hy-\npothesis generation methods are especially exciting since\nthey operate directly at the level of human-interpretable nat-\nural language concepts (Zhou et al., 2024; Batista & Ross,\n2024; Ludan et al., 2024; Zhong et al., 2024). Concretely,\nhypothesis generation methods take a dataset of texts linked\nto a target variable (e.g., headlines and their engagement\nlevel) and hypothesize natural language concepts that pre-\ndict the target variable (e.g., mentions being surprised or\nshocked or asks a question to the reader).\nHowever, basic hurdles impede the full usage of LLMs for\nhypothesis generation. Consider two natural approaches.\nThe first prompts an LLM with positive and negative ex-\namples and asks it to hypothesize concepts that distinguish\nthem (Zhou et al., 2024; Batista & Ross, 2024; Ludan et al.,\n2024). However, due to context window and reasoning limi-\ntations, we can only give LLMs a few training examples at\na time, making this hard to scale. A second approach aims\nto interpret a model fine-tuned to predict the target variable\n(Zhong et al., 2024), leveraging the full training dataset.\nHowever, neurons are often hard to interpret (Elhage et al.,\n2022). In summary, efficient and scalable learning of statis-\ntical relationships requires numerical representations of text\n(e.g., neurons), but interpreting neurons is hard.\nOur first contribution is a theoretical framework that con-\nnects model interpretation and hypothesis generation, clari-\nfying the challenge above. Proposition 3.1 gives a \"triangle\ninequality\" for hypothesis generation: if a neuron is predic-\ntive of the target variable, then a sufficiently high-fidelity\ninterpretation of the neuron is also predictive. This result di-\nrectly motivates a general hypothesis generation procedure:\n1. (Feature generation) Learn interpretable neurons (i.e.,\nthat tend to fire in the presence of human concepts).\n2. (Feature selection) Select neurons that predict the target\nvariable.\n3. (Feature interpretation) Generate high-fidelity natural\nlanguage interpretations of these neurons."}, {"title": "2. Related Work", "content": "2.1. Hypothesis Generation\nClassical text analysis methods, such as comparing n-\ngram frequencies between groups (Fightin' Words; Monroe\net al. (2009)) or topic modeling (LDA, Blei et al. (2003);\nBERTopic, Grootendorst (2022)) remain standard tools to\ndiscover relationships between text and target variables\n(Grimmer et al., 2022). A challenge with these methods\nis that their outputs-lists of words or documents are not\nimmediately interpretable by humans (Chang et al., 2009).\nAs a result, applying these methods for scientific discovery\nrequires sufficient domain expertise to prespecify hypothe-\nses and assess whether the data support them (Demszky\net al., 2019; Gentzkow et al., 2016; Sun et al., 2022).\nRecently, several LLM-based methods aim to automate in-"}, {"title": "2.2. Sparse Autoencoders", "content": "A recent line of work demonstrates that sparse autoencoders\n(Makhzani & Frey, 2014) are an effective architecture to\nlearn interpretable features from intermediate layers of large\nlanguage models (Cunningham et al., 2023; Bricken et al.,\n2023; Templeton et al., 2024; Bills et al., 2023). This litera-\nture is primarily motivated by mechanistic interpretability:\nsince the neurons in the hidden layer of sparse autoencoders\ncorrespond to interpretable features, they can be strategi-\ncally altered to steer the behavior of language models. In the\npresent work, we instead apply sparse autoencoders to learn\ninterpretable text features for the purpose of hypothesizing\ninterpretable relationships between text data and a target\nvariable. We follow O'Neill et al. (2024), who show that\nSAE features can guide semantic search applications, in\ntraining a sparse autoencoder directly on text embeddings."}, {"title": "3. Theoretical Framework", "content": "In this section, we derive a theoretical result that shows\nhow a language model with highly interpretable neurons\ncan be used for hypothesis generation, as well as where\nperformance loss arises in such a procedure.\nLet Z be an indicator for whether a neuron fires for a given\ntext input (i.e., whether the activation exceeds a threshold).\nLet 2 be an indicator for whether the text contains a speci-\nfied natural language concept. Let Y be the target variable.\nDefine the predictiveness of 2 as the separation score\n$S(\u017d) := |E[Y|2 = 1] \u2013 E[Y|\u017d = 0]|.$\nDefine S(Z) analogously. We measure the fidelity of the"}, {"title": "4. Methods", "content": "In the previous section, we established theoretically that we\ncan generate hypotheses by learning interpretable neurons,\nidentifying which neurons are predictive, and then gener-\nating high-fidelity interpretations of the neurons. We now\ndescribe how we can accomplish these steps in practice.\nWe now describe the details of HYPOTHESAEs. Given\na dataset of training examples {(xi, Yi)}i\u2208[N], where xi\nis the input text and yi is the target variable annotation,\nHYPOTHESAES outputs H natural language concepts that\nserve as hypotheses. The goal is for these natural language\nconcepts to predict the target variable."}, {"title": "4.1. Feature Generation: Training Sparse Autoencoders.", "content": "Let ei denote the text embedding of xi. We train a k-sparse\nautoencoder (Makhzani & Frey, 2014). k-sparse autoen-\ncoders have successfully generated interpretable features\nboth when applied to intermediate layers of language mod-\nels, as well as text embeddings (Gao et al., 2024; O'Neill\net al., 2024). The SAE encodes a text embedding ei as\nfollows (we use OpenAI's text-3-embedding-small in our\nexperiments):\n$Zi = TopK(Wencei + benc)$\n$\u00ea\u00bf = Wdecz + bdec,$\nwhere Wenc \u2208 RM\u00d7D, benc \u2208 RM, Wdec \u2208RD\u00d7M, bdec E\nIRD, and TopK sets all activations except the top k to zero.\nIn a basic k-sparse autoencoder, the loss for a single input\nis simply\n$L = ||ei - \u00eai||2.$\nWe follow (Gao et al., 2024; O'Neill et al., 2024) in further\nadding an auxillary loss to avoid \u201cdead latents.\" We describe\ndetails and hyperparameters in Appendix B.\nEmpirically, the dimensions of zi are often highly inter-\npretable, corresponding to human concepts. M sets the\ntotal number of concepts learned across the entire dataset,\nand k sets the number of concepts which can be used to\nreconstruct each instance. The output of this first step is an\nNX M activation matrix, ZSAE."}, {"title": "4.2. Feature Selection: Target Prediction with SAE Neurons.", "content": "To select a predictive subset of SAE neurons, we fit an\nL1-regularized linear or logistic regression predicting the\ntarget variable Y from the activation matrix ZSAE (Tibshi-\nrani, 1996). Formally, for regression\u00b3, we optimize\n$min L(\u03b2; 1) = || - ZSAEB||2 + 1||3||1,$\n\u03b2\nwhere y is a length-N vector, ZSAE is an N\u00d7M matrix, and\n\u1e9e is a length-M vector of feature coefficients for each SAE\nneuron. The L\u2081 penalty produces sparse coefficient vectors\n\u1e9e where some coefficients are exactly zero (indicating a\ndropped feature). To generate H hypotheses, we perform\nbinary search to identify a value of A which produces exactly\n\u0397 nonzero coefficients."}, {"title": "4.3. Feature Interpretation: Labeling Neurons with LLMs.", "content": "In this step, we generate high-fidelity interpretations (i.e.,\nnatural language concepts) of the subset of predictive neu-\nrons. To generate interpretations of a neuron, we prompt\nan LLM (GPT-40 in our experiments) with example texts\nfrom a range of neuron activations and instruct it to identify\na natural language concept that is present in high-activation\ntexts and absent in low-activation texts. We then generate\nmultiple interpretations by rerunning this procedure several\ntimes at temperature 0.7, and choose the best interpretation\naccording to a measure of fidelity we describe below.\nWe test a variety of approaches for generating interpretations.\nIn our final procedure, we sample high- and low-activating\ntexts from the neuron's positive activation distribution, with\nthe sampling bins given in Appendix B.2. (We restrict our\nsample to texts that activate the neuron to ensure some\nexamples contain the concept.) Treating these as true posi-\ntives and true negatives, respectively, we prompt an LLM\nto generate a natural language concept that distinguishes 10\nsamples of each class. We then define an interpretation's\nfidelity as its F1 score in this prediction task, using an LLM\n(GPT-40-mini in our experiments) to annotate 100 positive\nand 100 negative samples for the presence of the concept.\nThis approach produced predictive interpretations when\ncompared to other prompting and evaluation schemes,\nthough some other methods performed similarly (see Ap-\npendix C for details). Importantly, we found experimentally\nthat higher-fidelity interpretations according to our mea-\nsure have better predictive performance (Appendix C.1),\njustifying our approach to selecting interpretations."}, {"title": "5. Experiments", "content": "5.1. Datasets\nWe evaluate on synthetic and real-world datasets, described\nbelow. Appendix A provides more details on all datasets.\nSynthetic datasets: recovering known, reference hy-\npotheses. Our synthetic evaluation is motivated by real-\nworld settings in which there are multiple disjoint hypothe-\nses we would like to discover. We therefore use two datasets\nfrom prior work on interpretable clustering (Pham et al.,\n2024; Zhong et al., 2024): WIKI and BILLS. We construct\na target variable that is positive if a text belongs to one of\na pre-specified list of reference categories; these are the\nground-truth \"hypotheses\u201d to recover. Both datasets are\nlabeled by humans with granular hierarchical topics, such\nas Media and Drama: Television: The Simpsons in WIKI\nand Macroeconomics: Tax Code in BILLS.\nWe generate ground-truth labels based on the most frequent\ngranular topics: articles in any of the top-5 topics are labeled\n'1', and all others are labeled '0'. This produces 5 reference\nhypotheses, e.g., the bill is about Macroeconomics: Tax\nCode is one of them. For WIKI, we also include a more\nchallenging variation of recovering the top-15 most frequent\ntopics. WIKI contains 11,979 total articles (15% positive\nfor WIKI-5; 35% for WIKI-15), and BILLS contains 21,149\n(24% positive). For both datasets, we reserve 2,000 items for\nvalidation (i.e., SAE hyperparameter selection) and 2,000\nheldout items to evaluate hypotheses; we use the remaining\nitems for SAE training and feature selection.\nReal-world datasets: Generating hypotheses that predict\na target variable. We apply HYPOTHESAEs to three real-\nworld datasets which have been studied in prior work:\n\u2022 HEADLINES (Matias et al., 2021): Which features of\ndigital news headlines predict user engagement? Each\ninstance is a pair of differently-phrased headlines for\nthe same article; users on Upworthy.com were randomly\nshown one, and the task is to identify which headline re-\nceived higher click-rate. We reserve a large heldout set to\nimprove statistical power: our split sizes are 8.8K training,\n1K validation, and 4.4K heldout.\n\u2022 YELP (Yelp, 2024): Which features of Yelp restaurant\nreviews predict users' 1-5 star ratings? We use 200K\nreviews for training, 10K for validation, and 10K for\nheldout eval.\n\u2022 CONGRESS (Gentzkow & Shapiro, 2010): Which features\nof U.S. congressional speeches predict party affiliation?\nSpeeches are from the 109th Congress (2005\u201307) with\nbinary labels (Rep. or Dem.). Our split sizes are 114K\ntraining, 16K validation, and 12K heldout."}, {"title": "5.2. Metrics", "content": "Synthetic datasets. On the synthetic datasets, we eval-\nuate how well we recover the reference hypotheses. We\nrun HYPOTHESAES by setting H to the number of refer-\nences (e.g., 5 for WIKI-5). We use GPT-40-mini to annotate\neach inferred hypothesis on the heldout set, producing an\nNheldout \u00d7 H binary matrix of annotations. Following Zhong\net al. (2024), we compute the optimal matching between ref-\nerence and inferred hypotheses via the Hungarian algorithm\non the H \u00d7 H correlation matrix of reference vs. inferred\nhypothesis annotations, and we report three metrics:\n\u2022 F1 Similarity: For each matched (reference, inferred)\npair, we compute the F1-score between the presence of the\nground-truth reference topic and the inferred hypothesis\nannotations. We report the mean across the H pairs.\n\u2022 Surface Similarity: For each matched pair, we prompt\nGPT-40 to assess whether the hypotheses are the same,\nrelated, or distinct, with corresponding scores of 1.0, 0.5,\n0.0. To improve stability, we sample 5 outputs at tempera-\nture 0.7 and average them. We report the mean value of\nthis score across the H pairs. (Table 4 shows examples;\nFigure 5 provides the prompt.)\n\u2022 Overall AUC: Using annotations from the H inferred hy-\npotheses, we fit a multivariate logit and compute AUC to\nevaluate predictive performance. (Recall that the ground-\ntruth label is a logical OR of the reference topics.)\nThe first and second metrics are taken directly from Zhong\net al. (2024); they assess whether the inferred hypotheses\nqualitatively and quantitatively match the ground-truth. The\nthird metric assesses overall prediction quality. Some hy-\npotheses may not match a reference but still be predictive,\nso this metric rewards any interpretable hypotheses which\ncontribute predictive value, even if they are not optimal.\nReal datasets. On the real datasets, we generate 20 hy-\npotheses per method, and assess the hypothesis sets for two\nqualities: (1) breadth: how many distinct, predictive hy-\npotheses were identified? (2) predictiveness: how well do\nthe hypotheses collectively explain the label? We again com-\npute an Nheldout \u00d7 H annotation matrix for each method's\nhypotheses, and measure these constructs as follows:\n\u2022 Breadth: We fit a multivariate logit (for binary labels) or\nOLS (continuous) regressing the target variable against"}, {"title": "6. Results", "content": "6.1. Synthetic Datasets\nHYPOTHESAES recovers ground-truth hypotheses. Ta-\nble 2 shows quantitative metrics: HYPOTHESAES beats all\nbaselines on 8 of 9 dataset-metric pairs. As captured by sur-\nface similarity, most of HYPOTHESAES's inferred hypothe-\nses either perfectly match or are related to the references.\nOn WIKI-5 (Table 4), we retrieve two hypotheses exactly;\nthe other three are slightly too specific (HYPOTHESAES\ninfers a topic is about a New York State Route or highway,\nbut the reference includes all Northeastern roads, not just\nNew York) or broad (it infers historical battles but does\nnot specify battles since 1800), but all inferred hypotheses\nare always related to a reference, i.e., has surface similarity\n> 0.5. No baselines meet this standard on WIKI-5: NL-\nPARAM and HYPOGENIC output 3/5 and 2/5 hypotheses\nthat are unrelated to any of the reference topics, respectively.\nBERTOPIC performs well, but outputs redundant hypothe-\nses about state highways and misses 'battles' entirely.\nOur hypotheses also closely match ground-truth when used\nto annotate heldout data. The mean F1 score compares\nhow closely annotations according to an inferred hypothesis\n(e.g. is about a Simpsons episode) match ground-truth labels\n(whether the WIKI article belongs to the human-annotated\nSimpsons category). HYPOTHESAES beats all baselines\non this metric, especially outperforming NLPARAM (+0.36,\non average) and HYPOGENIC (+0.40). Finally, regressing\nthe label using each method's annotations, HYPOTHESAES\nachieves higher AUC than all baselines. Overall, HYPOTHE-\nSAES usually outperforms BERTOPIC-which is designed"}, {"title": "6.2. Real-World Datasets", "content": "Table 2 shows quantitative metrics for the real-world\ndatasets. Tables 1, 5, and 6 show all significant hypotheses\non HEADLINES, CONGRESS, and YELP respectively.\nHYPOTHESAEs identifies more distinct hypotheses than\nother methods. Compared to baselines, HYPOTHESAES\nproduces the most hypotheses which are significant in a\nmultivariate regression. Importantly, this evaluation reflects\nsuccessful generalization: the hypotheses are learned on\nthe training and validation sets, and they remain signifi-\ncantly associated with the target variable when GPT-40-\nmini annotates them on the heldout set. Across the three\ndatasets, 45 out of 60 candidate hypotheses are significant,\nsubstantially ahead of the 24, 23, and 20 for NLPARAM,\nBERTOPIC, and HYPOGENIC respectively.\nWhen evaluating overall predictive performance, HYPOTHE-\nSAES beats baselines on 8 of 9 comparisons, demonstrating\nstrongly predictive hypotheses. The exception is that HY-\nPOGENIC achieves 2% higher R2 on Yelp. However, HY-\nPOGENIC's most predictive hypotheses, such as expresses\ndisappointment with the overall dining experience, essen-\ntially restate the rating prediction task. While LLMs can\nperform this prediction well, these hypotheses do not reveal\nspecific insights for the task. In contrast, our hypotheses-in\nTable 6 for YELP-tend to be more specific: we find that\nnegative reviews mention food poisoning, rude and aggres-\nsive staff, or dishonest business practices."}, {"title": "6.3. Evaluating Hypothesis Novelty", "content": "Beyond quantitative comparisons against baselines, we eval-\nuate if HYPOTHESAES discovers new hypotheses even on\ndatasets that have been thoroughly analyzed in prior work.\nCONGRESS. A classic study from Gentzkow et al. (2016)\nidentifies bigrams and trigrams in Congressional speeches\nthat mark Republican (R) or Democrat (D) speakers. We\ncompare to their work by computing counts, in each speech,"}, {"title": "6.4. Costs", "content": "In Table 3, we report the runtimes, LLM inference to-\nken counts, and costs (at current OpenAI API pricing)\nfor all methods on CONGRESS; the trends are similar for\nall datasets. Runtimes include training the SAEs on one\nNVIDIA A6000 GPU. This step is fast because our SAEs\ncan afford to be small (~10\u00b3 features, compared to 107\nin Gao et al. (2024)), since our focus is to learn domain-\nspecific features rather than features of all language. HY-\nPOTHESAES is substantially cheaper than NLPARAM and\nHYPOGENIC: despite the fact that the latter methods are\ntrained on 20K examples on CONGRESS (vs. 114K for HY-\nPOTHESAES), HYPOTHESAES is ~30-50\u00d7 faster and\n10-50\u00d7 cheaper. This is because HYPOTHESAES's annota-\ntion requirement scales only with the number of hypotheses\nH, while these two methods scale with both H and N.\nBERTOPIC is cheapest, and remains a good baseline for a\nresource-limited analysis. However, HYPOTHESAES can\nalso be made cheaper by skipping the non-essential label\nvalidation step: instead of generating 3 candidate labels\nper neuron and choosing the highest-fidelity one, we can\nsimply generate 1 label per neuron and use it directly. This\nreduces cost to that of BERTOPIC, while still exceeding its\nperformance; we describe this ablation in Appendix B.2."}, {"title": "7. Conclusion", "content": "We propose HYPOTHESAES, a scalable method to gener-\nate interpretable hypotheses from black-box representations.\nOn three synthetic and three real-world social science tasks,\nHYPOTHESAES outperforms a topic model and state-of-\nthe-art LLM baselines, at less than 10x the time and cost\nof the latter. The method applies naturally to many tasks in\nsocial science (Ziems et al., 2024), healthcare (Hsu et al.,\n2023; Robitschek et al., 2025; Pierson et al., 2025), and\nbiology (Vig et al., 2021). More generally, there are many\napplications where we can accurately predict a target vari-\nable, but producing new scientific insights remains difficult.\nOur work helps bridge this gap."}, {"title": "B. Hyperparameters", "content": "B.1. SAE Training\nTo train SAEs, we take all hyperparameters besides M and k from O'Neill et al. (2024). We tune (M, k) on each dataset by\nmaximizing validation performance. Specifically, for each combination of (M, k), we evaluate validation AUC (or R2 on\nYELP) after training an L\u2081-regularized predictor with exactly H nonzero coefficients. We constrain M and k to powers of\ntwo to tractably limit the search space.\nThese parameters, broadly, tune the level of granularity of the concepts learned by each SAE neuron: M is the total number\nof concepts which can be learned across the entire dataset, and k is the number of concepts which can be used to represent\neach instance. For intuition, assume that with (M, k) = (16,4) on YELP, the SAE learns a single neuron that fires when\nreviews mention price; with (M, k) = (32, 4), the SAE may learn separate neurons for reviews which mention high prices\nvs. low prices; with (M, k) = (32, 8), the SAE may learn to represent more niche features altogether, such as the number of\nexclamation marks. In general, increasing either parameter yields more granular features, at the risk of producing some\nneurons that are uninterpretable or redundant.\nIn some cases we would like features at multiple levels of granularity: for example, we may want both the general feature\n\"mentions prices\" and the more specific feature \u201cmentions that the cocktails were cheap during happy hour\", but training a\nsingle large SAE (e.g. (1024, 32)) may only produce features as specific as the latter. A simple solution is to train multiple\nSAEs of different sizes, concatenate their activation matrices, and select features from any of them. We find that this\napproach is helpful (improves validation AUC) on the HEADLINES dataset. Ultimately, we use the following values:\n\u2022 WIKI-5: (32, 4).\n\u2022 WIKI-15 & BILLS: (64, 4).\n\u2022 HEADLINES: (256, 8); (32, 4).\n\u2022 YELP: (1024, 32).\n\u2022 CONGRESS: (4096, 32).\nOther hyperparameters are as follows (identical for all experiments):\n\u2022 Auxilliary k: 2k. This parameter is the number of latents for which gradients are computed. That is, we compute\na secondary reconstruction of the input using not just the top k latents but the top auxK > k latents, and include this\nauxiliary reconstruction's error in the loss. This addition helps reduce \"dead\" latents (neurons which never fire).\n\u2022 Auxilliary loss coefficient: 1/32. This parameter is the weight in the loss for the auxiliary reconstruction. (With this term,\nthe full loss is: L = ||e - \u00eatopK||2 + Waux ||e - \u0108auxK||3.)\n\u2022 Batch size: 512; learning rate: 5e-4; gradient clipping threshold: 1.0.\n\u2022 Epochs: Up to 200, with early stopping after 5 epochs of validation loss not decreasing.\""}, {"title": "B.2. Neuron interpretation", "content": "There are several parameters for neuron interpretation which we fix across experiments:\n\u2022 Interpretation model: GPT-40 (version 2024-11-20).\n\u2022 Temperature: 0.7.\n\u2022 Number of highly-activating examples: 10.\n\u2022 Number of weakly-activating examples: 10.\n\u2022 Maximum word count per example: 256 (examples longer than this are truncated)."}, {"title": "C. Supporting Experiments", "content": "Our final neuron interpretation procedure for our main experiments is described in \u00a74, with hyperparameters provided\nin Appendix B.2. Here, we describe several experiments to justify our procedure. The section is organized as follows.\nIn C.1, we show that our metric of interpretation fidelity-the F1 score comparing concept annotations against neuron\nactivations is a useful selection metric to improve downstream hypothesis quality. In C.2, we test several different\ninterpretation strategies and hyperparameters and measure their impacts on fidelity. In C.3, we analyze how much prediction\nsignal is lost at each stage of our method, shedding light on how future work can improve hypothesis quality."}, {"title": "C.1. Higher-fidelity interpretations improve hypotheses.", "content": "To test whether fidelity is a useful metric to select from different neuron interpretations, we conduct the following experiment\non the Yelp dataset:\n1. Train an SAE on Yelp (Step 1), and identify the top-20 predictive neurons via Lasso (Step 2).\n2. Generate 10 candidate interpretations (at temperature 0.7, and with different random samples in the interpretation\nprompt) for each neuron using the hyperparameters in B.2 (Step 3).\n3. Compute the fidelity for each candidate interpretation: that is, binarizing 100 examples in the top decile of the neuron's\npositive activations as positives, and 100 examples in the bottom decile of the neuron's positive activations as negatives,\ncompute the F1 score between the GPT-40-mini annotations according to the interpretation against the binarized neuron\nactivations."}, {"title": "C.2. Effects of the interpretation procedure on fidelity.", "content": "Having established that improving interpretation fidelity improves downstream predictiveness, we search across different\nhyperparameters for neuron interpretation to maximize fidelity. Our final procedure is to generate 3 candidate interpretations\nfor each neuron by prompting GPT-40 with 10 examples in the top decile of positive activations vs. 10 examples in the\nbottom decile of positive activations. We use the highest fidelity interpretation out of these 3. There are two sources of\nnon-determinism: each candidate interpretation is generated with a different random seed to sample examples, and we use\nan LLM temperature of 0.7.\nTo compare this strategy against other strategies or to compare strategy A and strategy B more generally-we compute\ninterpretations, and their F1 scores, for the top 100 neurons on YELP under both strategies separately. We run a t-test\ncomparing the pairs of resulting F1 scores to test whether one strategy produces consistently higher F1 scores than the other.\nWe observe the following:\n\u2022 Number of candidate interpretations: Relative to baseline (generating 3 candidate interpretations and taking the one\nwith highest F1 score), generating only a single candidate interpretation is significantly worse (-0.04). Generating 5\ncandidate interpretations improves significantly on 3 (+0.01 F1, paired t-test p = 0.003), but the improvement is modest\nand results in a slower, more expensive method. Therefore, we use only 3 candidate interpretations.\n\u2022 Top-random vs. high-weak sampling strategy: Prior work (e.g. Templeton et al. (2024); Bills et al. (2023)) interprets\nSAE neurons by prompting with the top-activating texts and random texts. Relative to our baseline (prompting with\nhighly-activating texts, but not necessarily top-activating; and weakly-activating texts instead of random), this alternate\napproach has no significant effect on F1 score, though the interpretations tend to skew towards higher precision and lower\nrecall (because the top-activating examples are more specific than examples in the top decile). Our interpretations tend to\nhave similar values of precision and recall, so we use them instead; but this is a hyperparameter practitioners can change\ndepending on their needs.\n\u2022 Continuous vs. binary prompt structure: Prior work (e.g. Zhong et al. (2024)) prompts the interpreter LLM with a\nsorted list of texts and their continuous activations. Relative to our baseline (showing separate lists of positive samples\nand negative samples), this significantly lowers F1 score (-0.06), so we use the binary prompt structure.\n\u2022 Chain-of-thought: Prior work (e.g. O'Neill et al. (2024)) uses a chain-of-thought reasoning prompt to interpret neurons,\nwhere the LLM is encouraged to iteratively discover a concept which all of the positive samples contain and none of the\nnegative samples contain. Relative to baseline, we found that CoT has an insignificant but weakly negative effect (-0.03\nmean F1), and also sometimes fails to output any interpretation. However, this is still a promising line for future work: it's\npossible that while GPT-40 does not reason well, better models or reasoning-specific models (OpenAI 01, DeepSeek R1,\netc.) will produce higher fidelity labels with chain-of-thought.\n\u2022 Sample count: Relative to baseline (prompting with 20 total samples, 10 highly-activating and 10 weakly-activating),\nprompting with 10 total samples lowers average F1 score by 0.03. Prompting with 50 samples has no effect, so we use 20\nto favor shorter prompts.\n\u2022 Temperature: Relative to baseline (temperature 0.7), using a lower (0.0) or higher (1.0) temperature has an insignificant\nbut weakly negative effect (-0.01 in mean F1), so we use 0.7."}, {"title": "C.3. Localizing losses in predictive performance to different steps of the method.", "content": "HYPOTHESAES starts from embeddings and ultimately produces hypotheses", "improvements": "if the\nSAE's full hidden representation is much less predictive of restaurant ratings than the input text embeddings, perhaps we\nneed a larger (more expressive) SAE, or more data to train the SAE's representations; meanwhile, if the largest performance\nloss comes from translating neurons into natural language hypotheses, perhaps we need a stronger LLM for interpretation.\nFigure 3 displays this experiment. At each stage, we fit a logistic or linear regression on the training set to predict the labels\non"}]}