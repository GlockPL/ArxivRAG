{"title": "DPU: Dynamic Prototype Updating for Multimodal Out-of-Distribution Detection", "authors": ["Shawn Li", "Huixian Gong", "Hao Dong", "Tiankai Yang", "Zhengzhong Tu", "Yue Zhao"], "abstract": "Out-of-distribution (OOD) detection is crucial for ensuring the robustness of machine learning models by identifying samples that deviate from the training distribution. While traditional OOD detection has predominantly focused on single-modality inputs, such as images, recent advancements in multimodal models have shown the potential of utilizing multiple modalities (e.g., video, optical flow, audio) to improve detection performance. However, existing approaches often neglect intra-class variability within in-distribution (ID) data, assuming that samples of the same class are perfectly cohesive and consistent. This assumption can lead to performance degradation, especially when prediction discrepancies are indiscriminately amplified across all samples. To address this issue, we propose Dynamic Prototype Updating (DPU), a novel plug-and-play framework for multimodal OOD detection that accounts for intra-class variations. Our method dynamically updates class center representations for each class by measuring the variance of similar samples within each batch, enabling tailored adjustments. This approach allows us to intensify prediction discrepancies based on the updated class centers, thereby enhancing the model's robustness and generalization across different modalities. Extensive experiments on two tasks, five datasets, and nine base OOD algorithms demonstrate that DPU significantly improves OOD detection performances, setting a new state-of-the-art in multimodal OOD detection, including improvements up to 80% in Far-OOD detection. To improve accessibility and reproducibility, our code is released at https://github.com/lili0415/DPU-OOD-Detection.", "sections": [{"title": "1. Introduction", "content": "Out-of-distribution (OOD) detection aims to identify samples that differ from the in-distribution (ID) data in ways that challenge the model's ability to generalize [2, 14, 16, 25, 36]. It is crucial for enhancing the safety and robustness of machine learning models across various domains, such as autonomous driving [8, 26], medical imaging [19], robotics [3, 9], and other applications [4, 12, 15, 21, 28, 42, 43, 48, 50]. In recent years, numerous OOD detection algorithms have been developed, spanning classification-based and distance-based methods [7, 17, 30, 31, 40, 49]. Traditionally, OOD detection has focused on single-modality inputs, such as images or videos. With the emergence of large Vision-Language Models [37, 52], researchers are exploring OOD detection with the assistance of language modalities [29, 33, 46]. However, their evaluations remain limited to benchmarks containing only images. Effectively leveraging multimodal features (e.g., video, optical flow, and audio) remains an open challenge, requiring further research.\nCurrent Work. Dong et al. [11] introduced the first multimodal OOD benchmark and framework, identifying the phenomenon of modality prediction discrepancy. This phenomenon reveals that softmax prediction discrepancies across different modalities are negligible for ID data but significant for OOD data. By amplifying this prediction discrepancy during training, they observed improvements in OOD detection performance. However, a key assumption in previous multimodal OOD detection studies is that all samples within a given class are entirely in-distribution [11, 49], implying perfect cohesion among samples within the same class. This assumption rarely holds true in real-world applications, where intra-class variability is common. As a result, applying uniform discrepancy intensification across all training samples can degrade the model's ID prediction accuracy [11]. When the discrepancy is intensified on class-center samples-typically exhibit consistent predictions across all modalities-this consistency is disrupted, causing significant model confusion and performance degradation, as shown in Appx. C.4 case study.\nOur Proposal. To tackle the challenge of intra-class variations in existing multimodal OOD detection methods, we introduce a novel approach called Dynamic Prototype Updating (DPU). DPU dynamically adjusts the multimodal prediction discrepancy to ensure high intra-class cohesion and clear inter-class separation, leveraging instance-level training invariance. The core idea of DPU is to update the prototype representations [24, 27] of each class at a dynamic, sample-specific rate, resulting in more precise and robust model performance. These prototype representations act as central reference points, capturing the key features of each class (see Fig. 2 for an overview). To establish a reliable representation space, we first introduce the Cohesive-Separate Contrastive Training procedure (\u00a73.3), which applies marginal contrastive learning to strengthen intra-class cohesion while preserving distinctions between individual samples. Building on this, we design the Dynamic Prototype Approximation mechanism (\u00a73.4), which adaptively refines prototype representations based on observed sample variances. This adaptive updating helps mitigate the negative impact of outliers on prototype evolution, stabilizing the learning process. Using these refined prototypes, we further adjust the multimodal prediction discrepancy for each sample according to its similarity to its class prototype (\u00a73.5). Finally, OOD models make predictions by leveraging both the joint probability distribution across all modalities and the distinct information from each modality. In summary, we make the following contributions:\n\u2022 New Observations in Multimodal OOD Detection. We are the first to identify and explore the negative impact of intra-class variations within ID data for OOD detection.\n\u2022 Novel, Model-Agnostic Framework. We propose DPU, a flexible, plug-and-play method that effectively handles intra-class variations and is compatible with various existing OOD detection models. As shown in Fig. 1, DPU enhances performance across various OOD methods.\n\u2022 Effectiveness. Comprehensive experiments demonstrate the effectiveness of the proposed method across two tasks, five datasets, and nine base OOD methods. DPU significantly improves the performance of all benchmark models, achieving new state-of-the-art results, including improvements of around 10% across all metrics for Near-OOD detection and up to 80% for Far-OOD detection."}, {"title": "2. Related Work (Extended Ver. in Appx. A)", "content": "OOD Detection. OOD detection identifies samples that deviate from the training distribution while preserving ID accuracy. Key methods include post hoc techniques such as Maximum Softmax Probability (MSP) [16] and enhancements like temperature scaling [30], along with activation-modifying techniques (e.g., ReAct [40]) and distance-based approaches (e.g., Mahalanobis [23]). Recent hybrid methods, including VIM [45], integrate features and logits, while our method uniquely addresses intra-class variability in ID data to enhance existing base OOD detection methods.\nMultimodal OOD Detection. Recent efforts extend OOD detection to multimodal contexts, particularly in vision-language systems [33, 46]. Approaches like Maximum Concept Matching (MCM) [33] focus on aligning features to determine OOD scores. Additionally, Dong et al. [11] introduced a new multimodal OOD benchmark that includes video, optical flow, and audio, and revealed prediction discrepancies across different modalities. However, these methods (used as baselines for comparisons in \u00a74) often overlook the rich contextual information provided by modalities, which our dynamic DPU framework leverages."}, {"title": "3. Proposed DPU Framework", "content": "3.1. Problem Statement and Preliminaries\nGiven a training set $D = \\{(x_i, y_i)\\}_{i=1}^N$, where each $x_i \\in X$ is an input sample and $y_i \\in Y = \\{1, 2, ..., C\\}$ is a class label, OOD detection aims to distinguish between ID and OOD samples. OOD samples exhibit semantic shifts compared to ID samples and do not belong to any class in $Y$. The model employs a feature extractor $g(\\cdot)$ to obtain features and a classifier $h(\\cdot)$ to generate predictions, yielding probability output $\\hat{y}$. When the input data includes multiple modalities, we extend this framework to multimodal OOD detection, which we define as follows:\nProblem 1 (Multimodal OOD Detection) Each training sample $x_i$ consists of $M$ modalities, denoted as $x_i = \\{x_i^k | k = 1,..., M\\}$. Multimodal OOD detection combines information from all modalities to make predictions.\nThe marginal distribution of ID data is denoted as $P_{in}$, while OOD samples encountered during testing are drawn from a different marginal distribution $P_{out}$. The objective of OOD detection is to construct a decision function $G$ that classifies each test sample $x \\in X$ as ID or OOD:\n$G(x; g, h) = \\begin{cases} 0 & \\text{if } x \\sim D_{out}, \\\\ 1 & \\text{if } x \\sim D_{in}. \\end{cases}$\nKey Challenges in Multimodal OOD Detection. In multimodal OOD detection, previous research has primarily focused on post-processing techniques applied to logit probabilities [16, 17, 40], with limited emphasis on optimizing the embedding space where learning occurs. While multimodal data offers potential synergy by allowing different modalities to complement each other, how best to leverage this synergy remains an open question.\nAn essential yet often overlooked challenge is managing the natural intra-class variations within multimodal ID data. In real-world applications, samples within the same class may exhibit diverse patterns across different modalities, introducing significant variability. The current leading approach, which uniformly amplifies prediction discrepancies across all samples [11], risks confusing those samples close to the class center, potentially degrading model performance by weakening class cohesion."}, {"title": "3.2. Overview of DPU", "content": "To address the challenge above, it is key to balance intra-class cohesion with inter-class separation. We propose a novel approach that dynamically scales the intensification of prediction discrepancies based on each sample's similarity to its class prototype. This adaptive strategy allows samples near the class center to maintain low discrepancy, preserving cohesive predictions, while more distant samples experience higher discrepancy to increase the model's sensitivity to outlying patterns. This forms the foundation of our proposed DPU. As shown in Fig. 2, DPU integrates three key components to effectively handle the unique challenges of multimodal OOD detection. The primary objective is to create a stable representation space with strong intra-class cohesion and distinct inter-class separation, enabling adaptive discrepancy intensification and enhancing the model's predictive accuracy across modalities.\nAs shown in the flowchart, Fig. 2, the first component, Cohesive-Separate Contrastive Training (CSCT) (\u00a73.3, Step 1 in the figure), enhances intra-class consistency while maintaining clear distinctions between classes, improving the model's generalization across data distributions. However, class outliers and noise can interfere with prototype representations, reducing reliability. To mitigate this, we introduce Dynamic Prototype Approximation (DPA) (\u00a73.4, Step 2 in the figure), which adaptively updates class prototypes by weighting samples according to their similarity to the prototype, ensuring prototypes remain representative of each class. Additionally, we incorporate Pro-ratio Discrepancy Intensification (PDI) and adaptive outlier synthesis (\u00a73.5, Step 3 in the figure) to enhance the model's ability to distinguish ID from OOD samples. These components adjust prediction discrepancies across modalities, strengthening the model's detection performance. Finally, DPU leverages both the joint probability distribution across all modalities and the unique information from each modality to make robust ID/OOD predictions."}, {"title": "3.3. Step 1: Cohesive-Separate Contrastive Training (CSCT) for Intra-class Cohesion", "content": "Motivation. In multimodal OOD detection, it is important to maintain strong intra-class cohesion ensuring that samples from the same class are represented consistently-while still capturing subtle variations within each class [1, 5]. Meanwhile, clear separation between different classes is essential for effective differentiation in the learned representation space. Balancing these objectives is key to achieving robust performance across different modalities.\nInspired by Arjovsky et al. [1], we employ the invariant risk minimization paradigm to (i) construct a representation space that is both intra-class cohesive and inter-class separated, and (ii) minimize and measure class-wise variances within batches. Specifically, we define a variant representation function $\\Phi : X \\rightarrow H$ that elicits an invariant predictor $\\omega \\circ \\Phi : H \\rightarrow Y$ across a set of positive environments $\\varepsilon$. This predictor is invariant if the classifier $w$ is optimal for all samples within the set. The learning objective is:\n$\\min_{\\omega, \\Phi} \\sum_{e \\in \\varepsilon} R^e(\\omega \\circ \\Phi)$,\ns.t. $w \\in \\arg \\min_\\omega R^e(\\Phi), \\forall e \\in \\varepsilon$.\nFollowing [1], this objective can be instantiated as:\n$L_{csct}(\\omega, \\Phi) = \\sum_{e \\in \\varepsilon} R^e(\\Phi) + \\lambda D(\\omega, \\Phi, e)$,\nwhere $R^e(\\cdot)$ is an empirical-based loss function, $D(\\cdot)$ is a parameterization of invariant risks, and $\\lambda$ is a hyperparameter (HP) controlling the balance between objectives. Eq. (2) aims to learn a feature representation $\\Phi(\\cdot)$ that induces a classifier $\\omega(\\cdot)$, which remains optimal across all samples in the positive environments $e \\in \\varepsilon$.\nTo implement Eq. (2) in our scenario, we use a two-part strategy. First, we apply robust marginal contrastive learning to construct a representation space that is both cohesive within each class and well-separated between classes. This approach aligns naturally with the goals of contrastive learning, which is designed to bring similar samples closer together while pushing dissimilar samples further apart [35]. Second, we measure and minimize class-wise variances within batches to capture subtle intra-class variations and ensure consistency in the learned representations. Together, these steps enable the model to maintain meaningful inter-class distinctions while stabilizing intra-class representations, creating a robust feature space.\nRobust Marginal Contrastive Learning. Consider a batch of video samples $V_1, V_2,..., V_n$ and their corresponding data from other modalities, $M_1^1, M_2^1,..., M_k^i$, where $k \\in \\{1,..., M\\}$ represents the modality type, and n is the batch size. For each video sample $V_j$, we define a positive set $PS_j$ consisting of samples from the same class and a negative set $NG_j$ composed of samples from different classes.\nTo enhance robustness and sensitivity to inter-class distinctions, we start with the standard InfoNCE loss, a popular contrastive learning objective [35], which is defined as:\n$L_{rmcl} = \\sum_{j=1}^n -\\log \\frac{f_{pos}}{f_{pos} + f_{neg}}$\nwhere $f_{pos}$ denotes the similarity scores for samples in the positive set $PS_j$, and $f_{neg}$ represents the similarity scores for samples in the negative set $NG_j$.\nWe compute the similarity scores for positive samples using the arc-cosine function, which allows us to effectively measure the angular distance between representations. We then add an angular margin $m$ to further enhance robustness by emphasizing inter-class distinctions. This adjustment helps to accentuate the differences among classes, improving the model's sensitivity to subtle variations within each class. We defer the details of the modifications to the contrastive loss to Appx. B.1. While contrastive learning can effectively separate different classes, capturing nuanced variations remains challenging, highlighting the need for our refined approach.\nVariance Representation Control. To ensure stable intra-class representations, we introduce an invariant representation regularization term. This regularization encourages invariant representations $\\Phi(\\cdot)$ to yield consistent prediction distributions across all environments $e \\in \\varepsilon$. We define it as:\n$L_{irm} = \\sum_{j=1}^n Var(L_i)$,\nwhere $L_i = \\{L_{rmcl}(j) | j \\in PS_j\\}$ is the set of loss values for samples within the same positive set. By minimizing the variance of these loss values, the model achieves stable and consistent representation learning for each class [1].\nCombining the objectives from robust marginal contrastive learning and variance representation control, we implement CSCT loss in Eq. (2) as:\n$L_{csct} = L_{rmcl} + \\lambda \\cdot L_{irm}$,\nwhere $\\lambda$ balances the influence of variance minimization. CSCT constructs a representation space that is cohesive within classes and distinct across classes, leveraging robust marginal contrastive learning and variance control. In the next step, we build on this representation space to dynamically update prototype representations for each class."}, {"title": "3.4. Step 2: Dynamic Prototype Approximation", "content": "Motivation. Building on the representation space created through CSCT, a dynamic approach to prototype learning is essential for capturing subtle intra-class variations and enhancing class representations. By updating prototypes based on sample-wise similarity, we reduce the influence of outliers, allowing each prototype to more accurately represent its class's central features.\nUnlike conventional methods that equally weight all samples within a class, we adjust prototypes according to the variance within each batch, enabling more representative and adaptive prototypes. Thus, We define the total prototype space as $Pty_\\chi \\in \\mathbb{D}^{L \\times Q}$, where L is the feature dimension, Q is the number of classes, and $k \\in \\{1, ...,M\\}$ denotes the modality. The prototype space is updated dynamically by variance observed during the CSCT process. For a given batch S, we construct positive sets $PS = \\{ps_1, ps_2,..., ps_p\\}$, where each set corresponds to a unique class label, and P is the number of distinct class labels in S. For each positive set in PS with class label y, we calculate the average representation embedding:\n$\\mathbb{H}^yk = \\frac{1}{N_y} \\sum_{i=1}^{N_y} F^k_i$\nwhere $N_y$ is the number of samples with label y in batch S, and $F^k$ represents the embedding (in modality k) for sample i. This average embedding serves as a central representation for all samples with label y in the batch.\nUsing this average embedding, we update the prototype for each class y through a moving average approach:\n$P_{ty}^k = P_{ty}^{k1} + (1 - \\beta) \\cdot \\frac{1}{\\sqrt{Var(L^y)} N_y} \\cdot (\\mathbb{H}_y^k - P_{ty}^k)$,\nwhere $\\beta$ and $\\eta$ are HPs that control the update rate and influence of variance, allowing prototypes to adapt to each batch. The update rate is higher when the variance $Var(L^y)$ is lower or when the batch size $N_y$ is smaller, enabling faster adaptation when the class representations are more consistent or when fewer samples are available."}, {"title": "3.5. Step 3: Pro-ratio Discrepancy Intensification", "content": "Motivation. In multimodal OOD detection, differences in predictions across various modalities are a strong signal for distinguishing ID samples from OOD samples [11]. The dynamic prototypes we have learned provide a basis for selectively amplifying these prediction discrepancies, enabling the model to more effectively detect OOD samples by emphasizing the separation between ID and OOD data.\nTo achieve this adaptive intensification, we define an intensification rate for each sample i with class label y, which scales the discrepancy by the similarity between the sample's feature representation and its class prototype:\n$L_{pdi} = -\\mu \\cdot (1 - Sigmoid(F_i^k \\cdot (P_{ty}^k))) Discr(p^{k_1}, p^{k_2})$,\nwhere $\\mu$ is an HP that controls the intensity, $F_u$ is sample's video feature embedding, $Sigmoid(\\cdot)$ modulates its similarity to the class prototype, $p^{k_1}$ and $p^{k_2}$ denote prediction probabilities of different modalities of sample i, and $Discr(\\cdot)$ represents a distance metric (we use the Hellinger distance following [11]) that quantifies the discrepancy between probability distributions across modalities.\nAdaptive Outlier Synthesis. To further improve DPU's OOD detection capability, we apply an adaptive outlier synthesis technique. The goal of this technique is to generate synthetic samples that help the model learn to distinguish between ID and OOD data with greater accuracy. Specifically, we create synthetic outliers by fusing prototypes from different modalities and classes.\nFor each class, we concatenate its prototypes across different modalities, denoted as $P = P^{ty_{i1}} \\bigoplus P^{ty_{i2}}$, where 11, 12 \u2208 {video, flow, audio}, and $\\bigoplus$ represents concatenation. To form a synthetic outlier, we select a class prototype $P_{y_1}$ and choose another prototype $P_{y_2}$ from the top K nearest prototypes of $P_{y_1}$. We fuse these prototypes as follows:\n$P_{fuse} = \\eta \\cdot P_{y_1} + (1 - \\eta) \\cdot P_{y_2}$,\nwhere $\\eta$ is an HP controlling the fusion balance.\nWe then divide $P_{fuse}$ into two separate fused representations, $P_{fuse}^{1}$ and $P_{fuse}^{2}$, and optimize the model using the following objective function:\n$L_{aos} = -(Discr(P_{fuse}^{1}, P_{fuse}^{2}) + E(P_{fuse}^{1}) + E(P_{fuse}^{2}))$,\nwhere $E(\\cdot)$ is an entropy function that measures prediction uncertainty. By optimizing this objective, the model learns to generalize and distinguish between ID and OOD samples more effectively, leveraging both adaptive intensification and synthetic outlier generation.\nFollowing [11], our multimodal OOD model leverages the joint probability distribution across all modalities and individual predictions from each modality. The model uses M modality-specific feature extractors and a shared classifier, with each modality k also having its own classifier for generating individual prediction probabilities.\nThe base loss $L_{base}$ combines the cross-entropy losses for both joint and individual modality outputs to encourage accurate predictions across modalities. Finally, the complete objective function for OOD detection integrates additional regularization components from the previous steps:\n$L = L_{base} + \\delta \\cdot L_{csct} + L_{pdi} + \\kappa \\cdot L_{aos}$,\nwhere $\\delta$ and $\\kappa$ are HPs that balance the influence of the CSCT, discrepancy intensification, and adaptive outlier synthesis losses, thereby optimizing the model's performance on OOD detection. See details of HP settings in Appx. B.2."}, {"title": "5. Conclusion, Limitations, and Future Work", "content": "The DPU framework addresses the critical challenge of intra-class variability in multimodal OOD detection through adaptive, sample-specific prototype updates. Extensive experiments on diverse datasets and tasks demonstrate DPU 's leading performance, achieving substantial improvements in OOD detection and, in some cases, perfect ID/OOD classification accuracy. These results highlight DPU's potential for robust deployment in high-stakes applications.\nLimitations and Future Work. While DPU performs well across a range of datasets, its effectiveness may vary with significantly larger or more complex multimodal datasets, which could introduce scalability challenges. Future work could focus on enhancing DPU 's efficiency for these larger datasets and optimizing it for real-time applications, such as autonomous driving and healthcare, where prompt responses are essential. Additionally, exploring cross-domain OOD detection would broaden DPU 's applicability, supporting its use in diverse real-world environments."}, {"title": "Broader Impact and Ethics Statement", "content": "Broader Impact Statement: DPU significantly advances multimodal OOD detection by addressing intra-class variability, a crucial factor for reliable model performance in fields like healthcare, autonomous systems, and security. By enhancing model robustness against unknown data distributions, DPU empowers applications in dynamic, high-stakes environments where accuracy and reliability are paramount. This adaptability ensures systems remain effective when confronted with new or evolving data patterns.\nEthics Statement: Our research complies with ethical standards, emphasizing privacy, fairness, and transparency. DPU is designed to mitigate risks associated with biased predictions and potential privacy violations, particularly relevant in sensitive applications like surveillance and medical diagnosis. Promoting more consistent and fair OOD detection, DPU helps reduce ethical concerns tied to unexpected model behaviors, especially meaningful for emerging ones."}, {"title": "A. Extended Related Work", "content": "Out-of-Distribution Detection. Out-of-Distribution (OOD) detection seeks to identify test samples diverging from the training distribution, while maintaining in-distribution (ID) classification accuracy. Common OOD methods include post hoc techniques and training-time regularization [49]. Post hoc approaches like Maximum Softmax Probability (MSP) [16], enhanced by temperature scaling and input perturbation [30], compute OOD scores from model outputs, with improvements by methods like MaxLogit [17] and energy-based approaches [31]. Activation-modifying techniques such as ReAct [40] and ASH [7], alongside distance-based approaches like Mahalanobis [23] and k-Nearest Neighbor (kNN) [41], leverage feature distances for detection. Recent hybrid methods, including VIM [45] and Generalized Entropy (GEN) [32], integrate features and logits to improve scoring. Our method uniquely addresses intra-class variability within ID data, using dynamic prototype updates based on sample-specific variance, which enhances both ID and OOD detection.\nMultimodal Learning. The multimodal learning field is progressing swiftly, driven by advancements in foundational models, larger datasets, and enhanced computational resources. Large pre-trained models, such as CLIP [37], have substantially boosted performance across diverse multimodal tasks. However, scaling alone does not resolve core challenges, such as out-of-distribution issues [18] and model biases [44]. Addressing edge cases and complex data formats in real-world scenarios remains essential for the safe and responsible deployment of these models.\nMultimodal Out-of-Distribution Detection. Recent research has expanded OOD detection to multimodal models, especially in vision-language systems [33, 46]. Approaches like Maximum Concept Matching (MCM) [33] align visual features with textual concepts to define OOD scores. Meanwhile, CLIPN [46] enhances CLIP by using contrasting prompts to distinguish between ID and OOD samples. However, these methods mainly focus on image-based benchmarks, limiting their use of complementary information from various modalities. In real-world applications, modalities such as LiDAR and cameras in autonomous driving or video, audio, and optical flow in action recognition provide valuable context that current methods do not exploit [38]. Our approach introduces a dynamic multimodal framework that effectively leverages complementary modalities by adjusting prototype updates based on intra-class variability."}, {"title": "B. Details on the Proposed DPU", "content": "B.1. Robust Marginal Contrastive Learning\nAs briefly discussed in \u00a73.3, we consider a batch of video samples $V_1, V_2, ..., V_n$ and their corresponding data from other modalities, $M_1^1, M_2^i,..., M_k^i$, where $i \\in \\{flow, audio\\}$ represents the modality type, and n is the batch size. For each video sample $V_j$, we define a positive set $PS_j$ consisting of samples from the same class, and a negative set $NG_j$ composed of samples from different classes. We start with the standard InfoNCE loss, a popular contrastive learning objective [35], which is defined as:\n$L_{rmcl} = \\sum_{j=1}^n -\\log \\frac{f_{pos}}{f_{pos} + f_{neg}}$\nwhere $f_{pos}$ denotes the similarity scores for samples in the positive set $PS_j$, and $f_{neg}$ represents the similarity scores for samples in the negative set $NG_j$.\nTo further improve robustness and sensitivity to inter-class distinctions, we introduce an angular margin m for positive samples to further enhance robustness by emphasizing inter-class distinctions, redefining $f_{pos}$ as:\n$f_{pos} = \\sum_{b \\in PS_j} exp \\frac{cos(\\theta_{b,j} + m)}{t}$,\nwhere t is a temperature hyper-parameter, and $\\theta_{b,j}$ denotes the arc-cosine similarity between representations F and F within the same modality. For the negative set $NG_j$, we similarly define:\n$f_{neg} = \\sum_{b \\in NG_j} exp \\frac{cos (\\theta_{b,j})}{t}$\nThis contrastive objective encourages the model to maximize intra-class similarity (positive pairs) and minimize inter-class similarity (negative pairs)."}, {"title": "B.2. Final OOD Detection and Integration", "content": "As briefly discussed in \u00a73.5, our multimodal OOD model makes predictions by leveraging: (i) the joint probability distribution inferred across all modalities and (ii) separate predictions from each modality independently.\nThe model employs M feature extractors $g_i(\\cdot)$ for each modality, alongside a shared classifier $h(\\cdot)$. Each feature extractor $g_i(\\cdot)$ generates an embedding F\u2081 for its respective modality i, which the classifier h(\u00b7) then combines to produce a joint probability distribution $\\hat{y}$:\n$\\hat{p} = softmax (h ([g_1(x_1), g_2(x_2),..., g_M(x_M)]))$.\nThis combined output provides an overall prediction for identifying whether $x_i$ belongs to the ID or OOD classes. Additionally, each modality k has its own classifier $h_k(\\cdot)$, which produces individual prediction probabilities $\\hat{p}_k = softmax (h_k (g_k (x_k)))$. This modality-specific information complements the joint prediction, offering finer-grained insights into each modality's confidence in classifying a sample as ID or OOD. The core OOD detection loss for a sample $(x_i, y_i)$ is:\n$L_{base} = \\sum_{k=1}^M CE (\\hat{p}^k, y_i) + CE (\\hat{p}, y_i)$,\nwhere CE() is the cross-entropy loss, reinforcing accurate predictions across joint and individual modality outputs.\nFinally, the complete objective function for OOD detection integrates additional regularization components from the previous steps:\n$L = L_{base} + \\delta \\cdot L_{csct} + L_{pdi} + \\kappa \\cdot L_{aos}$,\nwhere $\\delta$ and $\\kappa$ are HPs that balance the influence of the CSCT, discrepancy intensification, and adaptive outlier synthesis losses, thereby optimizing the model's performance on OOD detection."}, {"title": "B.3. Hyperparameter Optimization in DPU", "content": "As briefly discussed in \u00a74.2, for the HPs $\\lambda$, $\\delta$, and $\\kappa$, we set their values to 2, 0.2, and 0.5, respectively, to ensure they are within a similar order of magnitude. The value of $\\beta$ is set to 0.8 to control the update speed, with smaller values leading to faster updates. The parameter $\\gamma$ is assigned a very small value to prevent division by zero. The most influential HP is $\\mu$, which directly impacts the discrepancy intensification. Based on the fixed ratio proposed by Dong et al. [11], we aim to scale \u03bc to allow sufficient room for dynamic adjustment. Specifically, we multiply the base fixed ratio by {2, 3, 4} to determine the final value of \u03bc. A grid search process is employed to find the optimal value. Regarding batch size, we generally observe that a larger batch size during training yields better performance, which is characteristic of contrastive learning. However, an exception is the EPIC-Kitchen dataset, where the best performance is achieved with a batch size of 16. We attribute this to the specific data quality and distribution within the EPIC-Kitchen dataset."}, {"title": "C. Additional Experimental Settings and Results", "content": "C.1. Datasets\nAs briefly discussed in \u00a74.1, we evaluate our method across five datasets: HMDB51 [22], UCF101 [39], Kinetics-600 [20], HAC [10], and EPIC-Kitchens [6].\n1) HMDB51 [22] is a video action recognition dataset containing 6,766 video clips across 51 action categories. The clips are sourced from various media, including digitized movies and YouTube videos, and include both video and optical flow modalities.\n2) UCF101 [39] is a diverse video action recognition dataset collected from YouTube, containing 13,320 clips representing 101 actions. This dataset includes variations in camera motion, object appearance, scale, pose, viewpoint, and background conditions. It provides video and optical flow modalities.\n3) Kinetics-600 [20] is a large-scale action recognition dataset with approximately 480,000 video clips across 600 action categories. Each clip is a 10-second snippet of an annotated action moment sourced from YouTube. Following [11], we selected a subset of 229 classes from Kinetics-600 to avoid potential overlaps with other datasets, resulting in 57,205 video clips. Video and audio modalities are available, with optical flow extracted at 24 frames per second using the TV-L1 algorithm [51], yielding 114,410 optical flow samples.\n4) HAC [10] includes seven actions\u2014such as 'sleeping', 'watching TV', 'eating', and 'running'\u2014performed by humans, animals, and cartoon characters, with 3,381 total video clips. The dataset provides video, optical flow, and audio modalities.\n5) EPIC-Kitchens [6] is a large-scale egocentric video dataset collected from 32 participants in their kitchens as they captured routine activities. For our experiments, we use a subset from the Multimodal Domain Adaptation paper [34], which contains 4,871 video clips across the eight most common actions in participant P22's sequence ('put,' 'take,' 'open,' 'close,' 'wash,' 'cut,' 'mix,' and 'pour'). The available modalities include video, optical flow, and audio."}, {"title": "C.2. Tasks", "content": "As briefly discussed in \u00a74.2", "tasks": "Near-OOD detection and Far-OOD detection [11", "22": "and UCF101 [39", "20": "with approximately 250 clips per class", "datasets": "nHMDB51 as ID: We designate UCF101", "ID": "We designate UCF101, EPIC-Kitchens, HAC, and HMDB51 as OOD datasets, excluding any ID"}]}