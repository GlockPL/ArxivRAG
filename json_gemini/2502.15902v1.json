{"title": "IPAD: Inverse Prompt for AI Detection - A Robust and Explainable\nLLM-Generated Text Detector", "authors": ["Zheng Chen", "Yushi Feng", "Changyang He", "Yue Deng", "Hongxi Pu", "Bo Li"], "abstract": "Large Language Models (LLMs) have attained\nhuman-level fluency in text generation, which\ncomplicates the distinguishing between human-\nwritten and LLM-generated texts. This in-\ncreases the risk of misuse and highlights the\nneed for reliable detectors. Yet, existing\ndetectors exhibit poor robustness on out-of-\ndistribution (OOD) data and attacked data,\nwhich is critical for real-world scenarios. Also,\nthey struggle to provide explainable evidence\nto support their decisions, thus undermining\nthe reliability. In light of these challenges,\nwe propose IPAD (Inverse Prompt for AI\nDetection), a novel framework consisting of\na Prompt Inverter that identifies predicted\nprompts that could have generated the input\ntext, and a Distinguisher that examines how\nwell the input texts align with the predicted\nprompts. We develop and examine two ver-\nsions of Distinguishers. Empirical evalua-\ntions demonstrate that both Distinguishers per-\nform significantly better than the baseline meth-\nods, with version2 outperforming baselines by\n9.73% on in-distribution data (F1-score) and\n12.65% on OOD data (AUROC). Furthermore,\na user study is conducted to illustrate that IPAD\nenhances the AI detection trustworthiness by\nallowing users to directly examine the decision-\nmaking evidence, which provides interpretable\nsupport for its state-of-the-art detection results.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), characterized\nby their massive scale and extensive training data\n(Chen et al., 2024), have achieved significant\nadvances in natural language processing (NLP)\n(Ouyang et al., 2022; Veselovsky et al., 2023; Wu\net al., 2025). However, with the advanced capa-\nbilities of LLMs, they are subject to frequent mis-\nused in various domains, including academic fraud,\nthe creation of deceptive material, and the gen-\neration of fabricated information (Ji et al., 2023;\nPagnoni et al., 2022; Mirsky et al., 2023), which un-\nderscores the critical need to distinguish between\nhuman-written text (HWT) and LLM-generated\ntext (LGT) (Pagnoni et al., 2022; Yu et al., 2025;\nKirchenbauer et al., 2023).\nHowever, due to their sophisticated function-\nality, LLMs pose significant challenges in the\nrobustness of current AI detection systems (Wu\net al., 2025). The existing detection systems,\nincluding commercial ones, frequently misclas-\nsify texts as HWT (Price and Sakellarios, 2023;\nWalters, 2023) and generate inconsistent results\nwhen analyzing the same text using different de-\ntectors (Chaka, 2023; Weber-Wulff et al., 2023).\nStudies show false positive rates reaching up to\n50% and false negative rates as high as 100% in\ndifferent tools (Weber-Wulff et al., 2023) when\ndealing with out-of-distribution (OOD) datasets.\nAnother critical issue with the existing AI detec-\ntion systems is their lack of verifiable evidence (Ha-\nlaweh and Refae, 2024), as these tools typically\nprovide only simple outputs like \"likely written by\nAI\" or percentage-based predictions (Weber-Wulff\net al., 2023). The lack of evidence prevents users\nfrom defending themselves against false accusa-\ntions (Chaka, 2023) and hinders organizations from\nmaking judgments based solely on the detection\nresults without convincing evidences (Weber-Wulff\net al., 2023). This problem is particularly trouble-\nsome not only because the low accuracy of such\nsystems as mentioned before, but also due to the\nconsequent inadequate response to LLM misuse,\nwhich can lead to significant societal harm (Stokel-\nWalker and Van Noorden, 2023; Porsdam Mann\net al., 2023; Shevlane et al., 2023; Wu et al., 2025).\nThese limitations highlight the pressing need for\nmore reliable, explainable and robust detectors.\nIn this paper, we propose IPAD (Inverse Prompt\nfor AI Detection), a novel framework compris-\ning two key components as shown in Figure 1: a\nPrompt Inverter that reconstructs prompts from"}, {"title": "2 Methodology", "content": "In this section, we illustrate our method step by\nstep. First, we introduce the overall workflow. Af-\nter that, we demonstrate the details of supervised\nfine-tuning (SFT) the Prompt Inverter and Distin-\nguisher."}, {"title": "2.1 Workflow", "content": "IPAD consists of a Prompt Inverter and a Dis-\ntinguisher, both fine-tuned on Microsoft's open\nmodel Phi3-medium-128k-instruct, which together\nform a complete detection workflow as illustrated\nin Figure 1. For the Distinguisher, we develop two\nmodels and examine them in Section 3.\nThe Input Text (T) is either human-written\n(HWT) or LLM-generated (LGT), and it is pro-\ncessed by the Prompt Inverter to predict the most\nlikely prompt that could have generated it. This\nPredicted Prompt (P) is assumed to be the input\nthat an LLM would have used to produce the text.\n$$P = f_{inv}(T)$$\nwhere $f_{inv}$ stands for Prompt Inverter.\nFor the next step, the Predicted Prompt (P) is\nfed into an LLM (we use ChatGPT, i.e. gpt-3.5-\nturbo by default, and other LLMs for evaluations),\nto generate a corresponding Regenerated Text (T').\n$$T' = f_{LLM}(P)$$\nAfter that, we consider two Distinguishers. The\nfirst one is Prompt-Text Consistency Verifier, in"}, {"title": "2.2 Datasets", "content": "The datasets used to fine-tune the Prompt Inverter\ninclude several widely adopted resources in the\nfield. These are:"}, {"title": "2.2.1 Prompt Inverter", "content": "Given that essay data are diverse, we utilize\nonly the OUTFOX dataset (Koike et al., 2024).\nTo adapt this dataset for training our Distin-\nguisher, we enhance it to align with the model's\nrequirements. The original dataset consists of\n14,400 training triplets of essay problem state-\nments, student-written essays, and LLM-generated\nessays. To further process the data, we apply the\nPrompt Inverter to both student-written and LLM-\ngenerated essays, generating corresponding Pre-\ndicted Prompts. These Predicted Prompts are then\nused to regenerate texts via ChatGPT, i.e. gpt-3.5-\nturbo.\nThe final dataset is structured as follows:\nDistinguisher version1 Prompt-Text Consis-\ntency verifier: Instruction:\"Can LLM generate\ntext2 through the prompt text1? \" Input: text 1:\n{Predicted Prompt}; text 2: {LGT} (or {HWT})\nOutput: yes (or no)"}, {"title": "2.2.2 Distinguishers", "content": "which the Input Text (T) and the Predicted Prompt\n(P) are passed to the model.\nThe Prompt-Text Consistency Verifier deter-\nmines whether the Predicted Prompt (P) can rea-\nsonably generate the given Input Text (T) using an\nLLM. The model outputs either a \"yes\" or \"no\"\nresponse. If the Predicted Prompt (P) is likely to\nproduce the Input Text (T) when fed into the LLM,\nthe model is expected to output \"yes\", indicating\nthat the Input Text (T) is likely LGT. Conversely,\nif the Predicted Prompt (P) does not align well\nwith the Input Text (T), the model outputs \"no\",\nsuggesting that the Input Text (T) is less likely to\nhave been generated by the LLM with the Pre-\ndicted Prompt (P), and is therefore more likely to\nbe HWT.\n$$S = f_{PTCV}(T, P)$$\nwhere $f_{PTCV}$ stands for Prompt-Text Consistency\nVerifier in the Distinguisher.\nThe second Distinguisher is Regeneration Com-\nparator, which considers both the Input Text (T)\nand the Regenerated Text (T').\nThe\nRegeneration Comparator determines\nwhether the Input Text (T) aligns with the Regener-\nated Text (T'), and then outputs either a \"yes\" or\n\"no\" response. If the Input Text (T) is LGT, the\nmodel is expected to output \"yes,\" which indicates\nthat both the Input Text (T) and the Regenerated\nText (T') were generated by an LLM from similar\nprompts. Conversely, if the Input Text (T) is HWT,\nthe model is expected to output \"no,\" which signi-\nfies that the Input Text (T) is meaningfully distinct\nfrom the Regenerated Text (T') and thus unlikely\nto have been generated by an LLM.\n$$S = f_{RC}(T, T')$$\nwhere $f_{RC}$ stands for Regeneration Comparator in\nthe Distinguisher.\nFinally, for both Distinguishers,\n$$Y =\n\\{\n\\begin{array}{ll}\nLGT, & \\text { if } S=\\text { Yes } \\\\\nHWT, & \\text { if } S=\\text { No }\n\\end{array}$$\nwhere Y is the final decision of the Input Text (T)."}, {"title": "2.3 Training", "content": "The supervised fine-tuning (SFT) (Wei et al.,\n2022) process is performed on a dataset compris-\ning the above-mentioned 45,400 pairs for Prompt\nInverter and 28,800 pairs for both Distinguish-\ners. We utilize Microsoft's open model, phi3-\nmedium-128k-instruct, and we use low-rank adap-\ntation (LoRA) method (Hu et al., 2022) on the\nLLaMA-Factory framework (Zheng et al., 2024).\nWe train it using six A800 GPUs for 20 hours for\nPrompt Inverter, 7 hours for Distinguisher ver-\nsion1, and 4 hours for Distinguisher version2."}, {"title": "3 Experiments", "content": "We investigate the following questions through our\nexperiments:\n\u2022 Assess the robustness of IPAD (using various\nLLMs as generators, comparing with other de-\ntectors, and evaluating on out-of-distribution\n(OOD) datasets).\n\u2022 Independently analyze the necessity and ef-\nfectiveness of the Prompt Inverter and the\nDistinguishers.\n\u2022 Explore the explainability of IPAD (through\na user study and analysis of linguistic differ-\nences between prompts generated by HWT\nand LGT)."}, {"title": "3.1 Robustness of IPAD", "content": "The in-distribution experiments refer to the testing\nresults presented in (Koike et al., 2024), where\nthe data aligns with the training data used for the\nIPAD Distinguishers, thereby serving as our base-\nline. The OOD experiments refer to the DetectRL\nbaseline (Wu et al., 2024), which is a comprehen-\nsive benchmark consisting of academic abstracts\nfrom the arXiv Archive (covering the years 2002\nto 2017), news articles from the XSum dataset\n(Narayan et al., 2018), creative stories from Writ-\ning Prompts (Fan et al., 2018), and social reviews\nfrom Yelp Reviews (Zhang et al., 2015). It also\nemploys three attack methods to simulate complex\nreal-world detection scenarios, which includes the\nprompt attacks, paraphrase attacks, and perturba-\ntion attacks (Wu et al., 2024). All the testing sets\nhave 1,000 samples in our experiments.\nThe Area Under Receiver Operating Charac-\nteristic curve (AUROC) is widely used for assess-\ning detection method (Mitchell et al., 2023) be-\ncause it considers the True Positive Rate (TPR) and\nFalse Positive Rate (FPR) across different classifi-\ncation thresholds. Since our models predicts binary\nlabels, we follow the Wilcoxon-Mann-Whitney\nstatistic (Calders and Jaroszewicz, 2007), and the\nformula is shown in appendix A. The AvgRec is\nthe average of HumanRec and MachineRec. In\nour evaluation, HumanRec is the recall for detect-\ning Human-written texts, and MachineRec is the\nrecall for detecting LLM-generated texts (Li et al.,\n2024). The F1 Score provides a comprehensive\nevaluation of detector capabilities by balancing the\nmodel's Precision and Recall. We use AvgRec and\nF1 on in-distribution data, and we use AUROC\nfor OOD data to align the test benchmarks for the\nsame dataset."}, {"title": "3.1.1 Evaluation Baselines and Metrics", "content": "The results of IPAD for detecting the dataset OUT-\nFOX (Koike et al., 2024) across LLMs are pre-\nsented in Table 1 and Table 2, respectively. They\nshow that both versions are highly robust across\nvarious LLMs, while Regeneration Comparator is\na bit more efficient.\nAs for Regeneration Comparator, when the orig-\ninal generator and re-generator are the same model,\nthe performance is optimal. However, even when\nthe re-generator is different from the original gen-\nerator, the results remain impressive with ChatGPT\nused as the re-generator. These results imply that,\nin practical applications, it is possible to use a com-\nmon set of LLMs as re-generators. If one or more\ncorreponding Distinguishers from different LLMs\nclassify the results as 'yes', it can be inferred that\nthe text is likely to be LGT, whereas if all Dis-\ntinguishers classify the results as 'no', the text is\nmore likely to be HWT. Furthermore, for applica-\ntions aiming to save computational resources and"}, {"title": "3.1.2 Robustness across different LLMs", "content": "Table 3 compares the performance of two versions\nof IPAD with other detection methods in the OUT-\nFOX dataset with and without attacks (Koike et al.,\n2024). The results show that both versions of IPAD\ngenerally outperform other detectors, while that\nIPAD with Prompt-Text Consistency Verifier for\ndetecting ChatGPT with DIPPER attack performs\nworse. These results imply that IPAD with Regen-\neration Comparator demonstrates superior robust-\nness compared to alternative detection methods in\nthe OUTFOX dataset with and without attacks.\nTable 4 presents the performance of various de-\ntection methods on OOD datasets to assess their\ngeneralizability, where the baseline data refer to\nDetectRL (Wu et al., 2024). The results demon-\nstrate that IPAD with Regeneration Comparator\nconsistently outperforms all other baselines in all\nOOD datasets with and without attacks. In contrast,\nIPAD with Prompt-Text Consistency Verifier ex-\nhibits strong performance on OOD datasets without\nattacks but shows a noticeable drop in effectiveness\nwhen subjected to attacks. For instance, while it\nachieves competitive results on datasets like XSum\n(99.90%) and Writing (99.20%), its performance\nagainst attacks, such as Prompt Attack (86.90%)\nand Paraphrase Attack (82.72%), is significantly\nlower than IPAD with Regeneration Comparator.\nThis suggests that IPAD with Regeneration Com-"}, {"title": "3.1.3 Comparison of IPAD with other\ndetectors in and out of distribution", "content": "To prove that it is necessary to fine-tune on IPAD\nwith IPAD with Prompt-Text Consistency Verifier\nand Regeneration Comparator, we conducted abla-\ntion study to use the same finetune method on only\ninput texts and only predicted prompts. The in-\nstructions are \"Is this text generated by LLM?\", and\n\"Prompt Inverter predicts prompt that could have\ngenerated the input texts. Is this prompt predicted\nby an input texts written by LLM?\", respectively.\nThe results shown in Figure 2 from the ablation\nstudy show that fine-tuning on either only the input\ntext or only the predicted prompt leads to poor\nperformance. This underscores the importance of\nfine-tuning on a combination of both the input text\nand predicted prompt, as explored in the Prompt-\nText Consistency Verifier, or on the input text and\nregenerated text, as examined in the Regeneration\nComparator, for more effective detection."}, {"title": "3.1.4 Robustness conclusion", "content": "Our experimental results demonstrate that both\nIPAD versions exhibit strong performance across\ndifferent LLMs, outperforming existing detection\nmethods and maintaining robustness on OOD\ndatasets. The IPAD with Regeneration Compara-\ntor outperforming baselines by 9.73% (F1-score)\non in-distribution data and 12.65% (AUROC) OOD\ndata. Notably, IPAD with Regeneration Compara-\ntor achieves significantly better performance than\nIPAD with Prompt-Text Consistency Verifier in\nattack scenarios of 3.78% (F1-score). While IPAD\nwith Prompt-Text Consistency Verifier performs\nrobustly in standard settings, its performance de-\nclines when facing attacks. The calculation of these\nstatistics are shown in Appendix B."}, {"title": "3.2 Necessity and Effectiveness of Prompt\nInverter and Distinguishers", "content": "We use DPIC (Yu et al., 2024) and PE (Zhang\net al., 2024c) as baseline methods for prompt ex-\ntraction. DPIC employs a zero-shot approach using\nthe prompt states in Appendix C, while PE uses\nadversarial attacks to recover system prompts.\nIn our evaluation, we tested 1000 LGT and\n1000 HWT samples. We use only in-distribution\ndata for testing since only these datasets include\noriginal prompts. The metrics are all tested on\ncomparing the similarity of the original prompts\nand the predicted prompts. The results shown\nin Table 5 illustrate that IPAD consistently out-\nperforms both DPIC and PE across all four met-"}, {"title": "3.2.1 Necissity of the Prompt Inverter and\nDistinguishers", "content": "To examine the effectiveness of the IPAD Distin-\nguishers, we conducted a comparison study us-\ning the same dataset but different distinguishing\nmethods. The first and second methods employed\nSentence-Bert (Reimers and Gurevych, 2019) and\nBart-large-cnn (Yuan et al., 2021) to compute the\nsimilarity score between the input texts and the\nregenerated texts. We selected thresholds that max-\nimized AvgRec, which were 0.67 for Sentence-Bert\nand -2.52 for Bart-large-cnn. The classification rule\nis that the texts with scores greater than the thresh-\nhold will be classified as LGT, while the texts with\nscores less than or equal to the threshold will be\nclassified as HWT.\nThe third and fourth methods involved directly\nprompting ChatGPT as follows:\nInstruction: \"Text 1 is generated by an LLM.\nDetermine whether Text 2 is also generated by an\nLLM with a similar prompt. Answer with only YES\nor NO.\" Input: \"Text 1: {Regenerated Text}; Text\n2: {LGT} or {HWT}\".\nand Instruction: \"Can LLM generate text2\nthrough the prompt text1? Answer with only YES\nor NO.\" with Input: \"Text 1: {Predicted Prompt};\nText 2: {Input text}\".\nThe final results demonstrated that the other dis-\ntinguishing methods performed worse than the two\nIPAD Distinguishers, highlighting the superior\neffectiveness of the IPAD Distinguishers."}, {"title": "3.2.2 The effectivenss of the IPAD Prompt\nInverter", "content": "This subsection of the evaluation aims to explore\nthe linguistic features of prompts generated by"}, {"title": "3.2.3 The Effectiveness of the IPAD\nDistinguishers", "content": "The analysis is first conducted using the Lin-\nguistic Feature Toolkik (lftk), a commonly used\ngeneral-purpose tool for linguistic features extrac-\ntion, which provides a total of 220 features for text\nanalysis. Upon applying this toolkit, we identified\n20 features with significant differences in average\nvalues between the two groups, out of which 3\nfeatures showed statistically significant differences\nwith p-values less than 0.05. These 3 differences\ncan be summarized as one main aspects: syn-\ntactic complexity. Beyond these, we referred to\nthe LIWC framework, which defines 7 function\nwords variables and 4 summary variables. By com-\nparing the difference, two of these 11 features is\nsignificantly distinguishable: the pronoun usage\nand the level of analytical thinking.\nOne of the primary distinctions between the\nHWT prompts and the LGT prompts is sentence\ncomplexity. LGT prompts are typically more com-\nplex, characterized by longer sentence lengths\n(mean value of 1.514 and 1.794), higher syllable\ncounts (mean values of total syllabus three are\n1.572 and 3.042), and more stop-words (mean\nvalues of 9.88 and 10.045). HWT prompts, on\nthe other hand, are characterized by shorter, less\ncomplex sentences that are easier to process and\nunderstand, as examples shown in Appendix D Fig-"}, {"title": "3.3 Explanability Assessment of IPAD", "content": "To assess the explainability improvement of IPAD,\nwe designed an IRB-approved user study with ten\nparticipants evaluating one HWT and one LGT\narticle. We used IPAD version 2 due to its su-\nperior OOD performance and attack resistance.\nParticipants compared three online detection plat-\nforms with screenshots shown in Appendix E6\nwith IPAD's process (which displayed input texts,\npredicted prompts, regenerated texts, and final\njudgments). After evaluation, users rated IPAD\non four key explainability dimensions. Trans-\nparency received strong ratings (40%:5, 60%:4),\nwith users appreciating the visibility of interme-\ndiate processes. Trust scores were more varied\n(10%:3, 70%:4, 20%:5), but IPAD was generally\nconsidered more convincing than single-score de-\ntectors. Satisfaction was mixed (30%:3, 30%:4,\n40%:5), with users acknowledging better detection\nbut raising concerns about energy efficiency since\nIPAD runs three LLMs. Debugging received unani-\nmous 5s, as users could easily analyze the predicted\nprompt and regenerated text to verify the decision-\nmaking process. If needed, users could refine the\ngenerated content by adjusting instructions, such\nas specifying a word count, making IPAD a more\neffective and user-friendly tool compared to black-"}, {"title": "3.3.1\nDifferent Linguistic Features of HWT\nprompts and LGT prompts", "content": "This subsection of the evaluation aims to explore\nthe linguistic features of prompts generated by"}, {"title": "3.4 User Study", "content": "without access to internal data, and logit-based\nmethods like logit2prompt (Mitka, 2024), which\nrely on next-token probabilities but are constrained\nby access to logits. Adversarial methods can by-\npass some defenses but are model-specific and frag-\nile (Zhang et al., 2024d). Despite the success of\nsome zero-shot LLM-inversion based methods (Li\nand Klabjan, 2024; Yu et al., 2024), they are mostly\nnaive usage of prompting LLMs, which makes\nthem poor in prompt extraction accuracy and ro-"}, {"title": "4 Related Work", "content": "This paper introduces IPAD (Inverse Prompt\nfor AI Detection), a framework consisting of a\nPrompt Inverter that identifies predicted prompts\nthat could have generated the input text, and a Dis-\ntinguisher that examines how well the input texts\nalign with the predicted prompts. This design en-\nables explainable evidence chains tracing unavail-\nable in existing black-box detectors. Empirical\nresults show that IPAD surpasses the baselines on\nall in-distribution, OOD, and attacked data. Fur-\nthermore, the Distinguisher (version2) - Regener-\nation Comparator outperforms the Distinguisher\n(version1) - Prompt-Text Consistency Verifier, es-\npecially on OOD and attacked data. While the\nlocal alignment in veresion1 approach provides\nexplicit interpretability, it is more sensitive to ad-\nversarial attacks. In contrast, the global distri-\nbution in veresion2 matching approach implicitly\nlearns generative LLM's distributional properties,\nwhich offers more robustness while maintaining\nexplainability. This insight suggests that combin-\ning self-consistency checks of generative models\nwith multi-step reasoning for evidential explainabil-\nity holds promise for future AI detection systems\nin real-world scenarios. A user study reveals that\nIPAD enhances trust and transparency by allowing\nusers to examine decision-making evidence. Over-\nall, IPAD establishes a new paradigm for more\nrobust, reliable, and interpretable AI detection sys-\ntems to combat the misuse of LLMs."}, {"title": "4.1 AI detectors Methods and challenges", "content": "AI text detection methods can be broadly catego-rized into four approaches (Wu et al., 2025): wa-termarking, statistics-based methods, neural-basedmethods, and human-assisted methods.\nWatermarking technology inserts specific pat-terns into training datasets (Shevlane et al., 2023;\nGu et al., 2022) or manipulates the model output\nduring inference to embed a watermark (Lucas and\nHavens, 2023). However, watermarking needs toaccess of the LLM deployment and can face attacks,such as identifying and erasing the watermark (Houet al., 2024). Statistics-based methods analyzeinherent textual features to identify language pat-terns (Kalinichenko et al., 2003; Hamed and Wu,2023), but their effectiveness depends on corpussize and model diversity (Wu et al., 2025). Someother statistical methods use n-gram probability divergence (Yang et al., 2024b) or similarity betweenoriginal and revised texts (Mao et al., 2024; Zhuet al., 2023) while still face robustness challengesunder adversarial attacks (Wu et al., 2025). Neural-based methods such as RoBERTa (Liu et al.,2020), Bert (Devlin et al., 2019), and XLNet (Yanget al., 2019) have been robust in domain-specifictasks. Adversarial learning techniques are increasing being used (Yang et al., 2024a) to increaseeffectiveness in attacked datasets.\nIn addition to automated methods, human in-volvement plays a key role in detecting AI-generated text (Wu et al., 2025). Human-assisteddetection leverages human intuition and expertiseto identify inconsistencies such as semantic errorsand logical flaws that may not be easily caughtby algorithms (Uchendu et al., 2023; Dugan et al.,2023). Moreover, given the challenges of currentAI detection tools, which often lack verifiable evi-dence (Chaka, 2023), human involvement becomeseven more critical to ensure the reliable and ex-plainable detection."}, {"title": "4.2 Prompt Inverter techniques and\napplications", "content": "Prompt extraction techniques aim to reverse-engineer the prompts that generate specific out-puts from LLMs. Approaches include black-boxmethods like output2prompt (Zhang et al., 2024a),which extracts prompts based on model outputswithout access to internal data, and logit-basedmethods like logit2prompt (Mitka, 2024), whichrely on next-token probabilities but are constrainedby access to logits. Adversarial methods can by-pass some defenses but are model-specific and frag-ile (Zhang et al., 2024d). Despite the success ofsome zero-shot LLM-inversion based methods (Liand Klabjan, 2024; Yu et al., 2024), they are mostlynaive usage of prompting LLMs, which makesthem poor in prompt extraction accuracy and ro-"}, {"title": "5 Conclusion", "content": "While IPAD demonstrates SOTA performance, two\nlimitations warrant discussion: (1) The Prompt\nInverter may not fully reconstruct prompts con-\ntaining explicit in-context learning examples (e.g.,\nformatted demonstrations), as it prioritizes seman-"}, {"title": "6 Limitations", "content": ""}]}