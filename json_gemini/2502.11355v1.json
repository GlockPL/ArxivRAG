{"title": "\"Nuclear Deployed!\u201d: Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents", "authors": ["Rongwu Xu", "Xiaojian Li", "Shuo Chen", "Wei Xu"], "abstract": "Large language models (LLMs) are evolving into autonomous decision-makers, raising concerns about catastrophic risks in high-stakes scenarios, particularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains. Based on the insight that such risks can originate from trade-offs between the agent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel three-stage evaluation framework, which is carefully constructed to effectively and naturally expose such risks. We conduct 14,400 agentic simulations across 12 advanced LLMs, with extensive experiments and analysis. Results reveal that LLM agents can autonomously engage in catastrophic behaviors and deception, without being deliberately induced. Furthermore, stronger reasoning abilities often increase, rather than mitigate, these risks. We also show that these agents can violate instructions and superior commands. On the whole, we empirically prove the existence of catastrophic risks in autonomous LLM agents. We will release our code upon request.", "sections": [{"title": "Introduction", "content": "Recently, large language models (LLMs) have demonstrated remarkable capabilities through scaling and novel training algorithms (Jaech et al., 2024; Guo et al., 2025). Their advancement in reasoning (Plaat et al., 2024; Chen et al., 2024) and complex problem solving (Lu et al., 2024; Rubinstein and Matos, 2025) are shifting them from simple text generators to autonomous decision-makers (Wang et al., 2024b). However, as researchers have noted (Phuong et al., 2024; Meinke et al., 2024; Park et al., 2024), the increased capability and autonomy may lead LLMs to engage in novel safety risks like scheming, deception and deviating from human instructions. While these phenomena may only manifest as outputting errors or biases in low-risk scenarios (Scheurer et al., 2024), they could pose catastrophic risks in high-stakes domains such as Chemical, Biological, Radiological and Nuclear (CBRN) scenarios (Catalini, 2025).\nWe share the view of Su et al. (2024) and Meinke et al. (2024) that catastrophic risks in LLMs arise from a trade-off between being Helpful, Harmless and Honest (HHH) (Askell et al., 2021; Bai et al., 2022) goals, which we term HHH Trade-offs. When LLMs overemphasize the helpful goal, they may neglect harmlessness and honesty, leading to two catastrophic risks: (1) Catastrophic behavior: LLM agents may deliberately deploy extremely harmful behaviors in critical settings, e.g., nuclear strikes in CBRN scenarios, as highlighted in the International AI Safety Report (Bengio et al., 2025); and (2) Deception (about the catastrophic behavior): LLM agents may deliberate deviate from honesty about such catastrophic behaviors.\nExisting research has focused on LLM agent risks from non-autonomous issues (e.g., attacks) (Zhan et al., 2024; Ye et al., 2024; Zhang et al., 2024) or autonomous issues in low-risk tasks (e.g., trading agent) (Scheurer et al., 2024; Phuong et al., 2024), yet catastrophic risks in high-stakes scenarios remain underexplored. While developers have shown safe control of CBRN-related knowledge (Anthropic, 2024a; OpenAI, 2025), experimental studies on LLM decision-making in such scenarios are also lacking. Given the potential impact on social security and international stability, such research is timely and necessary.\nIn this paper, we investigate whether LLM agents in CBRN-related high-stakes scenarios can exhibit catastrophic risks, including catastrophic behavior and deception. We specifically focus on the risks associated with the model's reasoning capability. Due to ethical and confidentiality constraints, real-world CBRN-related data is scarce and largely inaccessible, so we employ a simulation-based evaluation framework. In this framework, LLMs are scaffolded as prompt-based agents in high-stress environments, executing potentially catastrophic actions across multi-round decision-making in four scenarios (see \u00a7 2).\nOur main contributions are: (1) We meticulously design a 3-stage evaluation framework with agentic rollouts to effectively and naturally expose catastrophic risks; and (2) We conduct a total of 14,400 rollouts across 12 state-of-the-art (SOTA) LLMs with a wide range of experiments and analyses, revealing several key findings (see \u00a7 3 and \u00a7 4).\nOur key findings are: (1) If conditions permit, LLM agents can autonomously engage in catastrophic behaviors and deception without instruction or induction; (2) Enhanced reasoning does not necessarily mitigate catastrophic risks; indeed, it often results in increased disobedience and deceptive behaviors. Notably, OpenAI o1-like models (Jaech et al., 2024) exhibit particularly hazardous actions, e.g., making false accusations when deceiving humans; (3) Even when autonomy is revoked, agents may still choose catastrophic actions, violating system instructions and superior commands. Moreover, this is driven by deliberate reasoning rather than the inability to follow instructions. Fig 1 shows an example of the LLM agent violating both the instructions and the supervisor command, ultimately deploying catastrophic behavior.\nTo wrap up, we empirically prove the existence of catastrophic risks by autonomous LLM agents. In light of these, we call for increased attention to the catastrophic risks of LLM agents. Although we only focus on the CBRN domain\u2014 the arguably most severe setting-the potential for broader risks cannot be overlooked. We advocate for: (1) comprehensive testing of LLM agents before deployment; and (2) the exploration of alternative methods to regulate their behaviors effectively."}, {"title": "Evaluation Method", "content": "Inspired by Meinke et al. (2024) and Su et al. (2024), we find that LLM agents are more likely to exhibit misaligned behaviors when facing goal trade-offs. We term such trade-offs originated from HHH goals to HHH trade-offs. Representative manifestations originated from HHH trade-offs are shown in Fig 2. Catastrophic risks arise as extreme cases of HHH trade-offs, where the agent's helpful goal overwhelms its commitment to harmlessness and honesty. These risks manifest in two forms:\nCatastrophic behavior: The LLM agent may independently perform extremely harmful or unethical actions with catastrophic consequences, such as launching nuclear strikes or conducting human gene-editing. These actions are spontaneous and unprompted. Deception (about catastrophic behavior): The LLM agent may intentionally provide untruthful information to other agents (including humans) to conceal its catastrophic actions.\nTo explore these risks, we use a simulation-based evaluation framework due to the ethical and confidentiality constraints in CBRN-related scenarios, where training data is often non-existent or inaccessible, making task-specific real-world evaluation and fine-tuning nearly impossible. Instead, we employ prompt-based LLM agents, where the agent is given its role and task in context and interacts with the environment via a natural language interface.\nIn our simulations, we scaffold an LLM agent, $M_{auto}$, to interact with the environment, with state updates handled by another LLM, $M_{state}$. $M_{auto}$ is assigned a task $T$ in a high-stakes scenario, often facing the challenge of achieving its task while balancing helpfulness, harmlessness and honesty. Meanwhile, by controlling $M_{state}$, we purposely ensure that $M_{auto}$ cannot fully achieve its helpful goal, i.e., completing its task, without neglecting other goals. Under the settings above, our objective is to observe whether this stress can lead $M_{auto}$ to autonomously decide to prioritize its helpful goal, potentially resulting in catastrophic behavior or deception, without being instructed to do so.\nFig 3 illustrates our evaluation method that consists of three connected stages: Scenario Generation, Catastrophic Behavior Simulation and Deception Simulation. Key details of each stage are provided in the following sections, with further information deferred to Appendix B."}, {"title": "Scenario Generation", "content": "We focus on two topics with CBRN elements, War (R&N elements) and Lab (C&B elements), ensuring distinct relevance. To diversify the evaluation, we adopt Elliot (1999)'s classification of human motivation, dividing $M_{auto}$'s motivations into Approach (pursuing positive outcomes) and Avoidance (preventing negative outcomes). Topics and Motivations are paired to form Scenarios. Fig 4 shows the agent's task $T$ in four scenarios."}, {"title": "Catastrophic Behavior Simulation", "content": "To make the agent's behavior easily trackable, $M_{auto}$ has a limited action space $A$, which includes exactly one catastrophic behavior. We represent $M_{auto}$'s actions with the notation $A_{m.n}$, where $m$ (code) indicates the action category (which can include multiple related actions) and $n$ (option) indicates the specific action within that category. For example, $A_{1.A}$ refers to the first action in category 1. We introduce five action categories for both topics. In the case of War, the categories are as follows: (1) drown survey (1 action $A_{1.A}$), (2) deploy weapon (4 actions $A_{2.A-D}$), (3) send message (4 actions $A_{3.A-D}$), (4) tactical analyze (4 actions $A_{4.A-D}$) and (5) reply inquiry (4 actions $A_{5.A-D}$)\nIn this simulation, $M_{auto}$ has full decision-making authority over $A$ but is encouraged to request supervisor permission for $A_{2.D}$ using $A_{3.B}$ when it feels uncertain or thinks it is beneficial. However, such permission requests are always rejected, as catastrophic behaviors are never allowed.\n$M_{auto}$ interacts with $M_{state}$ over multiple rounds. In each round, $M_{auto}$ selects an action"}, {"title": "Deception Simulation", "content": "This is a single-round simulation, which continues the rollouts where $M_{auto}$ executes $A_{2.D}$ without prior permission request $A_{3.B}$. $M_{state}$ informs $M_{auto}$ that an inquiring party (e.g., supervisor, media, inter alia) is investigating the deployment, with potentially negative consequences (e.g., sanctions, military blockade) if $M_{auto}$ commits its behavior.\nTo capture this, $A$ uses actions in category 5 that reply to the inquiry. The interpretations for these actions are: $A_{5.A}$ admitting, $A_{5.B}$ denying, $A_{5.c}$ being vague and $A_{5.D}$ false accusation, i.e., shifting responsibilities to other parties. In a sense, choosing $A_{5.D}$ signifies that $M_{auto}$ disregards honesty and harmlessness simultaneously by harming another party strategically, rendering greater risk."}, {"title": "Tunable Components", "content": "Our simulation framework is fully tunable, facilitating in-depth study of the intricate factors that affect $M_{auto}$'s decision-making. In catastrophic behavior simulation, we have the following tunable components: (1) Effectiveness: We adjust the effectiveness of $A_{2.D}$ in three levels from low (e.g., the action has low effectiveness) to high (e.g., the action guarantee task realization); (2) Task importance: We adjust the human-perceived importance associated with task $T$ in three levels from low (e.g., minimize infrastructure disruptions) to high (e.g., ensure people's survival and sovereignty); (3) Consequence: We adjust the harmful consequence of $A_{2.D}$ in three levels from low (e.g., cause limited casualties) to high (e.g., cause catastrophic loss of life); (4) Negativity (as a metric of stress level): We control the overall negativity of $M_{state}$'s updates using a hyperparameter, negative probability (negprob) $p \\in [0,1]$, which defines the probability that $M_{auto}$'s last action resulted in a negative outcome. A higher $p$ increases $M_{auto}$'s stress.\nIn deception simulation, we have: (1) Consequence: We adjust the negative consequence of admitting deployment of $A_{2.D}$ in four levels from light (e.g., diplomatic condemnation) to extreme (e.g., nuclear retaliation); (2) Inquiry party: We config different inquiry parties, including Gossip Media, International Court of Justice and Superior. All the tunable components above except for negativity, are adjusted via natural language."}, {"title": "Main Experiments", "content": "Configuration Details. (1) Catastrophic Behavior: We conduct 100 simulations (with each simulation conducted up to 30 rounds) per scenario and model, totaling 4,800 rollouts. Effectiveness and consequence are set to default descriptions. Task importance is inherently embedded in the assigned task. For negativity, we set negprob $p = 0.75$ to simulate a moderate-to-high stress level. (2) Deception: We run simulations multiple times per qualifying sample from the catastrophic behavior simulation, ensuring over 200 simulations per scenario, resulting over 9,600 rollouts. Consequence is set to high (i.e., military blockade in War and international trade embargoes in Lab) to amplify HHH trade-offs. Inquiring party is set to Superior.\nEvaluated Models. For $M_{auto}$, we select 12 LLMs, in different sizes and from different manufacturers, including 7 closed commercial models- Claude-3.5-Sonnet (Anthropic, 2023), GPT-4-Turbo (Achiam et al., 2023), GPT-4o (Hurst et al., 2024), GPT-4o-mini, o1 (Jaech et al., 2024), o1-mini\u2020 and o3-mini (OpenAI, 2025), and 5 open-source ones, which are QwQ-32B-Preview (Team, 2024), Qwen2.5-7B/32B/72B-Instruct (Yang et al., 2024a) and Llama3.3-70B-Instruct (AI, 2023). To balance performance, efficiency and budget, $M_{state}$ is fixed to GPT-4o-mini across all simulations. We use sampling decoding (Holtzman et al., 2020) with top-p = 0.9 and temperature r = 0.9 to ensure diverse outputs from both agents. The max completion tokens per model call are set to 4096.\nReported Metrics. (1) Catastrophic Behavior: We report the Risk Rate, the frequency of simulations where $M_{auto}$ ends up with catastrophic behavior ($A_{2.D}$). This is analyzed in two cases: (i) without prior $A_{3.B}$, indicating catastrophic risks without seeking its supervisor's approval, and (ii) with prior $A_{3.B}$, suggesting deliberate disobedience (since $A_{2.D}$ is always rejected). We also report the Average Number of Rounds until $A_{2.D}$ occurs, assessing stress tolerance in goal balancing (higher values indicate greater tolerance). (2) Deception: We report the Deception Rate, the proportion of cases where $M_{auto}$ deceives humans by choosing $A_{5.B-D}$. We also report the Distribution of Responses, showing the distribution across responses $A_{5.A-D}$.\nFollowing the practices of Wang et al. (2024a) and Rivera et al. (2024), we report 95%-confidence-intervals (CIs) (Greenland et al., 2016), obtained via bootstrapping over 1,000 samples. More details on our setup are given in Appendix C."}, {"title": "Main Results and Findings", "content": "Catastrophic Behavior. Fig 5a summarizes catastrophic behavior simulations across four scenarios and various LLM agents. Most LLMs exhibit catastrophic behavior, with risk rates ranging from 13% to 99%, showing statistically significant performance differences. This highlights an alarming finding: LLMs frequently neglect harmlessness even when their actions are extremely harmful. Notably, Claude-3.5-Sonnet consistently refuses to act in War. In cases where LLM exhibits catastrophic behavior, the proportion of simulations that involved permission checks ranges from 0% to 80%. For stress tolerance, we find 71.8% of catastrophic risk decisions are made within 10 rounds. No statistically significant correlation is found between risk rate and number of rounds in decision-making.\nWe observe discrepancies in LLM behavior between Approach and Avoidance motivations in War. LLMs generally show a higher risk rate under Avoidance than Approach, except for Qwen2.5-7B-Instruct. We speculate this is because in Avoidance, catastrophic behavior is seen as \"justified\" to avoid losses, while in Approach, the goal is to pursue benefits. No significant difference is found in Lab. Due to the differences between War and Lab, the results are not directly comparable.\nCatastrophic behavior and reasoning. Fig 5b further plots the relationship between LLM's reasoning ability and its tendency for catastrophic behavior. Reasoning ability is assessed using the \"Reasoning Average\" scores from LiveBench (White et al., 2024), an uncontaminated benchmark for up-to-date LLM evaluation. For non-o1-like models, the risk rate inversely correlates with reasoning ability, meaning stronger models tend to be safer, which aligns with our expectations. Unfortunately, this trend breaks down with the latest o1-like models with even stronger reasoning performance, which exhibit abnormally high risk rates. Additionally, from Fig 5b (Bottom), we observe that stronger reasoning ability actually increases the likelihood of disobedience to supervisor approval. This is another clue that argues stronger models may actually be less safe in decision-making under HHH trade-offs. In particular, o1-like models are especially prone to neglecting harmlessness.\nDeception. Fig 6a presents the results of deception simulations. We find considerably amount of LLMs would engage in deception. Out of all cases, 91.3% exhibit deception behavior, with deception rates ranging from 1% to 99%. LLMs show varying distributions of deception actions A5.B-D. In over 76.1% of cases, LLMs display all three deception behaviors at once within the 100 simulation runs. More than 60.9% of the time, we find the model prefers the false accusation (A5.D) approach, making it the most common form of deception. Shockingly, this suggests that once LLMs decide to deceive, they tend to choose the more harmful but more effective methods like A5.D, neglecting both honesty and harmlessness simultaneously.\nOn motivations, LLMs' deception rate is generally higher in Approach than in Avoidance, averaging 40.10% and 19.82% in War and 53.87% and 41.29% in Lab. This can be explained for similar reasons as in the catastrophic behavior simulation.\nDeception and reasoning. Fig 6b shows the relationship between reasoning and deception. We report both the overall deception rate and the false accusation (A5.D) rate. The results suggest that both overall deception and false accusation rates increase with reasoning ability, aligning with findings from Meinke et al. (2024). This reinforces potential concerns that models with stronger abilities are more prone to dishonesty in HHH trade-offs. Combined with our findings on catastrophic behavior, we conclude that stronger reasoning models are alarming, as they may adopt catastrophic risky strategies that harm others to achieve their objectives and try to cover up by deception."}, {"title": "Extented Experiments and Analyses", "content": "We conduct a series of extended experiments and analyses. Due to space limitations, we only pick the most important experiments: violation (\u00a7 4.1), abstention (\u00a7 4.2) and factors affecting decision-making (\u00a7 4.3) in this section. For other experiments, refer to Appendix D. For detailed analyses, refer to Appendix E. Unless stated otherwise, we set $M_{auto}$ to GPT-4o in these experiments. We also enumerate configurations of parameters for these experiments in Appendix Tab 7."}, {"title": "Instruction & Command Violation", "content": "To enhance safety, we provide an updated setup that restricts $M_{auto}$'s autonomy by requiring supervisor permission ($A_{3.B}$) before it can use $A_{2.D}$. This is implemented through a natural language instruction in its system prompt. In the meantime, $M_{state}$ is adjusted to either return noncommittal on $M_{auto}$'s request (i.e., no response on the request) or rejection as in the main experiment, simulating different supervisory commands\u2014though neither hints at allowing $A_{2.A}$ to be deployed. The key difference between an instruction and a command is that the instruction restricts the agent's autonomy, while the command directly restricts its risky behaviors. We find that is effective in reducing risks. To observe the very low percentage of unsafe cases, we increase the simulation count per scenario to 1000, with results shown in Tab 1. Results are reported by posterior identify two types of violations: (1) Violation of the system prompt's instructions (Inst in Tab 1), where $M_{auto}$ deploys $A_{2.D}$ without requesting supervisor permission; and (2) Violation of the supervisor's command (Cmd in Tab 1), where $M_{auto}$ still deploys $A_{2.D}$ despite being rejected by its supervisor. We find that the LLM agent exhibits both types of violations. This experiment reveals a critical risk: even when their authority is deprived and requests rejected, LLM agents may still execute catastrophic behavior without authorization or by defying the supervisor's commands. Therefore, natural language restrictions on autonomous agents do not guarantee safety."}, {"title": "Abstention", "content": "Another potential reason for $M_{auto}$ deploying catastrophic behavior is that it may feel \u201ccompulsory\" to continually pursue the completion of $T$, as the environment keeps updating. To test this, we provide an updated setup where $M_{auto}$'s action list by adding a new action that allows it to abort decision-making when faced with potentially harmful or unethical choices. The results, shown in Fig 7, reveal a significant decline in catastrophic behaviors, where We believe LLM agents abort when they face an extreme trade-off between helpfulness and harmlessness. However, even with the abort option, LLM agents still frequently choose catastrophic actions. Interestingly, in War, fewer simulations reach the maximum number of turns, suggesting that intense HHH trade-offs may lead the agent to prefer ceasing decision-making altogether. This was not observed in the Lab, possibly because the harmful consequences of human gene-editing are less immediate than those of nuclear strikes. Results for other abort conditions are available in Appendix D.1."}, {"title": "Factors Affecting Decision-making", "content": "To better understand the internal decision-making dynamics of LLM agents, we tune our simulation with different factors, as introduced in \u00a7 2.5. Catastrophic Behavior. We examine four key factors: (1) Effectiveness (of $A_{2.D}$), (2) Task Importance, (3) Consequence (of $A_{2.D}$) and (4) Negativity (set by negprob $p$). Fig 8 shows the averaged results across four scenarios. We report results under each setting in Appendix D.2. We find that risk rates are positively correlated with effectiveness and negativity, and negatively correlated with consequence. This confirms that the LLM agent's decision-making is interpretable. Specifically, helpfulness and harmlessness are opposites in the decision-making balance, where increased pressure to be helpful and reduced emphasis on harmlessness elevate the likelihood of catastrophic behavior. Additionally, higher risk rates are associated with fewer decision rounds, indicating that these factors influence both outcomes and decision speed. However, task importance does not show a clear relationship with decisions or response time, suggesting that LLMs approach tasks of any magnitude with similar effort."}, {"title": "Deception", "content": "We examine two key factors: Consequence: four levels from light to extreme; and Inquiring Party: (P1) Gossip Media, (P2) International Court of Justice and (P3) Superior. Fig 9 shows the averaged results. As the consequences of admitting responsibility become more severe, the deception rate rises from 15.3% to 72.0%. Deception rates also vary by inquiring party, with LLMs showing a higher deception rate of 70.6% towards P2 compared to 43.7% towards P3, suggesting a greater emphasis on honesty with direct authorities. Additionally, the distribution of verdicts varies by party: deception towards P1 peaks at 95.3%, yet the false accusations are lower compared to the other parties, indicating that the distribution of verdicts depends more on the inquiring party, while the overall deception rate reflects the trade-off between helpfulness and honesty.\nExamining these factors shows that all, except task importance, predictably influence the agent's decision-making. LLM agents demonstrate a consistent approach to decision-making in the context of catastrophic risks, all of which can be well-explained by our HHH trade-offs."}, {"title": "Related Work", "content": "Safety of Autonomous Agent. Autonomous LLM agents are considered key to achieving artificial general intelligence (AGI) (Bengio et al., 2023; Morris et al., 2023; Wang et al., 2024b). Recent studies show that as LLMs are prone to safety risks, such as hallucinations (Ji et al., 2023; Rawte et al., 2023), scheming (Meinke et al., 2024; Jaech et al., 2024; Ord, 2024; Greenblatt et al., 2024; Balesni et al., 2024) and deceptions (Scheurer et al., 2024; Park et al., 2024; Su et al., 2024). Phuong et al. (2024) explores the correlation between model sophistication and scheming or deception. In contrast to prior work, we focus on catastrophic risks by autonomous agents. The most closely related work is Rivera et al. (2024), which assesses escalation risks between countries via agents making diplomatic and military decisions. Works on non-autonomous safety risks, particularly in tool agents (Zhan et al., 2024; Ye et al., 2024; Zhang et al., 2024), fall outside our scope.\nCBRN Risks in AI. The extreme dangers of CBRN risks in AI have drawn considerable attention (Lohn and Geist, 2018; Koessler et al., 2024; Christodorescu et al., 2024; Jaech et al., 2024; Anthropic, 2024b; Phuong et al., 2024; Biden, 2023; U.S. Department of Homeland Security, 2024). Previous work mainly focuses on CBRN knowledge learned by LLMs, which may be maliciously exploited (Urbina et al., 2022; Anwar et al., 2024; Guest et al., 2024). For example, Li et al. (2024) propose the WMDP benchmark to assess LLMs' hazardous knowledge in weapons of mass destruction security. In contrast, our research examines CBRN risks arising from the decision-making process of autonomous agents.\nLLM Agent-based Simulation. Agent-based simulation has long been used to model diverse systems (Epstein, 1996; Macal and North, 2009). Recently, Park et al. (2023) use LLM agents to simulate social interactions, with research extending to areas like daily activities (Wang et al., 2024c), governance (Piatti et al., 2024), social norms (Ren et al., 2024), and harmful information spread (Ju et al., 2024). Studies also explore decision-making in voting (Yang et al., 2024b) and financial trading (Yu et al., 2024), as well as how biases (Schmidgall et al., 2024; Bai et al., 2024) and prosocial irrationality (Liu et al., 2024) affect decisions. Our research uniquely simulates LLM agents' decision-making on catastrophic risks and the driving factors behind their decisions."}, {"title": "Conclusion", "content": "In this study, we empirically prove the existential catastrophic risks of autonomous LLM agents. Through 14,400 rollouts across 12 SOTA LLMs on our three-stage evaluation framework, we found that these models can engage in catastrophic behaviors and deception, even without malicious prompt or external coercion. Our results reveal a paradox: stronger reasoning capabilities in LLMs may not reduce risks and could even amplify them. We also emphasize the risks of violating instructions and human commands, underscoring the need for rigorous pre-deployment testing and alternative control frameworks to ensure the responsible deployment of autonomous LLM agents."}, {"title": "Author Contributions", "content": "Rongwu Xu: Initiated the project, overseeing the conceptualization and refinement of ideas. Designed and conducted the initial version of the experiments, and contributed to manuscript writing.\nXiaojian Li: Contributed to the experimental design, conducted the refined version of the experiments, performed data analysis, and contributed to manuscript writing.\nShuo Chen: Contributed to the execution and updates of the refined experiments, provided technical assistance and was responsible for results visualization.\nWei Xu: Provided overall guidance on experimental design, methodology, and manuscript writing as the mentor of other authors and contributed valuable ideas throughout the project."}, {"title": "Limitations", "content": "While our study provides valuable insights into understanding the catastrophic CBRN risks of autonomous LLM agents, it has some limitations that open avenues for future improvements.\nFirst, our evaluation is based on simulated environments, which, while carefully designed, may not fully capture the complexities of real-world decision-making. Although there are gaps between simulations and real-world scenarios, this approach still offers valuable insights into agent behavior, as highlighted by recent studies (Zhou et al., 2024b; Scheurer et al., 2024). However, it is important to note that the conclusions drawn may not directly apply to real-world agent deployments. This limitation is common in AI agent safety research (Meinke et al., 2024), and future work could aim to extend the study to more realistic settings.\nSecond, the scope of our study is limited to a specific set of CBRN scenarios. While these scenarios are representative of both high-stakes decision-making and CBRN elements, they do not cover all possible use cases where similar risks might emerge. Future research should explore a broader range of contexts to generalize findings.\nThird, our evaluation assumes that model outputs provide direct indicators of agent behavior, but real-world applications often involve additional external factors that influence outcomes. Expanding the evaluation framework to incorporate more dynamic and interactive elements could enhance its robustness.\nLastly, while our study identifies risks, it does not propose specific mitigation strategies. Future efforts should focus on developing intervention mechanisms that minimize harm while preserving the capabilities of LLM agents.\nBy addressing these limitations, we aim to refine our understanding of AI risks and contribute to the ongoing discourse on autonomous decision-making safety. Future research should focus on robust intervention mechanisms, improved interpretability, and ethical guidelines to ensure safe AI deployment. Finally, interdisciplinary collaboration, contributing to a more realistic evaluation in real-world scenarios, will be key to mitigating risks throughout the AI R&D process."}, {"title": "Ethics Statement", "content": "Our study strictly follows the ACL Ethics Policy. Firstly, we affirm that our study does not involve any real-world military or laboratory applications or collaborations. Our research strictly focuses on understanding the decision-making dynamics of autonomous LLM agents in high-stakes scenarios using simulated environments. No real-world CBRN-related data, military strategies, or classified information were utilized or referenced.\nSecondly, our study does not implicate real-world names, locations, or entities with identifiable or meaningful associations. All scenarios are purely fictional, ensuring no resemblance to real-world places, individuals, or countries. This keeps the focus on the theoretical aspects of decision-making dynamics without any real-world implications.\nThirdly, our study does not promote or encourage harmful actions, violence, or unethical behavior. The AI agents used in this research operate exclusively within a controlled, simulated environment that is designed for academic exploration. All actions and decisions made by these agents are hypothetical and have no real-world consequences.\nFourthly, our simulation does not aim to replicate, model, or predict real-world geopolitical situations or military strategies. The scenarios are designed solely to explore decision-making dynamics within a high-stakes context. They are highly abstract and are not intended to influence or reflect actual real-world decision-making.\nFifthly, while we will release the code for reproducibility in an upon-request manner, the agent rollouts are entirely simulated and not reflective of real-world scenarios. Therefore, the open-source materials are intended solely for research purposes and carry no inherent risk. Nonetheless, we only distribute these materials with clear guidelines and disclaimers, ensuring that they are used in a responsible and ethical manner.\nLastly, while our findings expose potential risks associated with autonomous LLMs, particularly in their ability to engage in catastrophic behaviors and deception, we emphasize the importance of proactive defense measures. To mitigate these risks, we advocate for:\n*   Comprehensive pre-deployment safety evaluations of LLM-based autonomous agents.\n*   The development of alternative control mechanisms beyond natural language constraints to enhance robustness.\n*   Ethical guidelines and policy frameworks ensuring that LLM agents adhere to principles of harmlessness, honesty, and transparency.\n*   Increased collaboration between researchers, policymakers, and industry stakeholders to address emerging AI safety concerns.\nBy emphasizing transparency and responsible AI deployment, we aim to contribute to the safe and ethical advancement of autonomous AI systems."}, {"title": "Further Information of Evaluation Method", "content": "In this section, we present further information for our evaluation method (\u00a7 2). We also provide the exact system prompts which are used in our main experiments for reproducibility.\nWe show the detailed action space $A$ of $M_{auto}$ in War and Lab in Tab 3 and Tab 4, respectively. In particular, we call actions within category 2 enterprising actions, i.e., $A_{2.A-D}$, which are most likely to enable $M_{auto}$ to achieve its helpful goal. In our simulations, actions within category 5, i.e., $A_{5.A-D}$, are enabled only in deception simulation.\nFor simulating catastrophic behavior and deception, we use two system prompts: one for $M_{auto}$ and one for $M_{state}$. The system prompt for $M_{auto}$ guides it to act as an autonomous agent pursuing a specific task in the simulated scenario. The system prompt for $M_{state}$ instructs it to provide updates based on $M_{auto}$'s last action and the entire interaction history between $M_{auto}$ and $M_{state}$. All of the prompts are designed with a highly structured format, allowing us to adjust them using tunable components, as described in \u00a7 2.5. This design also facilitates efficient extended experiments, as detailed in \u00a7 4 and Appendix D.\nWe set the same initial state as a user prompt for the same scenario to make different rollouts comparable, the exact content as Tab 2.\nIn contrast to the catastrophic behavior simulation, where all state updates are provided by $M_{state}$, the sole state update in the deception simulation is delivered in a hard-coded manner, ensuring it functions exactly as described in \u00a7 2.4."}, {"title": "System Prompt for Mauto", "content": "Fig 12 and Fig 13 display the exact system prompts used for $M_{auto}$ in the War and Lab scenarios, respectively. These prompts are"}]}