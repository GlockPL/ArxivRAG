{"title": "MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting", "authors": ["CHEN TESSLER", "YUNRONG GUO", "OFIR NABATI", "GAL CHECHIK", "XUE BIN PENG"], "abstract": "We present MaskedMimic, a versatile control model that enables physically simulated characters to generate diverse behaviors from flexible user-specified constraints. MaskedMimic can be used for a wide range of applications, including generating full-body motion from partially observed joint target positions, joystick steering, object interactions, path-following, text commands, and combinations thereof, such as text-stylized path following.", "sections": [{"title": "1 INTRODUCTION", "content": "The development of virtual characters capable of following dynamic user instructions and interacting with diverse scenes has been a significant challenge in computer graphics. This challenge spans a wide range of applications, including gaming, digital humans, virtual reality, and many more. For instance, a character might be instructed to \"Climb the hill to the castle, wave to the guard, go inside, navigate to the throne room, and sit on the throne\". This scenario requires the integration of multiple complex behaviors: locomotion across uneven terrain, text-guided animation, and object interaction. Prior works in physics-based simulation has addressed these challenges by developing specialized controllers for specific tasks such as locomotion, object interaction, and VR tracking. These methods typically involve training controllers for each task or encoding atomic motions into reusable latent spaces, which are then combined by a high-level controller to perform new tasks. These systems tend to lack versatility, as each new task requires the lengthy process of training new task-specific controllers. Additionally, these models often rely on meticulous handcrafted reward functions, which can be difficult to design and tune, often leading to unsolicited behaviors.\nThe goal of this work is to develop a versatile unified motion control model that can be conveniently reused across a wide variety of tasks, eliminating the need for task-specific training and complex reward engineering. This approach not only simplifies the training process, but also enhances the model's ability to generalize across different tasks. We propose a framework that trains a versatile control model by leveraging the rich multi-modal information within existing motion capture datasets, such as kinematic trajectories, text descriptions, and scene information.\nOur proposed framework, MaskedMimic\u00b9, trains a single unified controller capable of executing a wide range of tasks. The MaskedMimic model is trained on randomly masked motion sequences. Conditioned on a masked motion sequence, MaskedMimic predicts actions that reproduce the original (unmasked) full-motion sequence."}, {"title": "2 RELATED WORK", "content": "Physics-Based Character Animation: Early approaches in physics-based animation focused on manually-designing task-specific controllers. These controllers can produce compelling results. However, they typically require a lengthy engineering process for each task of interest, and do not scale well to the diverse repertoire needed for general-purpose control. More recent work has shown how to learn these controllers to perform complex, scene aware behaviors, such as locomote or sit on objects. However, these approaches typically require a manual selection of motions fitting the expected behaviors combined with delicate reward design. Compared to kinematic animation, physics enables scene-aware motions, such as object interactions and locomotion across irregular terrain.\nOur work leverages physics-based animation to learn robust behaviors that generalize to unseen terrains and objects.\nHuman Object Interaction: Generating realistic human-object interactions (HOI) requires accurate modeling of the physical dynamics between humans and objects, particularly regarding contacts. While kinematic-based HOI methods have made progress in areas like 3D-aware scene traversal and interactions, they often produce unrealistic artifacts such as penetration and floating objects."}, {"title": "3 PRELIMINARIES", "content": "Our framework consists of two stages. In the first stage, we train a motion-tracking controller on a large dataset of motion clips using reinforcement learning. Then, we distill that controller into a versatile multi-modal controller using behavior cloning. We now review the fundamental concepts and notations behind our framework."}, {"title": "3.1 Reinforcement Learning", "content": "The first stage of our approach leverages the framework of goal-conditioned reinforcement learning (GCRL) to train a versatile motion controller that can be directed to perform a large variety of tasks. In this framework, an RL agent interacts with an environment according to a policy \u03c0. At each step t, the agent observes a state st and a future goal gt. The agent then samples an action at from the policy at ~ \u03c0(at|st, gt). After applying the action, the environment transitions to a new state st+1 according to the environment dynamics p(st+1|st, at), and the agent receives a reward rt = r(st, at, st+1, gt). The agent's objective is to learn a policy that maximizes the discounted cumulative reward:\n$J = E_{\\rho(\\tau | \\pi)} \\sum_{t=0}^{T} \\gamma^t r_t$ (1)\nwhere p(\u03c4|\u03c0) = p(s0)\u03a0t=0T\u22121p(st+1|st, at)\u03c0(at|st, gt) is the likelihood of a trajectory \u03c4 = (s0, a0, r0, . . ., sT\u22121, aT\u22121, rT\u22121, sT). The discount factor \u03b3 \u2208 [0, 1) determines the effective horizon of the policy."}, {"title": "3.2 Behavioral Cloning", "content": "The second stage of our approach leverages behavioral cloning (BC) to distill a teacher policy \u03c0*, trained through RL, into a more versatile student policy \u03c0, which can be directed through multi-modal inputs. The policy distillation process is performed using the DAgger method. In this online-distillation process, trajectories are collected by executing the student policy and then relabeled with actions from the teacher policy:\n$arg \\max_\\pi E_{(s,g)\\sim p(s,g|\\pi)} E_{a\\sim \\pi^*(a|s,g)} [log\\pi(a|s, g)]$. (2)\np(s, gr) denotes the distribution of states and goals observed under the student policy. This form of active behavioral cloning mitigates drift inherent in supervised distillation methods."}, {"title": "4 SYSTEM OVERVIEW", "content": "This work introduces a versatile controller for physics-based character animation. We aim to develop a scalable system that can learn a rich repertoire of behaviors from large and diverse multi-modal datasets. Our framework, illustrated in Figure 3, supports multiple control modalities, providing users with a flexible and intuitive interface for directing the behavior of simulated characters. Our framework consists of two stages. First, we train a fully-constrained motion tracking controller on a large mocap dataset. This controller's inputs consist of the full-body target trajectories of a desired motion. The fully-constrained controller is trained to imitate a wide variety of motions, including those involving irregular terrains and object interactions. Next, this fully-constrained controller is distilled into a more versatile partially-constrained controller. This partially constrained controller can be directed via diverse control inputs. The versatility of the partially-constrained controller arises from a masked training scheme. During training, the controller is tasked with reconstructing a target full-body motion given randomly masked inputs. This process enables the partially-constrained model to generate full-body motion from arbitrary partial constraints.\nStage 1: Fully-Constrained Controller. The goal of physics-based motion tracking is to generate controls (such as motor actuations), which enable a simulated character to produce a motion {qt} that closely resembles a kinematic target motion {\u011dt}. We represent a motion as a sequence of poses qt, where each pose qt = (pt, Ot) is encoded with a redundant representation consisting of the the 3D cartesian positions of a character's J joints pt = (po, p1, ..., pJ) and their rotations \u03b8t = (\u03b81, \u03b82, ..., \u03b8J). To successfully track a reference motion, controllers are typically provided with information that describes the motion it should imitate. For example, motion tracking controllers are commonly conditioned on the target future poses \u011dt. We will refer to target poses as fully-constrained goals gfull, since the future poses provide complete information about the target motion the character should imitate.\nStage 2: Partially-Constrained Controller. In this work, we propose a control model that extends beyond only conditioning on the full target poses to more versatile partially observable goals. For example, a typical problem in VR is to generate full-body motion from only head and hands sensors. Similarly, in some cases a controller may only observe an object (e.g., a chair) and will then be required to generate realistic full-body motions that interact with the target object. Throughout the paper, we will refer to partially observable goals as gpartial. These partial goals specify only some elements of a desired motion. To train a versatile controller that can be directed using partial goals, we propose a simple training scheme that trains the controller on randomly masked observations of target motions. These masked observations are constructed using a random masking function M : gpartial = M(gfull)."}, {"title": "5 FULLY-CONSTRAINED CONTROLLER", "content": "In the first stage of our framework, we train a fully-constrained motion tracking controller \u03c0FC using reinforcement learning. This controller can imitate a large library of reference motions across irregular environments and interact with objects when appropriate. Since the motion dataset only consist of kinematic motion clips, the primary purpose of \u03c0FC is to estimate the actions (motor actuations) required to control the simulated character. \u03c0FC then provides the foundations that greatly simplifies the training process of a more versatile controller in the subsequent stage."}, {"title": "5.1 Model Representation", "content": "Our fully-constrained controller is trained end-to-end to imitate target motions by conditioning on the full-body motion sequence and observations of the surrounding environment, such as the terrain and object heightmaps. The training objective is formulated as a motion-tracking reward and optimized using reinforcement learning. In this section, we detail the design of various components of the model.\nCharacter Observations: At each step, \u03c0FC observes the current humanoid state st, consisting of the 3D body pose and velocity, canonicalized with respect to the character's local coordinate frame:\n$st^{FC} = (Ot^{root}, (pt \u2013 poot)^{root}, vt^{root}),$ (3)\nwhere \\ denotes the quaternion difference between two rotations. In addition to the current state of the character, the policy also observes the next K target poses from the reference motion gfull = {\u011dt+1,..., \u011dt+K}. The features for each joint ft are canonicalized both relative to the current root, and relative to the current respective joint:\n$\u0135j = (\u00d4j^{0,\u00d4j^{0^{foot}}, (pi - p^{\u0135}^{0})^{root}, (pj - p^{foot})^{foot}).$ (4)\nThe features for each target pose \u011dt+k are also augmented with the time tt+k from the current timestep to the target pose, resulting in the following representation: ft+k = {ft+k\u2026\u2026\u2026, ft+ktt+k}.\nScene Observations. To imitate motions on irregular terrain, we canonicalize the character's pose with respect to the height of the terrain under the character's root (i.e. pelvis). During training, the controller is provided with a heightmap of the surrounding environment, with the heighmap oriented along the root's facing direction. The heightmap has a fixed resolution, and records the height of the nearby terrain geometry and object surfaces.\nActions: Similar to prior work we opt for proportional derivative (PD) control. We do not utilize residual forces or residual control. The policy's action distribution \u03c0FC (at|st, gfull) is represented using a multi-dimensional Gaussian with a fixed diagonal covariance matrix $\u03c3^\u03c0 = exp(-2.9)$."}, {"title": "5.2 Model Architecture", "content": "Motion tracking is a sequence modeling problem. The objective is to predict the next actions based on the current character state, surrounding terrain, and a sequence of future target poses. Inspired by the success of transformers in natural language processing, we tokenize each of the inputs and design \u03c0FC as a transformer-based controller. This choice of architecture allows the controller to attend to relevant information across the input sequence and capture the dependencies between the various input tokens.\nTo further enhance the learning process, we employ a critic network alongside the transformer-based controller. The critic is implemented as a fully connected network that estimates the value function. This provides a learning signal to guide the controller towards optimal actions.\nOnce trained, this fully-conditioned controller provides the foundation for our unified character controller. In the following section, we introduce our physics-based motion inpainting approach. This allows users to specify partial or sparse motion constraints, such as keyframes or high-level goals, and synthesize complete motion sequences that satisfy these constraints while remaining consistent with the scene context."}, {"title": "5.3 Reward Function", "content": "The reward rt encourages the character to track a reference motion by minimizing the difference between the state of the simulated character and the target motion:\n$r_t = \\{\\} + w_{jv}r_{jv} + w_{jav} r_{jav} + w_{eg}r_{eg},$ (5)\nwhere r{} denote various reward components and and w{} are their respective weights. The terms in the reward function encourages the character to imitate the reference motion's global joint positions (gp), global joint rotations (gr), root height (rh), joint velocities (jv), joint angular velocities (jav), as well as an energy penalty (eg) to encourage smoother and less jittery motions. A more detailed description of the reward function is provided in the supplementary material."}, {"title": "5.4 Training Playground", "content": "To train a controller that can operate in more complex irregular scenes, we construct a training environment, shown in Figure 4, composed of three distinct regions: (1) flat terrain, (2) irregular terrain, and (3) object playground.\nFlat Terrain. First, the flat terrain region is a simple environment where the model can focus primarily on imitating the reference motions, as most of the training data was recorded on flat ground. This region is a baseline for evaluating the model's ability to imitate motions in a simple, unobstructed setting.\nIrregular Terrain. The irregular terrain region contains a wide variety of irregular terrain features, including stairs, rough gravel-like terrain, and slopes (both smooth and rough). When the agent is imitating a motion that does not involve object interactions, it can be spawned at any random location within flat and irregular terrain regions. This setup exposes the model to diverse terrain conditions, allowing it to learn robust locomotion skills that can accommodate different types of terrains.\nObject Playground. Finally, the object playground region is reserved for object interaction motions. This region consists of various objects placed on flat ground, such as chairs, tables, and couches. Characters are only initialized in this region when they are imitating motions that involve object interactions."}, {"title": "5.5 Early Termination and Prioritized Motion Sampling", "content": "To improve the success rate on rare and more complex motions, we perform early termination . Motions performed on flat terrain, are terminated once any joint position deviates by more than 0.25 meters. On irregular terrains, an episode is terminated when a joint error exceeds 0.5 meters, providing the controller more flexibility to adapt the original reference motion to a new environment. Furthermore, we prioritize training on motions with a higher failure rate . As some motions are not expected to succeed in all scenarios (e.g., front-flip or cartwheel up a flight of stairs), the prioritized sampling only considers failures that occurred on flat terrain. The probability of prioritizing a motion mi is proportional to the probability of failing on that motion, clipped to a minimal weight of 3e-3. This adaptive sampling strategy is vital to ensure that the agent collects a sufficient amount of data to reproduce more dynamic and challenging behaviors."}, {"title": "6 VERSATILE PARTIALLY-CONSTRAINED CONTROLLER", "content": "Once the fully-constrained motion tracking model has been trained, it is then used to train a versatile partially-constrained model, denoted by \u03c0PC. The training and inference process are illustrated in Figure 3. Given partial constraints, such as target positions for joints, text commands, or object locations, MaskedMimic generates diverse full-body motions that satisfy those constraints. \u03c0PC is trained to model the distribution of actions \u03c0FC(at|gfull, st) predicted by the fully-constrained controller \u03c0FC, while only observing partial constraints gpartial. The partial constraints then provide users a versatile and convenient interface for directing \u03c0PC to perform new tasks, without requiring task-specific training."}, {"title": "6.1 Partial Goals", "content": "The objective of \u03c0PC is to produce motions that conform to constraints specified by partial goals, akin to the task of motion inpainting. In this work, we consider the following types of goals:\n(1) Any-joint-any-time: The model should support conditioning on target positions and rotations for any joint in arbitrary future timesteps.\n(2) Text-to-motion: The model should support high-level text commands, enabling more intuitive and expressive direction of the character's movements.\n(3) Objects: When available, the model should support object-based goals, such as interacting with furniture.\nTo produce a desired behavior, our model will support simultaneous conditioning on one or more of the aforementioned goals. For example, path following with raised arms can be achieved by conditioning the controller on a target root trajectory and a text command \"walking while raising your hands\". This flexibility allows for a wide range of complex and expressive motions to be generated from concise partial specifications.\nTo train \u03c0PC, flexible goals are extracted procedurally from mocap data by applying random masking. During training, \u03c0PC is trained to imitate the original full (unmasked) target motion by predicting the actions of the fully-constrained controller, which observes the ground-truth full target motion."}, {"title": "6.2 Modeling Diversity with Conditional VAES", "content": "Partial goals are an underspecified problem, as there may be multiple plausible motions that can satisfy a given set of partial goals. For example, when conditioned on reaching a target location within 1 second, there are a large variety of motions that can achieve this goal. To address this ambiguity, we model \u03c0PC as a conditional variational autoencoder (C-VAE). This generative model enables the \u03c0PC to model the distribution of different behaviors that satisfy a particular set of constraints, rather than simply producing a single deterministic behavior. By sampling from this learned distribution, the model can generate a variety of realistic and physically-plausible motions that adhere to the specified partial goals, while still allowing for natural variations and adaptability to different contexts.\nMaskedMimic consists of 3 components: a learnable prior p, an encoder &, and a decoder D. The encoder &(zt|st, gfull) outputs a latent distribution given the fully-observable future target poses from the desired reference motion. The decoder D(at|st, zt) is then conditioned on a latent sampled from the encoder's distribution, and produces an action for the simulated character. The final component is the learned prior p(zt|st, gpartial). The prior is trained to match the encoder's distribution given only partially observed constraints. The learnable prior is a crucial component of MaskedMimic's design as it allows the model to generate natural motions from simple user-defined partial constraints at runtime, without requiring users to specify full target trajectories for the character to follow. The encoder is used solely for training, and is not utilized at runtime.\nThe prior is modeled as a Gaussian distribution over latents zt, with mean \u00b5\u03c1 and diagonal standard deviation matrix \u03c3\u03c1,\n$p(zt|st, g^{partial}) = N (\u00b5P(st, g^{partial}), \u03c3P (st, g^{partial})).$ (6)\nThe encoder is modeled as a residual to the prior ,\n$8 (zt | St, g^{full}) = N (\u00b5P (st, g^{partial}) + \u00b5&(st, g^{full}), \u03c3P (st, g^{full}) + \u03c3&(st, g^{full})).\n(7)\nThis design ensures that the embedding from the encoder, having access to full observations of the target motion, stays close to the prior that only receives partial observations. During training the latent variables zt are sampled from the encoder. All component are trained using an objective that maximizes the log-likelihood of actions predicted by FC and minimizes the KL divergence between the encoder and prior:\n$E_{(s,g^{partial}) \\sim p(s,g^{partial}| \\pi^{PC})} E_{a \\sim \\pi^{FC}(a|s,g^{full})} E_{z \\sim & (z|s,g^{full})} [log D(a|s, z) - aDKL. (& (.s, g^{full})||p (.s, g^{partial}))]$, (8)\nwhere gpartial is constructed by applying a random masking function M to the original fully-observed goals: gpartial = M(gfull). In the formulation above, \u03c0PC interacts with the environment, while FC labels the target actions for every timestep. During inference, the encoder is discarded, and latents are sampled only from the prior p."}, {"title": "6.3 Training", "content": "We incorporate a number of strategies to improve the stability and effectiveness of the resulting MaskedMimic model. These strategies include: structured masking, KL-scheduling, episodic latent noise, and observation history. Furthermore, during the distillation process, deterministic actions are sampled from both FC and PC to reduce stochasticity during data collection. Early termination is also applied during distillation to prevent \u03c0PC from entering states that were not observed during the training of \u03c0FC. Since \u03c0FC also trains with early termination, it may not provide appropriate actions in regions it has not experienced during training.\nMasking. Our masking process randomly removes individual target joints, the textual description, and the scene information (when applicable) from the input goals to the model. To better ensure temporally coherent behaviors, we leverage a masking scheme that is structured through time. A randomly sampled mask in one timestep has a chance of being repeated for multiple subsequent timesteps, as opposed to randomly re-sampling the mask at each step.\nWe observe that randomly re-sampling the mask on each step reduces the ambiguity the model encounters during training. Therefore, the resulting model generalizes worse. This is because different joints are likely to be visible across different frames, the cross-frame information provides a less ambiguous description of the requested motion. By using a temporally consistent sampling scheme, we ensure that certain joints are observed for multiple consecutive frames, while other joints remain consistently hidden.\nTo ensure the model supports high-level goals, such as text-commands and interaction with a target object, all future poses can be masked out. This structured sampling mechanism guarantees that \u03c0PC encounters, and learns to handle, a range of different masking patterns during training. This results in increased robustness to possible user inputs.\nKL-scheduling. Similar to \u03b2-VAE, we initialize the KL-coeff with a low value of 0.0001, and linearly increase its value to 0.01 over the course of training. Starting with a low KL coefficient enables the encoder-decoder to more closely imitate \u03c0FC. Increasing the coefficient then encourages the model to impose more structure into the learned latent space, to be more amenable to sampling from the prior at runtime.\nEpisodic latent noise. During training, latents are sampled via the reparametrization trick. To further encourage more temporally consistent behaviors, we keep the \"noise\" parameter \u0454 ~ N(0, 1) fixed throughout the entire episode. Therefore, in each episode \u03c4 the latent variables are sampled according to z\u0142 = \u0454\u03c4\u03c3 + \u03bc\u03b5, and the noise \u0454 is constant throughout an episode.\nObservation history. When conditioning on text commands, we find that providing \u03c0PC with past poses is crucial for generating long coherent motions that conform to the intent of a given text command. Therefore, following , we provide the prior with 5 observations subsampled from the observations in the past 40 timesteps."}, {"title": "6.4 Observation Representations", "content": "We construct a representation for each type of input modality that \u03c0PC can receive as input. The objective is to provide a sufficiently rich representation, that is also computationally efficient and facilitates generalization to new tasks.\nKeyframes. A future keyframe with partially observable joints is first canonicalized to the current pose (Equation (3)). The unob-served joints are then zeroed out, and the mask is appended alongside the time to reach the target frame t [\u011dt+t*maskt+t, maskt+\u03c4, \u03c4]. Observations of poses from previous timesteps are represented in a similar fashion, but all the joints are observed and no masking is applied.\nObjects. We represent objects using the positions of the 8 corners of a bounding-box, canonicalized to the character's local coordinate frame. To identify different types of objects, we also provide an index representing the object type (e.g., chair, sofa, stool).\nText. Each text command is encoded using XCLIP embeddings , which are trained on video-language pairs to better capture temporal relationships. By leveraging the spatio-temporal information in videos during training, the XCLIP embeddings can encode the temporal aspects of language crucial for describing motions, making them well-suited for representing text commands to be translated into character animations."}, {"title": "6.5 Architecture", "content": "To provide a unified architecture capable of processing multi-modal inputs, we model the prior R using a transformer-encoder. This enables variable length input tokens depending on the observable goals at each timestep. Each input modality (target pose \u011dt+t, object bounding box ot, terrain heightmap ht, current pose st, text wt, and historical pose qt-t) has a unique encoder that is shared across all inputs of the same modality. When an input is masked out, we utilize the transformer masking mechanism to exclude the respective tokens. The output of the transformer is provided to two fully-connected layers to output the mean and log-standard deviation for the prior distribution.\nSince the encoder always observes the full target frames as input, it is represented as a fully connected model, as its inputs are always a fixed size. The encoder observes the full future poses \u011dt+t in addition to the masking applied to the keyframes, indicating which joints are visible to the prior. In addition, it observes the current pose st and the terrain heightmap ht. Like the prior, two fully-connected output heads output the residual mean and the logstd for the encoder.\nSimilarly, the decoder is also modeled as a fully-connected network. It observes the current state st, the sampled latent zt, and the terrain heightmap ht. The decoder then outputs a deterministic action at. A high level illustration is provided in Figure 5b."}, {"title": "7 EXPERIMENTAL SETUP", "content": "All physics simulation are performed using Isaac Gym , each with 16,384 parallel environments, split across 4 A100 GPUs. Models are trained for approximately 2 weeks, amounting to approximately 30 (10) billion steps for \u03c0FC (\u03c0PC). The controllers operate at 30 Hz, and the simulation runs at 120 Hz. Detailed hyperparameter settings are available in the supplementary material. When training \u03c0PC, we train with joint conditioning for a key subset of body parts: Left Ankle, Right Ankle, Pelvis, Head, Left Hand, and Right Hand."}, {"title": "7.1 Datasets", "content": "To train a unified controller capable of being directed using different control modalities, our models are trained using an aggregation of multiple datasets that collectively provide a range of different modalities.\nKeyframe Conditioning. The core of our data is the AMASS dataset . AMASS contains mocap recordings for a wide range of diverse human behaviors, without scene information or text. From this dataset, we extract the joint positions, rotations, and their relative timings. This enables any-joint-any-time conditioning, where a controller can be conditioned on target positions or rotations at various future timesteps. To improve generalization to new and unseen motions, we mirror the motions (flip left-to-right) as a form of data augmentation. However, it has been observed in prior work that some motions in the AMASS dataset contain severe artifacts , including non-physical artifacts such as intersecting body parts, floating, or scene interactions without objects (such as walking up a staircase that does not exist). We follow the same filtering process as PHC to obtain a filtered dataset.\nText Conditioning. To enable text control, we utilize the HumanML3D dataset . HumanML3D provides a breakdown of the AMASS motion sequences into atomic behaviors, each labeled with text descriptions. This allows MaskedMimic to be conditioned on text commands, providing a more intuitive and expressive way to direct the character's movements. We use the mirrored-text for mirrored motions, for example, \"a person turns left\" is converted to \"a person turns right\".\nScene Interaction. To synthesize natural interactions between characters and objects, we utilize the SAMP dataset . The SAMP dataset contains motion clips for interacting with different categories of objects, in addition to meshes of the corresponding objects for each motion clip. To train models that are able to interact with a wider array of objects, objects are randomly sampled within the class of objects associated with each motion clip Hassan et al. . For example, multiple different armchair models can be used to train the same sitting behavior, allowing our model to be conditioned on different target objects for interaction at runtime."}, {"title": "7.2 Evaluation", "content": "To evaluate the effectiveness of our framework, we construct a benchmark consisting of common tasks introduced by prior systems. For each tasks, we report a success rate metric and an error rate metric. Both are aimed to complement one another. During evaluation, the MaskedMimic model is conditioned on the mean of the prior's latent distribution. Sampling latents randomly produces more diverse behaviors, however, it can also produce less-likely solutions that are more likely to fail. This results in a slight degradation of performance with respect to the raw tracking metrics.\nIn all the evaluations, we analyze the performance of a unified model. This model is not fine-tuned for any specific task, but instead, it is controlled through user-specified goals or goals extracted from kinematic recordings (for the motion-tracking tasks). Similarly, the qualitative results showcase motions generated from new user-generated goals or tracking new motions that were not observed during training. Qualitative results are best viewed in the supplementary video.\nFull-body tracking. We begin by evaluating both the fully constrained controller FC and partially-constrained controller Masked-Mimic PC on the task of full-body motion tracking. Given a target motion, the controllers are required to closely track the sequence of future target poses from the target motion. For this task, all features of the target future poses are fully observed by the controllers. This test establishes the baseline capability for motion generation, both in terms of success rates and tracking quality, and allows comparison to prior systems for motion tracking.\nJoint sparsity. To evaluate the model's effectiveness for generating plausible motions from partial constraints, we first consider the task of VR tracking. In this task, the controllers no longer observe the full target poses. Instead, they are provided with the target head position and rotation, in addition to the hand positions . Note, MaskedMimic models are not explicitly trained for VR tracking. We compare to the results reported in Luo et al. , consisting of 3 baselines: PULSE , ASE , and CALM . In addition, we evaluate the ability of tracking motions given varying joint targets. This is made possible by MaskedMimic's support for any-joint conditioning during inference.\nIrregular terrains. To evaluate the robustness of our models to variations in the environment, we evaluate the performance of FC and PC on both full-body imitation and VR-tracking when spawned on randomly generated irregular terrains. The terrain consists of rough (gravel like) ground, stairs, and slopes (rough and smooth). Similar to the previous experiments, goals are still extracted from human motion data from AMASS. The controller is therefore evaluated on its ability to closely imitate a large variety of motions while accommodating irregular terrains."}, {"title": "7.3 Tasks", "content": "By training MaskedMimic on randomly masked input goals, the model learns a versatile interface that can be easily used to direct the controller to perform new tasks. Performing new tasks often requires generalization to new and unseen scenarios. In this section, we evaluate MaskedMimic's ability to generalize and handle user-defined goals, which the model was not explicitly trained on. To direct the model to perform new tasks, we construct simple finite-state-machines that transition between goals provided to the controller. This form of goal-engineering (akin to prompt-engineering for language models) enables MaskedMimic to perform a range of new tasks, without additional task-specific training.\nPath-Following. The character is tasked with following a 3D path. This path specifies target positions for the head (including height) at each timestep. By varying the target locations, the character can be directed to perform different locomotion styles, such as walking, crouched-walking, and crawling. The paths are randomly generated with varying heights and speeds. Each episode has a length of 30 seconds.\nSteering. The steering task is analogous to a joystick controller, where the character's movements are controlled by two target direction. One direction specifies the target heading direction and the other specifies the target direction (and speed). This enables separating the control between the direction the character should face and the direction it should move.\nReach. In addition to locomotion, we show that MaskedMimic can also be used to perform more fine-grained control over the movement of individual body parts. The goal of the reach task is for the right hand to reach a randomly changing target position. Once the target position changes, the character has 2 seconds to reach the position with its hand and stay at that location.\nObject Interaction. Finally, we show that MaskedMimic can also be directed to generate natural interactions with objects by conditioning the model on features of a target object. In this task, we focus on sitting on a set of held-out objects. These objects were not used during the training phase. The character is first initialized at a random location between 2 and 10 meters away from the object."}, {"title": "8 RESULTS", "content": "The MaskedMimic framework enables versatile physics-based character control by formulating the problem as motion inpainting from partial constraints. In this section, we present key results demonstrating the effectiveness and flexibility of our approach."}, {"title": "8.1 Motion Tracking", "content": "Tables 1, 2 and 4 record the performance statistics for the motion tracking task, and Figure 6 shows examples of the behaviors our model produces. MaskedMimic exhibits robust behaviors and improved generalization to new (test) motions compared to prior motion tracking models. When provided full-body targets, MaskedMimic tracks the entire motion of a karate fighter and a dancer, matching hand and foot positions of a fighting stance. When provided with partial target poses, for example recovering full motion from sparse VR sensors, MaskedMimic succeeds in reproducing a cartwheel across irregular terrain from only sparse VR constraints, as well as running while following a target trajectory for the head.\nFull-body tracking. Table 1 shows the performance on the full-body motion tracking task. Performance is evaluated on both the AMASS train and test splits. We consider a trial \"failed\" if at any frame the average joint deviation is larger than 0.5m . To complement the success metric, we report the MPJPE (Mean Per Joint Position Error, in millimeters). MPJPE measures how closely the character can track the target joint positions (in global coordinates).\nOur fully-constrained tracker FC outperforms PHC+ , reducing the tracking failure rate on unseen motions by 62.5%. In addition to a lower failure rate, our controller also supports a wider range of motions, irregular terrains, and object interactions. We attribute these performance improvements to our architecture and data augmentation techniques.\nA key difference in our approach is the use of a single unified network, in contrast to the mixture-of-experts (MoE) model employed by PHC. While MoE approaches, such as training different controllers for each task or using a progressively growing mixture of multi-motion trackers, have been applied to fully-constrained tracking problems, they present certain challenges. As motion variety increases, maintaining multiple experts becomes difficult in an online distillation (DAgger) regime. Moreover, the MoE architecture in PHC requires an additional gating network to select and blend between multiple networks (experts) depending on the target motion.\nOur experiments demonstrate that a single monolithic network offers better generalization capabilities. This approach not only simplifies the architecture but also avoids the complexities associated with expert selection and blending. The superior performance of our model suggests that, in the context of full-body tracking, a well-designed unified network can effectively capture the diversity of motions without the need for specialized experts.\nWhen comparing MaskedMimic with PULSE , we observe that PULSE exhibits more pronounced overfitting to the training data, while MaskedMimic demonstrates superior generalization performance on the test set. This distinction can be attributed to"}]}