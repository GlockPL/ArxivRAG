{"title": "TSO: SELF-TRAINING WITH SCALED PREFERENCE OPTIMIZATION", "authors": ["Kaihui Chen", "Hao Yi", "Qingyang Li", "Tianyu Qi", "Yulan Hu", "Fuzheng Zhang", "Yong Liu"], "abstract": "Enhancing the conformity of large language models (LLMs) to human preferences remains an ongoing research challenge. Recently, offline approaches such as Direct Preference Optimization (DPO) have gained prominence as attractive options due to offering effective improvement in simple, efficient, and stable without interactions with reward models. However, these offline preference optimization methods highly rely on the quality of pairwise preference samples. Meanwhile, numerous iterative methods require additional training of reward models to select positive and negative samples from the model's own generated responses for preference learning. Furthermore, as LLMs' capabilities advance, it is quite challenging to continuously construct high-quality positive and negative preference instances from the model's outputs due to the lack of diversity. To tackle these challenges, we propose TSO, or Self-Training with Scaled Preference Optimization, a framework for preference optimization that conducts self-training preference learning without training an additional reward model. TSO enhances the diversity of responses by constructing a model matrix and incorporating human preference responses. Furthermore, TSO introduces corrections for model preference errors through human and AI feedback. Finally, TSO adopts iterative and dual clip reward strategies to update the reference model and its responses, adaptively adjusting preference data and balancing the optimization process. Experimental results demonstrate that TSO outperforms existing mainstream methods on various alignment evaluation benchmarks, providing significant insight into preference data construction and model training strategies in the alignment domain.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning from Human Feedback (RLHF) has emerged as an effective method to fine-tune Large Language Models (LLMs) to align better with human users' expectations Schulman et al. [2017], Rafailov et al. [2024], Achiam et al. [2023]. It utilizes algorithms like Proximal Policy Optimization (PPO, Schulman et al. [2017]) and Direct Preference Optimization (DPO, Rafailov et al. [2024]). While PPO is known for its relatively good sample efficiency, it is challenging to train online and demands extensive tuning of hyperparameters. On the other hand, DPO is a lightweight and offline algorithm that directly optimizes policies, offering greater flexibility and easier implementation compared to PPO.\nHowever, offline preference optimization methods highly rely on the quality of pairwise preference samples. It is still challenging to continuously construct high-quality positive and negative preference instances as the ability of LLM improves without an explicit reward model. To address these issues, we propose that high-quality preference"}, {"title": "2 Related Work", "content": "RLHF & RLAIF Although instruction fine-tuning and SFT can somewhat enhance the models' alignment with human preferences, these methods are heavily dependent on the quality of data, which incurs significant time and monetary costs Yao et al. [2023], Touvron et al. [2023]. Alignment techniques, such as RLHF and RLAIF, leverage human or AI feedback to modulate and steer LLMs' behavior, thereby enhancing LLMs' comprehension of human requirements and refining their responses for improved alignment. Ouyang et al. [2022] employ a reward model derived from preference data as the reward function in the actor-critic approach, utilizing reinforcement learning techniques like Proximal Policy Optimization (PPO, Schulman et al. [2017]) to optimize the target policy. Rafailov et al. [2024] uses the target policy"}, {"title": "3 Self-Training with Scaled Preference Optimization (TSO)", "content": "In this section, we first outline the process of creating preference pairs using the model matrix, which involves cross-version response augmentation and cross-scale response augmentation. Next, we introduce human and AI feedback to correct validity bias in base model. Finally, we discuss our training strategy, the mini-batches iterative DPO, and introduce dual clip reward loss to balance the optimization process for both positive and negative responses."}, {"title": "3.1 Model Matrix Instructions Construction", "content": "As indicated by previous work Kaplan et al. [2020], there is a positive correlation between model size and capability. Larger models exhibit better performance, and models of new versions generally outperform older models during the iterative process. Consequently, we start from these two dimensions, integrating the model matrix to further increase the quality of positive instruction responses and the diversity of negative instruction responses.\nAt first, we introduce the definition of model matrix M. See Definition 1.\nDefinition 1 M : {Mv,s}v\u2208\u03bd,ses is a model matrix, where V denotes the model's version set and S denotes the model's size set. The element in M is Mv,s : (X, Y) \u2192 [0, 1]\u00a3 denotes the model distribution of version v and size s, where X is the set of prompts and y denotes the set of responses with a maximum length of L.\nAssuming our base model is Mub,st, we define the model set to generate chosen responses Mw and the model set to generate rejected responses Mr. See Equation 1 and Equation 2."}, {"title": "3.2 Evaluation Correction", "content": "The Self-Reward Yuan et al. [2024] method treats the model as an evaluator to score the generated responses. However, this process does not include human feedback, which can lead to out-of-distribution evaluation results. To incorporate human preferences, we use a scored dataset from human feedback to perform supervised fine-tuning on the model.\nSpecifically, we first utilize a base model and train a evaluator MFT according to the LLM-as-a-Judge Zheng et al. [2024] manner. Furthermore, we utilize the evaluation model to evaluate the inference results of the base model and expand the response dataset constructed by the model matrix. The expanded instruction dataset is as follows:"}, {"title": "3.3 Mini-Batches Iterative DPO", "content": "Drawing inspiration from the deep learning concept of mini-batches, we evenly divides the preference dataset into T mini-batches. Each DPO training session only processes a single mini-batch and continuously updates the responses of the reference model, aiming to fully exploit the potential of the preference dataset.\nMeanwhile, We found that during the optimization process using the original DPO loss (Equation 11), negative responses always had a dominant advantage. To balance the optimization process of positive and negative responses, we propose the Dual Clip Reward Loss(Equation 10)."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\n4.1.1 Base Model\nWe utilize transformer-based models with a LLaMa-like Touvron et al. [2023] architecture as our base model, which has 66B parameters (Mbase). Building upon this base model, we conduct further experiments to align the model with human preferences. Additionally, our model matrix also includes different versions and sizes of 13B, 66B and 175B models. To simplify the description, we denote these different sizes model as TSO-M-13B, TSO-M-66B, TSO-M-175B. For more details about the model architecture, please refer to Appendix B.\n4.1.2 Datasets\nWe've compiled a collection of 30,000 questions from both public datasets and our own sources. The public and custom datasets we constructed are summarized as follows:"}, {"title": "4.1.3 Evaluation Benchmark", "content": "To evaluate the human preference alignment effect of TSO, we employed both publicly available and proprietary automatic evaluation datasets, including AlignBench Liu et al. [2023b], MT-Bench Zheng et al. [2024], AlpacaEval-v2 Li et al. [2023], Arena-Hard Li et al. [2024], and our proprietary evaluation dataset TSO-Self-Bench-2K. All models subject to evaluation adhere to the same evaluation hyperparameters settings, ensuring the fairness and reproducibility of the results. More detailed information regarding the evaluation sets and evaluation hyperparameters settings are provided in Appendices C.3."}, {"title": "4.1.4 Training Detail", "content": "The specifics of the models within the model matrix are delineated in Appendix B. The utilized training hyperparameters are detailed as follows: for TSO training, the Adam optimizer Kingma and Ba [2014] is deployed, configured with a learning rate of 1 \u00d7 10-6, a weight decay rate of 0.05, Adam \u03b2\u2081 of 0.9, and Adam \u1e9e2 of 0.95. The strategy for learning rate adjustment employs a cosine function with a warm-up mechanism, where the learning rate decreases to a minimum of zero. For the preference dataset, a batch size of 256 is used, with each TSO training cycle consisting of two epochs. Following the original DPO setup Rafailov et al. [2024], both LDPO and Ldual-clip set \u03b2 at 0.1. The number of mini-batches T is 3, and the total number of iterations N is 3. Training of the TSO-M-66B DPO utilizes 64 Nvidia A800 GPUs, each with 80GB, processing an average of approximately 20 samples per second. Additional training specifics are provided in Appendix C."}, {"title": "4.2 Main Result", "content": "Utilizing the 66B base model (Mbase), we conduct a three-stage TSO optimization. At each stage, we deploy a total of 30K preference pair from a multi-model response preference dataset, supplemented by preference data refined by the model itself. To ensure consistency in data volume across comparative experiments such as DPO and IPO, we employ 30K preference pair from a single-model response preference dataset across three iterations. Results from multiple alignment evaluation sets are displayed in Table 2.\nCompared to Mbase, TSO-3 exhibited improvements of 0.88 and 0.56 on AlignBench and MT-Bench, on AlpacaEval-v2, the length-controlled win rate increased by 11.96%, and on Arena-Hard, it increased by 15.4%, respectively, indicating substantial effectiveness over the traditional DPO method.\nFurthermore, while the DPO method demonstrates effectiveness and stability across various evaluation sets, TSO-2 has consistently outperformed DPO. TSO-3 further enhances the foundation established by TSO-2, showing no signs of performance deceleration. Specifically, on AlignBench, the progression from TSO-1 to TSO-2 result in an increase of 0.29, and the advance from TSO-2 to TSO-3 yield an additional 0.22. This suggests that the diversity of negative samples derived from the multi-model response preference dataset, coupled with self-corrections by the model, benefits Mini-Batches Iterative DPO training.\nNegative Response Distribution To investigate the relationship between the negative response distribution engendered by various models and the base model, we streamlined the data generation process. We employed solely human and the latest version TSO-M-175B generated responses as positive inputs, and responses from a singular, weaker model as negatives, executing a single round of TSO training. Results are detailed in Table 3 and Table 4."}, {"title": "4.3 Ablation", "content": "In Section 3, we constructed multi-model preference data through a model matrix, significantly expanding the diversity of the training data. This was further refined through human and AI feedback for validity bias correction. Ultimately, we employed training strategies using Mini-Batches Iterative DPO and Dual Clip Reward Loss to update model responses and balance the optimization of positive and negative responses. To validate the effectiveness of our method, we posed the following questions and conducted experiments to address them systematically: Q1). Does the multi-model preference dataset help improve the alignment effect? Q2). Has Ldual-clip shown improvement compared to LDPO? Q3). Does the design of Mini-Batches Iterative DPO strategy have an effect? Q4). How is the model's evaluation and correction capability?"}, {"title": "4.3.1 Q1.", "content": "To ensure fairness in comparative evaluations, we have refined the Self-Reward methodology Yuan et al. [2024], hereafter referred to as Self-Reward\u2020. Unlike the original method, we employ an external reward model (Qwen2 72B Yang et al. [2024]) rather than the model itself to assess multiple generated responses from various dimensions, such as comprehension, conciseness, factuality, and logic. We select the response with the highest average score as the positive and that with the lowest score as the negative to generate preference data for DPO training. Similar to TSO, both methods implement a three-stages iterative learning process, continuously updating the reference model."}, {"title": "4.3.2 Q2.", "content": "To ascertain the effectiveness of Ldual-clip, we executed an ablation study by comparing its performance with that of the original DPO loss across multiple alignment evaluation sets, as delineated in Section 3. To streamline the training process, we extracted a subset of 10K preference data points and a single model response as the negative response from the initial 30K data points for training. The experimental results are delineated in Table 6. The findings demonstrate that, relative to the original DPO loss, our Ldual-clip achieves superior outcomes on several publicly available alignment evaluation sets, under identical base models and datasets.\nReward Curve & Explanation: we plot the changes for positive and negative responses' rewards during the optimization process, as shown in Figure 3 and Figure 4.\nIt can be observed that using the LDPO, due to the coupling of the positive and negative response losses, the negative responses dominate throughout the optimization process, leading to a decrease in the rewards for positive responses. Compared to the LDPO, Ldual-clip shows similar behavior to the LDPO in the early stages of optimization because neither positive nor negative responses are truncated during the initial phase. In the middle and later stages of optimization, the reward for positive responses increases, while the reward for negative responses decrease. This is due to the smaller margin for negative responses. The loss from negative responses begins to be truncated and thus ceases to contribute to the optimization process, while the effect of negative response optimization on positive responses decreases, resulting in an upward trend in the rewards for positive responses. This demonstrates that Ldual-clip can balance the optimization processes for positive and negative responses.\nMeanwhile, as \u03b3\u03b9 increases, the final rewards obtained by both positive and negative samples are generally reduced, and the absolute value of the reward margin gradually increases. Moreover, since Ldual-clip avoids the coupling of optimization between positive and negative samples, as \u03b3\u03b9 increases, the reward margin of CLIP will gradually surpass that of DPO."}, {"title": "4.3.3 Q3.", "content": "In the methodology outlined in Section 3, we segmented the dataset, updating the reference model's probability response to immediately reflect the target model's after each update and learning cycle within a single minibatch. However, does it enable better learning from the preference dataset? To investigate this, we intend to evenly split the original dataset (30K) into three distinct segments, with each DPO training session handling only mini-batches of 10K. To streamline the training process, responses from a single model were employed as negative responses. The comparative outcomes are presented in Table 7. The results indicate that the outcomes in the third stage excel over those of the initial DPO settings under equivalent data length. The rationale behind this phenomenon, from the perspectives of data utilization efficiency and gradient orientations, is elaborated in Appendix D.1."}, {"title": "4.3.4 Q4.", "content": "To validate the correction capabilities of the model's scoring ability following human and AI feedback, we designed the experiment described below. Initially, the unmodified Mbase directly scored the QA pairs generated by our model matrix, based on predefined criteria: factuality, conciseness, logic, and comprehension. Subsequently, we constructed a scoring dataset comprising both human and AI feedback, which was used to train the base- line model by Supervised Fine-Tuning. The fine-tuned model (Mbase-SFT) then scored the QA pairs using the same criteria to assess whether the supervised fine-tuning had corrected some of the samples that were out-of-distribution (OOD). We list two examples in Table 10 and 11. The former indicates that the Mbase-SFT corrected the evaluation for good cases among OOD samples, while the latter shows that the Mbase-SFT corrected the evaluation for bad cases among OOD samples."}, {"title": "5 Conclusion", "content": "We introduce TSO, a direct preference optimization method based on multi-model responses. By generating diverse responses through a model matrix, this approach aims to augment the variety of the preference dataset. Additionally, it incorporates feedback from both humans and AI to enhance the model's evaluation and correction capabilities, and to mitigate the preference deficiencies arising from solely relying on self-model response adjustments. The training strategy employed includes Mini-Batches Iterative DPO and Dual Clip Reward Loss. Our experiments validate the effectiveness of TSO and various training strategies, and confirm that the improvements in alignment are due to the response diversity provided by the model matrix. Furthermore, we explore the relationship between the distribution of negative responses and the foundational model, providing insights into the construction of preference pairs."}, {"title": "A Data", "content": "A.1 Data Distribution Analysis"}, {"title": "B Details of the Model Architecture", "content": "In our model matrix, we utilized three differently-sized models, including 13B, 66B, and 175B, all of which are based on the Llama2 Touvron et al. [2023] architecture. Specific architectural details of the models are shown in Table 8.All models in model matrix has been autoregressively pre-trained on several terabytes of corpora and subsequently supervised fine-tuned using a meticulously curated instructions datasets."}, {"title": "C Details of Training and Evaluation", "content": "C.1 Hardware and Software\nWe conducted our training using eight machines equipped with Intel (R) Xeon (R) Platinum 8468 processors featuring 40 cores and 500 GiB, each machine outfitted with 8 Nvidia 80GB A800 GPUs. The operating system used is Ubuntu 20.04.6. Pytorch's version is 2.1.0a0+gitdalccca.\nC.2 Training\nC.2.1 Method Introduction\nIn the comparative experiment 4.2, we employed the DPO, IPO, cDPO, RSO, and PPO methods. The details for TSO and DPO have already been extensively discussed in Section 3.3. We now supplement the loss functions used for IPO and CDPO. The definitions for the IPO and cDPO losses are presented in Equation 12 and Equation 13."}, {"title": "C.2.2 Hyperparameters Setting", "content": "For the training of TSO, DPO, IPO, CDPO, we uniformly use the same experimental configuration. Initially, we set the learning rate to le - 6 and employ a cosine scheduler to facilitate the reduction of the learning rate. The constraint coefficient for weight L2 regularization is set at 0.05, and the gradient norm clipping threshold is set at 1.0. Additionally, we use the Adam optimizer with parameters B\u2081 = 0.9, \u03b22 = 0.95, and e = 1e \u2013 8. The random seed is fixed at 43. The \u1e9e in Equation 10, 11, 12, 13 is all set to 0.1. Batch size for all above methods is 256.\nFor the clipping margins Yw and \u03b3\u03b9 in Ldual-clipof the TSO, we set them to 20 and 10 respectively."}, {"title": "C.3 Evaluation Hyperparameters", "content": "During the evaluation phase, we use a uniform inference setup for all models. Temperature is set to 0.7, TOPp for decoding is set to 0.9 and the maximum input and output token length is set to 2048."}, {"title": "C.4 Evaluation Benchmark", "content": "AlignBench Liu et al. [2023b] functions as a comprehensive, multi-dimensional benchmark for assessing the alignment performance of Chinese large language models. AlignBench has implemented a human-involved data construction process to ensure the dynamic updating of evaluation data. It utilizes a multi-dimensional, rule-calibrated model evaluation approach (LLM-as-Judge) and integrates Chain-of-Thought to produce multi-dimensional analyses and a definitive comprehensive score for model responses, thereby enhancing the evaluation's reliability and interpretability. We deploy GPT4-0613 to conduct multi-faceted evaluations of the model-generated outcomes, ranging from {1, 2, ..., 10}. The evaluation dimensions of AlignBench are displayed in Table 9.\nMT-Bench Zheng et al. [2024] is a challenging multi-turn benchmark that measures the ability of large language models (LLMs) to engage in coherent, informative, and engaging conversations. It is designed to assess the conversation flow and instruction-following capabilities of LLMs, making it a valuable tool for evaluating their performance in understanding and responding to user queries. We use GPT4-0613 to conduct multi-dimensional evaluations on the multi-round results generated by the model, with the rating scale ranging from {1, 2, ..., 10}. The evaluation dimensions of MT-Bench are displayed in Table 9."}, {"title": "D Ablation Supplementary", "content": "D.1 Mini-Batches Iterative DPO\nGradient Explanation Next, we will explain from the perspective of gradient magnitude why the introduction of mini-batches offers an advantage over the original DPO in terms of data information utilization efficiency. The derivative of LDPO with respect to the model training parameters @ can be obtained as follows:"}, {"title": "D.2 Evaluation and Correction Capability", "content": "To validate the correction capability in the model's scoring ability after receiving human and AI's feedback, we designed the following experiment. Initially, Mbase, with- out any modifications, directly scored the QA pairs, which is generated from our model matrix, based on predefined criteria (factuality, conciseness, logic, and comprehension). Subsequently, we constructed a scoring dataset comprising both human and AI feedback, which was used to train the base- line model by Supervised Fine-Tuning. The fine-tuned model (Mbase-SFT) then scored the QA pairs using the same criteria to assess whether the supervised fine-tuning had corrected some of the samples that were out-of-distribution (OOD). We list two examples in Table 10 and 11. The former indicates that the Mbase-SFT corrected the evaluation for good cases among OOD samples, while the latter shows that the Mbase-SFT corrected the evaluation for bad cases among OOD samples."}]}