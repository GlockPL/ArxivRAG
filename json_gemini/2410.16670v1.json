{"title": "COPS: Empowering LLM Agents with Provable\nCross-Task Experience Sharing", "authors": ["Chen Yang", "Chenyang Zhao", "Quanquan Gu", "Dongruo Zhou"], "abstract": "Sequential reasoning in agent systems has been significantly advanced by large language\nmodels (LLMs), yet existing approaches face limitations. Reflection-driven reasoning relies solely\non knowledge in pretrained models, limiting performance in novel scenarios, while experience-\nassisted reasoning often depends on external experiences and lacks clear principles for selecting\nrepresentative experiences. We address these limitations by proposing COPS (Cross-Task\nExperience Sharing), a generalizable algorithm that enhances sequential reasoning by cross-task\nexperience sharing and selection. In detail, COPS leverages agents' experiences on previous\ntasks, selecting distribution-matched experiences via a provable pessimism-based strategy to\nmaximize utility while minimizing risks from distribution shifts. Extensive experimental results\non benchmarks like Alfworld, Webshop, and HotPotQA demonstrate that COPS consistently\noutperforms state-of-the-art baselines, with superior sample efficiency suitable for resource-\nconstrained scenarios. Theoretically, we show that the performance of our algorithm depends on\nboth the quality of the pretrained LLM and the matching between the agent's task-dependent\ntrial distribution and that generated by the LLM. Our work bridges the gap between existing\nsequential reasoning paradigms and validates the effectiveness of leveraging cross-task experiences,\nshedding light on the potential to improve agents' generalization and adaptability across diverse\ntasks.", "sections": [{"title": "1 Introduction", "content": "Burgeoning agent systems driven by advanced large language models (LLMs, (Devlin et al., 2019;\nBrown et al., 2020; OpenAI, 2023; Hu et al., 2024a)) have demonstrated remarkable capabilities in\nsolving complex tasks through sequential reasoning (Qin et al., 2024; Hao et al., 2023; Huang et al.,\n2024b; Chen et al., 2024c,b; Li et al., 2023a). These agent systems employ two typical sequential\nreasoning paradigms: reflection-driven reasoning and experience-assisted reasoning. Reflection-\ndriven reasoning leverages a model's internal capabilities through methods such as reflection (Shinn\net al., 2024), long-term rollouts (Zhou et al., 2023), or chain-of-thought (CoT) reasoning (Wei et al.,\n2022). While this approach capitalizes on the knowledge within the pre-trained model, it faces\nnotable limitations. Specifically, relying solely on existing knowledge in the pre-trained model to\ngenerate rationales restricts the model's performance when encountering novel scenarios. Moreover,\nthere is an increased risk of hallucinations, where internal reasoning may lead to plausible but\nincorrect responses (Huang et al., 2023). These challenges highlight the need for integrating external\nexperiences to enhance the agent's sequential reasoning capabilities.\nIn contrast, experience-assisted sequential reasoning utilizes retrieval-based methods that enable\nthe agent to interact with a memory bank of experiences, allowing the model to overcome knowledge\ncutoffs, personalize responses, and reduce hallucinations. However, these experiences are often\nmanually curated or sourced from expert models (Raparthy et al., 2023), which is resource-intensive\nand poses scalability issues. Additionally, experience-assisted reasoning often lacks clear principles\nfor selecting representative examples (Kagaya et al., 2024), potentially underutilizing the value of\npast experiences. These limitations bring us to a critical research question:\nCan agent systems enhance sequential reasoning by sharing and selecting cross-task experiences?\nTo address this question, we propose CoPS (Cross-Task Experience Sharing), a theoretically\ngrounded algorithm that empowers agent systems through cross-task experience sharing and\nselection. CoPS demonstrates its generalizability by working effectively in both settings: utilizing\nfully external experiences in the offline setting and leveraging completely self-derived experiences\nin the online setting. By utilizing representative cross-task experiences, CoPS enables agents\nto improve performance on new, complex sequential reasoning tasks. Our key contributions are\nsummarized as follows:"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 LLM-powered Agents", "content": "In recent years, there has been a significant surge in research focused on LLM-powered agents\n(Chen et al., 2024c,b; Chan et al., 2023). React (Yao et al., 2022b) laid the foundation for much\nof the subsequent work on LLM agents, particularly those based on in-context learning (ICL).\nThe most relevant studies to CoPS include Shinn et al. (2024); Kagaya et al. (2024); Zhou et al.\n(2023); Raparthy et al. (2023). In Kagaya et al. (2024), a retrieval process for selecting in-context\ndemonstrations was proposed. However, their approach depends on frequent embedding queries\nduring the planning stage, leading to inefficiency issues even in smaller LLM settings. Additionally,\nRAP manually splits the agent's planning trajectory into multiple stages for each trial, with\nbenchmark-specific tailoring, significantly increases implementation complexity and raises scalability"}, {"title": "2.2 In-context Demonstrations Selection", "content": "The selection of demonstrations for ICL has been widely studied. Wang et al. (2024b) approached\nin-context demonstration selection from a Bayesian perspective, explicitly constructing a latent\nvariable for the selection process. However, their analysis did not account for the pre-trained\nknowledge distribution, and their results were primarily empirical. Yan et al. (2023) investigated\nthe impact of repetition in in-context demonstrations, conducting controlled experiments to assess\nhow repetitions in pre-trained knowledge influence results. Scarlatos and Lan (2023) developed\na reinforcement learning framework to select in-context examples, while Voronov et al. (2024)\nexamined the impact of prompt formatting on in-context learning performance. Additionally, Shum\net al. (2023) introduced an automatic CoT augmentation and selection method for ICL example\ndatasets. Hu et al. (2024b) analyzed the scaling of in-context demonstrations from a theoretical\nstandpoint, deriving general statistical bounds while accounting for pre-training errors. However,\ntheir focus was primarily on CoT in general ICL settings, not on the specific challenges faced by\nLLM agents interacting with environments and requiring feedback for optimization."}, {"title": "2.3 Theory of Agents", "content": "Several works have advanced the theoretical understanding of LLM agents. He et al. (2024)\nexplored the statistical theory of LLM agents through the lens of Bayesian aggregated imitation\nlearning. Lin et al. (2023) provided a theoretical analysis of transformers within the context of\nin-context reinforcement learning. Wang et al. (2024a) examined the training and generalization of\ntransformers for sequential reasoning, drawing parallels between transformer behavior and online\nlearning algorithms. Sumers et al. (2023) offered a cognitive perspective on LLM agents, while Park\net al. (2024) investigated the regret of LLM agents in sequential reasoning tasks, contributing both\ntheoretical and empirical insights that inform COPS's development."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Preliminary", "content": "We consider a sequential decision-making scenario, consisting of a task space M, a state space S, and\nan action space A. The state $s \\in S$ is defined as a descriptive sentence representing the history of\nthe current task. For example: \"You are in the middle of a room. Please find a path to reach the\napple.\" The action $a \\in A$ is a solution to the task, such as: \"Move right. The apple is on the table.\"\nThe agent interacts with the environment through trials. At the beginning of each trial, a task"}, {"title": "3.2 Proposed Method", "content": "We introduce our proposed method, COPS, based on distribution matching. COPS operates on\na trial-wise basis, making it suitable for both the offline setting, where the agent has access to\nan external static dataset containing experiences, and the online setting, where the agent gathers\nexperiences through interactions with the environment. Suppose our agent is at the start of a trial\nwith an initial state $s_1 \\sim P_M$. We introduce the key components of COPS as follows.\nMemory Bank The agent has access to a memory bank D containing experiences, either from a\npre-collected dataset (offline) or from previous experiences (online). We do not impose restrictions\non D, meaning that experiences in D exhibit great diversity. Specifically, an experience $\u03c4 \u2208 D$ may\ncorrespond to different tasks M or to varying solution strategies for the same task. Our goal is to\ndevelop a strategy for retrieving experiences from D that assist in decision-making for the current\ntask.\nCross-Task Experience Sharing COPS utilizes an external module called the decoder, denoted\nas Dec in Line 1. In general, the decoder outputs a task-dependent distribution of experiences\nconditioned on the initial state $s_1$, reflecting how the LLM would solve the task M associated with\n$s_1$ without explicit instructions. With the decoder's help, the agent's goal is to find a probability"}, {"title": "4 Experiments", "content": "In this section, we present our experimental study evaluating the practical performance of COPS\non real-world LLMs, specifically the Llama 3.1 models (Dubey et al., 2024). Our results show that\nCOPS achieves state-of-the-art (SOTA) performance in both task success rate and sample efficiency,\nsurpassing existing baselines to the best of our knowledge. A detailed description of our prompt\nformulation is provided in Appendix C. Notably, COPS is both simple to implement and generalizable"}, {"title": "4.1 Results and Analysis", "content": "In this section, we demonstrate that COPS outperforms all baselines across all benchmarks and model\nsizes, considering both sample efficiency and task success rate. Detailed performance illustrations\nover multiple trials are presented in Figure 2. Our hyperparameter details are provided in Table 5\nin Appendix B."}, {"title": "4.2 Ablation Study", "content": "In this section, we analyze how two key hyperparameters affect the performance of COPS: the\nscaling factor c in Equation (3.3) and the number of in-context experiences k placed at the beginning\nof prompts. We conducted experiments on the Alfworld benchmark using both Llama 3.1 8b and\nLlama 3.1 70b models.\nScaling factor For the scaling factor c, we tested four settings: c = 0, 1, 5 and 10, while keeping\nthe number of in-context experiences fixed at k = 5 (see Figures 3(a) and 3(b)). Our findings\nindicate that for smaller models like Llama 3.1 8b, a small but non-zero value of c (e.g., c = 1)\ngenerally yields better performance (Figure 3(a)). This suggests that moderate scaling effectively\nbalances model adaptability and robustness on less capable models.\nNumber of experiences Regarding the number of in-context experiences k, we evaluated values\nranging from 1 to 10, setting c = 0 (see Figures 3(c) and 3(d)). We observed that performance\nimproves as k increases up to k = 3, after which it plateaus for both model sizes. This result\nindicates that while increasing the in-context experience size enhances performance to a point,\nadding more than three experiences may not offer substantial gains.\nOur ablation study reveals that tuning key hyperparameters in CoPS is crucial for optimal\nperformance. Specifically, for smaller models, a small but non-zero scaling factor c (e.g., c = 1)\neffectively balances adaptability and robustness. Additionally, increasing the number of in-context\nexperiences k enhances performance up to k = 3, beyond which additional experiences offer minimal\ngains. These insights provide practical guidance for hyperparameter selection, ensuring that COPS"}, {"title": "5 Theoretical Framework of Experience-Assisted Agents", "content": "In this section, we develop the theoretical framework to demonstrate the effectiveness of COPS.\nFor simplicity, we analyze our algorithm in a bandit setting, where the maximum number of steps\nfor each experience is H = 1. Slightly different from the formulation in Section 3, we define an\nexperience as \u03c4 = s|a|r, consisting of an initial state s, an action a, and its reward r = r(s, a).\nWe introduce additional notations for clarity in our analysis. Let $T = \u03c4_1|\u03c4_2|...$ denote the\nexperience collection. The length of T is denoted by |T|, i.e., $T = (\u03c4_1, ...,\u03c4_{|T|})$. We use $T_t$ to\nrepresent the first t steps of the experience collection, i.e., $T_t = \u03c4_1|...|\u03c4_t$. For any experience\ncollection T, we assume |T| <T. We define T as the space of all trajectories, and $T_t$ as the space of\ntrajectories of length t. We denote a general algorithm as Alg(\u00b7|\u00b7,\u00b7,\u00b7) : M \u00d7 T \u00d7 S \u2192 \u2206(A), which\ntakes as input a task M\u2208 M, an experience collection T\u2208 T, and a state s \u2208 S, and outputs a\ndistribution over actions a \u2208 A. Note that some algorithms may not use the task M as input, in\nwhich case we write Alg(\u00b7|\u00b7,\u00b7). We denote $P_{M, Alg}$ as the distribution over the first t steps of an\nexperience collection under task M and algorithm Alg. For an algorithm Alg that takes M, T, s as\ninput, we define its posterior average as $\\overline{Alg(\u00b7|T, s)} = E_{M~P_M(\u00b7|T'=T,s'=s)}[Alg(\u00b7|M, T', s')]$, which is\nthe best Bayesian approximation of Alg given the experience collection T and current state s."}, {"title": "5.1 LLM Pretraining", "content": "We begin by describing the pretraining process for the LLM. Let $Alg_{\\Theta}(\u00b7|T, s) : T \u00d7 S \u2192 \u2206(A)$\nrepresent an LLM agent that outputs a distribution over A, where $\\theta \u2208 \\Theta$ is the parameter of the\nLLM, $\\Theta$ denotes the whole parameter space. We assume that there exists a pretraining dataset\n$D_{pre} = {T_1,...,T_{n_{pre}}}$, with $|Ti| = T \u2013 1$. Following the pretraining setup in Lin et al. (2023), we\nassume two algorithms: a context algorithm, $Alg_C(\u00b7|\u00b7, \u00b7) : T \u00d7 S \u2192 \u2206(A)$, and an expert algorithm,\n$Alg_E(\u00b7|\u00b7,\u00b7, \u00b7) : M \u00d7 T\u00d7S \u2192 \u2206(A)$. In general, the context algorithm provides a \u201cnatural\u201d action\nbased on the experience collection and current state, while the expert algorithm provides a more\ninformed action, given the task information, experience collection, and current state. Since the\nexpert algorithm has access to task information M, it typically produces better actions than the"}, {"title": "5.2 Algorithms for the Offline Setting", "content": "We consider the same offline setting as in Section 3. Suppose we have an offline dataset D, and the\nagent is given an initial state s. We formalize the experience selection problem as a distribution\nselection problem, where the agent has access to a candidate set of distributions, denoted by\n$P = {P_1(\u00b7|\u00b7, \u00b7), . . ., P_{|P|}(\u00b7|\u00b7,\u00b7)} \u2286 2^{\\overline{T-1}\u00d7S\u2192\u25b3(\\overline{T-1})}$. Each element in this set represents a mapping"}, {"title": "5.3 Algorithms for the Online Setting", "content": "We also consider an analysis for a variant of OFFLINECOPS to the online setting. Here, let\n$P = {P_1(\u00b7|\u00b7, \u00b7), . . ., P_{|P|}(\u00b7|\u00b7, \u00b7)} \u2286 2^{T^{t-1}\u00d7S\u2192(T^{t-1})}$ which includes mappings that map an experience\ncollection $T_{t-1}$ and a test state s to a distribution over $T^{t\u22121}$. Each $P^i$ can be thought as a strategy\nto pick the experience collection that depends on the past observations. At step t, we have history\n$H_{t\u22121} = {s_1, a_1, r_1,..., s_{t\u22121}, a_{t\u22121}, r_{t-1}}$. Then the agent receives $s_t \u223c P_{M_t}$, where $M_t \u223c P_M$. Then\nthe agent selects $P_t$ by some algorithm and samples $T_{t-1} \u223c P_t(\u00b7|H_{t\u22121}, s_t)$. Then the agent takes the\naction $a_t \u223c Alg(\u00b7|T_{t\u22121}, s_t)$. Her goal is to minimize the following regret:"}, {"title": "6 Conclusion, Limitation and Future Work", "content": "In this paper, we introduced COPS (Cross-Task Experience Sharing), a theoretically grounded\nalgorithm that empowers agent systems with cross-task experiences sharing. Using a pessimism-\nbased strategy to select relevant experiences, CoPS maximizes utility while minimizing the risks\nof distribution shifts. Our experiments on benchmarks like Alfworld, Webshop, and HotPotQA\ndemonstrate that COPS outperforms state-of-the-art methods in both success rates and sample\nefficiency. Theoretically, we show that our algorithm's performance depends on the LLM's pre-\ntrained quality and the matching between the cross-task experience distribution decided by the\ntrials selected by the agent, and a task-dependent experience distribution denoted by the LLM,\nproviding insights for improving experience retrieval methods.\nWhile COPS shows clear improvements over existing methods, it has several limitations. Its\neffectiveness heavily depends on the quality and diversity of the experiences in the memory bank,\nmeaning that outdated or poorly aligned experiences can reduce its performance. Additionally,\nCOPS is sensitive to hyperparameters like the scaling factor and the number of in-context experiences,\nwhich may require time-consuming tuning that doesn't always generalize well across different tasks\nor models. Finally, the theoretical guarantees we provide also rely on assumptions about the\naccuracy of the decoder and specific pre-training properties of the LLM, which may not always hold\nin real-world scenarios."}, {"title": "A Additional Details in Section 5", "content": null}, {"title": "A.1 Proof of Theorem 5.4", "content": "We prove Theorem 5.4 here. First, we need the following lemmas."}, {"title": "A.2 Proof of Theorem 5.5", "content": "Proof. Suppose we are at step t and we condition on all past history $H_{t\u22121} = (s_1, a_1,r_1,..., s_{t\u22121}, a_{t\u22121}, t\u22121)$.\nLet Mt be the task at t step and st be the state observed. Then with probability at least 1-1/mc-\u03b4,\nthe following event Et holds:"}, {"title": "B More Experiment Details", "content": "In this section, we provide additional details on our experiments in Section 4.1. The tables included\nbelow outline the hyperparameter settings that were used throughout the evaluation process."}, {"title": "C Prompt Template", "content": "Our prompt framework is designed with the considerations of simplicity, efficiency, and generaliz-\nability. Moreover, we aim to leverage the high-performing long-context capabilities of modern LLMs\n(Dubey et al., 2024) to a maximal degree. In one sentence to describe our prompting philosophy:\nprevious success trajectories are seen by the agent as experiences it has gone through in the same\ntrial. As an example from the Alfworld experiment, a in-context demonstration is insert into the"}]}