{"title": "Generalizable Articulated Object Perception with Superpoints", "authors": ["Qiaojun Yu", "Ce Hao", "Xibin Yuan", "Li Zhang", "Liu Liu", "Yukang Huo", "Rohit Agarwal", "Cewu Lu"], "abstract": "Manipulating articulated objects with robotic arms\nis challenging due to the complex kinematic structure, which\nrequires precise part segmentation for efficient manipulation.\nIn this work, we introduce a novel superpoint-based perception\nmethod designed to improve part segmentation in 3D point\nclouds of articulated objects. We propose a learnable, part-aware\nsuperpoint generation technique that efficiently groups points\nbased on their geometric and semantic similarities, resulting\nin clearer part boundaries. Furthermore, by leveraging the\nsegmentation capabilities of the 2D foundation model SAM, we\nidentify the centers of pixel regions and select corresponding\nsuperpoints as candidate query points. Integrating a query-based\ntransformer decoder further enhances our method's ability to\nachieve precise part segmentation. Experimental results on\nthe GAPartNet dataset show that our method outperforms\nexisting state-of-the-art approaches in cross-category part\nsegmentation, achieving AP50 scores of 77.9% for seen categories\n(4.4% improvement) and 39.3% for unseen categories (11.6%\nimprovement), with superior results in 5 out of 9 part categories\nfor seen objects and outperforming all previous methods across\nall part categories for unseen objects.", "sections": [{"title": "I. INTRODUCTION", "content": "Articulated objects, such as doors and drawers, are ubiq-\nuitous in daily life due to their kinematic connections.\nAs embodied intelligence technology continues to advance,\nit becomes increasingly important for robots to not only\nrecognize these objects [1]\u2013[4] but also manipulate them\neffectively by performing tasks like opening doors, closing\ndrawers, or even lifting pot lids. Existing approaches based on\nreinforcement learning (RL) and imitation learning typically\naddress the manipulation of articulated objects by predicting\naffordances and generating motion trajectories through learned\npolicies [5]\u2013[9]. However, these methods often face significant\nchallenges in generalizing to unseen objects, particularly when\nvariations in object geometry are introduced, thereby limiting\nthe transferability of the learned skills.\nIn contrast, by leveraging powerful vision models, part\nsegmentation-based approaches to articulated object modeling\noffer a more general solution, achieving accurate perception\nof articulated objects, forming the foundation for successful\nmanipulation [10]\u2013[12]. This precise and efficient perception\nenables robots to handle complex tasks with greater relia-\nbility [13]\u2013[15]. However, while these approaches provide\nsignificant advantages, prior part segmentation methods\ntypically segment 3D point clouds into different parts based on\nindividual point clouds [7], [16]\u2013[18]. Although these methods\ncan achieve high modeling accuracy with familiar objects,\nthey struggle to extract transferable information in the face\nof complex variations in point clouds, significantly reducing\nsegmentation accuracy with unseen objects. Superpoint-based\nmethods [19]\u2013[21] partition point clouds into point sets,\nknown as superpoints, which are groups of neighboring\npoints adapted to local complexity and aggregating geometric\ninformation. This superpoint-based approach not only reduces\ncomputational overhead but also enhances the model's ability\nto generalize across diverse object geometries by effectively\nintegrating local geometric features, thereby improving seg-\nmentation accuracy with unseen objects.\nIn this paper, we introduce Generalizable Articulated Ob-\nject Perception with Superpoints (GAPS), a novel approach\ndesigned to enhance part segmentation in diverse articulated\nobjects within 3D point clouds.GAPS improves superpoint\nboundary clarity through learnable part-aware superpoint\ngeneration techniques, ensuring more distinct superpoints.\nBuilding on this, it leverages the 2D foundation model\nSAM [22] to effectively segment pixel regions, where each\nregion's center uniquely identifies the corresponding 3D\nsuperpoints. These superpoints are then used as query points\nfor part segmentation, enabling a more generalizable and\nadaptable selection of query points. By utilizing a query-\nbased transformer decoder, GAPS achieves precise part seg-\nmentation across articulated objects. The main contributions\nof this paper are as follows:\n1) We design the learnable part-aware superpoint generation\nmethod that groups point clouds as superpoints based on\ngeometric and semantic similarities. Compared to rule-based\nsuperpoint generation, our approach is more effective in\nhandling smaller parts and achieving clearer boundaries.\n2) The 2D foundation model SAM segments images into\npixel regions, with each center mapping to a unique 3D\nsuperpoint. These superpoints act as query points, allowing the\ntransformer decoder to effectively capture local information,\nenabling GAPS to achieve precise part segmentation across\ndiverse articulated objects.\n3) We conduct experiments on the articulated object mod-\neling benchmark GAPartNet [4], where GAPS outperforms\nexisting state-of-the-art part segmentation methods in both\nseen objects and unseen cross-category generalization."}, {"title": "II. PRELIMINARY", "content": "We formulate the articulated object part segmentation task\nT as follows. An articulated object M consists of K variable\nmovable parts, represented as M = {m_i}_{i=1}^{K}. We observe the\nobject M using RGB-D cameras and project it into a point\ncloud P with N points, P = {p_i \u2208 R^3}_{i=1}^{N}. Superpoints\nare an over-segmented set of point clouds that adapt to local\ngeometric structures and capture contextual features. Given\na point cloud as P with N points as P = {p_i \u2208 R^3}_{i=1}^{N}\nand its corresponding features F = {f_i \u2208 R^d}_{i=1}^{N}, superpoint\ngeneration aims to construct O superpoints as S = {s_i \u2208\nR^3}_{i=1}^{O} and its corresponding features E = {e_i \u2208 R^d}_{i=1}^{O}\nfrom the point cloud P, assigning each point to one of the\nO superpoint centers with the highest probability. In this\nway, the superpoints S with corresponding features E can be\nused to represent the entire point cloud, with each superpoint\nencoding both local geometric and semantic features."}, {"title": "III. METHOD", "content": "In this section, we present an innovative methodology\nGAPS, as shown in Fig 1. For a given single-view point cloud\nof an object, we leverage the Point Transformer-V2 [23]\nto extract point-wise features, which are then processed\nthrough a part-aware superpoint generation module to produce\nsuperpoints, resulting in sharper and more well-defined\nboundaries between superpoints (Section III-A). By utilizing\nthe SAM-guided 2D information to identify corresponding\n3D superpoints as candidate 3D query points and combining\nit with a transformer decoder, we enable accurate part\nsegmentation of articulated objects (Section III-B)."}, {"title": "A. Part-aware Superpoint Generation", "content": "Superpoints are groups of 3D points semantically clustered\nbased on similar geometric features. In articulated object part\nsegmentation, we leverage these superpoints to capture local\ngeometric information, enhancing the model's generalization\nability and improving segmentation accuracy. Unlike previous\nmethods focused on instance segmentation, our task deals\nwith the complexity of articulated part segmentation, where\nparts vary significantly in size and have intricate connections.\nTo address this, we draw inspiration from SPNet [20] and\nemploy a learnable soft association map to model relationships\nbetween points and superpoints. This approach generates part-\naware superpoints, effectively addressing the challenges of\narticulated part segmentation.\nGiven an articulated object represented by a point cloud\nP = {p_i \u2208 R^3}_{i=1}^{N}, we initially use hand-crafted fea-\ntures [24] to construct a hard point-superpoint assignment.\nThe superpoint coordinates S and features E are derived\nas the averages of the coordinates and features of the\npoints assigned to each superpoint. However, this initial over-\nsegmentation, achieved through unsupervised optimization,\nmay not effectively capture fine part instances, leading to\nissues such as cross-part and nested-part segmentation. To\naddress this, we apply a refinement process that updates both\nthe point-superpoint assignment and the soft association map,\nimproving segmentation accuracy by better handling these\ncomplexities. To further enhance computational efficiency, we\nselectively build the association map using only the nearest 6\nsuperpoints to each point, denoted as A \u2208 R^{N\u00d76}. Specifically,\na cosine-similarity-like operation, implemented through an\nMLP, is used to update the point-superpoint association map.\nThe association between the i-th point and j-th superpoint\na_{ij}, is updated as follows:\na_{ij} = \\phi(p_i, s_j)g(p_i) \\cdot \\psi(f_i, e_j)h(f_i), \\tag{1}\nwhere \\phi(\\cdot, \\cdot): \\mathbb{R}^3 \\times \\mathbb{R}^3 \\rightarrow \\mathbb{R}^c and g(\\cdot): \\mathbb{R}^3 \\rightarrow \\mathbb{R}^c\nare two mapping functions in the coordinate space, while\n\\psi(\\cdot, \\cdot): \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^c and h(\\cdot): \\mathbb{R}^d \\rightarrow \\mathbb{R}^c are two mapping\nfunctions in the feature space, all implemented by MLP. Then\nwe normalize the association map of each point:\n\\bar{a}_{ij} = \\frac{\\exp(a_{ij})}{\\sum_{k=1}^6 \\exp(a_{ik})} \\tag{2}\nAfter updating the association map, we use the normalized\nassociation scores as weights to update the superpoints'\ncoordinates and features as follows:\ns_j = \\frac{\\sum_{i=1}^N \\bar{a}_{ij}p_i}{\\sum_{i=1}^N \\bar{a}_{ij}}, e_j = \\frac{\\sum_{i=1}^N \\bar{a}_{ij}f_i}{\\sum_{i=1}^N \\bar{a}_{ij}} \\tag{3}"}, {"title": "B. Superpoint-based Part Segmentation", "content": "Our approach to part segmentation of articulated objects\nleverages superpoints, enhanced by integrating SAM-guided\n2D information to identify corresponding 3D superpoints as\ncandidate query points. These superpoints, generated from\nsingle-view point clouds during the part-aware superpoint\ngeneration stage, capture local geometric features, providing\nrich encoding sensitive to the nuances of articulated parts and\nrobust to scale and size variations. Building on the SPFormer\nframework [25], we employ a 6-layer query decoder to refine\nthe segmentation process further.\nQuery Decoder Architecture. Each superpoint, repre-\nsented by the aggregated coordinates and features of its\nconstituent points, forms the basis of our segmentation\napproach. The coordinates of these SAM-selected 3D query\npoints are used to generate position embeddings, which serve\nas queries. The superpoint features, combined with their\ncorresponding position embeddings, are then fed into a query\ndecoder, employing a 6-layer transformer decoder architecture.\nThis structure leverages cross-attention mechanisms, where\neach superpoint query attends over all points to refine\nits representation. Let S = {s_1, s_2, ..., s_O} be the set of\nsuperpoint feature vectors, where s_i \u2208 R^d is the feature\nvector for the j-th superpoint, and O is the total number of\nsuperpoints. The corss-attention operation for the l-th layer\nof the decoder can be defined as:\nCrossAttention^{(l)}(Q, K, V) = \\text{softmax}(\\frac{Q K^T}{\\sqrt{d_k}}) V \\tag{5}\nwhere Q, K, and V are the query, key, and value matri-\nces derived from the superpoint features, respectively. The\noperation computes the attention-weighted sum over the\nvalues V, where the attention weights are determined by\nthe compatibility between the queries Q and keys K. The\nscaling factor \\sqrt{d_k} ensures stability during training.\nThe output of the cross-attention layers is then passed\nthrough feed-forward networks within each layer of the\ndecoder to progressively refine the superpoint representations.\nThe final superpoint query representation at the l-th layer is\ngiven by:\ns_j^{(l)} = \\text{FFN} \\big( \\text{CrossAttention}^{(l)}(Q^{(l)}, K^{(l)}, V^{(l)}) \\big) \\tag{6}\nwhere FFN denotes a feed-forward network that further\nprocesses the output of the cross-attention mechanism. Due\nto the presence of one or more superpoints within a part, each\nsuperpoint can query the corresponding part. Unlike bipartite\nmatching, we adopt many-to-one matching [26]. Formally,\nwe use a pairwise matching cost matrix C to evaluate the\nsimilarity between the queries and the articulation parts. Using\nthe cost matrix, we assign each query to its corresponding\nparts.\nC_{im} = \\begin{cases} C_{im} & \\text{if i-th query } \\in \\text{ m-th part} \\\\ + \\infty & \\text{otherwise} \\end{cases} \\tag{7}\nOnce the matching is completed, we know the class labels\nof the queries in advance and compute the cross-entropy loss\nL_{cls} for each query. we compute the segmentation mask loss,\nwhich consists of the binary cross-entropy loss L_{bce} and the\ndice loss L_{dice} for each matched query and part pair. We\ncompute the BCE score loss L_{score} to determine if the IoU of\nthe current query's corresponding part is greater than 50%.\nTherefore, the overall loss for articulation part segmentation\nis as follows:\nL_{sem} = \\lambda_{cls} L_{cls} + \\lambda_{bce} L_{bce} + \\lambda_{dice} L_{dice} + \\lambda_{score} L_{score} \\tag{8}\nIn our paper, the values of the loss function weights are\nset as follows: \\lambda_{cls} = 1.5, \\lambda_{bce} = 1.25, \\lambda_{dice} = 1.0, and\n\\lambda_{score} = 1.0, respectively."}, {"title": "IV. EXPERIMENT", "content": "We validate the articulated object segmentation using the\nGaPartNet dataset [4], rendering RGB-D images with anno-\ntations in the SAPIEN environment [27]. To evaluate cross-\ncategory generalizability, we split the 27 object categories\ninto 17 seen and 10 unseen categories, ensuring all 9 part\nclasses are represented in both. Following the 3D semantic\ninstance segmentation benchmarks in ScanNetV2 [28], we\nuse average precision (AP) as the performance metric for part\nsegmentation, with AP50 (IoU threshold of 50%) assessing\nboth per-part and overall segmentation accuracy."}, {"title": "B. Cross-category Part Segmentation", "content": "Table I presents the quantitative comparisons between our\nmethod and previous state-of-the-art methods, including Point-\nGroup [29], SoftGroup [30], AutoGPart [31], GAPartNet [4],\nand SPFormer [25]. Our method surpasses current state-of-\nthe-art approaches in both seen and unseen categories, with\nsuperior results in 5 out of 9 categories. While it shows a slight\nadvantage in seen categories, achieving an AP50 of 77.9%,\nnotably, the performance on the slider button category shows\nan absolute improvement of 11.1% compared to previous\nmethods. Our method performs better in unseen categories,\nachieving an AP50 of 39.3% and the best results across\nall part categories, which highlights its enhanced ability to\ngeneralize to novel objects."}, {"title": "C. Ablation Study", "content": "We conduct ablation studies to validate the effectiveness\nof the part-aware superpoint generation and SAM-guided 2D\ninformation transformer decoder in our method.\nIn Table II, we sequentially ablate the following: superpoint\nclustering (replaced with raw point clouds), parameterized\nqueries [25], and point-to-center queries [26]. Results show\nthat using superpoints instead of raw point clouds significantly\nimproves performance for both seen and unseen objects,\nas superpoint features capture more transferable geometric\ninformation. Compared to parameterized queries [25], the 3D\nposition embeddings of queries better integrate local features,\nresulting in more accurate part segmentation. Additionally,\nwe modified the query point generation method to a point-to-\ncenter query [26], which performed well for seen objects but\nstruggled to accurately locate centers in unseen objects. To\naddress this, SAM leverages 2D prior knowledge to precisely\nlocate part centers through back projection.\nWe visualize the segmentation results in Figure 2, Com-\npared to rule-based superpoints, our learnable superpoint-\nbased queries more effectively integrate local geometric\ninformation, enhancing the accuracy of local geometry\nmodeling and improving the overall stability of the model."}, {"title": "V. CONCLUSION", "content": "In this paper, we presented Generalizable Articulated\nObject Perception with Superpoints (GAPS), a novel approach\nfor enhancing part segmentation of articulated objects in 3D\npoint clouds. GAPS employs a learnable, part-aware super-\npoint generation technique to group points based on geometric\nand semantic similarities, resulting in clearer boundaries.\nFurthermore, the method leverages the 2D foundation model\nSAM to select candidate 3D query points and utilizes a\nquery-based transformer decoder for precise segmentation.\nGAPS demonstrated state-of-the-art performance on the\nGAPartNet benchmark, achieving AP50 scores of 77.9%\nfor seen categories and 39.3% for unseen categories, with\nimprovements of 4.4% and 11.6% for seen and unseen\ncategories, highlighting GAPS's generalization capabilities."}]}