{"title": "Seeing Faces in Things:\nA Model and Dataset for Pareidolia", "authors": ["Mark Hamilton", "Simon Stent", "Vasha DuTell", "Anne Harrington", "Jennifer Corbett", "Ruth Rosenholtz", "William T. Freeman"], "abstract": "The human visual system is well-tuned to detect faces of all shapes and sizes. While this brings obvious survival advantages, such as a better chance of spotting unknown predators in the bush, it also leads to spurious face detections. \"Face pareidolia\" describes the perception of face-like structure among otherwise random stimuli: seeing faces in coffee stains or clouds in the sky. In this paper, we study face pareidolia from a computer vision perspective. We present an image dataset of \"Faces in Things\", consisting of five thousand web images with human-annotated pareidolic faces. Using this dataset, we examine the extent to which a state-of-the-art human face detector exhibits pareidolia, and find a significant behavioral gap between humans and machines. We find that the evolutionary need for humans to detect animal faces, as well as human faces, may explain some of this gap. Finally, we propose a simple statistical model of pareidolia in images. Through studies on human subjects and our pareidolic face detectors we confirm a key prediction of our model regarding what image conditions are most likely to induce pareidolia. Dataset and Website: https://aka.ms/faces-in-things", "sections": [{"title": "1 Introduction", "content": "Pareidolia is a type of visual \"apophenia\", which refers to the perception of patterns in random data. This occurs frequently in human perception as we look at clouds, mountain skylines, and burnt toast. Pareidolia is even described in an exchange in Hamlet [45]. When it was first described, pareidolia was seen as an early symptom of psychosis [7, 46]. Today we know pareidolia is common among healthy humans [47] and infants [23]. It is also not confined to humans:"}, {"title": "2 Related Work", "content": "Face detection. One of the most famous early examples of face detection was the Viola-Jones face detector [54,55]. This detector used binary Haar features through simple-to-compute integral images and achieved greater precision and efficiency than early neural-network-based detectors [42, 43] and other feature-based methods [39,61]. Following the deep learning breakthroughs of the 2010s, methods transitioned from hand-crafted features to learned features, and convolutional neural networks (CNNs) achieved close to human levels of performance on ever-larger datasets [8,27-29, 33, 38, 50, 65]. For a broader survey of face detection methods, we refer the interested reader to [38,64]. In our work, we use the recent RetinaFace model [8] as a strong face detection baseline.\nNeuroscience of face pareidolia. The face is a highly unique stimulus for the human visual system [26,52]: we find faces easy to spot and difficult to ignore. Face detection can occur in both noise and highly degraded images [5]. Prior work shows that face detection occurs in a dedicated brain region, the Fusiform Face Area [34]. But exactly what constitutes a face for the visual cortex and what are the mechanisms underlying pareidolia? A recent study into the temporal dynamics of neuro-imaging data during pareidolic face viewing showed results consistent with \"a broadly-tuned face detection mechanism that privileges sensitivity over selectivity\" [58]. Pareidolic faces do more than give the impression of the presence of faces: [48] show that they can trigger an additional face-specific attentional process, consuming more time and processing power than similar non-pareidolic stimuli, and even enhancing the detection of face-pareidolic objects [49]. Analyses in [30] revealed a network of neurons in the brain specialized to detect face pareidolia. Their results suggested that face processing has a strong top-down component whereby sensory input with even the slightest suggestion of a face can result in the interpretation of a face. Such top-down information might be supportive of some form of inverse rendering as a cognitive mechanism to explain the remarkable robustness of human perception of faces in degraded viewing conditions [11]. While our dataset allows the study of several types of face detection models, we focus our study on feed-forward neural networks which are known to yield close to human performance on challenging \u201cin-the-wild\" datasets [62].\nFace pareidolia in computer vision. Face detection and face recognition have been core topics in computer vision for many decades, but the study of face pareidolia\u2014and its deep relationship with visual object representation learning has been relatively overlooked. Face pareidolia has some similarities with the problem of cross-modal recognition or cross-depiction: recognizing the"}, {"title": "3 Faces in Things Dataset", "content": "To address this gap, we begin by sampling candidate pareidolic images from the LAION-5B dataset [44]. This dataset consists of 5.85 billion CLIP-filtered image-text pairs, of which 40% of captions contain English. We use CLIP retrieval [2] to build a raw image set based on text queries including \"pareidolia\", \"faces in things\", \"accidental faces\", and \"[object] looks like a face\". We download images, check for duplicates, then downsample to 512 \u00d7 512 pixels while preserving the aspect ratio with white-space padding. We used the VGG Image Annotation tool [10] to manually annotate images, removing samples that contain the faces of actual humans or animals. Some examples of annotated images are shown in Fig. 2. Our annotations include the bounding boxes of pareidolic faces and basic facial attributes as summarized in Fig. 3. Though beyond the scope of the current paper, we note that these attributes could be useful for other future studies. We divide the dataset at random into training (70%) and testing (30%) sets. We refer to this as the 'Pareidolic' dataset."}, {"title": "4 Experiments", "content": "Datasets. We use the following additional datasets. Fig. 4 shows the average faces within our dataset (Pareidolic) and the WIDER FACE (Human), and AnimalWeb (Animal) datasets."}, {"title": "4.1 Does a SOTA Face Detector Exhibit Pareidolia?", "content": "We measure the Average Precision (AP) of the MobileNet and ResNet50 RetinaNet architectures on the Faces in Things dataset. The first row of Table 1 shows results for existing pre-trained models, and the second row shows those for models fine-tuned on the original WIDER FACE training data. These act as control groups to ensure our transfer learning procedure does not interfere with our measurement of the effects of other interventions. Though these models exhibit pareidolia to a small extent, they fall far short of a model fine-tuned to detect pareidolic faces. Fig. 5 also depicts some of these predictions with blue boxes. On the whole, the models trained only on human faces are largely silent across the Faces in Things dataset."}, {"title": "4.2 How Might Pareidolia Emerge?", "content": "The WIDER FACE dataset is known for its diversity of lighting, pose, makeup, emotion, and scale of faces. This fact, coupled with the results of Section 4.1 begs the question: What else do models need to experience pareidolia as humans do? The Faces in Things dataset provides a clean and robust setting to explore"}, {"title": "5 Modeling Pareidolia", "content": "Though many prior works have measured pareidolia, there has yet to be a simple mathematical model that describes the high-level structure of this phenomenon. In this section we provide two simple formal models of pareidolia and show that they both exhibit a testable prediction: the existence of a peak in pareidolic face detection as a function of an image's complexity. Section 5.4 presents experimental evidence of this \"pareidolic peak\" in both humans and machines."}, {"title": "5.1 Gaussian Model of Pareidolia", "content": "A model of pareidolia needs to describe two processes: (1) the random process that generates candidate images, and (2) the face detection process which determines when an image is pareidolic. We begin with a simple Gaussian model for"}, {"title": "P(ai) = \\int_{Y_i}P(a_i, y_i)dy_i", "content": null}, {"title": "= \\int_{Y_i} P(a_i| Y_i)P(y_i)dy_i", "content": null}, {"title": "= \\frac{1}{2\\pi \\gamma_i\\sigma_i} \\int_{Y_i} e^{-\\frac{(Y_i-a_i)^2}{2\\gamma_i^2}} e^{\\frac{y_i^2}{2\\sigma_i^2}} dy_i", "content": null}, {"title": "P(a_i) = \\frac{1}{\\sqrt{2\\pi(\\gamma_i^2 + \\sigma_i^2)}}e^{-\\frac{a_i^2}{2(\\gamma_i^2 + \\sigma_i^2)}}", "content": null}, {"title": "5.2 Predicting Peak Pareidolia", "content": "For a given mode's detection variance, $y_i^2$, and target mode coefficient, $a_i$, Eq. 4 allows us to find the optimal mode variance to generate pareidolia, i.e., to maximize P(az). Unfortunately, we seldom have the flexibility to design a random process one mode at a time. But we may have the option to select between image generation processes that have different numbers of modes, M. Since each mode"}, {"title": "P(a) = \\prod_{i}^{M} P(a_i)", "content": null}, {"title": "5.3 Higher-Level Feature Model of Pareidolia", "content": "The Gaussian model for pareidolia above lays out important aspects of pareidolia, but relies on a naive model of object detection, the squared distance from a template image. We assume that a more realistic model of human perception would incorporate higher-level features and introduce a still simple, yet more realistic, feature-based model."}, {"title": "P(n_i) = \\frac{(\\lambda_i B_i)^{n_i} e^{-\\lambda_i B_i}}{n_i!}", "content": null}, {"title": "P(O) = \\prod_{i}^{M} \\lambda e^{-\\lambda(M-1)}", "content": null}, {"title": "5.4 Measuring the Pareidolic Peak in Humans and Machines", "content": "Both mathematical models of Section 5 predict the existence of a peak of pareidolic face detection as a function of image complexity. We show the existence of this pareidolic peak in both humans and machines. In particular, we perform a psychophysics experiment where human subjects view noise images of varying complexity and report how many pareidolic faces they saw in each image, from zero to nine. Campbell [5] demonstrated that a 12x12 array of random, binary squares is sufficient to evoke human and animal faces. We generate noise images of varying complexity by randomly sampling Fourier coefficients and modulating these coefficients with a zero mean \u03c3\u00b2 variance Gaussian in Fourier space. We show some samples of these images on the x-axis of Fig. 11. Intuitively, the Gaussian envelope in frequency space filters out most frequencies higher than \u03c3"}, {"title": "6 Conclusion", "content": "We have taken initial steps towards the mathematical modelling of pareidolia and build a richly annotated dataset of images for face pareidolia. We showed through experiments on modern face detectors that detecting animal faces may partly explain the emergence of pareidolia in a complex vision system. The Faces in Things dataset can help the community address other questions about how and why pareidolic behavior emerges, a hallmark of humans' robust recognition system. We hope that our findings and dataset will spark further study of pareidolia and its potential use to improve computer vision systems."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Additional Information on Frequency-Dependent Noise\nGeneration", "content": "To generate noise of different frequencies for our experiments we leveraged the fact that low frequency information is localized close to the origin in the Fourier transform of an image. To this end, we can generate a random noise images by randomly sampling images in the Fourier space, filtering them, and transforming back to image space. Specifically, we modulate a random Fourier spectra by a Gaussian centered at 0 with a variable width. The width controls the frequency of the noise created. Larger width images let more frequencies pass through in Fourier space, and the resulting image has higher frequency patterns."}, {"title": "A.2 Human Psychophysics Experiment Setup", "content": "In the main experiment, 14 subjects (6 female, 8 male) were shown noise images filtered as described in A.1 at resolution 1024x1024. At each of 9 filter widths, 10 random noise pulls were created, making 90 unique images. Each unique image was repeated 3 times, for a total of and 270 presentations per subject. The images were shown in random order, which was different for each subject. Subjects were split into two groups of 7 subjects, with the two groups view different sets of 90 unique images. Experiments were performed in PsychoPy, and the experimental code both for generating and displaying experimental stimlui is available at https://github.com/vdutell/pareidolicNoise.\nSubjects were seated in a dimly-lit room in front of a laptop with screen resolution 2500x1664, with the 1024x1024 image subtending the entire screen vertically, with grey padding on the horizontal edges (except for 2 additional subjects in the control experiment described below, where image subtended half screen height). Subjects were instructed to sit at a comfortable distance from the screen (approximately 30 inches), and were asked to count the number of faces seen in each noise image, and report the number from [0-9] on the laptop keyboard, reporting 9 if they saw 9 or more. There were no time-outs, no response feedback, and subjects were instructed to self-pace. The experiment took approximately one hour to complete. Subjects were told that there were no \"correct\u201d answers, and instructed to count any face, animal or human as long as they \"felt they saw some kind of face\". Subjects were allowed to take breaks as needed.\nAll participants provided informed consent prior to participation, in compliance with the Common Rule (45 CFR 46), and this study was assessed as exempt from review by the Institutional Review Board, pursuant to 45 CFR 46.101(b)(2). Participants took between 45-90 minutes complete the study and were paid $20 for their participation. Subjects were not excluded for having corrective lenses, but confirmed to be able to see the screen clearly. One subject reported having vision issues beyond using corrective lenses (possible prosopagnosia, see below).\nFor the analysis, trials were removed where subjects responded in less than 100 milliseconds (likely to be a mistake), as well as trials where the subject took more than 2 minutes to respond (likely to be taking a break). No subjects were removed due to outlying or erroneous data. Subjects' time to completion varied from approximately 45-90 minutes.\nWe analyze the effect of different psychophysical conditions with mixed effects ANOVA for all 16 subjects using image seed group, gender, image field of view (FOV) as between-subject factors, and Gaussian filter width as a within-subject factor. Uncorrelated p-values are reported for the two ANOVA analyses (image seed and gender). FOV ANOVA is omitted due to unbalanced sample sizes (14 and 2). No significant differences were found in responses for the two subject groups that were shown image sets from two different random seeds (p > 0.2, Fig. 13, Left). This indicates that peak pareidolia is not an artifact of a 'lucky draw' from the random image set generated."}, {"title": "A.3 Additional Human Psychophysics Results", "content": "In addition to the peak in number of faces reported, we also found that subject's response time (RT) mirrored a similar curve for low to medium filter widths. However, for the largest filter widths shown, the RT curve did not fall off as steeply as the pareidolia curve (Fig. 13, Right). This indicates that subjects took time to look for faces in high frequency data and did not simply 'give up' on the task."}, {"title": "A.4 Additional Annotation Details", "content": "Each image in the Faces in Things dataset is annotated by the following series of questions:\n1. Is there a face?\n(Yes/No / Several)\n2. If Yes / Several: draw bounding box over face(s)\n3. Is the face difficult to spot?\n(Easy / Medium / Hard)\n4. Was the face generated by accident, or by design?\n(Accident / Design)\n5. What emotion does the face show?\n(Neutral / Happy / Sad / Surprised / Angry / Disgusted / Scared / Other)\n6. What does the face most resemble?\n(Human-Baby, Human-Child, Human-Adult, Human-Older, Alien, Animal, Cartoon, Robot, Other)\n7. What gender do you think the face is?\n(Neutral / Female / Male)\n8. Is this example of pareidolia amusing?\n(No / Somewhat / Yes)\n9. How common is this type of face pareidolia?\n(Uncommon / Somewhat / Common)\nFor consistency, a single annotator was tasked with annotating raw data until a set of five thousand face-containing images had been collected. Data was then manually checked by the authors to correct errors, confirm the reasonableness of the annotations and to flag any faces that were considered unsafe for viewing. Duplicates were automatically detected and removed by thresholding the similarity between or DINOv2 class tokens at 0.85. We note that the subjectivity of this task and that the annotations represent a biased view of the dataset from a single annotator's perspective. While it would be interesting to annotate the same data with multiple annotators to build a distribution of answers and model this subjectivity, it was outside the scope of our current project and we leave it as a direction for future work."}, {"title": "A.5 Additional Average Face Renderings", "content": null}, {"title": "A.6 Average Faces across Different Conditions", "content": "Out of curiosity, we plot in Fig. 17 the histogram-equalized averages for faces in our dataset that have been classified as Happy (31% of the data) or otherwise (Neutral / Sad / Surprised / Angry / Disgusted / Scared / Other)."}, {"title": "A.7 Why focus on Pareidolia?", "content": "Many computer vision researchers are inspired by the human visual system and its ability to robustly recognize patterns in the world. Face pareidolia is fascinating because it is a human visual representation phenomenon that is not well understood. Our dataset, models, and experimental analyses shed light on how and why it might arise. These contributions may help the community to: better understand and harness human visual attention (which is drawn towards face-like objects), reduce pareidolic false positives in face detectors, help designers avoid or create pareidolia, improve pareidolic animation, and create systems that understand how we perceive the world."}, {"title": "A.8 Analyzing the Viola-Jones face detector.", "content": "We perform a set of additional experiments with the Viola-Jones face detector and find that it also displays a \u201cpareidolic peak\" as seen in in Figure 18."}, {"title": "A.9 Fitting the Gaussian model of Pareidolia to human\nexperiments.", "content": "We fit our Gaussian model to human data in Figure 18. The fit parameters (\u03c3 = 6) are similar to the flot of Figure 9 plots (\u03c3 = 10)."}, {"title": "A.10 Simultaneous Classification and Detection.", "content": "We show a simultaneous classification analysis in Figure 19. We thank the reviewer as this further quantifies our findings that models are more likely to confuse animal and pareidolic faces with each other than with human faces."}]}