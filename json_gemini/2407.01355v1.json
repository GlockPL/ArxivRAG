{"title": "Hyperspectral Pansharpening: Critical Review, Tools and Future Perspectives", "authors": ["Matteo Ciotola", "Giuseppe Guarino", "Gemine Vivone", "Giovanni Poggi", "Jocelyn Chanussot", "Antonio Plaza", "Giuseppe Scarpa"], "abstract": "Hyperspectral pansharpening consists of fusing a high-resolution panchromatic band and a low-resolution hyperspectral image to obtain a new image with high resolution in both the spatial and spectral domains. These remote sensing products are valuable for a wide range of applications, driving ever growing research efforts. Nonetheless, results still do not meet application demands. In part, this comes from the technical complexity of the task: compared to multispectral pansharpening, many more bands are involved, in a spectral range only partially covered by the panchromatic component and with overwhelming noise. However, another major limiting factor is the absence of a comprehensive framework for the rapid development and accurate evaluation of new methods. This paper attempts to address this issue.\nWe started by designing a dataset large and diverse enough to allow reliable training (for data-driven methods) and testing of new methods. Then, we selected a set of state-of-the-art methods, following different approaches, characterized by promising performance, and reimplemented them in a single PyTorch framework. Finally, we carried out a critical comparative analysis of all methods, using the most accredited quality indicators. The analysis highlights the main limitations of current solutions in terms of spectral/spatial quality and computational efficiency, and suggests promising research directions.\nTo ensure full reproducibility of the results and support future research, the framework (including codes, evaluation procedures and links to the dataset) is shared on https://github.com/matciotola/hyperspectral_\npansharpening_toolbox, as a single Python-based reference benchmark toolbox.", "sections": [{"title": "I. INTRODUCTION", "content": "High resolution is the most desired feature in a remote sensing image. High spatial resolution enables precise detection"}, {"title": "II. METHODS", "content": "This section reviews SoTA solutions for HS pansharpening. However, rather than providing an exhaustive list of all papers published on this topic, we focus on a subset of representative methods, selected because they a) are distinctive from the methodological point of view; b) are competitive in terms of quality and/or computational efficiency; c) come with a publicly available software code (in any programming language) or are described with sufficient detail to be coded anew. All of them have been re-implemented (and assessed) in this work and eventually released as part of the proposed benchmarking toolbox.\nTab. I summarizes these methods, grouped according to the usual [2] categories of Component Substitution (CS), Multi-Resolution Analysis (MRA), Model-Based Optimization (MBO) which extends the usual Variational Optimization class, and DL. In the pansharpening literature, in addition to the fusion methods it is also custom to provide the simple expansion (EXP) of the HS (with no fusion with the PAN) obtained by an approximation of the ideal interpolator. This because such a product is a latent variable of many methods and represents a spectral benchmark. In the upcoming description, we will use the symbols recalled in Tab. II."}, {"title": "A. Component substitution", "content": "The methods [1], [2], [26] are based on the use of a suitable transformation that brings the data to a domain where spatial and spectral components can be easily separated. Then, the spatial component is replaced by the available PAN and the data are transformed back into the original domain. By using a linear transformation and considering the substitution of a single component, a fast pansharpening process is obtained [63]. These techniques were initially proposed for MS pansharpening and, subsequently, extended to the HS case. Preliminarily, the HS datacube $H \\in \\mathbb{R}^{w \\times h \\times B}$ is spatially resized to match the target pansharpening scale using (a polynomial approximation of) the ideal interpolator [1], yielding $\\hat{H} \\in \\mathbb{R}^{W \\times H \\times B}$. The component to be replaced, $I \\in \\mathbb{R}^{W \\times H}$, is obtained through a simple linear combination of the upscaled bands {$\\hat{H}^b$} of $\\hat{H}$, through suitably defined weights $w = [w_1,\\dots,w_B]^T$:\n$I = \\sum_{b=1}^{B} w_b \\hat{H}^b$.\nBoth P and I are defined in the panchromatic domain and their difference, after histogram equalization, represents the high-frequency details missing in the original image. Therefore, the pansharpened bands {$\\tilde{H}$} of $\\tilde{H} \\in \\mathbb{R}^{W \\times H \\times B}$ are obtained by injecting these details in the upscaled HS bands, weighted by suitable coefficients, the injection gains $g = [g_1,\\dots,g_B]$:\n$\\tilde{H}^b = \\hat{H}^b + g_b (P - I), \\, b = 1, ..., B$.\nBy changing the transformation, different combinations of injection gains and weights are needed.\n1) GSA: A powerful example of the CS approach is based on the orthogonal Gram-Schmidt (GS) decomposition of $\\hat{H}$ [64]. Different variants can be obtained by changing the definition of the first component of the decomposition, the one to be replaced. The basic implementation proposed in [64] employs a simple uniform average: $w_b = 1/B, \\, \\forall b$. The component substitution is then completed by inverting the orthogonal decomposition using the injection gains:\n$g_b = \\frac{Cov(H^b, I)}{Var(I)}, \\, \\, b = 1, ..., B$,\nwhere $Cov(\\cdot)$ and $Var(\\cdot)$ indicate the covariance and variance operators, respectively. This solution, however, does not take into account the different levels of correlation between P and each individual band of H, giving rise to heavy spectral distortion. For this reason, a more robust variant, known as Gram-Schmidt Adaptive (GSA), was proposed in [65] for MS pansharpening and later applied to the HS case [25]. The latter is the first method selected for our experimental assessment. GSA differs from the classical GS implementation in that the weights {$w$} used to compute I are not all equal but are estimated by minimizing the mean square error between I and a spatially degraded version of the PAN image.\n2) BT-H: Another CS method considered in this work is BT-H [52], which relies on an improved version of the Brovey Transform (BT) [66] that accounts for haze. In this case, space-variant injection gains are used, defined as\n$G_n^b = \\frac{H_n^b - h^b}{I_n - h}, \\, \\forall n$,\nwhere the ratio is meant to be pixel-wise, n indicates the pixel location, hb the haze in the b-th HS band and h is the haze in both P and I. Of course, the product between the gains and (P I) in Eq. 2 must be pixel-wise as well. The weights w are estimated by minimizing the mean square error between I and a reduced resolution version of P.\n3) BDSD-PC: In [67] both weights w and gains g are obtained by minimizing the mean squared distance between the fused image given by Eq. 2 and the reference (GT) pansharpened image. Since the latter is not available, the optimization problem is shifted to the reduced resolution domain by means of a suitable scaling of the input images. This approach, known as Band-Dependent Spatial-Detail (BDSD) injection method [67], is here considered in the version proposed in [53] where a Physical Constraint is introduced (BDSD-PC) to regularize the estimation of the coefficients.\n4) PRACS: The last CS solution considered in our study is the Partial Replacement Adaptive Component Substitution (PRACS) [54], where the replacement of I is done, band-wise, with a suited weighted sum of P and $\\hat{H}^b$ rather than just using P.\nIn general, CS methods are characterized by high spatial fidelity, low processing time, and robustness to registration errors and aliasing [68], [1]. On the downside, they tend to introduce significant spectral distortion due to the spectral mismatch between P and $\\hat{H}$ [69]. This becomes a rather serious issue in the HS case, as opposed to the MS case, because of the large number of spectral bands that present little or no correlation with the PAN."}, {"title": "B. Multiresolution analysis", "content": "Methods based on Multi-Resolution Analysis (MRA) [1], [2], [26] are formally very similar to CS methods, as they also involve the injection of spatial details in HS upscaled bands:\n$\\tilde{H}^b = \\hat{H}^b + G_b \\cdot (P - P_{lp}), \\, \\, b = 1, ..., B$,\nThe fundamental difference is in how these spatial details, or so-called PAN details, are extracted. In MRA, they are computed as the difference between P and its low-pass filtered version, $P_{lp}$, while in CS as the difference between P and a weighted average of H along the spectral dimension. The use of different filtering strategies and injection rules gives rise to different MRA solutions. SoTA injection schemes follow two approaches [70], commonly referred to as projective and multiplicative or high pass modulation (HPM). The former uses spatially constant gains Gb, while the latter allows them to vary spatially so that the injection of detail can be modulated in the spatial domain. In more detail, the HPM injection gains are defined as $G = H/P^n_p, n$, with the goal of reproducing the local intensity contrast of the PAN image [70]. Different MRA techniques, however, are characterized mainly by how the PAN image is low-pass filtered.\n1) Laplacian-based techniques: MTF-GLP-*: A popular option is to use a Gaussian filter matched with the Modulation Transfer Function (MTF) of the high spectral resolution sensor, usually designed based on information provided by the manufacturer, such as the gains at the Nyquist frequency. In the context of multiresolution decomposition, this approach is usually called Generalized Laplacian Pyramid (GLP), since the desired detail (P \u2013 Plp) is approximated by Laplacian filtering."}, {"title": "C. Model-based optimization", "content": "Another important category includes methods based on an explicit analytic model of the problem, to be solved through suitable optimization techniques. These are further divided into three subcategories: Bayesian [71], [20], [18], dictionary-based (namely, based on sparse representations) [60], and variational [61].\n1) Bayesian estimation: A mathematically appealing way to address pansharpening is to cast it as an inverse prob-lem in a probabilistic framework, to be solved by means of Bayesian estimation. Preliminarily, following a methodology well-known in linear HS unmixing [72], the target high-resolution HS image is assumed to belong to a low-dimensional subspace, such to be expressed as\n$\\tilde{H} = MX$,\nwhere $\\tilde{H}$ has been arranged in a $B\\times HW$ 2-D matrix by collapsing the two spatial dimensions, X is a $C \\times HW$ 2-D matrix, with $C \\ll B$, projection of $\\tilde{H}$ in the low-dimensional subspace, and M is the $B \\times C$ projection matrix, whose columns are the basis of the new subspace. The transformation M can be obtained via different approaches, e.g., principal component analysis [73] or vertex component analysis [74]. Once a solution, X, is found, it can be re-mapped into the original HS space through Eq. 6. The probabilistic modelling of the problem requires two items: a likelihood term, p(P, $\\tilde{H}$|X) relating the observations, $\\tilde{H}$ and P, to the reduced-rank representation X, and a prior term, p(X). Then, the pansharpened image, $X_{MAP}$, can be found according to the maximum a posteriori (MAP) criterion as:\n$X_{MAP} = \\underset{X}{argmax}p(P, \\tilde{H}|X)p(X)$.\nTo solve the problem, some reasonable simplifying hypotheses can be made [75], [76], [77]:\nP = $\\phi_P(MX) + N_P$,\n$\\tilde{H}$ = $\\phi_H(MX) + N_H$,\nwhere $\\phi(\\cdot)$ denotes a linear mapping between $\\hat{H}$ and P, $\\Pi(\\cdot)$ is the spatial degradation model (low-pass filtering and decimation) relating high and low-resolution HS images, $N_P$ and $N_H$ are zero-mean normal-distributed random matrices [78]. Assuming the conditional independence of the two noise terms, the problem simplifies further to\n$X_{MAP} = \\underset{X}{argmax}p(P|X)p(\\tilde{H}|X)p(X)$.\nTo proceed to the optimization phase the prior distribution p(X) must be set. In [71] a simple pixel-wise independent Gaussian prior is assumed, while a more complex prior based on sparse representation is considered in [20]. However, in both cases the resulting HS pansharpening algorithms provide unsatisfactory results, and hence they are not further considered here. On the contrary, the Hyperspectral SUperREsolution (HySURE) method proposed in [18] is among the most promising in the field, proving clearly superior to classical Bayesian solutions in our experiments. It is characterized by an edge-preserving regularizing prior, a form of vector total variation, whose objective is to promote piecewise-smooth solutions with discontinuity aligned across the HS bands. To limit complexity, optimization is pursued in a low-dimensional subspace through the Alternating Direction Method of Multi-pliers (ADMM).\n2) SR-D: To represent dictionary-based methods, we selected the Sparse Representation of Details (SR-D) proposed in [60]. With this approach, the spatial details to inject into H are built from a suitably learned dictionary of patches. Let Dh and D\u00b9 be two paired dictionaries of patches at high resolution and low resolution, respectively. With these data, we aim at estimating a desired high-resolution patch, yh, based on its low-resolution version, y\u00b9. In detail, we solve the following problem\n$\\hat{a} = \\underset{a}{argmin} ||a||_0  subject to y^l = D^l a$,\nwhere $||\\cdot ||_0$ is the $l_0$ pseudonorm, used to induce sparsity in the solution. In practice, the target patch is approximated by a linear combination of patches in the low-resolution dictionary, with the constraint to use the least possible patches. Once the weights of the linear combination are obtained, they are used to estimate the high-resolution target patch by using the very same linear combination of the paired high-resolution patches in Dh. Eventually, we have that yh = Dha, where yh is the pansharpened product in vector form. Of course, the whole method relies on a strong assumption of invariance across scales (see [60] for more details)."}, {"title": "D. Deep learning", "content": "As already said, DL is by far the most popular approach for MS and HS pansharpening, these days. Following our criteria, we selected seven SoTA methods for our toolbox, five of them based on supervised learning [34], [38], [35], [36], [62], and two [37], [50] on unsupervised learning. Due to the lack of GT, supervised DL models are trained following the classic Wald's protocol [79], as suggested in [31]. This consists of low-pass filtering and decimating (in both directions) both PAN and HS by the same factor R, equal to the PAN-MS resolution ratio. These downscaled images are then used as synthetic PAN and HS input for training the pansharpening network, using the original HS data as GT. The network trained on reduced resolution data is then used to pansharpen full resolution data, relying on a scale invariance assumption. This latter hypothesis, however, is rather shaky, which is why unsupervised DL methods are appearing with increasing frequency lately.\n1) HyperPNN: This model, proposed in [34], has a relatively shallow 7-layer architecture, comprising a spectral encoder, a spatial-spectral inference subnet, and a spectral decoder. More precisely, the network takes in input the interpolated HS image $\\hat{H}$ and the PAN P, yielding in output the pansharpened image $\\tilde{H}$ by composing the following three subnets:\ne: $\\tilde{H} \\in \\mathbb{R}^{W \\times H \\times B} \\rightarrow X = e(\\hat{H}) \\in \\mathbb{R}^{W \\times H \\times 64}$,\nf: $(X, P) \\in \\mathbb{R}^{W \\times H \\times 65} \\rightarrow \\tilde{Y} = f(X, P) \\in \\mathbb{R}^{W \\times H \\times 64}$,\nd: $\\tilde{Y} \\in \\mathbb{R}^{W \\times H \\times 64} \\rightarrow \\tilde{H} = d(\\tilde{Y}) \\in \\mathbb{R}^{W \\times H \\times B}$.\nBoth encoder (e) and decoder (d) work exclusively in the spectral domain, with 1\u00d71 convolutions (two layers each), and are responsible for spectral preservation. The middle subnet, f, works jointly in the spatial and spectral domains with three convolutional layers, each with a 3\u00d73 receptive field and 64 output features. Two variants are presented in [34], with or without skip connections over f. Here, we consider only the latter, the most effective, where the feature volume entering f is brought directly to the output so that the network can focus on reconstructing the residual. As loss, the authors use the mean square error (MSE), which is the baseline option to compare a predicted image to the corresponding GT.\nIt is worth underlining that the HyperPNN network, like other networks described later on, is designed to work with a fixed number of bands. This is a major limitation in HS pansharpening where the number of available bands changes from sensor to sensor and, due to noise, from image to image. In fact, to deal with the three images of the dataset (Tab. IV) the authors had to train three different image-dependent networks.\n2) HSpeNet: This model, proposed in [38], improves upon HyperPNN in two main aspects: architecture and loss. The network comprises an additional preprocessing subnet, g, that extracts suitable features from the PAN to feed the middle subnet\ng: $P \\in \\mathbb{R}^{W \\times H} \\rightarrow Q = g(P) \\in \\mathbb{R}^{W \\times H \\times 16}$,\nf: $(X, Q) \\in \\mathbb{R}^{W \\times H \\times 80} \\rightarrow Y = f(X, Q) \\in \\mathbb{R}^{W \\times H \\times 160}$,\nThe subnet g comprises two convolutional layers with 3\u00d73 receptive fields and 16 output features each. The feature fusion subnet f has been upgraded as well to a more effective 5-level DenseNet-like structure. Finally, a global skip connection has been introduced yielding an output of subnet d in the form:\n$\\tilde{H} = d(\\tilde{Y}, \\hat{H}) = d_0 (\\tilde{Y}) + \\hat{H} \\in \\mathbb{R}^{W \\times H \\times B}$\nwhere do is a single 1\u00d71 convolutional layer that transforms 160 feature maps in B detail bands that are added to the smooth component H to provide the final pansharpened image, $\\tilde{H}$. The other important difference with HyperPNN is the loss function, which includes an additional term based on the Spectral Angle Mapper (SAM) [80] to enforce spectral consistency.\n3) DHP-DARN: This method [35] relies on two key elements, the use of a deep hyperspectral prior (DHP) model aimed at improving the preliminary upscaling of H, and the use of the dual-attention residual network (DARN). Deep image priors (DIP) [81] are called upon to make up for the scarcity of data typical of many problems. The idea is that, lacking sufficient training data, the input to the network should be as close as possible to the expected output. In our case, the input, $\\hat{H}$, should be close to the expected result, $\\tilde{H}$, or, at least, spectrally consistent with the original $\\hat{H}$. The overall effect is a sort of prior regularization that prevents possible generalization issues. Unlike other upsampling options, that work band-wise regardless of spectral dependencies, the DHP module is tuned online on the very same target image to guarantee that the upscaled $\\hat{H}$, when degraded, will return to $\\hat{H}$. Then, the actual fusion process is carried out through the main network, denoted by the function fDARN, which consists of a sequence of three subnets by-passed by a global skip connection,\n$\\tilde{H} = f_{DARN} (\\hat{H}, P) + \\hat{H} = \\hat{H}_{res} + \\hat{H}$,\nwhere $\\hat{H}_{res}$ is the residue (or detail) component of $\\hat{H}$. The first and the third subnets of fDARN are stacked convolutional blocks while the central section is a sequence residual Channel-Spatial Attention (CSA) modules. Training is carried out using a l\u2081-norm loss function.\n4) DIP-HyperKite: Another approach based on the deep image prior, called DIP-HyperKite, has been proposed in [36]. In this case, the generated prior image $\\tilde{H}$ is forced to be consistent not only with $\\hat{H}$ but also with P. This is achieved through an additional loss term that compares P to a weighted average of H along the spectral dimension, where the weights are also learned. A residual learning scheme is used, like in DARN. However, an innovative architecture is proposed here, a sort of inverse U-Net [82] where pooling and unpooling operations are exchanged, with the latter working on the \u201cencoding\" side and the former moved to the \"decoding\u201d part. By doing so, in the central part of the network, the spatial resolution increases up to eight times in both directions compared to the target resolution. This overcomplete representation is used because the residue to be predicted, $\\tilde{H}$ H, is mostly concentrated in the higher frequencies. A spatial expansion allows to widening of the frequency domain beyond the limits set by the PAN resolution, increasing the network's ability to synthesize high-frequency spatial details. It goes without saying that the computational complexity increases considerably, both in the training and inference phases.\n5) HyperDSNet: This model [62] relies on the use of three key elements, as summarized below: a set of handcrafted operators d that extract additional differential features from the PAN; a subnet fDS that extracts multiscale Deep-Shallow (DS) features; a Spectral Attention (SA) module fSA that generates the output residues $\\tilde{H}$ H through a suitable combination of the extracted features.\nd: $P \\in \\mathbb{R}^{W \\times H} \\rightarrow Q = d(P) \\in \\mathbb{R}^{W \\times H \\times 6}$,\nfDS: $(\\hat{H}, P, Q) \\in \\mathbb{R}^{W \\times H \\times (B+7)} \\rightarrow F_{DS} \\in \\mathbb{R}^{W \\times H \\times B}$,\nfSA: $\\tilde{H} \\in \\mathbb{R}^{W \\times H \\times B} \\rightarrow W_{SA} = f_{SA}(\\tilde{H}) \\in \\mathbb{R}^{1 \\times 1 \\times B}$,\n0 : $\\tilde{H} = \\hat{H} + \\hat{H}_{res} = \\hat{H} + W_{SA} \\cdot F_{DS}$\nThe features Q, obtained using classical derivative operators such as Roberts, Prewitt and Sobel, feed the subsequent feature extractor fDS together with the input pair ($\\hat{H}$, P). The DS subnet, composed of a sequence of convolutional layers, provides output features extracted not only by the last layer but also by intermediate layers (hence deep-shallow), then reduced to B spectral channels. In parallel, the SA module computes the weights wsa used eventually in the output block to modulate on a per-band basis the detail injection strength. The whole network is trained end-to-end according to a traditional supervised scheme using a l\u2081-norm loss. Supervised DL-based methods have great potential, as testified by numerous success stories in closely related fields. Unfortunately, in the case of HS pansharpening there are many problems that prevent the desired results from being achieved:\n(a) The training is performed on synthetic data, obtained through resolution degradation processes, and there is no guarantee that a model trained on such data will work equally well on real full-resolution datasets. This is a general limit of any supervised pansharpening network.\n(b) The volume of data available for training, already limited by the lack of freely available HS datasets, is further reduced in the presence of resolution downgrading.\n(c) HS images are characterized by a varying number of spectral bands, due to differences among sensors and also to acquisition noise. However, none of the above models can work with a variable number of bands. Therefore, even when many images are available, they cannot be joined to form a single, training set and, in any case, these methods cannot easily generalize to new images.\nA partial response to these shortcomings is given by DHP-DARN and DIP-HyperKite, which use deep image priors to balance weakly trained networks. The more natural solution, however, is to use unsupervised training schemes [46], [83], [84], [47], which exploit only original full-resolution data to train the network with no need for GT. In this case, specific loss functions must be defined and carefully designed to drive the network towards the desired behavior. These losses comprise at least two terms, referred to as spectral ($L_s$) and spatial ($L_x$) consistency loss terms,\nL = L_x + \u03b2L_s = L_1(\\tilde{H}, \\hat{H}) + \u03b2L_s (\\tilde{H}, P).\nThe first term accounts for spectral fidelity and is usually computed by projecting $\\tilde{H}$ into the domain of H through a resolution downgrading and evaluating their distance by the l1 or 12 norms. The second term, responsible for the spatial quality of the fused image, is more difficult to define. The main options are: i) to synthesize a pseudo-PAN through a weighted average of the bands of $\\tilde{H}$ and compare it with P; ii) to compare each band of $\\tilde{H}$ individually with P and then summarize results. In both cases, the comparison should rely only on the high spatial frequency components of H.\n6) R-PNN: The unsupervised Rolling Pansharpening Neu-ral Network (R-PNN) [37] addresses explicitly the issues (a)-(c) mentioned before. It relies heavily on the target-adaptive strategy, originally introduced for the MS case in both supervised [85] and unsupervised [47] settings, which consists of fine-tuning the pre-trained network on the target data. R-PNN uses target adaptation in a \u201crolling\u201d modality, that is, spectral bands are pansharpened one at a time, by fine-tuning the network for the current band starting from the weights optimized for the previous one. Formally, let $\\Phi^0$ and $\\Phi^\\infty$ be the initial and final (tuned) net parameters for band b, then $\\Phi^{b+1} = \\Phi^\\infty$ are the initial parameters for band b + 1, to be adapted through a number of iterations proportional to the spectral distance between the two bands. Since adjacent bands are highly correlated, very few tuning iterations are sufficient to obtain accurate results, which limits computational complexity. In addition, the pansharpening network is a lightweight residual CNN, called Zoom-PNN (Z-PNN) [47], adapted to the single-band pansharpening case:\nf : ($\\tilde{H}^b, P) \\in \\mathbb{R}^{W \\times H \\times 2 \\times \u00de} \\rightarrow \\hat{H}^b = f(\\hat{H}^b, P) \\in \\mathbb{R}^{W \\times H}\nThe unsupervised loss, used both for pre-training and tuning, comprises a spectral term $L_s$ based on the l\u2081-norm and a spatial term \u0141s based on the local correlation coefficient [86].\n7) PCA-Z-PNN: A further adaptation of the Z-PNN method [47] to HS pansharpening is proposed in [50] based on PCA. The key observation is that the hundreds of bands comprising the HS image can be transformed into a new space where most of the energy is kept in a much smaller number of components. PCA is a natural candidate for such transformation and preliminary experiments on typical HS images show that it can compact about 99% of the energy in just 8 bands, that is the number of bands used in Z-PNN pansharpening.\nWith this premise, the method is easily explained. The input $\\hat{H}$ is first whitened using the PCA transform. Then, the first 8 principal components $H_{PCA}$ are pansharpened using Z-PNN in the target adaptive modality. Finally, the pansharpened components $\\hat{H}_{PCA}$ are concatenated with the remaining low-energy components $\\hat{H}_{rem}$ and transformed back to the original space. In formulas, the process can be summarized as follows:\nW = eig($\\hat{H}\\hat{H}^T$),\n$\\$\\mathbb{H} = \\hat{H}W$,\n$\\hat{H}_{PCA, \\hat{H}_{rem}} = \\mathbb{H}$],\\\n$\\hat{H}_{PCA} = f_{Z-PNN} (\\hat{H}_{PCA}, P)$,\n$\\tilde{H} = [\\hat{H}_{PCA}, \\hat{H}_{rem}]W^{-1}$,\nwhere, H is zero-meaned and reshaped as a $WH \\times B$ matrix, W is the B\u00d7B matrix whose columns are the eigenvectors of $\\hat{H}\\hat{H}^T$, fz-PNN is the pansharpening function, and (\u00b7)T and (.)-1 denote transpose and inverse, respectively.\nIt is worth underlining that the PCA rotation can change from one image to another with no harm since Z-PNN runs in the target-adaptive modality and adapts to the new statistics. Along the same line, following experimental evidence, it has been found effective to pansharp separately the set of bands falling in the visible spectrum and those ranging from near to shortwave infrared, applying the above-described scheme twice."}, {"title": "III. QUALITY ASSESSMENT", "content": "The goal of pansharpening is to take two low-quality images, having reduced spatial (the HS component) or spectral resolution (the PAN), and synthesize a high-quality image at full resolution that cannot be observed in reality. Since the desired full-resolution image is not observable, there is no GT for objectively measuring the quality of the synthesized image. As a consequence, assessing the quality of a pansharpening algorithm is by no means trivial and remains essentially an open problem, extensively investigated in the past two decades.\nA popular measurement protocol was proposed in [79], where fusion products are required to satisfy two properties: consistency and synthesis. The consistency property states that the pansharpened image, once degraded at the lower spatial resolution of the original HS, should be as similar as possible to the latter. Similarly, a proper spectral degradation process of the pansharpened image should provide a single-band image as similar as possible to the original PAN. By this definition, it is clear that consistency can be easily measured. However, it represents only a check, and even perfect consistency does not ensure that the pansharpened image has the desired quality. The synthesis property is more stringent, as it states that the pansharpened image should be as similar as possible to the HS image that would be acquired by the HS sensor if it had the same (high) spatial resolution as the PAN sensor. Unfortunately, lacking this latter image, the synthesis property cannot be directly assessed and it has a mostly ideal nature.\nOne can circumvent the problem by resorting to the so-called Reduced Resolution (RR) assessment [79], [87]. The idea is to run the pansharpening algorithm using as input the spatially downgraded versions of PAN and HS. The output of the algorithm will have the same resolution as the original HS, which can therefore serve as GT. Thanks to the presence of a reference image, RR quality assessment is simple and accurate, it only requires a metric for measuring the similarity of multi-band images. However, there is no guarantee that a method that works well on low-resolution data will work equally well on high-resolution data, namely that a sort of scale-invariance holds. In addition, the degradation process required by this protocol may introduce biases and errors. From this point of view, the choice of suitable filters is crucial to ensure the consistency of the pansharpening process. In particular, before decimating the HS image, filters that match the HS sensor's MTFs should be used [65]. For the PAN image, instead, an ideal filter is preferred [87], to preserve the details that would have been seen with a direct RR acquisition.\nTo overcome these problems, pansharpening products can also be evaluated at full resolution, using quality indices specifically developed for this purpose according to the Quality with No Reference (QNR) paradigm [88], [89]. Of course, in the absence of a GT, such quantitative measures remain arbitrary to some extent. Typically, two complementary quality indexes are considered to measure spatial and spectral consistency. These may follow opposite trends, with the paradox that the least spectral distortion is obtained when no spatial enhancement is introduced. Therefore, a suitable combination of them is necessary.\nIn the rest of this Section, we briefly review the quality indices considered in our toolbox. However, it is worth emphasizing once again that quality assessment in pansharpening is an ill-posed problem, with no simple solution, and is still the subject of intense research. Both the reduced resolution and full resolution approaches have advantages and disadvantages. A good practice, consistently followed in the literature, is to use a wide range of indices and to always integrate the numerical results with a critical visual inspection of the images by experts."}, {"title": "A. RR assessment", "content": "Following a common practice for pansharpening assessment [2", "87": [4], "ERGAS": "The Erreur Relative Globale Adimensionnelle de Synth\u00e8se (ERGAS) [90", "follows": "n$ERGAS = 100 \\frac{1"}, {"SAM": "The Spectral Angle Mapper (SAM) [80", "\u00fbB": "and v = [v1", "vB": ".", "have": "n$SAM = E  [arccos (\\frac{&lt;\\hat{v},v&gt;}{||\\hat{v}||_2 ||v||_2})"}]}