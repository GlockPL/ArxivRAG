{"title": "Hyperspectral Pansharpening: Critical Review, Tools and Future Perspectives", "authors": ["Matteo Ciotola", "Giuseppe Guarino", "Gemine Vivone", "Giovanni Poggi", "Jocelyn Chanussot", "Antonio Plaza", "Giuseppe Scarpa"], "abstract": "Hyperspectral pansharpening consists of fusing a high-resolution panchromatic band and a low-resolution hyperspectral image to obtain a new image with high resolution in both the spatial and spectral domains. These remote sensing products are valuable for a wide range of applications, driving ever growing research efforts. Nonetheless, results still do not meet application demands. In part, this comes from the technical complexity of the task: compared to multispectral pansharpening, many more bands are involved, in a spectral range only partially covered by the panchromatic component and with overwhelming noise. However, another major limiting factor is the absence of a comprehensive framework for the rapid development and accurate evaluation of new methods. This paper attempts to address this issue.\nWe started by designing a dataset large and diverse enough to allow reliable training (for data-driven methods) and testing of new methods. Then, we selected a set of state-of-the-art methods, following different approaches, characterized by promising performance, and reimplemented them in a single PyTorch framework. Finally, we carried out a critical comparative analysis of all methods, using the most accredited quality indicators. The analysis highlights the main limitations of current solutions in terms of spectral/spatial quality and computational efficiency, and suggests promising research directions.\nTo ensure full reproducibility of the results and support future research, the framework (including codes, evaluation procedures and links to the dataset) is shared on https://github.com/matciotola/hyperspectral_\npansharpening_toolbox, as a single Python-based reference benchmark toolbox.", "sections": [{"title": "I. INTRODUCTION", "content": "High resolution is the most desired feature in a remote sensing image. High spatial resolution enables precise detection"}, {"title": "II. METHODS", "content": "This section reviews SoTA solutions for HS pansharpening. However, rather than providing an exhaustive list of all papers published on this topic, we focus on a subset of representative methods, selected because they a) are distinctive from the methodological point of view; b) are competitive in terms of quality and/or computational efficiency; c) come with a publicly available software code (in any programming language) or are described with sufficient detail to be coded anew. All of them have been re-implemented (and assessed) in this work and eventually released as part of the proposed benchmarking toolbox.\nTab. I summarizes these methods, grouped according to the usual [2] categories of Component Substitution (CS), Multi-Resolution Analysis (MRA), Model-Based Optimization (MBO) which extends the usual Variational Optimization class, and DL. In the pansharpening literature, in addition to the fusion methods it is also custom to provide the simple expansion (EXP) of the HS (with no fusion with the PAN) obtained by an approximation of the ideal interpolator. This because such a product is a latent variable of many methods and represents a spectral benchmark. In the upcoming description, we will use the symbols recalled in Tab. II."}, {"title": "A. Component substitution", "content": "The methods [1], [2], [26] are based on the use of a suitable transformation that brings the data to a domain where spatial and spectral components can be easily separated. Then, the spatial component is replaced by the available PAN and the data are transformed back into the original domain. By using a linear transformation and considering the substitution of a single component, a fast pansharpening process is obtained [63]. These techniques were initially proposed for MS pansharpening and, subsequently, extended to the HS case. Preliminarily, the HS datacube $H \\in R^{w \\times h \\times B}$ is spatially resized to match the target pansharpening scale using (a polynomial approximation of) the ideal interpolator [1], yielding $\\hat{H} \\in R^{W \\times H \\times B}$. The component to be replaced, $I \\in R^{W \\times H}$, is obtained through a simple linear combination of the upscaled bands {$\\hat{H}$} of $\\hat{H}$, through suitably defined weights $w = [w_1,...,w_B]^T$:\n$I = \\sum_{b=1}^{B} w_b \\hat{H}^b$.(1)\nBoth P and I are defined in the panchromatic domain and their difference, after histogram equalization, represents the high-frequency details missing in the original image. Therefore, the pansharpened bands {$\\hat{H}$} of $\\hat{H} \\in R^{W \\times H \\times B}$ are obtained by injecting these details in the upscaled HS bands, weighted by suitable coefficients, the injection gains $g = [g_1,...,g_B]$:\n$\\hat{H}^b = \\hat{H}^b + g_b (P - I), b = 1, ..., B$,(2)\nBy changing the transformation, different combinations of injection gains and weights are needed.\n1) GSA: A powerful example of the CS approach is based on the orthogonal Gram-Schmidt (GS) decomposition of $\\hat{H}$ [64]. Different variants can be obtained by changing the definition of the first component of the decomposition, the one to be replaced. The basic implementation proposed in [64] employs a simple uniform average: $w_b = 1/B, \\forall b$. The component substitution is then completed by inverting the orthogonal decomposition using the injection gains:\n$g_b = \\frac{Cov(\\hat{H}^b, I)}{Var(I)}, b = 1, ..., B$,(3)\nwhere $Cov(\\cdot)$ and $Var(\\cdot)$ indicate the covariance and variance operators, respectively. This solution, however, does not take into account the different levels of correlation between P and each individual band of H, giving rise to heavy spectral distortion. For this reason, a more robust variant, known as Gram-Schmidt Adaptive (GSA), was proposed in [65] for MS pansharpening and later applied to the HS case [25]. The latter is the first method selected for our experimental assessment. GSA differs from the classical GS implementation in that the weights {$w$} used to compute I are not all equal but are estimated by minimizing the mean square error between I and a spatially degraded version of the PAN image.\n2) BT-H: Another CS method considered in this work is BT-H [52], which relies on an improved version of the Brovey Transform (BT) [66] that accounts for haze. In this case, space-variant injection gains are used, defined as\n$G_n^b = \\frac{H_n^b - h^b}{I_n - h}, \\forall n$,(4)\nwhere the ratio is meant to be pixel-wise, n indicates the pixel location, $h^b$ the haze in the b-th HS band and h is the haze in both P and I. Of course, the product between the gains and (P - I) in Eq. 2 must be pixel-wise as well. The weights w are estimated by minimizing the mean square error between I and a reduced resolution version of P.\n3) BDSD-PC: In [67] both weights w and gains g are obtained by minimizing the mean squared distance between the fused image given by Eq. 2 and the reference (GT) pansharpened image. Since the latter is not available, the optimization problem is shifted to the reduced resolution domain by means of a suitable scaling of the input images. This approach, known as Band-Dependent Spatial-Detail (BDSD) injection method [67], is here considered in the version proposed in [53] where a Physical Constraint is introduced (BDSD-PC) to regularize the estimation of the coefficients.\n4) PRACS: The last CS solution considered in our study is the Partial Replacement Adaptive Component Substitution (PRACS) [54], where the replacement of I is done, band-wise, with a suited weighted sum of P and $\\hat{H}^b$ rather than just using P.\nIn general, CS methods are characterized by high spatial fidelity, low processing time, and robustness to registration errors and aliasing [68], [1]. On the downside, they tend to introduce significant spectral distortion due to the spectral mismatch between P and $\\hat{H}$ [69]. This becomes a rather serious issue in the HS case, as opposed to the MS case, because of the large number of spectral bands that present little or no correlation with the PAN."}, {"title": "B. Multiresolution analysis", "content": "Methods based on Multi-Resolution Analysis (MRA) [1], [2], [26] are formally very similar to CS methods, as they also involve the injection of spatial details in HS upscaled bands:\n$\\hat{H}^b = \\hat{H}^b + G_b \\cdot (P - P_{lp}), b = 1, ..., B$,(5)\nThe fundamental difference is in how these spatial details, or so-called PAN details, are extracted. In MRA, they are computed as the difference between P and its low-pass filtered version, $P_{lp}$, while in CS as the difference between P and a weighted average of $\\hat{H}$ along the spectral dimension. The use of different filtering strategies and injection rules gives rise to different MRA solutions. SoTA injection schemes follow two approaches [70], commonly referred to as projective and multiplicative or high pass modulation (HPM). The former uses spatially constant gains $G_b$, while the latter allows them to vary spatially so that the injection of detail can be modulated in the spatial domain. In more detail, the HPM injection gains are defined as $G = \\hat{H}/P_{lp, n}^P$, with the goal of reproducing the local intensity contrast of the PAN image [70]. Different MRA techniques, however, are characterized mainly by how the PAN image is low-pass filtered.\n1) Laplacian-based techniques: MTF-GLP-*: A popular option is to use a Gaussian filter matched with the Modulation Transfer Function (MTF) of the high spectral resolution sensor, usually designed based on information provided by the manufacturer, such as the gains at the Nyquist frequency. In the context of multiresolution decomposition, this approach is usually called Generalized Laplacian Pyramid (GLP), since the desired detail (P - $P_{lp}$) is approximated by Laplacian filtering.\nIn this work, we consider several SoTA techniques belonging to this category. The MTF-GLP-FS method is based on the Full Scale (FS) projective injection rule proposed in [55] working directly at full resolution. It has proven especially valuable in fusion problems characterized by a large resolution ratio. The classical multiplicative injection model is considered in MTF-GLP-HPM [25], implemented with suitable histogram matching to account for possible spectral distortions. Another variant of the multiplicative injection rule is introduced in MTF-GLP-HPM-R [57]. Here, the \u201cR\u201d stands for regression, because the spectral matching between each $\\hat{H}^b$ and $P_{lp}$ is achieved employing multivariate linear regression, better motivated than the classical histogram matching procedure under a physical point of view [57].\n2) AWLP: Other solutions are based on the wavelet transform. One of the most popular is the Additive Wavelet Luminance Proportional (AWLP) method [58] where PAN detail extraction relies on undecimated \u00e0 trous wavelet transform. This method also uses a multiplicative injection scheme and histogram equalization.\n3) MF: An example of non-linear decomposition, called Morphological Filters (MF), is instead explored in [59], where the pyramid decomposition is built using morphological half-gradient filters.\nOverall, the main advantages of MRA approaches are temporal coherence [2], spectral consistency [1], and robustness to aliasing under proper conditions [68]. On the downside, they are more sensible than CS methods to mis-registration and spatial distortion and are more computationally demanding."}, {"title": "C. Model-based optimization", "content": "Another important category includes methods based on an explicit analytic model of the problem, to be solved through suitable optimization techniques. These are further divided into three subcategories: Bayesian [71], [20], [18], dictionary-based (namely, based on sparse representations) [60], and variational [61].\n1) Bayesian estimation: A mathematically appealing way to address pansharpening is to cast it as an inverse problem in a probabilistic framework, to be solved by means of Bayesian estimation. Preliminarily, following a methodology well-known in linear HS unmixing [72], the target high-resolution HS image is assumed to belong to a low-dimensional subspace, such to be expressed as\n$H = MX$,(6)\nwhere H has been arranged in a B\u00d7HW 2-D matrix by collapsing the two spatial dimensions, X is a C\u00d7HW 2-D matrix, with C \u226a B, projection of H in the low-dimensional subspace, and M is the B\u00d7C projection matrix, whose columns are the basis of the new subspace. The transformation M can be obtained via different approaches, e.g., principal component analysis [73] or vertex component analysis [74]. Once a solution, X, is found, it can be re-mapped into the original HS space through Eq. 6. The probabilistic modelling of the problem requires two items: a likelihood term, p(P, $\\hat{H}$|X) relating the observations, $\\hat{H}$ and P, to the reduced-rank representation X, and a prior term, p(X). Then, the pansharpened image, $X_{MAP}$, can be found according to the maximum a posteriori (MAP) criterion as:\n$X_{MAP} = argmax p(P, \\hat{H}|X)p(X)$.(7)\n$X$\nTo solve the problem, some reasonable simplifying hypotheses can be made [75], [76], [77]:\n$P = \\phi_P(MX) + N_P$,(8)\n$\\hat{H} = \\phi_H(MX) + N_H$,(9)\nwhere $\\phi(\\cdot)$ denotes a linear mapping between $\\hat{H}$ and P, $\\phi_H(\\cdot)$ is the spatial degradation model (low-pass filtering and decimation) relating high and low-resolution HS images, $N_P$ and $N_H$ are zero-mean normal-distributed random matrices [78]. Assuming the conditional independence of the two noise terms, the problem simplifies further to\n$X_{MAP} = argmax p(P|X)p(\\hat{H}|X)p(X)$.(10)\nTo proceed to the optimization phase the prior distribution p(X) must be set. In [71] a simple pixel-wise independent Gaussian prior is assumed, while a more complex prior based on sparse representation is considered in [20]. However, in both cases the resulting HS pansharpening algorithms provide unsatisfactory results, and hence they are not further considered here. On the contrary, the Hyperspectral SUperREsolution (HySURE) method proposed in [18] is among the most promising in the field, proving clearly superior to classical Bayesian solutions in our experiments. It is characterized by an edge-preserving regularizing prior, a form of vector total variation, whose objective is to promote piecewise-smooth solutions with discontinuity aligned across the HS bands. To limit complexity, optimization is pursued in a low-dimensional subspace through the Alternating Direction Method of Multipliers (ADMM).\n2) SR-D: To represent dictionary-based methods, we selected the Sparse Representation of Details (SR-D) proposed in [60]. With this approach, the spatial details to inject into $\\hat{H}$ are built from a suitably learned dictionary of patches. Let $D_h$ and $D_l$ be two paired dictionaries of patches at high resolution and low resolution, respectively. With these data, we aim at estimating a desired high-resolution patch, $y_h$, based on its low-resolution version, $y_l$. In detail, we solve the following problem\n$\\hat{\\alpha} = argmin ||\\alpha||_0 \\text{ subject to } y_l = D_l \\alpha$,(11)\nwhere $|| \\cdot ||_0$ is the $l_0$ pseudonorm, used to induce sparsity in the solution. In practice, the target patch is approximated by a linear combination of patches in the low-resolution dictionary, with the constraint to use the least possible patches. Once the weights of the linear combination are obtained, they are used to estimate the high-resolution target patch by using the very same linear combination of the paired high-resolution patches in $D_h$. Eventually, we have that $y_h = D_h \\hat{\\alpha}$, where $y_h$ is the pansharpened product in vector form. Of course, the whole method relies on a strong assumption of invariance across scales (see [60] for more details).\n3) TV: The variational optimization method proposed in [61], simply referred to as Total Variation (TV) here, is defined by the following TV-regularized least squares problem:\n$||y - MX||^2 + \\lambda TV(X)$,\n(12)\nwhere y is a suitably reshaped composition of the HS and PAN components, M = [$M_1$, $M_2$] consists of a decimation matrix $M_1$ and a weight matrix $M_2$ summarizing the (supposed) linear dependence between HS and PAN, TV(\u00b7) is an isotropic TV regularizer, and \u03bb is a balance parameter. The pansharpened image x is obtained by minimizing the above convex cost function using a majorization-minimization algorithm detailed in [61]."}, {"title": "D. Deep learning", "content": "As already said, DL is by far the most popular approach for MS and HS pansharpening, these days. Following our criteria, we selected seven SoTA methods for our toolbox, five of them based on supervised learning [34], [38], [35], [36], [62], and two [37], [50] on unsupervised learning. Due to the lack of GT, supervised DL models are trained following the classic Wald's protocol [79], as suggested in [31]. This consists of low-pass filtering and decimating (in both directions) both PAN and HS by the same factor R, equal to the PAN-MS resolution ratio. These downscaled images are then used as synthetic PAN and HS input for training the pansharpening network, using the original HS data as GT. The network trained on reduced resolution data is then used to pansharpen full resolution data, relying on a scale invariance assumption. This latter hypothesis, however, is rather shaky, which is why unsupervised DL methods are appearing with increasing frequency lately.\n1) HyperPNN: This model, proposed in [34], has a relatively shallow 7-layer architecture, comprising a spectral encoder, a spatial-spectral inference subnet, and a spectral decoder. More precisely, the network takes in input the interpolated HS image $\\hat{H}$ and the PAN P, yielding in output the pansharpened image $\\tilde{H}$ by composing the following three subnets:\n$e: \\tilde{H} \\in R^{W \\times H \\times B} \\rightarrow X = e(\\tilde{H}) \\in R^{W \\times H \\times 64}$,\n$f: (X, P) \\in R^{W \\times H \\times 65} \\rightarrow \\tilde{Y} = f(X, P) \\in R^{W \\times H \\times 64}$,\n$d : \\tilde{Y} \\in R^{W \\times H \\times 64} \\rightarrow \\tilde{H} = d(\\tilde{Y}) \\in R^{W \\times H \\times B}$\nBoth encoder (e) and decoder (d) work exclusively in the spectral domain, with 1\u00d71 convolutions (two layers each), and are responsible for spectral preservation. The middle subnet, f, works jointly in the spatial and spectral domains with three convolutional layers, each with a 3\u00d73 receptive field and 64 output features. Two variants are presented in [34], with or without skip connections over f. Here, we consider only the latter, the most effective, where the feature volume entering f is brought directly to the output so that the network can focus on reconstructing the residual. As loss, the authors use the mean square error (MSE), which is the baseline option to compare a predicted image to the corresponding GT.\nIt is worth underlining that the HyperPNN network, like other networks described later on, is designed to work with a fixed number of bands. This is a major limitation in HS pansharpening where the number of available bands changes from sensor to sensor and, due to noise, from image to image. In fact, to deal with the three images of the dataset (Tab. IV) the authors had to train three different image-dependent networks.\n2) HSpeNet: This model, proposed in [38], improves upon HyperPNN in two main aspects: architecture and loss. The network comprises an additional preprocessing subnet, g, that extracts suitable features from the PAN to feed the middle subnet\n$g: P \\in R^{W \\times H} \\rightarrow Q = g(P) \\in R^{W \\times H \\times 16}$,\n$f: (X, Q) \\in R^{W \\times H \\times 80} \\rightarrow \\tilde{Y} = f(X, Q) \\in R^{W \\times H \\times 160}$,\nThe subnet g comprises two convolutional layers with 3\u00d73 receptive fields and 16 output features each. The feature fusion subnet f has been upgraded as well to a more effective 5-level DenseNet-like structure. Finally, a global skip connection has been introduced yielding an output of subnet d in the form:\n$\\tilde{H} = d(\\tilde{Y}, \\tilde{H}) = d_0 (\\tilde{Y}) + \\tilde{H} \\in R^{W \\times H \\times B}$\nwhere $d_0$ is a single 1\u00d71 convolutional layer that transforms 160 feature maps in B detail bands that are added to the smooth component $\\tilde{H}$ to provide the final pansharpened image, $\\tilde{H}$. The other important difference with HyperPNN is the loss function, which includes an additional term based on the Spectral Angle Mapper (SAM) [80] to enforce spectral consistency.\n3) DHP-DARN: This method [35] relies on two key elements, the use of a deep hyperspectral prior (DHP) model aimed at improving the preliminary upscaling of $\\tilde{H}$, and the use of the dual-attention residual network (DARN). Deep image priors (DIP) [81] are called upon to make up for the scarcity of data typical of many problems. The idea is that, lacking sufficient training data, the input to the network should be as close as possible to the expected output. In our case, the input, $\\tilde{H}$, should be close to the expected result, $\\tilde{H}$, or, at least, spectrally consistent with the original $\\tilde{H}$. The overall effect is a sort of prior regularization that prevents possible generalization issues. Unlike other upsampling options, that work band-wise regardless of spectral dependencies, the DHP module is tuned online on the very same target image to guarantee that the upscaled $\\tilde{H}$, when degraded, will return to $\\tilde{H}$. Then, the actual fusion process is carried out through the main network, denoted by the function $f_{DARN}$, which consists of a sequence of three subnets by-passed by a global skip connection,\n$\\tilde{H} = f_{DARN} (\\tilde{H}, P) + \\tilde{H} = \\tilde{H}_{res} + \\tilde{H}$,(13)\nwhere $\\tilde{H}_{res}$ is the residue (or detail) component of $\\tilde{H}$. The first and the third subnets of $f_{DARN}$ are stacked convolutional blocks while the central section is a sequence residual Channel-Spatial Attention (CSA) modules. Training is carried out using a l\u2081-norm loss function.\n4) DIP-HyperKite: Another approach based on the deep image prior, called DIP-HyperKite, has been proposed in [36]. In this case, the generated prior image $\\tilde{H}$ is forced to be consistent not only with $\\tilde{H}$ but also with P. This is achieved through an additional loss term that compares P to a weighted average of $\\tilde{H}$ along the spectral dimension, where the weights are also learned. A residual learning scheme is used, like in DARN. However, an innovative architecture is proposed here, a sort of inverse U-Net [82] where pooling and unpooling operations are exchanged, with the latter working on the \u201cencoding\" side and the former moved to the \u201cdecoding\" part. By doing so, in the central part of the network, the spatial resolution increases up to eight times in both directions compared to the target resolution. This overcomplete representation is used because the residue to be predicted, $\\tilde{H} - \\tilde{H}$, is mostly concentrated in the higher frequencies. A spatial expansion allows to widening of the frequency domain beyond the limits set by the PAN resolution, increasing the network's ability to synthesize high-frequency spatial details. It goes without saying that the computational complexity increases considerably, both in the training and inference phases.\n5) HyperDSNet: This model [62] relies on the use of three key elements, as summarized below: a set of handcrafted operators d that extract additional differential features from the PAN; a subnet $f_{DS}$ that extracts multiscale Deep-Shallow (DS) features; a Spectral Attention (SA) module $f_{SA}$ that generates the output residues $\\tilde{H} - \\tilde{H}$ through a suitable combination of the extracted features.\n$d: P \\in R^{W \\times H} \\rightarrow Q = d(P) \\in R^{W \\times H \\times 6}$,\n$f_{DS}: (\\tilde{H}, P, Q) \\in R^{W \\times H \\times (B+7)} \\rightarrow F_{DS} \\in R^{W \\times H \\times B}$\n$f_{SA}: \\tilde{H} \\in R^{W \\times H \\times B} \\rightarrow W_{SA} = f_{SA}(\\tilde{H}) \\in R^{1 \\times 1 \\times B}$,\n$\\tilde{H} : \\tilde{H} + \\tilde{H}_{es} = \\tilde{H} + W_{SA} \\cdot F_{DS}$\nThe features Q, obtained using classical derivative operators such as Roberts, Prewitt and Sobel, feed the subsequent feature extractor $f_{DS}$ together with the input pair ($\\tilde{H}$, P). The DS subnet, composed of a sequence of convolutional layers, provides output features extracted not only by the last layer but also by intermediate layers (hence deep-shallow), then reduced to B spectral channels. In parallel, the SA module computes the weights $w_{sa}$ used eventually in the output block to modulate on a per-band basis the detail injection strength. The whole network is trained end-to-end according to a traditional supervised scheme using a l\u2081-norm loss. Supervised DL-based methods have great potential, as testified by numerous success stories in closely related fields. Unfortunately, in the case of HS pansharpening there are many problems that prevent the desired results from being achieved:\n(a) The training is performed on synthetic data, obtained through resolution degradation processes, and there is no guarantee that a model trained on such data will work equally well on real full-resolution datasets. This is a general limit of any supervised pansharpening network.\n(b) The volume of data available for training, already limited by the lack of freely available HS datasets, is further reduced in the presence of resolution downgrading.\n(c) HS images are characterized by a varying number of spectral bands, due to differences among sensors and also to acquisition noise. However, none of the above models can work with a variable number of bands. Therefore, even when many images are available, they cannot be joined to form a single, training set and, in any case, these methods cannot easily generalize to new images.\nA partial response to these shortcomings is given by DHP-DARN and DIP-HyperKite, which use deep image priors to balance weakly trained networks. The more natural solution, however, is to use unsupervised training schemes [46], [83], [84], [47], which exploit only original full-resolution data to train the network with no need for GT. In this case, specific loss functions must be defined and carefully designed to drive the network towards the desired behavior. These losses comprise at least two terms, referred to as spectral ($L_x$) and spatial ($L_s$) consistency loss terms,\n$L = L_x + \\beta L_s = L_1(\\tilde{H}, \\bar{H}) + \\beta L_s (\\tilde{H}, P)$.(14)\nThe first term accounts for spectral fidelity and is usually computed by projecting $\\tilde{H}$ into the domain of $\\tilde{H}$ through a resolution downgrading and evaluating their distance by the l1 or 12 norms. The second term, responsible for the spatial quality of the fused image, is more difficult to define. The main options are: i) to synthesize a pseudo-PAN through a weighted average of the bands of $\\tilde{H}$ and compare it with P; ii) to compare each band of $\\tilde{H}$ individually with P and then summarize results. In both cases, the comparison should rely only on the high spatial frequency components of H.\n6) R-PNN: The unsupervised Rolling Pansharpening Neural Network (R-PNN) [37] addresses explicitly the issues (a)-(c) mentioned before. It relies heavily on the target-adaptive strategy, originally introduced for the MS case in both supervised [85] and unsupervised [47] settings, which consists of fine-tuning the pre-trained network on the target data. R-PNN uses target adaptation in a \u201crolling\" modality, that is, spectral bands are pansharpened one at a time, by fine-tuning the network for the current band starting from the weights optimized for the previous one. Formally, let $\\Phi_b^{(0)}$ and $\\Phi_{b+1}^{(0)} = \\Phi_b^{(\\infty)}$ be the initial and final (tuned) net parameters for band b, then are the initial parameters for band b + 1, to be adapted through a number of iterations proportional to the spectral distance between the two bands. Since adjacent bands are highly correlated, very few tuning iterations are sufficient to obtain accurate results, which limits computational complexity. In addition, the pansharpening network is a lightweight residual CNN, called Zoom-PNN (Z-PNN) [47], adapted to the single-band pansharpening case:\n$f : (\\tilde{H}^b, P) \\in R^{W \\times H \\times 2 \\times B} \\rightarrow \\hat{H}^b = f(\\hat{H}^b, P) \\in R^{W \\times H}$(15)\nThe unsupervised loss, used both for pre-training and tuning, comprises a spectral term $L_x$ based on the l\u2081-norm and a spatial term $L_s$ based on the local correlation coefficient [86].\n7) PCA-Z-PNN: A further adaptation of the Z-PNN method [47] to HS pansharpening is proposed in [50] based on PCA. The key observation is that the hundreds of bands comprising the HS image can be transformed into a new space where most of the energy is kept in a much smaller number of components. PCA is a natural candidate for such transformation and preliminary experiments on typical HS images show that it can compact about 99% of the energy in just 8 bands, that is the number of bands used in Z-PNN pansharpening.\nWith this premise, the method is easily explained. The input H is first whitened using the PCA transform. Then, the first 8 principal components $\\hat{H}^{PCA}$ are pansharpened using Z-PNN in the target adaptive modality. Finally, the pansharpened components $\\hat{H}^{PCA}$ are concatenated with the remaining low-energy components $H_{rem}$ and transformed back to the original space. In formulas, the process can be summarized as follows:\n$W = eig(\\tilde{H}\\tilde{H}^T)$,\n$\\tilde{H}_w = \\tilde{H}W$,\n$[\\hat{H}^{PCA}, H_{rem}] = \\tilde{H}_w$,\n$\\hat{H}^{PCA} = f_{Z-PNN}(\\hat{H}^{PCA}, P)$,\n$\\hat{H} = [\\hat{H}^{PCA}, H_{rem}]W^{-1}$,\nwhere, H is zero-meaned and reshaped as a WH\u00d7B matrix, W is the B\u00d7B matrix whose columns are the eigenvectors of $\\tilde{H}\\tilde{H}^T$, $f_{Z-PNN}$ is the pansharpening function, and $(\\cdot)^T$ and $(\\cdot)^{-1}$ denote transpose and inverse, respectively.\nIt is worth underlining that the PCA rotation can change from one image to another with no harm since Z-PNN runs in the target-adaptive modality and adapts to the new statistics. Along the same line, following experimental evidence, it has been found effective to pansharp separately the set of bands falling in the visible spectrum and those ranging from near to shortwave infrared, applying the above-described scheme twice.\""}, {"title": "III. QUALITY ASSESSMENT", "content": "The goal of pansharpening is to take two low-quality images, having reduced spatial (the HS component) or spectral resolution (the PAN), and synthesize a high-quality image at full resolution that cannot be observed in reality. Since the desired full-resolution image is not observable, there is no GT for objectively measuring the quality of the synthesized image. As a consequence, assessing the quality of a pansharpening algorithm is by no means trivial and remains essentially an open problem, extensively investigated in the past two decades.\nA popular measurement protocol was proposed in [79], where fusion products are required to satisfy two properties: consistency and synthesis. The consistency property states that the pansharpened image, once degraded at the lower spatial resolution of the original HS, should be as similar as possible to the latter. Similarly, a proper spectral degradation process of the pansharpened image should provide a single-band image as similar as possible to the original PAN. By this definition, it is clear that consistency can be easily measured. However, it represents only a check, and even perfect consistency does not ensure that the pansharpened image has the desired quality. The synthesis property is more stringent, as it states that the pansharpened image should be as similar as possible to the HS image that would be acquired by the HS sensor if it had the same (high) spatial resolution as the PAN sensor. Unfortunately, lacking this latter image, the synthesis property cannot be directly assessed and it has a mostly ideal nature.\nOne can circumvent the problem by resorting to the so-called Reduced Resolution (RR) assessment [79], [87]. The idea is to run the pansharpening algorithm using as input the spatially downgraded versions of PAN and HS. The output of the algorithm will have the same resolution as the original HS, which can therefore serve as GT. Thanks to the presence of a reference image, RR quality assessment is simple and accurate, it only requires a metric for measuring the similarity of multi-band images. However, there is no guarantee that a method that works well on low-resolution data will work equally well on high-resolution data, namely that a sort of scale-invariance holds. In addition, the degradation process required by this protocol may introduce biases and errors. From this point of view, the choice of suitable filters is crucial to ensure the consistency of the pansharpening process. In particular, before decimating the HS image, filters that match the HS sensor's MTFs should be used [65]. For the PAN image, instead, an ideal filter is preferred [87], to preserve the details that would have been seen with a direct RR acquisition.\nTo overcome these problems, pansharpening products can also be evaluated at full resolution, using quality indices specifically developed for this purpose according to the Quality with No Reference (QNR) paradigm [88], [89]. Of course, in the absence of a GT, such quantitative measures remain arbitrary to some extent. Typically, two complementaryquality indexes are considered to measure spatial and spectral consistency. These may follow opposite trends, with the paradox that the least spectral distortion is obtained when no spatial enhancement is introduced. Therefore, a suitable combination of them is necessary.\nIn the rest of this Section, we briefly review the quality indices considered in our toolbox, also summarized in Tab. III. However, it is worth emphasizing once again that quality assessment in pansharpening is an ill-posed problem, with no simple solution, and is still the subject of intense research. Both the reduced resolution and full resolution approaches have advantages and disadvantages. A good practice, consistently followed in the literature, is to use a wide range of indices and to always integrate the numerical results with a critical visual inspection of the images by experts."}, {"title": "A. RR assessment", "content": "Following a common practice for pansharpening assessment [2], [87], [4], three well-established reference-based indexes have been implemented and used to assess the similarity between the fused products and the original HS image playing the role of ground truth.\n1) ERGAS: The Erreur Relative Globale Adimensionnelle de Synth\u00e8se (ERGAS) [90] assesses the distance between two images by generalizing the concept of root mean square error (RMSE) to the multi-band case, taking care to normalize, band by band, the radiometric error to the average intensity on the reference image. In detail, it is defined as follows:\n$ERGAS = 100\\frac{1}{B} \\sum_{b=1}^{B} (\\frac{RMSE_b}{\\mu_b^{GT}})^2$,(16)\nwhere $RMSE_b$ is the b-th band RMSE between predicted and reference images and $\\mu_b^{GT}$ is the average intensity of the b-th band of the reference. ERGAS equals zero if and only if the predicted image is identical to the GT, otherwise it gives a positive error measurement.\n2) SAM: The Spectral Angle Mapper (SAM) [80] quantifies the spectral similarity between prediction and reference image by measuring the average angle (typically in degrees) between predicted and reference pixel spectral signatures, $\\hat{v} = [\\hat{v_1}, \\hat{v_2},..., \\hat{v_B}]$ and $v = [v_1, v_2, ..., v_B]$. Mathematically, we have:\n$SAM = E \\Big[arccos (\\frac{\\hat{v} \\cdot v}{||\\hat{v}||_2 ||v||_2}) \\Big]$,(17)\nwhere (,) indicates the dot product, $||\\cdot||_2$ is the $l_2$ norm, $arccos(\\cdot)$ the (positive-valued) arccosine function, and E[] the spatial average. The optimal value of SAM is zero, obtained for predictions that are pixel-wise proportional to the GT. Therefore, SAM is invariant to spectral signature scaling, $\\hat{v} = \\alpha v$.\n3) $Q2^n$: The $Q2^n$ index [91] generalizes the single-band Universal Image Quality Index (UIQI) [92] to the case of images with multiple bands. Originally introduced for four spectral bands, it was later expanded to handle images with $2^n$ bands [91]. Each pixel of a B-band image is represented as a hyper-complex number with one real part and B\u22121 imaginary parts. By calling z and $\\hat{z}$ the hyper-complex representations of a GT pixel and its prediction, respectively, $Q2^n$ can be expressed as:\n$Q^{2n} = E \\Big[ \\frac{\\sigma_{\\hat{z} z}}{\\sigma_{\\hat{z}}\\sigma_z} \\frac{2\\mu_{\\hat{z}}\\mu_z}{\\sigma_{\\hat{z}}^2 + \\sigma_z^2}  \\frac{2|\\sigma_{\\hat{z} z}|}{\\mu_{\\hat{z}}^2 + \\mu_z^2} \\Big]$,(18)\nwhere $\\sigma_{\\hat{z} z}$, $\\sigma^2$ and $\\mu$ indicate covariance, variance and mean for hyper-complex variables, computed on 32\u00d732 patches, $|| \\cdot ||$ provides the vector module and, again, E[.] indicates global spatial average. The first factor accounts for the correlation between z and $\\hat{z}$, while the other two measure contrast and intensity biases jointly on all bands. Unlike ERGAS and SAM, $Q^{2n} \\in [0,1]$ has to be maximized, and the optimum value (one) is achieved when the first- and second-order statistics of predicted and GT images are equal and, their covariance is maximized."}, {"title": "B. FR assessment", "content": "Full-resolution assessment typically involves the computation of two distinct and complementary quality indexes [88], [89], [93], [86], although a few solutions based on other paradigms have also been proposed [96], [97], [98]. Here, we use two well-established indexes for assessing spectral and spatial distortion and hence the consistency with HS and PAN, respectively. Moreover, we consider a single full-resolution score that summarizes the two previous indexes.\n1) Khan's spectral distortion index: $D_\\lambda$ [93] is defined as:\n$D_\\lambda = 1 - Q^{2n} (\\bar{\\hat{H}}, \\hat{H})$,(19)\nwhere $\\bar{\\hat{H}}$ indicates the MTF-based low-pass filtered and decimated version of the pansharpened image $\\hat{H}$ while H is the original HS. Since the index is defined as $1 - Q^{2n}$, the optimum value is 0 (hence the \"distortion\" name), and it is obtained when the downscaled version of H matches the original HS in terms of first-and second-order statistics in the multiband space.\n2) Spatial distortion index $D_s$: This index was proposed in [94] to assess the spatial consistency between the fused and PAN images. It is based on the assumption that the PAN can be expressed as a linear combination of individual spectral bands having the same spatial resolution as the PAN and covering collectively the same bandwidth. In our context, this amounts to assuming that P can be approximated with arbitrary precision by combining the bands of the pansharpened image $\\hat{H}$ through suitable weights {$w$}:\n$\\hat{I} = \\sum_{b=1}^{B} w_b \\hat{H}^b$,(20)\nIn practice, the weights are estimated by minimizing the mean square error between $\\hat{I}$ and P and the distortion is given by:\n$D_s = \\frac{\\sigma_{I-P}^2}{\\sigma_P^2}$,(21)\nwhere $\\sigma_{I-P}^2$ and $\\sigma_P^2$ are the variance of $\\hat{I}$ \u2013 P and P, respectively, obtained by global averages. Ds can be interpreted as the fraction of the total variance of P that cannot be explained by $\\hat{I}$. Therefore, the optimal value of Ds is zero, obtained if and only if $\\hat{I}$ = P.\n3) Regression-based QNR: The $D_\\lambda$ and Ds indexes measure two different aspects of the quality of pansharpened images. An algorithm that minimizes one index may cause the other to overgrow, leading to poor overall quality. Therefore, they should be taken into account jointly, looking for the best trade-off between spatial and spectral distortion. This is the objective of the Regression-based QNR (RQNR) index defined as follows (see [89] for additional deteils):\n$RQNR = (1 - D_\\lambda)^{\\alpha} (1 - D_s)^{\\beta}$.(22)\nLacking any specific needs, here we set $\\alpha = \\beta = 1$, as also done in [4]. RQNR reaches its optimal value, 1, when both individual distortion indices vanish, and decreases rapidly as even one of them increases."}, {"title": "IV. BENCHMARKING DATASETS", "content": "Before presenting the proposed benchmarking dataset, it is worth reviewing the most popular datasets used for HS pansharpening evaluation. Since this topic is particularly important for DL-based methods, we limit attention to the latter, and in particular to those considered in the toolbox. In any case, these datasets are quite representative of the overall literature (DL or not) on HS pansharpening. They are gathered in Tab. IV, with all the details on their characteristics and use.\nFor each dataset, the HS and PAN columns indicate whether these two components are real (as distributed by the provider) or synthetic. The latter are obtained through spatial resolution downgrading ($\\tilde{H}_l$, P\u2081) or by averaging the HS bands, limiting the scope to the visible spectrum ($\\tilde{H}$). In most cases, the use of synthetic components is unavoidable because the original image is not multi-resolution and only the HS component is given, with no PAN. Sometimes, instead, truly multiresolution data are available, like the EO-1/ALI combination or the PRISMA images. In these cases, two options are possible: i) downgrade the resolution of both HS and PAN anyway, in order to use supervised training and full-reference quality metrics; ii) keep the original data to preserve information, and use some forms of unsupervised training together with no-reference quality metrics.\nContinuing to scan the columns of Tab. IV, we first find the resolution ratio, R, which goes from 3 to 6, the most challenging case. Actually, this value is arbitrary for all datasets without a real PAN component. Then, the number of bands (used/all) points out the critical issue of image structure variability that occurs not only for different sensors but also for different scenes acquired with the same instrument. The spectral range covered is rather uniform, going from the visible (0.4 \u00b5m on) to the short-wave infrared (up to 2.4-2.5 \u00b5m), with the exception of ROSIS and MV.C VNIR, which cover up to the near-infrared and CAVE, limited to the visible spectrum. The physical size of the pixels, known as spatial resolution or ground sample distance (GSD), is also quite variable, ranging from 1.3 m up to 30 m, referring to the pansharpened image domain, and accounting also for resolution downgrade if any. Sometimes, the resolution is very high. This is due to the use of airborne instruments (e.g., AVIRIS) operated at a much lower altitude than satellite ones. This is a further source of variability that must be taken into account.\nThe last three columns concern the dataset size, measured again in the output domain and including possible resolution downgrade. After the image size, its partitioning in training and test subsets is specified, together with its organization in patches, all relevant pieces of information for DL-based methods. The first five methods, from HyperPNN to HyperDSNet, based on supervised learning, experiment almost exclusively on RR images, using (non-overlapping) fragments of the same image for both training and testing. This practice, due to the inability to manage a number of bands that varies from image to image, makes it hard to assess the generalization ability of the methods. However, almost all supervised approaches present at least a test experiment on FR real images. In [38] the model HSpeNet, trained on the RR version of the EO-1/ALI - Halls Creek dataset, is then tested on the FR version with an assessment limited to the visual inspection. A similar test, using the RR and FR versions of the PRISMA - Bologna dataset, is proposed in [62] for the HyperDSNet model. In this case, a 3More precisely training and validation, just training for brevity."}, {"title": "A. The proposed dataset", "content": "A major contribution of this work is to provide a dataset that meets all the above requirements. Considering the restrictive data-sharing policies widespread in the remote sensing field, we decided to use the PRISMA (PRecursore IperSpettrale della Missione Applicativa) images, shared on-demand by the Italian Space Agency (ASI) for research purposes only. In particular, a set of 16 images has been carefully selected and organized as summarized in Tab. V, including both the full-resolution and reduced-resolution versions. The first set of 12 images is reserved for training and validation purposes (part A of the table). All images are rather large, 3600\u00d73600 pixels at the target 5m resolution, which allows us to extract 10 or more large tiles from each of them, with size 1152\u00d71152 at full resolution and 192\u00d7192 after the 6\u00d7 decimation needed for RR training. As always happens with HS images, not all bands have sufficient quality for further processing. Therefore, out of the 239 bands available, we identified a subset of 159 bands that have good quality in all training and test images. A second set of 4 images, is kept for testing (part B) and form the strict-sense benchmarking dataset. Again, both full and reduced-resolution versions are given. In the former case, only 1200\u00d71200 crops are used, to limit complexity, while the complete 600\u00d7600 RR images are used in the latter case. The images have been acquired over a period of more than 3 years, all over the world, and display a wide variety of land covers."}, {"title": "B. Is this worth the effort?", "content": "We have already underlined what possible advantages HS pansharpening research can benefit from the use of a large and well-structured shared dataset. Here, we perform a very simple yet enlightening experiment in RR space to support our claims.\nAs noted above, in the absence of suitable datasets, a common practice for DL-based methods is to use a single image split into two non-overlapping parts for training and testing. To simulate this condition, we train our 5 supervised models, following the original experimental configurations, on a fraction (about 3/4) of the Udine image, listed in Tab.V(B), and test them on the remaining part, call these parts Udine/1 and Udine/2. Numerical results are reported in Tab. VI (only ERGAS for brevity). All five methods work quite well (on visual inspection) with very close ERGAS indicators, ranging from 1.7 to 2.1. Then, we train the same models on the proposed training set, see Tab. V(A), obtaining slightly worse results in all cases. As expected the statistical \u201calignment\" between training and testing data offers some advantages. In fact, even if Udine/1 and Udine/2 are disjoint, they come from 4Much worse for DHP-DARN, which seems to be an outlier in this case, i.e., extremely sensitive to the training set. the same acquisition and share the same sensing geometry, daylight and atmosphere conditions, land cover, etc.\nThings change completely when models are tested on a new image, Cagliari, again in Tab.V(B). The models trained on the proposed dataset exhibit a rather stable performance, with a slight average improvement concerning Udine/2. Neither Udine nor Cagliari contribute to the training set, so these results suggest that the latter can more easily pansharpened because of its simpler structure (see also Fig. 2). However, the models trained on Udine/1 suffer a catastrophic impairment, with ERGAS increasing by more than 80% on average. This is because their weights, over-fit to a small training set, are unable to generalize well to new data. In summary, this experiment, although limited in scope, confirms that the use of large, well-structured datasets improves generalization and prevents biases in experimental results.\""}, {"title": "V. EXPERIMENTAL RESULTS", "content": "This work aims to provide a comprehensive overview of the current HS pansharpening framework. This entails not only presenting and implementing classical and latest methods and evaluation metrics but also establishing guidelines for benchmarking and assessing these techniques, as well as highlighting open challenges and promising research directions in the field. With these goals in mind, it is crucial to set the basic characteristics of an ideal HS pansharpening algorithm which should:\n(a) generalize across datasets;\n(b) generalize across scales;\n(c) preserve spectral features while raising spatial resolution;\n(d) provide perceptually good visual results (for an ideal observer);\n(e) be computationally efficient, especially at test time,\nProperties (a)-(c) require reliable tools for numerical assessment, such as the SoTA indexes recalled in Section III, while (d) remains a subjective evaluation, but is no less important. Keeping the above properties in mind, we aim to point out the strengths and especially the limitations of current SoTA algorithms, which represent the main open challenges for future research. Note that we are not trying to establish some ranking among the techniques included in the benchmark, none of which is uniformly superior to the others. Instead, we would like to suggest a performance analysis methodology as a guideline for future research in the field.\nLet us now delve into the numerical results summarized in Tab. VII and Tab. VIII for RR and FR datasets, respectively. For easier understanding, we have highlighted the 5 best and 5 worst performing techniques in green and red, respectively, with the best one emphasized in bold. Comments to the results are gathered and ordered according to the above list of quality features."}, {"title": "A. Generalization across datasets", "content": "To assess a method's ability to generalize across datasets, we fix the scale and index, e.g., ERGAS, reduced resolution, and analyze score variations across images. At reduced resolution (see Tab. VII) virtuous examples are MTF-GLP-FS and MTF-GLP-HPM-R, which exhibit high average scores for all three indexes, consistently across all test images. The same also holds for some DL-based methods, e.g., Hyper-DSNet, R-PNN, and others with worse average scores. Examples of methods with generalization issues at reduced resolution are MTF-GLP-HPM or PCA-Z-PNN. The former gets very good scores on Cagliari followed by bad ones on Udine. The latter performs well on Tabasco, but much worse on Ford.\nMoving to the full resolution results in Tab. VIII, consistent behaviour is observed across the four test scenes for almost all methods on the spectral distortion index $D_\\lambda$. However, while all reduced resolution indices can be considered alternative measures of overall quality, the full resolution indices $D_\\lambda$ and $D_s$ each focus on a specific feature, i.e. spectral or spatial quality. Therefore, they are more likely to be uniform across images, following the characteristics of the method. Inconsistencies are more easily observed in the RQNR hybrid index since it results from the combination of $D_\\lambda$ and $D_s$. This is the case, for example, of BT-H or BDSD-PC, which show opposite spectral and spatial behaviours resulting in unstable RQNR rankings."}, {"title": "B. Generalization across scales", "content": "When moving from the reduced-resolution to the full-resolution domain, image statistics change considerably, even if the resolution downgrade is carried out carefully accounting for the sensor MTF characteristics. This is all the more true in our case, due to the rather large resolution ratio (six) of the PRISMA images. Overall, the cross-scale statistical mismatch is more important than the cross-scene mismatch and cross-scale generalization is not easily achieved.\nTo simplify the analysis of results, we consider only average scores, focusing, for each method, on the coherence between rankings registered at RR with those occurring at FR. The results of Tab. VII and Tab. VIII fully confirm that the methods generalize much better across different scenes than across scales. Some methods, like PRACS and PCA-Z-PNN, which struggle a bit at RR, seem to work well on FR data, while others, in particular some supervised DL-based method, suffer a performance hit when switching to FR tests. Examples of methods that tend to perform fairly well (but not top) at both scales are GSA, Hyper-DSNet and R-PNN. From these numbers, a picture emerges in which most methods seem to be optimized for just one scale, suffering on the other. In some cases, these results are the effect of an explicit design choice, as in the case of supervised DL methods, where training must be carried out in the RR domain. Given this apparent compromise, that is, working well only at one scale, one might wonder which one is preferable. On the one hand, the ultimate goal is to work well on FR data. On the other hand, at this scale, only consistency measures and not objective quality indicators can be observed. Eventually, the question remains open."}, {"title": "C. Joint spectral-spatial quality", "content": "In fusing the PAN and HS components we aim to keep both the spatial detail of the former and the spectral richness of the latter. However, these goals are somewhat conflicting, and 5We note that cross-scale invariance is somehow a proxy of cross-sensor invariance because, even if the number/position of the spectral bands and the resolution ratio are the same, here the statistics change significantly, like for images acquired with different instruments. sometimes one property is obtained by sacrificing the other. To analyze this trade-off we can refer to the FR consistency indices, Dx and Ds, which evaluate spatial and spectral quality separately. Indeed, the FR results of Tab. VIII highlight several situations where this undesirable behaviour emerges clearly, with methods that show high scores on $D_\\lambda$ but poor performance on Ds, or viceversa. The hybrid RQNR score, which weights the two indicators, can help evaluate the correct balance achieved by a method, with poor balance, signalled by a low RQNR, indicating ineffective fusion. Experimental results show that some recent DL-based methods, like DIP-HyperKite, R-PNN and PCA-Z-PNN, offer excellent RQNR scores. However, quite dated classical methods, like GSA and PRACS, are no less competitive. This confirms the observation already made in the introduction, that DL has yet to realize its full potential for HS pansharpening.\nAll this said we must raise a warning on our quality metrics. Spectral quality is relatively easy to assess, and Khan's index Dx is quite robust and reliable. On the contrary, the assessment of spatial quality remains an open problem as testified by several recent works on the topic [86], [89]. Here, we decided to use Ds in continuity with the prevalent literature and also with the recent HS pansharpening challenge [4]. However, like other metrics, Ds has well-known shortcomings and failure cases. A good practice is to look first at the spectral quality, which can be reliably assessed through D\u03bb, and then move on to spatial quality, complementing numerical indicators with the visual inspection of sample results, which might spot undetected anomalies and incoherencies."}, {"title": "D. Subjective visual quality", "content": "Since HS images comprise a very large number of spectral bands, examining them all individually would be unreasonable. On the other hand, these bands form a limited number of groups with high interband correlation. Therefore, as a reasonable compromise, we decided to show only six bands per sample image, grouped into two sets for false colour display, taken in the visible and NIR-SWIR ranges respectively. The \"visible\" bands correspond roughly to the red, green and blue wavelengths. The NIR-SWIR bands were selected such to have the minimum mutual correlation possible and be maximally representative of such a wide spectral range. Furthermore, to save space and show images at a reasonable resolution for visual inspection, only a small but significant crop per image was selected for display."}, {"title": "E. Computational Time", "content": "All experiments were carried out on the same server, an NVIDIA DGX Station A100, equipped with a 64-core AMD EPYC 7742 processor, 504 GB of DDR4 RAM, and four NVIDIA A100-SXM4 GPUs with 40 GB of GDDR5 memory each. The conventional CS, MRA, and MBO methods were run on the CPU, while the DL-based methods were run on a single GPU. The average run time for each method on the 600\u00d7600 RR test images is reported in Fig. 9. The run times for the 4 times larger (1200x1200) FR test images scale linearly with the size and hence are not reported. However, computational scalability depends on the available hardware, so we do not draw general conclusions about that. Some methods, both DL-based and not, have negligible run time, a quality to be considered in practical applications. Others are exceedingly slow, such as the CPU-based BDSD-PC and the GPU-based DHP-DARN. In general, all DL-based methods that try to adapt to the variability of input images pay a price in terms of computational complexity, DHP-DARN and DIP-HyperKite to estimate the deep image prior, and R-PNN and PCA-Z-PNN for the target-adaptation phase."}, {"title": "VI. CONCLUSIONS", "content": "In this work, we proposed a benchmarking framework for HS pansharpening, aiming to provide the research community with an easy-to-use tool to develop and test new methods at a faster pace. Following a careful analysis of the SoTA, we have selected a series of reference methods, representative of the main and most promising approaches in the field. All methods have been re-implemented and are ready for use. In parallel, we designed a large dataset of real multiresolution PRISMA images (available on demand to the research community) to train and test all methods in a uniform environment and according to well-defined and stable protocols. To establish a solid starting point for future research, we tested all selected methods using established performance metrics. The analysis of the results allowed us to highlight the main problems of existing solutions and therefore the most promising research avenues.\nIn a way, we have taken a snapshot of the state of HS pansharpening research right now. However, this is not intended to freeze things: on the contrary, it aims to encourage further research, allow easier comparison of results, and provide a context for sharing tools, as well as ideas. In our vision, this framework is destined to expand with the help of interested researchers who can contribute new methods and data.\nWhile claiming the merits of this initiative, we also see its limits. The set of methods selected is certainly not exhaustive and all choices are to a certain extent arbitrary. In particular, we could not reimplement some DL-based methods due to the lack of code and a sufficiently detailed description. The dataset itself, while representing a notable improvement over existing ones, is limited to a single sensor, thus preventing true generalization across sensors from being tested. Better performance parameters could probably be used to evaluate quality at full resolution. In summary, there is much room for contributions and improvements."}]}