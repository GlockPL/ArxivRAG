{"title": "Lightweight Dataset Pruning without Full Training via Example Difficulty and Prediction Uncertainty", "authors": ["Yeseul Cho", "Changmin Kang", "Baekrok Shin", "Chulhee Yun"], "abstract": "Recent advances in deep learning rely heavily on massive datasets, leading to substantial storage and training costs. Dataset pruning aims to alleviate this demand by discarding redundant examples. However, many existing methods require training a model with a full dataset over a large number of epochs before being able to prune the dataset, which ironically makes the pruning process more expensive than just training the model on the entire dataset. To overcome this limitation, we introduce a Difficulty and Uncertainty-Aware Lightweight (DUAL) score, which aims to identify important samples from the early training stage by considering both example difficulty and prediction uncertainty. To address a catastrophic accuracy drop at an extreme pruning, we further propose a ratio-adaptive sampling using Beta distribution. Experiments on various datasets and learning scenarios such as image classification with label noise and image corruption, and model architecture generalization demonstrate the superiority of our method over previous state-of-the-art (SOTA) approaches. Specifically, on ImageNet-1k, our method reduces the time cost for pruning to 66% compared to previous methods while achieving a SOTA, specifically 60% test accuracy at a 90% pruning ratio. On CIFAR datasets, the time cost is reduced to just 15% while maintaining SOTA performance.", "sections": [{"title": "Introduction", "content": "Advancements in deep learning have been significantly driven by large-scale datasets. However, recent studies have revealed a power-law relationship between the generalization capacity of deep neural networks and the size of their training data (Gordon et al., 2021; Hestness et al., 2017; Rosenfeld et al., 2019), meaning that the improvement of model performance becomes increasingly cost-inefficient as we scale up the dataset size.\nFortunately, Sorscher et al. (2022) demonstrate that the power-law scaling of error can be reduced to exponential scaling with Pareto optimal data pruning. The main goal of dataset pruning is to identify and retain the most informative samples while discarding redundant data points for training neural networks. This approach can alleviate storage and computational costs as well as training efficiency.\nHowever, many existing pruning methods require training a model with a full dataset over a number of epochs to measure the importance of each sample, which ironically makes the pruning process more expensive than just training the model once on the original large dataset. For instance, several score-based methods (Gordon et al., 2021; He et al., 2024; Pleiss et al., 2020; Toneva et al., 2018; Zhang et al., 2024) require training as they utilize the dynamics from the whole training process. Some geometry-based methods, (Xia et al., 2022; Yang et al., 2024) leverage features from the penultimate layer of the trained model, therefore training a model is also required. Hybrid methods (Maharana et al., 2023; Tan et al., 2025; Zheng et al., 2022), which address the difficulty and diversity of samples simultaneously, still hold the same limitation as they use existing score metrics. Having to compute the dot product of learned features to get the neighborhood information makes them even more expensive to utilize.\nTo address this issue, we introduce Difficulty and Uncertainty-Aware Lightweight (DUAL) score, a metric that measures the importance of samples in the early stage of training by considering both prediction uncertainty and the example difficulty. Additionally, at the high pruning ratio\u2014when the selected subset is scarce\u2014we propose pruning-ratio-adaptive Beta sampling, which intentionally includes easier samples with lower scores to achieve a better representation of the data distribution (Acharya et al., 2024; Sorscher et al., 2022; Zheng et al., 2022).\nExperiments conducted on CIFAR and ImageNet datasets under various learning scenarios verify the superiority of our method over previous SOTA methods. Specifically, on ImageNet-1k, our method reduces the time cost to 66% compared to previous methods while achieving a SOTA 60% test accuracy at the pruning ratio of 90%. On the CIFAR datasets, as illustrated in Figure 1, our method reduces the time cost to just 15% while maintaining SOTA performance. Especially, our method shows a notable performance when artificial noise is added."}, {"title": "Related Works", "content": "Data pruning aims to remove redundant examples, keeping the most informative subset of samples, namely the coreset. Research in this area can be broadly categorized into two groups: score-based and geometry-based methods. Score-based methods define metrics representing the difficulty or importance of data points to prioritize samples with high scores. Geometry-based methods, whereas, focus more on keeping a good representation of the true data distribution. Recent studies proposed hybrid methods that incorporate the example difficulty score with the diversity of coreset.\nScore-based. EL2N (Gordon et al., 2021) calculates L2 norms of the error vector as an approximation of the gradient norm. Entropy (Coleman et al., 2020) quantifies the information contained in the predicted probabilities at the end of training. However, the outcomes of such \u201csnapshot\u201d methods differ significantly from run to run, making it difficult to obtain a reliable score in a single run, as can be seen in Figure 8, Appendix B.\nMethods using training dynamics offer more reliability as they incorporate information throughout an entire run of training. Forgetting (Toneva et al., 2018) score counts the number of forgetting events, a correct prediction on a data point is flipped to a wrong prediction during training. AUM (Pleiss et al., 2020) accumulates the gap between the target probability and the second-highest prediction probability. Dyn-Unc (He et al., 2024), which strongly inspired our approach, prioritizes the uncertain samples rather than typical easy samples or hard ones during model training. The prediction uncertainty is measured by the variation of predictions in a sliding window, and the score averages the variation throughout the whole training process. TDDS (Zhang et al., 2024) averages differences of Kullback-Leibler divergence loss of non-target probabilities for T training epochs, where T is highly dependent on the pruning ratio. Taking the training dynamics into account proves useful for pruning because it allows one to differentiate informative but hard samples from ones with label noise (He et al., 2024). However, despite the stability and effectiveness of these methods, they fail to provide cost-effectiveness as it requires training the model on the entire dataset.\nGeometry-based. Geometry-based methods focus on reducing redundancy among selected samples to provide better representation. SSP (Sorscher et al., 2022) selects the samples most distant from k-means cluster centers, while Moderate (Xia et al., 2022) focuses on samples with scores near the median. However, these methods often compromise generalization performance as they underestimate the effectiveness of difficult examples.\nRecently, hybrid approaches have emerged that harmonize both difficulty and diversity. CCS (Zheng et al., 2022) partitions difficulty scores into bins and selects an equal number of samples from each bin to ensure a balanced representation. D\u00b2 (Maharana et al., 2023) employs a message-passing mechanism with a graph structure where nodes represent difficulty scores and edges encode neighboring representations, facilitating effective sample selection. BOSS (Acharya et al., 2024) introduces a Beta function for sampling based on difficulty scores, which resembles our pruning ratio-adaptive sampling; we discuss the key difference in Section 3.3. Our DUAL pruning is a score-based approach as it considers difficulty and uncertainty by the score metric. Additionally, diversity is introduced through our proposed Beta sampling, making it a hybrid approach."}, {"title": "Proposed Methods", "content": "To address this, we use the prediction mean again for sampling. We utilize the Beta probability density function (PDF) to define the selection probability of each sample. First, we assign each data point a corresponding PDF value based on its prediction mean and weight this probability using the DUAL score. The weighted probability with the DUAL score is then normalized so that the sum equals 1, and then used as the sampling probability. To be clear, sampling probability is for selecting samples, not for pruning. Therefore, for each pruning ratio r, we randomly select (1 - r) \u00b7 n samples without replacement, where sampling probabilities are given according to the prediction mean and DUAL score as described. The detailed algorithm for our proposed pruning method with Beta sampling is provided in Algorithm 1, Appendix C.\nWe design the Beta PDF to assign a sampling probability concerning a prediction mean as follows:\n\u03b2r = C \u00b7 (1 \u2013 \u03bcr) (1 \u2013 rcD) (5)\nar = C - \u03b2r,"}, {"title": "Preliminaries", "content": "Let D := {(x1, y1), ... , (xn, yn)} be a labeled dataset of n training samples, where x \u2208 X \u2282 Rd and y \u2208 Y := {1, ... , C} are the data point and the label, respectively. C is a positive integer and indicates the number of classes. For each labeled data point (x, y) \u2208 D, denote Pk (y | x) as the prediction probability of y given x, for the model trained with k epochs. Let S \u2286 D be the subset retained after pruning. Pruning ratio r is the ratio of the size of D \\ S to D, or r = 1 -"}, {"title": "Difficulty & Uncertainty-Aware Lightweight Score", "content": "Following the approach of Swayamdipta et al. (2020) and He et al. (2024), we analyze data points from ImageNet-1k based on the mean and standard deviation of predictions during training, as shown in Figure 3. We observe data points typically \"flow\" along the \"moon\" from bottom to top direction. Data points starting from the bottom-left region with a low prediction mean and low standard deviation move to the middle region with increased mean and standard deviation, and those starting in the middle region drift toward the upper-left region with a high prediction mean and smaller standard deviation. This phenomenon is closely aligned with existing observations that neural networks typically learn easy samples first, then treat harder samples later (Arpit et al., 2017; Bengio et al., 2009; Jiang et al., 2020; Shen et al., 2022). In other words, we see that the uncertainty of easy samples rises first, and then more difficult samples start to move and show an increased uncertainty score.\nFigure 3 further gives a justification for this intuition. In Figure 2a, samples with the highest Dyn-Unc scores calculated at epoch 60 move upward by the end of training at epoch 90. It means that if we measure the Dyn-Unc score at the early stage of training, it gives the highest scores to relatively easy samples rather than the most informative samples. It seems undesirable that it results in poor test accuracy on its coreset as shown in Figure 9 of Appendix B.\nTo capture the most useful samples that are likely to contribute significantly to dynamic uncertainty during the whole training process (of 90 epochs) at the earlier training stage (e.g. epoch of 60), we need to target the samples located near the bottom-right region of the moon-shaped distribution, as Figure 2b illustrates. Inspired by this observation, we propose a scoring metric that identifies such samples by taking the uncertainty of the predictions and the prediction probability into consideration.\nHere, we propose the Difficulty and Uncertainty-Aware Lightweight (DUAL) score, a measure that unites example difficulty and prediction uncertainty. We define the DUAL score of a data point (x,y) at k \u2208 [T \u2013 J + 1] as\nDUALk (x, y) := (1 - Pk) \u2211;= [Pk+j (y|x) \u2013 Pk]\u00b2 (3)\nwhere Pk := Pk+j (y)) is the average prediction of the model over the window [k, k + J 1]. Note that DUALk is the product of two terms: (a) 1 \u2013 Pk quantifies the example difficulty averaged over the window; (b) is the standard deviation of the prediction probability over the same window, estimating the prediction uncertainty.\nFinally, the DUAL score of (x, y) is defined as the mean of DUALk scores over all windows:\nDUAL(x, y) = \u2211 DUALk (x,y) (4)\nThe DUAL score reflects training dynamics by leveraging prediction probability across several epochs It provides a reliable estimation to identify the most uncertain examples.\nA theoretical analysis of a toy example further verifies the intuition above. Consider a linearly separable binary classification task {(xi \u2208 R\", yi \u2208 {\u00b11})}^N_{i=1}, where N = 2 with ||x1|| \u226a (x1, x2) < ||x2||. Without loss of generality, we set y1 = y2 = +1. A linear classifier, f(x; w) = w\u1d40x, is employed as the model in our analysis. The parameter w is initialized at zero and updated by gradient descent. Soudry et al. (2018) prove that the parameter of linear classifiers diverges to infinity, but directionally converges to the L2 maximum margin separator. If a valid pruning method encounters this task, then it should retain the point closer to the decision boundary, which is x1 in our case, and prune x2.\nDue to its large norm, x2 exhibits higher score values in the early training stage for both uncertainty and DUAL scores. It takes some time for the model to predict x\u2081 with high confidence, which increases its uncertainty level and prediction mean, as well as for scores of x\u2081 to become larger than x2 as training proceeds. In Theorem 3.1, we show through a rigorous analysis that the moment of such a flip in order happens strictly earlier for DUAL than uncertainty.\""}, {"title": "Theorem 3.1 (Informal)", "content": "Define \u03c3(z) := (1 + e\u207b\u1dbb)\u207b\u00b9. Let V_t;J^(i) be the variance and \u03bc_t;J^(i) be the mean of \u03c3(f(x\u1d62; w_t)) within a window from time t to t + J. Denote T\u1d65 and T\u1d65\u2098 as the first time step when V_t;J^(1) > V_t;J^(2) and V^(1) (1 \u2013 \u03bc_t;J^(1)) > V^(2) (1-\u03bc_t;J^(2)) occurs, respectively. If the learning rate is small enough, then T_v\u2098 < T\u1d65."}, {"title": "Pruning Ratio-Adaptive Sampling", "content": "Since the distribution of difficulty scores is dense in high-score samples, selecting only the highest-score samples may result in a biased model (Choi et al., 2024; Maharana et al., 2023; Zhou et al., 2023). To address this, we design a sampling method to determine the subset S \u2282 D, rather than simply pruning the samples with the lowest score. We introduce a Beta distribution that varies with the pruning ratio. The primary objective of this method is to ensure that the selected subsets gradually include more easy samples into the coreset as the pruning ratio increases.\nHowever, the concepts of \u201ceasy\u201d and \u201chard\u201d cannot be distinguished solely based on uncertainty, or DUAL score."}, {"title": "Experiments", "content": "We assessed the performance of our proposed method in three key scenarios: image classification, image classification with noisy labels and corrupted images. In addition, we validate cross-architecture generalization on three-layer CNN, VGG-16 (Simonyan and Zisserman, 2015), ResNet-18 and ResNet-50 (He et al., 2015).\nHyperparameters. For training CIFAR-10 and CIFAR-100, we train ResNet-18 for 200 epochs with a batch size of 128. SGD optimizer with momentum of 0.9 and weight decay of 0.0005 is used. The learning rate is initialized as 0.1 and decays with the cosine annealing scheduler. As Zhang et al. (2024) show that smaller batch size boosts performance at high pruning rates, we also halved the batch size for 80% pruning, and for 90% we reduced it to one-fourth. For ImageNet-1k, ResNet-34 is trained for 90 epochs with a batch size of 256 across all pruning ratios. An SGD optimizer with a momentum of 0.9, a weight decay of 0.0001, and an initial learning rate of 0.1 is used, combined with a cosine annealing scheduler."}, {"title": "Experimental Settings", "content": "Here we clarify the technical details of our work. For training the model on the full dataset and the selected subset, all parameters are used identically except for batch sizes. For CIFAR-10/100, we train ResNet-18 for 200 epochs with a batch size of 128, for each pruning ratio {30%, 50%, 70%, 80%, 90%} we use different batch sizes with {128, 128, 128, 64, 32}. We set the initial learning rate as 0.1, the optimizer as SGD with momentum 0.9, and the scheduler as cosine annealing scheduler with weight decay 0.0005. For training ImageNet, we use ResNet-34 as the network architecture. For all coresets with different pruning rates, we train models for 300,000 iterations with a 256 batch size. We use the SGD optimizer with 0.9 momentum and 0.0001 weight decay, using a 0.1 initial learning rate. The cosine annealing learning rate scheduler was used for training. For a fair comparison, we used the same parameters across all pruning methods, including ours. All experiments were conducted using an NVIDIA A6000 GPU. We also attach the implementation in the supplementary material.\nFor calculating the DUAL score, we need three parameters T, J, and cD, each means score computation epoch, the length of the sliding window, and hyperparameter regarding the training dataset. We fix J as 10 for all experiments. We use (T, J, cD) for each dataset as follows. For CIFAR-10, we use (30, 10, 5.5), for CIFAR-100, (30, 10, 4), and for ImageNet-1k, (60, 10, 11). We first roughly assign the term cD based on the size of the initial dataset and by considering the relative difficulty of each, we set cD for CIFAR-100 smaller than that of CIFAR-10. For the ImageNet-1k dataset, which contains 1,281,167 images, the size of the initial dataset is large enough that we do not need to set cD to a small value to intentionally sample easier samples. Also, note that we fix the value of C of Beta distribution at 15 across all experiments. A more detailed distribution, along with visualization, can be found in Appendix C.\nExperiments with label noise and image corruption on CIFAR-100 are conducted under the same settings as described above, except for the hyperparameters for DUAL pruning. For label noise experiments, we set T to 50 and J to 10 across all label noise ratios. For cD, we set it to 6 for 20% and 30% noise, 8 for 40% noise. For image corruption experiments, we set T to 30, J to 10, and cD to 6 across all image corruption ratios.\nFor the Tiny-ImageNet case, we train ResNet-34 for 90 epochs with a batch size of 256 across all pruning ratios, using a weight decay of 0.0001. The initial learning rate is set to 0.1 with the SGD optimizer, where the momentum is set to 0.9, combined with a cosine annealing learning rate scheduler. For the hyperparameters used in DUAL pruning, we set T to 60, J to 10, and cD to 6 for the label noise experiments. For the image corruption experiments, we set T to 60, J to 10, and cD to 2. We follow the ImageNet-1k hyperparameters to implement the baselines."}, {"title": "Image Classification with Label Noise", "content": "We evaluated the robustness of our DUAL pruning method against label noise. We introduced symmetric label noise by replacing the original labels with labels from other classes randomly. For example, if we apply 20% label noise to a dataset with 100 classes, 20% of the data points are randomly selected, and each label is randomly reassigned to another label with a probability of 1/99 for the selected data points.\nEven under 30% and 40% random label noise, our method achieves the best performance and accurately identifies the noisy labels, as can be seen in Figure 11. By examining the proportion of noise removed, we can see that our method operates close to optimal.\nWe evaluated the performance of our proposed method across a wide range of pruning levels, from 10% to 90%, and compared the final accuracy with that of baseline methods. As shown in the Table 5-8, our method consistently outperforms the competition with a substantial margin in most cases. For a comprehensive analysis of performance under noisy conditions, please refer to Tables 5 to 7 for CIFAR-100, which show results for 20%, 30%, and 40% noise, respectively. Additionally, the results for 20% label noise in Tiny-ImageNet are shown in Table 8."}, {"title": "Cross-Architecture Generalization", "content": "We also evaluate the ability to transfer scores across various model architectures. To be specific, if we can get high-quality example scores for pruning by using a simpler architecture than one for the training, our DUAL pruning would become even more efficient in time and computational cost. Therefore, we focus on the cross-architecture generalization from relatively small networks to larger ones with three-layer CNN, VGG-16, ResNet-18, and ResNet-50. Competitors are selected from each categorized group of the pruning approach: EL2N from difficulty-based, Dyn-Unc from uncertainty-based, and CCS from the geometry-based group.\nFor instance, we get training dynamics from the ResNet-18 and then calculate the example scores. Then, we prune samples using scores calculated from ResNet-18, and train selected subsets on ResNet-50. The result with ResNet-18 and ResNet-50 is described in Table 3. Surprisingly, the coreset shows competitive performance to the baseline, where the baseline refers to the test accuracy after training a coreset constructed based on the score calculated from ResNet-50. For all pruning cases, we observe that our methods reveal the highest performances. Specifically, when we prune 70% and 90% of the original dataset, we find that all other methods fail, showing worse test accuracies than random pruning.\nWe also test the cross-architecture generalization performance with three-layer CNN, VGG-16, and ResNet-18 in Appendix B.3. Even for a simple model like three-layer CNN, we see our methods show consistent performance, as can be seen in Table 13 in Appendix B.3. This observation gives rise to an opportunity to develop some small proxy networks to get example difficulty with less computational cost. Transfer across models with similar capacities, e.g. from VGG-16 to ResNet-18 and vice versa, also supports the verification of cross-architecture compatibility."}, {"title": "Ablation Studies", "content": "Here, we investigate the robustness of our hyperparameters, T, J, and CD. We fix Jacross all experiments, as it has minimal impact on selection, indicating its robustness (Fig 10, Appendix B). In Figure 7, we assess the robustness of T by varying it from 20 to 200 on CIFAR-100. We find that while T remains highly robust in earlier epochs, increasing T degrades generalization performance. This is expected, as larger T overemphasizes difficult samples due to our difficulty-aware selection."}, {"title": "Effectiveness of Beta Sampling", "content": "We study the impact of our Beta sampling on existing score metrics. We apply our Beta sampling strategy to forgetting, EL2N, and Dyn-Unc scores of CIFAR10 and 100. By comparing Beta sampling with the vanilla threshold pruning using scores, we observe that prior score-based methods become competitive, outperforming random pruning when Beta sampling is adjusted."}, {"title": "Detailed Explanation about Our Method", "content": "In this section, we provide details on the implementation used across all experiments for reproducibility. Appendix C.2 presents the full algorithm for our pruning method, DUAL, along with the Beta sampling strategy. Additionally, in a later subsection, we visualize the selected data using Beta sampling.\nRecall that we define our sampling distribution Beta(\u03b1r, \u03b2r) as follows:\n\u03b2r = C (1 \u2013 \u03bcr) (1 \u2013 rcD) (6)\nar = C - \u03b2r,\nwhere \u03bcr \u2208 [0,1] is the probability mean of the highest DUAL score training sample. To ensure stability, we compute this as the average probability mean of the 10 highest DUAL score training samples. Additionally, as mentioned earlier, we set the value of C to 15 across all experiments. For technical details, we add 1 to \u03b1r to further ensure that the PDF remains stationary at low pruning ratios.\nWe illustrate the Beta PDF, as defined above, in Figure 16 for different values of cD. In both subplots, we set \u03bcr as 0.25. The left subplot shows the PDF with cD = 5.5, which corresponds to the value used in CIFAR-10 experiments, while the right subplot visualizes the PDF where cD = 4, corresponding to CIFAR-100."}, {"title": "Visualization of Selected Data with Beta Sampling", "content": "Here we illustrate the sampling probability of being selected into coreset, selected samples, and pruned samples in each figure when using the DUAL score combined with Beta sampling. As the pruning ratio increases, we focus on including easier samples."}, {"title": "Algorithm of Proposed Pruning Method", "content": "The detailed algorithms for DUAL pruning and Beta sampling are as follows:\ninput Training dataset D, pruning ratio r, dataset simplicity cD, training epoch T, window length J.\noutput Subset S \u2282 D such that |S| = (1 - r)|D| for (xi, yi) \u2208 D do for k = 1, ... , T \u2013 J + 1 do Pk (Xi, Yi)\u2190 \u2211=Pk+j (Yi | Xi)Uk (Xi, Yi)\u2190  [Pk+j (Yi | Xi) \u2013 Pk (Xi, Yi)]\u00b2 end for  DUAL(xi, Yi) \u2190  DUALk(Xi, Yi) end for if B-sampling then for (xi, Yi) \u2208 D do P(xi, Yi) \u2190 \u2211=1 Pk (Yi | Xi) (P(xi, Yi)) \u2190 PDF value of Beta(ar, \u03b2r) from Equation (5) (xi) \u2190 (P(Xi, Yi)) \u00d7 DUAL(Xi, Yi) end for  (xi)\u2190  (xi) S\u2190 Sample (1 - r)|D| data points according to (xi) else S\u2190 Sample (1 - r)|D| data points with the largest DUAL (xi, Yi) score end if"}, {"title": "Theoretical Results", "content": "Throughout this section, we will rigorously prove Theorem 3.1, providing the intuition that Dyn-Unc takes longer than our method to select informative samples."}, {"title": "Proof of Theorem 3.1", "content": "Assume that the input and output (or label) space are X = R\u207f and y = {\u00b11}, respectively. Let the model f : X \u2192 R be of the form f(x; w) = w\u1d40x parameterized by w \u2208 R\u207f with zero-initialization. Let the loss be the exponential loss, l(z) = e\u207b\u1dbb. Exponential loss is reported to induce implicit bias similar to logistic loss in binary classification tasks using linearly separable datasets (Gunasekar et al., 2018; Soudry et al., 2018).\nThe task of the model is to learn a binary classification. The dataset D consists only two points, i.e. D = {(x1, y1), (x2, y2)}, where without loss of generality y\u1d62 = 1 for i = 1,2. The model learns from D with the gradient descent. The update rule, equipped with a learning rate \u03b7 > 0, is:\nw\u2080 = 0\nw\u209c\u208a\u2081 = w\u209c \u2013 \u03b7\u2207_\u03c9 \u03a3 {l (f (x\u1d62; w\u209c))}_(i=1)^2\n= w\u209c + \u03b7 (e^(-y\u2081(1)) x\u2081 + e^(-y\u2082(1)) x\u2082)\n=: w\u209c + \u03b7 (e^(-y\u2081(1)) x\u2081 + e^(-y\u2082(1)) x\u2082)\nFor brevity, denote the model output of the i-th data point at the t-th epoch as y\u209c\u207d\u2071\u207e := f(x\u1d62; w\u209c). The update rule for the parameter is simplified as:\nw\u209c\u208a\u2081 = w\u209c + \u03b7 (e^(-y\u2081(1)) x\u2081 + e^(-y\u2082(1)) x\u2082) (7)\nWe also derive the update rule of model output for each instance:\ny_1^(t+1) = w_(t+1)^T x_1 = [ w_t + \u03b7 (e^(-y\u2081(1)) x\u2081 + e^(-y\u2082(1)) x\u2082)^T]x_1 = y_1^(t) + \u03b7 e^(-y\u2082(1)) \u2016x\u2081\u2016\u00b2 + \u03b7 e^(-y\u2081(1)) (x\u2081, x\u2082), (8)\ny_2^(t+1) = y_2^(t) + \u03b7 e^(-y\u2081(1)) \u2016x\u2082\u2016\u00b2 + \u03b7 e^(-y\u2082(1)) (x\u2081, x\u2082),"}, {"title": "Assumption D.1.", "content": "||x\u2082|| > 1, 4||x\u2081||\u00b2 < 2(x\u2081, x\u2082) < ||x\u2082||\u00b2. Moreover, (x\u2081, x\u2082) < ||x\u2081|| ||x\u2082||.\nUnder these assumptions, as (x\u2081, x\u2082) > 0, D is linearly separable. Also, notice that x\u2081 and x\u2082 are not parallel. Our definition of a linearly separable dataset is in accordance with Soudry et al. (2018). A dataset D is linearly separable if there exists w* such that (x\u1d62, w*) > 0, Vi."}, {"title": "Theorem D.2.", "content": "Let V_(t;J)^((i)) be the variance and \u0304\u03bc_(t;J)^((i)) be the mean of \u03c3(y_t^((i))) within a window from time t to t + J \u2013 1.\nDenote Tv and Tvm as the first time when V_(t;J)^((1)) > V_(t;J)^((2)) and V_(t;J)^((1)) (1 \u2013 \u0304\u03bc_(t;J)^((1))) > V_(t;J)^((2)) (1 \u2013 \u0304\u03bc_(t;J)^((2))) occurs, respectively. Under Assumption D.1, if \u03b7 is sufficiently small then Tvm < Tv."}, {"title": "Lemma D.3.", "content": "\u0394y\u209c := y\u209c^(2) - y\u209c^(1) is a non-negative, strictly increasing sequence. Also, lim_(t\u2192\u221e)\u0394y\u209c = \u221e."}, {"title": "Evolution of Weight", "content": "Define g(x, y) := e\u02e3 / (1+e^(y+x)), where x > 0 for y \u2208 R. The partial derivatives satisfy: \u2207\u2093g := (e^(-y)-e^(-x))/ (1+e^y)^2 (e^(y+x+1)) > 0, and \u2207\u1d67g := (e^(-y)-e^(-x))/ (1+e^y)^3 (e^(y+x+1)), \u2200 y if x > 0."}]}