{"title": "Leaf Angle Estimation using Mask R-CNN and LETR Vision Transformer", "authors": ["Venkat Margapuri", "Prapti Thapaliya", "Trevor Rife"], "abstract": "Abstract-Modern day studies show a high degree of cor-\nrelation between high yielding crop varieties and plants with\nupright leaf angles. It is observed that plants with upright\nleaf angles intercept more light than those without upright leaf\nangles, leading to a higher rate of photosynthesis. Plant scientists\nand breeders benefit from tools that can directly measure plant\nparameters in the field; i.e., on-site phenotyping. The estimation\nof leaf angles by manual means in a field setting is tedious and\ncumbersome. We mitigate the tedium using a combination of\nthe Mask R-CNN instance segmentation neural network, and\nLine Segment Transformer (LETR), a vision transformer. The\nproposed Computer Vision (CV) pipeline is applied on two\nimage datasets, Summer_2015-Ames_ULA and Summer_2015-\nAmes_MLA, with a combined total of 1,827 plant images\ncollected in the field using FieldBook, an Android application\naimed at on-site phenotyping. The leaf angles estimated by the\nproposed pipeline on the image datasets are compared to two\nindependent manual measurements using ImageJ, a Java-based\nimage processing program developed at the National Institutes\nof Health and the Laboratory for Optical and Computational\nInstrumentation. The results, when compared for similarity using\nthe Cosine Similarity measure, exhibit ~0.98 similarity scores on\nboth independent measurements of Summer_2015-Ames_ULA\nand Summer_2015-Ames_MLA image datasets, demonstrating\nthe feasibility of the proposed pipeline for on-site measurement\nof leaf angles.", "sections": [{"title": "I. INTRODUCTION", "content": "The yields and planting densities of maize in the United\nStates have increased concurrently during the past 50 years\n[1]. A comparative analysis of U.S. commercial maize hy-\nbrids released since the 1960s revealed that by selecting\nhigh yielding hybrids under high planting densities, breeders\nindirectly selected hybrids with upright leaf angles (LAs) [1].\nThe discovery was that upright LAs combined with higher\nplanting densities improve light distribution within the canopy.\nFor instance, modern hybrids intercept 14% more light than\nolder hybrids [1]. Leaf Angle Distribution (LAD) is a key\nparameter that describes the structure of horizontally homo-\ngeneous vegetation canopies. It is defined as the probability\nof a leaf element of unit size to have its normal within a\nspecified unit solid angle [2]. LAD affects the manner in which\nincident photosynthetically active radiation is distributed on\nplant leaves, thus directly affecting plant productivity. The\ntraditional technique to measure LAD involves the use of\nmechanical inclinometer, a precision instrument that measures\nthe angle of slope of an object with respect to its gravity\nby creating an artificial horizon. The use of the mechanical\ninclinometer makes the process of determining LAD laborious\nand time-consuming. Other techniques such as 3-D digitizing\nof individual plant elements using specialized instrumentation\n[3] and laser scanning [4] are available. However, these tech-\nniques are resource-demanding. A feasible, and inexpensive\nalternative is the use of digital imagery to estimate LA since\nimage analysis preserves the state of the canopy while provid-\ning enough insight into the morphometry. Image processing\ntools such as ImageJ [14] and Adobe Photoshop have built-\nin angle estimation tools that aid in the estimation of LA.\nThe pitfall is that the operation of the tools is manual which\nmeans that the estimations may be subject to user bias and\nusers may spend extended amounts of time to estimate LAs,\nin case of large datasets. In order to overcome the pitfalls of the\nmanual setup, this paper proposes an automated technique for\nLA estimation using Mask R-CNN [8] instance segmentation\nneural network, and Line Segment Transformer (LETR) [15],\na vision transformer."}, {"title": "II. RELATED WORK", "content": "Dzievit [1] conducted genetic mapping and a meta-analysis\nto dissect genetic factors controlling LA variation on maize.\nGenetic mapping populations were developed using inbred\nlines B73 (Iowa Still Stalk Synthetic), PHW30 (lodent, expired\nplant variety protection inbred), and Mo17 (Non-Stiff Stalk)\nthat have distinct LA architectures. The leaf angles were\nestimated using the ImageJ image processing tool.\nHerbert [5] described a technique for the estimation of leaf\nangles using stereo-photogrammetry. The technique permitted\naccurate measurement of leaf angle and position from several\nmeters away and had sufficient resolution to permit the anal-\nysis of complex phenomena such as the effect of leaf shape\nupon interception of light and photosynthesis.\nSinoquet et al. [3] proposed a method to measure light\ninterception by vegetation canopies using a 3-D digitizer and\nimage processing software. The 3-D digitizer allowed for\nsimultaneous acquisition of the spatial coordinates of leaf\nlocations and orientations. The software for image synthesis\nalso had the ability to make virtual photographs of the real\ncanopy. The information on light interception was derived\nfrom virtual images by using the simple features of image\nanalysis software.\nHasoi et al. [4] estimated the LAD of wheat canopy at\ndifferent growth stages such as tillering, stem elongation, flow-\nering, and ripening stages by using a high-resolution portable\nscanning lidar. The canopy was scanned three-dimensionally\nby optimally inclined laser beams emitted from several mea-\nsuring points surrounding the canopy and 3-D point cloud\nimages in each stage were obtained. After co-registration of\nlidar images between different measurement positions, leaves\nwere extracted from the images and each leaf was divided into\nsmall pieces along the leaf-length direction. Each of the pieces\nwas approximated as a plane, to which normal (perpendicular)\nwas estimated. The distribution of the leaf inclination angles\nwas derived from the angles of the normal with respect to the\nzenith.\nRyu et al. [6] examined the feasibility of seven techniques\nsuch as litterfall, allometry, LAI-2000, TRAC, digital hemi-\nspheric photography, digital cover photography, and traversing\nradiometer system to determine leaf area index across a 9-ha\ndomain in an oak-savanna ecosystem in California, USA. It\nwas shown that the combination of digital cover photography\nand LAI-2000 could provide spatially representative leaf area\nindex, gap fraction and element clumping index."}, {"title": "III. MATERIALS AND METHODS", "content": "Researchers at Iowa State University captured the images\nof several plants in a field using the Fieldbook android\napplication [24]. As for the current work, two datasets\nnamely, Summer_2015-Ames_ULA and Summer_2015-\nAmes_MLA wherein the former consists of 872 images and\nlatter, 955 images, are used as the experimental datasets.\nEach of the images in either dataset shows the intersection of\nthe leaf and stem. A sample image is as shown in Fig. 1.\nThe proposed leaf angle estimation procedure contains two\nbroad steps:\n1. Extraction of region of interest (ROI)\n2. Estimation of Leaf Angle\nExtraction of Region of Interest (ROI)\nThe first step in the extraction of ROI is the detection\nof leaf and stem within the image. The images captured in\nthe field contain numerous plants that look similar. Hence,\nit is important to determine the plant of interest and per-\nform a curated extraction of it for the estimation of LA.\nConventionally, a feasible technique for foreground extraction\nis OpenCV's GrabCut [20] algorithm. However, the results\nobtained from the application of the GrabCut algorithm on\neach of the datasets are not satisfactory. While the algorithm\ncaptures the foreground, it also captures part of the plants\nin the background. The captured plants in the background\nare essentially noise and detrimental to LA estimation. Fig. 2\nshows the botched foreground extraction performed on a plant\nimage from the Summer_2015-Ames_MLA dataset.\nIn order to better perform foreground extraction, the CNN\nmodel of Mask R-CNN [8] [9] is employed to extract fore-\nground and identify the region of interest appropriately. Mask\nR-CNN is a deep neural network aimed to solve the prob-\nlem of instance segmentation in computer vision. Instance\nSegmentation [10] is the task of precisely identifying the\npixels of each of the objects in the image. It is perhaps the\nhardest of the computer vision (CV) tasks of classification\n[11], semantic segmentation [12], object detection [13], and\ninstance segmentation.\n1) 175 images belonging to each of the Summer_2015-\nAmes_MLA and Summer _2015-Ames_ULA datasets\nare manually annotated using makesense.ai tool. The\ntool is free to use and generates annotations in the\nCOCO format that Mask R-CNN is able to consume. A\npolygonal annotation is drawn where the stem and leaf\nmeet each other. Fig. 3 shows a polygonal annotation\ndrawn using makesense.ai on an image with leaf and\nstem.\n2) The annotated images are trained on the Mask R-CNN\nnetwork for a total of 20 epochs. The loss of the network\nafter 20 annotations is 0.034. However, the loss is not\nsignificant since Mask R-CNN is used to extract the\nportion of the image where the leaf and stem meet,\nbut not to extract masks that are precise in shape and\nsize. All the annotated images are used for training\nonly. The dataset is not split into training, validation,\nand test datasets since the goal is not to evaluate the\nperformance of Mask R-CNN, but to extract regions of\ninterest in the image that are later processed to estimate\nthe angle between leaf and stem. Upon training, the\ntrained weights are used to extract the masks of each\nof the images in the Summer_2015-Ames_MLA and\nSummer_2015-Ames_ULA datasets. Fig. 4 shows the\nmask extracted by Mask R-CNN for the plant image\nshown in Fig. 3.\n3) The predicted mask indicates the region of interest that\ncorresponds to the leaf and stem. In order to extract\nthe region from the original image, the 'bitwise AND'\noperation is applied on the mask predicted by Mask R-\nCNN and original image. The resulting image gives the\nregion of interest in the original image. Fig. 5 shows\nthe region of interest extracted from the original image.\nPlease note that the image is zoomed in for clarity."}, {"title": "B. Estimation of Leaf Angle", "content": "The basis for the estimation of leaf angle is provided by [1].\nThe work describes the above-ear and below-ear leaf angles\nthat are measured by plant scientists. Fig. 6, an excerpt from\n[1], shows the exact position at which the angle between leaf\nand stem is to be measured. While leaves are typically long\nand tilt in different directions owing to environmental factors,\nthe angle between leaf and stem is measured exactly where\nthe leaf and stem conjoin.\nThe estimation of leaf angle is performed using Line\nSegment Transformer (LETR), a vision transformer [15].\nDerived from transformers in the world of Natural Language\nProcessing (NLP), a Vision Transformer (ViT) is a type of\nneural network that is used to perform CV tasks, such as\nimage classification, and object detection. The basic principle\nof the transformer used for NLP is the representation of words\nas sequences. Drawing inspiration from it, ViT works by\nrepresenting an image as a sequence of patches. A patch is\na small rectangular size of the image, typically 16x16 pixels.\nEach patch that is extracted from the image is represented\nas a feature vector. The feature vectors are extracted using\na convolutional neural network (CNN), such as ResNet-50 or\nVGG-16. The extracted feature vectors are input to an encoder\nnetwork. The encoder network is a stack of self-attention\n[16] layers, where self-attention is a mechanism that allows\nthe model to identify and establish long-range dependencies\nbetween the patches of images extracted from the original\nimage. The encoder network outputs a sequence of vectors\nthat may be fed into a multi-layer perceptron (MLP) for CV\ntasks such as classification, and detection.\nThe LETR vision transformer is an adaptation of the De-\ntection Transformer (DETR). The DETR is an object detec-\ntion model developed by Facebook (Meta) that leverages the\ntransformer architecture. The ResNet architecture is typically\npreferred as the feature extractor within DETR to extract\nfeatures from patches of an image. The DETR model follows\nthe encoder-decoder architecture similar to the conventional\ntransformer, with the use of multi head self attention being the\nkey difference between the two. Multi head self attention is the\nidea where the Attention module comprises N (multiple) at-\ntention heads and each attention head repeats its computations\nmultiple times in parallel. All the attention computations are\ncombined to yield a final attention score. Bipartite Matching\ncomputed using the Hungarian Algorithm [17] is used to\ncompute the difference between the ground truth bounding\nboxes and predicted bounding boxes of each of the objects in\nthe image. The LETR vision transformer follows the DETR\nmodel, but is designed specifically for the identification of line\nsegments. The LETR model improves upon the DETR model's\nability to detect geometric structures by augmenting the loss\nterm.\n1) Xu et al. trained the LETR model on two datasets\nnamely, Wireframe [18] and YorkUrban [19] using the\nfeature extractors of ResNet-101 and ResNet-50 re-\nspectively. The pretrained weights of the LETR model\ntrained on the Wireframe dataset are used for the exper-\niment.\n2) The images from the Summer_2015-Ames_ULA and\nSummer_2015-Ames_MLA datasets are evaluated using\nthe pretrained weights. The line segments output by the\nmodel are plotted on evaluation images, as shown in\nFig. 7.\n3) However, it is the case that several of the detected line\nsegments indicate the stem rather than the leaf, as shown\nin Fig. 7. The lines on the stem are irrelevant and must be\nignored for the estimation of leaf angle. Two constraints,\none on the slope and another on proximity to the image\nboundary are placed on the detected line segments to\nfilter out the ones that don't belong to the leaf.\na) Slope Constraint: The leaf is generally oriented in\ncomparison to the stem that is almost perpendicular\nto the ground, in the majority of cases. As a result,\nline segments whose orientations are between 80\nand 90 degrees are ignored, since they are likely\nto belong to the stem but not the leaf.\nb) Boundary Proximity Constraint: It is empirically\ndetermined that the line segments which belong to\nthe leaf are at least 100 pixels apart from the image\nboundary. The line segments that are closest to the\nimage boundary are those of the stem. Hence, the\nconstraint to ignore line segments that are within\n100 pixels of the image boundary works to ignore\nline segments that belong to the stem.\nBoth constraints are required to be satisfied for a\nline to be ignored. In certain images, the orientation\nof the leaf is between 80 and 90 degrees. In such\ncases, without the boundary constraint, the lines\npertinent to the leaf are ignored. The line segments\nthat pass the filter are assumed to be present on the\nleaf.\n4) Compute the slope of each of the line segments and\nfind the mode of the slopes. Mode refers to the most\nfrequently occurring item among a list of items.\n5) Retrieve the line segments with the most frequently\noccurring slope for further processing. Only the line\nsegments with the orientation of the mode are considered\nto eliminate any stray lines that may be detected on the\nleaf. In the absence of mode i.e. no two line segments\nhave the same slope, the line segment whose slope is\nthe median of all slopes is considered.\n6) Compute the orientation of the line segment made with\nthe horizontal as the tan inverse of slope i.e., angle =\n$tan^{-1}(slope)$. The orientation made by the line segment\nis deemed the orientation of the leaf with the stem."}, {"title": "IV. IMPACTING FACTORS", "content": "Impact of Image Sharpness\nImage Sharpness describes the clarity of detail in a photo.\nThe aspects of resolution and acutance primarily impact the\nsharpness of an image. It is imperative that the images be\nsharp for the line segments to be identified precisely by\nLETR. Multiple failures are observed with the detection of\nline segments in images of low sharpness. However, when the\nsharpness is increased, the line segments are well detected.\nMultiple Leaf and Stem Detections\nIn a few cases, the trained Mask R-CNN model detects\nmultiple instances on a given image, some not pertinent to\nthe leaf and stem. Fig. 8 shows two images where multiple\ninstances are detected. In such cases, the detection of the\ngreatest size (contour) is considered the instance of interest,\ni.e. leaf and stem. However, it may be flawed in cases where\nthe identified contour is larger than the actual leaf and stem\ninstance in the image."}, {"title": "V. RESULTS", "content": "The algorithm is applied to each of the images from the\nSummer_2015-Ames_ULA and Summer_2015-Ames_MLA\ndatasets, and the orientation between leaf and stem is esti-\nmated. The results are compared with the leaf angle mea-\nsurements made by two graduate students at Iowa State\nUniversity. The students worked independently of each other\nas they arrived at their estimates using the ImageJ tool. The\nmeasurements by the students and the proposed algorithm\nare compared using a metric known as Cosine Similarity\n[21]. Cosine Similarity is the cosine of the angle between\ntwo vectors that are typically non-zero and within an inner\nproduct space. It is useful to compare the similarity between\ntwo vectors represented in a higher-dimensional vector space.\nMathematically, it is defined as the division between the dot\nproduct of vectors and the product of the magnitude of each\nvector and is expressed as, similarity = A \u00b7 B/|A||B| where\nA and B are the two vectors compared for similarity. The\nmeasure is expressed as a value between 0 and 1.\nOn the Summer_2015-Ames_MLA dataset, the cosine sim-\nilarity between the proposed algorithm and student 1 is 0.98\nwhich indicates a ten-degree difference in orientation between\nvectors, and the proposed algorithm and student 2 is 0.99\nwhich indicates an eight-degree difference in orientation be-\ntween the vectors. On the Summer_2015-Ames_ULA dataset,\nthe cosine similarity between the proposed algorithm and\nstudent 1 is 0.998 which indicates a four-degree difference in\norientation between vectors, and the proposed algorithm and\nstudent 2 is 0.992 which indicates a seven-degree difference\nin orientation between the vectors.\nFurthermore, an intuitive outlier analysis is performed on\nthe results to gain insight into the similarity of estimation\nbetween the different techniques. The rule used to identify an\noutlier is, \u201cthe difference in estimates between the proposed\nalgorithm and a student is greater than eight degrees\". The\nanalysis assumes that the measurements made by the students\nare accurate. However, the estimations by the students also\nhave a variance between them. The variance in student esti-\nmates, given by \u2211(e1-e2)/n where el and e2 are estimates\nmade by students 1 and 2 respectively, and n is the total\nsize of the data sample, are 2.57 and 2.07 on Summer_2015-\nAmes_MLA dataset and Summer_2015-Ames_ULA dataset\nrespectively. Hence, the eight-degree range to identify outliers\nis reasonable. Applying the outlier rule, a total of 31 outliers on\nthe Summer_2015-Ames_MLA dataset and 16 outliers on the\nSummer_2015-Ames_ULA dataset are detected. Among the\nestimates that are not outliers, a variance of 3.73 degrees and\n3.11 degrees is identified between the proposed algorithm and\nstudent 1, and the proposed algorithm and student 2 respec-\ntively on the Summer_2015-Ames_MLA dataset whereas a\nvariance of 2.67 degrees and 2.72 degrees is identified between\nthe proposed algorithm and student 1, and proposed algorithm\nand student 2 respectively on the Summer_2015_Ames_ULA\ndataset. The results show that the proposed algorithm outputs\nresults that highly co-relate with the manual measurements,\ndemonstrating the feasibility of the technique for leaf angle\nestimation.\""}, {"title": "VI. FUTURE WORK AND CONCLUSION", "content": "Moving forward, the proposed algorithmic pipeline will be\nimplemented as part of a real-time Android application so\nplant scientists and individual enthusiasts may leverage the ap-\nplication on a day-to-day basis. The bottleneck for the mobile\napplication is the use of neural network models. The current\nMask R-CNN network will have to be adapted so it works\non mobile devices using networks such as Mobilenet SSD\nv1/v2 [23] as the backbone in place of the current ResNet-\n101 model. The annotation of images is another bottleneck\nthat is laborious and time-consuming. The use of a screen to\nact as the common background over the plants as images are\ncaptured is an efficient way to help identify the location of the\nplant in the image.\nOverall, the proposed technique demonstrates the idea of\nestimating the angle between leaf and stem using instance\nsegmentation neural networks and vision transformers, a task\nthat is laborious and cumbersome when performed manually.\nWe will implement the leaf angle estimation as a feature in\nthe FieldBook [24] Android application. The estimation of leaf\nangles using automation is novel and has the potential to turn\ninto a one-of-a-kind utility for the agricultural community."}]}