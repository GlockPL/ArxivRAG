{"title": "TASAR: Transfer-based Attack on Skeletal Action Recognition", "authors": ["Yunfeng Diao", "Baiqi Wu", "Ruixuan Zhang", "Ajian Liu", "Xingxing Wei", "Meng Wang", "He Wang"], "abstract": "Skeletal sequences, as well-structured representations of human behaviors, are crucial in Human Activity Recognition (HAR). The transferability of adversarial skeletal sequences enables attacks in real-world HAR scenarios, such as autonomous driving, intelligent surveillance, and human-computer interactions. However, existing Skeleton-based HAR (S-HAR) attacks exhibit weak adversarial transferability and, therefore, cannot be considered true transfer-based S-HAR attacks. More importantly, the reason for this failure remains unclear. In this paper, we study this phenomenon through the lens of loss surface, and find that its sharpness contributes to the poor transferability in S-HAR. Inspired by this observation, we assume and empirically validate that smoothening the rugged loss landscape could potentially improve adversarial transferability in S-HAR. To this end, we propose the first Transfer-based Attack on Skeletal Action Recognition, TASAR. TASAR explores the smoothed model posterior without re-training the pre-trained surrogates, which is achieved by a new post-train Dual Bayesian optimization strategy. Furthermore, unlike previous transfer-based attacks that treat each frame independently and overlook temporal coherence within sequences, TASAR incorporates motion dynamics into the Bayesian attack gradient, effectively disrupting the spatial-temporal coherence of S-HARs. To exhaustively evaluate the effectiveness of existing methods and our method, we build the first large-scale robust S-HAR benchmark, comprising 7 S-HAR models, 10 attack methods, 3 S-HAR datasets and 2 defense models. Extensive results demonstrate the superiority of TASAR. Our benchmark enables easy comparisons for future studies, with the code available in the supplementary material.", "sections": [{"title": "Introduction", "content": "Human Activity Recognition (HAR) has a wide range of application scenarios, such as human-computer interactions, bio-mechanics, and virtual reality (Sun et al. 2022; Guo et al. 2024a). Skeleton sequences has been widely used in HAR (Yan, Xiong, and Lin 2018), because skeleton data is a well-structured data representation of human behaviors and robust to lighting, occlusion, and view angles. Recent research (Wang et al. 2021; Diao et al. 2021) demonstrates that S-HARs are less robust than expected when adversarial perturbations are added to the testing data, causing skeletal classifiers to produce incorrect outputs. Previous skeleton-based attacks are mainly developed under the white-box setting (Liu, Akhtar, and Mian 2020; Tanaka, Kera, and Kawamoto 2022), where the attacker knows the structure and parameters of a given model, or under the query-based setting (Diao et al. 2021; Kang et al. 2023a), where the adversary can request numerous queries against the target model (Diao et al. 2024a). But both settings are impractical in real-world HAR scenarios, such as autonomous driving (Guo et al. 2024b), intelligent surveillance (Garcia-Cobo and SanMiguel 2023), and human-computer interactions (Wang et al. 2020), in which the white-box information and a large number of queries are not attainable.\nIn contrast, transfer-based attacks, which craft adversarial examples from surrogate models and then transfer them to target black-box models, present a more realistic threat under the free-query black-box setting. Although adversarial transferability has been widely studied across various tasks (Huang et al. 2023; Dong et al. 2018; Diao et al. 2024b), research on S-HAR remains limited. Recent studies have attempted to apply white-box S-HAR attacks against black-box models via surrogate models (Wang et al. 2021; Liu, Akhtar, and Mian 2020). However, their transferability is low and highly sensitive to the surrogate choice (Wang et al. 2023; Lu et al. 2023). Consequently, existing S-HAR attacks cannot be regarded as transferable. Similarly, previous transfer-based attacks (Dong et al. 2018; Xiong et al. 2022) are successful on image data, but show poor transferability when applied to skeletal motion. Contrary to the common belief that adversarial examples transfer well across different model architectures and parameters (Liu et al. 2016), existing attacks do not exhibit transferability on S-HAR, raising doubt on the usefulness of adversarial transferability in this domain (Lu et al. 2023). More importantly, the reason for this failure remains unclear.\nTo study this phenomenon, we begin by examining the factors that hinder adversarial transferability in S-HARs. Our preliminary experiments reveal that adversarial transferability is highly sensitive to the chosen surrogates (Tab. 1). This finding motivates us to further explore the differences between surrogate models from the view of loss surface smoothness, as previous research has demonstrated that the smoothness of the loss surface significantly impacts adversarial"}, {"title": "Related Work", "content": "Skeleton-Based Human Action Recognition. Early S-HAR research employed convolutional neural networks (CNNs) (Ali et al. 2023) and recurrent neural networks (RNNs) (Du, Wang, and Wang 2015) to extract motion features in the spatial domain and temporal domains, respectively. However, skeleton data, inherently a topological graph, poses challenges for feature representation using traditional CNNs and RNNs. Recent advances with graph convolutional networks (GCNs) (Kipf and Welling 2016) have improved performance by modeling skeletons as topological graphs, with nodes corresponding to joints and edges to bones (Yan, Xiong, and Lin 2018). Subsequent improvements in graph designs and network architectures include two-stream adaptive GCN (2s-AGCN) (Shi et al. 2019b), directed acyclic GCN (DGNN) (Shi et al. 2019a), multi-scale GCN (MS-G3D) (Liu et al. 2020), channel-wise topology refinement (CTR-GCN) (Chen et al. 2021) and auxiliary feature refinement (FR-HEAD) (Zhou, Liu, and Wang 2023). Alongside advancements in GCN-based models, recent studies are exploring temporal Transformer structures for S-HARS (Do and Kim 2024; Qiu et al. 2022; Guo et al. 2024b), but their vulnerability remains unexplored. This work is the first to assess the adversarial robustness of Transformer-based S-HARS.\nAdversarial Attacks on S-HAR. Adversarial attacks (Szegedy et al. 2013) highlight the susceptibility of deep neural networks and have been applied across different data types. Recently, attacks on S-HAR have garnered increasing attention. CIASA (Liu, Akhtar, and Mian 2020) proposes a constrained iterative attack via GAN (Goodfellow et al. 2014) to regularize the adversarial skeletons. SMART (Wang et al. 2021) proposes a perception loss gradient. Tanaka et al. (Tanaka, Kera, and Kawamoto 2022) suggest only perturbing skeletal lengths, and evaluate robustness via Fourier analysis (Tanaka, Kera, and Kawamoto 2024). These methods are white-box attacks, requiring full knowledge of the victim model. In contrast, BASAR (Diao et al. 2021, 2024a) proposes motion manifold searching to achieve the query-based black-box"}, {"title": "Methodology", "content": "We denote a clean motion $x \\in X$ and its corresponding label $y \\in Y$. Given a surrogate action recognizer $f_{\\theta}$ parametrized by $\\theta$, $f_{\\theta}$ is trained to map a motion $x$ to a predictive distribution $p(y \\mid x, \\theta)$. The white-box attack can be optimized by minimizing the predictive probability:\n$\\mathop{\\arg\\min}\\limits_{\\tilde{x}} p(y \\mid \\tilde{x}, \\theta)$ \n$||x-\\tilde{x}||_{p} \\le \\epsilon$ (1)\nwhere $\\tilde{x}$ is the adversarial example and $\\epsilon$ is the perturbation budget. $||\\cdot||_{p}$ is the $l_{p}$ norm distance. The procedure of transfer-based attack is firstly crafting the adversarial example $\\tilde{x}$ by attacking the surrogate model, then transferring $\\tilde{x}$ to attack the unseen target model. In Eq. (1), since the transferable adversarial examples are optimized against one surrogate model, the adversarial transferability heavily relies on the surrogate model learning a classification boundary similar to that of the unknown target model. While possible for image classification, it proves unrealistic for S-HAR (Wang et al. 2023; Lu et al. 2023)."}, {"title": "Motivation", "content": "Existing S-HAR attacks have shown outstanding white-box attack performance but exhibit low transferability (Wang et al. 2023). Similarly, previous transfer-based attacks (Dong et al. 2018; Xiong et al. 2022), successful on image data, also show poor transferability when applied to skeletal motion (Lu et al. 2023). Naturally, two questions occur to us: (1) Why do existing adversarial attacks fail to exhibit transferability in skeletal data? (2) Do transferable adversarial examples truly exist in S-HAR?\nTo answer these questions, we start by generating adversarial examples using various surrogate skeletal recognizers and then evaluate their adversarial transferability. Obviously, in Tab. 1, the transferability is highly sensitive to the chosen surrogates, e.g. CTR-GCN (Chen et al. 2021) as the surrogate exhibits higher transferability than ST-GCN (Yan, Xiong, and Lin 2018). This observation motivates us to further investigate the differences between surrogate models. Previous research (Wu and Zhu 2020; Qin"}, {"title": "A Post-train Bayesian Perspective on Attack", "content": "Unfortunately, directly sampling from the posterior distribution of skeletal classifiers is not a straightforward task due to several factors. First, directly sampling the posterior is intractable for large-scale skeletal classifiers. Although approximate methods such as MCMC sampling (Welling and Teh 2011) or variational inference (Blei, Kucukelbir, and McAuliffe 2017) are possible, sampling is prohibitively slow and resource-intensive due to the high dimensionality of the sampling space, which typically involves at least several million parameters in skeletal classifiers. In addition, skeletal classifiers normally contain a large number of parameters and are pre-trained on large-scale datasets (Liu et al. 2019).\nConsequently, it is not practical for end-users to re-train the surrogate in a Bayesian manner, as the training process is time-consuming.\nTo solve the above issues, we propose a new post-train Bayesian attack. We maintain the integrity of the pre-trained surrogate while appending a tiny MLP layer $g_{\\theta'}$ behind it, connected via a skip connection. Specifically, the final output logits can be computed as: $logits = g_{\\theta'} (f_{\\theta}(x)) + f_{\\theta}(x)$.\nIn practice, we adopt Monte Carlo sampling to optimize the appended Bayesian model:\n$\\mathop{\\max}\\limits_{\\theta'} E_{\\theta' \\sim p(\\theta'|D,\\theta)}P (y \\mid x, \\theta, \\theta')$ \n$\\approx \\mathop{\\max}\\limits_{\\theta'} \\frac{1}{K} \\sum_{k=1}^{K} p(y | x, \\theta, \\theta'_k), \\quad \\theta'_k \\sim p(\\theta' | D, \\theta)$ (3)\nwhere K is the number of appended models. Correspondingly, Eq. (2) can be approximately solved by performing attacks on the ensemble of tiny appended models:\n$\\mathop{\\arg\\min}\\limits_{\\delta} \\frac{1}{K} \\sum_{k=1}^{K} p (y | x + \\delta, \\theta,\\theta'_k), \\quad \\theta'_k \\sim p(\\theta' | D, \\theta)$ (4)\n$|| \\delta ||_{p} \\le \\epsilon$\nOur post-train Bayesian attack offers two advantages. First, the appended models are composed of tiny MLP layers, getting a similar memory cost to a single surrogate. Second, by freezing $f_{\\theta}$, our post-train Bayesian strategy keeps the pre-trained surrogate intact, avoiding re-training the pre-trained surrogate. More importantly, training on $g_{\\theta'}$ is much faster than on $f_{\\theta}$ due to the smaller model size of $g_{\\theta'}$."}, {"title": "Post-train Dual Bayesian Motion Attack", "content": "In our preliminary experiments, we found that a naive application of post-train Bayesian attack (Eq. (4)) already surpassed the adversarial transfer performance of existing S-HAR attacks, which demonstrates the effectiveness of smoothening the loss surface of surrogates. However, its performance remains slightly inferior to the Bayesian attack via re-training a Bayesian surrogate (Li et al. 2023)(Eq. (2)). This performance gap is understandable, as we avoid the prohibitively slow process of sampling the original posterior distribution $\\theta \\sim p(\\theta |D)$ by using a tiny Bayesian component for post-training instead. To further eliminate the trade-off between attack strength and efficiency, we propose a novel post-train dual Bayesian optimization for smoothed posterior sampling, to sample the appended models with high smoothness for better transferability (Fig. 2). Moreover, unlike previous transfer-based attacks that assume each frame is independent and ignore the temporal dependency between sequences, we integrate motion dynamics information into the Bayesian attack gradient to disrupt the spatial-temporal coherence of S-HAR models. We name our method Post-train Dual Bayesian Motion Attack.\nPost-train Dual Bayesian Optimization. This motivation is based on the view that models sampled from a smooth posterior, along with the optimal approximate posterior estimating this smooth posterior, have better smoothness (Nguyen et al. 2024). To this end, we aim for proposing a smooth posterior for learning post-train BNNs, hence possibly possessing higher adversarial transferability. Specifically, inspired by the observation that randomized weights often achieve smoothed weights update (Izmailov et al. 2018; Dziugaite and Roy 2017; Jin et al. 2023), we add Gaussian noise to smooth the appended network weights. This is achieved by a new post-train dual Bayesian optimization:\n$\\mathop{\\max}\\limits_{\\theta'} E_{\\theta' \\sim p(\\theta'|D,\\theta)} E_{\\Delta \\theta' \\sim \\mathcal{N}(0,\\sigma^{2}I)}P (y \\mid x, \\theta, \\theta' + \\Delta \\theta')$ (5)\nFor any appended model sampled from the posterior, Eq. (5) ensures that the neighborhood around the model parameters has uniformly low loss. We further use dual Monte Carlo sampling to approximate Eq. (5):\n$\\mathop{\\min}\\limits_{\\delta} \\frac{1}{KM} \\sum_{k=1}^{K}\\sum_{m=1}^{M} \\mathcal{L} (x, y, \\theta,\\theta'_k + \\Delta \\theta'_{km}),$ \n$\\Delta \\theta'_{km} \\sim \\mathcal{N} (0, \\sigma^{2}I)$ (6)\nwhere $\\mathcal{L}$ is the classification loss. Considering dual MCMC samplings computationally intensive, we instead consider the worst-case parameters from the posterior, followed by (Li et al. 2023). Hence Eq. (6) can be equivalent to a min-max optimization problem, written as:\n$\\mathop{\\min}\\limits_{\\delta} \\mathop{\\max}\\limits_{p(\\Delta \\theta') \\ge \\xi} \\frac{1}{K} \\sum_{k=1}^{K} \\mathcal{L} (x, y, \\theta, \\theta'_k + \\Delta \\theta')$ (7)\nThe confidence region of the Gaussian posterior is regulated by $\\xi$. We discuss the sensitivity to $\\xi$ in the Appendix B. The entanglement between $\\theta'$ and $\\Delta \\theta'$ complicates gradient updating. To simplify this issue, we utilize Taylor expansion at $\\theta'$ to decompose the two components:\n$\\mathop{\\min}\\limits_{\\delta} \\mathop{\\max}\\limits_{p(\\Delta \\theta') \\ge \\xi} \\frac{1}{K} \\sum_{k=1}^{K} [\\mathcal{L} (x, y, \\theta,\\theta'_k) + \\nabla_{\\theta'} \\mathcal{L} (x, y, \\theta, \\theta'_k)^{T} \\Delta \\theta']$ (8)\nSince $\\Delta \\theta'$ is sampled from a zero-mean isotropic Gaussian distribution, the inner maximization can be solved analytically. We introduce the inference details and mathematical deduction in Appendix A. As shown in Fig. 2, the loss landscape optimized by post-train Dual Bayesian is significantly smoother than vanilla post-train Bayesian."}, {"title": "Temporal Motion Gradient in Bayesian Attack", "content": "Post-train Dual Bayesian Motion Attack can be performed with gradient-based methods such as FGSM (Goodfellow, Shlens, and Szegedy 2014):\n$x = x + \\alpha \\cdot - sign(\\sum_{k=1}^{K}\\sum_{m=1}^{M}\\nabla\\mathcal{L} (x, y, \\theta,\\theta'_k + \\Delta \\theta'_{km}))$ (9)\nwhere $\\alpha$ is the step size. For notational simplicity, we notate the classification loss $\\mathcal{L} (x, y, \\theta,\\theta'_k + \\Delta \\theta'_{km})$ as $\\mathcal{L} (x)$. Assume a motion with t frames $x = [x_{1}, x_{2},...,x_{t}]$, this attack gradient consists of a set of partial derivatives over all frames $\\nabla_{\\mathcal{L}(x)} = [\\frac{\\partial\\mathcal{L}(x)}{\\partial x_{1}}, \\frac{\\partial\\mathcal{L}(x)}{\\partial x_{2}},...,\\frac{\\partial\\mathcal{L}(x)}{\\partial x_{t}}]$. The partial derivative $\\frac{\\partial\\mathcal{L}(x)}{\\partial x_{t}}$ assumes each frame is independent, ignoring the dependency between frames over time. This assumption is reasonable for attacks on static data such as PGD (Madry et al. 2017) while infeasible for skeletal motion attacks. In skeletal motion, most S-HAR models learn the spatial-temporal features (Yan, Xiong, and Lin 2018), hence considering motion dynamics in the computing of attack gradient can disrupt the spatial-temporal coherence of these features, leading to more general transferability. To fully represent the motion dynamics, first-order (velocity) gradient $(\\nabla_{\\mathcal{L}} (x))_{d_{1}}$ and second-order (acceleration) gradient information $(\\nabla_{\\mathcal{L}} (x))_{d_{2}}$ should also be considered. To this end, we augment the original position gradient with the motion gradient, then Eq. (4) becomes:\n$x = x + \\alpha \\cdot sign(\\sum_{k=1}^{K}\\sum_{m=1}^{M}\\mathcal{L}_{dyn} (x))$ (10)\n$\\mathcal{L}_{dyn} (x) = \\sum_{n=0}^{2} W_{n} (\\nabla_{\\mathcal{L}} (x))_{d_{n}}, \\quad \\sum_{n=0}^{2} n = 1$ (11)\nwhere $(\\nabla_{\\mathcal{L}} (x))_{d_{0}} = \\nabla\\mathcal{L}(x)$. Motion gradient can be computed by explicit modeling (Xia et al. 2015) or implicit learning (Tang et al. 2022). Given that implicit learning requires training an additional data-driven model to learn the motion manifold, which increases computational overhead, we opt for explicit modeling. Inspired by (Lu et al. 2023), we employ time-varying autoregressive models (TV-AR)(Bringmann et al. 2017) because TV-AR can effectively estimate the dynamics of skeleton sequences by modeling the temporary non-stationary signals (Xia et al. 2015). We first use first-order TV-AR($f_{d^{1}}$) and second-order TV-AR($f_{d^{2}}$) to model human motions respectively:\n$f_{d^{1}}: x_{t} = A_{t}x_{t-1} + B_{t} + \\varepsilon_{t}$ (12)\n$f_{d^{2}}: x_{t} = C_{t}x_{t-1} + D_{t} \\cdot x_{t-2}+ E_{t} + \\Upsilon_{t}$ (13)\nwhere the model parameters $\\beta^{1} = [A_{t}, B_{t}]$ and $\\beta^{2} = [C_{t}, D_{t}, E_{t}]$ are all time-varying parameters and determined by data-fitting. $\\Upsilon_{t}$ is a time-dependent white noise representing the dynamics of stochasticity. Using Eq. (12), the first-order motion gradient can be derived as:\n$\\frac{\\partial\\mathcal{L}(x)}{\\partial \\beta_{t-1}^{1}} = \\frac{\\partial \\mathcal{L}(x)}{\\partial x_{t}} \\frac{\\partial x_{t}}{\\partial A_{t}}$ +$\\frac{\\partial \\mathcal{L}(x)}{\\partial x_{t}^{2}} \\frac{\\partial x_{t}^{2}}{\\partial A_{t}}$ (14)\nSimilarly, second-order dynamics can be expressed as below by using Eq. (13):\n$\\frac{\\partial \\mathcal{L}(x)}{\\partial \\beta_{t-2}^{2}} = \\frac{\\partial \\mathcal{L}(x)}{\\partial x_{t}} \\frac{\\partial x_{t}}{\\partial A_{t}}$ +$\\frac{\\partial \\mathcal{L}(x)}{\\partial x_{t-1}} \\frac{\\partial x_{t-1}}{\\partial A_{t}}$ +$\\frac{\\partial \\mathcal{L}(x^{2})}{\\partial A_{t}}$ and $D_{t}$\nwhere $C_{t}$ =$ \\frac{\\partial (D_{t} +C_{t}C_{t-1})}{\\partial A_{t}}$ After computing $\\frac{\\partial x_{t-1}}{\\partial A_{t-1}}$ =$C_{t-1}x_{t-2}+ D_{t-1} x_{t-3}+ E_{t-1}+\\Upsilon_{t-1}$, we can compute $C_{t-1}$ Overall, the high-order dynamics gradients over all sequences can be expressed as $({\\nabla_{\\mathcal{L}} (x)})_{d_{1}} = [\\frac{\\partial\\mathcal{L}(x)}{\\partial \\beta_{1}^{d_{1}}}, ... \\frac{\\partial\\mathcal{L}(x)}{\\partial \\beta_{t}^{d_{1}}}]$ and $({\\nabla_{\\mathcal{L}}(x)})_{d_{2}} = [\\frac{\\partial\\mathcal{L}(x)}{\\partial \\beta_{1}^{d_{2}}}, ... \\frac{\\partial\\mathcal{L}(x)}{\\partial \\beta_{t}^{d_{2}}}]$"}, {"title": "RobustBenchHAR Settings", "content": "To our best knowledge, there is no large-scale benchmark for evaluating transfer-based S-HAR attacks. To fill this gap, we build the first large-scale benchmark for robust S-HAR evaluation, named RobustBenchHAR. We briefly introduce the benchmark settings here, with additional details available in Appendix C.\n(A) Datasets. RobustBenchHAR incorporates three popular S-HAR datasets: NTU 60 (Shahroudy et al. 2016), NTU 120 (Liu et al. 2019) and HDM05(M\u00fcller et al. 2007). Since the classifiers do not have the same data pre-processing setting, we unify the data format following (Wang et al. 2023). For NTU 60 and NTU 120, we subsampled frames to 60. For HDM05, we segmented the data into 60-frame samples.\n(B) Evaluated Models. We evaluate TASAR in three categories of surrogate/victim models. (1) Normally trained models: We adapt 5 commonly used GCN-based models, i.e., ST-GCN (Yan, Xiong, and Lin 2018), MS-G3D (Liu et al. 2020), CTR-GCN (Chen et al. 2021), 2s-AGCN (Shi et al. 2019b), FR-HEAD (Zhou, Liu, and Wang 2023), and two latest Transformer-based models SkateFormer(Do and Kim 2024) and STTFormer(Qiu et al. 2022). To our best knowledge, this is the first work to investigate the robustness of Transformer-based S-HARs. (2) Ensemble models: an ensemble of ST-CGN, MS-G3D and DGNN (Shi et al. 2019a). (3) Defense models: We employ BEAT (Wang et al. 2023) and TRADES (Zhang et al. 2019), which all demonstrate their robustness for skeletal classifiers.\n(C) Baselines. We compare with state-of-the-art (SOTA) S-HAR attacks, i.e. SMART (Wang et al. 2021) and CIASA (Liu, Akhtar, and Mian 2020). We also adopt the SOTA transfer-based attacks as baselines, including gradient-based, i.e., I-FGSM (Kurakin, Goodfellow, and Bengio 2018), MI-FGSM (Dong et al. 2018) and the latest MIG (Ma et al. 2023), input transformation method DIM (Xie et al. 2019), and ensemble-based/Bayesian attacks, i.e., ENS (Dong et al. 2018), SVRE (Xiong et al. 2022) and BA (Li et al. 2023). For a fair comparison, we ran 200 iterations for all attacks under $l_{\\infty}$ norm-bounded perturbation of size 0.01. For TASAR, we use the iterative gradient attack instead of FGSM in Eq. (10).\n(D) Implementation Details. Our appended model is a simple two-layer fully-connected layer network. Unless specified otherwise, we use K = 3 and M = 20 in Eq. (10) for default and explain the reason in the ablation study later. More implementation details can be found in Appendix C."}, {"title": "Evaluation on Normally Trained Models", "content": "Evaluation of Untargeted Attack. As shown in in Tab. 1, TASAR significantly surpasses both S-HAR attacks and transfer-based attacks under the black-box settings, while maintaining comparable white-box attack performance. Specially, TASAR achieves the highest average transfer success rate of 35.5% across different models and datasets, surpassing SMART (Wang et al. 2021) (the SOTA S-HAR attack) and MIG (Ma et al. 2023) (the SOTA transfer-based attack) by a large margin of 23.4% and 8.1% respectively. Moreover, TASAR shows consistent transferability across all surrogate models, target models and datasets. These improvements break the common belief that transfer-based attacks in S-HAR suffer from low transferability and highly rely on the chosen surrogate (Lu et al. 2023).\nEvaluation of Targeted Attack. In this section, we focus on targeted attacks under the black-box setting. Improving targeted attack transferability on S-HAR is generally more challenging than untargeted attacks. This is primarily due to the significant semantic differences between the randomly selected class and the original one. Attacking a 'running' motion to 'walking' is generally easier than to 'drinking'. This is why targeted attacks have lower success rate than untargeted attacks. However, Tab. 2 shows TASAR still outperforms the baseline under most scenarios. Moreover, TASAR can successfully attack the original class to a target with an obvious semantic gap without being detected by humans. The visual examples can be found in Appendix B.\nEvaluation on Ensemble and Defense Models\nEvaluation on Ensemble Models. TASAR benefits from the additional model parameters added by the appended Bayesian components. For a fair comparison, we compare it with SOTA ensemble-based methods, i.e., ENS (Dong et al. 2018) and SVRE (Xiong et al. 2022), and the Bayesian Attack (BA) (Li et al. 2023), because they also benefit from the model size. Unlike BA, which re-trains the surrogate into a BNN, we instead append a small Bayesian component for post-training. ENS and SVRE take three models ST-GCN, MS-G3D and DGNN as an ensemble of surrogate models, while BA and TASAR only take MS-G3D as the single substitute architecture. We choose ST-GCN, 2s-AGCN, MS-G3D, CTR-GCN, FR-HEAD as the target models, and evaluate the average white-box attack success rate (WASR), average black-box attack success(BASR) and the number of parameters in Fig. 3. We can clearly see that TASAR (blue line) achieves the best attack performance under both white-box and black-box settings, with an order of magnitude smaller model size. When using MSG3D (12.78M) as the surrogate model, the Bayesian components appended by TASAR only increase 0.012M parameters of the surrogate size, resulting in a memory cost comparable to that of a single surrogate. In contrast, the Bayesian surrogate model used by BA has 15 times more parameters (255.57M) than the single surrogate.\nEvaluation on Defense Models. As BEAT shows high robustness against S-HAR white-box attack (Wang et al. 2023), it is also interesting to evaluate its defense performance against black-box attack. We also employ the adversarial training method TRADES (Zhang et al. 2019) as a baseline due to its robustness in S-HAR (Wang et al. 2023). Obviously, in Tab. 3, TASAR still achieves the highest adversarial transferability among the compared methods against defense models, further validating its effectiveness."}, {"title": "Ablation Study", "content": "Dual MCMC Sampling. TASAR proposes a new dual MCMC sampling in the post-train Bayesian formulation (Eq. (10)). To see its contribution, we conduct an ablation study on the number of appended models (K and M in Eq. (10)). As shown in Tab. 4, compared with vanilla Post-train Bayesian strategy (M=0), the dual sampling significantly improves the attack performance. Furthermore, although TASAR theoretically requires intensive sampling for inference, in practice, we find a small number of sampling is sufficient (K = 3 and M = 20). More sampling will cause extra computation overhead. So we use K = 3 and M = 20 by default.\nTemporal Motion Gradient. TASAR benefits from the interplay between temporal Motion Gradient (MG) and Bayesian manner. We hence conduct ablation studies(MG/No MG) to show the effects of motion gradient and report the results in Tab. 5. Compared with TASAR without using motion gradient, TASAR with motion gradient consistently improves the attack success rate in both white box and transfer-based attacks, which shows the benefit of integrating the motion gradient into the Bayesian formulation.\nSurrogate Transferability\nIt is widely believed that transfer-based attacks in S-HAR are highly sensitive to the surrogate choice (Lu et al. 2023; Wang et al. 2023, 2021). In this subsection, we provide a detailed analysis of the factors contributing to this phenomenon. When looking at the results in Tab. 1 and the visualization of loss landscape in Fig. 2 and Appendix B, we note that loss surface smoothness correlates with the adversarial transferability. For example, CTR-GCN, manifesting smoother regions within the loss landscape, demonstrates higher transferability than ST-GCN and STTFormer. STTFormer trained on NTU 120 has a smoother loss surface than ST-GCN (see Appendix B), resulting in higher transferability than ST-GCN. For NTU 60, STTFormer shows a similar loss surface to that of ST-GCN and exhibits comparable transferability.Therefore, we suspect that the loss surface smoothness plays a pivotal role in boosting adversarial transferability for S-HAR, potentially outweighing the significance of gradient-based optimization techniques. Next, two-stream MS-G3D shows the highest transferability. Unlike other surrogates, which solely extract joint information, MS-G3D uses a two-stream ensemble incorporating both joint and bone features, thereby effectively capturing relative joint movements. In conclusion, we suggest that skeletal transfer-based attacks employ smoother two-stream surrogates incorporating both joint and bone information."}, {"title": "Conclusion", "content": "In this paper, we systematically investigate the adversarial transferability for S-HARs from the view of loss landscape, and propose the first transfer-based attack on skeletal action recognition, TASAR. We build RobustBenchHAR, the first comprehensive benchmark for robustness evaluation in S-HAR. We hope that RobustBenchHAR could contribute to the adversarial learning and S-HAR community by facilitating researchers to easily compare new methods with existing ones and inspiring new research from the thorough analysis of the comprehensive evaluations."}, {"title": "Reproducibility Checklist", "content": "This paper:\n\u2022 Includes a conceptual outline and/or pseudocode description of AI methods introduced (yes)\n\u2022 Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (yes)\n\u2022 Provides well marked pedagogical references for less-familiare readers to gain background necessary to replicate the paper (yes)\nDoes this paper make theoretical contributions? (no)\nDoes this paper rely on one or more datasets? (yes) If yes, please complete the list below.\n\u2022 A motivation is given for why the experiments are conducted on the selected datasets (yes)\n\u2022 All novel datasets introduced in this paper are included in a data appendix. (NA)\n\u2022 All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (NA)\n\u2022 All datasets drawn from the existing literature (potentially including authors' own previously published work) are accompanied by appropriate citations. (yes)\n\u2022 All datasets drawn from the existing literature (potentially including authors' own previously published work) are publicly available. (yes)\n\u2022 All datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing. (NA)\nDoes this paper include computational experiments? (yes) If yes, please complete the list below.\n\u2022 Any code required for pre-processing data is included in the appendix. (yes).\n\u2022 All source code required for conducting and analyzing the experiments is included in a code appendix. (yes)\n\u2022 All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (yes)\n\u2022 All source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from (yes)\n\u2022 If an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results. (yes)\n\u2022 This paper specifies the computing infrastructure used for running experiments (hardware and software)"}]}