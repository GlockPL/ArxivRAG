{"title": "AI-Enhanced Sensemaking: Exploring the Design of a Generative AI-Based Assistant to Support Genetic Professionals", "authors": ["Angela Mastrianni", "Hope Twede", "Aleksandra Sarcevic", "Jeremiah Wander", "Christina Austin-Tse", "Scott Saponas", "Heidi Rehm", "Ashley Mae Conard", "Amanda K. Hall"], "abstract": "Generative Al has the potential to transform knowledge work, but further research is needed to understand how knowledge workers envision using and interacting with generative AI. We investigate the development of generative Al tools to support domain experts in knowledge work, examining task delegation and the design of human-AI interactions. Our research focused on designing a generative AI assistant to aid genetic professionals in analyzing whole genome sequences (WGS) and other clinical data for rare disease diagnosis. Through interviews with 17 genetics professionals, we identified current challenges in WGS analysis. We then conducted co-design sessions with six genetics professionals to determine tasks that could be supported by an AI assistant and considerations for designing interactions with the AI assistant. From our findings, we identified sensemaking as both a current challenge in WGS analysis and a process that could be supported by AI. We contribute an understanding of how domain experts envision interacting with generative AI in their knowledge work, a detailed empirical study of WGS analysis, and three design considerations for using generative AI to support domain experts in sensemaking during knowledge work.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent technological advances in generative AI have the potential to transform knowledge work [35, 72, 90]. Knowledge work is the process of applying existing knowledge to solve problems, sharing knowledge with others, and creating new knowledge [41]. Sensemaking, a critical part of knowledge work, involves collecting, synthesizing, and building representations of information [66, 71, 87]. Unlike prior advancements in automation which mainly impacted routine or manual tasks, generative Al models can perform sensemaking tasks typically associated with knowledge work [15, 35, 72]. In this study, we examine the design of generative Al tools to support domain experts in their knowledge work, situating our research in the context of whole genome sequencing (WGS) analysis for rare disease diagnosis. During WGS analysis, an individual's genome is compared to reference genomes to detect variants that may be causing their disease. To analyze these variants, genetic professionals engage in sensemaking to build models of information known about genes and variants by finding, synthesizing, and interpreting information from several knowledge sources, a time- and effort- intensive process. Because new information about genes and variants is continuously being discovered, analysts must stay abreast of new scientific findings and engage in sensemaking to understand how these findings could impact previously unsolved cases. As fewer than half of individuals with rare diseases receive a diagnosis from the initial analysis of their WGS data, cases may be reanalyzed at later points to identify and apply any new information about gene-disease relationships [83]. Several challenges impede WGS analysis reaching a higher diagnostic yield, including the limited time of available genetics professionals to address the rising number of unsolved cases and limited computational support [2]. As a result, the development of AI tools has been identified as a key opportunity to increase diagnostic yield [18]. Although generative AI has the potential to support knowledge work, concerns have arisen that generative AI could intensify current social issues, such as disinformation, biased decision-making, deskilling, and job replacement [24, 90]. To mitigate these issues, a human-centered approach can be used to develop generative AI tools. In this approach, intelligent systems are designed with a focus on the people using the system and an understanding that they are part of a larger sociotechnical system [6, 70, 91]. We follow a human-centered approach in this study, focusing on three research questions (RQ): (RQ1) What are the current challenges and needs of genetic professionals performing WGS analysis? (RQ2) What tasks could an Al assistant support? and (RQ3) How do genetic professionals envision interacting with the AI assistant? To answer these questions, our study contains two phases: (1) needs elicitation to answer RQ1 and (2) design ideation to answer RQ2 and RQ3. In the first study phase (needs elicitation), we interviewed 17 genetics professionals to better understand their workflows, tools, and challenges. These genetics professionals included analysts directly involved in interpreting WGS data, as well as other roles participating in whole genome sequencing. In the second phase (design ideation), we ideated on the design of a generative AI assistant to support WGS analysis, focusing on the potential uses of large language models (LLMs). LLMs are built from large amounts of text data and can perform a variety of tasks, synthesizing knowledge across information sources and creating human-like content [7, 8, 84]. We conducted co-design sessions, including a group workshop and individual follow-up sessions, with six genetic professionals. In the group"}, {"title": "2 BACKGROUND: WHOLE GENOME SEQUENCING FOR RARE DISEASE DIAGNOSIS", "content": "Collectively, rare diseases have been estimated to affect up to half a billion people around the world [59]. For individuals with a rare disease, finding a diagnosis can take multiple years and involve specialist consultations, laboratory tests, imaging studies, and invasive procedures [76]. The aim of whole genome sequencing in rare disease diagnosis is to identify the variant or variants in an individual's genome that are causing their disease. A DNA sequence is extracted from the individual's sample (e.g., their blood or saliva) and aligned to reference sequences to detect differences (genetic variants) [2, 56]. Variant analysts then analyze the sequence by filtering, prioritizing, and classifying the variants. This process is computer-mediated with bioinformatics tools developed to support analysis and interpretation [20, 61]. Typically, bioinformatics pipelines will be used to annotate the variants with several different types of information, such details about the associated gene and the position of the variant within the gene [58]. This information can then be leveraged to filter the millions of variants to a smaller subset that may be further evaluated for disease causality. For example, if the analyst has access to medical information and samples from the individual's parents, they may apply"}, {"title": "3 RELATED WORK", "content": "Next, we discuss related work on designing human-centered Al systems and using Al to support knowledge workers in sensemaking."}, {"title": "3.1 Designing Human-Centered AI Systems with/for Domain Experts", "content": "Around 87% of AI systems do not reach deployment, because these systems are often solving the wrong problems [88]. Understanding the types and forms of AI support desired by different communities of practice has therefore been an open research area [90]. Appropriately matching the capabilities of a technology with the user tasks that could be most beneficially impacted by the technology has historically been a challenging problem [5]. This problem is further accentuated when designing AI-based technologies because Al models can generate many possible outputs and the capabilities of these models can be unclear [93]. Fundamental considerations in designing AI-based systems include determining the tasks supported by AI, the appropriate level of Al involvement in those tasks, and the design of interactions between users and AI [52, 82]. Participatory and community-based research approaches can be used to directly engage domain experts in envisioning and designing AI systems [17, 90]. These types of approaches are useful in designing human-centered AI systems, but several challenges exist [12, 93]. First, domain experts can face issues envisioning and critiquing potential uses of Al without appropriately understanding its capabilities and limitations [12, 93]. To mitigate this challenge, participants can be provided with knowledge about AI capabilities before starting design ideation [1, 96]. A second challenge in using participatory and community-based research approaches is encouraging participants to imagine novel human-AI interactions. Participants often fixate on existing interaction forms, such as chatbots, recommendations, and alerts [95]. During participatory design approaches, participants can be prompted to consider different degrees of AI delegation, such as: no AI, machine-in-the-loop, human-in-the-loop, and AI only [52]. In the machine-in-the-loop level of delegation, a person is responsible for performing a task, with assistance from Al when appropriate. In the human-in-the-loop level, the Al is largely performing a task, with confirmation from a person when necessary. Recent studies have used participatory approaches to engage domain experts in designing AI systems, identifying tasks in their workflows that could be supported by AI and factors influencing their desired interactions with AI [80, 94, 95, 97]. These domain experts included speech pathologists [80], radiologists [94, 97], and intensive care clinicians [95], with participants envisioning AI systems supporting tasks such as documentation, care coordination, and billing [80, 94]. When envisioning potential interactions with Al systems, domain experts often desired interactions that would facilitate"}, {"title": "3.2 Developing Tools to Support Knowledge Workers in Sensemaking", "content": "Knowledge workers aim to understand a body of knowledge and use this information to solve problems and generate new information for their organization [42]. A critical part of knowledge work involves sensemaking. The sensemaking process consists of two interconnected activities: (1) foraging (searching, filtering, and synthesizing information) and (2) sensemaking (building, refining, and presenting models of information) [66]. The process of sensemaking can differ depending on the domain. For example, prior research has focused on supporting researchers in making sense of scientific literature and identifying new research areas [13, 39, 40, 62, 99]. Clinicians, however, may have different needs when making sense of scientific literature. Clinicians often focus on assessing the applicability of papers to their patients and may stop searching for literature once they have gathered enough evidence to justify a decision [92]. In collaborative environments, such as healthcare settings, sensemaking is often a social and interactive activity that occurs between people [64]. Even when people do not directly collaborate, individuals may be able to leverage the artifacts and work of another person to improve their own sensemaking (known as distributed sensemaking) [25]. However, users may decide not to use the sensemaking artifacts of others if the work required to interpret those artifacts is too high. Conversely, creating sensemaking artifacts in a manner that is useful to other users may be too time- and effort-intensive for the current user [46]. When deciding to reuse knowledge from another person's sensemaking process, people often consider the trustworthiness of the other person, the context of their sensemaking (e.g., the goals motivating their sensemaking), and the thoroughness of their sensemaking [48]. Prior work has developed tools to support sensemaking in a range of contexts [13, 29, 39, 40, 43, 62, 68, 99]. A subset of tools help users in creating artifacts that could not only support their sensemaking, but be shared with others to support their sensemaking as well [46, 86, 98]. For example, Unakite [46] assists users in building comparison tables to better understand the tradeoffs between different options when searching for information online. However, these tools still require user effort to capture, synthesize, and organize information. To reduce this burden, automated approaches can be used. For example, Crystalline [47] leverages natural language processing (NLP) to automatically extract relevant information from webpages and organize this information into comparison tables to support decision making. With the recent advances in LLMs, these models can be even more effective at extracting and synthesizing information, avoiding some of the errors found with earlier NLP approaches [49]. This is largely due to LLMs being trained on incredibly vast amounts of diverse"}, {"title": "4 STUDY METHODS", "content": "Our study had two phases: (1) needs elicitation and (2) design ideation (). In the first phase, we conducted interviews with genetic professionals to understand the challenges faced during WGS analysis (RQ1 - needs elicitation). In the second phase, we conducted co-design sessions to design an AI assistant to support WGS analysis (RQ2 \u2013 AI tasks, RQ3 \u2013 AI interaction). This study was approved by our Institutional Review Board."}, {"title": "4.1 Primary Research Site and their Platform for Sequencing Analysis (seqr)", "content": "We have an ongoing collaboration with our primary research site, the Broad Institute, an academic research institute in the United States. The Broad Institute primarily performs genome sequencing for internal research studies, as well as studies led by external research collaborators. The Broad Institute developed and currently uses an open-source, web-based tool for rare disease case analysis and project management (seqr [61] ). To facilitate genome sequencing analysis, seqr supports variant filtration, annotation, and presumed causal variant identification. Seqr has a project page, where analysts can view all cases in a project and filter for the cases assigned to the analyst. After selecting a case, seqr opens the family page, which contains medical information about the individual with the rare disease and their family. Analysts can also record notes about the case on this page. Analysts can then use a search feature to investigate variants in the individual's genome. This search feature includes a list of predefined searches which can be customized as needed. The searches each return a list of gene-variant pairs. Each gene-variant pair is annotated with the information needed to interpret its pathogenicity and causality, such as the type of variant, information submitted about the variant in ClinVar, and any disease associations for the gene found in OMIM. Analysts can also use a dedicated section for taking notes about the gene-variant pair and can flag the gene-variant pair for further review or exclusion."}, {"title": "4.2 Participant Recruitment", "content": "Our participants were recruited from five different roles determined to be relevant to the task of WGS analysis in rare disease analysts: variant analyst (VA), laboratory director (LD), clinician (C), methods developer (MeD), and program manager (PM) (Table 1). Variant analysts, laboratory directors, and clinicians are directly involved in interpreting WGS data and many of the tasks performed by these three roles may overlap. The primary role of variant analysts, also known as variant scientists, genomic analysts, or clinical analysts, is analyzing and interpreting variants. They also have interpretation-related responsibilities, such as medical record review and case management. Laboratory directors review and approve the work of variant analysts, and conduct variant interpretation and project management. Clinicians may have various roles within the rare disease case analysis ecosystem including patient-facing roles of ordering genome analysis and relaying relevant findings to individuals with rare diseases or lending their clinical experience to molecular laboratories or research programs. Although methods developers and program managers may not be directly involved in interpreting WGS data, we included participants from these roles in the interviews because their work affects the analysis process. Methods developers (MD) create computational toolkits and platforms used in the analysis. Program managers (PM) are higher-level administrators who determine the processes of the lab, clinic, or research program. Because the genetics professional network is relatively small in rare disease, participant information is presented in aggregate to ensure anonymity (Table 1). We recruited 17 genetic professionals (seven variant analysts, two laboratory directors, two clinicians, two method developers, and two program managers) to participate in Phase I interviews. We first sent a call for participation email to our primary research site and recruited fourteen individuals. We also used snowballing sampling and recruited three additional participants from external sites: a variant analyst and a program manager outside of the United States, and a methods developer in the United States. For the Phase II design ideation sessions, we recruited six genetic professionals (five variant analysts and one clinician). Of these six participants, five had also participated in the earlier interviews. Recruitment for the Phase II was constrained to individuals actively performing case interpretation, primarily variant analysts. To ground the design thinking within a specific tool, we limited participants to those who had familiarity with seqr, the platform planned for use in our prototype development."}, {"title": "4.3 Phase I: Interviews (RQ1 \u2013 Needs Elicitation)", "content": "4. 3. 1 Protocol. We conducted all interviews over a video-conferencing platform. The interview structure and questions were developed and curated by our research team in collaboration with non-participant genetic professionals from our primary research site. In the first interview section, we asked about the participant's day-to-day responsibilities, challenges faced, and tools used. In the second section, we asked about the data used in their WGS analysis and the opportunities and limitations of the data. In the third section, we discussed their process for reanalyzing cases, asking questions about the initialization \"triggers\" for reanalysis, the reanalysis workflow, and challenges in reanalysis. The semi- structured interviews were conducted by a member of the research team with prior experience in genomic analysis. We recorded audio and video of the interviews after obtaining consent from each participant.\n4. 3. 1 Data Analysis. We performed an inductive, qualitative content analysis [16] of the interview transcripts, focusing on the (1) tools and data used in WGS analysis, (2) reanalysis initialization triggers, and (3) challenges in the initial analysis and reanalysis of WGS data. Transcripts were automatically generated from the interview recordings and then verified for accuracy. An HCI researcher on the team iteratively open-coded the transcripts and connected codes to develop a preliminary set of themes. The themes were discussed and further refined by our multidisciplinary research team, which included researchers with backgrounds in HCI, genomics, and computer science."}, {"title": "4.4 Phase II: Co-Design Sessions (RQ2 \u2013 AI Tasks, RQ3 \u2013 AI Interaction)", "content": "We conducted co-design sessions with variant analysts at our primary research site. Co-design sessions directly engage users in the design process, empowering them to brainstorm and prioritize new technologies [73]. We conducted two types of co-design sessions: (1) a group co-design workshop and (2) individual design walk-through sessions.\n4. 4. 1 Group Co-Design Workshop: Protocol. Six participants participated in the group workshop. Four participants joined the workshop in person and two joined remotely. Four researchers facilitated the workshop (one remotely and three in person). Two researchers have a background in HCI, one has a background in computer science and computational biology, and one has experience in WGS analysis. Drawing on prior work [1, 96], we blended user-centered and technology-centered approaches in conducting the workshop. The half-day workshop had four sections: (1) a discussion of the interview findings from Phase 1, (2) an overview of the recent advances in AI, (3) a brainstorming activity on the tasks supported by an Al assistant, and (4) a sketching activity on the design of interactions with an AI assistant. In the first section, we reviewed the findings from interviews to both validate findings with participants and prompt thinking about the challenges faced in WGS analysis. Next, we asked participants to discuss their experiences in using generative Al tools (e.g., ChatGPT, BingChat). A researcher on the team with experience in developing AI tools followed by presenting an overview of AI. We provided a brief history of AI, a description of foundational models, examples of generative AI tools, and limitations of generative AI (e.g., lack of explainability, possibility of hallucinations). To broaden the design thinking of participants, we also provided examples of systems that have other interaction mechanisms besides conversational user interfaces (e.g., [89]). We concluded the presentation by listing some capabilities of AI based on design resources from prior work [96]. Next, we conducted a brainstorming activity, where we asked participants to individually write down tasks that an AI assistant could support. Participants then shared their ideas with the wider group. Finally, we asked participants to individually sketch how they imagine interacting with an AI assistant in their existing platform for analysis and reanalysis. Participants shared these sketches with the group. One participant voluntarily left the workshop before the sketching activity because they no longer wanted to participate in the session.\n4. 4. 2 Group Co-Design Workshop: Data Analysis. We used affinity diagramming to analyze the data collected during the group co-design workshop. This method is well-suited for wide-ranging and unstructured types of data and can be used to highlight patterns between issues and areas for potential innovation [3, 53]. From the group co-design workshop, we collected three types of data: (1) a list of tasks generated during the brainstorming activity, (2) participants' sketches, and (3) notes taken by team researchers during the session. We analyzed the tasks brainstormed by participants by consolidating related tasks and iteratively grouping tasks into categories. We also analyzed participant sketches and their descriptions of the sketches to identify features and interaction mechanisms to be explored through prototyping. Drawing on these findings, we created a clickable prototype of an AI assistant within the analysis tool used at our primary research site.\n4.4.3 Individual Design Walk-Through Sessions: Protocol. We used the prototype developed from the group co-design workshop findings as a probe to elicit discussion and design exploration in individual design walk-through sessions. We conducted the individual design walk-through sessions one week after the group workshop. Five of the six participants from the group workshop participated in a session . The virtual 45-minute sessions began with a prioritization activity, asking the participant to select the top three tasks that an AI assistant should support during WGS analysis. We also asked if there were any tasks that the participant thought an AI assistant should not support. We then presented the participant with the AI assistant prototype. After participants discussed their initial impressions of the Al assistant's features, we asked follow-up questions to further probe their feedback, which we used to iteratively refine the prototype. The sessions were recorded with participant consent.\n4.4.3 Individual Design Walk-Through Sessions: Data Analysis. We analyzed the transcripts from the individual design walk-through sessions by conducting an inductive, qualitative content analysis [16]. During this analysis, we focused on understanding themes in participant feedback to the prototype. An HCI researcher on the team iteratively open-coded the transcripts and connected codes to form an initial set of themes. These themes were discussed and refined through discussions with the wider research team."}, {"title": "5 PHASE I FINDINGS: USER NEEDS AND CHALLENGES IN WGS ANALYSIS (RQ1)", "content": "In response to RQ1, we identified three main challenges in WGS analysis from our interview data. The first two challenges occur when an analyst is performing an initial analysis or reanalysis of an individual's WGS data. The third challenge occurs when analysts select cases for reanalysis."}, {"title": "5.1 Looking within a Case: Analyzing an Individual's WGS Data", "content": "During an initial analysis or reanalysis of WGS data, analysts need to review and interpret the variants in an individual's WGS data. Analysts may share their findings in several ways, including presenting variants of interest to team members for their feedback, submitting findings to data-sharing platforms, and preparing results for clinicians. Below we discuss two challenges that we identified in this process.\n5.1.1 Challenge One: Aggregating and Interpreting Information about a Gene and Variant. When reviewing and interpreting gene-variant pairs in a case, analysts need to engage in foraging and sensemaking to aggregate and synthesize information known about the gene and variant to interpret if the individual's variant could be causing their disease. Because information about genes and variants can be found in many different online sources, retrieving and synthesizing this information can be time-consuming: \"I just feel like we still have to go to so many different resources, particularly for literature searching... none of it is in one place. We're doing Google Scholar searches, we're going to HGMD [a database of papers on variants and genes [79]], we are pulling the literature from ClinVar and just concatenating and all that wastes so much time...\" [LD#1]. Analysts do not want to miss a piece of information that could change their interpretation of a case, which makes it challenging to decide when to stop investigating the information sources and move on to other variants or cases: \"I think a lot of time is also spent trying to make sure you haven't missed something\" [VA#6]. Several participants highlighted the need for computational tools to better support this process: \u201cAutomated information gathering and then on top of that, if the information could be organized in a useful way that reflects how I would then be categorizing that collated data, that would be helpful [VA#7]. Participants noted that reanalysis could be performed more efficiently if they had more effective tools for understanding the new information since the last analysis. When reanalyzing cases, analysts currently repeat sensemaking work done in the initial analysis to determine if there have been any changes in information about genes and variants: \u201cSometimes it feels like we're repeating work that we've already done before...we have to go out and check all these different databases once again, and oftentimes there isn't new information\" [VA#1]."}, {"title": "5.1.2 Challenge Two: Sharing Findings with Others", "content": "Analysts often share their findings with others to get feedback on their interpretations and identify any other information about a gene or variant. Although analysts can get feedback from other team members at group meetings, preparing presentations for the meetings can be time-consuming and tedious: \u201cCreating presentations can always be challenging, not because they're by nature difficult, but the tedious, repetitive nature... I feel like it's quite clunky to have to every time create a presentation from scratch because they all follow the same pattern, even in my last group\" [VA#6]. Analysts may also use data-sharing platforms to share and discuss their findings with other researchers or clinical groups outside their institution. Data-sharing platforms can facilitate connections with others who may be investigating a gene or variant, allowing analysts to obtain information that has not yet been formally published in a scientific paper. For example, when an analyst discovers a novel gene that they suspect may be related to a disease, they can submit information about the gene to the MatchMaker Exchange [65]. This platform connects the analysts to others who may be investigating that gene or have a case with a similar phenotype. As the platform has grown and become more saturated with submissions, manually following up with others interested has become more time- consuming: \"Chances are if you put a gene into MatchMaker Exchange, you're going match with at least one person and I've had in some cases, well over 50 matches... To follow up with these people individually by email... find out what their patient's presentation is, what the genotype is. All of that adds a lot of logistics time\" [VA#3]. In general, participants highlighted the need to improve data-sharing platforms so that people would be encouraged to share their findings and could easily do so in a format that is useful for others: \"Where is that sweet spot between sharing enough that somebody could analyze and reanalyze a case? And not so much that people start to skip out on it because they're like 'I just don't have time to fill this entire data model out\u201d [PM#2]."}, {"title": "5.2 Looking Across Cases: Selecting Cases for Reanalysis", "content": "Unsolved cases may be reanalyzed at later points in time to understand if new information about a gene or variant could be used to solve the case. We identified three types of potential initialization triggers for the reanalysis of WGS data: (1) time-based, (2) request-based, and (3) findings-based . With the time-based trigger, cases are reanalyzed because a certain amount of time has elapsed since the last analysis. With the request-based trigger, cases are reanalyzed because of a request from the individual, their clinician, or family. For groups that analyze cases as part of research studies, the request for reanalysis may come from their study collaborators. With the findings-based trigger, cases are reanalyzed because of a new scientific finding (e.g., a new scientific paper or new method) that could affect the case. Each clinical or research group performing the sequencing analysis often had policies around which of the three initialization triggers would lead to reanalysis of their cases. For example, one group at our primary research site tried to reanalyze cases every year (time-based trigger). Once reanalysis is triggered, the reanalysis workflow is similar to the initial analysis workflow. Analysts will frequently do a full reanalysis of the case, repeating the steps taken during the initial analysis. Reanalysis can take between minutes to several hours, depending on several factors, including the availability of sequences from the individual's family (which allow analysts to rule out variants based on patterns of inheritance) and the number of publications about a gene and variant. If the case has detailed notes from prior analyses, analysts can refer to those notes to reanalyze the case more quickly.\n5.2.1 Challenge Three: Prioritizing Cases for Reanalysis. Because less than half of cases are solved with the initial WGS analysis, the backlog of unsolved cases that need to be reanalyzed keeps increasing as analysts receive and analyze new cases. With this growing backlog, participants highlighted challenges in selecting cases for reanalysis, noting a need for better ways of prioritizing cases: \"Some of these families literally have a reminder in their diary for it's been a year, have you found anything new, which is heart-breaking... but at some point, we have to prioritize, do we look at [a case]"}, {"title": "6 PHASE II FINDINGS: TASKS AND DESIGN OF AI ASSISTANT TO SUPPORT WGS ANALYSIS (RQ2, RQ3)", "content": "In response to RQ2 and RQ3, we describe findings from the design ideation phase of the study, where we focused on \"designing the right thing\" (RQ2 - determining tasks of an AI assistant) and \"designing the thing right\" (RQ3 - exploring interactions with the AI assistant) [10, 96]."}, {"title": "6.1 Designing the \"Right Thing\": Determining Tasks of an AI Assistant (RQ2)", "content": "During the group co-design workshop, participants brainstormed tasks that could be supported by an AI assistant. After consolidating the list of tasks , we asked participants in the individual follow-up sessions to vote on the top three tasks that an Al assistant should support and describe the rationale behind their votes. Two tasks were prioritized by all participants: (1) flag cases for reanalysis based on new scientific findings and (2) aggregate and synthesize key information about a gene and variant from scientific publications. From the rationales for their votes, we identified two factors that were frequently considered by participants. First, participants often balanced the strengths of the AI and the limitations of analysts. They prioritized tasks that require a lot of effort or are currently not done and that they thought an Al assistant could perform well. For example, when asked why they prioritized aggregating and synthesizing publications about a gene and variant, one participant explained: \"I feel like I spend the most time digging into a gene, trying to figure out what's known about it... that's a lot of wasted analysis time that AI could do much faster than I can do...\" [VA#5]. Second, participants often considered the likelihood that AI support of a task would lead to external impact. For example, one participant [VA#2] discussed how they were deciding between voting on the \"aggregating and synthesizing publications\" and \"discovering new knowledge from existing data\" tasks. The participant prioritized aggregating and synthesizing publications because they thought support of this task would more likely lead to a returnable result for their patient."}, {"title": "6.2 Designing the \"Thing Right\": Exploring Interactions with an AI Assistant (RQ3)", "content": "Drawing on the findings from the group co-design workshop, we developed a prototype of an Al assistant to explore different interactions between genetic professionals and AI assistant during WGS analysis . In follow-up design walk-through sessions with variant analysts, we identified two themes in the feedback elicited from the prototype: (1) balancing comprehensive and selective evidence and (2) interpreting and verifying information as a collaborative effort.\n6.2.1 Design of AI Assistant Prototype. We created our prototype of an AI assistant within the existing sequencing analysis tool (seqr) used by our primary research site. To populate the information shown in the prototype, we used information from actual publications about a gene (ACTN2) [14, 34, 51, 74, 75]. The AI assistant performs three tasks: (1) flagging cases for reanalysis, (2) aggregating and synthesizing key information about a gene and variant, and (3) drafting presentations for variants of interest. We selected these tasks as they were brainstormed by multiple participants in the group workshop and included in their sketches. Below we further describe how participants envisioned interacting with the AI assistant in their sketches and how this informed the prototype design."}, {"title": "6.2.2 Themes in Feedback Elicited by Prototype", "content": "To elicit feedback from analysts on the design of the AI assistant, we presented the prototype in follow-up design walk-through sessions. We identified two themes in participant feedback about the prototype: (1) balancing comprehensive and selective evidence and (2) interpreting and verifying information as a collaborative effort.\nBalancing comprehensive and selective evidence. Participant reactions to the features for flagging cases for reanalysis and aggregating and synthesizing information from publications highlighted the need to consider the appropriate balance between comprehensive and selective evidence in these features. Participants often asked about the criteria used to determine when a case is flagged for reanalysis because of new papers, worried about \"too much noise\" [VA#4, VA#8] if the criteria were not selective enough. In contrast, participants preferred viewing more comprehensive evidence in the feature to aggregate and synthesize information from publications about a gene and variants. In the first iteration of the prototype, the Al assistant filtered papers presented in the table to match the individual's phenotype. However, participants had negative reactions to this selective filtering by the Al assistant. One participant [VA#5] found it hard to form a mental model of the complexities of a gene with some papers and variants filtered out. Another participant [VA#4] did not trust the AI assistant to appropriately match the individual's phenotype with the phenotypes from the papers. Most participants preferred to see all information at first and then apply their own filters.\nInterpreting, editing, and verifying Al-generated information as a collaborative effort. Participants envisioned collaboratively interpreting and verifying information aggregated and synthesized about a gene and variant. Because individuals in different cases may have variants in the same gene, participants often highlighted that the AI-generated table could be shared between analysts in their institution or even with those in other institutions. Participants envisioned reusing the work of others by viewing (1) if the row in the table had been verified by another user, (2) the edits made to the information in the table, and (3) notes taken by another user. Sharing these artifacts could reduce duplicated work and help analysts better interpret the information. Most participants appreciated the ability to view if a row had been verified by another analyst, noting that they may not dig further into the paper in a verified row unless it was highly applicable to their case. One participant, however, raised concerns that verifying information in the table would be too much \"additional work\" [VA#4]. Participants also discussed the level of edit access that should be given to others, noting the importance of being able to understand who edited the table and why they corrected the information generated by the AI assistant. One participant [VA#4] thought that analysts in training or outside their institution should be granted comment-only access, while other analysts in the institution could have edit access because they would \"use it responsibly\" [VA#4]. Another participant [VA#8] raised concerns that edits from others would mask the inaccuracies of the AI assistant, making it harder to calibrate their trust in the Al assistant. Participants also appreciated the ability to view notes taken by other analysts because the notes could help them understand the information faster while reducing duplicated work. To calibrate their trust in the note, participants wanted the ability to view who wrote the note (e.g., the Al assistant or another analyst). One participant [VA#2] explained how they might use information about the notetaker (e.g., their experience level) to determine their level of trust in the note. Participants appreciated the ability to view the notes of others but were concerned that there would be confusion about where to write different types of notes. They also felt that it might be overwhelming to identify the notes that could help them interpret their case. Currently, analysts often record information about publications in the notes section for a particular variant in the case. Because some notes may apply to other cases, one participant had experimented with concatenating all notes into a \"master knowledge set.\" However, they found this concatenation unhelpful because the notes were \u201call over the place\" [VA#5] with a mix of repeated and case-specific information."}, {"title": "7 DISCUSSION", "content": "In this study, we examined how generative AI could be used to support knowledge work, focusing on the use of LLM- based generative AI to aid genetic professionals in WGS analyses. Our findings from both study phases highlight that individual and collaborative sensemaking is a challenging aspect of WGS analysis that genetic professionals envision and prioritize for Al support. Many of the current challenges in WGS analysis involve making sense of the growing information about genes and variants. In the needs elicitation phase of the study, participants highlighted issues synthesizing information"}]}