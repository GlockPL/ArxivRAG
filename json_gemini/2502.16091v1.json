{"title": "Privacy-Aware Joint DNN Model Deployment and Partition Optimization for Delay-Efficient Collaborative Edge Inference", "authors": ["Zhipeng Cheng", "Xiaoyu Xia", "Hong Wang", "Minghui Liwang", "Ning Chen", "Xuwei Fan", "Xianbin Wang"], "abstract": "Edge inference (EI) is a key solution to address the growing challenges of delayed response times, limited scalability, and privacy concerns in cloud-based Deep Neural Network (DNN) inference. However, deploying DNN models on resource-constrained edge devices faces more severe challenges, such as model storage limitations, dynamic service requests, and privacy risks. This paper proposes a novel framework for privacy-aware joint DNN model deployment and partition optimization to minimize long-term average inference delay under resource and privacy constraints. Specifically, the problem is formulated as a complex optimization problem considering model deployment, user-server association, and model partition strategies. To handle the NP-hardness and future uncertainties, a Lyapunov-based approach is introduced to transform the long-term optimization into a single-time-slot problem, ensuring system performance. Additionally, a coalition formation game model is proposed for edge server association, and a greedy-based algorithm is developed for model deployment within each coalition to efficiently solve the problem. Extensive simulations show that the proposed algorithms effectively reduce inference delay while satisfying privacy constraints, outperforming baseline approaches in various scenarios.", "sections": [{"title": "INTRODUCTION", "content": "Nowadays, many Artificial Intelligence (AI) applications rely on remote cloud servers to perform complex inference tasks [1]. However, this approach introduces several challenges, including delayed response due to long-distance network transmission, scalability issues arising from high bandwidth consumption, and privacy concerns related to the forwarding and storage of user data over the network [2], [3]. Furthermore, the growing disparity between the limited resource on Mobile Devices (MDs) and the computational demands of AI services presents another significant hurdles [4], [5]. As AI models increase in size and complexity, both storage and computational requirements have escalated, making local processing increasingly difficult [6]. For example, the VGG19 model, with 143 million parameters and a storage requirement of about 600 MB, poses substantial obstacles for execution on resource-constrained MDs [7].\nTo address these challenges, Edge Inference (EI) has emerged as a promising solution, enabling low-delay decision-making by deploying AI inference services on edge servers [8], [9], [10], [11]. This approach involves offloading user requests to edge servers, which collaborate with mobile devices to complete inference tasks. By partitioning Al models and distributing parts of the computation across both edge servers and mobile devices, collaborative inference helps minimize data transmission, alleviate the computational burden on mobile devices, and capitalize on the computational power of edge servers [12], [13].\nDespite its promising potential, collaborative El faces two major challenges:\nResource Constraints: One of the primary obstacles is the limited storage and computational resources available on mobile devices and edge servers, especially in the context of a dynamic and diverse range of user requests [14]. Optimizing model deployment among MDs and edge servers along with user-server association strategies is crucial to overcoming these resource limitations. Model deployment involves determining how Al models are stored and accessed on edge servers, with the goal of reducing inference delays and improving service quality [15]. An effective deployment strategy optimizes model caching, reducing download and transmission times while dynamically allocating resources based on request probability and computational requirements, thus enhancing overall system responsiveness. Similarly, user association strategies focus on assigning user requests to the most suitable edge servers, balancing system load and maximizing resource utilization while minimizing transmission overhead. However, the limited resources of edge servers and the diverse nature of user requests make this optimization complex, requiring careful coordination of both model deployment and user association to fully leverage available resources and provide efficient, delay-minimized inference services.\nPrivacy-Communication-Computation Trilemma: Collaborative inference introduces a trilemma in balancing privacy, communication, and computation. Specifically, model partition directly impacts the computational burden on MDs and edge servers and the communication overhead between them [16], [17]. While partitioning models helps reduce the need for MDs to transmit raw data, by sending only processed feature maps from early layers, it still exposes the system to privacy risks. These feature maps are susceptible to model inversion attacks, which could lead to privacy breaches and the compromise of sensitive data [18], [19]. To highlight this, our experiments in Fig. 1 and 2 measure the privacy leakage during the model inference process using data similarity metrics such as Structural Similarity Index Measure (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Learned Perceptual Image Patch Similarity (LPIPS), which can illustrate the extent of privacy leakage under potential inversion attacks [18], [20]. This creates a fundamental trade-off, deeper local computations enhance privacy (lower SSIM), while shallower partitions reduce local computation at the cost of higher communication overhead and privacy risks. As a result, an effective model partition strategy must not only optimize computational and communication efficiency but also carefully manage privacy concerns.\nBuilding on these insights, this paper aims to strike a balance between minimizing inference delay and safeguarding user privacy, thereby enhancing the overall performance of EI. Specifically, given a edge network composed of multiple inference services based on various Deep Neural Network"}, {"title": "RELATED WORK", "content": "Extensive research efforts have been dedicated to partition-based collaborative edge inference paradigms, wherein DNN models are strategically dissected between edge servers and end-user devices to optimize computational load distribution and minimize communication overhead. For example, foundational researches such as [10], [21], [22] have established methodologies for model partition and inference optimization. For readers interested in a comprehensive overview of existing works, we recommend recent systematic surveys such as [4], [23], [24], [25]. Aligned with the scope of this work, we focus on two critical dimensions of edge-end collaborative inference: 1) model deployment/placement, or caching strategies for efficient DNN inference, and 2) privacy-aware mechanisms to mitigate data leakage risks during collaborative inference. Below, we discuss representative works in these areas.\n2.1 Model Deployment for Efficient DNN Inference\nRecent studies have made significant progress in optimizing Al deployment across edge-cloud environments. [15] develops a Lyapunov-based joint resource management scheme for IIoT systems, co-optimizing model deployment, task offloading, and resource allocation to minimize delay and error penalty while ensuring system stability. [26] introduces JointDNN, a collaborative inference engine that employs shortest path optimization and integer linear programming to enhance performance and energy efficiency between mobile devices and cloud servers. For large-scale deployments, [27] proposes MPDM, a multi-paradigm deployment model that uses heuristic algorithms to optimize accuracy, scale, and cost for deep learning inference services. Resource-constrained scenarios are addressed by [28], which focuses on neural network optimization for edge devices, leveraging logic circuits, in-memory computing, and learning automata algorithms to improve efficiency and accuracy.\nIn the context of edge server deployments, [29] establishes a joint caching and inference framework for foundation models, utilizing a least context algorithm to enhance inference accuracy and reduce system costs. Automation in cloud deployment is advanced by [30]'s AutoDeep framework, which integrates Bayesian Optimization and Deep Reinforcement Learning to optimize cloud configurations and device placement, achieving significant cost reductions through techniques like probing-informed block multiplexing. For heterogeneous edge environments, [31] designs a joint optimization framework for device placement and model partition, employing evolutionary algorithms and dynamic programming to maximize throughput and minimize inference time.\nDynamic co-inference scenarios are further explored by [32], which formulates model placement and online splitting in wireless networks as an optimal stopping problem, proposing algorithms to minimize energy and time costs. [33] addresses multi-task inference in vehicular edge computing through a share-aware joint deployment and task offloading framework, utilizing a time period-aware algorithm to reduce total response time. Finally, [34] investigates energy-optimal DNN placement in UAV-enabled edge networks, developing a Lyapunov-based online algorithm to minimize transmission delay and energy costs while stabilizing data queues.\nIn contrast to these studies that predominantly focus on isolated aspects of performance optimization, our work holistically investigates privacy-aware model deployment, adaptive model partition, and user-edge server association under resource and privacy constraints, a tripartite challenge unaddressed in existing literature, where privacy considerations are either oversimplified or entirely decoupled from system-level coordination.\n2.2 Privacy-Aware Collaborative DNN Inference\nRecent advancements in privacy-preserving deep learning inference for edge and IoT systems have addressed critical challenges in distributed intelligence. [35] introduces DistPrivacy, a distributed feature map allocation methodology that enhances data privacy for IoT surveillance systems through strategic data partition. Building on this foundation, [36] develops an adaptive DNN partition framework that dynamically balances privacy and performance under varying network conditions. The coalition-based approach by [37] optimizes IoT camera-edge node associations, simultaneously addressing privacy preservation, energy efficiency, and multi-view detection performance through game-theoretic optimization.\nFederated learning-based architectures have gained significant attention, with [38] proposing a quality-aware framework that integrates adaptive model splitting and device association to handle resource and data heterogeneity in edge environments. For device-edge co-inference scenarios, [39] designs a deep reinforcement learning-based model splitting mechanism that achieves optimal privacy-computation tradeoffs. Extending distributed inference capabilities, [40] presents RL-DistPrivacy, which employs reinforcement learning to optimize privacy-aware deep inference in latency-sensitive IoT systems.\nRecent innovations focus on enhancing both privacy and reliability: [41] develops a queue-optimized CNN distributed inference method that leverages reinforcement learning to minimize latency while ensuring customer privacy protection. Finally, [42] introduces Salted DNNs, a novel semantic rearrangement technique that uses cryptographic salt to enhance output privacy in split inference without compromising computational efficiency. Collectively, these works significantly advance the state-of-the-art in privacy-aware edge intelligence, yet they predominantly focus on isolated aspects of the privacy-performance trade-off, leaving opportunities for more holistic frameworks that integrate adaptive model deployment optimization, user-server association and comprehensive privacy preservation mechanisms."}, {"title": "SYSTEM MODEL AND PROBLEM FORMULATION", "content": "3.1 System Overview\nAs shown in Fig. 3, we consider a hierarchical edge network composed of a cloud server, multiple edge servers and mobile devices. Within the cloud server, there resides a DNN model library that encompasses a variety of well-trained models designed for inference services. Assuming there are a total of L distinct inference models, denoted as $\\mathcal{L} = \\{1,..., l, ..., L\\}$, and the data size of model l is $D_l$. Furthermore, M edge servers, collectively denoted as $\\mathcal{M} = \\{1, ..., m, ..., M\\}$, can offer limited storage resources and computing capabilities for deploying and executing DNN models, thereby providing inference services for a set $\\mathcal{N} = \\{1, ..., n, ..., N\\}$ of N mobile devices (MDs). In this paper, a sufficiently long system time period is divided into T equal-length time slots, denoted as $\\mathcal{T} = \\{1, ..., t, ..., T\\}$."}, {"title": "Model Deployment and Association Model", "content": "At each time slot t, each MD $n \\in \\mathcal{N}$ generates an inference service request $r_n(t)$, which can be described by a tuple $r_n(t) = \\{p_{l,n}(t), d_{l,n}(t)\\}$, where $p_{l,n}(t)$ represents the probability of MD n requesting model l at time slot t, which is assumed to follow a Zipf-like distribution and can be estimated as:\n\\[p_{l,n}(t) = \\frac{l^{-\\eta_n(t)}}{\\sum_{i=1}^L i^{-\\eta_n(t)}}, \\forall n \\in \\mathcal{N}, l \\in \\mathcal{L} \\tag{1}\\]\nwhere $\\eta_n(t)$ represents the parameter of the Zipf distribution, reflecting the shape of the content popularity distribution. The larger the value of $\\eta_n(t)$, the more concentrated the content popularity. $d_{l,n}(t)$ is the input data size for inference service l, and we assume that $d_{l,n}(t)$ follows a general random distribution within the range of $[d_{min}, d_{max}]$.\nAt each time slot t, each edge server can download part of the models and deploy them locally. Suppose a binary variable $x_{l,m}(t)$ is introduced to indicate whether model l is deployed on edge server $m \\in \\mathcal{M}$ at time slot t. Then, we have\n\\[x_{l,m}(t) = \\begin{cases} 1, & \\text{if model l is deployed on server m} \\\\ 0, & \\text{otherwise.} \\end{cases} \\tag{2}\\]\nAs the storage resource of edge server is limited, the number of models that can be deployed at the server m should also be constrained. Besides, since the service request may vary in different time slots, the model deployment strategy needs to be adjusted to update the model deployment status.\nBesides, let the binary variable $y_{m,n}(t)$ represent the user association status between MD n and server m at time slot t. Then, we have\n\\[y_{m,n}(t) = \\begin{cases} 1, & \\text{if MD n is associated with server m} \\\\ 0, & \\text{otherwise.} \\end{cases} \\tag{3}\\]\nIn this paper, we assume that each MD can only be served with only one server at each time slot."}, {"title": "Model Partition and Privacy Evaluation Model", "content": "If MD n associates with server m for model l, the edge server may need to partition the model into two parts, and deploy one part on the server, while the other on the MD, with a partition strategy $z_{l,m,n}(t)$. Assuming model l is a chained-DNN with $A_l$ layers, a model partition strategy $z_{l,m,n}(t)$ can be represented as $z_{l,m,n}(t) \\in \\mathcal{A}_l = \\{0,1, 2, ..., A_l\\}^1$. Specifically, if $z_{l,m,n}(t) = 0$ means that no model partition is deployed on the MD, and inference is conducted entirely on the edge server; If $z_{l,m,n}(t) = A_l$, it signifies that the entire model is deployed on the MD for full local processing.\nIn this paper, we use the evaluated SSIM value at the edge server to quantify the probability of data privacy leakage at different model layers. Specifically, when the data is directly sent to the edge server and SSIM value equals to 1, it is considered that the data privacy is exposed with probability 1. Conversely, when inference is conducted solely on the local device, it indicates data privacy is compromised with probability 0. We denote the SSIM value as $s_l(z_{l,m,n}(t))$, simplified as $s_{l,m,n}(t)$, which signifies that it is determined by $z_{l,m,n}(t)$.\nThen, under any given model partition strategy $z_{l,m,n}(t)$, the total amount of data that may lead to privacy leakage during the inference process of MD n can be represented as\n\\[\\Upsilon_n(t) = d_{l,n}(t)s_{l,m,n}(t) \\tag{4}\\]\nIn addition, it is necessary to conduct extensive preliminary experiments on the dataset using the corresponding models, to obtain the SSIM value. In particular, is there a correlation between the SSIM value and the number of model layers? If an approximate fitting function can be derived, it could be used to evaluate the SSIM value, thereby reducing the workload of preliminary experiments. To this end, we attempt to use a series of LeNet models and VGG models, plotting scatter plots and fitting curves of SSIM values against the model layer index. The results are presented in Fig. 4.\nAs shown in the figure, the SSIM values of both the LeNet and VGG models exhibit similar trends with respect to the model layer index. Through curve fitting, these trends can be approximated by a function of the following form:\n\\[s = \\omega_1 / (1 + \\exp(-\\omega_2(\\text{index} - \\omega_3))) + \\omega_4 \\tag{5}\\]\nwhere $\\omega_1, \\omega_2, \\omega_3$ and $\\omega_4$ are constants. For example, $\\omega_1 = 0.9031, \\omega_2 = -1.6683, \\omega_3 = 4.9119, \\omega_4 = 0.0983$ for LeNet models, while $\\omega_1 = 0.6957, \\omega_2 = -0.6047, \\omega_3 = 6.7718, \\omega_4 = 0.3371$ for VGG models. It is worth noting that these values may vary across different datasets, but the general trend remains consistent. Furthermore, we were unable to effectively fit this trend for the ResNet models. This is because, in ResNet, the model is partitioned based on model blocks, causing the SSIM values to drop sharply and then stabilize, as shown in Fig. 1. Therefore, for other models, if a similar trend can be observed in a pre-experiment with one model, the fitted function can be used to approximate the SSIM values of other variants in the series. This approach helps reduce the workload of pre-experimentation to some extent."}, {"title": "Inference Delay Model", "content": "Depending on the joint model deployment, user associations, and model partition strategies, the delay throughout the entire inference process will vary under different scenarios. This section presents how to calculate the inference delay.\nWe assume at the initial time slot t = 0, none of the models have been deployed (i.e., $x_{l,m} = 0$). At the beginning of time slot t, the requested models need to be downloaded from the cloud server to the edge server. Denote the model downloading delay as $\\tau_{l,m}^d(t)$, which can be calculated as:\n\\[\\tau_{l,m}^d(t) = \\begin{cases} D_l / R_m, & x_{l,m}(t) = 1 \\& x_{l,m}(t - 1) = 0 \\\\ 0, & \\text{otherwise.} \\end{cases} \\tag{6}\\]\nwhere $R_m$ is the transmission rate from the cloud server to the edge server m over the wired link.\nAssuming that MD n can obtain service based on model l from the associated edge server (i.e., $x_{l,m}(t) = 1$ and $y_{m,n}(t) = 1$), with the model partition strategy $z_{l,m,n}(t)$, then the model partition that needs to be downloaded from the edge server first, and the dowload delay can be calculated as:\n\\[\\tau_{m,n}^d(t) = \\frac{D_{md}(z_{l,m,n}(t))}{R_{n,m}(t)} \\tag{7}\\]\nwhere $D_{md}(z_{l,m,n}(t))$ is the data size of the model partition deployed at the MD. $R_{n,m}(t)$ is the transmission rate between MD n and its associated edge server m, which can be calculate as:\n\\[R_{m,n}(t) = B_{n,m}(t) \\log_2\\left(1 + \\frac{p_{n,m}d_{n,m}(t)}{n_0 B_{n,m}(t)}\\right) \\tag{8}\\]\nwhere $B_{n,m}(t) = B_m(t)/N_m(t)$ denotes the bandwidth allocated to the associated MDs by server m, $B_m(t)$ is total available bandwidth, and $N_m(t) = \\sum_{n=1}^N y_{m,n}(t)$ is the total number of associated MDs. $p_{n,m}$ is the transmit power allocated to MD n, $d_{n,m}(t)$ is the distance between server m and MD n. $n_0$ is the power spectral density of the additional Gaussian white noise.\nThen, the inference process is initiated by MD n, the local computation delay on the MD can be represented as:\n\\[\\tau_{l,m,n}^c(t) = \\frac{d_{l,n}(t)F_{md}(z_{l,m,n}(t))}{f_n} \\tag{9}\\]\nwhere $F_{md}(z_{l,m,n}(t))$ represents the unit computational load for inference that the MD undertakes under the partitioning strategy $z_{l,m,n}(t)$, and $f_n$ is the computation capability of MD n at time slot t.\nOnce the inference is completed on the MD side, the intermediate results (feature maps) output by the partitioned layer are sent to the server. The required transmission delay can be calculated as:\n\\[\\tau_{l,m,n}^{tr}(t) = \\frac{I(z_{l,m,n}(t))}{R_{n,m}(t)} \\tag{10}\\]\nwhere $I(z_{l,m,n}(t))$ represents the data size of the intermediate results output by the partitioned layer under partition strategy $z_{l,m,n}(t)$, and $R_{m,n}(t)$ is the uplink transmission rate.\nAfter the server receiving the intermediate output results, it will continue to perform inference on the remaining model partition, and the processing delay is:\n\\[\\tau_{l,m,n}^s(t) = \\frac{d_{l,n}(t)F^s(z_{l,m,n}(t))}{f_{n,m}(t)} \\tag{11}\\]\nwhere $F^s(z_{l,m,n}(t))$ represents the unit computational load that the server side undertakes under the partition strategy $z_{l,m,n}(t)$, and $f_{n,m}(t)$ is the computational resource allocated to the MD n for inference service l. Similarly, we assume that the edge server equally allocate its computational resources among the associated MDs:\n\\[f_{n,m}(t) = \\frac{f_m(t)}{N_m} \\tag{12}\\]\nwhere $f_m(t)$ is the total computational resources available of edge server m at time slot t, and $N_m$ denote the total number of MDs requesting resource allocation.\nOnce edge servers have completed the inference and obtained the results, they can return these outcomes to the MDs. However, due to the relatively small size of the data associated with the inference results, the transmission latency for this part can be considered negligible, the total inference delay experienced in the aforementioned process can be calculated as:\n\\[\\tau_{l,m,n}(t) = x_{l,m}(t)y_{m,n}(t) (\\tau_{l,m}^d(t) + \\tau_{m,n}^d(t) + \\tau_{l,m,n}^c(t) + \\tau_{l,m,n}^{tr}(t) + \\tau_{l,m,n}^s(t)) \\tag{13}\\]\nHowever, edge server m selected by MD n may decide not to deploy model l (i.e., $x_{l,m}(t) = 0$ and $y_{m,n}(t) = 1$). In this case, we assume that the edge server will forward the request to the cloud server, and the MD will experience a considerable delay $\\tau_{extra}(t)$ (much larger than the edge and local inference delay) due to the long distance transmission. For example, the extra delay can include the transmission delays occurring when MDs upload inference data to cloud server after model deployment is completed, as well as the delay in returning inference results. Besides, we no longer consider data privacy in this scenario, as cloud services may offer more resource-intensive privacy protection protocols, which are beyond the scope of this paper.\nTherefore, the final inference delay when MD n associated with edge server m under arbitrary model partition strategy $z_{l,m,n}$ can be expressed as:\n\\[\\mathcal{T}_n(t) = \\tau_{l,m,n}(t) + y_{m,n}(t) (1 - x_{l,m}(t)) \\tau_{extra}(t) \\tag{14}\\]\nFinally, the total inference delay of all MDs can be denoted as:\n\\[\\mathcal{T}(t) = \\sum_{n=1}^N \\mathcal{T}_n(t) \\tag{15}\\]"}, {"title": "Problem Formulation and Hardness", "content": "This paper aims to enhance the inference delay performance at the systemic level while simultaneously reducing the risk of inference data privacy leakage, thereby achieving a secure and efficient inference service framework. Furthermore, due to the stochastic and dynamic of inference service requests, optimizing any individual time slot alone cannot guarantee the long-term stability of the system. Therefore, this paper decides to minimize the long-term average inference delay under sustained privacy leakage constraints by jointly optimizing the model deployment strategy $\\mathcal{X}(t) = \\{x_{l,m}(t) | l \\in \\mathcal{L}, m \\in \\mathcal{M}\\}$, association strategy $\\mathcal{Y}(t) = \\{y_{m,n}(t) | m\\in \\mathcal{M}, n \\in \\mathcal{N} \\}$, and model partition strategy $\\mathcal{Z}(t) = \\{z_{l,m,n}(t) | l \\in \\mathcal{L}, m \\in \\mathcal{M}, n \\in \\mathcal{N}\\}$. The formulated optimization problem can be expressed as follows:\n\\begin{aligned}\n&\\textbf{P1:} \\quad \\min_{\\mathcal{X}(t),\\mathcal{Y}(t),\\mathcal{Z}(t)} \\quad \\underset{T \\to \\infty}{\\lim} \\frac{1}{T} \\sum_{t=0}^{T-1} \\left[\\mathcal{T}(t)\\right] \\\\\n& \\textbf{s.t.} \\quad \\textbf{C1}: x_{l,m}, y_{m,n}(t) \\in \\{0,1\\}, \\forall l \\in \\mathcal{L}, \\forall m \\in \\mathcal{M}, \\forall n \\in \\mathcal{N} \\\\\n& \\textbf{C2}: z_{l,m,n}(t) \\in \\{0, 1, 2, ..., A_l \\}, \\forall l \\in \\mathcal{L}, \\forall m \\in \\mathcal{M}, \\forall n \\in \\mathcal{N} \\\\\n& \\textbf{C3}: \\sum_{l=1}^L x_{l,m}(t) D_l \\leq C_m(t) \\\\\n& \\textbf{C4}: D_{md}(z_{l,m,n}(t)) \\leq D_l \\\\\n& \\textbf{C5}: \\sum_{m=1}^M y_{m,n}(t) = 1 \\\\\n& \\textbf{C6}: \\sum_{n=1}^N f_{n,m}(t) = f_m(t) \\\\\n& \\textbf{C7}: \\sum_{n=1}^N B_{n,m}(t) = B_m(t) \\\\\n& \\textbf{C8}: \\underset{T \\to \\infty}{\\lim} \\underset{T \\to \\infty}{\\lim} \\left[\\Upsilon_n(t)\\right] < \\bar{\\Upsilon}_n \\tag{16}\n\\end{aligned}\nwhere constraint C3 ensures that the total data size of model deployed on edge server m does not exceed its storage capacity $C_m(t)$. Constraint C4 ensures that the model partition deployed on the MD side is not larger than the entire model. Constraint C5 restricts that the MD can and must be associated with exactly one edge server within a single time slot. Constraint C6 and C7 ensure that the total computational resources and bandwidth allocated by the edge server do not exceed its maximum available capacity. Constraint C8 ensures that the each MD's long-term average data privacy leakage does not exceed the threshold $\\bar{\\Upsilon}_n$.\nIt is clear that the problem constituted above is a long-term constrained optimization problem and it is straightforward to obtain that it is an NP-hard problem. This is because the impact of model partition on the problem is difficult to express in terms of any deterministic continuous function. Next, we also prove that problem $\\textbf{P1}$ is an NP-hard problem even in a single time slot. i.e., without constraint C7, and we give the following Theorem 1.\nTheorem 1. The proposed problem (16) is NP-hard in a single time slot."}, {"title": "Lyapunov-Based Problem Transformation, Decomposition, and Solution Design", "content": "In this section, we initially transform the long-term optimization problem into a single-time-slot optimization problem based on Lyapunov optimization theory, thereby mitigating the impact of long-term constraint conditions. Subsequently, considering the distinct impacts of three different optimization strategies on the optimization objective, we decouple the problem for optimization. We first formulate the association problem between the MD and edge server as a coalition game problem. Then, we address the model deployment and partition optimization problem under a given coalition structure. Finally, through iterative optimization, we obtain an efficient suboptimal solution for the original problem, and analyze the properties of the proposed algorithms.\n4.1 Lyapunov-Based Problem Transformation\nIn problem (16), constraint C7 is used to stabilize the time-averaged maximum data privacy leakage. However, this approach requires complete information of the system for every time slot, which is not feasible to acquire in advance in real-world systems. To address this issue, we introduce accumulated data privacy leakage in Definition 1.\nDefinition 1. Let $\\Xi(t + 1)$ denote the accumulated data privacy leakage that exceeds the data privacy threshold over t time slots, which can be calculated as\n\\[\\Xi(t + 1) = \\left[\\Xi(t) + \\sum_{n=1}^N \\left(\\Upsilon_n(t) - \\bar{\\Upsilon}_n\\right)\\right]^+ \\tag{17}\\]\nwhere $\\Xi(0) = 0$ denote the initial value of the accumulated data privacy leakage. A higher value of $\\Xi(t)$ indicates that the data privacy leakage caused by inappropriate strategy-making exceeds the time-averaged privacy constraint. Based on Definition 1, we can transform the long-term constraint C7 in problem (16) as follows\n\\[\\underset{T \\to \\infty}{\\lim} \\frac{1}{T} \\sum_{t=0}^{T-1} E \\left[\\Xi(t)\\right] \\leq 0 \\tag{18}\\]\nThen, we introduce the most widely applied Lyapunov function $L(\\Xi(t)) = \\frac{1}{2}\\Xi^2(t)$, to measure the satisfaction status of the long-term privacy constraint. Thus, a smaller value of $L(\\Xi(t))$ means a better data privacy satisfaction. To ensure the value of $L(\\Xi(t))$ is small enough to meet long-term privacy constraints, we define $\\Delta(\\Xi(t))$ below by introducing Lyapunov drift function:\n\\[\\Delta(\\Xi(t)) \\triangleq E[L(\\Xi(t + 1)) - L(\\Xi(t)) | \\Xi(t)] \\tag{19}\\]\nBy expanding the above expression further, we can obtain that:\n\\[\\begin{aligned} \\Delta(\\Xi(t)) &= E [\\Xi^2(t + 1) - \\Xi^2(t) | \\Xi(t)] \\\\\n&= E \\left[ \\left(\\Xi(t) + \\sum_{n=1}^N (\\Upsilon_n(t) - \\bar{\\Upsilon}_n) \\right)^2 - \\Xi^2(t) | \\Xi(t) \\right] \\\\\n&= E \\left[ \\Xi \\sum_{n=1}^N (\\Upsilon_n(t) - \\bar{\\Upsilon}_n) | \\Xi(t) \\right] + \\frac{1}{2} E \\left[\\sum_{n=1}^N (\\Upsilon_n(t) - \\bar{\\Upsilon}_n)^2 | \\Xi(t) \\right] \\tag{20} \\end{aligned}\\]\nNext, within the Lyapunov drift-penalty framework, we incorporate equation (20) into the optimization objective to strike a balance between minimizing inference delay and ensuring long-term privacy constraints, as shown below:\n\\[E \\left[\\tau(t) | \\Xi(t) \\right] + \\Delta(\\Xi(t)) \\tag{21}\\]\nAccording to (19), the calculation of $\\Delta(\\Xi(t))$ requires the information of $L(\\Xi(t + 1))$, which will not be available in each time slot t. Thus, we tend to obtain the upper bound of (21) that can be calculated only based on the available information in time slot t. Firstly, it can be easily deduced that $\\Upsilon_n(t)$ will be no lower than 0, and $\\sum_{n=1}^N (\\Upsilon_n(t) - \\bar{\\Upsilon}_n)$ will be no higher than $\\sum_{n=1}^N \\bar{\\Upsilon}_n$. Then, define a constant $\\Theta = \\sum_{n=1}^N \\bar{\\Upsilon}_n$, the upper bound of $\\Delta(\\Xi(t))$ can be represented as:\n\\[\\Delta(\\Xi(t)) \\leq \\Xi(t) \\cdot E \\left[ \\sum_{n=1}^N (\\Upsilon_n(t) - \\bar{\\Upsilon}_n) | \\Xi(t) \\right] + \\Theta \\tag{22}\\]\nBased on (22), the upper bound of (21) can be deduced as:\n\\[E \\left[\\tau(t) | \\Xi(t) \\right] + \\Delta(\\Xi(t)) \\leq E \\left[\\tau(t) | \\Xi(t) \\right] - \\Xi(t) \\cdot E \\left[ \\sum_{n=1}^N (\\bar{\\Upsilon}_n - \\Upsilon_n(t)) | \\Xi(t) \\right] + \\Theta \\tag{23}\\]\nBased on (23), Problem $\\textbf{P1}$ can be approximated only rely on the information in each current time slot. Thus, it can be solved by finding the solution to the following problem $\\textbf{P2}$ that minimize the upper bound in each time slot over T:\n\\begin{aligned}\n&\\textbf{P2:} \\quad \\underset{\\mathcal{X}(t),\\mathcal{Y}(t),\\mathcal{Z}(t)}{\\min} \\quad \\alpha \\cdot \\tau(t) - \\Xi(t) \\cdot \\sum_{n=1}^N (\\bar{\\Upsilon}_n - \\Upsilon_n(t)) + \\Theta \\\\\n& \\textbf{s.t.} \\quad \\text{Constraints: (C1) - (C7)} \\tag{24}\n\\end{aligned}\nwhere $\\alpha$ is a positive constant that is used to achieve the tradeoff between delay minimization and satisfaction status of the long-term privacy constraints."}, {"title": "Coalition Formation Game Model for Edge Server Association", "content": "Using the Lyapunov optimization theory, we previously reformulated the problem into a single-slot optimization problem. However, as this problem is NP-hard, finding an optimal solution at a larger scale is exceedingly time-consuming. Therefore, we aim to find an efficient suboptimal solution for the above problem. Furthermore, by analyzing the interdependencies among the three optimization strategies, it can be discerned that the model partition strategy depends solely on the relationship between a specific edge server and its"}]}