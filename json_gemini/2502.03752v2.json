{"title": "PRISM: A Robust Framework for Skill-based Meta-Reinforcement Learning with Noisy Demonstrations", "authors": ["Sanghyeon Lee", "Sangjun Bae", "Yisak Park", "Seungyul Han"], "abstract": "Meta-reinforcement learning (Meta-RL) facilitates rapid adaptation to unseen tasks but faces challenges in long-horizon environments. Skill-based approaches tackle this by decomposing state-action sequences into reusable skills and employing hierarchical decision-making. However, these methods are highly susceptible to noisy offline demonstrations, resulting in unstable skill learning and degraded performance. To overcome this, we propose Prioritized Refinement for Skill-Based Meta-RL (PRISM), a robust framework that integrates exploration near noisy data to generate online trajectories and combines them with offline data. Through prioritization, PRISM extracts high-quality data to learn task-relevant skills effectively. By addressing the impact of noise, our method ensures stable skill learning and achieves superior performance in long-horizon tasks, even with noisy and sub-optimal data.", "sections": [{"title": "1. Introduction", "content": "Reinforcement Learning (RL) has achieved significant success in domains such as game environments and robotic control (Mnih et al., 2015; Andrychowicz et al., 2020). However, it struggles to adapt quickly to new tasks. Meta-RL addresses this limitation by enabling rapid adaptation to unseen tasks through meta-learning how policies solve problems (Duan et al., 2016; Finn et al., 2017). Among various approaches, context-based meta-RL stands out for its ability to represent similar tasks with analogous contexts and leverage this information in the policy, facilitating quick adaptation to new tasks (Rakelly et al., 2019; Zintgraf et al., 2019). Notably, PEARL (Rakelly et al., 2019) has been widely studied for its high sample efficiency, achieved through off-policy learning, which allows for the reuse of previous samples. Despite these strengths, existing meta-RL methods face challenges in long-horizon environments, where extracting meaningful context information becomes difficult, hindering effective learning.\nSkill-based approaches address these challenges by breaking down long state-action sequences into reusable skills, facilitating hierarchical decision-making and enhancing efficiency in complex tasks (Pertsch et al., 2021; 2022; Shi et al., 2023). Among these, SPiRL (Pertsch et al., 2021) defines skills as temporal abstractions of actions, employing them as low-level policies within a hierarchical framework to achieve success in long-horizon tasks. SiMPL (Nam et al., 2022) builds on this by extending skill learning to meta-RL, using offline expert data to train skills and a context-based high-level policy for task-specific skill selection. Despite these advancements, such methods are highly susceptible to noisy offline demonstrations, which can destabilize skill learning and reduce reliability. In real-world settings, noise often stems from factors like infrastructure aging or environmental perturbations, underscoring the importance of addressing this challenge (Brys et al., 2015; Chae et al., 2022; Yu et al., 2024).\nWhile other RL domains have explored methods to handle noisy demonstrations (Sasaki & Yamashina, 2020; Mandlekar et al., 2022), skill-based learning has largely remained underexplored in this context. To tackle this gap, we propose Prioritized Refinement for Skill-Based Meta-RL (PRISM), a robust framework that incorporates two key contributions: (1) A prioritized skill refinement framework that leverages an exploration policy to discover useful trajectories near noisy offline data, extracting high-quality data through prioritization and employing disentangled skill learning to integrate skills from both online and offline datasets. (2) Maximum return relabeling, a novel technique for evaluating noisy offline trajectories by relabeling their returns based on task relevance, ensuring the selection of high-quality data. By dynamically balancing and prioritizing data from online and offline datasets, PRISM ensures stable and effective skill learning even in noisy environments. These innovations significantly enhance the robustness and generalizability of skill-based meta-RL, delivering reliable performance in real-world noisy scenarios."}, {"title": "2. Related Works", "content": "Skill-based Reinforcement Learning: Skill-based RL has gained traction for tackling complex tasks by leveraging temporally extended actions. Researchers have proposed information-theoretic approaches to discover diverse and predictable skills (Gregor et al., 2016; Eysenbach et al., 2018; Achiam et al., 2018; Sharma et al., 2019), with recent work improving skill quality through additional constraints and objectives (Strouse et al., 2022; Park et al., 2022; 2023; Hu et al., 2024). In offline scenarios, approaches focus on learning transferable behavior priors and hierarchical skills from demonstration data (Pertsch et al., 2021; 2022; Xu et al., 2022; Shi et al., 2023). Building upon these foundations, various skill-based meta-RL approaches have been developed, from hierarchical and embedding-based methods (Nam et al., 2022; Chien & Lai, 2023; Cho & Sun, 2024) to task decomposition strategies (Yoo et al., 2022; He et al., 2024) and unsupervised learning frameworks (Gupta et al., 2018; Jabri et al., 2019; Shin et al., 2024).\nHierarchical Frameworks: Hierarchical approaches in RL have been pivotal for solving long-horizon tasks, where various methods have been proposed including goal-conditioned learning (Levy et al., 2019; Li et al., 2019; Gehring et al., 2021), and option-based frameworks (Bacon et al., 2017; Riemer et al., 2018; Barreto et al., 2019; Araki et al., 2021). Recent advances in goal-conditioned RL have focused on improving sample efficiency (Robert et al., 2024), offline learning (Park et al., 2024), and robust state representations (Yin et al., 2024). The integration of hierarchical frameworks with meta-RL has shown significant potential for rapid task adaptation and complexity handling (Frans et al., 2018; Fu et al., 2020a; 2023). Recent work has demonstrated that hierarchical architectures in meta-RL can provide theoretical guarantees for learning optimal policies (Chua et al., 2023) and achieve efficient learning through transformer-based architectures (Shala et al., 2024).\nRelabeling Techniques for Meta-RL: Recent developments in meta-RL have introduced various relabeling techniques to enhance sample efficiency and task generalization (Pong et al., 2022; Jiang et al., 2023). Goal relabeling approaches have extended hindsight experience replay to meta-learning contexts (Packer et al., 2021; Wan et al., 2021), enabling agents to learn from failed attempts. For reward relabeling, model-based approaches have been proposed to relabel experiences across different tasks (Mendonca et al., 2020), improving adaptation to out-of-distribution scenarios. Beyond these categories, some methods have introduced innovative relabeling strategies using contrastive learning (Yuan & Lu, 2022; Zhou et al., 2024) and metric-based approaches (Li et al., 2020) to create robust task representations in offline settings."}, {"title": "3. Background", "content": ""}, {"title": "3.1. Meta-Reinforcement Learning Setup", "content": "In meta-RL, each task T follows a task distribution p(T) and is defined as a Markov Decision Process (MDP) environment MT = (S, A, RT, PT, \u03b3), where S \u00d7 A represents the shared state-action space, RT denotes the reward function, PT is the state transition probability, and \u03b3 is the discount factor. At each time t, the agent selects an action at using the policy \u03c0, receives a reward rt = RT (st, at), and transitions to the next state st+1 ~ PT (\u00b7|st, at). The objective of meta-RL is to train a policy \u03c0 to maximize the return G = \u2211t\u03b3trt for the training task set Mtrain while enabling the policy to rapidly adapt to unseen test task set Mtest, where Mtrain \u2229 Mtest = \u2205."}, {"title": "3.2. Offline Dataset and Skill Learning", "content": "To address long-horizon tasks, skill learning from an offline dataset Boff := {\u03c40:H} is considered, which comprises sample trajectories \u03c4t:t+k := (st, at,\u00b7\u00b7\u00b7, st+k) without reward information, where H is the episode length. The dataset Boff is typically collected through human interactions or pretrained policies. Among various skill learning methods, SPiRL (Pertsch et al., 2021) focuses on learning a reusable low-level policy \u03c0\u03b9, using q(\u00b7|\u03c4t:t+H\u2082) as a skill encoder to extract the skill latent z by minimizing the following loss function:\n$E_{\\tau_{t:t+H_s}\\sim B_{off}, z\\sim q(\\tau_{t:t+H_s})}[L(\\pi_l, q, p, z)]$,\nwhere\n$L(\\pi_l, q, p,z) := \\sum_{k=t}^{t+H_s-1} log \\pi_l(a_k|s_k,z) + \\alpha_{ld}D_{KL}(q||N(0, 1)) + D_{KL}([q]||p)$, Hs is the skill length, \u03b1ld is the coefficient for KL divergence (KLD) DKL, [\u00b7] is the stop gradient operator, and N(\u03bc, \u03a3) represents a Normal distribution with mean \u03bc and covariance matrix I. Here, p(z|st) is the skill prior to obtain the skill distribution z for a given state st directly. Using the learned skill policy \u03c0\u03b9, the high-level policy \u03c0\u03b7 is trained within a hierarchical framework using standard RL methods."}, {"title": "3.3. Skill-based Meta-Reinforcement Learning", "content": "SiMPL (Nam et al., 2022) integrates skill learning into meta-RL by utilizing an offline dataset of expert demonstrations across various tasks. The skill policy \u03c0\u03b9 is trained via SPiRL, while a task encoder qe extracts the task latent eT ~ qe using the PEARL (Rakelly et al., 2019) framework, a widely-used meta-RL method. During meta-training, the high-level policy \u03c0\u03b7(z|s, eT) selects a skill latent z and executes the skill policy \u03c0\u03b9(a|s, z) for H, time steps, optimizing \u03c0\u03b7 to maximize the return for each task Tas:\n$min_{\\pi_{\\eta}} E_{\\tau \\sim B_T, e_t \\sim q_e(c_t)}[C_{RL}(\\tau_h) + \\alpha_d D_{KL}(T_h || P)]$,\nwhere \u03b1ld is the KL divergence coefficient, cT represents the contexts of high-level trajectories Th := (s0, a0, \u03a3H\u22121t=0 rt,\u00b7\u00b7\u00b7, sH, zHs,t=H\u22121 rt,\u00b7\u00b7\u00b7) for task T, CRL denotes the RL loss for \u03c0\u03b7, and B = {Th} is the high-level buffer that stores Tf for each T\u2208 Mtrain. Here, the reward sums \u2211(k+1)H\u22121t=kH rt are obtained via environment interactions of at ~ \u03c0\u03b9(\u00b7|st, zkH) for t = kH,\u00b7\u00b7\u00b7, (k+1)H \u2212 1 with k = 0, \u2026\u2026\u2026. During meta-test, the high-level policy is adapted using a limited number of samples, showing good performance on long-horizon tasks."}, {"title": "4. Methodology", "content": ""}, {"title": "4.1. Motivation: Challenges in Direct Skill Learning with Noisy Demonstrations", "content": "Existing skill-based meta-RL methods, as discussed in Section 3, rely on offline datasets assumed to contain expert demonstrations. However, real-world datasets often suffer from action noise due to aging robots, environmental perturbations, or sensor errors. Unlike online setups that adapt through re-training, static offline datasets are vulnerable to noise. This significantly hinders skill learning, particularly in long-horizon tasks with cumulative errors and manipulation tasks requiring precision. Fig. 2(a) compares skills learned from expert and noisy datasets for a microwave-opening task in the Kitchen environment. Skills from the expert dataset successfully complete the task, while those from noisy demonstrations fail to grasp or open the microwave. Fig. 2(b) shows that SiMPL, a skill-based meta-RL method, performs poorly on test tasks when relying on skills learned from noisy data compared to expert data. This stems from existing methods learning indiscriminately from offline datasets, regardless of trajectory quality. To tackle the challenges posed by noisy offline datasets, we propose prioritized skill refinement, integrating online exploration and maximum return relabeling to dynamically identify and refine task-relevant skills. The following sections detail how the proposed methods leverage these components to ensure robust skill learning for meta-RL under noisy conditions."}, {"title": "4.2. Prioritized Skill Refinement with Exploration", "content": "In this section, we introduce the prioritized refinement for skill-based meta-RL (PRISM) framework to address the limitations of traditional skill learning, which relies heavily on offline datasets often containing noisy and irrelevant trajectories, as shown in Fig. 2(a). To tackle this inefficiency, PRISM incorporates an exploration policy \u03c0exp(a|st, i) for each training task Ti, where i = 1,\u2026\u2026, NT, train, and NT,train denotes the number of training tasks in Mtrain. While research on improving exploration capabilities has been conducted across various domains (Ermolov & Sebe, 2020; Han & Sung, 2021; Chu et al., 2024; Jo et al., 2024), we utilize intrinsic motivation techniques (Burda et al., 2018) to identify task-relevant trajectories by performing exploration near the noisy offline dataset initially. However, the abundance of irrelevant trajectories in the noisy offline dataset can hinder efficient learning.\nTo address this, PRISM introduces two key buffers: the exploration buffer Bexp and the prioritized online buffer Bon. The exploration buffer Bexp stores all trajectories collected by the exploration policy, providing comprehensive samples necessary for RL training, defined as Bexp := {\u03c4exp}, where \u03c4exp = (s0, a0, r0, \u2026\u2026\u2026, sH) represents an exploration trajectory, and a ~ \u03c0exp(\u00b7|st, i) for t = 0,\u2026\u2026, H. The prioritized online buffer Bon, initialized with the offline dataset Boff, selectively retains high-quality trajectories based on their returns, defined as Bon := {\u03c4high}, where \u03c4high are trajectories prioritized by their highest returns G = \u2211H\u22121t=0 rt. These high-return trajectories include both exploration trajectories \u03c4exp collected by the exploration policy and low-level trajectories \u03c4\u1ec9 generated by the low-level policy \u03c0\u03b9(a|st, z) during the execution of the high-level policy \u03c0\u03b7, where z ~ \u03c0\u03b7(\u00b7|st, eTi) and eTi ~ qe.\nBy filtering and retaining only the most useful trajectories, Bon effectively guides the exploration policy, surpassing the noisy offline buffer in efficiency and utility, as inspired by self-supervised learning approaches (Xin et al., 2020; Kim et al., 2024). The exploration policy \u03c0exp is subsequently updated to minimize the following loss function, enabling it to focus on high-quality samples stored in Bon and discover additional task-relevant action sequences:\n$min_{\\pi_{exp}} \\sum_{i}^{N_{T,train}} E_{\\tau_{exp} \\in B_{exp} \\cup B_{on}}[C_{kld}(\\pi_{exp})] + E_{B_{on}}[D_{KL}(\\alpha || \\pi_{exp})$,\nwhere $C_{kld}$ is the KLD coefficient for $\\pi_{exp}$, and $\\alpha$ denotes the action data distribution derived from $B_{on}$. This setup ensures that the exploration policy focuses on task-relevant trajectories, starting near the offline dataset since the prioritized online buffer is initialized with it. Over time, as high-quality trajectories accumulate in the online buffer, the exploration policy progressively improves its ability to discover better paths and task-relevant actions.\nFurthermore, PRISM utilizes both the prioritized online buffer and the offline dataset to train the skill encoder q, skill prior p, and low-level policy \u03c0\u03b9, following the SPiRL framework (Pertsch et al., 2021). The refined low-level policy \u03c0\u03b9 is shared across tasks, while the high-level policy \u03c0\u03b7 leverages these refined skills to perform skill-based meta-RL within the hierarchical structure of SiMPL (Nam et al., 2022). The overall structure of PRISM is illustrated in Fig. 3. In this process, skills are iteratively refined using high-quality samples from the prioritized online buffer, enhancing the high-level policy's ability to improve task-solving performance on both training and test tasks."}, {"title": "4.3. Maximum Return Relabeling for Skill Prioritization", "content": "The proposed PRISM framework trains the low-level policy \u03c0\u03b9 by leveraging both the offline dataset Boff and the prioritized online buffers Bon for each training task Ti. While Bon provides high-quality trajectories tailored to training tasks, relying solely on it can lead to overfitting and poor generalization. Conversely, Boff includes essential trajectories for generalization but also contains noisy and irrelevant data, as illustrated in Fig. 1. To balance these, PRISM combines both Boff and Bon to effectively train \u03c0\u03b9.\nHowever, as Boff often includes noisy and irrelevant trajectories, uniform sampling from it can unnecessarily expand the skill space and hinder the high-level policy \u03c0\u03b7 from efficiently identifying task-relevant skills. To address this, PRISM employs maximum return relabelling, a method that evaluates the importance of trajectories in Boff by assigning hypothetical returns based on their task relevance. This prioritization ensures only the most useful trajectories contribute to skill learning. To compute the hypothetical return for each trajectory, PRISM trains a reward model \u0154(st, at, i) for each training task T. The reward model is optimized using the following loss function:\n$E_{(s_t,a_t,i)\\sim B_{exp} \\cup B_{on}} [(R(s_t, a_t, i) - r_t)^2]$,\nwhere the ground truth rewards r\u1ec9 are derived from the exploration buffers Bexp and prioritized online buffers Bon for all tasks. The trained reward model is used to assign hypothetical returns \u2211t\u03b3t\u0154(st, at, i), and the maximum return \u011c for each trajectory \u03c4 \u2208 Boff is then calculated as:\n$\\hat{G}(\\tau) := max_{t} {\\sum_{t'}^{T} \\gamma^{t'}R(s_t, a_t, i)}$.\nHere, & captures the highest return among all training tasks for a given trajectory 7, ensuring that only trajectories with significant potential for solving training tasks are prioritized. Using the estimated \u011c, we employ a prioritization method to select the most important trajectories for training the skill policy \u03c0\u03b9, skill encoder q, and skill prior p. The loss function Lskill (\u03c0\u03b9, q, p), modified from Eq. (1) as:\n$L_{skill}(\\pi_l, q, p) :=(1 - \\beta)E_{\\tau\\sim P_{B_{on}}}, [L(\\pi_l, q, p, z)]$\n$+$\\beta\\frac{1}{N_{T,train}}\\sum_{i}E_{(\\tau_{t:t+H_s},a_{t:t+H_s})\\sim B_{off}, z \\sim q(\\tau)} [L(\\pi_l, q, P, z)]$,\nwhere $L(\\pi_l, q, p,z) := \\sum_{k=t}^{t+H_s-1} log \\pi_l(a_k|s_k,z) + \\alpha_{ld}D_{KL}(q||N(0, I)) + D_{KL}([q]||p)$ is the skill loss defined in Eq. (1), and $P_{B_{off}}(\\tau) = Softmax(\\hat{G}(\\tau)/T)$ denotes the sampling probability for prioritization, computed by applying a Softmax function over all trajectories in Boff, with T > 0 as the temperature parameter. For Bon, which already contains high-return trajectories, trajectories are sampled uniformly. The control factor \u1e9e dynamically adjusts the balance between the offline and online datasets during training based on their average returns, given by\n$\\beta = \\frac{exp(G_{on}/T)}{exp(G_{on}/T) + exp(G_{off}/T)}$,\nwhere Goff represents the average \u011c across all trajectories in Boff, and Gon is the average actual return in Bon across all tasks. Fig. 4 illustrates the prioritization process, showing how \u1e9e dynamically balances contributions from offline and online datasets. This mechanism ensures the selection of task-relevant trajectories from both datasets, facilitating efficient training of the low-level policy. In Section 5, we evaluate the effectiveness of this approach in improving \u03c0\u03b7 training compared to using Boff without prioritization. We also analyze the evolution of \u1e9e during training and the characteristics of trajectories selected for skill learning.\nFor implementation, PRISM refines skills every Kiter iterations and reinitializes \u03c0\u03b7 to avoid non-stationary issues. Before training, the low-level policy \u03c0\u03b9, skill encoder q, and skill prior p are pre-trained on the offline dataset Boff using Eq. (1). Exploration policy is trained using the soft actor-critic (SAC) algorithm (Haarnoja et al., 2018), which incorporates entropy to encourage exploration. Intrinsic rewards for exploring novel trajectories are generated via random network distillation (RND) (Burda et al., 2018), leveraging a randomly initialized target network for exploration. The meta-training process of PRISM is outlined in Algorithm 1, with additional details on the meta-test phase, meta-RL losses with value functions provided in Appendix A."}, {"title": "5. Experiment", "content": "In this section, we evaluate the robustness of proposed PRISM framework to noisy demonstrations in long-horizon environments and analyze how skill discovery and refinement during training enhance performance."}, {"title": "5.1. Experimental Setup", "content": "We compare the proposed PRISM with 3 non-meta RL baselines: SAC, which trains test tasks directly without using the offline dataset; SAC+RND, which incorporates RND-based intrinsic noise for enhanced exploration; and SPiRL, which learns skills from the offline dataset using Eq. (1) and trains high-level policies for individual tasks. Also, we include 4 meta-RL baselines: PEARL, a widely used context-based meta-RL algorithm without skill learning; PEARL+RND, which integrates RND-based exploration into PEARL; SiMPL, which applies skill-based meta-RL using Eq. (2); and our PRISM. PRISM's hyperparameters primarily follow Nam et al. (2022), with additional parameters (e.g., temperature T) tuned via hyperparameter search, while other baselines use author-provided code. Results are averaged over 5 random seeds, with standard deviations represented as shaded areas in graphs and \u00b1 values in tables."}, {"title": "5.2. Environmental Setup", "content": "We evaluate algorithms across 4 long-horizon, multi-task environments: Kitchen and Maze2D from Nam et al. (2022), and Office and AntMaze, newly introduced in this work, as illustrated in Fig. 5. Offline datasets Boff are generated by policies perturbed from the expert policy using Gaussian action noise at varying levels o, tailored to each environment. Detailed descriptions are as follows:\nKitchen: The Franka Kitchen environment in D4RL benchmark (Fu et al., 2020b), proposed by Gupta et al. (2020), features a robotic arm completing 4 sequential subtasks selected from 7 available subtasks, receiving a reward of 1 for each successful subtask. The environment includes 25 meta-train and 10 meta-test tasks, with noise levels ranging from expert demonstrations (noise-free) to Gaussian action noise at \u03c3 = 0.1, 0.2, and 0.3.\nOffice: The Office environment, adapted from Pertsch et al. (2022) for meta-RL. Each task requires moving 3 randomly selected office objects into one of 3 containers on a desk, receiving a reward of 1 for each successful pick or place action. The environment includes 7 objects, 25 meta-train tasks, and 10 meta-test tasks, with noise levels ranging from expert to Gaussian noise with \u03c3 = 0.1, 0.2, and 0.3.\nMaze2D: Based on D4RL (Fu et al., 2020b), this navigation task requires a 2-DoF point mass agent to reach a goal point in a larger 20x20 maze than the default D4RL Maze2D, receiving a reward of 1 upon reaching the goal. The environment includes 40 meta-train tasks and 10 meta-test tasks, with noise levels ranging from expert to Gaussian action noise at \u03c3 = 0.5, 1.0, and 1.5.\nAntMaze: Similar to Maze2D from D4RL (Fu et al., 2020b), this environment uses a more complex ant agent in a smaller 10x10 maze to account for the increased difficulty, receiving a reward of 1 upong reaching the goal. The environment includes 20 meta-train tasks and 10 meta-test tasks, with noise levels ranging from expert to Gaussian action noise at o = 0.5, 1.0, and 1.5."}, {"title": "5.3. Performance Comparison", "content": "We compare the proposed PRISM with various baseline algorithms. Non-meta RL algorithms are trained directly on each test task for 0.5K iterations due to the absence of a meta-train phase. Meta-RL algorithms undergo meta-training for 10K iterations in Kitchen and Office, and 4K in Maze2D and AntMaze, followed by fine-tuning on test tasks for an additional 0.5K iterations. For PRISM, the skill update interval Kiter is set to 2K for Kitchen, Office, and AntMaze; 1K for Maze2D. To ensure a fair comparison, PRISM counts each update process from its exploration and high-level policies as one iteration. Table 1 presents the final average return across test tasks after the test iterations. From the result, SAC and PEARL baselines, which do not utilize skills or offline datasets, perform poorly on long-horizon tasks, yielding a single result across all noise levels. In contrast, SPiRL, SIMPL, and PRISM, which leverage skills, achieve significantly better performance.\nSPiRL and SiMPL, however, show sharp performance declines as dataset noise increases. While both perform well with expert data, SiMPL struggles under noisy conditions due to instability in its task encoder qe, sometimes performing worse than SPiRL. Here, the baseline results for Maze2D (Expert) are somewhat lower than those reported in the SiMPL paper. This discrepancy likely arises because, in constructing the offline dataset, we considered fewer tasks compared to SiMPL, resulting in trajectories that do not fully cover the map. Interestingly, minor noise occasionally boosts performance by introducing diverse trajectories that improve skill learning. In contrast, PRISM demonstrates superior robustness across all evaluated environments, consistently outperforming baselines at varying noise levels. For example, in the Kitchen environment, PRISM maintains strong performance under significant noise by effectively refining useful skills, while in Maze2D, higher noise levels lead to the discovery of diverse skills, achieving perfect task completion when \u03c3 = 1.5. These results highlight PRISM's ability to refine and discover robust skills, significantly enhancing meta-RL performance. Moreover, PRISM excels with both noisy and expert data, achieving superior test performance by learning more effective skills."}, {"title": "5.4. Visualization of Skill Refinement Process", "content": "To analyze skill learning and refinement, Fig. 7 shows the evolution of the buffer control factor 3 and skill refinement in Kitchen (\u03c3 = 0.3) and Maze2D (\u03c3 = 1.5). For Kitchen, the microwave-opening subtask is examined, while for Maze2D, skill improvements are observed in navigating the maze. In the early stages (1K iterations for Kitchen, 0.5K for Maze2D), pretrained skills learned solely from the offline dataset, are used as no skill updates occur until Kiter. This results in poor performance: in Kitchen, the agent fails to grasp the microwave handle, and in Maze2D, noisy trajectories lead to task failures. As training progresses, \u1e9e initially relies on offline data but gradually increases as high-quality samples accumulate in the online buffer. This shift enables task-relevant skill refinement and improved task-solving.\nAs a result, by iteration 5K in Kitchen, the agent learns to open the microwave, refining this skill to complete the task more efficiently by iteration 10K. In Maze2D, the agent explores more diverse trajectories over iterations, ultimately solving all training tasks by iteration 4K. These results highlight how PRISM refines skills iteratively by leveraging prioritized data from offline and online buffers. As shown in Table 1, the learned skills generalize effectively to unseen test tasks, demonstrating PRISM's robustness and efficacy. In addition, improvement in task representation through skill refinement, along with additional visualizations, including skill representations, are provided in Appendix D."}, {"title": "5.5. Ablation Studies", "content": "We evaluate the impact of PRISM's components and key hyperparameters in Kitchen (\u03c3 = 0.3) and Maze2D (\u03c3 = 1.5), focusing on the effect of the prioritization temperature T. Additional analyses, including component evaluation and KLD coefficient Akld for all noise levels, are in Appendix E.\nComponent Evaluation: To evaluate the importance of PRISM's components, we compare the meta-test performance of PRISM with all components included against the following variations: (1) Without Boff, relying solely on Bon; (2) Without PBoff, applying uniform sampling in Boff instead of maximum return relabeling; (3) Without Bon, using only Boff; and (4) Without Texp, removing the exploration policy. As illustrated in Fig. 8, performance drops significantly when either buffer is removed, showing that both are crucial for effective skill discovery. Uniform sampling in Boff also reduces performance, underlining the importance of maximum return relabeling. Lastly, excluding exp notably degrades results, emphasizing the critical role of exploration in skill refinement and trajectory discovery.\nPrioritization Temperature T: The prioritization temperature T adjusts the prioritization between online and offline buffers. Specifically, lower T biases sampling toward high-return buffers, while higher T results in uniform sampling. As shown in Fig. 9, meta-test performance varies with T. When T = 0.1, performance drops due to an excessive focus on one buffer, as the trends observed in the component evaluation. Conversely, high T = 2.0 also degrades performance by eliminating prioritization. These results highlight the importance of proper tuning: T = 1.0 for Kitchen and T = 0.5 for Maze2D achieve the best performance."}, {"title": "6. Conclusion", "content": "In this paper, we propose PRISM, a robust skill-based meta-RL framework designed to address noisy offline demonstrations in long-horizon tasks. Through prioritized skill refinement and maximum return relabeling, PRISM effectively prioritizes task-relevant trajectories for skill learning and discovery. Experimental results highlight its robustness to noise and superior performance, demonstrating its potential for efficient meta-RL in real-world applications."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here"}, {"title": "A. Implementation Details on PRISM", "content": "This section provides a detailed implementation of the proposed PRISM framework. As outlined in Section 4, PRISM begins with an initial skill learning phase (pre-train) to train the low-level skill policy, skill encoder, and skill prior. It then progresses to the meta-train phase, where skill model refinement is performed through skill exploration using the exploration policy, and skill-based meta-RL is executed using the high-level policy and task encoder. Finally, in the meta-test phase, rapid adaptation to the target task is achieved via fine-tuning based on the trained high-level policy and task encoder. Section A.1 details the initial skill learning phase, Section A.2 elaborates on the meta-train phase, and Section A.3 explains the meta-test phase. All loss functions in PRISM are redefined in terms of the neural network parameters of its policies and models. Additionally, the overall structure for the meta-train and meta-test phases is provided in Algorithms 1 and 2."}, {"title": "A.1. Initial Skill Learning Phase", "content": "Following SPiRL (Pertsch et al., 2021), introduced in Section 3, we train initial skills using the offline dataset Boff. The low-level skill policy \u03c0\u03b9,\u03c6, skill encoder q\u03c6, and skill prior p\u03c6 are parameterized by \u03c6 and trained using the following loss function (modified from Eq. (1)):\n$L_{spirt}(\\phi) := E_{(\\tau_{t:t+H_s},a_{t:t+H_s}) \\sim B_{off},\\z\\sim q(\\tau_{t:t+H_s},a_{t:t+H_s})}[L(\\pi_{l,\\phi}, q_{\\phi}, p_{\\phi}, z)]$\n$= E_{(\\tau_{t:t+H_s},a_{t:t+H_s})\\sim B_{off}, z \\sim q(\\tau_{t:t+H_s},a_{t:t+H_s})}[$\\sum_{k=t}^{t+H_s-1} log \\pi_{l,\\phi} (a_k|s_k, z) + \\alpha_{kld}D_{KL} (q_{\\phi}(\\cdot|\\tau_{t:t+H_s}, a_{t:t+H_s}) || N(0,I))$\\n$+D_{KL} ([q_{\\phi}(\\cdot|\\tau_{t:t+H_s}, a_{t:t+H_s})]| p_{\\phi})]$,\nwhere [] represents the stop gradient operator, which prevents the KL term for skill prior learning from influencing the skill encoder. Using the pre-trained \u03c0\u03b9,\u03c6, q\u03c6, and p\u5e03, PRISM refines skills during the meta-train phase to further enhance task-solving capabilities."}, {"title": "A.2. Meta-Train Phase", "content": "As described in Section 4", "processes": "Skill Exploration, which explores diverse skills near the prioritized on-policy buffer Bon; Prioritized Skill Refinement, which improves skills using Bon; and Skill-based Meta-RL, which trains the high-level policy and task encoder to effectively utilize learned skills for solving tasks. Detailed explanations for each process are provided below.\nSkill Exploration\nAs described in Section 4.2, the exploration policy \u03c0exp,\u03c8, parameterized by \u03c8, is designed to expand the skill distribution and discover task-relevant behaviors near trajectories stored in the prioritized on-policy buffer Bon for each training task T\u00b2. This buffer prioritizes trajectories that best solve the tasks. Additionally, the state-action value function Qexp,\u03c8, also parameterized by y, is defined to train the exploration policy using soft"}]}