{"title": "What should I wear to a party in a Greek taverna? Evaluation for Conversational Agents in the Fashion Domain", "authors": ["Antonis Maronikolakis", "Ana Peleteiro Ramallo", "Weiwei Cheng", "Thomas Kober"], "abstract": "Large language models (LLMs) are poised to revolutionize the domain of online fashion retail, enhancing customer experience and discovery of fashion online. LLM-powered conversational agents introduce a new way of discovery by directly interacting with customers, enabling them to express in their own ways, refine their needs, obtain fashion and shopping advice that is relevant to their taste and intent. For many tasks in e-commerce, such as finding a specific product, conversational agents need to convert their interactions with a customer to a specific call to different backend systems, e.g., a search system to showcase a relevant set of products. Therefore, evaluating the capabilities of LLMs to perform those tasks related to calling other services is vital. However, those evaluations are generally complex, due to the lack of relevant and high quality datasets, and do not align seamlessly with business needs, amongst others. To this end, we created a multilingual evaluation dataset of 4k conversations between customers and a fashion assistant in a large e-commerce fashion platform to measure the capabilities of LLMs to serve as an assistant between customers and a backend engine. We evaluate a range of models, showcasing how our dataset scales to business needs and facilitates iterative development of tools.", "sections": [{"title": "1 Introduction", "content": "The advent of large language models (LLMs) and generative artificial intelligence has transformed the landscape of natural language processing, not only in academic but also in industry settings [17, 27, 34]. LLMs, developed via large-scale pretraining and Reinforcement Learning from Human Feedback [2, 10, 40], have showcased heightened proficiency in language comprehension. Consequently, they have been instrumental in streamlining customer interactions and enhancing overall user satisfaction.\nIn particular, large language models are ushering a wave of new solutions in the domain of customer support services, where models are forming the basis of chatbots and assistant agents. Domains such as e-commerce and healthcare offer ample opportunity for breakthroughs in customer support through the deployment of model agents. In our work, we focus on the domain of online fashion retail and the use of conversational agents for customer support.\nBeyond its functional aspect of providing clothing, fashion enables individuals to express themselves, communicate their identity and values, as well as forge a sense of community and belonging. Online fashion retailers enable customers to browse a wide range of assortments conveniently, inspire customers to develop their personal taste and provide information to empower customers to make decisions with greater confidence.\nLarge language models provide an opportunity to change the status quo in online fashion retail. Customers, instead of interacting with a search engine that can not process abstract descriptions of fashion concepts (e.g., \"essentials\u201d or \u201curban\"), are now enabled to have a conversation to describe and refine their wants in an interactive way, using their own words, language and fashion ideas. An assistant agent can then interface with the retailer's search and recommendation engines to show products to the customer tailored to the conversation and their descriptions.\nIn our work we focus on the evaluation of the capabilities of an agent to interact with a customer and interface appropriately with a backend search engine, developing a dataset of conversations between customers and assistant agents in the fashion domain. With our dataset, we facilitate the iterative development of assistant agents, allowing for fair comparisons between different models, recording progress across model improvements and mapping performance metrics to model changes (Figure 1).\nOur dataset is made up of 1334 conversations each for English, German and French, plus an additional 87 conversations in Greek. Data is generated through a controlled simulation environment where an LLM-based customer agent interacts with an assistant agent to generate conversations. The customer agent is given the description of an item or a theme and is instructed to interact with the assistant in order to purchase an item that fits its goal, while the assistant is instructed to interact with the customer to aid them through their shopping trip, assisting in providing information, advising and showing items that the customer may be interested in. All conversations are subsequently verified manually for high quality. With our methodology, which leverages the power\""}, {"title": "2 Related Work", "content": "There has been considerable work towards evaluation of text generation, from the early days of textual overlap metrics [3, 22, 28] to methods based on the similarity of contextualized representations [16, 31, 39], as well as composite metrics to cover many aspects of conversational capabilities [33].\nState tracking evaluation methods offer a structured way to evaluate the capabilities of models to converse in desirable patterns and produce responses appropriate for each stage of the conversations [6, 7, 30]. Due to the highly stochastic and subjective nature of text generation and its evaluation, traditional metrics have been shown to oftentimes be uninterpretable [24, 26, 29]. Thus, there have been efforts to develop systems involving humans and manual quality assessment [1, 9, 14, 15, 35, 36].\nWith the recent rise of Large Language Models and their advanced conversational and reasoning capabilities [4, 17, 27, 34], evaluation of text generation models is moving away from singular metrics that measure particular aspects of language (such as grammaticality, readability, or similarity to a source text), towards the development of leaderboards based on benchmarks of a series of tasks and evaluations for the more reproducible comparison of powerful models [5, 19-21, 25, 32]."}, {"title": "3 Presentation of Evaluation Dataset", "content": "Our dataset contains conversations between a customer and an assistant agent on an online fashion retailer. The customer is given an item or a theme to shop for, and the assistant provides aid towards the successful completion of the shopping trip."}, {"title": "3.1 Data Creation", "content": "To ensure robust evaluation of conversational agents, we need quantities of data orders of magnitude larger than what manual generation capacities allow. To this end, we create our dataset in two steps: (i) conversation generation through a simulation environment, and (ii) manual quality verification."}, {"title": "3.1.1 Fashion Attributes", "content": "Focusing on the fashion domain, we create a dataset covering many aspects of fashion and terminologies that a customer would use when looking for items online. Namely, we cover six attributes: colour, type, material, fit, brand and size. Further, we cover eight apparel types: pants, trousers, shoes, jacket, coat, sweatshirt, hoodie and jeans. We use both 'pants' and 'trousers', two synonyms, to evaluate robustness to paraphrasing. The acceptable values for each attribute are the following:\n\u2022 Colour: Acceptable values are black, white, gray, blue, red, maroon, beige and green.\n\u2022 Type: The type or style of the item, such as utility, athletic, formal, summer and urban\n\u2022 Material: The main material of the item, such as leather, wool, cotton and fleece.\n\u2022 Fit: Acceptable values are slim, loose and comfortable.\n\u2022 Brand: Acceptable values are Converse, Nike, Carhartt, Pier One, Superdry, Levi's, Hugo Boss and PULL&BEAR.\n\u2022 Size: Acceptable values are S, M, L, small, medium and large.\nFinally, we cover nine open-ended themes customers might be shopping for: (i) hiking, (ii) Christmas dinner, (iii) football in winter, (iv) rooftop summer party, (v) techno club in Berlin, (vi) coffee house in Vienna, (vii) party in Greek taverna, (viii) sports in the summer, (ix) eccentric timelord book bazaar."}, {"title": "3.1.2 Generation of Conversations", "content": "Conversations are generated in a simulation environment, leveraging the generative capabilities of GPT. We simulate customer and assistant interactions on three levels: (i) template-based customer interactions, (ii) LLM-based customer given the description of an item to purchase, (iii) LLM-based customer given an abstract theme to shop for. The assistant is in all cases a production-level GPT-3.5 model prompted to aid customers through their fashion shopping journey.\nOn a high level, customer agents are given either an item description or a theme to shop for. Item descriptions are generated via the combination of attribute values and apparel types, such as colour, type, material, fit, brand and size of item (for further details on attributes, we refer the reader to Section 3.1.1). An example item description is: \"black utility Carhartt trousers\". In this example, the following attribute values were given: {colour: black, genre: utility, brand: Carhartt, material: null, fit: null, size: null} for the apparel type 'trousers'.\nFor a template-based customer agent (i), a single message is formatted using the template \"{I would like I am looking for} item_description\". Subsequently, this message is sent to the assistant agent and the response recorded as a single-interaction conversation. With the template-based customer agent we introduce conversations that are simple and brief, evaluating whether an examined assistant agent can fulfill their task for basic interactions.\nWith the LLM-based customer agents (ii & iii), we prompt GPT instances to simulate more complex and interactive conversations. The customer agent is given either the description of an item or a theme to shop for and is instructed to interact with the shopping assistant to complete their shopping trip. To simulate different customer behaviors for the LLM-based agents, different prompts were used, covering an array of customer behaviors and personalities. Agents are prompted to \"behave\u201d based on three personality traits:\n\u2022 Casual: You are a CUSTOMER who keeps things simple. You do not write too much, you write just enough. You are aware that you are speaking with a chat bot and not a human. You use the chat bot as a tool to get the item you need.\n\u2022 Indecisive: You are an indecisive and timid CUSTOMER. You don't really know what you want. Take your time when making a decision and wait for the assistant to ask you for information before you describe fully what you want.\n\u2022 Rude: You are a rude CUSTOMER. You are annoyed at the chat bot. Nevertheless, you want to buy clothes so you are negotiating with the assistant to find something. Try to find something, you are impatient and you don't want the conversation to go on for long.\nThe prompt given to customer agents is given in Table 1.\nAll types of customer agents can message only in text form, asking questions, requesting more information, declaring interest in presented items, describing their wants, among other actions that relate to their goal. The assistant agent can reply in one of two ways: (i) message in text form to provide details or ask clarification questions to identify the needs of the customer, or (ii) query a backend search engine to present a carousel of items to the customer. The conversation ends on the side of the customer agent, which is given the agency to end the conversation if a relevant item is found or if there is no more progress towards finding a relevant item. All messages and actions taken are recorded.\nSample data is shown in Table 2. Four examples are shown, in three of them the customer agent C is given the description of an item, while in the last one the agent is given an open-ended theme. In two of the three item-based descriptions, the customer has given all the information required for a correct query generation. The assistant agent A interacts with the customer to identify their needs and to show them [ITEMS] relevant to their needs. In the \"slim Levi's jeans\" example, the customer gives information little by little, and thus the assistant needs to show three different sets of items to the customer. We record the search queries generated by the assistant for each of these three instances."}, {"title": "3.1.3 Manual Verification of Conversations", "content": "For the verification of the quality of the generated dataset, we went over every conversation to ensure high standards of quality. Conversations are not edited, we only remove conversations that do not fit our quality standards. Our criteria for removing a conversation are incorrect format, error messages, incorrect customer behavior and unfaithful conversations. In more detail:\n\u2022 Incorrectly formatted conversations are removed. If the customer agent does not act as a customer, or the model is in some other way not responding according to the prompt, the conversation is removed.\n\u2022 Error messages present in a conversation precipitate the deletion of the conversation.\n\u2022 Incorrect customer behavior is a cause for deletion. If the customer agent does not behave as specified in the prompt, the conversation will be deleted. For example, if the customer is prompted to behave in a rude manner but instead behaves neutrally, the conversation is considered invalid.\n\u2022 Unfaithful conversations are removed. Oftentimes the customer will not strictly stick to the given item or theme description and will instead ask for incorrect details or declare interest in an item that does not fit the description. The customer agent needs to stay faithful to the original description, otherwise the whole conversation will be removed.\nTo expand our dataset to more languages, we translate the curated conversations to German, French and Greek. We use GPT-4 to translate both customer and assistant messages. The rest of the attributes, such as queries, remain the same (we assume search engines take in English queries as input and therefore we do not translate them).\nFor German and French we translated all messages and then proceeded to verify 100 randomly-selected conversations for each language. Performance on these examples was deemed acceptable and thus we included all 9k translated messages of each language. For Greek, we found translation performance to be subpar. Therefore, we do not evaluate on the entire dataset, but instead a smaller, more carefully curated set of 200 messages. We consider the Greek subset of our data as a small-scale evaluation case to investigate model performance in lower-resource languages."}, {"title": "3.2 Data Description", "content": "Each conversation in the dataset is broken down into pairs of messages between customer and assistant agents, as well as metadata for each interaction such as item or theme description, queries, as well as action taken by the assistant. In detail, our dataset is broken down into eight fields:\n(i) id: The conversation-level unique identifier (every message within the same conversation have the same value in this field)\n(ii) goal: The goal description given to the customer agent. For evaluation, we compare against this field.\n(iii) properties: Same as goal field but comma-separated. Values are ordered per property and can be parsed into a dictionary of property-value pairs with keys {colour, type, material, fit, brand, apparel, size}.\n(iv) all_info: Boolean field denoting whether the conversation contains all the information the customer agent was given or not.\n(v) customer: The message of the customer at this point in the conversation.\n(vi) assistant: The message of the assistant at this point in the conversation.\n(vii) queries: The queries generated by the assistant.\n(viii) action: The action taken by the assistant. Can either be 'message' or 'search'.\nWe report dataset statistics in Table 3. Statistics are comparable across languages (excluding data size, where the Greek set was kept small). There are slightly more tokens in German than in the other languages, both for customer and assistant messages. Each conversation has approximately six messages in total (including customer and assistant messages), with around 66% of all assistant actions being search. Note that since the German and French sets were translated from the English set, volume statistics (such as number of messages) are the same."}, {"title": "3.3 Evaluation Details", "content": "For the evaluation of performance in the case where the customer agent is given the description of an item (both in the ASSISTANTEVAL and QUERYGENEVAL tasks), we compare the expected query (i.e., item description) with the output query using BERTScore [39]. BERTScore is a method proposed to evaluate natural language generation tasks via leveraging the capabilities of BERT [11] to produce contextualized embeddings."}, {"title": "3.4 Models", "content": "We evaluate a series of models using our dataset. Llama2 [34] is an open-source large language model pretrained on publicly available data. We evaluate the llama-2-7b-chat variant, with 7B parameters. Mistral [17] is another open-source model pretrained on publicly available data and further finetuned for dialogue, shown to outperform LLama2-13B in multiple tasks. We evaluate the Mistral-7B-Instruct-v0.1 model, with 7B parameters. GPT-based models is a large language model and a service that has revolutionized academia and industry alike. Two models are evaluated: gpt-3.5-turbo-0613 and gpt-4-0613. In total, we compare three different prompts for GPT-3.5 to evaluate a range of behaviors. One instance is prompted to simply act as an assistant (I), another instance is prompted to actively include specific terms the customer mentions (II) and a third is prompted to both include specific terms as used by the customer and to actively ask questions to better identify the wants of the customer (III). Full prompts are given in Table 5. We compare these large language models with a low-cost, off-the-shelf unsupervised keyword extractor, Yake [8]. Yake is a keyword extractor based on statistical text features, trained as a domain- and language-agnostic extractor, allowing us to use the tool across all our examined languages. Finally, as a lower bound we provide a popularity and a random baseline. For the popularity baseline, we assume a model that always generates the query for the popular \"black Nike shoes\" item (translated to German, French and Greek). For the random baseline, we assume a model that as a query always generates \"Lorem ipsum dolor sit amet\" (in the Greek alphabet for the Greek set)."}, {"title": "4 Results", "content": "Below we showcase how we can use our evaluation dataset for the development of models. We analyse results on our main task of evaluating capabilities of the assistant to interface customers and a backend engine (ASSISTANTEVAL), as well as the subtask of specifically evaluating the query generation capabilities of models (QUERYGENEVAL)."}, {"title": "4.1 ASSISTANTEVAL", "content": "We showcase (Table 6) how we can use our dataset to benchmark conversational agents as fashion shopping assistants for the ASSISTANTEVAL task, evaluating the capabilities of models to translate the wants of a customer into concrete queries to interface with a backend search engine.\nWe found that the open-source models were not able to consistently interact with the customer agent. Often there would be role-switching (i.e., the assistant agent would behave as a customer), incorrect formatting of responses (e.g., the assistant agent would add 'ASSISTANT:' as a message prefix) or skipping query generation. Our findings corroborate previous work showing how LLMS (especially open-source ones) struggle with producing formatted output [21, 37]. Deployment of open-source LLMs currently requires significant engineering efforts, and we are instead focusing on GPT-based models.\nGPT-4 performs the best for all languages, although GPT-3.5 (III) performs competitively, potentially because the prompt for that model was engineered to instruct the model to actively ask questions about specific attributes. While the other two models (GPT-3.5 (I) and (III)) are competitive in German, French and Greek, they are considerably worse for English, where GPT-4 and the more finely-engineered GPT-3.5 (III) are superior. In Table 6 we show the correlation between the total cost of a single run of evaluation and the performance of each model. As expected GPT-4 performs the best, although at a higher cost."}, {"title": "4.1.1 Fashion Attribute Performance Analysis", "content": "We perform a qualitative analysis of performance of GPT-4 (our best performing model) across fashion attributes. In developing a useful assistant in the fashion domain, analysing how it performs for different attributes is vital. For this analysis, we compute the precision of exact matches between output queries and the input item description given to the customer agent. We perform this analysis for all examined fashion attributes, calculating exact matches for the attribute values as described in Appendix 3.1.1. Results are shown in Table 7.\nPerformance for all three languages follows similar patterns, although performance in German is worse across almost all attributes. In attributes with clear definitions, such as colour, material and fit, performance is adequate, while for attributes such as type and apparel, which are more open-ended (e.g., type can be described abstractly with terms such as 'summer' or 'utility', and apparel can have overlap between terms, such as 'jacket' and 'coat'), performance is worse. Notable exception is size, with very low performance, potentially because of a deficiency of GPT to extract correct size labeling (size is often denoted with a single number or a letter, such as 'S' or '42'). Another exception is the clearer 'brand' attribute, where performance is also low. We hypothesize the low performance for brands is because zero-shot named entity recognition with GPT is subpar [38]."}, {"title": "4.1.2 Open-ended Themes", "content": "We present a comparison of GPT models in aiding customers shop for a theme instead of concrete item descriptions. We assign output queries to the theme with the highest cosine similarity based on text-embedding-ada-002 representations, as described in Section 3.3. We report the Precision@3 score in Table 8. GPT-4 performs the best overall, especially in the English set, while for French and German performance is competitive for many models. Performance in the other languages is more uniform. In Table 8 we show that GPT-4 performs the best, albeit at a higher cost."}, {"title": "4.2 QUERYGENEVAL", "content": "Evaluating the query generation capabilities of tools is crucial in the development of assistant agents. If a customer clearly describes what they are looking for, any agent should be able to generate appropriate queries. In this subtask, we assume that the customer has given all the information necessary for a successful interaction. In our dataset, we manually mark conversations that contain all the necessary information (i.e., fashion attributes). We perform a comparison of model performance for conversations that contain all the attributes mentioned in the item description. Namely, each model is given as input the entire conversation and generates a single query which is compared to the item description as usual. We measure the F1 score as produced by BERTScore and show results in Table 9.\nGPT-4 performs the best in English by a margin of 1.2. The gap between GPT-3.5 and GPT-4 narrows in German and GPT-3.5 performs marginally better in French and Greek. From the open-source models, Llama2 performs the best in English, while performance in the other languages is similar between the open-source LLMs and Yake, the unsupervised keyword extraction tool. We hypothesize that Yake performs similarly to open-source LLMs since it has been specifically developed to extract keywords, a task pivotal for query generation.\nIn Table 9, we show a cost analysis (in dollars) for evaluating Yake, Llama2, Mistral, GPT-3.5 and GPT-4. Yake is an unsupervised model that can be run with (virtually) no cost on a CPU. We run Llama2 and Mistral on an Amazon Sagemaker instance, costing 0.399 dollars per hour. The GPT models are called through an API with an associated cost. In the analysis, we show performance of each model and how much it costs in total to evaluate for the QUERYGENEVAL task in English. While GPT-4 performs the best, it is also the most expensive. Llama2 and Mistral perform similarly, with GPT-3.5 being the second cheapest model while at the same time being the second in performance."}, {"title": "5 Ethical Considerations", "content": "In this section we consider the ethical implications of our work. While on a direct level our work does not impact society or peoples, since we do not involve human participants in our study and we do not examine human-generated data, our work could have larger-scale implications. Generative AI poses a threat to workers across fields of occupation, replacing workers' output with cheaper and faster generative AI \"labor\". Industries such as art have been impacted heavily [12, 18], as well as many white-collar positions, with already many people struggling with layoffs and loss of employment.\nAside from impact on the labor market, generative AI models may be biased in their interactions with the customer. Bias in AI has been extensively studied [13, 23], and care should be taken to avoid biased behavior."}, {"title": "6 Conclusion", "content": "With the recent transformation of the field of NLP ushered in by the advent of large language models, a series of breakthroughs in many domains is taking place. To ensure the productive and iterative application of LLM-powered agents, efficient and scaleable evaluation is vital.\nIn our work, we focus on the evaluation of the capabilities of assistant agents to interface between customers and a backend search engine. Specifically, we showcase an application of our evaluation methodology and framework on the domain of online fashion retail, where LLM-powered assistants can aid customers in their shopping trip, answering questions, recommending products and interacting with the customer.\nOur evaluation aims at evaluating these capabilities across languages. Data is generated via a simulation environment where a customer and an assistant agent are interacting with each other, with subsequent messages and interactions of the agents verified manually for high-quality. We propose the use of our multilingual dataset to evaluate fashion assistants via the replay of conversations, ensuring evaluation that is fair and reproducible, and perform a comparison of a series of models, from open- to closed-source. With our dataset and methodology, we facilitate the iterative development of assistant agents in a business setting."}]}