{"title": "Enhancing Vectorized Map Perception with Historical Rasterized Maps", "authors": ["Xiaoyu Zhang", "Guangwei Liu", "Zihao Liu", "Ningyi Xu", "Yunhui Liu", "Ji Zhao"], "abstract": "In autonomous driving, there is growing interest in end-to-end online vectorized map perception in bird's-eye-view (BEV) space, with an expectation that it could replace traditional high-cost offline high-definition (HD) maps. However, the accuracy and robustness of these methods can be easily compromised in challenging conditions, such as occlusion or adverse weather, when relying only on onboard sensors. In this paper, we propose HRMapNet, leveraging a low-cost Historical Rasterized Map to enhance online vectorized map perception. The historical rasterized map can be easily constructed from past predicted vectorized results and provides valuable complementary information. To fully exploit a historical map, we propose two novel modules to enhance BEV features and map element queries. For BEV features, we employ a feature aggregation module to encode features from both onboard images and the historical map. For map element queries, we design a query initialization module to endow queries with priors from the historical map. The two modules contribute to leveraging map information in online perception. Our HRMapNet can be integrated with most online vectorized map perception methods. We integrate it in two state-of-the-art methods, significantly improving their performance on both the nuScenes and Argoverse 2 datasets. The source code is released at https://github.com/HXMap/HRMapNet.", "sections": [{"title": "1 Introduction", "content": "High-definition (HD) maps comprise positions and structures of vectorized map elements (e.g., lane divider, pedestrian crossing, and road boundaries), playing a vital role in the navigation of self-driving vehicles. Traditionally, HD maps are constructed offline, utilizing SLAM-based methods and complex pipelines"}, {"title": "2 Related Work", "content": "Since map elements are commonly constructed in BEV space, online map perception benefits a lot from BEV feature learning, which transforms image features from surrounding cameras of a self-driving vehicle to BEV space. For example, lift image features to 3D space and utilize pooling to produce BEV features. learn BEV representations in a transformer architecture. Map perception is generally compatible with all of these methods."}, {"title": "2.1 Map Perception with Single Frame", "content": "At early stages, map perception mainly focuses on lane detection , road topology reasoning or map segmentation , which are mainly constructing rasterized maps and need post-processing to produce desired vectorized map elements for downstream applications. For example, predicted rasterized maps are clustered to acquire final vectorized maps in HDMapNet . VectorMapNet and MapTR are pioneer works to predict vectorized map elements directly. VectorMapNet designs a map element detector and a polyline generator to produce final vectorized maps. MapTR proposes a unified permutation-equivalent modelling and utilizes the DETR paradigm to predict vectorized map elements directly. Following these breakthroughs, vectorized map perception becomes popular in autonomous driving research, leading to the development of many methods for improving performance. The evolved version MapTRv2 adds decoupled self-attention in the decoder and auxiliary losses, improving the accuracy largely. ScalableMap exploits the structural property of map elements and designs a progressive decoder for long-range perception. MapVR introduces differentiable rasterization and rending-based loss for superior sensitivity. Furthermore, instead of simple point set representation, BeMapNet predicts B\u00e9zier control points and PivotNet predicts pivot points instead of a fixed number of points for accurate results."}, {"title": "2.2 Map Perception with Complementary Information", "content": "The above methods predict map elements using a single frame, which limits further improvements. Recent advancements have expanded beyond single-frame perception, incorporating complementary information. For instance, extra standard-definition (SD) maps are explored to help HD map perception and lane-topology understanding . Satellite maps are used to augment onboard camera data for map perception in . These methods require extra data for online perception and thus increase cost in practical applications.\nTemporal information is a more accessible complement for online perception. It has been widely used in BEV feature learning and object detection . For vectorized map perception, StreamMapNet leverages temporal information through query propagation and BEV feature fusion. SQD-MapNet introduces stream query denoising to facilitate temporal consistency. In these methods, short-term previous frames in temporal are utilized for perception."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Overview", "content": "Our proposed HRMapNet is designed as a complement to SOTA online vectorized map perception methods. As illustrated in Fig. 2, HRMapNet maintains a global historical rasterized map (described in Sec. 3.2) to aid online perception. Given surrounding images as input, 2D features are extracted from a shared backbone and transformed to BEV space. We introduce a map encoder and feature aggregation module (described in Sec. 3.3) to obtain enhanced BEV features from both onboard cameras and retrieved local maps. Additionally, we also design a novel query initialization module (described in Sec. 3.4), working before the original map decoder. This module aims to endow base queries with prior information from local maps, enabling more efficient search for desirable map elements. Finally, vectorized map elements are predicted directly from the prediction head and can be rasterized to merge into the global map."}, {"title": "3.2 Global Rasterized Map", "content": "We firstly introduce the rasterized map used to save historical information. As illustrated in the bottom left corner of Fig. 2, vectorized maps are rasterized to update a global map. Here we adopt the rasterization method used in [19]. Briefly, the label of each pixel in the rasterized map is determined based on its distance to vectorized elements' boundary. From each online prediction at i-th timestamp, we can obtain a semantic mask, referred as local rasterized map \\(M\u2208 {0,1}^{H\u00d7W\u00d7N}\\), where H and W denote the spatial shape of the perception range in BEV space; N is the number of map element categories and N = 3 as in previous methods [25,48], representing lane divider, pedestrian crossing and road boundary, respectively. Therefore, \\(M(p) \u2208 {0,1}^{1\u00d7N}\\) indicates whether and what map elements exist at position p = (x, y) for each category; the value 1 indicates existence and the value 0 indicates non-existence.\nSuch rasterized map we utilize is analogous to occupancy grid map , a well-established concept in robot navigation and mapping . Occupancy grid mapping is extensively studied for updating a global map from local observations. We use a similar method to update the global rasterized map \\(M^{g} \u2208 R^{H^{g}\u00d7W^{g}\u00d7N}\\), where \\(H^{g}\\) and \\(W^{g}\\) denote the spatial shape of the global map. For map updating, the local coordinate p of each pixel of M is firstly transformed to the global coordinate \\(p^{g}\\) based on the ego-pose \\(T_{i} = [R_{i}, t_{i}]\\):\n\\(p^{g} = f_{g\u2192l}(p; T_{i}) = round(R_{i}p + t_{i})\\)   (1)\nwhere \\(R_{i}\\) and \\(t_{i}\\) are relative rotation and translation, and round(\u00b7) denotes the rounding function which converts a continuous coordinate to a discrete coordinate in the rasterized map. Alternatively, given global coordinate \\(p^{g}\\) and pose \\([R_{i}, t_{i}]\\), its corresponding local coordinate p is\n\\(p = f_{l\u2192g}(p^{g}; T_{i}) \triangleq round(R_{i}^{T}(p^{g} - t_{i}))\\)   (2)\nThen the global map \\(M^{g}\\) can be updated based on the local map M for each category. Take one category for example:\n\\(M^{g}(p^{g}) \\leftarrow  \\begin{cases}  M^{g}(p^{g}) + S^{+} & \\text{if } M(p) = 1 \\\\ M^{g}(p^{g}) - S^{-} & \\text{if } M(p) = 0  \\end{cases}\\)   (3)\nwhere p is determined by Eq. (2), \\(M^{g}\\) records the status for each map element category, \\(S^{+}\\) and \\(S^{-}\\) are defined values to update status based on local prediction results. This simple method integrates local results into a global map efficiently, facilitating continuous refinement and updating. For online perception, a local rasterized map is retrieved from the global map based on the ego-pose \\(T_{i}\\), and a threshold \\(S_{th}\\) is used to determine whether map elements exist for each category:\n\\(M(p) =  \\begin{cases}  1 & \\text{if } M^{g}(p^{g}) > S_{th} \\\\ 0 & \\text{if } M^{g}(p^{g}) \\leq S_{th}  \\end{cases}\\)   (4)\nwhere \\(p^{g}\\) is determined by Eq. (1)."}, {"title": "3.3 BEV Feature Aggregation", "content": "Various methods have been proposed to transform features from perspective view to BEV space, serving as a fundamental module in map perception. For example, MapTRv2 utilizes BEVPoolv2 to acquire BEV features \\(F_{l} \u2208 R^{H\u00d7W\u00d7C}\\), where C is the number of feature channels. We keep this module unchanged and introduce an aggregation module to enhance BEV features with prior information from local maps.\nIn HRMapNet, the retrieved local map \\(M^{l}\\) serves as priors indicating where deserves more attention for map element perception. Therefore, inspired by FB-BEV , we add additional BEV queries at locations where map elements exist in the local map (i.e., \\(M^{l}(p) \\neq 0\\)). These additional BEV queries are projected onto images to extract relevant features through spatial cross-attention . Then, additional BEV features are acquired where map elements exist in the local map. For the locations where no map elements exist (i.e., \\(M^{l}(p) = 0\\)), corresponding additional BEV features are set as zeros. These additional BEV features are formulated as \\(F_{M} \u2208 R^{H\u00d7W\u00d7C}\\).\nIn the feature aggregation module, \\(F_{l}\\), \\(F_{M}\\) and \\(M^{l}\\) are fused together:\n\\(F_{BEV} = Conv(Concat(F_{l} + F_{M}, M^{l}))\\)   (5)\nHere, \\(F_{M}\\) is added to \\(F_{l}\\) to compensate for deficiencies. Additionally, \\(M^{l}\\) can be regarded as special BEV features with clear semantic information. Thus we concatenate it with BEV features and use convolution to acquire the enhanced BEV features \\(F_{BEV} \u2208 R^{H\u00d7W\u00d7C}\\) for further processing."}, {"title": "3.4 Query Initialization", "content": "In addition to fusing the retrieved rasterized map into BEV features, a novel query initialization module is designed to facilitate efficient search for desirable map elements. Within a DETR paradigm, a set of learnable queries will interact with extracted features to search for desirable elements. Without prior information, queries would search from random and prediction results are refined gradually through several decoder layers.\nIn HRMapNet, the retrieved rasterized map provides prior information about where map elements may exist and thus can facilitate map element queries search for desirable elements efficiently. As illustrated on the right of Fig. 2, the proposed query initialization works before the original map decoder. Base queries will firstly interact with prior features embedded from the local map.\nIn detail, for a valid location where map elements exist in the retrieved local map \\(M^{l}\\), its position p is related to a learnable position embedding \\(PE(p) \u2208 R^{1\u00d7C}\\); and the semantic vector \\(M^{l}(p)\\) is projected to a label embedding \\(LE(p) \u2208 R^{1\u00d7C}\\) using a linear projection. Then, we acquire a map prior embedding for each valid position:\n\\(ME(p) = PE(p) + LE(p)\\)   (6)\nMap prior embedding \\(ME(p) \u2208 R^{1\u00d7C}\\) encodes where map elements may exist based on the retrieved local map. To fuse priors, base queries interact with a set of map prior embeddings through cross-attention [38]. Then, initialized queries search desirable elements in BEV features through original decoder layers. Moreover, to improve efficiency and save memory consumption, the retrieved local rasterized map \\(M^{l}\\) is downsampled before extracting map prior embeddings."}, {"title": "3.5 Implementation Details", "content": "Training. The prediction head and training loss remain identical to SOTA vectorized map perception methods. Taking MapTRv2 as an example, the prediction head predicts a class score and sequential 2D point positions for each element. Classification loss, point-to-point loss and edge direction loss are used for training; one-to-many loss [17], dense prediction loss and depth loss are used as auxiliary supervision. In each training epoch, the global map is set from empty and updated gradually from prediction results.\nTesting. To ensure fair comparison, the global map is also initialized as empty by default and updated from prediction results. Since testing frames are typically evaluated in chronological order, most testing frames can still benefit from the updated global map from their preceding frames."}, {"title": "4 Experiments", "content": "To demonstrate the effectiveness, we integrate HRMapNet with two SOTA online vectorized map perception methods, MapTRv2 and StreamMapNet ."}, {"title": "4.1 Experimental Setup", "content": "Datasets. We evaluate HRMapNet on two commonly used self-driving datasets, nuScenes and Argoverse 2 . The nuScenes dataset provides 6 surrounding images captured across 1000 scenes in 4 locations, while Argoverse 2 provides 7 surrounding images captured across 1000 scenes in 6 cities. The two datasets comprise multiple traversals in both training and validation sets, providing diverse and comprehensive data for evaluation.\nMetric. Following MapTRv2 and StreamMapNet, three map element categories (i.e., lane divider, pedestrian crossing and road boundary) are predicted. Chamfer distance is used to determine whether the prediction matches with the ground truth under three thresholds (0.5 m, 1.0 m, and 1.5 m). The mean average precision (mAP) is calculated for the three categories."}, {"title": "4.2 Comparison with SOTA Methods", "content": "Comparison on nuScenes. As illustrated in Tab. 1, we compare HRMapNet with SOTA vectorized map perception methods. HRMapNet largely outperforms the methods using only single frame images (i.e., VectorMapNet, PivotNet, BeMapNet, MapTR, StreamMapNet and MapTRv2). Specifically, when trained with 24 epochs, HRMapNet boosts the performance of StreamMapNet and MapTRv2 by +5.9 mAP and +5.7 mAP, respectively. When trained with 110 epochs, HRMapNet still outperforms MapTRv2 by +4.9 mAP under the same setting.\nIn comparison to methods introducing complementary information, HRMapNet also stands out for its comprehensive utilization of all historical information. Besides onboard images, P-MapNet adds extra SDMap from OpenStreetMap , but the improvement is lower than ours. SQD-MapNet is a concurrent work which leverages stream query denoising strategy to benefit from the results of previous frames. Since all predicted results are preserved in a global rasterized map, HRMapNet leverages not only temporal information but all past results for online perception, and thus achieves superior performance.\nMoreover, HRMapNet maintains high efficiency in terms of inference speed when integrated with StreamMapNet and MapTRv2, running at 21.1 FPS and 17.0 FPS, respectively. This ensures HRMapNet not only enhances performance but also remains practical for real-time applications in autonomous driving.\nComparison on Argoverse 2. The results on Argoverse 2 dataset, as presented in Tab. 2, further demonstrate the effectiveness of HRMapNet. HRMapNet achieves significant enhancements across both StreamMapNet and MapTRv2, with an increase of +2.8 mAP for StreamMapNet, surpassing the performance of SQD-MapNet which leverages temporal information. Similarly, when compared to MapTRv2, HRMapNet exhibits superior performance with a gain of +4.0 mAP. These results underscore the effectiveness and versatility of our method across different methodologies and datasets.\nComparison on new split sets. The above experiments are conducted on the commonly used original dataset split, in which overlap of locations exist between training and validation sets. StreamMapNet proposes new splitting methods for nuScenes and Argoverse 2, in which training and validation data are separated in locations. We also provide results on these new split sets in Tab. 3. On the new split data, StreamMapNet utilizes query propagation and BEV fusion to integrate temporal information. We do not use these two temporal fusion modules and only integrate utilizing a global rasterized map. HRMapNet still improves the base StreamMapNet by over +3.0 mAP on these two datasets, which reinforces the value of integrating a global rasterized map."}, {"title": "4.3 Ablation Study", "content": "In this subsection, we provide some ablation studies of our method, which are conducted on nuScenes with 24 epochs and use MapTRv2 as the baseline.\nFeature aggregation and query initialization. For online perception, our method leverages global map information in both BEV features and queries. We provide an ablation study about these two modules in Tab. 4. From MapTRv2, integrating global map information into BEV features brings an improvement of +3.1 mAP. Introducing query initialization further improves the performance by +2.6 mAP. Both components have significant positive effect for integrating global map information into online perception.\nTo further demonstrate that the proposed query initialization helps search for map elements more efficiently, an ablation study of decreasing decoder layers is provided in the supplementary material.\nMap resolution in query initialization. In query initialization, all valid positions where map elements exist are embedded as map priors to endow in-"}, {"title": "4.4 Extra Results for Practical Usage", "content": "Robustness to localization error. As described in Sec. 3.2, the global map is updated from local predicted rasterized maps based on ego-pose. In autonomous driving, ego-pose can be localized with high accuracy using GNSS modules or SLAM-based methods . To assess the robustness of HRMapNet to localization errors, we conduct additional experiments on the nuScenes dataset, as presented in Tab. 6. The model is based on MapTRv2 and trained with 24 epochs; all results are from the same model with varying levels of localization errors. We add random noises to both translation and rotation of ego-pose, and thus both map updating and retrieving would be affected.\nThe results clearly demonstrate the robustness of HRMapNet to localization errors, particularly in terms of translation. One contributing factor is the relatively small map resolution (0.3m) utilized in our method. Despite varying levels of localization errors, HRMapNet consistently achieves comparable results, experiencing only a 1 mAP drop in most cases. Even in the worst case with the largest noise, a historical map still brings benefits (63.8 mAP) compared to the baseline, MapTRv2 (61.5 mAP).\nConsidering localization with 0.1 m error for translation and 0.01 rad error for rotation is a common requirement in autonomous driving , these extra results indicate the effectiveness of HRMapNet in practical usage."}, {"title": "Different initial maps", "content": "In the above experiments, HRMapNet is tested with an empty initial global map for a more fair comparison. The global map is updated gradually from perception results and benefits later prediction. For many frames, the online perception actually only benefits from short-term previous frames in temporal, which weakens the power of using a global historical map.\nHere, we provide extra results with pre-built initial maps in Tab. 7. The model used here is the same as in Tab. 1, integrated with MapTRv2 and trained 24 epochs. Note that the model is not re-trained or finetuned, and we only test it with different initial maps. Here are two kinds of maps can be provided.\nFor the \"validation map\u201d, the same model is tested with validation data twice. The first time is running with an empty initial map. The map is updated gradually as validation data comes in and the final global map is saved. This global map is loaded for the second validation. There is actually no extra data input, the model constructs a global map by itself and use it again for validation. With the help of this more complete map, the performance of the same model is further enhanced by +5.4 mAP.\nBesides, the global map constructed during training is saved and loaded again for validation. As stated in Sec. 4.2, there are overlaps in location between training and validation data. Thus this training map can also benefit online perception for validation. Because this training map is more accurate, the performance is improved largely by +16.5 mAP.\nWe provide these extra results to show the potential of HRMapNet for practical usage in autonomous driving, including crowdsourcing online map perception. Provided with an easily maintained global map, which may even be constructed by other vehicles, the performance of online vectorized map perception can be improved largely."}, {"title": "4.5 Qualitative Results", "content": "In Fig. 3, we show some qualitative comparisons in three challenging scenarios: severe occlusion, rainy day and poor lighting at night. The online map perception relying only on onboard sensors can be easily affected by these inevitable factors. Leveraging a historical rasterized map, HRMapNet helps to improve online map perception ability to handle such challenges. More visualized results and analysis are included in the supplementary material."}, {"title": "5 Discussion and Conclusion", "content": "In this paper, we propose to leverage historical information by maintaining a global rasterized map for improved online vectorized map perception. The global rasterized map can be constructed and maintained easily from past prediction results. We utilize such historical rasterized maps as complementary information for both BEV feature aggregation and query initialization. The proposed framework is compatible with most existing online vectorized map perception methods. It is demonstrated our proposed HRMapNet can boost two SOTA online vectorized map perception methods by a large margin. We expect HRMapNet can be a basis for crowdsourcing map perception: an accurate global rasterized map is maintained by a crowd of self-driving vehicles and serves as priors for accurate online vectorized map perception for each vehicle.\nLimitations. Our proposed HRMapNet mainly focuses on how to leverage a historical rasterized map for online vectorized map perception. We do not design elaborate map maintaining methods and only use a simple yet effective one from robotic occupancy grid mapping to merge local predictions to a global map. In practice, more intelligent methods could be explored and utilized, such as  for collaborative semantic mapping;  utilizing recurrent neural networks;  produces consistent rasterized maps from multiple predictions."}, {"title": "A Performance under Original MapTRv2 Setting", "content": "Here we provide the validation results on Argoverse 2 dataset in Tab. 8, under the original MapTRv2 setting. Here all training data at 10 Hz is used for training, and 2.5 Hz validation data is extracted for validation. Our HRMapNet still outperforms the baseline, MapTRv2."}, {"title": "B Performance under Challenging Conditions", "content": "To further demonstrate the improvements of utilizing a HRMap, we summarize the performance under different conditions in Tab. 9. It is clear the improvement is significant especially for these challenging conditions."}, {"title": "C Performance for Long Range", "content": "We test HRMapNet (StreamMapNet as baseline) by just increasing perception range. As in Tab. 10, with online constructed map, HRMapNet also boosts the baseline a lot. For such far ranges, we suggest loading a pre-built map like in P-MapNet for practical usage. We further test HRMapNet with a pre-built map from training data, and mAP is improved to 57.3 and 40.2, respectively. Besides, we believe HRMapNet can achieve better results with more careful settings, such as suitable map resolution, query number."}, {"title": "D Ablation on Decoder Layers", "content": "Our method keeps the map decoder module the same as the baseline methods , and 6 decoder layers are used in these methods. With the help of retrieved rasterized map as priors, the proposed query initialization module helps map element queries search for desirable map elements more efficiently. Here, we provide an ablation study of the number of decoder layers in Tab. 11 to further demonstrate the searching efficiency improved by this module. The results of not using query initialization are also listed for comparison.\nIncreasing decoder layers commonly brings higher accuracy. The comparison indicates using only 4 decoder layers with query initialization have already achieved good results, better than the method using 6 decoder layers without query initialization. It is because query initialization endows map element queries with priors."}, {"title": "E Discussion about Localization Error", "content": "By default, we train and test HRMapNet with the groundtruth ego-pose, which is a common practice in temporal-based map perception and object detection methods, such as StreamMapNet and BEVFormer . In the main body of the paper, it is demonstrated that HRMapNet has certain robustness to pose error even without specific design. The noise level is set according to common requirements in self-driving.\nTo further deal with localization error in practice, we can add pose noise as augmentation during training, and the results are listed in Tab. 12. The robustness is further enhanced even for large pose errors, with some compromise of accuracy. Furthermore, as a future work, we can change convolution-based to attention-based BEV feature aggregation to alleviate misalignment caused by large localization errors."}, {"title": "F Visualized Prediction Results", "content": "In Fig. 4, Fig. 5, and Fig. 6, we provide more visualized comparison results. All three methods are trained with 24 epochs. Ours is based on MapTRv2. The last column are visualized retrieved local rasterized maps. With the help of the local map, HRMapNet commonly achieves more accurate results, such as predicting map elements which are hard to recognize in images because of occlusion, bad weather or at long range."}, {"title": "G Visualized Global Rasterized Map", "content": "We provide some visualized examples of the final global rasterized maps in Fig. 7 and Fig. 8. These global maps are constructed from empty ones, and updated gradually by prediction results of testing data. We also provide a supplementary video for this process."}]}