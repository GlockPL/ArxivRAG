{"title": "CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models", "authors": ["Shubham Bharti", "Shiyun Cheng", "Jihyun Rho", "Martina Raut", "Xiaojin Zhu"], "abstract": "We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large language models. CHARTOM consists of specially designed data visualizing charts. Given a chart, a language model needs to not only correctly comprehend the chart (the FACT question) but also judge if the chart will be misleading to a human reader (the MIND question). Both questions have significant societal benefits. We detail the construction of the CHARTOM benchmark including its calibration on human performance.", "sections": [{"title": "Introduction", "content": "For AI to better assist humans, it must know not just factual truth but also how humans perceive the truth. The two could differ for many reasons such as information asymmetry or cognitive ability. If AI can detect the difference, it can help humans accordingly. The ability for an individual to reason about how others think (rather than the factual truth) is known as theory of mind [30]. A classic example is the Sally-Anne test [38, 2]: Sally hides a marble in a basket then leaves the room. Anne moves the marble to a box while Sally is away. Sally comes back. At this point, one can ask an observer two types of questions:\n\u2022 FACT question: Where is the marble?\n\u2022 MIND question: Where will Sally look for the marble?\nThere is a recent surge of interest in AI theory of mind, where large language models take the place of the observer. Several researchers found current AI competent at theory of mind tasks, performing near or surpassing human performance [20, 35], though some others remained skeptical [36, 33]. Concerned with limitations of AI theory of mind benchmarks, recent work"}, {"title": "Background on Data Visualizing Charts", "content": "Our theory of mind benchmark tests visual perception instead. Specifically, our focus is on misleading data visualizing charts, e.g. bar charts, line graphs, pie charts etc. Charts are widely used for conveying quantitative information to human readers. They lend credibility to accompanying messages and hence have a strong impact on human decision making. On the flip side, however, if charts are misleading, they can amplify the impact of misinformation [22, 29]. Figure 1 reproduces a misleading chart whose y-axis is inverted, which may give the impression that enacting Florida's \u201cstand your ground\u201d law reduced gun deaths. Misleading charts are unfortunately prevalent in many areas of our lives. For example, a review of medical advertisements found that 1/3 of charts were misleading [7]. A review of charts in Korean news about the COVID-19 pandemic showed that about 30% of bar charts and 44% of pictorial charts were misleading [21]. A U.S.-based analysis of visuals that had been identified as misleading by human fact checkers showed that more than half of these visuals were used as evidence for fake news about COVID-19 [4]. Because journalists often use charts to make narratives appear objective and to engage viewers emotionally [32], misleading charts can make an inaccurate story more deceptive [8, 37].\nAs individuals increasingly rely on digital media to inform their opinions and decision- making processes, it is essential to ensure that the visual representations of data they encounter are accurate and trustworthy. Studying misleading charts can lead to AI with the ability to automatically detect them and alert readers, thus empowering people to make well- informed"}, {"title": "Our Contributions", "content": "Our contribution is to create a visual theory-of-mind benchmark CHARTOM (for CHARt Theory of Mind) consisting of specially designed charts. The benchmark covers common varieties of chart types, and we planted representative visual manipulations into the charts. The key to the benchmark is to obtain MIND ground truth, i.e. \u201cdegree of misleadingness to humans\", for each chart. This ground truth enables future benchmark users to judge whether their AI correctly answers the MIND questions. We obtained the MIND ground truth via human experiments."}, {"title": "The CHARTOM Benchmark", "content": "In this section we present our visual theory of mind benchmark CHARTOM. We first detail our design of charts to accommodate both FACT and MIND type of questions in the benchmark, then explain how we obtain the MIND ground truth with human experiments."}, {"title": "Design of the CHARTOM Benchmark", "content": "To explain our CHARTOM benchmark, let us start with an example."}, {"title": "HMI: Ground Truth for MIND Questions", "content": "A key part of our benchmark is the ground truth answer for the MIND question on each chart, namely whether that chart is misleading to typical human readers."}, {"title": "Estimating HMI from Human Experiments", "content": "We propose Human Misleadingness Index (HMI), a number between 0 and 1 for each chart where a larger value means the chart is more misleading to humans. HMI is estimated with the following process."}, {"title": "Defining Acceptable Human Answers", "content": "Recall the FACT questions can be either multiple choices, sorting, or free text entry. It is straightforward to judge human answers for multiple choice and sorting questions (where we require all items are in the correct order), but it is more subtle for free text entry. For the purpose of calculating HMI, on free text entries we allow approximately correct answers. For example, we already discussed that the FACT answer is 1.2 for Figure 2. However, many participants understandably answered other numbers around 1.2, and a few participants answered drastically different numbers, see Table 2. A few remarks are in order:\n\u2022 For both the original (G7-Q1-1) and manipulated (G7-Q1-2) charts, the mode of the answer distribution is at the correct FACT answer 1.2.\n\u2022 For the manipulated chart G7-Q1-2, many participant answered 6. This answer was not observed for the original chart. We believe those participants were misled by the vertical perceptual bar heights. In other words, they used the wrong formula\n$\\frac{1}{\\text{(1 vertical unit)}}$ \u00d7 (6 vertical units) = 6 million bags.\n\u2022 The answer 300 is also frequent. We believe those participants simply read off the y-axis number of the 2019 bar.\n\u2022 Many participants answered other numbers around 1.2, 6, or 300.\nWe encountered similar diversity of human answers on other free text entry questions. These answers are indicative of the diverse cognitive processes in our participants, and are of independent interest to psychologists. From a computational perspective, we simplified the cognitive processes to define acceptable answers and assume that a participant will use one of following three problem solving strategies :\n1. The correct strategy. This leads to the correct FACT answer. For G7-Q1-1 and G7-Q1-2 the answer would be 1.2.\n2. The misled strategy where they \u201cfall for the manipulation\u201d. For G7-Q1-2 the answer would be 6.\n3. The skimming strategy where they just took the y-axis at face value. For G7-Q1-1 and G7-Q1-2 the answer would be 300."}, {"title": "HMI Discussions", "content": "Figure 3 shows the HMI values in the CHARTOM benchmark. These values are sorted from small (not misleading to humans) to large (very misleading). In addition, each HMI value is color coded to indicate whether the chart is the original version (blue) or the manipulated version (red). Generally speaking, the original charts have small HMI and the manipulated ones have large HMI. But this depends on the type of manipulation. As the HMI values in Table 1 indicates, some manipulations greatly misled people: inconsistent x-axis, inverted y-axis, dual axes plots, 3D effect, etc. While interestingly, some other manipulations did not confuse people much: compressed y-axis and pictorial bars are actually benign. Such differences are of interest to cognitive science, and are discussed in [31]."}, {"title": "Suggested MLLM Evaluation Criteria", "content": "We do not anticipate a particular output format for future MLLMs on our benchmark questions, but it should always be possible to manually compare the MLLM answers to our answer keys:\n\u2022 On FACT questions, we suggest using accuracy, defined as the number of correct answers over the number of FACT questions. FACT questions come in three types: multiple choice, ranking, and numerical (free text) answer. For the first two, correctness is precisely defined by the answer key (this means a ranking answer is correct if all items are ranked correctly). But for numerical answers, the answer key provides the exact answer assuming perfect perception. This is too strong an assumption. To judge numerical answers, we suggest using a \u00b110% tolerance interval: if the answer key is a, then an MLLM answer in the interval [0.9a, 1.1a] is deemed correct. Note this interval is for evaluating MLLMs, and is distinct from how we estimated HMI in section 2.2.2.\n\u2022 On the numerical part of the MIND questions, we suggest using mean squared error: $\\frac{1}{n}\\sum_{i=1}^{n}(m_i - a_i)^2$ where n is the number of MIND questions, $m_i$ is an MLLM's answer on the ith MIND question, and $a_i$ is the HMI answer key to that MIND question.\n\u2022 On the justification part of the MIND questions, we suggest qualitative comparisons to the \u201cchart manipulations\u201d file provided in the benchmark. That file records the single manipulation we planted to the manipulated (suffix _2) version of each chart pair. If a manipulated chart has a high HMI value, a capable MLLM should identify that manipulation as an important cause of misleadingness."}, {"title": "Conclusion", "content": "We described a new theory of mind benchmark CHARTOM for MLLMs based on visual chart comprehension. An important part of the ground truth is established by our human experiments. We hope our benchmark helps the development of future MLLMs."}]}