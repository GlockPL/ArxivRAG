{"title": "RAG with Differential Privacy", "authors": ["Nicolas Grislain"], "abstract": "Retrieval-Augmented Generation (RAG) has emerged as the dominant\ntechnique to provide Large Language Models (LLM) with fresh and relevant\ncontext, mitigating the risk of hallucinations and improving the overall quality\nof responses in environments with large and fast moving knowledge bases.\nHowever, the integration of external documents into the generation process\nraises significant privacy concerns. Indeed, when added to a prompt, it is not\npossible to guarantee a response will not inadvertently expose confidential\ndata, leading to potential breaches of privacy and ethical dilemmas. This paper\nexplores a practical solution to this problem suitable to general knowledge\nextraction from personal data. It shows differentially private token generation\nis a viable approach to private RAG.", "sections": [{"title": "Introduction", "content": "Retrieval-Augmented Generation (RAG, (Lewis et al. 2021)) has become a popular\napproach to enhance the capabilities of Large Language Models (LLMs) by supplying\nthem with up-to-date and pertinent information. This method is particularly valuable\nin environments where knowledge bases are large and rapidly evolving, such as news\nwebsites, social media platforms, or scientific research databases. By integrating\nfresh context, RAG helps mitigate the risk of \u201challucinations\u201d\u2014instances where\nthe model generates plausible but factually incorrect information\u2014and significantly\nimproves the overall quality and relevance of the responses generated by the LLM.\nHowever, incorporating external documents into the generation process introduces\nsubstantial privacy concerns. When these documents are included in the input\nprompt for the LLM, there is no foolproof way to ensure that the generated response\nwill not accidentally reveal sensitive or confidential data (Qi et al. 2024). This\npotential for inadvertent data exposure can lead to serious breaches of privacy and\npresents significant ethical challenges. For instance, if an LLM is used in a healthcare\nsetting and it accidentally includes patient information from an external document\nin its response, it could violate patient confidentiality and legal regulations.\nThis paper describes a practical solution (DP-RAG) aimed at addressing these\nprivacy concerns with Differential Privacy (DP). The solution is based on two pillars:"}, {"title": "Related Work", "content": "In general there are two families of approaches to add new knowledge to an LLM. The\nfirst is Fine Tunning (FT) and the other is Retrieval Augmented Generation (RAG).\nIn both these approaches, adding privacy can be done, through simple heuristics\nwith human validation such as masking or using a systematic and principle-based\napproach such as Differential Privacy."}, {"title": "Private Fine-Tuning", "content": "A straightforward approach to adding knowledge to an existing LLM is to continue\nits training with the new knowledge, to Fine Tune (FT) it. However, this raises\nchallenges when dealing with private data, as LLMs tend to memorize training data.\n(see (Shokri et al. 2017) or (Carlini et al. 2021)).\nTo mitigate this privacy risk, it is possible to redact sensitive content prior to the\nFT process (aka. masking), but this operation is not very reliable and requires\njudgment on what should be redacted. This is a difficult manual operation based\non the perceived sensitivity of each field and how it can be used to re-identify an\nindividual, especially when combined with other publicly available data. Overall, it\nis very easy to get wrong; leaning too much on the side of prudence can yield useless\ndata, while trying to optimize utility may result in leaking sensitive information.\nA solution to this problem is to leverage Differential Privacy, a theoretical framework\nenabling the computation of aggregates with formal privacy garantees (See (Dwork,\nRoth, et al. 2014)).\nThe most common approache to Private LLM FT is to use Differentially-Private-\nStochastic-Gradient-Descent (DP-SGD, see (Abadi et al. 2016) and (Ponomareva et\nal. 2023)). DP-SGD is about clipping gradients and adding them some noise while\nrunning your ordinary SGD (or standard variants such as Adam, etc.). This method\nrequires the data to be organized per privacy unit (typically a privacy unit will be a\nuser). Every training example should belong to one and only one privacy unit\u00b9.\nBut, when new documents are frequently added to the private knowledge base FT\nmay not be the best approach."}, {"title": "Private RAG", "content": "When FT is not the best approach to adding new knowledge and RAG would be\npreferred, DP-FT cannot help with privacy. In these cases, DP can still be leveraged\nin different ways. A straightforward approach to DP RAG is to generate synthetic\ndocuments with differential privacy out of the private knowledge base and then\nretrieve documents from this synthetic knowledge base instead of the private one.\nAnother approach is to generate the LLM response in a DP way.\nThe approach of generating synthetic documents usable for RAG in privacy-sensitive\ncontexts has been explored by (Zeng et al. 2024) but without DP guarantees. There\nare three main approaches to the problem of generating DP Synthetic Data (SD):\nFine-Tuning a pre-trained generative model with DP to generate synthetic\ndocuments.\nUse some form of automated prompt tuning to generate synthetic prompts or\ncontext documents.\nAnd use DP aggregated generation.\nFine-Tuning a pre-trained generative model with DP can be done with DP-SGD\n((Abadi et al. 2016) and (Ponomareva et al. 2023)) as mentioned above. An\napplication to synthetic text generation is described there: (Yue et al. 2023).\nThis method is technically complex, as, DP-SGD can be challenging to implement\nefficiently (Bu et al. 2023).\nIn (Hong et al. 2024), the authors use an automated prompt tuning technique\ndeveloped in (Sordoni et al. 2023) and (Zhou et al. 2023) and make it differentially\nprivate. From the evaluations presented, it seems to compare favorably to DP-FT\nsynthetic data approaches. Similar methods, based on DP-automated prompt tuning\nare exposed in (Lin et al. 2024) for images and (Xie et al. 2024) for text.\nA last approach to generating synthetic data is based on DP aggregation of\ndata. (Lebensold et al. 2024) or (Wu et al. 2023) show how to aggregate images or\ntext in their embedding space (aka. Embedding Space Aggregation). Aggregating\ndata privately is also the approach of (Tang et al. 2024), but they do it at the token\nlevel.\nThis last method greatly inspired the approach described in this document, though\nnot for SD, but to directly generate RAG output from private documents."}, {"title": "DP-RAG", "content": "To overcome the limitations of DP FT or SD-based RAG, we developed and tested\nDP-RAG: a novel approach, build upon recent works on DP In-Context Learning\n(ICL) such as (Wu et al. 2023) and particularly (Tang et al. 2024).\nContrary to (Wu et al. 2023), we aggregate outputs token by token.\nOur token aggregation method is different from both methods exposed in:\n(Tang et al. 2024) (Gaussian and Report Noisy Max)."}, {"title": "Overview of DP-RAG", "content": "DP-RAG is made of two main components:\nA method to collect documents related to the question in a way that does not\nprevent its output to be used in a DP mechanism.\nA method to use the collected documents to prompt a LLM and produce a\nresponse with DP guarantees.\nTo understand the need for these components, let's describe what RAG is usually\nmade of (see also (Lewis et al. 2021)) and introduce some notations (see Fig. 1).\nA LLM: $\\mathcal{L}$ is a function, taking some text, in the form of a sequence of tokens:\n$x = (x_1,x_2,...,x_n)$ as input and outputting a probability distribution of the next\ntoken $x_{n+1}$ conditional on x:\n$\\mathcal{L}(s,x) = \\mathcal{L}(s, (x_1, x_2,...,x_n)) = Pr(x_{n+1} = s|\\mathcal{L}, x_1, x_2, ..., x_n)$\nWe assume we have a set of N documents: $D = {d_1,d_2, ...,d_v} \\subset \\mathcal{D}$ containing\ndomain specific knowledge. These documents are also sequences of tokens: $d_i =$\n$(d_{i,1}, d_{i,2},..., d_{i,l_i})$. We will, for simplicity, denote $(d_i, d_j)$ the concatenation of two\nsequences of token, or a sequence and one token.\nWe also assume we have a similarity function $S : \\mathcal{D}^2 \\leftrightarrow [-1,1]$ which value is\nclose to 1 when two documents are very similar, close to 0 when independent, and\nclose to -1 when conveying opposite meaning. In this work S will be the cosine\nsimilarity between some embeddings of the documents, mapping them to some\nadequate d-dimensional vector space: $\\mathbb{R}^d$:\n$S(d_i, d_j) = \\frac{\\langle E(d_i), E(d_j) \\rangle}{||E(d_i)||_2||E(d_j) ||_2}$\nWhen receiving a query in the form of a sequence of token: $q = (q_1,q_2,..., q_{n_q})$, the\nsimilarity between q and each document is computed and the top k documents in\nterm of similarity are collected:\n$d_{i_1}, d_{i_2},...d_{i_k}$ with $S(q, d_{i_1}) \\geq S(q, d_{i_2}) \\geq ... \\geq S(q, d_{i_k})$\nThen a new query $q_{RAG}$ is built by concatenating the original query q with the top k\ndocuments and other elements (the operation is denoted $(.,...,.)_{RAG}$)\n$q_{RAG} = (q, d_{i_1}, d_{i_2},... d_{i_k})_{RAG}$\nThe augmented query is then sent to the LLM to compute the distribution of the\nnext token (the first token of the response)\n$\\mathcal{L} (r_1, (q, d_{i_1}, d_{i_2},... d_{i_k})_{RAG})$"}, {"title": "Differential Privacy and its application to RAG", "content": "The token is generated by sampling according to the distribution\u00b2 or by selecting\nthe mode of the distribution\u00b3.\nThe tokens of the response are then generated one by one in an auto-regressive\nmanner. The generated response tokens are concatenated to the input sequence:\n$\\mathcal{L}(r_{j+1}, \\langle \\langle q, d_{i_1}, d_{i_2},... d_{i_k} \\rangle _{RAG}, r_1, r_2,..., r_j \\rangle )$\nIn the private variant of the problem (DP-RAG), we also assume the documents are\nprivacy sensitive, and make the additional assumption that each document relates\nto only one individual that we call privacy unit (PU)4.\nA (randomized) algorithm: A provides $(\\epsilon, \\delta)$-Differential Privacy if and only if for\nall event S and neighboring datasets $D_0$ and $D_1$, we have:\n$Pr[A(D_0) \\in S] < e^{\\epsilon} Pr[A(D_1) \\in S] + \\delta$\nThis means that for datasets that differ by one individual (i.e. neighboring datasets)\nthe algorithm's outputs are statistically indistinguishable. This property guarantees\nthat no bit of information can reasonably be learned about an individual. See (Dwork,\nRoth, et al. 2014) for a thorough introduction to DP.\nThe two main challenges to implementing RAG with DP guarantees consist in:"}, {"title": "Privacy Unit Preserving Document Retrieval", "content": "As mentioned above, DP deals with the concept of neighboring datasets. For this\nreason, it is convenient to assign each document to one and only one individual, or\nprivacy unity (PU). Adding or removing one PU, comes down to adding or removing"}, {"title": "Differentially Private In-Context Learning", "content": "one document. In this context, one should be careful with the selection of the top-k\nmost relevant documents. Indeed, when selecting the top-k documents, adding or\nremoving one document may affect the selection of other documents.\nIn DP-RAG, the similarity of each document with the query is computed:\n$s_1, s_2,...,s_N = S(q, d_1), S(q, d_2), ..., S(q, d_N)$\nTo estimate a threshold to select the top k documents with DP, we designed a utility\nfunction to be plugged into an exponential mechanism (Dwork, Roth, et al. 2014)\n$\\mathcal{U}_{top-k}(\\tau): [0,1] \\rightarrow \\mathbb{R}\n\\sum_{i}[0,s_i] (\\tau) - k$\nThis top-k utility has sensitivity 1, we can sample a threshold $\\tau_{DP}$ from the probability\ndensity function:\n$\\tau_{top-k} \\propto exp (\\frac{\\epsilon \\mathcal{U}_{top-k}(\\tau)}{2})$\nIt is easy to show $\\tau_{top-k}$ is $\\epsilon$-DP (see. (Dwork, Roth, et al. 2014)).\nThe DP top-k threshold $\\tau_{top-k}$ sampled from the exponential mechanism is then used\nto select all the documents whose similarity is above $\\tau_{top-k}$.\nWhile this threshold, works well in practice, it selects a fixed number of documents\n(~k). We may be interested in selecting fewer when the top scores are more concen-\ntrated on few documents (the query is selective), and select more when the scores are\nevenly spread across many documents (the query has a low selectivity). To adjust tothis need, we designed a slightly different utility function:\n$\\mathcal{U}_{top-p}(\\tau): [0,1] \\rightarrow \\mathbb{R}\n\\sum_{i}[0,s_i] (\\tau)w(s_i) - p\\sum w(s_i)$\nwith:\n$w(s) = exp(\\alpha \\frac{s - s_{max}}{s_{max} - s_{min}}) \\in [0, 1]$ when $\\alpha > 0$\nand similarly:\n$\\tau_{top-p} \\propto exp (\\frac{\\epsilon \\mathcal{U}_{top-p}(\\tau)}{2})$\nThis utility function is parametrized by \u03b1 which contrasts the differences\nin scores, and p which select the share of total document weight we want to select\nwith the mechanism.In DP ICL, instead of sampling the next token from a query enhanced with many\ndocuments:\n$\\mathcal{L}_{j+1}(\\cdot) = \\mathcal{L} (\\cdot, (\\langle q, d_{i_1}, d_{i_2},... d_{i_k} \\rangle _{RAG}, r_1, r_2,..., r_j))$"}, {"title": "Evaluation", "content": "we compute the distributions of the next token for many enhanced queries, each of\nthem with just one document:\n$\\mathcal{L}_{j+1,i_1} (\\cdot) =\n\\mathcal{L}(\\cdot, (\\langle q, d_{i_1} \\rangle _{RAG}, r_1, r_2,..., r_j))$\n$\\mathcal{L}_{j+1,i_2} (\\cdot) =\n\\mathcal{L}(\\cdot, (\\langle q, d_{i_2} \\rangle _{RAG}, r_1, r_2,..., r_j))$\n$\\mathcal{L}_{j+1,i_k} (\\cdot) =\n\\mathcal{L}(\\cdot, (\\langle q, d_{i_k} \\rangle _{RAG}, r_1, r_2,..., r_j))$\nWe also compute the distribution of the next token, with some public context:\n$\\mathcal{L}_{j+1,pub}(\\cdot) =\n\\mathcal{L}(\\cdot, (\\langle q, d_{pub} \\rangle _{RAG}, r_1, r_2,..., r_j))$\nFollowing (Tang et al. 2024), we sample a next token based on a DP aggregation of\nthe k + 1 distributions.\nContrary to (Tang et al. 2024) where they compare two mechanisms: Gaussian and\nReport Noisy Max, and use a public prior with Reduce Vocab Publicly (RVP) we\nintroduce a different mechanism:\nWe use an exponential mechanism with a utility aggregating transformed\nlog-probability vectors from all the enhanced queries.\nWe do not use Reduce Vocab Publicly (RVP), but a soft version, consisting in\nusing the log-probabilities of a public response to boost or mute some tokens\nin a soft way.\nWe sample the next token from an exponential mechanism where the utility is the\naggregation of some function: $\\mathcal{I}_{clipped}$ modulated by the log-probabilities associated\nwith the public query. The larger, the \u03b8, the closer the response will be to the one\nwithout the private documents. This replaces RVP from (Tang et al. 2024).\n$\\mathcal{U}_{ICL}(r) = \\theta \\cdot ln (\\mathcal{L}_{j+1,pub}(r)) + \\sum \\mathcal{I}^{clipped}_{j+1,i_j}(r)$\nIn the previous expression $\\mathcal{I}^{clipped}$ is a clipped version of $\\mathcal{I}^{centered}$. $\\mathcal{I}^{centered}$ is clipped to\nbound its sensitivity ($\\infty$-norm) in the exponential mechanism to some C.\n$\\mathcal{I}^{clipped}_{j+1,i_j}(r) =\n\\mathcal{I}^{centered}_{j+1,i_j}(r) \\cdot min \\bigg[1, \\frac{C}{\\underset{s}{max} |\\mathcal{I}^{centered}_{j+1,i_j}(s)|} \\bigg]$\nWhere $\\mathcal{I}^{centered}$ is a centered version of $\\mathcal{I}^{norm}$ to minimize its $\\infty$-norm without changing\nits impact in the mechanism:\n$\\mathcal{I}^{centered}_{j+1,i_j}(r) =\n\\mathcal{I}^{norm}_{j+1,i_j}(r) - \\frac{\\underset{s}{max} \\mathcal{I}^{norm}_{j+1,i_j}(s) + \\underset{s}{min} \\mathcal{I}^{norm}_{j+1,i_j}(s)}{2}$\nWhere $\\mathcal{I}^{norm}$ is a transformation of L putting more emphasis on the large values of L.\n$\\mathcal{I}^{norm}_{j+1,i_j}(r) =\nexp \\bigg[\\alpha \\cdot \\bigg(ln \\mathcal{L}_{j+1,i_j}(r) - \\underset{s}{ln max} \\mathcal{L}_{j+1,i_j}(s) \\bigg)\\bigg] - 1$\nIndeed, for $\\alpha = 1$ we simply compute a scaled and shifted version of the probability:\n$\\mathcal{I}^{norm}_{j+1,i_j}(r) =\n\\frac{\\mathcal{L}_{j+1,i_j}(r)}{\\underset{s}{max} \\mathcal{L}_{j+1,i_j}(s)} - 1$\nfor a very small, we compute the log-probabilities:\n$\\mathcal{I}^{norm}_{j+1,i_j}(r) \\approx ln \\mathcal{L}_{j+1,i_j}(r) - \\underset{s}{ln max} \\mathcal{L}_{j+1,i_j}(s)$\nand for a very large, we get an indicator function:\n$\\mathcal{I}^{norm}_{j+1,i_j}(r) \\approx 0$\nif r\n$\\argmax \\mathcal{L}_{j+1,i_j}(s)$ and -1 elsewhere\nAfter the utility is computed, the next token is sampled from:\nr$\\propto exp \\bigg(\\frac{\\epsilon \\mathcal{U}_{ICL}(r)}{2C}\\bigg)$\nIn this formula, the larger the $\\epsilon$ (privacy loss), or the smaller the clipping C the\ncloser we are to the most likely token.\nThe small (pure) privacy losses incurred during the retrieval phase and the ICL\nphase are aggregated in a simple privacy accountant: github.com/google/differential-privacy/tree/main/python/dp_accounting.\nThe code of DP-RAG is available on github.com/sarus-tech/dp-rag.\nThe DP-RAG algorithm, was tested on synthetic documents available on Huggingface\nhuggingface.co/datasets/sarus-tech/medical_dirichlet_phi3. The main benefit of\nusing synthetic data is to make sure the LLM used does not know anything about\nthe data a priori.\nHere are a few examples documents:\nOverall DP-RAG, provides a viable approach to private RAG in contexts where\ndocuments are organized by individual (e.g. Electronic Health Records financial"}, {"title": "Conclusion", "content": "statements) and where sufficiently many documents cover the question asked so that\nno one individual has an impact on the response.\nTo improve the accuracy / privacy tradeoff, one can:\nAsk question with shorter responses (and limit the number of tokens generated).\nMake sure many documents are related to the question.\nIncrease the impact of the public prior (0) if some elements of the response are\npublic."}]}