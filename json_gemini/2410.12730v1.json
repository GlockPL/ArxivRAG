{"title": "Counterfactual Generative Modeling with\nVariational Causal Inference", "authors": ["Yulun Wu", "Louie McConnell", "Claudia Iriondo"], "abstract": "Estimating an individual's potential outcomes under counterfactual treatments is a\nchallenging task for traditional causal inference and supervised learning approaches\nwhen the outcome is high-dimensional (e.g. gene expressions, facial images) and\ncovariates are relatively limited. In this case, to predict one's outcomes under\ncounterfactual treatments, it is crucial to leverage individual information contained\nin its high-dimensional observed outcome in addition to the covariates. Prior\nworks using variational inference in counterfactual generative modeling have been\nfocusing on neural adaptations and model variants within the conditional variational\nautoencoder formulation, which we argue is fundamentally ill-suited to the notion\nof counterfactual in causal inference. In this work, we present a novel variational\nBayesian causal inference framework and its theoretical backings to properly handle\ncounterfactual generative modeling tasks, through which we are able to conduct\ncounterfactual supervision end-to-end during training without any counterfactual\nsamples, and encourage latent disentanglement that aids the correct identification\nof causal effect in counterfactual generations. In experiments, we demonstrate the\nadvantage of our framework compared to state-of-the-art models in counterfactual\ngenerative modeling on multiple benchmarks.", "sections": [{"title": "1 Introduction", "content": "In traditional causal inference, heterogeneous treatment effect is typically formulated as the inter-\nvention model p(Y|X, do(T)) (or E[Y|X, do(T)]) with outcome Y, covariates X, treatment T, and\nestimated by the observed covariate-specific efficacy p(Y|X, T) under the ignorability assumption\n[35; 31] using various supervised learning and estimation techniques. However, in cases such as the\nsingle-cell perturbation datasets [8; 28; 38] where Y has thousands of dimensions while X has two\nor three categorical features, such model could hardly be relied on to produce useful individualized\nresults. To construct outcome Y' under a counterfactual treatment T' in such high-dimensional out-\ncome scenario, it is important and necessary to learn the individual-specific efficacy p(Y'|Y, X,T,T')\nconditioning on the factual outcome Y itself, which leverages the rich information embedded in the\nfactual outcome that cannot be recovered by the handful of covariates. For example, given a cell\nwith type A549 (X) that received SAHA drug treatment (T), we may want to know what its gene\nexpression profile (Y') would look like if it had received Dex drug treatment (T') instead. In this case,\nwe would want to take the profiles of other cells with type A549 that indeed received Dex as reference,\nsuch that the counterfactual construction Y' exhibits the treatment characteristics of Dex on A549,\nbut would also want to extract as much individual information as possible from its own expression\nprofile (Y) such that the counterfactual construction could preserve this cell's individuality.\nYet the lack of observability of counterfactual outcome Y' makes this objective intractable and hence\npresents a major difficulty for supervised learning when Y is taken as an input. In previous works\nthat involve self-supervised counterfactual generation [19; 26; 54; 29; 53; 17; 25; 37; 9; 40] based\non Variational Autoencoders (VAE) [18] and Generative Adversarial Networks (GAN) [10], there\nis no counterfactual supervision during training, and hence no explicit regulation on the trade-off\nbetween treatment characteristics and individuality. When such regulation does not exist, there is\na disconnection between training and counterfactual inference. To see this, consider a conditional\ngenerative model go (y, t) that is only supervised on the reconstruction loss with respect to outcome y\nduring training. In this case, the model is free to ignore the extra treatment t and converge to a state\nsuch that go (y, t) = go(y, 0) [5], hence preserving maximum individuality yet minimum treatment\ncharacteristics in counterfactual construction go(y, t') during inference. This is particularly an issue\nfor high-fidelity counterfactual generation with state-of-the-art Hierarchical Variational Autoencoders\n(HVAE) [46; 6], as the latent representations possess even larger number of bottleneck dimensions\ncompared to the observed outcome y. In more recent works on counterfactual generative modeling\nwith HVAEs and diffusion models [36; 27; 33], only Ribeiro et al. [33] touched on the issue of\ncounterfactual supervision, yet its proposed method does not have a theoretical backing in variational\ninference, nor could it be conducted end-to-end during training. As we will see in the proceeding\nsections of this work, there is a fundamental incompatibility of the conditional VAE formulation with\ncounterfactual generative modeling, which is the root cause of such issue. In fact, this incompatibility\nalso exhibits itself in another important aspect of counterfactual generative modeling which is latent\ndisentanglement \u2013 the partially abducted exogenous noise (i.e. shallow-level latent representation) in\nconditional HVAEs and diffusion models is able to encode individuality better than traditional VAEs,\nbut at the same time is still heavily entangled with treatment variables and often result in lingering\ncharacteristics of the factual treatment in counterfactual construction [27; 36]. More introduction\nregarding latent disentanglement can be found in Appendix A.\nOur contributions in this work are summarized as follow: 1) we propose a formulation and stochastic\noptimization scheme for counterfactual generative modeling by specifically formulating counter-\nfactual variables and using variational inference to derive the evidence lower bound (ELBO) for\nthe individual-level likelihood p(Y'|Y, X,T,T') \u2013 a well-motivated and fitting objective for coun-\nterfactual generative modeling instead of the marginal-level (interventional) conditional likelihood\np(Y|X,T) or even joint likelihood p(Y, X, T) used by VAE-based prior works. To emphasize,\nthe contribution here is not to propose another model variant under the conditional VAE\nformulation such as HVAEs or diffusion models [36; 27; 33], but to fundamentally change the\nVAE formulation. We call this framework Variational Causal Inference (VCI). An explanation to\nwhy the conditional VAE formulation (including diffusion models) is ill-suited to counterfactual\ngenerative modeling and a straightforward comparison between VAE's formulation and ours can be\nfound in Appendix B. 2) Our proposed optimization scheme allows us to conduct counterfactual\nsupervision end-to-end during training alongside self-supervision, even without any counterfactual\nobservations, which has not been done in prior works to the best of our knowledge. 3) This workflow\nof constructing and supervising counterfactual outcomes during training presents us the unique op-\nportunity to naturally enforce latent disentanglement through distribution alignment on matching\npairs [41; 24; 3] without having paired data, which has not been done in counterfactual generative\nmodeling, to the best of our knowledge. 4) We further propose a robust estimation scheme for high-\ndimensional marginal causal parameters leveraging this individual-level counterfactual construction\nframework. No prior work in counterfactual generative modeling conducts such asymptotically\nefficient marginal estimation upon acquiring individual predictions, to the best of our knowledge.\n5) In experiments, our proposed method is evaluated on datasets with vector outcomes \u2013 single cell\nperturbation datasets, as well as datasets with image outcomes \u2013 facial imaging and handwritten digits\ndatasets, and compared to state-of-the-art models in the two domains. The results show that ours\noutperformed state-of-the-arts in both domains with notable margins. Related work and comparative\nanalysis can be found in Appendix C."}, {"title": "2 Proposed Method", "content": ""}, {"title": "2.1 Formulations and Intuitions", "content": "Let outcome Y : \u03a9 \u2192 (V, \u03a3y) be a random vector, X : \u03a9 \u2192 (X, Ex) be a mix of categorical\nand real-valued covariates, T : \u03a9 \u2192 (\u03a4, \u03a3\u03c4) be a categorical or real-valued treatment (or multiple\ntreatments) and Z : \u03a9 \u2192 Rh be the latent feature vector. Suppose the causal relations between\nrandom variables (and random vectors) follow a structural causal model (SCM) [31] defined in\nFigure 1, where we adopt twin networks [1] to formulate counterfactuals Y' and T' as separate\nvariables apart from Y and T having a conditional distribution p(Y', T'|Z, X) identical to that of\nits factual counterpart p(Y, T|Z, X). Different from Vlontzos et al. [51], we introduce latent Z in\nhigh-dimensional outcome settings as a feature vector of Y such that random factor ey does not\ncapture any more uncertainty that can be retrieved from its counterfactual counterpart, i.e. ey | Y'\nand similarly ey \u2013 Y. Under the consistency assumption (Assumption 2 and Remark 2), ey = 0\nand there is no more uncertainty in Y beyond Z, i.e. the shared random factor Uy fully captures\nthe exogenous noise injected to Y and Y'. Hence, Z in this setting can be seen as an unobserved\nsummary random vector of the covariates X and exogenous noise Uy. Note that the consistency\nassumption is not generally required for our subsequent variational inference, but is required for\nY!\nYdo(T') to be formally defined as the counterfactual of Y under the three layer causal hierarchy of\nPearl [32]. The ignorability assumption (Assumption 1 and Remark 1) is generally required, under\nwhich do(T' = t') and T' = t' result in the same outcome distribution on Y' conditioned on X or Z.\nSee Appendix D for the connection between our formulation and the traditional SCM formulation.\nUnder this formulation, Z has a posterior distribution p(Z|Y, T, X) given the observed variables; Y\nand Y' can be constructed by p(Y|Z,T) and p(Y'|Z, T') respectively with latent features Z. Similar\nto prior works in autoencoder [50; 2], we estimate the latent recognition model (i.e. exogenous\nnoise abduction model) and outcome construction model with deep neural network encoder q and\ndecoder pe. Given a sample b = (x, t,t', y), while the reconstruction yo,\u00a2 of y is self-supervised,\nwe can only assess the counterfactual construction Yo, under t' by looking at its resemblance to\nsimilar individuals that indeed received treatment t'. Hence, naively, we may conduct counterfactual\nsupervision during training with a semi-autoencoding (SAE) loss function as follow\n\nLe,$(b) = L2(y0,\u00a2, Y) \u2013 w \u2022 \\lp(Y'|x,t') (Yo,\u00a2)\n\nwhere w is a scaling coefficient and \u00ee\u00ee is the traditional covariate-specific outcome model fit on the\nobserved variables (X, T, Y) (notice that p(Y'|x,T' = t') = p(Y|x,T = t')). The intuition is\nthat, if yo, is indeed one's outcome under t', then the likelihood of yo, coming from the outcome\ndistribution of individuals with the same attributes x that factually received treatment t' should be\nhigh. In practice, we can fit \u00ee\u00ee(Y|X, T) in an end-to-end fashion using a discriminator D(X, T, Y)\non factual triplets (x, t, y) and counterfactual triplets (x, t', yo,$) with the adversarial approach [10].\nSpecifically, we use discriminator loss LD(b, ye,$) = \u2212 log[D(x, t, y)] \u2013 log[1 \u2013 D(x, t', yo,$)] to\ntrain model D, and use generator loss l\u00f4(Y'|x,t') (Ye,$) = log D(x, t', yo,\u00a2) in Le,$(b) (notice that\np(X,T) = p(X,T')). This end-to-end approach prevents (pe, q$) from exploiting a pre-trained\np model. The SAE trained through the adversarial approach motivates the derivation of our main\ntheorem presented in the next section, and serves as a baseline comparison to our main algorithm\nin the ablation study of the experiments section. In cases where covariates are limited and discrete\nsuch as the single-cell perturbation datasets, \u00ee\u00ee(Y|X, T) can simply be a smoothed empirical outcome\ndistribution under treatment T stratified by covariates X."}, {"title": "2.2 Variational Causal Inference", "content": "Now we present our main result by rigorously formulating the objective, providing a probabilistic\ntheoretical backing, and specifying an optimization scheme that reflects the intuition described in the\nprevious section. Suppose we want to optimize p(Y'|Y, X, T, T') instead of the traditional outcome\nlikelihood p(Y|X, T). The following theorem induces the evidence lower bound (ELBO) and thus\nprovides a roadmap for stochastic optimization:\nTheorem 1. Suppose a collection of random variables W follows a causal structure defined by the\nBayesian network in Figure 1. Then J(D) = log[p(Y'|Y, X, T, T')] has the following variational\nlower bound:\n\nJ(D) \u2265 Ep(Z|Y,T,x) log [p(Y|Z,T)] \u2013 D [p(Y|X,T) || p(Y'\\X,T')]\nDKL [P(Z|Y,T, X) || p(Z|Y',T', X)]\n\nwhere D[p || q] = log p - log q.\nProof of Theorem 1 and proofs of all theoretical results in the following sections can be found\nin Appendix H. Theorem 1 suggests that, in order to maximize the individual-specific outcome\nlikelihood p(Y'|Y, X, T, T'), we can instead maximize the following estimated ELBO parameterized\nwith estimating models pe and q:\n\nJe,$(b) = Eq4(z|y,t,x) log [pe(y|z, t)] + log [p(yo,|x, t')]\nDKL [q$(Z|y,t,x) || q\u00a2(Z|Yo,\u00a2,t',x)],\n\nwhere yo,\u00a2 = Epo(Y'|Eq&(Z|y,t,z) Z,t') Y' and \u00ee\u00ee can be estimated with the approaches described at the\nend of Section 2.1. Note that p(y|x, t) does not impose gradient on (pe, q$) and is thus omitted in\nJe,$(b). The stochastic optimization of ELBO is hence conducted on the objective Jpdata (Po, q$) =\nEpdata Je,$(b) where t' ~ Pdata(Tx). A figure demonstrating the workflow of this stochastic\noptimization scheme can be found in Figure 2.\nAs can be seen, Equation 3 consists of the expected individual-specific reconstruction like-\nlihood Eq$(z|y,t,x) log[pe(Y|z,t)] and the covariate-specific counterfactual outcome likelihood\nlog[p(yo,|x, t')], echoing the intuition highlighted by Equation 1, with an additional divergence term\n-DKL[q$(Z|y,t,x) || q\u00a2(Z|Yo,\u00a2,t',x)] that regularizes the similarity across latent distributions\nunder different potential scenarios. Note that this framework is fundamentally different from any\nVAE based formulation such as CVAE [42] or CEVAE [26], since the variational lower bound is\nderived directly from the causal objective and the KL-divergence term serves causal purposes that are\nentirely different from the prior latent bounding purpose of VAEs. An explanation of this divergence\nterm is given in the paragraph below. A comparison between VCI and conditional VAE using standard\nVAE notations is given in Appendix B. A summary of different optional gradient detaching patterns\nfor training stability is given in Appendix J. An approach to conduct counterfactual supervision\nimplicitly using traditional variational inference is given in Appendix E.\nDivergence Interpretation The divergence term -DKL[q\u00a2(Z|y,t,x) || q\u00a2(Z|Yo,\u00a2,t', x)] is the\nkey of the framework and it serves two purposes. Firstly, it adds robustness to latent encoding\nby encouraging the encoding model to recognize individual features contained in both factual and\ncounterfactual outcomes. In fact, these common features are well disentangled from the treatment\nvariables, which we will discuss in-depth in the next section. Secondly, it encourages the preservation\nof individuality in counterfactual outcome constructions. To see this, notice that if the counterfactual\noutcome construction was only penalized by the covariate-specific likelihood loss \u2013 log[p(ye,$|x, t')],\nthe counterfactual decoding model po(Y'|z\u00f8, t') could learn to completely discard the identity of\nindividual subjects represented in latent features zo once it detects a counterfactual treatment t' \u2260 t\nin the inputs. Such behavior is regulated by the divergence term, since otherwise we would not be\nable to recover a latent distribution q$(Z|Yo,\u00a2, t', x) close to q$(Z|y, t, x).\nRobust Marginal Estimation Similar to the augmented inverse propensity weighted (AIPW)\nestimator in traditional causal inference, the individual-level model predictions acquired by our\nframework can also aid the robust estimation of average treatment effect (p) = Ep[Ydo(T'=0)] for a\ntreatment level a of interest if such estimation is desired. Under the formulation in Figure 1, we have\nthe following robust estimator that is asymptotically efficient under some regularity conditions [47]:\n\nIn(0) =\n1-2\nn\nk=1\n(I(tk = a)\n\u00ea(tkxk)\nYk +\n1_ I(tk = a)\n\u00ea(tkxk)\nEpa [Y'|zk,\u00a2,T' = a]},\n\nwhere ok = (xk, tk, Yk) ~ p(x, t, y) is the vector of observed triplet of the k-th individual, zk,\u03c6 =\nq(Yk, tk, xk), and \u00ea is an estimation of the propensity score that satisfies the positivity assumption\n[34]. See Appendix G for the derivation of this estimator. Notice that the regression adjustment\nterm Epo [Y'|Zk,, T' = \u03b1] in ours has the ability to vary across different individuals within the same\ncovariate group (which is not the case for that of the AIPW estimator in traditional causal formulation)\nand it is meaningful to conduct the robust estimation of covariate-specific marginal treatment effect\n\u039e(p) = Ep[Ydo(X=c,T'=a)] for a given covariate e of interest, see Appendix G for details."}, {"title": "2.3 Latent Identifiability and Disentanglement", "content": "In any deep probabilistic model, there is a certain degree of freedom and arbitrarity to the learnt latent\ndistribution, even with latent restrictions. Naturally, one might question the latent identification of our\nvariational causal framework: when identifying counterfactual Y' under T', does the learnt latent Z\ninevitably carry over some characteristics of the factual treatment in Y and T? Ideally, we would like\nto aid counterfactual construction by attaining a clean disentanglement between the learnt latent and\nthe observed treatment, such that the counterfactual generation does not preserve influence from the\nfactual treatment. Such disentangled exogenous noise abduction fits the semantic of our generating\nprocess (Figure 1), where Z and T are not descendant of each other and ZTX, as well as the\nsemantic of causal identification in the traditional formulation, where the identifiability of Ydo(T)\nrelies on the ignorability assumption that exogenous noise does not contain unobserved confounders.\nIn this section, we show that optimizing the VCI framework in fact grants such disentanglement\nbetween Z and T under mild assumptions, leading to a more in-depth understanding of the purpose\nand behavior of the latent divergence term in VCI's optimization scheme.\nSimilar to Shu et al. [41], we formally define and measure disentanglement in terms of (encoder)\nconsistence and (encoder) restrictiveness, except that our definitions are generalized to the case of\nstochastic models and models with auxiliary attributes. For notation simplicity of the oracle operation\non functions with auxiliary inputs, we use \u03b3\u03bf\u03b6(\u03b1; b) to denote (c, b) where c = ((a) if ( is a\ndeterministic function and c ~ (a) if ( is a probability measure. The definitions of consistence,\nrestrictiveness, and disentanglement are then given in Appendix F."}, {"title": "3 Experiments", "content": "We present experimental results of our framework on two datasets with vector outcomes (sci-Plex\ndataset from Srivatsan et al. [44] (Sciplex) and the CRISPRa dataset from Schmidt et al. [38]\n(Marson)) and two datasets with image outcomes: Morpho-MNIST [4] and CelebA-HQ [15]. Results\nfrom the former are compared to state-of-the-art models in single-cell perturbation prediction and\nresults from the latter are compared to state-of-the-art models in counterfactual image generation.\nIn both cases, our model exhibited superior performance against benchmarks. Details of model and\ndataset settings for each experiment can be found in Appendix M."}, {"title": "3.1 Single-cell Perturbation Datasets", "content": "Same as Lotfollahi et al. [25], we evaluate our model and benchmarks on a widely accepted and\nbiologically meaningful metric \u2013 the R2 (coefficient of determination) of the average prediction\nagainst the true average from the out-of-distribution (OOD) set on all genes and differentially-\nexpressed (DE) genes. Definitions of OOD set and DE genes can be found in Appendix M.1. Results\nover five independent runs are shown in Table 1."}, {"title": "3.2 Morpho-MNIST", "content": "In an ideal world, we would like to evaluate model performance against benchmarks simply by\nmeasuring the mean squared error (MSE) between counterfactual construction and the counterfactual\ntruth for each individual. For this reason, we specifically choose the Morpho-MNIST [4] dataset to\npresent our main evaluation results on image generations because exact counterfactual errors can be\nmeasured even with the existence of exogenous noise, similar to the evaluations in Ribeiro et al. [33],\nwithout having to resort to Composition, Effectiveness, and Reversibility (more details in Appendix\nL). Morpho-MNIST is such a dataset that, unlike other real-world datasets, the counterfactual \"truth\"\ncan be computed based on the selected counterfactual treatments (modifying thickness or modifying\nintensity of hand-written digits). Similar to Ribeiro et al. [33], we evaluate model constructions under\nsingle modifications as well as mix of modifications, and compare them to state-of-the-art models\non high-fidelity image counterfactual generation. Contrary to Ribeiro et al. [33], the magnitude\nof modification is randomly sampled, which makes the task significantly harder. The results are\nshown in Table 2 (standard error in Table 6) which demonstrated that ours beat state-of-the-arts\nby a wide margin. Note that Ribeiro et al. [33] only evaluates mean absolute error (MAE) on\nthickness and intensity of counterfactual constructions, which took treatment characteristics into\naccount but completely left out how much individuality of the original images has been preserved,\nwhereas image MSE is a comprehensive metric that takes both factors into account and should be the\nprimary evaluation metric when the counterfactual truth is available. To investigate the impact of\ncounterfactual supervision and latent divergence terms in optimizations, we present an ablation study\nbelow to compare VCI to SAE and hierarchical autoencoder (HAE) with the same neural architecture."}, {"title": "3.3 CelebA-HQ", "content": "For any real-world dataset where the counterfactual truth is not available, there is not a definitive\nmetric to evaluate how good the counterfactual constructions really are. For that reason, facial imaging\ndatasets are prevailing benchmarks for examining counterfactual goodness because even in the lack\nof a quantitative metric, human can still judge the quality of counterfactuals with common sense\nwithout the need for any domain knowledge. Same as Monteiro et al. [27], we use the CelebA-HQ\n[15] dataset on 64\u00d764 resolution to evaluate the model's capability of counterfactual constructions\non two factors \u2013 smiling and glasses. Some sampled results from our model are shown in Figure 5.\nWhile MSE of the counterfactual construction cannot be measured, readers can empirically compare\nours to prior work such as CHVAE [27] which serves as the backbone for the state-of-the-art model\nin high fidelity counterfactual image generation MED [33]. Aside from compiling counterfactual\ncredibility across all generation tasks, ours show a strong capability of disentangling factual treatment\nfrom observed images in counterfactual construction, which can be particularly observed in the\ntask of \"removing glasses\u201d as shown in Figure 5d. Note that contrary to prior works, we construct\nand supervise counterfactual outcomes during training, and the reconstruction and counterfactual\nconstruction share the same decoding mechanism. Hence, \u201creconstruction\" is really counterfactual\nconstruction with the factual label. This can be most clearly observed in the third set of comparison\nin line 2 of Figure 5a, where the original image has a label of \"not smiling\" yet the person is really\nin-between smiling and not smiling. In this case, the \u201creconstruction\" is really a counterfactual\nconstruction with treatment abiding to the original given label. Lastly, we include a discussion here\nregarding counterfactual fairness. Note that adding glasses is a relatively easy engineering task, but\nrelatively hard deep learning task on CelebA-HQ due to the small sample size of images with glasses.\nAs can be seen from Figure 5c, our model not only learned to attach glasses at the right position, it\nalso learned to attach glasses suitable to a person's style and context. However, readers may naturally\nstart to think about the counterfactual fairness issue that might come with it, such as attaching certain\ntype of glasses more frequently on certain race. We note that our framework can reduce spurious\nor unintended relations between given factors, as long as practitioners measure these factors during\ndata collection and include them as observed labels for the composed datasets, since the learnt latent\nrepresentations of our framework are encouraged to be disentangled from the observed factors. In\nthe case of CelebA, style of glasses is unmeasured, and the model cannot reasonably tell if attaching\ndifferent types of glasses on different demographics is unintended or not."}, {"title": "4 Conclusion", "content": "In this work, we introduced a variational Bayesian causal inference framework for estimating high-\ndimensional counterfactual outcomes, as well as consequent robust marginal estimators. With this\nframework, treatment characteristics and individuality in constructed outcomes can be explicitly\nbalanced and optimized, and learnt latent representations are disentangled from the treatments. As\nfor limitations, this work requires the common causal assumption that there exists no unobserved\nconfounders, through which causal effects are identifiable. Besides, when multiple treatments are\nconcerned, this work does not explicitly address the situation where a causal relation exists between\ntreatments. Thus, a straightforward extension of this work could be to incorporate explicit modeling\nof the causal relation among treatment variables and update the descendants of given counterfactual\ntreatment, such as Pawlowski et al. [29], before feeding them into our counterfactual outcome\nconstruction framework."}, {"title": "A Introduction on Latent Disentanglement", "content": "The study of latent disentanglement in deep causal modeling has mainly focused on two areas. Most\nprior works lie in the first area that is causal representation learning, in which researchers seek to\nlearn latent variables corresponding to true causal factors and identify the structure of their causal\nrelations [53; 12; 11; 21; 20], predominantly leveraging recent advances in non-linear independent\ncomponent analysis (ICA) [14; 16] and identify causal structure up to Markov equivalence [43]\nor graph isomorphism under rather heavy assumptions. The second area and the area this work is\nconcerned of is counterfactual generative modeling, in which the focus is to conduct disentangled\nexogenous noise abduction that aids counterfactual outcome generation [25; 40]. In this context, the\ngoal is to acquire latent representations disentangled from the observed treatment to aid the correct\nidentification of the causal effect of counterfactual treatments."}, {"title": "B Comparison of Conditional VAE and VCI", "content": "For readers that are more familiar with deep generative modeling than causal inference, we provide a\nstraightforward comparison between conditional VAE and VCI (fundamentally a comparison between\nconditional generative modeling and counterfactual generative modeling) in this section using VAE's\nnotations. For data x and condition c, the differences in formulations and objectives are shown in\nFigure 6 and Table 3. In causal inference, counterfactual outcome is an individual-level concept \u2013\nfor a given individual that we observed outcome/data x under treatment/condition c, what would\ntheir outcome have been if they had received treatment/condition c' instead? This is a \u201cwould have\"\nquestion, meaning that we seek to find out what the alternative outcome x' would be in a parallel\nworld where everything in the state of the universe remained the same, except that the treatment c'\n(and its impact) was different. With this in mind, it is not hard to see why the conditional generative\nmodeling formulation is unorthodox for counterfactual generative modeling \u2013 the ELBO serves\nto optimize the marginal-level (interventional) likelihood p(xc) and the learnt model generates\nsamples towards the marginal-level (interventional) distribution p(x|c) during inference time, and\nthe question being asked here is \"what will an outcome x be under condition c?\". This is a \"will\"\nquestion - it is generating any outcome under condition c, not given a specific individual, not given\na specific state of the universe. Therefore, contrary to prior works in HVAEs and diffusion models\n[36; 27; 33] which focus on model design adaptations to make the conditional VAE formulation work\nin counterfactual generative modeling, we first derive the orthodox formulation and objective for\ncounterfactual generative modeling, then make the necessary model designs afterwards according to\nthe newly derived ELBO.\nAs can be seen in Table 3, the difference in motivations between the two objectives manifest them-\nselves in the derived ELBO in some intuitive ways: VAE does not have the motivation to maximize\ncounterfactual outcome likelihood, hence no such term log[p(x',$|c')] to perform counterfactual\nsupervision; VCI does not have the motivation to denoise or to generate samples from pure noise\nduing inference time, hence no such term DKL [q$(z|x, c) || P\u04e9(z|C)] to bound latent distribution on\nsome marginal prior."}, {"title": "B.1 Diffusion Models", "content": "Due to the popularity of the diffusion models [13], we feel the necessity to include a discussion here\nspecifically about its compatibility with counterfactual generative modeling. It is very important to\nnote that, although diffusion model is the state-of-the-art in generative and conditional generative\nmodeling, it has not been shown that it has better capability in counterfactual generative modeling\nthan ordinary HVAEs with learnable encoder [27; 33], and the reason is straightforward once\nthe readers truly understand the fundamental incompatibility between the goal of counterfactual\ngenerative modeling and the diffusion mechanism. As discussed above, the heart and soul of\ncounterfactual generative modeling, in contrast to conditional generative modeling, is the abduction\nof exogenous noise. The diffusion mechanism, on the other hand, inherently contradicts exogenous\nnoise abduction: not only does it work towards the wrong interventional objective as a model under the\nVAE formulation, the individuality contained in the factual outcome is also guaranteed to be discarded\nunder the unlearable diffusing encoder. Prior works that attempted at utilizing diffusion models in\ncounterfactual generative modeling [7; 36] clearly exhibited this flaw: as can be seen from the CelebA\nresults in Dash et al. [7], exogenous noise/individuality has been largely discarded in counterfactual\ngeneration \u2013 because the diffusion model is doing what it is intended to do: diffusing exogenous and\ngenerating samples that fit seeminglessly into the conditional/marginal likelihood p(x|c); Sanchez &\nTsaftaris [36] on the other hand, hardly showed any evidence that the model is capable of preserving\nexogenous noise/individuality \u2013 MNIST was the only benchmark in their experiments and only\nintervening digit was performed, which does not have measurable counterfactual truth as intervening\nthickness and intensity in Morpho-MNIST (in general, the interventions performed in Sanchez\n& Tsaftaris [36] all drastically changed the objects in the original images, which makes it quite\nimpossible to tell whether exogenous noise are preserved either quantitatively or qualitatively).\nWe want to clarify that we think Dash et al. [7] and Sanchez & Tsaftaris [36] are meaningful and\nwell-written works that conducted the important advancement of experimenting diffusion models\nin counterfactual generative modeling, however, diffusion models have not shown the capability of\nexogenous noise abduction on the level of state-of-the-art HVAEs [27; 33] as it stands."}, {"title": "C Related Work", "content": "Key prior works in deep counterfactual modeling fall primarily into three categories: variational-based\nmethods CEVAE [26", "29": "DiffSCM [36", "27": "MED [33", "19": "GANITE [54", "25": "hybrid method DEAR [40"}]}