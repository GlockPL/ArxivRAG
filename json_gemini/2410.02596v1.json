{"title": "Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks", "authors": ["Rui Hu", "Yifan Zhang", "Zhuoran Li", "Longbo Huang"], "abstract": "Generative Flow Networks (GFlowNets) are a novel class of generative models designed\nto sample from unnormalized distributions and have found applications in various important\ntasks, attracting great research interest in their training algorithms. In general, GFlowNets are\ntrained by fitting the forward flow to the backward flow on sampled training objects. Prior work\nfocused on the choice of training objects, parameterizations, sampling and resampling strategies,\nand backward policies, aiming to enhance credit assignment, exploration, or exploitation of the\ntraining process. However, the choice of regression loss, which can highly influence the exploration\nand exploitation behavior of the under-training policy, has been overlooked. Due to the lack of\ntheoretical understanding for choosing an appropriate regression loss, most existing algorithms\ntrain the flow network by minimizing the squared error of the forward and backward flows in\nlog-space, i.e., using the quadratic regression loss. In this work, we rigorously prove that distinct\nregression losses correspond to specific divergence measures, enabling us to design and analyze\nregression losses according to the desired properties of the corresponding divergence measures.\nSpecifically, we examine two key properties: zero-forcing and zero-avoiding, where the former\npromotes exploitation and higher rewards, and the latter encourages exploration and enhances\ndiversity. Based on our theoretical framework, we propose three novel regression losses, namely,\nShifted-Cosh, Linex(1/2), and Linex(1). We evaluate them across three benchmarks: hyper-grid,\nbit-sequence generation, and molecule generation. Our proposed losses are compatible with\nmost existing training algorithms, and significantly improve the performances of the algorithms\nconcerning convergence speed, sample diversity, and robustness.", "sections": [{"title": "1 Introduction", "content": "Generative Flow Networks (GFlowNets), introduced by Bengio et al. (2021a,b), represent a novel\nclass of generative models. They have been successfully employed in a wide range of important\napplications including molecule discovery (Bengio et al., 2021a), biological sequence design (Jain\net al., 2022), combinatorial optimization (Zhang et al., 2023b), and text generation (Hu et al., 2023),\nattracting increasing interests for their ability to generate a diverse set of high-quality samples.\nGFlowNets are learning-based methods for sampling from an unnormalized distribution. Com-\npared to the learning-free Monte-Carlo Markov Chain (MCMC) methods, GFlowNets provide an"}, {"title": "2 Related Work", "content": "Generative Flow Networks (GFlowNets). GFlowNets were initially proposed by Bengio et al.\n(2021a) for scientific discovery (Jain et al., 2023a) as a framework for generative models capable\nof learning to sample from unnormalized distributions. The foundational theoretical framework\nwas further developed by Bengio et al. (2021b). Since then, numerous studies have focused on\nenhancing GFlowNet training from various perspectives, such as introducing novel balance conditions\nand loss functions (Malkin et al., 2022a; Madan et al., 2023), refining sampling and resampling\nstrategies (Shen et al., 2023; Rector-Brooks et al., 2023; Kim et al., 2023b; Lau et al., 2024),\nimproving credit assignment (Pan et al., 2023a; Jang et al., 2023) and exploring different options\nfor backward policies (Shen et al., 2023; Mohammadpour et al., 2024; Jang et al., 2024).\nPeople also try to extend the formulation of GFlowNets to more complex scenarios, allowing con-\ntinuous space (Lahlou et al., 2023), intermediate rewards (Pan et al., 2022), stochastic rewards (Zhang\net al., 2023c), implicit reward given by priority (Chen & Mauch, 2023), conditioned rewards (Kim\net al., 2023a), stochastic transitions (Pan et al., 2023b), non-acyclic transitions (Brunswic et al.,\n2024), etc. Equipped with these techniques, GFlowNets are applied to a increasingly wide range\nof fields including molecular discovery (Jain et al., 2023b; Zhu et al., 2024; Pandey et al., 2024),\nbiological sequence design (Jain et al., 2022; Ghari et al., 2023), causal inference (Zhang et al., 2022;\nAtanackovic et al., 2023; Deleu et al., 2024), combinatorial optimization (Zhang et al., 2023b; Kim\net al., 2024), diffusion models (Zhang et al., 2023a; Venkatraman et al., 2024) and large language\nmodels (Hu et al., 2023; Song et al., 2024).\nTheoretical aspects on GFlowNets and f-divergence. From a theoretical perspective,\nGFlowNets are closely related to variational inference (VI, Malkin et al. 2022b) and entropy-\nregularized reinforcement learning (RL) on deterministic MDPs (Tiapkin et al., 2024; Mohammad-\npour et al., 2024). All of them can be viewed as solving distribution matching problems, and the\ngradients of their training objectives are consistent with that of the reverse KL divergence. The\nproperties of divergence measures and their effects as training objectives have been studied by Minka\net al. (2005). The idea of introducing a more general class of divergence measures has successfully\nimproved the performances of a variety of algorithms for training generative models, including GAN\n(Nowozin et al., 2016; Arjovsky et al., 2017), VAE (Zhang et al., 2019), VI (Li & Turner, 2016; Dieng\net al., 2017), Distributional Policy Gradient (DPG for RL, Go et al. 2023), and Direct Preference\nOptimization (DPO for RLHF, Wang et al. 2023). Garg et al. (2023) proposes to use the Linex\nfunction of the TD error to learn a soft Q-function that solves the soft Bellman equation.\nDifferent from the aforementioned studies, in this work, we establish a theoretical framework for\nthe regression loss component of GFlowNets and prove that different regression losses correspond to\nspecific divergence measures. By analyzing the zero-forcing and zero-avoiding properties of these\ndivergence measures, we can opt for the desired regression loss for enhancing exploitation and/or\nexploitation in GflowNets training algorithms."}, {"title": "3 Preliminaries of GFlowNets and f-Divergence", "content": "In this section, we first present preliminaries of GFlowNets and the f-divergence, which will be the\nfoundation of our subsequent exposition."}, {"title": "3.1 GFlowNets", "content": "A GFlowNet is defined on a directed acyclic graph G = (V, E) with a source nodes, and a sink\nnode sf, such that every other vertex is reachable starting from so, and sf is reachable starting from\nany other vertex. Let T be the collection of all complete trajectories, and \u2211 be the corresponding\n\u03c3-algebra, then a flow is a measure F on (T, \u03a3).\nFurther, we define state-flow, edge-flow and total flow by\nF(s) := F({T : s \u2208 T}),\nF(s \u2192 s') := F({t : (s \u2192 s') \u2208 t}),\nZ := F({T}) = F(so) = F(sf).\nA flow then induces a forward probability PF(s'|s) and a backward probability PB(s|s'), defined\nas:\nPF(s'|s) := P(s \u2192 s'|s) = $\\frac{F(s\\rightarrow s')}{F(s)}$,\nPB(s|s') := P(s \u2192 s'|s') = $\\frac{F(s \\rightarrow s')}{F(s')}$.\nMarkovian flow is a special family of flows such that at each step, the future behavior of a particle\nin the flow stream only depends on its current state. Formally speaking, let i be any trajectories\nfrom so to s, then P(s \u2192 s'|1\u03b9) = P(s \u2192 s'|s) = PF(s'|s). We focus on Markovian flows in the\nfollowing.\nA set of (not necessarily complete) trajectories C is a cut if and only if for any complete\ntrajectory T, there exists \u03b9\u2208 C such that is a part of \u03c4. Here we view vertices and edges as\ntrajectories of length 1 or 2 and further extend the definition of F to all trajectories as\nF(\u03b9) = F({\u03c4 : \u03b9 is a part of \u03c4}).\nA minimal cut is a cut such that the sum of flows in the cut is minimized. According to the\nmax-flow min-cut theorem, this amount is equal to Z the total flow. Let C be the collection of all\nminimal cuts, then for each minimal cut C\u2208 C, let p\u2282(1) := F(1) for all \u03b9 \u2208 C, then p(\u00b7) can be\nviewed as an unnormalized distribution over C.\nLet the terminating set Sf be the collection of nodes that directly link to sf. Note that\nC = {(s \u2192 s') : s' = sf} is a minimal cut, so p(.) induces a distribution over Sf. We denote it as\npr and its induced probability distribution as Pr (called the terminating probability):\n\u2200s \u2208 Sf, pr(s) = F(s \u2192 sf),\nPT(s) = $\\frac{PF(s)}{{\\sum_{s'\\in S_f}{PF(s')}}} = \\frac{F(s \\rightarrow S_f)}{Z}$.\nThe ultimate goal of training a GFlowNet is to match pr with R, so that the forward policy draws\nsamples from PT = PR, where PR denotes the normalized probability distribution defined by R."}, {"title": "3.2 f-Divergence", "content": "The f-divergence is a general class of divergence measures (Liese & Vajda, 2006; Polyanskiy, 2019):\nDf(p||q) = $\\sum_{x\\in X}{q(x)f(\\frac{p(x)}{q(x)})} + f'(x)p ({x \\in X : q(x) = 0})$,"}, {"title": "4 Training Generative Flow Networks", "content": "In this section, we present our perspective on analyzing GFlowNet training algorithms in detail. In\nSection 4.1, we provide a general framework with five customizable components to unify existing\ntraining algorithms. In Section 4.2, we dive deep into the regression loss component, which has\nbeen overlooked in existing research, and establish a rigorous connection between it and divergence\nmeasures. In Section 4.3, we further show how to utilize this connection for designing and analyzing\nobjective functions."}, {"title": "4.1 A Unified Framework for GFlowNet Training Algorithms", "content": "Consider the following general objective function for forward policy:\nLO,po,\u00b5,PB,9 = $\\sum_{O\\in O}{\\mu(o)g (log \\frac{PB(o;\\theta)}{PF(o; \\theta)})}$    (4.1)\nThis formulation is defined by five key components. i) The set of training objects O, which can\ninclude states, transitions, partial trajectories, or complete trajectories. (ii) The parameterization\nmapping pe, which defines how the parameters of the flow network represent the forward flow PF and\nthe backward flow pr. (iii) The sampling and resampling weights \u03bc, which influence how training\nobjects are sampled and weighted. (iv) The choice of backward policy PB, which can be either fixed\nor learned. (v) The regression loss function g, ensuring that the forward and backward policies align\nwhen minimized.\nWhile most GFlowNets training objectives are not explicitly written in this form, Equation (4.1)\nunifies all existing training objectives.  below presents a categorization of existing algorithms\naccording to the components they specify.\nIn previous literature, g(t) = $t^2$ has been the only choice for regression loss, and the term\n$g (log \\frac{PB(o;\\theta)}{PF(o; \\theta)})^2 = (log \\frac{PB(o;\\theta)}{PF(o; \\theta)})$ is usually referred to as the balance loss. It is specified by the\ntraining objects and parameterization mapping. Popular balance losses are flow matching (FM)"}, {"title": "4.2 The Information-Theoretic Interpretation of Training Objectives", "content": "Based on our proposed framework, We establish a novel connection between the g functions and the\nf-divergences. The result is summarized in Theorem 4.1 below.\nTheorem 4.1. Let @ be the parameters for forward policies. For each minimal cut C\u2208 C, the\nrestrictions of both forward and backward flow functions on C can be viewed as unnormalized\ndistributions over it, denoted as pr and pf, respectively.\nIf there exists w : C \u2192 R+ such that \u03bc(\u03bf) = PF (0) Ec\u2208c,o\u2208c w(C) for any o \u2208 O, then\nVOFLO,Pop, PB,9 = $\\nabla_{\\theta_F} \\sum_{C \\in C}{w(C)D_f(p_C^B || p_C^F)}$, where f(t) = $\\int_{1}^{t}{\\frac{g'(log s)}{s} ds}$.    (4.2)"}, {"title": "4.3 Designing New Regression Losses", "content": "Equipped with the connection established in Theorem 4.1, we now show how one can build upon\nit and design regression losses with two important properties: zero-forcing and zero-avoiding.\nA zero-forcing objective leads to a conservative result, while a zero-avoiding objective offers a\ndiverse approximated distribution. As pointed out by previous studies (Minka et al., 2005; Go\net al., 2023), zero-forcing property encourages exploitation, while zero-avoiding property encourages\nexploration. Therefore, a zero-avoiding loss may converge faster to a more diverse distribution,\nwhile a zero-forcing one may converge to a distribution with a higher average reward.\nTo this end, we first study the effect of using different divergence measures as optimization\nobjectives.\nProposition 4.4 (Liese & Vajda (2006)). Denote f(0) = limt\u21920+ f(t), f'(\u221e) = limt\u2192+\u221e $\\frac{f(t)}{t}$,\n1. Suppose f(0) = \u221e, then Df(p||q) = \u221e if p(x) = 0 and q(x) > 0 for some x.\n2. Suppose f'(x) = \u221e, then Df(p||q) = \u221e if p(x) > 0 and q(x) = 0 for some x.\nIn particular, Da(p||q) for a \u2264 0, including reverse KL divergence, satisfies the first condition,\nwhile Da(p||q) for a \u2265 1, including forward KL divergence, satisfy the second condition. Proposition\n4.4 leads to the following results of approximating a distribution by using f-divergence.\nProposition 4.5 (Liese & Vajda (2006)). Let S be a subset of the collection of distributions over\nX. Let pis \u2208 arg minges Df (p||q).\n1. Zero-forcing: Suppose f(0) = \u221e, then ps(x) = 0 if p(x) = 0.\n2. Zero-avoiding: Suppose f'(x) = \u221e, then ps(x) > 0 if p(x) > 0.\nProposition 4.5 suggests that when S does not cover the target distribution p, the best approxi-\nmation may vary according to the divergence chosen as the objective.\nSince the objective functions for GFlowNets with varying regression losses are closely related to\ndifferent divergence measures, we similarly define their zero-forcing and zero-avoiding properties."}, {"title": "Definition 4.6.", "content": "An objective function L for training GFlowNets is\n1. Zero-forcing: if for any parameter space \u0472 and 0* = arg min\u0259\u2208e L(0),\n\u2200s \u2208 Sf : R(s) = 0 \u21d2 Pr(s; 0*) = 0,\n2. Zero-avoiding: if for any parameter space \u0472 and 0* = arg ming\u2208e L(0),\n\u2200s \u2208 S : R(s) > 0 \u21d2 Pr(s;0*) > 0.\nIn such cases, we also say that the regression function g itself is zero-forcing or zero-avoiding.\nWe then have the following theorem regarding the zero-forcing and zero-avoiding objective\nfunctions and regression losses of GFlowNets.\nTheorem 4.7. Let L be an objective function for training GFlowNets, whose regression loss\ng corresponds to Df according to Theorem 4.1. If Df is zero-forcing, then Land g are both\nzero-forcing. If Df is zero-avoiding, then L and g are both zero-avoiding."}, {"title": "5 Experiments", "content": "In this section, we consider four representative g-functions  and evaluate their performances\nover Flow-matching GFlowNets, Trajectory-balance GFlowNets, Detailed-balance GFlowNets, and"}, {"title": "5.1 Hyper-grid", "content": "We first consider the didactic environment hyper-grid introduced by Bengio et al. (2021a). In this\nsetting, the non-terminal states are the cells of a D-dimensional hypercubic grid with side length H.\nEach non-terminal state has a terminal copy. The initial state is at the coordinate x = (0,0,\u2026\u2026,0).\nFor a non-terminal state, the allowed actions are to increase one of the coordinates by 1 without\nexiting the grid and to move to the corresponding terminal state.\nThe reward of coordinate x = (x1,\u06f0\u06f0\u06f0 ,xD) is given according to\nR(x) = Ro + R1$\\prod_{i=1}^{D}{1[\\frac{X_i}{H}-0.5> 0.25]}$ + R2$\\prod_{i=1}^{D}{1[0.3 <\\frac{X_i}{H}-0.5<0.4]}$,\nwhere 0 < -R1 < R0 \u00ab R2. Therefore, there are 2D reward modes near the corners of the\nhypercube.\nIn our experiments, we set D = 4, H = 20, Ro = 10~4, R\u2081 = \u22129.9 \u00d7 10\u22125, R2 = 1-10-6. The\nbackward policy is learned using the same objectives as the forward policy. We use the forward\npolicy to sample training objects. We plot the empirical L\u2081 errors between PT and PR in . Additional details can be found in Appendix E.1.\nAs shown in , the quadratic loss (baseline) leads to the slowest convergence among\nthe four choices. This is because it has the poorest exploration ability. Despite the differences in\nconvergence speed, the L\u2081 errors between PT and PR are almost the same at convergence when\nusing different regression functions."}, {"title": "5.2 Bit-sequence generation", "content": "In our second experimental setting, we study the bit-sequence generation task proposed by Malkin\net al. (2022a) and Tiapkin et al. (2024). The goal is to generate binary strings of length n given\na fixed word length k|n. In this setup, an n-bit string is represented as a sequence of n/k k-bit"}, {"title": "5.3 Molecule generation", "content": "The goal of this task is to generate binders of the sEH (soluble epoxide hydrolase) protein by\nsequentially joining \u2018blocks' from a fixed library to the partial molecular graph (Jin et al. (2018)).\nThe reward function is given by a pretrained proxy model given by Bengio et al. (2021a), and then\nadjusted by a reward exponent hyperparameter \u00df, i.e., R(x) = R(x)\u00df where R(x) is the output of"}, {"title": "6 Conclusion", "content": "In this work, we develop a principled and systematic approach for designing regression losses for effi-\ncient GFlowNets training. Specifically, we rigorously prove that distinct regression losses correspond\nto specific divergence measures, enabling us to design and analyze regression losses according to the\ndesired properties of the corresponding divergence measures. Based on our theoretical framework, we\ndesigned three novel regression losses: Shifted-Cosh, Linex(1/2), and Linex(1). Through extensive\nevaluation across three benchmarks: hyper-grid, bit-sequence generation, and molecule generation,\nwe show that our newly proposed losses are compatible with most existing training algorithms\nand significantly improve the performance of the algorithms in terms of convergence speed, sample\ndiversity, and robustness."}, {"title": "A Unifying Training Algorithms of GFlowNets", "content": "An objective function for training GFlowNets is specified by five key components, the training\nobjects O, the parameterization mapping pe, the sampling and resampling weights \u03bc, the backward\npolicy PB and the regression loss g. Most existing algorithms specify only one to two of the former\nfour components."}, {"title": "A.1 Training Objects and Parameterization Mapping", "content": "The choice of these two components are usually coupled since the parameters are mapped to the flow\nfunctions defined on training objects. The choice of training objects include states, edges, partial\ntrajectories and complete trajectories, corresponding to Flow-Matching GFlownets (FM-GFN, Bengio\net al. 2021a), Detailed-Balance GFlowNets (DB-GFN, Bengio et al. 2021b), Sub-Trajectory-Balance\nGFlowNets (STB-GFN, Madan et al. 2023) and Trajectory-Balance GFlowNets (TB-GFN, Malkin\net al. 2022a), respectively. Detailed-Balance GFlowNets and Sub-Trajectory-Balance GFlowNets can\nbe parameterized in different ways, the variants of which are Forward-Looking GFlowNets (FL-GFN,\nPan et al. 2023a) and DAG GFlowNets (DAG-GFN, also called modified-DB or modified-STB,\nDeleu et al. 2022; Hu et al. 2023). These algorithms can be summarized in Table 5.\nFlow-Matching GFlowNets (FM-GFN). An FM-GFN is parameterized by an edge-flow\nfunction F: E \u2192 R+. It uniquely determines a valid flow network if and only if the flow-matching\nconditions hold:\n\u2200s\u2208V\\{80, 8f}, $\\sum_{(s'\\rightarrow s) \\in E}{F(s'\\rightarrow s)} = R(s) + \\sum_{(s\\rightarrow s'') \\in E \\\\ s'' \\neq sf}{F(s\\rightarrow s'')}$\nThe flow-matching loss for state s is defined as\nLFM(8) = $\\frac{1}{2}(log \\frac{PB(s)}{PF(S)})^2$"}, {"title": "A.2 Sampling and Resampling Weights", "content": "There exist various strategies to sample training objects to enhance exploration and hence accelerate\nconvergence. The usual practice is to use the forward policy, the backward policy, a tempered or\ne-noisy version of them, an offline dataset, or a mixture of these strategies. Other choices include\nusing a reward prioritized replay buffer (Shen et al., 2023), applying Thompson sampling (Rector-\nBrooks et al., 2023) or local search (Kim et al., 2023b) to the sampled trajectories for extra\nsamples, increasing greediness according to state-action value Q (Lau et al., 2024), etc. The sampled\nobjects may also be reweighed. For example, STB-GFN weights each partial trajectory by a factor\nproportional to \u03bb', where l is its length and A is a hyper-parameter."}, {"title": "A.3 Backward Policy", "content": "The most common choice of PB is to either fix it to be uniform or simultaneously train it using the\nsame objective as the forward policy. Other criteria include matching a (possibly non-Markovian)\nprior (Shen et al., 2023), maximizing the entropy of the corresponding forward policy (Mohammad-\npour et al., 2024) and learning a pessimistic one that focuses on observed trajectories (Jang et al.,\n2024)."}, {"title": "B Theorem 4.1 and its Proof", "content": "Theorem B.1 (An extension of Theorem 4.1). Let OF and e\u00df be the parameters for forward and\nbackward policies, respectively. For each minimal cut C\u2208 C, the restrictions of both forward and\nbackward flow functions on C can be viewed as unnormalized distributions over it, denoted as p\nand pf, respectively.\nIf there exists w : C \u2192 R+ such that \u03bc(\u03bf) = PF(0) \u2211c\u2208c,o\u2208cw(C) for any o \u2208 O, then\nVOFLO,Pe,\u00b5, PB,9 = V0F \u2211w(C)Df\u2081 (PS||PF), where f1(t) = $\\int_{1}^{t}{\\frac{g'(logs)}{s} ds}$\nCEC\nV0BLO,P0,\u00b5,PB,9 = 70B \u2211W(C)Df2 (PE||PF), where f2(t) = g(logt)\nCEC\nIf there exists w : C \u2192 R+ such that \u03bc(\u03bf) = PB(0) \u2211c\u2208c,o\u2208cw(C) for any o \u2208 O, then\nV0FLO,Pe,\u00b5,PB,9 = \u2207OF \u2211w(C)Df3 (PB||PF), where f3(t) = tg(logt)\nCEC"}, {"title": "C Interpretation of Theorem 4.1 for Different Kinds of Losses", "content": "C.1 Flow Matching Loss\nFor any s \u2208 V, let l(s) be the length of the longest trajectory from so to s. For any (s \u2192 s') \u2208 E, if\nl(s) +1 < l(s'), then we insert 1(s') \u2013 l(s) 1 virtual states on this edge, denoted as s(s\u2192s'),1 for\nl(s) < 1 < l(s'), and define\nPF($(ss'),1) = PB(s(s\u2192s'),1) = F(s \u2192 s')\nthen these virtual states have no contribution to the total loss, thus we can assign to them arbitrary\nweights.\nLet Vi be the collections of states in layer i, and let w(Vi) = 1, then we have\n\u03bc(s) = ppt(s) (s) = PF(s)"}, {"title": "C.2 Detailed Balance Loss", "content": "For any s \u2208 V, let l(s) be the length of the longest trajectory from so to s. For any (s \u2192 s') \u2208 E, if\nl(s) +1 < l(s'), then we insert l(s') \u2013 l(s) \u2013 1 virtual states on this edge, denoted as s(s\u2192s'),1 for\nl(s) <l <l(s'), and define\nPF(ss') =PF(s \u2192 s')\nPB(ss') = $\\begin{cases}PB(S \\rightarrow s') \\\\ l<l(s')\\PB(S \\rightarrow s') = l(s')\\end{cases}$\nthen these virtual transitions have no contribution to the total loss, thus we can assign to them\narbitrary weights.\nLet Er be the collections of edges from layer i to layer i + 1, and let w(Ei) = 1, then we have\n\u03bc(s \u2192 s') = pel(s) (s \u2192 s') = pr(s \u2192 s')"}, {"title": "C.3 Sub-Trajectory Balance Loss", "content": "Assume that G is a graded DAG with L + 1 layers. Suppose T = (50 = So, $1,\u2026\u2026,SL = $f) is a\ncomplete trajectory, we use Ti:j = (Si, Si+1,\u2026\u2026,Sj) to denote a partial trajectory. Let Tij be the\ncollections of trajectories from layer i to layer j, then\n\u03bc(\u03b9) = $\\sum_{T:L=\\tau_{i:j}}{\\frac{PF(T)}{\\sum_{0<i<j<L}{\\frac{\\lambda^{j-i}}{\\lambda^{i-i}}}}}$\\\nHence w(Ti:j) = $\\frac{\\lambda^{j-i}}{\\sum_{0<i<j<L}{\\lambda^{i-i}}}$ and 0 otherwise."}, {"title": "D Proof of Theorem 4.7", "content": "Theorem D.1. Let L be an objective function for training GFlowNets, whose regression loss\ng corresponds to Df according to Theorem 4.1. If Df is zero-forcing, then Land g are both\nzero-forcing. If Df is zero-avoiding, then L and g are both zero-avoiding.\nProof. Assume that Df is zero-forcing, and Pr(s;0*) > 0 for some terminating state s. Then there\nexists a trajectory T = (so,\u2026\u2026, s, sf) such that PF(T;0) > 0, thus\nPF(0) = PF(o) > 0\nfor any o\u2208\u03c4, \u03bf \u2208 C, w(C) > 0. Since Df is zero-forcing, PB(0) = pg(o) > 0 for any o \u2208 \u03c4, meaning\nthat PB(T) > 0 and R(s) > 0. Thus, R(s) = 0 implies Pr(s; 0) = 0, so Lis zero-forcing, and then\ng is zero-forcing as well.\nSimilarly, assume that Df is zero-avoiding, and R(s) > 0 for some terminating state s. Then\nthere exists a trajectory T = (so,\u2026\u2026, s, sf) such that PB(T) > 0, thus\nPg(0) = P(0) > 0\nfor any o \u2208 \u03c4, \u03bf \u2208 C, w(C) > 0. Since Df is zero-avoiding, PF(0) = P(0) > 0 for any \u03bf \u2208 \u03c4, meaning\nthat PF(T;0) > 0, so Pr(s; 0) > 0. Thus, R(s) > 0 implies that Pr(s;0) > 0, so L is zero-avoiding,\nand then g is zero-avoiding as well."}, {"title": "E Experimental Details", "content": "E.1 Hyper-grid\nOur implementation of the baselines is based on Tiapkin et al. (2024). All models are parameterized\nby an MLP with 2 hidden layers of 256 neurons. We train the model with Adam optimizer using a\nbatch size of 16 and a learning rate of 0.001. For the TB case, we use a larger learning rate of 0.1\nfor learnable total flow 2. For STB parameter \u5165, we use the value of 0.9 following Tiapkin et al.\n(2024) and Madan et al. (2023). We repeat each experiment 3 times using different random seeds.\nIn each run, we train the models until 800k trajectories have been collected, and the empirical\nsample distribution is computed over the last 80k seen trajectories."}, {"title": "E.2 Bit-sequence Generation", "content": "In this experiment, our implementation of the baselines is based on Tiapkin et al. (2024) and Pan\net al. (2023a). The model is a 3-layer Transformer with 64 hidden units and 8 attention heads per\nlayer. We train the model with Adam optimizer using a batch size of 16 and a learning rate of\n0.001. For the TB case, we use a larger learning rate of 0.002 for learnable total flow 2. For STB\nparameter \u5165, we use the value of 1.5. Following Tiapkin et al. (2024), we use a reward exponent\nof 2. To calculate the Spearman Correlation, we use the same Monte-Carlo estimation for Pr as\nZhang et al. (2022) and Tiapkin et al. (2024), namely\nPr(x) ~ $\\frac{1}{N} \\sum_{i=1}^{N}{\\frac{PF(T^i)}{PB(T^i|x)}}$\nwith N = 10. We repeat each experiment 5 times using different random seeds."}, {"title": "E.3 Molecule Generation", "content": "In the molecule generation experiment, our implementation of the baselines is based on Tiapkin\net al. (2024). We use Message Passing Neural Networks (MPNN) as the model architecture. We\ntrain the model with Adam optimizer using a batch size of 4 and a learning rate of 0.0005. We use a\nreward exponent of 4, and the STB parameter A is set to 0.99. We repeat each experiment 4 times\nusing different random seeds. In each run, We train the models for 50000 steps, generating 200k\nmolecules."}]}