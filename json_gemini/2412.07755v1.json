{"title": "SAT: Spatial Aptitude Training for Multimodal Language Models", "authors": ["Arijit Ray", "Jiafei Duan", "Reuben Tan", "Dina Bashkirova", "Rose Hendrix", "Kiana Ehsani", "Aniruddha Kembhavi", "Bryan A. Plummer", "Ranjay Krishna", "Kuo-Hao Zeng", "Kate Saenko"], "abstract": "Spatial perception is a fundamental component of intelligence. While many studies highlight that large multimodal language models (MLMs) struggle to reason about space, they only test for static spatial reasoning, such as categorizing the relative positions of objects. Meanwhile, real-world deployment requires dynamic capabilities like perspective-taking and egocentric action recognition. As a roadmap to improving spatial intelligence, we introduce SAT, Spatial Aptitude Training, which goes beyond static relative object position questions to the more dynamic tasks. SAT contains 218K question-answer pairs for 22K synthetic scenes across a training and testing set. Generated using a photo-realistic physics engine, our dataset can be arbitrarily scaled and easily extended to new actions, scenes, and 3D assets. We find that even MLMs that perform relatively well on static questions struggle to accurately answer dynamic spatial questions. Further, we show that SAT instruction-tuning data improves not only dynamic spatial reasoning on SAT, but also zero-shot performance on existing real-image spatial benchmarks: 23% on CVBench, 8% on the harder BLINK benchmark, and 18% on VSR. When instruction-tuned on SAT, LLaVA-13B matches larger proprietary MLMs like GPT4-V and Gemini-3-1.0 in spatial reasoning. Our data/code is available here.", "sections": [{"title": "1. Introduction", "content": "Cognitive scientists posit that spatial reasoning is not merely a subset of human cognitive abilities but rather the fundamental underpinnings of most intellectual processes; spatial reasoning in school children improves their aptitude in geometry, physics, and even linguistic reasoning. Hence, it is perhaps not surprising that many idioms utilize space to explain concepts: \"hitting a wall,\u201d \u201ca step in the right direction,\" and \"thinking outside the box.\"\nDespite their widespread adoption and promise as human-level intelligent agents, multimodal language models (MLMs) still struggle to reason spatially. Most of these studies focus on simple spatial questions in static scenarios, such as the relative positions of static objects in a fixed scene. Meanwhile, many downstream applications, such as smart glasses and embodied AI, could benefit from dynamic capabilities involving movement, like perspective-taking and egocentric action recognition. Such dynamic capabilities are well-studied in human developmental cognition and argued to be fundamental to intelligence; children understand the consequences of their movement (the moving room test), understand how entities in the scene move, and take the perspectives of other humans when planning.\nTo promote progress towards dynamic spatial understanding for MLMs, we propose Spatial Aptitude Training (SAT), an approach for generating spatial question-answer (QA) data without any human supervision to train and evaluate MLMs. Annotating scenes with 3D information is expensive; instead, SAT leverages 22K ProcTHOR scenes composed of 1K assets to generate 218K QA pairs. With perfect 3D information and control of the assets, SAT goes beyond static object relationships to questions that require reasoning based on objects moving and egocentric actions in the scene. Since our data is generated procedurally by composing assets, it can be scaled up without human annotation. Hence, SAT is more flexible than 3D datasets, which are not composable and have fewer object classes (~98).\nWith SAT, we analyze what kinds of training data improve spatial reasoning in MLMs. We focus on two kinds of spatial reasoning data. First, spatial-QA about simpler object relations in static scenes, to impart reasoning about the relative locations of objects in the scene (e.g. where is object X with respect to object Y? behind, above, left, or right?) as well as counting. Next, we evaluate the effect of dynamic spatial tasks,. This includes egocentric movement, object movement, allocentric perspective, goal aiming, and action consequences. These complex spatial tasks go beyond static object relationships, assessing the MLM's capability to reason about spatial movements, perspective changes, and certain degrees of spatial causality.\nWe use LLaVA-1.5-13B, a widely adopted open-source MLM, as our base model for evaluations. To test"}, {"title": "2. Related work", "content": "Our work draws inspiration from fundamental schools of thought in neuroscience that suggest spatial intelligence is a core foundation for most cognitive abilities.\n3D Spatial Understanding Benchmarks 3D and spatial understanding in vision have been extensively studied. Holistic 3D scene understanding mostly focuses on estimating the layout of an indoor scene as opposed to grounding the 3D nature of the objects of the scene with spatial language words. This is also true for models for localization with coordinates for both 3D and 2D, such as predicting segmentation maps, object localization, and tracking. Various works in the joint 3D and language understanding space like fine-grained scene captioning, open-vocabulary classification and localization, question answering and language models deal with full 3D scans. In contrast, our framework emphasizes understanding the 3D spatial configuration of the objects from 2D images since most open MLMs operate on RGB images and 3D scans are expensive to compute, especially in dynamically changing environments. While there are works that deal with the 3D structure of a 2D scene, most of them do not have high-resolution images (since they are rendered from point clouds) and diverse object annotations since they are expensive to annotate and collect. In contrast, SAT does"}, {"title": "3. SAT: Spatial Aptitude Training", "content": "Our goal is to improve the 3D spatial reasoning capabilities of MLMs in situations involving both static and dynamic scenes. Existing 3D datasets have few object annotations and are not controllable/interactive. Since obtaining varied 3D annotations with interactive movements of the camera and objects on real images is expensive and tedious, we propose to teach the model such spatial reasoning using data from procedurally generated photo-realistic environments. The resulting data generation pipeline, SAT, serves as both instruction-tuning data for MLMs and as a benchmark to test the dynamic spatial reasoning capabilities not present in existing benchmarks. While it may be possible to pseudo-annotate 3D spatial information on real images, we show later that this might require extensive cleaning.\nIn total, our dataset contains 218K questions across 22K procedurally generated scenes from ProcTHOR-10K dataset of indoor apartment buildings. We generate template-based static as well as dynamic spatial questions-answer pairs (QAs) that go beyond the kinds of questions in existing benchmarks. We first collect attribute descriptions (e.g., brown wooden chair) for assets with multiple variations (e.g. chairs) by instructing humans to describe the asset in a few words. We then use GPT4-0 refine the descriptions into compact phrases. Only some object/asset descriptions required human annotation as a one-time cost. As scenes can be composed arbitrarily from these assets, we can scale up the scenes and QA generation without any human annotation. Next, we outline the generation process for the two kinds of spatial questions- static and dynamic."}, {"title": "3.1. Generating Static Spatial QAs.", "content": "Aligning with existing contemporary benchmarks for spatial reasoning, we first generate instruction-tuning data for static spatial reasoning that deal with relative relations of objects. Overall, we generate 127K static spatial QA pairs across 8K images across the following types:\nRelative spatial relations. We generate questions about the relative location of one object in the scene to other objects. We form two kinds of questions - (1) judging if object X is to the left, right, above, or below object Y. For example, Is the wine bottle to the left or right of the plate? (2) judging if object A or B is closer to another object C. For example, Which object is closer to the wine bottle- the cup, or the plate? Given the camera parameters, we first project the objects' poses into the camera coordinate system and then generate the corresponding answers. More details about the camera coordinate normalization are in the appendix.\nRelative Depth. We generate questions about judging whether object X is closer to the camera than object Y. For example, Is the wine bottle closer to the camera than the plate? In our simulator, we calculate the distance between each object and the camera to generate the answer.\nCount. Since it is known that many MLMs struggle with counting, we include counting questions. For example, How many cups are visible in the image? Since the metadata from our simulator provides the number of object instances with their attributes in a scene, we can automatically obtain answers to this type of question for free."}, {"title": "3.2. Generating Dynamic Spatial QA", "content": "Grounded in spatial cognitive tests, we outline five different complex tasks that require reasoning about egocentric actions taken, object movements, and allocentric perspectives. We generate 86K QAs on 13K images. The high-level idea behind generating such QAs is illustrated in Figure 2. Given a frame in a simulated environment, we take an action and formulate QAs based on how the 3D orientation of objects changes based on the action taken. Below, we outline the specific approach for each of the question types.\nEgocentric Movement. This is based on the \"moving room test\" , a fundamental test designed to assess and improve spatial cognitive development in children. This test aims to measure if an agent can judge how they moved given two frames. This is useful beyond just measuring spatial cognition since high accuracy on this task can help pseudo-annotating navigation data from just egocentric videos. We take a random action from the choices of rotating left or right by an angle sampled randomly. We also randomly choose to move forward by a random distance. We take the first frame and the frame after taking this movement action. Based on the actions taken, we formulate a question of the type: How did the camera taking the video likely move? with the answer being the action sequence taken. We only take at most two steps (rotating and moving forward) since we want to ensure only one correct answer of what movement happened from the first frame to the end frame. We have 6.9K training image-QA pairs of this type. The test performance on this task is denoted as EgoM in the tables.\nObject Movement. Similar to above, we randomly choose"}, {"title": "4. Experiments", "content": "Using our SAT data generation pipeline, we investigate the effects of kinds of instruction-tuning data on the spatial performance of MLMs. Specifically, we wish to compare our synthetic SAT over pseudo-annotated questions on real images using off-the-shelf depth models and ablate the effect of static and dynamic spatial QAs."}, {"title": "4.1. Experimental setup", "content": "Evaluation benchmarks. We use 4 spatial benchmarks for our evaluation. CVBench, BLINK, and Visual Spatial Relations (VSR) measure static spatial understanding on real images. These constitute contain 7K image-QA pairs (around 2.6K for CV Bench, 400 for BLINK, and a 4K set for VSR). We use three spatial splits of BLINK - Multiview reasoning (MV), Relative Depth (RelDep), and Spatial Relations (SpRel). Finally, our SAT test set measures dynamic spatial understanding on 4K QAs on 805 images. Since perspective is overrepresented in our data, we subsample to keep all tasks roughly balanced. Hence, we have 647 object movement, 647 egocentric movement, 592 goal aim, 1336 action consequence, and 778 perspective questions on 805 images.\nInstruction-tuning training datasets. Next, we outline the various instruction-tuning sources we use for imparting spatial reasoning:\nSAT Static: Using SAT, we first analyze the effect of tuning with only static spatial questions. These tasks are most aligned with the types of questions in existing benchmarks. This set contains 8K images and 127K QAs. This is denoted in row (d) in Table 5.\nSAT Dynamic: This set contains dynamic spatial questions. We note that we have proportionally more perspective questions than other tasks and we empirically find perspective easier to overfit to. Hence, we subsample only 2.6K perspective image-QAs and keep other splits the same as described"}, {"title": "5. Results", "content": "Stronger closed-source and spatially-tuned models struggle on SAT dynamic QAs despite performing well on static. We note that closed-source models (GPT4-0, Gemini-1.5-pro) and spatially-tuned Robopoint struggle on complex QAs (in Table 2. For GPT4-0 and Robopoint, they perform well on static QAs (in Tables 3, 4), but not on SAT. We see that perspective is challenging when tested zero-shot since whether something is to the left or right often flips when taking the perspective of another viewer. Aiming to goal is easier for spatially stronger models like RoboPoint and GPT4-0.\nTuning on SAT improves performance across both dynamic and static spatial QAs. Tuning on SAT QAs improves performance on the test set for dynamic spatial reasoning on SAT as shown in Table 2. While this is not surprising, this also improves performance on static spatial questions on BLINK (by 8%), CV-Bench (by 23%) and VSR (by 18%) benchmarks as shown in Table 5 compared to off-the-shelf LLaVA (rows a vs d, e). BLINK is harder - for instance, spatial relations that have abstract relationships (especially with people) not present in our synthetic data (e.g. \"looking away\"). An example is shown in Figure 3 (bottom row, 3rd image). Multiview Reasoning gains are also modest, and the same issue is observed on the related Egocentric Movement split on SAT data.\nOur 13B tuned model matches/outperforms some larger proprietary or spatially-tuned models We compare our performance with that of closed-source models reported by"}, {"title": "6. Conclusion", "content": "Limitations. We instruction-tune an MLM for spatial reasoning, LLaVA. While we do remember pretraining commonsense as noted in Table 6, we haven't explored improving other capabilities like math and science reasoning. The scope of the paper, however, is to analyze what kinds of data improve spatial performance, and not a large-scale training of a new MLM. Additionally, further analysis is necessary on more recent MLMs.\nFuture work. Although our study focuses on evaluating the spatial reasoning capabilities of MLMs, it can be extended in various avenues. For instance, to determine the kinds of embodied applications that benefit from improved complex spatial reasoning. As a preliminary study, we checked the action prediction accuracy (from a choice of going left/right/forward) for a given frame on the SPOC EasyObjectNav benchmark. Our model (SAT Dynamic) scores an accuracy of 51% compared to 40% for a model trained only on basic spatial questions. This suggests that"}, {"title": "7. Dataset Details", "content": "Human Performance on our SAT We conduct a human study with experts to measure the quality of our dataset. We observe that spatial awareness demands more mental power since one has to pay more attention and reason about how the orientation of the scene changed or would change based on an action. We conduct an expert human study,\nwhere we ask anonymized graduate students to answer 200 randomly sampled questions from our test set using the interface showed in Figure 4 We see that humans are 92.8% accurate on our SAT dataset. This is still a significant gap\ncompared to the performance of best existing MLM (around\n51%). We will release the dataset on Huggingface.\n7.1. More details on dataset creation\nWe first take an apartment from ProcTHOR-10K and place the camera at a position where many objects are visible. We do this by randomly choosing 20 points to place the camera and then choosing the point with max objects visible.\nNormalizing the camera coordinates In ProcTHOR, in the camera view, the y coordinate is the height coordinate, which means the y increases pointing upwards (e.g. the ceiling has a greater y than the floor). Hence, from the bird's eye view, the coordinates are x and z. The rotation of the camera is such that it is always parallel to the x-z plane. Hence the rotation is described as angle clockwise around the y-axis with the camera pointing to the positive z-axis as a 0-degree rotation.\nGiven a camera rotation, we normalize the view by translating to (0,0) for x and z by subtracting the camera $x_0$ and $y_0$. Further, we rotate the x-z plane such that the camera points to the positive z-axis.\nFor rotation, we use the formula:\n$R=\\begin{bmatrix}\ncos(\u03b1) & -sin(\u03b1) \\\\\nsin(\u03b1) & cos(\u03b1)\n\\end{bmatrix}$\nHence, the normalized x', z' for any object is computed\nusing:\n$\\begin{bmatrix}\nx' \\\\\nz'\n\\end{bmatrix}= R\\begin{bmatrix}\nx - x_0 \\\\\nz - y_0\n\\end{bmatrix}$"}, {"title": "7.2. SAT Static Spatial QAs", "content": "Relative spatial relations. For instance, if the value of x' for\n\"chair\" is lower than that of \"table\", the chair is to the left of\nthe table. We can also compute the distance between objects.\nWe randomly choose 3 objects. We compute the pairwise\ndistances using their (x, y, z) 3D coordinates. Based on\nwhether object 1 is closer or further to object 2, we make\nQAs like \"Is the couch closer to the lamp or the table?\"\nRelative Depth. Similarly, if the value of z' for, say, \"lamp\"\nis greater than that of \u201ccouch\u201d, we say the \u201clamp\" is further\naway from the camera than the couch."}, {"title": "7.3. SAT Dynamic Spatial QAs", "content": "Egocentric Movement. We first choose an image\nframe. Next, we first choose to rotate left or right from\nangles 20, 30, 40, 50, 60 chosen randomly. We use the\ncontroller.step(action='RotateRight',\ndegrees=angle) function in the AI2THOR\nplatform. Next, we move forward with probability\n0.5 by a random distance from 20 to 40 centimeters\n(controller.step(action='MoveAhead',"}, {"title": "8. Tuning details", "content": "LLaVA All our numbers are reported after training until\nthe learning curve plateaus at around 200K iterations. We\nuse LORA tuning with a rank of 128 and an alpha of\n256. We found tuning the image encoder ViT also with\nLORA to be important for performance. We tune the query\nand key projection layers for all the transformer layers with\nLORA. We use learning rate of 5e - 6, a batch size of 1 and\ngradient accumulation as 8 (effective batch size of 8), weight\ndecay of 0, with a cosine annealing scheduler and a warm\nup of 1000 steps. We use standard next-token prediction\nloss from LLaVA official implementation. We train using\ntwo 48 GB NVIDIA A6000/RTX6000ada/L40 and we use\nHuggingface accelerate for the multi-GPU training. Each\ntraining takes around 48 hours. For each of the experiments\nwe tune until the training loss plateaus. We see this requires\naround 200K steps for the static QAs and around 300K steps\nfor the dynamic QAs. We notice no further improvement\nif we keep training for more iterations with the static QAs.\nDuring inference, we use a greedy sampling with temper-\nature 0 following the standard hyperparameters as in the\nhuggingface codebase.\nThe inference is possible using a single 48GB GPU.\nFollowing LLaVA convention, we use <image>\ntokens to represent images. This is the exact prefix we use.\nA chat between a curious human\nand an artificial intelligence"}, {"title": "9. Some extra ablations", "content": "Simply using more instruct tuning data instead of our\ndata We wish to answer if the gains trivially come from\njust simply more training on data. Hence, we run a naive\nbaseline of training on more LLaVA instruct tuning data.\nUnsurprisingly, this does not lead to any gains in spatial\nperformance. Results are shown in row b in Table 7.\nAdding precise QAs to the dynamic mix does not help\nperformance Interestingly, we see no improvement when\nwe add precise QA mixed with our full dynamic SAT data\n(row e). While some tasks improve slightly like depth on\nCVBench and spatial relations on BLINK, overall perfor-\nmance decreases slightly. This could be due to precise QAs\nbeing somewhat ill-defined for a single image. However, this\nrequires further exploration and we leave this to future work."}, {"title": "10. More Qualitative Results", "content": "More examples and Failure cases We show more qualita-\ntive results in Figure 9. We especially want to investigate\nsome failure cases and hence we display more cases here\nwhere our model fails. First, we note that the kinds of cam-\nera movements in SAT are sometimes different from those"}]}