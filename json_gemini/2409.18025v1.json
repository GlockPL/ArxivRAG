{"title": "An Adversarial Perspective on Machine Unlearning for AI Safety", "authors": ["Jakub \u0141ucki", "Boyi Wei", "Yangsibo Huang", "Peter Henderson", "Florian Tram\u00e8r", "Javier Rando"], "abstract": "Large language models are finetuned to refuse questions about hazardous knowledge, but these protections can often be bypassed. Unlearning methods aim at completely removing hazardous capabilities from models and make them inaccessible to adversaries. This work challenges the fundamental differences between unlearning and traditional safety post-training from an adversarial perspective. We demonstrate that existing jailbreak methods, previously reported as ineffective against unlearning, can be successful when applied carefully. Furthermore, we develop a variety of adaptive methods that recover most supposedly unlearned capabilities. For instance, we show that finetuning on 10 unrelated examples or removing specific directions in the activation space can recover most hazardous capabilities for models edited with RMU, a state-of-the-art unlearning method. Our findings challenge the robustness of current unlearning approaches and question their advantages over safety training.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are pretrained on trillions of tokens crawled from the Internet (Dubey et al., 2024). Due to the unprecedented size of the training corpora, it is nearly impossible to discard all dangerous or otherwise harmful information available online. As a consequence, LLMs are capable of generating toxic, illicit, biased and privacy-infringing content (Wen et al., 2023; Karamolegkou et al., 2023; Nasr et al., 2023). Since models are constantly becoming more capable, this knowledge may pose increasing risks as it can make hazardous information more easily accessible for adversaries.\nLLMs often undergo safety finetuning to reject unethical requests and produce safe responses (Bai et al., 2022). Yet, despite these safeguards, researchers continuously discover jailbreaks that bypass safeguards and elicit harmful generations from LLMs (Wei et al., 2024a). Robustness of these safeguards remains an open research question (Casper et al., 2023; Anwar et al., 2024) and machine unlearning (Cao and Yang, 2015; Bourtoule et al., 2021) has emerged as a promising solution. It aims to completely remove hazardous knowledge from LLMs, preventing its extraction even after jailbreaking. State-of-the-art methods, like RMU (Li et al., 2024), can reduce accuracy on hazardous knowledge benchmarks to random chance. However, unlearning is not foolproof, as hazardous knowledge can still be recovered after the process (Patil et al., 2024; Shumailov et al., 2024). This raises an important question: Does unlearning truly remove hazardous knowledge, or does it simply \"obfuscate\" this knowledge similarly to refusal safety training?\nIn this work, we challenge the fundamental differences between unlearning and traditional safety finetuning from an adversarial perspective. We use the accuracy on the WMDP benchmark (Li et al., 2024) to measure the hazardous knowledge contained in LLMs. We argue that, from the perspective"}, {"title": "2 Related Work", "content": "Safety training and jailbreaks. Large language models are finetuned to refuse questions about hazardous knowledge with safety methods like DPO (Rafailov et al., 2024) or PPO (Ouyang et al., 2022). Zou et al. (2024) recently introduced circuit breakers that use representation engineering to orthogonalize directions corresponding to unwanted concepts. The robustness of existing safeguards is limited (Casper et al., 2023; Anwar et al., 2024) and researchers often find jailbreaks to bypass protections and elicit hazardous knowledge (Wei et al., 2024a). Jailbreaks can rely only on prompting strategies (Shah et al., 2023; Huang et al., 2023), exploit white-box access to optimize prompts (Zou et al., 2023; Andriushchenko et al., 2024) or ablate model activations (Arditi et al., 2024).\nUnlearning. Unlearning aims to update the weights of a model to remove specific knowledge so that it cannot be accessed in any form (Cao and Yang, 2015; Bourtoule et al., 2021). In the context of language models, unlearning work has expanded across topics like fairness, privacy, safety or hallucinations (Jang et al., 2022; Yao et al., 2023; Chen and Yang, 2023; Wu et al., 2023; Li et al., 2024; Liu et al., 2024b). Unlearning is usually evaluated using narrow topics (e.g. Harry Potter) or fictional information (Eldan and Russinovich, 2023; Maini et al., 2024; Shi et al., 2024; Wei et al., 2024c). Our work focuses on unlearning methods for safety. These methods try to eliminate dangerous knowledge to prevent adversaries from accessing it, even after jailbreaking attempts. The most notable method for this purpose is RMU (Li et al., 2024), which was introduced alongside WMDP, a benchmark for evaluating hazardous capabilities. General-purpose unlearning algorithms like negative preference optimization (NPO) (Zhang et al., 2024) can also be adapted for this purpose.\nUnlearning robustness. Initial unlearning evaluations for LLMs relied on simple classification metrics (Eldan and Russinovich, 2023) which do not account for all possible ways in which a language model can represent and output the target information. Recent works (Jin et al., 2024; Hong et al., 2024; Lynch et al., 2024) have adopted an adversarial approach to test whether there exist ways to extract the information that was supposedly unlearned. For instance, Lynch et al. (2024) showed that knowledge could be extracted at comparable rates from both original and unlearned models by probing internal representations. In the context of unlearning hazardous capabilities, RMU reports robustness under some white-box jailbreaks like GCG or probing, but finds that finetuning unlearned models can easily disable the protections (Li et al., 2024). In this work, we devise novel methods to extract hazardous knowledge from unlearned models without updating the weights. The importance of meticulous evaluations, has been demonstrated by an earlier work on word embedding debiasing, which revealed the lack of robustness of the respective methods (Gonen and Goldberg, 2019)."}, {"title": "3 Experimental Setup", "content": "This works focuses exclusively on unlearning methods for safety that remove hazardous knowledge (e.g. bioweapons) from large language models, as introduced by Li et al. (2024). In practice, unlearning relies on forget and retain sets. The first contains information relevant to the domain to be unlearned (e.g. enhanced pandemic pathogens) while the second includes neighboring information"}, {"title": "3.1 Threat Model", "content": "We assume white-box access to an unlearned model, allowing modification of its weights and intervention in the activation space during inference. Additionally, we assume access to the original model prior to unlearning or to an equivalent model obtained by removing unlearning protections through finetuning, as demonstrated later. Although white-box access differs from the threat model for protections we study (RMU assumes only black-box access), it provides valuable insights into the effectiveness of unlearning in removing knowledge from model weights. Furthermore, with the rise of powerful open-source large language models, robust unlearning in white-box scenarios is an increasingly relevant desiderata."}, {"title": "3.2 Unlearning Methods and Safety Training Baseline", "content": "We evaluate the most powerful unlearning method for hazardous knowledge to date: RMU (Li et al., 2024; Kadhe et al., 2024). Additionally, we implement NPO (Zhang et al., 2024) that has been widely used as a general-purpose unlearning method for fact and concept removal (Shi et al., 2024), but its effectiveness for hazardous knowledge removal remains unexplored. We specifically use NPO+RT, a variant of NPO including an additional retain loss. Finally, we include DPO (Rafailov et al., 2024) as a baseline for safety training to contrast it with unlearning methods. For more details about the methods, see Appendix B."}, {"title": "3.3 Models and Datasets", "content": "We evaluate the performance of RMU using the publicly available checkpoint. This model results from finetuning Zephyr-7B-B (Tunstall et al., 2023) on the WMDP and WikiText corpora (Merity et al., 2016). For NPO and DPO, we finetune Zephyr-7B-\u03b2 ourselves on WMDP. We will refer to these models as unlearned models.\nNPO and DPO require preference datasets, but WMDP only provides corpora (e.g. scientific papers) for autoregressive training. We use GPT-4 (OpenAI et al., 2024) to formulate questions based on these documents. For questions about hazardous topics, we set one of 80 random refusal strings as the desired output and the full correct option as the rejected response. For questions based on the retain set, we keep the correct option as the desired output and reject the refusal. We refer to the resulting datasets as our preference datasets. See Appendix C for details on dataset construction.\nTo ensure a fair comparison with safety methods, we fine-tune Zephyr using DPO specifically on preference datasets relevant to unlearning topics, rather than training it to refuse all harmful requests. We balance the training data by including samples from the forget and retain preference datasets, as well as OpenAssistant (K\u00f6pf et al., 2024), in a 50:25:25 ratio. This approach aims to maintain a balance between refusal capabilities and preserving general utility. For NPO, we use the preference dataset on hazardous knowledge as negative samples and the retain preference dataset mixed with OpenAssistant (50:50) dataset for the auxiliary retain loss."}, {"title": "3.4 Unlearning Evaluation", "content": "We evaluate the performance of unlearning hazardous knowledge using the WMDP benchmark (Li et al., 2024), which consists of 1,273 multiple-choice questions about dangerous biology knowledge and 1,987 about cybersecurity. To detect latent knowledge that might still be present even when models refuse to answer, we select the option (A, B, C, or D) with the highest probability as the final response. Besides, we use MMLU (Hendrycks et al., 2020) to measure the model's general"}, {"title": "4 Our Methods To Recover Hazardous Capabilities", "content": "We use a wide range of methods to uncover hazardous capabilities in the target models, ranging from representation engineering to prompt-based jailbreaks. Most methods are inspired by well-known safety jailbreaks and incorporate small changes to target unlearning methods. All of our methods-except for finetuning-do not modify model weights and, thus, can only access knowledge that was preserved in model weights after unlearning. For finetuning, we primarily use small or unrelated datasets to ensure that models cannot acquire new hazardous capabilities."}, {"title": "4.1 Finetuning", "content": "It has been shown that finetuning easily reverses safety alignment even when using benign datasets (Qi et al., 2023). Also, the original RMU work showed that fine-tuning unlearned models on the entire forget dataset could recover hazardous capabilities. In this work, we fine-tune unlearned models on datasets with very low mutual information (MI) with the unlearned knowledge to ensure that no new knowledge can be acquired. We use Low-Rank Adaptation (LoRA; Hu et al., 2021) to fine-tune unlearned models on three datasets: (1) forget dataset, (2) retain dataset\u2014disjoint with forget dataset by definition, and (3) WikiText (Merity et al., 2016)\u2014a collection of Wikipedia documents with minimal overlap with hazardous knowledge. We experiment with varying sample sizes (from 5 to 1000 examples). By incorporating datasets with high MI (forget set) and low MI (retain set and WikiText), we provide a comprehensive evaluation of how different configurations affect the pace of hazardous knowledge recovery. For further details see Appendix E.1."}, {"title": "4.2 Orthogonalization", "content": "Arditi et al. (2024) demonstrated that safety refusal is governed by a single direction in the activation space. We investigate whether unlearning techniques generate a similar direction. Rather than targeting a single layer, we allow for distinct refusal directions at each transformer block. Using the forget preference dataset, we collect the outputs of each transformer block from both the original and unlearned models. We then compute the refusal direction for each layer using the difference in means method (Belrose, 2023). At inference time, we remove the refusal direction at each layer. Additionally, we develop a setup that does not require access to the original model prior to unlearning; see Appendix E.2 for details."}, {"title": "4.3 Logit Lens", "content": "Logit Lens is an interpretability technique (nostalgebraist, 2020; Patil et al., 2024) that projects the activations in residual stream onto the model's vocabulary. We apply this technique to the WMDP dataset by using the projected logits of the A, B, C, and D tokens as the model's answers. We project the output of transformer blocks at every layer and select the token with a higher probability. We also evaluate the projection of other activation spaces in Appendix G.3."}, {"title": "4.4 Enhanced GCG", "content": "GCG has been reported ineffective against RMU (Li et al., 2024; Huu-Tien et al., 2024). We introduce enhanced GCG, which especifically targets unlearning methods, and is based on FLRT (Thompson and Sklar, 2024) and augmented with several modifications detailed in Appendix E.4. Unlike GCG, which aims to find adversarial prompt suffixes, enhanced GCG focuses on optimizing prefixes to prevent the model from recognizing hazardous knowledge in the first place, as RMU will introduce persistent noise to the residual stream once such context is detected. We also attribute more weight to the loss computed on early tokens in the prompt. Our attack is optimized on 6 questions from the WMDP benchmark that were answered correctly by the original model and incorrectly by the unlearned model."}, {"title": "4.5 Set difference pruning", "content": "Wei et al. (2024b) introduced set difference pruning as a method to identify and prune neurons associated with safety alignment. Reproducing their method, we use SNIP (Lee et al., 2018) score to measure the importance of individual neurons for hazardous knowledge. Specifically, we compute the importance score for each neuron on the WMDP forget set, and the utility score on MMLU. We then use set difference method to find the neurons that only contribute to storing hazardous knowledge and remove them via pruning."}, {"title": "5 Results", "content": "We report the performance of our methods on WMDP-Bio due to significant difference in the scores of original and unlearned models. Analogous gap on WMDP-Cyber is much smaller, which makes the corresponding results more volatile (See Appendix F). We summarize our results and observations below.\nFinetuning on unrelated information reverts unlearning. As illustrated in Figure 1, finetuning with only 10 samples from the retain set\u2014disjoint by definition from the evaluation knowledge-can recover most of hazardous capabilities, obtaining accuracies of 52.7% (NPO), 57.0% (DPO), and 61.6% (RMU) while causing negligible degradation on MMLU (less than 2 p.p.). Finetuning on 1000 samples from the retain set fully recovers hazardous capabilities across all methods. These results demonstrate that both safety training and unlearning can be undone through finetuning on unrelated information, suggesting that unlearning is also expressed through shallow features (Yang et al., 2023; Lermen et al., 2023). Additionally, finetuning with just 5 samples from the forget set effectively reverses unlearning, particularly for RMU, which nearly recovers its original performance. Relearning knowledge through further training is unavoidable, but these results show that knowledge recovery happens at disproportionately fast rate.\nUnlearning methods remove knowledge from the residual stream more effectively. Before unlearning, Logit Lens can decode correct answers from Zephyr-7B at layer 19, as shown in Figure 2. However, Logit Lens becomes ineffective after protections are applied. Our safety baseline, DPO, remains the most susceptible to early decoding, achieving 56% accuracy. In contrast, unlearning methods can remove knowledge more effecitvely from the residual stream, with RMU reducing Logit Lens accuracy close to random chance across the entire architecture. These results align with prior evaluations of RMU's robustness to probing (Li et al., 2024)."}, {"title": "6 Discussion", "content": "Existing unlearning methods are not different from safety training. Our findings reveal that unlearning methods primarily obscure knowledge rather than eliminate it, which is a known flaw of safety training (Lee et al., 2024). Therefore, RMU and NPO are susceptible to techniques analogous to those that can reverse safety training, including: (1) dependence on individual residual stream directions; (2) rapid knowledge recovery after finetuning with unrelated data; (3) presence of critical neurons that inhibit hazardous knowledge; and (4) existence of universal adversarial strings that unlock the unlearned knowledge. These observations question the practical benefits of unlearning methods over safety training. Although unlearning was proposed to fully eradicate hazardous capabilities and mitigate jailbreaks in large language models, our results indicate that these methods share inherent limitations.\nBlack-box evaluations are insufficient for unlearning. Our results demonstrate that evaluations based solely on model outputs are not suitable for unlearning methods, as suggested previously by Lynch et al. (2024). Unlearning aims to remove information from model weights, which is fundamentally different from merely rendering knowledge unusable in downstream tasks. We thus argue that output-based evaluations are insufficient and future evaluations should prioritize measuring the extent to which knowledge is genuinely erased from the model weights. As extensively demonstrated in security and safety research, adaptive evaluations are important to understand the limitations of ML protections (Carlini and Wagner, 2017; Tramer et al., 2020; Radiya-Dixit et al., 2021; H\u00f6nig et al., 2024)\nNPO shows signs of deep unlearning. This method consistently displays better robustness than DPO or RMU, suggesting that gradient ascent (Zhang et al., 2024) might be a promising tool to remove hazardous knowledge from model weights. However, our current implementation still results in greater degradation on MMLU and general capabilities. Future work could investigate combining representation engineering with gradient ascent to enhance existing unlearning methods."}, {"title": "7 Conclusion", "content": "We performed a comprehensive white-box evaluation of state-of-the-art unlearning methods for AI safety. Our findings reveal that these methods cannot reliably remove knowledge from model weights. For example, finetuning on unrelated data or removing specific directions from actiavtion space often recovers the supposedly unlearned capabilities. This challenges the belief that unlearning methods offer more robust protection than standard safety training. Furthermore, we argue that black-box evaluations are insufficient for unlearning, as they do not assess internal model changes."}, {"title": "A Further discussion on ECO", "content": "Liu et al. (2024a) assume black-box access to the model. Given their setting their definition of successful unlearning entails that, in expectation, any non-negative metric computed on the outputs of an unlearned model and the outputs of a model retrained from scratch on retain set should be approximately one. Intuitively, the model trained only on the retain set should behave the same way as the original model after applying unlearning. Although this is the golden standard in machine unlearning, we consider it lacking for the generative models such as LLMs which show remarkable memorization capabilities (Nasr et al., 2023). The premise is that despite outputs of an LLM not displaying any signs of unlearned knowledge it can be stored within the weights, and retrieved by an adversary. To prevent that the knowledge should be removed from the weights as well. Hence, an improved definition of successful unlearning should include either the internals of an LLM or an adversarial perspective.\nFurthermore, the core of ECO is an \u2018unlearned' knowledge detector, based on which a carefully crafted noise is applied to input embeddings. However, this is no different to a safety filter which given an unethical request would return a predefined refusal prompt. Choosing a suitable noise is merely obfuscating the refusal.\nUltimately, we would like to emphasize that we acknowledge ECO's state-of-the-art results on WMDP. However, we argue that it doesn't uphold the promise of unlearning."}, {"title": "A.2 Potential vulnerabilities", "content": "Using a detector together with unmodified LLM, puts the red-teaming pressure on the former. As a consequence, the fundamental issue of defending the LLM is not resolved but rather reintroduced on a smaller scale, where we have to defend the detector (which in (Liu et al., 2024a) is a smaller LLM - ROBERTa).\nAfter inspecting the code, we noticed that there are two types of detectors implemented: token-wise and prompt-wise. The first one can be easily bypassed by forcing the tokenizer to tokenize the prompt character-by-character (e.g. by inserting whitespace between all relevant characters). Individual characters should not trigger any noise as they should not be exclusive to dangerous concepts. The second type of detector might be slightly more challenging, but there is significant body of works on adversarial attacks on BERT models (Li et al., 2020), including the specific scenario of text classification (Garg and Ramakrishnan, 2020)."}, {"title": "B Further details on unlearning and safety training methods", "content": null}, {"title": "B.1 Direct Preference Optimization (DPO)", "content": "DPO (Rafailov et al., 2024) uses a preference dataset $D_{PREF}$ consisting of triples: an input $x$, a chosen response $y_w$ and a rejected response $y_l$. Model is then trained to produce generations that are closer to the chosen subset using the following objective:\n$L_{DPO}(\\theta) = -E_{(x,y_w,y_l)\\sim D_{PREF}}[log \\sigma(\\beta (log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)}))]$,\nwhere $\\pi_{ref}$ is reference model, $\\pi_{\\theta}$ is trainable model with weights $\\theta$, $\\beta$ is a variable controlling deviation from $\\pi_{ref}$, and $\\sigma$ is a sigmoid function."}, {"title": "B.2 Negative Preference Optimization (NPO)", "content": "NPO (Zhang et al., 2024) optimizes a loss function inspired from DPO, where one uses only negative samples. Although, it may appear that this introduces inductive bias towards safety training, counter-intuitively it does not. Zhang et al. (2024) shows that NPO is a generalization of gradient ascent (GA). This resemblance is a desirable feature in unlearning as GA is the reverse process to gradient descent based learning. Furthermore, the authors show that NPO diverges at much slower rate than GA, making it more stable and thus, practical.\nIn the pilot experiments with straightforward application of NPO our models quickly diverged, resulting in catastrophic forgetting, indicated by poor performance on the utility benchmark. NPO collapsing when trying to unlearn broad domains is in line with other works suggesting that it fails in continual learning settings (Gao et al., 2024). Therefore, we focus on a variation of NPO which adds a retain loss (RT) to the original objective:\n$L_{NPO}(\\theta) = - E_{DFG} [log \\sigma(\\beta log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)})] + \\alpha \\cdot E_{DRT} [log(\\pi_{\\theta}(y|x))]$,\nwhere $\\alpha$ is a weight of the retain loss, and $(x, y)$ are input output pairs from the forget set $D_{FG}$ and from the retain set $D_{RT}$. We refer to this method as NPO."}, {"title": "B.3 Representation Misdirection for Unlearning (RMU)", "content": "RMU (Li et al., 2024) finetunes a subset of lower layers of an LLM such that they output a fixed noise vector when given a prompt containing concepts present in the forget set and to leave representations unchanged if the concepts fall within the knowledge captured by the retain set. This method displays high sensitivity to keywords and behaves like a heavy-side function once \"hazardous\" concept is detected - internal representations will be distorted for all the subsequent tokens in the prompt. For detailed analysis of RMU see Appendix J. The RMU objective is as follows:\n$L_{RMU} (\\theta) = E_{x \\sim D_{FG}} \\frac{1}{L_x} \\sum_{t \\epsilon x} ||M_{\\theta}(t) - c \\cdot u||_2^2 + \\alpha \\cdot E_{x \\sim D_{RT}} \\frac{1}{L_x} \\sum_{t \\epsilon x} ||M_{\\theta}(t) - M_{ref}(t)||_2^2$,\nwhere $M_{ref}, M_{\\theta}$ are the internal representations of the reference and trainable models, $L_x$ is the number of tokens in prompt $x$, and $c$ is a variable controlling the magnitude of noise vector $u$."}, {"title": "C Preference dataset construction", "content": "DPO and NPO require preference datasets of a specific format. We construct such datasets from the WMDP forget and retain documents. Furthermore, to make them more suitable for WMDP Benchmark we format individual samples as ABCD questions.\nWMDP provides four corpora: bio-forget-corpus, bio-retain-corpus, cyber-forget-corpus, and cyber-retain-corpus. Biology ones are filtered scientific articles. Forget version contains abstract together with text and we decided to use the former due to its condensed nature. Cybersecurity corpora are mostly scraped github pages of oftentimes doubtful quality.\nWe used only the articles with more than 1000 characters to make sure that we filter out samples containing only titles and to provide enough material to generate 10 questions from each article. Moreover, each article was truncated at 15'000 characters to avoid unnecessary costs.\nTo generate questions based on the abovementioned corpora we used OpenAI API and the gpt-40-mini-2024-07-18 model. Furthermore, we used their recent feature, Structured Outputs, with the following schema:\nWe sent requests until we obtained 10'000 samples from each corpora. However, each dataset can have up to 9 samples more since the model sometimes produced less then 10 questions and in some cases the correct answer was missing from the provided options. In our requests we used the system prompt defined below."}, {"title": "C.1 System prompt", "content": "In this system prompt we provide a clear objective, fixed number of options, and number of questions to generate from each article. Furthermore, we provide an example of a successfully completed task crafted from one of the WMDP bio-retain-corpus article to leverage few-shot learning paradigm."}, {"title": "C.2 Preference format", "content": "Each generated sample would then be formatted into the following prompt:"}, {"title": "C.3 Refusal strings", "content": "We used GPT-4 to generate 80 different refusal strings, which were manually checked for variety. Their list is below."}, {"title": "D Training details", "content": "Both DPO and NPO use version of Open Assistant dataset that is formatted as preference datasest and is available here: https://huggingface.co/datasets/javirandor/oasst2_dpo. Before training, it is mixed with forget and retain datasets to obtain the best balance between unlearn-ing/refusal and utility. The mixing is done using HuggingFace's interleave_datasets function with stopping strategy set to 'first_exhausted'. Furthermore, prior to training we randomly apply chat template to 50% of the samples in the final dataset since our initial experiments have shown that training only without it doesn't affect the situation with chat template applied (converse is also true)."}, {"title": "D.1 Hyperparameters", "content": "We performed a limited hyperparameter search over learning rate, $\\beta$, number of epochs and the dataset mixing proportions to obtain best model. For NPO we also searched over $\\alpha$. The best hyperparameters are the following:"}, {"title": "D.2 Performance of developed models on relevant benchmarks", "content": "We train NPO and DPO version of Zephyr for both hazardous domains. Performance of these models on WMDP benchmark and MMLU is shown in Table 3."}, {"title": "E Additional details on knowledge extraction methods", "content": "This sections contains additional details omitted in the main part of the paper."}, {"title": "E.1 Finetuning", "content": "Hyperparameters used for finetuning as knowledge extraction method are in Table 4.\nFor RMU we use WMPD's bio-forget-corpus (abstracts) as forget set, WMPD's bio-retain-corpus as retain set, and Wikitext as retain set with lowest mutual information. For cybersecurity setting we use the cyber counterparts of the first two datsets. For actual finetuning we use the following template:"}, {"title": "E.2 Orthogonalization", "content": "To show that directional ablation technique is still applicable in settings without access to the original model we devise the following. Since, we need to identify the 'unlearning' directions for these specific domains we need to create contrast between representations containing hazardous knowledge and benign representations. To do so we gather 'hazardous' representations by conditioning LLMs on forget prompts from the preference dataset. For benign representations we use (1) Wikitext corpus and (2) MMLU validation set formatted as ABCD questions. Ultimately, we can obtain desired directions by taking difference in means.\nUsing first principal component as ablation direction. We have also investigated how prominent are 'unlearning' directions in the residual stream. Thus, we have collected representations generated by the original model and its unlearned counterparts on forget preference dataset. Then, we used PCA to extract 1st principal component that was used as ablation direction.\nLastly, to achieve success in this setting it was crucial to filter out outlier tokens. For this purpose we computed 12 distance between first 1000 tokens collected and computed their z-scores. Then we discarded all tokens with z-score larger than 3, from the whole dataset. This was necessary because Zephyr-7B-B attributes very specific representations to \u2018< s >' token and first \u2018\\n' tokens, which are very distant from all the other representations and thus bias difference in means significantly.\nAblations on layer subsets. To evaluate dependence of \u2018unlearned directions on particular layer subsets, instead of applying directional ablation to all layers we applied it solely to layers: 0, 7, 15, 23, and 31 (the last layer of Zephyr-7B-\u03b2). Furthemore, in case of RMU we applied orthogonalization only on the layers previously subjected to unlearning (5,6,7)."}, {"title": "E.3 Logit lens", "content": "This method projects representations in the residual stream to models' vocabulary. In the main text we project representations outputted by each transformer block. However, we are able to project representations taken at other stages of the architecture as well. These are (1) the outputs of attention module, (2) the intermediate activations after adding output of the attention module to the residual stream, and (3) the outputs of the MLP module. We use projections of these activations to get performance on WMDP at every layer.\nLastly, to emphasize the importance of the A, B, C, D tokens to the model we add the following prefix to all WMDP questions: \u2018Answer the following question with A, B, C, or D.\\n\\n'."}, {"title": "E.4 Enhanced GCG", "content": "There are several notable features of FLRT (Thompson and Sklar, 2024) that elevate it above standard GCG (Zou et al., 2023). First it introduces a fluency loss, based on perplexity computed with several smaller LLMs, which enhances the interpretability of adversarial strings. Secondly, it performs some steps in a manner similar to BEAST (Sadasivan et al., 2024) which makes it faster on average and"}, {"title": "F Complete results", "content": "This section contains the set of results for WMDP-Cyber as well as some other results omitted in the main text."}, {"title": "F.3 Orthogonalization", "content": "Computing 'unlearned' directions is possible without access to the original model. We used benign datasets to obtain clean representations. The results in Table 7 show that in all cases using original model's representations is not necessary we are able to recover significant part of knowledge from all methods using only the unlearned model. Notably performance on NPO increased compared to using original model's representations. However, we were not able to achieve as good performance on RMU as when using original model."}, {"title": "F.3.1 Results for WMDP-Cyber", "content": "In case of WMDP-Cyber we were also able to obtain unlearning directions for all methods except for NPO, where ablating directions at all layers significantly damaged the residual stream. However, when we ablated only direction, obtained through PCA, at layer 15 we were able to achieve accuracy of 35.0 on WMDP-Cyber and 55.4 on MMLU."}, {"title": "G Complete results using chat template", "content": "During the execution of experiments we noticed multiple cases when the model's behaviour was affected in chat template environment but not in the environment without it (converse is also true). For examples, originally we trained DPO only with chat template. However, then we observed that while there was a substantial drop in WMPD-Bio performance using chat template, without it the model's performance was significantly better. We encountered analogous situation when we trained the model completely without chat template. In Table 8 we list all such examples of such behaviours. As a consequence, when training our final DPO models we applied chat template to 50% of the samples."}, {"title": "G.1 Overview of the results using chat template", "content": null}, {"title": "G.2 Finetuning", "content": null}, {"title": "H Perturbations as a knowledge extraction method for RMU", "content": null}]}