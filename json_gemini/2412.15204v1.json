{"title": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks", "authors": ["Yushi Bai", "Shangqing Tu", "Jiajie Zhang", "Hao Peng", "Xiaozhi Wang", "Xin Lv", "Shulin Cao", "Jiazheng Xu", "Lei Hou", "Yuxiao Dong", "Jie Tang", "Juanzi Li"], "abstract": "This paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding. To ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7% accuracy under a 15-minute time constraint. Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1% accuracy. In contrast, the ol-preview model, which includes longer reasoning, achieves 57.7%, surpassing the human baseline by 4%. These results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2.", "sections": [{"title": "Introduction", "content": "Over the past year, research and products on long-context large language models (LLMs) have made remarkable progress: in terms of context window length, advancing from the initial 8k to the current 128k and even 1M tokens (OpenAI, 2024c; Anthropic, 2024; Reid et al., 2024; GLM et al., 2024); and achieving promising performance on long-context benchmarks. However, beneath these advancements lies an urgent and practical question: Do these models truly comprehend the long texts they process, i.e., are they capable of deeply understanding, learning, and reasoning based on the information contained in these long texts?\nCritically, existing long-context understanding benchmarks (Bai et al., 2024b; Zhang et al., 2024d; Hsieh et al., 2024) fail to reflect the long-context LLMs' deep understanding capabilities across diverse tasks. They often focus on extractive questions, where answers are directly found in the material, a challenge easily handled by modern long-context models and RAG systems, as evidenced by their perfect recall in the Needle-in-a-Haystack test (Kamradt, 2023). Furthermore, many of these benchmarks rely on synthetic tasks, which limits their applicability to real-world scenarios, and their adopted metrics like F1 and ROUGE are unreliable.\nTo address these issues, we aim to build a benchmark with the following features: (1) Length: Context length ranging from 8k to 2M words, with the majority under 128k. (2) Difficulty: Challenging enough that even human experts, using search tools within the document, cannot answer correctly in a short time. (3) Coverage: Cover various realistic scenarios. (4) Reliability: All in a multiple-choice question format for reliable evaluation.\nWith the above goal in mind, we present Long-Bench v2. LongBench v2 contains 503 multiple-choice questions and is made up of 6 major task categories and 20 subtasks to cover as many realistic deep comprehension scenarios as possible, including single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding (detailed in Table 1). All the test data in LongBench v2 are in English, and the length distribution of each task category is shown on the left of Figure 1.\nTo ensure the quality and difficulty of test data, we combine automated and manual reviews during data collection. We first recruit 97 data annotators with diverse academic backgrounds and grades from top universities and then select 24 data reviewers from this group. Annotators provide data including long documents, questions, options, answers, and evidence. We then leverage three long-context LLMs for an automated review, where a question is considered too easy if all three LLMs answer it correctly. Data passing the automated review are assigned to the reviewers, who answer the questions and determine whether the questions are appropriate (meet our requirements) and if the answers are correct. In our criteria, a qualified data point should have (1) an appropriate question with an objective, correct answer; (2) sufficient difficulty, such that all three LLMs cannot answer correctly at the same time, and the human reviewer cannot answer correctly within 3 minutes, even with searching tools within the document. If data do not meet these criteria, we request modifications from the annotator. We also set length and difficulty incentives to encourage longer and harder test data. Figure 1 (right) visualizes the distribution of expert solving times along with human accuracy. Overall, our data shows a median word count of 54k and an average of 104k words. Human experts are able to achieve an accuracy of only 53.7% within 15 minutes, compared to 25% accuracy with random guessing, highlighting the challenging nature of the test. In the evaluation, the best-performing model achieves only 50.1% accuracy when directly outputting the answer. In contrast, the 01-preview model, which incorporates longer reasoning during inference, reaches 57.7%, surpassing human experts. This implies that Long-Bench v2 places greater demands on the reasoning ability of current models, and incorporating more inference-time thinking and reasoning appears to be a natural and crucial step in addressing such long-context reasoning challenges. We hope Long-Bench v2 will accelerate the exploration of how scaling inference-time compute will affect deep understanding and reasoning in long-context scenarios."}, {"title": "Related Work", "content": "We divide existing long-context benchmarks for LLMs into two types. The first consists of comprehensive benchmarks that combine multitasks such as QA, retrieval, and summarization. Sorted by publication date, these benchmarks include ZeroSCROLLS (Shaham et al., 2023), L-Eval (An et al., 2024), LongBench (Bai et al., 2024b), BAMBOO (Dong et al., 2024), LooGLE (Li et al., 2023), \u221e-bench (Zhang et al., 2024d), Ruler (Hsieh et al., 2024), and HELMET (Yen et al., 2024). It is noteworthy that most of these multitask benchmarks were proposed last year, which corresponds to the thrive of long-context LLMs, whose context length has been extended to 128k tokens or more (Anthropic, 2024; OpenAI, 2024c; Reid et al., 2024; GLM et al., 2024; Dubey et al., 2024) through continual training (Xiong et al., 2024; Fu et al., 2024; Bai et al., 2024a; Gao et al., 2024).\nThe other category of long-context benchmarks is more targeted, evaluating models on specific types of long-context tasks, including document QA (Ko\u010disk\u1ef3 et al., 2018; Dasigi et al., 2021; Pang et al., 2022; Wang et al., 2024a), summarization (Zhong et al., 2021; Huang et al., 2021; Wang et al., 2022), retrieval and attributing (Kamradt, 2023; Kuratov et al., 2024; Song et al., 2024; Laban et al., 2024; Zhang et al., 2024b; Vodrahalli et al., 2024), conversation (Bai et al., 2024a), coding (Liu et al., 2023; Bogomolov et al., 2024), many-shot learning (Agarwal et al., 2024), and long-text generation (Bai et al., 2024d; Wu et al., 2024b).\nIn our view, existing long-context benchmarks generally have the following issues: (1) Lack of deep reasoning: While a few benchmarks contain longer examples of around 100k, most of these data have not been human-examined, and many of these samples can be solved through shallow understanding such as retrieval, thus failing to reflect a model's deep reasoning capabilities. (2) Unreliable metrics: Many datasets use metrics like ROUGE and F1 for evaluation, which are known to be unreliable (Novikova et al., 2017). Additionally, some datasets adopt LLM-as-a-judge (Zheng et al., 2023) for evaluation, which can be costly and may introduce biases in their assessments (Bai et al., 2024c; Ye et al., 2024). To construct a more challenging, reliable, and comprehensive long-context bench-"}, {"title": "LongBench v2: Task and Construction", "content": "Our design principle focuses on four aspects: (1) The context should be sufficiently long to cover scenarios ranging from 8k to 2M words, with a relatively even distribution across texts up to 128k words. (2) The question should be challenging, requiring the model to deeply understand the context to answer. It should avoid questions that can be answered based on memory or those where the answer can be directly extracted from the context. (3) The data should cover a wide range of real-world long-context scenarios and reflect the model's holistic ability to reason, apply, and analyze information drawn from the lengthy text. (4) The data should be in English and in a multiple-choice question format, containing a long text, a question, four choices, a groundtruth answer, and an evidence. Distractors should be included to prevent the model from guessing the correct answer based on option patterns.\nBased on the testing scenarios and the types and sources of long texts, we propose six major task categories and further divide them into 20 subtasks. We introduce the tasks included in LongBench v2 in the following. A list of task statistics and detailed descriptions can be found in Table 1 and Appendix B.\nSingle-Doc QA. We integrate subtask categories from previous datasets (Bai et al., 2024b; An et al., 2024) and expand them to include QA for academic, literary, legal, financial, and governmental documents. Considering that detective QA (Xu et al., 2024) requires in-depth reasoning based on case background, we introduce such a task that requires identifying the killer or motive based on information provided in detective novels. We also include Event ordering, where the goal is to order minor events according to the timeline of a novel.\nMulti-Doc QA. To distinguish from single-doc QA, multi-doc QA requires answers drawn from multiple provided documents. Besides the categories in single-doc QA, multi-doc QA also includes multi-news QA, which involves reasoning across multiple news articles, events, and timelines.\nLong In-context Learning. Learning from a long context, such as acquiring new skills, requires the ability to comprehend and reason based on that context. Hence, we consider it as a major category of tasks. LongBench v2 includes several key tasks, including User guide QA, which answers questions with information learnt from user guides for electronic devices, software, etc.; New language translation (Tanzer et al., 2024; Zhang et al., 2024a), which involves learning to translate an unseen language from a vocabulary book; Many-shot learning (Agarwal et al., 2024), which involves learning to label new data from a handful of examples.\nLong-dialogue History Understanding. LLMs, as more intelligent chatbots or agents, require enhanced memory capabilities to handle longer histories. Therefore, we integrate long-dialogue history understanding tasks to test whether LLMs can handle information from long conversation histories. These tasks are divided into two subtasks based on the source of the conversation history: one involving the history of interactions between multiple LLM agents, i.e., Agent history QA (Huang et al., 2024), and the other involving the dialogue history between a user and an LLM acting as an assistant, i.e., Dialogue history QA (Wu et al., 2024a).\nCode Repository Understanding. Code repository contains long code content, and question answering over a code repository requires understanding and reasoning across multiple files, making it a common yet challenging long-context task.\nLong Structured Data Understanding. In addition to textual data, much information is presented in structured forms, so we introduce the long structured data QA task to test the LLM's understanding of long structured data, including reasoning on long tables, i.e., Table QA (Zhang et al., 2024c), and answering complex queries on knowledge graphs (KGs), i.e., Knowledge graph reasoning (Cao et al., 2022; Bai et al., 2023). We anonymize the entities in the KG to prevent the model from directly deriving the answers through memorization.", "Data Collection": "To collect high-quality and challenging data for long-context tasks, we hire 97 annotators who are either holding or pursuing a bachelor's degree from top universities and are proficient in English, with detailed statistics shown in Appendix C.2. We also select 24 professional human experts based on their major and year of study for conducting manual reviews. Figure 2 illustrates the overall pipeline of our data collection process, which consists of five steps: document collection, data annotation,"}, {"title": "Data Statistics", "content": "We categorize the 503 data entries in Longbench v2 based on their difficulty, length, and task types. According to the difficulty criteria defined in the previous section, 192 are classified as \u201cEasy\u201d, while 311 are deemed \"Hard\". Based on word count, the data is divided into three groups: \u201cShort\" (<32k), \"Medium\" (32k-128k), and \u201cLong\u201d (>128k), containing 180, 215, and 108 entries, respectively, exhibiting a relatively balanced distribution. For the data distribution across task types, please see Table 1. Also, the questions with answers A, B, C, and D account for approximately 19%, 25%, 30%, and 26% of the total, respectively, showing that the distribution of answers across the four options is relatively even. We also analyze the proportion of data submissions rejected during manual review and find that 4% of the submissions are rejected for illegal question; 7% are rejected for insufficient difficulty; and 4% are rejected for wrong answer."}, {"title": "Evaluation", "content": "Setup. We evaluate 10 open-source LLMs, all of which have a context window size of 128,000 tokens, along with 6 proprietary LLMs. We apply middle truncation as described in Bai et al. (2024b) for sequences exceeding the model's context window length. Given the complex reasoning required by our test data, we adopt two evaluation settings: zero-shot and zero-shot + CoT. Following Rein et al. (2023), in the CoT setting, the model is first prompted to generate a chain of thought (Wei et al., 2022), after which it is asked to produce the final answer based on the chain of thought. For details on reproducing our results, please refer to Appendix D. The code is available at https://github.com/THUDM/LongBench.\nResults. We report the evaluation results along with human expert performance in Table 2. The results under the CoT evaluation setting are highlighted with a gray background, while the highest scores among open-source models and proprietary models are in bold. The results indicate that Long-Bench v2 presents a significant challenge to the current model\u2014The best-performing 01-preview model achieves only 57.7% accuracy, which is 4% higher than the performance of human experts under a 15-minute time limit. Additionally, the scaling law effect on our benchmark is striking: smaller models such as GLM-4-9B-Chat, Qwen2.5-"}, {"title": "Retrieval-Augmented Baselines", "content": "Based on recent studies (Jiang et al., 2024; Jin et al., 2024; Leng et al., 2024), we explore incorporating retrieval-augmented generation (RAG, Lewis et al. (2020)) into long-context LLM and evaluate its performance on LongBench v2. We first split the long context into chunks of 512 tokens with GLM-4-9B tokenizer. Then, we use Zhipu Embedding-3 to encode the query, i.e., the concatenation of the question and choices, and the chunks, and sort the chunks based on embedding similarity. During evaluation, we retrieve the top-N most similar chunks and concatenate them in their original order to form the context input for the model. The model is then prompted to answer the question in a zero-shot setting. For each evaluated model, we take N = 4, 8, 16, 32, 64, 128, and 256, and the evaluation results form a curve presented in Figure 4.\nWe observe that Qwen2.5 and GLM-4-Plus show no significant improvement as the retrieval context length increases beyond 32k. Both models perform"}, {"title": "Measuring Memorization of Context", "content": "For an effective long-context benchmark, it is essential to ensure that LLMs cannot rely solely on memorizing previously seen data to answer questions. This necessitates the models to actively read and comprehend the provided long material in order to solve the problems. Following Bai et al. (2024b), we also evaluate the models' performance when providing only the questions, without the accompanying long context. The performance comparison between with (w/) and without (w/o) the context is presented in Table 3. As shown, without context, most models achieve an overall accuracy ranging from 25% to 30%, which is comparable to random guessing. When comparing scores across different tasks, the memorization effect appears minimal for tasks II, III, and VI. The models perform best without context on tasks I and V, likely because they may have seen some of the documents, novels, or code repositories during training."}, {"title": "Conclusion", "content": "Our work introduces LongBench v2, a challenging multitask benchmark for long-context understanding and reasoning, carefully annotated and reviewed by human experts. LongBench v2 presents an equal challenge to both humans and state-of-the-art AI systems, with human performance at 50.1% and the best LLM achieving 57.7% accuracy, providing a reliable evaluation standard for the development of future superhuman AI systems. Our evaluation results also bring forward insights into the impact of scaling inference-time compute and RAG in long-context reasoning."}, {"title": "Limitations", "content": "We acknowledge certain limitations in our work, which we outline below: 1. Benchmark size: The benchmark's size may not be sufficiently large. While this can be seen as an advantage for quick evaluation, it could also lead to less stable results that are more vulnerable to randomness. Due to resource constraints, we are unable to expand the dataset at this time. Collecting the current 503 high-quality samples cost us 100,000 CNY and took more than two months. 2. Language: The current dataset is limited to English only. As a result, our benchmark does not yet capture the performance of models across multiple languages. 3. Length distribution inconsistencies: The length distribution across different tasks is uneven, with certain tasks concentrated around specific lengths. These differences in task distributions across length ranges make it difficult to provide a fair comparison of a single model's performance across length intervals. We recommend conducting comparisons between models on a per-interval basis. For instance, model A may outperform Model B in the short length range, while model B may outperform model A in the long length range. This would suggest that model B is better at handling longer tasks than model A."}, {"title": "Specific Document QA (Academic)", "content": "Task Description: Ask questions based on academic articles (papers, textbooks), excluding content related to charts and figures within the text.\nExample Questions: 1. Which methods were used to collect data in the study? 2. In what ways does the author's argument align or conflict with the findings of Smith et al. (2020)?"}, {"title": "Single-Document QA (Literary)", "content": "Task Description: Ask questions about literary works, potentially covering characters, plot, writing style, and central themes.\nExample Questions: 1. What are the key traits that define [character]'s personality? 2. What is the turning point in the novel, and how does it impact the characters? 3. What message does the author seem to be conveying through the ending?"}, {"title": "Single-Document QA (Legal)", "content": "Task Description: Ask questions based on legal documents, referencing scenarios like legal consultations, case analysis, or legal document review.\nExample Questions: 1. What is the basis of the defendant's defense? 2. How is the estate distributed according to the will? 3. What are the conditions for tax incentives mentioned in this regulation?"}, {"title": "Single-Document QA (Financial)", "content": "Task Description: Ask questions based on financial documents, including but not limited to financial report analysis, market analysis, investment strategies, and risk assessment.\nExample Questions: 1. Based on the report, how do changes in operational expenses align with the company's revenue growth strategy? 2. What macroeconomic indicators are likely to impact the company's performance in the next fiscal year, and how are they addressed in the document? 3. How does the document evaluate the impact of regulatory changes on the company's capital structure?"}, {"title": "Single-Document QA (Governmental)", "content": "Task Description: Ask questions based on government reports and official documents, potentially covering policies, regulations, and public facilities.\nExample Questions: 1. What are the main allocations for healthcare in this year's government budget? 2. Who qualifies for the education grants mentioned in this document? 3. How does this policy address the concerns of small businesses?"}, {"title": "Single-Document QA (Detective)", "content": "Task Description: Ask questions based on a detective or mystery novel. Questions must be inferable after reading most of the novel, such as who the murderer is or what the method of the crime was, without the full reasoning or answer being directly present in the text.\nExample Questions: 1. Who murdered Mary?"}, {"title": "Single-Document QA (Event ordering)", "content": "Task Description: Given a long text (usually a novel) and 4 plot events from the novel in random order, the model is required to select the correct sequence of the plot development.\nExample Questions: 1. Order the four events in their original order..."}, {"title": "Multi-Document QA (Academic)", "content": "Task Description: Ask questions based on academic articles (papers, textbooks), excluding content related to charts and figures. Questions must require using the information from at least 2 documents to be answered, with no irrelevant documents.\nExample Questions: 1. What are the improvements of the method in paper A compared with paper B?"}, {"title": "Multi-Document QA (Legal)", "content": "Task Description: Ask questions based on legal documents, requiring at least 2 documents. Questions must require information from each document to be answered, and there should be no irrelevant documents.\nExample Questions: 1. Is Zhang's crime a case of imagined concurrence or statutory concurrence of crimes?"}, {"title": "Multi-Document QA (Financial)", "content": "Task Description: Ask questions based on financial documents, requiring at least 2 documents. Questions must require information from each document to be answered, and there should be no irrelevant documents.\nExample Questions: 1. How has the R&D investment of the enterprises changed in the past ten years?"}, {"title": "Multi-Document QA (Governmental)", "content": "Task Description: Ask questions based on government reports and official documents, requiring at least 2 documents. Questions must require information from each document to be answered, and there should be no irrelevant documents.\nExample Questions: 1. How do the public transportation policies outlined in the 2022 Urban Development Report align with the environmental sustainability goals stated in the 2023 National Green Initiative document?"}, {"title": "Multi-Document QA (Multi-news)", "content": "Task Description: Ask questions based on news articles, requiring at least 2 articles. Questions must require synthesizing information from multiple documents to be answered, and there should be no irrelevant documents.\nExample Questions: 1. How have the top three positions in the medal leaderboard for the 2024 Paris Olympics changed over time?"}, {"title": "Long In-context Learning (User guide QA)", "content": "Task Description: Given a long user guide, e.g., electronic device manual, software manual, musical instrument tutorial, annotate questions that require a deep understanding of the long text.\nExample Questions: 1. I want to do time-lapse photography, how do I shoot it? 2. In what situations is it more effective to use parfor in MATLAB? 3. How can you change the timbre and achieve different expressive styles by controlling the force and speed of your key presses?"}, {"title": "Long In-context Learning (New language translation)", "content": "Task Description: Translation tasks involving the rare languages Zhuang (vocabulary book and translation corpus from Zhang et al. (2024a)) and Kalamang (vocabulary book and translation corpus from Tanzer et al. (2024)), requiring reading a vocabulary book to complete.\nExample Questions: 1. Translate the following kalamang into English: Wa me kariak kaia kon untuk emumur kalo tumun amkeiret mu wara nanet."}, {"title": "Long In-context Learning (Many-shot learning)", "content": "Task Description: Given many-shot examples, answer the query based on the given examples. All label information is anonymized and can only be learned from the examples. This task primarily involves multi-class classification datasets, including the named entity recognition dataset FewNERD (Ding et al., 2021), the relation classification dataset DocRED (Yao et al., 2019), the event detection dataset MAVEN (Wang et al., 2020), and the sentiment classification dataset GoEmotions (Demszky et al., 2020).\nExample Questions: 1. What is the entity type of \"glucagon\u201d? 2. What is the relation type between \"The Bone Forest\" and \"Robert Holdstock\u201d? 3. What is the event type of \u201cbecame\u201d? 4. What are the emotions of the document \u201cI'm more interested in why there are goldfish in the picture...\"?\""}, {"title": "Long-dialogue History Understanding (Agent history QA)", "content": "Task Description: Based on the agent dialogue history as context, ask questions about the content of the history. Specifically, we provide annotators with LLMs' dialogue history on playing games, which is derived from the GAMA-Bench (Huang et al., 2024). This dataset includes eight classical multi-agent games categorized into three groups: Cooperative Games, Betraying Games, and Sequential Games. In our task, we use them as context and annotate questions for the agent interaction history.\nExample Questions: 1. Which player is the most selfish one in the fourth round of the game?"}, {"title": "Long-dialogue History Understanding (Dialogue history QA)", "content": "Task Description: Given a multi-turn chat history between a user and an AI assistant, raise a question than demands understanding the dialogue history. To ensure the length of the history, we sample data from LongMemEval (Wu et al., 2024a), which consists of over 500 sessions for each chat history that challenges the long-term memory capabilities of LLMs. We take the chat history as context and raise new questions for long-dialogue understanding.\nExample Questions: 1. How long have I been living in my current apartment in Shinjuku?"}, {"title": "Code Repository Understanding (Code repo QA)", "content": "Task Description: Based on a specific branch or commit of a codebase, annotate questions that require careful reading of multiple parts of the code or a deep understanding of the code's content to answer.\nExample Questions: 1. For the current Megatron-LM framework, if I want to use the THD data format while enabling Context Parallel, how should I modify the experiments for rotary_pos_embedding?"}, {"title": "Long Structured Data Understanding (Table QA)", "content": "Task Description: Given a long table (e.g., financial report) or several interconnected tables, annotate questions that require integrating multiple cells or combining information from multiple tables. We provide annotators with long tables from the dataset proposed by TableLLM (Zhang et al., 2024c).\nExample Questions: 1. For the industry fields involving entertainment, which grows most largely from 2021 to 2023?"}, {"title": "Long Structured Data Understanding (Knowledge graph reasoning)", "content": "Task Description: Given a large-scale knowledge graph, annotate questions and corresponding answers that require integrating multiple entities. We construct the knowledge graph (extracted from Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014)) and the complex logical queries based on the KQAPro dataset (Cao et al., 2022). Groundtruth answers are automatically derived by running the corresponding KoPL program (Cao et al., 2022; Yao et al., 2023) on the graph."}]}