{"title": "Evaluating Human Trust in LLM-Based Planners: A Preliminary Study", "authors": ["SHENGHUI CHEN", "YUNHAO YANG", "KAYLA BOGGESS", "SEONGKOOK HEO", "LU FENG", "UFUK TOPCU"], "abstract": "Large Language Models (LLMs) are increasingly used for planning tasks, offering unique capabilities not found in classical planners such as generating explanations and iterative refinement. However, trust-a critical factor in the adoption of planning systems-remains underexplored in the context of LLM-based planning tasks. This study bridges this gap by comparing human trust in LLM-based planners with classical planners through a user study in a Planning Domain Definition Language (PDDL) domain. Combining subjective measures, such as trust questionnaires, with objective metrics like evaluation accuracy, our findings reveal that correctness is the primary driver of trust and performance. Explanations provided by the LLM improved evaluation accuracy but had limited impact on trust, while plan refinement showed potential for increasing trust without significantly enhancing evaluation accuracy.", "sections": [{"title": "1 INTRODUCTION", "content": "Planning is the process of determining a sequence of actions to transit from an initial state to a desired goal state. Planners-systems designed to generate such action sequences under given constraints-play a critical role in automating decision-making processes in domains such as robotic navigation, logistics optimization, and medical scheduling.\nTraditional planners, while effective in structured and predictable environments, often struggle with rigidity and lack of explainability. In contrast, Large Language Models (LLMs) have recently demonstrated strong performance in various domains, including text generation [17], question answering [24, 25], and code completion [18]. Unlike traditional planners, LLMs support multi-plan generation (i.e., return multiple plans to enable users to choose), dynamic adjustments based on externally given information, and understandable communication with humans via natural language. These strengths have sparked growing interest in using LLMs as planners across diverse domains, including robotics [11, 26, 33, 45], healthcare [3, 27], and law [4, 42].\nHowever, the increasing use of LLM-based planners raises concerns, particularly regarding trust. Trust, defined as the willingness to rely on automated systems [16], is vital for the adoption of planning systems. Without trust, even systems with superior technical capabilities may struggle to gain acceptance in practical settings [40]. Planning tasks are uniquely challenging due to their reliance on high correctness, sequential reasoning, and the need for robust adaptation to dynamic environments [2]. These factors amplify the importance of trust, as both over-trust and under-trust can introduce errors or inefficiencies in planning and can have cascading effects on task success [15, 36]. Thus, fostering appropriate trust levels in LLM-based planners is essential for maximizing their potential while minimizing risks.\nWhile prior research has explored factors influencing trust in LLM-based systems, such as anthropomorphic cues [7], the framing and presence of explanations [30], and user interface design [35], factors influencing human trust in LLMs in the context of planning tasks remain underexplored. As the Planning Domain Definition Language (PDDL) has become a common benchmark for evaluating the planning capabilities of LLMs [31, 32], existing work primarily focuses on technical performance metrics, such as plan correctness and efficiency. To the best of our knowledge, no prior studies have empirically investigated human trust in LLM-based planners compared to classical PDDL solvers in a PDDL domain. This work bridges this gap by conducting an exploratory user study that evaluates trust in a PDDL domain. Specifically, LLMs possess unique capabilities and limitations compared to classical PDDL planners [8, 20] that may affect trust levels. For instance, LLMs can generate natural language explanations to clarify why specific decisions were made [13, 41] and iteratively refine their outputs based on user feedback [6, 22, 34, 44]. These capabilities have been shown in other contexts to enhance user trust by making the planning process more transparent and interactive [14, 28]. However, LLMs also exhibit significant limitations, such as their inability to reliably generate or validate plans independently, even for relatively simple tasks [12, 32, 38, 39]. These capabilities and limitations highlight the need for a deeper understanding of the interplay among correctness, explanation, and refinement.\nTrust can be evaluated using Likert-scale user questionnaires [5, 19, 43] and broader instruments like the Propensity to Trust scale [21], which assesses general attitudes toward machines. This study combines 7-point Likert scale trust scores as a subjective metric with users' evaluation accuracy of generated plans as an objective metric.\nKey findings of our study reveal correctness as the dominant factor influencing both evaluation accuracy and trust, with the PDDL solver achieving the highest scores in both metrics. While explanations provided by the LLM planner enhanced evaluation accuracy, they had minimal effect on trust. Conversely, plan refinement showed potential to increase trust without improving evaluation accuracy. Notably, the results on refinement suggest that LLMs can gain user trust without genuine improvements in their planning capabilities, as the refined plan is generated using the same underlying model. This phenomenon underscores a critical implication: as most LLMs are fine-tuned via subjective human feedback [6, 34], the fine-tuned models tend to generate outputs that comply with human preferences, i.e., humans perceive as appealing or trustworthy, instead of objective correctness. Such tendencies could be exploited, intentionally or inadvertently, to potentially foster overtrust. Overall, the study results offer practical insights for designing human-centered AI planning systems."}, {"title": "2 METHODS", "content": "We evaluate factors influencing user trust in planners by comparing a language-model-based planner, denoted as an LLM Planner (GPT-40 [1]), with a traditional graph-search-based planner, denoted as a PDDL Solver (Fast Downwards [10]). Unlike the PDDL Solver which relies on graph search algorithms, the LLM Planner can reason through the planning problem, explain its proposed solution, and iteratively refine the solution based on external feedback."}, {"title": "2.1 Planning Problem", "content": "A planning problem consists of a planning domain (aspects of a problem that remains consistent, i.e. objects, predicates, actions) and a problem description (particular instance of a planning task, i.e. initial state, goal state), expressed in PDDL. We present an example of the gripper problem in Appendix A.\nWe select the gripper planning problems from the International Planning Competition [37] for plan generation and evaluation. In a gripper planning problem, a robot moves balls between a set of rooms using two grippers. The objective is to create a plan-a sequence of actions-for the robot to move the balls to the defined target rooms. We present a few running examples of the gripper problem in Appendix B (Figure 9)."}, {"title": "2.2 PDDL Solver", "content": "The PDDL Solver takes the planning domain and the problem description as inputs and then generates a plan (a sequence of actions with specific input parameters) described in PDDL. Next, we convert the generated plan into natural language for user studies following the procedure in [29] and display it to users. We present an example in Figure 1. The planner either generates a correct plan defined as the shortest path between the initial and goal states or returns a signal indicating that no solution exists for the given problem."}, {"title": "2.3 LLM Planner", "content": "The LLM Planner addresses planning problems by querying a large language model, using a structured prompt format. The planner then retrieves a natural language plan from the language model. To ensure the output adheres to the desired format, we include a few in-context examples within the prompts. We present an example of this in Appendix B.\nUnlike the PDDL Solver, the LLM Planner may generate incorrect plans that violate the problem specifications (e.g., preconditions of actions) or fail to achieve the goal, as language models may struggle with large state spaces compared to classical planners.\nLLM Planner with Explanation (LLM+Expl). To examine the influence of explanation on user trust, we create a natural language explanation of each generated plan. The trust improvement by adding explanations will motivate training an LLM to explain its plan. This explanation includes an assessment of the plan's correctness, identification of any violations of action preconditions, and an analysis of inconsistencies between the final state achieved and the intended goal state. If a plan is correct, the explanation is \"the plan successfully satisfies the goal conditions.\" If a plan is incorrect, we identify the underlying cause as either a violation of action preconditions or a failure to achieve the goal state. In cases involving precondition violations, we specify the action responsible for the issue.\nFor example, consider the action \"robot moves from room 1 to room 2,\" but the robot is initially located in room 3. This scenario constitutes a violation of the precondition for the \"move\" action. In the latter case, we describe the differences between the final state achieved and the intended goal state, e.g., \"fail to move ball 2 to room 2.\" This function enables the user to better understand why actions are chosen and their effect on the overall plan. We present examples of explanations in Appendix B (Figure 10).\nLLM Planner with Refinement (LLM+Refine). Refining an LLM-generated plan is also possible. So, we offer a prompting mechanism for the LLM Planner to refine the generated plan according to the user feedback. We present a sample user interface on the left of Figure 11 in Appendix B. The mechanism works as follows: First, request the user to indicate the step number where refinement should begin. Second, send the planning domain, problem description, and the original plan to the language model. Next, query the model to rewrite the subsequent steps starting from the user-specified step number. Finally, replace the original plan with the newly refined plan and display it to the user. This mechanism enables the user to focus on a subset of steps, facilitating a deeper interpretation of those actions. However, the correctness of the refined plan is still not guaranteed."}, {"title": "3 USER STUDY DESIGN", "content": "We conducted a user study via Qualtrics to evaluate human trust in plans generated by the planners discussed above. footnoteThis study was approved by IRB#7035 and University of Texas at Austin IRB #6873. Study details are included in Appendix C."}, {"title": "3.1 Participants", "content": "We recruited 30 participants through Prolific [23] (fluent English speakers over the age of 18). After informed consent, a reCAPTCHA test was administered as a bot check. To encourage engagement and accurate responses, participants were offered bonus payments based on their evaluation accuracy, defined as correctly accepting correct plans and rejecting incorrect ones.\nThe participants (80% male, 17% female, and 3% preferred not to say) had an average age of 34.00 (SD=10.11). Regarding prior experience with large language models (LLMs), 80% of participants reported having used LLMs before, while 20% had not. When asked about the frequency of using LLMs specifically for planning tasks, 33% indicated that they use them frequently, 43% occasionally, and 23% never."}, {"title": "3.2 Procedure", "content": "Participants began with a demo session to familiarize themselves with the Gripper problem task and the study interface. The main part of the study comprised four sessions, each corresponding to a different AI planner: Planner A (PDDL), Planner B (LLM), Planner C (LLM+Expl), and Planner D (LLM+Refine). The four sessions were presented in a randomized order to counterbalance any ordering effects.\nEach session contained two tasks, each involving a new Gripper problem (unique initial and goal conditions) with similar difficulty (similar number of plan steps for equal number of rooms and balls). The order of tasks within each session was also randomized. In each task, participants were first presented with a plan generated by the planner and asked to rate their trust in the planner (trust before). Participants were then shown an intervention, which varied depending on the session planner:\n\u2022 For PDDL and LLM, the intervention provided only the consequence of the plan, e.g., \u201cThis plan is correct/wrong!\u201d.\n\u2022 For LLM+Expl, the intervention included both the consequence of the plan and an explanation of the outcome, e.g., \"This plan is wrong because the robot misses the steps of moving ball4 from room4 to room1.\"\n\u2022 For LLM+Refine, participants were first asked to choose between two lines of the plan as a starting point for refinement. A revised plan was then generated beginning from the selected line.\nAfter the intervention, participants were again asked to rate their trust in the planner (trust after). Additionally, participants were asked to decide whether to accept or reject the plan before the intervention for PDDL and LLM planners but after for the other two. This allows evaluation of plan correctness prior to consequences for PDDL and LLM, while focusing on responses to interventions for the others.\nAt the end of the study, participants were informed of their evaluation accuracy as the total number of correctly evaluated tasks out of 8 total tasks (2 tasks per session, 4 sessions in total). The procedure is detailed in Figure 2."}, {"title": "3.3 Independent Variables", "content": "We employed a within-subject design where each participant completes four sessions, each involving one of four planners. The PDDL solver provides 100% correct plans, while the other three planners deliver 50% correct plans. We set this accuracy to ensure non-perfect but meaningful performance across two tasks per session, approximating the observed accuracy in practice [9, 46]. The LLM+Expl planner explains why the plans are correct or not, and the LLM+Refine planner allows participants to refine the plans. The independent variables include correctness (comparing PDDL with LLM), explanation (comparing LLM with LLM+Expl), and refinement (comparing LLM with LLM+Refine)."}, {"title": "3.4 Dependent Measures", "content": "For each session, participants evaluated two tasks. We measured user performance on evaluation accuracy by the number of correctly evaluated tasks (0, 1, or 2). Participants also rated their trust in the planner on a 7-point Likert scale (1 = strongly disagree, 7 = strongly agree) both before and after the intervention. We also measured participants' propensity to trust at the end of each session using a six-item scale [21] to assess their general tendency to trust AI planners. The response options were on a 5-point Likert-type scale ranging from 1 (strongly disagree) to 5 (strongly agree). In our survey, we adapted the scale by replacing the term \u201cmachine\" with \"AI planner\" to reflect the context of our study better (see Appendix C.2 for the exact scale we used)."}, {"title": "3.5 Hypotheses", "content": "We formulated the following hypotheses to examine the effects of correctness, explanations, and refinement on user performance (plan evaluation accuracy) and trust:\n\u2022 H1: Planners that are more correct increase evaluation accuracy.\n\u2022 H2: Planners that provide explanations increase evaluation accuracy.\n\u2022 H3: Planners that allow for plan refinement increase evaluation accuracy.\n\u2022 H4: Planners that are more correct improve user trust.\n\u2022 H5: Planners that provide explanations improve user trust.\n\u2022 H6: Planners that allow for plan refinement improve user trust."}, {"title": "4 RESULTS & ANALYSIS", "content": "This section presents findings from our user study on evaluation accuracy, user trust, and the propensity to trust scale."}, {"title": "4.1 On Evaluation Accuracy", "content": "Figure 4 presents the average number of correctly evaluated tasks for each planner, with error bars indicating standard deviations. We used the Wilcoxon signed-rank test to evaluate our hypotheses H1-H3.\nFor H1, participants achieved an average accuracy of 1.76 \u00b1 0.50 with the PDDL solver, compared to 1.52 \u00b1 0.56 for the LLM planner. This result supports our hypothesis that correctness is a key determinant of evaluation accuracy. However, the difference was not statistically significant (W = 18, Z = \u22124.31, p = 0.071, r = -0.801). We suspect that increasing the sample size could reduce this uncertainty and strengthen the observed trend.\nFor H2, evaluation accuracy improved for the LLM planner when explanations were provided (LLM+Expl), reaching 1.76 \u00b1 0.43. This difference was statistically significant (W = 5, Z = \u22124.59, p = 0.020, r = -0.853), supporting our hypothesis that planners with explanations increase evaluation accuracy.\nFor H3, the results for the LLM planner with refinement (LLM+Refine) did not align with our hypothesis. Participants achieved an average accuracy of 1.38 \u00b1 0.61 with the LLM+Refine planner, compared to 1.52 \u00b1 0.56 for the basic LLM planner. Although this result deviates from our hypothesis that refinement would improve evaluation accuracy, the difference is not statistically significant (W = 22, Z = -4.23, p = 0.285, r = -0.785). As a result, we cannot conclusively confirm or refute H3. One possible explanation for this deviation is overtrust: Participants may assume that the opportunity to revise the plan ensures the planner would correct itself, leading them to evaluate the revised plan less critically and, consequently, with lower accuracy.\nThus, the data suggests support for H1, confirms H2, and suggests rejection of H3."}, {"title": "4.2 On Trust", "content": "Figure 3 shows participants' average self-reported trust levels before and after each intervention, measured on a 7-point Likert scale, with error bars representing standard deviations. We used the Wilcoxon signed-rank test to evaluate our hypotheses H4-H6.\nFor H4, Figure 3a shows that PDDL achieved statistically significantly higher trust levels than LLM both before the intervention (W = 134.5, Z = \u22125.75, p < 0.001, r = -0.742) and after (W = 19, Z = \u22126.60, p < 0.001, r = -0.852). In terms of trust dynamics, participants' trust in PDDL significantly increased from 5.68 \u00b1 1.66 to 6.27 \u00b1 1.02 (W = 10, Z = -6.66, p = 0.001, r = -0.860). In contrast, trust in LLM showed a slight decrease from 3.97 \u00b1 2.22 to 3.85 \u00b1 2.30, though this change was not statistically significant (W = 215.50, Z = -5.15, p = 0.722, r = -0.665). These findings support the hypothesis that the correctness of planners is a key factor influencing human trust.\nFor H5, Figure 3b shows no statistically significant difference in trust levels between LLM and LLM+Expl, both before and after the intervention. This result challenges our hypothesis that providing explanations would increase trust when correctness is controlled. One possible interpretation is that participants primarily value the objective correctness of the plans, with explanations offering little benefit unless correctness improves. Alternatively, explanations may help participants calibrate their trust by revealing the planner's limitations, allowing them to adjust their trust to appropriate levels. This insight suggests that improving trust in LLMs for planning tasks may require prioritizing the objective correctness of the plans over supplementary explanations.\nFor H6, Figure 3c shows a slight increase in trust levels with LLM+Refine. On average, trust rose from 3.97 \u00b1 2.22 to 4.12 \u00b1 2.25 before the intervention and from 3.85 \u00b1 2.30 to 4.45 \u00b1 2.00 after. While this trend is not statistically significant, it suggests a potential positive effect of refinement on human trust with the LLM planner.\nThus, the data supports H4, suggests rejection of H5, and suggests support of H6."}, {"title": "4.3 Propensity to Trust Scale", "content": "The propensity to trust scale consists of six questions used to assess an individual's tendency to trust machines based on their current behaviors [21]. While we did not have specific hypotheses tied to this scale, we included it to gather initial insights for future exploration. We found that four questions showed statistically significant differences between planners using the Wilcoxon signed-rank test and are displayed in Figure 5. Full results are provided in the appendix.\nFor Q1 and Q6, we observe a clear shift toward agreement after the PDDL condition compared to the initial baseline. This suggests that participants were more inclined to trust AI planners following the PDDL session, likely due to the 100% correctness of PDDL plans, which appears to boost trust. In contrast, for Q1, Q4, and Q5, we see a notable reduction in agreement after interacting with the LLM planner compared to the PDDL solver. This decrease aligns with the reduced correctness of the LLM plans (50%), highlighting the importance of correctness in maintaining trust in AI planners. Interestingly, Q4 reveals that providing explanations (LLM+Expl) helps recover participants' agreement levels compared to the basic LLM condition. However, this positive effect of explanations on trust propensity is limited, as it is only observed in one of the six questions.\nThese results underscore that correctness remains the dominant factor influencing participants' general trust attitude towards AI planners, with explanations offering only minimal benefit when correctness is suboptimal."}, {"title": "5 DISCUSSION", "content": "Summary. Our findings provide significant insights into the influence of correctness, explanations, and refinement on evaluation accuracy and user trust in AI-based planners. In particular, the findings are three-fold: (1) The correctness of the generated plans is the most significant factor that impacts the evaluation accuracy and user trust in the planners. As the PDDL solver is more capable of generating correct plans, it achieves the highest evaluation accuracy and trust. (2) The explanation component of the LLM planner improves evaluation accuracy, as LLM+Expl achieves higher accuracy than LLM alone. Despite this improvement, LLM+Expl minimally impacts user trust. However, alternative explanation methods may influence user trust differently from the manually generated explanations used in our approach. (3) The refinement procedure in the LLM planner does not lead to a significant improvement in evaluation accuracy; however, it exhibits a positive influence on user trust that may indicate an overtrust in some situations. Finally, the propensity-to-trust analysis identifies correctness as the primary determinant of user trust, whereas explanations provided limited improvement in scenarios where the planner's accuracy is diminished.\nFuture Research. Future steps in this research include expanding user studies with larger sample sizes to improve generalizability and including additional planning problems per session for a more comprehensive evaluation. Next, we will explore alternative methods for generating plan explanations beyond manual creation to identify approaches that more effectively enhance user trust. Additionally, we will examine user trust by employing multiple LLM-based planners with varying levels of planning accuracy to better understand the interplay between planning correctness and user trust. Furthermore, we aim to enable real-time user-planner interaction, allowing users to provide feedback and refine plans collaboratively, thereby fostering a more dynamic and user-centric planning process."}, {"title": "A GRIPPER PLANNING PROBLEM", "content": "The types and predicates collaboratively define the states of a planning environment. Then, the actions define the transition of the environment. Each action consists of a set of input parameters, a precondition, and an effect. We consider the precondition and effect as the initial and final state of the action.\nThe types of objects of the gripper problem are defined as\n1 (: types room ball robot gripper)\n2; there are several balls distributed in several rooms and a robot with two grippers.\nand the predicates are defined as\n1 (:predicates (at-robby ?r robot ?x room); a predicate indicating the robot's location\n(at ?o ball ?x room); a predicate indicating the ball's location\n(free ?r robot ?g gripper); a predicate indicating whether the robot's gripper is free\n(carry ?r robot ?o ball ?g gripper)); indicating the ball carried by a gripper\nAn action moving the robot from a room to another room is defined as\n1 (:action move\n: parameters (?r robot ? from ?to room); we specify the initial and target rooms\n: precondition (and (at-robby ?r?from)); the robot has to be in the initial room\n: effect (and (at-robby ? ? to) (not (at-robby ?r?from))))\nFurthermore, we have actions \"pick (a ball with a gripper)\" and \"drop (a ball).\"\n(:action pick\n: parameters (?r robot ?obj object ? room room ?g gripper)\n: precondition (and (at ? obj ? room) (at-robby ?r ? room) (free ?r?g))\n: effect (and (carry ?r?obj ?g)\n(not (at ?obj ?room))\n(not (free ?r?g))))\n(:action drop\n: parameters (?r robot ?obj object ?room room ?g gripper)\n: precondition (and (carry ?r ? obj ?g) (at-robby ?r?room))\n: effect (and (at ?obj ?room)\n(free ?r?g)\n(not (carry ?r ?obj ?g))))\nAn example of the initial and goal states are\n1 (:init (at-robby robot1 room1) (free robot1 rgripper1) (free robot1 1gripper1)\n(at ball1 room1) (at ball2 room3) (at ball3 room1) (at ball4 room2))\n(:goal (and (at ball1 room1) (at ball2 room3) (at ball3 room1) (at ball4 room2)))"}, {"title": "B ADDITIONAL DETAILS ON LLM PLANNER", "content": "Figure 6, 7, and 8 show the complete prompt for querying the language model to solve a planning problem. The blue text represents the prompts to the language model, while the red text corresponds to the responses generated by the language model."}]}