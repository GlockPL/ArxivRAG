{"title": "CoDiff: Conditional Diffusion Model for Collaborative 3D Object Detection", "authors": ["Zhe Huang", "Shuo Wang", "Yongcai Wang", "Deying Li", "Lei Wang"], "abstract": "Collaborative 3D object detection holds significant importance in the field of autonomous driving, as it greatly enhances the perception capabilities of each individual agent by facilitating information exchange among multiple agents. However, in practice, due to pose estimation errors and time delays, the fusion of information across agents often results in feature representations with spatial and temporal noise, leading to detection errors. Diffusion models naturally have the ability to denoise noisy samples to the ideal data, which motivates us to explore the use of diffusion models to address the noise problem between multi-agent systems. In this work, we propose CoDiff, a novel robust collaborative perception framework that leverages the potential of diffusion models to generate more comprehensive and clearer feature representations. To the best of our knowledge, this is the first work to apply diffusion models to multi-agent collaborative perception. Specifically, we project high-dimensional feature map into the latent space of a powerful pre-trained autoencoder. Within this space, individual agent information serves as a condition to guide the diffusion model's sampling. This process denoises coarse feature maps and progressively refines the fused features. Experimental study on both simulated and real-world datasets demonstrates that the proposed framework CoDiff consistently outperforms existing relevant methods in terms of the collaborative object detection performance, and exhibits highly desired robustness when the pose and delay information of agents is with high-level noise. The code will be released.", "sections": [{"title": "I. INTRODUCTION", "content": "3D object detection [1]-[6] is a fundamental task in au-tonomous driving, primarily focusing on the localization andidentification of specific vehicles in real-world scenarios [7].It is also attracting considerable interest in domains such asas drones, robots, and the metaverse [8]-[12]. However, 3Dobject detection with a single agent has inherent limitations,such as occlusion and distant objects [6], [13], [14]. Recently,researchers have addressed these limitations by enabling mul-tiple agents to share complementary perception informationthrough communication, leading to more comprehensive andaccurate detection. Cooperative 3D object detection [15]-[18] is an important field that has made significant progressin terms of high-quality datasets and methods. However, italso faces many challenges, including time delays [19], posemisalignment [17], communication bandwidth limitations[16], and adversarial attacks [20].\nTo effectively share information, multiple agents need totransmit real-time, precisely aligned features to synchronize their data within a consistent timestamp and spatial co-ordinate system, which is the foundation for maintaining effective collaboration. However, in real-world scenarios, this information may suffer from time delays and spatial misalignment due to the following reasons: 1) Unstable communication between agents, such as congestion and inter-ruptions, leading to time asynchrony issues; 2) The 6-degree-of-freedom (DoF) poses estimated by each agent's localiza-tion module are imperfect, resulting in unavoidable relative pose errors. These factors severely affect the reliability and quality of information exchange between agents, leading to inaccurate features during fusion. Some previous studies have considered various methods to address these issues. CoAlign [21] introduces a novel agent-object pose graph modeling approach to enhance pose consistency between collaborative agents. RoCo [17] designs an object matching and optimization strategy to correct pose errors. V2XViT [22] incorporates latency as an input for feature compen-sation, while CoBEVFlow [19] introduces the concept of feature flow to handle irregular time delays. However, in practice, time delays and pose errors may usually coexist and entangle with each other, and existing methods are unable to address both issues simultaneously. As a result, they still produce feature representations with harmful noise, hindering current collaborative perception systems from reaching their full potential.\nTo address these limitations, we propose a novel hybrid collaborative framework called CoDiff, which leverages Con-ditional Diffusion Probabilistic Models (CDPM) to address spatial and temporal noise issues in information sharing, thereby improving the quality of multi-agent fused features. In CoDiff, we treat the time delays and pose errors between multiple agents as a unified noise to be learned and handled. We employed diffusion models for feature fusion, replacing the existing feature fusion methods based on regression mod-els and attention mechanisms. CoDiff is primarily consists of two components: perception compression and conditional"}, {"title": "II. RELATED WORK", "content": "Diffusion Models for Perception Tasks. Diffusion mod-els [10]-[12], [25], known for their powerful denoisingcapabilities, have attracted considerable interest in domainssuch as natural language processing, text transformation, andmultimodal data production. Recently, an increasing numberof researchers have started investigating the use of diffusionmodels in perception tasks. DDPM-Segmentation [26] isthe initial study that uses diffusion models for semanticsegmentation. On the other hand, DiffusionDet [27] considersobject detection as a noise-to-box task, aiming to provideaccurate object bounding boxes by progressively denoisingrandomly generated proposals. Diffusion-SS3D [28] utiliesthe powerful properties of diffusion models in a semisupervised 3D object detection framework, with the goal ofgenerating more dependable pseudo-labels. Expanding uponthis, Diff3DETR [29] extends the approach of Diffusion-SS3D by proposing the first diffusion-based DETR frame-work. In addition, DifFUSER [30] utilises the noise reductioncapabilities of diffusion models to address noise for multimodal fusion (such as image, text, video). In this work, weware motivated to further explore the potential of employingthe diffusion model to generate a high-quality representation,proposing the first diffusion-based multi-agent cooperativeperception 3D detection method.\nCollaborative 3D Object Detection. Collaborative 3Dobject detection [21]-[23], [31], [32] is a particular appli-cation in multi-agent systems that allows multiple agentsto share information to overcome the inherent limitationsof single-agent perception. Many researchers have designedvarious methods to enhance the perception performance androbustness of collaborative perception systems. In terms ofperception performance, the study in the literature [16],[22], [23], [32] implemented a transformer architecture toaggregate information from different agents. In terms ofrobustness, CoBEVFlow [19] creates a synchrony-robustcollaborative system that aligns asynchronous collaborationmessages sent by various agents using motion compensation.HEAL [15] smoothly integrates emerging heterogeneousagent types into collaborative perception tasks. CoAlign [21]uses an agent-object pose graph to address pose inaccuracies,and RoCo [17] tackles pose errors with object matching andgraph optimization techniques. However, we observe thatthese methods tend to address only a single aspect of theproblem and fail to ensure both perceptual performance androbustness of the model under multiple noise conditions. Incontrast, our CoDiff can simultaneously address the effectsof various types of noise, such as time delays and pose errors,thereby further improving the accuracy of collaborative 3Dobject detection."}, {"title": "III. PROBLEM DEFINITION AND PRELIMINARIES", "content": "A. Collaborative 3D object detection.\nAssuming there are N agents in the scene, given the pointclouds of these agents as input, the goal of collaborative 3Dobject detection is to classify and locate the 3D boundingboxes of objects within the scene through the cooperation ofthe agents. This process can be formally expressed as:\n$\\begin{aligned}F_{i} & =\\operatorname{Enc}\\left(X_{i}\\right), i=1, \\ldots, N, & \\text { (1a) }\\\\F_{i}^{\\prime} & =\\operatorname{Agg}\\left(F_{i},\\left\\{M_{j \\rightarrow i}\\right\\}_{j=1,2, \\ldots, N ; j \\neq i}\\right), & \\text { (1b) }\\\\O_{i} & =\\operatorname{Dec}\\left(F_{i}^{\\prime}\\right) . & \\text { (1c) }\\end{aligned}$\nWithin the collaborative perception framework, each agent $i$can extract features $F_{i}$ from the raw point cloud observations$X_{i}$ using an encoding network as in Step (1a). $M_{j i}$ in Step(1b) denotes the collaboration message sent from agent $j$ toagent $i$. After receiving all the $(N-1)$ messages, agent$i$ will aggregate its feature $F_{i}$ with these messages via afusion network, producing a fused feature $F_{i}^{\\prime}$ as in Step (1b).Finally, Step (1c) uses a decoding network to convert $F_{i}^{\\prime}$ tothe final perception output $O_{i}$. As seen above, provided thatall networks are well trained, the fused features $F_{i}^{\\prime}$ in Step"}, {"title": "IV. OUR PROPOSED METHOD", "content": "CoDiff is designed as a generative model specificallymulti-agent 3D object detection which is capable of simul-taneously addressing pose errors and time delays. Figure2 shows the overall architecture of CoDiff. It consists oftwo key ideas. Firstly, we propose a perceptual compressionmodel to compress high-dimensional feature maps into alow-dimensional, latent space. This compression focuses onthe essential semantic information of the data. Secondly, wedesign a conditional diffusion model that treats feature mapsfrom different agents as conditions, combining the model'sgenerative capabilities with these conditions to produce moreaccurate and fine-grained multi-agent fused features. Thesefeatures are ultimately fed into the detection head to achieveprecise 3D object detection results.\nA. Perceptual Compression Model\nMost feature fusion methods employ autoregressive,attention-based transformer models, which are trained inhigh-dimensional feature spaces [16], [22]. However, weobserve that diffusion models incur costly function evalu-ations during high-resolution feature sampling, presentingsignificant challenges in terms of computation time andenergy resources. Therefore, we aim to perform sampling ina lower-dimensional space, where diffusion models can learnwithin this perceptually equivalent, lower-dimensional space,thereby reducing computational complexity. This approachhas demonstrated its feasibility in the domain of high-resolution image synthesis [35]. Based on the above analysis,we propose a perceptual compression model that abstractshigh-dimensional feature details into a lower-dimensional,latent space. This space is more suitable for likelihoodbased generative models because it makes computationsmore efficient and enabling a focus on the important semanticaspects of the data.\nOur perceptual compression model is based on a neuralnetwork autoencoder architecture. This autoencoder com-"}, {"title": "B. Diffusion Model", "content": "prises an encoder and a decoder, focusing on compress-ing and reconstructing input tensors along the channel di-mension. The encoder employs a series of convolutionallayers, batch normalization, and ReLU activation functionsto compress high-dimensional input tensors into a lower-dimensional latent space. The decoder, using a similar struc-ture, aims to reconstruct a low-dimensional latent represen-tations back to the original dimensionality. This autoencodereffectively reduces computational overhead while preservingkey features of the data.\nPrecisely, given a feature map $F_{i} \\in R^{H \\times W \\times C}$, where$H, W, C$ represents height, width and channels, respectively.The encoder $\\mathcal{E}$ encodes $F_{i}$ into a latent representation$z=\\mathcal{E}\\left(F_{i}\\right)$ using a compression rate $\\mathcal{T}$, and the decoder$\\mathcal{D}$ reconstructs the feature from the latent space using thesame compression rate. Therefor, the process of compressingfeatures $F_{i}$ into the latent space is:\n$\\begin{equation}F_{i}=\\mathcal{D}(z)=\\mathcal{D}\\left(\\mathcal{E}\\left(F_{i}\\right)\\right)\\tag{4}\\end{equation}$\nThe perceptual compression loss $\\mathcal{L}_{c m p}$ defined as follows\n$\\begin{equation}\\mathcal{L}_{c m p}=D_{K L}\\left(F_{i} \\|\\| \\hat{F}_{i}\\right)\\tag{5}\\end{equation}$\nwhere $D_{K L}(p \\|\\| q)$ denotes the Kullback-Leibler (KL) [36]divergence of distribution $p$ from distribution $q$.\nWith the perceptual compression model trained in theprevious step, the diffusion model can now be trained inthis lower-dimensional, computationally more efficient latentspace. Our goal is to generate noise-free features $\\hat{F}$ (Step(1b)) through this latent space and the decoder $\\mathcal{D}$, whichshould represent the aggregation of features from multipleagents. We can decode the latent representation of features$\\hat{F}$ from the latent space back to the original feature spacewith a single pass through $\\mathcal{D}$ during training.\nInspired by literature [35], diffusion models can leverageconditional mechanisms $p(x \\mid y)$ to control the target gen-eration process, thereby improving the model's accuracy.We find that in multi-agent collaborative perception, theinformation from individual agents can be naturally used asconditional inputs to generate the final fused features. Thus,the generation of multi-agent features can be controlled bymanipulating the input conditions $y$.\nThe Design of Condition. In practice, we select thefeature information from $N-1$ individual agents as the condition $Y_{c o n d}$. These informations are obtained by generatingfeatures $F_{j}$ through point cloud encoders, which are thentransmitted to the Ego agent $i$, denoted as $M_{j \\rightarrow i}$. We strictlyfollow the standard DPM model to add noise, while thedifference is that we employ condition-modulated denoising,which is shown in Figure 2. By progressively denoising thesamples, we hope that the conditional diffusion model canact as a more flexible condition generator, aiding in learningfine-grained object information, such as precise boundariesand highly detailed shapes, especially under noisy conditions.Given noisy feature $z_{t}$ and condition $Y_{c o n d}=\\left\\{M_{j \\rightarrow i}\\right\\}$at time step $t, z_{t}$ is further encoded and interacts with"}, {"title": "C. Detection Head", "content": "$\\\\\\B. Latent Diffusion Model\n$Y_{c o n d}$ through concatenation. This form $p(z_{t} \\mid Y_{c o n d})$ canbe implemented with a conditional denoising autoencoder$\\epsilon_{\\theta}\\left(z_{t}, t, Y_{c o n d}\\right)$. A Unet-style structure [37], whose com-ponents include an encoder and a decoder, severs as thedenoising network. Based on feature-conditioning pairs, wethen learn the conditional LDM via\n$\\begin{equation}\\mathcal{L}_{L D M}:=\\mathbb{E}_{x(x), y, \\epsilon \\sim \\mathcal{N}(0,1), t}\\left[\\left\\|\\epsilon-\\epsilon_{\\theta}\\left(z_{t}, t, Y_{c o n d}\\right)\\right\\|_{2}^{2}\\right] \\tag{6}\\end{equation}$\nThe definition of a symbols can be refer to the equation (3).\nAfter generating the feature map $\\hat{F}_{i}$, we decode them intothe detection layer $D e c(\\cdot)$ to obtain final object detections$O_{i}$. The classification output is the confidence score of beingan vehicle or background for each anchor box, the regres-sion output is $(x, y, z, w, l, h, \\theta)$, representing the centroidcoordinates, size and yaw of the anchor boxes, respectively."}, {"title": "D. Training details and loss function", "content": "Following common practice [35], [38], [39], we separatetraining into two distinct stages. First, we train a perceptioncompression model to provide a low-dimensional represen-tation space. Then, we train the latent diffusion model inthis space. In the detection task, we use the cross entropyand weighted smooth L1 loss for the classification $\\mathcal{L}_{c l s}$ andregression $\\mathcal{L}_{r e g}$. The total loss is the weighted sum of thediffusion loss in Equation (6) and the detection loss.\n$\\begin{equation}\\mathcal{L}_{t o t a l}=\\mathcal{L}_{L D M}+\\lambda_{c l s} \\mathcal{L}_{c l s}+\\lambda_{r e g} \\mathcal{L}_{r e g}\\tag{7}\\end{equation}"}, {"title": "V. EXPERIMENT RESULT", "content": "We validate our CoDiff on both simulated and real-worldscenarios. The task of the experiments on the three datasets ispoint-cloud-based 3D cooperative object detection. Follow-ing the literature, the detection performance are evaluatedby using Average Precision (AP) at Intersection-over-Union(IoU) thresholds of 0.50 and 0.70.\nA. Datasets\nDAIR-V2X [24]. DAIR-V2X is a large-scale vehicle-infrastructure cooperative perception dataset containing over100 scenes and 18,000 data pairs, featuring two agents:vehicle and road-side unit (RSU), capturing simultaneousdata from infrastructure and vehicle sensors at an equippedintersection as an autonomous vehicle passes through.V2XSet [22]. V2XSet is built with the co-simulationof OpenCDA [42] and CARLA [43]. It is a largescale simulated dataset designed for Vehicle-to-Infrastructure(V2X) communication. The dataset consists of a totalof 11,447 frames, and the train/validation/test splits are6,694/1,920/2,833, respectively.\nOPV2V [23]. OPV2V is a large-scale dataset specificallydesigned for vehicle-to-vehicle (V2V) communication. It isjointly developed using the CARLA and OpenCDA simula-tion tools [43]. It includes 12K frames of 3D point cloudsand RGB images with 230K annotated 3D boxes."}, {"title": "E. Ablation Studies", "content": "To determine the optimal compression rate $\\tau$ during theperceptual compression, we conduct ablation experimentson V2XSet dataset, as shown in Table II. We found thatsetting $\\tau$ to 32 and compressing the feature dimensions to8 achieved the optimal mAP. To validate the effectivenessof the conditional diffusion module, Table III shows theresults of using or not using conditions, as well as howthe conditions were applied in the model. Please note: (i)The diffusion conditions enhance the stability of the model;(ii) Using Concat for condition fusion is superior to directlyapplying element-wise addition. In testing, we used twosamplers: DDPM [44] and DDIM [45]. To achieve optimalperformance, we tested the effect of different samplers andsampling steps on the results. The experimental results areshown in Table IV. We selected DDPM with 8 sampling stepsas the default solver in the experiments due to its simplicityand efficiency. We also measure the inference time and theparameters of our proposed CoDiff on a single NVIDIA2080Ti GPU in Table V. At 1-step DDPM sampling, CoDiffcan reach the inference time of 127.7 (ms) with 769.17 (MB)parameters."}, {"title": "VI. CONCLUSION", "content": "This paper proposes a novel robust collaborative per-ception framework CoDiff for 3D object detection. Thisframework leverages diffusion models to address informationinaccuracies caused by pose errors and time delays betweenmultiple agents, enhancing the precision and reliability ofinformation exchange. Comprehensive experiments demon-strate that CoDiff achieves outstanding performance acrossall settings and exhibits exceptional robustness under extremenoise conditions. In future work, we aim to further explorethe potential of CoDiff and expand its application to abroader range of perception tasks."}]}