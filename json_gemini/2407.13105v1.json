{"title": "SURVEY ON PLAGIARISM DETECTION IN LARGE LANGUAGE MODELS: THE IMPACT OF CHATGPT AND GEMINI ON ACADEMIC INTEGRITY", "authors": ["Shushanta Pudasaini", "David Lillis", "Luis Miralles-Pechu\u00e1n", "Marisa Llorens Salvador"], "abstract": "The rise of Large Language Models (LLMs) such as ChatGPT and Gemini has posed new challenges for the academic community. With the help of these models, students can easily complete their assignments and exams, while educators struggle to detect AI-generated content. This has led to a surge in academic misconduct, as students present work generated by LLMs as their own, without putting in the effort required for learning. As AI tools become more advanced and produce increasingly human-like text, detecting such content becomes more challenging. This development has significantly impacted the academic world, where many educators are finding it difficult to adapt their assessment methods to this challenge.\nThis research first demonstrates how LLMs have increased academic dishonesty, and then reviews state-of-the-art solutions for academic plagiarism in detail. A survey of datasets, algorithms, tools, and evasion strategies for plagiarism detection has been conducted, focusing on how LLMs and AI-generated content (AIGC) detection have affected this area. The survey aims to identify the gaps in existing solutions. Lastly, potential long-term solutions are presented to address the issue of academic plagiarism using LLMs based on AI tools and educational approaches in an ever-changing world.", "sections": [{"title": "1 Introduction", "content": "1 out of 10 assignments was detected using AI tools from more than 200 million writing assignments reviewed by the AI detector engine of Turnitin over the past year [1]. Specifically, the introduction of ChatGPT, a revolutionary LLM-based conversational engine, has significantly changed various industries and domains [2], including academia. The capabilities of highly advanced LLMs have affected the academic world in various ways. For example, students are using ChatGPT to complete their homework assignments [3] and to pass challenging exams like the Graduate Record Examination (GRE) and Scholastic Assessment Test (SAT) [4]. This has raised concerns about the current evaluation systems used in academic institutions. Lecturers and universities are struggling to detect fraudulent activities by students [5], with plagiarism being one of the major issues. In the past, plagiarism was mostly done by presenting a document which included paragraphs from other sources without being referred to, but with the emergence of LLMs, students can now use LLMs to generate text and complete their assignments entirely. This act of using the text generated by LLMs and claiming their own work is referred to here as AI-generated plagiarism. This research explores the impact of LLMs on plagiarism and mainly focuses on detecting AI-generated plagiarism.\nIn a survey by Intelligent.com conducted in May 2023 among 3,017 high school and college students, it was found that nearly one-third of the students admitted to using ChatGPT to complete their homework [6]. The dependency of students on such tools leads to the loss of creativity and learning ability [7]. Due to such threats, several universities have decided to ban the usage of ChatGPT [8]. Apart from students, researchers have also been misusing this technology. Anyone with little knowledge and research experience can use ChatGPT to write academic content. A recent publication in Nature states that at least four preprints were submitted to their journal, including ChatGPT as a co-author [9]. The disruption created by this phenomenon in the field of scientific publishing has been such that journals have strictly banned listing ChatGPT as a co-author [10] and set out new authorship guidelines for AI-generated text [11]. Due to such problems, LLMs have been a major issue in academia.\nLLMs, or language models, are deep learning-based models designed for various Natural Language Processing (NLP) tasks. They offer remarkable capabilities such as brainstorming, generating counterarguments, creating summaries and abstracts, and correcting grammar. LLMs like ChatGPT can rephrase text and produce text almost indistinguishable from human writing[12]. These models can handle simple tasks such as generating an essay on a given topic as well as complex tasks like writing a research paper on a challenging problem [9]. Rehan Haque, Founding Director of Metatalent.ai, suggests that we have reached a point where entire programming projects can be done using AI algorithms such as LLMs and use another AI tool to modify to make AI undetectable [13]. Thus, the rise of such generative AI tools and their capacity to generate even more human-like text poses a significant threat to academic integrity [14].\nAfter the release of ChatGPT on November 30, 2022, research has been conducted on both sides: developing more intelligent LLMs and models that can detect such AI-generated content. The task of detecting whether a text is generated by AI-based algorithms or humans is termed Artificial Intelligence Generated Content (AIGC) detection [15]. From the 94 million training parameter ELMO model in 2019 to the recent 1.76 trillion parameter GPT-4 model, the evolution of the LLMs and their capabilities have been growing daily [16]. With the projected rapid development of highly capable LLMs, the quality of outputs is increasing, making it more difficult to detect [17].\nDifferent approaches, such as training classifiers, watermarking, and zero-shot approaches, have been developed to detect text generated from LLMs. Based on these approaches, algorithms such as DetectGPT [18], RADAR [19], Ghostbuster [20], GPT-Sentinel [21] amongst others are being developed to identify AI-generated content. OpenAI, the creator of ChatGPT, introduced its AIGC detection tool two months after its release. However, OpenAI states that the detector is not fully reliable [22]. Similarly, several AIGC detector tools and software such as CopyLeaks, Turnitin, GPTZero, and Crossplag have been released for the general use of the public to identify AI-generated content. On the other hand, different techniques to attack or evade such AIGC detectors have also been developed and are an active area of research [23]. Evasion techniques such as prompting [24], recursive paraphrasing [25], authorship obfuscation [26], and sentence or word substitution have been developed to point out the failures in the AIGC detector tools.\nThe main contribution of this research is the comprehensive survey of existing algorithms, tools, datasets, and evasion strategies developed for addressing academic misconduct, particularly in plagiarism detection and AIGC detection. Through a simple experiment, we evaluated the reliability of existing tools in detecting misconduct. Based on the experiment and survey, we discussed the reliability and feasibility of addressing the problem. In addition, various alternative educational solutions to address the issue have also been discussed.\nThis paper is organized as follows. Section 2 identifies different problems due to academic dishonesty with the emergence of LLMs and demonstrates how such LLMs have affected academia by presenting different ways such tools are improvising academic cheating. A survey of existing algorithms, datasets, and tools for detecting academic cheating methods such as plagiarism and generating text with AI is performed in this section 3. Different evasion techniques have also been discussed in Section 3. Based on the detection algorithms and evasion strategies survey, the limitations and gaps observed in current solutions have been discussed in Section 4. Finally, the feasibility of technical solutions for AIGC detection and other alternative educational solutions have been discussed in Section 5"}, {"title": "2 The plagiarism problem in Academia", "content": "This section discusses the impact of LLMs in academia, how LLMs have been utilized for academic misconduct, and the proliferation of LLMs, which has resulted in increasingly human-like text that poses a serious threat to academic integrity."}, {"title": "2.1 Rise of Large Language Models (LLMs)", "content": "In recent years, deep learning models have been capable of generating different types of data. For example, DALL-E 3 from OpenAI can generate images, while models like ChatGPT, Gemini, and Perplexity can generate text. AudioGen and MusicGen from MetaAI can generate audio, and GPT-4 from OpenAI can generate multimodal data, meaning it can handle different types of data. These models require a simple, optimized prompt so that users can get the desired data. This domain of AI, referred to as Generative AI, is growing exponentially. The market size of generative AI was reported to be 44 billion USD in 2023 and is projected to reach 66.62 billion USD in 2024 [27]. OpenAI, one of the leading companies in this field, reported that 100 million use ChatGPT every week and over 2 million developers are building generative AI applications from the API service provided by the company [28].\nThe content generated from Generative AI tools is AI Generated Content (AIGC). With the increasing use of Generative AI tools, a significant amount of AIGC has been detected on the internet, with some estimates suggesting that 10% of internet content is already AI-generated [29]. AIGC presents various threats and challenges, including ethical concerns, harmful or inappropriate content, bias, over-reliance, misuse, digital divide, academic misconduct, security, and privacy [30]. This research mainly focuses on AI-generated textual content.\nLanguage Modelling(LM) began with statistical learning methods such as building word prediction models called N-gram language models based on Markov assumption in the 1990s [31]. However, these N-gram language models suffered from issues like increased computational complexity and overfitting because of high dimensional data, also called the curse of dimensionality. The focus then shifted towards Neural Language Models (NLMs), which utilized deep learning architectures like multi-layer perceptron (MLPs) [32] and recurrent neural networks (RNNs) [33] to characterize the probability of word sequences. Subsequently, Pre-trained Language Models (PLMs) such as ELMO [34] and BERT [35] were introduced, which could capture the context-aware representation of any text. LLMs have been developed based on pre-existing PLMs, which have completely transformed the landscape of text generation. These LLMs are now readily available to the public through products from tech giants like ChatGPT and Gemini. They enable remarkable text generation, allowing individuals to generate ideas, create academic content, summarize large amounts of text, generate code, paraphrase any text, and more. However, this widespread accessibility has raised serious ethical concerns regarding its use [36].\nThe growth of LLMs has been reshaping the AI landscape due to their rapid growth in size and capabilities at a staggering pace [37]. This is evident in the timeline of various GPT models released by OpenAI in recent years"}, {"title": "2.2 Academic Misconduct in the LLMs Era", "content": "According to a survey in the USA, it was found that 82% of undergraduate students admitted to some form of misconduct while submitting their assignments [39]. Academic misconduct refers to actions that violate the originality of academic work, such as ghostwriting, plagiarism, data fabrication, deceit, and generation using Artificial Intelligence (AI). Among these, generation using AI is the most frequent and recent form of misconduct [40]. The research primarily focuses on this aspect. Recent generative AI tools such as ChatGPT, GPT-4 Vision, SORA from OpenAI, Gemini from Google, and Perplexity from Perplexity AI are capable of generating various types of content, including text, images, video, and code in multiple programming languages. The introduction of generative AI, particularly ChatGPT from OpenAI, has posed challenges in academia [40].\nStudents can utilize models like ChatGPT for various tasks, such as generating an essay for a given topic, brainstorming ideas, summarising textual contents, solving programming assignments, solving complex mathematical equations, etc. Furthermore, ChatGPT now offers a feature where users can upload a PDF file and the AI will read the document and generate relevant text based on its contents [41]. This has unfortunately made plagiarism even easier [42]. In addition to completing assignments, ChatGPT can assist with solving problems in various examinations, potentially raising concerns about the credibility of the examination process. Using different strategies, such as prompting, ChatGPT provides a range of suggestions for cheating in university exams [43]. It has demonstrated the ability to pass rigorous examinations, including the US Medical Licensing Exam [44]. ChatGPT has also shown promising results in other exams such as the bar examination, Scholastic Assessment Test (SAT), Graduate Record Examination (GRE), 2020 USA Biology Olympiad Semifinal Exam, college-level microbiology quiz, and Stanford Medical School clinical reasoning final [4]. Moreover, tools like ChatGPT can handle complex programming assignments as well [45].\nStudents and researchers have been using ChatGPT to write research papers and get published [46]. Recent Google search results have proved the use of ChatGPT in scholarly publishing [47], with publishers making policies to moderate the usage of ChatGPT in academic publishing [10]. Therefore, some major recent academic threats raised are using generative AI tools such as ChatGPT to submit school/college assignments, pass licensure examinations, and publish research papers."}, {"title": "3 Existing Solutions to AI-Generated Plagiarism", "content": "Academic cheating was a serious issue even before the rise of LLMs because of easy access to information and other works on the internet [48]. Different solutions for Author Identification [49], and Plagiarism Detection [50] have been developed. The solutions developed are being used by many academic institutions and publishers [51], but they are not completely robust [52]. However, with the introduction of LLMs, the problem of academic misconduct is even more serious because of the different use cases of LLMs to perform academic misconduct, such as completing student assignments, passing online examinations, paraphrasing texts to escape plagiarism and generating academic content for research papers. Due to the different problems raised by LLMs, research studies have been performed on AIGC detection. This section discusses plagiarism and plagiarism detection, along with how LLMs have changed plagiarism detection in recent years. Similarly, the section discusses the datasets, algorithms and tools for AIGC detection and the evasion techniques developed to fool such AIGC detectors."}, {"title": "3.1 Plagiarism Detection", "content": "The American Historical Association (AHA) formally defined plagiarism in 1987 as failure to acknowledge the work of another [53]. Plagiarism, a part of academic cheating, is much more widespread than usually recognised according to a few studies [54].\nThe typology of plagiarism may vary according to data type or level of obfuscation. Foltnek et al. [55] presented different typologies defined in several research papers and put forward a new typology for plagiarism according to the level of obfuscation as character-preserving plagiarism, syntax-preserving plagiarism, semantics-preserving plagiarism, idea-preserving plagiarism, and ghostwriting. Plagiarism may also be monolingual or cross-lingual; monolingual plagiarism is done in one language, and cross-lingual plagiarism is done in multiple languages [56]. The plagiarism type may also vary according to the data types, such as code plagiarism and text plagiarism.\nPlagiarism detection also may be categorized based on different factors. Based on the number of languages used, plagiarism detection may be monolingual or cross-lingual. Likewise, plagiarism detection may be extrinsic or intrinsic. Suppose plagiarism is detected only using the text itself. In that case, it is termed intrinsic plagiarism detection, whereas if plagiarism is detected in comparison with other text, it is termed extrinsic plagiarism detection [57]. Plagiarism detection can be categorised according to their approach. N-gram-based, vector-based, syntax-based, semantic-based, fuzzy-based, structural-based, and stylometric-based [58].\nAs the internet grows larger day by day, with more textual information and new tools for plagiarism, LLMs are one of them, and the concern of plagiarism detection is even more serious. A large survey conducted for 12 years at 24 universities in the US presenting that almost 95% of the students admit to plagiarism at least once [59] demonstrates the need for plagiarism detection."}, {"title": "3.1.1 Plagiarism Detection After LLMs", "content": "Plagiarism and its Detection have changed their course since the rise of LLMs. Although many paraphrasing tools exist, such as Wordtune and Quillbot, students can also use LLMs to paraphrase the given text and attempt to escape plagiarism. On the other hand, LLMs can also be used as a plagiarism detection tool. There are several commercial tools to check plagiarism scores. However, students can simply use ChatGPT for free to check whether a text is plagiarised. This helps the students estimate the likelihood of getting caught and manipulate the copied text to fool plagiarism detection tools. Birock et al. [80] performed four prompt engineering experiments to determine the efficiency of ChatGPT as a plagiarism testing tool and found the fourth prompt to work very well. In another experiment by Khali et al. [73] on 50 essays generated by ChatGPT and tested, ChatGPT showed superior performance in plagiarism detection than other traditional plagiarism detection tools like iThenticate.\nStudents can also use both ChatGPT and paraphraser to fool the existing plagiarism detection and AIGC detection tools. Instead of copying the content written by others and paraphrasing the content using several tools to fool plagiarism detectors, students can simply generate textual content from ChatGPT and apply paraphrasing to make the content non-AIGC detectable and plagiarism-free. To demonstrate this, we conducted a simple experiment in which a student was assigned to write an article on quantum computing, including its use cases. We generated the article using ChatGPT, paraphrased the generated text using QuillBot, and tested its originality. Similarly, we copied an article from an online blog from Investopedia and copied that content to ChatGPT to paraphrase and again tested the originality of the text. The testing used the plagiarism detection tool Turnitin and the AIGC detection tools GPTZero and DupliChecker. The output text was identified as being 100% unique, demonstrating that students can easily complete their assignment using such a combination of tools, and lecturers will not be able to find out."}, {"title": "3.2 AI Generated Content (AIGC) Detection", "content": "The major objective of AIGC Detection is detecting whether a given piece of text is generated using an AI system or written by a human. It is very difficult even for a human to differentiate because of the high capability of LLMs nowadays to produce more and more human-like text. In an experiment, English instructors hired to differentiate AI-generated and human-written essays were only able to get 67% accuracy [81]. It is more difficult to distinguish text generated by language models from human-written text compared to distinguishing text generated by different language models [82]. In an experiment, AI models were able to generate more high-quality argumentative essays than German high school students in an online forum [83]. Similarly, in another experiment, six undergraduate and PhD students were asked to annotate 50 documents as AI-generated or human-written. They could only achieve 59% accuracy [84]. Thus, AIGC detection is a very complex task.\nHuman essays have more spelling and grammar errors and personal experiences. In contrast, machine essays have more similar examples and repetitive expressions, according to a quantitative analysis performed between human essays and ChatGPT-written essays [81]. Machine-generated text has a more complex syntactic structure and uses more normalization, whereas human essays tend to be more lexically complex [81, 83]. Similarly, human-written text tends to have higher perplexity compared to AI-generated text [85]. To demonstrate such linguistic differences in human-written and AI-generated text, a human was told to write a paragraph on \"cow\", and the prompt \"Write a paragraph on Cow\" was injected in ChatGPT. The grammatical mistakes and use of personal experience can be seen in the human-written text. In contrast, syntactically complex sentence structure and repetition of words can be seen in the text generated from ChatGPT"}, {"title": "3.2.1 Open Source Datasets for AIGC Detection", "content": "As the term \"Garbage In, Garbage Out\" in machine learning represents, high-quality data is vital for efficient machine learning research [86]. AIGC detection, when modelled as a straightforward binary classification problem, requires a high-quality, balanced labelled dataset comprising a wide variety of human-written and AI-generated texts. However, when AIGC detection is solved from other approaches, such as zero-shot detection methods, a dataset may not be required for training [87]. Building a high-quality dataset is a very laborious process that involves data scraping from various sources, data cleaning, data preprocessing, and data annotation. The human-written part of most open-source AIGC detection datasets was built using three major techniques. First is directly taking from other open-source datasets such as the wikiHow text dataset [88]. This technique was used to build the GPT-Sentinel dataset [89]. Second is using an available corpus such as in-class or homework exercises, Test of English as a Foreign Language (TOEFL) writing tasks, and GRE writing tasks were used to build ArguGPT dataset [81]. Third is manually collecting observations from online sources such as the CHEAT dataset built by searching and extracting human-written abstracts from IEEE Xplore [90]. On the other hand, the AI-generated text part in those open-source datasets is built using different prompting strategies applied to OpenAI models using the OpenAI API service [89, 90]. Some papers claim very high results in AIGC detection using specific custom datasets, but the dataset is not open-sourced [89]. The datasets used in different research papers to solve AIGC detection are presented"}, {"title": "3.2.2 Watermarking Based Approaches", "content": "Watermarking-based approaches embed subtle signatures like a cryptographic pseudo-random function in LLM-generated text, which can be decrypted further to check whether the text is generated by the particular LLM. The input and output in text generation models like LLMs are always tokens. LLMs constantly generate a probability distribution over the next predicted token, conditional on the previous string of tokens. Based on such probability distribution, the next token is sampled randomly according to a parameter called temperature. Now, instead of randomly selecting the next token in the output, a cryptographic pseudorandom function can be introduced, which is unnoticeable to the end users but can be further decrypted to see whether that particular LLM generates a given text or not.\nWatermarking in natural language was done for other purposes like information hiding even before the release of ChatGPT [96]. Techniques like morphosyntactic alterations and synonym substitution were used for watermarking natural language [97, 98]. In a workshop on LLMs and Transformers, Scott Aaronson revealed that he proposed a watermarking scheme based on the \"Gumbel Softmax Rule\", whose prototype was later implemented by another scientist from OpenAI, Hendrick Kirchner which seems to work even with a few hundred tokens [99]. The problem with these traditional watermarking techniques was that they could not preserve the semantic meaning between previous and watermarked texts, i.e., the original and watermarked texts had very different meanings. Yang et al. [100] introduced context-aware lexical substitution for watermarking and achieved a 2.19 Z-score which represents the watermark strength. Higher z-scores indicate that z-scores of these features in the watermarked text deviate from the mean values found in human-written text in a controlled manner. Sahar Abdelnabi and Mario Fritz [101] introduced the first end-to-end Adversarial Watermarking Transformer (AWT) model just a month after the GPT-2 release. AWT model was able to achieve a z-score of 2.73. Yang et al. [102] again developed another framework for watermarking black box language models with a z-score of 3.63.\nKirchenbauer et al. [103] proposed a watermarking technique in June 2023. This algorithm reduced the False Positive Rate (FPR) to 0 and could be used without prior knowledge of any LLM parameters or its Application Programmable Interface (API). However, in October 2023, Zhao et al. [104] proved that the Kirchenbauer et al. [103] watermarking technique failed when a paraphrasing attack was performed using ChatGPT, DIPPER-1, DIPPER-2 and BART. Along with these results, Zhao et al. [104] also proposed another watermarking solution called Unigram-Watermark, which outperformed the previous solution proposed by Kirchenbauer et al. [103] for paraphrasing attacks. Still, the watermarking techniques faced the challenge of maintaining the detectability of inserted watermarks and semantic integrity of generated text [105]. To solve this, Huo et al. [105] proposed a multiobjective optimization (MOO) approach for watermarking, which outperformed previous watermarking techniques and was robust against copy-paste attacks and paraphrasing attacks. Again, in March 2024, a new watermarking framework, WaterMax, was introduced by Giboulot et al. [106], which results in high detectability, outperforming previous watermarking techniques and is also designed in such a way that it leaves the LLM untouched, maintaining the quality of the generated text."}, {"title": "3.2.3 Zero-shot Based Approaches", "content": "Zero-short approaches are those in which pretrained neural network models can predict unseen classes [107]. While typical text classification models predict whether an observation falls into a specific class or not after training the model on a huge set of labelled datasets, the zero-shot learning approach utilizes the ability of pretrained language models to generalize unseen text observations or even new datasets [108].\nGiant Language Model Test Room (GLTR) is a pioneering work in AIGC detection based on zero-shot learning. Gehrmann et al. [109] developed GLTR that can detect whether a text is generated by a model or not with a proper visual footprint of generated text through the tool that also proves the prediction. Mireshghallah et al. [110] experimented to test whether other language models can be used to detect machine-generated text from one language model and found out that smaller language models like OPT-125M are better AIGC detectors than large language models like GPTJ-6B. DNA-GPT was introduced in the AIGC detection space, which claimed to surpass the result of OpenAI text detector [22] on four English and German datasets [111]. The main idea of DNA-GPT is to compare the probability divergence between the actual tokens and generated tokens [111].\nDetect-GPT was another milestone in AIGC detection because it raised the state-of-the-art zero-shot detection of AIGC detection from the previous highest 0.81 AUROC to 0.95 AUROC [112]. Detect-GPT was built on the principle that machine-generated text mostly occupies negative curvature regions of log probability generated from the model [112]. Recently, Fast-DetectGPT increased the AUROC to 0.98 and obtained a speed 340 times faster compared to DetectGPT [113]. Fast-DetectGPT introduced the notion of conditional probability curvature to clarify disparities in vocabulary selections observed between machine-generated text and human-written text [113]."}, {"title": "3.2.4 Training Classifier Based Approaches", "content": "AIGC detection can be framed as a binary text classification problem where we have the given text as input and AI-generated and human-written as two classes. After the release of ChatGPT in November 2022, OpenAI itself came up with an OpenAI Text Classifier in Jan 2022 [116]. Liu et al. [81] introduced ArguGPT a classification model trained on sentence-level and essay-level data from TOEFL and GRE tasks achieving 90% accuracy. Oghaz et al. [117] implemented different machine-learning algorithms like Multinomial Naive Bayes, Random Forest, Support Vector Machines (SVM), and K-nearest neighbours (KNN) as well as deep learning algorithms such as Bidirectional Long Short Term Memory (LSTM) networks, DistilBERT, and RoBERTa on a custom dataset of question answers mainly on computer science, artificial intelligence, and cyber security. In his experiment, the Roberta-based custom deep learning model achieved the highest performance with an F-score of 0.992 and an accuracy of 0.991 [117].\nMost text classification algorithms have applied common text preprocessing techniques such as tokenization, stemming, lowercasing, and stopword removal. Similarly, TFIDF has been used for feature extraction from the text [82]. Hijaku et al. [82]implemented the XGBoost algorithm with TFIDF and handcrafted features, which were further visualized using SHAP analysis. Katib et al. [118] introduced a new approach to train a binary classification model for AIGC detection, the Tunicate Swarm Algorithm with Long Short-Term Memory Recurrent Neural Network (TSA-LSTMRNN). The new approach achieved 93.17% F-score and 93.83% accuracy on human- and ChatGPT-generated datasets, respectively [118]. This experimentation was done on the CHEAT dataset. Wissam Antoun et al. [119] implemented different transformer-based algorithms such as Roberta, ELECTRA, CamemBERTa, and XLM-R on a modified Human ChatGPT Comparision Corpus (HC3) dataset containing English and French text on which a 99.86 F-Score was achieved. GPT-Sentinel was another pioneer work based on binary classification using transformer-based networks like the Robustly Optimized BERT Pretraining Approach (Roberta) and Text-to-Text Transfer Transformer (T5) achieving over 97% accuracy [89]. The current state-of-the-art classification algorithm is the Ghostbuster algorithm, which claims to achieve 99.0 F1, which is 5.9 F1 higher than the best pre-existing model [84]."}, {"title": "3.2.5 Detection Tools", "content": "With the development of various algorithms to solve the AIGC detection problem, several tools have been made public utilizing those state-of-the-art models and algorithms. OpenAI itself announced the release of the OpenAI text detector, which was shut down later because of its low accuracy rate [22]. Dreamsoft Innovations also introduced GPTKit in the market, which claimed to have 93% accuracy [122]. Later, DetectGPT was also released along with the Application Programmable Interface (API) for developers. Originality.ai claims to be the most accurate AI detector for different LLMs like ChatGPT, GPT-4, Bard, Claude 2 and others [123]. Arslan Akram [124] tested six different AIGC detection tools with 11,580 testing samples from different datasets and concluded that the originality.ai tool was particularly effective across the test dataset. Some of the most commonly used tools to detect text generated from LLMs, along with their accuracy obtained from the experiment conducted by Akram [124], are presented.\nWhile most of the research papers for AIGC detection focus on simply giving output as AI-generated or not, some available online AIGC detectors also give results in the probability of a particular text being human-written or generated from AI. We tested the results from the AI detectors from Copyleaks, ZeroGPT, Quillbot, Writer, GPTZero, Detecting-AI, Sapling, Undetectable, and Crossplag. Excluding the AI detector of Copyleaks, all the other AI detectors gave the output in probability. The AI Detector of Quillbot also classified the given text in terms of AI-generated, AI-generated + paraphrased, human-written and human-written + paraphrased. Undetectable also had the feature to humanize any AI-generated text. The Detecting-AI outputs the prediction from different prediction models."}, {"title": "3.2.6 Techniques to evade AIGC Detection Tools", "content": "With the development of different AIGC detection algorithms, people have also come up with different evasion techniques and algorithms to fool such AIGC detectors. These evasion techniques vary with the solution applied for AIGC detection. For instance, for AIGC detection systems against watermarking, techniques such as watermark stealing [125], Self Color Testing-based Substitution (SCTBS) [126], and language translation [127] have been developed. Similarly, paraphrasing, word substitution, and sentence substitution are used to evade binary classification training-based AIGC detection models [128] [129]. Other most common evasion techniques used to fool AIGC detectors are prompting [130], adversarial attacks [128], single space adding technique [23] and reinforcement learning [131]. These evasion techniques are tested on different AIGC detection models and have successfully fooled those models, resulting in a high drop in AUC and detection accuracy."}, {"title": "4 Limitations and Gaps in Current Solutions", "content": "Although several solutions for AIGC detection have been developed, they can not be relied upon. Several cases of false positives have been reported [133], which questions the feasibility of developing a completely reliable solution. From this extensive survey, several gaps were identified in the existing solutions, which, if further research, can lead to a reliable solution for the problem."}, {"title": "4.1 Current Scenario and Reliability of AIGC Detection", "content": "As reported by Turnitin in 2023, over 16,000 academic institutions, publishers, and corporations use the Turnitin software to prevent plagiarism [51]. However, 97% of the institutions have not implemented any formal policy on using AI tools by students, and 71% of the instructors have never used AI writing tools [134]. On the other hand, 51% of the students responded to continue using generative AI tools even if prohibited by the institution [134]. A survey among 1147 instructors reported that most of the instructors permitted the use of generative AI writing tools for brainstorming ideas for an assignment, helping edit writing and outlining a structure for an assignment. In contrast, the instructors were against using generative AI writing tools for writing a small and large part of an assignment or even an entire assignment [134]. The data indicates that every organisation should develop and regulate a formal policy on using generative Al writing tools. Along with such a policy, an efficient and reliable tool that detects AI-generated content is essential. Turnitin launched the AI writing detection tool on April 4, 2023, and since over 50 million submissions have been processed globally [51]. However, Turnitin has acknowledged the misclassification of human-written text as AI-generated or false positives. Such false positives have serious implications for using such tools in real settings."}, {"title": "4.2 Gaps Identified in Existing Solutions", "content": "There were several gaps identified in existing solutions for academic misconduct. From the perspective of datasets built and used for AIGC detection, most algorithms that come up for AIGC detection are built using text observations that are copy-pasted from LLM tools like ChatGPT. Most of the implementations are tested on naive datasets, i.e. datasets which contain human-written and direct ChatGPT-generated text, resulting in very high evaluation metrics but poor performance in real settings. The existing solutions claim high accuracy on the dataset because most of these datasets are built with human-written and text generated from LLMs directly. However, in real settings, students apply several evasion techniques like paraphrasing to fool the detectors. This is also because the models developed have not been cross-tested against other datasets. Thus, a benchmark dataset for AIGC detection representing those observations in which evasion techniques like paraphrasing have been performed should be developed and open-sourced.\nFrom the perspective of the methodology of developing AIGC detection algorithms, the existing research studies rely on the whole text to identify whether students are cheating or not. If solutions are developed on the whole text representations, students can alter the text with different evasion techniques discussed above. However, suppose solutions are developed with the main gist of the text, which contains the main topics or keywords and the summary. In that case, the solution may tolerate different evasion techniques.\nFrom the perspective of model results, most of the research implementations have been done on outputting the probability of AIGC on the whole text. Research on detecting AIGC probability paragraph-wise or sentence-wise would give justifiable predictions. The AIGC detection models should be able to generate the probability of AI-generated in different parts of the text. Additionally, there seems to be a gap in adding explainability to the prediction of models. Further research can be done by employing post hoc (after training and prediction) model-agnostic techniques like SHAP to approximate each word's attribution to the final prediction. This has been done to explain the predictions of text classification and sentiment analysis models [135] and can be done to explain the predictions of AIGC detection algorithms.\nFrom the perspective of the overall solution to fight academic misconduct, the major gap is that most of the research studies to solve this problem have been focused on simply classifying AI-generated or human-written and plagiarized or not. Other alternatives to solve the problem, such as preventing academic cheating, remodelling assessment strategies, and promoting ethical use of AI in academia, have not been focused compared to technical solutions like plagiarism and AIGC detection."}, {"title": "5 Discussion", "content": "It is evident from the experimentation performed and analysis from the survey of existing solutions in section 3 that a reliable solution to prevent academic misconduct due to LLMs is essential. This section of the paper discusses the possibility of solving the problem of AI plagiarism. Apart from the technical solutions, other alternative educational solutions that academic institutions should focus on to solve the problem have also been discussed."}, {"title": "5.1 Feasibility of AIGC Detection", "content": "The survey was conducted on different aspects of text generation using recent LLMs, and the major question was whether it is possible to detect AI-generated text. Heikkil\u00e4archive stated that it is extremely unlikely that there will be a tool that can detect AI-generated text with 100% certainty in her article in MIT Technology review [136]. Muhammad Abdul-Mageed, a professor at the University of British Columbia, argues that it is difficult to do AIGC detection because the major objective of any LLMs is to generate more and more human-like text [137]. The other main problem with AIGC detection is that if someone comes up with a great AIGC detection tool, there will be another LLM shortly on which it will fail. William H. Walters found that most AI detectors can distinguish papers generated from the GPT3.5 model and human-written papers but fail to identify papers generated from the recent GPT-4 model [138]. Findings from Weber-Wulf et al. [139] experiment on fooling 12 publicly available AIGC detectors and two commercial systems, Turnitin and PlgairismCheck, mostly used by universities by applying evasion techniques like translation and paraphrasing, suggested that a reliable solution for AIGC detection does not exist and may not exist."}, {"title": "5.2 Alternative Educational Solutions", "content": "After analysis of the preventive measures for academic cheating with more than 50 examples, including experience from various academic institutions, Bylieva et al. [140] concluded that the problem of academic cheating can not be solved only with pure technical measures because such technical war can last forever. Thus, academic institutions should consider and adopt non-technical alternative educational solutions. One of the major steps that should be considered is generating awareness of academic misconducts like plagiarism and improper use of AI tools like ChatGPT. In a survey conducted with 3405 students at an Australian university, it was reported that only 50% of the students had read the plagiairism policy, and half of the students did not know what behaviour constitutes plagiairism[141]. The office in a university that deals with complaints stated that most complaints were about the university failing to warn the students about plagiarism and its consequences [142]. European Network for Academic Integrity (ENAI) also presented recommendations for academic institutions to promote the ethical use of AI tools like ChatGPT in academia. The recommendations include training both students and teachers about the ethical use of AI and the limitations of the bias and accuracy existing in such AI tools. Universities should also clarify the circumstances of using AI tools properly [143]. Thus, proper awareness of academic misconduct is vital.\nAnother solution to academic misconduct is to incorporate AI ethically and creatively in academic settings. Dr Amin Davodi expresses that students should be taught to use AI tools to learn, and there will be less chance of using AI for cheating [144]. Universities should allow using tools like ChatGPT, but only up to a certain threshold. Students can use such tools for reviewing and correcting but cannot entirely copy-paste the content generated by those tools. This can be facilitated by setting up a threshold for ChatGPT-generated content in student's assignments. ChatGPT has been listed as a coauthor in research papers published [145]. Universities and scientific publishers can create a threshold value for permitting the correct use of tools like ChatGPT. Jo Ann Oravec [146] proposes to design assignments so that ChatGPT can be transparent. The assignments can include the steps to compare the student's solution with ChatGPT, review and rewrite the assignment [146]. This way, tools like ChatGPT can be used more constructively. As recommended by ENAI, appropriate citation of the use of AI tools should be done for the ethical usage of AI [143]. Abd-Elaal et al. [147] concluded that academic institutions should improve their plagiarism and fabrication policies. Likewise, ENAI also recommends that national guidance and institutional-level policies be developed concerning AI's ethical use in academia [143].\nAnother solution for preventing academic misconduct is the improvement of current assessment strategies in universities. The assessment should focus more on assessing student's critical thinking and problem-solving skills rather than memorizing skills. Simple factual questions can be directly solved by using LLMs. Thus, students should be assigned creative projects with open-book exam questions that require critical thinking and problem-solving skills [148]. Another strategy that lecturers can use to asses students is by giving them visual or interactive questions, for instance, instructing a student to watch a video and answer questions based on the events in that video. This way, students cannot cheat on ChatGPT. Likewise, instructing students to give oral presentations or assessing students through viva examinations are some strategies that can prevent cheating [149]. Another efficient trick to preparing assessments to prevent cheating is to prepare timed assignments [150]. With timed assignments, students cannot answer all questions by getting answers from ChatGPT in a limited time [150]."}, {"title": "6 Conclusion", "content": "In this research work, different ways by which academic misconduct is done and can be prevented were discussed. Specifically, the change in the landscape of academic cheating after introducing LLMs was discussed. We presented how the introduction of cheating with generative AI tools has changed other cheating forms like plagiarism. We surveyed both sides, i.e., the development of algorithms and tools to prevent AI-generated text and plagiarism and the evasion techniques applied to fool such algorithms and tools. With such evasion techniques and a combination of different tools, we demonstrated that existing solutions for plagiarism detection and AI-generated detection can not be relied upon.\nThe survey identified several gaps in the existing datasets and solutions for plagiarism detection and AIGC detection. Similarly, considering the longevity of the technical war between AIGC detection and fooling AIGC detectors, we proposed other alternative educational solutions that academic institutions should use. Drawing upon the current scenario caused by LLMs in academia, the issue looks pretty concerning, and hence, policies and regulations regarding the ethical use of such tools in academia should be developed.\nSeveral future works can be done to improve the field of AIGC detection. Regardless of the amount of research done to solve the problem, there is a big question about the full reliability of existing AIGC detection tools and solutions. This is partly because there is no existing quality benchmarking for the problem. This can be partly solved by developing a benchmark AIGC detection quality dataset. Other experiments can be performed to solve the problem by applying a mixture of different methods to build more accurate AIGC detection algorithms. Apart from technical solutions, experiments on non-technical solutions can also be performed to solve the problem of AI-based plagiarism. Additionally, research on the explainability of the outputs of different AIGC detection models can be done to build trustworthy solutions. Thus, developing and releasing a quality benchmark dataset for the task, developing AIGC detection models with a mixture of different models, and adding explainability to the predictions coming from AIGC detection models are some of the major future works that can done."}]}