{"title": "Label-Efficient Data Augmentation with Video Diffusion Models for Guidewire Segmentation in Cardiac Fluoroscopy", "authors": ["Shaoyan Pan", "Yikang Liu", "Lin Zhao", "Eric Z. Chen", "Xiao Chen", "Terrence Chen", "Shanhui Sun"], "abstract": "The accurate segmentation of guidewires in interventional cardiac fluoroscopy videos is crucial for computer-aided navigation tasks. Although deep learning methods have demonstrated high accuracy and robustness in wire segmentation, they require substantial annotated datasets for generalizability, underscoring the need for extensive labeled data to enhance model performance. To address this challenge, we propose the Segmentation-guided Frame-consistency Video Diffusion Model (SF-VD) to generate large collections of labeled fluoroscopy videos, augmenting the training data for wire segmentation networks. SF-VD leverages videos with limited annotations by independently modeling scene distribution and motion distribution. It first samples the scene distribution by generating 2D fluoroscopy images with wires positioned according to a specified input mask, and then samples the motion distribution by progressively generating subsequent frames, ensuring frame-to-frame coherence through a frame-consistency strategy. A segmentation-guided mechanism further refines the process by adjusting wire contrast, ensuring a diverse range of visibility in the synthesized image. Evaluation on a fluoroscopy dataset confirms the superior quality of the generated videos and shows significant improvements in guidewire segmentation.", "sections": [{"title": "Introduction", "content": "Guidewires are frequently used in interventional surgeries to navigate blood vessels and guide other medical devices, such as catheters, stents, or balloons, to precise locations within the body. Guidewire segmentation in X-ray fluroscopy videos is a critical task in computer-aided cardiac interventional procedures. It often serves as a prerequisite for functions such as 4D guidewire reconstruction (V\u00f6th et al. 2023) and the co-registration of angiography and intravascular ultrasound images (Prasad et al. 2016). This task is inherently challenging due to factors such as image noise, complex scenes, projective foreshortening, object occlusion, rapid guidewire motion, and the thin, elongated shape of the guidewire.\nAccurate and sufficient annotated data are essential for training guidewire segmentation models, but labeling thin and elongated guidewires is both error-prone and labor-intensive. To address this challenge, this paper proposes a method to generate synthetic videos paired with guidewire masks as augmented data, aimed at enhancing the performance of downstream segmentation models (Fig. 1). We trained a generative diffusion model (DM) on a small labeled dataset along with a larger unannotated dataset, enabling the generation of multiple annotated fluoroscopy videos with varying backgrounds and wire appearances from a given semantic wire mask.\nIn medical data synthesis, Generative Adversarial Networks (GANs) (Goodfellow et al. 2014) encounter challenges such as unstable training and limited video fidelity and diversity, particularly due to the complex distribution of medical images and small dataset sizes. Denoising Diffusion Probabilistic Models (DDPMs) (Ho, Jain, and Abbeel 2020; Song et al. 2020) have shown promise in generating diverse medical images (Yu et al. 2023; Du et al. 2023; Pan et al. 2023b). Similarly, video DDPMs (Blattmann et al. 2023b,a; Ho et al. 2022) demonstrate potential in synthesizing medical videos conditioned on semantic contents (Lugmayr et al. 2022; Wang et al. 2022; Du et al. 2023)."}, {"title": "", "content": "However, our task of synthesizing fluroscopy videos for data augmentation in wire segmentation poses unique challenges and specific requirements. Firstly, it is challenging to acquire fluoroscopy images/videos at the scale of natural ones, and annotating wires in numerous videos is almost impractical. Therefore, it is beneficial to minimize the labeling effort while preserving the fidelity and variability of the synthesized videos. Secondly, since guidewires are extremely thin (typically 2-3 pixels wide) and tend to be occluded by other objects or noise, detection often relies on the contrast between fast-moving guidewires and the slower-moving background, which needs to be reflected in synthesized videos. Thirdly, the nuanced and varied appearance of wires against complex backgrounds in fluoroscopy videos requires a conditioning approach that accurately captures wire masks and generates diverse wire appearances. Inadequate representation of these masks compromises the fidelity and variability of wire depiction in the synthesized videos, reducing their value for segmentation tasks.\nWe propose a label-efficient data augmentation method for guidewire segmentation in cardiac fluroscopy videos, named Segmentation-guided Frame-consistency Video Diffussion Model (SF-VD). First, we use two separate 2D diffusion models (DMs) to independently learn scene distribution and cross-frame motion: the first model synthesizes a static image conditioned on a wire mask, while the second model generates subsequent frames based on the static image and the wire masks for those frames. This strategy allows the first model to effectively leverage data with limited labeled frames, while the second model ensures cross-frame consistency by training on fully annotated videos. Together, these models generate more diverse backgrounds than would be possible using only fully annotated video datasets. Additionally, the second model employs a \u201cframe-consistency\u201d sampling strategy that produces more realistic and diverse videos compared to 3D models, especially given our limited data. The second key idea is to modulate wire contrast and enhance the variability of wire appearance and visibility in synthetic frames. This is achieved by using a segmentation model to guide the diffusion reverse process, similar to the approach used in classifier guidance (Dhariwal and Nichol 2021).\nWe benchmarked the proposed method against various data augmentation methods including other video diffusion models, using several segmentation model architectures, which included both generic state-of-the-art (SOTA) models and those specifically designed for guidewire segmentation. Our results demonstrate that our method outperforms other data augmentation approaches and enhances the segmentation performance of all tested models.\nIn summary, the key contributions of this work are:\n\u2022 Introduced SF-VD, a label-efficient video synthesis model that enhances downstream wire segmentation performance. To the best of our knowledge, this is the first work on fluoroscopy video synthesis and the first to use a generative model for data augmentation in guidewire segmentation.\n\u2022 Proposed a two-model strategy to independently learn scene and motion distributions, addressing the challenges of limited medical data and guidewire annotations.\n\u2022 Proposed a frame-consistency strategy to ensure coherent motion and content across fluoroscopy frames.\n\u2022 Introduced diffusion guidance by a segmentation network to adjust guidewire contrast, improving visualization and diversity in fluoroscopy videos."}, {"title": "Related Work", "content": "Video Diffusion Model Recent advancements in DMs have driven progress in video generation. Several studies (Ho et al. 2022; Ho et al. 2022; Liu et al. 2024) train 3D DMs to model video sequences as volumetric images. However, optimizing these models is challenging due to the high-dimensional nature of the data, which requires a large amount of video data and compute (Chen et al. 2024). In response, some researchers used pretrained 2D image DMs for consistent frame generation. They either integrated additional structures to fine-tune the pretrained image DMs (Yang, Srivastava, and Mandt 2023; Ge et al. 2023; Gu et al. 2023; Guo et al. 2023; Blattmann et al. 2023a), or employed a completely training-free approach (Khachatryan et al. 2023; Wu et al. 2022), using the image DMs as a generative prior to generate videos. These methods reduce optimization challenges and improve video fidelity. However, directly applying these techniques to our problem is not optimal, since fine-tuning pretrained image DMs still requires a substantial number of labeled videos, which are costly to obtain for medical data. Moreover, precise pixel-level conditioning is crucial for our task, and maintaining this accuracy during sampling from a pretrained image DM is not straightforward. To address these challenges, we propose training a DM from scratch with pixel-wise wire conditioning and a two-model strategy, which independently models scene and motion distributions to reduce the amount of required data.\nConditional Diffusion Model Recent advancements in conditional DMs have significantly enhanced personalized, task-specific image and video generation by optimizing condition integration for better control. Key approaches include 'network injection' DMs, which embed conditions within the denoising network for controlled generation (Wang et al. 2022; Avrahami et al. 2023), and 'process injection' DDPMs, which modify the diffusion trajectory by incorporating conditions directly into the diffusion steps (Lugmayr et al. 2022; Mao, Wang, and Aizawa 2023; Liang et al. 2024). Classifier-guided-based DMs utilize an auxiliary network to preprocess conditions before injection, offering precise guidance (Dhariwal and Nichol 2021; Gafni et al. 2022; Zhang, Rao, and Agrawala 2023). Classifier-free DMs (Ho 2022; Nichol et al. 2021) employ a simpler architecture without explicit condition encoding, making them suitable for applications such as text-guided, segmentation-guided generation, and colorization. In the medical field, these methods have been adapted for conditional image generation, such as SegGuidedDiff (Konz et al. 2024) for generating breast MRI and abdominal CT images with organ segmentation maps, Med-DDPM (Dorjsembe et al. 2023) for"}, {"title": "Guidewire Segmentation in Fluoroscopy Videos", "content": "Advancements in deep learning, particularly with convolutional neural networks (CNNs) (Milletari, Navab, and Ahmadi 2016; Isensee et al. 2020) and vision Transformers (Tang et al. 2022; Pan et al. 2023a), have shown promise in accurately segmenting medical objects, including organs and interventional devices (Liu et al. 2020; Gonz\u00e1lez, Bravo-S\u00e1nchez, and Arbelaez 2020). Innovative solutions such as pyramid attention recurrent networks (PARN) (Zhou et al. 2020), temporal Transformer network (TTN) (Zhang et al. 2021), and 4D recurrent networks (4D-RCN) (Lee et al. 2020) have been proposed to enhance feature capture across video frames, leading to improved guidewire segmentation. Additional strategies like real-time guidewire network (RTGN) using a preliminary guidewire detection network for bounding box setup (Zhang et al. 2022), and Ariadne+ (Caporali et al. 2022), which applies graph theory for post-process refinement and false negative reduction, have been proposed. These models focus on development of neural network architectures and segmentation algorithms, but the scarcity of annotated training videos is overlooked, which often limits their performance on out-of-distribution videos. We propose a label-efficient data augmentation method using conditional DMs to improve segmentation network training with synthetic data that cover a wide range of guidewire contrast, background appearances, and motions."}, {"title": "Methodology", "content": "Overview The aim of the proposed video generation method, SF-VD, is to take a sequence of guidewire masks (as illustrated in Fig. 1) and synthesize fluoroscopy videos with guidewires at the specified pixels, which exhibit good variation, fidelity, and cross-frame consistency. The method aims to efficiently utilize fluoroscopy videos and mask annotations, given their limited availability, and to serve as a data augmentation tool for training wire segmentation models.\nTo address of the problem of overfitting when directly modeling the complex condition distribution of fluoroscopy videos, we propose to decompose the distribution P(V|M) into two components: a scene distribution P(I(0)|M(0)), which accounts for factors such as imaging angles, imaging parameters, objects in view, and patient anatomy; and a motion distribution P(I(1)...I(N\u22121)|M(1)...M(N\u22121), I(0)), which captures object motion due to heartbeats and breathing. The strategy also helps to leverage the video with partially labeled frames, which can be used to learn the scene distribution.\nWe trained two 2D DMs to independently model each component and employed a three-stage approach (one stage to sample the scene distribution; two stages to sample the motion distribution) to synthesize videos, as shown in Fig. 2. In addition, a segmentation-guided mechanism was introduced to refine the diffusion reverse process throughout, ensuring controllable wire depiction across frames."}, {"title": "Modeling the Scene Distribution", "content": "Modeling a distribution with DMs involve two primary Gaussian processes: forward and reverse diffusion(Nichol and Dhariwal 2021). In the forward process, an uncorrupted image x0 undergoes T diffusion steps, progressively incorporating noise \u03f5, resulting in a sequence of increasingly noisy frames: {x0,...,xT}. This process is modeled as a Gaussian process with a specified noise schedule {\u03b10,\u2026\u2026., \u03b1T}: xt ~ N(\u221a\u03b1tx0, (1 \u2212 \u03b1t)I), where \u03b1t is the product of the noise coefficients up to time t, \u03b1t = \u03a0ti=1\u03b1i.\nNext, a network parameterized by \u03b8 is trained to reverse the forward process, starting from a noise sample and sequentially removing noise \u03f5 to recover the original data.\nTo model the scene distribution of fluoroscopy videos, we want to generate an image conditioned on a guidewire mask. Given a noisy sample xt and the time step t, the model predicts the previous less noisy sample xt-1 with the mask M:\nxt\u22121 ~ p(xt | M) = N(\u03bc\u03b8(xt, t, M), \u03c3\u03b8(xt, t, M)I)\n\u03bc\u03b8(xt, t, M) = 1/\u221a\u03b1t(xt \u2212 \u221a1 \u2212 \u03b1t\u03f5\u03b8(xt, t, M))\nand \u03f5\u03b8(xt, t, M) = exp(v\u03b8vf + (1 \u2212 \u03c5\u03b8)v\u03b3)\n(1)\nHere, the condition mask M is channel-wise concatenated with xt, and fed to the network along with the time step t to estimate the noise \u03f5\u03b8 and variance v\u03b8. vf and v\u03b3 are two variance-related constants defined in IDDPM (Nichol and Dhariwal 2021). The optimization for \u03bc\u03b8 and \u03c3\u03b8 follows IDDPM, as shown in Appendix A.\nThe network was trained on a mixture of annotated and unannotated images to increase background variability. The strategy aligns with the formulation of the classifier-free (CF-) DDPM (Ho 2022), as the network simultaneously models both the unconditional and conditional image distributions. CF-DDPM approximates the conditional score function as:\n\u2207xtlog p(xt | M) = \u2207xtlog p(xt) + \u2207xtlogp(M | xt)\n\u2248\u2207xtlog p(xt) + w(\u2207xtlog p(xt | M) \u2212 \u2207xtlog p(xt))\n(2)"}, {"title": "", "content": "Unlike the original CF-DDPM where variance is constant, we incorporate a predicted variance to express the score function as:\n\u2207xtlog p(xt | M) = \u03f5\u03b8(xt, t, M)/\u03c3\u03b8(xt, t, M)\n(3)\nRewriting Eqs. 2 and 3 into the format of Eq. 1, we have the updated noise as:\n\u03f5\u03b8(xt, t, M) = (1 \u2212 w)\u03f5\u03b8(xt, t) + w\u03f5\u03b8(xt, t, M)\n(4)\nwhere w is empirically selected as 0.7. For the variance \u03c3\u03b8(x, t), we retain the original form \u03c3\u03b8(x, t, M) since the updated form does not improve performance, likely because the unconditional variance is difficult to estimate accurately."}, {"title": "Modeling the Motion Distribution", "content": "To model the motion distribution, we propose to generate another frame based on a previously generated frame and a mask with a 2D DM. Note that modeling motion in cardiac fluoroscopy videos is typically simpler than in natural videos because, in fluoroscopy, objects tend to stay within the field of view and their appearance remains relatively stable. Therefore, a 2D conditional DM is sufficient for the task and helps prevent overfitting with limited training data.\nSimilar to Eq. 2, with the conditions of an initial frame F and a mask M, we have:\n\u2207xtlogp(xt | F, M) =\u2207xtlog p(xt | M) + \u2207xtlog p(F | xt, M)\n\u2248\u2207xtlog p(xt | M) + w(\u2207xtlog p(xt | F, M)\n\u2212\u2207xtlog p(xt | M))\n(5)\nSimilarly, the predicted variance in Eq. 1 is \u03c32\u03b8(xt, t, F, M), and the predicted noise is:\n\u03f5\u03b8(xt, t, M, F) = (1 \u2013 w)\u03f5\u03b8(xt, t, M) + w\u03f5\u03b8(xt, t, M, F)\n(6)"}, {"title": "Frame-consistency Sampling", "content": "The networks described above enable us to generate the first frame of a video, followed by each subsequent frame sequentially. However, this approach produced videos with jittery background motion, negatively impacting segmentation performance (see Appendix D). To address this, we introduce \u201cframe-consistency\" sampling, where a frame is generated with the conditions of the preceding and following frames. Denoting these frames as F1 and F2, similar to Eqs. 2 and 5, the conditional score function is:\n\u2207xtlogp(xt | F1, F2, M) =\u2207xtlog p(xt, F1, F2, M) \u2013 \u2207xtlog p(F1, F2, M)\n=\u2207xtlog p(xt | M) + \u2207xtlog p(F1 | xt, M)\n+\u2207xtlogp(F2 | xt, F1, M)\n(7)\nSimilarly, the predicted noise is\n\u03f5\u03b8(xt,t, M, F1, F2) = (1 \u2013 2\u03c9)\u03f5\u03b8(xt, t, M) +w\u03f5\u03b8(xt, t, M, F1) + w\u03f5\u03b8(xt, t, M, F2)\n(8)"}, {"title": "A Three-stage Approach", "content": "Taken together, as shown in Fig. 2, we first use the scene-distribution model \u03b8s to generate the first (leading) frame, followed by the motion-distribution model \u03b8m to generate the last (concluding) frame based on the leading frame. Next, we use the frame-consistency sampling to generate the middle frame, conditioned on the leading and concluding frames. The remaining frames are generated in the same manner (the sampling order of a 16-frame video is shown on the left in Fig. 2c), by iteratively generating frames equidistant from the previously generated ones until all frames are completed.\nIn practice, the motion-distribution model \u03b8m also takes the frame distance between the generated frame and the conditional frame as inputs. w in Eqs. 6 and 8 were set to -2.5 and -1.5 when generating the concluding frame and intermediate frames, respectively."}, {"title": "Segmentation-guided Mechanism", "content": "To modulate the wire contrast in fluoroscopy videos, a segmentation network \u03c6, trained on the 2D fluoroscopy dataset, was used as a sampling guidance. The segmentation-guided mechanism is integrated into the estimated mean in the reverse process as follows:\n\u03bctfinal =\u03bc(xt,t, M, F1, F2) +\u03b3\u03c3\u03bf (xt, t, M)\u00b2 log p\u03c6(M|xt)\n(9)\nwhere \u03b3 controls the strength of the wire contrast. Typically, \u03b3 is randomly chosen from 0 (no segmentation-guided enhancement) to 15 to maintain visual realism."}, {"title": "Experiments", "content": "Datasets Due to the absence of a publicly available cardiac fluoroscopy dataset, all data were collected from hospitals and manually annotated. Two datasets were used in this study: 1) a fully annotated fluoroscopy video (F-video) dataset consists of 400 2D+time fluoroscopy videos of 16 frames per video, from patients in cardiac interventional surgeries. Each video was normalized to an intensity range of -1 to 1. All frames are annotated with guidewire delineations by certified professionals. The frames were resized to 512 \u00d7 512 pixels. 2) A partially annotated fluoroscopy image (P-image) dataset includes 409 2D+time fluoroscopy videos. Of these, 81 videos contain partial frames with guidewire annotations, and the rest videos have no annotations at all. Each video was normalized to an intensity range of -1 to 1. We collect 4000 annotated frames and 10000 unannotated frames, with all images resized to 512x 512 pixels, as the final dataset.\nSF-VD was trained using both the P-image and F-video datasets, with both used for the scene-distribution model \u03b8s, and the latter for the motion-distribution model \u03b8m. Please refer to Appendix B for implementation details.\nVideo Quality Analysis We evaluated the quality of synthesized videos using three metrics: Fr\u00e9chet Video Distance (FVD) (Unterthiner et al. 2018), Diversity Score (DS) (Pan et al. 2021), and Overfitting Score (OS) (Pan et al. 2021). FVD assesses the realism and diversity of the videos, DS measures the diversity among the videos, and OS evaluates whether the videos replicate training data or generate novel content.\nWire Segmentation Evaluation In addition, we evaluated the impact of fluoroscopy videos synthesized by SF-VD on a downstream wire segmentation task. Segmentation models were trained using 80% of the F-video dataset, augmented with 1200 synthesized videos featuring random wire contrasts generated by SF-VD. Performance was validated on another 10% of the F-video, and tested on the rest 10% of the F-video."}, {"title": "Results", "content": "Fig. 3 displays synthesized videos from the SF-VD, VD, and LVDM models. SF-VD generates realistic videos with diverse anatomical backgrounds and guidewire appearances. In contrast, VD, a 3D DDPM, results in noisy backgrounds and structural artifacts, highlighting the challenges of high-dimensional video generation without extensive datasets. LVDM-generated videos are visually realistic but exhibit limited variability, suggesting potential overfitting to a small annotated dataset. Conversely, SF-VD effectively produces varied videos by leveraging unannotated data. In addition, SF-VD generates realistic diaphragm motions, a capability that other methods fail to achieve. Fig. 4 shows synthetic videos with different wire contrasts, achieved through the segmentation-guided mechanism, which enhances the generalization capability and performance of the downstream segmentation network (Appendix D).\nFor quantitative evaluation,"}, {"title": "Ablation Study", "content": "We investigate the impact of Frame-Consistency (FC) and Segmentation-Guided (SG) strategies on the wire segmentation performance of V-Net. FC is designed to generate more realistic anatomical backgrounds and motion within videos, whereas SG focuses on modeling the diverse appearances of wires. Experiments without FC generate frames in a chronological order.\nResults in Appendix D reveal that incorporating FC significantly improves metrics that measure region-overlap accuracy, including Dice, sensitivity, and precision. In contrast, distance-measuring metrics such as HD, G2RE, and R2GE show only minor changes with the addition of FC. Note that these distance-based metrics are more sensitive to small, isolated false-positive regions located far from the wires. Therefore, the likely explanation is that FC, by generating videos with more temporally coherent and realistic background, enables segmentation networks to learn features that better distinguish foreground and background motions, and thus reduce the extent of false positives and negatives. However, FC does not fully eliminate isolated false-positive regions. This could be because other structures in cardiac fluoroscopy, such as ribs, likely exhibit motion patterns distinct from the background while also presenting sharp edges, which may be misclassified as wires.\nSG, by contrast, greatly reduces HD and G2RE, effectively minimizing distance-based errors. It also improves R2GE, especially when FC is applied, although its influence on region-overlap metrics remains minor. This suggests that while SG does not substantially reduce false-positive or false-negative areas, it effectively eliminates isolated false-positive regions. A possible explanation is that by diversifying wire contrast in training data, SG helps segmentation networks learn more robust features for distinguishing wires from wire-like structures, such as rib edges. This finding highlights the importance of incorporating high variability in wire appearance within training data.\nThe simultaneous application of FC and SG yields the best results for Dice, HD, sensitivity, precision, and G2RE. This demonstrates that combining FC and SG strategies enhances both region-based and boundary-based metrics, achieving a well-rounded improvement."}, {"title": "Limitation and Future Works", "content": "The proposed SF-VD is relatively inefficient due to its dual DDPM architecture. It requires 40 seconds to produce a 16-frame video on an A100 GPU, limiting both the number of videos and frames generated. Improving efficiency through a single model to handle all stages or by integrating high-efficiency DDPMs, such as the One-step Consistency Model (Song et al. 2023) or DiffuseVAE (Pandey et al. 2022), is crucial for efficient video generation within the proposed three-stage framework.\nAdditionally, current methods rely on existing wire mask sequences, limiting variability in wire motion. Future research will explore synthesizing both wire motion and background dynamics in the videos. For instance, synthesizing diverse wire mask sequences to guide video generation will allow for greater variability.\nWe also acknowledge that a limited number of training videos may affect the quality and variability of the generated outputs, potentially impacting downstream segmentation. Future work will analyze the relationship between training data size and segmentation performance to optimize data requirements.\nLastly, we aim to expand SF-VD to generate videos of other interventional devices, including catheters, stents, and catheter tips, to enhance segmentation and tracking for these devices."}, {"title": "Conclusion", "content": "This study presents the SF-VD framework, a pioneering effort in medical video synthesis, aimed at creating a diverse library of fluoroscopy videos from existing guidewire annotations and augmenting the training data for deep learning models for wire segmentation. Our evaluations demonstrate the efficacy of these synthesized videos in improving deep learning-based wire segmentation. Future work will explore the potential of SF-VD framework for a broader range of interventional device-related tasks in fluoroscopy videos."}, {"title": "Appendix", "content": "A Practical Optimization of Diffusion Process\nTo estimate the \u00b5\u03b8(xt, t, M) as shown in Section \"Methodology: Modeling the Scene Distribution\", we need to optimize \u03f5\u03b8(xt, t, M):\nargmin \u03b8L = argmin \u03b8MAE(\u03f50, \u03f5\u03b8(xt, t, M))\n(1)\nTo estimate the \u03c3\u03b8(xt, t, M) also shown in Section \u201cMethodology: Modeling the Scene Distribution\u201d, we need to optimize v\u03b8(xt, t, M):\nargmin \u03b8L = argmin \u03b8(DKL(N(\u03bc, \u03c32) | N(\u03bc, \u03c3\u03b8(\u03c5\u03b8(xt, t, M)))))\n(2)"}, {"title": "Implementation Details of Experiments", "content": "B.1 Metrics\nDue to the thin and narraw structure of wires, we used a diverse set of metrics for comprehensive evaluation.\nB.2 Computing Infrastructure\nOur experiments were implemented using the PyTorch 2.0.1 framework with CUDA 11.7, running in Python 3.8.0 on Ubuntu 20.04.3 LTS. The computations were performed on a system equipped with a single NVIDIA A100 GPU with 40GB of memory, and Intel Xeon Gold 6226R Processor.\nB.3 Network Implementation and Training\nSF-VD was trained using both the P-image and F-video datasets, with both used for the scene-distribution model \u03b8s, and the latter for the motion-distribution model \u03b8m. SF-VD was optimized with the AdamW optimizer at a learning rate of 10-4. Training was conducted for 200 epochs for 2D fluoroscopy image synthesis and 500 epochs for 2D+time fluoroscopy video synthesis. Each frame of the synthesis videos are generated with 256 timesteps.\nA 2D segmentation network based on DynUnet, an adaptation of nnUnet (Isensee et al., 2020), was trained for the segmentation-guided mechanism. Training utilized all annotated frames from the P-image and F-video dataset with Mixup augmentation. The AdamW optimizer was used with a learning rate of 1 \u00d7 10-5, a batch size of 3, and training lasted for 500 epochs, achieving full convergence.\nTraining for all segmentation models followed the strategies used for the segmentation-guided mechanism model, with variations in the number of epochs. Models were trained for 1000 epochs, consistent with the pre-defined epoch count for Dynvnet. For CL and MVI, pre-training was performed for 600 epochs. Due to the larger dataset (more than four times larger), training epochs for models with SF-VD, VD (Ho et al., 2022), and LVDM augmentations were reduced to 250 epochs to ensure fair comparison."}, {"title": "Full Evaluation for Downstream Segmentation Task using Synthetic Video Augmentation", "content": "Networks"}, {"title": "Ablation Studies for Frame-consistency and Segmentation-guided Strategies", "content": ""}, {"title": "Pseudocodes", "content": "A Practical Optimization of Diffusion Process\nTo estimate the \u00b5\u03b8(xt, t, M) as shown in Section \"Methodology: Modeling the Scene Distribution\", we need to optimize \u03f5\u03b8(xt, t, M):\nargmin \u03b8L = argmin \u03b8MAE(\u03f50, \u03f5\u03b8(xt, t, M))\n(1)\nTo estimate the \u03c3\u03b8(xt, t, M) also shown in Section \u201cMethodology: Modeling the Scene Distribution\u201d, we need to optimize v\u03b8(xt, t, M):\nargmin \u03b8L = argmin \u03b8(DKL(N(\u03bc, \u03c32) | N(\u03bc, \u03c3\u03b8(\u03c5\u03b8(xt, t, M)))))\n(2)"}]}