{"title": "PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback", "authors": ["Kanika Goswami", "Ryan Rossi", "Puneet Mathur", "Franck Dernoncourt"], "abstract": "Scientific data visualization is pivotal for transforming raw data into comprehensible visual representations, enabling pattern recognition, forecasting, and the presentation of data-driven insights. However, novice users often face difficulties due to the complexity of selecting appropriate tools and mastering visualization techniques. Large Language Models (LLMs) have recently demonstrated potential in assisting code generation, though they struggle with accuracy and require iterative debugging. In this paper, we propose PlotGen, a novel multi-agent framework aimed at automating the creation of precise scientific visualizations. PlotGen orchestrates multiple LLM-based agents, including: (1) a Query Planning Agent that breaks down complex user requests into executable steps; (2) a Code Generation Agent that converts pseudocode into executable Python code; and three retrieval feedback agents (3) a Numeric Feedback Agent, (4) a Lexical Feedback Agent, and (5) a Visual Feedback Agent that leverage multimodal LLMs to iteratively refine the data accuracy, textual labels, and visual correctness of generated plots via self-reflection. Extensive experiments show that PlotGen outperforms strong baselines, achieving a 4-6% improvement on the MatPlotBench dataset, leading to enhanced user trust in LLM-generated visualizations and improved novice productivity due to a reduction in debugging time needed for plot errors.", "sections": [{"title": "1 Introduction", "content": "Scientific data visualization is essential for transforming raw data into visual representations, enabling the communication of complex information, the analysis of patterns in large datasets, and the formulation of evidence-based narratives. Professional data analysts and researchers have access to several sophisticated toolkits (e.g. Matplotlib, Seaborn and Plotly) to aid in the creation of diverse visualizations. However, novice users often face significant challenges when attempting to produce informative and accurate visualizations from raw data. These challenges stem from the difficulty of interpreting user requirements, choosing the appropriate visualization tool from the vast array of available options, and mastering the technical intricacies of graph plotting. Consequently, there is a growing need for more intuitive automated systems that support users with limited technical expertise to generate high-quality infographics.\nRecently, Large Language Models (LLMs) have demonstrated remarkable capabilities in assisting a wide range of complex tasks, such as software development[14], web navigation[8], and document editing[10], due to their proficiency in code generation[17], function-calling [16], tool utilization [13, 19], and self-reflective learning[7, 20]. Recent advancements in LLM-based agents have opened new avenues for exploring their potential in automating the generation of scientific data visualizations based on user-defined queries, potentially lowering the barriers for novice users and enhancing accessibility to advanced data visualization techniques. Modern code-focused LLMs, such as CodeLlama [17], WizardCoder[9], and OpenAI GPT-4 have shown strong coding proficiency in Python to generate 2D and 3D chart figures via in-context learning. However, they struggle to faithfully adhere to user specifications due to hallucinations in code generation. AI-assisted data visualization often requires iterative refinement, where users critique intermediate plot outputs and adjust code generation to meet specific customization goals aligns with user expectations in terms of visual appearance, text label clarity, and precise data representation.\nTo address these challenges, we propose PlotGen (see Fig. 1), a novel multi-agent framework designed to generate accurate scientific data visualizations based on user specifications by leveraging multimodal LLMs to iteratively debug the intermediate figures output by a code LLM via multimodal self-reflection. PlotGen consists of three unique multimodal feedback agents that provide sensory feedback-visual, lexical, and numerical-to the code generation agent, helping to rectify errors during the plotting process and better fulfill user queries through self-reflection. The system orchestrates multiple LLM agents, including: (1) a Query Planning Agent that decomposes complex user requests into a sequence of executable steps"}, {"title": "2 Related Work", "content": "Code LLMs: With the advent of ChatGPT, several propriety LLMs like GPT-3.5, GPT-4, and Claude Sonnet-3.5 have emerged with increasingly strong code generation abilities. Moreover, numerous open-source LLMs such as CodeLlama [18], DeepSeekCoder [6], [9], [23] have also come out that are on par in producing executable code.\nLLM-based Data Visualization: Several previous works have attempted to automate data visualization generation from natural language. [5] was the first attempt to use LSTM to convert JSON data into Vega-Lite visualizations. [4] explored use of LLMs to generate visualization code. Recent works studied the utility of LLMs like ChatGPT for generating charts from ambiguous natural language [3, 21]. [2] expanded this line of work to include multimodal LLMs for chart plotting. [25] tried to involve human feedback to refine the LLM generated plots via reinforcement learning. [26] proposed a framework to provide visual feedback to LLMs for iterative refinement. Our work is different from existing works as it explores the use of multimodal feedback via LLM self reflection to resolve errors related to numeric values, lexical labeling and visual aesthetics.\nLLM Agents: Recent years have seen a proliferation of frameworks that utilize Large Language Models (LLMs) to test their applications in practical scenarios [11, 15, 27, 28]. The development of OpenAgents [24] marked the introduction of an accessible platform that implements LLM-powered agents for daily use through three specialized components: Data Agent, Plugins Agent, and Web Agent. A groundbreaking simulation system that replicates human behavioral patterns was developed by [12], enabling software agents to computationally reproduce authentic human actions and interactions. In the gaming realm, Voyager [22] emerged as the pioneer LLM-controlled autonomous agent within Minecraft, engineered to continuously discover its surroundings, develop diverse abilities, and generate novel discoveries without human intervention. The software development sphere saw innovation through ChatDev [14],"}, {"title": "3 Methodology", "content": "Task Description: Given a user request $U$, specifying the visualization requirements including chart type, spatial arrangement of chart entities, and aesthetic preferences, along with a data table $D = \\{ d_{1,1}, d_{1,2}... d_{n,m} \\}$ as inputs, the scientific data visualization task should output a figure $F$. We propose PlotGen to automate this challenging task that orchestrates a multi-agent LLM framework consisting of five agents as follows:\n(1) Query Planning Agent: Query Planning Agent uses an LLM to breakdown a complex user request into a sequence of executable steps via chain-of-thought prompting. Each step in the thought chain corresponds to an explicit instruction to the code LLM responsible for generating the visualization. It highlights the programming language required and intermediate programming steps that specify the function calls, parameters, and return types. The output also instructs the code LLM on the data file and its format provided for visualization, along with the visual characteristics desired by the user.\n(2) Code Generation Agent: The Code Generation Agent is the most crucial component of the framework as it is responsible for generating an error-free executable code that can transform the user provided data into a scientific visualization as per user request. We use specific code LLMs such as GPT-3.5, GPT-4o for this purpose. However, code LLMs are prone to programming errors that may hinder the final plot generation. Hence, we include a self-reflection step in the code generation agent by iterating on the debugger error response in case of failed code execution. The debugger error message is passed as feedback to the code LLM to fix issues related to syntax, library imports, function arguments, and data formatting. To prevent an infinite loop of iteration over coding errors, we restrict the number of iterations of self-debugging.\nMultimodal Feedback: Humans tend to repeatedly refine their drafts to reach satisfactory outputs based on sensory feedback. We hypothesize that the code generation agent should also have the ability to receive external feedback to rectify mistakes during the plotting process to better fulfill the user\u2019s queries. Some of the errors may be difficult to diagnose in code but become apparent when observed visually through \"eyes\", reading out the textual phrases, or asking probing questions about the data points. To this end, we introduce Multimodal Feedback Agents that provide visual, lexical, and numerical feedback to the code agent for self-reflection and improving the final figure output.\n(3) Numeric Feedback Agent: The numeric agent is responsible for ensuring the accuracy of underlying data in the figure by ensuring all data rows and columns are appropriately plotted and the right kind of plot has been selected for visualization. Numeric feedback agent uses GPT-4V to de-render the draft figure to get back the underlying data from the plot. It then compares the de-rendered data with the original data set to check for any discrepancies between the two. If the figure has the correct plot, the two data collections should"}, {"title": "4 Experiments", "content": "Multimodal LLMs: We experiment with GPT-4V [1] for multi modal feedback agents.\nCode LLMs: For code generation agent in PlotGen, we experiment with both closed source (GPT-3.5, GPT-4) as well as open source LLMs (Magicoder-SDS-6.7B [23] and WizardCoder-Python33B V1.1 [9]) . We set the decoding temperature to 0 for all code LLMs and use their respective APIs for closed source LLMs.\nDataset: We use MatPlotBench [26] as benchmark dataset to assess the effectiveness of our proposed framework. This corpus contains 100 high-quality user queries and raw data mapped to ground-truth visualizations figures close to real-world scenarios.\nBaselines: We benchmark PlotGen against several strong baselines: (1) Direct Decoding: The LLM directly generates the code for rendering the visualization. (2) Zero-Shot Chain-of-thought prompting: The LLM is prompted to inference with the zero-shot chain of thought mechanism based on the user description. (3) MatPlotAgent: We benchmark against MatPlotAgent [26] which utilizes GPT-4 as code LLM and GPT-4V for visual feedback. We experiment all baselines with various code LLM backbones - GPT-3.5, GPT-4, Magicoder-SDS-6.7B [23], and WizardCoder-Python33B V1.1 [9].\nEvaluation: Following [26], we use LLM-as-a-judge automatic scoring metric between 0 to 100 to evaluate model-generated visualizations with corresponding ground-truth as a reference. The authors show that automatic evaluation scores provided by GPT-4V"}, {"title": "5 Results and Discussion", "content": "Performance Evaluation: Table 1 compares the performance of PlotGen with baseline methods on the MatPlotBench dataset. We observe that PlotGen significantly outperforms strong baselines across both open-source and closed source code LLMs. The direct decoding methods show degraded performance due to their inability to correct common plotting mistakes in text labels and visual appearance. We observe that naive zero-shot CoT mechanism does not significantly enhance the performance of code LLMs. While MatPlotAgent shows better performance, it struggles to handle complex charts that require fine-grained numerical accuracy and precise visual-textual alignment of chart labels with plot components. PlotGen effectively captures user requirements, translates them into error-free code, and reduces errors caused by the lack of multimodal perception in baseline methods. PlotGen shows best performance with GPT-4 backbone, where inclusion of lexical, numerical, and visual feedback helps it to recover from various concurrent errors during the plotting process, demonstrating the effectiveness of our multi-agent approach.\nAblation Study: We conduct an ablation study to verify the effectiveness of each feedback agent in PlotGen. Ablation of the Lexical Feedback agent causes 5-7% performance deterioration as it is unable to iteratively refine textual details such as titles, subtitles, tick labels, and legend text, which is crucial for clarity in data interpretation. The Numerical Feedback agent helps reduce ambiguity in data plotting by verifying the underlying data trends and corresponding chart types. Its usefulness is particularly pronounced in co-located charts with complex legend hierarchies that are prone to mispredictions due to LLM hallucinations. We observed a severe performance drop (10-15%) across all code LLMs in the absence of the Visual Feedback agent, which plays a critical role in maintaining visual aesthetic quality as per user requirements. Without the visual feedback, PlotGen runs into common plotting mistakes related to color schemes, scaling, layout, tick labels and legend mapping.\nUser Evaluation: Five participants evaluated 200 randomly sampled user requests from the MatPlotBench dataset to study the usefulness and accuracy of the plots generated by PlotGen. The evaluation results demonstrated strong positive reception, with participants rating the attributions as Completely Accurate (40.5%) or Somewhat Accurate (24.5%) for natural language based plot generation in PlotGen. Plots were found to be more \"Completely Inaccurate\" in 10.6% of the cases. Participants described the outputs useful in reducing time spent debugging plotting errors."}, {"title": "6 Conclusion", "content": "In this paper, we introduced Plot Gen, a novel multi-agent framework designed to automate the process of scientific data visualization through a multimodal self-reflection. By coordinating multiple LLM-based agents responsible for query planning, code generation, and multimodal feedback, PlotGen addresses the challenges faced by novice users in generating accurate visualizations according to user preferences. PlotGen significantly improves the quality of LLM-generated plots by 10-12% compared to strong baselines, enhancing user trust, reducing debugging time, and improving accessibility. Future work will extend Plot Gen beyond traditional table data visualization to explore its application in real-time environments such as interactive dashboards, Virtual Reality simulations and visual arts."}]}