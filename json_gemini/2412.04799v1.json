{"title": "Estimating the treatment effect over time under general interference through deep learner integrated TMLE", "authors": ["Suhan Guo", "Furao Shen", "Ni Li"], "abstract": "Understanding the effects of quarantine policies in populations with underlying social networks is crucial for public health, yet most causal inference methods fail here due to their assumption of independent individuals. We introduce DeepNetTMLE, a deep-learning-enhanced Targeted Maximum Likelihood Estimation (TMLE) method designed to estimate time-sensitive treatment effects in observational data. DeepNetTMLE mitigates bias from time-varying confounders under general interference by incorporating a temporal module and domain adversarial training to build intervention-invariant representations. This process removes associations between current treatments and historical variables, while the targeting step maintains the bias-variance trade-off, enhancing the reliability of counterfactual predictions. Using simulations of a \"Susceptible-Infected-Recovered\" model with varied quarantine coverages, we show that DeepNetTMLE achieves lower bias and more precise confidence intervals in counterfactual estimates, enabling optimal quarantine recommendations within budget constraints, surpassing state-of-the-art methods.", "sections": [{"title": "I. INTRODUCTION", "content": "To mitigate the spread of infectious diseases, public health decision-makers face a challenging health-economy trade-off [1]: balancing strict shelter-in-place policies to prevent outbreaks with the economic costs and societal impacts of such measures. Accurate estimation of the effects of varying quarantine coverages is therefore crucial, as it enables policymakers to design effective strategies that minimize both health risks and economic disruption. While randomized controlled trials (RCTs) are the gold standard for causal inference, they are often costly, time-consuming, and, in some cases, impractical, especially for interventions applied at the network connectivity [2]. Leveraging the wealth of social network data and related health information from observational studies offers a feasible and scalable alternative for estimating these treatment effects in real-world conditions.\nA substantial body of work [3]\u2013[7] has been developed under the Stable Unit Treatment Value Assumption (SUTVA) [8], which assumes that an individual's potential outcome is unaffected by the treatment status of their neighbors. However, this assumption breaks down in social network settings, where the interactions between connected individuals are central to the intervention itself. Some methods have been introduced to address network interference, though they primarily focus on static settings [9]\u2013[11]. However, single-time-point interventions may not always reflect real-world conditions, as interventions often evolve alongside the progression of an epidemic. For instance, a shelter-in-place order might be lifted after the disease's incubation period if no new cases are detected. These time-varying interventions introduce time-varying confounding, where patients' histories\u2014including their covariates and responses to past interventions-inform future intervention decisions. Addressing these challenges requires models that can handle both network interference and time-varying interventions to assess intervention effectiveness in real-world, dynamic scenarios accurately [12]. Estimating treatment effects across varying network connectivities over time presents valuable opportunities, such as optimizing quarantine effectiveness, assessing how network size influences treatment impact, and identifying optimal coverage levels and timing for quarantine orders. These insights can enable decision-makers to craft more effective, data-driven policies that optimize health outcomes and resource allocation.\nThe biggest challenge when estimating the effects of intervention on the network involves correctly handling the time-dependent network intervention through the change in treatment [13]. As illustrated in Figure 1, if a model only accounts for the last time step, it may miss critical historical connections and mistakenly attribute observed outcomes to current network structures rather than to previous exposures. For instance, consider individual A, a node with no immediate infected neighbors in the current time step. However, data from earlier time steps reveal that individual A was previously connected to multiple infected nodes and became infected before these connections were removed through quarantine. Lacking this historical context, we might incorrectly infer that the intervention in network structure had a harmful effect. Existing methods for causal inference on network structure interventions, typically designed for static settings, are inadequate for such longitudinal contexts. These methods rely on single-time-point intervention setups and observational data limited to the last time step, making them ill-suited for estimating counterfactual outcomes where epidemic progression and temporal dependencies are key factors. To effectively capture epidemic dynamics and enable accurate counterfactual prediction, causal inference methods designed for network interference must be adapted to account for longitudinal data.\nTime-varying interventions are inherently present in observational data, as public health officials implement mitigation strategies that adapt to the evolving threat of public health hazards [14]. However, these policies embedded in the observational data can introduce significant bias to deep learning models, diminishing their ability to estimate counterfactuals under alternative intervention policies accurately. Addressing these challenges requires developing robust models that can account for both the temporal dependencies and networked nature of interventions, ultimately improving the reliability of intervention-effect estimates and informing public health decision-making.\nTo address time-varying confounding, traditional methods such as Marginal Structural Models (MSMs) [15], [16] use the inverse probability of treatment weighting (IPTW), creating a pseudo-population that removes dependencies between treatment probabilities and time-varying confounders. However, MSMs can yield poor estimates if IPTW contains extreme weights or the model is misspecified. Machine learning approaches like R-MSN [17], CRN [18], and Causal Transformer [19] apply recurrent neural networks or transformer models to manage temporal correlations, but they assume no interference among individuals and focus on sequence prediction of treatment effects, which is not our primary focus. Targeted Maximum Likelihood Estimation (TMLE), adapted for social network data, introduces a targeting step that optimizes the bias-variance tradeoff for the parameter, ensuring doubly robust estimation for integrating machine learning techniques [20]. There have also been attempts to extend the TMLE method with transformer models to handle the longitudinal setting, but their major focus is to facilitate statistical inference in longitudinal settings concerning survival outcomes under dynamic interventions [21].\nWe introduce DeepNetTMLE, a novel deep learning-based Targeted Maximum Likelihood Estimation (TMLE) approach designed to estimate treatment effects with time-dependent covariates under conditions of general interference. Our main contributions are as follows:\nCounterfactual Estimation of Social Network Connec-tivity Interventions. To estimate counterfactual outcomes for Non-Pharmaceutical Interventions (NPIs), such as quarantine orders issued during pandemics to block human-to-human transmission, we integrate deep learning networks with the well-established TMLE method, preserving its statistical rigor in determining causal effects from observational data. TMLE typically involves four steps: estimating the outcome model, estimating weights, targeting, and estimating the causal effect. DeepNetTMLE innovates by replacing the outcome model with a deep learner and refining the targeting step to en-hance prediction accuracy. Through counterfactual estimation of intervention outcomes, DeepNetTMLE can address critical public health questions, such as determining the most cost-effective quarantine coverage and optimizing quarantine bud-gets, without tracking the initially infected individuals.\nDomain-Invariant Representations Over Time. Deep-NetTMLE constructs intervention-invariant representations throughout the progression of infectious diseases, effectively breaking the association between historical network connectiv-ity and intervention assignments to eliminate bias from time-dependent confounders. Using domain adversarial training [22]\u2013[24], DeepNetTMLE trains the TMLE outcome model to create domain-invariant representations for counterfactual estimation. Experimental results demonstrate that these rep-resentations successfully mitigate bias from time-varying con-founders, enabling reliable counterfactual outcome estimation. This work is the first to apply domain adaptation for treatment effect estimation under general interference, presenting a novel approach to handling time-varying confounders in this context.\nDisease-Agnostic Counterfactual Data Sampling. Es-timating counterfactuals typically requires a counterfactual dataset, which is often unattainable in real-world scenarios. For DeepNetTMLE, we developed a data generation mechanism that relies solely on the probability of exposure as input, allowing the network to infer disease-related information di-rectly from observational data. This capability enables Deep-NetTMLE to be applied to emerging infectious diseases using only observational data, without requiring detailed disease-specific information that may not be readily available.\nIn our experiments, we evaluate DeepNetTMLE in a realis-tic setup using a Susceptible-Infected-Recovered (SIR) model to simulate the epidemic dynamics modified from [11]. We demonstrate that DeepNetTMLE outperforms current state-of-the-art methods in predicting counterfactual outcomes, espe-cially as interventions become more stringent. Such causal effects are well-suited for testing in simulation environments, given the high financial costs of implementing these interven-tions in real-world scenarios."}, {"title": "II. RELATED WORKS", "content": "Two primary techniques for estimating the average treatment effect (ATE) in causal inference are G-computation [25] and Inverse Probability Weighting (IPW) [15]. However, both G-computation and IPW estimators require the correct model specifications to yield consistent estimates and lack theo-retical guarantees for normal distribution, making variance estimation for confidence intervals challenging when using flexible machine-learning models. In contrast, Targeted Maxi-mum Likelihood Estimation (TMLE) leverages asymptotically linear estimation and the efficient influence function (EIF) to construct an estimator with optimal statistical properties. This approach ensures that the final estimator is consistent if either the outcome model or the intervention probability model is correctly specified, achieving double robustness [20].\nWhen the potential outcome of one individual is influ-enced by another's treatment assignment, the standard no-interference assumption in most causal inference methods is violated. A most prominent example would be the vaccination for infectious diseases, which creates an indirect effect of treatment, defined as the treatment's effect on connected individuals, in addition to the direct effect, defined as the effect of the treatment on his/her own outcome [26]. Using contagion as the motivating example, works have been developed to establish the theoretical ground of assessing causal effects for social network data under weak dependence assumption [27], [28]. TMLE has been proposed to estimate the doubly robust causal inference under general interference in obser-vational studies [29], [30]. Maximum likelihood estimation seeks to minimize a global measure (e.g., mean squared error), while the targeting step is introduced to achieve an optimal bias-variance trade-off. TMLE has been extended to various contexts, including randomized trials [31], stochastic interventions for a single time-point observational data [32], complex observational longitudinal studies [33], and social network observational data [11]."}, {"title": "B. Treatment Effect Over Time", "content": "The need to estimate the average effects of time-varying treatments originated in epidemiology, with early methods like G-computation, marginal structural models (MSMs), and structural nested models addressing this demand [15], [25], [34]. Since linear regression approaches struggle with complex temporal dependencies, Bayesian non-parametric methods are proposed [35]\u2013[37], which are not compatible with multi-dimensional outcomes and static covariates.\nIn the potential outcomes framework, methods such as recurrent marginal structural networks (RMSNs) [17], counter-factual recurrent network (CRN) [18], G-Net [38], and Causal Transformer [19] are introduced to handle biases from time-varying confounders. RMSNs enhance MSMs with recurrent neural networks to estimate the inverse probability of treat-ment weighting (IPTW) for forecasting treatment responses. CRN reduces the link between patient covariate history and treatment assignment by generating balanced representations that only predict outcomes. G-Net applies g-computation with LSTMs to predict counterfactual outcomes of time-varying treatments under alternative dynamic strategies. Causal Trans-former uses the transformer architecture to model time depen-dencies, employing a counterfactual domain confusion loss to correct for biases from time-varying confounders. There have also been efforts to extend TMLE with transformer models to address longitudinal settings. However, the primary focus of these works has been to support statistical inference for survival outcomes under dynamic interventions, without considering interference [21]."}, {"title": "C. Intervention-Invariant Representations for Treatment Effect Estimation", "content": "Considerable research has been dedicated to obtaining distribution-invariant representations for control and treatment groups to estimate counterfactual outcomes [4], [39]\u2013[43]. Most of them focus on static settings, and some have adapted deep learning algorithms to the estimation [5], [44]. Works including CRN and Causal Transformer, create temporally balanced representations to mitigate bias and estimate counter-factual outcomes in sequential treatment scenarios. However, to develop intervention-invariant representations under general interference and time-varying confounding, sequential gener-ation of treatment and outcome data poses challenges. This is because treatment updates may not occur at every step, and outcome data is typically available only at the final time step.\nDomain adaption is the process of transferring a model's knowledge to a different yet related data distribution. Typically, a model is trained on a source dataset and applied to an unlabeled target dataset. Domain align-ment is generally achieved through one of three pri-mary approaches: discrepancy-based, adversarial-based, and reconstruction-based methods [45]. Discrepancy-based meth-ods seek to minimize divergence by measuring divergence or distance between the distributions. Reconstruction-based methods are first proposed by Ghifary et al. [46] that a rep-resentation is learned to accurately classify the labeled source data and reconstruct source and target data. Adversarial-based methods use adversarial techniques to align distributions by extracting features that effectively classify labeled source data but appear indistinguishable between source and target domains. This category includes state-of-the-art approaches like DANN [22] and ADDA [47]. DANN employs a gradi-ent reversal layer to extract domain-invariant features, while ADDA uses adversarial adaptation with a GAN loss to align a pre-trained source classifier with the target distribution. To prevent the negative transfer, multi-adversarial domain adaptation (MADA) extended the domain discriminator to each class to reduce the negative transform and prevent the wrong classification [48]. Dual Adversarial Domain Adapta-tion (DADA) simultaneously performs domain-level and class-level alignment using a common discriminator [49]."}, {"title": "III. PROBLEM DEFINITION", "content": "An adjacency matrix $G = (V, E)$ is used to represent the social network of the population of interest, where $|V| = N$ represents the $N$ individuals and $|E| = M$ denotes $M$ edges between individual pairs. Unlike the typical adjacency matrix, a self-loop is excluded from $G$ because interference is considered only between different individuals. $G_{ij} = 1$ indicates that there is a connection between individual i and j, whereas $G_{ij} = 0$ denotes the absence of connection. Given that the network structure is dynamic, we define the network over time as $G(t) = [\\gamma(0), ..., \\gamma(T)]$.\nWe denote the observed outcome for individual i over T time steps as $[v_i(0), ..., v_i(T)]$. To account for interference, both an individual's own exposure and the exposure of others in the population must be considered. The exposure for an individual is defined as $A_i(t) = [\\alpha_i(0),...\\alpha_i(T)]$, and the exposure of others is represented as a summary measure from the immediate contacts according to G, given by:\n$A_{-i}(t) = [\\alpha_{-i}(0),...,\\alpha_{-i}(T)] = [\\sum_{j=1}^{N}I(\\alpha_j(0)=1)G_{ij}(0),...,\\sum_{j=1}^{N}I(\\alpha_j(T)=1)G_{ij}(T)]$\nFor each individual i, the baseline covariates are represented by $X_i = [\\xi_i(0), ..., \\xi_i(T)]$, capturing both static and dynamic characteristics. The static covariates remain constant across time, while dynamic covariates are updated at regular intervals to reflect any changes over time. We further define $X = [\\xi(0),...,\\xi(T)]$ as the summary covariates corresponding to the baseline covariates. These summary covariates are recalibrated at each time step to ensure they remain aligned with the ground truth quarantine measure. To avoid further complicating the notation, we drop the temporal dimension in the following illustration unless otherwise mentioned.\nLet $Y_i(a_i, a_{-i})$ represent the potential outcome for individ-ual i, where $a_i$ denotes the exposure of individual i, and $a_{-i}$ refers to the exposure statuses of all other individuals in the population, excluding individual i. In this model, we assume that the exposure statuses from an individual's immediate contacts are sufficient to describe the interference effects on their potential outcome. Therefore, we simplify the potential outcome to $Y_i(a_i,a')$, where $a_i \\in A = {0,1}$ denotes the treatment assignment (e.g., whether an individual is exposed to a treatment or intervention), and $a' \\in A^s$ captures the summary of the exposures from the immediate contacts of individual i, reflecting the indirect treatment effects. This weak dependence assumption allows us to model the problem in a more tractable way, where the exposure status of an individ-ual's contacts $a'$ is used to characterize the interference effects. The goal of this study is to estimate the average outcome under a given policy $w$ for the population. Specifically, we aim to estimate the following causal quantity:\n$\\psi = \\frac{1}{N} \\sum_{i=1}^{N}E[Y_i(a_i, a') Pr^*(A_i=a_i, A_{-i}=a'|X_i, X, G)]$,\nwhere $Pr^*$ denotes the probability under policy $w$, and this probability must be estimated based on available data, as it is not directly observed. The expression reflects the expectation of the potential outcome $Y_i(a_i, a')$ under the intervention (or treatment) $a_i$, weighted by the probability of the exposure assignment $a_i$ and the associated exposures $a'$, conditional on the individual covariates $X_i$, the summary covariates $X$, and the network structure G.\nTo ensure the causality in the estimation of $\\psi$, three as-sumptions are made, including causal consistency, conditional exchangeability, and positivity. Details can be found in Sup-plementary Section I.\nIn a real-world setting, when estimating the average out-come $\\psi$ under policy $w$, we lack access to the counterfactual ground truth. This absence prevents us from using counter-factual information to directly fit the model, as it remains unobservable outside of simulations. However, in our simu-lation experiments, we can observe the counterfactual ground truth. To realistically reflect practical application constraints, we exclude this ground truth data during the training phase of the model and instead, reserve it solely for evaluating the performance of our proposed algorithm.\nTo assess the performance, we define the following metrics:\nBias: The mean difference between the estimated outcome and the ground truth across U simulation processes, calculated as:\n$Bias = \\frac{1}{U} \\sum_{k=1}^{U} (\\psi_k - \\hat{\\psi}_k)$.\nEmpirical Standard Error (ESE): The standard deviation of the simulation estimates for each policy. Let $b_k = \\hat{\\psi}_k - \\psi_k$, ESE is defined as:\n$ESE = \\sqrt{\\frac{1}{U} \\sum_{k=1}^{U} (b_k - Bias)^2}$.\n95% Confidence Interval (CI) Coverage: The proportion of 95% CIs containing the true mean of the outcome. Defining the lower and upper bound of CI as LCI and UCI, we have,\n$LCI = \\hat{\\psi}_k - \\frac{\\sigma}{ \\sqrt{N}} z_{1-\\frac{\\alpha}{2}}$,\n$UCI = \\hat{\\psi}_k + \\frac{\\sigma}{ \\sqrt{N}} z_{1-\\frac{\\alpha}{2}}$,\nwhere $\\sigma^2$ is the estimated variance, The $z_{1-\\frac{\\alpha}{2}}$ is the $1-\\frac{\\alpha}{2}$ quantile of a standard normal distribution, and $\\alpha = 0.05$ for 95% CI. The 95% CI coverage is then calculated as:\n$Cover_k = \\begin{cases} 1, & \\text{LCI} < \\psi \\text{ and UCI} > \\psi \\\\ 0, & \\text{Otherwise}, \\end{cases}$,\n$Cover = \\frac{1}{U} \\sum_{k=1}^{U} Cover_k$.\nIn this evaluation framework, a smaller Bias and ESE indicate a more accurate and precise estimation of the average outcome, while a higher 95% CI coverage suggests that the confidence intervals are capturing the true mean of the outcome more consistently, reflecting a reliable estimation process. Details on the variance estimation methodology are provided in Section IV-E."}, {"title": "IV. DEEPNETTMLE", "content": "To estimate $\\psi$, the DLNetworkTMLE consists of five steps:\n1) Estimate the outcome model $f(\u00b7)$\n2) Estimate Inverse Probability of Treatment Weighting (IPTW) with exposure model $g(\u00b7)$ and summary expo-sure model $h(\u00b7)$\n3) Estimate the correcting intercept $\\epsilon$, also known as \u201ctar-geting\u201d\n4) Estimate the $\\psi$ using Monte Carlo approach\n5) Estimate the direct variance $\\hat{\\sigma_d}$ and latent variance $\\hat{\\sigma_l}$\nIn this approach, models estimated from the observed data are denoted by $f(\u00b7), g(\u00b7), h(\u00b7)$, while those estimated with Monte Carlo-sampled data are denoted with an asterisk (e.g., $f^*(\u00b7), g^*(\u00b7), h^*(\u00b7)$). This five-step estimation process corre-sponds to a single simulation, and for simplicity, the subscript k, denoting a specific simulation iteration, is omitted."}, {"title": "A. Estimate the outcome model", "content": "The outcome model in this framework is designed to predict the outcome variable based on exposure variables and covari-ates that reflect temporal dynamics in population exposure. For the deep learning extension of networkTMLE, exposures and covariates across selected time steps are incorporated into the model, enabling it to capture changes in exposure across the population over time. This is expressed as:\n$E [Y_i(t) | A_i(t), A_{-i}(t), X_i, X, G] = f(A_i(t), A_{-i}(t), X_i, X, G)$,\nwhere the function $f(\u00b7)$ represents the model that estimates the expected outcome $Y_i(t)$ given individual and summary exposures, covariates, and network structure.\nThe first step involves estimating this outcome model using observed data, which we consider as a representative sample from the broader network super-population. Traditional mod-els, such as the Generalized Linear Model (GLM), struggle to handle temporal dependencies. Therefore, in the absence of a deep learning model, only data from the final time step is used to estimate the outcome $Y_i(T)$. To ensure compatibility with the GLM approach, we retain only the final estimate, denoted as $\\hat{Y_i}(T) = c(f(\u00b7))$. The model obtained here will then serve as a key contributor in Step 4 of the estimation procedure."}, {"title": "B. Estimate IPTW", "content": "The Inverse Probability of Treatment Weighting (IPTW) weights are introduced to account for dependencies among individuals within the networked data, as outlined in semi-parametric estimation and targeted maximum likelihood meth-ods. The weights are structured to balance observed and sampled data through a decomposition into a numerator and denominator, which are fitted on sampled data and observed data, respectively. This is shown as:\n$W_i = \\frac{c(g^*(X_i(t), X(t)))h^*(\\xi_i(T), \\xi(T))}{c(g(X_i(t), X(t)))h(\\xi_i(T), \\xi(T))}$.\nIn the exposure model, we could potentially utilize a deep learning framework to capture temporal dependencies within the data, and, accordingly, we incorporate a reservation func-tion $c(\u00b7)$ at the beginning of the model to account for this. However, in the current study, we have opted to use a Generalized Linear Model (GLM) with a Gaussian distribution as the exposure model. This choice allows us to focus on examining the effects contributed specifically by the deep learning model in the outcome estimation, isolating its impact without additional complexity from a deep learning-based exposure model.\nThe specific form of the summary exposure $A'$ is deter-mined by the choice of summary measure $S$. For instance, if $S$ represents summation, $A'$ becomes a count variable indicating the exposure among immediate contacts. This structure intro-duces a class imbalance in the sampled data due to the range of possible values $A'$ can take, which grows incrementally. Consequently, we employ a Generalized Linear Model (GLM) with a Poisson distribution to estimate $A(T)$ rather than using deep learning models, which may be less effective given the class imbalance. In this framework, the sampled data refers to M copies generated to represent random draws from a broader super-population, indexed by $l = 1,..., M$. Starting from the initial time step $t = 0$, the generation process begins by sampling the exposure variable $A_i(1)$ based on a policy-specified probability $Pr_w$ from distribution $Bern(Pr_w(A_i(1) = 1|X(0), X(0)) = p_w)$. The sampled summary exposure measure for each time step is then recal-culated as $A'_{-i}(1) = \\sum_{j=1}^{N}I(A^l_1 = 1)G(0)$ reflecting the exposure from immediate contacts. For each copy, this sampled data is generated sequentially across time steps, resulting in a final dataset that matches the observed data regarding the number of time steps. Beyond generating exposure variables, other data elements within each simulated policy are created following the same approach outlined in Supplementary Section II. Importantly, the true outcome and exposure models, as well as the identities of initially exposed individuals, are hidden in the process to prevent information leakage that could bias the model evaluation."}, {"title": "C. Targeting", "content": "The targeting step aims to estimate the offset $\\epsilon_i$, which measures the discrepancy between the estimated outcome and the true outcome, acting as a correcting intercept. This offset is obtained by fitting a logistic regression model using weighted maximum likelihood with the estimated Inverse Probability of Treatment Weighting (IPTW) weight $W_i$, shown as:\n$logit(Pr(Y_i = 1)) = \\epsilon_i + logit(\\hat{Y_i})$.\nWhen deep learning models are used for outcome estimation, predicted outcomes $\\hat{Y}$ are often dichotomized to 0s and 1s by the sigmoid(\u00b7). This transformation can lead to extreme values of $\\epsilon_i$, which are implausible. Therefore, for deep learning models, $\\hat{Y_i}$ values are clipped within the range (0.05,0.95), and any resulting out-of-bounds $\\epsilon_i$ values are corrected to 0 with a threshold set to 10."}, {"title": "D. Estimate the average outcome", "content": "The average outcome $\\psi$ should be calculated using the Monte Carlo approach. Hence, the previously sampled data $A_i^l(t)$ and $A'_{-i}^l(t)$ for $l = 1,..., M$ can be reused to predict the outcome $\\hat{Y_i^l}(t)$ under policy w. Utilizing the targeting offset $\\epsilon_i$ as the correction, we define the procedure as follows:\n$\\hat{Y_i^l}(T) = c(f(A_i^l(t), A'_{-i}^l(t), X_i^l, X) + \\epsilon_i)$,\n$\\hat{\\psi} = \\frac{1}{MN} \\sum_{l=1}^{M} \\sum_{i=1}^{N} \\hat{Y_i^l}(T)$.\nHere, $\\hat{Y_i^l}(T)$ represents the corrected predicted outcome. The network modules in our proposed methods are com-posed of linear layers, forming the foundational architecture. By incorporating Multi-Layer Perceptrons (MLPs) into dif-ferent modules within the network, we aim to empirically demonstrate that deep learning models are effective outcome estimators. Additionally, this setup highlights the flexibility of our framework, allowing more specialized network architec-tures, such as graph neural networks (GNNs), to be integrated seamlessly. This adaptability suggests that our framework can leverage the strengths of various neural network structures depending on the context and data requirements. The network structure is illustrated in Figure 2.\nDynamic treatment strategies across an observational pe-riod introduce time-varying confounding, which can bias out-come estimation. To address this, we construct a treatment-invariant representation for each individual, denoted as $K_i(t) = [\\kappa_i(0),..., \\kappa_i(T)]$, where $Pr(k_i(t)|a_i(t) = 0) = Pr(k_i(t)|a_i(t) = 1)$, for each t\u2208 [0,T]. Given access to potential outcomes at each time step, an optimal approach generates $\\kappa_i(t = Q)$ for each specific time Q, necessitating a sequential design for the network. A significant advantage of sequential networks, particularly those with an encoder-decoder structure, is their capacity to generate multi-step-ahead outcome predictions based on treatment plans. This approach, as used in prior studies, enables the network to an-ticipate outcomes across multiple time steps, enhancing the ro-bustness of the outcome estimation given dynamic treatments [18], [19]. For our study, the outcome inference is a point estimate, and the primary outcome of interest is the cumulative infliction of infections throughout the endemic period T. This cumulative outcome is not available at intermediate time steps t\u2208 [0, T-1]. Hence, the sequential networks are less optimal due to their slow inference speed. To overcome this limitation, instead of treating the different treatments at each time step as distinct domains, we focus on constructing intervention-invariant representations at the final time step by ensuring that $Pr(K_i(T)|A_i(T) = 0) = Pr(K_i(T)|A_i(T) = 1)$. Here, $K_i(T)$ and $A_i(T)$ denotes that the feature is estimated with an intervention implemented over the entire endemic span. To generate $K_i(T)$, we introduce a temporal module that consists of linear projection layers organized in a U-net style. This module maps the temporal information from the input feature $S_i$ to the desired output dimension. The transformation is described as follows:\n$K_i(T) = f_t(S_i(T); \\theta_t)$.\nIf the temporal span of the input data is equal to 1, this tem-poral module simplifies to the identity function. The backbone of the network is the feature extraction module, composed of linear layers designed to project features to a target dimension. This module first processes input features and then passes them to the temporal module to aggregate intervention effects along with baseline covariates, ensuring that both the direct and indirect exposures are effectively captured in the final representation. The feature extraction is expressed as:\n$S_i(T) = f_b(X_i(T), X(T), A_i(T), A(T); \\theta_b)$.\nPresuming that the distribution for different interventions $A_i(T) = 0$ and $A_i(T) = 1$ to be similar but shifted, we would like the downstream structure to be able to distinguish outcomes for the observed intervention while failing to dis-tinguish between samples from observational and counterfac-tual domains. By organizing the loss function to minimize outcome classification error while maximizing intervention classification error, the model encourages the representations to encode outcome-related features without explicitly encoding intervention specifics. The prediction for outcome $v_i(T)$ and intervention $\\delta_i(T)$ are defined as:\n$v_i(T) = f_y(f_t(f_b(X_i(T), X(T), A_i(T), A(T); \\theta_b); \\theta_t); \\theta_y)$,\n$\\delta_i(T) = f_a(f_t(f_b(X_i(T), X(T), A_i(T), A(T); \\theta_b); \\theta_t); \\theta_a)$.\nThis architecture seeks to isolate outcome features while discouraging intervention-related biases. The loss function is composed of two parts, the outcome loss $L_y$ and the intervention loss $L_a$, i.e.,\n$L_y(\\theta, \\theta_t, \\theta_y) = -(v_i(T)log(\\hat{v}_i(T)) + (1 - v_i(T))log(1 - \\hat{v}_i(T)))$,\n$L_a(\\theta, \\theta_t, \\theta_a) = -(a_i(T)log(\\hat{a}_i(T)) + (1 - a_i(T))log(1 - \\hat{a}_i(T)))$,\n$L = \\sum_{i=1}^{N} L_y(\\theta, \\theta_t, \\theta_y) - \\lambda L_a(\\theta_i, \\theta_t, \\theta_a)$,\nwhere the hyperparameter $\\lambda$ controls the balance between outcome prediction and intervention invariance, increasing exponentially during training to promote this invariance over time [50]. By training with Eq. (19), the model seeks the saddle point:\n$(\\hat{\\theta_b}, \\hat{\\theta_t}, \\hat{\\theta_y}) = arg \\min L(\\theta_b, \\theta_t, \\theta_y, \\theta_a)$,\n$\\hat{\\theta_a} = arg \\max L(\\theta_b, \\theta_t, \\theta_y, \\theta_a)$.\nTo achieve this with backpropagation, we employ a gradient reversal layer (GRL) between the temporal and intervention modules. The GRL behaves as an identity during forward propagation but reverses the gradient sign in backward propa-gation by multiplying it by \u2013 $\\lambda$. This allows the model to reach the saddle point using standard backpropagation and stochastic gradient descent (SGD) adapted optimizers, effectively disen-tangling outcome and intervention representations within the network without having to reinvent optimizers."}, {"title": "E. Estimate the variance", "content": "Two types of variance\u2014direct and latent\u2014are estimated to account for different levels of dependence among observations:\nThe direct variance is short for the direct transmission vari-ance, which assumes that dependence between observations is fully captured by the measured covariates of immediate contacts, shown as:\n$\\hat{\\sigma^2_d} = \\frac{1}{N} \\sum_{i=1}^{N} (W_i * (Y_i - \\hat{Y}_i))^2$.\nOn the other hand, the latent variance expands the dependence between observations to the second-order contacts, defined as the immediate neighbors of one's immediate neighbors. The latent variance is estimated by:\n$\\hat{\\sigma^2_l} = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{N} G_{ij}(W_i * (Y_i - \\hat{Y}_i) * W_j * (Y_j - \\hat{Y}_j))$,\nwhere $G_{ij}$ denotes the connectivity between node i and node j and when i = j, $G_{ij} = 1$. Incorporating latent variance to account for second-degree network dependencies indeed provides a more realistic representation of infectious disease transmission dynamics, as it captures the influence not only of direct contacts but also of indirect interactions within a network. This approach likely improves the robustness of estimations by reflecting the more complex, layered nature of transmission paths in real-world settings."}, {"title": "V. EXPERIMENTS WITH QUARANTINE SIMULATION MODEL", "content": "The experiments are conducted using simulated datasets for infectious diseases. The simulation entails a compart-ment model, namely the Susceptible-Infected-Recovered (SIR) model, which describes the human-to-human transmission of the infectious agents. The hypothetical exposure measure quarantine compliance represented a \u201cleaky\u201d model, such that the quarantine reduces the probability of infection given a single exposure to an infectious agent. The spillover effect of quarantine is composed of the contagion effect, which means quarantined individuals are less likely to become infected and thus less likely to transmit, and"}]}