{"title": "Energy consumption of code small language models serving with runtime engines and execution providers", "authors": ["Francisco Dur\u00e1n", "Matias Martinez", "Patricia Lago", "Silverio Mart\u00ednez-Fern\u00e1ndez"], "abstract": "Background. The rapid growth of Language Models (LMs), particularly in code generation, requires substantial computational resources, raising concerns about energy consumption and environmental impact. Optimizing LMs inference for energy efficiency is crucial, and Small Language Models (SLMs) offer a promising solution to reduce resource demands.\nAim. Our goal is to analyze the impact of deep learning runtime engines and execution providers on energy consumption, execution time, and computing-resource utilization from the point of view of software engineers conducting inference in the context of code SLMs.\nMethod. We conducted a technology-oriented, multi-stage experimental pipeline using twelve code generation SLMS to investigate energy consumption, execution time, and computing-resource utilization across the configurations.\nResults. Significant differences emerged across configurations. CUDA execution provider configurations outperformed CPU execution provider configurations in both energy consumption and execution time. Among the configurations, TORCH paired with CUDA demonstrated the greatest energy efficiency, achieving energy savings from 37.99% up to 89.16% compared to other serving configurations. Similarly, optimized runtime engines like ONNX with the CPU execution provider achieved from 8.98% up to 72.04% energy savings within CPU-based configurations. Also, TORCH paired with CUDA exhibited efficient computing-resource utilization.\nConclusions. Serving configuration choice significantly impacts energy efficiency. While further research is needed, we recommend the above configurations best suited to software engineers' requirements for enhancing serving efficiency in energy and performance.", "sections": [{"title": "1. Introduction", "content": "The widespread adoption of deep learning models, particularly Language Models (LMs), has surged remarkably in recent years. This trend has led to an exponential increase in the computational resources needed to support these models [1], raising concerns such as environmental impacts [2]. The focus within the deep learning community has predominantly been on enhancing model accuracy, often overlooking the associated costs. For instance, Benchmarks like GLUE or SuperGLUE encourage competition based on accuracy, often without much consideration for reducing energy consumption [3]. State-of-the-art models like GPT-4 and Claude 2 demand significant computational resources, primarily accessible to companies like OpenAI, Anthropic, or Google, leading to a large environmental footprint [4]. Notably, Schwartz et al. [5] emphasized the need for deep learning research to prioritize efficiency, advocating for a balance where improvements in accuracy are not achieved at the expense of a larger carbon footprint.\nThe escalating computational demand of deep learning models, has led to an increase in research focused on improving the sustainability of these systems [6, 7]. Although the majority of this research targets the training phase, a growing body of work acknowledges the significance of both training and inference in achieving a greener deep learning [8, 9]. However, there remains a noticeable gap in studies dedicated to the inference phase[10]. This oversight is important as inference can also contribute substantially to the overall energy footprint of deep learning systems, especially when deployed at scale [11]. Addressing this gap is crucial to reduce overall environmental impact and encourage the development of energy-efficient deep learning systems.\nDuring inference, trained models are integrated into deep learning systems to allow users to make predictions, a process known as deep learning serving [12, 13]. Critical and popular architectural decisions in deep learning serving include the selection of serving infrastructure, with the runtime engine being one of the most commonly used options, and the choice of an execution provider. These decisions are driven by the community's focus on optimizing performance and simplifying deployment to handle growing deep learning systems' complexity [14, 15]. The runtime engine is a software component that loads the model, performs inference, and returns results to the user. It can also apply optimizations, such as graph optimizations or just-in-time compilation [16]. The execution provider, which is the library that serves as the backend of the runtime engine, optimizes model inference based on the available hardware [17, 18]. These two key decisions are able to significantly affect the energy consumption, execution time, and computing-resource utilization of the deep learning system [16].\nIn this paper, we investigate the energy consumption of different configurations of deep learning serving infrastructures, focusing on runtime engines and execution providers. We concentrate on one particular deep learning task, specifically code generation, as it is one of the most utilized tasks of LMs and has a potential impact on software development productivity [19]. Code LMs have demonstrated impressive capabilities in code generation [20]. Their success largely stems from the Transformer architecture, which excels at learning complex patterns and scales efficiently [21, 22]. This has enabled state-of-the-art performance in models like Codex [23], the backbone of GitHub Copilot\u00b9, a tool that provides intelligent code suggestions to developers.\nThese advanced code LMs, however, often contain millions to billions of parameters [20, 24], requiring vast amounts of training data and substantial computational resources. In contrast, Small Language Models (SLMs) have emerged as a promising alternative, offering similar capabilities but with fewer computational demands, making them accessible to individual developers. Given their efficiency and lower resource requirements, our research focuses specifically on SLMs for code generation, a field where further exploration could lead to significant advancements [4].\nWe evaluated the impact of various deep learning serving decisions, specifically the choice of runtime engine and execution provider, through a technology oriented experiment using twelve code SLMs. The experiment utilized a generated input dataset from the HumanEval benchmark [23], allowing us to evaluate the energy consumption, execution time, and resource utilization of each serving configuration. These configurations were based on combinations of different runtime engines and execution providers. We tested several runtime engines, including"}, {"title": "2. Background", "content": "In this section, we introduce (i) the deep learning serving configurations: duplet of a runtime engine and an execution provider; (ii) the type of deep learning models we use, namely code generation SLMs."}, {"title": "2.1. Deep learning serving configurations", "content": "In previous work [16], we identified four distinct types of deep learning serving infrastructures within the deep learning serving architectural design decisions: i) No runtime engine and web server, ii) Runtime engine and web server, iii) deep learning-specific software, and iv) End-to-end deep learning cloud service.  illustrates the first two serving infrastructures alongside their transversal decisions. While these transversal decisions can improve the interaction between users and the ML application, they are influenced by the underlying serving infrastructure. Additionally, we include the execution provider, which acts as the backend of the runtime engine, optimizing execution for specific hardware.\nRuntime engine. The runtime engine is a software component that loads the model into the system's memory from a model registry, performs inference, and returns the results to the user. Optimized runtime engines (e.g. ONNX runtime, OpenVINO runtime, or Torch JIT) offer a flexible interface that facilitates the integration of hardware-specific libraries and implements model optimizations, including graph optimizations or just-in-time compilation [16, 26]. This setup can operate independently or be integrated with a web server, depending on the serving requirements. For instance, the optimized ONNX runtime can be used to load ONNX models and wrap them as a service using a web framework like FastAPI. Once the service is running, the user can make predictions through the service, using a communication protocol like gRPC (see figure 6 from [12]). Additionally, there is the option of using the deep learning framework directly without an optimized runtime engine. This is the simplest approach, as deep learning frameworks are primarily designed for training but can also be used for inference [27]. Several runtime engines are available, including:\n\u2022 Torch (TORCH). Inference is conducted directly through the deep learning framework, for instance, the PyTorch library. This approach relies on the framework's inherent capabilities for model execution without additional runtime optimizations. This approach serves as the baseline, representing a straightforward and unoptimized runtime engine.\n\u2022 ONNX Runtime (ONNX). The inference is made through a runtime engine, which applies optimizations to the ONNX model. This is one of the most widely used options, with tools like the Optimum library facilitating the use of the ONNX runtime. It is the default optimized runtime for models in ONNX format [12].\n\u2022 OpenVINO Runtime (OV). The inference is made through the OpenVINO Toolkit [28] runtime engine, which applies optimizations through an Intermediate Representation (IR) and is converted using its Model Optimizer. This runtime engine has been the subject of several studies for its efficiency [29], and libraries like Optimum provide support for OpenVINO runtime integration [30].\n\u2022 Torch JIT (JIT). A runtime engine from PyTorch, it compiles PyTorch models into TorchScript, an intermediate representation that can be run independently of Python. It is the default optimized runtime for models in TorchScript format [12].\nExecution providers. These are libraries that act as the back-end of the runtime engine, optimizing execution for specific hardware (CPU, GPU, FPGA, etc.) [18, 17]. There are various execution providers, including:\n\u2022 CPU Execution Provider (CPU). Default backend library for CPU [17].\n\u2022 CUDA Execution Provider (CUDA). CUDA is the most popular backend library and enables hardware"}, {"title": "2.2. Code generation Small Language Models (SLMs)", "content": "Increasing evidence indicates that SLMs deliver remarkable results, especially in code-related tasks such as code generation [32, 33, 34]. This is pertinent for practitioners using limited resources or concerned with privacy issues [35, 36], situations where using Large Language Models (LLMs) is neither feasible nor desirable. Moreover, relying on closed-source LLMs like OpenAI's ChatGPT accessible via API can introduce prohibitive costs, privacy concerns, and disproportionate energy consumption for single inferences for few tokens [37]. The exclusive capability of big tech companies to deploy state-of-the-art LLMs, due to their extensive computational and financial resources, leaves small to medium tech companies facing a significant challenge [38]. These smaller companies often operate with constrained CPU or GPU resources, necessitating innovative approaches to make use of LMs effectively. In this context, SLMs emerge as a critical solution, not only enabling a wider range of companies and deep learning practitioners to harness the power of deep learning but also contributing to environmental sustainability efforts by reducing the carbon footprint associated with large-scale computations [38]."}, {"title": "3. Related work", "content": "There is a notable gap in studies addressing the deployment efficiency of trained deep learning models, despite inference costs potentially surpassing training due to the cumulative effect of using the model or system many times [39, 4]. Studies like Zhang et al. [40] examined hardware and software impacts on inference efficiency, while Georgiou et al. [27] compared TensorFlow and PyTorch's energy and performance. Koubaa et al. [41] explored deployment strategies and model formats. Other works, including Hampau et al. [42] and Klimiato et al. [43], investigated containerization and deep learning serving infrastructures' effects on energy and performance. Escribano et al. [44] focused on cloud computing energy consumption for NLP models, omitting deep learning serving infrastructure comparison.\nA recent exploration of the code LMs ecosystem by Yang et al. [45] offers a comprehensive analysis of code LMs, shedding light on ecosystem dynamics, model reuse strategies, and documentation practices. Their findings emphasize emerging approaches such as quantization and fine-tuning, which could directly impact deployment efficiency and robustness. In other work, Vartziotis et al. [46] compared the sustainability of the generated code itself by three LMs like GitHub Copilot, OpenAI ChatGPT-3, and Amazon CodeWhisperer. Their study emphasizes on the potential of generative LMs to contribute to environmentally conscious software development through green coding practices.\nFew studies have considered the runtime engine in the inference phase. Hampau [42] et al. analyzed the energy consumption from three \u201ccontainerization strategies\". The difference with our paper is that we have decided to focus two different dimensions of the decisions deep learning practitioners have on the deep learning Serving Infrastructure, while they studied a single runtime engine without containerization, the same runtime engine with docker and an option without a runtime engine in a different containerization option WASM, and with a specific emphasis on Computer Vision (CV) tasks. Zhang et al. [12] conducted a large study on four different deep learning Serving Infrastructures: two deep learning-specific software and two runtime engines. Although they explored the performance of ONNX Runtime and Torch.JIT, their findings primarily reported on latency, with experiments conducted on versions containerized using Docker and communicated through gRPC API. With regard to execution providers, Alizadeh et al. [18] analyzed the performance and energy differences between CUDA and TensorRT when paired with the ONNX runtime engine and a ResNet model, finding that TensorRT outperformed CUDA. In our research, we examine the default CPU execution provider and CUDA execution provider across multiple code generation SLMs.\nThe field of software architecture for deep learning based systems is still evolving, presenting significant challenges for practitioners in making design decisions that optimize efficiency [47, 5]. While some studies have proposed architecture-based approaches to improve the efficiency and sustainability of deep learning systems [9, 48], research specifically addressing the serving infrastructure of these systems remains limited [16]. This work contributes to the field by addressing the underexplored area of serving infrastructure selection, complementing existing efforts in architectural decision-making for greener deep learning systems.\nContrary to previous research, our study focuses on analyzing the performance of various deep learning serving configurations during the inference phase of SLM for code generation. Our analysis specifically targets runtime engines and execution providers, and their impact on code SLMS, evaluating critical factors such as energy consumption, execution time, and computing-resource utilization."}, {"title": "4. Research methodology", "content": "The research goal [49] of this study is to analyze deep learning runtime engines and execution providers for the purpose of understanding their impact with respect to energy consumption, execution time, and computing-resource utilization from the point of view of software engineers conducting inference in the context of code SLMs.\nTherefore, we intend to address the following research questions (RQ):\nRQ1: How is the energy consumption affected by the execution provider and runtime engine used for inference from code SLMs?\nRQ2: How is the execution time affected by the execution provider and runtime engine used for inference from code SLMs?\nRQ3: How is the computing-resource utilization (CPU usage, RAM usage, GPU usage, GPU memory utilization, and GPU used memory) affected by the execution provider and runtime engine used for inference from code SLMs?"}, {"title": "4.2. Experimental units", "content": "In this subsection, we detail the experimental units and the process used to construct the input dataset for our study. The experimental units refer to the selected code SLMs used for inference, while the dataset construction draws from the widely-used HumanEval dataset [23]. The experimental units were subjected to treatments in the form of different deep learning serving configurations to evaluate them during inference.\nThe selection process for the code generation SLMs in this study was conducted with a systematic and structured approach [50], reflecting the state of available models as of November 6th, 2024. Given the time required for executing experiments, and analyzing results, this study might not include other recent code SLMs after such date.\nUsing the Hugging Face API, metadata for 104,816 text-generation decoder models was retrieved, including detailed model card information. A series of filters were applied to refine the selection: models exceeding 5 billion parameters were excluded to focus on SLMs, and priority was given to models with potential HumanEval results explicitly mentioned in their metadata, model cards, or flagged as gated models. Only base models, identified within their respective model trees, were retained, narrowing the pool to 340 models. Manual verification ensured that the final 29 models were genuinely base models, free from mislabeled fine-tuned or quantized versions, and had HumanEval evaluation results to confirm their suitability for code-related tasks. From this pool, 12 code SLMs, presented in Table 1, were randomly selected to ensure diversity and representativeness. A replication package containing the extracted data is available for reproducibility and future research (see data availability statement in Section 1)."}, {"title": "4.3. Dataset construction", "content": "Our input dataset was derived utilizing a methodology similar to those outlined by Fried et al. and Bavarian et al. [51, 52]. Specifically, we selected a subset of prompts from the HumanEval dataset [23] by taking each problem and processing it as follows: we tokenized the entire problem and then randomly selected a sequence of 10 to 15 tokens. This token sequence was decoded and used as an input prompt for the model, with the aim of generating additional tokens based on the sampled input. This method enabled us to create a dataset that was both controlled to fit our computational setup and diverse enough to effectively evaluate the small language models (SLMs) in code generation tasks with respect to our independent variables."}, {"title": "4.4. Study design", "content": "We conduct a technology oriented experiment [53] following the guidelines by Jedlitschka et al. [54]. We selected twelve trained models and structured our investigation into a multi-stage pipeline, as illustrated in Figure 2. This pipeline consists of five main stages [55, 56]:\n1. Data management: This foundational phase is critical in setting up our experiment to test code generation models. We start with the HumanEval dataset [23], which includes 164 coding problems, and adapt it to better suit the specific requirements of our study involving SLMs. This adaptation involves generating a new dataset from the HumanEval problems for the code generation task (see also 4.3).\n2. Modelling: Our study focuses on analyzing different configurations of deep learning serving infrastructures (see subsection 4.5.1), utilizing pre-trained models. This approach is in alignment with the objectives of our study (see also 4.2).\n3. Development: This initial stage involves model conversion and the development of the serving infrastructure, which includes the REST API. Model conversion is aimed at adapting the pre-trained models to be compatible with the used runtime engines (see data availability statement in Section 1).\n4. Operation: After the development phase, the operation stage starts with the deployment of models across various serving configurations using a Uvicorn server, which is a high-speed ASGI web server and"}, {"title": "4.5. Variables", "content": "In the following we define the variables of our experimental design grouped into two categories, as summarized in Table 2."}, {"title": "4.5.1. Independent variables", "content": "In this study we define one independent variable, Serving Configuration, which is a duplet of a runtime engine and an execution provider (see section 2). The setup consists of evaluating different deep learning serving configurations. We initially obtain eight possible configurations"}, {"title": "4.5.2. Dependent variables", "content": "To measure the impact of our independent variable to energy efficiency, and performance efficiency, the study tracks some of the most widely measured and relevant metrics in the field according to the Green Software Measurement Model [58]: the energy consumption, execution time, and computing-resource utilization (CPU usage, RAM usage, GPU usage, GPU memory utilization, and GPU used memory) during the inference (see also Table 2)."}, {"title": "4.6. Experiment setting", "content": "All these experiments have been conducted with an NVIDIA GeForce RTX 4090 GPU with 24 GB memory with CUDA Version 12.4, and an AMD Ryzen 9 7950X 16-Core Processor CPU, 2.5 GHz frequency."}, {"title": "4.7. Experiment execution", "content": "In this study, we deploy a server that receives HTTP requests from a client. The server hosts the developed API, which processes input prompts and returns the model's output, specifically the generated decoded tokens. The experiment involves selecting one of the seven serving configurations and performing the following steps for each configuration: for each model, the model is loaded, and inference is conducted on all 164 requests using the input dataset. During the inference process for each serving configuration, we collect the dependent variables using the three profilers described in section 4.6. To ensure stability, we run the server for five minutes before starting the inferences and for another five minutes after completing the inferences with each serving configuration. The experiment is repeated 10 times to ensure consistency in the results, as this number of repetitions was sufficient to observe minimal variability and provide reliable medians across configurations. Still, each repetition of the experiment involves 7 serving configurations, 12 models, and 164 input requests per model, resulting in a substantial amount of data collected."}, {"title": "4.8. Data analysis", "content": "To address the research questions (RQs), we define the null hypotheses showed in Table 3 for each dependent variable and its performed tests.\nFor each hypothesis, we follow a systematic analysis process: (1) use box plots to illustrate the distributions for each dependent variable, comparing between configurations using the same model; (2) assess if the measurements are normally distributed and have equal variances across the different treatments of each RQ. Utilize the Shapiro-Wilk test to check for the normality of the data. To check the homogeneity of variances, we use a Levene test for equality of variances. (3) assess the statistical significance (i.e., p-value) of the findings.\nIn our data analysis, we conducted two types of statistical tests based on the distribution of the dependent variables. For configurations where the data is normally distributed, we applied Welch's ANOVA to assess differences between serving configurations, since the dependent variables do not exhibit equality of variances, followed by the Games-Howell test for post-hoc analysis. For non-normally distributed data, we used the Kruskal-Wallis test, with Dunn's test as the post-hoc analysis to identify specific differences between groups. The details of the analysis and test results are available in the replication package for further examination (see data availability statement in Section 1)."}, {"title": "5. Results", "content": "In this section, we report the results of our experiment, addressing our research questions and highlighting key takeaways. We initially evaluated seven deep learning serving configurations. Following the experiments, we reviewed the outputs of the scripts and inferences, identifying error-prone responses in the TorchScript configurations. As a result, these configurations were excluded from further analysis. The results presented here focus on comparisons across the final five selected configurations while using results from twelve different code SLMs, as shown in Tables 4-5 and Figures 3 - 5 (one color for each model in the box plots)."}, {"title": "5.1. Energy consumption of code SLMs for code generation (RQ1).", "content": "Tables 4 and 5 present the aggregated median values of energy consumption by model size category. For CPU execution provider configurations (where the GPU is not utilized), only global energy, based on CPU energy, is reported. For CUDA execution provider configurations, the results include CPU energy, GPU energy, and global energy, calculated as the sum of CPU and GPU energy.\nThe median energy consumption for configurations using the CUDA execution provider is consistently lower than that of the CPU execution provider. Notably, the  configuration stands out as the most energy-efficient across all models, a trend that is also evident in the aggregated results.\nThe  configuration exhibits the highest median energy consumption values among the CUDA execution provider configurations. Similarly,  configurations consume the most energy across the majority of models when using the CPU execution provider. This pattern is also reflected in the aggregated results.\nONNX optimizations are significantly more effective when paired with the CPU execution provider, demonstrating better energy efficiency compared to the deep learning framework alone. Among the optimized runtime engines, ONNX, along with OV, consumes less energy, with ONNX emerging as the most energy-efficient in CPU execution provider configurations. However, when using the CUDA execution provider, ONNX fails to provide the same level of energy savings, as it does not outperform the baseline TORCH runtime engine, which remains the most energy-efficient. This discrepancy in energy performance likely stems from the way the ONNX runtime engine interacts with each execution provider.\nWhile using the CPU execution provider, it was observed that the extended execution times (see Table 4) led to a significant increase in energy consumption by the CPU. This prolonged inference time, especially with more complex models, may cause CPU configurations to use substantially more energy. The inefficiency of the CPU execution provider in energy conservation highlights the need for optimized resource allocation and management in deep learning serving environments to minimize unnecessary energy consumption. In contrast, CUDA configurations, despite utilizing both CPU and GPU resources, were far more energy-efficient for intensive tasks due to their significantly faster execution times.\nOur results showcase considerable differences in the energy consumption of different configurations. There is statistic significance (Welch's ANOVA test p-value) between the deep learning serving configuration and the response variables. Furthermore, Games-Howell post-hoc test reveals that all configurations are statistically different from each other (H.1.0). These results are provided in the replication package (see data availability statement in Section 1).\nAnswer to RQ1: Energy consumption is significantly affected by the serving configuration, with CUDA configurations being more efficient than CPU configurations.  provides the best balance between energy efficiency and reliability, while optimized runtime engines like OV and ONNX, particularly ONNX, reduce energy consumption in CPU configurations but are less effective with CUDA compared to TORCH."}, {"title": "5.2. Execution time of code SLMs for code generation (RQ2).", "content": "Tables 4 and 5 present the aggregated median values of execution time by model size category. This analysis highlights a significant difference in execution time, with configurations utilizing the CUDA execution provider generally achieving faster execution times compared to those using CPU execution provider. This underscores the efficiency of GPU acceleration in handling computationally intensive tasks.\nExtreme cases are observed with models such as phi2 using , where the median execution time (5983.52 seconds) is more than 20 times longer than with the  configuration (251.97 seconds), highlighting significant bottlenecks in CPU-based processing for complex computational tasks. Similar extreme cases are noted within the largest model size category (Category 3) using TORCH, where the CPU execution provider's execution time (5668.91 seconds) exceeds the CUDA execution provider's time (260.62 seconds) by more than 20 times.\nSimilar to its impact on energy efficiency, ONNX optimizations are clearly more effective with the CPU execution provider, but not as successful with the CUDA execution provider, comparing to Torch configurations. When using the CPU execution provider, the optimized runtime engines, particularly OV and ONNX, perform faster than the deep learning framework alone, with OV showing the best results. However, ONNX behaves differently when paired with the CUDA execution provider, where it fails to match or outperform TORCH in terms of speed. Again, this discrepancy may be due to how each execution provider interacts with the ONNX runtime engine. This results can also be observed in Tables 4 and 5.\nWelch's ANOVA confirmed significant differences in execution times across different runtime engines. The subsequent Games-Howell post-hoc tests annotate that all configurations are statistically distinct from one another, confirming the marked impact of the execution provider and runtime engine choice on the efficiency of computational tasks (H.2.0). The only exceptions were that the execution times of the  and  configurations did not differ significantly for three out of the twelve models (bloomz-560m, codeparrot-small, and tiny-starcoder), resulting in the rejection of H.2.0 for only these models.\nAnswer to RQ2: Execution time is significantly influenced by the serving configuration, with CUDA configurations consistently outperforming CPU configurations.  delivers the best performance, while OV and ONNX optimize execution time, especially OV, with CPU execution provider but do not achieve comparable improvements with CUDA execution provider, reflecting similar patterns seen in energy consumption."}, {"title": "5.3. Computing-resource utilization of code SLMs for code generation (RQ3).", "content": "Table 3 presents a summary of the statistical analysis conducted to assess the computing-resource utilization across the different serving configurations. These analyses were applied to key metrics such as CPU usage, RAM usage, GPU usage, GPU memory utilization, and GPU used memory.\nFor statistical evaluation, different tests were applied depending on the normality and variance homogeneity of the data. Welch's ANOVA was used for data with unequal variances but normal distributions, while the Kruskal-Wallis test was employed for non-normally distributed data. The significance of differences between configurations was assessed using these tests, with p-values indicating significant differences with values below a = 0.05. Furthermore, post-hoc analyses using Games-Howell or Dunn's test were performed to identify specific pairwise differences. In terms of computing-resource utilization metrics, all serving configurations showed significant differences from one another (H.3.1.0, H.3.2.0, H.3.3.0, H.3.4.0, and H.3.5.0), with a few exceptions. Specifically, RAM usage between certain serving configuration pairs\u2014such as  and ,  and ,  and \u2014did not exhibit significant differences in two out of the twelve models for each pair. As a result, H.3.2.0 was rejected for those specific cases.\nIn the results of GPU metrics, we focus exclusively on the three CUDA execution provider configurations. Comparisons with CPU execution provider configurations are omitted, as they do not utilize the GPU, making such comparisons irrelevant.\nCPU usage: The visual data and median values indicate that configurations using the CPU execution provider generally exhibit higher CPU usage than those employing the CUDA execution provider, which aligns with expectations given the operational differences between the two. For CUDA execution providers, CPU bottlenecks are minimal since the majority of computations are offloaded to the GPU. However, when using CPU execution provider, as it is expected that all the operations are executed by the CPU, selecting the serving configuration may impact CPU usage.\nThe  configuration consistently demonstrated the lowest CPU usage among all configurations, highlighting its efficiency in resource utilization. This configuration achieves an effective balance between GPU and CPU usage, optimizing the model operations for improved performance.\nCPU usage for configurations utilizing the ONNX runtime engine is slightly higher when paired with the CUDA execution provider, with median values ranging from 2.98% to 4.02%. While differences in CPU usage are observed, no consistent pattern emerges across models when the CPU execution provider is employed.\nRAM usage: Results show that configurations using the CPU execution provider generally consume more RAM compared to those using the CUDA execution provider. This discrepancy can be attributed to the fact that CPU execution provider manages all computational resources directly within the system's main memory, whereas CUDA execution provider leverages both RAM and dedicated VRAM, effectively distributing memory load.\nThe  configuration consistently demonstrated the lowest RAM usage, ranging from 4.77% to 11.45% in 9 out of 12 models. This highlights its efficiency in resource utilization, as it achieves a balance between computational performance and minimal RAM consumption. TORCH configurations, in general, exhibit lower RAM usage compared to other runtime engines.\nIn contrast,  and  configurations exhibited significantly higher RAM usage, with  ranging from 9.96% to 30.16% and  from 8.85% to 20.88% in 10 out of 12 models. These findings indicate that optimized runtime engines like ONNX and OV demand more system resources due to their memory management strategies. While they improve energy efficiency and execution time in some scenarios, this often comes at the cost of increased RAM usage. Furthermore, in CUDA execution provider configurations, ONNX exhibits higher RAM usage but performs worse in terms of both time and energy efficiency compared to TORCH, further underscoring TORCH's superior balance of performance and resource utilization.\nGPU usage: results show that the configuration  reaches the highest levels of GPU utilization (from 21.08% up to 86.98%), leveraging available GPU resources effectively. Conversely, the  configuration shows the lowest GPU usage (from 18.07% up to 75.49%) among the CUDA execution provider options, suggesting it may not exploit the computational potential of the GPU as efficiently as . This trend is also reflected in the aggregated results across the three groups.\nGPU memory utilization: Our GPU utilization memory results reveal that  configurations exhibited the highest memory utilization (from 11.57% up to 83.59%, all models), effectively leveraging GPU capabilities for intensive memory operations. In contrast,  configurations showed the lowest memory utilization (from 2.34% up to 27.33%), suggesting either conservative use or inefficiencies in exploiting GPU VRAM operations compared to configurations using .\nThese findings emphasize the crucial role of runtime engine selection in maximizing the memory efficiency of CUDA execution provider, with significant variations noted in how different engines manage and utilize VRAM.\nGPU used memory:Regarding GPU allocated memory (GPU used memory), the results show varying patterns among CUDA execution provider configurations. For 7 out of 12 models, the  configuration exhibited higher levels of memory allocation, while the  configuration showed higher memory allocation for the remaining 5 models. Specifically,  configurations ranged from 12.01% to 59.16%, whereas  configurations ranged from 5.12% to 71.87%. These findings highlight differences in memory allocation strategies and the relative efficiency of Torch in allocating GPU memory.\nAnswer to RQ3: Computing-resource utilization is significantly influenced by the serving configuration, with OV or ONNX within CPU configurations improving energy or time efficiency but increasing RAM demands. In contrast, CUDA configurations, particularly , show more efficient resource utilization, achieving higher GPU utilization and better memory allocation, reducing the load on CPU and RAM."}, {"title": "6. Discussion and implications", "content": "In this section, we analyze and interpret the study's findings for each dependent variable, discuss their implications, and highlight key insights derived from the research.\nEnergy consumption. In our analysis, we observe that configurations utilizing the CUDA execution provider consistently consume less energy compared to those utilizing the CPU execution provider. This significant difference underscores the inherent efficiencies of CUDA execution provider, which enables hardware-accelerated computation on NVIDIA GPUs, effectively managing parallel processing tasks in code generation models.\nInterestingly, ONNX runtime configurations show greater optimization when paired with CPU execution provider compared to CUDA execution provider, when compared to other runtime engines under the same execution provider. This enhanced performance with CPU execution provider could stem from the specific libraries used (such as Transformers and Optimum HuggingFace's libraries [59", "63": ".", "64": "or that CUDA execution provider in ONNX runtime does not fully support CUDA kernels used for the SLMs [65"}, {"63": "found that configurations using  outperformed those using  in terms of speed for Brain-Computer Interface applications. However, our results contrast this finding, as  proved faster in our experiments. This discrepancy highlights how the performance of optimized runtime engines can vary significantly across different deep learning"}]}