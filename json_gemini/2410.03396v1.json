{"title": "GraphCroc: Cross-Correlation Autoencoder for Graph Structural Reconstruction", "authors": ["Shijin Duan", "Ruyi Ding", "Jiaxing He", "Aidong Adam Ding", "Yunsi Fei", "Xiaolin Xu"], "abstract": "Graph-structured data is integral to many applications, prompting the development of various graph representation methods. Graph autoencoders (GAEs), in particular, reconstruct graph structures from node embeddings. Current GAE models primarily utilize self-correlation to represent graph structures and focus on node-level tasks, often overlooking multi-graph scenarios. Our theoretical analysis indicates that self-correlation generally falls short in accurately representing specific graph features such as islands, symmetrical structures, and directional edges, particularly in smaller or multiple graph contexts. To address these limitations, we introduce a cross-correlation mechanism that significantly enhances the GAE representational capabilities. Additionally, we propose the GraphCroc, a new GAE that supports flexible encoder architectures tailored for various downstream tasks and ensures robust structural reconstruction, through a mirrored encoding-decoding process. This model also tackles the challenge of representation bias during optimization by implementing a loss-balancing strategy. Both theoretical analysis and numerical evaluations demonstrate that our methodology significantly outperforms existing self-correlation-based GAEs in graph structure reconstruction. Our code is available in https://github.com/sjduan/GraphCroc.", "sections": [{"title": "1 Introduction", "content": "Graph-structured data captures the relationships between data points, effectively mirroring the inter- connectivity observed in various real-world applications, such as web services [3], recommendation systems [39], and molecular structures [17, 18, 12]. Beyond the message passing through node connections [50], the exploration of graph structure representation is equally critical [38, 15, 33, 19, 9]. This representation is extensively utilized in domains including recommendation systems, social network analysis, and drug discovery [42], by leveraging the power of Graph Neural Networks (GNNs). Specifically with L layers in GNN, a node assimilates structural information from its L-hop neighborhood, embedding graph structure in node features.\nGraph autoencoders (GAEs)[19] have been developed to encode graph structures into node embed- dings and decode these embeddings back into structural information, such as the adjacency matrix. This structural reconstruction process can be performed either sequentially along nodes [11, 22, 47] or in a global fashion [33]. While there has been significant advancement in both methods, most studies primarily focus on node tasks, which involve a single graph, such as link prediction [35] and node classification [15], with decoding strategies typically reliant on \u201cself-correlation\". We define this term as the correlation of node pair from the same node embedding space. Given an n-node graph with embedding dimension d' on each node, its node embedding is $Z \\in \\mathbb{R}^{n \\times d'}$, thus the self-correlation is expressed as $z_i^T z_j$ between two nodes. Correspondingly, the \u201ccross-correlation\u201d depicts the node pair"}, {"title": "2 GAE Structural Reconstruction Analysis", "content": "2.1 Preliminary\nGraph Neural Network As Graph Neural Networks (GNNs) have been defined in various ways, without loss of generality, we adopt the computing flow in [45] to define the graph structure and a general GNN model. A graph $G = (V, E)$ comprises n nodes, each with a feature represented by a d-dimensional vector, resulting in a feature matrix $X \\in \\mathbb{R}^{n \\times d}$. The set of edges E is depicted by an adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$, which indicates the connections between nodes in G. A GNN model $f(X, A)$ is utilized to summarize graph information for downstream tasks.\nThe feed-forward propagation of the l-th layer in GNN $f(\\cdot)$ is\n$h_{l+1} = \\sigma(\\hat{D}_l^{-\\frac{1}{2}} \\hat{A}_l \\hat{D}_l^{-\\frac{1}{2}} h_l W_l)$\n$h_l \\in \\mathbb{R}^{n \\times d_l}$ is the input feature of the l-th layer, where $h_1 = X$ and $d_l$ is the feature dimension of each node specified by each layer. $\\hat{A}_l = A_l + I$ is the adjacency matrix (self-loop added) of the input graph structure in each layer. Note that $A_l$ will be consistent in the absented pooling layer scenario, such as the node classification task, yet it can vary along GNN layers in graph tasks due to the introduction of graph pooling layers [9]. Subsequently, we use the diagonal node degree matrix"}, {"title": "2.2 Deficiencies of Self-Correlation on Graph Structure Representation", "content": "To generalize the discussion, we set the representation capability of the encoder as unrestricted, i.e., allowing Z to be generated through any potentially optimal encoding method. On the decoding side, self-correlation is applied following Eq.2. We identify and discuss specific (sub)graph structures that are poorly addressed by current self-correlation methods:\nIslands (Non Self-Loop). Both the inner product and the L2-norm of a node embedding pair fail to accurately represent nodes without self-loops, where $A_{i,i} = 0$. Given that $z_i^T z_i > 0$ and $C(1 - ||z_i - z_i||^2) = C > 0$, the predicted value $\\mathcal{I}(A_{i,i})$ defaults to 1 when threshold 0.5 is applied to the sigmoid output. This limitation underlies the common homophily assumption in previous research [42], that all nodes contain self-loops; it treats self-loops as irrelevant to the graph's structure. However, there is a huge difference between the self-connection on one node and the inter-connection between nodes in some scenarios; for example, on heterophilous graphs [51], nodes are prone to connect with other nodes that are dissimilar \u2014 such as fraudster detection.\nTopologically Symmetric Structures. If graph structure is symmet- ric along an axis or a central pivot, as demonstrated in Figure 1, the self-correlation method cannot represent these structures as well.\nDefinition 2.1 (Topologically Symmetric Graph). A symmetric graph $G = (V, E)$ has the structure and node features topologically symmetric either along a specific axis or around a central node. For an axisymmetric graph, the feature matrix is denoted as $X = \\{X_1,..., X_{n_1}\\} \\cup \\{X_{l1}, ..., X_{ln_2} \\} \\cup \\{X_{r1},..., X_{rn_2}\\}$, such that $n_1 + 2n_2 = n$. $X_i$ represents the node features on the axis, and for each paired node off the axis, $X_{li} = X_{ri}$. The connections are also symmetric, satisfying $A_{li,:} = A_{ri,:}$. In a centrosymmetric graph, the pivot node has feature $X_1$, and other nodes share the same feature $X_2 = ... = X_{n-1}$. Additionally, the adjacency relationships for these nodes are identical, with $A_{1,:} = A_{j,:}$ for all $i, j \\in [2, n]$.\nLemma 2.2. Given an arbitrary topologically symmetric graph $G = (V, E)$ and an encoder $f(X, A)$, the self-correlation decoder output will always have $\\mathcal{I}(A_{li,ri}) = 1$.\nProof. Due to the symmetry on graph $G = (V, E)$ from the definition above, we have $f(X_i, A_{i,:}) = f(X_j, A_{j,:})$ for nodes i and j that are symmetric about the axis or pivot. Thus, we can derive $z_i = z_j$. For the prediction on link between i and j, we have $\\bar{A}_{i,j} = \\text{sigmoid}(z_i^T z_j) = \\text{sigmoid}(z_i^T z_i) \\geq 0.5$. Similarly, for the L2-norm method, we have $\\bar{A}_{i,j} = \\text{sigmoid}(C(1 - ||z_i - z_j||^2)) = \\text{sigmoid}(C(1 - ||z_i - z_i||^2)) = \\text{sigmoid}(C) > 0.5$. In both decoding methods, the decoder is prone to predict the edge between two symmetric as positive, $\\mathcal{I}(\\bar{A}_{i,j}) = 1$."}, {"title": "2.3 Our Cross-Correlation Approach for Better Structural Representation", "content": "Instead of self-correlation, we advocate cross-correlation to reconstruct the graph structure, denoted by $\\bar{A} = \\text{sigmoid}(PQ^T)$, where $P, Q \\in \\mathbb{R}^{n \\times d'}$. This approach allows us to decouple the variables involved in calculating the correlation, thus overcoming the inherent limitations of self-correlation.\n2.3.1 How Does Cross-Correlation Mitigate Deficiencies of Self-Correlation?\nExpressing Islands. For each node $i \\in [1,n]$, the sign of $p_i^T q_j$ can be flexibly determined by $p_i$ and $q_j$, allowing it to be either positive or negative. Consequently, the presence of an island can be effectively modeled using $\\bar{A}_{r,j} = \\text{sigmoid}(p_i^T q_j)$ or $\\text{sigmoid}(C(1 - ||p_i - q_j||^2))$. This approach avoids the limitations associated with self-correlation, which restricts the sigmoid input to positive.\nExpressing Symmetric Structure. Cross-correlation is particularly effective in capturing topological symmetric structures. Given a node pair (i, j) that is topologically symmetric about an axis or pivot, for undirected graphs, we have $p_i = p_j$ and $q_i = q_j$. However, since $p_i$ and $q_j$ (as well as $p_j$ and $q_i$) are not directly dependent on each other, the sign of $p_i^T q_j = p_i^T q_i$ can be either positive or negative. Therefore, $\\mathcal{I}(A_{i,j}) = \\mathcal{I}(\\text{sigmoid}(p_i^T q_j))$ is able to yield 0 or 1, depending on the specific values of node embedding $p_i$ and $q_j$. This flexibility can be supported by the L2-norm decoding as well.\nExpressing Directed Graph. A similar interpretation extends to the representation of directed graphs. For two nodes i and j, the directed edges can be defined by $p_i^T q_j$ for one direction and $p_i^T q_i$ for the other. Since these four latent vectors do not have explicit dependencies among them, the directions of the edges can be independently determined using cross-correlation, capturing the directional connections between nodes.\n2.3.2 Cross-Correlation Provides Smoother Optimization Trace\nWe highlight the superiority of cross-correlation over self-correlation in the optimization process of GAE training. Considering the decoder optimization problem, where we aim to satisfy the constraints:\n(\\text{self-correlation}) \\; \\mathcal{I}(\\text{sigmoid}(z_i^T z_j)) = A_{i,j}, \\; (\\text{cross-correlation}) \\; \\mathcal{I}(\\text{sigmoid}(p_i^T q_j)) = A_{i,j}\nfor each element in matrix A. This involves finding $Z \\in \\mathbb{R}^{n \\times d'}$ or $P, Q \\in \\mathbb{R}^{n \\times d'}$ that maximize the number of satisfied constraints. Additionally, for an undirected graph, the symmetry $A_{i,j} = A_{j,i}$ imposes the requirement that $\\mathcal{I}(\\text{sigmoid}(p_i^T q_j)) = \\mathcal{I}(\\text{sigmoid}(p_i^T q_i))$, ensuring that both $p_i^T q_j$ and $p_i^T q_i$ should have the same sign.\nIn the case of cross-correlation, where P and Q are independently determined, and all constraints can well align with the generation of P and Q. However, this is not the case in self-correlation, where $z_i^T z_j = z_j^T z_i$ inherently overloads the symmetry constraint. For example, if $A_{i,j} = A_{j,i} = 1$, we only require $p_i^T q_j > 0$ and $p_i^T q_i > 0$ for cross-correlation, while they become restrictive as"}, {"title": "3 GraphCroc: Cross-Correlation-Based Graph Autoencoder", "content": "Encoder Architecture Our work scales the normal single-graph representation to multi-graph for graph tasks, and we do not specify the encoder structure, but free it as the downstream tasks require. For graph tasks, the GNN model has a sequence of message passing layer and pooling layer to coarsen the graph to higher-level representation. In the end, a readout layer [8, 9, 31] is applied to summarize the graph representation to the latent space. We define the encoder as\n$\\text{encoder (ours):} \\; Z' = \\Phi(Z'|G) = f(X, A)$\nwhere $Z' \\in \\mathbb{R}^{n' \\times d'}$ has a reduced number $n'$ of node embeddings. Besides, we exclude the readout layer from the encoder, yet assign it to the start of downstream tasks.\nTwo-Way Decoder for Cross-Correlation To separately produce two node embeddings, P and Q, we divide the decoding process into two parallel and self-governed decoders. Unlike [20], we leave"}, {"title": "4 Evaluation", "content": "4.1 Experimental Setup\nDataset We assess GraphCroc in various graph tasks. Specifically, we utilize datasets for molecule, scaling from small (PROTEINS [2]) to large (Protein-Protein Interactions (PPI) [12], and QM9 [29]), for scientific collaboration (COLLAB [46]), and for movie collaboration (IMDB-Binary [46]). Further details on these datasets are provided in Appendix F.1.\nGraphCroc Structure During evaluation, our GraphCroc structure, especially the GCN layer number, is determined by the scale of graph. Assuming the average node number in a graph task is \u00f1, we set up an empirical layer number $L \\propto \\sqrt[n]{ \\prod_{i=1}^n (0.9 - i) } = 2$, so that the number of nodes can be smoothly reduced to two at the end of encoding [9]. Besides, we use node embedding dimension $d' \\approx \\bar{n}$, following analysis in Appendix B. Other detail is provided in Appendix F.2.\n4.2 GraphCroc on Structural Reconstruction\nTable 1 demonstrates the graph reconstruction capabilities of GAE models for multi-graph tasks by presenting ROC-AUC scores on adjacency matrix reconstruction. We compare our model against prevalent self-correlation kernels in GAEs and a cross-correlation method designed for directed graphs. Comparing the basic inner-product methods, GAE and VGAE [19], the variational extension in VGAE does not obviously improve the performance over GAE in graph tasks. However, by enhancing the GAE architecture itself, i.e., using our GraphCroc architecture, the reconstruction efficacy is significantly increased; GraphCroc under self-correlation can achieve 3/5 second bests among all GAE models. This underscores the graph representation capability of the U-Net structure [9]. Additionally, the L2-norm decoding method generally outperforms the inner-product approach (GAE and VGAE), although it struggles with large graphs such as PPI, which requires too much GPU memory to be trained during our reproduction. On the other hand, the cross-correlation method (DiGAE) provides a consistent albeit modest representation across different graph sizes. This demonstrates the cross-correlation ability to represent multiple graphs in various scenarios. However, the GNN architecture limits its capability to capture enough structural information. By integrating the strengths of cross-correlation with the U-Net architecture, our GraphCroc GAE model consistently excels over other methods, offering significant advantages in all the graph tasks tested. Even on large graphs, such as PPI with over 2,000 nodes, GraphCroc can still achieve an AUC score over 0.98. To further demonstrate the effectiveness of the cross-correlation mechanism, we evaluate GAE models with alternative architectures under cross-correlation, in Appendix G.1. The reconstruction results for these architectures are consistent with those in Table 1, though they exhibit slightly lower AUC compared to GraphCroc.\nWhile the AUC score indicates how well a model reconstructs edges, it does not measure the model's ability to exactly reconstruct a whole graph. To address this, we employ the Weisfeiler-Leman isomorphism test (WL-test)[41], which assesses the structural equivalence between the original and reconstructed graphs."}, {"title": "4.3 GraphCroc on Other GAE Strategies", "content": "To make cross-correlation more pronounced in our evaluation, the above experiments implement only the basic inner-product representation, i.e., $\\text{sigmoid}(PQ^T)$. In addition, previous studies have extensively explored data augmentation and training enhancements to optimize GAE models. Specifically, we examine the performance of GAE with cross-correlation applied to different training strategies in Figure 6, using the PROTEINS dataset as a case study.\nWe evaluate three other prevalent decoding enhancements to compare their performance with the standard inner-product decoding (baseline) under the cross-correlation method. The variational decoding [19] generates node embeddings from a Gaussian distribution, with mean/std determined by the decoder outputs. Although a similar final AUC was achieved, it falls short of the baseline on the PROTEINS task at early convergence. For the other two strategies, edge masking [35] and L2-norm representation [26], they facilitate faster convergence during the initial training stages. However, we find that the enhancement of these strategies is highly dependent on graph tasks. Our further analysis on other graph tasks (Appendix G.4) demonstrates that the L2-norm and masking could converge to worse structural reconstructions than our baseline training. Therefore, we still advocate our training and representing methods for GAE models on various graph tasks."}, {"title": "4.4 GraphCroc on Graph Classification Tasks", "content": "One common application of autoencoder models is leveraging their potent encoders for downstream tasks. We evaluate our GraphCroc model by employing its encoder in graph classification tasks, as summarized in Table 2. Notably, generative approaches like GraphMAE, S2GAE, StructMAE, and our GAE model tend to surpass traditional unsupervised and contrastive learning methods. Although contrastive learning incorporates negative sampling, its effectiveness is limited in multi-graph tasks. This finding corroborates the observations in Tab.4 of [44], which indicate that while negative sampling substantially boosts performance in single-graph tasks (e.g., node classification), it has little impact on graph classification tasks. In contrast, GAE models deliver robust graph representations, particularly for small-to-moderate-sized graphs, enhancing their utility in graph classification. Furthermore, our GraphCroc model significantly outperforms self-correlation methods (GraphMAE and S2GAE) in representing graph structures, demonstrated in Table 1, enabling the encoder to effectively capture the structural information of input graphs. Consequently, classifiers leveraging our encoder can achieve high performance with finetuning over only several epochs. Continued optimization of our classification models promises to further elevate their performance in graph classification tasks, surpassing other GAE-based models."}, {"title": "4.5 GAE Attack Surface Analysis", "content": "Research in vision tasks demonstrates that manipulating the latent space with perturbations enables AE to produce adversarial examples with stealthiness and semanticity [6, 16, 32, 40, 43]. Given AE's success in vision domain, we raise the concern whether a GAE can be utilized to generate adversarial graph structures by modifying the latent vectors? Current graph adversarial attacks directly modify the input of GNNs, highly inefficient due to the discreteness of graph structures [7, 37]. By directly conducting attacks in the latent space, GAE could be a potentially efficient attack surface.\nIn Table 3, we demonstrate the effect of small perturbations on the latent space us- ing random noise injection, PGD [27], and C&W adversarial noise injection [7] on graph classification tasks. We limit all at- tacks on the latent space with a maximum query number of 400 and report the classi- fication accuracy of perturbed graphs and the number of edge changes. Note that we focus solely on the structure modification without changes on the node features. Our observations indicate that conducting adversarial attacks on the latent space of the graph autoencoder effectively reduces model accuracy, although it could yield significant edge changes. Compared to adversarial attacks on graph structures using discrete optimization methods, our latent attacks demonstrate comparable performance in terms of accuracy and can be performed in batches with high efficiency. Nevertheless, due to the discrete nature of graph structures, the distortion in edge changes is hard to be always controlled at a low level. Our evaluation of GraphCroc's latent space reveals a potential vulnerability, indicating that adversarial attacks on a graph autoencoder's latent space can provide efficient structural adversarial attacks."}, {"title": "5 Conclusion", "content": "Graph autoencoders (GAEs) are increasingly effective in representing graph structures. In our research, we identify significant limitations in the self-correlation approach employed in the decoding processes of prevalent GAE models. Self-correlation inadequately represents certain graph structures and requires optimization within a constrained space. To address these deficiencies, we advocate cross-correlation as the decoding kernel. We propose a novel GAE model, GraphCroc, which incorporates cross-correlation decoding and is built upon a U-Net architecture, enhancing the flexibility in GNN design. Our evaluations demonstrate that GraphCroc outperforms existing GAE methods in terms of graph structural reconstruction and downstream tasks. In addition, we propose the concern that well-performed GAEs could be a surface for adversarial attacks."}, {"title": "A Related Work", "content": "Graph representation has been explored through various methods. Auto-regressive models [24, 49, 14] generate graph structures by sequentially querying the connectivity between node pairs, which can be computationally expensive for large graphs, e.g., $n^2$ queries required for the adjacency matrix. Similarly, diffusion-based graph models [21] construct graph structures through multiple steps, such as degree matrix reconstruction [5]. These methods primarily focus on graph generation, creating rational graph structures from random noisy node islands.\nIn contrast, Graph Autoencoder (GAE) methods represent graph structures as node embeddings, designed to reconstruct the graph either sequentially [47, 22] or globally [33, 19, 15, 30, 20]. The very beginning graph structure representation is proposed in [19] with self-correlation (applied with Eq. 1 and 2) and further expresses the node embedding with a variational approach. Later, GAE has been widely explored with sequential and global generating methods. For the sequential GAE models, GraphRNN [47] proposes an autoregressive model, which generates graphs by summarizing a representative set of graphs and decomposes the graph generation into a sequence of node and edge formations. Similarly, [22] targets molecule generation and proposes to regard the graph structure as a parse tree from a context-free grammar, so that the VGAE can apply encoding and decoding to these parse trees. However, the sequential graph strategies usually are time-consuming or requiring expensive processing.\nOn the other hand, the global methods, which directly encode the graph in latent space and decode to the entire graph structure, have better scalability on larger and complicated graph structures and can be time efficient. GraphVAE [33] follows the VGAE idea and proposes an autoencoder model that can generate the graph structure, node features, and edge attributes all at once. EGNN [30, 26] decodes the node embedding to graph structures by applying the L2-norm between embeddings. GraphMAE [15] applies the masking strategy and targets to reconstruct the node features of various scales of graphs, where its GAE architecture also follows the classical GAE model. DiGAE [20] lies in the structural reconstruction on directed graphs, and firstly proposes to use the cross-correlation to express the node connection from two embedding spaces. Although these methods are effective recover node connections on a single graph, and even some of them tried the reconstruction of whole graph on graph tasks, there is no explicit evaluation to demonstrate how the global GAE model perform when it generate graph structure once and on moderate to large graph tasks. Besides, previous work follows the self-correlation strategy, which has been proven less effective than cross-correlation on graph tasks, in our work."}, {"title": "B Dimension Requirement to Well Represent Graph Structure", "content": "As the node embedding dimension d' is underexplored before, it is mostly regarded as a hyperpa- rameter to set up in advance. On the other hand, d' highly effects the encoder representing ability, which is based on the graph scale. There is necessity to discuss the dimension requirement of node embeddings for a certain graph scale.\nRemark. We share a toy example to demonstrate how the d' design has an impact on the encoder ability. Assume an extreme case d' = 1, each node is represented by a scalar. The node embeddings $(z_i, z_j, z_k) \\in \\mathbb{R}^3$ can never represent a connection set $(A_{i,j}, A_{i,k}, A_{j,k}) = (0,0,0)$. Because if $\\exists (z_i, z_j, z_k) \\in \\mathbb{R}^3$ such that $\\mathcal{I}(\\bar{A}_{i,j}, \\bar{A}_{i,k}) = (0, 0)$, i.e., $z_i z_j < 0$ and $z_i z_k < 0$, then we must have $\\text{sign}(z_jz_k) = \\text{sign}(z_jz_i^T z_iz_k) = -1$. This will always yield to $\\mathcal{I}(\\bar{A}_{j,k}) = 0 \\neq A_{j,k}$. The similar result can be directly observed when d' = 2 and there are four nodes, then $\\nexists (z_i, z_j, z_k, z_l) \\in \\mathbb{R}^4$ which can represent connection set $(A_{i,j}, A_{i,k}, A_{i,l}, A_{j,k}, A_{j,l}, A_{k,l}) = (0, 0, 0, 0, 0, 0)$.\nLemma B.1. For self-correlation method in the decoder, to make the connection constraints always solvable on the n-node scenario, i.e., requiring $z_i^T z_j > 0$ or $z_i^T z_j < 0$ for each node pair, the node embedding dimension d' need to satisfy d' \\geq (n - 1) at least.\nProof. We first prove that for n nodes, there is always existing a connection case, such as no connection on all node pairs, that $\\{z\\} \\in \\mathbb{R}^{n \\times d'}$ can represent when d' < (n \u2212 1). We consider the case that $A_{i,i} = 1$ and $A_{i,j} = 0$ for all $i, j \\in [1, n]$, such as $z_i^T z_j < 0$. When $d' < (n \u2212 1)$, e.g., d' = (n-2), there will be at most (n \u2013 2) linearly independent node vectors. Assume the first (n-2) vectors $z_1$ to $z_{n-2}$ are linearly independent, then we will always find a linear combination"}, {"title": "C Specific Graph Structure Representation", "content": "In Sec. 2.2 and 2.3, we explore the limitations of self-correlation and the effectiveness of cross- correlation in expressing specific graph structures. Given that previous GAE research often evaluates undirected asymmetric graph structures, the evaluation on special graph structures is usually over- looked. Hereby we evaluate how our method GraphCroc and other GAE models perform on specific graph structures as aforementioned.\nIsland (without self-loop) and symmetric graph structure. We generate 4 topologically symmetric graphs devoid of self-loops. Thus, the task is to have the evaluated GAE learn to reconstruct these graph structures and assess their performance. The visualization of their reconstruction is presented"}, {"title": "D Node Embedding Divergence in GraphCroc Decoder", "content": "The difference between two latent embeddings (denoted as $P$ and $Q$) is fundamental to cross- correlation as opposed to self-correlation in which $P = Q$; therefore, it is necessary to make them not converge to each other. One method of explicitly controlling this divergence is by incorporating regularization terms into the loss function, such as cosine similarity ($\\text{cos}(P, Q)$).\nOur decoder architecture inherently encourages differentiation between $P$ and $Q$ since they are derived from two separate branches of the decoder. This structure can allow $P$ and $Q$ to di- verge adaptively in response to the specific needs of the graph tasks. If a graph cannot be well"}, {"title": "E Loss Balancing Derivation", "content": "We adopt binary cross-entropy (BCE) loss to evaluate reconstruction between prediction $\\bar{A} = \\text{sigmoid}(PQ^T)$ and ground truth A. Our loss balancing is based on an i.i.d. assumption between $\\mathcal{L}_0$ and $\\mathcal{L}_1$, where the loss definition follows $\\mathcal{L}(i,j) = BCE(\\bar{A}_{i,j}, A_{i,j})$ on the node pair $(i, j)$. For a certain graph G, we assume there are $c_0$ zero elements and $c_1$ one elements in A, where $c_0 > c_1$. To balance the loss between zeros and ones, we apply scaling factors on each element loss: $\\mathcal{L}(\\bar{A}, A) = \\alpha_0 \\sum \\mathcal{L}^0 + \\alpha_1 \\sum \\mathcal{L}^1$. The scaling has two targets: to keep the loss magnitude and to balance the zero/one influence. Thus, we construct the following linear equations:\n$\\begin{cases}\n\\alpha_0 c_0 \\mathcal{L}^0 + \\alpha_1 c_1 \\mathcal{L}^1 = c_0 \\mathcal{L}^0 + c_1 \\mathcal{L}^1 \\\\\n\\alpha_0 c_0 \\mathcal{L}^0 = \\alpha_1 c_1 \\mathcal{L}^1\n\\end{cases} \\implies \\begin{cases}\n\\alpha_0 = \\frac{c_0 + c_1}{2c_0} \\\\\n\\alpha_1 = \\frac{c_0 + c_1}{2c_1}\n\\end{cases}$\nThe scaling factors $\\alpha_0$ and $\\alpha_1$ are derived."}, {"title": "F Supplementary Experimental Setup", "content": "F.1 Dataset\nWe provide the graph detail of graph tasks selected in our evaluation, in Table 5. For IMDB-B and COLLAB without node features, we take the one-hot encoding of degree as the node features.\nF.2 Other Experimental Setup Information\nWe provide a three-layer GraphCroc to demonstrate the detailed data flow and the model structure, in Figure 9. Besides, we list the detailed configuration of GraphCroc model and corresponding training setup for all graph tasks in Table 6. The reconstruction results in Table 1 are not provided in average on multiple runs, because the reproduction on several experiments is too heavy loaded. For example, due to the large graph size, the default setting (vector dimension of 128 and layer number of 4) in EGNN when reproducing the PPI task will cause the out-of-memory issue on the 40GB A100 GPU. While reducing the dimension to 16 and the layer number to 3 allows model and data to fit just into GPU (38.40GB/40GB), this significantly simplified model fails to adequately learn the structure of the PPI graphs and performs poorly compared to other GAE methods. In addition, given that graph reconstruction must be conducted by graph and QM9 task has massive graphs, even one model training on QM9 will take over 2 GPU days."}, {"title": "F.3 Adversarial Attack on GraphCroc", "content": "In Section 4.5, we evaluate the GAE performance as an adversarial attack surface. Specifically, given a pretrained encoder $\\Phi(Z|G)$ which encodes the graph into a latent space and a downstream classifier $f(Z)$ for the graph classification task, we aim to generate a perturbed latent representation $Z'$ and leverage a reconstructor $\\Theta$ to rebuild the graph structure $G' = (X, \\Theta(Z'))$. The goal of the adversarial structure is to cause the encoder and downstream classifier to misclassify the graph, i.e., $f(\\Phi(G')) \\neq y$, where y is the original label, and the only difference between G and G' is the adjacency matrix A. We assess two gradient-based adversarial attacks on latent space.\nProjected Gradient Descent (PGD) [27]: PGD iteratively perturbs the input to maximize the classification loss of $f(Z)$:\n$\\delta_{t+1} = \\text{Proj}_{||\\delta||_1<\\epsilon}(\\delta_t + \\alpha \\cdot \\text{sign}(\\nabla_{\\delta_t} \\mathcal{L}(f(Z), y)))$\nCarlini & Wagner (C&W) [4]: C&W finds adversarial latent vectors by solving an optimization problem:\n$\\delta^* = \\underset{\\delta}{\\text{arg min}} \\; ||\\delta||_1 + c \\cdot (\\underset{i \\neq y}{\\text{max}} f(Z)_i - f(Z)_y + k)$\nHere, $f(Z)_y$ denotes the logits output of the classifier for the true class y, and k is a confidence parameter that controls the confidence of the misclassification. This optimization can be solved with gradient descent. Hence, the final adversarial graph structure will be $G' = (X, \\Theta(Z + \\delta^*))$. To enhance the performance of adversarial perturbation, we fine-tune the reconstructor during the adversarial attack. Specifically, to ensure the effectiveness of the reconstructed adversarial example, we optimize by minimizing the distance between the perturbed latent representation and the latent space of the reconstructed graph structure:\n$\\Theta^* = \\underset{\\Theta}{\\text{arg min}} \\; ||Z + \\delta^* - \\Phi(X, \\Theta(Z + \\delta^*)||$"}, {"title": "G Supplementary Experiment Results", "content": "G.1 Structural Reconstruction of Cross-Correlation on Other Architectures\nIn addition to the GCN kernel used in our GraphCroc model, we extend our analysis to include other widely used graph architectures such as GraphSAGE [13], GAT [36], and GIN [45]. To incorporate these architectures into the cross-correlation framework, we replace the GCN module with corresponding operations while preserving the overarching structure, which includes the encoder,"}]}