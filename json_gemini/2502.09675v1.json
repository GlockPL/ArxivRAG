{"title": "Multi-level Conflict-Aware Network for Multi-modal Sentiment Analysis", "authors": ["Yubo Gao", "Haotian Wu", "Lei Zhang"], "abstract": "Multimodal Sentiment Analysis (MSA) aims to recognize\nhuman emotions by exploiting textual, acoustic, and visual\nmodalities, and thus how to make full use of the interac-\ntions between different modalities is a central challenge of\nMSA. Interaction contains alignment and conflict aspects.\nCurrent works mainly emphasize alignment and the inher-\nent differences between unimodal modalities, neglecting the\nfact that there are also potential conflicts between bimodal\ncombinations. Additionally, multi-task learning-based con-\nflict modeling methods often rely on the unstable generated\nlabels. To address these challenges, we propose a novel\nmulti-level conflict-aware network (MCAN) for multimodal\nsentiment analysis, which progressively segregates alignment\nand conflict constituents from unimodal and bimodal repre-\nsentations, and further exploits the conflict constituents with\nthe conflict modeling branch. In the conflict modeling branch,\nwe conduct discrepancy constraints at both the representation\nand predicted output levels, avoiding dependence on the gen-\nerated labels. Experimental results on the CMU-MOSI and\nCMU-MOSEI datasets demonstrate the effectiveness of the\nproposed MCAN.", "sections": [{"title": "1. INTRODUCTION", "content": "In recent years, multimodal sentiment analysis (MSA) has\nattracted increasingly widespread attention [1, 2, 3, 4]. Be-\ncause of the heterogeneity among multimodal data, how to\neffectively fuse the representations of different modalities\nand ensure the semantic integrity of modalities is an impor-\ntant research topic in the community of MSA [5]. Some of\nthe earlier works focus on the interaction between different\nmodalities on low-level features, which results in limited\nfusion performance [1, 6, 7]. Inspired by the attention mech-\nanism's [8] high-level relationship modeling capabilities, in-\ncreasing MSA methods introduced attention when fusing uni-\nmodal representations. For example, Multimodal transformer\n(MulT) [2] employs the cross-modal attention mechanism\nto capture multimodal sequence interactions across different\ntime steps. Some other works, such as Text Enhanced Trans-\nformer Fusion Network (TETFN) [9], Fine-grained Tri-modal\nInteraction Model (FGTI) [4], multimodal 3D stereoscopic\nattention [10], etc. have also witnessed the success of the\nattention-based methods in MSA application.\nThese methods fuse cross-modal features well but ignore\nthe inherent information and potential conflicts of individual\nmodalities, making the fused information somewhat incom-\nplete. Some studies have noted this problem, either mapping\nunimodal representations to modality-invariant and modality-\nspecific spaces and modeling them separately subsequently\nfor fusion [3, 11, 12], or leveraging the multi-task learning\n(MTL) framework to model inter-modal differences in a su-\npervised learning mode through unimodal label generation\n[13, 14] or manual annotation [15].\nHowever, these approaches still suffer from some short-\ncomings. First, there is still a potential conflict between emo-\ntional information contained by different bimodal combina-\ntions. Considering only inter-unimodal differences is not suf-\nficient. For example, the combination of a smiling expres-\nsion and a positive word is positive, whereas audio represents\nsarcasm. In this case, the combination of textual and visual\nmodalities and the combination of textual and acoustic modal-\nities would conflict with the emotional polarity. Secondly, for\nthose methods based on MTL, manual annotation of unimodal\nlabels is costly, whereas label generation methods [13, 14]\nrely on the quality of unimodal and cross-modal representa-\ntions, and binary partitioning of the representation center may\nsuffer from insufficient granularity.\nTo address these challenges, we propose a multi-level\nconflict-aware network (MCAN) that models consistency and\ndiscrepancy from different levels. Specifically, the MCAN\nis divided into the main branch and the conflict modeling\nbranch. Wherein, the main branch progressively models\nthe relationship between unimodal and bimodal representa-\ntions utilizing Micro Multi-step Interaction Network (Micro-\nMSIN) and Macro Multi-step Intersection Network (Macro-\nMSIN) and segregates the inter-unimodal and inter-bimodal\nconflict components hierarchically, then feeds them to the\nconflict modeling branch. The conflict modeling branch mod-\nels inter-unimodal and inter-bimodal conflicts through mi-\ncro conflict-aware cross-attention (Micro-CACA) and macro\nconflict-aware cross-attention (Macro-CACA), respectively."}, {"title": "2. METHODOLOGY", "content": "The framework of the proposed multi-level conflict-aware\nnetwork (MCAN) is shown in Figure 2. MCAN first con-\nducts feature extraction for the three input modalities. For\nlanguage modality, we feed the input text into BERT to ob-\ntain the language feature $F_t$. While LSTM is adopted to\ncapture the intra-modality interaction $F_v$ and $F_a$ for visual\nand audio modalities.\n2.1. Main Branch\nThe function of the main branch is to progressively fuse and\nalign cross-modal representations of different granularities\nand to segregate conflict constituents. The two core com-\nponents of the main branch are Transformer-style modules:\nMicro-MSIN and Macro-MSIN. Micro-MSIN receives $F_t$\nand $F_v$, $F_t$ and $F_a$ as inputs, and obtains the outputs $F_{t,a}$ and\n$F_{t,v}$. Then, inspired by [16, 17], we conduct Singular Value\nDecomposition (SVD) of $F_{t,a}$ and $F_{t,v}$, and reconstruct the\ntop $k$ singular values and the corresponding eigenvectors\ninto alignment constituents ($F_{t,a}^{aligned}$ and $F_{t,v}^{aligned}$), which\nare fed to the Macro-MSIN. The remaining singular values\nand their corresponding eigenvectors are reconstructed into\nconflict constituents ($F_{t,a}^{conflict}$ and $F_{t,v}^{conflict}$) to be delivered\nto the conflict modeling branch.\nMacro-MSIN receives $F_{t,a}^{aligned}$ and $F_{t,v}^{aligned}$ as inputs\nand obtains the fused representation $F_c$, the aligned con-\nstituent $F^{aligned}$, and the conflicting constituent $F^{conflict}$\nthrough a similar computational process to that of Micro-\nMSIN. The purpose of Macro-MSIN is to fully fuse and align\nthe bimodal representations and separate out the conflict con-\nstituents between the bimodal representations. The cascade\nof Micro-MSIN and Macro-MSIN can make the modeling of\nMSA modal relationships more adequate and complete.\n2.1.1. Micro Multi-step Interaction Network\nThe Micro-MSIN modules receive the $F_t$ and $F_a$, $F_t$ and $F_v$\nas inputs. Following previous work [18, 14, 19, 20, 21], we\ntreat the textual modality as the main contributing modality\nand thus do not set Micro-MSIN between $F_a$ and $F_v$. It con-\nsists of multiple layers of Cross-Transformers. Taking the\naudio-text pairs as an example, the outputs of $(i - 1) \u2013 th$\nlayer are $F_t^{(i-1)} \\in \\mathbb{R}^{n_t \\times d}$ and $F_a^{(i-1)} \\in \\mathbb{R}^{n_a \\times d}$, which will\nbe fed to i th Cross-Transformer layer. For textual modal-\nity, $F_t^{(i-1)}$ is transformed into Query to interact with audio\nmodal input features, which are transformed into Key and"}, {"title": null, "content": "Value. The computation for the multi-head cross-modal at-\ntention of textual modality is given as follows:\nheadit = SoftMax( $(F_t^{(i-1)}W_Q)^T(F_a^{(i-1)}W_K)\\over \\sqrt{d_k}$ ) $F_a^{(i-1)}W_V$ \n(1)\nMultiHeadit = Concat (head1,..., heade)W_O (2)\nwhere $W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d_k}$, $W_O \\in \\mathbb{R}^{e d_k \\times d}$, e is the\nnumber of attention heads. For audio modal, $F_a^{(i-1)}$ will be\ntransformed into a Query and $F_t^{(i-1)}$ will be transformed into\nKey and Value, then conduct attention computation. Then,\nthe output of cross-modal attention is processed by residual\nconnection, layer normalization and feed-forward neural net-\nwork (FFN), which is similar to na\u00efve Transformer, and yield\noutput of the i \u2013 th interaction layer $F_g^{(i)}, g \\in t,a$. Assuming\nthat the Micro-MSIN has a total of I layers, the output of the\nlast layer is noted as $F_{t,a}$.\n$F_{t,a}$ = Concatenate($F_t^I, F_a^I$) (3)\nTo retain the alignment constituents and separate the con-\nflict constituents to the greatest extent possible, we perform\nSVD, $F_{t,a} = U \\Sigma V^T \\in \\mathbb{R}^{m \\times n}, \\Sigma \\in \\mathbb{R}^{h \\times h}$. In this case,\nthe largest k singular values and the corresponding eigenvec-\ntors are considered to be the parts with significant alignment\ndenoted as $F_{t,a}^{aligned}$, while the remaining singular values and\nthe corresponding eigenvectors are regarded as the parts with\ninsignificant alignment, i.e., conflicting, and are denoted as\n$F_{t,a}^{conflict}$.\n$F_{t,a}^{aligned}$ = $U_{m \\times k} \\Sigma_{k \\times k} V_k^T$\n$F_{t,a}^{conflict}$ = $U_{m \\times (h-k)} \\Sigma_{(h-k) \\times (h-k)} V_{h-(k) \\times n}^T$ (4)\nFor text-visual pairs, the similar computation process is con-\nducted, which yields $F_{t,v}^{aligned}$ and $F_{t,v}^{conflict}$ as outputs.\n2.1.2. Macro Multi-step Interaction Network\nMacro-MSIN serves to model the alignment constituents\nand conflict constituents between bimodal representations.\nMacro-MSIN receives $F_{t,a}^{aligned}$ and $F_{t,v}^{aligned}$ as inputs, and\nits outputs are shown in the following calculations:\n$Z^{aligned}, Z^{conflict}$ = Macro-MSIN($F_{t,a}^{aligned}, F_{t,v}^{aligned}$) (5)\nMicro-MSIN is more fine-grained compared to Macro-MSIN,\nand they are cascaded to progressively align cross-modal rep-\nresentations at different levels and effectively disentangle\nconflict knowledge.\n2.2. Conflict Modeling Branch\nThe conflict modeling branch was designed to receive con-\nflict constituents at different levels from the main branch,\nand model task conflict in terms of both representations and\npredicted outputs. It mainly consists of Micro Conflict-aware\nCross-Attention (Micro-CACA) and Macro Conflict-aware\nCross Attention (Macro-CACA), which are employed for fur-\nther modeling of conflicts between unimodal representations\nand bimodal representations, respectively."}, {"title": null, "content": "2.2.1. Micro Conflict-aware Cross-attention\nThe role of Micro-CACA is to adaptively fuse conflict con-\nstituents into unimodal representations. To illustrate with the\nconflict\ncase of text-visual pairs, the conflict constituent $F_{t,v}^{conflict}$\nfrom the main branch will be transformed into Query. The\noutput of the textual modality obtained after Micro-CACA\nprocessing is $F_t^v$\n$F_t^v$ = SoftMax( $F_{t,v}^{conflict} W_Q^O (F_t W_K^O)^T\\over \\sqrt{d_c}$ ) $F_t W_V^O$ (6)\nSimilarly, we can obtain Micro-CACA outputs $F_a^t$ and\n$F_v^t$ for visual and acoustic modalities. In particular, the two\nMicro-CACAs will generate two textual modal representa-\ntions, which we average as the final outputs.\nTo further emphasize the discrepancy between unimodal\nrepresentations, we impose orthogonal constraints on $F_t^a, F_t^v$\nand $F_a^v$:\n$L_{mi c r o}^{o r t h o}$ = $\\sum_{p \\in {t, v, a}} \\sum_{q \\neq p}$ $\\langle F_t^p, F_t^q \\rangle^2_F$ (7)\nFurthermore, we set individual FFN prediction heads for\n$F_t^a, F_t^v$ and $F_a^v$ and encourage them to generate distinct predic-\ntions as much as possible to further emphasize the conflicting\naspects between unimodal representations at the level of the\nprediction outputs.\n$L_{mi c r o}^{d i f f}$ = $\\sum_{p \\in {t, v, a}} \\sum_{q \\neq p}$ $|| \\hat{y}^p - \\hat{y}^q ||_2^2$ (8)\n2.2.2. Macro Conflict-aware Cross-attention\nThe process of Macro-CACA is similar to that of Micro-\nCACA. Macro-CACA receives the separated conflict con-\nstituents of the main branch Macro-MSIN and transforms"}, {"title": null, "content": "them into the Query of cross attention to capture and adap-\ntively fuse inter-bimodal (between $F_{t,a}$ and $F_{t,v}$) conflicts.\nSimilarly, the discrepancy constraints at the representation\nlevel and the predicted output level of Macro-CACA are\nrepresented as follows:\n$L_{m a c r o}^{o r t h o}$ = $|F_{t,v}^{m a c r o} - F_{t, a}^{m a c r o}|_F^2$ , $L_{m a c r o}^{d i f f}$ = $| \\hat{Y}_{t,v} - \\hat{Y}_{t,a} |^2$ (9)\nwhere $F_{t,v}^{macro}$ and $F_{t,a}^{macro}$ are features extracted by Macro-CACA,\n$\\hat{y}_{t,v}$ and $\\hat{y}_{t,a}$ are predicted outputs of $F_{t,v}^{macro}$ and $F_{t,a}^{macro}$. The final\nloss function is represented as follows:\n$L = L_{main} + \\alpha(L_{micro}^{ortho} + L_{macro}^{ortho}) + \\beta(L_{micro}^{diff} + L_{macro}^{diff})$ (10)\nwhere $L_{main}$ is mean squared error loss, $\\alpha$ and $\\beta$ are trade-off\nparameters to control the intensity of conflict modeling.\n3. EXPERIMENT\n3.1. Datasets, Metrics and Implementation Details\nWe evaluate MCAN on CMU-MOSI [22] and CMU-MOSEI\n[23] datasets, which are the most widely used benchmark for\nMSA. Five different metrics are employed to evaluate the per-\nformance of MCAN and baselines: binary accuracy (Acc2),\n7-class accuracy (Acc7), F1 Score (F1), Pearson correlation\n(Corr), and mean absolute error (MAE). For the Experimental\nsetting, $\\alpha$ and $\\beta$ are set to 1e-2 and 1e-3, respectively. Adam\nis adopted as the optimizer with an initial learning rate 5e-5\nfor BERT and 1e-4 for other parameters. Additionally, We se-\nlect the top-44 singular values and the corresponding eigen-\nvectors for generating the alignment constituents\n3.2. Comparison with Baselines\nTo validate the effectiveness of our proposed method, the\nbaselines we chose cover classical MSA methods, and recent"}, {"title": "4. CONLUSION", "content": "In this paper, we develop a novel MCAN for MSA. To balance the\ndiscrepancies between unimodal and bimodal representations while\nfusing and aligning cross-modal representations, MCAN is divided\ninto a main branch and a conflict modeling branch, which are jointly\ntrained in a multi-task learning manner. The former progressively\nextracts different levels of cross-modal alignment and segregates\nthe conflict constituents through the cascade of Micro-MSIN and\nMacro-MSIN, while the latter receives these conflict constituents\nand further models the conflicts. The experimental results show\nthat MCAN outperforms the current state-of-the-art methods. In\nfuture work, we will endeavor to further analyze the modal conflict\nproblem at the optimization level (e.g. gradient) and improve the\nproposed method.\n5. ACKNOWLEDGE\nThe work was supported by the National Natural Science Foundation\nof China (No.72271017)"}]}