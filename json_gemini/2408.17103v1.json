{"title": "Understanding the User: An Intent-Based Ranking Dataset", "authors": ["Abhijit Anand", "Jurek Leonhardt", "Venktesh V", "Avishek Anand"], "abstract": "As information retrieval systems continue to evolve, accurate evaluation and benchmarking of these systems become pivotal. Web search datasets, such as MS MARCO, primarily provide short keyword queries without accompanying intent or descriptions, posing a challenge in comprehending the underlying information need. This paper proposes an approach to augmenting such datasets to annotate informative query descriptions, with a focus on two prominent benchmark datasets: TREC-DL-21 and TREC-DL-22. Our methodology involves utilizing state-of-the-art LLMs to analyze and comprehend the implicit intent within individual queries from benchmark datasets. By extracting key semantic elements, we construct detailed and contextually rich descriptions for these queries. To validate the generated query descriptions, we employ crowdsourcing as a reliable means of obtaining diverse human perspectives on the accuracy and informativeness of the descriptions. This information can be used as an evaluation set for tasks such as ranking, query rewriting, or others.", "sections": [{"title": "1 INTRODUCTION", "content": "In information retrieval (IR), a core challenge in building ranking models is to explicitly or implicitly aligning the actual user intent with the machine intent, i.e., the intent as understood by the ranker. This misalignment stems from the inherent complexity and variability in how users articulate their information needs versus how these needs are interpreted and processed by retrieval systems. This misalignment might be due to multiple reasons - ambiguity, poorly formulated queries, complex queries, or a retrieval set that lacks relevant documents [5, 13].\nMost current research on ranking models in IR is based on training parameterized models over large training datasets from MS MARCO [15]. However, to the best of our knowledge, there exist no recent datasets that attempt to measure the chasm between user intent and machine intent. The current practice of measuring ranking performance is through sparsely [15] or densely annotated ad-hoc ranking test sets [7-11] that provide queries and corresponding relevance annotations. While these test sets allow for determining the overall effectiveness of a ranker, they fail to provide a way of measuring the extent to which the ranking models understand the true intent of the user. For example, consider the query \"what are the three countries in 1984\". While the intent-to identify the three countries mentioned in George Orwell's novel \u201c1984\u201d seems clear, it remains difficult to rank effectively because it requires specific contextual knowledge that may not be directly available in the retrieved documents. Another example is the query \"slow cooking food\" (cf. Fig. 1). Although this query appears to be straightforward, it can have multiple intents. This multiplicity of potential intents complicates the ranking process, as the system needs to correctly infer and prioritize the user's actual intent to provide relevant results. Knowing the user's intent allows the model to retrieve and rank documents most relevant to that intent, thereby addressing a critical challenge in handling ambiguous queries.\nIn this paper, we specifically focus on a subset of these challenges: queries that contain multiple intents. We propose a new dataset named DL-MIA (MS MARCO Intent Annotations), which"}, {"title": "2 RELATED WORK", "content": "Several ranking datasets have been published that consider the concept of what we refer to as user intents. Most notably, the data provided for the TREC-WEB track [5] customarily includes topics (queries) along with topic descriptions as well as, in many cases, subtopics. These subtopics represent various distinct aspects that each topic may have. The data further includes relevance judgments for documents from the CLUEWEB collections w.r.t. the topics and subtopics. However, the TREC-WEB track has been discontinued after 2014, and CLUEWEB corpora are not freely available. Our dataset is similar, as the subtopics are essentially user intents.\nThe MS MARCO ranking dataset [15], which has emerged as one of the most widely used collections for IR-related tasks in recent years, contains a large number of training and evaluation queries. Furthermore, the TREC-DL track [7, 10] provides annotated test sets of queries and corresponding relevance annotations. More recently, the second version of the MS MARCO corpus, which is significantly larger than the first version, was released to be used in the TREC-DL 2021 track and onward [8, 9, 11].\nMackie et al. [13] showed that queries (topics) within TREC-DL vary with respect to their complexity (and, hence, difficulty) and released the DL-HARD dataset. Along with relevance annotations, this dataset assigns intent categories to each query. Similarly, intent taxonomies have been proposed for web search in general [3] as well as legal case retrieval [20]. The difference compared to our work is that we annotate specific user intents rather than categories.\nAnother related line of work deals with the reformulation of complex queries. Mackie et al. [14] recently released the CODEC collection for document and entity ranking, which also contains query reformulations. Salamat et al. [18] showed that the way queries are worded has an impact on their corresponding ranking performance. Our proposed user intents can be seen as reformulations that focus on specific aspects of the original query."}, {"title": "3 THE DL-MIA DATASET", "content": "In this section, we introduce the DL-MIA dataset by outlining the creation and annotation process and presenting some statistics."}, {"title": "3.1 Dataset Creation", "content": "The process of creating the dataset comprises several key stages: generating candidate intents using an LLM (Section 3.1.1), clustering and manual refinement of intents (Section 3.1.2), crowdsourcing annotations (Section 3.1.3), merging similar intents (Section 3.1.4) and QRel creation (Section 3.1.5). This process is illustrated in Fig. 2."}, {"title": "3.1.1 Generating Candidate User Intents.", "content": "For all queries in the TREC-DL-21 and '22 test sets, we retrieve all relevant passages using their respective QRel files. We then cluster similar passages per query. To achieve this, we first obtain passage embeddings using Sentence-BERT [16] and then group passages into the same cluster if their pairwise cosine similarity exceeds a threshold of 0.8. In the next step, we select the query and passages from the clusters to give to the LLM to generate five distinct intents relevant to the query-passage pairs. We employ the GPT-4 model with the prompt given below. We use a temperature value of 0.6 to control randomness which helps in getting diverse intents."}, {"title": "3.1.2 Clustering and Intent Selection.", "content": "After generating intents, we cluster similar intents using the SBERT embedding and cosine similarity approach as described above. We group intents that are similar in meaning if their pairwise cosine similarity exceeds a threshold of 0.9. This clustering process helps in reducing redundancy and coming up with distinct intents. After clustering, we do manual selection, where we examine the clustered intents and choose the most relevant ones for each cluster. We do this to remove irrelevant intents or hallucinated text by the LLM. If any intents are found to be incomplete or poorly written, they are manually rewritten to improve their clarity and comprehensiveness. This ensures that the intents are well-defined and useful for the next stages of the dataset creation process. After this process, only queries with 2 or more intents were selected which resulted in 26 queries."}, {"title": "3.1.3 Crowdsourcing Annotation.", "content": "The next step involves crowdsourcing to annotate the intents with the relevant passages. Our pool of annotators comprises volunteers who are computer scientists and graduate school students familiar with ranking tasks for search. Annotators are presented with a query and a passage and are asked to determine which of the provided intents the passage satisfies. Additionally, annotators are given the option to add or modify intents if they find that the existing ones do not capture the"}, {"title": "3.1.4 Manual Review and Merging of Intents.", "content": "In order to improve data quality and avoid redundancy, we conduct a manual review and merge intents. We evaluate the intents suggested by the annotators and integrate them into the existing set of intents where appropriate. E.g., in Fig. 2 we merge \"when to call 311\" and \"when to call 311 rather than 911\" into a single intent. Any passage-intent pair which does not have at least two annotators is dropped to ensure that the final set of intents reflects a consensus among multiple annotators. The merging process also helps in consolidating similar intents and removing any redundant or less relevant ones. After this process, we end up with 24 queries. We further elaborate on different scenarios we encountered during this phase in Appendix D"}, {"title": "3.1.5 Scoring and Creating QRel File.", "content": "Finally, we score the intentpassage pairs and create a QRel file for ranking. The scoring is based on the annotations provided by the participants. Each intentpassage pair is scored as follows: a score of 0 is assigned if no annotator marked the intent, a score of 1 is assigned if at least one annotator marked the intent, and a score of 2 is assigned if all annotators marked the intent. These scores reflect the level of agreement among the annotators and the relevance of the intent to the passage. The final query-intent-passage-score mappings are compiled into a QRel file, which is used for ranking. This QRel file serves as ground-truth for evaluating information retrieval systems, ensuring that the dataset can be effectively used for further research and application."}, {"title": "3.2 Statistics", "content": "Initially, the dataset included 118 queries from TREC-DL-21 and '22. Through a process of clustering and intent selection, 26 queries were identified as suitable for annotations, as these queries had two or more distinct intents (69 in total). After annotation (Sec. 3.1.3), a manual review and merging of intents were performed (Sec. 3.1.4). This process was necessary because the number of intents increased from 69 to 171 due to annotators adding custom intents. Hence, this review process was crucial in refining the dataset and ensuring the accuracy and clarity of the intents. After this rigorous review, 24 queries and 69 intents were finalized for inclusion in the dataset with 2655 relevance annotations present in the final QRel file. The distribution of relevant passages per intent is shown in Fig. 3.\nBecause annotators were able to add custom intents, computing established agreement measures is difficult as the intents annotated by humans may have different granularities; however, the relevance"}, {"title": "3.3 Tasks and Evaluation", "content": "The DL-MIA dataset can be used for several tasks, such as:\nIntent-based ranking aims at improving the document ranking by understanding different user intents and ensuring that the returned documents are relevant to the intent. This can be evaluated using metrics like nDCG@10.\nDiversity of search results aims at ensuring that document rankings provide diverse sets of responses that cover various aspects of the query to satisfy users information needs, evaluated using metrics like a-nDCG@10.\nIntent-based summarization aims at generating a summary that covers multiple intents of a query, evaluated using metrics such as ROUGE or BLEU.\nUser and machine intent alignment aims at bridging the gap between user and machine intent through query rewriting to fully specify the intent [2]. DL-MIA aids in training generative models that can generate intents more aligned with real-world user intents."}, {"title": "4 EXPERIMENTS", "content": "In order to demonstrate the utility of DL-MIA, we conduct experiments using a number of simple baselines: BM25 [17] is a lexical model which is also used as a first-stage retriever for re-rankers. BERT [12] is a cross-attention re-ranker (BERT-base, 12 layers). The input length is restricted to a maximum of 512 tokens. The model is trained on MS MARCO passage data using a pointwise ranking loss objective with a learning rate of 1e-5. COLBERTV2 [19] is a multi-vector late-interaction re-ranking model that computes token-wise representations for the query and document and estimates relevance using the MaxSim operation."}, {"title": "4.1 Results", "content": "We report results on two of the tasks outlined in Section 3.3, namely intent-based ranking and diversity of search results. We present these results in Table 1. Note that we evaluate two settings: First, we use the original queries, but evaluate using the user intent-based QRels (i.e., assuming that the user had one specific intent in mind). Second, we treat the user intents as queries directly. The results show that, unsurprisingly, specifying the actual user intent as the query results in better performance than using the (more general) original queries. We additionally demonstrate the diversity ranking performance of various models using the a-nDCG@10 metric. To achieve this in the second setting (where user intents are treated as queries), we employ reciprocal rank fusion [6] with k = 60. This technique is applied to the intent-based rankings to generate a unified ranking for the original query. Overall, COLBERTV2 shows the best performance.\nFinally, we closely examine the ranking performance corresponding to each user intent in Fig. 4. The results are in line with Table 1.\nThe key takeaway from these results is the necessity of specifying concrete user intents; in other words: if a user has a specific information need, it is necessary to provide that intent as a clear, unambiguous query to a search engine."}, {"title": "5 CONCLUSION", "content": "In this paper, we have created the DL-MIA dataset to understand user intents, thereby satisfying information needs more effectively. We have used queries from TREC-DL-21 and TREC-DL-22, generated intents using an LLM, and crowd-sourced relevance annotations. DL-MIA can be used for a variety of tasks; we present performance of different models on ranking and diversity tasks, showing the importance of this dataset for fulfilling user information needs. For future work, we plan to extend DL-MIA to include queries from TREC-DL-19, TREC-DL-20, and DL-HARD."}, {"title": "A GENERATING INTENTS USING LLM", "content": "The objective of this step is to generate a diverse set of user intents that accurately reflect the informational needs expressed by the user query. To achieve this, we first retrieve all relevant passages for each query from the QRel file. We observed a significant amount of redundancy among these passages, which could lead to duplication in intent generation when using a large language model (LLM). \u03a4\u03bf mitigate this, we cluster similar relevant passages for each query before proceeding with intent generation. Several methods were explored, including entailment-based approaches, but we found that clustering using cosine similarity with Sentence-BERT [16] (detailed in Appendix B) yielded the best results. For the query \"What is 311 for\" in Table 2, there are 53 relevant passages. After applying passage clustering, these were reduced to 18 distinct clusters. Next, we give the query and 18 passages too LLM for intent generation. Subsequently, we generated intents using an LLM, resulting in 10 intents, as shown in Table 2.\nWe experimented with various prompts (some examples are shown in Fig below) following the Context-Aware Query Expansion (CAQE) method [1], which generates intents based on a query and a relevant document/passage pair. However, this approach resulted in a large number of intents, many of which were duplicates, as they were derived from individual <query, passage> pairs. To address this issue, we expanded the context to include a list of passages, which allowed us to generate a smaller number of high-quality intents by considering the collective context of all relevant passages (prompt in Section 3.1.1)."}, {"title": "B INTENT CLUSTERING", "content": "As outlined in Section A, we perform clustering both before (passage clustering) and after (intent clustering) intent generation using a large language model (LLM) to eliminate redundancy. The same clustering approach is applied in both stages. For a given query, we first obtain embeddings for all passages or intents using SentenceBERT (SBERT). Next, we select a passage/intent Pi/I\u00a1 and identify all other passages/intents that have a pairwise cosine similarity"}, {"title": "C ANNOTATION VIA CROWDSOURCING", "content": "The collection of user intent annotations for DL-MIA is performed using a custom web application we implemented using the OTREE framework [4].\nEach participant is presented with detailed instructions how to perform the task (cf. Fig. 5) in the beginning. The subsequent pages display one (sub)query each along with a list of the corresponding passages and intent candidates (cf. Fig. 6). The interface ensures that each passage either has at least one annotated relevant intent or it is specifically indicated that the passage in question is not relevant to the query at all. By displaying all passages and intents within the same page we make sure that the participant always maintains a mental overview over the distinct intents (i.e., aspects) of the current query.\nThe collected data is stored in a PostgreSQL database. After the collection is complete, OTREE provides functionality to export the relevant data in CSV format. Our web application for data collection is publicly available."}, {"title": "D CLEANUP AND MERGING OF INTENTS POST-ANNOTATION", "content": "After the annotation phase, we perform a manual analysis of intents and corresponding relevance annotations for the passages. We show some of the scenarios and related intents in Table 3. We observe from example one that Intent 2 and Intent 3 are semantically similar and can be considered as redundant and hence are merged. When merging the intents, we also combine the ratings suing the following guidelines:\n\u2022 We set the score based on number of annotations for the intents to be merged that indicate the level of agreement between annotators and relevance to the query\n\u2022 For instance, if two annotators annotate 1 for both intents paired with a certain passage, we assign the relevance score as 2 for the merged intent.\n\u2022 If only one annotator assigned a score of 1 to both intents for the same passage we assign the score as 1.\nApart from redundant intents, we also observed cases where the machine generated or the custom intents from the user were not relevant to the core aspects of the query. For instance, in example 2 in Table 3, \"the cost of Tuk-tuks\u201d is irrelevant to the query which"}]}