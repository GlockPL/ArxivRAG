{"title": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!", "authors": ["Dacheng Li", "Shiyi Cao", "Tyler Griggs", "Shu Liu", "Xiangxi Mo", "Eric Tang", "Sumanth Hegde", "Kourosh Hakhamaneshi", "Shishir G. Patil", "Matei Zaharia", "Joseph E. Gonzalez", "Ion Stoica"], "abstract": "Large reasoning models (LRMs) tackle complex reasoning problems by following long chain-of-thoughts (Long CoT) that incorporate reflection, backtracking, and self-validation. However, the training techniques and data requirements to elicit Long CoT remain poorly understood. In this work, we find that a Large Language model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank adaptation (LoRA). With just 17k long CoT training samples, the Qwen2.5-32B-Instruct model achieves significant improvements on a wide range of math and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0% (+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's score of 44.6% and 59.1%. More importantly, we find that the structure of Long CoT is critical to the learning process, whereas the content of individual reasoning steps has minimal impact. Perturbations affecting content, such as training on incorrect samples or removing reasoning keywords, have little impact on performance. In contrast, structural modifications that disrupt logical consistency in the Long CoT, such as shuffling or deleting reasoning steps, significantly degrade accuracy. For example, a model trained on Long CoT samples with incorrect answers still achieves only 3.2% lower accuracy compared to training with fully correct samples. These insights deepen our understanding of how to elicit reasoning capabilities in LLMs and highlight key considerations for efficiently training the next generation of reasoning models. This is the academic paper of our previous released Sky-T1-32B-Preview model.", "sections": [{"title": "1. Introduction", "content": "Large reasoning models (LRMs) leverage long chain-of-thoughts (Long CoTs) with reflection, backtracking, and self-validation to tackle challenging reasoning tasks (Jaech et al., 2024; Guo et al., 2025; Team, 2024). However, the process of eliciting Long CoTs from available LLMs remains unclear, as existing methods are either closed-sourced (Jaech et al., 2024; Team, 2024) or expensive to replicate (Guo et al., 2025).\nIn this paper, we first show that, surprisingly, an LLM can be cheaply and easily taught to produce Long CoT responses, significantly improving its reasoning capabilities. In particular, we find that this learning process can be both data-efficient and parameter-efficient. By performing fully supervised fine-tuning (SFT) with only 17k samples generated by DeepSeek R1, the Qwen2.5-32B-Instruct model achieves performance competitive with OpenAI o1-preview across a wide range of math and coding tasks (Team, 2024; Yang et al., 2024; Jaech et al., 2024). In particular, it achieves 90.8% in Math-500 (+6.0%), 56.7% in AIME 2024 (+40.0%), 85.0% in AMC 2023 (+17.5%), 60.3% in OlympiadBench (+12.7%) and 57.0% in LiveCodeBench (+8.1%) (Jain et al., 2024). Even further, the model can achieve o1-preview performance by updating fewer than 5% parameters with LoRA fine-tuning (Hu et al., 2021). We show that the model successfully learns to reflect and revise its intermediate thoughts (e.g., frequently using reasoning keywords such as \"Alternatively\u201d and \u201cWait, but\u201d) and adopts long, coherent CoTs to tackle challenging problems (Fig. 1).\nMoreover, we identify the Long CoT structure as the key characteristic of distilled data for eliciting strong reasoning performance rather than the specific contents of individual reasoning steps within the Long CoT. To test this, we conduct two sets of controlled studies by altering either the content of individual reasoning steps or the overall logical structure. To alter content, we perturb samples by replacing numbers with random digits or deleting reasoning keywords."}, {"title": "2. Related work", "content": "Test Time Scaling for Large Language Models Scaling test-time compute has proven effective in enhancing the the reasoning capabilities of LLMs. This can be broadly categorized into two directions: single long CoT and repeatedly sampled CoT. The former trains models, such as OpenAI o1, DeepSeek R1, and Qwen QwQ, to generate individual, long CoT responses with in-context reflection and backtracking to handle complex reasoning tasks (Guo et al., 2025; Jaech et al., 2024; Team, 2024). Alternatively, repeated sampling methods, such as Best-of-N or search-guided generation (e.g., MCTS), improve reasoning performance by invoking multiple responses from the model, sometimes guided by search algorithms and reward models (Snell et al., 2024; Brown et al., 2024). In this paper, we focus on distilling the ability to generate individual, Long CoTs, and show it can be done in a data- and parameter-efficient manner.\nTraining to improve reasoning capabilities of LLMs LLM reasoning capabilities can be improved by approaches such as iterative self-improvement and reinforcement learning (RL) (Zelikman et al., 2022; Lightman et al., 2023; Lambert et al., 2024; Yuan et al., 2024; Guo et al., 2025). More recently, Tulu-3 introduces Reinforcement Learning with Verifiable Rewards (RLVR) to improve performance in tasks such as math and coding (Hendrycks et al., 2021c; Jain et al., 2024; LI et al., 2024). PRIME proposes a RL-based method without process labels (Yuan et al., 2024). The recent release of DeepSeek R1 (Guo et al., 2025) demonstrates that LLMs can learn to produce long CoT and improve reasoning using a pure RL-based approach. Instead of bootstrapping reasoning ability, this paper focuses on the surprising data- and parameter-efficiency of distilling reasoning abilities from an existing reasoning model to an LLM.\nDistillation Distilling the outputs or logits generated by a larger or more capable model has become a standard technique to enhance model performance (Hinton, 2015). Typically, responses generated by higher-quality models are used to perform supervised fine-tuning on smaller models (Lambert et al., 2024). The Vicuna model, for instance, demonstrates that ChatGPT-generated responses can be used to effectively and cheaply distill high-quality chatting capabilities (Zheng et al., 2023). In this paper, we show that reasoning capabilities can also be cheaply distilled. We note that concurrent work has also observed similar trends in distilling reasoning capability (Min et al., 2024; Huang et al., 2024). Our paper differs from these recent works by demonstrating that reasoning distillation can be achieved efficiently with minimal parameter updates. We also provide an in-depth analysis of the key factors driving reasoning improvements, including the roles of the reasoning structure and content, as well as comprehensive evaluations and ablations across different data sizes and teacher models."}, {"title": "3. Simple distillation is effective", "content": "In this section, we present our distillation process and show that a small amount of well-curated data, along with a simple parameter-efficient fine-tuning method (e.g., LoRA), can effectively improve reasoning capabilities in a large language model.\n3.1. Experiments Setup\nDistillation data curation. We use DeepSeek-R1 (Guo et al., 2025) and QwQ-32B-Preview (Team, 2024), two open-source models with reasoning capabilities, to generate our distillation data. We select difficult prompts from the AMC/AIME 1, Math, and Olympiad subset from the Numina-Math dataset (LI et al., 2024), as Min et al. (2024) implies that hard problems can improve performance. We also incorporate coding problems from APPS (Hendrycks et al., 2021a) and TACO (Li et al., 2023) datasets. Specifically, we use GPT-4o-mini to classify the difficulty of the prompts according to the AoPS standard (Achiam et al., 2023), and select math problems of difficulty higher than Leval 3, Olympiad higher than Level 8, and all AIME/AMC problems. We verify the correctness of the traces by checking against ground truth solutions using exact matching for math problems and code execution for coding problems. In total, we curated 12k math and 5k coding problems with correct responses from QwQ to serve as our training data. For R1 samples, we directly use the public R1-17k reasoning dataset\u00b2 that is curated following a similar procedure.\nTraining details. We perform training using Llama-Factory (Zheng et al., 2024). We train the Qwen2.5-32B-Instruct using a batch size of 96, learning rate 1e-5 with a warm-up ratio of 0.1 and linear learning rate decay (Yang et al., 2024), following similar hyperparameters in (Min et al., 2024). We use the next token prediction loss as the training objective (Radford, 2018). We use the same hyperparameters except a 1e-4 learning rate for LoRA fine-tuning.\nEvaluation setup. We evaluate our models on five popular reasoning benchmarks for math and coding, including Math-500, OlympiadBench, AIME-20243, AMC234 (Hendrycks et al., 2021c; He et al., 2024) and LiveCodeBench (Jain et al., 2024). For LiveCodeBench, we report a weighted average accuracy across its easy, medium, and hard difficulty levels.\n3.2. Key Insights\nSmall amount of data is enough. In Fig. 1b, we present the performance of models fine-tuned with the 17k R1 trained"}, {"title": "4. Long CoT: Structure Is The Key", "content": "Motivated by the observation that fine-tuning with a small number of samples can significantly enhance model reasoning performance, we investigate the key factors driving this improvement. Specifically, we explore the contributions of two dimensions to the learning process:\n1.  The local content within a reasoning step, including the correctness of the final answer, numbers in math derivations, and the use of reasoning keywords.\n2.  The global reasoning structure, including reflection, self-validation, and backtracking across multiple reasoning steps to form a logically coherent long CoT.\nTo understand their impact, we conduct two studies: (1) we perturb the content within individual reasoning steps such as the final answer, numerical digits, and reasoning keywords(\u00a74.1), and (2) we modify the global reasoning structure by inserting, deleting, and shuffling reasoning steps(\u00a74.2). We compare the performance of models trained on perturbed samples against both the base Qwen2.5-32B-Instruct model (i.e., Original) and model trained on correct, unperturbed samples (i.e., Correct), as shown in Tab. 2. Our findings show that the learning process is highly sensitive to modifications in the global reasoning structure, but remarkably tolerant to errors in the local contents.\nExperiment setup In this section, we use QwQ-32B-Preview to produce the distillation data and select a subset of 4618 correct responses as the training set (out of the"}, {"title": "4.1. Wrong or Corrupted Local Content", "content": "To study the importance of local content within individual steps, we preserve the overall reasoning structure while systematically perturbing the local content in training samples with different approaches.\nWrong Answer Samples. During our training data curation process in \u00a73, we only include samples that yield correct final answers. To assess whether correctness of the final answer is necessary for learning reasoning patterns, we instead train the model using an equivalent number of samples (4.6k) that lead to the wrong answer. Surprisingly, we find that training the base model without any samples that reach a correct final answer still achieves an average accuracy of 63.1% across benchmarks, only 3.2% lower than training with entirely correct samples.\nDigits Corrupted Samples. Building on the previous experiment, we next examine the role of correctness in the intermediate reasoning steps. To evaluate this, we corrupt correct samples by replacing each digit with a random number between 0 and 9. Note that this is a severe corruption that can lead to nonsensical statements such as \"1+1=3\". Surprisingly, even when 70% of the digits are corrupted, the model still maintains an average performance of 62%, only 4.3% below the correct sample baseline, demonstrating robustness to incorrect content. However, when all digits are corrupted, the average performance plunges to 2.7%.\nReasoning Keyword Removal. Given the prevalence of reasoning keywords in responses from LRMs (e.g., 'wait', 'let me think again', 'but'), one theory is that these specific phrases may invoke the reflection and back-tracking necessary to elicit strong reasoning performance. To evaluate it, we use GPT-4o-mini to identify sentences with occurrences of these reasoning keywords and randomly remove a fraction of them (e.g., 20%, 50%, 100%). Our results show that even after removing all (100%) such keywords, the model still achieves an average accuracy of 63%, which is within 3.3% of accuracy from the model trained with correct samples. This suggests that these particular keywords do not fundamentally impact the model reasoning performance.\nConclusion. We find that errors in local content \u2013 such as incorrect mathematical derivations or missing reasoning keywords - have minimal impact on overall performance."}, {"title": "4.2. Corrupted Global Reasoning Structure", "content": "Next, we examine the importance of reasoning structure by performing three modifications to the reasoning traces: deletion, insertion, and shuffle. We first note that our system prompt (Appendix C) instructs the model to generate responses with thoughts enclosed in the tags 'begin_of_thought' and 'end_of_thought' and the final solution and step-by-step explanation in 'begin_of_solution' and 'end_of_solution'. All modifications are performed on the thoughts, while the solution block is left unmodified.\nWe use Llama-3.3-70B-Instruct (Dubey et al., 2024) to separate each reasoning trace into distinct reasoning steps, with boundaries determined by occurrences of backtracking, self-validation, reflection, or other breaks from a linear sequence of thoughts. We then generated nine modified variants of the dataset by applying each modification (insertion, deletion, and shuffle - illustrated in Fig. 3) to 33%, 67%, or 100% of reasoning steps in the 4,618 correct traces. Each variant is used to train the base model, Qwen2.5-32B-Instruct, and"}, {"title": "5. Ablation Study", "content": "In this section, we conduct a series of ablation studies to answer the following questions:\n1. (\u00a75.1) Does fine-tuning on Long CoT data lead to degraded performance on non-reasoning tasks?\n2. (\u00a75.2) How much does the Long CoT fine-tuning enhance the performance of different student models?\n3. (\u00a75.3) How does Long CoT model performance compare to the Best-of-N sampling performance of the base model?\n4. (\u00a75.4) How does Long CoT fine-tuning compare to Short CoT fine-tuning with the same dataset?\n5.1. Performance on Non-Reasoning Benchmarks\nWhile simple distillation enhances reasoning capabilities, it is essential to ensure that these improvements do not come at the cost of catastrophic forgetting or a decline in general language understanding and instruction-following abilities, which are crucial for broader task generalization.\nTo assess this, we evaluate the performance of our SFT and LoRA fine-tuned models mentioned in \u00a73 on a diverse set of benchmarks: MMLU (multi-task language understanding), ARC-C (science exam question), IEval (instruction-following), and MGSM (multilingual grade-school math problems) (Hendrycks et al., 2021b; Clark et al., 2018; Mitchell et al., 2023; Cobbe et al., 2021).\nAs shown in Tab. 3, the base instruction model (Qwen2.5-32B-Instruct) performs well in all these tasks. The QwQ model, despite its strong reasoning capabilities, suffers significant degradation in instruction-following (i.e., 42.5% on IEval) and multilingual tasks (i.e., 19.1% on MGSM). In contrast, fine-tuning (through both SFT and LoRA) only on a small amount of Long CoT reasoning data from R1 or QwQ allows the distilled models to retain most of the base instruction model's capabilities, avoiding the drastic performance drop seen in QwQ.\n5.2. Effect on Different Student Models\nIn this section, we examine whether Long CoT reasoning capabilities can be elicited with different student models via fine-tuning (as described in \u00a73). Specifically, we train with the 17k samples on Qwen2.5-7B-Math, Qwen2.5-7-Base, Qwen2.5-7B-Instruct, Llama-3.1-8B, Qwen2.5-14B-Base, Qwen2.5-14B-Instruct, Gemma2-27B-it and Qwen2.5-32B-Instruct (Yang et al., 2024; Dubey et al., 2024; Team et al., 2024). We find that seven out of eight models improve noticeably across multiple benchmarks, showing the effect of Long CoT as a general improvement across models. However, not all models have showed the same degree of improvements as for Qwen2.5-32B-Instuct. These findings suggest promising future directions for understanding the performance upper bound and data efficiency with various teacher and student models in the space of reasoning.\n5.3. Comparison to Best-of-N\nAs discussed in \u00a75.2, not all student models achieve significant performance improvements through Long CoT fine-tuning. We hypothesize that this variation is influenced by several factors, such as the extent to which the training data distribution differs from that of the student models and the inherent capabilities of the student models in these tasks. In this section, we compare the test-time scaling (Ahn et al., 2024; Snell et al., 2024) performance of the base model with its performance after Long CoT fine-tuning to understand the relationship between a model's ability to benefit from Long CoT fine-tuning and its intrinsic capabilities."}, {"title": "5.4. Comparison to Short CoT Fine-tuning", "content": "In this section, we provide a direct comparison to training with short CoT. In particular, we compare results"}, {"title": "6. Conclusion", "content": "Large reasoning models unlock new capabilities by using a longer chain of thoughts that involves reflection and backtracking to answer challenging problems. In this paper, we show that such capability can be easily fine-tuned using a few thousand examples and with low-rank adapters. We further show that the key to the learning process is the logical structure of the samples rather than the content of individual reasoning steps. Finally, we discuss several ablations with various teacher-student models and compare them to the best-of-N approach. Together, our work deepens the understanding of what is needed to instill large language models with strong reasoning capabilities and identify potential future directions."}, {"title": "Impact Statement", "content": "This paper aims to contribute to the advancement of Machine Learning. While our work may have various societal implications, we do not find any that require specific emphasis currently."}, {"title": "A. Short and Long CoT Response Examples", "content": "Question\nFor how many integers n does the expression\n$\\sqrt{\\frac{\\log(n^2) - (\\log n)^2}{\\log n - 3}}$\nrepresent a real number, where log denotes the base 10 logarithm?"}, {"title": "Short CoT answer (1/2)", "content": "To solve the problem, we start by analyzing the given expression and the conditions under which it represents a real\nnumber. The expression is:\n$\\sqrt{\\frac{\\log(n^2) - (\\log n)^2}{\\log n - 3}}$\nFor the square root to represent a real number, the argument of the square root, which is the fraction inside, must be\nnonnegative. Additionally, since we are dealing with logarithms, n must be a positive integer. Let's break down the\nsolution into detailed steps:\n1. Condition for the Denominator The denominator of the fraction is $\\log n - 3$. For the fraction to be nonnegative,\nwe need $\\log n - 3 > 0$. This implies that $\\log n > 3$, which translates to\n$n > 10^3 = 1000$.\nTherefore, n must be greater than 1000.\n2. Simplifying the Numerator The numerator of the fraction is $\\log(n^2) \u2013 (\\log n)^2$. Using logarithmic properties, we\ncan rewrite $\\log(n^2)$ as $2\\log n$. Thus, the numerator becomes\n$2 \\log n - (\\log n)^2$.\n3. Analyzing the Fraction Substituting the simplified numerator into the fraction, we get:\n$\\frac{2 \\log n - (\\log n)^2}{\\log n - 3}$\nFor this fraction to be nonnegative, given that the denominator is positive (from step 1), the numerator $2 \\log n \u2013\n(\\log n)^2$ must also be nonnegative. This leads to the inequality:\n$2 \\log n - (\\log n)^2 \\geq 0$.\n4. Solving the Inequality The inequality\n$2 \\log n - (\\log n)^2 \\geq 0$\ncan be rewritten as\n$(\\log n)^2 - 2 \\log n \\leq 0$.\nFactoring,\n$\\log n (\\log n - 2) \\leq 0$.\nSolving this inequality, we find that\n$0 \\leq \\log n \\leq 2$.\nConverting back to n, this implies:\n$1 \\leq n \\leq 100$.\n5. Combining Conditions From step 1, we have $n > 1000$, and from step 4, we have $1 \\leq n \\leq 100$. However,\nthese two conditions are contradictory. The correct interpretation should be that n must satisfy both the condition\nof being greater than 1000 (from the denominator analysis) and the condition derived from the inequality, which\nwas misinterpreted in the combination step. The correct combination should focus on the range derived from the\ninequality, which is 1 < n < 100, and disregard the incorrect application of the denominator condition in this\ncontext."}, {"title": "Short CoT answer (2/2)", "content": "6. Conclusion The integers n that satisfy the given conditions are those for which\n$1 \\leq n \\leq 100$.\nCounting these integers, we find there are\n100\nsuch integers."}, {"title": "Long CoT answer (1/4)", "content": "<| begin_of_thought | > So I've got this problem here:\n$\\sqrt{\\frac{\\log(n^2) - (\\log n)^2}{\\log n - 3}}$\nAnd I need to find out for how many integer values of n this expression represents a real number. The logarithm\nhere is base 10.\nFirst off, since it's a square root, the expression inside the square root has to be non-negative for the whole thing to\nbe a real number. So, that means:\n$\\frac{\\log(n^2) \u2013 (\\log n)^2}{\\log n - 3} \\geq 0$\nAdditionally, the argument of the logarithm has to be positive because logarithms are only defined for positive real\nnumbers. So, $n^2 > 0$, which is true for all $n \\neq 0$, and n > 0 since $n^2 > 0$ is always true for $n \\neq 0$, but we need to\nensure n > 0 because the logarithm of a negative number is undefined in real numbers.\nSo, n has to be a positive integer.\nNow, let's simplify the expression inside the square root.\nI know that $\\log(n^2) = 2 \\log n$, because of the logarithm power rule. So, substituting that in:\n$\\frac{2 \\log n - (\\log n)^2}{\\log n - 3}$\nLet me denote $\\log n = x$ to make it easier. So, the expression becomes:\n$\\frac{2x - x^2}{x - 3}$\nI can factor the numerator:\n$2x \u2212 x^2 = x(2 \u2013 x)$\nSo, the expression is:\n$\\frac{x(2 \u2013 x)}{x - 3}$\nI need to analyze when this fraction is non-negative.\nTo do that, I can look at the sign of each factor:\n- $x = \\log n$\n- $2 - x = 2 - \\log n$\n- $x - 3 = \\log n - 3$\nSo, the expression is:\n$\\frac{\\log n (2 - \\log n)}{\\log n - 3}$\nI need to determine when this fraction is greater than or equal to zero.\nLet me consider the critical points where each factor is zero:\n1.  $\\log n = 0 \\Rightarrow n = 10^0 = 1$\n2.  $2 - \\log n = 0 \\Rightarrow \\log n = 2 \\Rightarrow n = 10^2 = 100$\n3.  $\\log n - 3 = 0 \\Rightarrow \\log n = 3 \\Rightarrow n = 10^3 = 1000$\nThese points divide the number line into intervals where the expression doesn't change sign.\nSo, the intervals to consider are:\n- 1 < n < 100\n- 100 < n < 1000\n- n > 1000"}, {"title": "Long CoT answer (2/4)", "content": "Wait, but I need to consider n as positive integers, and n \u2265 1.\nBut actually, n has to be at least 1, but n = 1 makes $\\log n = 0$, which is a critical point. So, let's consider the\nintervals:\n- 1 < n < 100\n- n = 100\n- 100 < n < 1000\n- n = 1000\n- n > 1000\nI need to test the sign of the expression in each interval.\nFirst, let's consider n = 1:\n$\\log 1 = 0$, so the numerator is 0, and the denominator is $0 - 3 = -3$, so the fraction is $0/ - 3 = 0$, which is\nacceptable since it's zero.\nNext, for 1 < n < 100:\nLet's pick n = 10, for example.\n$\\log 10 = 1$\nSo, x = 1\nThen, $2-x = 1$\n$\\frac{x}{x-3} = \\frac{1}{-2}$\nSo, $\\frac{1}{-2} < 0$, which is not acceptable.\nWait, but earlier I thought the expression inside the square root needs to be non-negative. So, this interval doesn't\nwork.\nNext interval: n = 100\n$\\log 100 = 2$\nNumerator: $2 \\cdot (2-2) = 0$\nDenominator: 2-3=-1\nSo, $0/ - 1 = 0$, which is acceptable.\nThen, 100 < n < 1000\nLet's pick $n = 1000^{0.5} = 10^{2.5} \\approx 316.2$, but since n is integer, let's take n = 300\n$\\log 300 \\approx 2.477$\nSo, x \u2248 2.477\n2-x = -0.477\nx-3 = -0.523\nSo, $\\frac{2.477(-0.477)}{-0.523} \\approx 2.26 > 0$, which is acceptable.\nNext, n = 1000\n$\\log 1000 = 3$\nDenominator: 3 \u2013 3 = 0, so the expression is undefined. Not acceptable.\nFinally, n > 1000\nLet's pick n = 10000\n$\\log 10000 = 4$\nSo, x = 4\n2-x=-2\nx-3=1\nSo, $\\frac{4(-2)}{1} = -8 < 0$, which is not acceptable.\nSo, summarizing:\n- n = 1: acceptable\n- 1 < n < 100: not acceptable\n- n = 100: acceptable\n- 100 < n < 1000: acceptable\n- n = 1000: not acceptable\n- n > 1000: not acceptable\nTherefore, the acceptable ranges are:\n- n = 1"}, {"title": "Long CoT answer (3/4)", "content": "- n = 100\n- 100 < n < 1000\nWait, but n = 100 is included in 100 < n < 1000? No, n = 100 is a separate point.\nActually, n = 100 is the upper limit of 100 < n < 1000, but since n = 100 is acceptable, I need to include it.\nWait, no, n = 100 is a separate point where the expression is zero, which is acceptable.\nSo, the acceptable n are:\n- n = 1\n- 100 < n < 1000\nWait, but earlier I thought n = 1000 is not acceptable, but n = 1000 is not included in 100 < n < 1000, which is\ncorrect.\nSo, now, I need to count the integer values of n in these ranges.\nFirst, n = 1: 1 value\nSecond, 100 < n < 1000: that's from 100 to 999, inclusive.\nSo, 999-100 + 1 = 900 values.\nTherefore, total integers are 1 + 900 = 901.\nBut wait, earlier I thought n = 100 is acceptable, and it is included in 100 < n < 1000, so no need to count it\nseparately.\nWait, no, n = 100 is included in 100 < n < 1000, which already includes 100 to 999, so total is 900 values.\nPlus n = 1, so total is 901.\nBut, I need to make sure that for all n in 100 < n < 1000, the expression is positive.\nWait, except for n = 100, where it's zero, which is acceptable.\nBut in the interval 100 < n < 1000, it's positive, as I tested earlier.\nSo, yes, total is 901.\nTherefore, the number of integer values of n that satisfy the condition is 901.\n**Final Answer**\n901"}, {"title": "Long CoT answer (4/4)", "content": "2. Finding critical points\nThe critical points are where the numerator or denominator equals zero:\n- x = 0 \u21d2 n=1 - 2 - x = 0 \u21d2 x = 2 \u21d2 n = 100 - x - 3 = 0 \u21d2 x = 3 \u21d2 n = 1000\n3. Analyzing intervals\nThe intervals to consider are:\n- 1 < n < 100 - n = 100 - 100 < n < 1000 - n = 1000 - n > 1000\nTesting the sign of the expression in each interval:\n- For n = 1: The expression equals 0, which is acceptable. - For 1 < n < 100: The expression is negative, not\nacceptable. - For n = 100: The expression equals 0, acceptable. - For 100 < n < 1000: The expression is positive,\nacceptable. - For n = 1000: The expression is undefined, not acceptable. - For n > 1000: The expression is\nnegative, not acceptable.\n4. Counting acceptable integers\nThe acceptable ranges are:\n- n = 1 - 100 < n < 1000\nThe number of integers in 100 < n < 1000 is 900 (from 100 to 999 inclusive). Adding n = 1 gives a total of:\n1 + 900 = 901\nThus, the number of integer values of n that satisfy the condition is:\n901"}, {"title": "B. Reasoning keywords", "content": "We create a bank of reasoning keywords by manual inspection of reasoning responses from QwQ and R1. We identify these following words and phrases as frequent indicators of reflection, backtracking, and self-validation: \u201cAlternatively\u201d, \u201cWait\u201d, \"Just to be thorough\u201d, \u201cJust to make sure\u201d, \u201cLet me just double-check\u201d, \u201cLet me try another\u201d, \u201cLet me verify\", \"Let me check\", \u201cHmm\u201d, \u201cBut\u201d, \u201cMaybe I should consider\u201d, \u201cMaybe I can consider\"."}, {"title": "C. Long CoT System Prompt", "content": "We adopt the following system prompt from Still-2 (Min et al.", "2024)": "nLong CoT System Prompt\nYour role as an assistant involves thoroughly exploring questions through a systematic long thinking process before\nproviding the final precise"}]}