{"title": "Revolutionizing Undergraduate Learning: CourseGPT and Its Generative AI Advancements", "authors": ["Ahmad M. Nazar", "Mohamed Y. Selim", "Ashraf Gaffar", "Shakil Ahmed"], "abstract": "Integrating Generative AI (GenAI) into educational contexts presents a transformative potential for enhancing learning experiences. This paper introduces CourseGPT, a generative AI tool designed to support instructors and enhance the educational experiences of undergraduate students. Built on open-source Large Language Models (LLMs) from Mistral AI, CourseGPT offers continuous instructor support and regular updates to course materials, enriching the learning environment. By utilizing course-specific content, such as slide decks and supplementary readings and references, CourseGPT provides precise, dynamically generated responses to student inquiries. Unlike generic AI models, CourseGPT allows instructors to manage and control the responses, thus extending the course scope without overwhelming details. The paper demonstrates the application of CourseGPT using the CPR E 431: Basics of Information System Security course as a pilot. This course, with its large enrollment and diverse curriculum, serves as an ideal testbed for CourseGPT. The tool aims to enhance the learning experience, accelerate feedback processes, and streamline administrative tasks. The study evaluates CourseGPT's impact on student outcomes, focusing on correctness scores, context recall, and faithfulness of responses. Results indicate that the Mixtral-8x7b model, with a higher parameter count, outperforms smaller models, achieving an 88.0% correctness score and a 66.6% faithfulness score. Additionally, feedback from former students and teaching assistants on CourseGPT's accuracy, helpfulness, and overall performance was collected. The outcomes revealed that a significant majority found CourseGPT to be highly accurate and beneficial in addressing their queries, with many praising its ability to provide timely and relevant information.", "sections": [{"title": "I. INTRODUCTION", "content": "There is a growing recognition of the transformative po-tential of AI-driven technologies in enhancing academic andstudent learning experiences as the educational landscapecontinues to evolve. Integrating intelligent virtual assistantspowered by LLMs is a promising solution for improving stu-dent interactions with course materials and seeking academicguidance. Adopting LLM-based assistants in educational set-tings represents a shift towards personalized and interactivelearning environments with domain-specific knowledge. Theseassistants provide personalized support, real-time feedback,and domain-specific knowledge access, enabling deeper stu-dent engagement and more effective learning outcomes.\nThis work evaluates RAG-based-LLM effectiveness as intel-ligent university course assistants. We aim to assess how differ-ent LLM sizes, ranging from 7 billion to 47 billion parameters,impact their ability to assist students with questions regardingthe course. We identify the strengths and limitations of eachLLM by establishing a shared knowledge base and employingrigorous evaluation methodologies. LLM assessment includesanalyzing the correctness, context recall, and faithfulness ofresponses generated by each LLM within the course's scope.CourseGPT utilizes Mistral LLMs [1], [2].\nCourseGPT is fine-tuned for the Iowa State University CPR\nE 431: Introduction to Wireless Network Security course.CourseGPT was deployed on a server and provided to a fewstudents who completed the course in previous semesters anda previous teaching assistant. These participants were surveyedon CourseGPT's accuracy, helpfulness, and performance.\nAn LLM assistant can be integrated into a virtual teachingassistant using GPT3. The virtual intelligent teaching assistantsystem at the framework's core is a voice-enabled helpercapable of answering various course-specific questions, suchas administrative and logistical questions and course poli-cies [3]. Integrating an RAG-LLM-based assistant within auniversity system marks a notable advancement in studentsupport mechanisms [4]. The AI-teaching assistant operateslike traditional university forums while attending to studentinquiries with interactive and informative capabilities.\nIt is worth noting that the recent CourseGPT-zh is in-dependent of our CourseGPT. CourseGPT-zh is a course-oriented education LLM that leverages a high-quality question-answering corpus distillation framework with prompt opti-mization. CourseGPT-zh introduces a novel method for dis-crete prompt optimization based on LLM-as-Judge, aligningLLM responses with user needs while saving response length[5]. Our CourseGPT involves domain-specific knowledge todeploy as a student assistant that provides answers to questionsto specific class-related knowledge.\nThis work is organized as follows: Section III showsCourseGPT's design and implementation technical details.Section IV describes RAG-LLM evaluation methodologies andmetrics. RAG-LLM evaluation results and analyses achievedfrom deploying CourseGPT are described in Section V. Thechallenges associated with RAG-LLM deployment and fine-"}, {"title": "II. COURSEGPT: IMPROVING STUDENT SUCCESS", "content": "A. CourseGPT Impact\nUndergraduate teaching often necessitates concise instruc-tions within fine-grained modules and more individualizedsupport to guide students step by step, ideally on a weeklyor bi-weekly basis. CourseGPT enhances this approach byenabling instructors to create detailed, modular content us-ing its interactive interface. This tool provides students withpersonalized support through immediate answers and real-timeguidance, encouraging them to ask more questions without fearof judgment. Consequently, students can focus on and com-plete each module in a single interactive session, significantlyreducing the frustration associated with waiting for responsesfrom instructors or teaching assistants.\nThe immediate and personalized feedback provided byCourseGPT promotes an excellent educational experience byincreasing student engagement and productivity. Students aremore inclined to seek help and clarification on various topics,knowing they will receive prompt and relevant responses. Thisnot only enhances their understanding of the course materialbut also builds their confidence in mastering complex subjects.By fostering a more interactive and responsive learning envi-ronment, CourseGPT aligns with any school's strategic goalsof promoting educational experience excellence and enhancingknowledge and discovery.\nCourseGPT's versatility makes it suitable for a wide rangeof courses with different instructional materials, includingmathematics, programming, and natural language artifacts.The success of similar AI tools, like ChatGPT, across variousdomains evidences this adaptability. Our pilot class, CPR E431: Basics of Information System Security, includes all threetypes of materials, demonstrating CourseGPT's capability tohandle diverse content. This pilot will serve as a proof ofconcept that can be generalized to a broader range of courses,benefiting a larger number of students across the Arts, Sci-ences, and Engineering disciplines. Although the initial pilotwill not address materials such as drawings and graphics,future iterations of CourseGPT may expand to include theseas well, further broadening its applicability.\nB. Enhancing of Students' Learning Outcomes\nCourseGPT will serve as a valuable tool for instructors andstudents, providing specific guided procedures to implementthree student outcomes from ABET: SO1, SO4, and SO7,thereby enhancing specific learning outcomes. This will guideus in designing the tool while offering instructors the necessaryprocedures to implement them, resulting in an improvedstudent learning experience, regardless of whether the classis intended for ABET accreditation.\n1) SO1: An ability to identify, formulate, and solve complexengineering problems: CourseGPT assists instructors in break-ing down their material into three phases. Students will engagein broad, targeted searches, asking multiple questions to iden-tify problems. They will then receive feedback on formulatingthese problems. Solving the problems requires students to askmore questions and seek similar solutions to validate theirown. These activities benefit from CourseGPT's interactiveand individualized feedback during real-time sessions.\n2) SO4: An ability to recognize ethical and professionalresponsibilities: Drawing from years of teaching experience,this outcome greatly benefits from CourseGPT by encourag-ing critical thinking. Linking a project's impact to global,economic, environmental, and societal contexts requires longdiscussions and repeated, focused questions. CourseGPT'sreal-time, interactive sessions of intelligent discussions sig-nificantly enhance students' understanding of their project'simpact on its environment.\n3) SO7: An ability to acquire and apply new knowledge:Achieving this ability goes beyond class material. Traditionalinstruction methods are limited to classrooms and the over-whelming information on the internet. CourseGPT allows in-structors to compile supplemental material, facilitating interac-tive discussions between students and CourseGPT. This preciseguidance helps students acquire and apply new knowledgeeffectively.\nIn summary, CourseGPT represents a significant advance-ment in educational technology, providing personalized, real-time support that enhances undergraduate learning experiencesand outcomes. By facilitating complex problem-solving, pro-moting ethical responsibilities, and encouraging new knowl-edge acquisition, CourseGPT aligns with every school's strate-gic educational goals. The pilot phase will establish a founda-tion for broader implementation, benefiting a larger cohort ofstudents and setting a new standard for educational excellencein higher education."}, {"title": "III. COURSEGPT IMPLEMENTATION", "content": "CourseGPT expends advanced LLM computational tech-niques with RAGs at the framework's core. RAGs enableknowledge base integration into the generative process. Thisintegration is vital in ensuring generated content relevanceand accuracy, especially in a university course environmentwhere new findings and teaching material are added. We chosetext embeddings that satisfy our operational constraints toconstruct a robust knowledge base system while optimizingretrieval precision. This knowledge base guarantees that theretrieved information is relevant and contextually appropriatefor the generative tasks. CourseGPT relies on LLMs as theytransform raw RAG data and text embeddings into coherentand contextually rich responses to user prompts.\nThis section provides details of each CourseGPT compo-nent, clarifying their contributions and collective impact onthe system's functionality. By exploring these elements, weoffer insights into how CourseGPT facilitates precise, updated,context-aware information retrieval and generation, setting anew standard for interaction within intelligent systems. Fig. 1illustrates the workflow of CourseGPT, depicting the integrateddata extraction, processing, embedding, and knowledge baseutilization. This section also depicts the iterative process ofembedding user prompts to reference relevant material in theknowledge base to provide relevant RAG-LLM answers.\nA. Retrieval Augmented Generation\nRAG-based techniques are crucial for LLM advancement.While LLMs have demonstrated impressive results, there isroom for improvement, especially in handling specializedknowledge tasks. RAG addresses this by enhancing LLMswith external, up-to-date information sources. RAGs over-come outdated knowledge and hallucination by combining theLLM's intrinsic knowledge with vast repositories from exter-nal databases [6]. RAGs retrieve relevant document chunksbased on searches utilizing semantic similarity calculations,resulting in more accurate and reliable outputs [6]. RAGs arebuilt on retrieval, generation, and augmentation techniques.\nThis work utilizes LLMs' advanced language understandingand generation capabilities as the generative component ofthe RAG framework to discover critical differences betweendifferent LLM models. This approach ensures continuousknowledge updates and domain-specific integration, makingRAG a promising solution for domain-specific LLMs.\nB. Embeddings\nThe effectiveness of RAG depends on the quality andrelevance of the retrieved passages used as input to thegenerator. To ensure the retrieval of pertinent information fromthe ARA documentation, we utilize conversation chains tostart prompting and answering and employ text embeddingsto create a retrieval knowledge base for the LLMs.\nText embeddings are critical in enhancing textual data'ssemantic understanding and representation. Extracted textchunks from the pre-processed data are transformed intohigh-dimensional vector representations, facilitating more nu-anced similarity assessments and improving the accuracy ofpassage retrieval using the pre-trained gte-large-en-v1.5 textembeddings model from AliBaba-NLP [7]. These embeddingssupport a context length of up to 8192. These embeddingswere trained through multi-stage contrastive learning. The firststage involves a preliminary Masked Language Modelling pre-training on shorter 800M text pair lengths. The second stageinvolves data resampling, which reduces the short text propor-tions and a continuation of the Masked Language Modellingpre-training. This model was chosen due to the considerationof its size and its performance on the Massive Text EmbeddingBenchmark (MTEB) leaderboard [8].\nCourseGPT trains the RAG-LLM model by incorporatingrelevant passages from the course textbook, course slidedecks, syllabus, schedule, and supplementary materials. Thisprocess involves utilizing conversation chains and text em-beddings to process and generate responses. Integrating theRAG component with advanced data extraction techniquesenhances CourseGPT's ability to provide contextually rich andaccurate responses to user queries relevant to the retrievedknowledge base data. The top-p section headers are searchedfor retrieval, and the entire content section is retrieved. Top-p sampling chooses from the smallest possible set of wordswhose cumulative probability exceeds the probability p. Theprobability mass is then redistributed among this set of words.The size of the set of words can dynamically increase anddecrease according to the next word's probability distribution.We choose the top-p, (p = 0.95), samples for our approach.\nC. Mistral LLMs\nLLMs are a robust computational model for performingvarious natural language processing tasks. These models learnstatistical relationships from vast text data during their in-tensive training. LLMs excel at text generation, a form ofgenerative AI. LLMs predict the next token or word given aninput text, creating coherent and contextually relevant output.The largest and most capable LLMs are built using a decoder-only transformer architecture [9]. These models are trained onbillions of parameters.\nMistral-7b is a 7-billion-parameter language model devel-oped by Mistral AI and is engineered for superior performanceand efficiency, which leverages grouped-query attention forfaster inference, coupled with sliding window attention, toeffectively handle sequences of arbitrary length with a reducedinference cost. Mistral-7b achieves a 2x speed improvementfor sequences up to 16k with a window of 4k due to itsinnovative sliding window attention mechanism [1].\nMixtral-8x7b is a Sparse Mixture of Experts languagemodel developed by Mistral AI. Mixtral-8x7b has the samearchitecture as Mistral-7B, except each layer consists of 8feedforward blocks. At each layer, a router network selectstwo feedforward blocks for every token to process the currentstate and combine their outputs. Each token can access 47B pa-rameters but only uses 13B active parameters during inferencedue to feedforward blocks. Mixtral-8x7b utilizes a subset of itsparameters for each token, allowing for faster inference speedsat low batch sizes and higher throughput at large batch sizes.Mixtral-8x7b's efficient parameter usage ensures competitiveperformance. Mixtral-8x7b was trained with a context size of32k tokens [2]."}, {"title": "D. Retrieval Material and Knowledge Base", "content": "The training materials sourced for CourseGPT are essentialto refining the RAG-LLM model. The retrieval material in-cludes the course textbook, slide decks, the syllabus and sched-ule, and supplementary resources such as API documentationintroductions to command modules. These materials constitutea rich knowledge base relevant to the course curriculum.\nThe training materials offer comprehensive course contentcoverage with details on topics, concepts, and methodologiesrelevant to the course. They include APIs associated with thecourse material, resource specifications, in-depth informationon course-related topics, and supplementary information pro-vided to students as optional readings.\nThe research materials were pre-processed and cleaned be-fore fine-tuning. Our data extraction process involved parsingand organizing textual information into a cohesive, uniformformat to enhance consistency and readability. Redundantor extraneous content, such as duplicate entries, formattingirregularities, and non-informative sections, were removed."}, {"title": "IV. EVALUATION METHODOLOGY", "content": "Establishing clear and measurable criteria to evaluate em-ployed LLM efficacy within the course context is crucial. Thissection details assessment metrics for the LLMs' correctnessscores, faithfulness, and context recall. Each metric providesvaluable insights into the LLMs' capabilities, enabling acomprehensive analysis of their practical utility in a real-worldeducation context. Below, we detail the methodologies forcomputing these metrics.\nA. Correctness Scores\nWe evaluated answer correctness to determine the accuracyof each LLM's responses to queries. This metric involvedcomparing the generated answer to the ground truth and as-sessing the level of alignment between the two. A higher scoreindicates greater accuracy and a closer match to the groundtruth. Answer correctness is based on semantic similarityand factual accuracy, combined using a weighted scheme todetermine the overall correctness score [10], [11].\nAnswer semantic similarity assesses the semantic resem-blance between the generated answer, $G_A$, and the groundtruth, $G_t$. This evaluation is based on the ground truth and theanswer [10], [11]. A higher score signifies a higher alignmentbetween the generated answer and the ground truth. Answersemantic similarity is calculated using the embeddings modelto vectorize the ground truth answer and the generated answer,then computing the cosine similarity, $S_{cos}$, between the twovectors. The cosine similarity of the embedded, vectorizedgenerated answer, $G_A$, and the embedded, vectorized groundtruth, $G_t$, is given as follows:\n$S_{cos} = cos(\\theta) = \\frac{G_t \\cdot G_A}{||G_t||||G_A||}$  (1)\nFactual similarity quantifies the factual overlap between thegenerated answer and the ground truth answer as follows:\n$F= \\frac{TP}{(|TP| +0.5 \\times (|FP| + |FN|))}$ (2)\nwhere TP are true positives, facts, or statements present inboth the ground truth and the generated answer, and FP arefalse positives, which are facts or statements present in thegenerated answer but not in the ground truth. FN are falsenegatives, facts, or statements present in the ground truth butnot in the generated answer.\nAnswer correctness is given by the weighted average of $S_{cos}$and F, where $W_{S.cos}$ = 0.25 and $W_F$ = 0.75, as follows:\nAnswer Correctness = $ \\frac{W_{S.cos} * S_{cos} + W_F * F}{W_{S.cos SCOS} + W_F}$ (3)\nB. Context Recall\nContext recall measures the RAG-LLM's context alignmentextent of the retrieved context with the ground truth. Theground truth attributable to the relevant context is $G_{tc}$. Con-text recall is calculated using the ground truth and the retrievedcontext.\nEach sentence in the ground truth answer is analyzed to de-termine if it can be attributed to the retrieved context. Ideally,All sentences in the ground truth answer are attributable to theretrieved context.\nContext recall is calculated as follows:\nContext Recall = $\\frac{G_{tc}}{Number of Sentences in G_t}$ (4)\nC. Faithfulness\nAmong the metrics evaluated on the LLMs, one importantmeasure is model faithfulness. This metric assesses the accu-racy of the generated answer in terms of factual consistencywith the provided context. It is calculated based on the answerand the retrieved context [10], [11]. To be considered faithful,the generated answer must make claims that can be logicallydeduced from the given context. A set of claims from thegenerated answer is identified and compared with the givencontext to assess the faithfulness of the LLMs. The answeris deemed faithful if all the claims can be inferred from thecontext. Faithfulness is expressed in terms of $N_{Gc}$, the numberof claims in the generated answer that can be inferred fromthe given context, and $N_C$, the total number of claims in thegenerated answer, as:\nFaithfulness = $\\frac{N_{Gc}}{|N_C|}$ (5)"}, {"title": "V. EVALUATION RESULTS", "content": "This work assesses and compares the performance of theRAG-LLMS Mistral-7b and Mixtral-8x7b in answeringstudent questions related to specific course-related material.The evaluation metrics included correctness scores, answerrelevancy, and faithfulness scores. These metrics provide in-sight into the RAG-LLM's capabilities and suitability forcourse-related tasks. The tests were conducted using an LLMevaluator. A test set of N = 50 questions was generated, andthe LLM answers were evaluated on their overall correctness,context recall, and faithfulness. The knowledge base consistsof categorized topics extracted by the evaluation tool to assesswhere CourseGPT requires improvements and further fine-tuning.\nA. Correctness Scores\nLarger models, such as Mixtral-8x7b, have a higher capacityto represent and process complex domain-specific knowledge,including intricate concepts, terminologies, and relationships present in the input context and the shared knowledge base.This capacity enables them to generate more accurate andcontextually relevant responses across domain-specific queries.Larger models benefit from increased exposure to diverseexamples and data during training. This exposure allows themto generalize better and adapt more effectively to varyinginput contexts and query types within the domain-specificknowledge domain. This exposure also increases accuracyand reliability in generating correct responses across differentscenarios and use cases.\nLarger models exhibit greater robustness to noise, ambi-guity, and variability in the input context and the sharedknowledge base. They can filter irrelevant information and dis-cern subtle differences, producing more precise and accurateresponses.\nThe RAG-LLM model's correctness score distributionsacross different topics in our test set are shown in Fig. 2.The topics involved information regarding Wireless Security,Authentication, Encryption, Malware, Identity Management,Intrusion Detection Systems (IDS), Information System Secu-rity, Denial-of-Service (DoS), and Access Control, and othertopics consisting of more general information not falling intoany of the topics mentioned above. Fig. 2 shows that the trendin higher correctness scores depends on the size of the modelutilized with the same knowledge base. The \"Others\" topicresults have been excluded from Fig. 2 for simplicity.\nOur analysis revealed a significant difference in correctnessscores between the RAG-LLMs. Mixtral-8x7b achieved thehighest correctness score at 88.0%, followed by Mistral-7b,which scored 78.2%. This outcome shows that models withlarger parameter sizes generally exhibit enhanced accuracywith higher correctness in generating contextually appropriateresponses. It is worth noting that Mistral-7b performed as wellas Mixtral-8x7b in the Authentication, Identity Management,IDS, Information System Security, and DoS topics despitenot being a smaller model, as seen in Fig. 2. The average RAG-LLM correctness scores for all topics are shown in Fig. 3.\nThe effectiveness of a model in generating correct responsesis influenced by the distribution and training data coveragerelated to different topics within the knowledge base. Bothmodels were exposed to diverse and representative examplesfor specific topics during training. As such, the smaller modellearns to generalize and perform comparably to the largermodel in those areas.\nSmaller models may benefit from greater sample efficiency,requiring fewer examples or less training data to learn effec-tively. The smaller model may extract and leverage relevantpatterns and information efficiently, leading to comparableperformance to the larger model if the training data for specifictopics is limited but informative.\nThe LLM model performance depends on its size andthe optimization techniques and strategies employed duringtraining. Effective regularization, fine-tuning, and optimizationstrategies can mitigate the limitations of smaller models andenhance their performance across various topics within theknowledge base. Both models share the same underlyingarchitecture.\nB. Context Recall\nThe RAG-LLMs demonstrated commendable context recallperformance in our analysis, indicating a strong alignmentwith the source material and minimal deviation. However,minor performance differences between the models were no-table due to the LLM's size. In the analysis, context recallwas similar based on the answers given on the knowledgebase. This finding suggests that models with higher parametercounts might be better at adhering strictly to the informationprovided in the knowledge base. However, the improvementis only slight since a shared knowledge base is used, andboth LLMs perform considerably well while respecting theretrieved context.\nAnalysis revealed that, on average, Mixtral-8x7b was moreadept at generating relevant answers across the tested questionsthan its counterpart, but only slightly. Fig. 3 illustrates thedistribution of context recall scores for the RAG-LLMs andshows how closely aligned the retrieved LLM context wasto the ground truths. These results show that each RAG-LLM provided aligning and relevant information to answerprompts. Mixtral-8x7b scored an average context recall scoreof 99.6%, while Mistral-7b scored 99.0%. These findings showthat the LLMs provide aligning and relevant information givena specific context. The context recall scores are similar dueto the LLMs' underlying model architecture and the sharedknowledge base usage.\nC. Faithfulness\nThe RAG-LLM faithfulness evaluation showed that Mixtral-8x7b scored the highest in model faithfulness, followedby Mistral-7b. Mixtral-8x7b attained a faithfulness score of66.6%, and Mistral-7b scored 60.2%. Our evaluation showsthat using larger LLMs increases the faithfulness scores con-siderably and that the generated answer accuracy regardingfactual consistency with the provided context is much higherthan in smaller model sizes. A larger LLM has a higher ca-pacity to capture and understand complex patterns in languagedue to its increased number of parameters and layers. Thisenhanced capacity enables larger LLMs to generate text thatmore closely resembles human-authored text and maintainshigher faithfulness to the input prompt or context.\nIn domain-specific applications like CourseGPT, accuracyand reliability are crucial. Students rely on CourseGPT foraccurate information and guidance on course content, assign-ments, and exams. A larger LLM's higher faithfulness meansit is more likely to provide accurate and relevant responses thatalign closely with the input provided by students or instructors.Faithfulness scores of the RAG-LLMs are shown in Fig. 3.\nD. Survey Results\nThe survey assesses the satisfaction and performance ofCourseGPT based on the feedback from six participants com-prising former students and an experienced teaching assis-tant. The survey result analysis unveils valuable insights intoCourseGPT's perceived effectiveness and utility in educationalcontexts. The survey questions were scored based on the 5-point Likert Scale.\nThe first survey question rated CourseGPT's helpfulness inaddressing queries. Four participants rated CourseGPT withthe highest score of 5, indicating exceptional helpfulness inaddressing their queries. Two participants rated CourseGPTwith a score of 4, affirming its commendable assistance inproviding relevant and timely information. The first question'ssurvey results are shown in Fig. 4.\nThe second survey question asked for a rating of informationaccuracy provided by CourseGPT. The responses to this ques-tion demonstrate confidence in CourseGPT's ability to pro-vide accurate information. Specifically, three participants ratedCourseGPT with the highest score of 5, emphasizing their trustin the information accuracy delivered by the CourseGPT. Twoparticipants rated CourseGPT with a score of 4, indicating solid reliability in the information provided. However, oneparticipant rated CourseGPT with a score of 2, suggestinga need for improvement in ensuring information accuracyfor specific queries. The second question's survey results areshown in Fig. 5.\nThe third survey question rated the participants' satisfactionwith CourseGPT's overall performance. The survey findingsindicate a mixed response regarding participants' satisfactionwith CourseGPT's overall performance. While two participantsrated CourseGPT with the lowest score of 2, suggesting areasfor enhancement in performance, an equal number of partici-pants rated CourseGPT with the highest score of 5, reflecting ahigh level of satisfaction with its overall performance. This rat-ing divergence highlights the need for continuous refinementand optimization of CourseGPT to effectively address varyinguser expectations and preferences. The third question's surveyresults are shown in Fig. 6.\nThe survey results provide valuable insights into the per-ceived strengths and areas for improvement of CourseGPT ineducational settings. Despite receiving high ratings for help-fulness and information accuracy from most participants, themixed response regarding overall performance highlights theimportance of ongoing refinement and enhancement efforts.\nAddressing the concerns raised by participants who ratedCourseGPT with lower scores will further enhance its ef-fectiveness and user satisfaction. Additionally, leveraging thepositive feedback and high ratings received in certain areas canserve as a foundation for promoting CourseGPT's adoptionand utilization in future courses and educational endeavors.\nA directed effort towards improving CourseGPT's perfor-mance, addressing user feedback, and refining its capabilitieswill be essential in solidifying its position as a valuable assetin modern education. CourseGPT can realize its potential asa transformative tool in streamlining personalized and com-pelling student and educator learning experiences by strivingfor optimal user experience.\nE. Result Discussions\nAnalyzing different-sized LLMs offers valuable insightsfor optimizing workflows and improving retrieval supportmechanisms for students enrolled in a course. The analysishighlights significant variances in correctness, context recall,and faithfulness scores among the LLMs, all determining"}, {"title": "VI. CHALLENGES", "content": "Integrating CourseGPT is a significant advancement inenhancing education. Several challenges require addressing tomaximize the effectiveness of CourseGPT's goals. Identifyingand mitigating these challenges allows CourseGPT to reach itsfull potential as an influential student assistant. This integrationalso results in enhanced productivity, efficient and quickerquestion-answering systems, and accelerated innovation.\nA. Memory Management and Computational Efficiency\nDeploying CourseGPT presents a challenge in memorymanagement and computational efficiency. CourseGPT's re-liance on extensive data and computational resources requiredto operate the LLMs and RAG effectively can cause memoryissues, especially in environments with limited resources. Thecomplex computations involved and the size of LLMs, such asthose with more than 10 billion parameters, can strain systemresources, resulting in latency issues, slowdowns, or systemcrashes. Memory management techniques and optimizationstrategies with supporting infrastructure can mitigate thesechallenges and ensure smooth operation.\nAs the volume and complexity of student inquiries within the class scope grow, ensuring the scalability and adaptabilityof CourseGPT becomes essential. The system must be ableto handle a diverse range of queries and research scenariosefficiently while maintaining responsiveness and accuracy.\nScalability challenges arise concerning LLM model sizesand the RAG framework's capacity to retrieve and generate rel-evant information in real-time. Addressing these challenges re-quires continuous optimization and refinement of CourseGPTarchitecture to accommodate evolving research requirements.\nC. Data Quality and Relevance\nOne of the key obstacles we face is guaranteeing thequality and appropriateness of the data CourseGPT employsfor knowledge retrieval and creation. The precision and scopeof the class documentation, from which the data is directlyextracted, profoundly affects CourseGPT's ability to offervaluable aid to students.\nSustaining data integrity and relevance in constantly evolv-ing research environments, where new information is fre-quently generated and updated, presents persistent challenges.It is crucial to continue monitoring, updating, and validatingcourse material and implement robust data preprocessing andfiltering mechanisms to overcome these hurdles."}, {"title": "VII. CONCLUSION", "content": "CourseGPT signifies a significant advancement in integrat-ing generative AI into educational environments, demonstrat-ing notable improvements in student engagement and learningoutcomes. By leveraging the power of large language models,CourseGPT provides accurate, context-aware, and immediateresponses to student inquiries, thus fostering a more inter-active and personalized learning experience. ImplementingCourseGPT in courses such as CPR E 431 has shown promis-ing results, with increased correctness and faithfulness scores,underscoring the potential of large models like Mixtral-8x7b inenhancing educational support systems. Moving forward, wewill focus on refining CourseGPT's capabilities and addressingchallenges related to computational efficiency, scalability, anddata relevance. Continuous improvements and adaptations willbe necessary to ensure that CourseGPT remains a robust and"}]}