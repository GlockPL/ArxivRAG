{"title": "Online Location Planning for AI-Defined Vehicles: Optimizing Joint Tasks of Order Serving and Spatio-Temporal Heterogeneous Model Fine-Tuning", "authors": ["Bokeng Zheng", "Bo Rao", "Tianxiang Zhu", "Chee Wei Tan", "Jingpu Duan", "Zhi Zhou", "Xu Chen", "Xiaoxi Zhang"], "abstract": "Advances in artificial intelligence (AI) including foundation models (FMs), are increasingly transforming human society, with smart city driving the evolution of urban living. Meanwhile, vehicle crowdsensing (VCS) has emerged as a key enabler, leveraging vehicles' mobility and sensor-equipped capabilities. In particular, ride-hailing vehicles can effectively facilitate flexible data collection and contribute towards urban intelligence, despite resource limitations. Therefore, this work explores a promising scenario, where edge-assisted vehicles perform joint tasks of order serving and the emerging foundation model fine-tuning using various urban data. However, integrating the VCS AI task with the conventional order serving task is challenging, due to their inconsistent spatio-temporal characteristics: (i) The distributions of ride orders and data point-of-interests (PoIs) may not coincide in geography, both following a priori unknown patterns; (ii) they have distinct forms of temporal effects, i.e., prolonged waiting makes orders become instantly invalid while data with increased staleness gradually reduces its utility for model fine-tuning. To overcome these obstacles, we propose an online framework based on multi-agent reinforcement learning (MARL) with careful augmentation. A new quality-of-service (QoS) metric is designed to characterize and balance the utility of the two joint tasks, under the effects of varying data volumes and staleness. We also integrate graph neural networks (GNNs) with MARL to enhance state representations, capturing graph-structured, time-varying dependencies among vehicles and across locations. Extensive experiments on our testbed simulator, utilizing various real-world foundation model fine-tuning tasks and the New York City Taxi ride order dataset, demonstrate the advantage of our proposed method.", "sections": [{"title": "I. INTRODUCTION", "content": "With the rapid growth of urban population worldwide, urban management faces increasing challenges, giving rise to the concept of smart cities, which aim at improving urban lives through environmental monitoring [1], traffic control [2], healthcare [3], etc. Urban data is essential for smart city applications, and how to obtain and use data effectively is a key issue. Fortunately, mobile crowdsensing (MCS) [4] provides a useful way to collect data, making use of users' mobile devices (or users themselves) as sensing units to complete large-scale and complex social sensing tasks. Compared to MCS, the drawbacks of using fixed devices for data collection include mainly high installation and maintenance costs, as well as very limited coverage [5]. Building on the success of MCS, vehicle crowd-sensing (VCS) present unprecedented opportunitiesleveraging the mobility of vehicles, including unmanned and electric types, equipped with high-precision sensors, can collect various types of data, such as air quality data, traffic conditions, and street view images, to assist government agencies in better city management. Notably, ride-hailing vehicles are particularly advantageous for VCS tasks, due to their centralized ride-hailing platform management, which reduces the cost of deploying and executing crowdsensing tasks, and utilizes the data and computing resources from ride-hailing vehicles to maximize the VCS task utilities.\nOpportunities and challenges. In the meanwhile, foundation model (FM)-powered AI applications have revolutionized numerous aspects of human lives, including healthcare, education, industry, etc. FMs, e.g., BERT, GPT-4, ViT, serve as foundation for different downstream tasks in languages, vision, graph processing, and multimodal applications. Among them, a notable emerging category is urban foundation models (UFMs), which are trained on diverse modalities of urban data and are designed to interpret them effectively. UFMs supported applications include environmental monitoring, urban planning, energy management, and our envisioned future in-vehicle infotainment (IVI) applications [6]. While a substantial body work has developed various UFMs and downstream tasks [7], [8], [9], [10], optimization-based strategies that leverage existing infrastructures of MCS and ride-hailing vehicles to execute UFM-based tasks for utility maximization, along with fulfilling traditional order serving tasks, are under-explored. This is mainly due to: (i) UFMs trained by various, multi-modal urban data still require fine-grained adaptation for distinct downstream urban tasks; but fine-tuning using spatio-temporal heterogeneous data and performed by a large number of moving vehicles typically needs complex quantitative modeling to maximize the VCS utility. (ii) Deploying model fine-tuning has to be balanced with the conventional order tasks of ride-hailing vehicles."}, {"title": "Model Fine-Tuning and Order Serving Tasks Scenario.", "content": "In this work, we take the first attempt to comprehensively study the scenario where ride-hailing vehicles participate in VCS tasks using parameter efficient fine tuning (PEFT) techniques [11] to adapt distinct UFMs, while maintaining the opportunities to pick up passengers for order serving. As illustrated in Fig. 1, each vehicle can cruise in the city and gain monetary utilities in both tasks. There are a set of data points of interest (PoI) distributed in cities, which represent the locations of RSUs equipped with edge servers, where any one or more types of labeled dataset can be generated in real-time nearby and stored. Due to the large volume, data stored in the RSU server can be discarded in a certain period of time. In practice, these data can be descriptive features and feedbacks (labels) of recommendation or generative AR applications, generated by nearby visitors or residents. They can also be traffic/environment monitoring data with labels generated by running efficient deep learning model inference deployed in the RSU. The government or any company that collaborates with the ride-hailing vehicle company has multiple types of VSC tasks to fulfill, each of which needs certain locations of data for fine-tuning UFMs. Whenever a vehicle chooses to perform a VSC task, it needs to download the corresponding UFM from the cloud, move to associated locations for collect real-time data, fine-tune the Low-Rank Adapters (LoRA) of the UFM on the route, and push the fine-tuned adapters to a nearby RSU or the cloud for the task owner to use in UFM updates and inference. Meanwhile, passengers' order requests also emerge at various times and locations, waiting for ride-hailing vehicles to serve. Therefore, at any given time, each vehicle can cruise to a certain location (squared grid), pick up the passengers to serve the orders before order acceptance deadlines, and/or collect data from a PoI along the route, or just stay in its current place. As depicted in Fig. 2, both delays in order acceptance and in data collection for fine-tuning can significantly impact the vehicle's utility. Our goal is to provide best next-step location decisions on behalf of each vehicle, to maximize the overall utility of all the vehicles in performing the joint tasks of order serving and UFM model fine-tuning.\nDespite promising benefit that can be brought to smart city development by leveraging UFMs, PETF techniques, and the advantages (mobility, resources) of ride-hailing vehicles, there are three types of technical challenges that we need to address.\n\u2022 High-dimensional states in vehicle location planning for spatio-temporal heterogeneous UFM fine-tuning. In this work, our goal is to maximize the long-term, overall utility of all the tasks that fulfilled by the ride-hailing vehicles. Given the uncertain effects of different factors on the performance of model fine-tuning using urban data, a natural and advantageous choice of optimization framework is reinforcement learning (RL), which can effectively learn the optimal online decisions through sequential interactions with dynamic environments. However, employing global information for centralized decision-making frequently results in state space expansion and poor scalability. Distributed decision-making methods, e.g, multi-agent RL (MARL), prove more apt in addressing these issues when compared to centralized approaches. But an associated challenge arises, i.e., the optimization performance can be suboptimal, resulting from independent decisions based solely on local states. Moreover, the widely adopted performance metric for UFM-based downstream is inference accuracy, but the controllability of our framework is in vehicles' locations and their choices of tasks. The underline map-"}, {"title": "\u2022 Inconsistent geo-distributions of data PoIs and orders.", "content": "Ride-hailing orders are mostly concentrated in certain areas of the city, such as the central business district. There may be very few ride-hailing orders in remote areas, such as the outskirts of the city. Unlike order-serving, many UFM based VCS tasks require vehicles to collect data from every corner of the city [12], [13], [14]. There is a potential scenario where increased attention is directed towards PoIs located in remote suburban areas. An example of such a scenario is the collection of air quality data from environmental monitoring stations situated in forested regions. Collecting data distributed in areas beyond the city center in a timely manner becomes challenging when vehicles are overly focused on seeking ride orders and concentrated in the city center. To simultaneously enhance overall the efficiency of order fulfillment and data collection, it is imperative to strategically allocate vehicles among different regions based on the distribution of orders and data points. In order to increase the effectiveness of MARL algorithms, it is crucial to select the right association between an algorithmic \"agent\" and an environmental item (a location, a vehicle, or a task). Besides, adopting MARL may fall short of handling this inconsistent geo-distributions and balancing the vehicles onto different location, as each agent's decisions may only focus on its local perspective, while allowing full environmental states for each vehicle hinders the scalability and violates our original goal to mitigate the high-dimensionality of state space."}, {"title": "\u2022 Different utility characteristics and requirements of order-serving and model fine-tuning.", "content": "Since each order-serving task has a pickup deadline and a fixed route determined exogenously (e.g., shortest path or minimum travel time), the objective is to maximize high-value order completions within certain distance constraints. In contrast, fine-tuning FMs requires vehicles to collect fine-tuning data of sufficient volume and quality. Achieving sufficient data volume may involve guiding the RL agent to visit data-rich locations or multiple locations for broader collection. Data quality factors may further include similarity to the base foundation model's training data and alignment with downstream tasks' inference data. Moreover, order serving and data collection tasks have different time-sensitivity. Long delays in response to an order prompt the passenger to cancel the request, rendering it an invalid order. Differently, since fine-tuning data is generated in real time, it possesses a characteristic of data freshness which should effect the model performance in a fine-grained manner. For instance, inference accuracy of a fine-tuned model probably benefits from vehicles that promptly reach data collection points and quickly update fine-tuned PEFT adapters to minimize latency between fine-tuning and inference. These varying demands add complexity to RL agents' learning processes in balancing the gains of the two jointly executed tasks."}, {"title": "II. RELATED WORK", "content": "In this section, we provide insightful existing works in five related categories of research areas. We also highlight the key differences between these works and our study, in each paragraph.\nVehicle Crowdsensing (VCS). In VCS systems, vehicles move within defined ranges and collect data for performing VCS tasks, e.g., machine learning driven traffic monitoring. Various studies consider optimizing different metrics for these tasks, such as the volume of data collected, the geographic fairness of the collection, or the sensing coverage. For example, Fan et al. [15] formulate the VCS problem as a stochastic dynamic program and achieve fine-grained spatio-temporal sensing coverage at the minimum long-term cost. Other works [16], [17], [18] focus on protecting the privacy of the vehicles participating in the crowdsensing tasks. In [19], researchers explore the optimal selection of mobile vehicles using a deep learning-based offline algorithm that predicts vehicle mobility. Recently, to overcome the constraint of ground-only operations, unmanned aerial vehicles (UAVs) have been incorporated for data collection, collaborating with ground vehicles. Yu et al. [20] maximize the amount of collected data and minimize energy consumption through the cooperation between unmanned ground vehicles (UGVs) and UAVs. Furthermore, the EHTA framework [21] introduces an environmentally-aware heterogeneous task allocation mechanism to ensure fairness and efficiency in vehicular crowdsensing. In terms of traffic monitoring, Li et al. [22] introduced a novel security model to define the misbehavior of malicious drivers in traffic monitoring via fog-assisted vehicular crowdsensing. Notably, our approach diverges from these studies as we study a new scenario where ride-hailing vehicles can gain utilities in joint tasks of order serving and model fine-tuning.\nModel Fine-tuning in Vehicular Networks. Numerous studies have explored the integration of machine learning (ML) tasks within vehicular networks, demonstrating their potential to enhance intelligent transportation systems. For instance, ML algorithms have been employed for tasks such as real-time traffic prediction [23], [24], dynamic routing optimization [25], and autonomous vehicle decision-making [26]. These studies underscore the growing reliance on vehicular networks for executing computationally intensive tasks in distributed environments. Recent works have increasingly focused on fine-tuning models for vehicular networks, particularly leveraging RSUs and edge devices for collaborative learning. GIOV [27] proposed a federated learning framework using RSUs to fine-tune models for adaptive vehicular applications, achieving enhanced performance in resource-constrained environments. Similarly, GAI-IOV [28] explored the deployment of pre-trained generative models for personalized content delivery in connected vehicles, addressing privacy concerns through local fine-tuning mechanisms. Otoum et al. [29] introduced an energy-efficient strategy for fine-tuning models across vehicular networks, focusing on minimizing latency and resource consumption during updates. In parallel, fine-tuning UFMs has proven transformative in addressing specific challenges in urban environments. For example, GeoSAM [30] enhances the SAM [31] model for mobility infrastructure segmentation by integrating automated visual prompt generation, showcasing its potential for urban planning. RingMo-SAM [32] customizes SAM's prompt encoder for multi-source remote sensing segmentation, significantly improving performance with complex urban datasets. Additionally, GeoCLIP [33] fine-tunes the CLIP [34] framework to align images with geographic coordinates for global-scale geo-localization, utilizing innovative GPS encoding and hierarchical representations to enhance performance even with sparse training data.In contrast to these studies, our work introduces a novel scenario that synergistically benefits a diverse array of smart city applications. We not only mathematically formulate the online joint optimization of order-serving and vehicular model fine-tuning, capturing the intricate dynamics between vehicles and their urban environments, but also consider critical factors often overlooked. Specifically, our approach emphasizes the age of fine-tuning data, recognizing its significance in maintaining model relevance in dynamic urban contexts, and examines the impact of data volume on fine-tuning accuracy, ensuring robust performance even under constrained conditions. This holistic perspective enhances both the practicality and effectiveness of vehicular model fine-tuning in real-world applications.\nVehicle Dispatching. Recently, with the rapid development of ride-hailing platforms, such as Didi [35] and Uber [36], more and more private car owners are opting to become ride-hailing drivers. In a large-scale fleet system, it makes sense to reasonably dispatch vehicles to maintain a balance between supply and demand. Traditionally, vehicle dispatching problems are commonly approached as classic combinatorial optimization problems, exemplified by the Traveling Salesman Problem (TSP) or the Vehicle Routing Problem (VRP). For instance, Zhang et al. [37] propose a novel combinatorial optimization model for the dispatching problem maximizing the global success rate by optimally matching drivers and riders, thus enhancing overall travel efficiency and improving the user experience. In [38], a data-driven optimization method is deployed on the transportation network company's side to efficiently and effectively schedule the vehicles'cruising plan. Yuen et al. [39] propose performing dynamic programming to help drivers find the route most likely to pick up passengers and maximize the probability of finding additional compatible customers while minimizing detours beyond a permissible threshold. Other works may choose different optimization goals such as vehicle cruising time minimization [40] or constraints such as energy consumption limits [41], [42]. These works require a relatively complex mathematical model to model the distribution of orders or environmental dynamics, e.g., traffic or weather conditions, and their common focus is order serving provided to passengers. Our research distinguishes itself from theirs as our vehicles also bear the responsibility of data collection and model fine-tuning."}, {"title": "III. PROBLEM FORMULATION", "content": "In this section, we provide a detailed description of our scenario. We consider a vehicular network, consisting of a large number of moving ride-hailing vehicles that are managed by a cloud platform (such as DiDi or Urber) within a certain geographical range and can interact with RSUs equipped with edge servers. Beyond the routine operations of picking up and dropping off passengers, each vehicle actively engages in urban sensing tasks. Leveraging the installed professional sensors and smartphones, the vehicles can collect data from PoIs distributed across the designated area. We define $M \\equiv {m | m = 1, 2, ..., M}$ to represent the set of vehicles in the system. The activity range of each vehicle is limited to the target area. Similar to many prior studies [50], [51], we discretize the target area into $G$ grids, represented by set $G = {g | g = 1, 2, ..., G}$. Similarly, the time horizon is also divided into several discrete time slots, represented by $T \\equiv {t | t = 1, 2, ..., T}$. The set of orders and the set of PoIs at time slot $t$ are $O_t$ and $P_t$, respectively. At time slot $t$, the set of orders is denoted as $O_t$, and the set of Points of Interest (PoIs) is represented as $P_t$. To account for task-specific requirements, $P_t$ can be decomposed into subsets, where each subset corresponds to a distinct type of data PoI associated with a particular task. Formally, this decomposition can be expressed as:\n$P_t = \\bigcup_{k \\in K} P_{t,k}, P_{t,k} \\cap P_{t,j} = \\emptyset, \\forall k \\neq j,$\nwhere $K$ is the set of task types, and $P_{t,k}$ denotes the subset of PoIs related to task $k$. For instance, $P_{t,1}$ represent PoIs contributing to Vision Transformers (ViT) [57] based tasks such as image classification, while $P_{t,2}$ corresponds to PoIs relevant to SAM-based tasks like image segmentation. Since the number of orders and PoIs change at each time slot, the size of $O_t$ and $P_t$ can be distinct across $t$. We consider vehicles that currently not serving orders or collecting data as available vehicles. For grid $g$, we use $O^g_t$, $M^g_t$ and $P^g_t$ respectively to represent the set of orders, the set of available vehicles, and the set of PoIs in grid $g$ at time slot $t$. The index of the grid (interchangeable with location in this paper) where vehicle $m$ is located at time slot $t$ is $g^m_t$, where we have $g^m_t \\in G$. To maximize the effectiveness of order-serving and data collection, each available vehicle $m$ needs to decide whether to accept an order $o \\in O_t$, collect data from a PoI $p \\in P_t$, or travel to another grid. Once an available vehicle $m$ accepts an order or collects data from a PoI, it becomes unavailable and will revert to being available again after completing passenger drop-off or data collection. For convenience, some of the important notations used in the paper are listed in Table I."}, {"title": "B. Primer on Model Fine-tuning with LoRA", "content": "Low-Rank Adaptation (LoRA) is a powerful technique designed to efficiently fine-tune large pre-trained models, without the need to retrain all model parameters. Traditional fine-tuning methods often involve updating the entire model's weights, which can be computationally expensive and resource-intensive, especially with massive models. LoRA addresses this by observing that the difference between the pre-trained weights and the fine-tuned weights often lies in a low-dimensional subspace. Thus, LoRA introduces a low-rank approximation to model these changes, significantly reducing the number of parameters that need to be updated. The benefits of LoRA include a substantial reduction in memory and computational costs, as well as the ability to adapt large models to new tasks with minimal overhead. Formally, LoRA modifies the weight matrix $W_0 \\in \\mathbb{R}^{d \\times k}$ of a pre-trained model by introducing a low-rank update, represented as $W_0 + \\Delta W = W_0 + BA$, where $B \\in \\mathbb{R}^{d \\times n}$ and $A \\in \\mathbb{R}^{n \\times k}$, with $\\eta \\ll \\min(d, k)$. The matrices $B$ and $A$ are the only trainable parameters, while the original weights $W_0$ are frozen. The model input is denoted as $x$, and the output is represented as $h$. The forward pass incorporating the LoRA module is given by:\n$h = W_0x + \\gamma \\eta BAx,$\nwhere $\\gamma$ is a scaling factor, and $\\eta$ is the rank of the low-rank matrices. This formulation shows how LoRA modifies the original model by adding a low-rank update in parallel to the pre-trained weights. The rank $\\eta$ controls the capacity of the low-rank adaptation, allowing for efficient fine-tuning with minimal changes to the original model. Additionally, LoRA preserves computational efficiency during inference, making it particularly useful for deployment in resource-constrained environments."}, {"title": "C. QoS and Optimization Modeling for Vehicular Joint Tasks", "content": "Subsequently, we define the Accumulated Driver Income (ADI), the Accumulated Data Utility (ADU), and the QoS. Following this, we give the mathematical form of the problem and the optimization goal.\nDefinition 1. (ADI) We use $O_s^g$ to denote the set of orders accepted by vehicles in grid $g$ at time slot $t$. For order $o$ within the set $O_t$, we denote by $\\sigma(o)$ the price of order $o$. ADI represents the total income of all drivers over all time slots, so the expression for ADI is as follows:\n$ADI = \\sum_{t=1}^T \\sum_{g=1}^G \\sum_{o \\in O_s^g} \\sigma(o).$\nThere are several PoIs distributed within each grid, and vehicles can collect data from these Pols. Each PoI $p \\in P_t$ is associated with a certain volume of data $d(p)$ that needs to be collected. To maintain data integrity and prevent excessive bandwidth usage, we assume that an available vehicle can collect data from only one Pol at a time, and the collection process continues until the pending data volume at the Pol is reduced to 0.\nDefinition 2. (ADU) We define ADU as the sum of the data utility collected by all vehicles in all time slots. The expression for ADU is defined as:\n$ADU = \\sum_{t=1}^T \\sum_{g=1}^G \\sum_{p \\in P_c^g} u,$\nwhere $P_c^g$ is the set of PoIs collected in grid $g$ at time slot $t$. We use ADU to indicate the quality of VCS in the following sections."}, {"title": "Freshness and quantity based data utility.", "content": "Many existing studies tend to overlook the importance of data freshness in model fine-tuning, yet we explicitly take into account the effects of using various freshness degrees of collected data to fine-tune the foundation model using LoRA techniques, and we focus on the inference accuracy of such fine-tuned models varying data freshness. Intuitively, the freshness of data should hold significant relevance in numerous real-time VCS tasks [58]. For instance, in traffic monitoring, the most recent traffic data is more beneficial for intelligent transportation systems [59]. But these works are not foundation fine-tuning tasks and the task performance has different metrics from ours. To verify this intuition, we conducted a series of experiments on various large model fine-tuning tasks, including LoRA-based fine-tuning for image classification using ViT and image segmentation using SAM. As Fig. 3 show, the relationship between fine-tuning accuracy and data freshness exhibits a complex and often unpredictable pattern. Specifically, accuracy tends to follow a concave or even linear decay as data freshness decreases, though the exact functional form is task-dependent and difficult to predict. This underscores the challenge of modeling data freshness in real-world applications. To address this uncertainty, we propose leveraging DRL to learn optimal fine-tuning strategies for varying data freshness. Additionally, our experiments reveal that accuracy improves with the volume of data, but the relationship is not linear. As the amount of data increases, the fine-tuning accuracy shows a logarithmic growth, and beyond a certain threshold, further increases in data volume yield diminishing returns. This phenomenon is consistent with the scaling laws for neural language models [60], which highlights the diminishing impact of additional training data after reaching a critical dataset size. These results emphasize the importance of considering both data freshness and quantity when fine-tuning UFMs, and highlight the potential of DRL in optimizing these factors for improved task performance."}, {"title": "Definition 3. (QoS)", "content": "To finally capture the overall performance of order-serving and fine-tuning tasks, we propose a QoS function to evaluate the overall utility of all the vehicles in the network that accomplish both order serving and model fine-tuning tasks, i.e.,\n$QoS = \\alpha ADI + \\beta ADU,$\nwhere $\\alpha$ and $\\beta$ are importance factors that balance the contributions of ADI and ADU, respectively. These weights are determined collaboratively by stakeholders, such as government entities or organizations overseeing vehicle-sensing coordination tasks and ride-hailing companies responsible for passenger service. By setting these hyperparameters in advance, the system adapts to operational priorities and ensures alignment with strategic objectives.\nIn our settings, the operations that each available vehicle can perform are denoted as x, y, and z, corresponding to dispatching, order acceptance, and data collection, respectively. The dispatching decision x exerts a certain impact on both y and z, and has a long-term impact on QoS. Specifically, in real-world applications, the distribution of orders in the city is non-uniform, and a sequence of dispatch decisions for a vehicle will allocate the vehicle to grids with varying order quantities. The vehicle assigned to grids with a lower supply-demand ratio has more opportunity to match an order successfully."}, {"title": "IV. PROPOSED SOLUTION: GNN-ENHANCED MARL", "content": "In this section, we provide a comprehensive description of the algorithms designed to address the challenges posed by the joint scenarios of ride-hailing vehicle dispatching and crowdsensing. Our algorithm leverages MARL and integrates with other technologies to optimize the QoS as defined in (6)."}, {"title": "A. Algorithm Overview", "content": "Due to the complexity of joint optimizing decision variables for fleets in urban environments, we employ a decentralized optimization framework. For each time slot, decision variables x, y and z can be independently determined by each available vehicle. Optimizing the QoS becomes feasible if the framework can learn the statistical distribution of orders and PoIs while gaining insights into their time sensitivity. The utilization of a distributed optimization framework effectively mitigates the challenge of the decision space rapidly expanding with the size of the vehicle set. The demand for online optimization and distributed decision-making motivates us to use MARL, renowned for its excellent performance in long-term and coupled decision-making [61]. We illustrate the structure of our framework in Fig. 4. Our system is based on the actor-critic MARL algorithm, with each agent making independent decisions. Additionally, we incorporate GNN for raw state embedding. Furthermore, the framework integrates the RankTuner module, enabling dynamic adjustment of LoRA ranks to balance fine-tuning accuracy and efficiency. In the subsequent sections, we provide a concise overview of key settings such as states, actions, and rewards, followed by an explanation of GNN-based state embedding."}, {"title": "B. MARL Statement", "content": "Formally, we model the joint optimization problem as a Markov game, which is represented by a tuple (S, A, P, R), where S, A, R and P are the set of states, actions, rewards, and state transition probability. We define the important components in the MARL framework as follows.\nAgent. We consider a vehicle as an agent. As our objective is to optimize the overall income and data utility of all vehicles, each vehicle can be considered as a homogeneous agent performing cooperative tasks. We still use M to represent the set of agents.\nState space. Intuitively, a global environmental state should encompass factors such as the distribution of drivers, the distribution of orders, the distribution of PoIs, the current time slot t, the generation time and estimated travel time for each order, and the data volume and Aol for each PoI. At the beginning of each time slot t, each vehicle m gets a local state correlated with the global environment state st \u2208 S, which can be written as sm,t. The challenge arises in determining whether to aggregate all the relevant factors into a large state space for every agent or to partition the state space into subspaces for each agent. Both approaches prove to be inefficient. Moreover, the continuous changes in the number of orders and Pols lead to a dynamic change in the state space dimension over time. Selecting a fixed dimension for the state space becomes impractical, posing challenges for implementing the MARL algorithm. To tackle these challenges, we leverage Relational Graph Convolutional Networks (R-GCN) [62] to encode the features of each agent. The output of R-GCN, denoted as s'm.t, serves as the embedding state of each agent, integrating the raw state and reducing the raw state dimension to a fixed dimension. The concrete representation of the state space and the detailed process of state embedding will be presented in the subsequent subsection.\nAction space. At every time slot, each available agent m takes an action am,t according to its policy after getting the embedding state s'm.t. The action am,t indicates whether the agent should be dispatched to a neighboring grid, remain in the current grid and whether it should accept an order or"}, {"title": "Tm,t = 0.", "content": "In this paper, remaining in the current grid is conceptualized as a special form of dispatching. We assign a reward of 0 for dispatching since dispatching doesn't directly yield rewards, although it can influence subsequent actions. Simultaneously, this is implemented to discourage agents from repeatedly dispatching between specific grids to gain rewards. Despite the reward value being 0 for dispatching, our expectation is that, through MARL, agents can still learn the influence of dispatching on order-serving and data collection, enabling a more effective dispatching policy."}, {"title": "If agent m decides to accept an order at time slot t, the reward of m is written as:", "content": "Tm,t = \\alpha \\cdot \\sigma(o^m_t),\nwhere $o^m_t$ represents the order that be accepted by m, and $\\sigma(o^m_t)$ represents the price of $o^m_t$. Here, \u03b1 is a weight, which is the same as in expression (5)."}, {"title": "If agent m decides to collect data from a Pol at time slot t, the reward of m is written as:", "content": "r_{m,t} = \\beta u = \\beta f_k(d, \\Lambda)d(p^m_{t,k}), \\Lambda^m_{t,k}).\nHere, $r_{m,t}$ equals the utility of the data collected by m, and $p^m_t$ is the PoI collected by agent m at time slot t with task k. \u03b2 is also a weight like \u03b1. The function $f_k(d, \\Lambda)$ encapsulates the fine-tuning accuracy under the combined influence of two key factors: d, the amount of collected data, and \u039b, the degree of data freshness (also referred to as AoI). The subscript k indicates the task-specific nature of the function, as different tasks may exhibit unique dependencies on data quantity and freshness. This function quantifies how the accuracy of the fine-tuned model varies based on these two variables. As both data quantity and freshness directly impact the utility function, the interplay between these variables determines the model's fine-tuning performance, with"}, {"title": "C. State Embedding", "content": "Reviewing the structure and influencing factors of our optimization problem, the arrival of ride orders and the generation of Pols follow a prior but unknown distribution. To ensure adaptability to the dynamic and highly stochastic environment, each agent requires a policy network with high generalization, utilizing ample state information to formulate decisions. These decisions aim to optimize the overall QoS throughout numerous time slots from a long-term perspective. Therefore, continuous monitoring of the states of vehicles, orders, and Pols is crucial for each agent. First, we define the features of raw state for the agent m, namely sm,t = {Im,t, Um,t, Vm,t, Hm,t}, serving as the input of RGCN and is formalized as follows."}, {"title": "D. Heuristic-based Rank Selection Integrated with MARL", "content": "Since LoRA is employed for task fine-tuning, selecting an appropriate rank is crucial. According to relevant studies [63], a larger rank generally leads to better fine-tuning accuracy but at the cost of increased fine-tuning time. An interesting and important trade-off here is that if choosing a larger rank greedily for obtaining a higher accuracy per task, the longer fine-tuning time will very likely reduce the expected total number of fine-tuning tasks that a vehicle can accomplish in the entire time span. Therefore, the best rank of a fine-tuning adaptor should be carefully chosen to achieve the total utility.\nOur experiments, as summarized in Table II, show that the choice of rank significantly impacts both accuracy (in terms of accuracy discounts compared to the best accuracy) and fine-tuning time across different tasks, such as image classification and image segmentation. To address this, we propose the RankTuner, a dynamic adjustment mechanism designed to determine the most suitable rank for fine-tuning when the optimal rank is unknown. The RankTuner operates as follows: Initially, a rank is randomly selected as the benchmark. During each iteration, the algorithm compares the current ADU to the previous round. If the ADU improves, the algorithm maintains the current direction (increasing or decreasing the rank) to further explore better settings. If the ADU decreases, the algorithm reverts to the previous rank and switches the direction. The rank is constrained within a predefined allowable range to ensure feasibility."}, {"title": "E. Training", "content": "Our MARL model is based on multi-agent proximal policy optimization (MAPPO) [64]. Each agent has a flag bit to indicate whether the agent is available or not. We ignore the output of the action by non-available agents. Due to the fewer dispatching destinations in the boundary grids than in the non-boundary grids, we mask the corresponding dispatching action for agents in boundary grids. For every policy update step, we collect a batch of trajectories from the environment and compute the loss function according to [65]. Our MARL model is trained online because online training has a higher utilization rate of sampled data. We apply some implementation techniques, including Generalized Advantage Estimation (GAE) with advantage normalization and value-clipping."}, {"title": "V. EVALUATION", "content": "In this section, we assess the efficacy of the proposed methodology through a comprehensive evaluation. We conducted experiments based on real-world datasets of orders and compared the results with several comparison algorithms. The experimental results show that our method has better performance."}, {"title": "A. Experiment Settings", "content": "Model configurations. Our algorithm is implemented by PyTorch and DGL. The actor network of each agent is a threelayer fully connected network, where the size of the hidden layer is set to 64. The structure of the Critic network is the same as that of the actor network. The hidden layer dimension of R-GCN is 128, and the output embedding state dimension is"}, {"title": "V. EVALUATION", "content": "B. Performance\nWe analyze the experimental results as follows. The distribution of Pol is distribution 1.\n1) QoS: We demonstrate the QoS of our method and the other 5 baselines during the training in Fig. 8(a). The experimental results indicate that"}]}