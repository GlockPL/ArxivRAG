{"title": "EVALUATING LLMS FOR TEXT-TO-SQL GENERATION WITH COMPLEX SQL WORKLOAD", "authors": ["Limin Ma", "Ken Pu", "Ying Zhu"], "abstract": "This study presents a comparative analysis of the a complex SQL benchmark, TPC-DS, with two existing text-to-SQL benchmarks, BIRD and Spider. Our findings reveal that TPC-DS queries exhibit a significantly higher level of structural complexity compared to the other two benchmarks. This underscores the need for more intricate benchmarks to simulate realistic scenarios effectively. To facilitate this comparison, we devised several measures of structural complexity and applied them across all three benchmarks. The results of this study can guide future research in the development of more sophisticated text-to-SQL benchmarks.\nWe utilized 11 distinct Language Models (LLMs) to generate SQL queries based on the query descriptions provided by the TPC-DS benchmark. The prompt engineering process incorporated both the query description as outlined in the TPC-DS specification and the database schema of TPC-DS. Our findings indicate that the current state-of-the-art generative AI models fall short in generating accurate decision-making queries. We conducted a comparison of the generated queries with the TPC-DS gold standard queries using a series of fuzzy structure matching techniques based on query features. The results demonstrated that the accuracy of the generated queries is insufficient for practical real-world application.", "sections": [{"title": "1 Introduction", "content": "The task of generating SQL queries from natural language (NL) has been a long-standing problem in the field of natural language processing (NLP) and databases. The task is important as it can enable non-expert users to interact with databases without having to learn SQL. The task is also important for database administrators who can use the generated SQL queries as a starting point for further optimization. The task is also important for data scientists who can use the generated SQL queries to analyze data and generate insights.\nIn recent years, large language models (LLMs) have shown impressive performance on a wide range of NLP tasks. LLMs are pre-trained on large amounts of text data and fine-tuned on specific tasks. LLMs have been shown to achieve state-of-the-art performance on a wide range of NLP tasks, including text-to-SQL generation. However, the task of generating SQL queries from NL is challenging due to the complex structure of SQL queries and the need for accurate decision making.\nIn the effort to evaluate the performance of LLMs on the task of generating SQL queries from NL, two major benchmarks have been proposed: BIRD and Spider. BIRD is a benchmark for text-to-SQL generation that consists of over"}, {"title": "2 Related Work", "content": "The field of text-to-SQL has seen significant progress owing to the integration of large language models (LLMs) [Naveed et al.(2023)Naveed, Khan, Qiu, Saqib, Anwar, Usman, Barnes, and Mian, Bae et al.(2024)Bae, Kyung, Ryu, Cho, Lee, Kweon, Oh, Ji, Chang, Kim, et al.]. Innovative techniques have been proposed for improving the accuracy and efficiency of SQL query generation from natural language inputs. We discuss several key works in this domain, and present their contributions and how they relate to our work.\nOwda et al. [Owda et al.(2011)Owda, Bandar, and Crockett] present an early system that incorporates Information Extraction (IE) techniques into an Enhanced Conversation-Based Interface of Relational Databases (C-BIRD) to generate dynamic SQL queries. Their approach allows conversational agents to interact with users in natural language, generating SQL queries without any user requirement of prior SQL knowledge. Since then, LLM-based solutions have dominated the field of text-to-SQL. Furthermore, two major benchmarks, BIRD and Spider, have been proposed to evaluate the performance of LLMs on the task of generating SQL queries from NL. Yu et al. [Yu et al.(2018)Yu, Zhang, Yang, Yasunaga, Wang, Li, Ma, Li, Yao, Roman, et al.] introduce Spider, a large-scale, complex, and cross-domain semantic parsing and text-to-SQL dataset. Spider includes 10,181 questions and 5,693 unique complex SQL"}, {"title": "3 Features And Complexity Measures of SQL Queries", "content": "Consider a SQL query Q as shown in Figure 1. Given the nested structures of SQL, Q can be a complex self-referencing tree of SQL queries. In this section, we will define a number of features that can be used to characterize the structural complexity of SQL queries. We argue that the bag-valued features are useful in performing semantic comparison of pairs of SQL queries.\nLet SQ(Q) be all sub-queries in Q regardless of where it appears in Q. We define a collection of features derived from Q and its sub-queries. These features are divided into two categories: bag-valued features and numeric features. Namely, a feature F is a mapping:\n$F_{bag}: SQL \\rightarrow Bags \\qquad F_{count}: SQL \\rightarrow N$\nWe note that any bag feature F is also naturally a numeric feature $F^{\\#}$: $F^{\\#}(Q) = |F(Q)|$."}, {"title": "3.1 Bag-valued Features", "content": "Columns: the set of distinct columns included in the select column expressions of Q and its subqueries.\n$Cols(Q) =\\{c \\in Columns : c \\in SelectExpressions(Q)\\}$\n$\\bigcup_{Q' \\in SQ(Q)} Cols(Q')$\nRelations: Tables(Q) the set of distinct relations in the SQL query. This is computed as:\n$Relations(Q) =\\{r \\in Relations : r \\in FromClause(Q)\\}$\n$\\bigcup_{Q' \\in SQ(Q)} Relations(Q')$\nWhere predicates: WHERE(Q) the set of distinct where basic predicates in the SQL query and its subqueries. Each basic predicate is given in the form of\n(pred, expr.,...)\nwhere pred is the boolean operator supported by SQL, and expr is an expression over column names, constants, and functions. All columns aliases are normalized to the canonical form.\nJOINS: JOIN(Q) is the set of joins of Q is defined to be pairs of physical relations (t, t') that participate in a JOIN in the logical query plan of Q and its subqueries.\nAggregation: Agg(Q) is the set of aggregated columns in Q and its subqueries. Each aggregated column is given in the form of (agg, expr, groupby columns) where agg is the aggregation function (e.g., SUM, AVG, COUNT, etc.) and expr is an expression over column names, constants, and functions.\nFunctions: Func(Q) is the set of SQL functions that appear in Q and its subqueries."}, {"title": "3.2 Numeric Features", "content": "Common Table Expressions: CTE(Q) is the number of common-table-expressions defined by Q. We use CTE count as an indicator of the structural complexity and readability of the query Q.\nSub-queries: we measure ||SQ(Q)||, the number of subqueries of Q as an indicator of the structural complexity of Q."}, {"title": "4 TPC-DS As A Complex Text-to-SQL Benchmark", "content": "We compare the complexity of WHERE clauses in the SQL queries from the three SQL benchmarks. Complexity of the WHERE clause is measured by the number of basic predicates in the WHERE clause. A basic predicate is one that has a single condition, such as year = 1998. Compound predicates comprising multiple conditions, such as\nyear = 1998 AND ss_quantity BETWEEN 81 AND 100, are counted as multiple basic predicates. The counts of basic predicates in WHERE clauses for the three benchmarks are plotted in a histogram in Fig. 4d. While the BIRD and SPIDER benchmarks are similar in their distribution of WHERE clause predicate counts, the TPC-DS benchmark clearly exhibits substantially greater WHERE clause complexity in its SQL queries. TPC-DS has such greater WHERE clause complexity than the other two benchmarks that its distribution of the counts hardly overlaps with the others. Almost all TPC-DS queries have multiple times the WHERE predicate counts than all the queries from the other two benchmarks.\nAnother metric used to measure the complexity of SQL queries is the number of Common Table Expressions (CTE). The role of a CTE is to minimize repetition of subqueries as well as making the overall SQL statement more structured and thus easier to maintain. We count the number of CTE in each query from the three benchmarks, and plot the histogram in Fig. 4e. It can be seen that both BIRD and SPIDER workloads do not require CTE due to their low degree of semantic complexity. In contrast, TPC-DS includes heavy usage of CTE in the gold queries.\nWe also count the number of distinct columns referenced in SQL queries. The number of distinct columns involved in a query is an indicator of the semantic complexity of that query. A column is included in the count whether it"}, {"title": "5 Evaluating LLMs Using TPC-DS Workload", "content": "We have used 11 different LLMs to generate SQL queries based on the TPC-DS workload. The LLMs are: gemini-1.5, gpt-4, codestral, mixtral-8x22b, mistral-large, llama3-70b, codellama-70b, llama3-8b, codellama-13b, codellama-35b, and codellama-7b. We have used the following prompt to generate the queries.\nSystem Prompt:"}, {"title": "5.1 Structural Complexity of Generated Queries", "content": "In order to compare the structural complexity of the queries generated by the different LLMs, we again look at several count measures, such as number of columns and joins. For each type of measure and each LLM, the mean of the counts in the queries is calculated, and then normalized over the mean count for TPC-DS queries. These normalized mean counts are given in Table. 1. We can see that gemini 1.5, gpt-4, mixtral and llama3-70b match the closest the structural complexity of TPC-DS. The other LLMs generate markedly less complex queries.\nIt is encouraging that large LLMs are capable of generating queries that can match the complexity of TPC-DS workload. But more importantly we want to evaluate the quality of these queries with respect to the gold queries given by TPC-DS. In the next section, we will evaluate each of the generated queries using the bag-valued features as defined in Section 3."}, {"title": "5.2 Accuracy of Generated Queries", "content": "We have defined 7 bag-valued features defined in Section 3. Each feature is a function mapping SQL to a bag of discrete values. To compare two queries: Q the generated query by a LLM, and Q* the corresponding gold query given by TPC-DS benchmark, we use the Jaccard similarity coefficient between the two bags of values for each feature given by:\n$Sim_F(Q, Q^*) = \\frac{|F(Q) \\cap F(Q^*)|}{|F(Q) \\cup F(Q^*)|}$\nTable 2 shows the mean similarity between the queries generated by 11 LLMs and the TPC-DS queries. All LLMs are instructed with the instructions given in Section 5. Five of the top performing LLMs are visualized in a radar plot in Figure 3 and Figure 5. One can see that the accuracy of the generated queries is insufficient for practical real-world application. In particular, none of the LLMs are able to generate queries that match the WHERE predicates and JOIN pairs used by TPC-DS. This highlights the unique challenges posed by the TPC-DS workload in comparison with the BIRD and SPIDER benchmarks."}, {"title": "6 Conclusion and Future Work", "content": "We have compared TPC-DS with existing text-to-SQL benchmarks, BIRD and SPIDER, and found that TPC-DS queries exhibit a significantly higher level of structural complexity compared to the other two benchmarks. We have also evaluated the performance of 11 LLMs in generating SQL queries based on the TPC-DS workload. Our findings indicate that the current state-of-the-art generative AI models fall short in generating accurate decision-making queries. The accuracy of the generated queries is insufficient for practical real-world application.\nTowards a satisfactory solution to complex SQL generation, we identify the following areas of future work.\nEvaluating incremental SQL generation: our experiments show the need of incremental SQL generation to improve the accuracy of generated queries. This motivates us to investigate novel prompt strategies. Based on the observation in Figure 3, we propose to generate WHERE clause and JOIN pairs as a separate LLM prompt, and use the results to prompt LLMs for the rest of the query.\nFine-tuning smaller models: our experiments show that smaller models are not able to match the performance of larger models in the context of complex SQL generation. In situations where cloud based LLMs are not feasible (due to cost or privacy concerns), we propose to investigate fine-tuning of smaller models to improve their performance. In particular, smaller models can certainly be improved in terms of syntax and schema accuracy during generation. Also, ensemble methods involving multiple fine-tuned LLMs can be used to combine the outputs of multiple smaller models to improve the overall accuracy.\nHuman-in-the-loop: Complex SQL generation is a challenging task, and the burden of SQL generation should not be entirely on the AI model. We propose to develop a novel human-in-the-loop workflow in which the AI model can identify the parts of the query that are difficult to generate, and prompt the user to provide additional information."}]}