{"title": "Practical Application and Limitations of AI Certification Catalogues in the Light of the AI Act", "authors": ["Gregor Autischer", "Kerstin Waxnegger", "Dominik Kowald"], "abstract": "In this work-in-progress, we investigate the certification of AI systems, focusing on the practical application and limitations of existing certification catalogues in the light of the AI Act by attempting to certify a publicly available AI system. We aim to evaluate how well current approaches work to effectively certify an AI system, and how publicly accessible AI systems, that might not be actively maintained or initially intended for certification, can be selected and used for a sample certification process. Our methodology involves leveraging the Fraunhofer AI Assessment Catalogue as a comprehensive tool to systematically assess an AI model's compliance with certification standards. We find that while the catalogue effectively structures the evaluation process, it can also be cumbersome and time-consuming to use. We observe the limitations of an AI system that has no active development team anymore and highlighted the importance of complete system documentation. Finally, we identify some limitations of the certification catalogues used and proposed ideas on how to streamline the certification process.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI) has evolved, over several decades, to complex machine learn-\ning (ML) algorithms and neural networks that are common-place today. However,\nin recent years, AI systems were increasingly integrated into our daily lives, moving"}, {"title": "2 Current State of AI Regulation", "content": "Rapid advancement and widespread adoption of AI in various industries require the\ndevelopment of comprehensive regulatory frameworks, and increasing AI deployment\nin more critical areas appears to require stricter regulation [17]. Some bodies like the\nUS Food and Drug Administration have already approved certain AI applications,"}, {"title": "2.1 EU Artificial Intelligence Act", "content": "The European Union's AI Act is currently the most significant and far-reaching regu-\nlatory initiative in the field of AI, and it took effect on 1st of August 2024. The impact\nof the AI Act will extend far beyond the EU's boarders, potentially setting global\nstandards for AI management. The AI Act uses a risk-based approach to achieve the\nfollowing goals [20]:\n\u2022 Ensure AI safety and compliance: guarantee that AI systems in the EU are safe and\nadhere to laws protecting fundamental rights and EU values, safeguarding users'\nprivacy and preventing discrimination.\n\u2022 Certainty for AI investment: establish a clear legal framework to foster innovation\nand investment in AI, providing businesses and investors with regulatory clarity.\n\u2022 Enhance governance: strengthen enforcement mechanisms to effectively apply\nexisting laws on fundamental rights and AI safety requirements.\n\u2022 Unify AI market: foster a single, cohesive market for trustworthy AI applications\nacross the EU, preventing fragmentation through harmonized regulations."}, {"title": "2.1.1 Scope", "content": "The AI Act covers various actors and scenarios within the AI ecosystem [20]. The reg-\nulation applies to providers, deployers, importers, and distributors of AI systems, as\nwell as product manufacturers incorporating AI systems into their products, regardless\nof their location, if the AI system or its output is used within the European Union. The\nAI Act explicitly excludes, among others, AI systems developed purely for scientific\nresearch and development. It also excludes AI systems published under open-source\nlicences. However, the regulation applies if an organisation brings an open-source sys-\ntem to market or uses it as a prohibited AI system, a high-risk AI system or an AI\nsystem with special transparency obligations. The AI Act imposes obligations across\nthe entire AI value chain, ensuring a comprehensive approach to AI governance and\nsafety within the European market."}, {"title": "2.1.2 Definition of AI", "content": "The AI Act adopts a broad and technology-neutral definition of AI systems, focus-\ning on their functional characteristics rather than specific technologies or methods.\nAccording to the AI Act, an AI system is defined as a machine-based system designed\nto operate with varying levels of autonomy, which potentially exhibits adaptiveness"}, {"title": "2.1.3 AI Risk Categories", "content": "The AI Act categorizes AI systems into various risk levels and imposes corresponding\nrequirements, with stricter regulations for higher-risk applications [20]. Key categories\ninclude:\n1. Prohibited AI systems.\n2. High-risk AI systems.\n3. General-purpose AI systems.\n4. AI systems with special transparency obligations.\n5. Limited-risk AI Systems.\nProhibited systems include, but are not limited to, systems that manipulate behaviour,\nexploit vulnerabilities, or create facial recognition databases from untargeted scraping.\nThe AI Act defines high-risk AI systems as those that pose significant risks to health,\nsafety, or fundamental rights, and that are either used as products (or components of\nproducts) covered by specific EU legislation and require third party certification, or\nthat are listed in Annex III of the AI Act [20]. Annex III includes several areas such as\nbiometric identification, emotion recognition systems, management of critical infras-\ntructure, education, employment, and law enforcement. General-purpose AI systems\nface some regulation, but it is less stringent than for high-risk systems. The AI Act\nalso provides an exception for certain AI systems that, despite falling under Annex III\ncategories, may not be considered high-risk. This is the case if they perform narrow\nprocedural tasks or do not significantly influence human decision-making, provided\nthey do not involve profiling of natural people. AI systems with special transparency\nobligations, according to Article 50, require clear disclosure when users interact with\nAI or encounter AI-generated content. Limited-risk AI systems are only subject to\nvoluntary codes of conduct for ethical and responsible use, according to Article 95."}, {"title": "2.2 EU Artificial Intelligence Liability Directive", "content": "Complementing the AI Act, the European Commission proposed the AI Liability\nDirective in September 2022. This directive aims to modernize and enhance the EU's\nliability framework for AI systems [22], and also to ensure that individuals who suf-\nfer damages from AI systems receive equivalent protection to those harmed by other"}, {"title": "2.3 Other Global Initiatives", "content": "While the EU has developed one of the most comprehensive regulatory efforts, other\ncountries and regions have also proposed AI regulations. The following examples repre-\nsent some relevant international regulatory efforts. For instance, the US has discussed\na Blueprint for an AI Bill of Rights, which outlines principles for the design and deploy-\nment of AI systems, although they currently only have a patchwork of state laws [24].\nThe UK has discussed a sector-led approach to AI regulation, and Japan has devel-\noped AI Guidelines emphasizing a multi-layered governance framework [25]. These\ninitiatives do not have legal enforceability. They still highlight the global acknowledg-\nment of the necessity for AI regulation and certification efforts. The current state of\nAI regulation evolves rapidly, with the EU taking a leading role by introducing the AI\nAct."}, {"title": "3 Certifying AI", "content": "Organizations use certification as an important and established tool to prove that\ntechnical systems meet certain standards or regulations. Successful certifications play\na vital role in proving a system's compliance with applicable norms. It also plays\na crucial role in establishing trust in a system among its users [26]. For traditional\nsoftware projects, where every block of code can undergo review line by line, companies\nhave long-established certification processes [15].\nOrganizations and regulatory bodies are still in the early stages of certifying AI\napplications. This is partly because policymakers and other actors have just recently\nbegun developing comprehensive legal frameworks for AI. A prime example is the\naforementioned EU AI Act. Despite ongoing efforts by international standardization\norganizations like ISO, IEC, and IEEE to create guidelines and standards, they have\nyet to fully establish a certification process [14]. These efforts aim to address the dis-\ntinct challenges presented by AI technologies. However, the absence of comprehensive\nlegal frameworks has hindered the development of robust certification processes for\nAI. Even with recent legislative developments, the rapidly evolving nature of AI tech-\nnology continues to pose significant challenges for creating and maintaining effective\ncertification standards [27]. This dynamic landscape requires certification processes\nthat are both adaptable and rigorous, capable of evolving alongside the technology\nthey aim to regulate.\nCertifying AI systems also presents unique challenges due to their complex and\noften opaque nature. Often, humans do not directly program logical rules to model"}, {"title": "3.1 AI Certification Catalogues", "content": "CEN, CENELEC and ETSI are leading European standardization bodies, they bridge\nthe gap between EU regulations and practical certification frameworks designed to\nevaluate and certify AI systems. They integrate these guidelines with European legisla-\ntive priorities, and ensure consistency across the European standardization landscape\n[29]. Several organizations have created catalogues and guidelines to evaluate, test,\nand certify AI systems. Prominent examples include the Fraunhofer AI Assessment\nCatalogue by Poretschkin et al. [16], the white paper 'Trusted Artificial Intelligence'\nby Winter et al. [15], and the white paper 'Auditing Machine Learning Algorithms' by\nthe supreme audit institutions of various countries [30]. These frameworks provide dis-\ntinct methodologies and list criteria to certify AI applications, addressing aspects such\nas fairness, autonomy and control, transparency, reliability, safety and security, and\ndata protection. With this work, we focus primarily on the Fraunhofer Certification\nCatalogue and how it applies to a concrete AI application."}, {"title": "3.1.1 Fraunhofer AI Assessment Catalogue", "content": "The Fraunhofer AI Assessment Catalogue emphasizes the necessity of implementing\nstringent quality standards to ensure AI systems are reliable, safe, aligned with societal\nvalues and compliant with the law, particularly in sensitive application contexts [16].\nThe catalogue identifies several key challenges in assessing and ensuring AI quality.\nThese include the complex value chain involved in AI development and the difficulty\nin explaining the inner workings of AI models. The authors of the catalogue argue that\nthese challenges necessitate a systematic approach to quality implementation in AI\ndevelopment and highlight the importance of unbiased expert assessment in establish-\ning trust in AI applications. A significant focus of the catalogue lies in operationalizing\nquality requirements for AI. While there are established guidelines for reliable AI, the\ncatalogue points out that the specifics of their practical application remain largely\nunclear. The paper proposes a risk-based AI assessment approach and introduces an\nAI assessment catalogue. This catalogue provides a structured approach for certify-\ning AI applications across different dimensions of trustworthy AI: fairness, autonomy\nand control, transparency, reliability, safety and security, and data protection. The"}, {"title": "3.1.2 Trusted Artificial Intelligence", "content": "This white paper published by T\u00dcV Austria and Johannes Kepler University Linz\nwants to outline a structured approach to certifying ML applications [15]. It describes\nkey ML principles and discusses relevant aspects and challenges in the context of\ncertification. It emphasizes that while ML systems are complex, they are not black\nboxes, but rather white boxes whose operations can be analysed in detail. The paper\nintroduces a certification approach for ML applications, focusing initially on super-\nvised learning tasks with low-risk potential. It focuses mainly on technical aspects\nand cybersecurity measures. The paper presents a summary of the certification frame-\nwork. However, since the actual catalogue remains inaccessible to the public, auditors\ncannot use it directly to certify an AI model. Instead, it serves as a reference point,\nhighlighting key areas to consider during the certification process."}, {"title": "3.1.3 Auditing Machine Learning Algorithms", "content": "A collaboration of European public auditing institutions released this white paper. It\nhighlights the increasing use of AI and ML in public services, emphasizing the need for\nnew certification methodologies [30]. It identifies several risks, including an over-focus\non numerical metrics at the expense of compliance and fairness, miscommunication\nbetween product owners and developers, over-reliance on external expertise, and uncer-\ntainty regarding personal data use. To address these challenges, the paper proposes\na certification framework covering the entire AI application lifecycle. The audit areas\nfocus on data understanding, model development, performance, and ethical consider-\nations such as explainability and fairness. To aid in this process, the paper introduces\na helper tool, in the form of a spreadsheet. Auditors can use it to prepare and con-\nduct AI audits efficiently. The authors stress that specialized knowledge and skills are\nrequired for ML certifications. They emphasize that the proposed audit catalogue and\nhelper tool should be continuously refined and updated. Ultimately, this paper aims\nto provide guidance and good practices to enable auditors to navigate different parts\nof the certification process."}, {"title": "4 Our AI Application: Facial Emotion Recognition", "content": "To undertake a certification process, the first requirement is a system that either\nrequires certification or is eligible for it. The system should incorporate an AI com-\nponent, ideally leveraging ML techniques. Furthermore, the AI component must be\nintegrated into a larger, comprehensive system, as certification typically applies to\nentire systems rather than isolated components. Specifically, the Fraunhofer Certifi-\ncation Catalogue mandates a well-defined assessment object, which requires the AI\ncomponent to be part of a larger, integrated system [16]."}, {"title": "4.1 Our Decision to Use the EmoPy Framework and RIOT Project for AI Certification", "content": "After careful consideration, we decided to use the EmoPy Framework [31] and its\nimplementation within the RIOT Project [32] for this sample certification. Several key\nfactors influenced this choice, to ensure that both the framework and the project are\nsuitable for the certification process. Firstly, the EU AI Act's relevance significantly\ninfluenced our decision. Emotion recognition, the primary focus of the EmoPy Frame-\nwork, aligns with the potential coverage of the EU AI Act. While open-source models\nlike EmoPy are generally exempt, this system may fall under the AI Act's scope if\nit was brought to market as a high-risk AI system, a prohibited AI system or an AI\nsystem with special transparency obligations.\nAnother critical factor was the open-source nature and transparency of EmoPy.\nEmoPy is fully open-source, enabling an in-depth look into the technical details and\nmaking the system fully transparent, which should make the certification process pos-\nsible. The codebase of EmoPy is relatively small and manageable, making it easier to\nunderstand and verify. Yet, it is sufficiently large to make a certification worthwhile.\nAdditionally, the framework includes thorough documentation, with multiple articles\nand resources that describe the model selection process. This level of documentation\nis critical for reproducibility attempts of the AI models [33, 34], and with this also for\nthe certification process, as it provides clear insights into the design and functionality\nof the model, facilitating a thorough evaluation. While solid documentation is essential\nfor any certification, it is especially critical for our sample certification, since no com-\npany or active development team is managing the project anymore. Although no active\ndevelopment team provides ongoing support, contacting authors and lead developers\nof the EmoPy framework articles and code, proved beneficial. They kindly addressed\nquestions about the framework and the RIOT setup. Beyond this input, we needed to\nextract all necessary information from the provided documentation. This reliance on\nstatic resources poses a limitation compared to standard certification processes, where\nongoing interactions with active developers should be possible.\nFrom a technical perspective, we considered the suitability of the EmoPy frame-\nwork for sample certification a key factor. Its technical characteristics and comprehen-\nsive documentation make it well-suited for the certification process. Additionally, the\nintegration of EmoPy within the RIOT project provides a complete system context,\nwhich is essential for certification. Traditional certification catalogues often struggle\nto validate standalone ML models. However, the RIOT project offers a comprehen-\nsive framework where the AI component is embedded within a broader system. This\nintegration is critical, as certification typically applies to an entire system rather than"}, {"title": "4.2 Brief Overview of the EmoPy Framework", "content": "The EmoPy framework provides multiple neural network architectures for facial\nexpression recognition, including ConvolutionalNN, TransferLearningNN, and Convo-\nlutionalLstmNN. These architectures vary in complexity, with the ConvolutionalNN\nbeingthe simpleste, and TransferLearningNN (using Google's Inception-v3) being the\nmost complex. The authors experimented with different architectures and found that\nthe ConvolutionalNN provides the best overall performance [35]. The EmoPy docu-\nmentation suggests using two publicly available datasets for training and evaluation:\nthe Microsoft FER2013 dataset and the Extended Cohn-Kanade dataset. The key\ngoals of the EmoPy project are to provide free, open-source, and easy-to-use facial\nexpression recognition capabilities, and to advance research in this field by making the\nmodels and datasets publicly available. The Framework was also used for the emotion\nrecognition in the RIOT Project."}, {"title": "4.3 Brief Overview of the RIOT Project", "content": "In a nutshell, RIOT is a live-action film that dynamically responds to emotions,\nutilizing facial emotion recognition technology to guide viewers through an ongoing"}, {"title": "4.4 The AI Application to be Certified", "content": "For this sample certification, the RIOT installation provides the complete system con-\ntext for the certification process. The EmoPy framework, which provides ML-Models\nto detect emotions, serves as the core framework within this installation. Our certi-\nfication process focuses on validating the AI system within the RIOT context. The\nsurrounding code, setup, and information that comprise the entire art installation are\nrelevant, as we cannot certify the AI system independently of these components. How-\never, this approach has shortcomings, as not all the required information is available.\nOur approach allows for an exploration of the certification process while acknowl-\nedging its limitations and academic nature. To facilitate the certification process, we\nprepared a complete overview of all available information about the AI application,\nwhich is summarized below. We also compiled a complete technical overview of the\nsystem, that we certified."}, {"title": "4.4.1 Compilation of System Resources", "content": "RIOT Installation\n\u2022 GitHub repository of the RIOT art installation [37]\n\u2022 Article on the RIOT art installation [32]\n\u2022 TED Talk on the RIOT art installation [38]\n\u2022 Article that describes different art installations (one of them is the RIOT art\ninstallation) [39]\n\u2022 Article on Karan Palmer (the artist behind the RIOT Art Installation) [40]\n\u2022 Short description of the RIOT art installation by Karan Palmer [36]\n\u2022 Video that showcases and describes the RIOT art installation [41]\nEmoPy Framework\n\u2022 GitHub repository of the EmoPy Framework [31]\n\u2022 Article describing the EmoPy Framework and technical decisions that were made\nin more detail [35]"}, {"title": "4.4.2 Technical Overview", "content": "The AI Application that we attempted to certify has the following specifications:\nThe developers trained the ConvolutionalNN from the EmoPy Framework, as this\narchitecture had the best performance during early testing. The layers and parameters\nof the ConvolutionalNN can bee seen in Table 1."}, {"title": "5 AI Certification Approach", "content": "We apply the Fraunhofer AI Certification Catalogue to an existing AI system to\nexplore and evaluate the certification process. We chose the facial emotion recog-\nnition component of the RIOT art installation, which uses the EmoPy framework.\nWe selected this system because it is open-source, appears well-documented, and is\nintegrated into a larger application context. Additionally, as we have discussed previ-\nously, it is potentially covered by the EU AI Act. In this work-in-progress, we use the\nFraunhofer Catalogue as the primary certification framework due to its comprehen-\nsive nature and full public availability. The catalogue provides a structured approach\nto AI certification, addressing multiple dimensions of risk. The Fraunhofer Catalogue\noutlines the certification process, which involves these key steps [16]:\n1. First Step: get an overview of the System (AI Profile (PF)) and define the AI-System and the boundaries to the surrounding system.\n2. Second Step: define the life cycle of the AI application.\n3. Main Step: get an overview over all the risk dimensions.\n(a) Protection requirements analysis: determine which risk dimensions apply.\n(b) Risk Analysis: for each applicable dimension:\n(i) Risk analysis and objectives.\n(ii) Criteria for achieving objectives.\n(iii) Measures.\n(iv) Overall assessment of a risk area.\n(v) Summary of each dimension.\n(vi) Cross-dimension assessment.\n4. Drawing conclusions and making a certification decision based on the success of the cross-dimensional assessment.\nAfter completing the certification process with the Fraunhofer Catalogue, we anal-\nyse and address several key aspects of the process, and we draw conclusions about the\nchallenges we encounter. In this paper, we explore the challenges of selecting an appro-\npriate AI application for certification. We also evaluate how effectively the Fraunhofer\ncertification process worked for this specific case, and highlight its strengths and areas\nfor improvement. Furthermore, we present a comparative analysis, examining how the\nother two introduced catalogues differ from the Fraunhofer approach and how they\ncould potentially enhance or complement the certification process. Lastly, we evaluate\nthe limitations of this approach. Our analysis addresses the constraints of the cho-\nsen AI system and certification catalogue, as well as the applicability of the findings\nto other AI applications and certification scenarios. With this comprehensive evalu-\nation, we aim to provide valuable insights into the practical implementation of AI\ncertification processes and contribute to the ongoing discourse on AI certification."}, {"title": "5.1 Before the Certification Process", "content": "Before starting the certification process, we completed several preparatory steps.\nFirst, we selected the AI application. We forked the GitHub repository of the EmoPy\nproject and identified the correct dependencies to enable a detailed examination of"}, {"title": "5.2 AI Profile", "content": "The Fraunhofer Catalogue outlines the first formal step in the certification process:\ncompleting the AI Profile. This step was straightforward due to the thorough research\nwe conducted in the preparation phase. The AI Profile offered a structured overview\nof the system's functionality, intended application context, and key characteristics."}, {"title": "5.3 Life Cycle of the AI Application", "content": "Following the AI Profile, we conducted the life cycle overview. Although not explicitly\nstated as a distinct step in the Fraunhofer Catalogue, we found it beneficial to gain\na thorough understanding of the AI system's development and operation stages. The\nAI life cycle encompasses all the stages an AI system undergoes, from planning and\ndevelopment to deployment, operation, ongoing maintenance, and potentially contin-\nued model training, ensuring trustworthiness and compliance throughout its use. We\nadapted the questions for this life cycle overview from a table in the Fraunhofer Cata-\nlogue, covering aspects such as data acquisition, model development, and operational\nconsiderations."}, {"title": "5.4 Protection Requirement Analysis", "content": "The protection requirement analysis serves as an important first step in the certifi-\ncation process, identifying the risk dimensions that require more in-depth analysis.\nThis analysis involves evaluating the potential impact of the AI system across var-\nious dimensions such as fairness, reliability, and data protection. We examined all\ndimensions and identified several with medium risk. A summary of the Protection\nRequirement Analysis can be seen in Table 3. For the purposes of this work, we selected\ntwo of the required risk dimensions, namely reliability and fairness, for detailed analy-\nsis. This selection helps us to focus the sample certification and manage the scope of the\ncertification process. Exploring these two dimensions is also sufficient to understand\nthe certification procedure and draw the appropriate conclusions."}, {"title": "5.5 Risk Analysis", "content": "The risk analysis forms the core of the certification process. We carried out this step\nby working through a questionnaire from the Fraunhofer Catalogue. The questions\naddressed different aspects of the selected risk dimensions. For each dimension, we\ncovered topics such as data quality, model design, testing procedures, and operational\nconsiderations. A summary of the conclusion of our Risk Analysis for our investi-\ngated dimension can be seen in Table 4. Following the individual dimension analyses,\nwe conducted a cross-dimensional assessment to identify potential trade-offs or inter-\nactions between the examined dimensions. This step is crucial to ensure a complete\nunderstanding of the AI system's performance and risks. In our sample certification,\nwe were unable to conclude the certification attempt with a positive result, but found\nsignificant shortcomings of the AI Application."}, {"title": "6 Results and Main Findings", "content": "We performed the certification of the chosen facial emotion recognition system. The\ncore of the certification process with the Fraunhofer Catalogue involves the Protection"}, {"title": "6.1 Selecting the AI System", "content": "In a conventional certification scenario, the process of selecting an AI system for cer-\ntification is usually not a consideration, as the system to be certified is predetermined\nby the organization seeking certification. However, for the purposes of this work, the\nselection of an appropriate AI system represented a crucial first step that significantly\ninfluenced the subsequent certification process and what can potentially be learned\nfrom it. The selected system provided a mostly robust foundation for the certification\neffort. Its existing application context, and good documentation of both the AI model\nand its surrounding system, enabled meaningful progress through the certification\nprocess. During the certification process, we saw the importance of considering both\ntechnical factors and solid documentation when choosing an AI system for certification."}, {"title": "6.1.1 Initial Considerations and Challenges", "content": "The selection of an appropriate AI system requires careful consideration of multiple\ninterconnected factors. While an initial approach might suggest identifying and select-\ning a standalone AI model or neural network, this proves insufficient when considering\nthe comprehensive requirements of a certification processes. Particularly, the Fraun-\nhofer Catalogue, which we primarily used in this paper, takes an extensive look at\nhow to define the system and its boundaries with other software components. This\ndefinition cannot be found with a standalone AI model."}, {"title": "6.1.2 Context and Embedding Requirements", "content": "The certification process inherently demands a broader contextual framework than\nwhat might be immediately apparent. Rather than existing in isolation, the AI system\nmust be embedded within a larger operational context and demonstrate clear use case\napplications. While it would be theoretically possible to construct artificial use cases\nfor the certification purpose, such an approach could result in a suboptimal certifica-\ntion scenario. In this paper, therefore, we have chosen an AI application, which has\nalready established a real-world application context. This characteristic proved invalu-\nable, as it enabled a more natural translation into a certifiable system, providing the"}, {"title": "6.1.3 Documentation Requirements", "content": "Documentation emerged as another critical factor in the selection process, on two\ndistinct levels. First, the AI model itself must be thoroughly documented, providing\ntechnical specifications and operational parameters. Second, and equally important,\nthe surrounding system infrastructure must be comprehensively documented to make\ncertification feasible. This documentation requirement significantly narrows the field\nof suitable candidates for certification studies. A particular challenge encountered in\nthis work relates to the absence of a development team. When selecting an existing\nmodel for certification, there is typically no active development team invested in the\ncertification process. This situation creates a significant constraint. Without the ability\nto request additional documentation or engage in an iterative process with developers,\nthe available documentation must be sufficiently comprehensive from the outset. Any\ninformation gaps that arise during the certification process cannot be supplemented\nor clarified."}, {"title": "6.2 The Certification Process", "content": "We used the Fraunhofer catalogue as the primary basis for this certification due to its\ncomprehensive and detailed nature. Although we considered other catalogues, such as\nthe T\u00dcV catalogue, they presented significant limitations. The incomplete publica-\ntion of the T\u00dcV catalogue made it unsuitable for use in the certification process. The\nAuditing Machine Learning Algorithms catalogue, while fully published and poten-\ntially suitable for certification, employs a substantially different approach compared to\nthe Fraunhofer catalogue. Its less step-by-step nature potentially presents additional\nchallenges for those with limited certification experience."}, {"title": "6.2.1 Main Challenges During the AI Certification Process", "content": "The system's documentation\nWe encountered several key challenges during the certification process. One funda-\nmental challenge is that the system's documentation was not originally intended for\ncertification purposes. Additionally, the development process did not require extensive\nand detailed documentation. This limitation created occasional gaps in documenta-\ntion that would be essential for a complete certification. In some instances, we made\nadequate substitutions beforehand to create a more realistic certification scenario.\nThe documentation of a system is key to certification. Our choice of a publicly avail-\nable system that was not intended for certification has its shortcomings. This choice\nmakes a sample certification, such as the one we attempted here, more difficult and\npotentially less meaningful."}, {"title": "No active development team", "content": "The absence of an active development team emerged as a critical limitation in the\ncertification process. Without ongoing development support, we could not implement\nthe typical feedback loop, where certification findings would normally lead to docu-\nmentation improvements and system adjustments. In a standard certification scenario,\nidentified gaps or shortcomings trigger an iterative process of enhancement, with the\ndevelopment team actively working to make the system more certifiable. However, in\nthis paper, we had to evaluate the system purely based on its existing documentation\nand state. Therefore, we could only have two possible outcomes: either certifiable or\nnot certifiable with the available materials.\nThis limitation became more complicated by the fact that the system was orig-\ninally developed several years ago, and the entire development team had moved on\nfrom the project. The lead developer generously provided time to answer questions.\nHowever, because the project is old, certain details became less accessible or clear over\ntime. This combination of inactive development and the system being old created a\nstatic evaluation scenario, rather than the dynamic, iterative process that typically\ncharacterizes successful certification efforts where development teams actively work\ntowards certification compliance."}, {"title": "6.2.2 Specific Observations on the Fraunhofer Catalog", "content": "The implementation of the Fraunhofer catalogue revealed several notable characteris-\ntics and challenges. The catalogue's documentation-centric approach makes it nearly\nimpossible to use for code-only projects, as it focuses exclusively on documentation\nrather than direct code examination. While code can inform the certification process\nand documentation creation, the catalogue never directly addresses or describes code.\nThe catalogue's high specificity and detail provide comprehensive coverage, reducing\nthe likelihood of overlooking critical aspects. However, this thoroughness occasion-\nally results in similar or nearly duplicate questions, increasing the time required for\ncertification completion. The strong documentation focus means less direct attention\nto mathematical or technical system operations. While the neural network struc-\nture remains important for documentation purposes, the Fraunhofer Catalogue only\nrequires it to be examined implicitly rather than explicitly. This approach can be\nadvantageous when dealing with proprietary information, as documentation alone\nmight suffice for a potential certification. A particular strength of the Fraunhofer cat-\nalogue lies in its clear differentiation between the AI model, system, and embedding\ncode, which proves crucial in determining certification scope and requirements. This\ndistinction helps ensure appropriate certification coverage."}, {"title": "T\u00dcV Catalogue", "content": "One notable limitation of the Fraunhofer catalogue is the lack of guidance on how to\nanswer the posed questions. In this regard, a more technology-centric catalogue like the\none from T\u00dcV could provide valuable complementary guidance. The T\u00dcV catalogue,\ndespite its publication limitations and restricted focus on ML and supervised learning\nsystems, offers useful insights into the technical aspects of ML systems operations."}, {"title": "Auditing Machine Learning Algorithms Catalogue", "content": "The Auditing Machine Learning Algorithms Catalogue presents a markedly different\nstructural approach compared to Fraunhofer's. Its topic-based organization consoli-\ndates related questions for instance, grouping all data-related questions together\ncontrasting with Fraunhofer's distributed approach where data-related questions\nappear across various subsections. This structural difference complicates the poten-\ntial combination of these catalogues. However, the auditing catalogue's reduced\nduplication could potentially streamline the certification process."}, {"title": "6.3 Learnings and Recommendations", "content": "The certification process revealed several significant insights regarding both method-\nological approaches and practical certification challenges."}, {"title": "6.3.1 Catalogue-specific Observations", "content": "The Fraunhofer catalogue, while demonstrating robust effectiveness, revealed both\nstrengths and limitations in practical application. Its exhaustive and detailed nature\nensures comprehensive coverage, but is time intensive. This thoroughness, while ben-\neficial for certification rigour, needs to be balanced against practical time constraints\nin real-world scenarios. The evaluation of alternative catalogues provided additional\ninsights. The T\u00dcV catalogue's incomplete publication status rendered it unsuitable\nfor standalone certification efforts. The Auditing Machine Learning Algorithms cata-\nlogue showed promise for certification purposes, potentially offering a more streamlined\napproach compared to the Fraunhofer methodology. However, its less structured nature\nsuggests a need for deeper AI system expertise. But it might potentially be a faster\ncertification process while maintaining quality standards."}, {"title": "6.3.2 Limitations", "content": "Several key limitations emerged during the certification process. The absence of an\nactive development team is limiting, as it prevented the implementation of the typical\nfeedback loop essential for certification refinements. This limitation transformed the\ncertification process into evaluation rather than an iterative improvement process,\nhighlighting the importance of ongoing development support for successful certification\nefforts. Documentation gaps cannot be addressed through subsequent submissions.\nThis emphasized the importance of comprehensive initial documentation of the chosen\nsystem.\nAny actors performing a future sample certification of systems that are not actively\ndeveloped should keep this in mind, and they should choose a system with the most\ncomprehensive documentation."}, {"title": "6.3.3 Recommendations for future Sample Certifications", "content": "The experience gained from this study suggests several crucial considerations for future\ncertification efforts. For AI application selection, auditors should identify AI systems\nthat exist within a broader application setting with surrounding code. The AI system"}, {"title": "6.3.4 Recommendations for Real-World Applications", "content": "Our findings yield several practical recommendations for real-world certification imple-\nmentations. The Fraunhofer catalogue, while highly detailed and extensive, requires\nsignificant time investment for thorough completion. However, its precision and\ncomprehensiveness make it a valuable tool for certification processes. Particularly note-\nworthy are the initial sections of the catalogue, specifically the AI lifecycle overview,\nwhich prove especially effective in providing auditors"}]}