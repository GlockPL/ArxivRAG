{"title": "QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV Cache", "authors": ["Rishabh Tiwari", "Haocheng Xi", "Aditya Tomar", "Coleman Hooper", "Sehoon Kim", "Maxwell Horton", "Mahyar Najibi", "Michael W. Mahoney", "Kurt Keutzer", "Amir Gholami"], "abstract": "Large Language Models (LLMs) are increasingly being deployed on edge devices for long-context settings, creating a growing need for fast and efficient long-context inference. In these scenarios, the Key-Value (KV) cache is the primary bottleneck in terms of both GPU memory and latency, as the full KV cache must be loaded for each decoding step. While speculative decoding is a widely accepted technique to accelerate autoregressive decoding, existing methods often struggle to achieve significant speedups due to inefficient KV cache optimization strategies and result in low acceptance rates. To address these challenges, we propose a novel self-speculative decoding framework, QuantSpec, where the draft model shares the architecture of the target model but employs a hierarchical 4-bit quantized KV cache and 4-bit quantized weights for acceleration. QuantSpec maintains high acceptance rates (>90%) and reliably provides consistent end-to-end speedups upto ~ 2.5\u00d7, outperforming other self-speculative decoding methods that use sparse KV cache for long-context LLM inference. QuantSpec also reduces the memory requirements by ~ 1.3\u00d7 compared to these alternatives.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have been widely used in recent years, revolutionizing natural language processing (NLP) and artificial intelligence (AI) applications. As their applications expand, there is a growing demand to deploy LLMs in long-context settings - handling extended text inputs such as document summarization, lengthy conversations, or comprehensive instructions. The model must maintain coherence in such contexts and track intricate details across extended sequences. However, long-context inference presents significant challenges in terms of efficiency and scalability. For example, token eviction (Zhang et al., 2024c; Ge et al., 2023; Liu et al., 2024b) and KV cache quantization (Liu et al., 2024c; Hooper et al., 2024; Kang et al., 2024; Hooper et al., 2024) have been proposed to improve the efficiency for long-context inference. However, they often entail noticeable degradation in generation quality.\nOne promising alternative to enhance the efficiency of LLMs while preserving generation quality is speculative decoding (Leviathan et al., 2023; Chen et al., 2023; Kim et al., 2024). This method accelerates inference by using a smaller (draft) model to rapidly generate candidate tokens, and uses the original (target) model to verify these tokens to ensure generation quality. However, the efficient application of speculative decoding in long-context settings has not been thoroughly explored.\nTraditional speculative decoding approaches often rely on using smaller models as the draft model in order to minimize the memory-bandwidth overhead of loading the model weights of the larger target model. In long-context scenarios, however, the primary bottleneck shifts from model weights to the KV cache, which grows linearly with the context length. Additionally, since small models do not usually possess good long-context understanding ability, the acceptance rates of the candidate tokens by the target model drop significantly, leading to suboptimal speedup. Moreover, traditional speculative decoding methods maintain the KV cache for both the target model and the draft model, causing a large memory footprint. Therefore, finding a solution that both optimizes the KV cache's memory efficiency and improves the acceptance rate within speculative decoding is essential for performant LLMs in long-context applications.\nTo mitigate these issues and to enable efficient and accurate long-context inference, we propose QuantSpec, a self-speculative decoding method that utilizes 4-bit weights and a 4-bit hierarchical KV cache to speedup long-context inference. In particular we make the following contributions:\n\u2022 We perform a comprehensive analysis of LLM inference to identify bottlenecks across various context lengths, demonstrating that quantizing the KV cache improves efficiency for long contexts, while quantizing model weights is more beneficial for short contexts (see Section 3.1).\n\u2022 We introduce a novel hierarchical quantization technique that enables bit-sharing between the target and draft models' KV caches, eliminating the need for additional memory for the draft model (see Section 4.2).\n\u2022 We propose a double full-precision cache buffer used for storing the most recent KV cache in full precision to improve acceptance rates and also eliminate wasteful quantization and dequantization operations (see Section 4.3).\n\u2022 We show that using a quantized KV cache leads to better acceptance rates between the target and the draft model, and thus leads to better overall speedups (see Section 5.2).\n\u2022 We implement custom CUDA kernels for attention with our hierarchical quantized KV cache achieving up to ~ 2.88\u00d7 speedups at 4-bit precision relative to FP16 FlashAttention kernels. (see Section 5.2.1)"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Efficient Long-Context Inference", "content": "An important challenge in optimizing long-context inference lies in reducing memory and computation requirements while retaining high performance on tasks that involve long sequences. Sparse attention mechanisms (Liu et al., 2021; Xiao et al., 2023b; Yao et al., 2024; Tang et al., 2024; Yang et al., 2024; Liu et al., 2024b; Ge et al., 2023; Jiang et al., 2024) have been widely adopted to manage the quadratic complexity of traditional full attention in long contexts. These techniques typically maintain efficiency by dropping non-essential Key-Value (KV) pairs from the cache. Token pruning (Fu et al., 2024) selectively computes the KV for tokens relevant for next token prediction. KV Prediction (Horton et al., 2024) improves prompt processing time by predicting the KV cache needed for autoregressive generation. Retrieval-augmented generation (Tan et al., 2024; Liu et al., 2024a) enhances the accuracy of language model outputs by combining generative models with external retrieval mechanisms whose context length is very long."}, {"title": "2.2. Quantization", "content": "Quantization has emerged as a powerful technique to reduce the memory footprint and computational complexity in large-scale neural networks. Weight-only quantization (Lin et al., 2024; Kim et al., 2023a; Shao et al., 2023; Chee et al., 2024) focuses on reducing the precision of model weights to reduce the memory requirements of the model. As models grow larger, the memory footprint of KV caches can become substantial, especially for long input sequences. KV cache quantization (Liu et al., 2024c; Hooper et al., 2024; Kang et al., 2024) addresses this issue by quantizing the key and value caches to enable longer sequence inference."}, {"title": "2.3. Speculative Decoding", "content": "Speculative decoding has become an important technique for improving the inference efficiency of LLMs (Leviathan et al., 2023; Chen et al., 2023; Kim et al., 2024). It uses a smaller draft model to rapidly generate candidate tokens, which are then verified by a larger target model to ensure correctness. Parallelization in speculative decoding has also been studied to enhance the efficiency by predicting multiple tokens at one time (Cai et al., 2024; Bhendawade et al., 2024; Li et al., 2024b; Chen et al., 2024b). We include additional related works in Appendix B.\nSelf-speculative decoding is the class of speculative decoding methods in which the draft model shares the same architecture as that of target model for better alignment. Recent works like Magicdec (Anonymous, 2025) and TriForce (Sun et al., 2024) have shown that self-speculation with sparse KV can effectively speedup the draft model in long-context settings, where KV is the main bottleneck. While this design avoids loading the entire KV cache throughout the autoregressive generation process, KV cache sparsification can lead to noticeable performance degradation as evidenced in previous works (Zhang et al., 2024c; Liu et al., 2024b; Ge et al., 2023; Zhou et al., 2024). This can potentially yield a mismatch between the draft and target model's predictions (i.e., lower acceptance rate), which is a critical factor in overall speedup. QuantSpec addresses this limitation by proposing a draft model with a novel hierarchical quantized KV cache, which maintains a higher acceptance rate between the draft and target models, therefore leading to better speedup. Note that our method can be combined with sparse KV methods (Sun et al., 2024; Anonymous, 2025) for additional speedup, which we leave for future work."}, {"title": "3. LLM Inference Bottlenecks", "content": ""}, {"title": "3.1. Arithmetic Intensity", "content": "To understand the primary bottlenecks in LLM inference and to motivate our method, we perform a thorough analysis of inference under several different regimes. These regimes include a combination of small versus large batch sizes and short versus long context lengths during both the prefill and decoding stages. We use arithmetic intensity as the central metric in our analysis, where arithmetic intensity is defined as the number of floating point operations (FLOPs) that can be performed per byte loaded from memory, or memory operations (MOPs) (Williams et al., 2009):\nArithmetic Intensity = $\\frac{\\text{#FLOPS}}{\\text{# MOPS}}$\nArithmetic intensity allows us to classify which regimes of LLM inference are compute-bound or memory-bound and determine appropriate optimizations to improve latency. Compute-bound operations are limited by the hardware's peak FLOP/s (FLOPs per second) performance and benefit from algorithmic improvements that reduce computational complexity (e.g., subquadratic attention). On the other hand, memory-bound operations are limited by the hardware's memory bandwidth (GB/s) and benefit from techniques that optimize memory load-store operations, such as quantizing the weights of a model even if they are later scaled up to a higher precision during computation to preserve accuracy.\nFor a finer-grained analysis, we break down the major operations in the Transformer into two categories: linear, which consists of the weight-to-activation matrix multiplications (i.\u0435., WQ, WK, Wv, Wout, mlp_up_proj, mlp_down_proj, and the linear classification layer), and attention, which consists of the activation-to-activation matrix multiplications (i.e., query \u00d7 key and attention weights \u00d7 values). Note that the aggregate of all Transformer operations includes the above operations as well as non-linear operations like activation functions in the feed-forward network, softmax in the attention mechanism, and layer normalization. Because we are interested in studying the linear and attention operations, we do not explicitly focus on the non-linear operations and classification layer in our asymptotic analysis, although we include them in our final results."}, {"title": "3.1.1. ASYMPTOTIC ANALYSIS OF ARITHMETIC INTENSITY FOR PREFILL AND DECODING", "content": "During prefill, the model weights are only loaded once to process all tokens in the input and generate the first token. Because the context length can range from a couple thousand to hundreds of thousands of tokens, this phase consists of large matrix-matrix multiplications (matmuls) with high arithmetic intensities. Table 1 shows asymptotic analysis of arithmetic intensity for prefill and decoding broken up into linear, attention, and aggregate operations for batch size B, sequence length $S_L$, hidden dimension d, and a generation length of k tokens. During prefill, the aggregate arithmetic intensity is similar to the arithmetic intensity of the linear projections when $S_L < d$ because self-attention is relatively inexpensive for short contexts. Thus the linear projections dominate latency in this regime. However, as the context length increases and $S_L \\gg d$, the aggregate arithmetic intensity reflects the arithmetic intensity of attention, which begins to dominate latency since self-attention incurs additional cost with longer context lengths. Note that our analysis assumes the use of FlashAttention (Dao et al., 2022), such that the attention scores matrix which grows on the order of $O(B \\cdot S_L^2)$ is never fully materialized, and thus the memory operations for this matrix are limited to $O(BS_L)$.\nOn the other hand, in the decoding stage, generating k tokens requires loading and storing the weights and KV cache k times. Since the input at each iteration is a single token per sequence in the batch ($x \\in \\mathbb{R}^{B \\times 1 \\times d}$), these operations mainly consist of small matmuls with low arithmetic intensity. For short context lengths where $S_L \\ll d$, the aggregate arithmetic intensity for decoding again reflects the arithmetic intensity of the linear projections as loading and storing a small KV cache is relatively inexpensive compared to loading and storing the model weights. However, as the context length grows ($S_L \\gg d$), the load-store operations for the large KV cache exacerbate and dominate latency, and the aggregate arithmetic intensity reflects the arithmetic intensity of attention. Ultimately, the aggregate arithmetic intensity for decoding is much lower than that of prefill:\n$\\begin{array}{c}\\begin{array}{ccc}O(BS_L), & & S_L \\ll d \\\\O(S_L), & & S_L \\gg d\\end{array} & > & \\begin{array}{ccc}O(B), & & S_L \\ll d \\\\O(1), & & S_L \\gg d\\end{array}\\\\text { prefill } & & \\text { decode }\\end{array}$\nWhile the aggregate arithmetic intensity for prefill scales proportionally to the context length which can be in the hundreds of thousands, the aggregate arithmetic intensity for decoding does not scale with the context length at all. Moreover, using larger batch sizes only seems to increase the arithmetic intensity for decoding in the short-context setting. For long contexts, decoding has an extremely low arithmetic intensity irrespective of the batch size since every sequence in the batch undergoes self-attention separately and therefore cannot benefit from batching in the same way linear layers do."}, {"title": "3.1.2. COMPUTE VERSUS MEMORY-BOUND REGIMES", "content": "The asymptotic analysis suggests that in general, decoding suffers from low arithmetic intensities compared to prefill in all regimes. However, to decide which optimizations will most effectively improve latency, all regimes must be classified as either compute-bound or memory-bound. Whether an operation is compute or memory-bound depends on the hardware it is being run on as well as the magnitude of the arithmetic intensity achieved by the operation.\nWe utilize an analytical roofline model (Williams et al., 2009; Kim et al., 2023a;b) to help determine which regimes are compute or memory-bound in a practical inference setting. The roofline model defines a ridge point which is calculated as\n$\\frac{\\text { peak compute performance (FLOP/s) }}{\\text { peak memory-BW (GB/s) }}$\nNote that the ridge point has the same units as arithmetic intensity (FLOPs/byte). In the roofline model, any operation with an arithmetic intensity smaller than the ridge point is memory-bound, and any operation with an arithmetic intensity greater than the ridge point is compute-bound. For our analysis, we extrapolate this to a ridge plane and use hardware specifications for an NVIDIA A6000 GPU to study inference for the Llama-2-7B model in 16 bit precision.\nFor optimizing speculative decoding, we specifically focus on the decoding phase, although we include results for prefill in Appendix C.1. Figure 2 shows the arithmetic intensity for generating 1k tokens at different context lengths and batch sizes for the Linear/Attention components as well as the aggregate arithmetic intensity. To decide the ideal quantization strategy for different regimes, we consider the aggregate arithmetic intensity, which is colored by the percentage of the total latency taken up by attention and provides a complete view of decoding in all regimes. Based on these results, we can clearly see that in the small batch + short context regime, the memory operations for the linear projections dominate latency, so weight quantization could provide considerable speedup in this regime. In the small batch + long context, large batch + short context, and large batch + long context regimes, attention dominates latency due to the expensive load-store operations for the large KV cache. KV cache quantization could help provide performance improvements in these regimes. In the small batch + medium context and short context + medium batch regimes, the linear and attention operations are approximately equivalent in their contributions to total latency. Thus, both weight and KV cache quantization are ideal here."}, {"title": "4. QuantSpec", "content": ""}, {"title": "4.1. Overview of QuantSpec", "content": "In this section, we introduce QuantSpec, a self-speculative decoding framework designed to accelerate both short- and long-context generation by quantizing the model weights and KV cache into INT4 precision. We begin by noting that self-speculative decoding is particularly well-suited for long-context generation, as the draft model shares the same architecture as the target model. This architectural alignment improves both the acceptance rate and the model's ability to handle long contexts effectively. However, a naive implementation of self-speculative decoding (e.g. based on sparse KV) would require maintaining a separate, fully quantized copy of the KV cache, leading to inefficiencies in memory usage and computational overhead.\nTo address this limitation, QuantSpec introduces a novel hierarchical KV cache design, which we discuss in detail in Section 4.2. This design enables dynamic switching between INT4 and INT8 representations of the KV cache without the overhead of on-the-fly quantization. By eliminating redundancy between the draft and target models' KV caches, our method significantly reduces the total memory footprint while preserving efficiency. We also address the inefficient combination of conventional quantization strategies with the reject-and-revert-back mechanism specific to speculative decoding methods by proposing a full-precision KV cache buffer in QuantSpec. As we further explain in Section 4.3, this helps achieve high acceptance rates for the draft model and thus results in greater end-to-end speedup."}, {"title": "4.2. Hierarchical KV Cache", "content": "We propose a 4-bit hierarchical KV cache wherein we strategically structure each tensor's representation such that the draft and target models are able to dynamically reconstruct their KV cache without any on-the-fly quantization overhead. Firstly, we observe that using an INT8 KV cache for the target model is comparable in terms of accuracy and performance with the same target model using an FP16 KV cache. To demonstrate this, we conduct a perplexity analysis for Llama-2-7B on the WikiText-2 (Merity et al., 2016) and C4 (Raffel et al., 2020) datasets in Table 2, which shows that the target model with an INT8 KV cache maintains competitive generation quality with respect to the FP16 baseline while using half the KV cache's memory.\nHaving observed this, we further note that an INT8 KV cache can be represented as an INT4 KV cache plus its INT4 residual. This works by decomposing an INT8 value into two INT4 components corresponding to its first and second 4-bit segments, which we call the upper and lower 4-bits. This effectively allows us to use a hierarchical design to represent the KV cache of the draft model in INT4 and the target model in INT8 at the same time, removing the need to store a separate INT4 copy.\nOur method is visualized in Figure 3. During prefill, QuantSpec quantizes the FP16 KV cache to form the upper and lower INT4 representations. To obtain the upper 4-bits $C_{\\text{INT}4}^U$ and lower 4-bits $C_{\\text{INT}4}^L$ values, we first calculate $C_{\\text{INT}4}^U$, then quantize the quantization error $E_{\\text{INT}4}$ to get $C_{\\text{INT}4}^L$. $C_{\\text{INT}4}^U \\in [0,15]$ uses asymmetric and round-to-nearest quantization. Since the distribution of $E_{\\text{INT}4}$ is symmetric and has an expectation close to zero, for $C_{\\text{INT}4}^L \\in [-8,7]$ we use symmetric and round-to-nearest quantization to better match the distribution of errors.\nThen during decoding, when using the draft model to generate candidate tokens, we only load the upper 4-bit representation in our kernel and dequantize it for inference. When verifying the drafted tokens using the target model, we utilize both the upper and lower 4-bit representations to reconstruct the KV cache in the higher INT8 precision. To represent the INT8 KV cache $C_{\\text{INT}8}$ as the upper INT4 KV cache $C_{\\text{INT}4}^U$ and the lower INT4 cache $C_{\\text{INT}4}^L$, the INT8 KV cache can be expressed as $C_{\\text{INT}8} = 2^4 C_{\\text{INT}4}^U + C_{\\text{INT}4}^L$, where we multiply by $2^4$ to align their represented values. The asymmetric quantization for the KV cache can be represented as $C_{\\text{FP32}} = C_{\\text{INT}8} S_{\\text{INT}8} + Z_{\\text{INT}8}$, where $S_{\\text{INT}8}$ is the scaling factor, $Z_{\\text{INT}8}$ is the zero point, and $C_{\\text{INT}8} \\in [0, 2^8 - 1]$. In this scenario, its 4-bit representation can be viewed as\n$C_{\\text{FP32}} = (2^4 C_{\\text{INT}4}^U + C_{\\text{INT}4}^L) S_{\\text{INT}8} + Z_{\\text{INT}8} = 2^4 C_{\\text{INT}4}^U S_{\\text{INT}8} + C_{\\text{INT}4}^L S_{\\text{INT}8} + Z_{\\text{INT}8} = C_{\\text{INT}4}^U S_{\\text{INT}4} + C_{\\text{INT}4}^L + Z_{\\text{INT}4}$,\nwhere $Z_{\\text{INT}4} = Z_{\\text{INT}8}, S_{\\text{INT}4} = 2^4 S_{\\text{INT}8}$"}, {"title": "4.3. KV Cache with Double Full Precision Buffer", "content": ""}, {"title": "4.3.1. CHALLENGES WITH KV CACHE QUANTIZATION AND SPECULATIVE DECODING", "content": "The key and value caches have each been found to exhibit unique characteristics indicating that they should be quantized with different strategies (Liu et al., 2024c). Specifically, quantizing the key cache along the channel axis and quantizing the value cache along the token axis minimizes quantization error (as shown in Appendix D Table 5), and therefore leads to a higher acceptance rate in speculative decoding. We apply asymmetric quantization and per-group quantization to both the key and value caches in INT4 precision, and we set the group size G to be equal to the head dimension to reduce overhead. These quantization techniques are illustrated in Appendix D Figure 7.\nHowever, these quantization strategies for the key and value caches pose efficiency challenges when combined with speculative decoding. Regarding the value cache wherein the values are quantized along the token axis, the naive strategy of directly quantizing newly generated tokens at each decoding step is expensive, as it introduces high computational overhead that occurs very frequently. Moreoever, regarding the key cache for which we apply quantization along the channel axis, the naive approach is to store multiple tokens in full precision until they equal the quantization group size, and then quantize them. However, since the KV cache for the most recent tokens is no longer preserved in full precision after quantization, this strategy adversally affects the acceptance rate, thus reducing the effectiveness of speculative decoding. Moreover, since speculative decoding may result in frequent rollbacks due to the target model rejecting the draft tokens, the quantized KV cache for the rejected tokens need to be discarded and replaced with new tokens in the quantization group. This leads to repeated quantization and dequantization, slowing down the decoding process."}, {"title": "4.3.2. ADAPT TO SPECULATIVE DECODING USING FULL PRECISION BUFFER", "content": "To enhance efficiency and ensure compatibility with speculative decoding, we propose maintaining a double full-precision buffer of size 2G, where G is the quantization group size. This buffer is divided into two equal parts: CF\u2081 and CF\u2082, each of size G. During prefill, we quantize the input tokens in batches of G while ensuring that at least G but no more than 2G of the most recent tokens remain in full precision. This ensures that CF\u2081 is always filled. In the decoding stage, newly generated tokens are stored in full precision in the second buffer, CF\u2082. Once the full-precision buffer reaches its maximum capacity of 2G, we wait for the target model to verify the generated tokens. If any tokens are rejected, we first remove the corresponding full-precision KV cache entries. Then, we quantize CF\u2081 and append it to the quantized KV cache. We then move CF\u2082 to CF1, which fully occupies CF\u2081 while leaving CF\u2082 empty and ready for tokens generated in future decoding steps. This whole process is visualized in Figure 8.\nUsing this design, we ensure that (1) at every step CF\u2081 is always filled, so there are at least recent G tokens kept in full precision, which is beneficial for the acceptance rate. (2) Quantization and KV cache movement will only happen every G decoding steps, which significantly reduces the overhead. (3) The design is compatible with speculative decoding since we can discard the KV cache for rejected tokens very flexibly by only operating on the second full-precision buffer CF\u2082 and without needing extra quantize and dequantize operations. We also show that our method is fully compatible with FlashDecoding in Appendix E."}, {"title": "4.3.3. SUMMARY", "content": "In summary, QuantSpec allows the draft and target models to share the same architecture in a self-speculative decoding manner, ensuring greater consistency between drafting and verification as opposed to traditional big-little speculative decoding methods. Our approach is mainly designed for long-context scenarios, where efficient KV cache management is critical, but it also supports short contexts where using weight quantization becomes more critical. We quantize the KV cache using our hierarchical INT4 design and use a double full-precision cache buffer for higher acceptance rates and flexibility with speculative decoding. Then in the decoding stage, when generating draft tokens, we only load the upper 4-bit of the KV cache and achieve speedup by significantly reducing the memory load/store operations. When verifying these draft tokens, we load both the upper and lower 4-bit KV cache representations and dequantize them into their INT8 representation to achieve performance that is comparable with an FP16 KV cache. If the full-precision buffer is saturated, after verification we quantize and clear one-half of the full-precision buffer to prepare for the next round of generation. The whole algorithm can be visualized in Figure 3 (and Algorithm 1 in Appendix)."}, {"title": "5. Evaluation", "content": "In this section, we evaluate the performance of QuantSpec across multiple datasets and context lengths. Our evaluation focuses on three key dimensions: (1) the acceptance ratio between the draft and target models, (2) GPU memory consumption, and (3) end-to-end serving speedup. We begin by presenting a detailed benchmarking of acceptance rate, memory usage, and end-to-end speedup across different datasets. Then, we highlight the performance gains achieved by our custom kernels for quantized KV cache."}, {"title": "5.1. Setup", "content": "All experiments are performed on a node equipped with 8 NVIDIA RTX A6000 GPUs. We evaluate QuantSpec using long-context variants of LLaMA-2 and LWM models as target models. For benchmarking decoding speedup, we use PG-19 (Rae et al., 2019), (a language modeling benchmark) and two long context summarization datasets, namely BENCH Sum (Zhang et al., 2024b; Yen et al., 2024) and Multi-LexSum (Shen et al., 2022; Yen et al., 2024). More details about the datasets are provided in Appendix F. Following Anonymous (2025), we compare against two recent sparse KV-based self-speculative decoding baselines: StreamingLLM (Anonymous, 2025; Xiao et al., 2023a) and SnapKV (Anonymous, 2025; Li et al., 2024a). To ensure a fair comparison, the draft KV budget for the baselines is set to one-fourth of the context length, matching our 4-bit quantized KV cache. We fix the quantization group size at 128, the residual length R for the KV cache at 256, and limit the number of output tokens to 90. The optimal speculation length \u03b3 for each dataset is determined through a hyperparameter search for each dataset-model pair. Details of the hyperparameter search are provided in Appendix G."}, {"title": "5.2. Speedup Evaluation", "content": "Table 3 shows the acceptance rate, GPU memory required, and speedup achieved compared to autoregressive decoding. We observe that QuantSpec provides consistently better speedups for all context lengths. For short and medium context lengths (e.g. 8k and 32k prompt length), QuantSpec achieves ~1.61\u00d7 to ~2.08\u00d7 speedups respectively on the Multi-LexSum dataset. For longer context lengths (e.g. 128k), our speedups are even greater, up to ~2.49x, all while using lower GPU memory than the baselines. We also see that acceptance rates of QuantSpec are considerably higher than the baselines for summarization tasks (refer to Appendix H for detailed comparison); this shows that for such tasks where the whole context is important, sparse KV cache methods are much more lossy, whereas quantization preserves most of the information in the context. Consequently, QuantSpec proves to be a more reliable choice, delivering consistent speedups across varying context lengths and query complexities."}, {"title": "5.2.1. KERNEL SPEEDUPS", "content": "In Table 4 we show the speedup achieved using our custom attention kernel that makes use of quantized KV cache versus the standard FP16 FlashAttention kernels. For a context length of 128k, our INT4 attention kernel is ~ 2.88\u00d7 faster than the standard FlashAttention kernel."}, {"title": "5.3. Ablation Results", "content": "We present an extensive ablation study of QuantSpec focusing on the contribution of weight versus KV cache quantization in the final speedup.\nWeight versus KV Quantization: Figure 4 illustrates the speedup ratio of QuantSpec compared to autoregressive baseline as context length increases. The figure benchmarks QuantSpec with KV cache-only quantization, weight-only quantization, and both. The results are aligned with the analysis done in Section 3.1, showing that for short contexts most of the speedup comes from quantizing weights, for medium length prompts both weight and KV cache quantization contribute to the final speedup, and KV cache quantization is most effective for long contexts."}, {"title": "6. Conclusions", "content": "In this paper, we have introduced a novel approach to enhance the efficiency and scalability of Large Language Models (LLMs) in long-context settings through quantized speculative decoding. Our method addresses the increasing memory and computational demands by optimizing the Key-Value (KV) cache operations, which become a significant bottleneck as the context length grows. We propose a double full-precision cache buffer to resolve conflicts between per-group quantization and speculative decoding. Our comprehensive approach shows that by integrating advanced quantization techniques with speculative decoding, it is possible to significantly improve processing speed without compromising the accuracy and performance of LLMs. This work paves the way for more scalable and effective deployment of LLMs in applications that require extensive contextual understanding, offering a robust solution to the challenges posed by long-context settings."}, {"title": "Appendix", "content": ""}, {"title": "A. Attention Module's Inference Workflow", "content": "The inference of LLMs can be divided into 2 parts: the prefill stage and the decoding stage. In the prefill stage, for the input sequence $X \\in \\mathbb{R}^{B \\times S_L \\times d}$, the KV cache update rule can be calculated as\n$Q = XW_Q, C_K = XW_K, C_V = XW_V$,\nwhere we denote the query, key, and value weight matrices as $W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}$ and denote the key and value caches as $C_K$ and $C_V$ respectively. B refers to batch size, $S_L$ refers to sequence length, and d refers to hidden size. We then calculate the multi-head attention (MHA) as:\n$O = MultiHeadAttn(Q, C_K, C_V)$.\nIn the decode stage, for input token $x \\in \\mathbb{R}^{B \\times 1 \\times d}$, we first calculate the query, key, and value of the current token:\n$q = xW_Q, c_k = xW_K, c_v = xW_V$,\nthen concatenate the KV cache with the current token's key and value to update the KV cache:\n$C_K = concat(C_K, c_k), C_V = concat(C_V, c_v)$.\nThen, the multi-head attention output is calculated:\n$O = MultiHeadAttn(q, C_K, C_V)$."}, {"title": "B. More Related Works", "content": "We list some related works that we find interesting, but can not elaborate on in the related works section due to space limitations.\nEfficient Long Context Inference Some research maintains the full key-value pairs but dynamically loads them from high-bandwidth memory (Yang et al., 2024; Tang et al., 2024), and usually achieves higher performance at the cost of higher memory consumption. Shared KV cache across tokens (Nawrot et al., 2024) and layers (Brandon et al., 2024) provides a new way to reduce the KV cache budget through sharing.\nQuantization Any Precision representation (Park et al., 2024) incorporates multiple precision levels (e.g., INT2, INT4, and INT8) within a single representation, eliminating the need to store separate KV caches for each precision and allowing the framework to dynamically select the optimal precision based on the complexity of the task. Training quantization (Peng et al., 2023; Xi et al., 2024b; Fishman et al., 2024; Xi et al., 2024a) reduces the bit precision of various model parameters, gradients, and activations to accelerate training. Attention quantization (Chen et al., 2024a; Zhang et al., 2024a; Shah et al., 2024) reduces the computational overhead associated with attention computations, which becomes dominant in the prefill stage of the long-context inference setting.\nSpeculative Decoding Zhao et al., (Zhao et al., 2024) explored complementary quantization schemes in speculative decoding with QSpec, enhancing efficiency without significant performance degradation. Sirius (Zhou et al., 2024) finds that contextual sparsity will lead to poor performance under the speculative decoding"}]}