{"title": "D2Styler: Advancing Arbitrary Style Transfer with Discrete Diffusion Methods", "authors": ["Onkar Susladkar", "Gayatri Deshmukh", "Sparsh Mittal", "Parth Shastri"], "abstract": "In image processing, one of the most challenging tasks is to render an image's semantic meaning using a variety of artistic approaches. Existing techniques for arbitrary style transfer (AST) frequently experience mode-collapse, over-stylization, or under-stylization due to a disparity between the style and content images. We propose a novel framework called D\u00b2Styler (Discrete Diffusion Styler) that leverages the discrete representational capability of VQ-GANs and the advantages of discrete diffusion, including stable training and avoidance of mode collapse. Our method uses Adaptive Instance Normalization (AdaIN) features as a context guide for the reverse diffusion process. This makes it easy to move features from the style image to the content image without bias. The proposed method substantially enhances the visual quality of style-transferred images, allowing the combination of content and style in a visually appealing manner. We take style images from the WikiArt dataset and content images from the COCO dataset. Experimental results demonstrate that D2Styler produces high-quality style-transferred images and outperforms twelve existing methods on nearly all the metrics. The qualitative results and ablation studies provide further insights into the efficacy of our technique. The code is available at https://github.com/Onkarsus13/D2Styler.", "sections": [{"title": "1 Introduction", "content": "Style transfer (ST) is essential in editing images and generating new artistic images. Given a content image and a style image, Style transfer (ST) synthesizes a new image by transferring the style from the style image while preserving the substance from the content image. Neural style transfer (NST) has been studied extensively in recent years [14]. NST seeks to learn how humans perceive images, as transferring style without significantly altering the semantic content in the target image requires a single network to disentangle the style and the content. After the introduction of pioneering work by Gatys et al. [14], many works have proposed improvements in use of a single feed-forward method [22], loss function, use of regularization, and normalization techniques.\nWhile GANs have been used to perform NST [24], GANs are challenging to train and offer no control over the style and content of the output image. Researchers have used standalone flow-based models and GAN plus flow-based models for image generation. These models allow control over the output image attributes [13,2]. Recently, diffusion models have become popular for image generation [34,37,38]. These models allow generating new images based on text prompts, image in-painting, and conditional image-to-image translation. These models also allow arbitrary style transfer by giving an image and a text prompt specifying the style we need to transfer. However, these models face a strict trade-off between style transfer and content preservation.\nWe present D2Styler, a technique to perform arbitrary style transfer for a given content image and a style image. Figure 1 illustrates D2Styler output. D2Styler uses a pretrained VQ-GAN encoder [11] to encode the content and style images. Then, it models their combined latent space by a conditional diffusion model. This diffusion model is conditioned on the features extracted by matching the statistics of the content and style images. To achieve this, an AdaIN [21] layer with a pre-trained convolutional encoder network is used. Our key idea is that this approach provides a context for the diffusion decoder to predict the masked inputs correctly. The resultant learned latent code is passed through the VQ-GAN decoder to obtain the style-transferred image. Our key contributions are:\n1.  We introduce a pioneering approach that combines discrete diffusion with AdaIN, uniquely addressing the prevalent issues of mode-collapse and over-stylization in existing style transfer methods. This integration not only stabilizes the training process but also ensures the preservation of content integrity while applying diverse artistic styles, a critical improvement over prior methodologies.\n2.  We propose using AdaIN [21] features to guide the diffusion decoder, conditioning the model on matched statistics between style and content features. This approach enables more precise control over the style transfer process, ensuring that the output closely aligns with the desired stylistic attributes while maintaining the integrity of the content."}, {"title": "2 Related Work", "content": "Neural Style Transfer (NST): NST has been extensively studied in non-photorealistic rendering and texture generation [10]. Image analogy-based algorithms (e.g., [18]) examine the relationship between two input images and transfer the features to create a stylized image. However, when applied to arbitrary settings, these methods face scalability challenges. Gatys et al. [14] employ a Gram matrix to extract features from a pre-trained DNN with an iterative optimization network to produce stylized images using multi-level feature correlations. Since then, many works have addressed key NST issues, including speed, control, quality, photorealism and temporal style transfer.\nAs a workaround for slow iterative optimization strategies in NST, feed-forward networks are trained to minimize the same losses based on Gram matrices [14]. These feed-forward frameworks are faster than iterative optimization and appropriate for real-time deployment [22]. Ulyanov et al. [42] propose feed-forward network enhancements to improve example quality and variety. However, these techniques can transfer a limited number of styles because of their training process. To overcome this constraint, Dumoulin et al. [9] suggest a framework based on a conditional instance normalization layer that can transfer 32 styles. Li et al. [32] propose a framework that can transfer 300 styles. However, these methods cannot transfer arbitrary styles.\nAdaIN [21] is a pioneering method in addressing the problem of arbitrary style transfer (AST). It uses a feed-forward network to match style and content feature statistics in an intermediate layer. However, AdaIN does not generalize well and faces mode collapse. The WCT technique [31] matches content and style covariance utilizing whitening and color transform. AdaAttN [33] considers both high-level and low-level features through adaptive attention normalization. The AST methods involving encoder-decoder architectures are prone to information loss due to pooling layers of the encoder network. This causes deformation of the output content.\nGenerative models: The Variational Auto-encoder (VAE) [25] proposes maximizing a lower bound on data probability to learn latent space representation. It learns the manifold representation of the input data distribution and generates new samples from the learned continuous latent space. Despite the impressive results of GANs in image-to-image translation and style transfer, they still suffer from mode collapse, complicated training, and instability. To overcome these problems, VQ-VAE [43] learns a discrete latent space instead of a continuous one. This inspired the development of VQ-GAN [11], which uses transformers and discrete latents. Other works, such as DALL.E [36] and Cogview [8], have also leveraged discrete latent and auto-regressive methods to achieve remarkable results in image generation. However, models based on discrete representations face challenges, such as increased accumulated errors, decreased speed for high-resolution images, and directional bias.\nDiffusion models: The denoising diffusion probabilistic model is a generative model inspired by thermodynamics's \"diffusion\" process. Discrete diffusion has been applied to text-generation [20] and image-generation [3], however, these models were restricted to generating 32x32 images. The recently proposed VQ-diffusion [15] technique enables efficient text-to-image generation and provides results comparable with the continuous paradigm.\nKwon et al. [28] propose utilizing style and structure losses to direct the sampling process for text-guided image translation. Wang et al. [45] fine-tune diffusion models by integrating CLIP, enabling them to learn style references through text prompts. Everaert et al. [12] recommend fine-tuning Stable Diffusion with a new noise distribution that mimics the style images' distribution. Diffusion-Enhanced PatchMatch [16] incorporates patch-based techniques with whitening and coloring transformations in the latent space. StyleDrop [40] model, which is based on the generative vision transformer Muse [5] rather than text-to-image diffusion models, produces content in diverse visual styles. Models like DreamStyler [1] exhibit advanced textual inversion, utilizing techniques such as BLIP-2 [30] and an image encoder to generate content by inverting text and content images while associating style with text. Kim et al. [23] fine-tune a pretrained DDIM to generate images based on text descriptions, introducing a local directional CLIP loss that ensures the direction between the generated image and the original image closely matches the direction between the reference (original domain) and target text (target domain). Chandramouli et al. [4] employ a deterministic forward diffusion approach, achieving the desired manipulation by using the target text to condition the reverse diffusion process. Prompt-to-Prompt [17] aims to preserve some original image content by modifying the cross-attention maps. Lastly, Plug-and-Play [41] investigates injecting spatial features and self-attention maps to uphold the overall structural integrity of the image."}, {"title": "3 D2Styler: A novel AST framework", "content": "D2Styler harnesses the power of discrete diffusion and AdaIN to address the inherent challenges of style transfer. Building upon the discrete representational capabilities of VQ-GANs, D2Styler pioneers a unique approach to arbitrary style transfer that promises enhanced visual quality and reduced mode collapse by intelligently navigating the latent space of content and style images. Unlike traditional feed-forward methods, D2Styler uses the high-fidelity image generation powers of diffusion models and the fine-tuned control provided by Vector-Quantized feature spaces introduced by VQ-GAN [15]. Figure 2 shows the architecture of D2Styler. This architecture functions in two distinct stages: stage 1 and stage 2. During stage 1, the style and content images are taken as inputs and turned into condensed features using latent discrete diffusion. Stage 2 leverages these condensed features to generate the final stylized image.\nStage 1: In stage 1, both style and content images are encoded into continuous latent vectors using a VQ-GAN encoder trained on the OpenImages dataset [27]. These vectors are then projected to the closest codebook item in the discrete latent space. This mapping of continuous vectors to the adjacent discrete codebook vectors is known as quantization (illustrated as Q(.) in Figure 2). The discretized nature of these vectors facilitates the grouping of similar data points, enhancing the subsequent diffusion sampling within a confined vector space. Once quantized, vectors are then flattened, concatenated, and sent to the TransDiffuser (Section 3.1). Inside the TransDiffuser, these discrete vectors go through the diffusion process, which is influenced by AdaIN features (Section 3.2). The outcome of this process is refined denoised features. These features proceed to the next stage (stage 2), where they play a pivotal role in reconstructing the final stylized image.\nStage 2: It uses a pre-trained VQ-GAN decoder trained on OpenImages. This decoder takes improved discrete features from the TransDiffuser as input and creates a stylized picture as output. In particular, when the Stage 2 decoder is being fine-tuned, gradients from Stage 1 are blocked. This is shown in Figure 2 by the blue arrow. The Stage-2 decoder incorporates a perceptual loss mechanism. This involves calculating the L1 distance between the hidden representation of the ground truth image and the image generated by the decoder. To do this, a pre-trained VGG model is used to derive these hidden representations for both the ground truth and the generated image. A perceptual loss term, denoted as $L_{style}$, is computed between a style image and the generated image. Similarly, another term, $L_{content}$, is computed between a content image and the generated image. Together, these losses ensure that small differences at the pixel level have less effect on the network. This strikes a balance between keeping the information and sharing the desired style. This method avoids excessive stylization or under-stylization in the final image. We also compute a $L_{feature}$ loss between AdaIN features and the generated image. This loss ensures that the stylized image retains its original content while adding flair. Formulation of these losses is given in 3.3"}, {"title": "3.1 Trans Diffuser", "content": "Figure 3 shows the architecture of TransDiffuser. It is designed to model discrete diffusion processes on quantized vectors. Gu et al. [15] propose use of diffusion to model the discrete vector-quantized latent space of VQ-GAN. We extend this to style transfer. In TransDiffuser, we take a quantized vector ($Q_{cs}$) as input.\nDuring forward diffusion, this input vector ($Q_{cs}$) undergoes a gradual corruption process orchestrated by Markov chain $p(Z_{t-1}|Z_{t-2})$. To achieve this, tokens in $Z_{t-2}$ are randomly masked. This iterative process unfolds across a fixed number of time steps (t), generating a sequence of latents ($z_1, ..., z_t$) that progressively accumulate noise. After the forward process, as visualized in Figure 2, the reverse process comes into play, starting with the noisy latent variable $z_t$. This reverse process sequentially removes noise from the latent variables, eventually reconstructing the original data ($z_o$). Throughout this reverse process, AdaIN features ($A(X_c, X_s)$) are injected into each network block ($p_\theta(Z_{t-2}|Z_{t-1}, A(X_c, X_s))$). These AdaIN features act as conditional cues for the diffusion process, effectively managing the equilibrium between content and style.\nTo train TransDiffuser, we utilize the MLM and ELBO loss functions [7,15]. The MLM loss facilitates the reconstruction of masked tokens, while the ELBO loss captures the probabilistic nature of content and style representations. This dual-loss approach ensures that the generated images exhibit a diverse and meaningful range of possible outcomes, striking a balance between style and content features. The resulting images maintain their original content while adopting the desired style."}, {"title": "3.2 AdaIN feature extraction", "content": "Within the context of our approach to reverse diffusion, we establish the necessary conditions by harnessing both the input style image ($x_s$) and content image ($x_c$). A pivotal step involves crafting a feature map that amalgamates insights from both the style and content images. This is achieved through the utilization of pretrained CNN encoders, VGG-16, in our case. To extract relevant content information, we subject the content image to the VGG encoder, extracting feature maps from specific layers such as 'conv1_2', 'conv2_2', 'conv3_2', and 'conv4_2'. We employ a multiscale extractor to adapt these extracted feature maps for compatibility with the subsequent AdaIN block. This selection of layers is particularly significant due to their ability to encapsulate high-level content details within the image [14]. Likewise, the style image is also processed through the VGG encoder to extract feature maps from its final layer. These extracted feature maps from both the content and style images are then input into the AdaIN block. AdaIN aligns the statistical characteristics of the style and content features. These processed features are subsequently utilized during the reverse process in the TransDiffuser decoder, as detailed in Section 3.1. Notably, the introduction of a parameter $\\alpha$, in combination with the AdaIN features, enables precise control over the infusion of style into the resulting stylized image. The influence of using alternative style-content CNN encoders, besides VGG-16, is explored in Section 5."}, {"title": "3.3 Loss functions:", "content": "We optimized our network using a combination of four loss functions as follows.\n1.  Diffusion Loss ($L_{diff}$): The diffusion loss measures the ability of the model to reverse the diffusion process and generate a realistic quantized vector from noise. This loss is often implemented as a denoising score matching or noise prediction loss. During the forward diffusion process, we diffuse the quantized vector from VQ-GAN to get p(xt|xo). This vector is passed through the Trans-Diffuser module (Pe), which is conditioned on the AdaIN features obtained from A(Xse, Xce). Here, Xse and Xce are the representations from the pre-trained CNN model when we pass the style Is and content Ic images, respectively. At stage 1, we compute the diffusion loss as follows:\n$L_{diff} = -log(P_{\\theta}(p(x_t|x_o), t, A(X_{se}, X_{ce})))$\nIn stage 2, the predicted quantized vector from P\u03b8 is passed to the VQ-GAN decoder D to get the final stylized image (\\^x). During this training stage, we block the gradient from Stage 2 to Stage 1 and only train the decoder (D(.)) using the following losses:\n2.  Style Loss ($L_{style}$): Measures the difference between the style representations of the style image and the representations from the generated image through pretrained vgg-network. This ensures the output image adopts the stylistic elements (e.g., textures, colors, patterns) of the style image.\n$L_{style} = ||vgg(I_s) \u2013 vgg(\\^x)||\n3.  Content Loss ($L_{cont}$): Measures the difference between the feature representations of the content image and the generated image.\n$L_{cont} = ||vgg(I_c) \u2013 vgg(\\^x)||\n4.  Feature Loss ($L_{feat}$): Computes the L1 loss between the AdaIN features and the VGG representations of the generated image. AdaIN dynamically adjusts the normalization parameters (mean and variance) of the feature maps based on the statistics of the input image. This ensures that the normalization process is tailored to the specific content and style images used. This dynamic adjustment helps better align the feature statistics of the generated image with those of the style image, leading to a more faithful style transfer.\n$L_{feat} = ||A(X_{se}, X_{ce}) - vgg(\\^x)||\nDuring the training stage 2, we freeze the VGG encoder and AdaIN A(.) encoder is unfrozen."}, {"title": "4 Experiments and results", "content": "Dataset: We select 100,000 images from the COCO dataset as content images and 78,669 images from the WikiArt dataset [39] as style images. We employ a many-to-many strategy to create 1 million content and style image pairs. From these pairs, we randomly select 900,000 pairs for training and use 20,000 and 80,000 pairs for validation and testing, respectively. We employed following metrics: (1) GM for faithful style adoption (2) SSIM for image similarity (3) LPIPS for perceptual resemblance and (4) PD for measuring perceptual dissimilarity."}, {"title": "4.1 Quantitative Results", "content": "As shown in Table 1, D2Styler method outperforms all previous techniques on nearly all the metrics, demonstrating its robust capability in style transfer tasks. D2Styler also shows relatively small inference time. D2Styler successfully combines effectiveness and efficiency, making it a leading solution in style transfer.\nThe DiT method, while slightly lagging behind D2Styler in most metrics, achieves a slightly better performance in the Gram Matrix metric. This might be due to its training on a larger and more diverse dataset and its greater complexity in terms of model parameters. We have taken a pretrained DiT and finetuned it on our dataset. D\u00b2Styler uses a pretrained VQ-GAN, however, the dataset on which DiT has been pretrained is larger than the one on which VQ-GAN has been pretrained."}, {"title": "4.2 Qualitative results", "content": "As depicted in Figure 4, D2Styler effectively preserves the original content of the images while imbuing them with the desired artistic styles from the style images. The method maintains the original image's underlying structure and details while adopting the reference image's style, producing high-quality, visually appealing images. This capability is evident across various examples shown, where the essence and details of the original images are preserved, yet the style is convincingly and beautifully applied. This includes maintaining structural information without sacrificing the tiny details found in the style images.\nStyTr2 is limited in accurately mapping the reference image's style onto the content image, producing unsatisfactory results. Cartoon-Flow, while excelling at maintaining the visual integrity of the content image, occasionally fails to adapt the reference image's aesthetic to the output image. For instance, Cartoon-Flow can preserve the facial structure of Brad Pitt (content image) in the output image, but it cannot account for the sketch style present in the style image, hence its output image is devoid of the desired artistic effect.\nBoth Cartoon-Flow and StyTr2 struggle to incorporate the color information present in the style image, as demonstrated in the third and fourth rows of Figure 4. They are unable to generate the color information of the lion images in the style image. Similarly, for the images in the fifth row, both models produce an image with the same structural quality as the content image but fail to produce the yellow box texture present in the style image on the output image.\nAdaAttN provides the cross attention between the style and the content features, however, it cannot properly retain similar content due to feature collapse. The use of AdaIN features as a condition to the Transdiffuser block helps in a significant boost in performance. AdaIN features are crucial for the effectiveness of style transfer methods because they separate content and style by adjusting the mean and variance of content features to match those of style features. This dynamic adjustment allows the network to adapt to a wide range of styles efficiently, enhancing the quality and visual appeal of the transfer. AdaIN simplifies the training process by focusing on normalization parameters rather than complex style representations, providing better control over the degree of stylization. Without AdaIN, style transfer methods would struggle to achieve the same level of performance, flexibility, and scalability. From Figure 4, we note that for most images, AdaAttN cannot preserve the content features. In contrast, D2Styler proves to be superior at capturing tiny details in the style image and accurately transferring them to the content image without sacrificing the structural information of the original image. Additionally, D\u00b2Styler can robustly handle a wide range of styles and produce high-quality outputs that accurately represent the style of the reference image. Our method achieves comparable text alignment to the Diffusion-based methods for generating content images, i.e., StyTr2. This indicates that our method does not compromise the original style control capabilities of SD while learning the style of the reference images. The substantial advantage reflected in the image quality metric compared to all other methods corroborates the practicality of our approach. In summary, D2Styler achieves an optimal balance between style image fidelity and content image similarity with the most pleasing image quality.\nFigure 5 presents the qualitative results of D2Styler on the COCO dataset, effectively demonstrating the capabilities of D2Styler. For instance, the second row illustrates the application of a classical painting style, which imparts a rustic, textured effect reminiscent of brushstrokes and the color palette of the original artwork. Similarly, the pencil sketch style, shown in another row, transforms content images into monochromatic drawings, emphasizing lines and shading to create a hand-drawn appearance. These results underscore the versatility and robustness of D2Styler in blending various artistic styles with diverse content images, while preserving the essential characteristics of both. The model's ability to maintain the structural integrity of the content while accurately reflecting the stylistic nuances of the artistic images demonstrates its potential applications in artistic creation and advanced image processing.\nFigure 6 further compares qualitative results of various techniques. InST (column #3) [48] often lacks coherence in preserving the content structure. It fails to semantically transfer the color in a one-to-one correspondence. They used an additional tone transfer module [21] to align the color of the content and reference images. Different methods have different preferences for retaining the colors of the content image.\nStyleTr2 (column #5) provides reasonable style transfer results but sometimes over-emphasizes the style, leading to over-stylization. This happens because of an imbalance of content loss and style loss. If the weighting for the style loss is too high relative to the content loss, the model might prioritize transferring stylistic elements over preserving the original content structure, resulting in over-stylization. On the other hand, AdaAttN utilizes cross-attention mechanisms for style transfer but occasionally suffers from feature collapse, affecting content preservation (e.g., column #6 row #8). The per-point basis of AdaAttN leads to style degeneration, thus the stylized output is not consistent with the input reference. The content leak issue usually occurs in the stylization process because CNN-based feature representation may not sufficiently capture details in the image content.\nFurthermore, the robustness and generated visual effects of CartoonFlow (column #4) [29] may degrade due to the limited capability of the feature representation. By contrast, D2Styler leverages the capability of transformer-based architecture to capture long-range dependencies, hence, it significantly alleviates the content leak issue. The flow-based model has limited capability of feature representation, hence, the ArtFlow (column #7) [2] results generally suffer from insufficient or inaccurate style. The border of stylized images may present undesirable patterns due to numerical overflow.\nStable Diffusion (column #8) generates high-quality images, albeit with increased computational time (Table 1), due to the significant effort required to interpolate style and content features in its latent space to achieve perfect stylization. In contrast, D2Styler demonstrates superior performance in both style fidelity and content preservation. This superiority stems from our use of diffusion"}, {"title": "4.3 Versatility of D2Styler", "content": "Controlling the style. To control the amount of style output, D2Styler introduces a weight parameter named \u03b1 ($\\alpha \\in [0,1]$) for the AdaIN context features. As shown in Figure 7, the amount of style in the image is proportional to the value of the \u03b1 parameter.\nUse of the VQ-GAN encoder at inference. At inference, we can use either (1) the samples from the diffusion mask prior or (2) the VQ-GAN encoded version of the input content and style image. The former gives more priority to the style texture, whereas the latter retains more content, leading to more visually pleasing images, as shown in Figure 8. Using the encoder also results in a slightly higher (i.e., better) SSIM metric as opposed to starting from the diffusion mask prior. Using the encoder gives a head start to the denoising process, as it contains some prior information about the content. By contrast, this prior information is absent when we start from the mask tokens themselves. Table 1 reports GM and SSIM scores for both types of inference strategies.\nMulti-style transfer: We realize multi-style transfer by passing a linear combination of AdaIN features for each style-content combination. Each image's contribution can be controlled by using a weight for each style-content feature being combined. An example generation is shown in Figure 9. Evidently, the AdaIN layer serves as a context for the diffusion model, and its features can be used to control the generation of a stylized output image."}, {"title": "5 Ablation Study", "content": "1.  Effect of CNN encoders: Table 2 shows the results with different CNN encoders in D2Styler. Notice that using CNN encoders such as VGG, ResNet and Efficient Net leads to better results than use of transformers. NST transfers the style of a style image to a content image while keeping its spatial information. Since transformers do not incorporate locality bias inherently, they often fail to learn hierarchical representations from an input image effectively. Due to this, the model faces challenges in decoupling style and content information from an image, leading to poor performance in NST.\n2.  Effect of loss functions: Table 3 shows the impact of various loss functions. The best scores are obtained by using a combination of $L_{feature}$, $L_{style}$, and $L_{content}$ losses, because this combined loss considers both style data from the encoder model and high-level features of the content. In contrast, using only $L_{style}$ (style loss) neglects content preservation, and using only $L_{content}$ (content loss) overlooks style consistency. Adding $L_{feature}$ (feature loss) to $L_{style}$ and $L_{content}$ makes the results even better by forcing a close match between the output features and the normalization data of the Adaptive Instance Normalisation (AdaIN) layer. Notably, using only $L_{feature}$ loss leads to better performance than most previous techniques [26,14,13,46,29,47]. This validates the efficacy of our feature-matching approach. The above insights can be confirmed from Figure 10, which illustrates the output images with various loss functions.\n3.  Effect of the number of Trans Diffuser blocks: We evaluate three versions of D2Styler, namely small, medium, and large, which utilize two, four and six TransDiffuser blocks, respectively. With the increasing number of blocks, the network can more accurately model the style transfer process, which improves the scores (Table 4). Interestingly, even small and medium versions of D2Styler can outperform most previous techniques (refer Table 1). This proves the efficacy of our technique.\n4.  Effect of changing the number of diffusion steps: Table 5 shows the results with different numbers of diffusion steps. Clearly, increasing the number of diffusion steps consistently improves image quality and structural similarity, although the returns become marginal after 25 steps. Our method demonstrates that with just 5 diffusion steps, it matches the inference times of GAN and flow-based baselines (refer to Table 1), and also outperforms them in all quantitative metrics. This indicates our method's robustness and superior performance compared to previous baselines. Figure 11 shows the output images with different numbers of diffusion steps."}, {"title": "6 Conclusion", "content": "We propose a novel AST technique, named D2Styler, by combining the benefits of discrete diffusion with discrete representational capacity of VQ-GANs. We propose a novel way of guiding the diffusion process by incorporating Adaptive Instance Normalisation (AdaIN) features. This allows transferring features from the style image to the content image without bias. D\u00b2Styler produces style-transferred images that are both visually appealing and accurate to the original content image in terms of their semantic significance. D2Styler outperforms previous techniques and solves the problems of mode collapse, over-stylization, and under-stylization. Future work will focus on generalizing our technique to a wide range of image-processing tasks, e.g., editing and synthesis."}]}