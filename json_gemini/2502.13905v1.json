{"title": "Partially Observable Gaussian Process Network and Doubly Stochastic Variational Inference", "authors": ["Saksham Kiroirwal", "Julius Pfrommer", "J\u00fcrgen Beyerer"], "abstract": "To reduce the curse of dimensionality for Gaussian processes (GP), they can be decomposed into a Gaussian Process Network (GPN) of coupled subprocesses with lower dimensionality. In some cases, intermediate observations are available within the GPN. However, intermediate observations are often indirect, noisy, and incomplete in most real-world systems. This work introduces the Partially Observable Gaussian Process Network (POGPN) to model real-world process networks. We model a joint distribution of latent functions of subprocesses and make inferences using observations from all subprocesses. POGPN incorporates observation lenses (observation likelihoods) into the well-established inference method of deep Gaussian processes. We also introduce two training methods for POPGN to make inferences on the whole network using node observations. The application to benchmark problems demonstrates how incorporating partial observations during training and inference can improve the predictive performance of the overall network, offering a promising outlook for its practical application.", "sections": [{"title": "1 INTRODUCTION", "content": "Conventionally, a process is considered a single-process black box with input(s) and some output(s) being observed. However, systems are hardly single-process and comprise multiple sub-processes where intermediate outputs from each subprocess can be observed Fenner et al. [2005]. The respective sub-processes can be stochastic as well. Gaussian processes (GP) are a popular probabilistic frame-work to model nonlinear input-output dependencies. They have been widely used in various applications, such as the monitoring of key performance indicators for processes [Kontar et al., 2017], control [Likar and Kocijan, 2007] and Bayesian optimization [Frazier and Wang, 2015]."}, {"title": "2 PROCESS NETWORK WITH PARTIAL OBSERVABILITY", "content": "We consider a stochastic process P comprised of subprocesses, {P(w)} for w \u2208 W, (which can be stochastic) as shown in Figure 1. The process of stochasticity can come from either a lack of knowledge of the process or other hidden influences or random noise (w) ~ p(e(w)). Process P is a DAG where the nodes represent the subprocesses. Each subprocess P(w) is governed by a transformation function f(w) which takes as input(s) some adjustable parameters x(w) and the output(s) of parent subprocesses PPa(w), where Pa(w) represents the direct parents of w. Using the transformation function, the contactenated input(s) (x(w), PPa(w)) are transformed into output(s) f(w). \nIn most cases, the output(s) f(w) of a subprocess P(w) cannot be fully observed and are observed indirectly/partially using an \"observation lens\" as \u1ef9(w), hence the name partially observable process network. An example of such an observation lens can be the Gaussian observation noise of a sensor. In probabilistic modeling, the \"observation lens\" is often modeled as the likelihood function \u1ef9(w) ~ p(y(w)|f(w)), which could be as simple as additive Gaussian noise or something more complex. The true output(s) f(w) remain latent. We assume that, the observation lens is always present whenever an output is referred to as \"observed\" unless stated as \"latent\" or \"true\" output. The \"latent\" output of a parent subprocess becomes the input for a child subprocess.\nAt this point we are able to define each subprocess P(w) using a tuple (p(f(w)|x(w), fPa(w)), p(y(w)|f(w))), where x(w) represents the adjustable input parameters, fPa(w) represents the latent output(s) from the parent subprocess(es), p(f(w)\\x(w), Pa(w)) represents the probability distribution over the transformation function and p(y(w)|f(w)) represents the observation likelihood or lens with which the latent output of the subprocess P(w) is observed.\nThe process network P can be represented using a DAG, G. The nodes are topologically ordered such that w' < w, w' \u2208 Pa(w) for all w \u2208 W. We use the terms subprocess and node interchangeably to represent a subprocess in G. x(w) are addressed as adjustable input nodes or parameters. The final/end process of the process P is defined as the subprocess(es) P(w) for which there are no child nodes. The corresponding observed output(s) \u1ef9(w) are called the observed final output(s).\nThe problem statement is to model the subprocess output(s), and the end process output using the adjustable inputs of different subprocess(es) and all indirect observations of the process network made using different observation likelihoods. We assume that the data generation DAG or the causal path is known. We refrain from augmenting the intermediate observations with adjustable inputs to avoid blowing up the input dimensionality for the used model. We discuss the proposed solution in section 4."}, {"title": "3 BACKGROUND", "content": "In this section we discuss Gaussian processes (GP), deep GP, and existing GP networks as well as their limitations."}, {"title": "3.1 GAUSSIAN PROCESS", "content": "Considering the multi-process system in Figure 1, it is conventionally modeled as a single-process black box with inputs xn := xm) \u2208 X and observed outputs \u0177n = \u1ef9n2). A Gaussian process represents a stochastic process as a distribution over the infinite-dimensional functions, which can be evaluated at an input location in X. A finite set of observed outputs evaluated at respective input locations represents a multivariate normal distribution. For a given input location, xn, of dimension Dr, the GP prior is represented as\np(fn|xn) = N(m(xn), k(xn,xn)),\nexact GP prior\nwhere m(\u00b7) : RD \u2192 R is the mean function and k(\u00b7, \u00b7') : XxX \u2192 R is the covariance function or kernel [Rasmussen, 2003].\nFor N observations, the hyperparameters of the GP are optimized by minimizing the negative MLL for the observed outputs using (1) as\nLGP = - \u2211log Ep(fn|xn) [P(Yn|fn)],\nn=1\nwhich can be calculated in closed form for a Gaussian likelihood and scales with O(N\u00b3) [Rasmussen, 2003]. The concepts of the single task GP can be extended to vector-valued stochastic process fn(\u00b7) : RD1 \u2192 RDy in which the observed output is a vector, of dimension Dy, for each input location. A popular way of modeling correlation between the outputs is using the Linear Model of Coregionalization (LMC) as explained by Alvarez et al. [2012], Van der Wilk et al. [2020]."}, {"title": "3.2 STOCHASTIC VARIATIONAL GAUSSIAN PROCESS", "content": "Stochastic Variational Gaussian Processes (SVGP) by Hensman et al. [2013, 2015] are useful for large datasets and non-Gaussian likelihoods. It assumes a set of inducing locations, Z = {z}{=1 \u2208 X, and inducing points which are, function value evaluations u = {Wi}{=1 at {zi}{=1. The joint distribution of inducing points u can be represented as\np(u; Z) = N(m(Z), k(Z, Z')).\nAdditionally, the inducing points are also assumed to have a marginal distribution q(u) = N(\u03bcu, \u03a3u). A joint multivariate normal distribution p(fn, u|xn, Z) can be defined using (1) and (3). For further details, readers are encouraged to refer to Leibfried et al. [2020].\nSVGP approximates the exact posterior with a variational posterior\nq(fnxn, Z) = Eq(u) [p(fn|u)] = N(\u03bcq(f), \u03a3q(f)),"}, {"title": "3.3 DEEP GAUSSIAN PROCESS", "content": "Deep Gaussian Process (DGP) introduced by Damianou and Lawrence [2013] provides a hierarchical model, in which independent GPs are stacked in layers as {f(1) \nSimilar to the idea of inducing points in the input domain of SVGP, inducing points {U(1) \u2208 RI(1)\u00d7Di}L I(1)\u00d7DL are introduced at inducing locations {Z(1-1) \u2208 RI(1)\u00d7Di-1}=1, where Do = D and I(l) is the number of inducing points for layer l. Since the GPs are independent of each other Salimbeni and Deisenroth [2017], the marginal distribution for a layer depends only on the distribution of the previous layer, and the marginal distribution for layer L can be expressed using (4) and (5) as\ng(f(4)) = g(f(0)df(-1),"}, {"title": "3.4 GAUSSIAN PROCESS NETWORK AND THEIR LIMITATIONS", "content": "Gaussian Process Networks (GPN) coined by Friedman and Nachman [2000] and extended by Giudice et al. [2024] addresses the learning the Bayesian network structure and not the inference, which is different from our work. Gaussian Process Regression Networks (GPRN) by Friedman and Nachman [2000], Wilson et al. [2011] provide a different perspective by combining modeling the final output as a linear combination of Gaussian processes (like neural network structure). GPRN does not incorporate the intermediate observations and caters to a problem statement different from ours.\n The implemented setup poses four major limitations:\n1.  In most real-world cases, one can only observe the state space partially using an indirect observation lens. The latent outputs of the subprocesses are often hidden. Because of this, it can be considered that it is the latent/true outputs f(w) that influence the process and not the indirect observations y(w)."}, {"title": "4 PARTIALLY OBSERVABLE GAUSSIAN PROCESS NETWORK", "content": "We present our main contribution, the Partially Observable Gaussian Process Network (POGPN). Instead of node observations sharing a common distribution space, we propose that the latent functions reside in the same space and influence the child subprocess nodes. POGPN represents the process network in section 2 using a DAG where the nodes, w \u2208 W, are topologically ordered such that w' < w,\u2200w' \u2208 Pa(w). Each node is modeled as a GP(w)similar to the GPN setup in section 3.4, generalized as a vector-valued function f(w) ~ GP(w) (\u00b7, \u00b7') and can or cannot be observed using an arbitrary likelihood p(y(w) \\f(w)). The generalized notation allows for nodes to have different dimensionality. This formulation allows us to consider DGP a special POGPN case where only the last node observations are available. POGPN, with its assumption of arbitrary observation likelihood and common latent function space, provides a way to model continuous and categorical observations with the same model.\nUnlike the existing GPNs, a subprocess P(w) takes the parent node latent GP functions fPa(w) as the input rather than an instance of the noisy indirect observations \u1ef9 Pa(w). Since we can express the node's latent function using a distribution, we can take the expectation over the parent node distribution. The expectation over possible parent node output(s) provides robustness against parent subprocess stochastic-ity and can separate the observation lens from the actual process. This setup also allows for arbitrary observation likelihoods as the observation is separated from the network. \nEvidence Lower Bound (ELBO). Similar to DGP in section 3.3, we introduce inducing points, ZPa(w), ZX(w), in the space of the parent nodes and node inputs respectively, such that Z(w) = (ZPa(w), z\u0142 (w)). We wish to approximate the posterior p({f(w)}w\u2208w|{yw); xw) }wew) with the variational posterior q({f(w)}w\u2208w) which can be expressed using (8) as\nq({f(w)}wew) = I q(f(w); {fPa(w), x(w)}, Z(w)).\nWEW\nThe Kullback Leibler (KL) [Shlens, 2014] divergence between the variational posterior and true posterior can be expressed as\n-KL(q({f(w)}w\u2208w)||p({f(w)}w\u2208w|{y{w), x(w)}wew))\n= Eq({f(w)}wew) [logp({y{w}}w\u2208w|{f(w), x(w)}wew)]\nLog Likelihood Loss (LL loss)\n-KL(q({f(w)}w\u2208w)||p({f(w)}w\u2208w)) + Evidence\nKL loss\nThe ELBO for POGPN can then be defined as the combination of the \"LL loss\" and the \"KL loss\" term of (11). We now show how the terms of the ELBO can be simplified so that inference can be performed.\nThe conditional distribution p({yw)}wew|{f{w}}wew) can be simplified using the DAG structure as\np({y{w)}w\u2208w|{f(w)}wew)\n= p(y(W)\\{f(w)}w\u2208W)p(y(W\u22121), {f(w)}w\u2208w)\n= \u03a0pyw)|{f(w)}wew) = \u03a0p(yw)|f(w))\nWEW\nWEW\nwhere W = |W|. Using (10) and (12), the \"LL loss\" in Equation(11), can be expressed as\nEq({f(w)}wew) [logp({y{w)}w\u2208w|{f(w), x(w)}, (w)}w\u2208W)]\n=Eg({f(w)}wew) [logp(yw)|f(w))]\nWEW\n= \u2211\u0395(f)) Eq(f(w)) [logp(y(w)\\f(w))],\nWEW\nwhere the marginal q(f(w)) can be calculated using (4) and (5) as\nq(f(w))\n=\nw\n/ \u03a0\nJEW\nq(f(j) \\fPa(j), x(j), Z(i)) dfpa(j).\nFor N(w) observations for each node w, the inference can be made by minimizing the negative EBLO for POGPN as\nLPOGPN\nELBO\n= \u03a3\nNode\nELBO\nWEW\n=\nN(w)\n1\n\u03a3 c(w) \u03a3\u0395g(f(w)) [log plyw)|f(w))]\nWEW\nn\nLL loss\n+\u03b2\u03a3 KL(q(u(w))||p(u(w))),\nWEW\nKL loss\na normalization constant c(w) is introduced to keep the likelihood loss from different nodes comparable when the dimensionality of the node is not the same. We propose to keep c(w) = Dy(w) to keep the \"LL loss\" term of different nodes comparable and give equal importance to each node, where Dy(w) is the dimension of the observed output y(w). However, c(w) incorporate importance-based training where the emphasis lies on a particular node. If a node w' has no observation likelihood, then w' will contribute to only the \"KL loss\" and not to the \"LL loss\".\nPredictive Log Likelihood (PLL) loss. Using the inspiration from the PLL loss in 7 [Jankowiak et al., 2020b], the \"LL loss\" for PLL can be defined for POGPN as\nLLPLL =\n1\nN(w)\n\u03a3(w)log(f) [P(y(w)\\f(w))],\nWEW\nn=1\nLL loss\nwhile the \"KL loss\" is the same as (15). Like DGP inference, we use MC samples to calculate the \"LL loss\" for POGPN ELBO in (15). It is common to calculate log probabilities to avoid loss of precision, and a direct summation of log probabilities of MC samples would lead to expected log marginal likelihood rather than log expected marginal likelihood in (16). Using Jensen's inequality, one can prove that the former is an unbiased estimator of the latter. Jankowiak et al. [2020a] provides the sigma point method as a solution, but it is not easy to scale. We propose a more straightforward approach, where we use logsumexp to calculate the log of expected likelihood marginal (LL loss) of PLL using MC samples as\nLLPLL =\n1\nWEW\nN(w)\nS\n\u03a3\u03a3log((()))\nn=1\ns=1\nwhere fw) ~ q(f(w)). This formulation has a tighter lower bound to the log expected marginal likelihood in comparison to the unbiased estimator. For generalization, we call ELBO and PLL the \"loss\" for POGPN. MC samples, used while training, can be considered analogous to training the child process on many hypothesized parent true/latent outputs, and the variational inference allows for robustness against the stochasticity of parent subprocesses.\nWe now present two methods, namely ancestor-wise and node-wise, for training POGPN . These methods can use either of the factorized losses, ELBO (15) or PLL (16). For W nodes, N observations, and I points for each node, the computational complexity is O(W(NI\u00b2 + I\u00b3)).\nAncestor-wise Training. Algorithm 1 called POGPN-AL can be implemented using (15) and (16), where Anc(Wobs) represents the set of all ancestors of each node w \u2208 Wobs; \u5165(w) represent the GP hyperparameters (mean, kernel and variational) and (w) and likelihood hyperparameters of node w. We call Algorithm 1 ancestor-wise training as itupdates the parameters of all ancestor GPs of the observed nodes. This method is similar to the traditional method of training DGP, just that we consider multiple observation nodes in POGPN. It is beneficial when either all network nodes are observed or the nodes further in the graph are observed, and one wishes to condition the ancestor node(s) based on the observations of the child/successor node(s).\nNode-wise Training. Algorithm 2 called POGPN-NL, follows a coordinate ascent method for updating individual node GP hyperparameters \u5165(w) and likelihood hyperparameters \u03b8(w) for w \u2208 Wobs. With experimentation, we found that calculating updated q(F(w)) by looping over the observed nodes helps node-wise training converge to a global minimum. This is not the case when q(F(w)) is calculated only once outside the loop over nodes. Algorithm 2 explains the node-wise coordinate ascent method."}, {"title": "5 EXPERIMENTS", "content": "In this section, we conduct a comprehensive comparison of the performance of POGPN with various models, including independent GPs (IGP), Semi-Parameteric Latent Factor Model (SLFM), GPRN Wilson et al. [2011] and GPAR Re-queima et al. [2019]. POGPN is implemented using the gpytorch package Gardner et al. [2018]. Here, we use a squared exponential kernel and constant mean for all experiments, as in other models. Similarly, we take the number of inducing locations the same as used by the D-GPAR-NL model from Requeima et al. [2019] to ensure proper comparison. We use the ICM variational approximation to model the multi-task nodes as proposed by Van der Wilk et al. [2020]. The detailed construction procedure of POGPNS has been included in the supplementary section.\nJura dataset\u00b9. There are 259 locations from a mining area, for which the amount of zinc, nickel, and cadmium found is given. Along with these locations, there are another 100 locations with only zinc and nickel values available. The existing experiments use this information to predict the amount of cadmium for the remaining 100 locations. However, the original dataset also records two categorical observations: land use (4 classes) and the type of rock (5 classes) found at every location. Since all previous models cannot do classification and regression using one model, they do not use this information. However, we make a POGPN, as shown in Fig. 4, that can also use the categorical observations to predict the final output. We use softmax likelihood to model the multi-class observations and multi-task Gaussian likelihood (using LMC) to model mineral observations. For the latent function, we assume a two-dimensional multi-task GP as the latent function for categorical nodes \"Rock\" and \"Land\" and a three-dimensional GP for regression node \"Zn, Ni, Cd.\" A detailed explanation of the POGPN structure has been shown in the supplementary section.\n The number of inducing locations is 259, equal to the number of locations for fully observed data. The values are log standardized for evaluation, used for training, and then transformed back, and the mean absolute error is calculated. It can be seen that POGPN outperforms all other models. This shows POGPN as a new state-of-the-art multi-task that can even use multimodal intermediate information.\nEEG dataset\u00b2. The dataset consists of electrode measurements from the scalp of different subjects. Each sensor records 256 voltage measurements. The data focuses on the measurements from sensors F1, F2, F3, F4, F5, F6, and FZ from the first trial of control subject 337. The task is to predict F1, F2, and FZ measurements for the last 100 timestep, given the full observation of F3, F4, F5, and F6, and the first 156 measurements of F1, F2, and FZ. The values are standardized before training. We make a POGPN as shown in Fig. 5a, where the intermediate node is a four-dimensional multi-task GP, and the final node is a three-dimensional multi-task GP with respective multi-task Gaussian likelihoods.\nInducing locations are in the \"Time\" domain and kept the same as the total time steps (256 points) as used by Re-queima et al. [2019] and remain constant throughout the training process. For evaluation, the values are standardized before training and transformed back before prediction evaluation. POGPN consistently outperforms GPAR, showing significant improvements in SMSE and MLL."}, {"title": "6 DISCUSSION", "content": "We propose a Partially Observable Gaussian Process Network (POGPN) to model process networks where subprocesses can be stochastic, and the intermediate outputs can be observed using an observation lens, modeled as the observation likelihood. We develop a trainable loss, namely ELBO (Evidence Lower Bound) and Predictive Log Likelihood (PLL), for POGPN that makes inferences on the joint distribution of latent space and not just independent sub-processes. The inference can be made using MC samples, which can be considered analogous to training the child process on many hypothesized parent true/latent outputs. POGPN can incorporate continuous observations and non-Gaussian observations like categorical data. In our experience, we have not found any Gaussian process framework encompassing regression and classification within a single model. This setup makes POGPN very versatile and close to the real-world process networks where subprocesses can have arbitrary likelihood. We propose an ancestor-wise and a node-wise training method for POGPN. Experiments show the superior performance of POGPN-PLL over other existing Gaussian process networks. For further research, one can implement a message-passing algorithm to accommodate for parallel inference or use more complex observation likelihoods like a neural network."}, {"title": "A EXPERIMENTAL SETUP OF JURA DATASET", "content": "We make a POGPN, as shown in Fig. 7, that can also use the categorical observations to predict the final output. The valuesare log standardized before modeling and then transformed back for evaluation."}, {"title": "B EXPERIMENTAL SETUP OF EEG DATASET", "content": "We make a POGPN as shown in Figure 8, where the values are standardized before training and then back-standardizedbefore prediction. The number of inducing locations for the \"Time\" node is initialized as the 256-time steps and is keptnon-learnable. For the fully observed dataset where F1, F2, F3, F4, F5, F6 and FZ are observed for 156 timesteps, numberof epochs = 300 and for the partially observed dataset of only F3, F4, F5 and F6 number of epochs = 150. For all kernels,squared exponential kernel and constant mean have been used, and the number of MC samples = 20. Latent functions"}]}