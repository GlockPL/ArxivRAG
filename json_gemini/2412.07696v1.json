{"title": "SimVS: Simulating World Inconsistencies for Robust View Synthesis", "authors": ["Alex Trevithick", "Roni Paiss", "Philipp Henzler", "Dor Verbin", "Rundi Wu", "Hadi Alzayer", "Ruiqi Gao", "Ben Poole", "Jonathan T. Barron", "Aleksander Holynski", "Ravi Ramamoorthi", "Pratul P. Srinivasan"], "abstract": "Novel-view synthesis techniques achieve impressive results for static scenes but struggle when faced with the inconsistencies inherent to casual capture settings: varying illumination, scene motion, and other unintended effects that are difficult to model explicitly. We present an approach for leveraging generative video models to simulate the inconsistencies in the world that can occur during capture. We use this process, along with existing multi-view datasets, to create synthetic data for training a multi-view harmonization network that is able to reconcile inconsistent observations into a consistent 3D scene. We demonstrate that our world-simulation strategy significantly outperforms traditional augmentation methods in handling real-world scene variations, thereby enabling highly accurate static 3D reconstructions in the presence of a variety of challenging inconsistencies.", "sections": [{"title": "1. Introduction", "content": "View synthesis, the task of creating images from unobserved camera viewpoints given a set of posed images, has seen remarkable progress in recent years. Current algorithms are able to render detailed photorealistic novel views of complicated 3D scenes. However, these techniques tend to assume that the provided input images are consistent that the geometry and illumination of the scene is static during capture. Typical captures of real-world scenes seldom obey this constraint; people and objects may move and deform, and lights may move or change brightness.\nMoreover, casual captures outside of tightly-controlled settings tend not only to be inconsistent but also sparse, containing only a small number of observed views. Methods for sparse view synthesis are usually trained on synthetic or captured multiview datasets that are consistent by design, and therefore fail to generalize to the inconsistencies seen in real-world casual captures (see Fig. 6 as an example)."}, {"title": "2. Related Work", "content": "We address the task of view synthesis from sparse and inconsistent images of a scene. While existing techniques address view synthesis from densely-sampled inconsistent inputs or sparse consistent inputs, to our knowledge no existing method is capable of synthesizing novel views of full scenes from images that are both sparse and inconsistent."}, {"title": "2.1. Robust view synthesis", "content": "Prior methods for robust view synthesis typically require dense captures with hundreds of images and focus on explicitly modeling a specific source of inconsistency (either motion or lighting) as part of reconstruction."}, {"title": "Scene dynamics", "content": "In the case of scene dynamics, existing methods start with a dense video and attempt to recover motion flows or trajectories to explain the observed motion. Early approaches based on Neural Radiance Fields [33] optimized time-varying flow fields to explain observed motion as deformations of an underlying consistent scene representation [12, 23, 34\u201336, 48]. Later NeRF-based methods improved quality further through prior integration [24, 29]. The most recent state-of-the-art methods have adopted 3D Gaussian representations and optimized explicit motion trajectories for this particle-based scene representation [21, 45, 51], often leveraging strong priors from pretrained monocular depth [13, 19, 59], optical flow [47], or tracking models [10, 18, 57]. These temporal priors have been crucial for rendering high-quality novel views in the dense capture setting, but tend to break down when applied to sparse or unordered captures."}, {"title": "Lighting inconsistencies", "content": "Existing structure-from-motion pipelines display remarkable robustness to lighting variation [37\u201339], enabling 3D reconstruction from large-scale in-the-wild images [1]. To model inconsistencies due to changing scene lighting, 3D reconstruction and view synthesis techniques use per-image \u201cappearance embeddings\" that allow for the appearance of scene content to vary across observations [8, 20, 31, 32, 53, 58, 61]. This strategy can successfully model lighting inconsistencies given dense captures with smoothly-varying appearance changes, but is unable to reconcile large changes in appearance in sparsely-sampled captures."}, {"title": "2.2. Sparse view synthesis", "content": "In novel view synthesis settings with only a few captured views, most methods rely on strong priors learned from large multiview datasets. Some methods train feedforward models to directly predict 3D representations that can be used for view synthesis [7, 16, 17, 49, 60, 64]. Others rely on pretrained monocular depth, multiview stereo, or inpainting networks and rely on test-time optimization to fit a scene [11, 42, 43, 50, 54]. A recent class of methods has achieved high visual quality by directly generating images from novel viewpoints using diffusion models conditioned on observed image(s) and target camera poses [15, 26, 27, 30, 41, 56]. In particular, the multiview"}, {"title": "3. Simulating World Inconsistencies with Video Models", "content": "Training a robust view synthesis model is challenging due to the lack of paired training data of inconsistent captures and target consistent images. Most existing multiview datasets only contain captures of consistent scenes, so simply scaling such data is not sufficient for robust view synthesis. Gathering images from multiple viewpoints, each under multiple scene deformations or lighting settings, would be extremely onerous. Heuristic data augmentation strategies such as random transformations, tints, and sparse flow fields cannot adequately capture the diversity and 3D nature of scene motions and lighting changes, as displayed in Fig. 2. Conversely, synthetic datasets like Objaverse [9] only contain simple object-level motion and fail to enable generalization to real-world scenes.\nThe key idea in our work is to leverage generative video models to create a robust view synthesis dataset from existing consistent multiview image datasets. For each 3D scene, we desire a dataset that contains (1) a set of consistent multiview images xi, (2) inconsistent images 2\u2081 where the scene has undergone some transformation such as a deformation or"}, {"title": "3.1. Video model augmentation", "content": "We propose to generate a realistic and diverse dataset of inconsistent conditioning images by simulating dynamic motion and lighting inconsistencies with pretrained image-to-video generative models. Starting with a multiview capture (taken from existing large-scale multiview image datasets), we first generate, for each view, a video from a static camera with simulated scene changes (motion or lighting). By sampling frames from these videos, we can obtain inconsistent observations for each captured viewpoint. Other image editing approaches such as InstructPix2Pix [6] could potentially be used to perform this inconsistency transformation, but these methods often fail to produce substantial variation in the layout of the image which are needed to simulate dynamic inconsistency.\nTo generate videos with simulated scene changes, we use an image- and text-conditioned video diffusion model that samples from p(v|I, c), where V is a video, I is a conditioning frame that V should include, and c is a text caption. By setting I to an image from a multiview capture xi and choosing c, we may simulate inconsistencies on top of the image. Note that the video must not contain camera motion in order to preserve the accuracy of existing camera parameters.\nWe simulate the two most prominent inconsistencies: dynamic motion and lighting changes. For dynamics, we use the Mannequin Challenge dataset [22]. This dataset is a natural choice as it includes static multiview captures of scenes with content that would typically be dynamic in casual captures. For our lighting-robust model, we simulate lighting changes on the RealEstate10k [63] dataset, which contains"}, {"title": "4. Harmonization through multiview diffusion", "content": "We use our multiview simulated world inconsistencies dataset (x, \u1fb6, \u03c0) to learn a generative model that can map from sparse inconsistent captures to a consistent set of images, as displayed in Figs. 4 and 5. We call this model a \"harmonization\" model as it brings the inconsistent input images into harmony."}, {"title": "4.1. Architecture", "content": "We build our harmonization model on top of CAT3D [15], a latent multiview diffusion model that directly predicts target images conditioned on posed input images and target camera poses. To incorporate inconsistent observed images as conditioning, we simply concatenate latents of the inconsistent images \u017ei = E(xi), encoded by the VAE encoder &, to the target raymaps and noisy latents. Additionally, we concatenate a binary image mask (either all ones or all zeros) to each input to denote the reference image, i.e., the \"desired state\" with which all other outputs should be consistent."}, {"title": "4.2. Training", "content": "Our goal is to learn a generative model that produces consistent output image sets with N images, given a reference image latent zo signifying the desired scene state and n \u2264 N observed inconsistent image latents \u017ei:\n$$P(z_{1:N} | z_0, z_{1:n}, \\pi_{0:N}).$$"}, {"title": "4.3. 3D reconstruction", "content": "Having trained the harmonization model, we can sample consistent latents 21:7 and decode them into images 21:7 with the VAE decoder (visualized in Figs. 4, 5, and 6). We then have a total of 8 consistent images: the initial observed target xo and model outputs 21:7. While 3D reconstruction from such a small image collection is infeasible, we can use multiview diffusion models trained on consistent images such as CAT3D [15] to \"densify\" the sparse consistent capture into a dense consistent capture with enough views to train a NeRF. Instead of directly sampling the original 3-image conditional CAT3D model, we finetune it to condition on 5-frames, finding that the additional context outperforms the"}, {"title": "5. Experiments", "content": "We evaluate our method for the two most common sources of inconsistency during casual multiview capture: scene dynamics and lighting changes."}, {"title": "5.1. Scene Dynamics", "content": "Dataset For scene dynamics, we evaluate our method on DyCheck [14], a dataset of 7 multiview videos where the assumed input is a monocular video with significant scene and camera motion. In this setting, we select 7 sparse frames uniformly in time as a consistent conditioning set, and uniformly select 4 target time images (top left of Fig. 4) per scene for which to compute metrics. Note that prior works which handle scene dynamics assume an ordered dense capture [21, 29, 51].\nBaselines Considering this task as view synthesis from 8 inputs, we compare our performance to the state-of-the-art method for sparse view synthesis, CAT3D [15]. We evaluate"}, {"title": "5.2. Lighting Changes", "content": "Dataset For lighting changes, we are not aware of an existing dataset of posed images that contains multiple illumination conditions and multiple \"ground truth\" images under a consistent lighting. Note that the widely-used Phototourism dataset [44] contains only one image under each lighting. We create a new dataset of real-world scenes captured under 3 separate lighting conditions. To construct this dataset, we take 3 monocular videos of a scene in 3 different lighting conditions, using approximately the same camera trajectory for each."}, {"title": "5.3. Ablations", "content": "In this section, we show the key contributions of our method by ablating important design decisions. Specifically, we demonstrate the importance of using our simulated inconsistency data by evaluating against heuristic augmentations and a synthetic data alternative. For dynamics, we evaluate against a warping-based heuristic augmentation where we apply sparse flow fields; an example can be seen in Fig. 2. Interestingly, we find that the resultant model simply copies all \"real-looking\u201d pixels, indicating that such warping does not adequately bridge the domain gap to real motions.\nWe also compare to the alternate approach of generating a synthetic training dataset by animating 40k+ Objaverse assets [9, 25] with associated motions. Due to the small motion magnitude and the domain gap from object-level renderings to real scene-level data, the method significantly underperforms. The quantitative ablation results can be seen in the"}, {"title": "6. Discussion", "content": "We have proposed SimVS, a method for high-quality 3D generation from casual captures even in the presence of severe illumination changes and significant scene motion. We believe this represents a step forward in simplifying the capture and creation of 3D scenes.\nLimitations Our method requires accurate camera poses, which can be difficult to compute for sparse captures with significant inconsistencies using traditional techniques such as COLMAP. However, recent methods such as DUST3R [52] and the dynamics-robust follow-up MonST3R [62] have shown tremendous promise for camera pose estimation. When there is very little overlap between views, our method can struggle to reconcile the given observations.\nConclusion Our work demonstrates the power of using video models to generate data for challenging tasks where collection is expensive and challenging. We believe the approach proposed here will scale well with the ever-improving quality of video models. Moreover, our method is not specific to a particular architecture or task: our method may be applied to make DUSt3R [52]-style models more robust and our harmonization network could be implemented with a camera-controlled video model to directly synthesize multiview-consistent videos in one sampling pass."}, {"title": "A. Supplemental Video", "content": "Alongside this PDF, we provide a supplemental video of video results, comparisons to baselines, and ablations. We highly encourage the reader to view the video supplement."}, {"title": "B. Evaluation Details", "content": "We use the pretrained CAT3D [4] model provided by the authors for the lighting benchmarks along with the default implementation of ZipNeRF with GLO [1]. For the dynamics benchmark, we use a CAT3D model finetuned to condition on 5 images and predict 3. The lower variance of the conditioning provides a slight benefit as seen in Tab. 1."}, {"title": "C. Comparison to Shape of Motion [6]", "content": "We include an additional baseline comparison to Shape of Motion [6], the current state-of-the-art method for 4D reconstruction. We consider this type of method to be slightly orthogonal to our approach; incorporating priors such as static masks and monocular depth may improve our results further.\nAs in our experiments in the main paper, we provide this baseline with a set of unordered sparse images from the DyCheck [3] dataset. We compare only on the scenes that Shape of Motion benchmarked, and therefore exclude Space-Out and Wheel.\nWe use the refined poses and aligned depth from the original paper and train the model to render the standard 360x480 images, center-cropped to a square aspect ratio as in the comparisons included in the main paper. As specified in their GitHub repository, we computed the video masks with Track Anything [7], which shows some robustness to the sparse inputs. However, TAPIR [2] seemed to struggle to compute reasonable tracks given sparse inputs. We show qualitative results in Fig. 1 and quantitative results in Tab. 2. Due to the inability of Shape of Motion to predict scene content outside of the frustums of the input images, we show results with covisibility masks as well. As evidenced by the metrics and qualitative results, Shape of Motion struggles to recover a cohesive representation under the sparse and unordered input setting of this paper."}, {"title": "D. Additional visualizations", "content": "We show the ability of our model to effectively and flexibly incorporate more information in Fig. 2, reducing the uncertainty in its prediction with larger context. We also show samples from the lighting dataset in Fig. 4. Due to privacy concerns, we do not show samples from the dynamics dataset, which consists of humans."}, {"title": "E. Training Details", "content": "We finetune the pretrained CAT3D [4] model with 0 initialization for the input conditioning convolution layer to accept the inconsistent latents z. We train with a batch size of 64"}, {"title": "F. Video Model Prompt Details", "content": "In this section, we specify the details of the prompting for the video model including the meta-prompt, example prompts, and list of prompts for lighting."}, {"title": "F.1. Lighting prompts", "content": "For lighting, we sample the prompts from the following set:\n1. \"a bright light casts shadows\"\n2. \"the light slowly dims from bright to dark\"\n3. \"an object flies around the room, casting hard shadows\"\n4. \"a transition from a bright day to a dark night\"\n5. \"the shadows and lights move\"\n6. \"a strobe light flashes\""}, {"title": "F.2. Dynamics prompts", "content": "For dynamics, we sample about 10k total prompts using the meta-prompt given in Fig. 3. We include 20 examples below:\n1. \"They walk quickly along the path, the child struggling to keep up while carrying the bottle.\"\n2. \"The boys playfully pose for a photo.\"\n3. \"The mechanics are actively repairing the car, with tools moving and parts being replaced.\"\n4. \"The girls are collaboratively typing on the laptop.\"\n5. \"The chef moves through the train serving food to passengers.\"\n6. \"Children run through the play tunnel and climb onto the boat.\"\n7. \"The children run around the line, crossing it repeatedly during the game.\"\n8. \"The girl walks past a classroom art display.\"\n9. \"Two people actively select books and papers from the table.\"\n10. \"The puppeteer manipulates the"}, {"title": "G. Details of Lumiere sampling", "content": "For sampling from the Lumiere model, we utilize a random-frame variant where the input frame can be anywhere in the video (not just the first frame). This variant is trained by sampling a random frame for each training video and concatenating the input to every frame along the channel dimension, identically as the Lumiere inpainting model.\nWe use the following camera-based negative prompt to induce the desired characteristics in the output video and alleviate Lumiere's tendency to output still videos:\nCnegative = \"frozen, photograph, fixed\nlighting, moving camera, zoom in,\nzoom out, bird view, panning view,\n360-degree shot, orbit shot,\narch shot\"\nWe use 250 DDPM sampling steps for the image- and text-conditioned Lumiere base model at a resolution of 128x128. We then upsample that video conditioned only the original prompt to a size of 1024x1024 with 250 sampling steps and resize to the desired size of 512x512. We set the guidance weight to 6 for both processes."}]}