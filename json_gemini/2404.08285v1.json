{"title": "A Survey of Neural Network Robustness Assessment in Image Recognition", "authors": ["Jie Wang", "Jun Ai", "Minyan Lu", "Haoran Su", "Dan Yu", "Yutao Zhang", "Junda Zhu", "Jingyu Liu"], "abstract": "In recent years, there has been significant attention given to the robustness assessment of neural networks. Robustness plays a critical role in ensuring reliable operation of artificial intelligence (AI) systems in complex and uncertain environments. Deep learning's robustness problem is particularly significant, highlighted by the discovery of adversarial attacks on image classification models. Researchers have dedicated efforts to evaluate robustness in diverse perturbation conditions for image recognition tasks. Robustness assessment encompasses two main techniques: robustness verification/ certification for deliberate adversarial attacks and robustness testing for random data corruptions. In this survey, we present a detailed examination of both adversarial robustness (AR) and corruption robustness (CR) in neural network assessment. Analyzing current research papers and standards, we provide an extensive overview of robustness assessment in image recognition. Three essential aspects are analyzed: concepts, metrics, and assessment methods. We investigate the perturbation metrics and range representations used to measure the degree of perturbations on images, as well as the robustness metrics specifically for the robustness conditions of classification models. The strengths and limitations of the existing methods are also discussed, and some potential directions for future research are provided.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep learning has achieved remarkable success in computer vision tasks such as image recognition, object detection, autonomous driving, and medical imaging analysis. However, as deep neural networks (DNNs) are increasingly deployed in safety- and security- critical applications, ensuring the quality and reliability of these models has become a pressing concern. Deep learning introduces new failure mechanisms and modes to traditional systems, presenting challenges in evaluating and assuring the quality of intelligent systems. In addition to failures caused by code defects, inadequate prediction and decision-making capabilities due to limited training data, pose significant constraints on their reliable operation in complex and dynamic real-world environments. This deviation between the training and operating environments, known as dataset shift, gives rise to the robustness problem in deep learning. Robustness has thus emerged as a crucial quality characteristic for trustworthy AI. It requires Al systems to exhibit robust behavior throughout their lifecycle, operating normally and posing no unreasonable safety risks under normal use, foreseeable use or misuse, or other unfavorable conditions [1]. In 2022, the ISO 25000 series, with the publication of ISO/IEC DIS 25059:2022 [2], recognizes the importance of robustness as a sub-characteristic of reliability in the quality model for Al systems. The nonlinear and nonconvex behavior of deep neural networks makes their robustness problem serious and difficult to evaluate. The presence of adversarial samples in image classification neural networks highlighted the vulnerability of deep learning models to small input perturbations, which can lead to significant output deviations [3]. In addition to adversarial attacks, neural network models also suffer from robustness problems when operating in natural environments, where there are various kinds of data corruption. A new research topic focuses on robustness assessment of neural networks in the face of real-world data corruption perturbations. This corruption robustness (CR) differs from earlier studies on adversarial robustness (AR) according to the definitions in [4]. The CR specifically refers to unintended changes in data. In the field of computer science, data corruption is \"errors in computer data that occur during writing, reading, storage, transmission, or processing, which introduce unintended changes to the original data. \" [5]. In the physical environment, typical data corruption perturbations for images include Gaussian blur, rain and snow-induced blur, brightness variations, spatial flipping, and signal transmission distortion.\nTwo main types of assessment methods are employed for evaluating DNN robustness: robustness verification and robustness testing. Robustness verification aims to determine if a network is robust within a specific perturbation range and has made significant progress through formal and statistical verification techniques. Robustness testing involves constructing test datasets with perturbations to evaluate the correctness of a model's output to these perturbed inputs. Current research on robustness testing primarily focuses on addressing issues such as test input generation, test oracle generation, and test adequacy analysis. Existing methods are primarily proposed for adversarial attacks and AR, aiming to identify the minimum perturbation degree that misleads the model output and serve it as the measurement of robustness. The assessment of CR often adopts a benchmark testing approach, which requires the construction of effective and widely recognized benchmarks for robustness evaluation.\nWhile some surveys [6, 7] provide reviews on the development and current state of AR, there is a need for a comprehensive summary that clarifies the relationship between AR, CR, and other existing robustness concepts, as well as the assessment methods and metrics for each type of robustness. We aim to bridge this gap by conducting a systematic analysis of the latest progress of neural network robustness assessment encompassing both AR and CR within the domain of image recognition. In total, more than 3,000 relevant documents were searched, with the time periods spanning from 1975 to 2023. Over 400 papers related to the topics of robustness verification, testing, and assessment were analyzed, including 30 review papers and 15 standards. The total number of references cited in this paper is 168. In this survey, we provide a comprehensive overview of the research conducted on the quantitative assessment of neural network robustness in terms of concepts, metrics, and assessment methods. Our work makes the following key contributions:\n\u2022 Robustness concepts. We conduct a detailed analysis of the robustness concepts defined for Al systems and neural networks as per existing standards and research papers. We examine the interplay between robustness and other essential AI quality characteristics. The definition of robustness for neural networks aligns with the system-level definition but more detailed. The existing concepts that describe or measure the robustness of neural networks from different perspectives is discussed. By organizing these concepts, we provide a structured framework for comprehending the various dimensions of robustness.\n\u2022 Robustness metrics. We summarize the metrics commonly employed to measure the robustness of neural networks within"}, {"title": "2 Background", "content": "To provide a more effective overview of neural network robustness assessment, we first outline the background of robustness assessment with the aim of clarifying the different research objects of Al robustness, establishing the concepts of system-level robustness and algorithm-level or model-level robustness, and thus better summarizing the terms and concepts used in robustness assessment."}, {"title": "3 Robustness Terms and Concepts", "content": "In the field of trustworthy AI, the definitions of robustness in current standards and reviews primarily pertain to AI systems, which refers to Al systems combining hardware and software or software subsystems implementing AI algorithms. The definition of robustness for algorithm-level Al models, such as neural network models, aligns with the system-level definition, but there are some specific robustness concepts. In Section 3.1, we provide a concise summary of the existing robustness definitions applicable to AI systems, and in Section 3.2, we conduct an analysis of the robustness terms and concepts relevant to neural network."}, {"title": "3.1 Concepts in AI System", "content": ""}, {"title": "3.1.1 Definition of Robustness", "content": "Robustness is an important property that has been extensively studied in the field of trustworthy AI. There is now a widely accepted definition of robustness, which refers to the ability of (degree to which) an AI system to maintain its level of performance under any circumstances (including external interference or harsh environmental conditions) [1, 2, 8, 13]. Such interferences or perturbations may arise in various system components, including data, learning program, and framework. Robustness to changes in input data is of great significance and has received the most attention. The concepts of robustness and security in Al systems are closely related and are often considered equivalent in existing research, particularly in the context of adversarial robustness.\nIn 2022, ISO/IEC DIS 25059:2022 [2] published, it extends SQuaRE (Systems and Software Quality Requirements and Evaluation) to Al systems and defines the quality model for Al systems based on the system/software quality model [17]. In this updated model, robustness is introduced as a new sub-characteristic of reliability. As defined in this standard, robustness refers to \"degree to which an Al system can maintain its level of performance under any circumstances.\" This definition is derived from ISO/IEC DIS 22989:2021[1]. Notably, the refinement from \"ability\" to \"degree\" emphasizes the quantitative aspect of robustness, reinforcing the requirement for a measurable level of robustness. The standard also provides some circumstances of robustness including:"}, {"title": "3.1.2 Robustness Related to Input Data Perturbations", "content": "According to the definitions above, Al system robustness encompasses a wide range of anomalies originating from various sources, including data, hardware, and operating environment. This is a broad definition of robustness. Existing researches primarily focus on abnormal conditions related to input data. These abnormalities may include unseen, biased, adversarial, or invalid data [2], as well as domain change of input [13, 14]. This case is a confined meaning of robustness. The robustness associated with abnormal input data is also referred to as out-of-distribution (OOD) [21-23] issue in many studies.\nISO/IEC TR 24028:2020 [8] highlights that robustness involves the capability of a system to handle unknown data and operate effectively in rapidly changing environment. ISO/IEC DIS 22989:2021 [1] and ISO/IEC TR 24029-1:2021 [13] define the terms typical data or typical inputs to refer to known inputs that the AI system has been trained on, robustness is associated with atypical data, one example is domain change. ISO/IEC DIS 24029-2:2022[14] defines domain as \"set of possible inputs to a neural network characterized by attributes of the environment\", and highlights that the domain reflects the limitations of current AI technology, i.e., neural networks can only achieve their goals on appropriate inputs, and robustness is closely related to the domain in which they operate. Domain change is also called dataset shift in some studies [4, 24].\nDIN SPEC 92001-2:2020 [4] introduces two types of robustness: adversarial robustness (AR), which addresses deliberate attacks through carefully crafted harmful inputs, known as adversarial samples, and data corruption robustness (CR), which focuses on handling differences between datasets during development and deployment phases, called data distributional shift or dataset shift. This standard also considers a specific aspect of data corruption robustness, namely spatial robustness, which pertains to the module's ability to handle geometric transformations like translation and rotation.\nSome examples of data anomalies or perturbation scenarios considered in existing studies on AI system robustness include:\n1) Change in the domain or dataset shift;\n2) Unseen data, which refers to data that is not present in the training set;"}, {"title": "3.1.3 Relationship between Robustness and Other Quality Characteristics", "content": ""}, {"title": "3.1.3.1 Relationship to Trustworthiness, Resilience and Reliability", "content": "ISO/IEC TR 24028:2020 [8] defines trustworthiness as the ability to meet stakeholders' expectations in a verifiable way and gives some characteristics of trustworthiness, such as reliability, availability, resilience, security, privacy, etc. While not explicitly including the robustness property, it is noted that robustness assurance is a part of AI trustworthiness verification. On the other hand, the standard also points out that robustness encompasses resilience, reliability and potentially more attributes, as related to proper operation of a system as intended by its developers. The same view in ISO/IEC DIS 22989:2021 [1] about the relationship between robustness, trustworthiness, resilience, and reliability. Zhang et al. [12] equate robustness with resilience in the context of machine learning (ML) systems.\nAccording to the Al quality model defined in ISO/IEC DIS 25059:2022 [2], robustness is a sub-characteristic of reliability, alongside with maturity, availability, fault tolerance, and recoverability. Robustness is a newly introduced sub-characteristic specifically for Al systems, while others are defined in the original system/software quality model. Reliability is related to the specified functions, conditions, and operating times. Considering the challenges of specifying the function of an Al system due to its uncertainty and complexity, it is reasonable to consider robustness as a sub-characteristic of reliability assurance. However, it should be noted that ISO/IEC TR 24028:2020 [8] and ISO/IEC DIS 22989:2021 [1] think that robustness should encompass reliability.\nIn summary, robustness is recognized as an attribute of AI trustworthiness. Trustworthiness is a broader concept, reliability, resilience, etc. are also included, which is consistent with traditional software or systems. However, the relationship between robustness, resilience, and reliability is still not fully understood. But it is clear that robustness plays an important role in ensuring Al reliability because of the complex operating environments and limited training data of models."}, {"title": "3.1.3.2 Relationship to Fault Tolerance", "content": "The AI quality model [2] considers robustness and fault tolerance as two sub-characteristics of reliability. ISO/IEC 25010:2011 [17] defines fault tolerance as the degree to which a system, product or component operates as intended despite the presence of hardware or software faults. ISO/IEC TR 24028:2020 [8] describes fault tolerance for Al systems as the ability to continue to operate when disruption, faults and failures occur within the system, potentially with degraded capabilities. In the field of software engineering, IEC 62628-2012 [25] refers to fault tolerance as a strategy for software fault control and trustworthiness realization, and is employed during design and implementation to ensure software trustworthiness.\nAccording to the definitions above, fault tolerance primarily deals with internal anomalies within a system, while the current definition of AI system robustness focuses on external disturbances or changes in environmental conditions, particularly regarding changes in data inputs during runtime. So one can distinguish between the two properties in terms of internal and external exceptions. However, a different view is shown in other studies. Barmer et al. [24] suggested that the robustness of an AI system is associated with model errors and unmodeled phenomena, which are internal exceptions due to the algorithm and training process. The definition of ML system robustness by Zhang et al. [12] supports this viewpoint, considering perturbations in various aspects including data, learning programs, and framework, involving both internal and external exceptions.\nFurther research is needed to clarify the relationship between robustness and fault tolerance in Al systems. Understanding the function of Al systems and effectively categorizing exceptions and perturbations is essential for drawing conclusive insights. Moreover, it is important to recognize the critical role of data in AI systems, with data being treated as a component within the system. The quality of data during the training phase, including both train set and test set, represents a significant risk resource for Al systems. Therefore,"}, {"title": "3.1.3.3 Relationship to Security and Safety", "content": "Barmer et al. [24] conducted a study on robust and secure AI, providing a clear distinction between the concepts of robustness and security in Al systems. Robustness is concerned with the ability to maintain correct outputs for unmodeled parts of the input space, particularly the perturbations due to changes (e.g., noise, sensor degradation, context shifts, etc.) in the operating environment or new environments. They categorized robustness into 1) robustness against model errors and 2) robustness against unmodeled phenomena. Security, on the other hand, addresses hazards from specific threat patterns, such as 1) intentional subversion and 2) forced failure, i.e., perturbations associated with malicious attacks, such as adversarial attacks.\nDIN SPEC 92001-2:2020 [4] further divides robustness into adversarial robustness (AR) and corruption robustness (CR). AR deals with scenarios involving active adversaries and requires continuous defense against attacks. CR is caused by natural factors. Regarding their relationship, AR is considered a security concern while CR is viewed as a safety issue.\nZhang et al. [12] defined security of ML systems as resilience against potential harm, danger, or loss made via manipulating or illegally accessing ML components, similar definitions by Barmer et al. [24] and DIN SPEC 92001-2:2020 [4]. They also note that robustness and security are closely related, as low robustness can make a system vulnerable to adversarial attacks or data poisoning. However, security encompassed more than just robustness and include aspects like model stealing or refining.\nCheng et al. [26] introduced four dependability metrics for neural networks, including robustness, interpretability, completeness, and correctness. Robustness addresses various effects such as distortion or adversarial perturbation, and is closely related to security. In neural network robustness assessment, the terms robustness and safety are often used interchangeably, where robustness boundary verification is also referred to as maximum safety radius verification [27-30]. Bu Lei et al. [31] also pointed out that robustness is a prerequisite for security.\nIn conclusion, while there are attempts to differentiate between robustness and security in standards, in current research and practice, they are often treated as equivalent concepts."}, {"title": "3.1.3.4 Relationship to Uncertainty", "content": "Chen et al. [32] conducted an empirical study that demonstrated a positive correlation between adversarial robustness and prediction uncertainty. They categorized uncertainty in deep learning into three types: model uncertainty, data uncertainty, and prediction uncertainty. Prediction uncertainty refers to the level of confidence exhibited by a model in its predictions, where lower confidence indicates higher prediction uncertainty. They explained that DNNs with deterministic classification boundaries are more susceptible to adversarial attacks. On the other hand, models with higher prediction uncertainty achieve a better balance by ensuring that the classification boundaries are maximally distant from each class of data, making the attack more difficult. Building upon this insight, robustness of a model can be enhanced by increasing the uncertainty of its predictions, and the accuracy can also be maintained."}, {"title": "3.1.3.5 Relationship to Functional Correctness", "content": "ISO/IEC DIS 25059:2022 [2] introduces a new sub-characteristic called functional adaptability in the AI quality model and modifies the definition of functional correctness. It is often difficult for Al systems to provide functional correctness because a certain error rate is allowed in their output. Functional adaptability refers to the ability of the system to adapt itself to a changing environment it is deployed in by learning from new training data, operational input data, and previous actions. As can be seen from the definitions, AI system robustness is also concerned with situations such as runtime input data and the results of previous behaviors and can reflect performance changes in these situations, functional adaptability highlights learning from these situations."}, {"title": "3.1.3.6 Relationship to Explainability", "content": "ISO/IEC 23894:2023 [19] discusses the relationship between explainability and robustness of Al systems. Explainability is defined as the property of an AI system that the important factors influencing a decision can be expressed in a way that humans can understand. Since the behavior of DNNs is difficult to understand, it is important to effectively express these factors to understand why the AI system makes certain decisions and whether it can make the right decision in all situations. The uncertainty and lack of explainability of AI behavior lead to risks that will affect many attributes of AI, such as robustness. The inherent difficulty in explaining the nonlinear nature of neural networks always leads to unexpected behavior, making it difficult to evaluate and analyze their robustness."}, {"title": "3.2 Concepts in Neural Network", "content": "The neural network model is a key component of Al systems, and the definition of its robustness follows the definition for Al system. However, analyzing the robustness of neural networks is challenging due to their complexity, which includes nonlinearity, nonconvexity, and limited interpretability. Researchers have dedicated considerable effort to studying the robustness of neural networks. As a result, a wide range of research findings has emerged, covering different concepts that describe or quantify the robustness of neural networks from multiple perspectives. To maintain symbol consistency throughout this survey, symbols referenced from various papers have been adjusted."}, {"title": "3.2.1 Local Robustness and Global Robustness", "content": "According to ISO/IEC DIS 24029-2:2022 [14], robustness properties can be categorized as either local or global. Local robustness refers to the model's ability to maintain its output within specific regions in the input space, while global robustness examines or measures the model's overall robustness across the entire input space."}, {"title": "3.2.1.1 Local Robustness", "content": "The present definitions and studies of local robustness primarily focused on a given test input. Specifically, local robustness is defined within the neighborhood of a sample, which refers to a range of variations or perturbations near the sample. To illustrate, let's consider an image correctly classified as a car. In this case, the local robustness can be defined such that any image generated by rotating the original image within a range of 5 degrees should also be classified as a car.\nIn the work by Leino et al. [36], local robustness is defined as: a neural network model $F$ is said to be $\\delta$-locally-robust at point $x$ if it makes the same prediction on all points in the $\\delta$-ball centered at $x$, which can be described as follows:\nLocal Robustness: If a neural network model $F$ is $\\delta$-locally-robust at a given point $x$ with respect to the distance metric $||\\cdot||$, it implies that for $\\forall x'$, the following condition holds true:\n$||x - x' || \\leq \\delta \\rightarrow F(x) = F(x')$\nThe same definition can also be found in the study conducted by Katz et al. [27]. They describe local robustness in the context of classification models and infinite norm. Specifically, a network is $\\delta$-locally-robust at input point $x$ if for every $x'$ such that $||x - x' || \\leq \\delta$, the network assigns the same label to $x$ and $x'$.\nZhang et al. [12] introduced the concept of \"Local Adversarial Robustness\" within the context of machine learning systems. This definition diverges from the definitions presented in the aforementioned papers by explicitly characterizing $x'$ as the test input generated by an adversarial perturbation to $x$. By incorporating this notion, the definition of local robustness encompasses local adversarial robustness as a distinct case that elucidates the manner or origin of $x'$ generation.\nIt is obvious that local robustness primarily pertains to the robustness around an input point. Many researches on evaluating neural network robustness has concentrated on local robustness."}, {"title": "3.2.1.2 Global Robustness", "content": "There is a lack of an agreed definition of global robustness and the meaning varies across different studies. Effective methods for analyzing, evaluating, and quantifying global robustness remain scarce. The following are some existing definitions of global robustness.\nKatz et al. [27] define local robustness as the measure for a specific input $x$, while global robustness applies to all inputs simultaneously. They analyze global robustness by comparing the outputs of input $x$ and its perturbed inputs $x'$ on two replicas of DNN, $N_1$ and $N_2$. They describe global robustness as follows: Let $x_1$ and $x_2$ denote separate input variables for $N_1$ and $N_2$, respectively, and $x_2$ represents an adversarial perturbation of $x_1$ that satisfies $||x_1 - x_2 ||_{\\infty}\\leq \\delta$. $N_1$ and $N_2$ assign output values $p_1$ and $p_2$ respectively. If $|p_1 - p_2| \\leq \\epsilon$ holds for every output, the network is $\\epsilon$-globally-robust. Zhang et al.[12] define global robustness as:\nGlobal Robustness 1: A neural network model $F$ is $\\epsilon$-globally-robust if for $\\forall x,x'$,\n$||x \u2212 x'||_p \\leq \\delta \\rightarrow |F(x) \u2013 F(x')| \\leq E$\nLeino et al. [36] argue that it is impossible to achieve local robustness at every single point simultaneously. Unless the model is entirely degenerate, there will always exist a point arbitrarily close to the decision boundary. To address this challenge, they propose the concept of global robustness by introducing an additional class denoted as \"$\\\\_$\", which signifies the classifier's refusal to provide a prediction and indicates a violation action. When a point is classified as $\\\\_$, it implies that it cannot be certified as globally robust. By introducing the $\\\\_$ class, the classifier ensures the existence of at least one partition of width $\\delta$ between any pair of regions with different predicted labels to be assigned as $\\.\\\\_$. In other words, no two points within a distance of $\\delta$ would be labeled with different non-$\\\\_$ classes. To define the relation \"$\\\\_$=\" based on the $\\\\_$ class, they denote $C_1 \\\\_= C_2$ as $C_1 =\\\\_$ or $C_2 =\\\\_$ or $C_1 = C_2$. This"}, {"title": "3.2.2 Adversarial Robustness, Corruption Robustness and Semantic Robustness", "content": "Adversarial robustness is a widely studied concept in neural network robustness. In many cases, neural network robustness assessment is conducted on adversarial examples generated by adversarial attack algorithms, and this evaluation result is called adversarial robustness [12]. It is generally accepted that the robustness against adversarial perturbations is closely related to their security [4, 12, 27-30]. The defense techniques [43-50] for enhancing adversarial robustness is also an important research topic.\nThe concept of corruption robustness (CR) is defined in DIN SPEC 92001-2:2020 [4] and contrasted with adversarial robustness (AR). AR refers to the ability of an AI module to cope with adversarial examples (or adversarial perturbations), while CR focuses on noisy signals or changes in the underlying data distribution. Within CR, there is a subcategory called spatial robustness, which pertains to robustness against geometric transformations like translation and rotation. These two concepts describe and categorize robustness in terms of the different ways in which data perturbations can be generated. AR focuses on carefully crafted adversarial inputs, considering an active adversary in an ongoing \"arms race\" between attacks and defense strategies. On the other hand, CR addresses issues arising from natural factors such as hardware deterioration or shifts in data distribution. Both AR and CR aim to ensure the model performance on inputs that are unlikely or out of the distribution of training data. Adversarial examples can be seen as an intentionally designed change to degrade the performance. While CR is optimization-free because there is no intentional design.\nThe term \"semantic perturbation\" has led to the concept of semantic robustness. Huang et al. [51] explored four semantic perturbations that significantly affect the performance of image recognition systems, including rotation, scaling, cropping, and tilting, and suggested that it is not necessary to verify the complete robustness of neural networks against small perturbations such as adversarial perturbations under the condition of limited resources, but focus on the robustness against specific perturbations under specific risk scenarios. Similarly, Mohapatra et al. [52] studied the verification of semantic robustness against the change on brightness, lightness, contrast, and rotation. Studies on semantic robustness shares similarities with CR but is not equivalent. Researchers added small perturbations on semantic features to analyze semantic robustness, referring to these perturbations as \"semantic adversarial attacks\". It mainly focuses on small perturbations on semantic feature rather than pixel modifications, which is different from traditional pixel-based adversarial attacks (such as FGSM, C&W, etc.), as studied in papers [53-58]. Some works also verified and estimated the semantic robustness bounds on semantic adversarial attacks and semantic feature-based metrics [53].\nOverall, these concepts of robustness are defined based on different perturbation sources or generation methods and are used to evaluate the resilience of neural networks against specific perturbations."}, {"title": "3.2.3 Pointwise Robustness", "content": "Bastani et al. [39] introduced the concept of \"pointwise robustness\". It used to determine the minimum perturbation needed to change the output of a neural network in the neighborhood of a given input point $x$, i.e., the closest adversarial samples to that point [3]. This adversarial perturbation refers to a small alteration in the input that does not significantly impact human judgment but does affect the predicted output of the neural network. Therefore, pointwise robustness is an approximation of local robustness bound, more precisely, an upper bound. Pointwise robustness is based on the definition of local robustness (denoted as $(x, \\delta)$-robust in the paper), as follows:\n$(x, \\delta)$-Robust: A neural network model $F$ is robust at $x$ if a small perturbation $\\delta$ to $x$ does not affect the assigned label. A neural network model $F$ is considered $(x, \\delta)$-robust if for $\\forall x'$ satisfies:\n$||x' - x|| \\leq \\delta, F(x') = F(x)$\nPointwise Robustness: The pointwise robustness $\\rho(F, x)$ of a neural network model $F$ at a specific input point $x$ is defined as the minimum value of $\\delta$ for which $F$ fails to satisfy the $(x, \\delta)$-robust property, i.e.:\n$\\rho(F,x) \\triangleq inf[\\delta \\geq 0|F \\text{ is not } (x, \\delta)\\text{-robust}]$\nThe concept of pointwise robustness emerges from a broader emphasis on determining the performance boundaries of a neural network in the context of robustness assessment. Adversarial attack algorithms and testing techniques can establish a lower bound on the magnitude of input perturbations that violate the robustness condition, while definition of the robustness boundary is formulated as an upper bound, indicating that any perturbation beyond this boundary range will lead to incorrect model outputs."}, {"title": "3.2.4 Robustness Bounds", "content": "Many researches have investigated the maximum robustness range of neural networks. This involves analyzing the maximum range of input perturbations that a model can tolerate while still producing correct outputs. Formal verification methods [27, 59-61], such as satisfiability modulo theory (SMT) and mixed-integer linear programming (MILP), are used to determine the robustness bound. Additionally, there are various approximation methods [62-66] to verify the robustness range, they provide an approximate lower bound. These methods aim to ensure that the model remains resilient to perturbations within this boundary but may not provide guarantees against perturbations outside the boundary.\nJi et al. [67] highlighted that the robustness boundary is for a specific sample, represents the maximum perturbation range that the sample can tolerate while still ensuring correct predictions by the model. In other words, within this boundary, the model's classification decision for the given sample remains unchanged. It can be seen that the robustness boundary is also a local robustness definition. They also provided a definition of the robustness boundary for deep learning classification models as follows:\nRobustness Bound: let the dimension of the input sample $x$ be $d$, the number of output categories be $K$, the neural network model be $F: R^d \\rightarrow R^K$, and the categories of the input sample be $c = \\text{argmax}F_i(x), j = 1,2, ..., K$. Under the assumption of the $l_p$ space, the model provides the $\\delta$-robustness guarantee to indicate that the model's classification decision for $x$ does not vary within $\\delta$-size around this sample $l_p$-space.\nDenoting the exact bound and lower bound of the maximum robustness range differs from the upper bound defined by pointwise robustness in section 3.2.3 [31, 67], and pointwise robustness can also be considered as a robustness bound. Differences and connections between the two concepts can be understood as follows:\n1) Robustness exact bounds and approximate lower bounds refer to the maximum provable robustness of a neural network. In contrast, pointwise robustness is an approach based on testing that can only provide guarantees that the model is not robust for perturbations exceeding a certain threshold.\n2) Pointwise robustness approximates the critical value from the region where the region where the adversarial attack algorithm is not robust. On the other hand, approximate verification of the maximum robust range aims to approximate the critical value from the region where the model remains robust. Therefore, pointwise robustness, based on adversarial attacks, represents the minimum attack radius rather than the maximum robust radius.\n3) The lower bound of robustness guarantees that the model remains robust within a specific region, while the upper bound of"}, {"title": "3.2.5 Probabilistic Robustness", "content": "Probabilistic robustness quantifies the level of robustness using a statistical approach. The concept was introduced by Mangal et al. [68], they argued that the traditional definition of robustness against adversarial inputs is a worst-case analysis and unlikely to be satisfied and verified by neural networks, so a probabilistic description of robustness is proposed. Unlike adversarial robustness, probabilistic robustness does not require the nearest counterexample of the input point. Instead, it calculates the probability that the model remains correct behavior within an acceptable range of perturbations. Their definition is as follows:\nProbabilistic Robustness 1: Let $F$ represent the function represented by the neural network. $F(x)$ denotes the output produced by the neural network for a given input $x$. Let $x$ and $x'$ be pairs of real inputs with a distance not exceeding $\\delta$. The symbol $||\\cdot||$ represents the distance metrics on the input and output spaces. $D$ denotes the input distribution, $k$ is a Lie constant, and $\\epsilon$ represents the specified robustness condition. If the following inequality holds:\n$P_{x,x'\\sim D}(||F(x') \u2013 F(x)|| \\leq k * ||x' \u2013 x|| | ||x' \u2013 x|| \\leq \\delta) \\geq 1 \u2212 \\epsilon$\nThen $F$ is considered to satisfy probabilistic robustness, and the robustness probability is $\\epsilon$.\nWebb et al. [69] also defined probabilistic robustness and explored approach to estimate it through statistical verification. Their definitions are presented below (with appropriate notation modifications for consistency within this paper):\nProbabilistic Robustness 2: The neural network $F$ produces its final output through the softmax layer function, denoted as $F(x) = \\text{softmax}(z(x))$. An input $x'$ is considered an adversarial sample if for $x' = x + d, \\text{argmax}_iz(x')_i \\neq \\text{argmax}_iz(x)_i$. Consequently, we define the difference function $s(x') = \\text{max}_{i\\neq c}(z(x')_i \u2013 z(x')_c)$, where $c$ represents the label of $x$. When $s(x') \\geq 0$, it indicates that the input is an adversarial sample (perturbed in a way that can be addressed by the method). To quantify the probability of model failure, we integrate over the probability of $s(x') \\geq 0$ with respect to the assumed input distribution. Formally, this can be expressed as follows:\n$[p,s] \\triangleq P_{x\\sim p(:)}(s(x') \\geq 0) = \\int_{X} I[s(x') \\geq 0]p(x)dx$\nProbabilistic robustness provides a broader definition of robustness. However, the accuracy of estimation depends on two factors: proper assumptions regarding the distribution of perturbations and the sufficient test cases."}, {"title": "3.2.6 Targeted Robustness", "content": "Targeted robustness is a concept specifically defined for classification tasks", "70": "introduced this notion to ensure that a classification model does not assign a given input to a given target label. The neural network attacks in the NIPS competition organized by Kaggle is also divided into two categories", "71": "as opposed to non-targeted attacks that aim for misclassification without specifying a particular target class [72", "92001-2": 2020, "4": "also defines targeted attacks as those where the adversary tries to produce inputs that force the output of the classification model to be a specific target class. Another relevant concept is top-k prediction robustness, which refers to the ability of a model to ensure that the correct category remains within the top-k predictions for perturbed inputs [73"}]}