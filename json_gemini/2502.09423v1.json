{"title": "Transformer-Enhanced Variational Autoencoder for Crystal Structure Prediction", "authors": ["Ziyi Chen", "Yang Yuan", "Siming Zheng", "Jialong Guo", "Sihan Liang", "Yangang Wang", "Zongguo Wang"], "abstract": "Crystal structure forms the foundation for under-\nstanding the physical and chemical properties\nof materials. Generative models have emerged\nas a new paradigm in crystal structure predic-\ntion(CSP), however, accurately capturing key\ncharacteristics of crystal structures, such as peri-\nodicity and symmetry, remains a significant chal-\nlenge. In this paper, we propose a Transformer-\nEnhanced Variational Autoencoder for Crystal\nStructure Prediction (TransVAE-CSP), who learns\nthe characteristic distribution space of stable ma-\nterials, enabling both the reconstruction and gen-\neration of crystal structures. TransVAE-CSP in-\ntegates adaptive distance expansion with irre-\nducible representation to effectively capture the\nperiodicity and symmetry of crystal structures,\nand the encoder is a transformer network based on\nan equivariant dot product attention mechanism.\nExperimental results on the carbon_24, perov_5,\nand mp-20 datasets demonstrate that TransVAE-\nCSP outperforms existing methods in structure\nreconstruction and generation tasks under various\nmodeling metrics, offering a powerful tool for\ncrystal structure design and optimization.", "sections": [{"title": "1. Introduction", "content": "Crystal structure prediction (CSP) methods play a crucial\nrole in the discovery of novel materials and remain a per-\nsistent challenge in condensed matter physics and materials\nchemistry. With the advancement of first-principles calcula-\ntions, such as Density Functional Theory (DFT)(Hohenberg\n& Kohn, 1964) and structure search algorithms, including\nsimulated annealing (Kirkpatrick et al., 1983) and genetic\nalgorithms(Wu et al., 2013), the precision and efficiency of\nCSP have improved significantly. These methods typically\ninvolve generating candidate crystal structures and perform-\ning DFT calculations to screen them. Notable examples\ninclude USPEX(Lyakhov et al., 2013) and AIRSS(Pickard\n& Needs, 2011). However, these approaches are computa-\ntionally intensive and time-consuming. Traditional compu-\ntationally driven CSP methods often lack effective learning\npriors and are generally limited to structural searches within\na single system. Furthermore, due to the inherent random-\nness of the search space, they require substantial compu-\ntational resources and are susceptible to getting trapped in\nlocal optima.\nWith the rapid advancement of deep learning technologies,\nCSP has progressed into a new phase of data-driven. Neural\nnetwork models now substitute DFT calculations to pre-\ndict material properties(Merchant et al., 2023), significantly\nenhancing the efficiency of structure searches. Generative\nmodels, including variational autoencoder(Kingma, 2013),\nadversarial generative networks(Goodfellow et al., 2014),\nand diffusion models(Ho et al., 2020) with prior knowledge,\nare increasingly used in crystal structure generation. These\nmodels are capable of learning the feature space distribution\nof sample crystal structures and directly sampling from this\nspace to generate crystal structures that adhere to the learned\ndistribution. This approach significantly improves search\nefficiency compared to traditional CSP methods.\nCSP methods based on generative models have progres-\nsively evolved from utilizing composition information to di-\nrectly constructing complex crystal structures. Early works,\nsuch as MatGAN(Dan et al., 2020) and Binded-VAE(Oubari\net al., 2012), rely exclusively on crystal structure compo-\nsition information for training, capturing implicit rules of\nchemical composition. However, these models serve primar-\nily as reference models for structure generation and have\na limited capacity to produce diverse structures. More ad-\nvanced methods, including Composition-Conditioned Crys-\ntal GAN(Kim et al., 2020) and CCDCGAN(Long et al.,\n2021), have made significant progress in feature representa-\ntion, but they still constrain the crystal structure space and\nare limited to specific compound groups. In contrast, works\nfor complex systems, such as CDVAE(Xie et al., 2021),\nDiffCSP(Jiao et al., 2023), UniMat(Yang et al., 2024), and\nMatterGen(Zeni et al., 2025), have achieved crystal structure\ngeneration without being restricted by representation space,\naligning with the primary research goals in materials dis-\ncovery. Therefore, optimizing existing model frameworks\nand exploring new approaches to continuously enhance both\nthe efficiency and quality of structure generation represent\nkey directions for advancing the application of artificial\nintelligence in materials science.\nThe development of CSP methods highlights that the ac-\ncuracy of algorithms and models for crystal structure char-\nacterization is a critical factor influencing the stability and\nprecision of generated structures. This accuracy remains\na significant challenge in generative models for CSP. In\nparticular, capturing the periodicity, symmetry, and other\nfundamental characteristics of crystal structures continues\nto be a major obstacle in CSP methodologies. To address\nthese challenges, we propose a Transformer-Enhanced Vari-\national Autoencoder (TransVAE-CSP) for crystal structure\nprediction, as shown in Fig. 1. This model facilitates the\naccurate reconstruction and generation of crystal structures.\nThe primary contributions of this work are as follows:\n\u2022 We developed an adaptive distance expansion method\nto characterize crystal structures and introduced an\ninnovative hybrid function method to optimize a single"}, {"title": "2. Related Work", "content": "In this section, we will introduce the primary challenges\nencountered by CSP methods (Section 2.1), commonly used\ncrystal structure characterization techniques (Section 2.2),\nencoding network models (Section 2.3) and crystal genera-\ntion models (Section 2.4)."}, {"title": "2.1. Problem Formulation", "content": "A crystal structure consists of atoms interacting with each\nother, where each atom is characterized by its element type\nand coordinates. The goal of learning its representation is to\ntrain an encoder that accurately maps the crystal structure\ninto a representation within a sampling space. To preserve\nthe properties of the crystal structure after transformations,\nthe structure must maintain translation, rotation, inversion,\nand permutation invariance(Keating, 1966; Lin et al., 2022).\nAmong these, translation, rotation, and inversion operations\nform E(3) symmetry, while translation and rotation oper-\nations form SE(3) symmetry. However, current models\noften struggle to effectively capture key features of crystal\nstructures like symmetry and periodicity, particularly global\nsymmetry, which can result in generated crystal structures\nfailing to retain correct physical properties. Effectively\nincorporating these symmetries into material structure repre-\nsentation and neural networks has emerged as a significant\nchallenge in crystal structure prediction. This issue also\nrepresents a classic problem in structural encoding."}, {"title": "2.2. Crystal Representation", "content": "The crystal structure representation methods form the foun-\ndation for studying the relationship between crystal prop-\nerties and structures. Previous representation techniques,\nsuch as those based on simple matrices(Oubari et al., 2012;\nDan et al., 2020; Kim et al., 2020; Long et al., 2021), are\ninadequate for representing complex systems. The advent\nof graph representation methods has significantly improved\nthe ability of models to learn crystal properties(Xie & Gross-\nman, 2018; Sch\u00fctt et al., 2018; Chen et al., 2019; Chen &\nOng, 2022; Yuan et al., 2024). Notably, the incorporation\nof information regarding three-body interactions has further\nimproved the predictive power of these models."}, {"title": "2.3. Equivariant Network", "content": "To facilitate the representation of crystal structure symme-\ntries in generative network models, e3nn (Geiger et al.,\n2022), based on previous work(Thomas et al., 2018; Weiler\net al., 2018), is specifically designed to handle the symmetry\nof E(3). E3nn simplifies the process of constructing and\ntraining neural networks with these characteristics.\nEquivariant networks(Kondor et al., 2018; Unke et al., 2021;\nTh\u00f6lke & Fabritiis, 2022; Brandstetter et al., 2022) uti-\nlize geometric functions constructed from spherical har-\nmonics and irreducible features to implement 3D rotation\nand translation equivariance, as proposed in Tensor Field\nNetworks (TFN)(Thomas et al., 2018). The SE(3) Trans-\nformer(Fuchs et al., 2020) employs equivariant dot prod-\nuct (DP) attention(Vaswani, 2017) with linear messages.\nEquiformer (Liao & Smidt, 2023) integrates MLP atten-"}, {"title": "2.4. Crystal Generative Model", "content": "The generative model paradigm has been extensively uti-\nlized to generate material structures, including VAEs(Oubari\net al., 2012; Noh et al., 2019; Hoffmann et al., 2019;\nRen et al., 2020; Court et al., 2020; Xie et al., 2021),\nGANS(Nouira et al., 2018; Dan et al., 2020; Kim et al.,\n2020; Long et al., 2021), and diffusion models(Jiao et al.,\n2023; Yang et al., 2024; Zeni et al., 2025). Most of these\nstudies focus on binary compounds(Noh et al., 2019; Long\net al., 2021), ternary compounds(Nouira et al., 2018; Kim\net al., 2020), or simpler materials within the cubic sys-\ntem(Hoffmann et al., 2019; Court et al., 2020). Xie et al.(Xie\net al., 2021) incorporated the diffusion concept into the de-\ncoder component of the VAE, addressing the challenge of\npredicting material coordinates, while also retaining the\ncrystal reconstruction capabilities of the VAE. The diffusion\nmodel(Jiao et al., 2023; Yang et al., 2024; Zeni et al., 2025)\nfacilitates applications in more complex systems, such as the\nMaterials Project dataset(Jain et al., 2013), and is capable\nof performing conditional generation tasks."}, {"title": "3. Methods", "content": "In this section, we will provide a comprehensive introduc-\ntion to the TransVAE-CSP that we developed. As illustrated\nin Fig. 2, the model consists of two primary components:\ntraining and generation. It uses well-established and highly\naccurate CDVAE. Building upon this work, we have im-\nplemented a new encoder network to train a high-quality\nlearning model.\nDuring the training process, TransVAE-CSP focuses primar-\nily on optimization of the crystal structure representation\nand the encoder neural network. In Sections 3.1 and 3.2,\nwe will elucidate the design concepts and work principles\nof our proposed adaptive distance expansion method for\ncrystal structure representation, as well as the Transformer\nnetwork encoder that utilizes the equivariant dot product\nattention mechanism. We provide a detailed explanation of\nhow to capture the periodicity and symmetry characteristics\nof crystal structures, thereby enabling effective encoding of\nthe crystal structure. Finally, Section 3.3 will present a com-\nprehensive overview of the module design and workflow of\nTrans VAE-CSP."}, {"title": "3.1. Adaptive Distance Expansion", "content": "The distance between atoms in a crystal structure directly\ninfluences its stability and physical properties. Employing\nmathematical methods or functions to describe the relation-\nships between atomic distances is a common approach in"}, {"title": "3.2. Equivariant Dot Product Attention Network", "content": "The encoder utilizes an adapted SE(3)-Transformer net-\nwork (Fuchs et al., 2020), with the core structure being the\nequivariant dot product attention network (Liao & Smidt,\n2023), as shown in Fig. 3. Node i serves as the center,\nand the purpose of this network is to analyze the neigh-\nboring environmental information of the central node to\nobtain its higher-order features. $X_i$ outputs the query $q_i$\nthrough a linear layer (Liao & Smidt, 2023). The features\nof $X_i$ and its neighbor node $X_j$ are combined after passing\nthrough linear layers to obtain the initial message:$x_{ij} =$\n$Lineardist (x_i) + Linearsrc(x_j)$. Then, $X_{ij}$ and its distance\nexpansion $r_{ij}$ are processed via the DTP network (Howard,\n2017):\n$f_{ij} = Linear ( \\psi(\\lVert r_{ij} \\rVert) \\bigoplus [SH(\\gamma_{ij}), RBF_{ij}]) $ (1)\nSH is the spherical harmonic embedding, $w(||r_{ij}||)$ is the\nweight of the Tensor Product. $F_{ij}$ represents the fused\nfeatures of the target node and the source node, and is used to\nderive the attention weights and messages. We split $f_{ij}$ into\ntwo incompatible features, key $k_{ij}$ and value $V_{ij}$. Then, we\nperform a scaled dot product between $q_i$ and $k_{ij}$ (Vaswani\net al., 2017) to obtain the attention weights. Finally, the\nweights and messages are multiplied, and the higher-order\nfeature messages of all target nodes are obtained through\nReshape and aggregation functions, as well as linear layers:\n$Y_i = Linear (\\sum \\alpha_{ij} \\bigoplus V_{ij}) $ (2)\nThe scatter operation performs a summation aggregation on\nthe messages calculated for the same target atom and all\nits neighbors according to the index, in order to obtain the\nenvironmental feature message for each atom."}, {"title": "3.3. Overall of Architecture", "content": "Figure 2 shows the framework of TransVAE-CSP. Building\non the work of CDVAE (Xie et al., 2021), our research\nfocuses on optimizing crystal structure representation and\nthe encoder network.\nRepresentation Any crystal structure is an infinite rep-\netition of a unit cell in 3D space. A unit cell can be\nrepresented as follows, where A represents the type of\natoms, N represents the number of atoms, X represents\nthe atomic coordinates, and L represents the lattice pa-\nrameters. If the matrix N contains elements in R, let A =\n($a_0, a_1,..., a_{n\u22121}$) \u2208 $E^N$ represent the set of all elements\nin the matrix. Let X = ($x_0, X_1,...,X_{m\u22121}$) \u2208 $R^{N\u00d73}$ and\nL = (a, b, c, d, e) \u2208 $R^6$.\nEmbedding This module is adapted from(Liao & Smidt,\n2023), as shown in Fig. 4, and consists of Atom Embedding\nand Edge Degree Embedding. Atom Embedding uses a\nlinear layer to one-hot encode the atom type. Edge Degree\nEmbedding first expands the RBF features (marked in red)\nof the constant vector, interaction information, and atom\npair distance. It then encodes the local geometric message\nthrough two linear layers and an intermediate DTP layer,\nfollowed by aggregation to encode the message. The form\nof the DTP layer is the same as in formula 1. Finally, the\naggregated features are scaled by dividing by the square\nroot of the average degree in the training set (the calculation\nmethod is provided in Appendix B.1) to ensure that the\nstandard deviation of the aggregated features is close to\n1. After adding the Atom Embedding and Edge Degree"}, {"title": "4. Results", "content": "We evaluated the effectiveness of the fusion model in various\ntasks related to generating crystal structures and used the\ntest standards from Xie et al. (Xie et al., 2021) to compare\nthe results. We assessed our work in 4.1 crystal structure\nreconstruction and 4.2 crystal structure generation. Addi-\ntionally, we demonstrate in 4.3 that the radial basis function\nexhibits varying adaptability across different datasets. The\ndatasets employed in the experiment consist of three high-\nquality datasets of differing complexities. Perov-5 (Castelli\net al., 2012a;b) focuses on perovskite materials and contains\n18,928 perovskite materials with similar structures but dif-\nferent elemental compositions, including 56 elements, with\neach structure containing 5 atoms per unit cell. Carbon-\n24 (Pickard, 2020) focuses on carbon-based materials and\ncontains 10,153 carbon materials, which consist solely of\ncarbon elements, with each unit cell containing between\n6 and 24 atoms. MP-20 selects 45,231 stable inorganic\nmaterials from Materials Project (Jain et al., 2013), which\nincludes most experimentally generated materials with up\nto 20 atoms per unit cell. We applied a 60-20-20 split in\naccordance with Xie et al. (Xie et al., 2021)."}, {"title": "4.1. Crystal Structure Reconstruction", "content": "In the crystal reconstruction task, we compared our model\nwith three baselines from (Xie et al., 2021): FTCP (Ren\net al., 2022), Cond-DFC-VAE (Court et al., 2020), and CD-\nVAE (Xie et al., 2021). CDVAE encodes the atoms, atomic\npair distances, and angles of three atoms in the crystal struc-\nture. It predicts the composition, lattice, and number of\natoms from the latent variable space z, initializes a crystal\nstructure with random atomic types and coordinates, uses\nthe Langevin dynamics method to denoise the atomic types\nand coordinates, and finally generates a new structure."}, {"title": "4.1.2. METRICS", "content": "We follow the common practice (Xie et al., 2021; Jiao\net al., 2023) of predicting crystal structures by testing the\nlatent intermediate variables corresponding to all struc-\ntures in the test set. We use the StructureMatcher\nclass from Pymatgen, setting the thresholds to stol=0.5,\nangle_tol=10, and 1to1=0.3 to measure whether the\nreal structure matches the reconstructed structure, thus cal-\nculating the match rate to evaluate the reconstruction per-\nformance. The RMSE is calculated as the average over\nthe successfully matched reconstructed structures and real\nstructures, normalized by $ /\\sqrt{N}$, where V is the volume\nof the structure's unit cell and N is the number of atoms in\nthe unit cell."}, {"title": "4.1.3. RESULTS", "content": "The indicators of the reconstructed structure are presented\nin Table 1. Unlike previous studies, we adaptively employ\ndifferent radial basis functions (RBFs) for distance expan-\nsion across various data sets by adjusting coefficients during\nmodel training. We compare the training outcomes and au-\ntomatically select the radial basis function that demonstrates\nthe best performance for each data set. The optimal results\nare achieved using Gaussian, Hybrid, and Bessel RBFs for\nMP_20, Carbon_24, and Perov_5, respectively. This selec-\ntion is influenced by the structural complexity of the data set,\nthe long-range interactions between atoms, and the diver-\nsity of samples, as well as the adaptability of RBFs within\nthe atomic pair distance distribution space of different data\nsets. In terms of indicator performance, our model exhibits\na slightly lower Match Rate than FTCP for Perov_5 and a\nmarginally higher RMSE than CDVAE for MP_20. Other\nindicators surpass the baseline models to varying extents.\nFor detailed information on model training, please refer to\nAppendix B.2.\nThe comparison between the reconstructed structure and the\nreal structure is shown in Table 2. The observation direction\nof each pair of structures remains the same. Considering\nthat our model maintains the invariance of 3D space under\ntranslation, rotation, and periodicity, the visualization results\nof atoms at corresponding symmetric points in the recon-\nstructed and real structures may differ due to their different\norigin positions. However, by applying lattice symmetry\noperations (such as translation), the alignment of atomic\npositions within the unit cell can be achieved."}, {"title": "4.2. Crystal Structure Generation", "content": "For structure generation, we adopt ab initio crystal gen-\neration. We compare our model with six baselines (Jiao\net al., 2023): FTCP (Ren et al., 2022), Cond-DFC-VAE\n(Court et al., 2020), G-SchNet, P-G-SchNet (Gebauer et al.,\n2019), CDVAE (Xie et al., 2021), and DiffCSP (Jiao et al.,\n2023). DiffCSP requires the input of specified atom types\nand the number of atoms to complete the generation task.\nFor consistency, we sample the number of atoms from the\npre-computed distribution in the training set (Hoogeboom\net al., 2022), allowing DiffCSP to achieve the same ab initio\ncrystal generation as the other works."}, {"title": "4.2.2. METRICS", "content": "Similar to (Xie et al., 2021), we employ three metrics to eval-\nuate generation performance: Validity (including Structure\nValidity and Composition Validity), Coverage (comprising\nCoverage Recall and Coverage Precision), and Property\nStatistics. These metrics assess the validity of the predicted\ncrystal, the similarity between the test set and the generated\nsamples, and the distribution distance of property calcula-"}, {"title": "4.3. Comparison of RBF Strategies", "content": "We explored the impact of radial basis function expansion\nmethods for encoding interatomic distances in crystal struc-\nture characterization on the model. Here, we conducted\ncomparative tests on radial basis functions based on Gaus-\nsian functions, Bessel functions, and hybrid functions of the\ntwo on the mp_20, carbon_24, and perov_5 datasets. The\ntest indicator is the convergence of the loss value over 200\niterations on the training set and the validation set during\ntraining. The results are shown in Appendix B.4. On the\nmp_20, carbon_24, and perov_5 datasets, the model opti-\nmization effects of the Gaussian, hybrid, and Bessel radial\nbasis function distance expansion methods are the best."}, {"title": "5. Conclusion and Outlook", "content": "In this work, we propose a crystal structure generation\nmethod based on the Transformer-Enhanced Variational\nAutoencoder for Crystal Structure Prediction (TransVAE-\nCSP). Through experiments, we demonstrate that the equiv-\nariant dot-product attention Transformer network outper-\nforms traditional Graph Convolutional Networks (GCNs)\nin understanding the chemical and physical properties of\ncrystal structures. We innovatively introduce an adaptive\ndistance expansion method, and the experimental results fur-\nther validate that this approach achieves the desired objec-\ntives, providing a new optimization perspective for crystal\nstructure representation. Moreover, compared to diffusion\nmodels, the Variational Autoencoder (VAE) shows unique\nadvantages in structure reconstruction and distribution con-\nsistency verification, highlighting its potential in crystal\nstructure generation.\nFuture work will optimize the VAE-based crystal structure\ngeneration model to match the diffusion models, while en-\nhancing the framework for efficient prediction of structures\nwith specified elemental compositions. This advancement\nwill drive crystal structure prediction technologies and en-"}, {"title": "A. Radial Basis Function (RBF)", "content": "The gaussian RBF formula is followed:\n$\\Phi_1(x) = exp( - \\frac{\\lVert x - c_i \\rVert ^2}{2\\sigma_i^2})$ (3)\nwhere $c_i$ is the center point, $\u03c3_i$ is the breadth of basis func-\ntions. The Gaussian RBF expression for mapping distances\ninto N-dimensional feature spaces is represented by Formula\n4, which requires initializing N parameters $c_i$ and $\u03c3_i$.\n$\\Phi(x) = [\\Phi_1(x), \\Phi_1(x), \\Phi_2(x), ..., \\Phi_N(x)]$ (4)"}, {"title": "A.2. Bessel RBF", "content": "Bessel RBF is typically constructed using Bessel function.\nThe formula is followed as: $f(x) = J_v(||x \u2212 c||)$, where\n$J_v(r)$ represents the v-th order Bessel function, where v is\ntypically a non-negative real number. The variable r is\ndefined as the Euclidean distance between the input vector x\nand the center point c, expressed as r = ||x - cll. The distance\nis transformed into a high-dimensional feature space using\nthe following formulation.\n$\\Phi(x) = [J_v(||x - c_1 ||) cos (w_1||x - c_1 ||),$\n$J_v(||x - c_2||) cos (w_2||x - c_2||),$\n$...,$\n$J_v(||x - c_n ||) cos (w_n ||x - c_n ||)]$ (5)\nAmong the formula, w denotes the frequency parameter,\nwhich is utilized to regulate the oscillation frequency of the\nfunction, while cos represents the cosine function."}, {"title": "B.1. Global Variables for Datasets", "content": "The encoder network must provide several statistics from\nthe sample dataset: the maximum number of nodes, the aver-\ngage number of nodes, and the average coordination number.\nThese statistics are essential for performing more accurate\nscaling operations across different sample spaces within\nthe network. The maximum number of nodes refers to the\nhighest number of atoms present in the unit cell across all\ncrystal structures in the statistical dataset. The average num-\nber of nodes is determined by calculating the total number\nof nodes for all structures in the sample dataset, expressed\nmathematically as $ \\frac{1}{N} \\sum M_{NUMnode}$, where $NUMnode$\ndenotes the number node of a structure in a unit cell, N\ndenotes the total number of all structures. The coordination\nnumber of each atom is computed using the CrystalNN\nclass from the Pymatgen library, which is a tool designed\nfor analyzing atomic neighbors in crystalline materials. The"}]}