{"title": "Building 6G Radio Foundation Models with Transformer Architectures", "authors": ["Ahmed Aboulfotouh", "Ashkan Eshaghbeigi", "Hatem Abou-Zeid"], "abstract": "Foundation deep learning (DL) models are general models, designed to learn general, robust and adaptable representations of their target modality, enabling finetuning across a range of downstream tasks. These models are pretrained on large, unlabeled datasets using self-supervised learning (SSL). Foundation models have demonstrated better generalization than traditional supervised approaches, a critical requirement for wireless communications where the dynamic environment demands model adaptability. In this work, we propose and demonstrate the effectiveness of a Vision Transformer (ViT) as a radio foundation model for spectrogram learning. We introduce a Masked Spectrogram Modeling (MSM) approach to pretrain the ViT in a self-supervised fashion. We evaluate the ViT-based foundation model on two downstream tasks: Channel State Information (CSI)-based Human Activity sensing and Spectrogram Segmentation. Experimental results demonstrate competitive performance to supervised training while generalizing across diverse domains. Notably, the pretrained ViT model outperforms a four-times larger model that is trained from scratch on the spectrogram segmentation task, while requiring significantly less training time, and achieves competitive performance on the CSI-based human activity sensing task. This work demonstrates the effectiveness of ViT with MSM for pretraining as a promising technique for scalable foundation model development in future 6G networks.", "sections": [{"title": "I. INTRODUCTION", "content": "Foundation models (FMs) are first trained on a large, often unlabeled dataset, allowing them to build broad, adaptable representations that can be finetuned for various downstream tasks. This initial pretraining stage is done using self-supervised learning (SSL), where the model learns underlying patterns and relationships within the data without relying on labeled examples [1]-[3]. The model ideally develops a robust understanding of its target modality, which, in our case, is radio spectrograms. In fields like computer vision and natural language processing, FMs have set new benchmarks [4]-[7], often surpassing supervised learning models, specifically designed for individual tasks. This is largely due to their ability to generalize: FMs learn flexible and transferable representations that make them better suited to handle variations in data, perform across diverse tasks, and adapt to new contexts. Generalization is especially valuable when labeled data is scarce, as foundation models can perform well with minimal additional labeled samples.\nDeep learning (DL) has demonstrated strong potential when applied to individual wireless tasks, including automatic modulation classification [8], channel estimation [9], constellation and waveform design [10], among others. However, these models are highly specialized, and there are concerns about their ability to generalize effectively in real-world scenarios. Wireless signals are subject to time-varying impairments, and the communication environment is constantly changing, which can degrade a DL model's performance if it fails to adapt. Introducing the concept of FMs for wireless can potentially overcome these limitations [11], [12].\nWe propose FMs for wireless signals as a solution to address these challenges. By capturing over-the-air radio signals and pretraining FMs through SSL, there is no need for labeled data. Additionally, these pretrained models can then serve as backbones for multiple tasks, reducing computational costs. Most importantly, FMs are expected to achieve better generalization by leveraging their broad, transferable representations, making them well-suited to handle diverse and dynamic wireless environments. The primary contributions of our paper are:\n\u2022 We propose and demonstrate the effectiveness of a Vision Transformer (ViT) as a radio foundation model for spectrogram learning. Adopting ViT as the FM offers enhanced flexibility, particularly in handling variable input sequences, and increased scalability, as training and evaluation can be parallelized. ViT also captures long-term dependencies through its attention mechanisms.\n\u2022 We introduce a Masked Spectrogram Modeling (MSM) approach to pretrain the ViT in a self-supervised fashion, and thoroughly evaluate key design considerations of the masking procedure and transformer size on performance.\n\u2022 By finetuning across two downstream tasks, we demonstrate that the ViT radio FM effectively learns features that generalize across diverse domains, achieving competitive\u2014or even superior\u2014performance with 4x smaller model sizes compared to baselines.\n\u2022 We demonstrate the effectiveness of the proposed foundation model by utilizing a real-world dataset that is captured over-the-air in a software-defined radio testbed. Upon acceptance, the datasets and code will be publicly available to encourage further research within the community on FM for wireless.\nThe remainder of the paper is structured as follows: Section II presents the datasets utilized for pretraining the foundation model, and for the CSI-based human activity sensing and spectrogram segmentation tasks. Section III outlines the ViT architecture and algorithm of the self-supervised foundation"}, {"title": "II. TESTBED AND DATASETS", "content": "We use three datasets in this paper. The first, the Real-time Radio Dataset (RRD), consists of over-the-air radio recordings captured in real-time with a software-defined radio (SDR) test bed built using PlutoSDRs. The second, the Human Sensing Dataset (HSD), utilizes Wi-Fi channel state information (CSI) to detect human activity in an indoor environment. The third dataset, the Segmentation Dataset (SD), simulates 5G New Radio (NR) and LTE transmissions in neighboring frequency bands."}, {"title": "A. Real-time Radio Dataset (RRD)", "content": "The RRD dataset consists of recordings of IQ samples, representing both in-phase (I) and quadrature (Q) components of the RF signal. Each recording is captured with a center frequency (ranging from 2.4 to 2.65 GHz), sampling frequency (between 10 MHz and 60 MHz), and duration, typically averaging around 100 ms. Data collection took place in downtown Toronto, Canada, resulting in 240 recordings, which cover approximately 24 seconds of RF activity. This dataset is used for initial model pretraining.\nSpectrogram Computation. We create spectrograms from IQ recordings through the following steps: 1) Divide each recording into non-overlapping 16 ms segments; 2) Compute the spectrogram for each segment using the short-time Fourier transform (STFT); 3) Resize each spectrogram to a 224 \u00d7 224 shape; 4) Convert the spectrogram to log scale; 5) Normalize and standardize using dataset-wide statistics.\nLearning performance is generally robust to these specific parameter choices, which are selected to balance computational efficiency and preserve information-rich content."}, {"title": "B. Human Activity CSI-Based Sensing Dataset (HSD)", "content": "The HSD dataset contains CSI measurements for six human activities: running, walking, falling, boxing, arm circling, and floor cleaning [13]. Each subject performs these activities between a pair of Wi-Fi access points, each equipped with three antennas. CSI is measured for each activity, across 114 subcarriers and 3 channels (one per antenna) over 2000 samples at a 500 Hz rate. Each recording is thus a 3D tensor of shape 3 \u00d7 114 \u00d7 2000, paired with its activity label. The CSI is processed by resizing each recording to a shape of 3x224x224. Then, each channel is normalized and standardized using dataset-wide statistics. A sample from each class of the dataset is illustrated in Figure 1 where the horizontal axis is time and the vertical axis is frequency."}, {"title": "C. NR-LTE Segmentation Dataset (SD)", "content": "The SD dataset is created by generating NR and LTE signals, each transmitted through its respective wireless channel in adjacent, non-overlapping bands. We use the Matlab Communication Toolbox for signal generation, following the guidelines in [14].\nA spectrogram of the NR-LTE signal mixture is computed and resized to 224 \u00d7 224. A corresponding label image is also created, marking NR signals as 1, LTE signals as 2, and noise as 0. For more details about data generation, refer to [15].\nA sample is illustrated in Figure 2 where the horizontal axis represents time and the vertical axis represents frequency."}, {"title": "III. VISION TRANSFORMER FOUNDATION MODEL FOR SPECTROGRAM LEARNING", "content": null}, {"title": "A. Masked Spectrogram Modeling (MSM)", "content": "We introduce the Masked Spectrogram Modeling (MSM) approach using Vision Transformers (ViT). In this method, we divide each spectrogram image into p\u00d7p patches and randomly sample a subset of these patches using a uniform distribution."}, {"title": "B. Spectrogram Masked ViT Autoencoder", "content": "As shown in Figure 3, we use an encoder-decoder architecture based on a ViT masked autoencoder [7], [16]. This design is asymmetric in several respects. The encoder processes the visible patches outputting feature tokens, and the decoder handles the feature and mask tokens. The decoder reconstructs the original spectrogram by attending to the feature tokens provided by the encoder."}, {"title": "Objective", "content": "We train the model in a self-supervised way to reconstruct the masked patches. The loss function LMSM of MSM task can be written as:\n\n$L_{MSM} = \\frac{1}{NM} \\sum_{n=1}^{N} \\sum_{ij} ||vec(x_{ij}^{(n)}) - vec(\\hat{x}_{ij}^{(n)})||_2^2 I_{mask} (n, i, j)$         (1)\n\nwhere N is the batch size, M is the total number of patches, $X_{ij}^{(n)} \\in \\mathbb{R}^{p \\times p}$ is the input patch at position (i, j) in sample n, and $\\hat{X}_{ij}^{(n)} \\in \\mathbb{R}^{p \\times p}$ denotes the reconstructed patch at position (i, j) for sample n. The vectorization operation vec flattens each patch into a vector, $|| \\cdot ||_2$ is the L2 norm and $I_{masked} (n, i, j)$ is an indicator function that outputs 1 if patch (i, j) in sample n was masked and 0 otherwise.\nThe encoder of the self-supervised pretrained ViT masked autoencoder serves as our radio foundation model which can be finetuned for downstream tasks. We finetune for two downstream tasks: CSI-based human activity sensing and spectrogram segmentation, introduced next."}, {"title": "C. CSI-based Human Activity Sensing", "content": "The task is to classify CSI measurements into one of six distinct human activity classes. We utilize the ViT encoder from the pretrained model as a feature extractor, adding a linear layer as a classification head on top. The ViT encoder is entirely frozen, only the linear classifier is finetuned on the dataset. The pretrained model was originally trained on single-channel spectrograms, whereas here the input is a three-channel tensor representing the CSI. To accommodate this difference, the positional embeddings are modified to align with the new input, while the remainder of the encoder remains unchanged.\nThe CSI data is divided into patches in the same manner as the spectrograms. The model outputs a softmax probability vector, and the loss function is the label smoothing cross-entropy,"}, {"title": null, "content": "defined as:\n\n$L_{HSD} = - \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{i=1}^{C} y_{i}^{(n)} \\cdot (1 - \\alpha) + \\frac{\\alpha}{C} \\cdot log(p_{i}^{(n)})$       (2)\n\nwhere N is the batch size, C = 6 is the number of classes, $y_{i}^{(n)} \\in [0, 1]$ is the true label for class i (either 0 or 1 for sample n), $\\hat{y}_{i}^{(n)} \\in [0, 1]$ is the model's predicted probability for class i, and $\\alpha \\in (0, 1)$ is the smoothing factor. Unlike traditional cross-entropy, label smoothing distributes a small probability to incorrect labels, preventing the model from becoming overly confident which enhances generalization. The degree of smoothing is determined by \u03b1."}, {"title": "D. Spectrogram Segmentation", "content": "The task is to segment the input spectrogram into three classes: noise, NR signal, and LTE signal. We use the pretrained ViT encoder as a feature extractor, adding two standard transformer decoder blocks on top as a segmentation head. The ViT encoder is kept frozen, and only the decoder is finetuned. Since the input is a spectrogram, no modifications are made to the positional embeddings. The model's output is a 3D tensor providing a probability distribution for each pixel in the segmented spectrogram. We use label smoothing cross-entropy as the loss function, which is defined as follows:\n\n$L_{SG} = - \\frac{1}{NM} \\sum_{n=1}^{N} \\sum_{k=1}^{C} \\sum_{ij} y_{ijk}^{(n)} \\cdot (1 - \\alpha) + \\frac{\\alpha}{C} \\cdot log(\\hat{y}_{ijk}^{(n)})$           (3)\n\nHere, M is the total number of pixels in the segmented image, C = 3 is the number of classes, $y_{ijk}^{(n)} \\in [0, 1]$ is the correct label at pixel (i, j) for class k in sample n, $\\hat{y}_{ijk}^{(n)} \\in [0, 1]$ is the predicted probability at pixel (i, j) for class k, and \u03b1 is the smoothing factor."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "We perform self-supervised pretraining with masking on the RRD dataset, then evaluate the learned representations by finetuning. For finetuning, the decoder is discarded, and the frozen ViT encoder serves as a feature extractor, with only the task-specific head updated. No masking is done during finetuning. Three models are pretrained: ViT-S (small), ViT-M (medium), and ViT-L (large). First, we evaluate the models' reconstruction performance across various masking ratios, followed by assessing generalization capabilities on the CSI Sensing and Segmentation datasets."}, {"title": "A. Reconstruction Performance", "content": "First, we showcase reconstruction examples for ViT-M from which it is clear that the model exhibits strong performance. To evaluate the reconstruction capability of the models, we need a robust reconstruction accuracy metric. Relying solely on visual comparison is not enough. Hence, we transform each spectrogram into a resource grid composed of resource blocks. average pooling is first applied without overlap between pooled patches (i.e., stride equal to the kernel size). A threshold, \u03b4, is then applied to the pooled grid to binarize it, designating vacant resource blocks as 0 and occupied ones as 1. The threshold \u03b4 is determined empirically by the formula:\n\n$\\delta = \\mu + 0.5 \\times \\sigma$      (4)\n\nwhere \u03bc and \u03c3 represent the mean and standard deviation of the spectrogram, respectively. We then evaluate the models' reconstruction performance across various masking ratios. Although higher masking ratios increase reconstruction difficulty, the model is expected avoid collapse. the ideal masking ratio for pretraining is around 70% to 80%."}, {"title": "B. Finetuning Performance", "content": "To evaluate finetuning performance on the HSD and SD classification and segmentation datasets, we use confusion matrices (per-class accuracy) and overall accuracy. We present finetuning results for the HSD dataset first, followed by the SD dataset. the pretrained ViT encoder as a feature extractor which is kept frozen, and finetune the task-specific head. The highest accuracy is achieved by ViT-M trained from scratch. We attribute this difference to the inherent distinctions between CSI data and spectrograms, suggesting that more extensive pretraining could reduce this gap. The primary source of accuracy differences lies in the"}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed ViT as a radio foundation model for spectrogram learning which offers superior modelling capabilities, support for variable-length input sequences and computational efficiency. We also introduce a Masked Spectrogram Modeling (MSM) approach to pretrain the ViT in a self-supervised fashion, and thoroughly evaluate the effects of masking ratios and transformer size on performance. Experimental results indicate that the ViT-based model generalizes well to unseen datasets, achieving comparable or superior performance to larger models trained from scratch, while utilizing fewer resources. Notably, the pretrained ViT model surpasses a four-times larger scratch-trained model on the spectrogram segmentation task and achieves competitive performance on the CSI-based human activity sensing task. We believe that this ViT-enabled MSM will enable scalable, large-scale pretraining, fostering the development of robust radio foundation models capable of generalizing across a wide range of tasks."}]}