{"title": "Combining Cognitive and Generative AI for Self-explanation in Interactive AI Agents", "authors": ["Shalini Sushri", "Rahul K. Dass", "Rhea Basappa", "Hong Lu", "Ashok K. Goel"], "abstract": "The Virtual Experimental Research Assistant (VERA) is an inquiry-based learning environment that empowers a learner to build conceptual models of complex ecological systems and experiment with agent-based simulations of the models. This study investigates the convergence of cognitive AI and generative AI for self-explanation in interactive AI agents such as VERA. From a cognitive AI viewpoint, we endow VERA with a functional model of its own design, knowledge, and reasoning represented in the Task-Method-Knowledge (TMK) language. From the perspective of generative AI, we use ChatGPT, LangChain, and Chain-of-Thought to answer user questions based on the VERA TMK model. Thus, we combine cognitive and generative AI to generate explanations about how VERA works and produces its answers. The preliminary evaluation of the generation of explanations in VERA on a bank of 66 questions derived from earlier work appears promising.", "sections": [{"title": "1 Introduction", "content": "Interactive AI agents with self-explanation capabilities foster understanding, transparency, and trust in users across a wide range of domains and applications [1, 2]. By self-explanation, we mean Interactive AI agents that can explain their reasoning and behaviors. By generating human-understandable explanations, self-explainable AI can enhance user learning and trust [3]. Studies have shown the benefits of self-explanation in multimedia learning environments, facilitating intrinsic motivation, visual processing, and learning outcomes [4]. Additionally, emerging methods leveraging situation awareness holds promise for generating explanations of autonomous agents' behaviors, ultimately improving trust and comprehension [5].\nThis research contributes to the goal of enhancing user trust and learning through self-explanation in the Virtual Experimental Research Assistant (VERA; [6, 7]), an interactive learning environment for inquiry-based learning. In this paper, we explore how VERA explains its internal workings to users, potentially fostering trust and enhancing the learning experience."}, {"title": "1.2 VERA: Inquiry-based Modeling", "content": "VERA (http://vera.cc.gatech.edu) is an interactive learning environment for supporting inquiry-based learning. It helps learners construct conceptual models of ecological systems and evaluate them through agent-based simulations. VERA is an AI agent because of three capabilities. First, it uses an ontology of the ecology domain in the representation and construction of conceptual models. Second, it automates the retrieval of species' and related ecological relations' information from the Smithsonian Institute's Encyclopedia of Life (EOL; eol.org), a comprehensive digital library of biodiversity [8], and automatically inserts parameter values for the agent-based simulations. Third, it automatically compiles the conceptual model into agent-based simulations in NetLogo [9]. Thus, this platform aligns with the research focus on self-explanation in educational AI assistants."}, {"title": "1.3 Cognitive and Generative AI Convergence", "content": "This research explores the potential of combining Cognitive AI and Generative AI approaches for self-explanation capabilities in VERA. Cognitive AI is centered around understanding human cognitive processes and developing cognitively-inspired AI agents, while Generative AI methods demonstrate powerful capabilities for various natural language processing tasks like entity recognition, intent classification, and question-answering based on a text corpus [10]."}, {"title": "2 Related Work", "content": "Early research on self-explanation in Interactive AI agents highlighted the importance of explicitly representing the agent's knowledge of its design [11, 12]. This explicit representation allows the generation of explanations about the tasks the agent performs, the domain knowledge it uses, and the methods it applies. This led to the questions of how to effectively identify, acquire, represent, store, access, and use this design knowledge for generating explanations in interactive agents [13, 14]. One solution lies in viewing the AI agent as an abstract device, equipping it with meta-knowledge about its design, and enabling it to introspect and generate explanations based on its understanding of its structure, behaviors and functions [12].\nThere has been ongoing research into an Interactive AI agent's ability to provide self-explanation [15, 16]. In prior work on the Skillsync project for skill-based linking employers and colleges preparing prospective employees [17], we used a Task-Method-Knowledge model of Skillsync to generate explanations of its reasoning and recommendations [18]. A Task-Method-Knowledge (TMK) model captures an agent's design, knowledge, and reasoning processes into a unified structured representation [19, 20].\nWith the rise of Large Language Models (LLMs) [21], Generative AI methods have been integrated to enhance self-explanation in Interactive AI agents. In previous work on the SAMI project on connecting online learners with one another [18, 22, 23], we integrated cognitive AI methods based on the TMK model of SAMI with generative AI methods to generate explanations of SAMI's reasoning and recommendations [24].\nWhile these bodies of work serve as the background and context for our work, in the next section we describe how our work makes a novel contribution to the literature through generation of self-explanations for VERA, an interactive agent that supports inquiry-based modeling in the domain of ecology. In Section 3, we first describe the TMK model of VERA as an interactive agent. We then combine this with generative AI methods to explore how VERA can introspect on its TMK self-model to provide reasoned explanations to a user's query about VERA's functioning."}, {"title": "3 Methodology", "content": "We present a novel approach to self-explanation in interactive agents such as VERA grounded in the agent's theory of its own mind. A theory of mind refers to an agent's capacity to ascribe mental states to others as well as to oneself. Here mental states refer to goals, desires, knowledge, beliefs, thoughts, emotions, etc. Recently theory of mind has emerged as a theoretical lens to understanding and designing human-AI interaction [25]."}, {"title": "3.1 Theoretical Foundations for self-explanations using TMK", "content": "We posit that if an interactive agent has theory of its own mind, then it can use the self-theory to explain its reasoning and how the reasoning led to specific decisions. We use Task-Method-Knowledge (TMK) models to capture elements of an interactive agent's theory of its mind. We view the AI agent as an abstract device. This device comprises a design with well-defined functions, constituent components with their own functionalities, and causal mechanisms that orchestrate these component functions to achieve the overall agent's goals. Here, hierarchy refers to the layered structure of the design, causality describes the cause-and-effect relationships between components and functions, and teleology signifies the inherent goal-oriented nature of the design. Notably, TMK offers a natural mapping between its functions and tasks, and between its methods and mechanisms, aligning seamlessly with the proposed view of an interactive agent as an abstract device."}, {"title": "3.2 Research Questions and Hypotheses", "content": "Based on this theoretical foundation, we formulate the following research questions (RQ) and corresponding research hypotheses (RH):\nRQ1: How may an IA introspect on its design and explain its functioning?\nRH1: By representing the design as a TMK model, the IA can introspect on its design and explain its own functioning.\nRQ2: How may an IA reflect on its design and explain its results for a given input instance?\nRH2: By processing through the TMK model, the IA can construct a derivational knowledge trace for the given instance and then generate an explanation by reflecting on the trace.\nIn the following two subsections, we provide insights to these RQs and RHs. First, from a cognitive AI perspective, we describe our approach for representing the interactive agent's design. Then, by leveraging methods from generative AI, we describe how an IA introspects over its design and produces explanations about its functioning. The implementation of cognitive and generative AI methods for self-explanations in"}, {"title": "3.3 Cognitive AI: TMK model of VERA", "content": "Ask-TMK's cognitive AI capabilities leverage VERA's Task Method Knowledge (TMK) representation-a comprehensive self-model encompassing goals, internal processes, states, concepts, relationships, and transitions. This teleological structure empowers Ask-TMK to actively monitor VERA's current state, reason about goal achievement, and systematically pinpoint the methods and concepts essential for fulfilling objectives [19].\nTo provide Ask-TMK with a structured knowledge representation of VERA, we manually constructed a TMK model an abstract description of VERA's design. \"TMK\" is an acronym for \"Task-Method-Knowledge\", three core aspects of any TMK model. They are as follows:\n\u2022 Task. This part of the TMK model refers to VERA's objectives, describing its aim, purpose, or the task being modeled. Tasks are expressed through the inputs (\"givens\") and the resultant outputs (\"makes\"). For instance, in Figure 1, we consider VERA's task of \"Finishing an Ecology Experiment\". As the input to this task, a VERA project must be created, and the subsequent output is a conceptual ecological model. TMK models are inherently hierarchical, meaning that top-level goals of VERA can be decomposed into sub-goals. As shown in Figure 1, VERA's top-level goal (highlighted in green) is to \"Finish an Ecology Experiment\". To accomplish this, depending on the context, there are two immediate subgoals (highlighted in yellow): \"Edit a (conceptual ecological) Model\" or \"Finish a Simulation\". For more details about how VERA works, see our previous work [6, 7].\n\u2022 Method. This module of the TMK model describes how VERA accomplishes its Task. Methods are normally described by deterministic finite state machines (FSM) which in turn are defined by a set of states and transitions, see Figure 1 (highlighted in purple). Similar to tasks, methods are also hierarchical. Therefore, top-level methods can be broken down into submethods.\n\u2022 Knowledge. This final module of the TMK model corresponds to the definitions of the concepts and logical expressions used to specify the Tasks and Methods. This includes normal first-order logic operations and relations to connect with user supplied values [19, 20].\nUsing VERA's software documentation, a TMK model was manually created by core developers. The amount of effort required to produce a TMK model is dependent on the level of abstraction to model the interactive agent. Initially TMK models are designed using a symbolic representation (see 1) and subsequently manually converted to a JSON representation. Subsequent explanation generation utilizes these pre-built modules, resulting in a fully automated workflow. To further streamline this process and reduce upfront investment, we plan to explore utilizing off-the-shelf soft-"}, {"title": "3.4 Generative AI for VERA self-explanations", "content": ""}, {"title": "3.4.1 ChatGPT, LangChain, and Chain-of-Thought", "content": "We provide an overview of several Generative AI methods employed within Ask-TMK. We focus on three key components: ChatGPT [26], LangChain [27], and Chain-of-Thought [28], highlighting their roles in generating user explanations based on VERA'S TMK model. We then go through a working example in Section 3.5.1.\nAsk-TMK leverages ChatGPT, specifically GPT-3.5 Turbo, to generate natural language explanations for users. Upon receiving a user question, Ask-TMK utilizes the Large Language Model (LLM) to search and retrieve the relevant TMK documents. Similar to prior work [24], we use LangChain to create prompts that guide the LLM towards generating informative explanations. Using the process of iterative refinement [29], LangChain introspects over relevant documents from VERA'S TMK model to answer user queries.\nAsk-TMK leverages Chain-of-Thought to generate explanations with reasoning, for \"methods\" specific questions. Chain-of-Thought is a reasoning technique that enables the LLM to explicitly reveal the steps it undergoes when arriving at an answer [28]. Ask-TMK integrates Chain-of-Thought during the reasoning stage by employing LangChain to construct prompts that guide the LLM to break down complex methods within the TMK model into subtasks and submethods."}, {"title": "3.4.2 Experimental Setup", "content": "The experimental setup involved configuring the GPT-3.5 Turbo model to generate responses, with constraints to ensure deterministic output. Specifically, the responses were limited to a maximum of 1920 tokens, the temperature was set to 0, and verbose mode was disabled. For document retrieval, a FAISS-based search system [30] was employed, configured with a k-value of 4 to return the top four most relevant documents. Document embeddings were created using OpenAIEmbeddings, and the search space comprised documents categorized as Task, Method, or Knowledge. The k-value [30] refers to the number of nearest neighbors considered in a k-nearest-neighbor search, which is a common operation in similarity search algorithms. Memory augmentation was achieved by incorporating the \"software_qa_prompt\" to facilitate the recall of previously presented information. Lastly, as input to Ask-TMK, the self-explanation module received a \"question\" variable as input to generate its responses."}, {"title": "3.5 Combining Cognitive and Generative AI", "content": "Inspired by prior work [24], we have chosen to benchmark VERA's self-explanation system using a bank of 66 questions that aim to test our research questions and hypotheses in Section 3.2"}, {"title": "3.5.1 How does combining Cognitive and Generative AI generate explanations?", "content": "We demonstrate how VERA's innovative self-explanation system integrates Cognitive AI with Generative AI to produce detailed explanations. Figure 2 depicts the collaborative operation of these two fundamental AI paradigms within the system. Cognitive AI plays a pivotal role in the initial phases of query processing, facilitating the structured identification of the pertinent TMK modules by enabling a teleological structure and organization of VERA's self-model as briefly outlined in section 3.3\nGenerative AI takes on a prominent role during the subsequent stage of explanation generation. Here, it utilizes the retrieved TMK components, potentially refining them to better suit the user's query context. This refined knowledge is then employed by the system's Large Language Model (LLM) component to generate a coherent and contextually appropriate explanation tailored to the user's needs.\nThus, this combination ensures that explanations are both accurate and contextually relevant, enhancing the user's understanding of complex queries. The detailed explanation of each stage and how they interact is in Section 3.5.2. Additionally, we go over a working example with a question taken from our bank."}, {"title": "3.5.2 A Working Example", "content": "We walk through an example question here taken from our bank of 66 questions. Consider the following scenario:\nUser question: \"How can I best utilise the output of the system in VERA?\"\n1. Stage 1: Question Classification\nThis stage is responsible for categorizing user questions to determine their relevance to VERA's internal model (TMK) and allocate resources efficiently for response generation. It operates as follows:\n\u2022 Input: The user question serves as input to a classifier powered by LangChain. This classifier uses pre-defined classes (outlined below) to categorize questions and identify the most relevant parts of TMK for answering.\n\u2022 Classification Process: The classifier, utilizing GPT-3.5 Turbo, distinguishes question types and retrieves relevant models and corresponding documents based on tasks, methods, or knowledge within TMK.\n\u2022 Class Utilization:\n Mmodel Class: This class, used for 'Method' related questions, employs Chain-of-Thought Prompting during later stages to fetch relevant tasks and corresponding methods. It focuses on presenting intermediate steps within TMK, making it suitable for 'How' questions.\nMultimodels Class: Handling all other question types, this class retrieves all relevant TMK documents without utilizing Chain-of-Thought during later stages. It aims to provide comprehensive responses covering various aspects of TMK.\nCant_answer Class: Dedicated to cases where the system cannot answer a question, this class ensures efficient resource allocation by redirecting such queries appropriately.\n\u2022 Based on this classification, the system determines which information from the TMK to provide to the next stages. Further, by tailoring response generation based on the specific information needs of each question type, this approach optimizes resource utilization and enhances the relevance and accuracy of responses.\n\u2022 Output of this stage for our working example: In this case, it classifies the question as \"Multimodels\" and loads all the parts of the TMK. If a question is classified a \"Mmodels\", only Task and Method parts of the TMK model is loaded:\nPre-defined Class identified \"Multimodels\"\nMethod names: Loads various methods such as \"create simulation\", \"run simulation\", etc.\nTask names: Loads tasks like \"finish ecology experiment\", \"create simulation\", etc.\nKnowledge names: Loads knowledge names such as \"Ecology Model\", \"VERA\", etc.\n2. Stage 2: Localization"}, {"title": "\u2022 Input", "content": "This stage receives the classified question and the complexity factor, 'k'-value from the previous stage. Please see 3.4.2 for more details on k-value.\n\u2022 The complexity factor influences the level of detail required in the explanation and correlates with the number of documents to be searched during FAISS search. [30]\n\u2022 In this stage, FAISS similarity search [30] 3.4.2 is employed to pinpoint the most relevant elements within the TMK, and LangChain is subsequently utilized to construct a prompt incorporating this relevant TMK information.\n\u2022 Output of this stage for our working example: The system identifies and retrieves the most relevant TMK components along with their similarity scores. In the context of Ask-TMK, a similarity score represents a numerical value between 0 and 1 that indicates how relevant a specific element from the TMK model is to the user's question. FAISS provides us this similarity score for all of the k-documents.\n Ask-TMK: Explanation module within VERA. Similarity Score: 65.16%\nEcology Model: Digital representation of an ecological system. Similarity Score: 65.04%\nWhat if Experiment: Enables users to predict outcomes of future changes to an ecological system. Similarity Score: 64.32%\nUser: A person interacting with VERA to explore ecological systems and run simulations. Similarity Score: 63.24%\n3. Stage 3: Explanation Generation"}, {"title": "\u2022 Input", "content": "This stage receives the identified relevant TMK elements from the localization stage.\n\u2022 It leverages LangChain once more to prompt the LLM to construct the final explanation for the user. We leverage LangChain to create a prompt providing all the necessary details including TMK and the context of the question to generate a natural language explanation tailored to the user's question and the 'k' value (complexity factor)."}, {"title": "\u2022 Output", "content": "Intermediate Steps The LLM generates an answer in multiple steps, progressively refining the explanation. The intermediate steps are below:\n Analyze the results of ecological simulations and draw conclusions based on parameter changes.\n Leverage the ecological model for visualization, simulation, and \"what if\" scenarios.\n Gain a deeper understanding of ecological system dynamics and make informed decisions based on the output.\n Utilize the \"what if\" experiment feature to predict future outcomes by modifying parameters."}, {"title": "Output:Refinement and Final Answer", "content": "The explanation is refined into a single, cohesive answer. The direct answer from the system is given below.\n\"The output of the system in VERA can be best utilized by the user to analyze the results of ecological simulations and draw conclusions about the behavior of the ecological systems based on the parameter changes they have made. As a user, you can visually represent the relationships between organisms and their environment, simulate interactions and behaviors within the model, and explore \"what if\" scenarios by running simulations with different initial conditions or changing parameters. This allows you to gain a deeper understanding of the dynamics of ecological systems and make informed decisions based on the output of the system. Additionally, VERA's \"what if\" experiment feature enables users to predict the outcome of future changes to an ecological system by modifying parameters and observing the resulting simulation, providing valuable insights for decision-making.\"\n\u2022 As outlined previously in section 3.4, VERA leverages Chain-of-Thought for more intricate explanations, particularly when methods are involved. Chain-of-Thought enables the LLM to break down complex methods within the TMK into smaller tasks and subtasks, providing a more detailed explanation."}, {"title": "4 Preliminary Results", "content": ""}, {"title": "4.1 Evaluation of the self-explanation Method", "content": "We evaluated the self-explanation system (e.g., Ask-TMK, hereafter referred to as the system) implemented within VERA to assess its ability to provide informative and relevant explanations to user queries. This evaluation focused on the system's capacity to explain its internal workings and functionalities."}, {"title": "4.1.1 Question Set and Adaptation to VERA", "content": "A set of 66 high-level, non-context-dependent questions was derived from established Explainable AI (XAI) question banks [31, 32] and used in our previous work. These questions were then adapted to VERA's specific context to ensure their relevance to the system's functionalities and user interaction. We used the same set of questions to benchmark how VERA did with regards to our previous work. The initial pool of questions was taken from established question banks from relevant research papers, focusing on those aligned with our prior work [24]. Further, the categorization of questions into relevant groups and the definitions of those categories was taken directly from the existing literature and question bank classifications used in prior works, such as those by Liao et al. (2020) [31] and Sipos et al. [32] (2023). SAMI developers, then, collaboratively reviewed these questions to ensure their relevance to SAMI's functionalities and objectives. This iterative process involved either directly accepting relevant questions or modifying them to better align with SAMI's specific context. The focus on relevance resulted in a variation in the number of questions across different categories, reflecting the inherent differences in the types of explanations SAMI can generate compared to other AI systems. These questions from our prior work were then taken by the developer for Ask-TMK in VERA and adapted to VERA's specific context in order to benchmark the performance of self-explanation in VERA."}, {"title": "4.1.2 Evaluation Methodology", "content": "The evaluation process involved the following steps:\n1. Question Selection and Adaptation: As mentioned previously, relevant questions were selected from XAI question banks and adapted to VERA's specific functionalities and user interaction. Additionally, questions addressing VERA-specific aspects were created."}, {"title": "2. Explanation Generation", "content": "Each of the 66 adapted questions was presented to VERA's self-explanation method via a user interface and the generated explanations were documented."}, {"title": "3. Evaluation Methodology", "content": "To assess the effectiveness of VERA's self-explanation method in conveying information within a learning environment, we employed three established metrics commonly used to evaluate generative and cognitive AI systems: Recall, Precision, and Accuracy [33, 34, 35] . In this initial assessment, we focused on evaluating explanations from an AI research perspective, excluding user-specific metrics. To evaluate VERA's responses, the Ask-TMK developer independently assessed each explanation against pre-defined criteria established from an AI research perspective [35, 34, 33]. These criteria focused on aspects defined above and the justification regarding why a certain rating was chosen was documented. Another research scientist reviewed some of these initial ratings and the justifications for any discrepancies in the ratings were documented.\nOur future work will involve user-centered studies to evaluate comprehensibility by diverse user groups and refine VERA's self-explanation method for optimal user experience.\nWhile evaluating VERA using the same set of 66 questions previously employed with SAMI [24] suggests promise for generalizability, we acknowledge the need for further investigation. Future work will involve deploying VERA in diverse classroom settings to gather real-world data and comprehensively assess its generalizability across various learning environments.\nThis focus on real-world deployment will also allow us to delve deeper into the equity and bias aspects of VERA's self-explanation approach (Ask-TMK). We will explore potential biases within the training data and consider how to ensure fairness and inclusivity in VERA's explanations across diverse user groups."}, {"title": "4.2 Summary and Analysis of results", "content": "The results have been summarized in Table 1. We examine the performance of the self-explanation system the interactive agent, VERA, based on a user evaluation summarized in Table 1. The evaluation involved 66 questions taken from previous work as outlined earlier and categorized based on the type of information they sought."}, {"title": "4.2.1 Overall Performance", "content": "The self-explanation method achieved high recall, precision, and accuracy across most question categories, indicating its effectiveness in retrieving relevant information and generating accurate explanations."}, {"title": "4.2.2 Category-wise breakdown", "content": "Input Questions (4): These questions focused on the VERA's training data and achieved perfect scores across all metrics.\n2. Output Questions (22): This category, inquiring about how to utilize the VERA's output, had a slight decrease in precision (one medium score) compared to other categories. This was due to an occasional explanation that was accurate but not maximally helpful for optimal output utilization.\n3. \"How\" (Global) Questions (17): These questions aimed at understanding the general workings of the system. The system performed very well here, achieving high scores across all metrics.\n4. \"Why Not\" Question (1): This category, with only one question, showed perfect performance.\n5. \"Others\" Questions (10): These questions covered various topics unrelated to the core functionality. The system performed well here, with high scores across all metrics.\n6. \"Others\" (Context) Questions (3): These context-related questions received perfect scores across all metrics.\n7. VERA Specific Questions (9): These questions focused on understanding specific outputs from VERA simulations. Again, the system exhibited high performance here."}, {"title": "4.2.3 Potential Areas of Improvement", "content": "Overall, the self-explanation method demonstrates promising performance across most question categories. High recall, precision, and accuracy indicate that the system effectively retrieves relevant information and provides accurate explanations.\nAs pointed out earlier in 4.1.2, the current system has undergone preliminary evaluation led by the developers, focusing on AI research perspectives. It has not yet been deployed in classroom environments. We acknowledge the potential for unintentional biases stemming from our deep familiarity with the Ask-TMK system's internal mechanisms, which may have influenced question framing and answer interpretation. It is anticipated that deployment in real classrooms will introduce a layer of human-centric evaluation currently lacking, potentially yielding divergent insights. Future research will prioritize the incorporation of these critical human evaluations to improve the system's relevance and performance within educational settings. For future work, we plan to:\n1. Test the system with more questions to determine if precision scores vary or if we encountered an occasional outlier.\n2. Conduct user studies to understand how the self-explanation system performs with different user groups."}, {"title": "5 Conclusion", "content": "The Ask-TMK module in VERA uses a theory of VERA's mind to explain how it works through question answering. Ask-TMK's theory of VERA's mind is captured in the language of Task-Method-Knowledge (TMK) models that specify how VERA uses its domain knowledge and reasoning methods to achieve its goals. We tested the Ask-TMK self-explanation system within VERA with the question bank established in previous work. Our preliminary analysis shows that the self-explanation system effectively leverages cognitive Al's structured knowledge for information retrieval and generative AI's capabilities to deliver relevant and accurate explanations. The system maps user queries to the relevant Task, Method, and Knowledge components within the TMK model, thereby generating responses that explain how VERA works. In our use case, this integration enables factually accurate, complete, and precise explanations and demonstrates promising performance across various question types."}]}