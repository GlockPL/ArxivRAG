{"title": "Assistance or Disruption? Exploring and Evaluating the Design and Trade-offs of Proactive AI Programming Support", "authors": ["Kevin Pu", "Daniel Lazaro", "Ian Arawjo", "Haijun Xia", "Ziang Xiao", "Tovi Grossman", "Yan Chen"], "abstract": "AI programming tools enable powerful code generation, and recent prototypes attempt to reduce user effort with proactive AI agents, but their impact on programming workflows remains unexplored. We introduce and evaluate Codellaborator, a design probe LLM agent that initiates programming assistance based on editor activities and task context. We explored three interface variants to assess trade-offs between increasingly salient AI support: prompt-only, proactive agent, and proactive agent with presence and context (Codellaborator). In a within-subject study (N = 18), we find that proactive agents increase efficiency compared to prompt-only paradigm, but also incur workflow disruptions. However, presence indicators and interaction context support alleviated disruptions and improved users' awareness of AI processes. We underscore trade-offs of Codellaborator on user control, ownership, and code understanding, emphasizing the need to adapt proactivity to programming processes. Our research contributes to the design exploration and evaluation of proactive AI systems, presenting design implications on AI-integrated programming workflow.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have enabled generative programming assistance tools to provide powerful in-situ developer support for novices and experts alike [54\u201356, 70, 86]. Many existing LLM-based programming tools rely on user-initiated interactions, requiring prompts or partial code snippets as input to provide sufficient context and to trigger help-seeking [52, 65, 71, 76, 86]. These systems offer help in the form of code output and natural language explanations to assist users with coding tasks. However, research indicates that users invest considerable effort in formulating prompts, interpreting responses, assessing suggestions, and integrating results into their code [52, 69, 86].\nTo alleviate the user intent specification costs, recent AI programming tools and designs aim to become more autonomous, allowing the system to initiate interaction and provide proactive assistance. Tools like Github Copilot [4] and Visual Studio's IntelliCode [83] offer the auto-completion feature, which fills the code line as the user is typing, to alleviate prompt engineering efforts and proactively provide in-situ support. However, the AI tool's generated code is not always accurate and the output still requires significant user effort to verify [60, 69, 85, 86]. To address this, commercial and research prototypes constructed intelligent Al agents with distinct focuses on the timing of assistance, the representation of the agent, and the scope of the interaction context, aiming to work with the user as collaborators and tackle coding tasks autonomously and preemptively. These approaches involve advancing on the timing of auto-completion feature to proactively make intelligent file changes with an AI caret in the code editor [13], representing the Al's presence in the editor with an automated AI cursor [14], or grounding interactions context in different scopes, such as conversational dialogue [5, 19, 76], specific code lines [4, 52], or an agent-managed workspace to tackle software engineering tasks autonomously [15, 89, 92]. However, the effects of these new designs of system-driven programming assistance on the human workflows, compared to the existing user-initiated paradigm, remain to be explored. Visions of more \u201cproactive\u00b9\u201d AI programmers additionally raise questions about the potential for harm. Researchers have raised on concerns that excessive automation without proper human control can lead to unreliable and unsafe systems [79], and thus some Al systems deliberately avoid proactive AI assistance [76]. Therefore, there remain the questions of when should AI programming tools provide proactive support, how should the support be delivered, and where should the user interact with such support. Subsequently, what are the effects of a proactive AI programming tool on user experience? In which programming processes and tasks is proactivity helpful, and where might it be harmful?\nThis research explores the design space of proactive AI programming tools in three dimensions - the timing of assistance, the representation of the AI programming tool, and the scope of interaction context. We then evaluate the effects of this human-AI interaction paradigm on software engineering practice, illustrating the advantages and drawbacks to provide insights for future designs. We were guided by these research questions:\n\u2022 RQ1: How can we design proactive assistance in an Al programming tool to reduce user effort?\n\u2022 RQ2: What are the benefits and drawbacks of a proactive AI programming tool compared to user-initiated systems?\n\u2022 RQ3: In which programming processes and task contexts can proactivity be helpful, and where can it be harmful?\nTo answer these research questions, we incorporate theories of interruption management, social transparency, and help-seeking behavior in programming (Fig.1) to identify specific design rationales for each dimension. Informed by prior literature, we develop Codellaborator, a technology probe [49] that employs an AI programming agent providing proactive timings of assistance to explore different forms of human-AI programming collaboration. Codellaborator's proactive abilities allow it to initiate interaction via messages (Fig.2.a,e) in response to various user activities in the coding environment, and also to commit code edits directly in the editor (Fig.2.c). To mitigate potential disruptions, we derive three design rationales for the timing to introduce assistance and operationalize them into six design principles in the context of a coding task and an editor environment (Table 1). To evaluate designs of Al agent representations in the editor, we additionally implemented presence features for the AI agent. In Codellaborator, the agent presence is represented by a cursor and caret (Fig.2.d,b), capable of autonomous movement around the editor, signaling its action, status, and attention focus. To evaluate different scopes of interaction, both the agent and the user can utilize global chat messages on the side-panel, or initialize locally-scoped threads of conversations called \"breakouts\", anchored tospecific locations in the editor as context (Fig.2.a).\nTo study the impact of proactive support in an autonomous coding agent on human-AI collaborative programming workflows, we conducted a within-subject experiment using three versions of Codellaborator with 18 participants. In the PromptOnly condition, the ablated system only responds to user prompts and in-line code comments, similar to ChatGPT [2] and Github Copilot [4] with low to no proactive features. In the CodeGhost condition, the system proactively initiates interactions and assistance, but the interactions are constrained in the global context of chat messages and direct code changes, with no visual representation of the agent. In the Codellaborator condition, all of the agent's visual representation, scopes of interaction, and proactive timing features are utilized.\nOur study showed that, through the heuristic-based timing to provide contextualized assistance at task boundaries, the CodeGhost condition reduced the time users took to comprehend system responses compared to the PromptOnly condition. But it also caused workflow disruptions and diminished users' awareness of Al's actions, as participants reported a lack of clear signals for agent interaction and working context. In contrast, the Codellaborator condition, with its agent visual presence and flexible context scope, significantly lessened these disruptions and improved users' awareness of the AI, leading to a user experience more akin to collaborating with a partner than using a tool. Participants felt ambivalent to adopt highly proactive programming assistants. Many embraced the efficiency and capability to allow developers to focus on high-level designs rather than low-level work, but some participants experienced a loss of code understanding, expressing concerns on maintainability and extendability of the code artifact.\nIn the discussion, we summarize our findings and propose five design implications for proactive assistance in human-AI programming. Through these findings, we present a deeper understanding of the impacts of proactive AI support on programming experience and identify key areas that require further research. In this work, we contribute:\n\u2022 A design exploration to enable different interaction timings, visual representations, and interaction scopes of proactive assistance that expand upon existing Al programming systems.\n\u2022 Codellaborator, as a technology probe that implements a proactive Al agent to study in-situ assistance and communication in programming support.\n\u2022 A empirical study to assess the impact of proactive agent support in a code editor, providing design implications for future Al programming tools."}, {"title": "2 Related Work", "content": "AI-assisted programming tools are increasingly integrated in developers' workflows. However, even tools that prioritize productivity, such as Github Copilot [4], do not consistently demonstrate a significant improvement over traditional code completion tools such as IntelliSense [6, 56, 86]. Existing research to improve Al programming support has focused on improving the quality and usability of the code generation. For example, recent works made advancements in structuring code generation [93], expanding support to specific task domains (e.g., data analysis) [65], or highlighting high-probability tokens to reduce uncertainties [88]. However, Al-generated assistance could result in discrepancies with the user's expectations, creating barriers to interpreting and utilizing the code output, or even steering the AI in the desired direction in the first place [86]. Another recurrent concern is the potential mismatch in expertise levels between developers and AI agents, leading to reduced productivity in pair programming scenarios [61]. To improve the usability of generated code, researchers enhanced the context within the scope of interaction in the code editor. For example, Yan et al. proposed Ivie, which generates visible explanations positioned adjacent to the code [91]. Similarly, recent systems improved the discoverability of the code suggestion scope [85] or provided more in-IDE code contexts to scaffold user understanding [71]. Alternatively, some tools integrates dialogue-based interactions to enable holistic queries at the editor level and enhanced interaction histories [5, 19, 76]. Expanding the scope further, Meta-Manager enhances developers' sensemaking by collecting and organizing meta-information, such as code provenance and design rationale, making it easier to answer complex questions about the code base [46]. However, it remains unclear whether these approaches to scoping human-AI collaboration will be effective in a system-initiated paradigm and further research is needed to investigate the impact of different scopes of system-initiated actions on developer experience.\nExisting AI programming tools predominantly operate within the command-response paradigm, where the user triggers help-seeking and obtains generated code and explanations. An emerging approach to enhance AI programming is leveraging LLMs' generative power to build intelligent programming agents that proactively support users and autonomously complete tasks. Efforts to develop proactive tools that provide automatic support have been explored across various domains, including personalized notifications for weather or calendars [77, 82], health and fitness interventions [75, 78], and support for office workflows [28, 51, 63]. Well-designed, effective invocation of systems' proactive assistance can lower the cost of user manipulation, resolve uncertainties preemptively, and lead to unintentional learning of the system's functionalities [1, 47, 63]. Meanwhile, poorly designed proactive assistance can lead to negative user experiences, diminished control [27, 66, 68], and in some cases, rendering the tool ineffective [28, 51, 66]. While general guidelines on designing mixed-initiative interfaces and human-AI interactions have been established [23, 47], it is uncertain how the design principles can translate to concrete system designs under the context of LLM-assisted programming. For instance, while the timing of assistance is one key metric of human-AI interaction design, existing Al programming systems almost always provide immediate response upon output generation without considering interruption to the user's workflow.\nRecent research and commercial prototypes have explored many representation of the Al agent to facilitate proactive support in programming. Some approaches include expanding on the auto-completion feature to proactively make intelligent file changes with an AI caret in the code editor [13], or manifesting the Al's presence in the editor with an automated Al cursor [14] that mimic the user's workflow and automate repetitive tasks. Some tools take an additional step towards fully autonomous Al and construct a group of AI agents capable of composing the task plan with executable steps and hosting its own workspace with code editor, console, and web browser to autonomously tackle software engineering tasks in response to a single user prompt [15, 89, 92]. However, the accuracy of the task completion suggests that the fully autonomous agent might not yet be fully scaled to real-life programming tasks. For example, based on evaluation on SWE-bench [53], a dataset designed to assess Al agent's capabilities in real-world Github issues, SWE-agent and OpenDevin reported 12.5% and 26.0% task completion rate. This prompts a more balanced design where both the human and the Al agent are engaged in the programming process.\nFurther, the impact of proactive programming assistance for human users, as opposed to the current prompt-initiated paradigm, remains to be formally assessed. Similarly, the resulting benefits and drawbacks to user experience from employing these specific design approaches need to be measured. Our research not only aims to explore how to effectively design and integrate proactive AI assistance into developers' workflows but also seeks to gain a deeper understanding of the impact on the programming experience through a comprehensive study."}, {"title": "2.2 Proactive Assistance and Interruption", "content": "Designing a proactive AI assistant that enables positive experiences and outcomes is a challenging endeavor. The nuances of effective human-to-human collaboration are still not fully understood and vary greatly depending on the context, making it difficult to craft effective human-AI collaboration paradigms. Past research has shown that two factors, i.e., proactivity and interruption, play pivotal roles in shaping the outcomes of team collaborations. Prior work in psychology has revealed that proactivity, when effectively managed, can provide positive affective outcomes during collaborative work [57]. However, the current landscape of human-AI collaboration is often characterized by either human-dominant or AI-dominant dynamics. In such situations, both human and Al agents operate reactively. This paradigm often leave the cognitive burdens for human developers due to the expression, sensemaking, and verification process for the code assistance [60, 86].\nInterruption, or \"an event that breaks the coherence of an ongoing task and blocks its further flow, though allowing the primary task to resume once the interruption is removed\" has been a subject of study for decades [64]. Numerous studies have highlighted the detrimental effects interruptions can have on users' memory, emotional well-being, and ongoing task execution [24, 35, 48]. Specifically, in the context of software engineering, Solingen et al. defined interruption as a multi-phase process that occurs when a developer stops their planned activities, handles the interruption, then finally recovers by returning to the point in their work at which they were interrupted [87]. In existing practices, Ko et al. observed developers daily activities and identified multiple interruptions per day, mainly due to communication requests and notifications [58]. Parnin et al. also found that developers spend significant time rebuilding context after each interruption, creating \"resumption lag\" which increases errors and frustration [73, 74]. To mitigate the challenges posed by interruptions, we drew insights from psychology and behavioral science to foster collaborations that would be perceived as less disruptive. For instance, as the perceived level of disruption is influenced by a user's mental load at the time of the interruption [24, 25, 35], we designed interactions that were aware of a user's working context before generating notifications. Furthermore, prior works have highlighted that people experience varying degrees of disruption during different sub-tasks [35, 48, 50, 67]. We apply these principles when designing the timing of service of our probe to adopt a proactive collaborator role at moments when the programming task context was most appropriate."}, {"title": "2.3 Help-Seeking and Collaboration in Programming", "content": "Our research was additionally informed by existing research on collaboration during software engineering, specifically help-seeking behaviors and pair programming."}, {"title": "2.3.1 Help-Seeking in Programming", "content": "Developers often forage information from the code itself to resolve their issues, during processes like debugging [41]. To facilitate this process, researchers have built tools to scaffold navigation and understanding the source code [36]. Documentation is another source developers rely on to find assistance. However, studies have identified challenges for users to pinpoint relevant information and to maintain the documentation in an up-to-date state [22, 38]. Annotations on documentation, as demonstrated in Adamite, can support comprehension and foster collaboration by addressing gaps in traditional documentation [45]. To seek more targeted help, Community Question-Answering (CQA) websites, such as Stack Overflow [12], also allow developers to post questions but also archive answers for future reference, a concept rooted in Answer Garden's creation of an \"organizational memory\" [20, 21]. To more seamlessly connect developer's working context to help seeking, researchers have connected the integrated development environment (IDE) with web browser [42], web-based Q&A search [34], and annotated the source code with browser histories [44]. However, many questions that are well-suited for an intelligent agent are misaligned with the design of CQA websites. A previous study utilized a \u201chypothetical intelligent agent\" as a probe to understand developers' ideal help-seeking needs [32]. The findings, along with other studies' results, highlighted several limitations of CQA sites, including delayed feedback, lack of context, and the necessity for self-contained questions [62]. Consistent with this, prior work has advocated for systems that intuitively captured a developer's context and used it to enable developers to identify the scope of assistance by selecting a code snippet, asking the system to \"please refactor this\", and promptly receiving pertinent responses [31]. Like described in Section 2.1, existing AI programming tools often employ different scopes of interaction with the intelligent assistant, translating the consideration of interaction context scope from human help-seeking to human-AI collaboration. Inspired by this research, we designed our probe to allow users to seek help with different granularity of support, receiving assistance on the overall codebase via a global chat interface and on specific code snippets via localized conversation threads. This way, we can evaluate the impacts of different scope of interactions in a more system-initiated programming paradigm."}, {"title": "2.3.2 Pair Programming", "content": "Pair programming is a paradigm where two users collaborate in real-time while at a single computer, with one user writing the code (i.e., the driver) and the other reviewing the code (i.e., the observer) [33]. Pair programming has been shown to lead to better design, more concise code, and fewer errors within approximately the same person-hours [29, 90]. Other research has reported that these benefits may have been due to the awareness of another's focus within the code, which can be invaluable for problem-solving [59]. For example, Stein and Brennan found that when novices observed the gaze patterns of expert programmers during code reviews, they pinpointed bugs faster [80]. However, the most prominent challenges associated with pair programming include cost inefficiency, scheduling conflicts, and personality clashes [29]. Facilitating visible presence and actions between collaboration partners has previously demonstrated its efficacy in physical workspaces [37]. Our design probe loosely adopted the pair programming paradigm where the AI agent and the user can adapt and exchange the roles of the driver and the observer. We also implemented visible presence and clear context information to enhance mutual awareness between the user and the AI."}, {"title": "3 Design Goals", "content": "To investigate the effects of a proactive Al programming agent on user workflow, we used a technology probe an instrument that is deployed to find out about the unknown-returning with useful or interesting data [49] to explore the design space of the timing, representation, and scope of interaction (RQ1) with the following design considerations:\n\u2022 DG1: Establish heuristics for timely proactive AI assistance by anticipating programmer needs based on editor activities and offering suggestions, insights, or corrections to support their tasks.\n\u2022 DG2: Represent AI agent's presence using visible cues to indicate its actions, intentions, and decision-making processes, enhancing the user's awareness of the agent's assistance.\n\u2022 DG3: Provide flexible scopes of interaction by designing interactions at both a global code editor level and at a local code-line level to meet different abstractions of user need and improve context management.\n\u2022 DG4: Support different mechanisms in the probe and creating different versions of the system to structure comparisons and evaluations of different designs"}, {"title": "4 The Codellaborator probe", "content": "In this section, we introduce three main components of the Codellaborator probe and how they are implemented to achieve our design goals: timely proactive support, Al agent's visual representation, and multi-level scopes of interaction, supporting modular comparisons of different mechanisms. Below we detail the design and implementation of the probe."}, {"title": "4.1 Timely Proactive Programming Assistance", "content": "Timing services based on context is a key consideration in Al system design [23]. To explore the design for proactive timings specifically in the context of programming support, we adopted a set of findings from research on interruption management, and distilled them down to three proactivity design principles [25, 35, 50, 67]. To operationalize, we instantiated six proactivity features in our design probe to minimize interruptions to the user (DG1), summarized in Table 1. The proactive assistance in Codellaborator serves to present one design approach that expands upon existing AI programming features, allowing us to investigate the effects of an Al agent equipped with highly proactive capabilities on users' programming workflow.\nThe first principle states that the most opportune moments for interruption occur during periods of low mental workload [25, 35]. In our system, we predict low mental workloads when users are not performing actions, such as writing code, moving around the file, and selecting ranges (i.e. when the user is idle). Since idleness could also mean the user is engaged in thoughts, the AI agent only intervenes after an extended period of inactivity in both editing and cursor movement, which could signal that the user is mentally stuck and needs assistance (Table 1, 1). This is a rough estimation to interpret the user's working states, and future works could employ more advanced models to identify the user's cognitive process.\nThe second principle posits that people perceive interventions as less disruptive at the beginning of a task or subtask boundaries [35, 50, 67]. Task boundaries are defined as when one subtask is completed (evaluation) or when the next subtask begins (goal formulation) [67]. To convert this implication to a system feature, we used event listeners to detect users' task beginnings and boundaries in programming, specifically, when the user completed a block of code (i.e., after outdenting in Python), executed the code, or made a multi-line edit (i.e. pasting a block of code) (Table 1, 2-4).\nFinally, we draw inspiration from existing Al programming tools [4, 5, 71] and propose the third principle: intervene when users are communicating through implicit signals. Existing systems don't always use direct messages as the means of human-Al communication. For example, in Github Copilot [4], creating a new line after a comment prompts the tool to generate code based on the comment content. Another example is in Nam et al.'s work, where the system uses the user's current selection in the editor as context to provide code generation [71]. While these features demonstrate a low level of proactivity individually, we assimilate the existing designs to enhance the proactivity of Codellaborator.\nIn each proactivity feature, the Al agent receives the user's caret position and local code context. It then leverages the LLM to reason, triage, and decide whether to intervene, selecting an option from the defined list of editor actions if deemed necessary. We also implemented adaptiveness within the AI agent's proactivity. For example, each time an idle or selection intervention (Table 1.1,6) was ignored by the user (i.e. no follow-up interaction with the agent), we imposed a penalty on the action and increased the time threshold to trigger an intervention to make it less frequent in the future to decrease unnecessary interventions. Additionally, we prioritized the user's initiative and actions over the Al agent's, providing the user with ultimate control. When the user was initiating a conversation with the assistant, we canceled pending AI agent actions and active API requests, so as to not disrupt the user's train of thought and wait for their updated input. However, to support parallel workflows (e.g., when the user is writing code while the agent proactively modifies other parts of the code), the agent does not cancel its actions if the user is making changes in the editor. To enable users to control the amount of visual signals they receive, the chat interface could be fully collapsed, providing more space for the code editor and hiding potentially distracting messages.\nIn general, these guidelines lead the probe system to create proactive LLM-based agents for coding support. The agent accounts for the user's current text context and past interactions before evaluating which action to take and when to intervene. When appropriate, the agent can proactively make requests to the LLM and take agent actions to help facilitate direct communication and collaboration with the user. With this approach, we attempt to improve the intervention timing using both rule-based heuristics and LLM decision-making predictions, constraining the model to take feasible and reasonable actions in a programming assistance context. We conduct interaction-level analysis in a user study to evaluate the effectiveness of each timing principle and offer design insights for future proactive AI programming systems."}, {"title": "4.2 AI Agent's Visual Representation in the Code Editor", "content": "To explore the effects of enhanced visual representation, our prototype makes visible the agent's actions, status, and process (DG2). Codellaborator manifests the Al's presence with visual cues guided by the Social Transparency theory [40, 81] and existing design explorations for programming agents [13, 14, 39]. Specifically, we were inspired by the concept of interaction transparency, which posits that the visibility of the presence of other parties and sources of information in human collaboration can reduce interaction friction [40, 43].\nTo do so, we added a visual caret and cursor in the editor workspace automated by the AI agent. The AI caret indicated its position in the text buffer and moved as the Al agent selected and edited code (Fig.2.b). The AI cursor, which moved independently of the caret, serves as an indicator of the AI agent's \"attention\" and demonstrates its actions (Fig.2.d). For example, to rewrite a block of code, the Al cursor would select a range of code, delete the range, and stream the new code in a typing motion, similar to how a human user would act. We introduced human elements into the Al agent's actions to elicit the social collaboration heuristics to facilitate better human-AI collaboration [39].\nWhile the Al agent is processing or taking action, a thought-bubble overlay floats adjacent to the cursor and contains an emoji and short text to convey the Al agent's working state (Fig.2.d). For example, a writing hand emoji with \u201cWriting code...\u201d indicated that the Al agent was writing code in the editor, whereas a tool and laptop emoji with \"Program executing...\" indicated that it was analyzing the program execution output. The system also indicated its working progress by streaming a response with a \"pending\" indicator in the chat panel and a \"Loading\" signal on the AI cursor, thus preventing confusion about whether the system was responsive or stalled. This design enabled users to be aware of system actions even when the chat panel was collapsed, allowing them to decide whether to engage based on the status displayed. The option to attend to the textual messages or the visual presence afforded different levels of interaction details, handing users control over the amount of information to be received from the AI agent."}, {"title": "4.3 Different Scopes of Interaction", "content": "To examine the effects of different scopes of human-AI interaction within the code editor (DG3), Codellaborator affords two channels for either the user or the agent to initiate an interaction. This was inspired by the literature on help-seeking in programming, where users often need to define the context (i.e. relevant code snippets [31], code annotations [45], or search history [44]) to request assistance. Existing AI programming tools often adopt conversational interactions on a dedicated interface or provide in-line generated suggestions. To evaluate the effects of different interaction scopes, Codellaborator includes a global chat side panel, as well as local threads of conversations anchored in specific code lines, called the \"breakout\". The user can initiate breakout chats for specific local context; the agent also automatically summarizes interactions and arranges them in relevant locations in the code editor.\nTo facilitate the transition between global and local scopes, the AI agent consistently tracked many forms of context, such as the user's current caret position in the file, the contents of the file, the user's activity (or lack thereof), and the editor console output. Using this context, the AI agent organized the past conversation context by grouping semantically relevant messages by topic and \"breaking out\" the subset of messages from the chat. The selected messages were collapsed in the main chat panel and anchored to an expandable thread at the appropriate area of code in the editor (Fig.2.e). This enabled the conversation about a specific code section to be placed directly in a localized context, so the link between the code and the process that created it was represented visually. Breakouts also provide an easy way to access past conversations without needing to scroll through the chat when the collaboration session is prolonged. The breakout function also required that the Al agent provide a short summary, which was displayed in the chat in a collapsed component (Fig.2.e). The collapsed component preserved the provenance of the original messages and also provided a button to navigate to the attached thread in the code editor. The new breakout chat remained interactive, so the user could continue the conversation with the AI agent and suggest edits, ask for explanations, and more, in situ (Fig.2.a). The user could also manually initiate a breakout in the file. By doing this, they anchored their queries to a specific line, providing the AI agent with a local scope of context to inform their responses and/or actions."}, {"title": "4.4 Probe System Implementation", "content": "Codellaborator was implemented as a React [11] web application using TypeScript. The front-end code IDE was built on top of the Monaco Editor [8]. The code execution relied on a separate web server powered by Node.js [9] and the Fastify library [3]. The scope of Codellaborator enabled users to execute single-file Python3 code for proof of concept. The back-end of the system was powered by the GPT-4 [72] large-language model that was connected via the OpenAI API [10]. Specifically, we used the gpt-4-0613 model which enabled function-calling\u00b2. This allowed the system to define functional tools that the LLM was aware of and could employ to make editor changes if it saw fit according to a defined schema."}, {"title": "5 Evaluation", "content": "To study the benefits and drawbacks of our design probe (RQ2) and understand the human-AI interaction workflows in different programming processes (RQ3), we conducted an in-person user study where participants collaborated with three versions of Codellaborator in pair-programming sessions to specifically evaluate and compare each designed mechanisms (DG4)."}, {"title": "5.1 Participants", "content": "We recruited 18 upper-level CS students from our university (8 female, 10 male; mean = 21.3 years, SD = 1.49 years, range = 19-24 years; denoted as P1-P18). Participants had a mean coding experience of 5.6 years. Fifteen of the participants had used LLM-based AI tools like ChatGPT [2], and thirteen participants used AI tools for programming at least occasionally. Participants were recruited via a posting in the Discord and Slack channels for CS students at the university. Each study session lasted around 90 minutes and participants were compensated with $40. The study was approved by the ethics review board at our institution."}, {"title": "5.2 Study Design", "content": "We conducted a within-subject study involving three conditions on three system prototypes to examine the effects of different proactivity designs compared to a fully user-initiated baseline (RQ2, DG4). The condition orders were counterbalanced to account for ordering and learning effects. The underlying LLM in all three conditions was initialized with the same system prompt (Appendix A) and parameters (e.g. version, temperature, etc.). The there conditions are described below:\n\u2022 The PromptOnly condition was an ablated version of Codellaborator, similar to existing AI programming tools like Github Copilot [4] and ChatGPT [2] where users prompt using code comments or chat messages to receive Al response. The system only reacts to users' explicit requests. This system did not have access to proactively make code changes in the editor.\n\u2022 The CodeGhost condition constructed an Al agent that takes proactive actions, such as sending messages or writing code in the editor, to provide help based on user activity and timing principles. In this condition, the ablated system did not include any additional indicators of the AI agent's presence, and did not support the localized, threaded, scope of interactions.\n\u2022 The Codellaborator condition utilized the same Al agent and proactivity features found in the CodeGhost condition and additionally utilized the AI agent's visual features and interacted with the user at different scopes of context. In this condition, the full system represented the AI via its autonomous cursor, caret, and intention signal bubble (Fig.2.b,d). Moreover, users were allowed to use breakouts to start localized threads of conversations at different parts of the code (Fig.2.a). Codellaborator also automatically grouped relevant messages and organized them into breakouts to manage interaction context (Fig.2.e)."}, {"title": "5.3 Tasks", "content": "Three programming tasks involving implementing a small-scale project in Python (full descriptions in Appendix B; Task 1: event scheduler, Task 2: word guessing game, Task 3: budget tracker) were used in the study. The tasks were derived from LeetCode [16-18] coding problems, which present adequate challenges for our participant pool within the scope of the study. The particular tasks are selected to reflect typical programming activities commonly encountered by developers, including working with data structures, control flow, and basic algorithms. We adopted test-driven development by providing users with a unit test suite for each task, where they had to implement the specification to pass the test cases. The tasks were designed to balance practical relevance and study feasibility, allowing participants to showcase problem-solving skills and creative design choices within a constrained time frame. Although LeetCode problems are often designed with one or two optimized solutions, we modified the problems so users could take multiple approaches. For instance, one of our tasks involved implementing an event scheduler. While a brute-force approach could solve the problem, participants could also explore different data structures (e.g., priority queues, dictionaries) to optimize efficiency, or modularize their solution to enhance readability and maintainability. This flexibility allowed us to observe variations in user decision-making. The tasks were intentionally open-ended and can be completed through various designs so that participants could not directly use task specification as LLM prompt to solve the problem deterministically. In an attempt to maintain consistent generation quality across participants, the backend GPT-4 model was set to 0 temperature to reduce randomness. Based on a pilot study with 6 users using the Codellaborator condition prototype, we found that participants were able to complete each task within 20-30 minutes, thus showcasing similar task difficulties. During the study, the tasks were randomly assigned to each condition to reduce bias."}, {"title": "5.4 Procedure", "content": "After signing a consent form, participants completed each of the three coding tasks. Before each task, participants were shown a tutorial about the system condition they would use for the task. Participants were asked to discuss the task with the AI agent at the start of each task to calibrate participants' expectations of the AI agent and to reduce biases from prior AI tool usage. Participants were given 30 minutes per task and were asked to adopt a think-aloud protocol. After each task, participants completed a Likert-scale survey (anchors: 1 strongly disagree to 7 strongly agree) about their experience in terms of the sense of disruption, awareness, control, etc. (Fig.6). After completing all three tasks, participants underwent a semi-structured interview for the remainder of the study. Each session was screen- and audio-recorded and lasted around 90 minutes."}, {"title": "5.5 Data Analysis", "content": "We conduct in-depth qualitative and quantitative analysis on the collected data. Qualitatively, the semi-structured interview responses were individually coded by three researchers. Subsequently, thematic analysis [30, 84"}]}