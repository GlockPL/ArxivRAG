{"title": "HYBRID DEEP CONVOLUTIONAL NEURAL NETWORKS COMBINED\nWITH AUTOENCODERS AND AUGMENTED DATA\u0391 \u03a4\u039f PREDICT THE\nLOOK-UP TABLE 2006", "authors": ["Messaoud Djeddou", "Aouatef Hellal", "Ibrahim A. Hameed", "Xingang Zhao", "Djehad Al Dallal"], "abstract": "This study explores the development of a hybrid deep convolutional neural network (DCNN) model\nenhanced by autoencoders and data augmentation techniques to predict critical heat flux (CHF)\nwith high accuracy. By augmenting the original input features using three different autoencoder\nconfigurations, the model's predictive capabilities were significantly improved.\nThe hybrid models were trained and tested on a dataset of 7225 samples, with performance metrics\nincluding the coefficient of determination (R\u00b2), Nash-Sutcliffe efficiency (NSE), mean absolute\nerror (MAE), and normalized root-mean-squared error (NRMSE) used for evaluation. Among the\ntested models, the DCNN_3F-A2 configuration demonstrated the highest accuracy, achieving an\nR\u00b2 of 0.9908 during training and 0.9826 during testing, outperforming the base model and other\naugmented versions.\nThese results suggest that the proposed hybrid approach, combining deep learning with feature\naugmentation, offers a robust solution for CHF prediction, with the potential to generalize across\na wider range of conditions.", "sections": [{"title": "1. INTRODUCTION", "content": "The Critical Heat Flux (CHF) is the threshold at which the heat transfer between a heated\nsurface and a surrounding fluid reaches its maximum capability, resulting in a significant decrease\nin heat transfer efficiency. Exceeding this limit in the context of nuclear reactors can lead to a\nrange of unwanted outcomes, including thermal excursion, material degradation, and potentially\ndeadly reactor instabilities. Therefore, it is crucial to comprehend the underlying concepts that\ngovern CHF (Critical Heat Flux) in order to mitigate operating risks and optimize reactor\nperformance. CHF, is a complicated phenomenon influenced by various factors including flow\nrate, pressure, quality, geometry, and surface qualities [1].\nThe prediction of the CHF phenomenon has attracted significant investigation from researchers\ndue to its importance in nuclear reactor technology. There are three main types of approaches for\nCHF predicting:\nEmpirical CHF correlations:\n\u2022 Analytical CHF models;\nCHF look-up table methods.\nEvery approach has its advantages, but still comes with certain disadvantages such high\nnonlinearity, complex phenomenology, and uncertainty associated with CHF [2].\nThe CHF look-up table is essentially a normalized database that includes CHF values predicted\nbased on several factors, including mass flux (G), coolant pressure (P), and thermodynamic quality\n(x_e_out).\nCurrently, nuclear reactor thermal hydraulics codes like RELAP5 3D, COBRA-TF, and TRACE\nuse empirical correlations or tabulated look-up tables based on vast experimental data across\ndifferent operating situations to tackle CHF issues [3][4]."}, {"title": "2. MATERIALS AND METHODS", "content": "The remarkable advancement in artificial intelligence (AI) in the past three decades has led to an\nincreasing interest in using Ml-based machine learning models for the prediction of critical Heat\nFlux (CHF) since the 1990s. Several machine learning-based studies have been proposed to predict\nthe CHF look-up table, such as references [5, 6, 7, 8, 9, 10]. Applying a single prediction model\nmay entail some limitations in terms of accuracy. On the other hand, it has been demonstrated that\nthe combination of prediction capabilities from many methodologies enhances the accuracy of\npredictions, as evidenced by studies [11, 12, 13, 14].\nThis paper presents a new hybrid model based on auto-encoders (AE) as feature fusion techniques\nrefer to procedures that integrate variables to eliminate redundant and useless information coupled\nwith deep convolutional neural networks (DCNN), for the prediction task of CHF look-up table."}, {"title": "2.1 Lookup Table", "content": "The 2006 Groeneveld Lookup table (LUT) is a commonly used model for estimating the margins\nof CHF (Critical Heat Flux) in a nuclear reactor. The LUT serves as a tool to forecast CHF in a\nvast range of variables and is a data-derived estimation of the CHF phenomenon. It can be defined\nas a standardized database for vertical 8 mm water-cooled tubes [14]. The LUT will serve as a\nbenchmark for evaluating the outcomes of our prediction techniques.\nThe LUT 2006 employs pressure, mass flux, and quality as the primary parameters, as they are\nwidely recognized as the most influential factors. Furthermore, a correction term is defined for the\ntube's diameter as stated in reference [14]:\n$CHF = CHF_{3mm} \\times [\\frac{D_{real}}{8}]^{0.5}$"}, {"title": "2.2 Data description", "content": "CHF8mm represents the normalized CHF value obtained from the lookup table, specifically for a\ndiameter of 8mm. Dreal refers to the actual diameter in millimeters. When retrieving CHF8mm from\nthe LUT, it is necessary to do interpolation because to the restricted data points in the table. This\ninterpolation is accomplished using a linear interpolator. The author considered the length of the\ntube to be a second-order parameter and hence did not include it in the approximation, as long as\nthe L/D ratio was sufficiently large.\nThe entire database was built based on the Groeneveld CHF LUT 2006 using 7245 data entries.\nThe training/test partition typically involves randomly partitioning the data into a training set used\nfor models training and a test set used to validate the proposed models. In this study, 80% of the\ndata are used as the training set and 20% as the test set. The input and output variables considered\nin the study and their descriptive statistics are reported in Table 1."}, {"title": "2.3 Auto-encoders", "content": "The training dataset with the three input attributes listed in Table 1 is prepared and Standardization\ncenters data around a mean of zero and a standard deviation of one.\nThe concept of autoencoders was initially described in [15] as a type of neural network that is\ntrained to accurately recreate its input. The primary objective is to acquire knowledge through self-\ndirected methods, resulting in a \"informative\" presentation of the data that can be used to diverse\noutcomes.\nAuto-encodeurs structure is composed of two interconnected networks:\n1. Encoder network: converts the initial input, with a high number of dimensions, into a latent\ncode with a lower number of dimensions. The size of the input is greater than the size of\nthe output.\n2. Decoder network: The decoder network extracts the information from the code, often by\nusing progressively larger output layers."}, {"title": "2.4 Deep Convolutional Neural Networks", "content": "The model includes an encoding function g(.) that is parameterized by q, and a decoding function\nf(.) that is parameterized by 0. The low-dimensional code obtained from input x in the bottleneck\nlayer is denoted_as_z = g(x), z=g(x), and the reconstructed input is represented as x' =\nfo(g(x)).\nThe parameters (0,0) are jointly trained to provide a reconstructed data sample that matches the\noriginal input, x \u2248 fe(g(x)), or in simpler terms, to learn an identity function. The metric applied\nto measure the variance between original input and the reconstructed is mean squared error (MSE)\nloss:\n$L_{AE}(\\theta, \\Phi) = \\frac{1}{2} \\sum_{i=1}^{n} (x^{(i)} \u2013 f_{\\theta} (g_{\\Phi}(x^{(i)})))^2$"}, {"title": "2.5 Deep Performance evaluation", "content": "Convolutional neural networks, as defined in [16], are a particular kind of neural network\ncharacterized by several layers and a feed-forward configuration. The object is a combination of\none or more convolutions arranged in layers. Feasible arrangements comprise of input, output, and\nhidden layers. The input and output layers serve distinct purposes, whereas the hidden layer is\ncommonly employed for performing multiplication or dot product operations. One can create many\ntypes of layers, including completely connected layers, normalization layers, and pooling layers\n[17].\nA deep convolutional neural network (DCNN) is a specific kind of neural network. network that\nconsists of multiple convolutional layers. This architecture allows the network to efficiently\nprocess a vast volume of data and produce accurate results. Please refer to Figure 3 for a visual\nrepresentation of this architecture. DCNN, with its weight sharing structure and pooling\nalgorithms, effectively reduces the parameter count and surpasses DNNs, especially in the analysis\nof visual pictures. CNN, or Convolutional Neural Network, is spatially invariant, meaning it does\nnot encode an object's location and orientation. If the precise positioning of data is crucial, the\napplication of CNN may not be straightforward. Convolutional Neural Networks (CNNs) are\nbecoming more used in a range of water and wastewater treatment applications.\nTraining and testing process of DCNN models were assessed using statistical parameters such as\nthe normalized root mean square error (NRMSE), mean absolute error (MAE), coefficient of\ndetermination (R2), and Nash-Sutcliffe efficiency coefficient (NSE). These parameters were\nexpressed as follows:\n$NRMSE = \\sqrt{\\frac{\\sum_{i=1}^{N} (CHF_{measured}-CHF_{Predicted})^2}{N-1}} / CHF$\n$MAE = \\frac{1}{N} \\sum_{i=1}^{N}|CHF_{measured} \u2013 CHF_{predicted}|$\n$NSE = 1 \u2013 \\frac{\\sum_{i=1}^{N} (CHF_{measured}-CHF_{Predicted})^2}{\\sum_{i=1}^{N} (CHF_{measured}-CHF_{measured})^2}$\n$R^2 =  \\frac{(\\sum_{i=1}^{N} (CHF_{measured}-CHF_{measured}) (CHF_{Predicted}-CHF_{predicted}))^2}{\\sum_{i=1}^{N} (CHF_{measured}-CHF_{measured}) (CHF_{Predicted}-CHF_{predicted})}$\nwhere: CHF: Critical Heat Flux (MW/m\u00b2),CHF: mean CHF (MW/m\u00b2)."}, {"title": "3. RESULTS AND DISCUSSION", "content": "The DCNN model consisted of 5 convolutional layers, with a similar outcome where higher layers\ndid not significantly improve loss. The training and testing were carried out on a computer\nequipped with an ASUS AMD Rayzen TM 5 R5-3550H CPU at 3.7 GHz and 16 GB of RAM.\nThe use of 3 different auto-encoders for features augmentation led to the increase of features as\nrepresented in Table 2."}, {"title": "3.1 Simple model (DCNN 3F)", "content": "Table 4 shows that the simple model in training phase produces an NRMSE value of the simple\nmodel is equal to 0.1154 indicates that, on average, the model's predictions deviate by\napproximately 11.54% from the actual values in the training dataset. With an MAE of 385.17\n(MW/m\u00b2), the model's average prediction error in the training dataset is around 385 (MW/m\u00b2).The\nR2 value of 0.9875 indicates that the model explains approximately 98.75% of the variance in the\ntraining data. With an NSE of 0.9867, the model's performance on the training dataset is quite high.\nIn testing phase, The NRMSE value of 0.1478 indicates that the model's predictions deviate by\napproximately 14.78% from the actual values, the MAE value of 420.79 (MW/m\u00b2). The R\u00b2 value\nof 0.9791 for the testing dataset indicates that the model explains approximately 97.91% of the\nvariance in the testing data. The NSE value of 0.9781 for the testing dataset indicates high\nefficiency in predicting the target variable. The model demonstrates strong performance on both\nthe training and testing datasets across multiple evaluation metrics, indicating good generalization\nability\nThe fitting between the measured and predicted CHF is shown in figure 5."}, {"title": "3.2 Hybrid models", "content": "It can be observed from table 4, that using different auto-encoders four feature augmentation\ncan increase the performance outcomes in CHF prediction. Furthermore, an explanation of the\nresults revealed that for predicting the CHF, DCNN_3F-A2 with NRMSE (0.1003), MAE\n(331.67), R\u00b2 (0.9908) and NSE (0.9899) values in the training phase, proved merit over DCNN_3F-\nA1 and DCNN_3F-A3.\nIn testing phase, the DCNN_3F-A2 with NRMSE (0.1356), MAE (354.344), R\u00b2 (0.9826) and\nNSE (0.9816) outperforms the DCNN_3F-A1 and DCNN_3F-A3 and therefore emerged as a\nreliable model.\nFigure 8 show the fitting between the measured and predicted CHF values in the entire dataset."}, {"title": "4. CONCLUSION", "content": "This research introduced the novel application of a hybrid approach using Auto-encoders (AE) and\ndeep convolutional neural networks (DCNN) for predicting Critical Heat Flux (CHF). Auto-\nencoders were employed as a technique for enhancing features. The rebuilt features were combined\nwith the main features and utilized as inputs for various hybrid deep convolutional neural networks\n(DCNN).\nThe performance trends across the models highlight the importance of carefully selecting the\nnumber of augmented features when designing hybrid models. While feature augmentation\ngenerally improves model performance, an optimal balance must be struck to avoid overfitting or\nintroducing unnecessary complexity. The DCNN_3F-A2 model's superior performance indicates\nthat two additional features, derived from autoencoders, provided the best enhancement in\npredictive accuracy without overwhelming the model with excessive information.\nIn conclusion, the study confirms that hybrid models leveraging feature augmentation through\nautoencoders can significantly improve the predictive performance of deep learning models in\ncomplex tasks such as CHF prediction. However, the effectiveness of this approach is highly\ndependent on the appropriate selection and number of augmented features.\nThe suggested AE-DCNN (3F-A2) model serves as a baseline for predictive modeling of Critical\nHeat Flux (CHF) and can be utilized as a tool in experimental management."}]}