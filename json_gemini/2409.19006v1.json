{"title": "Towards Automated Patent Workflows: AI-Orchestrated Multi-Agent Framework for Intellectual Property Management and Analysis", "authors": ["Sakhinana Sagar Srinivas", "Vijay Sri Vaikunth", "Venkataramana Runkana"], "abstract": "\"Patents are the currency of innovation, and like any currency, they need to be managed and protected\" (Gavin Potenza). Patents, as legal documents that secure intellectual property rights, play a critical role in technological innovation. The growing complexity of patent documents and the surge in patent applications have created a need for automated solutions in patent analysis. In this work, we present PatExpert, an autonomous multi-agent conversational framework designed to streamline and optimize patent-related tasks. The framework consists of a meta-agent that coordinates task-specific expert agents for various patent-related tasks and a critique agent for error handling and feedback provision. The meta-agent orchestrates specialized expert agents, each fine-tuned for specific tasks such as patent classification, acceptance, claim generation, abstractive summarization, multi-patent analysis, and scientific hypothesis generation. For multi-patent analysis, the framework incorporates advanced methods like Graph Retrieval-Augmented Generation (GRAG) to enhance response accuracy and relevance by combining semantic similarity with knowledge graphs. Error handling is managed by critique agents (Gold-LLM-as-a-Judge and Reward-LLM-as-a-Judge), which evaluate output responses for accuracy and provide iterative feedback. The framework also prioritizes explainability, ensuring transparent justifications for decisions made during patent analysis. Its comprehensive capabilities make it a valuable tool for automating complex patent workflows, enhancing efficiency, accuracy, and compliance in patent-related tasks. Empirical evidence demonstrates significant improvements in patent processing tasks, concluding that the framework offers a robust solution for automating and optimizing patent analysis.", "sections": [{"title": "1 Introduction", "content": "\"An invention is something that was 'impossible' up to then; that's why governments grant patents.\" (Robert A. Heinlein). A patent is a legal document that grants the inventor exclusive rights to make, use, and sell their invention for a specified period in exchange for public disclosure of the invention's details. Patent documents are essential for securing intellectual property rights and serve as a public record of technological innovation. While these documents may vary slightly by jurisdiction (e.g., USPTO, JPO, EPO), they generally consist of several key sections that ensure both legal protection and clear communication of the invention. A patent typically includes a title, abstract, background, summary, detailed description, claims, and illustrations. The title highlights the invention's main innovation, while the abstract provides a brief overview of the invention, focusing on its purpose, key features, and potential applications. The background section outlines existing solutions and the technical challenges the invention addresses. The detailed description provides in-depth technical specifications, including various embodiments (specific versions or implementations of the invention), processes (methods or procedures involved in making or using the invention), and use cases (practical applications or scenarios where the invention could be applied), explaining how the invention addresses the identified challenges. The claims accurately determine the scope of legal protection, outlining the legal boundaries of coverage. These claims can be"}, {"title": "2 Proposed Method", "content": "The PatExpert framework utilizes tool learning, where a meta-agent interacts with sub-agents (tools or expert models), each specializing in a specific patent-related task. This enhances the framework's ability to efficiently solve complex patent tasks by leveraging specialized models. In this paper, \"tools\" and \"expert models\" are used interchangeably to refer to the sub-agents that the meta-agent interacts with to perform specific patent-related tasks. Powered by computational engines like Meta-Llama-3.1-405B, the meta-agent serves as the central orchestrator, managing expert models (e.g., utilizing GPT-4o-mini) with precision. It autonomously addresses patent-related tasks by interpreting complex queries, reasoning, and planning-i.e., determining the step-by-step (multistep) approach and expert models required to solve the task\u2014and utilizing expert models to generate accurate, coherent responses. By learning to select and apply the appropriate expert models, the meta-agent can perform tasks beyond its pre-trained knowledge. This approach improves response accuracy and relevance, making the framework more adaptable to real-world applications. Each expert model"}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Datasets & Experimental Settings", "content": "We utilized the Harvard USPTO Patent Dataset (HUPD [13]), a large-scale, well-structured, and multi-purpose corpus of english-language patent applications filed with the USPTO between 2004 and 2018. With over 4.5 million patent documents, HUPD is significantly larger than comparable datasets (BIGPATENT [12], CLEF-IP [9], USPTO-2M [5]). Unlike other datasets that focus solely on granted patents, HUPD includes both accepted and rejected filings, providing opportunities to study the decision-making process for patent acceptance. In summary, the HUPD dataset serves as a versatile resource for advancing research in patent analysis and NLP, with applications such as patent acceptance prediction, classification, abstractive summarization, claim generation, multi-patent analysis, and scientific hypothesis generation. For our experiments, we utilized patent data from only two years (2015-2017) (https://huggingface.co/datasets/HUPD/hupd), which was split in an 80-10-10 ratio for training, validation, and testing. We report the framework's performance on the unseen test set."}, {"title": "3.2 Experimental Settings", "content": "In this work, chaining experiments for patent-related tasks involves a sequential process where specific sections of patent documents feed into each task, creating a streamlined workflow that enhances efficiency and accuracy. For patent summarization utilizes the title, abstract, background, and description to generate concise overviews, highlighting key technical details. For claim generation, the framework uses the title, abstract, background, summary, and description to automatically generate precise legal claims that reflect the patent's technical contributions. For patent classification and acceptance prediction, sections such as the title, abstract, background, summary, patent claims are analyzed by the framework to categorize patents by their technological domain and predict acceptance. Scientific hypothesis generation draws on the title, abstract, background, summary, descriptions and patent claims to identify assumptions and generate hypotheses about the patent's innovations. Multi-patent analysis leverages the title, abstract, background, summary, description, and claims from multiple patents to perform comparative analysis, using knowledge graphs to organize and retrieve information, facilitating the extraction of trends and relationships. This interconnected process ensures each task is optimized, streamlining patent analysis. Summary section in granted patents written by experts serve as ground truth. For claim generation, the legal claims in granted patents can serve as reference text. Ground truth for hypothesis generation can be based on expert-curated (Gold-LLMs like GPT-4o) hypotheses extracted from patent documents. Ground truth for multi-patent analysis can be created by benchmarking against expert (Gold-LLMs such as GPT-4o) analyses of trends, relationships, and innovations across patents. In our work, we fine-tune expert models like GPT-4o mini using datasets tailored to patent-related tasks. GPT-4o mini can be fine-tuned (see: https://openai.com/index/gpt-4o-fine-tuning/, https://platform.openai.com/docs/guides/fine-tuning) on OpenAI's servers using domain-specific datasets. This process is facilitated through OpenAI's APIs, where users upload datasets, define hyperparameters, and initiate fine-tuning. The server-side infrastructure ensures that users do not need to manage hardware or computational resources, making GPT-4o mini easily accessible for various applications via OpenAI's API platform. Fine-tuning requires adjusting key hyperparameters"}, {"title": "3.3 Evaluation Metrics", "content": "We evaluate the tool learning framework [10] using metrics across four stages: task planning, tool selection, tool calling, and response generation. In task planning, tool usage awareness (TUA) measures the framework's ability to correctly identify when external tools are needed based on the Number of Correct Tool Recognitions user's query. It is calculated as $\\frac{\\text{Number of Correct Tool Recognitions}}{\\text{Total Opportunities for Tool Recognition}}$, with values ranging from 0 to 1, where higher values indicate better tool awareness. Pass Rate (PR) evaluates how effectively the framework executes the planned tasks by measuring the ratio of successful task completions to total attempts, calculated as $\\frac{\\text{Number of Successful Task Executions}}{\\text{Total Number of Tasks Attempted}}$. The PR value ranges from 0 to 1, with higher values indicating better task execution success. Accuracy (Acc) measures how well the framework decomposes a user query into sub-tasks and sequences them correctly. It is calculated as $\\frac{\\text{Number of Correct Subtasks in Correct Order}}{\\text{Total Number of Subtasks Generated}}$, with values ranging from 0 to 1, where higher values indicate greater precision in task planning. Dependency Graph Consistency (DGC) measures the framework's ability to maintain the correct order and relationships among sub-tasks, expressed as $\\frac{\\text{Number of Consistent Dependencies Maintained}}{\\text{Total Number of Dependencies}}$. DGC also ranges from 0 to 1, with higher values indicating better robustness in handling complex task structures. These evaluation metrics evaluate the framework's accuracy, efficiency, and adaptability in dynamic problem-solving. In tool selection, key metrics include Recall@K, NDCG@K, and COMP@K. Recall@K quantifies the proportion of relevant tools within the top-K selections, with values ranging from 0 to 1, where higher values indicate better performance. It is calculated as Recall@K = $\\frac{1}{|Q|} \\sum_{q=1}^{|Q|} \\frac{|T_q^* \\cap T_q^K|}{|T_q^*|}$, where $Q$ is the set of queries, $T_q^*$ denotes the relevant (ground-truth) tools for query $q$, and $T_q^K$ represents the top-K tools selected by the framework. COMP@K is a binary completeness measure, defined as COMP@K = $\\frac{1}{|Q|} \\sum_{q=1}^{|Q|} I(T_q^* \\subseteq T_q^K)$, where the indicator function $I(T_q^* \\subseteq T_q^K)$ equals 1 if all relevant tools (i.e., $T_q^*$) are included in the top-K set $T_q^K$, and 0 otherwise. Recall@K focuses on how many relevant tools are retrieved (partial match), while COMP@K focuses on whether all relevant tools are retrieved (complete match). NDCG@K, which stands for Normalized Discounted Cumulative Gain at K, evaluates the ranking quality of retrieved tools by considering both their relevance and position, with values ranging from 0 to 1. Retrieving relevant tools is important, but retrieving them at higher ranks (earlier in the list) is more valuable. It is calculated as NDCG@K = $\\frac{1}{|Q|} \\sum_{q=1}^{|Q|} \\frac{DCG@K_q}{IDCG@K_q}$, where $DCG@K = \\sum_{i=1}^{K} \\frac{2^{g_i}-1}{log_2(i+1)}$ is the Discounted Cumulative Gain (DCG), and IDCG@K is the Ideal DCG, with $g_i$ representing the relevance score of each tool at position i, as graded by human experts. NDCG@K accounts for both the relevance of retrieved tools and their ranking. In the tool calling stage, several metrics are used to evaluate performance, including parameter consistency, error rate, parameter coverage, and execution accuracy. Parameter Consistency (PC) measures the proportion of correctly identified and formatted parameters across all queries, calculated as PC = $\\frac{1}{|Q|} \\sum_{q=1}^{|Q|} \\frac{|P_q \\cap P_R|}{|P_R|}$, where Q is the set of queries, $P_R$ represents the ideal or ground truth set of parameters that the framework should extract for query q, and $P_q$ is the set of attempted parameters. It calculates the average proportion of correctly identified parameters out of the total ground truth parameters across all queries. Error Rate (ER) evaluates the proportion of incorrectly formatted or missing parameters, defined as ER = $\\frac{1}{|Q|} \\sum_{q=1}^{|Q|} \\frac{|P_E|}{|P_R|}$, where $P_E$ represents the set of erroneous parameters for query q. Execution Accuracy (EA) assesses the success of tool invocations, expressed as EA = $\\frac{1}{|Q|} \\sum_{q=1}^{|Q|} I(E_q=1)$, where $I(E_q=1)$ is an indicator function that equals 1 if the tool invocation for query q is successful and 0 otherwise. Each of these metrics ranges from 0 to 1, with higher values indicating better performance in the tool calling stage. For response generation, metrics include BLEU, ROUGE-L, Exact Match (EM), and F1 Score. BLEU measures n-gram precision: BLEU = $BP. exp(\\sum_{n=1}^{N} W_n log p_n)$ with values ranging from 0 (worst) to 1 (best). $W_n$ represents the weight for each n-gram length (e.g., unigrams, bigrams), and $p_n$ is the precision of n-grams, indicating the proportion of n-grams in the candidate text that match the reference text. BP (brevity penalty) penalizes overly short translations. ROUGE-L evaluates the longest common"}, {"title": "3.4 Results", "content": "Table 1 presents the comparative performance of various language models and the PatExpert framework in task planning, evaluated using four key metrics: Tool Usage Awareness (TUA), Pass Rate (PR), Accuracy (Acc), and Dependency Graph Consistency (DGC). Scores range from 0 to 1, with higher values indicating better performance. Table 2 compares the models' performance in tool selection based on the Recall@K, NDCG@K, and COMP@K metrics. Similarly, Table 3 outlines tool calling performance using Parameter Consistency (PC), Error Rate (ER), and Execution Accuracy (EA). Scores range from 0 to 1, with higher values indicating better performance, except for Error Rate, where lower values are preferable. Table 4 compares models using Exact Match (EM) and F1 scores for classification and acceptance prediction tasks. Table 5 shows performance on abstractive summarization using BLEU and ROUGE-L metrics. Table 6 presents patent claim generation results, while Table 7 displays results for multi-patent analysis, both evaluated using BLEU and ROUGE-L metrics. Table 8 presents a user-centric evaluation of various language models and the PatExpert framework, assessed using metrics such as Likert-Scale Satisfaction (LSS), Task Completion (TC), Context Awareness (CA), Adaptability (AD), Error Handling (EH), and Qualitative Feedback (QF). Table 9 compares the quality of knowledge graph construction between the PatExpert framework utilizing GPT-4o and various language models, using metrics such as Triple Accuracy (TA), Modularity (Mod), Conductance (Cond), and Graph Completeness (GC). In our experiments, baseline results for closed-source models like OpenAI GPT-4, Gemini 1.5 Pro, and Claude 3 were obtained without fine-tuning, given the impracticality of such an approach due to their large size and resource demands on consumer hardware. These proprietary models, accessed via their APIs, were evaluated directly on patent-related tasks without additional tuning. In contrast, our proposed framework PatExpert employs a fine-tuning strategy for the computational engines utilized by expert sub-agents, such as GPT-4o mini, customizing them for specific patent-related tasks. This fine-tuning enables the framework to achieve higher task-specific accuracy and relevance, especially in complex scenarios requiring patent expertise. While the frozen baseline models generally perform well due to their large-scale pre-training, PatExpert's fine-tuned agents demonstrate competitive results, especially in specialized patent workflows. This highlights the benefit of task-specific fine-tuning in improving expert model performance for domain-specific applications. Across all tables, PatExpert consistently outperforms other models, achieving the highest scores in task planning, tool selection, and tool calling metrics. It leads with a TUA of 0.94, Acc of 0.91, and DGC of 0.95, reflecting its superior ability to recognize tools and maintain task consistency. In tool selection, PatExpert also excels with a Recall@K of 0.95 and an NDCG@K of 0.93, demonstrating high accuracy in selecting and ranking relevant tools. Finally, in tool calling, it shows exceptional performance with a PC of 0.95 and an EA of 0.96, along with the lowest ER of 0.04, indicating minimal errors and high execution accuracy. OpenAI GPT-4 and Claude 3 Opus follow closely in all metrics, while models like Gemini 1.5 Pro and Gemini 1.5 Flash perform moderately. OpenAI GPT-4 Turbo and Claude 3 Haiku score the lowest. Tables 4, 5, 6, and 7 demonstrate that the PatExpert framework consistently surpasses other models with a significant margin in tasks such as patent classification, acceptance, abstractive summarization, claim generation, and multi-patent analysis."}, {"title": "4 Conclusion", "content": "In this work, we introduce PatExpert, an autonomous multi-agent conversational framework for automating and optimizing patent-related tasks, including classification, acceptance prediction, claim generation, multi-patent analysis, and scientific hypothesis generation. The framework employs a meta-agent to orchestrate task-specific expert agents, dynamically selecting and invoking them based on task requirements. This approach enhances patent workflows' efficiency and accuracy while emphasizing explainability and transparency. The integration of a critique agent, utilizing Gold-LLM-as-a-Judge and Reward-LLM-as-a-Judge, ensures robust error handling and iterative feedback, contributing to the framework's reliability. Advanced methodologies such as Graph Retrieval-Augmented Generation (GRAG) and synthetic data generation using the Mixture-of-Agents (MoA) approach have improved Retrieval-Augmented Fine-Tuning (RAFT) of expert models for multi-patent analysis. Empirical results show that PatExpert improves efficiency and precision in patent processing, reducing manual effort and enhancing compliance. Future work will expand the framework's capabilities to handle multilingual processing and patent translation."}, {"title": "A.1 Multi-Patent Analysis", "content": "Multi-patent analysis involves examining and comparing multiple patents to identify trends, relationships, and insights across innovations or technologies. General-purpose LLMs using Retrieval-Augmented Generation (RAG) are not inherently equipped to incorporate external information in open-domain question answering (ODQA) to ground responses in factual data for patent-related tasks. We propose a methodology to enhance LLMs performance in ODQA for multi-patent analysis tasks by utilizing the Retrieval-Augmented Fine-Tuning (RAFT) methodology. RAFT integrates dynamically retrieved content during fine-tuning, improving LLMs ability to generate accurate, context-grounded responses. Our approach includes generating high-quality synthetic RAFT datasets (question-context-answer (QCA) triples) tailored for patents using a multi-step pipeline involving the Mixture-of-Agents (MoA) methodology, where multiple Gold-LLMs collaborate to generate contextually accurate answers. Quality evaluation techniques ensure the robustness and effectiveness of the synthetic RAFT dataset. The expert model for the multi-patent analysis task is fine-tuned on the synthetic dataset, enabling it to produce more accurate responses. Additionally, we construct a knowledge graph from patent documents to facilitate efficient semantic search and retrieval of relevant information. During inference, the expert model accesses the knowledge graph to provide auxiliary context, generating coherent, grounded, and accurate responses to complex patent-related queries. In the following sections, we discuss the synthetic data generation process, including the creation of question-context-answer (QCA) triples, the Mixture-of-Agents (MoA) approach, and quality evaluation techniques. We also cover the integration of the knowledge graph and its impact on improving the expert model's performance in patent analysis tasks during inference."}, {"title": "A.1.1 Synthetic Data Generation", "content": "Retrieval-Augmented Generation (RAG [6]) provides LLMs with relevant external information from document databases, enabling them to generate outputs that are more contextually accurate, detailed, and grounded for ODQA tasks. This approach helps overcome the limitations of static, pre-trained knowledge in LLMs. In traditional RAG, documents are parsed and processed to extract text, then divided into smaller chunks using fixed-size chunking strategies to facilitate more precise retrieval of relevant content. Each chunk is embedded into a low-dimensional dense vector space that captures its semantic content, allowing for efficient indexing and retrieval of relevant chunks in response to queries. This method enhances generation by conditioning the language model on retrieved, contextually relevant chunks, leading to more accurate and grounded outputs. However, traditional RAG methods face several challenges that limit their effectiveness. These methods primarily rely on small text chunks, requiring the retriever to search through a large database to find relevant information. This can be inefficient, as the retriever often needs to recall numerous text chunks, sometimes necessitating re-ranking to optimize performance. Moreover, small text chunks can lead to semantic incompleteness and the loss of critical details due to document truncation. Dividing crucial context or concepts into multiple segments can impair coherence. Choosing an optimal chunk size is challenging: if chunks are too small, context is lost; if they are too large, retrieval becomes less precise-clearly, one size does not fit all. General-purpose LLMs are typically pre-trained on large text corpora using self-supervised learning techniques, such as predicting the next word in a sentence (autoregressive models) or filling in masked tokens (masked language models). To adapt LLMs for specific tasks, they undergo fine-tuning on task-specific datasets, enhancing their ability to follow instructions, improve contextual understanding, and solve complex problems. Despite these advancements, LLMs are generally not pre-trained or fine-tuned to inherently incorporate external retrieved context from databases, which is crucial for generating more accurate answers in ODQA. To address these limitations, the Retrieval-Augmented Fine-Tuning (RAFT [15]) methodology optimizes LLMs to integrate retrieved content from external databases during fine-tuning. This approach enables language models to combine their internal parametric knowledge with dynamically retrieved external information, allowing for accurate and grounded responses in ODQA tasks. RAFT effectively overcomes the limitations of LLMs having only limited pre-trained, task-specific knowledge and their inability to utilize relevant external information from databases. However, synthetic RAFT datasets tailored for patent-related tasks are not readily available and require custom creation. To overcome these challenges, we employ a meticulous multi-step approach involving synthetic data generation and"}, {"title": "A.1.2 Knowledge Graph Modeling for Semantic Search and Retrieval", "content": "In Graph Retrieval-Augmented Generation (GRAG [3, 4]), structured information is extracted from unstructured documents using parsing techniques and integrated into a knowledge graph. This facilitates efficient indexing and retrieval of contextually relevant content from knowledge graphs, enhancing language models for better performance in ODQA tasks. The process begins with parsing documents to extract structured data, such as text. This information is then integrated into graph databases such as Neo4j Aura, Amazon Neptune, or NebulaGraph, which are designed for storing and querying graph data. These databases organize the information into nodes and relationships within property graphs, preserving contextual and semantic information. The graph database enables efficient querying of property graphs, leading to more accurate responses compared to traditional keyword-based searches. Traditional RAG approaches rely on unstructured, plain-text retrieval,"}, {"title": "A.2 Scientific Hypothesis Generation", "content": "In this work, we employed teacher-student transfer learning via knowledge distillation using Gold-LLMs, such as GPT-4o, to extract or generate scientific hypotheses from granted patents. This process created a synthetic dataset to fine-tune expert models, such as GPT-4o-mini, for the task of patent hypothesis generation. It is important to note that hypothesis generation provides a broader, conceptual explanation of the problem the invention solves and how it works, rather than focusing on legal protection. Patent claims, by contrast, define the specific legal boundaries and technical implementations that the patent protects, emphasizing the structural or procedural aspects of the invention in a precise and formal manner. We utilized Gold-LLMs, such as GPT-4o, to analyze a set of granted patents, extracting Subject-Action-Object (SAO) triplets to uncover the key hypotheses and innovations within the patent documents. The approach began by parsing the granted patents, focusing on key sections such as claims and detailed descriptions, which outline the scope of the patent's innovation. The Gold-LLM identified and extracted SAO triplets, which are essential for identifying the central technical contributions of the patents. In this context, the subject represents the core invention or technology, the action describes the method or process being applied, and the object defines the outcome or product of the action. The Gold-LLM then synthesized the SAO triplets for each patent, generating a comprehensive hypothesis that represents the innovations and technical contributions of the patents. This hypothesis provides a high-level overview of the key technologies and innovations within each patent, allowing for the identification of unique contributions, and potential advancements. By extracting a hypothesis for each patent, it becomes possible to cross-"}]}