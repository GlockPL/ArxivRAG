{"title": "CONFORMAL GENERATIVE MODELING WITH IMPROVED SAMPLE EFFICIENCY THROUGH SEQUENTIAL GREEDY FILTERING", "authors": ["Klaus-Rudolf Kladny", "Bernhard Sch\u00f6lkopf", "Michael Muehlebach"], "abstract": "Generative models lack rigorous statistical guarantees for their outputs and are therefore unreliable in safety-critical applications. In this work, we propose Sequential Conformal Prediction for Generative Models (SCOPE-Gen), a sequential conformal prediction method producing prediction sets that satisfy a rigorous statistical guarantee called conformal admissibility control. This guarantee states that with high probability, the prediction sets contain at least one admissible (or valid) example. To this end, our method first samples an initial set of i.i.d. examples from a black box generative model. Then, this set is iteratively pruned via so-called greedy filters. As a consequence of the iterative generation procedure, admissibility of the final prediction set factorizes as a Markov chain. This factorization is crucial, because it allows to control each factor separately, using conformal prediction. In comparison to prior work, our method demonstrates a large reduction in the number of admissibility evaluations during calibration. This reduction is important in safety-critical applications, where these evaluations must be conducted manually by domain experts and are therefore costly and time consuming. We highlight the advantages of our method in terms of admissibility evaluations and cardinality of the prediction sets through experiments in natural language generation and molecular graph extension tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Generative models have found extensive applications across various domains, including the generation of images (Rombach et al., 2022), molecules (Vignac et al., 2023), and natural language (Achiam et al., 2023). While these models are becoming increasingly performant, caution is required when decisions are based on generated outputs. This is particularly relevant for scenarios where errors have severe consequences. A prime example is the phenomenon of \u201cconfabulations\u201d (or \u201challucintations\") in large language models-responses that lack factual support, potentially leading to substantial harm if trusted (Bai et al., 2024; Weidinger et al., 2021). Therefore, developing effective measures to ensure strong statistical guarantees on the predictions of these models is imperative.\nThe field devoted to obtaining rigorous statistical guarantees for the predictions of machine learning models is called risk control (Angelopoulos et al., 2022; 2021; Bates et al., 2021). A prominent line of work within this field is called conformal prediction (Vovk et al., 2005; Shafer & Vovk, 2008). Conformal prediction techniques typically use a heuristic uncertainty estimate from a black box machine learning model to construct a non-conformity measure (e.g., Kato et al. 2023) that allows for generating prediction sets by selecting examples in the prediction space Y based on their non-conformity values. A typical guarantee is that with high probability, the true label y \u2208 Y (as sampled from the generative process) for a test example x is contained in this prediction set. Conformal prediction is appealing due to its simplicity and strong non-asymptotic, distribution-free guarantees (Angelopoulos & Bates, 2021). However, conformal prediction methods are typically designed for classification or low-dimensional regression tasks and they suffer from the curse of dimensionality when the prediction space Y is combinatorially large or infinite, as it is the case for drug design or natural language generation."}, {"title": "2 OBJECTIVE", "content": "We consider generating a prediction set $C_x(\\Lambda) = \\{Y\\}$, parameterized by a calibration parameter $\\Lambda \\in \\mathbb{R}^K$ computed from a labeled i.i.d. calibration set $\\{(X_i, Y_i)\\}_{i=1}^n$ with feature variable $X_i$ and ground truth prediction $Y_i$. The main objective of the present work is for $C_x$ to be admissible for a new example, with high probability. Formally, we say that $A$ satisfies (conformal) admissibility control at a given level $\\alpha \\in (0, 1)$ if\n$\\mathbb{P}(\\exists Y \\in C_x(X_{n+1}) : a(Y, Y_{n+1}) = 1) \\geq 1 - \\alpha,$\nwhere $(X_{n+1}, Y_{n+1})$ is is an additional independent sample from the same distribution as the calibration set and $a: \\mathcal{Y}^2 \\to \\{0,1\\}$ measures admissibility between generated sample $Y \\in \\mathcal{Y}$ and $Y_{n+1} \\in \\mathcal{Y}$ (Fisch et al., 2021). For instance, $a$ could measure whether a textual response from a language model $Y$ for the input $X$ is similar to a ground truth textual response $Y_{n+1}$. For completeness, we demonstrate the relationship between admissibility control (1) and the more general notion called conformal risk control (Angelopoulos et al., 2022) in App. A. For notational convenience, we introduce the set-level admissibility\n$A(C_x(x), y_t) := 1\\{\\exists y \\in C_x(x) : a(y, y_t) = 1\\}.$\nWe will generally drop dependence of $A$ on $x$ and $y_t$ and simply write $A(C_x)$ instead of $A(C_x(x), y_t)$. We also introduce the (total) admissibility\n$\\mathbb{A}(\\Lambda) := \\mathbb{P}(A(C_x(X_{n+1}), Y_{n+1})),$  which is the quantity we desire to bound from below by $1 - \\alpha$."}, {"title": "3 PRELIMINARY: CONFORMAL PREDICTION", "content": "Conformal prediction (specifically, split conformal prediction (Papadopoulos et al., 2002; Lei et al., 2015)) provides a way of generating a prediction set $C_x(x) = \\{y\\}$, parameterized by a one-dimensional calibration parameter $\\lambda \\in \\mathbb{R}$. The typical setup in conformal prediction is to control (marginal) coverage, which is defined as\n$\\mathbb{P}(Y_{n+1} \\in C_x(X_{n+1})) \\geq 1 - \\alpha.$\nConformal prediction techniques assume a (possibly random) non-conformity measure $v(x, y)$, which is typically chosen as a heuristic estimate for how incompatible x and y are. For instance, if Y is a finite set of labels and X corresponds to the feature variable (e.g., an image), a simple choice for v is to consider the softmax scores of a classifier and use $v(x, y) = 1 - softmax(x, y)$. Given a choice of non-conformity v and a calibration parameter $\\lambda$, the prediction set is constructed as\n$C_x(x) := \\{y \\in \\mathcal{Y} : v(x,y) \\leq \\lambda\\}.$\nIt can be shown that (4) is satisfied (independent of the choice of v) when calibrating $\\lambda$ as follows:\n$\\lambda^* (\\{v(x_i, Y_i)\\}_{i=1}^n ; \\alpha) = quantile (\\{v(x_i, Y_i)\\}_{i=1}^n ; \\frac{\\lfloor(1 - \\alpha)(n+1)\\rfloor}{n}),$\nwhere quantile denotes the empirical quantile of v, evaluated at the calibration points $\\{v(x_i, Y_i)\\}_{i=1}^n$. We note that (4) can be seen as a special case of admissibility control (1), where the function $a(y, y_t)$ (2) returns 1 if and only if y and $y_t$ are identical. In the current study, we extend this concept to accommodate the typically vast output space $\\mathcal{Y}$ - for instance, encompassing all possible sentences in natural language. This extension allows the function $a$ to accommodate cases where $y$ and $y_t$ are not identical but are semantically equivalent or similar. Without such a generalization, prediction sets would need to be excessively large to ensure admissibility control at the desired level (1), since they would have to account for all possible variations in structuring a sentence."}, {"title": "4 SEQUENTIAL CONFORMAL PREDICTION FOR GENERATIVE MODELS", "content": "SCOPE-Gen relies on i.i.d. calibration data $\\{(x_i, y_i)\\}_{i=1}^n$ from the true generating process. In order to control $\\mathbb{A}(\\Lambda)$ (3) at level $\\alpha$, we utilize conformal prediction \u00a7 3. The main challenge of our setting is that the output space $\\mathcal{Y}$ is combinatorially large or even infinite. This means that the prediction set determined as in (5) (with $x = \\lambda \\in \\mathbb{R}$) is intractable. Thus, we construct a prediction set directly from i.i.d. samples from a generative model $G$.\nSequential Prediction (Alg. 1). The underlying idea of SCOPE-Gen is to construct the prediction set $C_x$ in multiple sequential steps $s = 0, 1, 2$, as shown in Alg. 1 and Fig. 1. On a higher level, this process can be divided into two stages:\n1. Generation Stage (s = 0): An initial prediction set $C_x^{(0)}$ is generated by iteratively adding new i.i.d. samples from a generative model $G$, conditioned on $x$ (line 5 and line 13). A new sample $y$ is added until the value of a non-conformity measure $v$, which is updated at each iteration of the while loop (line 10), exceeds the value of the calibration parameter $\\lambda^{(0)}$ (line 12). We assume that $v$ is increasing at each iteration.\n2. Filter Stage (s > 0): A generated prediction set $C_x^{(s-1)}$ is filtered (or pruned) to $C_x^{(s)}$, as to further reduce prediction set size (i.e., $|C_x^{(s)}| \\leq |C_x^{(s-1)}|$). In this stage, filters $s = 1,2$ are be applied sequentially. Each filter is parameterized by an individual calibration parameter $\\lambda^{(s)}$ and is computed by means of a sub-sampling operation $sub\\_sample(C_x^{(s)}, C_x^{(s-1)})$. This sub-sampling is performed (line 9) until the non-conformity value $v$ exceeds $\\lambda^{(s)}$ (line 12; analogous to the generation stage) or there are no more samples left to add (line 8)."}, {"title": "4.1 GREEDY FILTERS", "content": "A core ingredient of SCOPE-Gen lies in the greedy filters. The term greedy reflects the incremental growth of the set of filtered points, $C_x^{(s)}$, over the iterations j in Alg. 1. We consider two types of filters: 1) quality filter and 2) diversity filter. Both of them are defined via a sub-sampling function sub_sample and a non-conformity update function update.\nDiversity Filter. We take inspiration from farthest point (sub-)sampling, which is common in computer vision applications (e.g., Moenning & Dodgson (2003); Qi et al. (2017)):\n$\\tilde{Y} \\sim Uniform(C_x^{(s-1)})$, if $C_x^{(s)} = \\O$\n$sub\\_sample(C_x^{(s)}, C_x^{(s-1)}) = \\{ arg \\underset{Y' \\in C_x^{(s-1)}\\\\C_x^{(s)}}{max} \\underset{y'' \\in C_x^{(s)}}{min} d(y', y''), else.$\nIntuitively, this method chooses the point in $C_x^{(s-1)}\\\\C_x^{(s)}$ with the maximum distance (as measured by some distance function d) from the already sampled subset $C_x^{(s)}$, if $C_x^{(s)}$ is non-empty. For empty $C_x^{(s)}$, it chooses a point of $C_x^{(s-1)}$, uniformly at random. For this filter, the update rule is specified as\n$update(v, y) = \\{\n-d_{max}, if v = 0 \\\\\n\\underset{y'\\in C_x^{(s)}\\\\{y\\}}{min}d(y, y'), else,\nwhere $d_{max}$ is an upper bound on the distance function (see App. C.2 for details).\nQuality Filter. Using a quality function q, we choose the point $C_x^{(s-1)}\\\\C_x^{(s)}$ with the maximum quality:\n$sub\\_sample(C_x^{(s)}, C_x^{(s-1)}) = arg \\underset{Y' \\in C_x^{(s-1)}\\\\C_x^{(s)}}{max} q(y').$\nFor the update function of the non-conformity, we introduce the natural choice\n$update(v, y) = -q(y).$\nBy default, if both filters are applied, we first apply the diversity filter."}, {"title": "4.2 CALIBRATION", "content": "For calibration, we split the i.i.d. calibration data $\\{(x_i, y_i)\\}_{i=1}^n$ into three non-overlapping subsets:\n$\\{(x_i, y_i)\\}_{i=n^{(s-1)}}^{n^{(s)}}$, $\\{(x_i, y_i)\\}_{i=n^{(1)}}^{n^{(2)}},\\{(x_i, y_i)\\}_{i=n^{(0)}}^{n^{(1)}}$,\nwhere $n^{(2)} \\leq n$. This step is paramount, since otherwise, the admissibility controls in (7) are not independent. The calibration happens sequentially (s = 0,1,2), similarly to the prediction step, where calibration for $\\lambda^{(s)}$ is performed using $\\{(x_i, y_i)\\}_{i=n^{(s-1)}}^{n^{(s)}}$. We now proceed to explain the algorithmic implementation of the generation and filter calibration, displayed in Alg. 2 and Alg. 3."}, {"title": "5 EXPERIMENTS", "content": "We compare our method against prior work and derived baselines to empirically demonstrate the advantages of SCOPE-Gen on real-world natural language tasks and a molecular generation task. As the motivation of the present work lies in sample efficiency, we generally focus on small calibration sets. The experiments in the main text consider the sum non-conformity measure with $\\gamma = 0.5$ (10). We show qualitative evaluations in App. F, further quantitative evaluations in App. E.2 and an analysis of empirically achieved admissibility (3) in App. E.1.\nWe compare our method against conformal language modeling (CLM; Quach et al. (2024)) and a baseline derived from it. CLM controls a different notion of admissibility than we do. Specifically, CLM controls\n$\\mathbb{P} [\\mathbb{P} (A(C_x) = 1 | \\mathcal{D}_{cal}) \\geq 1 - \\beta^{(1)}] \\geq 1 - \\beta^{(2)},$\nwhere $\\mathcal{D}_{cal}$ is shorthand for the calibration data $\\{(x_i, y_i)\\}_{i=1}^n$. We note that (12) controls admissibility (1) at level $\\alpha$ if $\\alpha = \\beta^{(1)} + \\beta^{(2)} - \\beta^{(1)}\\beta^{(2)}$ (we show this in App. B). In order to tighten the gap,"}, {"title": "5.1 SETUP", "content": "For sake of demonstration, all experiments shown here use automated admission functions. We leave experiments that involve admissibility checks made by human domain experts for future work. We only provide a general description in the main text and refer to App. D.1 for details regarding the setup and App. D.2 for illustrative examples of each task.\nNatural Language Generation. For all tasks, the default prediction upper bound is chosen as $max = 20$. We adopt three tasks from Quach et al. (2024) related to natural language generation. 1) TriviaQA (Joshi et al., 2017): Open-domain question answering using LLama-2 (7B) (Touvron et al., 2023), without fine-tuning. An answer y is admissible if it exactly matches one of the ground truth answers $y_t$. 2) MIMIC-CXR (Johnson et al., 2019): Radiology report generation using a pre-trained vision transformer (Dosovitskiy, 2020) and GPT2-small (Radford et al., 2019). Admissibility is assessed using soft CheXbert labels (Smit et al., 2020). 3) CNN/DM (Hermann et al., 2015): News"}, {"title": "5.2 RESULTS", "content": "SCOPE-Gen consistently outperforms CLM in terms of amount of admissibility checks (# Queries) and runtime (Time), as can be seen in Tab. 1. For instance, on MIMIC-CXR, SCOPE-Gen requires only $\\frac{4.214}{20} \\approx 21\\%$ of the admissibility queries required for CLM. The \u201cCLM reduced max\u201d baseline demonstrates that counteracting this disadvantage by reducing the maximum sample size max comes at a high price to pay: The amount of rejected calibrations increases greatly (e.g., 81.7% on MIMIC-CXR), while still being far from SCOPE-Gen in terms of performance. This decreased performance is due to the fact that the admissibility guarantee in CLM holds for this specified max parameter, also during inference. In the inference phase, however, an upper bound on the prediction set size is often irrelevant, because no more admissibility checks must be performed. In addition, SCOPE-Gen consistently generates smaller prediction sets when the count or sum non-conformity updates are chosen (see (10)). The only setting where CLM outperforms SCOPE-Gen in terms of prediction set size is for the max baseline, but not across all experiments (shown in App. \u0395)."}, {"title": "6 RELATED WORK", "content": "Conformal prediction and the more general field known as distribution-free risk control have motivated a plethora of developments related to various aspects such as causality (Lei & Cand\u00e8s, 2021; Schr\u00f6der et al., 2024), non-exchangeable data (Barber et al., 2023; Angelopoulos et al., 2024; Oliveira et al., 2024), conditional coverage (Vovk, 2012; Lei & Wasserman, 2014; Cauchois et al., 2021; Deutschmann et al., 2023), and many others. The present work is also closely related to active learning (Settles, 2009), which employs an oracle-in-the-loop framework to reduce the number of queries made to the oracle. To reduce the scope of this section, we proceed to highlight advancements in the applications of risk control in the context of sequential filtering and generative models.\nRisk Control and Sequential Filtering. Sequential (or iterative/cascading) filtering procedures are ubiquitous in the machine learning literature, as demonstrated by, e.g., Deng & Rush (2020); Lahmiri"}, {"title": "7 DISCUSSION & LIMITATIONS", "content": "Data Splitting. SCOPE-Gen partitions calibration data across the prediction steps to control the factors of the Markov chain (7). However, this segmentation leads to increased variance and looser conformal prediction sets due to reduced calibration set sizes at each step. A promising alternative could involve controlling the total risk through family-wise error rate control (Benjamini & Braun, 2002). This approach could be particularly advantageous when combined with our sequential filtering strategy, which only requires accounting for K distinct tests rather than the gk tests required by the method of Quach et al. (2024) (where K is the amount of calibration parameters and g is the amount of points on a parameter grid required for CLM).\nConformal Prediction vs. Learn-then-Test. In general, the learn-then-test framework is more flexible than conformal prediction as it does not require assumptions of monotonicity or uni-dimensional calibration parameters. Nonetheless, the present work demonstrates that reformulating the given problem into multiple conformal prediction problems has benefits (as shown experimentally \u00a7 5). In addition, the intuitive admissibility guarantee of conformal prediction (1) requires specification of a single parameter only, in comparison to the complex two-parameter guarantee of learn-then-test (12). While the latter guarantee bounds the conformal admissibility guarantee, we note that the optimization strategy employed in our experiments (see \u00a7 5) would in practice have to be conducted on a separate data set (due to data contamination), leading to decreased sample efficiency."}, {"title": "8 CONCLUSION", "content": "In the present work, we introduced SCOPE-Gen, a sample-efficient conformal prediction method for generative models. This method sequentially prunes an initial prediction set from a generative model using greedy filters, allowing the total admissibility to factorize as a Markov chain. Consequently, each pruning step can be calibrated separately. Experimentally, SCOPE-Gen has shown to require fewer queries of an admissibility function to maintain control over the total admissibility at a desired level, compared to prior work. This efficiency is particularly valuable in domains where admissibility assessments are costly, such as those requiring evaluations by human domain experts. We hope that our work inspires further research in risk control for generative models, with a focus on integrating application-specific constraints and desiderata, such as sample efficiency."}, {"title": "9 REPRODUCIBILITY STATEMENT", "content": "Our anonymized source code is available in the supplementary material. Detailed instructions for reproducing all experiments are provided in the README.md file at the top level of the code directory. All parameters for running the experiments can be found in the config folders within the script folders of each experiment folder. In addition, we will release our code on GitHub upon acceptance of our manuscript."}, {"title": "A RELATIONSHIP BETWEEN RISK CONTROL AND ADMISSIBILITY CONTROL", "content": "Conformal risk control (Angelopoulos et al., 2022) considers the generation of a prediction set Cx with calibration parameters A computed from a labeled i.i.d. calibration set $\\{(X_i, Y_i)\\}_{i=1}^n$, a new input/condition Xn+1 with label Yn+1 and a loss function l. The desired property of interest for CA is called risk control, defined as\n$\\mathbb{E}[l(C_x(X_{n+1}), Y_{n+1})] < \\alpha,$\nwhere a > 0. A classic example is the coverage loss l(Cx(x), yt) = 1{yt \u00a2 Cx(x)} (Vovk et al., 2005), which gives (13) the simple interpretation that Cx(Xn+1) contains Yn+1 with probability \u2265 1 a. In the present work, we consider the loss\nl(Cx(x),y) = 1{\u2260y \u2208 Cx(x) : a(y, y\u207a) = 1}.\nWhen inserting (14) into (13), our definition of admissibility control (1) arises naturally:\n$\\mathbb{E}[1\\{\\exists Y \\in C_x(X_{n+1}) : a(Y,Y_{n+1}) = 1\\}] < \\alpha$\n$\\Leftrightarrow \\mathbb{E}[1\\{\\exists Y \\in C_x(X_{n+1}): a(Y, Y_{n+1}) = 1\\}] \\geq 1 - \\alpha$\n$\\Leftrightarrow \\mathbb{P}(\\exists Y \\in C_x(X_{n+1}) : a(Y, Y_{n+1}) = 1) \\geq 1 - \\alpha.$"}, {"title": "B BOUNDING CONFORMAL ADMISSIBILITY BY LEARN-THEN-TEST ADMISSIBILITY", "content": "Proof. Let \u03b2(1) \u2208 (0,1) and \u03b2(2) \u2208 (0, 1) be chosen arbitrarily, such that \u03b2(1) + \u03b2(2) - \u03b2(1)\u03b2(2) = \u03b1. We further suppose that the learn-then-test guarantee\n$\\mathbb{P} [\\mathbb{P} (A(C_x) = 1 | \\mathcal{D}_{cal}) \\geq 1 - \\beta^{(1)}] \\geq 1 - \\beta^{(2)}$\nis satisfied. We now show that the admissibility is bounded from below as\n$\\mathbb{E}[1\\{A(C_x) = 1\\}] \\geq 1 - \\alpha.$\nTo this end, we observe that the admissibility\n$\\mathbb{E} [1\\{A(C_x) = 1\\}] = \\mathbb{E} [\\mathbb{P} [A(C_x) = 1 | \\mathcal{D}_{cal}]]$\ncan be split into two parts\n$\\mathbb{E} [1\\{A(C_x) = 1\\}] = \\mathbb{E} [1\\{\\mathbb{P} [A(C_x) = 1|\\mathcal{D}_{cal}] \\geq 1 - \\beta^{(1)}\\}\\mathbb{P} [A(C_x) = 1| \\mathcal{D}_{cal}]] +$\n$\\mathbb{E} [1\\{\\mathbb{P} [A(C_x) = 1|\\mathcal{D}_{cal}] < 1 - \\beta^{(1)}\\}\\mathbb{P} [A(C_x) = 1 | \\mathcal{D}_{cal}]] .\nWe note that (17) is bounded from below by 0. We further bound (16) as follows\n$\\mathbb{E} [1\\{\\mathbb{P} [A(C_x) = 1|\\mathcal{D}_{cal}] \\geq 1 - \\beta^{(1)}\\}\\mathbb{P} [A(C_x) = 1 | \\mathcal{D}_{cal}]]$\n$\\geq (1 - \\beta^{(1)}) \\cdot \\mathbb{P} [\\mathbb{P} [A(C_x) = 1 | \\mathcal{D}_{cal}] \\geq 1 - \\beta^{(1)}]$\n$\\geq (1 - \\beta^{(1)}) (1 \u2013 \\beta^{(2)}),$\nwhere the last inequality holds by the learn-then-test guarantee (15)."}, {"title": "C IMPLEMENTATION DETAILS", "content": "C.1 CHOICE OF THE INDIVIDUAL ADMISSIBILITY LEVELS\nThe most straightforward choice is to equally distribute the risk among prediction steps, resulting in\n$\\alpha^{(s)} = 1 - (1 - \\alpha)^{1/K},$\nwhere K is the amount of prediction steps (typically, K = 3). We note that (18) is somewhat analagous to a Bonferroni correction in that it also equally distributes a correction. In practice, when working with a upper bound on the sample size max (outlined in \u00a7 4.2), however, lower admissibility levels can often be attained by different choice of a(s). Specifically, we choose a larger risk level for the generation step (s = 0) and distribute the remaining risk equally among the other step(s) (s > 0). Specifically, we choose\n$\\alpha^{(0)} = 1 - (1 - \\alpha)^{(M-1)/M}$\nand\n$\\alpha^{(s)} = 1 - (1 - \\alpha)^{1/(M(K-1))}, \\forall s > 0.$\nFor all experiments (\u00a7 5), we choose M = 5."}, {"title": "C.2 DISTANCE UPPER BOUND", "content": "All distance functions in the present work are derived from text-based similarity functions such as ROUGE (Lin, 2004). Such metrics typically range from 0 to 1 and we can convert them to distance functions simply by negating them (such that they range from -1 to 0). Thus, for such metrics, we obtain a distance lower bound by choosing $d_{max} = \u22121$. If d is not bounded, we can turn it into a bounded distance d, for example by taking\n$d(y, y') := \\frac{1}{1 + d(y, y')},$\nassuming positivity of d."}, {"title": "C.3 GRID SPECIFICATION FOR CLM", "content": "In general, we observe empirically that coverage is minimized when \u03b2(2) is much smaller than \u03b2(1) (usually for $\u03b2^{(2)} \u2248 \u03b2^{(1)}/10$). For a given \u03b1, we set the grid over \u03b2(2), denoted as \u03b2(2), by selecting 10 equidistant points between \u03b1/15 and \u03b1/5. This can be mathematically represented as follows:\n$\u03b2_{i}^{(2)} \\big[\\frac{\u03b1}{15}+i \\frac{\\frac{\u03b1}{5} - \\frac{\u03b1}{15}}{9} : i = 0,1,..., 9\\big].$\nThe corresponding values $B_{i}^{(1)}$, accordingly are\n$\\beta^{(1)} = \\big[ \\frac{1 - \u03b1 - \u03b2_{i}^{(2)}}{1 - \u03b2_{i}^{(2)}} : i = 0,1,...,9 \\big].$"}, {"title": "D EXPERIMENTAL DETAILS", "content": "D.1 SETUP DETAILS\nNatural Language Generation. All of our experiments regarding natural language generation roughly follow the setup from (Quach et al., 2024). Thus, we only highlight differences to the setup considered there:\nTriviaQA. We remove duplicates automatically rather than calibrating a parameter to do this, in order to improve statistical efficiency of both methods. For CLM, we set the diversity parameter $\u03bb^{(2)} = 0.5$ such that all duplicates are removed and for SCOPE-Gen, we use a fixed duplicate removal filter instead of a diversity filter at the end of the prediction pipeline.\nMIMIC-CXR. We slightly modify the admissibility criterion due to insufficient implementation details available. Following the approach by Quach et al. (2024), which uses exact CheXbert prediction matches as labels, we obtain few admissible examples (\u2248 30%). To address this problem, we adjust the setup by extracting soft labels from the CheXbert model (using weights from the original work). Specifically, we employ a labeling function based on the F1 score\n$F1 := \\frac{2}{recall^{-1} + precision^{-1}},$\nwhere both precision and recall are computed from 14 clinically relevant labels output by the CheXbert model. This F1 score aligns with the evaluation metric used in the original work by Smit et al. (2020). We consider a prediction admissible if F1 > 0.6, a manually chosen threshold. We compare all baselines using the predictions of this model.\nMolecular Graph Extension. The weights of the used DiGress model can be found on the official GitHub page. In order to condition the generated molecular graphs on a given substructure, we employ a form of masking in the reverse diffusion process (from noise to data distribution). This is similar to masking approaches in computer vision, such as the one proposed by Lugmayr et al. (2022). However, unlike Lugmayr et al. (2022), we mask the given graph structure directly (rather than applying noise to it) and do not repeat the reverse diffusion process multiple times."}, {"title": "E ADDITIONAL EXPERIMENTS", "content": "E.1 ADMISSIBILITY ANALYSIS\nWe demonstrate the empirical admissibility levels for the MIMIC-CXR experiment at admissibility level \u03b1 = 0.3, varying calibration set size n and the used non-conformity measure. The rest of the experimental setup is identical to the setup used for all other experiments (see caption of Tab. 1)\nAs shown in the histograms of Fig. 3, for all non-conformity measures, the admissibility achieved by SCOPE-Gen (blue) is slightly conservative for low sample sizes and converges to the desired level as the calibration set size n is increased from 300 to 1200. This is similar for CLM (orange), where the achieved admissibility (using the optimization strategy described in \u00a7 5) also becomes less conservative for large sample sizes. However, in contrast to SCOPE-Gen, CLM is generally more conservative. In addition, this conservativeness is highly influenced by the choice of non-conformity measure: While the CLM gap becomes tighter for the max and sum non-conformities, this is not the case for the count non-conformity, where a large gap persists even for n = 1200.\nA further observation is that SCOPE-Gen tends to reject far fewer calibration sets at small calibration set size (n = 300). This can be seen from the bars at coverage 1., which indicate calibration sets that are rejected (rejecting a calibration set amounts to returning the entire output space Y (see \u00a7 2), which results in a 100% admissibility). We see that the blue bar (SCOPE-Gen) tends to be smaller than the orange bar (CLM)."}, {"title": "E.2 FURTHER QUANTITATIVE EVALUATIONS", "content": "We demonstrate further experiments that showcase the performance for the different non-conformity measures (count and max (\u03b3 = 0.3) in addition to sum (\u03b3 = 0.5); see (10)), including two more baselines (baselines 3) and 4) introduced in \u00a7 5).\nWe see in Tab. 2 that typically either SCOPE-Gen or one of its ablations performs best in terms of the considered evaluation metrics. An exception occurs on the CNN/DM and Molecules experiment when the max non-conformity is used, where CLM performs best in terms of prediction set size (for CNN/DM, 2.751 compared to 3.872 achieved by \u201cSCOPE-Gen gen. only", "SCOPE-Gen gen. only\" generates the smallest prediction sets. For instance, on MIMIC-CXR, \u201cSCOPE-Gen gen. only": "chieves a mean set size of 6.003 for the sum non-conformity in comparison to 7.390 when using SCOPE-Gen with both filters. However,", "only": "s consistently more expensive in terms of admissibility queries than the standard version SCOPE-Gen, by a considerable gap. This can be explained by the fact that the filters require admissibility checks in a different order than the generation step, during calibration (visualized in Fig. 2). For instance, the quality filter orders the different examples from the previous prediction set according to their estimated quality. Typically, outputs with high quality estimates are also more likely to be admissible (we recall that we do not need to keep sampling beyond the first admissible example \u00a7 4.2). This lies in contrast to the order of samples considered in the calibration of the generation parameter, where such an ordering is not performed."}, {"title": "F QUALITATIVE RESULTS", "content": "We show randomly chosen prediction sets from SCOPE-Gen in comparison to prediction sets generated by CLM. We demonstrate examples for TriviaQA and MIMIC-CXR."}, {"title": "F.1 TRIVIAQA", "content": "We calibrate both methods on a single (same) calibration set of size 1200 and \u03b1 = 0.2. Admissibility of an answer is denoted"}]}