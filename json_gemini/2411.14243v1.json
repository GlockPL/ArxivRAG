{"title": "AnywhereDoor: Multi-Target Backdoor Attacks on Object Detection", "authors": ["Jialin Lu", "Junjie Shan", "Ziqi Zhao", "Ka-Ho Chow"], "abstract": "As object detection becomes integral to many safety-critical applications, understanding its vulnerabilities is essential. Backdoor attacks, in particular, pose a significant threat by implanting hidden backdoor in a victim model, which adversaries can later exploit to trigger malicious behaviors during inference. However, current backdoor techniques are limited to static scenarios where attackers must define a malicious objective before training, locking the attack into a predetermined action without inference-time adaptability. Given the expressive output space in object detection\u2014including object existence detection, bounding box estimation, and object classification\u2014the feasibility of implanting a backdoor that provides inference-time control with a high degree of freedom remains unexplored. This paper introduces AnywhereDoor, a flexible backdoor attack tailored for object detection. Once implanted, AnywhereDoor enables adversaries to specify different attack types (object vanishing, fabrication, or misclassification) and configurations (untargeted or targeted with specific classes) to dynamically control detection behavior. This flexibility is achieved through three key innovations: (i) objective disentanglement to support a broader range of attack combinations well beyond what existing methods allow; (ii) trigger mosaicking to ensure backdoor activations are robust, even against those object detectors that extract localized regions from the input image for recognition; and (iii) strategic batching to address object-level data imbalances that otherwise hinders a balanced manipulation. Extensive experiments demonstrate that AnywhereDoor provides attackers with a high degree of control, achieving an attack success rate improvement of nearly 80% compared to adaptations of existing methods for such flexible control.", "sections": [{"title": "1. Introduction", "content": "Deep neural networks (DNNs) have revolutionized object detection [1, 23, 47-49], powering applications across autonomous vehicles [9, 22, 54], surveillance systems [27-29,43], medical imaging [30, 35, 60], and beyond [6, 32, 50, 55]. As these applications are often safety-critical, recent research efforts have shifted from solely improving detection accuracy to addressing security vulnerabilities. Among the various threats to DNNs, backdoor attacks are considered one of the serious threats in the industry [31,33]. In such attacks, a victim model appears to function normally until a secret pattern, such as a small white patch, is presented, which then causes the model to misbehave intentionally.\nMost backdoor research has centered on image classifiers, leaving two gaps in understanding object detection's vulnerabilities. First, existing studies generally assume a static, highly restrictive attack scenario where the attacker predefines a single malicious behavior and implants a corresponding trigger [33] (e.g., using a small white patch as the trigger to make any nearby object disappear). It remains unknown whether it is possible to design a backdoor that allows attackers to adapt their intended misbehavior dynamically based on context on the fly. Second, an intuitive approach to achieve dynamic behavior (i.e., implanting multiple triggers, one for each possible malicious behav-"}, {"title": "2. Related Work", "content": "Backdoor Attacks on Image Classification. Pioneered by BadNet [24], backdoor attacks exploit the excessive learning ability of DNNs [33] to link a hidden trigger to a certain output by modifying a portion of the training data to (i) attach the trigger and (ii) alter the ground-truth label. Extensive efforts have been dedicated to improving the stealthiness of backdoor attacks by designing invisible triggers in the image domain [5, 18, 34, 40, 44] or the feature space [7, 17, 51, 62], with some methods even avoiding label modification [14, 51, 53]. Recently, traditional static approaches have evolved into multi-target attacks [13,19,26,57,59] that inject multiple carefully crafted triggers into the victim model for flexible control. However, as to be shown, applying these attacks directly on object detection yield near-zero success rates because of the large number of triggers to be implanted.\nBackdoor Attacks on Object Detection. The pervasive use of object detection in safety-critical scenarios has motivated investigations like BadDet [2] on its backdoor resilience. They predefine a malicious behavior, such as making all or a certain class of objects vanished [2, 3, 8, 16, 41, 42,61], generated [2,3,8,61], or misclassified [3, 16,36,56]. These efforts currently focus on how to exploit the unique properties of object detection to create more effective yet stealthy triggers, covering the physical space [16,41,42,45, 61] and the digital space using co-existence of natural objects as a trigger [3, 36]. Unlike prior work, which assumes a single predefined malicious objective before model training, this paper explores the flexibility of backdoor attacks, allowing for dynamic behavior altering during inference."}, {"title": "3. Background", "content": "Consistent with prior multi-target backdoor attacks, we consider the threat model, where the adversary has complete control of the training process of an object detector. Once the victim model is trained, it can be released through, e.g., model zoos for downloading by model users. During the inference phase, the adversary attempts to control the victim output by specifying attack configurations and submits the trigger-injected input to the victim model."}, {"title": "3.1. Threat Model", "content": "We denote an object detection model as F\u03b8. Given a test image x, the predicted output \\(\\hat{Y}\\) is denoted as: \\(\\hat{Y} = F_{\\theta}(x) = \\{(B_i, C_i) | i = 1, 2, ..., n; C_i \\in \\{1, 2, ..., m\\}\\} \\), where Bi represents the i-th bounding box predicted by the model, and \\(\\hat{C_i}\\) denotes the corresponding predicted class label among m classes. The parameters of the model is learned by solving the following optimization problem:\n\\(\\theta^* = arg \\min_{\\theta} E_{(x,Y) \\sim D} [L_{cls}(C_i, \\hat{C_i}) + L_{loc}(B_i, \\hat{B_i})]\\)  (1)\nwhere D denotes the training dataset, Lcls is the classification loss between the true class Ci and predicted class \\(\\hat{C_i}\\), and Lloc is the localization loss between the true bounding box B\u2081 and predicted bounding box \\(\\hat{B_i}\\).\nBackdoor attacks are generally poisoning-based, meaning that the attacker adds triggers to images in the dataset"}, {"title": "3.2. Object Detection and Backdoor Attack", "content": "Fig. 2 shows the overview of AnywhereDoor. We jointly optimize the victim object detector F\u03b8 and the trigger generator network \\(G_{\\phi} : R^d \\rightarrow T\\), where \\(R^d\\) is the space of intent embeddings e, whose composition will be discussed in Sec. 4.1, and T represents the space of generated triggers. Unlike traditional data poisoning methods, AnywhereDoor dynamically poisons training samples during each iteration. Specifically, for every mini-batch sampled from the training dataset, a portion of the clean samples is strategically replaced and poisoned (as will be explained in Sec. 4.3). The poisoned image \\(x' = f(x, G_{\\phi}(e))\\), which is obtained via trigger mosaicking process that will be introduced in Sec. 4.2, is then passed to the detection model F\u03b8, which outputs the detection result \\(\\hat{Y}\\), i.e. \\(\\hat{Y} = F_{\\theta}(x') = \\{(B_i, \\hat{C_i}) | i = 1, 2, ..., n\\}\\).\nAs mentioned in Sec. 3.2, we define a poisoning function P(.) to convert the correct label Y to the poisoned label Y' = P(Y) to align with the attacker's intent. To cover as many potential attacker intents as possible, we propose five attack scenarios, as illustrated by the comparison between clean detection results and the outcomes of these scenarios shown in Fig. 3. Note that the five attack scenarios need to be covered simultaneously during a single training process.\nWe define five attack scenarios as follows:\n\u2022 Untargeted Removal"}, {"title": "4. Methodology", "content": "\u2022 All bounding boxes Bi and class labels Ci are removed: Y' = \u00d8. As shown in Fig. 3 (b), all bounding boxes are eliminated.\n\u2022 Targeted Removal\nThe bounding boxes corresponding to a specific target class Ct are removed, while others are retained: Y' = \\{(B_i, C_i) | C_i \\neq C_t\\}. As shown in Fig. 3 (c), the bounding box of person is eliminated while the other bounding box labeled as motorbike remains.\n\u2022 Untargeted Misclassification\nThe class label of every bounding box is changed to the next class, i.e. Y' = \\{(B_i, C'_i) | i = 1, 2, ..., n; C_i \\in \\{1,2,..., m\\}; C'_i = C_i \\% m + 1\\}. As shown in Fig. 3 (d), all bounding boxes are labeled as a wrong class.\n\u2022 Targeted Misclassification\nOnly the objects of a target class Ct are misclassified as a different class \\(C_t\\), while other classes remain unchanged: Y' = \\{(B_i,C'_t) | C_i = C_t\\} \\cup \\{(B_i, C_i) | C_i \\neq C_t\\}. As shown in Fig. 3 (e), victim class person is labeled as target class car, meanwhile motorbike keeps its label unchanged.\n\u2022 Untargeted Generation\nAll detected objects are duplicated with perturbations in their bounding box locations and sizes. The output becomes Y' = \\{(B_i + \\Delta B,C_i) | k = 1, 2, . . ., K\\}, where K represents the number of duplicates, and \\(\\Delta B\\) denotes the perturbations. As shown in Fig. 3 (f), based on the ground truth bounding boxes, non-existing objects are fabricated.\nThe learnable paramters of the object detection model and trigger generator, \u03b8 and \\(\\phi\\) respectively, are learned through the following optimization process:\n\\(\\theta^*, \\phi^* = arg \\min_{\\theta, \\phi} E_{(x', Y') \\sim D} [L_{cls}(C_i, \\hat{C_i}) + L_{loc}(B_i, \\hat{B_i})]\\) (2)"}, {"title": "4.1. Objective Disentanglement", "content": "The essence of backdoor attacks on object detection models lies in exploiting DNNs' excessive learning ability to link triggers with malicious outputs [33]. Existing backdoor attacks on object detection [2, 8,41] rely on fixed trigger patterns, limiting the attack to a single predefined behavior. This limitation arises due to the exponential growth in pattern-prediction associations when introducing varied triggers and behaviors, which exceeds the network's learning capacity, drastically reducing ASR by making it harder to differentiate between the many pattern-prediction pairs.\nOur proposed strategy, objective disentanglement, decouples the attack into removal and generation components, which can constitute the five attack scenarios. They are represented by two sub-vectors that make up the intent embeddings, as illustrated in Fig. 4, each being a zero-filled, one-"}, {"title": "4.2. Trigger Mosaicking", "content": "Unlike image classifiers, which process the image globally, modern object detectors divide images into sub-regions or grid cells [23, 47-49], focusing on localized areas to predict object locations and classes. Thus, using a full-size mask as the trigger [13] may result in shattered trigger and information loss with such a region-based processing manner. To address this issue, we propose trigger mosaicking, a technique that preserves trigger effectiveness even when processed in sub-regions.\nAs discussed in Sec. 3.2, given a clean input \u00e6 and trigger t, we obtain a poisoned image x' = f(x,t). Now we define the function f as: \\(f(x,t) = \\Pi_{[0,1]}[x + \\Gamma(\\epsilon \\cdot sigmoid(t))]\\). Here, \u0393(\u00b7) denotes the operation of expanding t to match \u00e6 in size by tiling it horizontally and vertically, padding uncovered regions with zero pixels. The function \\(\\Pi_{[0,1]}\\) clips pixel values to the valid range of [0, 1], and \\(\\epsilon\\) controls the trigger's maximum pixel change, impacting its stealthiness and effectiveness (will be discussed in Sec. 5.4).\nThis repetitive trigger design is essential for object detection, ensuring the trigger's presence across regions and aligning with varying receptive fields, thereby enhancing effectiveness in manipulating targeted bounding boxes."}, {"title": "4.3. Strategic Batching", "content": "In object detection backdoor attacks, the training unit is individual objects rather than entire images, as in image classification. We observed that object detection datasets suffer from severe object imbalance between classes. This imbalance involves two aspects: the variation in the occurrence frequency of different classes, and the differing co-existence rates among classes within images. As shown in Fig. 5, classes such as person dominate the datasets in terms of object number. In targeted attack scenarios requiring specific target classes, na\u00efve poisoning strategies that employ random selection proves ineffective, as it often under-trains frequent classes, lowering overall ASR. Additionally, Fig. 6 reveals a strong co-existence bias, where frequently co-existing classes, e.g. person, are more likely to become non-poisoned objects in poisoned samples during training, diluting backdoor effects.\nTo address these troubles, we propose strategic batching. Traditional poisoning methods typically rely on a fixed poisoned dataset and standard training procedures, limiting the attacker's control to data manipulation. Our approach, by contrast, introduces dynamic poisoning, selecting poisoned samples during the training process, as illustrated by Fig. 7. This strategy involves two key steps:\n(1) Target Class Selection: To mitigate class imbalance, we sample target classes based on their occurrence distribution, assigning a higher probability to more frequent classes."}, {"title": "5. Experiments", "content": "We conduct experiments across multiple object detection models and datasets to evaluate the effectiveness of our method. The experiments are implemented utilizing the mmdetection toolbox [4].\nThe datasets include two widely used benchmarks for object detection: PASCAL VOC07+12 [20, 21] containing 20 classes and MSCOCO [38] containing 80 classes. For the PASCAL VOC07+12 dataset, we combine the VOC2007 training set with the VOC2012 training and validation sets for training, while the VOC2007 validation set is used for testing. Due to the complexity of the targeted misclassification task, we selected five traffic-related classes from each dataset, restricting misclassification to occur only among these classes. The selected classes are person, car, bus, bicycle, and motorbike (referred to as motorcycle in MSCOCO).\nThe models evaluated include Faster-RCNN [49] with ResNet-50-FPN [25,37] backbone, DETR [1] with ResNet-50 backbone, and YOLOv3 [48] with DarkNet-53 [46] backbone, representing both single-stage and two-stage detectors, as well as models leveraging the Transformer architecture. To reduce computational costs, all models are initialized with weights pre-trained on the MSCOCO training set."}, {"title": "5.1. Experimental Settings", "content": "Our objectives are twofold: (1) To maintain the victim model's performance on clean samples (without trigger), minimizing the difference between Baseline mAP (normal performance without attacks) and Clean mAP (post-attack performance on clean samples). We use mAP@50 for our evaluations. (2) To achieve high attack success rate (ASR) on trigger-injected samples. The ASR measures how well the model's malicious behavior aligns with the attacker's intent. We define ASR in an object-based way, focusing on the number of bounding boxes rather than the number of samples. Formally, ASR can be expressed as:\n\\(ASR = \\frac{# of successfully manipulated bboxes}{# of total bboxes}\\)  (3)\nWe report ASR metrics for all five attack scenarios. The exact detailed calculations of ASR varies across the scenarios and can be found in the supplementary materials.\nWe initialize model parameters using pre-trained weights from MSCOCO. Faster R-CNN is pretrained for 12 epochs, then backdoor-trained for another 12 with SGD at a 0.02 learning rate. DETR is fine-tuned with AdamW at a 0.0001 learning rate for 150 epochs after 150 epochs of pre-training. YOLOv3 undergoes 273 epochs of pre-training followed by 30 epochs of poisoned training with SGD at a 0.0001 learning rate. The trigger generator for all models is trained with Adam and a learning rate of 0.1.\nBatch sizes are 8 for PASCAL VOC07+12 and 2 for MSCOCO, with a default poisoning rate of p = 0.5, meaning half of each batch is poisoned (e.g., 4 out of 8 samples for PASCAL VOC07+12 and 1 out of 2 for MSCOCO). The trigger is of size 3 \u00d7 30 \u00d7 30, with a maximum pixel change, controlled by \\(\\epsilon\\) = 0.05, ensuring pixel perturbation does not exceed 5%."}, {"title": "Metrics", "content": "Following the settings presented in Sec. 5.1, we evaluate the effectiveness of AnywhereDoor. Tab. 1 summarizes the performance of the three models on two datasets in terms of the clean performance (Clean mAP) and attack success rate (ASR) across five different attack scenarios."}, {"title": "5.2. Attack Effectiveness Evaluation", "content": "The Clean mAP reflects model performance on clean test samples after backdoor training, while Baseline mAP indicates performance without backdoor attacks. As shown in Tab. 1, Faster R-CNN and DETR on PASCAL VOC07+12 exhibit minimal Clean mAP degradation of 1.3 and 0.3, respectively. YOLOv3 shows an improvement of 5.5 over its Baseline mAP. On MSCOCO, Faster R-CNN and DETR suffer higher Clean mAP losses, sacrificing 15.5 and 22.2 to achieve high ASR. YOLOv3 again demonstrates strong Clean mAP preservation, with only a 2.6 loss.\nASR quantitatively measures the effectiveness of backdoor attacks. The attack scenarios vary in difficulty; typically, untargeted attacks achieve higher ASR than targeted ones. In Tab. 1, Faster R-CNN on PASCAL VOC07+12 attains over 80% ASR in all scenarios, with untargeted removal and misclassification surpassing 97%. DETR achieves over 90% ASR except in targeted misclassification. YOLOv3, while maintaining robust Clean mAP and excelling in removal attacks, lags behind Faster R-CNN and DETR in misclassification and generation. On MSCOCO, Faster R-CNN and DETR reach over 95% ASR in untargeted attacks; however, due to MSCOCO's increased complexity, with 80 classes compared to PASCAL VOC07+12's 20, targeted attacks are more challenging, resulting in ASR above 40%. YOLOv3 shows similar characteristics on MSCOCO: superior Clean mAP preservation and high removal ASR, but comparatively lower ASR in misclassification and generation scenarios.\nTab. 5 shows the visualization results for some samples. Since the hyperparameter epsilon, which controls the transparency of the trigger superimposed on the image, is small, it is almost impossible for our attack to leave a trace on the picture, thus avoiding the possibility of being detected by human censorship. A more detailed visual sample with explainable heatmaps from GradCAM [52] is shown is Tab. 3. Overall, despite the challenges posed by targeted attacks"}, {"title": "Attack Success Rate (ASR)", "content": "The configuration that integrates all three techniques, AnywhereDoor, is shown in blue. Obviously, it achieves high ASR across all five attack scenarios while maintaining a decent Clean mAP. Its dominance over other configurations illustrates the effectiveness of our proposed techniques, resolving the limitations seen in complex attack scenarios, especially targeted misclassification, where it achieves 80% higher ASR than baseline."}, {"title": "5.3. Ablation Study", "content": "experiments on Faster R-CNN and PASCAL VOC07+12 dataset with default hyperparameters illusrtated in Sec. 5.1.\nAs shown in Fig. 8, the radar chart compares different configurations on Clean mAP and five ASR metrics. The red polygon represents the model trained without objective disentanglement, achieving high Clean mAP but low ASR across all attack scenarios. If we remove trigger mosaicking, depicted by the green polygon, the model performs better in untargeted misclassification and generation but achieves only moderate ASR in other attack scenarios. In contrast, the model that discards strategic batching (depicted in yellow) improves ASR of all untargeted attacks but struggles with targeted scenarios, especially targeted misclassification, showing near-zero ASR."}, {"title": "5.4. Parameter Study", "content": "presents the performance of AnywhereDoor under different parameter settings. The experiments utilized the Faster R-CNN model and the PASCAL VOC07+12 dataset, with the default parameter settings described in Sec. 5.1. Overall, AnywhereDoor exhibits low sensitivity to parameter changes within a reasonable range, with performance metrics showing minor fluctuations within acceptable limits. It can be observed that the reasonable range for the learning rate is between 0.01 and 0.15, and the trigger generator is less sensitive to the learning rate compared to the object detection model. As the poisoning rate increases, various ASR metrics rise, while the Clean mAP slowly decreases. An epsilon value of at least 0.05 is required to achieve satisfactory performance; further increases in epsilon do not significantly enhance ASR and instead make the trigger on the images visible, reducing the attack's stealthiness. When epsilon is 0.05, the trigger on the images is almost imperceptible to the naked eye, as illustrated by the visualization results in Tab. 5."}, {"title": "5.5. Resilience Against Defenses", "content": "Tab. 4 evaluates AnywhereDoor's robustness against six common defenses, including input- and model-based mitigation methods. We use Faster R-CNN and PASCAL VOC07+12 with default hyperparameters from Sec. 5.1. JPEG compression quality is set to 85. Both mean and median filters use a kernel size of 3. Fine-tuning and fine-pruning undergo 12 retraining epochs, with pruning-based methods applied at a pruning rate of 0.3.\nInput-based Mitigation Defenses. Input sanitization methods, such as JPEG compression [15], mean filter, and median filter [58], aim to suppress triggers by modifying the input. As shown in Tab. 4 (a), these methods preserve clean mAP well but fail to eliminate the backdoor, as shown by the ASR values that remain high or even increase in some cases. This insensitivity of ASR indicates that AnywhereDoor's trigger patterns are resilient to minor input distortions, making these defenses ineffective.\nModel-based Mitigation Defenses. Model sanitization techniques, including Fine-tuning, Pruning, and Fine-pruning [39], were alternative approaches to counter backdoor attacks. Tab. 4 (b) shows that fine-tuning slightly reduces ASR in targeted misclassification but leaves most ASR values above 80%. Pruning and fine-pruning significantly lower ASR but also drastically reduce clean mAP to 26.8, compromising model utility. This trade-off highlights that model-based defenses partially counteract backdoors but at a high cost to performance.\nOverall, none of the these defense methods could completely eliminate the backdoor implanted by AnywhereDoor, underscoring the resilience of our approach to these mitigations."}, {"title": "6. Conclusion", "content": "We presented AnywhereDoor, a backdoor attack that allows multi-target manipulation of object detection models, introducing unprecedented flexibility beyond prior attacks reliant on predefined triggers and behaviors. By leveraging joint training of a trigger generator with the victim model and dynamic sample poisoning, AnywhereDoor overcomes challenges specific to object detection via three techniques:"}, {"title": "7. Transferability Study", "content": "We"}]}