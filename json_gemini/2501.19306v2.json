{"title": "SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling", "authors": ["Jiefeng Chen", "Jie Ren", "Xinyun Chen", "Chengrun Yang", "Ruoxi Sun", "Sercan \u00d6. Ar\u0131k"], "abstract": "Recent advancements in Large Language Models (LLMs) have created new opportunities to enhance performance on complex reasoning tasks by leveraging test-time computation. However, conventional approaches such as repeated sampling with majority voting or reward model scoring, often face diminishing returns as test-time compute scales, in addition to requiring costly task-specific reward model training. In this paper, we present Self-Enhanced Test-Time Scaling (SETS), a novel method that leverages the self-verification and self-correction capabilities of recent advanced LLMs to overcome these limitations. SETS integrates sampling, self-verification, and self-correction into a unified framework, enabling efficient and scalable test-time computation for improved capabilities at complex tasks. Through extensive experiments on challenging planning and reasoning benchmarks, compared to the alternatives, we demonstrate that SETS achieves significant performance improvements and more favorable test-time scaling laws.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have revolutionized artificial intelligence by demonstrating remarkable capabilities in planning, reasoning, and problem-solving across diverse tasks (Achiam et al., 2023; anthropic, 2024; Team et al., 2024; Touvron et al., 2023). Their success stems not only from \"training scaling\", i.e. their ability to leverage vast datasets and computational resources during training (Kaplan et al., 2020), but also from their ability to benefit from increased compute at test-time to better address more challenging queries \u2013 commonly referred to as \u201ctest-time (inference) scaling\" (Snell et al., 2024; Wu et al., 2024).\nConventional test-time scaling approaches, such as repeated sampling (Brown et al., 2024), involve generating multiple candidate solutions and selecting the optimal one using techniques like majority voting or task-specific reward models. While these approaches can be effective in certain scenarios, they have notable limitations. The performance improvements from repeated sampling often quickly plateau as test-time compute increases. Moreover, the reliance on task-specific reward models adds significant training overhead, limiting both efficiency and scalability.\nTo effectively enable more optimal scaling for test-time compute with a canonical framework, we propose an alternative approach that leverages the inherent self-verification and self-correction capabilities of LLMs. Such strategies had been underexplored, likely due to the limited effectiveness of self-correction in earlier generations of LLMs (Huang et al.). However, recent advancements in LLMs have led to significantly improved self-verification and self-correction abilities. These improvements present an opportunity to rethink test-time scaling by moving beyond repeated sampling and reward model dependency, potentially achieving greater efficiency and generalizability in solving complex tasks.\nWe introduce Self-Enhanced Test-Time Scaling (SETS) that combines Sampling, Self-Verify and Self-Correct operations to scale test-time compute. We show that this approach yields more effective test-time compute scaling compared to notable alternatives such as repeated sampling, as demonstrated for recently-developed advanced LLMs. We evaluate SETS on two challenging benchmarks: NATURAL"}, {"title": "2. Related Work", "content": "Approaches for Test-Time Compute. Recent studies have explored leveraging additional test-time compute to enhance the performance of LLMs. The scaling behaviors of various test-time compute approaches, such as repeated sampling or more sophisticated sampling strategies (Welleck et al., 2024), as well as search utilizing a verifier or a reward model, iterative refinement have been studied. Brown et al. investigated repeated sampling and showed that for tasks that have an answer verifier, the scaling law continues to hold, but for tasks without a verifier, the scaling law plateau beyond several hundred samples. Snell et al. explored scaling test-time compute by iteratively refining answers and using a separate verifier model, but their approach relied on specifically fine-tuned models. In contrast to these, we propose a novel method that leverages the inherent self-verification and self-correction capabilities of LLMs, eliminating the need for any additional fine-tuned models. We hypothesize that the rapid advancements in LLMs have led to significant improvements in various capabilities, including self-verification and self-correction. Consequently, recent powerful LLMs possess sufficiently strong self-verification and self-correction abilities to enable effective scaling of test-time compute.\nTest-Time Scaling Laws and Model Sizes. The trade-off between model sizes and test-time compute allocation is also of paramount interest. Wu et al. examined the trade-off between model sizes and generating additional tokens using strategies such as greedy search, majority voting, Best-of-N, and demonstrated that a small model paired with advanced inference algorithms can outperform larger models given the same computation budget. Zhang et al. extended the study from scaling a single LLM to a mixture of multiple LLMs, and proposed an algorithm to find the optimal compute allocation among the mixture, customized for a given task. Chen et al. observed that in multiple choice QA tasks the scaling law based on majority vote only holds for easy queries not for hard queries. In our work, we also study how the scaling law behave differently for different models, as well as at different difficulty levels of the queries, when self-verification and self-correction are utilized in the test-time inference."}, {"title": "3. Method", "content": "In the proposed Self-Enhanced Test-Time Scaling (SETS) framework, our goal is to enable the strategic utilization of increased compute to enhance the accuracy of LLM-generated outputs by leveraging the models' inherent self-verification and self-correction capabilities.\nWe consider three core operations in the design: Sampling, Self-Verify, and Self-Correct, as shown"}, {"title": "4. Scaling Laws for Test-Time Compute", "content": "We define test-time compute-optimal scaling as the strategy that selects hyperparameters \u03b8 for a given test-time approach to maximize performance within a compute budget C on a specific dataset D and LLM F at test time. Formally,\n\\(\\theta^*(C|D, F) = \\arg \\max_{\\theta \\in \\Theta} M(\\theta|D,F), s.t. H(\\theta) \\leq C,\\) (2)\nwhere \u0398 are candidate values of hyperparameters for the test-time strategy, H is the cost function that maps hyperparameters \u03b8 to the average amount of compute used for each input (e.g. average number of output tokens), and M is a performance metric such as accuracy. For the proposed method SETS, \u0398 contains two variables m and n.\nWe obtain the scaling law curve with the x-axis corresponding to budget C and the y-axis corresponding to performance M(\\theta*(C|D, F)). For any given cost x = H(\u03b8), we identify the optimal performance y = M(\\theta*(x|D, F)) among all candidate hyperparameter values in \u0398, representing this as a point (x, y) on the scaling curve. Then we connect adjacent points to plot the curve."}, {"title": "5. Experiment", "content": "5.1. Setup\nDatasets. We experiment on four benchmarks that contain complex instructions and require advanced reasoning and planning for accurate responses: Trip Planning, Meeting Planning, and Calendar Scheduling in NATURAL PLAN (Zheng et al., 2024), and LiveBench Reasoning (White et al., 2024). The details of these benchmarks can be found in Appendix A.\nPrompts. We design tailored prompts for three key operations \u2013 Sampling, Self-Verify, and Self-Correct (provided in Appendix B) to enable these operations using LLMs. For NATURAL PLAN tasks, we use two approaches to improve accuracy: (1) adding additional instructions to encourage LLMs to generate Chain-of-Thought (CoT) (Wei et al., 2022), like listing constraints in the problem before generating the solution, and (2) using controlled generation with Langfun (Peng, 2023) to obtain structured solutions (refer to Appendix C for details). We do zero-shot prompting for Self-Verify and Self-Correct \u2013 using only instructions without including any few-shot examples.\nBaselines. For fair comparison, we adopt the following baselines that sample responses multiple times and choose the final response without requiring additional model training or external reward models. BoN stands for Best-of-N."}, {"title": "5.2. Results", "content": "Improved Test-time Scaling via Self-Verify and Self-Correct. The proposed method SETS consistently outperforms the non-oracle repeated sampling baselines (Figure 2), yielding increasing accuracy gains as the test-time compute increases. For BoN with Majority Vote, the accuracy typically saturates quickly with the increase in the amount of test-time compute. BoN with Self-Verify or Self-Eval improves over BoN with Majority Vote on the NATURAL PLAN tasks, but not on the LiveBench Reasoning task. In contrast, SETS strategically allocates the compute to both self-verification and self-correction, yielding consistent accuracy improvements across all datasets.\nNotably, on Trip Planning, SETS nearly matches the performance of BoN+Oracle that uses ground-truth labels for solution selection. This indicates that when the LLM possesses strong self-verification and self-correction capabilities, SETS provides an efficient way to scale test-time compute and thus enhance overall accuracy. Further analysis with an oracle reveals that SETS with oracle selection has a marked advantage over BoN with oracle selection on Trip Planning and Meeting Planning, where solutions are long-form, while the advantage is less pronounced on the other two tasks, where solutions are short-form. This may suggest that SETS is more effective on tasks with larger and more complex solution space. Interestingly, on LiveBench Reasoning, BoN with oracle selection surpasses SETS with oracle selection. This may be attributed to lower self-verification accuracy on this task, as shown in Table 2. These findings are consistent when using the number of API calls as the measure of compute cost (Figure 6).\nBetter Confidence Calibration. We study whether SETS would yield better confidence calibration compared to the common baseline \u2013 BoN+Majority Vote. For both BoN+Majority Vote and the proposed SETS, we select the final solution y* among the m solutions y\u00b9, . . ., ym using Eq (1). We use the majority vote frequency as the confidence score: \\(\\frac{1}{m}\\sum_{i=1}^{m} 1(y_i = y^*)\\), also known as self-consistency score (Wang et al., 2022). A well calibrated confidence score can be used for selective prediction (Chen et al., 2023; Geifman and El-Yaniv, 2017; Ren et al., 2022) where the model is allowed to abstain from making predictions when the confidence is low.\nWe evaluate the calibration performance using Area Under the Receiver Operating Characteristic Curve (AUROC \u2191), Area Under the Accuracy-Coverage Curve (AUACC\u2191) (Xin et al., 2021), and Expected Calibration Error (ECE \u2193) (Guo et al., 2017; Hendrycks and Gimpel, 2016; Kadavath et al., 2022; Wang et al., 2022). These metrics assess how well a model's confidence score predicts the correctness of its responses, enabling it to abstain from low-confidence predictions (Chen et al., 2023;"}, {"title": "6. Conclusions", "content": "We introduce Self-Enhanced Test-Time Scaling (SETS), a novel approach to scaling test-time compute by leveraging LLMs' self-verification and self-correction capabilities. Unlike prior work relying on specifically fine-tuned models for test-time improvement, SETS strategically allocates compute to iteratively revise the initially sampled responses via self-verification and self-correction. Our method achieves higher quality outputs and demonstrates increasing returns in test-time scaling on the NATURAL PLAN and LiveBench Reasoning benchmarks, contrasting with the diminishing returns observed in the baseline approaches of repeated sampling. Beyond accuracy gains, SETS's confidence estimation via majority voting also leads to better calibration compared to the Self-Consistency baseline.\nWhile promising, SETS's effectiveness depends on the underlying LLM's self-verification and reasoning capabilities, which can vary across models and tasks. Future work will focus on enhancing these inherent self-critique and self-improvement abilities within LLMs. Additionally, we aim to explore SETS's adaptability to broader task domains and optimize its efficiency for low-resource settings."}, {"title": "Appendix", "content": "Table of Contents\nA Datasets\nB Prompts\nB.1 Sampling Prompt\nB.2 Self-Verify Prompt\nB.3 Self-Correct Prompt.\nB.4 Multi-choice QA Task Prompt for Self-Evaluation\nC Controlled Generation\nD Additional Results\nD.1 Cost Estimation using Number of API Calls\nD.2 Evaluating Self-Verification Performance\nD.3 Evaluating Self-Correction Performance\nD.4 The Effect of Self-Correction Rounds\nD.5 The Effect of Different LLMs\nD.6 The Impact of Task Difficulty.\nE Examples for Three Core Operations\nE.1 Sampling Example\nE.2 Self-Verify Example\nE.3 Self-Correct Example"}, {"title": "A. Datasets", "content": "We perform experiments on 4 datasets: Trip Planning, Meeting Planning and Calendar Scheduling from the NATURAL PLAN benchmarks (Zheng et al., 2024), and the LiveBench Reasoning benchmark (White et al., 2024).\nNATURAL PLAN provides 5 examples as few-shot exemplars for each task (i.e. the 5-shot setting). NATURAL PLAN also provides a controlled variable (e.g. number of people, number of cities, number of days, etc) that can indicate the difficulty level of each task. We utilize this controlled variable to understand the performance of different methods on easy and hard subset of the NATURAL PLAN datasets. In Trip Planning and Meeting Planning, the ground-truth solutions are long-form and contain multiple steps while in Calendar Scheduling, the ground-truth solutions are short-form and contain only one step.\nLiveBench Reasoning has three tasks: spatial, zebra_puzzle and web_of_lies_v2, each containing 50 test examples. The ground-truth solutions in LiveBench Reasoning are short-form and contain only one phrase.\nWe summarize the statistics of these datasets in Table 3 and show some examples of different datasets' solution formats in Listing 1."}, {"title": "B. Prompts", "content": "In this section, we present the prompts used for Sampling, Self-Verify, and Self-Correct operations.\nB.1. Sampling Prompt\nFor NATURAL PLAN benchmarks, we construct the sampling prompt by adding some additional instructions to the original task description prompt.\nSampling Prompt for Trip Planning and Calendar Scheduling\n{planning_task_description_with_demos}\nPlease first list all the constraints in the problem and then output a final solution that satisfies all the constraints.\nSampling Prompt for Meeting Planning\n{planning_task_description_with_demos}\nPlease first list all the constraints and optimization goals in the problem and then output a final solution that satisfies all the constraints and optimization goals.\nFor the LiveBench reasoning benchmark, we use the original prompt in the dataset as the sampling prompt."}, {"title": "B.2. Self-Verify Prompt", "content": "For the NATURAL PLAN benchmarks, we use the following Self-Verify prompt:\nSelf-Verify Prompt for NATURAL PLAN\n{planning_task_demos}\nYou are an expert at {task_type}. You are given a TASK of {task_type} request, and a PROPOSED SOLUTION. Your job is to:\n1. List all constraints in the TASK.\n2. Verify if the PROPOSED SOLUTION satisfies each of the constraints with justifications.\n3. Write a line of the form \"The proposed solution is correct\" or \"The proposed solution is incorrect\" at the end of your response based on your analysis.\nTASK:\n{planning_task_description_without_demos}\nPROPOSED SOLUTION:\n{solution}\nFor the LiveBench Reasoning benchmark, we use the following Self-Verify prompt:\nSelf-Verify Prompt for LiveBench Reasoning\nYou are an expert in solving problems that require reasoning. You are given a QUESTION and a PROPOSED ANSWER. Your job is to:\n1. Transform the PROPOSED ANSWER into a statement given the QUESTION and identify all constraints in the QUESTION for verifying the statement.\n2. Think step by step to verify if the statement satisfies each of the constraints.\n3. Write a line of the form \"The statement is correct\" or \"The statement is incorrect\" at the end of your response based on your analysis.\nQUESTION:\n{question}\nPROPOSED ANSWER:\n{answer}"}, {"title": "B.3. Self-Correct Prompt", "content": "For the NATURAL PLAN benchmarks, we use the following Self-Correct prompt:\nSelf-Correct Prompt for NATURAL PLAN\n{planning_task_demos}\nYou are an expert at {task_type}. You are given a TASK of {task_type} request. You are also given a set of solution-analysis pairs. Your job is to outline your step-by-step thought process for deriving a new solution.\nTASK:\n{planning_task_description_without_demos}\n{solution_and_analysis}\nFor the LiveBench Reasoning benchmark, we use the following Self-Correct prompt:"}, {"title": "B.4. Multi-choice QA Task Prompt for Self-Evaluation", "content": "For the NATURAL PLAN benchmarks, we use the following multi-choice QA task prompt:\nMulti-choice QA Task Prompt for NATURAL PLAN\n{planning_task_demos}\nYou are an expert at {task_type}. You are given a TASK of {task_type} request. You are also given a set of possible solutions. Your job is to outline your step-by-step thought process for selecting the best solution.\nTASK:\n{planning_task_description_without_demos}\n{solution_choices}\nFor the LiveBench Reasoning benchmark, we use the following multi-choice QA task prompt:\nMulti-choice QA Task Prompt for LiveBench Reasoning\nYou are an expert in solving problems that require reasoning. You are given a QUESTION and a set of possible answers. Your job is to outline your step-by-step thought process for selecting the best answer.\nQUESTION:\n{question}\n{answer_choices}"}, {"title": "C. Controlled Generation", "content": "For the NATURAL PLAN tasks, we use the controlled generation to improve the accuracy with Langfun. We show the solution Python class definition for different datasets below."}, {"title": "D. Additional Results", "content": "D.1. Cost Estimation using Number of API Calls\nWe use the average number of API calls to measure the computational cost. Figure 6 shows the scaling law curves where the x-axis is the average number of API calls and y-axis is the accuracy. The findings are the same as those where we use average number of output tokens to measure the cost.\nD.2. Evaluating Self-Verification Performance\nWe study whether more self-verification samples will improve the self-verification performance. We ask the LLM to self-verify its own proposed solution (sampled with temperature= 0) multiple times"}, {"title": "D.3. Evaluating Self-Correction Performance", "content": "We study the self-correction performance of different models by using the SELF-REFINE (Madaan et al., 2024) algorithm. We ask the LLM to self-correct its own proposed solution (sampled with temperature= 0) for up to n rounds. The results in Figure 8 show that GEMINI-1.5-Pro-002 has the strongest self-correction performance: after self-correction, the accuracy boosts significantly across all datasets. On Trip Planning, GEMINI-1.5-Pro-002 keeps improving the accuracy until n = 10 with self-correction. On Meeting Planning and Calendar Scheduling, GEMINI-1.5-Pro-002 also improves the accuracy with self-correction, but the accuracy starts to saturate when n = 5. On LiveBench Reasoning, GEMINI-1.5-Pro-002 boosts the accuracy with one round self-correction and the accuracy saturates after one round. Claude-3.5-Sonnet has strong self-correction performance on Trip Planning, but has limited self-correction performance on other datasets. On Meeting Planning and Calendar Scheduling,"}, {"title": "D.4. The Effect of Self-Correction Rounds", "content": "In Figure 9, we show the results for studying the effect of self-correction rounds on Meeting Planning and LiveBench Reasoning. On Meeting Planning, the accuracy roughly plateau around Max Number of Rounds n = 4 while on LiveBench Reasoning, setting the Max Number of Rounds n = 1 leads to the best performance. On both Meeting Planning and LiveBench Reasoning, we also observe that higher number of samples typically leads to an increased accuracy."}, {"title": "D.5. The Effect of Different LLMs", "content": "In Figure 10, we show the results for studying the effect of different LLMs on Meeting Planning and LiveBench Reasoning. On Meeting Planning, SETS significantly outperforms BoN+Majority Vote with GEMINI-1.5-Flash-002, but SETS doesn't perform well with Claude-3.5-Sonnet and GEMINI-1.5-Pro-001. On LiveBench Reasoning, SETS outperforms BoN+Majority Vote with GEMINI-1.5-Pro-001, but SETS doesn't perform well with Claude-3.5-Sonnet and GEMINI-1.5-Flash-002."}, {"title": "D.6. The Impact of Task Difficulty", "content": "In Figure 11, we show the scaling law curves for Meeting Planning Hard and Easy tasks. For Meeting Planning, if the number of people for a task is not greater than 5, then we treat it as an easy task; otherwise, we treat it as a hard task. Our findings are consistent with those described in Section 5.2."}, {"title": "E. Examples for Three Core Operations", "content": "In this section, we show the detailed prompts and responses for the three core operations (Sampling, Self-Verify, and Self-Correct) employed within SETS on Trip Planning.\nE.1. Sampling Example"}, {"title": "E.2. Self-Verify Example", "content": "Self-Verify Prompt for Improved Test-Time Scaling\nE.3. Self-Correct Example"}]}