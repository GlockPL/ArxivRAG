{"title": "More complex environments may be required to discover benefits of lifetime learning in evolving robots", "authors": ["Ege de Bruin", "Kyrre Glette", "Kai Olav Ellefsen"], "abstract": "It is well known that intra-life learning, defined as an additional controller optimization loop, is beneficial for evolving robot morphologies for locomotion. In this work, we investigate this further by comparing it in two different environments: an easy flat environment and a more challenging hills environment. We show that learning is significantly more beneficial in a hilly environment than in a flat environment and that it might be needed to evaluate robots in a more challenging environment to see the benefits of learning.", "sections": [{"title": "Introduction", "content": "In evolutionary robotics, one of the main challenges is the co-evolution of morphology and control (Cheney et al., 2016). This is because we are optimizing two components of a robot at the same time, and a control setting that works for one robot morphology might not work for its offspring at all. So an evolutionary process might get stuck at a local optimum, as it is unable to find optimal control for certain robot morphologies. Adding a lifetime learning loop for the robot to optimize its control is an intuitive way to deal with this problem (Eiben and Hart, 2020) and has shown good results (Miras et al., 2020a; Luo et al., 2022) even with randomly initialized controllers (Zhao et al., 2020; Gupta et al., 2021).\nWe investigate how adding a control learning phase to a robot's lifetime affects its performance in several types of environments, as a change in environment can produce different robots (Miras et al., 2020b), even though that is not as trivial as might be expected(Miras and Eiben, 2019). The robot's morphology is evolved using an evolutionary algorithm, while its control is optimized using Bayesian Optimization (Lan et al., 2021) with three different learning budgets: one where there is no control optimization done and the parameters are fully random, and learning budgets of 30 and 50 iterations. We compare two types of environments, an \"easy\" flat environment and a more complex hilly environment, and want to see how environment complexity relates to control learning. The main contribution of this work is that lifetime learning is more beneficial in a more challenging environment than in a completely flat environment."}, {"title": "Methods", "content": "For this work, we make use of Revolve2 1 as a modular robot framework. The morphologies are built using three different building blocks, namely a core module, a brick module, and a hinge module. Every robot has exactly one core module.\nThe control of the robot is a decentralized approach, where every hinge has its sine wave to control its angle. Every hinge also has a touch sensor that is used as input for controlling its and its neighbours' angles. The following equations are used for control:\n\n\u04e8 = A * sin(\u00a2 + P) + O\n\u00a2 = \u00a2 + \u2206 * F\nS\u2081 = s * W * sin(\u00a2 + O)\nS\u2082 = SN * WN * sin(\u00a2 + O)\n\n(1)\nIn these equations, is the output for the controller, A is\nthe amplitude, P is the phase offset and F is the frequency.\nThe input of a hinge's touch sensor is s, which is a binary\nvalue, and W is the weight of the touch sensor. The input\nof neighbouring hinge's touch sensors, where a neighbour\nis defined as a hinge within a manhattan distance of 2, is\nSN, which is a binary value as well that is 1 if at least one\nneighbour's touch sensor is activated, and WN is the weight\nof the neighbour touch sensor. Finally, O is the phase offset\nwhen a touch sensor is activated. A, P, W, WN and O are\nthe five learnable parameters, and F is set to 4. Even though\nthe control is decentralized, hinges might share parameters.\nWe will experiment with different numbers of sets of control\nparameters, that can be used by the hinges."}, {"title": "Genotype", "content": "The robot morphology is directly encoded in the genotype. Every robot has a core module and the first robots are initialized by adding several random modules in random possible slots iteratively. If a hinge module is added, it chooses one of the possible controller parameters randomly. Crossover"}, {"title": "Evolution and Learning", "content": "All experiments will evolve the robot morphology, where every morphology will go through a learning phase to find the best control parameters. The morphology evolution loop is a standard evolutionary algorithm, with tournament selection as survivor and parent selection. For the survivor selection, both the original population and offspring population are evaluated in the tournaments.\nThe learning algorithm is Bayesian Optimization, where controller parameters are learned from scratch, meaning there is no controller inheritance. We use the Matern 5/2 kernel with a length scale of 0.2, and the Upper Confidence Bound as an acquisition function with an exploration variable of 3, which has been used and worked well before (Lan et al., 2021; van Diggelen et al., 2021). Before starting the Bayesian Optimization, 10% of the learning budget will be used to generate random samples, these are sampled using Latin Hypercube Sampling. We experiment with different numbers of learning iterations, namely 1, 30, and 50. A learning budget of 1 means that there is no Bayesian Optimization used and a robot morphology's performance is based on only a single run. In this case, the control parameters are fully random at each evaluation."}, {"title": "Simulation", "content": "As Revolve2 is used as the framework for robot evolution, the robots are simulated using MuJoCo. The robots are eval-"}, {"title": "Results", "content": "To see how adding a controller learning phase affects the performance, we compare the fitness over the number of morphologies evaluated, and over the number of function evaluations. Comparing on morphologies makes sense if the cost of creating a robot is higher than evaluating a robot and visa versa. The top two plots of Figure 2 show the results on the flat environment. When comparing the fitness over morphologies, it is clear that the runs with a single learning evaluation perform relatively poorly. This makes sense, as experiments with a higher learning budget have more function evaluations per morphology to find better control parameters. This is why we also plot the fitness over function"}, {"title": "Conclusion", "content": "We compared different learning budgets when evolving morphology and control on two types of terrains and showed that learning control is more beneficial in a more challenging environment. That learning is more beneficial is visible when comparing on how many morphologies are evaluated, but when comparing on function evaluations the difference is more explicit in a more challenging environment than in an easier environment. In a flat environment, morphology evolution seems to solve the problem of locomotion without the need for learning, and to study the effect of intra-life learning a more challenging environment is needed.\nFor future work, we would like to compare the results to an evolutionary search with no learning, as currently there is no evolutionary search on controller parameters and this might improve performance in the no-learning case significantly. This can also be extended by adding it to the learning experiments, adding Lamarckian inheritance to the robots' control. There is also some work to be done on optimizing the robots' control, for example optimizing the sine wave equations, removing unnecessary parameters, and optimizing the number of controllers a robot can choose from to find a good balance between complexity and reusability."}]}