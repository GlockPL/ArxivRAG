{"title": "TEST TIME LEARNING FOR TIME SERIES FORECASTING", "authors": ["Panayiotis Christou", "Shichu Chen", "Xupeng Chen", "Parijat Dube"], "abstract": "Time-series forecasting has seen significant advancements with the introduction of token prediction mechanisms such as multi-head attention. However, these methods often struggle to achieve the same performance as in language modeling, primarily due to the quadratic computational cost and the complexity of capturing long-range dependencies in time-series data. State-space models (SSMs), such as Mamba, have shown promise in addressing these challenges by offering efficient solutions with linear RNNs capable of modeling long sequences with larger context windows. However, there remains room for improvement in accuracy and scalability.\nWe propose the use of Test-Time Training (TTT) modules in a parallel architecture to enhance performance in long-term time series forecasting. Through extensive experiments on standard benchmark datasets, we demonstrate that TTT modules consistently outperform state-of-the-art models, including the Mamba-based TimeMachine, particularly in scenarios involving extended sequence and prediction lengths. Our results show significant improvements in Mean Squared Error (MSE) and Mean Absolute Error (MAE), especially on larger datasets such as Electricity, Traffic, and Weather, underscoring the effectiveness of TTT in capturing long-range dependencies. Additionally, we explore various convolutional architectures within the TTT framework, showing that even simple configurations like 1D convolution with small filters can achieve competitive results. This work sets a new benchmark for time-series forecasting and lays the groundwork for future research in scalable, high-performance forecasting models.", "sections": [{"title": "1 Introduction", "content": "Long Time Series Forecasting (LTSF) is a crucial task in various fields, including energy, industry, defense, and atmospheric sciences. LTSF uses a historical sequence of observations, known as the look-back window, to predict future values through a learned or mathematically induced model. However, the stochastic nature of real-world events makes LTSF challenging. Deep learning models, including time series forecasting, have been widely adopted in engineering and scientific fields. Early approaches employed Recurrent Neural Networks (RNNs) to capture long-range dependencies in sequential data like time series. However, recurrent architectures like RNNs have limited memory retention, are difficult to parallelize, and have constrained expressive capacity. Transformers [25], with ability to efficiently process sequential data in parallel and capture contextual information, have significantly improved performance on time series prediction task [26, 16, 20, 3]. Yet, due to the quadratic complexity of attention mechanisms"}, {"title": "2 Related Work", "content": "Transformers for LTSF Several Transformer-based models have significantly advanced long-term time series forecasting (LTSF), with iTransformer [18] and PatchTST [21] being notable examples. iTransformer uses multimodal interactive attention to model both temporal and inter-modal dependencies, making it ideal for multivariate time series. However, its reliance on multimodal data can raise computational costs when such interactions are absent. PatchTST, inspired by Vision Transformers (ViTs) [6], divides input sequences into patches to capture long- and short-term dependencies, but its performance depends on choosing the correct patch size, and it may reduce interpretability.\nOther influential models include Informer [31], which reduces complexity with sparse self-attention but can miss finer details in multivariate data; Autoformer [28], which captures periodicity and trends but struggles with non-periodic data; Pyraformer [16], which handles multi-scale patterns via a hierarchical structure but increases computational cost; and Fedformer [32], which combines time and frequency representations to reduce overhead but may underperform on noisy time series. Each model improves LTSF in specific ways but also introduces its own limitations.\nState Space Models for LTSF [8, 9, 10] are efficient sequence models designed for long-term time series forecasting (LTSF) due to their linear complexity. S4 models use four key components: A (discretization step size), A (state update matrix), B (input matrix), and C (output matrix), operating in two modes: linear recurrence for autoregressive inference and global convolution for parallelizable training. The recurrent mode stores historical information similar to RNNs, while the convolutional mode transforms the recurrence into a convolution via Toeplitz matrices, making computation more efficient. However, S4 struggles with time-invariance issues, limiting its ability to selectively memorize or forget data. Mamba [7] addresses this by making B, C, and A dynamic functions of the input, creating time-dependent parameters that adapt to varying contexts, filtering noise and maintaining Transformer-level performance with linear complexity. SIMBA [23] further enhances these models by incorporating a simplified block-sparse attention mechanism, blending state space models with attention to efficiently capture long-range dependencies while reducing computational overhead. This hybrid approach allows SIMBA to handle large-scale, noisy, or irregular time series more effectively than purely recurrent models like S4. TimeMachine[1] extends these innovations by using a quadruple Mamba setup to manage both channel mixing and channel independence. TimeMachine avoids the quadratic complexity of Transformers like iTransformer and PatchTST by using multi-scale context generation and selective scanning, enabling it to capture long-range dependencies efficiently while maintaining high performance in long-term forecasting tasks.\nLinear RNNs for LTSF RWKV-TS [11], is a novel linear RNN architecture specifically designed for time series tasks. Compared to traditional RNN models like LSTM and GRU, RWKV-TS achieves O(L) time and memory complexity, enhances long-range information capturing capabilities, and exhibits high computational efficiency and scalability. Orvieto et al. (2023 [22]) demonstrated that carefully designed deep RNNs can match the performance of deep state-space models (SSMs) on long-range reasoning tasks. They introduced the Linear Recurrent Unit (LRU), an RNN block that achieves similar performance to S4/S4D/S5 on the Long Range Arena benchmark while maintaining their computational efficiency. TTT [24] layers introduce a new approach where the hidden state is treated as a trainable machine learning model and it learns not only during training but at test time as well. Instead of using a single fixed hidden state vector, TTT employs dynamically updated weights (in the form of matrices inside linear or MLP layers) to compress and store historical data. This dynamic adjustment during test time allows TTT to better capture long-term relationships by continuously incorporating new information. TTT also utilizes self-supervised training during the testing process, updating model weights in real-time to reconstruct corrupted input tokens. This method provides an efficient alternative to self-attention, maintaining linear complexity while offering the same interface as RNN layers, making it easily integrable into larger architectures. Additionally, TTT is designed to parallelize effectively, overcoming the typical scaling issues of RNNs. By improving the ability to process longer contexts with linear complexity, TTT offers a compelling alternative to Transformer-based architectures, which struggle with quadratic complexity due to self-attention.\nMLPs and CNNs for LTSF Recent advancements in long-term time series forecasting (LTSF) have introduced more efficient architectures that avoid the complexity of attention mechanisms and recurrence. One notable model is TSMixer [2], an MLP-based approach that separates temporal and feature interactions through time-mixing and channel-mixing operations, achieving linear scaling with sequence length. Despite its simplicity, TSMixer performs competitively across benchmarks, showing that complex architectures like Transformers and RNNs are not always necessary. However, MLP-based models can struggle with capturing long-range dependencies, and their performance may depend heavily on hyperparameter tuning, especially for smaller or highly non-linear datasets. Convolutional neural networks (CNNs) have also proven effective for LTSF, particularly in capturing local temporal patterns. ModernTCN[5] revisits temporal convolution networks (TCNs) and introduces CNN-based architectures for efficient long-term forecasting. By using dilated convolutions and a hierarchical structure, ModernTCN captures both short- and long-range dependencies while"}, {"title": "3 Model Architecture", "content": "The task of Time Series Forecasting can be defined as follows: Given a multivariate time series dataset with a window of past observations (look-back window) L: $(x_1, . . ., x_L)$, where each $x_t$ is a vector of dimension M (the number of channels at time t), the goal is to predict the next T future values $(x_{L+1},..., x_{L+T})$.\nThe TimeMachine [1] architecture, which we used as the backbone, is designed to capture long-term dependencies in multivariate time series data, offering linear scalability and a small memory footprint. It integrates four Mamba [7] modules as sequence modeling blocks to selectively memorize or forget historical data, and employs two levels of downsampling to generate contextual cues at both high and low resolutions.\nHowever, Mamba's approach still relies on fixed-size hidden states to compress historical information over time, often leading to the model forgetting earlier information in long sequences. TTT [24] uses dynamically updated weights (in the form of matrices inside linear or MLP layers) to compress and store historical data. This dynamic adjustment during test time allows TTT to better capture long-term relationships by continuously incorporating new information. Its Hidden State Updating Rule is defined as:\n$W_t = W_{t-1} - \\eta\\nabla l(W_{t-1}; x_t) = W_{t-1} \u2013 \\eta\\nabla||f(x_t; W) \u2013 x_t ||^2$\nWe incorporated TTT into the TimeMachine model, replacing the original Mamba block. We evaluated our approach with various setups, including different backbones and TTT layer configurations. Additionally, we introduced convolutional layers before the sequence modeling block and conducted experiments with different context lengths and prediction lengths.\nOur goal is to improve upon the performance of the state-of-the-art (SOTA) models in LTSF using the latest advancements in sequential modeling. Specifically, we integrate Test-Time Training (TTT) modules [24] into our model for two key reasons, TTT is theoretically proven to have an extremely long context window, being a form of linear RNN [22], capable of capturing long-range dependencies efficiently. Secondly, the expressive hidden states of TTT allow the model to capture a diverse set of features without being constrained by the architecture, including the depth of the hidden layers, their size, or the types of blocks used."}, {"title": "3.1 General Architecture", "content": "Our model architecture builds upon the TimeMachine model [1], introducing key modifications, as shown in Figure 1a, 1b and 1c. Specifically, we replace the Mamba modules in TimeMachine with TTT (Test-Time Training) modules [24], which retain compatibility since both are linear RNNs [22]. However, TTT offers superior long-range dependency modeling due to its adaptive nature and theoretically infinite context window. A detailed visualization of the TTT block and the different proposed architectures can be found in Appendix B\nOur model features a two-level hierarchical architecture that captures both high-resolution and low-resolution context, as illustrated in Figure la. To adapts to the specific characteristics of the dataset, the architecture handles two scenarios-Channel Mixing and Channel Independence-illustrated in Figure 1b and 1c respectively, while Figure la depicts the Channel Mixing scenario. A more detailed and mathematical description of the normalization and prediction procedures can be found in Appendix B."}, {"title": "3.2 Hierarchical Embedding", "content": "The input sequence BML (Batch, Channel, Length) is first passed through Reversible Instance Normalization [12] (RevIN), which stabilizes the model by normalizing the input data and helps mitigate distribution shifts. This operation is essential for improving generalization across datasets.\nAfter normalization, the sequence passes through two linear embedding layers. Linear El and Linear E2 are used to map the input sequence into two embedding levels: higher resolution and lower resolution. The embedding operations $E_1: R^{M\\times L}\\rightarrow R^{M\\times n1}$ and $E_2 : R^{M\\times n1} \\rightarrow R^{M\\times n2}$ are achieved through MLP. n1 and n2 are configurations that take values from {512, 256, 128, 64, 32}, satisfying $n_1 > n_2$. Dropout layers are applied after each embedding layer to prevent overfitting, especially for long-term time series data. As shown in Figure 1a."}, {"title": "3.3 Two Level Contextual Cue Modeling", "content": "At each of the two embedding levels, a contextual cues modeling block processes the output from the Dropout layer following E1 and E2. This hierarchical architecture captures both fine-grained and broad temporal patterns, leading to improved forecasting accuracy for long-term time series data.\nIn Level 1, High-Resolution Contextual Cues Modeling is responsible for modeling high-resolution contextual cues. TTT Block 3 and TTT Block 4 process the input tensor, focusing on capturing fine-grained temporal dependencies. The TTT Block3 operates directly on the input, and transposition may be applied before TTT Block4 if necessary. The outputs are summed, then concatenated with the Level 2 output. There is no residual connection summing in Level 1 modeling.\nIn Level 2, Low-Resolution Contextual Cues Modeling handles broader temporal patterns, functioning similarly to Level 1. TTT Block 1 and TTT Block 2 process the input tensor to capture low-resolution temporal cues and add them togther. A linear projection layer (P-1) is then applied to maps the output (with dimension $R^{M\\times n2}$) to a higher dimension $R^{M\\times n1}$, preparing it for concatenation. Additionally, the Level 1 and Level 2 Residual Connections ensure that information from previous layers is effectively preserved and passed on."}, {"title": "3.4 Final Prediction", "content": "After processing both high-resolution and low-resolution cues, the model concatenates the outputs from both levels. A final linear projection layer (P-2) is then applied to generate the output predictions. The output is subsequently passed through RevIN Denormalization, which reverses the initial normalization and maps the output back to its original scale for interpretability. For more detailed explanations and mathematical descriptions refer to Appendix B."}, {"title": "3.5 Channel Mixing and Independence Modes", "content": "The Channel Mixing Mode(Figure 1a and 1b) processes all channels of a multivariate time series together, allowing the model to capture potential correlations between different channels and understand their interactions over longer time. Figure la illustrates an example of the channel mixing case, but there is also a channel independence case corresponding to Figure 1a, which we have not shown here. Figures 1b and 1c demonstrate the channel mixing and independence"}, {"title": "4 Experiments and Evaluation", "content": "To validate the new architecture, we conducted extensive experiments, exploring various configurations of model parameters to assess the impact on performance. This included evaluating different sequence and prediction lengths, and experimenting with alternative architectures applied in cascade on the forecasting task, leveraging the flexibility provided by TTT modules."}, {"title": "4.1 Original Experimental Setup", "content": "We evaluate our model on seven benchmark datasets that are commonly used for LTSF, namely: Traffic, Weather, Electricity, ETTh1, ETTh2, ETTm1, and ETTm2 from Wu et al. [28], and Zhou et al. [31]. Among these, the Traffic and Electricity datasets are significantly larger, with 862 and 321 channels, respectively, and each containing tens of thousands of temporal points."}, {"title": "4.2 Quantitative Results", "content": "Across all seven benchmark datasets, our TimeMachine-TTT model consistently demonstrated superior performance compared to SOTA models. In the Weather dataset, TTT achieved leading performance at longer horizons (336 and 720), with MSEs of 0.248 and 0.341, respectively, outperforming TimeMachine, which recorded MSEs of 0.256 and 0.342. The Traffic dataset, with its high number of channels (862), also saw TTT outperform TimeMachine and iTransformer at both short (96-step MSE of 0.394 vs. 0.397) and long horizons (720-step MSE of 0.464 vs. 0.467), highlighting the model's ability to handle multivariate time series data.\nIn the Electricity dataset, TTT showed dominant results across all horizons, achieving an MSE of 0.140 at horizon 96 and 0.201 at horizon 720, outperforming TimeMachine and PatchTST. For ETTh1, TTT was highly competitive, with strong short-term results (MSE of 0.358 at horizon 96) and continued dominance at medium-term horizons like 336, with an MSE of 0.429. While ETTh2 showed TimeMachine slightly ahead at short horizons, TTT closed the gap at longer horizons (MSE of 0.445 at horizon 720 compared to 0.411 for TimeMachine).\nFor the ETTm1 dataset, TTT outperformed TimeMachine at nearly every horizon, recording an MSE of 0.315 at horizon 96 and 0.431 at horizon 720, confirming its effectiveness for long-term industrial forecasting. Similarly, in ETTm2, TTT consistently performed best at shorter horizons (MSE of 0.177 at horizon 96) and remained highly competitive at longer horizons, maintaining its lead over TimeMachine at horizon 720 (MSE of 0.362 vs. 0.371)."}, {"title": "5 Prediction Length Analysis and Ablation Study", "content": "5.1 Experimental Setup with Enhanced Architectures\nTo assess the impact of enhancing the model architecture, we conducted experiments by adding hidden layer architectures before the sequence modeling block in each of the four TTT blocks. The goal was to improve performance by enriching feature extraction through local temporal context. As shown in Figure 3. We tested the following configurations: (1) Conv 3: 1D Convolution with kernel size 3, (2) Conv 5: 1D Convolution with kernel size 5, (3) Conv Stack 3: two 1D Convolutions with kernel size 3 in cascade, (4) Conv Stack 5: two 1D Convolutions with kernel sizes 5 and 3 in"}, {"title": "5.2 Experimental Setup with Increased Prediction & Sequence Lengths", "content": "For the second part of our experiments, we extended the sequence and prediction lengths beyond the parameters tested in previous studies. We used the same baseline architectures (MLP and Linear) with the Mamba backbone as in the original TimeMachine paper, but this time also included the best-performing 1D Convolution architecture with kernel size 3.\nThe purpose of these experiments was to test the model's capacity to handle much longer sequence lengths while maintaining high prediction accuracy. We tested the following sequence and prediction lengths, with L = 2880 and 5760, far exceeding the original length of L = 96:"}, {"title": "5.3 Results and Statistical Comparisons for Proposed Architectures", "content": "General Performance The proposed architectures\u2014TTT Linear, TTT MLP, Conv Stack 3, Conv Stack 5, Conv 3, Conv 5, and Inception-demonstrate varying levels of performance across prediction horizons. TTT Linear performs well at shorter horizons, with an MSE of 0.268 and MAE of 0.298 at horizon 96, but experiences increasing error at longer horizons, with an MSE of 0.357 at horizon 336. TTT MLP follows a similar trend but with slightly worse overall performance. Conv 3 and Conv 5 outperform the Linear and MLP models at shorter horizons, achieving comparable MSE (0.269) and MAE (0.297) at horizon 96, but fall behind the more complex Conv Stack models at longer horizons. Conv Stack 5 performs best at shorter horizons with an MSE of 0.261 and MAE of 0.289 at horizon 96, while Conv Stack 3 shows a slight advantage at longer horizons with an MSE of 0.359 at horizon 336. The Inception architecture provides stable performance across horizons, closely following the Conv Stack models, with an MSE of 0.361 at horizon 336.\nImpact of Architectures The Conv Stack 5 architecture demonstrates the best overall performance among all convolutional models, particularly at shorter horizons, achieving the lowest MSE (0.261) and MAE (0.289) at horizon 96. Conv 3 and Conv 5 perform better than the simpler Linear and MLP models but are consistently outperformed by the more complex stacked architectures. At horizon 720, Conv 5 shows a marginal improvement over Conv 3, with an MSE of 0.371 compared to 0.377. The Inception architecture performs similarly to Conv Stack 5, offering consistent results across all horizons and proving particularly effective for complex, long-term predictions. Across all horizons, MSE and MAE increase as the horizon lengthens, but Conv Stack 3, Conv Stack 5, and Inception handle this increase more effectively than TimeMachine and other simpler models, which exhibit approximately 5% to 10% higher errors at longer horizons."}, {"title": "5.4 Results and Statistical Comparisons for Increased Prediction and Sequence Lengths", "content": "Both shorter and longer sequence lengths affect model performance differently. Shorter sequence lengths (e.g., 2880) provide better accuracy for shorter prediction horizons, with the TTT model achieving an MSE of 0.332 and MAE of 0.356 at a 192-step horizon, outperforming TimeMachine. Longer sequence lengths (e.g., 5760) result in higher errors, particularly for shorter horizons, but TTT remains more resilient, showing improved performance over TimeMachine."}, {"title": "5.5 Evaluation", "content": "The results of our experiments indicate that the TimeMachine-TTT model outperforms the SOTA models across various scenarios, especially when handling larger sequence and prediction lengths. Several key trends emerged from the analysis:\n\u2022 Improved Performance on Larger Datasets: On larger datasets, such as Electricity, Traffic, and Weather, TTT models demonstrated superior performance compared to TimeMachine. For example, at a prediction length of 96, the TTT architecture achieved an MSE of 0.283 compared to TimeMachine's 0.309, reflecting a notable improvement. This emphasizes TTT's ability to effectively handle larger temporal windows.\n\u2022 Better Handling of Long-Range Dependencies: TTT-based models, particularly Conv Stack 5 and Conv 3, demonstrated clear advantages in capturing long-range dependencies. As prediction lengths increased, such as at 720, TTT maintained better error rates, with Conv Stack 5 achieving an MAE of 0.373 compared to TimeMachine's 0.378. Although the difference narrows at longer horizons, the TTT architectures remain more robust, particularly in handling extended sequences and predictions.\n\u2022 Impact of Hidden Layer Architectures: While stacked convolutional architectures, such as Conv Stack 3 and Conv Stack 5, provided incremental improvements, simpler architectures like Conv 3 and Conv 5 also delivered competitive performance. Conv Stack 5 showed a reduction in MSE compared to TimeMachine, at horizon 96, where it achieved an MSE of 0.261 versus TimeMachine's 0.262.\n\u2022 Effect of Sequence and Prediction Lengths: As the sequence and prediction lengths increased, all models exhibited higher error rates. However, TTT-based architectures, particularly Conv Stack 5 and Conv 3, handled these increases better than TimeMachine. For example, at a sequence length of 5760 and prediction length of 720, TTT recorded lower MSE and MAE values, demonstrating better scalability and adaptability to larger contexts. Moreover, shorter sequence lengths (e.g., 2880) performed better at shorter horizons, while longer sequence lengths showed diminishing returns for short-term predictions.\nIn summary, our experiments reveal that the TimeMachine-TTT model, especially with architectures like Conv Stack 5 and Conv 3, significantly outperforms existing models when larger sequence and prediction lengths are employed. These findings highlight the importance of increasing both sequence and prediction lengths in time series forecasting, particularly when utilizing architectures capable of dynamically capturing long-term dependencies. Even modest modifications, such as Conv 3, can lead to substantial improvements, emphasizing the effectiveness of simpler yet powerful architectures."}, {"title": "6 Conclusion and Future Work", "content": "In this work, we improved the state-of-the-art (SOTA) model for time series forecasting by replacing the Mamba modules in the original TimeMachine model with Test-Time Training (TTT) modules, which use linear RNNs to capture long-range dependencies. Extensive experiments showed that the TTT architectures\u2014MLP and Linear-performed well, with MLP slightly outperforming Linear. Exploring alternative architectures, particularly Conv Stack 5, improved performance at longer prediction horizons. The most significant gains came from increasing sequence and prediction lengths, where our TTT models consistently matched or outperformed the SOTA model, particularly on larger datasets"}, {"title": "A Appendix A: TTT vs Mamba", "content": "Both Test-Time Training (TTT) and Mamba are powerful linear Recurrent Neural Network (RNN) architectures designed for sequence modeling tasks, including Long-Term Time Series Forecasting (LTSF). While both approaches aim to capture long-range dependencies with linear complexity, there are key differences in how they handle context windows, hidden state dynamics, and adaptability. This subsection compares the two, focusing on their theoretical formulations and practical suitability for LTSF."}, {"title": "A.1 Mamba: Gated Linear RNN via State Space Models (SSMs)", "content": "Mamba is built on the principles of State Space Models (SSMs), which describe the system's dynamics through a set of recurrence relations. The fundamental state-space equation for Mamba is defined as:\n$h_k = A h_{k-1} + B u_k, u_k = C h_k,$"}, {"title": "A.2 TTT: Test-Time Training with Dynamic Hidden States", "content": "On the other hand, Test-Time Training (TTT) introduces a more flexible mechanism for updating hidden states, enabling it to better capture long-range dependencies. TTT uses a trainable hidden state that is continuously updated at test time, allowing the model to adapt dynamically to the current input. The hidden state update rule for TTT can be defined as:\n$z_t = f(x_t; W_t), W_t = W_{t-1} - \\eta\\nabla l(W_{t-1};x_t),$\nwhere:\n\u2022 $z_t$ is the hidden state at time step t, updated based on the input $x_t$.\n\u2022 $W_t$ is the weight matrix at time step t, dynamically updated during test time.\n\u2022 $l(W; x_t)$ is the loss function, typically computed as the difference between the predicted and actual values: $l(W;x_t) = ||f(x_t; W) \u2013 x_t||^2$.\n\u2022 \u03b7 is the learning rate for updating $W_t$ during test time.\nThe key advantage of TTT over Mamba is the dynamic nature of its hidden states. Rather than relying on fixed state transitions, TTT continuously adapts its parameters based on new input data at test time. This enables TTT to have an infinite context window, as it can effectively adjust its internal representation based on all past data and current input. This dynamic adaptability makes TTT particularly suitable for LTSF tasks, where capturing long-term dependencies is crucial for accurate forecasting."}, {"title": "A.3 Comparison of Complexity and Adaptability", "content": "One of the major benefits of both Mamba and TTT is their linear complexity with respect to sequence length. Both models avoid the quadratic complexity of Transformer-based architectures, making them efficient for long time series data. However, TTT offers a distinct advantage in terms of adaptability:\n\u2022 Mamba:\n$O(L \\times D^2)$,\nwhere L is the sequence length and D is the dimension of the state space. Mamba's fixed state transition matrices limit its expressiveness over very long sequences.\n\u2022 TTT:\n$O(L \\times N \\times P)$,\nwhere N is the number of dynamic parameters (weights) and P is the number of iterations for test-time updates. The dynamic nature of TTT allows it to capture long-term dependencies more effectively, as it continuously updates the weights $W_t$ during test time."}, {"title": "B Appendix B: Model Components", "content": "B.1 TTT Block and Proposed Architectures\nBelow we illustrate the components of the TTT block and the proposed architectures we used in our ablation study for the model based on convolutional blocks:"}, {"title": "B.2 Prediction", "content": "The prediction process in our model works as follows. During inference, the input time series $(x_1, . . ., x_L)$, where L is the look-back window length, is split into M univariate series $x^{(i)} \\in R^{1\\times L}$. Each univariate series represents one channel of the multivariate time series. Specifically, an individual univariate series can be denoted as:\n$x^{(i)}_{1:L} = (x^{(i)}_1,...,x^{(i)}_L)$ where $i = 1,..., M$.\nEach of these univariate series is fed into the model, and the output of the model is a predicted series $\\hat{x}^{(i)}$ for each input channel. The model predicts the next T future values for each univariate series, which are represented as:\n$\\hat{x}^{(i)} = (x^{(i)}_{L+1},...,x^{(i)}_{L+T}) \\in R^{1\\times T}$.\nBefore feeding the input series into the TTT blocks, each series undergoes a two-stage embedding process that maps the input series into a lower-dimensional latent space. This embedding process is crucial for allowing the model to learn meaningful representations of the input data. The embedding process is mathematically represented as follows:"}, {"title": "B.3 Normalization", "content": "As part of the preprocessing pipeline, normalization operations are applied to the input series before feeding it into the TTT blocks. The input time series x is normalized into $x^0$, represented as:\n$x^0 = [x_1^0,...,x_M^0],  x_i^0 \\in R^{M\\times L}$.\nWe experiment with two different normalization techniques:\n\u2022 Z-score normalization: This normalization technique transforms the data based on the mean and standard deviation of each channel, defined as:\n$x^0_{ij} = \\frac{x_{i,j} \u2013 mean(x_{i,:})}{\\sigma_j}$\nwhere $\\sigma_j$ is the standard deviation of channel j, and $j = 1, . . ., M$.\n\u2022 Reversible Instance Normalization (RevIN) [13]: RevIN normalizes each channel based on its mean and variance but allows the normalization to be reversed after the model prediction, which ensures the output predictions are on the same scale as the original input data. We choose to use RevIN in our model because of its superior performance, as demonstrated in [1].\nOnce the model has generated the predictions, RevIN Denormalization is applied to map the normalized predictions back to the original scale of the input data, ensuring that the model outputs are interpretable and match the scale of the time series used during training."}, {"title": "C Appendix C: Analysis on Increased Prediction and Sequence Length", "content": "C.1 Effect of Sequence Length\nShorter Sequence Lengths (e.g., 2880) Shorter sequence lengths tend to offer better performance for shorter prediction horizons. For instance, with a sequence length of 2880 and a prediction length of 192, the TTT model achieves an MSE of 0.332 and an MAE of 0.356, outperforming TimeMachine, which has an MSE of 0.342 and an MAE of 0.359. This indicates that shorter sequence lengths allow the model to focus on immediate temporal patterns, improving short-horizon accuracy.\nLonger Sequence Lengths (e.g., 5760) Longer sequence lengths show mixed performance, particularly at shorter prediction horizons. For example, with a sequence length of 5760 and a prediction length of 192, the TTT model's MSE rises to 0.509 and MAE to 0.442, which is better than TimeMachine's MSE of 0.546 and MAE of 0.459. While the performance drop for TTT is less severe than for TimeMachine, longer sequence lengths can introduce unnecessary complexity, leading to diminishing returns for short-term predictions.\nC.2 Effect of Prediction Length\nShorter Prediction Lengths (96, 192) Shorter prediction lengths consistently result in lower error rates across all models. For instance, at a prediction length of 96 with a sequence length of 2880, the TTT model achieves an MSE of 0.283 and an MAE of 0.322, outperforming TimeMachine's MSE of 0.309 and MAE of 0.337. This demonstrates that both models perform better with shorter prediction lengths, as fewer dependencies need to be captured.\nLonger Prediction Lengths (720) As prediction length increases, both MSE and MAE grow for both models. At a prediction length of 720 with a sequence length of 2880, the TTT model records an MSE of 0.517 and an MAE of 0.445, outperforming TimeMachine, which has an MSE of 0.535 and MAE of 0.456. This shows that while error rates increase with longer prediction horizons, TTT remains more resilient in handling longer-term dependencies than TimeMachine."}]}