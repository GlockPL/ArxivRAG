{"title": "Few-shot Policy (de)composition in Conversational Question Answering", "authors": ["Kyle Erwin", "Guy Axelrod", "Maria Chang", "Achille Fokoue", "Maxwell Crouse", "Soham Dan", "Tian Gao", "Rosario Uceda-Sosa", "Ndivhuwo Makondo", "Naweed Khan", "Alexander Gray"], "abstract": "The task of policy compliance detection (PCD) is to determine if a scenario is in compliance with respect to a set of written policies. In a conversational setting, the results of PCD can indicate if clarifying questions must be asked to determine compliance status. Existing approaches usually claim to have reasoning capabilities that are latent or require a large amount of annotated data. In this work, we propose logical decomposition for policy compliance (LDPC): a neuro-symbolic framework to detect policy compliance using large language models (LLMs) in a few-shot setting. By selecting only a few exemplars alongside recently developed prompting techniques, we demonstrate that our approach soundly reasons about policy compliance conversations by extracting sub-questions to be answered, assigning truth values from contextual information, and explicitly producing a set of logic statements from the given policies. The formulation of explicit logic graphs can in turn help answer PCD-related questions with increased transparency and explainability. We apply this approach to the popular PCD and conversational machine reading benchmark, ShARC, and show competitive performance with no task-specific fine-tuning. We also leverage the inherently interpretable architecture of LDPC to understand where errors occur, revealing ambiguities in the ShARC dataset and highlighting the challenges involved with reasoning for conversational question answering.", "sections": [{"title": "1 Introduction", "content": "Policy compliance detection (PCD) is the task of automatically determining if a scenario complies with or violates a policy. Automated methods for PCD can be used across a variety of domains, e.g. detecting regulatory violations, determining benefits eligibility, or enforcing social media guidelines. In most real-world applications, scenarios are typically underspecified, making it difficult to determine compliance without obtaining additional information. In a conversational setting, additional information can be obtained by generating follow-up questions in a task referred to as conversational machine reading (Saeidi et al., 2018). One way to approach the PCD task is to decompose policies into questions and combine those questions into logical expression trees that can be used to arrive at the final answer (Saeidi et al., 2021).\nLarge language models (LLMs) are good candidates for conversational PCD, given their impressive natural language generation abilities that resemble deep understanding and emergent reasoning. However, LLMs have been shown to have issues with factuality (Ji et al., 2023) and reasoning (Valmeekam et al., 2022). Many approaches seek to improve factuality and reasoning capabilities by having LLMs \u201cshow their work\" (Wei et al., 2022; Yao et al., 2024), though even these elicited \"thoughts\" may not accurately reflect the factors that influence LLMs outputs (Turpin et al., 2024). To mitigate the risks of incorrect or unfaithful explanations, a variety of self-improving LLMs have been proposed (Wang et al., 2022; Pan et al., 2023) as a way to decompose the opaque nature of LLM inference.\nIn this paper, we propose Logical Decomposition for Policy Compliance (LDPC), which uses LLM-based reasoning that can be applied to the PCD task. Specifically, our system decomposes a natural language policy into yes/no questions that can be composed into an explicit logical formula to be used for PCD and for asking follow up questions. Importantly, the PCD decision is deterministically inferred from the logical formula, as opposed to being generated in an opaque manner and tenuously connected to an explanation or chain of thought.\nWe make the following contributions in this paper:\n1. We present Logical Decomposition for Policy Compliance (LDPC): an inherently inter-"}, {"title": "2 Related Work", "content": "This paper builds upon prior work in two general areas: conversational machine reading (Section 2.1) and reasoning with large langauge models via prompt-based instruction (Section 2.2)."}, {"title": "2.1 Conversational Machine Reading", "content": "Conversational machine reading is a type of question answering task that requires the joint interpretation of a scenario and some background information to answer a question, where the appropriate response to the question may not be an answer at all, but rather, a follow-up question (Saeidi et al., 2018). It was introduced as a more challenging and realistic alternative to question answering tasks where answers were spans of text that could be extracted from a paragraph, as in the highly influential Stanford Question Answering Dataset (Rajpurkar et al., 2016). We leverage two influential datasets in this domain: ShARC (Saeidi et al., 2018) and QA4PC (Saeidi et al., 2021)."}, {"title": "2.1.1 ShARC", "content": "The ShARC dataset (Saeidi et al., 2018) is a dialogue-based question-answering dataset constructed from 948 distinct text snippets. These snippets are taken from government websites and describe a policy. Each snippet features an input question and a corresponding \u201cdialog tree.\u201d These trees branch based on yes/no answers to follow-up questions at each step. The dataset comprises all individual branching paths, referred to as \"utterances\", from these trees. There are 6,058 utterances in total. Additionally, 6,637 scenarios provide extra information about the user, enabling the skipping of certain questions. Scenarios modify dialog trees, resulting in 37,087 distinct utterances when combined with negative sampled scenarios. After removing unreachable portions of dialog trees, the final dataset consists of 32,436 utterances. For each utterance, a model must predict one of the following classes: irrelevant, yes, no, follow-up question. If the model decides that a follow up question is the most appropriate response, it must also generate that follow up question. The dataset is divided into train, development, and test sets, with sizes of 21,890, 2,270, and 8,276, respectively. Evaluation metrics include Micro and Macro Accuracy and BLEU. Micro and Macro Accuracy assess the model's ability to classify each utterance, while BLEU evaluates the quality of generated questions.\nMany techniques have been introduced to compete for top performance on ShARC. For example, (Zhong and Zettlemoyer, 2019) train a model to extract rules from policy text and determine which rules are entailed by the conversation history. Using self-attention layers for the input and extracted rules, a decision module is able to learn how to rephrase rules into follow up questions and when to ask those questions. In contrast, (Lawrence et al., 2019) use bidirectional attention to make predictions on ShARC. The dialogue graph modeling (DGM) algorithm (Ouyang et al., 2020) improves on Discern (Gao et al., 2020) by using explicit discourse graph and implicit discourse graph models with graph convolution network (GCN). ET5 (Zhang et al., 2022) proposes a end-to-end framework with T5 to fully exploit the entailment reasoning in decision making steps by using a shared encoder (with a multi-task architecture) and separate decoders for reasoning and answer generation. T-reasoner (Sun et al., 2022) integrates a reasoning module to model condition relationships and verify consistent answers within user scenarios, identifying unsatisfied conditions. It breaks up the passage into individual conditions, which are then checked by the reasoning module. However, progress on ShARC must be interpreted judiciously, as (Verma et al., 2020) find spurious clues and patterns that make trained models susceptible to artificial gains in performance."}, {"title": "2.1.2 QA4PC", "content": "Question answering for policy compliance (QA4PC) (Saeidi et al., 2021) augments 30% of the ShARC dataset with questions to policies and expression trees. Each expression tree consists of a set of questions and logical operators that combine the answers to determine a final answer to a user question. By using the provided expression trees, a model explicitly checks each condition to obtain the answers, with improved accuracy and transfer capability (with new policies in testing). Logic Expression Tree (Kotonya et al., 2022) infers expression trees automatically from policy texts with a few supervised models. Authors introduce constrained decoding using a finite state automaton to ensure the generation of valid trees. Overall, these papers aim to improve the ability of language models to perform complex reasoning and decision-making tasks through training supervised by ShARC and QA4PC datasets."}, {"title": "2.2 Reasoning with LLMs via Prompting", "content": "Many recent works have explored prompting strategies to address complex reasoning problems. These methods avoid finetuning altogether and instead use elaborate prompts to elicit from the LLM an explicitly written out, step-by-step solution to the problem. In the original chain-of-thought (Wei et al., 2022) work, prompting involved demonstrating a number of examples (i.e., few-shot prompting) that had questions paired with reasoning traces.\nThe work of (Kojima et al., 2022) showed that prompting the LLM to produce a reasoning trace could be done without any few-shot examples whatsoever by simply initializing the prompt with the words \"Let's think step-by-step\". Soon after, more elaborate zero-shot prompting strategies like Plan-and-Solve prompting (Wang et al., 2023) and REACT (Yao et al., 2022) were introduced, where the LLM was prompted to solve the problem by first planning out a sequence of steps to follow, and then solving those steps in the order as described.\nSelf-consistency (Wang et al., 2022) is a decoding technique designed to leverage the notion that multiple alternative reasoning paths may lead to the correct answer. In the self-consistency framework, a diverse set of outputs are first sampled from the LLM. Then, the response returned to the user is the answer most commonly produced by the LLM after stripping away the chain of reasoning produced in the response. This is equivalent to marginalizing over the alternative reasoning paths to select the most consistently produced answer.\nFor dataset creation, the Orca (Mitra et al., 2023) framework explored tuning a smaller model with synthetic data produced by a model that had been prompted with a number of different problem-solving prompts. Self-taught Reasoner (STaR) (Zelikman et al., 2022) proposes a technique to iteratively leverage a small number of example rationales to bootstrap reasoning capabilities of LLMs. It first includes a few example rationales in its prompts, and for wrong answers, tries to derive the answer and rationale again given the correct answers, and lastly fine-tunes on all the rationales that led to correct answers."}, {"title": "3 Problem Formulation", "content": "Given a dataset $D$ with size $D = |D|$ with input $X = {P,C'}$ and target output $O$, where $P = {p_i}_{i=1}^n$ is a set of policy documents, and $C = {(q_i, s_i, h_i)}_{i=1}^{P_i}$ is each context for the corresponding policy document $p_i$, containing user scenarios $s_i$, user question $q_i$, and dialogue history $h_i$. We wish to answer each input question $q_i$ with either Yes, No, Irrelevant, or follow up questions. Hence, the output of the model is $O = {(answer_i, questions_i)}_{i=1}^{P_i}$, containing the decision $answer_i$ on policy compliance and followup questions $questions_i$ when the decision answer requires so. We will drop index $i$ to simplify notations when there is no ambiguity."}, {"title": "4 Proposed Approach", "content": "We propose a neuro-symbolic system that uses an LLM to decompose a piece of policy text into distinct conditions that are phrased as simple yes/no questions. The LLM then re-composes those conditions into symbolic logical formulae, which can be deterministically reasoned over using a simple three-valued logic and evaluated into an appropriate conversational response. Our pipeline has the following components that we describe below: (1) question-policy relevance, (2) policy decomposition, (3) logical formulation, (4) logical evaluation."}, {"title": "4.1 Question-Policy Relevance", "content": "The first step in the proposed pipeline is to determine whether the user question is relevant to the policy. This is achieved by using the all-MiniLM-L12-v2 sentence transformer, which maps sentences and paragraphs to a 384-dimensional dense"}, {"title": "4.2 Policy Decomposition", "content": "In the policy decomposition step, an LLM, prompted with 20 in-context examples, breaks down the policy into basic yes or no questions that need to be answered in order to answer the user's question. The in-context examples are taken from the QA4PC dev set, since QA4PC includes decomposed questions from policies and the logical representations of those policies. The in-context examples are followed by the ShARC dataset item on which we want the model to perform inference.\nQuestion Relevance To ensure accurate and relevant responses during the policy decomposition stage, a filtering process is implemented to eliminate non-pertinent questions generated by the LLM. This process takes into account the user's query, chat history, and policy to prevent unnecessarily lengthy logical structures and varying logics for the same input. The LLM is asked to determine the relevance of each generated question, responding with a binary Yes or No answer. If the response is No, the question is filtered out. This process is repeated for each generated question."}, {"title": "4.3 Logic Formulation", "content": "At the logic formulation stage, the model is prompted with the input and output of the policy decomposition stage and creates a logical formula that can be used to answer the user's question. We again adopt a few-shot in-context learning strategy to generate the logical formulae via an LLM. The LLM is asked to \"combine the question variables into a python boolean expression\u201d. The valid logic operators used are and, or, and not.\nSelf-Consistency In this work, we adopt self-consistency during the generation of logical forms. In particular, rather than greedily decoding one logical form for a particular policy, our approach first samples $k$ alternative logical forms from the LLM. The logical forms are then grouped together into equivalence classes, i.e., all logical forms that are logically equivalent are put into the same partition. Last, the returned logical form is then selected to be one of the constituents of the largest partition of the sampled logical forms (our approach takes the logical form from the selected partition that is shortest).\nAs an example, consider a situation where the LLM produced three logical forms: $\\neg(A \\land B)$, $(\\neg A \\lor \\neg B)$, and $(A \\land B)$. First, the sampled expressions would be grouped into buckets based on logical equivalence, leading to two sets $S_1 = {\\neg(A \\land B), (\\neg A \\lor \\neg B)}$ and $S_2 = {(A \\land B)}$. Then, the bucket selected would be the one with the largest size, in this case being $S_1$. From $S_1$, the shortest logical form (in terms of number of symbols) would be returned, thus resulting in $\\neg(A \\land B)$ being the returned expression."}, {"title": "4.4 Logic Evaluation", "content": "In our approach, we evaluate the generated propositional formula using a three-valued logic that introduces a Maybe (m) value to indicate an unanswered question. The particular three-valued logic we use is commonly referred to as Kleene's strong three-valued logic in the non-classical logic literature. Our set of truth values is $T = {F, m, T}$, and we assume an ordering where $F< m <T$.\nWe consider the operations $\\neg,\\land, \\lor$ on T where $\\neg := {(T, F), (F,T), (m, m)}$ and for all a, b $\\in$ T,\n$a\\land b := min(a, b)$\n$a \\lor b := max(a, b)$\nA conjunction operator evaluates to Maybe if the operands are True and Maybe (or both Maybe); the disjunction operator evaluates to Maybe if the operands are False and Maybe (or both Maybe), and the negation of Maybe is Maybe. Otherwise, all operands work as expected in binary logic.\nThis approach allows us to effectively handle uncertain or incomplete information, which is particularly useful in question and answering systems. If the generated logic evaluates to Maybe, it signals that a follow up question is required. If the generated logic evaluates to True or False, it means that the answer to the user's question is Yes or No, respectively. We use the following question-answering process discussed below to determine the values for logical variables."}, {"title": "4.5 Question Answering", "content": "We use a ROBERTa-large model fine-tuned on the MNLI corpus and further fine-tune on the ShARC NLI data. We fine-tuned for 5 epochs, using a batch size of 32, learning rate of 1e-5, and weight decay of 0.1. We used the fine-tuned entailment model to determine if the user scenario can answer a generated question. To do this, we first convert the generated questions into statements. We then pass the user scenarios and the statements to the fine-tuned model, which determine whether the scenario entails the statement. In other words, the model determines whether the scenario provides sufficient information to answer the question. The model returns one of three possible values based on its evaluation: \"entails,\" \"contradiction,\u201d or \u201cneutral.\""}, {"title": "4.6 Overall Algorithm", "content": "Algorithm 1 outlines our approach's pipeline. The pipeline takes in I, a tuple containing the policy (p), the user question (q), the user scenario (s), and the chat history (h). The pipeline consists of several functions that take in textual inputs, format them, and pass them to their respective language models to generate output. Specifically, S determines the semantic similarity between the user question and policies, while D represents the question decomposition process that takes in a LLM (MD), policy, user question, and chat history, and returns generated questions and their IDs as a dictionary. Q represents the question answering step, which takes in the user scenario and question, and R determines if a generated question is relevant to the policy and user question, and makes sense given the chat history. Finally, L represents the call to a LLM (Mc) to generate the logic formula that best represents the policy.\nLines 6-13 assign answers to corresponding question IDs, with answers being taken from the chat history if the question is present, or determined using Q. Lines 14-23 represent the question filtering process in cases where there are too many"}, {"title": "5 Experiments and Analysis", "content": "This section details the experiments and analysis used to evaluate the proposed approach."}, {"title": "5.1 Datasets", "content": "As described in Section 2.1, the ShARC and QA4PC datasets enable the assessment of conversational machine reading and policy compliance detection systems. We take advantage of the curated expression trees and policy questions in QA4PC to decide which model to use in our pipeline and we use ShARC to assess the overall effectiveness of our pipeline. Importantly, our question decomposition and logic formulation modules are not trained on these datasets, but perform inference from in-context learning."}, {"title": "5.2 Model Choice", "content": "Models were selected for the pipeline based on their ability to perform the policy decomposition and logic formulation tasks. We chose to test instruct and code models that are publicly available and have permissive licenses. The models we tested were the 8b, and 70b llama3 instruct models (Int), the 34b codellama model and the 8x7B mixtral instruct model. The rationale for including code models is that the logic formulation step requires manipulation of abstract symbols, where code models may excel. The models were evaluated against the 193 QA4PC hand-annotated training examples.\nIn the first experiment, the models were given the policy and the questions needed to ask the user. The models were tasked with organizing the questions into a logical formula that best represented the policy. The models were measured by the proportion of the 193 annotated examples for which they generated a propositional logical formula that is equivalent to the ground truth formula.\nIn the second experiment, the models were only given the policy and asked to perform both the question decomposition and logical formulation tasks. Again, the model output was evaluated using logical equivalence. For both experiments, we also tested how the number of in-context examples affects performance. Results were collected over 5 separate runs. In a run, for each $k\\in {0,1,3,5,9, 15, 20}$ the prompt for the LLM is constructed from $k$ in-context examples, which are sampled randomly from a pool of 20 hand-crafted examples that do not appear in the test set.\n performed the best for the first experiment, with an accuracy score of approximately 92%. The  achieved an accuracy score of approximately 85%. This is notable because  is significantly smaller than . Additionally,  performed similarly to codellama (a 34-billion parameter model) and mixtral (which uses 13B active parameters during inference). The graph also shows that the models were not positively affected by increasing the number of in-context examples.\n shows the results of the second experiment, where the models were tasked with performing question decomposition and logical formulation. The accuracy of the models decreased, as expected since it is a more complex task for the"}, {"title": "5.3 ShARC Results", "content": "Table 2 presents micro and macro accuracy scores for the ShARC dev and test sets. Our approach achieves near state-of-the-art performance, with a micro accuracy score of 79.0%, just 1.5% behind BiAE's 80.5%. For macro accuracy, our approach ranks fourth with a score of 79.7%, 3.5% behind BIAE. Our approach showed a decline in performance on the test set compared to the development set. Specifically, the micro accuracy dropped from 79.0 to 70.2, and the macro accuracy dropped from 79.9 to 72.8. Despite this decline, our approach still performed well, ranking fourth in micro accuracy and fifth in macro accuracy among all the models tested. Furthermore, our approach uses 20 in-context examples where as the approaches that we compared against have been trained on thousands of examples.\nTable 3 presents the BLEU scores for our approach alongside those of other methods. BLEU scores measure the positional similarity of the predicted follow-up questions with the expected follow-up questions. Unfortunately, our approach performed poorly in terms of BLEU scores. Specifically, it achieved the third lowest score for BLEU-1 and the lowest score for BLEU-4 on the dev set, and the second lowest score for both BLEU-1 and BLEU-4 on the test set. However, it's important to note that our approach uses prompting and was not fine-tuned to the data like the other methods. As a result, it could not fully capture the style of follow-up questions in the data. Additionally, BLEU's sensitivity to word order leads to issues when comparing two follow-up questions. This sensitivity overemphasize word-level similarity, and neglects other important aspects of text similarity, such as semantic meaning or context."}, {"title": "5.4 Error Analysis", "content": "To better understand the limitations of our approach, we conducted an error analysis on a randomly selected subset of items in the ShARC dev set for which our model made incorrect class pre-"}, {"title": "6 Conclusion", "content": "In this paper we introduce Logical Decomposition for Policy Compliance (LDPC). We first test the ability of multiple language models on the policy decomposition and logical formulation tasks using the QA4PC dataset. We then use the best identified model to build a pipeline that uses few-shot in-context learning to decompose policies into yes/no questions and assemble them into logical formulae for answering policy-related questions in a conversational setting. We additionally use a self-consistency decoding technique for producing logical formulae. We achieve competitive results on the ShARC dataset. However, we also observe that ShARC contains ambiguous items with questionable reference answers, underscoring the subjective nature of these questions and the need for careful scrutiny of LLM reasoning benchmarks."}, {"title": "7 Limitations", "content": "Our system demonstrates that in-context learning is sufficient for achieving competitive performance on ShARC, but limitations of this approach include the simplicity of some of our system components and limitations of the ShARC dataset itself.\nSelf-consistency The purpose of this decoding strategy is to increase diversity of reasoning paths under the hypothesis that correct answers can be reached in a variety of ways. However, as shown in our error analysis, the vast majority of logic formulations for each dev set item were homogeneous. This question could be further explored with other kinds of non-greedy decoding.\nLimited expressivity of three-valued logic Although we believe the simplicity of three-valued logic to be a feature in an initial implementation, it is likely that more complex policies and scenarios require greater expressivity.\nSimple QA Model We opted for a very common approach of using a language model fine-tuned on natural language inference. However, question answering errors were common in our analysis, occurring almost as frequently as logic formulation errors. This suggests that there is still room for improvement in this area. More powerful QA models can be explored.\nChallenges to deployment Although our system's competitive performance on ShARC without ShARC-specific fine-tuning for (de)composition is noteworthy, the results do not suggest this approach for readily use in a real-world application. Deploying such an application could have a serious impact on people's lives, such as determining eligibility for services or benefits. Possible mitigation methods include labor intensive fine-tuning, alignment, and implementation of other task- domain- or culturally-specific safeguards.\nData ambiguity issues Lastly, our error analysis findings on the frequency of borderline instances in ShARC are consistent with prior work on spurious patterns in the dataset. Our approach is partly shielded from those spurious patterns owing to the lack of task-specific fine-tuning. However, this certainly limits the extent to which we can make general claims based on ShARC-based metrics."}, {"title": "A Prompt Formats", "content": "At the policy decomposition and logic formulation steps of the pipeline, we provide each model with prompts that follow the following template:\n[INSTRUCTION]\n[IN-CONTEXT EXAMPLES]\n[PARTIAL EXAMPLE]\nFor policy decomposition, the instruction was:\n\"You are given a policy and an input question. You must think logically. Do not extrapolate. Your job is to decompose the policy into its basic rules. Rephrase the rules in the form of basic questions that need to be answered to answer the input question. Here are some examples to help guide you to do what I want.\"\nFor logic formulation, the instruction was:\n\"You are a highly rational and concise assistant. You are given a policy, input question and a decomposition of the policy into the basic rules in the form of questions that need to be answered to answer the input question. Please combine the question variables into a python boolean expression that can be evaluated to answer the input question.\""}]}