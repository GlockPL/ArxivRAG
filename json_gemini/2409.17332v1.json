{"title": "Block Expanded DINORET: Adapting Natural Domain\nFoundation Models for Retinal Imaging Without Catastrophic\nForgetting", "authors": ["J. Zoellin", "C. Merk", "M. Buob", "A. Saad", "S. Giesser", "T. Spitznagel", "F. Turgut", "R. Santos", "Y. Zhou", "S. Wagner", "Y.C. Tahm", "P. A. Keane", "D. Cabrera DeBuc", "M. D. Becker", "G. M. Somfai"], "abstract": "Background: Integrating deep learning into medical imaging is poised to greatly advance diagnostic\nmethods, but it faces challenges with generalizability, which can foster distrust in artificial intelligence and\nexacerbate ethnic biases. Foundation models, based on self-supervised learning, address these issues,\nimprove data efficiency and reduce ethnic biases. Natural domain foundation models show promise for\nmedical imaging, but systematic research evaluating domain adaptation of these models, especially using\nself-supervised learning and parameter-efficient fine-tuning, remains underexplored. Additionally, little\nresearch addresses the issue of catastrophic forgetting observed during fine-tuning of foundation models.\n\nMethods: We adapted the DINOv2 vision transformer for retinal imaging classification tasks using self-\nsupervised learning and generated two novel foundation models termed DINORET and BE DINORET.\nPublicly available color fundus photographs from multiple datasets were employed for model\ndevelopment and subsequent fine-tuning for diabetic retinopathy staging and glaucoma detection. We\nintroduced block expansion as a novel method for domain adaptation in retinal imaging and assessed the\nmodels for catastrophic forgetting. Models were benchmarked to RETFound, a state-of-the-art foundation\nmodel in ophthalmology.\n\nFindings: DINORET and BE DINORET demonstrated competitive performance on retinal imaging tasks, with\nthe block expanded model achieving the highest scores on most datasets. Block expansion successfully\nmitigated catastrophic forgetting. Our few-shot learning studies indicated that DINORET and BE DINORET\noutperform RETFound in terms of data-efficiency.\n\nInterpretation: The study highlights the potential of adapting natural domain vision models to retinal\nimaging using self-supervised learning and block expansion. BE DINORET offers robust performance and\nhigh data efficiency without sacrificing previously acquired capabilities. Our findings suggest that these\nmethods could enable healthcare institutions to develop tailored vision models for their patient\npopulations, enhancing global healthcare inclusivity.", "sections": [{"title": "1. Introduction", "content": "Integrating deep learning (DL) into medical imaging offers significant advancements in diagnostics (1-3).\nHowever, DL models face challenges with generalizability, particularly when facing distribution shifts\nbetween training datasets and clinical settings, leading to reduced trust in artificial intelligence (AI) among\nphysicians and the public and substantial ethnic bias in DL models (4\u201311). Additionally, supervised training\nof DL models requires large, labeled datasets, which is often impractical and limiting (2,12,13). Self-\nsupervised learning (SSL) addresses this by utilizing large datasets without annotation, thus reducing\nmanual workload and expanding model development (14-17).\n\nFoundation models (FMs), pre-trained on large image datasets using SSL, show robust feature\nrepresentation capabilities and are easily adapted to various tasks (10,16,18\u201324). They are particularly\nbeneficial for institutions with limited data and help mitigate ethnic bias, thus ensuring equitable\ndiagnostic outcomes (6,10,16,24-28). Domain adaptation involves fine-tuning a pre-trained FM on a target\ndomain with minimal additional training data (29\u201331). This process leverages the model's pre-existing\nknowledge while adjusting it to the nuances of the new domain. In the context of medical imaging, domain\nadaptation is crucial for ensuring that FMs can accurately interpret and analyze medical images, which\noften differ significantly from the data used during initial training (10,20,23,25).\n\nAdapting natural domain FMs like the DINOv2 Vision Transformer (ViT) model to medical imaging can\nsubstantially improve DL models and data efficiency (32-35). Still, effective fine-tuning strategies are\nwarranted to address domain shifts (32,34). However, while there is a growing interest and active research\nin adapting DINOv2 for various medical applications (19,35\u201338), no study has investigated SSL for natural\ndomain adaptation for retinal imaging classification tasks.\n\nColor fundus photographs (CFPs) are a widely utilized imaging modality in ophthalmology. They enable the\ndiagnosis and monitoring of retinal diseases and offer insights into systemic health, such as the ability to\npredict major cardiovascular event (39\u201346). Efficient adaptation using SSL would enable the development\nof numerous FMs tailored to the needs of individual healthcare institutions with only moderate\ncomputational and image requirements (10).\n\nCatastrophic forgetting, where models lose performance on original data after fine-tuning on new data,\nremains a challenge in machine learning (47\u201349). Adapting foundation models for retinal imaging while\nmitigating catastrophic forgetting is a crucial area of research. Parameter-efficient fine-tuning (PEFT)\nmethods, like block expansion (BE), minimize trainable parameters while preserving model features,\nthereby enhancing generalizability and preventing catastrophic forgetting (50\u201352). To the best of our\nknowledge, we were the first researchers to explore BE in the context of PEFT in computer vision (53).\nDespite its success in language models, BE still has limited exploration in computer vision and medical Al\n(51-54).\n\nGiven DINOv2's promising results in medical image classification, we aim to explore SSL strategies to\nadapt it to CFPs and generate novel foundation models for Ophthalmology termed DINORET and BE\nDINORET. We propose methods for generating medical FMs using SSL and BE and compare these against\nRETFound (10), an influential FM in ophthalmology. This work makes the following contributions in five\nkey areas:"}, {"title": "2. Methods", "content": "For the scope of this study, only publicly available CFP datasets were employed. All datasets are either\nfreely available or can be obtained after registration. Ground truth labels for diabetic retinopathy (DR)\nstages (according to the International Clinical Diabetic Retinopathy Scale (ICDR)) (55) and the presence of\nglaucoma are all publicly available.\n\nAs shown in Table 1, we employed three datasets for SSL post-pretraining: Kaggle Eye Picture Archive\nCommunication System (EYEPACS) (11,56,57), DDR (58), and Artificial Intelligence for Robust Glaucoma\nScreening (AIROGS) (59). The EYEPACS and DDR datasets contain images of healthy eyes and eyes with\nvarious stages of DR, while the AIROGS dataset includes images of healthy and glaucomatous eyes. We\nused the five datasets listed in Table 2 for downstream classification tasks. These include the Asia Pacific\nTele-Ophthalmology Society (APTOS) (60), Methods to Evaluate Segmentation and Indexing Techniques in\nthe Field of Retinal Ophthalmology (MESSIDOR-2) (61\u201363), Indian Diabetic Retinopathy Image Dataset\n(IDRID) (64,65), The Diabetic Retinopathy Two-field image Dataset (DRTID) (66) for DR and PAPILA (67) for\nglaucoma. More detailed information on each dataset can be found in Supplementary Material 1."}, {"title": "2.2 Model Architectures", "content": "In this study, several ViT models (68) were compared, focusing on three fundamentally different\narchitectures, as detailed in Table 3."}, {"title": "2.3 Pre-Processing", "content": "The three datasets we used for SSL post-pretraining were aggregated into a single CFP-Large dataset. With\none exception (used for the ablation study), low-quality images were removed from the dataset using part\nof the AutoMorph pipeline (71,72). In addition, AutoMorph was used to crop the images to fit the round\nshape of CFPs, as shown in Supplementary Figure 1. Every CFP image was split into a variable number of\npatches. During the supervised fine-tuning, the only preprocessing operation was resizing the images to\n224x224 pixels."}, {"title": "2.4 Self-Supervised Post-Pretraining Method", "content": "In our study, post-pretraining refers to updating the weights in the BB of the natural domain pre-trained\nVIT DINOv2 with SSL. The same post-pretraining method (contrastive SSL with a modified DINOv2 pipeline)\nwas used for the DINORET and BE DINORET models, with one key difference regarding the architecture\n(33,73). For BE DINORET, BE was included, and all the weights of the original transformer blocks were\nfrozen, while only duplicated blocks were updated. For DINORET, all weights were unfrozen. DINOv2 did\nnot undergo any additional SSL before supervised fine-tuning. A detailed explanation of SSL post-\npretraining approaches is provided in Supplementary Material 2."}, {"title": "2.5 Supervised Fine-Tuning", "content": "Two fundamentally different methods were used during task-specific fine-tuning (e.g., DR staging) with\nsupervised learning (SL) on the target datasets.\n\nFrozen BB Fine-Tuning: Frozen BB fine-tuning, also known as linear probing, involved keeping the weights\nof the BB layers fixed and only updating the parameters of an additional linear classification head, as shown\nin Figure 2 (13,23,34,51,74).\n\nUnfrozen BB Fine-Tuning: Unfrozen BB fine-tuning involved updating the parameters of the entire model,\nincluding the BB layers, during supervised training, as illustrated in Figure 2 (13,32,34,51). The gradients\nderived from the task-specific loss function are propagated through all model layers, enabling parameter\nupdates and fine-tuning the entire network to the new data (51)."}, {"title": "2.6 Image Classification", "content": "Disease classification was performed using a linear classification head consisting of a linear layer followed\nby a softmax function. This layer projects the high-dimensional output of the ViT, such as the [CLS] token\nor average patch embedding, to the desired number of target classes."}, {"title": "2.7 Hyperparameters", "content": "The exact supervised fine-tuning procedure was similar among all models, except for RETFound, where we\nused the hyperparameters suggested by Zhou et al. (10,70). The hyperparameters used for training the\nDINOv2-based models are detailed in Supplementary Material 3, and the final models were always chosen\nbased on the best qKappa checkpoint on the validation set."}, {"title": "2.9 Evaluation Metrics and Hypothesis Testing", "content": "We reported metrics that ensure optimal comparison with other publications, including the F1 score,\noverall accuracy (Acc), Area Under the Receiver Operating Characteristic Curve (AUC ROC), and quadratic\nweighted Kappa (qKappa) (75). As commonly used in DR classification (11,62), a binary classification score\nwas introduced, where no DR (stage 0) and mild DR (stage 1) were collectively grouped as non-referable\nDR. In contrast, moderate (stage 2), severe (stage 3), and proliferative DR (stage 4) were grouped as\nreferable DR (rDR) (62,76,77). To evaluate the performance on the original pre-training domain, the k-\nnearest neighbors (kNN) score on the validation set of the ImageNet-1k dataset was reported (69,78). The\nformulas used to obtain these metrics can be found in our source code or Supplementary Material 3. We\nrefrained from inferential statistics for experiments lacking repeated runs and having invariant splits. For\nexperiments with variable seeds and multiple independent runs, hypothesis testing was conducted using\neither ANOVA or Kruskal-Wallis tests based on the data's distributional characteristics, assessed by the\nShapiro-Wilk (for normality) and Levene tests (for homoscedasticity). Post-hoc comparisons were\nperformed using Tukey's Honestly Significant Difference (HSD) test following ANOVA, which adjusts p-\nvalues for multiple comparisons, and Dunn's test following Kruskal-Wallis, with the latter applying\nBenjamini-Hochberg (BH) corrections for multiple comparisons and both raw and adjusted p-values were\nreported. Additionally, hierarchical linear mixed models (LMMs), estimated via maximum likelihood (ML),\nwere fit across pooled data, and a likelihood ratio test (LRT) was used to compare the full models against\nreduced models. The significance level for all tests was set at an alpha of 0.05."}, {"title": "2.10 Experiments", "content": "SSL Parameters:\nWe assessed the impact of filtering the CFP-Large dataset using AutoMorph and excluding bad-quality\nimages as defined by the algorithm (71). We also analyzed the effect of expanding BE DINORET by a\nvarying number of blocks (specifically, 1-12 blocks) and evaluated models trained with smaller or larger\npatches (1-25 patches). Models were tested on the full APTOS dataset and a few-shot study with 16\nsample images per class was performed.\n\nSupervised Fine-tuning Parameters:\nWe tested the influence of using either the [CLS] token, the average of all patch embeddings, or both\nconcatenated for image classification. Additionally, we investigated the use of a distance-weighted cross-\nentropy loss (CEL) compared to a vanilla CEL for classification tasks."}, {"title": "2.10.2 Model Evaluation Studies", "content": "\u2022\nMulti-Source Domain Fine-Tuning (MSDFT): Following Met et al. (77), we combine all DR datasets\ninto unified training, validation, and test sets. Models are fine-tuned on the joint training set and\nevaluated on individual and joint test sets."}, {"title": "2.10.3 Catastrophic Forgetting", "content": "To determine if our SSL-based strategies for domain adaptation of DINOv2 to CFPs make models\nsusceptible to catastrophic forgetting, DINORET, BE DINORET, and DINOv2 were tested on the validation\nsubset of the ImageNet-1k dataset using the kNN algorithm with k=20 of the embeddings ([CLS]) token. To\ninvestigate if the embeddings generated from CFPs by DINORET and BE DINORET substantially differ from\nunmodified DINOv2, we retrieved all embeddings ([CLS] tokens) generated by the three models on the test\ndataset of APTOS and performed a linear discriminant analysis (LDA) of the embeddings across all\ndimensions."}, {"title": "3. Results:", "content": "AutoMorph classified 47,496 images in the CFP-Large dataset as ungradable. Removing these ungradable\nimages improved DR staging performance on APTOS across all metrics except for AUC ROC, which\nremained unchanged (Supplementary Data 1). Supplementary Figure 2 shows that varying the number of\ntransformer blocks in BE DINORET led to inconsistent evaluation results while splitting CFPs into patches\nduring post-pretraining enhanced performance (Supplementary Figure 3). Supplementary Data 1 and\nSupplementary Results 1 provide comprehensive results from the studies on SSL and SL hyperparameters."}, {"title": "3.2 Multi-Source Domain Fine-Tuning (MSDFT)", "content": "MSDFT was performed exclusively for DR staging using a combined training and validation set of 4724\nimages from all four DR datasets. MSDFT fine-tuned models were evaluated on all individual test sets. All\nresults obtained from our MSDFT experiments are depicted in Figure 3. The confusion matrices can be"}, {"title": "3.3 Performance Assessment on Test Sets", "content": "In this assessment, supervised training, validating, and testing on a single dataset was performed for 5\ndatasets (4 for DR and 1 for glaucoma). Unfrozen models outperformed their frozen counterparts 80% of\nthe time for AUC ROC, 90% for qKappa, Acc, and F1, and in all cases for rDR Acc. For unfrozen models,\nresults varied by metric and dataset, as displayed in Figure 5. BE DINORET displayed the best qKappa score\non Messidor and the best AUC ROC score on PAPILA. DINORET had the best AUC ROC score on Messidor\nand DRTID, while DINOv2 had the best qKappa score on IDRID and DRTID. RETFound achieved the best\nAUC ROC and qKappa scores on APTOS and the best AUC ROC score on IDRID. As illustrated in Figure 6,\nunfrozen BE DINORET most commonly ranked amongst the two best performing models, only being\nmatched by RETFound for AUC ROC and Acc and by DINOv2 for F1."}, {"title": "3.4 Cross-evaluation:", "content": "Compared to previous experiments in this study, frozen models performed better during cross-\nevaluations, outperforming their unfrozen counterparts in 39.6% for AUC ROC and 20.84% for qkappa\n(45.8% for Acc, 43.75% for F1, 29.2% for rDR Acc). As shown in Supplementary Figure 5, when averaging\nscores on a single test dataset across the three cross-evaluations, RETFound most frequently achieved the\nhighest average (avg) AUC ROC and avg qKappa scores, followed by the other models. A detailed depiction\nof all results can be found in Supplementary Figure 6 and Supplementary Results 2."}, {"title": "3.5 Data Efficiency", "content": "All runs were performed in quintuplicate until the sample count could not be increased further, resulting\nin up to 16 training images per class for Messidor, IDRID, and DRTID and up to 32 or 128 images for PAPILA\nand APTOS, respectively. Supplementary Figures 7-11 depict the average score (across 5 runs) achieved by\nthe models at a given training sample count. Supplementary Data 3 includes all results obtained from these\nexperiments. Our models' precision across the 5 replicates significantly increased with training sample\ncounts, alongside a statistically significant increase in qKappa and AUC ROC and a significant increase in\nperformance when unfreezing BBs (all p = < 0.01, LMM with an LRT).\n\nUnfrozen Backbones:\nOverall, for unfrozen fine-tuning between sample sizes of 4-64, BE DINORET most frequently ranked as the\nmodel with the highest AUC ROC score, followed by DINORET and DINOv2, and lastly, RETFound (Figure\n7). Regarding qKappa, DINORET outperformed other models, most frequently achieving the highest"}, {"title": "3.6 Catastrophic Forgetting", "content": "While SSL domain adaptation of DINORET diminished its performance on the original domain, i.e. natural\nimages, BE DINORET did not suffer from such a decrease and retained previously acquired capabilities, as\nshown in Table 4."}, {"title": "4. Discussion", "content": "Our research highlighted that DINOv2 exhibits remarkable out-of-the-box performance on retinal image\nclassification tasks (32\u201334). When fine-tuning models with a frozen BB, DINOv2 outperformed RETFound\nin all performed experiments. Although unfrozen RETFound seemingly outperformed DINOv2 when\ntraining and testing on a single dataset or during cross-evaluations, in MSDFT experiments DINOv2\noutperformed RETFound (10). Additionally, DINOv2 demonstrated superior data efficiency over RETFound.\n\nWe introduced strategies for adapting natural domain ViTs to the medical domain with SSL. Our findings\ndemonstrated that these novel ViTs can improve performance for classification tasks, especially domain\nadaptation with BE and SSL, which displayed considerable promise. Unfrozen BE DINORET most commonly\nranked as the best model for MSDFT experiments, and it most frequently was among the best two models\nwhen training and testing on a single dataset. When evaluating data efficiency, frozen BE DINORET"}, {"title": "5. Conclusion", "content": "We demonstrate that the natural domain FM DINOv2 performs strongly for retinal imaging classification\ntasks, outperforming a state-of-the-art domain-specific model in most experiments conducted in this study\n(10). Additionally, we proposed two new pipelines for domain adaptation of natural domain FMs to the\nclinical domain with SSL and show increased performance of our domain adapted models DINORET and BE\nDINORET. This approach will allow healthcare institutions across the globe to develop FMs customized to\ntheir patient population and help mitigate the prevalent biases in medical Al and our models mark an\nimportant addition to the field of Al in ophthalmology. Furthermore, we show that our proposed BE\nstrategy avoids catastrophic forgetting and allows for continuous model improvement while retaining\nmaximum generalizability. BE could potentially allow model optimization for specific patient populations\nwithout sacrificing knowledge of images from a different demographic, thereby ensuring fair access to\nmedical Al across ethnic and socioeconomic groups. Our approach paves the way for adapting natural"}, {"title": "8.1 Supplementary Material 1", "content": "Eye Picture Archive Communication System (EYEPACS)\nThe Kaggle-EYEPACS dataset is a large, open-access collection of CFPs for diabetic retinopathy (DR)\nresearch (1-3). This dataset, initially provided for a Kaggle competition, encompasses 88,702 macula-\ncentered CFPs of diverse resolutions, obtained using various cameras across multiple locations (primarily\nin the USA). For our study, 88,699 images of the original dataset were used (three images appeared\ndamaged), with 17,965 images being classified as ungradable by AutoMorph (4).\n\nArtificial Intelligence for Robust Glaucoma Screening (AIROGS)\nThe Artificial Intelligence for RObust Glaucoma Screening (AIROGS) dataset was released as part of the\nInternational Symposium on Biomedical Imaging (ISBI) 2022 challenge program (5). 101,442 CFPs from\n54,274 subjects (test subset) are publicly available and utilized in this study (except 175 damaged images).\nThe dataset contains images classified as referable glaucoma (3.2%) and non-referable glaucoma (96.8%)\nand images were obtained using several devices (5). A total of 25,266 images were classified as ungradable\nby AutoMorph (4).\n\nDDR\nThe DDR dataset comprises 13,673 CFPs sourced from 147 hospitals across 23 provinces in China. The\nimages are derived from 9,598 patients, with a nearly even gender distribution of 48.23% male and 51.77%\nfemale and a mean age of 54 years (6). The dataset utilizes 42 types of fundus cameras, ensuring diverse\nimaging conditions. For our experiments, all images (except 69 damaged images) were utilized and filtered\nusing AutoMorph (4), leading to the exclusion of 4,305 images deemed ungradable.\n\nAsia Pacific Tele-Ophthalmology Society (APTOS)\nThe APTOS 2019 dataset, sourced from a Kaggle competition and provided by the Aravind Eye Hospital in\nIndia, features a comprehensive collection of 5,590 CFPs (7). These images, predominantly macula-\ncentered, were captured by technicians in various rural areas of India, showcasing a diverse range of\nenvironmental and operational conditions. Images were graded by physicians according to the ICDR scale\n(8). For our research, we focused on the training subset of the APTOS dataset (APTOS public), which\nincludes 3,662 CFPs, since ground-truth labels were not available for the test portion of the dataset (APTOS\nprivate). The training set from the APTOS competition was divided into distinct segments for training,\ntesting, and validation, adhering to a ratio of 70:15:15 and maintaining an equal distribution of labels"}, {"title": "8.2 Supplementary Material 2", "content": "In our study, post-pretraining refers to updating the weights in the BB of the natural-domain pre-trained\nVIT DINOv2, whose weights and biases are initialized (17). During post-pretraining, weights across layers\nare updated using the DINOv2 SSL pipeline on CFPs (17). For DINORET, all blocks were unfrozen, updating\nweights across all blocks (Figure 1), while for BE-DINORET, the 12 original ViT blocks were frozen and only\nthe parameters within duplicated blocks were changed during SSL on CFPs (Figure 1). The data used for\npost-pretraining consisted of the CFP-Large dataset (a combination of Kaggle-EYEPACS, AIROGS and DDR),\ncollectively amounting to 203,570 images. These images are all used without ground-truth labels for SSL.\nTo increase the number of images, all CFPs are split into patches (the amount depending on the model\nconfiguration, as depicted in Supplementary Figure 2, and as applied in similar research (18). Each patch\n(1-25 per CFP) is resized to 224 x 224 pixels. Finally the models are fine-tuned using a modified DINOv2\npipeline, which is a form of contrastive SSL (19), with an adapted code from the original DINOv2 repository\n(17,20). These adaptations include BE for BE models only (21) and a custom dataset (CFP-Large).\nFurthermore, the job submission methods are modified to accommodate our Simple Linux Utility for\nResearch Management (SLURM) environment and aggregation steps are included in the training loop,\nallowing for a larger batch size and consideration of the available Video Random Access Memory (VRAM).\nOtherwise, the code is identical to the one in the public DINOv2 repository. For evaluation of post-\npretraining approaches, each model is fine-tuned for several downstream tasks using supervised learning\n(SL), including DR staging on the full APTOS dataset and a few-shot-study on this dataset with 16 training\nimages per class. Based on the performance on these downstream tasks, model evaluation, selection and\nhyperparameter tuning is performed. As suggested in previous studies, we disable the Koleo regularizer to\naccelerate and stabilize the training process at scale (18)."}, {"title": "8.3 Supplementary Material 3", "content": "F1 Score:\nPrecision: Precision = TP / (TP + FP)\nRecall: Recall = TP / (TP + FN)\nF1 Score = 2 * (Precision * Recall) / (Precision + Recall)\nWhere:\nTP = True Positives\nFP = False Positives\nFN = False Negatives\n\nOverall Accuracy (Acc):\nAccuracy: Accuracy = (TP + TN) / (TP + TN + FP + FN)\nWhere:\nTP = True Positives\nTN = True Negatives\nFP = False Positives\nFN = False Negatives\n\nArea Under the Receiver Operating Characteristic Curve (AUC ROC)\nAUC ROC=$\\{0\\}^{1}TPR(t)d(FPR(t))$\nWhere:\nTPR(t) = TPR at threshold t.\nFPR(t) = FPR at threshold t.\nTrue Positive Rate (TPR): TPR (Sensitivity) = TP / (TP + FN)\nFalse Positive Rate (FPR): FPR (1 - Specificity) = FP / (FP + TN)\nTP = True Positives\nFN = False Negatives"}, {"title": "8.4 Supplementary Results 1", "content": "In these studies, we evaluated if using the average patch embeddings instead of the [CLS] token or both\nconcatenated for image classification improves DR staging with DINORET, BE DINORET, or DINOv2. We\nperformed experiments similar to the cross-evaluation experiments. In short, models were trained on all\nfour DR datasets individually, with both frozen and unfrozen BBs, and evaluated on the test sets of each\nDR dataset, resulting in 16 total experiments per model. Subsequently, we compared DR staging\nperformance when using the [CLS] token, the average patch embeddings, or both concatenated. As\nillustrated in Supplementary Table ST1.1, using the [CLS] token for DR staging resulted in a better qKappa\nscore than the other embeddings in 6/16 tasks for unfrozen DINOv2, 3/16 for unfrozen DINORET, and 8/16\nfor unfrozen BE DINORET. Supplementary Figure SR1.1 depicts all results obtained from these\nexperiments. Additionally, we performed a few-shot study on all DR datasets for the DINOv2 model, where\nruns were performed in quintuplicate and until the sample count on the training dataset could not be\nincreased further, as illustrated in Supplementary Figure SR1.2. For unfrozen DINOv2, using the patch\nembeddings resulted in a significantly higher qKappa score on APTOS with 32 training images per class,\ncompared to the [CLS] token (p = 0.049) and the concatenated embeddings (p = 0.028, ANOVA with Tukey\nHSD), and a significantly higher qKappa score on IDRID compared to the [CLS] token with 16 training images\nper class (p = 0.034, ANOVA with Tukey HSD). All other differences lacked significance, as shown in\nSupplementary Data 2. Finally, we investigated whether the role of using a distance weighted, scaled cross\nentropy loss (CEL) function would improve DR staging with fine-tuned models. All 16 cross-evaluation\ncomparisons were repeated with DINOv2, using both the vanilla CEL and the scaled CEL. The median score\nwas higher for the scaled CEL for all evaluation metrics, except for qKappa, as depicted in Supplementary\nFigure SR1.3."}, {"title": "8.5 Supplementary Results 2", "content": "As shown in Supplementary Table SR2.1, RETFound ranked amongst the best 2 models in 75% (out of 12\ncross-evaluation experiments) for AUC ROC and 50% for qKappa (always unfrozen, never frozen). DINORET\nranked amongst the best 2 models in 50% for AUC ROC (25% frozen, 25% unfrozen) and 75% for qkappa\n(25% frozen, 50% unfrozen). DINOv2 ranked amongst the best 2 models in 66.6% of cross-evaluations for\nAUC ROC (50% frozen, 16.6% unfrozen) and 33.3% for qKappa (25% unfrozen, 8.3 percent frozen). The BE\nDINORET model only ranked amongst the best two models in 8.3% for AUC ROC (always frozen) and 41.6%\nfor qkappa (33.3% unfrozen, 8.3% frozen)."}, {"title": "8.6 Supplementary Results 3", "content": "For a pooled analysis, a hierarchical LMM was fit across all runs on all datasets, in the range approximately\nlinearly increasing with the few-shot sample count (8-32). For both qKappa and AUC ROC, models were\nsignificant predictors of score outcomes (p = < 0.01, LRT). Post-hoc pairwise comparisons of models were\nconducted using estimated marginal means (EMMeans) with Kenward-Roger adjustment and compared\nvia a Tukey HSD test. Unmodified DINOv2 significantly outperformed RETFound for AUC ROC and qKappa,\nDINORET and BE DINORET also outperformed RETFound, but differences were only significant for AUC ROC\n(BE DINORET) and qKappa (DINORET) individually, as shown in Supplementary Data 6. Differences between\nBE DINORET, DINORET, and unmodified DINOv2 were all insignificant. Supplementary Figures SR3.1 and\nSR3.2 present diagnostics from the model fits and Supplementary Figure SR3.3 displays the fitted average\nAUC ROC and qKappa scores for each model at a given sample count."}, {"title": "9. Computational Resources", "content": "All experiments shown in this paper were run on a NVIDIA A10 cloud GPU with a 30 core CPU. The total\ntraining and evaluation time was approximately 1.5 weeks. This does not include the computation required\nfor development."}]}