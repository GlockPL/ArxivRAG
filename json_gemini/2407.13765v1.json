{"title": "Latent Causal Probing: A Formal Perspective on Probing with Causal Models of Data", "authors": ["Charles Jin"], "abstract": "As language models (LMs) deliver increasing performance on a range of NLP tasks, probing classifiers have become an indispensable technique in the effort to better understand their inner workings. A typical setup involves (1) defining an auxiliary task consisting of a dataset of text annotated with labels, then (2) supervising small classifiers to predict the labels from the representations of a pretrained LM as it processed the dataset. A high probing accuracy is interpreted as evidence that the LM has learned to perform the auxiliary task as an unsupervised byproduct of its original pretraining objective. Despite the widespread usage of probes, however, the robust design and analysis of probing experiments remains a challenge. We develop a formal perspective on probing using structural causal models (SCM). Specifically, given an SCM which explains the distribution of tokens observed during training, we frame the central hypothesis as whether the LM has learned to represent the latent variables of the SCM. Empirically, we extend a recent study of LMs in the context of a synthetic grid-world navigation task, where having an exact model of the underlying causal structure allows us to draw strong inferences from the result of probing experiments. Our techniques provide robust empirical evidence for the ability of LMs to learn the latent causal concepts underlying text.", "sections": [{"title": "1 Introduction", "content": "As large LMs pretrained on massive amounts of unlabeled text continue to reach new heights in NLP tasks (and beyond), the question of what kinds of information such models encode about their training data remains a topic of intense discussion and research. One prominent technique is to supervise small probing classifiers to extract some linguistically relevant property from the representations of the pretrained LM (Shi et al., 2016; Adi et al., 2017; Alain & Bengio, 2018), with the intuition being that the success of the probe reveals the LM has, in fact, learned to encode the property of interest as a byproduct of its training.\nDespite their widespread usage, however, probes themselves are also an active area of research, with a number of interconnected open questions in the design and interpretation of probing experiments (Belinkov, 2022), including:\n(Q1) Control and interpretation. Given that the probe itself is directly supervised to perform the auxiliary task, the observed outcomes could depend not only on the information inherently encoded in the LM but also the ability of the probe to extract the information itself. For instance, researchers have found that training probes to predict randomized labels can often yield comparably high accuracies on certain tasks, calling into question the significance of prior results (Hewitt & Liang, 2019). As a result, drawing robust conclusions from the classification accuracy of a probe remains up for debate.\n(Q2) Classifier selection and training. To combat the risk of measuring the probe's capacity to learn the auxiliary task, researchers often limit probes to low capacity architectures such as linear classifiers (Maudslay et al., 2020). However, other works have countered with evidence that LMs encode more complex concepts using non-linear representations, which can only be accurately measured using higher capacity classifiers (Belinkov & Glass, 2019;"}, {"title": "(Q3) Auxiliary task design.", "content": "Finally, as large, pretrained LMs have progressed from producing human-like text to exhibiting increasingly \u201cintelligent\u201d behaviors such as reasoning and in-context learning (Brown et al., 2020), there is an emerging need to better understand the limitations and capabilities of LMs along dimensions such world knowledge and theory of mind. These domains present a distinct set of challenges compared to traditional linguistic tasks such as part-of-speech tagging and dependency parsing.\nThe theoretical section of this paper develops a formal perspective on probing using the language of structural causal models (SCM). Specifically, given a causal model which explains the distribution of tokens observed during training, we pose the central hypothesis as determining whether the LM has learned to represent the latent causal variables of the SCM: concepts that explain how the text was generated, but are never directly observed during training. We then introduce probes as a means of empirically testing such hypotheses, by extracting the value of the latent concepts given only the LM representations as input. Our setting naturally captures broader questions about the inductive bias of LMs trained solely on text, and the latent concepts they acquire over the course of training (Q3)."}, {"title": "2 Structural causal models of text", "content": "This section introduces the setting of our framework for probing, which is based on the idea that the text used to train LMs may exhibit latent causal structure; we formalize these concepts using the approach of structural causal models."}, {"title": "2.1 Background: structural causal models", "content": "Structural causal models are graphical models which represent causal relationships in a data generation process as a directed graphical model (Pearl et al., 2000). We illustrate the key concepts with an example; we refer the reader to Pearl (2010) for a more comprehensive overview. Suppose that we are interested in the effect the weather has on employees bringing an umbrella to work. In this case, we may hypothesize a SCM like the one in Figure 1a. Each node represents a different random variable: the weather, the weather forecast, whether the employee's morning gets off to a late start, and whether the employee brings an umbrella to work. Nodes without parents are exogenous variables, whose causes are left unexplained; they are often used to model nature, randomness, or other aspects of physical reality, such as genetic or environmental factors. The exogenous variables in Figure la are the weather and having a late start. Nodes with a parent indicate the possibility of a causal relationship, e.g., the edge from weather to forecast indicates that the weather might influence the forecast. In particular, every missing edge in the SCM asserts the lack of a causal relationship. A standard assumption of causal analysis is that the underlying causal graph is Markovian (or acyclic); we adopt this assumption as well."}, {"title": "Mediators and moderators.", "content": "Returning now to our original question of how the weather affects employees bringing an umbrella to work, we note the SCM hypothesizes 3 possible causes: the weather, the weather forecast, and having a late start. The forecast is a mediator because total causal effect of the weather on umbrella in partially transferred by the path-specific effect over the weather-forecast-umbrella pathway (Avin et al., 2005; Imai et al., 2010). A natural question is how much the forecast is responsible for the increase in likelihood that an employee brings an umbrella to work when, for instance, the weather changes from sunny to rainy. One answer is given by the necessary indirect effect, which quantifies how much the presence of the causal path through the mediator contributes to the total measured effect (Weinberger, 2019):\n$NIE_{rain,sun}(Forecast) = E[Umbrella | Weather = rain]$\n$- E [Umbrella | Weather = rain, do(Forecast = sun)],$\nwhere $do (Forecast = sun)$ is a causal intervention. The intervention can be conceptualized as forcing the weather station to forecast a sunny day independent of the weather, thereby severing the weather-forecast-umbrella pathway. Figure 1b depicts the SCM post-intervention.\nThe late start variable is not a mediator of the weather-umbrella causal effect (because there is no path from the weather to umbrella that passes through it), but it could still be a moderator: variables that do not directly mediate a causal effect, but affect the strength (and possibly direction) of another causal path (Baron & Kenny, 1986). For instance, the forecast's effect on whether an employee brings an umbrella (i.e., the NIE) might be lower if the employee has a late start and rushes out the door without checking the forecast."}, {"title": "2.2 Case study: causal structure in programming languages", "content": "Jin & Rinard (2023) propose an experiment to study whether LMs are able to ground a sequence of actions into a sequence of states, having only seen examples of the initial and final state during training. Specifically, they train a 350M parameter Transformer (Vaswani et al., 2017) on a corpus of specification-program pairs using a standard causal language modeling objective. The programs are strings in a grid-world navigation language with 5 actions (move, turn_right, turn_left, put_marker, pick_marker), sampled uniformly between lengths 6 and 10, inclusive. The specifications consist of the initial and final state, which are 8x8 grids. Executing the program navigates a single robot in the initial state to the final state. We refer to Jin & Rinard (2023) for more details about the language.\nFigure 2 displays an SCM of the data generation process (along with an example assignment of values to each variable). The exogeneous variables are the initial state and the program actions. Each action produces a latent state (green), save for the last action, which is observed as the final state. A training sample consists of the sequence: $s_0, s_n, p_1, ..., p_n$, where each grid world is converted to text by scanning in row order, with one token per entry."}, {"title": "3 Latent causal probing", "content": "We present latent causal probing, a formal framework for empirically testing the hypothesis\nLanguage models are latent causal concept learners.\nAt a high level, given an SCM that models the training data as the observed variables, we probe the LM for representations of the latent variables of the SCM. Our main insight, as illustrated in Figure 2, is that knowing the latent value of $s_{n-1}$ could help predict the observed value of $p_n$; hence, an LM trained to predict $p_n$ might eventually induce the existence of the latent variable $s_{n-1}$. More generally, we refer to any latent variable with a causal effect on the distribution of the training data as a latent causal concept."}, {"title": "3.1 Probing for latent variables", "content": "We begin by defining the auxiliary task and dataset for probing. Fix some structural causal model M, and let $U_M$ be the latent variable of interest. Given some text x, we use $V_M(x)$ to denote the value of the latent variable in the SCM of text x. For instance, the value of $U_M = s_1$ in the sample x from Figure 2 is the grid depicted in the $s_1$ node. We assume that the value of each latent variable is uniquely determined by x and M.\nGiven a language model LM with parameters $\\theta$, we denote an arbitrary representation function as $LM(x;\\theta)$. The auxiliary dataset consists of input features {$LM(x;\\theta) | x \\in X$} and labels {$V_M(x) | x \\in D$}, where $D = {x_i}_{i=1}^{N}$ is a corpus of text. We then split $D$ into two auxiliary datasets: one for calibration and one for measurement. The probe is trained"}, {"title": "3.2 Causal mediation analysis of probing", "content": "We next turn to controlling for the probe (Q1). Intuitively, the challenge is any measurement using a supervised probe conflates the LM's representation of the auxiliary labels with the probe's ability to learn the auxiliary task (Hewitt & Liang, 2019). While there exist a number of proposals for controlling for the contribution of the probe, such techniques typically do not provide any formal guarantees, rendering their correct application and interpretation a challenge (Belinkov, 2022)."}, {"title": "3.3 Discussion", "content": "We summarize the latent causal probing framework as follows:\n1. Select a set of exogenous, latent, and observed variables and pick a hypothesis class M of SCMs (from the set of all Markovian SCMs over the variables).\n2. Fix a specific target SCM $M \\in M$ and a set of latent variables $v \\in M$ to test.\n3. Construct the auxiliary dataset and create the bound vs. free partition (if possible).\n4. Identify a valid baseline M' and perform the mediation analysis.\nA significant measurement $acc(M, M) \u2013 acc(M, M') > 0$ is interpreted evidence that the LM encodes causal concepts in its representations. We conclude with some remarks.\nInterventions, and probing for non-causal latent variables. Our mediation technique requires knowing \u201cwhat would the text have been if the underlying dynamics were different?\u201d, which could be difficult (especially in non-synthetic domains). Similarly, for non-causal latent variables, such as part-of-speech, producing a hypothesis class M with more than one SCM may not be possible: what would the data look like in a counterfactual world in which \u201cdog\u201d is actually an adverb? Unfortunately, our analysis shows that a baseline M' which induces a different distribution of text is a necessary precondition, since otherwise $NIE_{M,M'}(\\theta_{LM})$ and $NIE_{M',M}(\\theta_{LM})$ cannot both be positive (as M and M' are indistinguishable when used to train the LM). Intuitively, we interpret this result as saying any measurement is inherently biased when the auxiliary task has only one \u201cright\u201d answer.\nProbe architecture and hyperparameters. Our framework also explicates the role of the probe's architecture and other hyperparameters in the training process, such as the optimizer, learning rate, dataset size, etc., as potential moderators, but not mediators (Q2). In other words, so long as there exists a choice of hyperparameters such that the NIE is positive, the analysis concludes that there exists a causal effect mediated by the model's parameters (although certain settings of the moderator variables could offer additional interpretations). Practically speaking, our framework also offers a novel way to interpret (and justify) complex probes (Voita & Titov, 2020; Pimentel & Cotterell, 2021)."}, {"title": "4 Experiments", "content": "We conduct empirical study of whether an LM, trained from scratch on a corpus of program data, learns the latent causal concepts in the underlying data generation process."}, {"title": "4.1 Methods", "content": "We describe the key steps according to the framework in Section 3.3; Appendix A.1 contains full experimental details (e.g., LM and probe architecture and training, LM representations).\nHypothesis class. The exogenous variables are the initial state and program. The latent variables are the intermediate states, and the observed variables are the initial and final state and the program. The hypothesis class M is all Markovian SCMs over the variables.\nTarget SCM and latent variables. The target SCM M \u2208 M is the true data generation process in Figure 2. The target latent variables consist of the robot's position, facing direction, and whether the robot is facing a rock for each intermediate state.\nAuxiliary dataset construction and bound and free latent variable outcomes. For the auxiliary dataset, we use the same data generation process, except that programs range in length between 1 and 15, and we replace the final state in the specification with the initial state. Due to the size of the training corpus (1 million samples), we assume the LM observed all combinations of the exogenous variables. Because the programs in the LM training corpus are between length 5 and 9, the bound latent variables are $s_6$ to $s_{10}$ (they are observed during training as the final state). The free latent variables are $s_1$ to $s_5$ and $s_{11}$ to $s_{15}$.\nValid baseline. We construct valid baselines according to a counterfactual state of the world where the intermediate states are generated by executing the program according to a different set of causal dynamics. Specifically, we define M' using the same SCM, but permute the causal dynamics of the turn_right, turn_left, and move actions (e.g., the robot turns left when executing a turn_right action). As M and M' are clearly symmetric from a language modeling perspective, Definition 3.1 (and hence Proposition 3.2) holds."}, {"title": "4.2 Results", "content": "Figure 4 plots the main results. For all four measurements (deductive knowledge, inductive bias, deductive bias, and inductive knowledge) and across all three probes (linear, 1-layer MLP, 2-layer MLP), the mediated measurements are significantly positive (above the dashed line at 0%) by the end of training. We thus conclude that a positive fraction of the observed measurements of causal concepts can be attributed to what is learned by the LM's representations. We also note that the linear probe exhibits the lowest mediated measurements of the 3 probes across all four tasks. This suggests that more complex auxiliary tasks require more complex probes and, more generally, highlights the importance of probing frameworks that can accommodate deeper probes. Appendix A.2 contains further results and analyses."}, {"title": "5 Related work", "content": "Causal interpretability of LMs Several prior lines of work apply causal techniques to the interpretability of LMs. These works typically intervene on either the model's representations (Elazar et al., 2021; Geiger et al., 2021; Meng et al., 2022; Abraham et al., 2022; Li et al., 2022) or the model's inputs (Kaushik et al., 2020; Vig et al., 2020; Gangal & Hovy, 2020; Amini et al., 2023), and analyze the causal effect on the LM's outputs. In contrast, we present a formal framework that, conceptually, intervenes on the model's training data, and measures the causal effect on the LM's internal representations as a proxy for knowledge. To the best of our knowledge, Elazar et al. (2022) is the only other work that studies the causal relationship between the training data and the LM, presenting a technique for estimating the causal effect of dataset statistics on the factuality of LM's outputs.\nCausal knowledge and reasoning in LMs A number of benchmarks test for causal knowledge in pretrained LMs by eliciting outputs on causal reasoning tasks; we refer to Yang et al. (2023) for a survey. Recently, however, researchers have raised concerns that performance on such benchmarks may not correspond to true causal reasoning (Zhang et al., 2023; Ze\u010devi\u0107 et al., 2023). As Yang et al. (2023) state, \u201cThe issue here is that the LLM does not need to actually reason at all: it can simply access its training dataset, which contains millions of stories about weather and umbrellas, and approximately retrieve a response.\u201d Similarly, Wu"}, {"title": "Frameworks for probing", "content": "A number of works have proposed frameworks toward a more rigorous understanding of probing. One line takes an information-theoretic view on the information represented by the LM (Zhu & Rudzicz, 2020; Pimentel et al., 2020; Voita & Titov, 2020; Pimentel & Cotterell, 2021). In contrast, we use probes to identify causal effects and inductive biases. Immer et al. (2022) propose an interpretation of probing as quantifying the inductive bias of pretrained representations for downstream tasks, but their framework differs significantly from ours in that the model is understood as a representation-probe pair. In contrast, our approach unifies the LM training and probe calibration procedures under a single causal framework for analysis, yielding formal guarantees for the control of probes. Our analysis also reveals settings in which prior techniques, such as control tasks (Hewitt & Liang, 2019), can yield biased estimates of the intended auxiliary measurement."}, {"title": "6 Conclusion", "content": "This paper presents latent causal probing, a probing framework that studies whether LMs learn latent causal concepts as a byproduct of the language modeling objective. Our framework offers robust tools for interpreting experiment results through the lens of causal analysis, and in particular, rigorously controls for the probe's contribution in learning the auxiliary task. Experimentally, we extend a previous study of whether Transformers can infer the intermediate states that underlie a sequence of actions. Our results provide strong empirical evidence that LMs can induce latent causal concepts from textual pretraining."}, {"title": "A Additional experimental details and results", "content": "This section present some ablation studies on the set up of the probing experiments."}, {"title": "A.1 Language model and probe details", "content": "Following Jin & Rinard (2023), the language model is a 350M parameter CodeGen model (Nijkamp et al., 2023) taken from the HuggingFace Transformers library (Wolf et al., 2020). The model was trained for 2.5 billion tokens, which was roughly 6 passes or 80000 training batches over the training corpus. We refer to Jin & Rinard (2023) for further details.\nWe next describe the design and training of the probing classifiers; these notes apply to all the probing experiments, unless otherwise noted. The linear probe is a single linear layer. The MLP probes have ReLU, batch_norm, then dropout(p=.2) after each linear layer. The hidden dimensions of the 1-layer and 2-layer MLP probes were (256,) and (256, 1024), respectively. The auxiliary datasets consisted of 500000 randomly selected samples. To extract representations from the LM, we use the same strategy as Jin & Rinard (2023), averaging the LM hidden states over the layer dimension after processing each program token. Probes were trained using AdamW (Loshchilov & Hutter, 2019) with weight decay of 1e-4. The learning rate starts at 0.01, then decays by .1 at 75% and 90% through training. All probes are trained for 2000000 steps using a batch size of 256.\nFor the mediated results reported in Figure 4, we generated the auxiliary dataset using an SCM that maps turn_right to turn_left, turn_left to move, and move to turn_right."}, {"title": "A.2 Ablation studies", "content": "This section present some ablation studies on the set up of the probing experiments."}, {"title": "A.2.1 Valid baseline selection", "content": "To test the sensitivity of the mediated results (and hence, overall conclusions) on the choice of valid baseline, we generate two additional auxiliary datasets with the following SCMs:\n1. swap move and turn_left\n2. swap turn_right and turn_left\nThe results are plotted in Figure 5 and Figure 6, respectively. We find that the mediated measurements in the first case are nearly identical to those in Figure 4, despite only swapping two actions (instead of permuting 3). However, in the second case, the mediated measurements are essentially noise, centered around 0. We attribute this to the fact that the resulting labels are extremely similar, as, in most cases, the robot is simply reflected along the starting axis. We emphasize that a negative result for one valid baseline does not constitute evidence to reject the hypothesis, and that a single positive result from a valid baseline is sufficient to accept the hypothesis."}, {"title": "A.2.2 Probe architecture and hyperparameters", "content": "We next ablate the probe architecture and hyperparameters by adopting the settings used in Jin & Rinard (2023). The differences are: no dropout, a batch size of 1024, training the probe for 10000000 steps, and using 100000 samples in the auxiliary dataset. We use the same valid baseline as in Figure 4 of the main text.\nThe results are plotted in Figure 7. We observe that the general trends are preserved, and all four mediated measurements ending above 0% by the end of training. However, we note that both deductive and inductive knowledge measure slightly lower, which is an example of the moderating effect of the probe architecture and training hyperparameters. We attribute the effect to the increased batch size and lack of dropout, which could encourage the probe to converge more quickly to a global optimum, given that the risk of overfitting is low (due to the large size and high quality of the training dataset). This is also consistent with (1) the general intuition that simpler (or less optimal) probes are a proxy for \u201cease of extraction,\u201d which is often interpreted as evidence that the representations are \u201cmore aligned\" with the target features (Hewitt & Liang, 2019), and (2) the theoretical findings in Pimentel et al. (2020), who conclude that probes of infinite capacity are most informative for measuring syntactic knowledge."}, {"title": "B Comparison with Jin & Rinard (2023)", "content": "In this section, we highlight several key departures from the experimental design in Jin & Rinard (2023).\nFirst, they do not split their auxiliary dataset into bound and free latent variable outcomes, and hence their results do not yield fine-grained interpretations about probing with different calibration and measurement datasets.\nSecond, our analysis reveals the presence of possible confounders in the design of their interventional baseline, leading to uncontrolled effects. In particular, the auxiliary dataset is constructed using programs generated by the LM itself, rather than randomly sampled as we do. Intuitively, this means that the LM \u201csees\u201d both $s_0$ and $s_n$, which reveals information about the original casual dynamics. Formally, the representations of the LM used for probing mediates all 3 causal pathways, rather than the simple causal pathway from the LM training data (as in Figure 3), and hence their interventional baseline is not a proper measurement of the causal effect mediated by the LM representations. Our solution is to use randomly sampled programs and replace the occurrence of $s_n$ with $s_0$ in the construction of the auxiliary dataset, which breaks this causal dependence.\nFinally, Jin & Rinard (2023) do not verify that their interventional baselines satisfy the conditions in Equations (1) and (2). In particular, one of their baselines map the put_marker and pick_marker actions to turn_right and turn_left, respectively, in addition to permuting the turn_right, turn_left, and move actions. Because the extracted features all relate to the position and direction of the robot, the new dynamics could present a more difficult task (for both the LM and the probe) due to replacing what were effectively no-ops (put_marker and pick_marker) with new operations that affect the position or direction (turn_right, turn_left, and move). Hence, the observed drop in accuracy post-intervention could be attributable to increased task difficulty, rather than the learned representations of the LM."}, {"title": "C Proofs", "content": "Proof of Proposition 3.2. The proof follows directly from substituting the appropriate assumptions into the definitions of NIE. Recall that\n$NIE_{M,M'}(\\theta_{LM}) := acc(M, M) \u2013 acc(M', M)$\n$NIE_{M',M}(\\theta_{LM}) := acc(M', M') \u2013 acc(M, M'),$\nand, by Definition 3.1,\n$acc(M', M') \\geq acc(M, M)$\n$acc(M, M') \\geq acc(M', M)$.\nApplying Equation (5) to the definitions of NIE,\n$NIE_{M,M'}(\\theta_{LM}) \\leq acc(M', M') \u2013 acc(M', M)$\n$= NIE_{M',M}(\\theta_{LM}).$\nApplying Equation (6) to the definition of NIE,\n$NIE_{M,M'}(\\theta_{LM}) = acc(M, M) \u2013 acc(M', M)$\n$> acc(M, M) \u2013 acc(M, M').$\nHence,\n$acc(M, M) \u2013 acc(M, M') \\leq NIE_{M,M'}(\\theta_{LM}) \\leq NIE_{M',M}(\\theta_{LM})$"}]}