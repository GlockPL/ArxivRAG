{"title": "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories", "authors": ["Ben Bogin", "Kejuan Yang", "Shashank Gupta", "Kyle Richardson", "Erin Bransom", "Peter Clark", "Ashish Sabharwal", "Tushar Khot"], "abstract": "Given that Large Language Models (LLMs) have made significant progress in writing code, can they now be used to autonomously reproduce results from research repositories? Such a capability would be a boon to the research community, helping researchers validate, understand, and extend prior work. To advance towards this goal, we introduce SUPER, the first benchmark designed to evaluate the capability of LLMs in setting up and executing tasks from research repositories. SUPER aims to capture the realistic challenges faced by researchers working with Machine Learning (ML) and Natural Language Processing (NLP) research repositories. Our benchmark comprises three distinct problem sets: 45 end-to-end problems with annotated expert solutions, 152 sub-problems derived from the expert set that focus on specific challenges (e.g., configuring a trainer), and 604 automatically generated problems for larger-scale development. We introduce various evaluation measures to assess both task success and progress, utilizing gold solutions when available or approximations otherwise. We show that state-of-the-art approaches struggle to solve these problems with the best model (GPT-4o) solving only 16.3% of the end-to-end set, and 46.1% of the scenarios. This illustrates the challenge of this task, and suggests that SUPER can serve as a valuable resource for the community to make and measure progress.", "sections": [{"title": "1 Introduction", "content": "Research and scientific discoveries often rely on the reproducibility of experiments conducted by other researchers and the ease with which scientists can build upon each other's work. In the context of empirical ML and NLP research, it is often crucial for researchers to be able to execute and reproduce diverse research experiments from open-source repositories, whether to verify existing results or to test them under new conditions.\nIn practice, even when research code is available, running code from arbitrary repositories is often non-trivial and time-consuming. Experimentation frequently requires substantial effort to set up and execute them: installing the environment, making non-trivial configuration changes, resolving outdated package dependencies, fixing bugs, and determining the correct execution commands, among other tasks. All of this requires a considerable understanding of the documentation and repository code, knowledge about fixing issues (e.g., CUDA errors), as well as the ability to modify the code appropriately. These steps are especially time-consuming for research repositories \u201cin-the-wild\", as support and documentation may not be available.\nIn this work, we ask: Can LLMs automate the set up and execution of tasks in research repositories? Consider the research task in Fig. 1 where the agent is asked to use a research code repository to train a model with a new optimizer, and evaluate its performance on a custom dataset. A successful agent would need to set up the experiment by installing dependencies, downloading the provided data, and making code changes to load it (first three cells in the figure), then execute the training script while responding to unexpected issues such as an incompatible dependency (fourth and fifth cell), and finally report the result metrics (last cell).\nWhile LLM-based agents have recently been used to produce execution commands from popular research repositories , execute popular ML repositories , or resolve repository issues , no existing benchmark evaluates agents on the common problem faced by many researchers: both setting up and executing experiments using research repositories in-the-wild, i.e., less popular repositories that are not typically well-documented or maintained, which make experiments harder to configure and execute. As a recent study shows , both novice and advanced researchers find the challenge of \"setting up the code base\" to be the most difficult part of reproducing experiments.\nTo encourage research on this problem, we introduce SUPER (Setting UP and Executing tasks from Research repositories), a benchmark focusing on such lower-profile research repositories. SUPER consists of three distinct problem sets. The Expert set contains 45 manually curated problems solved by experts. The Masked set includes 152 sub-problems derived from the expert set through our proposed \"Code Masking\u201d mechanism, where we remove parts of the expert-written code to generate a diverse set of sub-problems targeting specific challenges. Each sub-problem addresses a specific challenge, such as installing dependencies and resolving conflicts, configuring experimental data, setting up hyper-parameters, resolving runtime exceptions, correctly executing scripts, etc. Lastly, the Auto set contains an additional 604 automatically generated tasks with an even more diverse set of repositories and challenges. It can potentially be used in future work for development, fine-tuning, or training using environment feedback.\nTo evaluate agents on the Expert and Masked"}, {"title": "2 Related Work", "content": "Coding benchmarks: While early code benchmarks mainly focused on synthesizing simple functions from descriptions, recent benchmarks have shifted to more complex competitive programming problems and evaluating proficiency with popular data science libraries. Unlike these, we follow the recent trend on evaluating LLMs in more natural programming scenarios, such as programming with external tools and APIs, code editing and debugging , resolving GitHub issues and understanding and cod-"}, {"title": "3 Benchmark Construction", "content": "In this section we describe the process of building the SUPER benchmark. The SUPER benchmark consists of 3 sets serving different purposes. The Expert set (\u00a73.1) contains manually written problems, solved by experts. The Masked set (\u00a73.2) contains sub-problems extracted from the Expert set using the gold solution, which provide easier and more focused sub-problems. Fig. 2 provides a high-level overview of the construction pipeline of these two sets. Finally, the Auto set (\u00a73.3) contains automatically generated problems which can be used for development and improvement of agents.\nEnvironment Setup. Running research-oriented repositories often necessitates both being able to run system shell commands (e.g. to install dependencies and run scripts) and stateful Python commands. Previous work and environments typically support only one of these (e.g., only system shell commands  or only Python commands ). Instead, we build an environment that allows running both of these commands with a Jupyter notebook as engine. In this setup, each execution code is equivalent to running a notebook cell, which contains Python code and/or bash commands, and where state is reserved between cell executions (e.g., each cell can use any of the previously defined Python variables). The execution of each cell returns an observation string."}, {"title": "3.1 Expert Set", "content": "We construct the Expert set by (1) identifying a set of relevant code repositories from research papers and manually writing research-oriented tasks based on them and (2) asking human experts to provide end-to-end solutions for these tasks (\u00a73.1.1). We then use the expert solutions as the basis for outcome-based evaluation, where we compare the agent's answer to the gold answer, and a more lenient landmark-based evaluation that indicates progress toward correctly solving the task, even if the solution is not entirely correct (\u00a73.1.2)."}, {"title": "3.1.1 Construction", "content": "Tasks. We create tasks motivated by the following two common settings: (1) reproducing numbers from research papers by running specific experiments, and (2) running modified experiments with different datasets, models, or configurations.\nWe start by collecting repositories from the \u201cPapers With Code\u201d (github.com/paperswithcode/ paperswithcode-data) database, which contains research papers linked to their GitHub repositories, along with some additional metadata such as the modality of the datasets used. We only sample research papers with \u201cText\u201d modalities and select repositories from 2021 or beyond.\nWe then manually review the sampled repositories and write tasks that involve running a single experiment that is mentioned either in the repository's \"readme\" file or under a script available in the repository, if such can be found. Whenever possible, we make the task more challenging by requiring the experiment to be run on a new dataset or model, other than the one described in the available documentation. In these cases, we select either datasets available on Hugging-Face Hub (https://huggingface.co/datasets) or provide a Google Drive link where the dataset can be found. The challenge of running on a specific dataset varies in difficulty: it could involve only a single configuration line change if the dataset is already supported, or creating a new dataset reader, adjusting column names, etc.\nFor each task, we define (1) the target Github repository, (2) the task definition (e.g., \u201ctrain a model...\"), (3) the metrics or output to be reported (e.g., \"F1 metric on the validation set\u201d), along with a specific structure of how the answer should be formatted, and (4) implementation instructions (e.g. specific hyper-parameters). The implementation instructions are important for two reasons:\""}, {"title": "3.1.2 Evaluation", "content": "Accuracy Evaluation. As described in \u00a73.1.1, experts provide us a deterministic solution for each task, which we then execute in our environment to get the gold answer, allowing us to evaluate agents based on their outcome. Answers consist of several values (e.g., numbers for metrics, string for model predictions). We define the accuracy metric as the portion of correctly answered values: where the predicted answer precisely matches the gold one (up to a $10^{-2}$ error). Unlike reference based evaluation used in cloze tests and various prior coding benchmarks , outcome-based evaluation allows for alternate valid solutions.\nLandmark-Based Evaluation. Sometimes an indication of whether the model was precisely correct may be too strict, \"punishing\u201d models that make progress but don't reach the end. E.g., an agent that loads the data but doesn't train would have the same accuracy as an agent that fails at the start.\nTo measure progress towards the final goal, we use the gold task notebooks to identify landmark outputs; outputs from the environments that act as \"evidence\" that a particular step was run successfully. E.g., the explicit output string \u201c*** training completed ***\" in Figure 1 or the string \"Loading data... 100%\" implying successful data loading.\nImportantly, a perfect landmark score does not entail a perfect accuracy score, as landmarks only"}, {"title": "3.2 Masked Coding sub-problems Extraction", "content": "Solving an end-to-end execution task can often be long and complex, consisting of multiple non-trivial steps. As such, evaluating agents on their ability to run an entire task provides a sparse signal of success, where agents have to complete numerous steps correctly to succeed, making it harder to \"hill-climb\" results. Instead, we want to evaluate models in a more fine-grained way that will allow us to get success signals for any incremental progress towards the task. To this end, we propose to focus on a specific sub-problem from the task solution at a time, leveraging the expert solutions from the expert set.\nWe turn to an approach loosely inspired by cloze tests  and masked language models (MLM; ): given a gold solution for a task we remove (mask) some part of it (e.g., code that solves a dependency installation issue), and manually define the sub-problem such that an agent only needs to solve this narrower aspect of the overall task, as captured by the removed cells. While the ideal goal is for the agents to complete tasks in an end-to-end manner, extracting"}, {"title": "3.3 Automatically Generated Tasks", "content": "The Expert and Masked sets provide validated problems and reproducible solutions, allowing for more accurate evaluation of agents. However, creating expert tasks is both time-consuming and costly, and the limited number of tasks hinders their use for agent improvements, such as fine-tuning models based on trajectories and environment feedback. To address this, we automatically generate tasks using an LLM (namely, GPT-4o)."}, {"title": "3.3.1 Construction", "content": "Generation involves two steps: (1) filtering suitable repositories, and (2) generating tasks based on the readmes of these repositories.\nFiltering. We start with the same list of repositories from \u00a73.1.1, selecting those listed in 2021 or later, resulting in a total of 5915 repositories. Many of these repositories cannot be trivially used to generate tasks due to various limitations: some do not support running any experiment, some have missing dataset links, some require GPU hardware, and some depend on APIs of other LLMs (such as OpenAI or Anthropic), which could be unavailable or incur costs for users. To alleviate these issues, we employ a combination of heuristic and LLM-based filtering methods. Specifically, we keep repositories that mention specific datasets and models, do not use APIs, and do not require GPUs. Detailed information on the filters and the prompt used for LLM-based filtering can be found in Appendix B.\nCreating Tasks. Given the filtered repositories, we prompt the LLM with the contents of each repository's README file and instruct it to create an experiment-running task. This includes defining the goal, dataset, model, and script to be run (\u201cRun probability-based prompt selection on the SST-2 dataset using opt-125m as the base model with the script \u2018run_prompt_selection.py\u2018\u201d). We also specify that the LLM should choose the smallest model within model families (e.g., BERT-base if BERT models are supported). See Appendix B for further details on the generation process. To verify the quality of the generated set, we sample 100 generated tasks and find that 81% of the samples are feasible; among the rest, the most prominent issue was missing resources (dead or gated links to datasets, missing code) and conceptually flawed"}, {"title": "3.3.2 Evaluation", "content": "Without expert solutions, we cannot evaluate based on outcomes or landmarks. Instead, we use a simple heuristic metric, termed Script-Executed, to ensure the model sets up and executes the experiment without unresolved issues: we check if the script the agent was asked to run executes without exceptions for a minimum duration (see Appendix B.2 for details). The minimum duration ensures that the script was successful and didn't just fail silently. While this method does not guarantee perfect evaluation, we find it surprisingly effective, as we show in our analysis in \u00a74.3."}, {"title": "3.4 SUPER Benchmark", "content": "The Expert set consists of 45 collected tasks, where each problem is paired with a gold output for outcome-based evaluation and a small set of landmarks for our softer evaluation metric (an average of 3 landmarks per problem).\nTo roughly estimate how much lines of code"}, {"title": "4 Experiments", "content": "Experimental Setup. We limit the execution time (i.e. time to run commands, not counting the API reply time) of each problem to 30 minutes. We run all tasks on compute instances using sandboxed execution in Modal (https://modal.com),"}, {"title": "4.1 Baselines", "content": "In this section, we describe the three baseline agents that we evaluate on SUPER: ReAct , our improved version ReAct-SUPER, and SWE-Agent . All of our agents are given access to a Jupyter notebook environment where they can execute Python or Bash commands. Other than the execute action, they can also submit an answer when done. For sub-problems, we execute the provided \u2018pre-execute' cells (\u00a73.2) and pass them as an existing history of actions to each agent. With end-to-end tasks (Expert and Auto), we simply prompt the agent with the task description.\nReAct is a baseline agent that iteratively prompts the underlying LLM to output both an action and a natural language \"thought\", providing the interaction history as context. Each step, the generated action is executed against the environment and a <thought, action, observation> tuple is added to the history, until the agent submits an answer, or exceeds token or compute limitations.\nOne challenge associated with running experiments is that output observations can get extremely"}, {"title": "4.2 Results", "content": "Expert Set. We show results for the experts set in Table 4, with the results for the most performant LLM averaged across three seeds (decoding temperature is 0.2). The low accuracies (12.2-16.3) suggest that current agents cannot yet perform this task well. However, in some cases, agents make some progress towards the goal, as evident by the landmarks metric, suggesting that agents could still be helpful in setting up repositories.\nMasked Set. We show results in Table 6 on the Masked set, demonstrating that SWE-agent correctly solves a significant portion (46.1%) of the challenges that are required to set-up and execute experiments from research repositories, but that most sub-problems are still unsolved. The higher landmarks evaluation score (74.9%) suggests that agents often make progress towards solving the sub-problems, even if some of the steps might not necessarily be correct.\nWe find that SWE-agent performs better than ReAct-SUPER with GPT-4o as the LLM, but slightly worse with all weaker models, suggesting that weaker models were less effectively in leveraging SWE-Agent tools. The open-source Mixtral and Llama reach significantly lower scores on both the Masked and Expert sets.\nAuto Set. We show in Table 5 results for the Auto tasks, where ranking of models and agents are"}, {"title": "4.3 Error Analysis", "content": "The Masked set categorizes each problem, allowing us to break down performance of agents (Table 3). We find that the hardest categories for the agent are data (27%), configuration (38%) and goal (43% accuracy), whereas CPU, issues and dependencies are easier (73%, 61% and 54% respectively). These findings suggest that agents are better at solving sub-problems where there is a specific error message to be solved (such as CPU support errors, incompatible dependencies, or exceptions) than more open-ended problems such as configuring data loading for a custom dataset.\nSpecifically, for the latter case, we find that agents commonly skip going through the repository to understand relevant code. For example, they often hallucinate arguments of scripts or functions instead of looking up how they should be called (e.g., adding n_examples=10 when no such"}, {"title": "5 Conclusion", "content": "Our work introduces SUPER, a benchmark designed to evaluate LLM-based agents on executing tasks from code repositories, focusing specifically on low-profile research repositories encountered in the wild. We show empirically that our benchmark is difficult, even for the current best commercial LLMs such as GPT4, both on landmark and end-to-end task evaluations (e.g., GPT-4o solving only 46.1% of the sub-problems). Our benchmark also"}, {"title": "Limitations", "content": "Dataset Size. The dataset size of our benchmark, comprising 45 and 152 sub-problems, is smaller compared to some other benchmarks available for agent evaluation, which could potentially affect the statistical significance of performance evaluations. However, the use of smaller, high-quality benchmarks is not uncommon. For instance, benchmarks such as HUMANEVAL , CLASSEVAL , and BAMBOOGLE contain 164, 100, and 125 examples respectively, and are widely used for assessing model performance. In addition, recent work has suggested that reducing large datasets to as few as 100 examples does not diminish their effectiveness . Moreover, smaller-sized datasets offer the advantage of being less expensive to operate, thus providing better accessibility for researchers with limited resources, particularly when running interactive agents in environments that generate long outputs. Finally, our provided Auto set with 604 problems offers problems purposed for development, which alleviates the risk of overfitting to the evaluation sets.\nProgramming Languages and Domains. We have only collected solutions written in Python, and our environment only supports that programming language. We focus mostly on text-based repositories. While the challenges associated with running these repositories likely overlap with other domains, increasing the diversity of the repository domains could be beneficial.\nEvaluation Based on External Resources. Running benchmarks in realistic environments often depend on external resources. In our case, agents rely on availability of resources such as GitHub, pip and datasets, which we cannot control across runs. While completely sand-boxed setups could have allowed for a more controlled evaluation, we opt for fidelity, similarly to e.g. benchmarks for web agents that rely on access to real websites ( inter alia)."}, {"title": "Ethical Considerations", "content": "While autonomous research execution agents could significantly enhance research advancements, there is a risk of over-reliance on these agents, which could lead to conclusions drawn based on incorrect implementations of agents, and careless actors not checking the agent's reproduction work carefully."}]}