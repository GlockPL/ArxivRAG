{"title": "SALT: INTRODUCING A FRAMEWORK FOR HIERARCHICAL SEGMENTATIONS IN MEDICAL IMAGING USING SOFTMAX FOR ARBITRARY LABEL TREES", "authors": ["Sven Koitka", "Giulia Baldini", "Cynthia S. Schmidt", "Olivia B. Pollok", "Obioma Pelka", "Judith Kohnke", "Katarzyna Borys", "Christoph M. Friedrich", "Benedikt M. Schaarschmidt", "Michael Forsting", "Lale Umutlu", "Johannes Haubold", "Felix Nensa", "Ren\u00e9 Hosch"], "abstract": "Background:\nTraditional segmentation networks approach anatomical structures as standalone elements, overlook-ing the intrinsic hierarchical connections among them. This study introduces Softmax for Arbitrary Label Trees (SALT), a novel approach designed to leverage the hierarchical relationships between labels, improving the efficiency and interpretability of the segmentations.\nMaterials and Methods:\nThis study introduces a novel segmentation technique for CT imaging, which leverages conditional probabilities to map the hierarchical structure of anatomical landmarks, such as the spine's division into lumbar, thoracic, and cervical regions and further into individual vertebrae. The model was developed using the SAROS dataset from The Cancer Imaging Archive (TCIA), comprising 900 body region segmentations from 883 patients. The dataset was further enhanced by generating additional segmentations with the TotalSegmentator, for a total of 113 labels. The model was trained on 600 scans, while validation and testing were conducted on 150 CT scans. Performance was assessed using the Dice score across various datasets, including SAROS, CT-ORG, FLARE22, LCTSC, LUNA16, and WORD. Additionally, 95% confidence intervals (CI) were computed using 1000 rounds of bootstrapping.\nResults:\nAmong the evaluated datasets, SALT achieved its best results on the LUNA16 and SAROS datasets, with Dice scores of 0.93 (95% CI: 0.919, 0.938) and 0.929 (95% CI: 0.924, 0.933) respectively. Additionally, the model demonstrated reliable accuracy across other datasets, scoring 0.891 (95% CI: 0.869, 0.906) on CT-ORG and 0.849 (95% CI: 0.844, 0.854) on FLARE22. Moreover, the LCTSC dataset showed a score of 0.908 (95% CI: 0.902, 0.914) and the WORD dataset also showed good performance with a score of 0.844 (95% CI: 0.839, 0.85). Furthermore, SALT is capable of segmenting a whole-body CT with 1000 slices in an average of 35 seconds.\nConclusion:\nSALT used the hierarchical structures inherent in the human body to achieve whole-body segmentations with an average of 35 seconds per CT scan. This rapid processing underscores its potential for integration into clinical workflows, facilitating the automatic and efficient computation of full-body segmentations with each CT scan, thus enhancing diagnostic processes and patient care.", "sections": [{"title": "1 Introduction", "content": "Computed Tomography (CT) imaging stands out as one of the most comprehensive [13, 20, 36, 41] tools in the field of diagnostic imaging, with the number of CT scans increasing by 4% per year worldwide [48]. With the rising volume of CT scans, radiologists also face a growing workload, highlighting the importance of adopting automated solutions for support. Moreover, CT scans contain a substantial volume of information, and only a portion is used for specific diagnosis purposes. A substantial amount of potentially clinically valuable information remains unexploited and is the focus of current research [6, 18, 53]. In this context, deep learning networks have the capability of automating tasks and extracting information from scans with little additional cost besides algorithm training. In particular, automated segmentation of CT scans is now a widespread technique for identifying key anatomical features such as organs, tissues, and vessels [26, 43, 53]. These detailed segmentations aid radiologists in making accurate diagnoses [3] and have been linked to indicators of a patient's well-being [1, 19]. This capability enables automated quantification of segmentations, with metrics such as organ volumetries gaining recognition for their role in predicting overall survival outcomes [17, 22]. Furthermore, CT scans enable the automated calculation of Body Composition Analysis (BCA), which quantifies the amount of fat, muscle, and bone [14, 27, 38, 39] and is proving to be valuable in monitoring disease progression [19, 23, 33] and predicting patient survival outcomes [15, 21].\nThe currently existing models for full-body segmentation, such as TotalSegmentator [16, 53], often rely on multiple models for segmenting CT scans when the number of labels becomes too extensive for a single model to handle efficiently. Additionally, completing a full segmentation can take several minutes, which is impractical for scenarios where a segmentation algorithm is expected to constantly operate in the background as part of a hospital's data acquisition process.\nIn response to this problem, we introduce the Softmax for Arbitrary Label Trees (SALT) framework, an approach that employs a single, robust model to efficiently manage a broad spectrum of labels. The SALT framework harnesses hierarchical relationships to segment a vast range of anatomical landmarks, reflecting the natural tree-like organization of the human anatomy. By employing conditional probabilities, this framework models the intricate relationships between these landmarks, capturing the complex network of connections among various anatomical structures. In this study, an application of the SALT framework to 3-dimensional CT scans using a nnUNet [16] architecture is presented. However, the flexibility of this framework allows for its adaptation to other contexts, accommodating both 2D and 3D imaging across diverse image types, and could be used with a wide range of deep learning algorithms. This adaptability underscores the SALT framework's potential as a universal tool for medical image analysis. This optimization aims to resolve existing bottlenecks and substantially improve the utility of CT scan data in real-time clinical settings, facilitating the seamless operation of the segmentation algorithm within the hospital's data acquisition process."}, {"title": "2 Materials and Methods", "content": ""}, {"title": "2.1 Datasets", "content": "This study used a selection of datasets available on The Cancer Imaging Archive (TCIA) [9] to train and evaluate the SALT approach. For the training, 750 CT scans from the Sparsely Annotated Region and Organ Segmentation (SAROS) [9, 25] dataset were used (600 for training and 150 for validation). In this dataset, the segmentations target anatomical landmarks that are relevant for body composition analysis (BCA) [14, 27]. The annotations cover a wide range of areas such as the abdominal and thoracic cavities, bones, brain, mediastinum, muscles, pericardium, spinal cord, and subcutaneous tissue. In addition to these annotations, segmentations of organs, vessels, and specific muscles and bones were generated using Version 1 of the TotalSegmentator models [16, 52, 53] for the same dataset of 750 scans. The TotalSegmentator predictions were then fused with the SAROS annotations to create a single dataset of 750 scans containing all labels. For SAROS, smaller labels such as the thyroid, submandibular, and parotid glands were not included in the final segmentation. The SAROS segmentations encompass larger and more general areas, so the TotalSegmentator predictions were superimposed on the SAROS labels, as a subclassification of larger areas. This fusion is illustrated in Figure 1, which highlights the natural tree-like organization of the human body. For example, the body encompasses the thoracic cavity, which itself includes organs like the lungs and heart. These organs, in turn, can be subdivided further into more specific segments, such as the lobes of the lungs and the atria and ventricles of the heart.\nFor the evaluation, an additional independent test set of 150 CT scans from SAROS and other publicly available datasets were used: CT Volumes with Multiple Organ Segmentations (CT-ORG) [5, 9, 44] Fast and Low-resource Semi-supervised Abdominal Organ Segmentation (FLARE22) [31, 32], Lung CT Segmentation Challenge (LCTSC) [9, 54, 55], Lung Nodule Analysis 2016 (LUNA16) [2, 50], and Whole Abdominal Organ Dataset (WORD) [30]. However, the \"gallbladder\u201d label from the WORD dataset was omitted because the masks were either very few points and not an"}, {"title": "2.1.1 Dataset Postprocessing", "content": "The labels from all datasets underwent postprocessing to ensure the hierarchical structure by either merging or splitting the original segmentations. An example of merging is the lung label, which was not present in the TotalSegmentator labels but could be inferred by merging the upper, lower, middle left, and right lobes. In some cases, splitting was necessary to ensure the tree structure, as each label can only have one parent. For instance, the aorta passes through the mediastinum, pericardium, and abdominal cavity, which would imply three parents. To preserve the tree structure, the aorta label was split into three separate regions: \u201caorta thoracica pass pericardium\u201d, \u201caorta thoracica pass mediastinum\u201d, and \"aorta abdominalis\u201d. Similar splitting was performed for the inferior vena cava and pulmonary artery. Furthermore,"}, {"title": "2.2 Modeling Conditional Probabilities for Trees", "content": "As visible from Figure 2, the hierarchical labels can be represented as an unweighted tree. An arbitrary tree can be defined by its adjacency matrix A of size N \u00d7 N, where N is the number of nodes and each value \\(a_{i,j}\\) of the matrix represents an edge between parent node i and child node j. Since the tree is unweighted, A has only binary elements [8], and \\(a_{i,j} = 1\\) represents a connection between parent node i and child node j.\nAnother useful structure is the reachability matrix R, which encodes in row i all the nodes j that can be reached from node i, and in column j all the traversed nodes between the root and node j. This matrix is also binary and it can be derived from the adjacency matrix using matrix multiplications and additions:\n\\(R = \\sum_{v=0}^{H} A^{v} = A^{0} + A^{1} + ... + A^{H}\\),\nwhere H is the height of the arbitrary tree. Each power \\(A^{v}\\) represents the nodes that can be reached from any node with a path of size v, so the matrix \\(A^{H+1}\\) will be a zero matrix, as no two nodes have a distance that is larger than H. Another component is the sibling matrix S, which is also binary and encodes all local neighbors on the same level:\n\\(S = A^{T} X A\\)\nAn example for all three matrices, A, R, and S, based on the employed dataset and the associated label structure is shown in Figure 3.\nUsing these matrices, the assignment of a voxel of the body to a specific class can be represented through conditional probabilities based on the tree structure. For instance, the probability of a voxel to belong to the right lower lobe of the lung depends on the probabilities of it belonging to the right lung, the lungs as a whole, the thoracic cavity, and the body. This concept is analogous to the one of Bayesian networks [46], which are acyclic directed graphs where each node contains probability information, and edges represent a direct influence between the nodes. In a Bayesian network, all relationships are expressed using conditional probabilities and can be simplified using the chain rule [46]. This can also be applied to our hierarchical tree, but a normalization step needs to be added to ensure that each node represents a probability. This can be done using a softmax function [7], which is commonly employed as an activation function in neural networks for multi-class classification tasks. Its outputs are probabilities that indicate the likelihood of the input being assigned to each class, and its sum is 1, just like for probabilities.\nThese concepts can be used to build a deep learning model that uses conditional probabilities and the softmax function as activation layer for segmentation. The model takes an input and outputs a feature map x with N channels, which is the number of nodes of the hierarchical tree. To enforce the hierarchical relationships, a final probability function for each class c can serve as the activation layer.\nThe probability \\(P(y = c|x)\\) that the final class y corresponds to class c can be computed using the chain rule as the product of the probabilities from the root node to node 40. The probability of each node is normalized using the softmax"}, {"title": "2.3 Model Preprocessing and Training", "content": "The SALT architecture in this paper consists of a DynUNet from the MONAI framework [10] (version 1.1, PyTorch [42] version 1.14), which is a reimplementation of the architecture utilized by nnUNet [16]. The output feature map of the DynUNet model has as many channels as there are nodes in the hierarchical tree. Due to SALT's flexibility, the DynUNet could be substituted by any other model. The previously described probability functions were used to create an activation layer, which was used to generate the final probabilities, as illustrated in Figure 4.\nInitially, all CT scans underwent several pre-processing steps: Left-Posterior-Inferior voxel reorientation, resampling to a voxel spacing of 1.5x1.5x1.5 mm, normalization of intensity values within the Hounsfield Units to scale the range from -1024 to 1024 to between 0 and 1, and random crops of size 192x192x48. The model was trained for 1000 epochs with 600 CT scans for training and 150 for validation. The AdamW optimizer [28] was used with an initial learning rate of 0.00025 and a weight decay of 0.00005. The learning rate was reduced during training using a cosine function [29].\nThe model used a hybrid loss based on categorical cross-entropy loss [35] and Dice loss [34], which are commonly used for medical segmentation tasks [16]. In contrast to the common use of these losses, SALT optimizes multiple classes at the same time by constructing an encoding of the nodes using the reachability matrix. Let R be the reachability matrix of size (N, N), where N is the number of nodes in the tree, and let y be the ground truth label with |V| elements (each corresponding to a voxel's label). We construct a new label y' of size (|V|, N) by indexing the reachability matrix R using the ground truth y. This operation maps each voxel label to its corresponding encoding column in R, or in other words, y' corresponds to the traversed nodes from the root to the voxel's label. Considering the prediction \u0177 of size (|V|, N), the cross-entropy and the Dice loss can be computed using y' and \u0177. Unlike the conventional softmax approach, this method optimizes each node along with all its ancestors, which is a result of the label encoding and the implementation of chained conditional probabilities within the activation layers.\nFor the calculation of the evaluation metrics during training, a similar approach that uses the tree structure to encode each node was used. Technical details about this method are presented in Appendix C. Furthermore, the trained model and the code are available for review on GitHub under the following link: https://github.com/UMEssen/SALT."}, {"title": "2.4 Evaluation", "content": "The model's performance was assessed with the Dice score [11] and with the normalized surface Dice score (NSD) [37]. The NSD measures the frequency at which the surface distance between volumes is under 3 mm, a metric previously employed by TotalSegmentator [53]. For a better comparison with existing models, the same metrics were also utilized to evaluate the predictions from Version 2 of the TotalSegmentator [16, 53] on the datasets from Table 1. Additionally, 95% confidence intervals (reported in square brackets) were computed by bootstrapping the scores of the evaluated CT scans. The bootstrapping was performed with 1000 iterations and the 2.5 and the 97.5 percentiles were utilized as upper and lower bound of the confidence intervals. To evaluate the variations in scores across datasets, the distributions of the Dice and NSD scores were compared for datasets that share the same organ labels.\nFor a better comparison, the evaluation datasets were also used to generate predictions from Version 2 of the TotalSegmentator [16, 53] and the same evaluation scores were computed. Moreover, the speed of the model was"}, {"title": "3 Results", "content": ""}, {"title": "3.1 Segmentation Evaluation", "content": "The trained model showed a Dice of 0.891[0.887,0.896] and an NSD of 0.931 [0.927, 0.936]. An overall score for the different datasets is presented in Table 2 together with the scores obtained by Version 2 of the TotalSegmentator. The Dice scores and the NSD scores for the datasets are reported in Table 3, Table 4, Table 5, Table 6, Table 7 and Table 8 of Appendix B. Notably, the lungs, the liver, the spleen, and the stomach achieved the best scores across the different datasets.\nAdditionally, an evaluation of the speed of the model at inference time was also performed, which can be reviewed in Table 9. It is relevant to make a distinction between the inference time (the time the model takes to make a prediction) and the total time, as a large portion of the inference time is just spent postprocessing and storing the result. In Figure 5, a comparative analysis of SALT's speed against both Version 1 and Version 2 of the TotalSegmentator is presented. This comparison clearly demonstrates that SALT consistently outperforms its counterparts, showing a notable speed advantage, especially with larger CT scans. Additionally, a speed comparison was conducted between SALT and the faster variants of TotalSegmentator that utilize lower spacing, with the findings detailed in Figure 9 of Appendix A. These \"fast\u201d alternatives of TotalSegmentator, which use a single model instead of five, also exhibit better speed. In these comparisons, the 3mm model from Version 2 of TotalSegmentator was consistently slower than or comparable to SALT. However, for larger images, the 3mm model of Version 1 and the 6mm model of Version 2 were faster.\nFurthermore, the inner workings of the model can be better understood by examining the conditional probabilities used at various stages. As an example, the process of predicting the vertebra L4 is illustrated in Figure 6. This visualization effectively demonstrates how the model relies on these conditional probabilities to formulate its predictions."}, {"title": "3.2 Failure Analysis", "content": "In many cases, the results for the same classes varied across the datasets. To investigate these discrepancies, statistical tests were performed on datasets sharing the same labels to discover whether there were significant differences in the results. The results are reported in Table 10 of Appendix \u0412.\nAn example of this discrepancy is the brain class, which however did not present a significant difference in Table 10 due to the few CT scans with this label. For the SAROS dataset, the brain class achieved a Dice score of 0.758, while for the CT-ORG dataset, it only obtained a Dice score of 0.486. This is because two CT scans of the CT-ORG dataset were classified as containing a brain segmentation by the model, but only one was annotated as such in CT-ORG. The existing segmentation achieved a Dice score of 0.973. The other case should also have been segmented, as the image, despite not showing the entire brain, still includes a visible brainstem (Figure 7B). The TotalSegmentator also produced a similar segmentation as our model (Figure 7C). In SAROS, in nine cases where the brain was not part of the CT scan, the model incorrectly predicted the brain class in other areas of the body.\nIn the LUNA16 dataset, the lung segmentations consistently achieved good results. However, for the middle lobe, there were some significant differences in the segmentations. In fact, sometimes the middle lobe was wrongly segmented, and in Figure 7E and Figure 7F it is possible to see that for these examples, the TotalSegmentator produced similar segmentations as our model. This is also further proven by the 0.848 Dice score of the TotalSegmentator for this class (Table 6 of Appendix B).\nFor the pericardium class, the annotations from the LCTSC datasets followed another definition and resulted in an overall Dice score of 0.894 compared to a Dice score of 0.952 for the SAROS dataset. According to the LCTSC guidelines, the heart is contoured around the pericardial sack. This annotation is thus not compatible with the ones produced by our model, which segments the entire area within the pericardial sack (Figure 7H).\nFor the bone class, where the model achieved a Dice score of 0.911 for the SAROS dataset, the Dice score for the CT-ORG dataset was 0.872. Visually, it can be seen that the model tends to include more contours than the ones from this dataset (Figure 71, Figure 7J, and Figure 7K). In particular, our segmentations also include the cortical bone, while the TotalSegmentator and the CT-ORG segmentations tend to leave it out. Moreover, in the SAROS dataset, cartilage was annotated as bone in the rib cage, and since the rib cage is considered part of the bones, this causes a discrepancy in the segmentations. In Version 1 of the TotalSegmentator, the segmentation of the cartilage was not present, but it was added in Version 2 of the tool, as can be seen in Figure 7K.\nThe adrenal glands class consistently achieved the lowest Dice scores between 0.65 and 0.706. Visually, our model tends to segment a larger area that also contains the organ's wall (Figure 7L, Figure 7M, and Figure 7N). This is also proven by the better NSD scores (between 0.887 and 0.957). For the TotalSegmentator, this class also produced similar results in the WORD dataset but had a better Dice score for the FLARE22 dataset (Table 4 of Appendix B).\nFor the colon segmentation, the model occasionally encountered challenges in differentiating between the colon and the small bowel. This issue is exemplified in Figure 70 and Figure 7P, where the patient underwent a right hemicolectomy, resulting in the removal of the ascending colon and a portion of the transverse colon. Due to the presence of significant air within the small bowel, the model erroneously identified it as part of the colon. However, as can be seen in Figure 7Q, the TotalSegmentator also made this mistake, even resulting in a lower Dice score for this class compared to SALT (0.773 vs. 0.808)."}, {"title": "4 Discussion", "content": "In this work, we propose the SALT framework to provide a single activation layer for hierarchical probability modeling. While we have illustrated its application in medical imaging with a nnUNet model, the activation layer could be adapted to various hierarchical contexts and could be used with any model. The power of this framework lies in its ability to train a single segmentation model to recognize over 100 labels hierarchically. Throughout the training process, the loss function is computed over the full tree, allowing for the optimization of each node at every stage of training. This hierarchical training ensures that the model respects the natural structure of the data, for instance, ensuring that the colon is identified within the abdominal cavity. Additionally, the proposed model takes an average of 35 ms per slice, meaning that a full whole-body CT scan with 1000 slices can be computed in 35 seconds, which is faster than existing models [53]."}, {"title": "4.1 Performance Comparison", "content": "In general, SALT obtained consistently good results for the kidneys (Dice 0.92, except for the CT-ORG dataset), the liver (Dice above 0.95), the spleen (Dice above 0.92), the stomach (Dice above 0.9), and the lungs (Dice above 0.91,"}, {"title": "4.2 Clinical Applicability", "content": "The extraction of biomarkers from medical segmentations is an active field of research that has a variety of clinical applications, such as making accurate diagnoses [3], monitoring a patient's well-being [1, 19], or predicting overall survival [17, 22]. This also includes BCA biomarkers, which are also relevant for disease progression [19, 23, 33] and predicting patient survival outcomes [15, 21]. Moreover, segmentations can also be used to predict contrast phases [4]."}, {"title": "4.3 Limitations & Future Work", "content": "A limitation of this study is the tendency of the model to include the walls of organs and vessels in the annotations, which differs from the annotations of the other datasets. While this approach may be advantageous for certain classes, such as bones, where it includes the cortical bone area, it may be disadvantageous for others, such as adrenal glands, where it may inadvertently include abdominal fat. This may be due to differences in annotation between the large-scale segmentations of the SALT dataset (which typically include the walls) and the TotalSegmentator predictions. Moreover, SALT also performed worse in the aorta and inferior vena cava classes (0.89 and 0.859 respectively) compared to TotalSegmentator (0.936 and 0.912 respectively). This discrepancy might be attributed to the necessity of dividing these structures into multiple regions to ensure the tree structure, which may have confused the model. This limitation arises from the required tree-like class hierarchy of the model, as not all classes are strictly assignable to a single parent node. A graph-like topology, represented by a directed acyclic graph, would greatly extend the representation capabilities, as a node could potentially have multiple parent nodes, which could be implemented as a union of all parent probabilities. However, the overall softmax properties would no longer hold, as the probabilities would not sum up to one. Another interesting extension would be to define the hierarchical tree using existing ontologies, such as the Foundational Model of Anatomy [45] or SNOMED [47], to provide standardization.\nAn additional extension could be used for multi-label segmentation, where each voxel gets more than one class assigned. This is especially important for overlapping concepts, such as that subcutaneous tissue or muscle can be present in all parts of the body (arms, head, legs, and torso). Moreover, this work was also intended to work with multiple datasets, such that, without any modifications, a model could be trained using different sources without needing to fuse different datasets on one single segmentation.\nFurthermore, future work should aim to refine the segmentation capabilities of the model to ensure that organ walls are only included in the annotations when relevant, which raises the larger problem of unifying datasets annotated with different annotation guidelines. The introduction of advanced post-processing methods could also improve the quality of the resulting segmentations, leading to more accurate and clinically useful outcomes."}, {"title": "5 Conclusion", "content": "In conclusion, the presented SALT framework offers a new approach to medical imaging by utilizing the hierarchical nature of the human anatomy to achieve comprehensive and efficient segmentations across 113 body regions. The model's capability to process a whole-body CT scan in an average of 35 seconds paves the way for integration into clinical workflows, enabling the immediate computation of crucial biomarkers upon performing a CT scan. While this work primarily targeted the segmentation of body regions in CT scans, SALT defines an activation function that can be used for any application or domain with a hierarchical structure. Future enhancements will focus on refining the segmentation accuracy, exploring graph-like topologies for more complex anatomical structures, and extending the model's application to multi-label segmentation and multi-dataset integration."}, {"title": "C Implementation Details", "content": "To evaluate the model's performance during training, a different Dice score formulation was devised. This approach uses the tree structure to create an encoding for each node, thereby eliminating the need to merge masks for evaluation purposes. The standard definition of Dice score can be computed using true positives (TP), false positives (FP), and false negatives (FN):\n\\(Dice = \\frac{2 \\cdot TP}{TP + FP + FN}\\)\nLet yc and \u0177c be binary vectors representing the ground truth and the prediction for a specific class c. Then TP, FP, and FN can be computed for the same class c using simple logical operations such as a logical and (^) and logical not (\u00ac)"}]}