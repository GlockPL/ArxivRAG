{"title": "Complex LLM Planning via Automated Heuristics Discovery", "authors": ["Hongyi Ling", "Shubham Parashar", "Sambhav Khurana", "Blake Olson", "Anwesha Basu", "Gaurangi Sinha", "Zhengzhong Tu", "James Caverlee", "Shuiwang Ji"], "abstract": "We consider enhancing large language models (LLMs) for complex planning\ntasks. While existing methods allow LLMs to explore intermediate steps to make\nplans, they either depend on unreliable self-verification or external verifiers to\nevaluate these steps, which demand significant data and computations. Here, we\npropose automated heuristics discovery (AutoHD), a novel approach that enables\nLLMs to explicitly generate heuristic functions to guide inference-time search,\nallowing accurate evaluation of intermediate states. These heuristic functions are\nfurther refined through a heuristic evolution process, improving their robustness\nand effectiveness. Our proposed method requires no additional model training\nor fine-tuning, and the explicit definition of heuristic functions generated by the\nLLMs provides interpretability and insights into the reasoning process. Extensive\nexperiments across diverse benchmarks demonstrate significant gains over multiple\nbaselines, including nearly twice the accuracy on some datasets, establishing\nour approach as a reliable and interpretable solution for complex planning tasks.\nOur code is released as part of the Sys2Bench library (https://github.com/\ndivelab/sys2bench/).", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are increasingly being applied to tasks that require structured\nreasoning and decision-making, extending beyond traditional NLP applications such as translation\nand summarization. These models have demonstrated potential in complex reasoning tasks, including\narithmetic problem-solving [Cobbe et al., 2021], logical reasoning [Saparov and He, 2023], vision-\nbased reasoning [Parashar et al., 2024], and multi-step problem-solving [Yang et al., 2018]. Building\non these strengths, researchers have begun using LLMs for complex planning tasks [Parashar et al.,\n2025], which require sequential decision-making, exploration of intermediate steps, and strategic\nthinking. To further enhance LLMs' capabilities in such tasks, test time inference techniques [Wei\net al., 2022, Wang et al., 2022, Hao et al., 2023] like Tree-of-Thought [Yao et al., 2024] enable\nLLMs to search over the intermediate steps, improving their ability to arrive at correct solutions.\nFrom solving puzzles to generating action plans and strategies, the use of LLMs for planning tasks\ncontinues to expand, showcasing their potential as reliable tools for tackling challenges that go\nbeyond traditional NLP applications.\nWhile test-time inference techniques have achieved notable success, there remain opportunities\nto improve them. Early approaches, such as Chain-of-Thought [Wei et al., 2022], rely on a linear\nreasoning process, which often fails to explore diverse solution paths or recover from errors introduced\nduring intermediate steps. This limitation may result in suboptimal performance, particularly in"}, {"title": "Background and Related Work", "content": "We define a planning task specifically designed for solving with an LLM as {S, 80, G, A\u0473, To}. Here\nS represents the state space, containing all possible states. The initial state is denoted as s0 \u2208 S, and\nthe goal states are represented as G = {90,91,\u2026\u2026, gm}, where each g\u00ec \u2208 S is a possible goal state.\nNote, in some cases, there may be only one goal state, i.e., G = {go}. The action space Ae(s) is\ngenerated by a pre-trained LLM with parameters @ based on the current state s, providing a set of\nvalid actions for state s. The transition function To : S \u00d7 A \u2192 S, also parameterized by the LLM,\npredicts the outcome of applying an action a \u2208 Ag(s) to a state s. A solution to the planning task is\nrepresented as II = (80, a1, 81, A2, \u2026\u2026, an, Sn) where si+1 = To(Si, ai+1) and\nSn E G.\nLLMs for Planning. LLMs have demonstrated potential for reasoning and common-sense capabili-\nties, driving their growing adoption in the planning tasks across various domains. Specifically, LLMs\nhave been widely used as policy models for decision-making and planning in diverse interactive envi-\nronments, including robotics [Driess et al., 2023, Huang et al., 2022, Kannan et al., 2024, Singh et al.,\n2023], multimodal games [Fan et al., 2022, Wang et al., 2023], and text-based environments [Liu\net al., 2023, Yao et al., 2022, Shinn et al., 2024]. To further enhance their performance on planning\nproblems, researchers have developed test-time inference techniques that adaptively guide the model's\nreasoning and decision-making processes during inference time. For example, Chain of Thought\n(CoT) prompting [Wei et al., 2022] guides LLMs to explicitly generate intermediate reasoning steps,\nyielding state-action traces for planning tasks. Building on this, Tree of Thoughts (ToT) [Yao et al.,\n2024] and Graph of Thoughts (GoT) [Besta et al., 2024] explore multiple paths to improve decision-\nmaking and robustness. Similarly, XoT [Ding et al., 2023] trains an auxiliary policy model to assign\nrewards to solutions. Techniques like RAP [Hao et al., 2023] utilize Monte Carlo Tree Search to\nnavigate the expansive action space, while FoR [Yu et al., 2024] introduces a fine-tuning approach to\ndiscover diverse and creative solutions across multiple planning tasks. Despite their effectiveness,\nthese methods often rely on self-evaluation, which can be prone to reliability issues [Huang et al.,\n2023, Stechly et al., 2024], or require additional models and fine-tuning, which entail significant\ncomputational overhead.\nLLMs for Automated Heuristic Discovery. Automatic heuristic design refers to the process of\ngenerating, optimizing, or adapting heuristic functions automatically to solve problems. Traditional\nheuristic functions are typically handcrafted based on domain expertise, but automatic methods use"}, {"title": "Automated Heuristics Discovery", "content": "Existing methods rely on LLMs to either self-verify or use additional models to evaluate the quality of\neach intermediate state si [Yao et al., 2024, Ding et al., 2023]. However, prior research [Huang et al.,\n2023, Stechly et al., 2024] has shown that self-verification is often unreliable. Meanwhile, additional\nmodel training requires significant costs in terms of data and computational resources. To overcome\nthese challenges, we propose AutoHD, a novel approach that enables LLMs to explicitly generate\nheuristic functions to guide inference-time search. Using the exceptional code generation capabilities\nof LLMS, AutoHD generates heuristic functions as Python code, enabling more accurate evaluation\nof intermediate states. To further improve the performance and robustness of these heuristic functions,\nwe introduce a heuristic evolution process that iteratively refines them. Our framework reduces the\ninherent randomness of self-verification and enhances interpretability by providing explicit reasoning\nfor why LLMs prioritize certain states. See Figure 1 for a comparison between our proposed method\nand existing approaches."}, {"title": "Heuristic Function Proposal", "content": "A diverse set of initial heuristic functions is generated by prompting a pre-trained LLM, offering a\nflexible and automated solution that avoids reliance on hand-crafted components or additional models.\nThe LLM generates both a natural language description and the corresponding Python code for each\nheuristic. The natural language description provides an intuitive, high-level overview of the heuristic"}, {"title": "Heuristic Guided Inference-time Search", "content": "Given an LLM-proposed heuristic function H, it efficiently guides the search process. This heuristic\nfunction determines the order in which states are explored, ensuring that the search algorithm\nprioritizes promising paths while pruning unimportant ones. In this way, the heuristic plays a crucial\nrole in balancing between exploring new states and exploiting known promising paths. Specifically,\nat a state s, the LLM generates the corresponding action space A\u0189(s), which contains all feasible\nactions from that state. For a given action a \u2208 A\u0189(s), the LLM predicts the resulting next state s' as\ns' = To(s, a). The proposed heuristic function H then computes a heuristic value v for the predicted\nstate s', providing an estimation of desirability. Formally, the heuristic values for all possible next\nstates are computed as\nvi = H(To(s, ai)), for ai \u2208 Ao(s).\nThese values, collectively represented as V = {V1, V2, \u00b7 \u00b7 \u00b7 }, guide the search algorithm by prioritizing\nthe exploration of states with lower heuristic values. The heuristic values can be integrated into\nvarious search algorithms, such as A*, beam search, or greedy search, to guide the exploration\nprocess. These values provide an estimation of the potential value of states, enabling the algorithms\nto prioritize or prune states dynamically and adapt to the specific requirements of the search strategy.\nIn our work, we explore two search algorithms below.\nGreedy Breadth-First Search. Greedy breadth-first search (Greedy BFS) is a heuristic-driven search\nalgorithm that combines the exploration of breadth-first search with greedy prioritization. At each\nstep, it evaluates all states in the frontier, which is the set of all states that have been generated but not\nyet explored or expanded, and selects the one with the smallest heuristic value. In this way, Greedy\nBFS effectively chooses the most promising states at each step. Let Q represent the frontier set at a\ngiven step. For each state s' \u2208 Q, the algorithm evaluates the heuristic value H(s'). It then selects\nthe next state to expand as s = argmin\u00a7'\u2208QH(s'), where H(s') is the heuristic value of state s'. The\nalgorithm terminates when a goal state is reached or the search budget is exhausted. See Algorithm 2\nfor details.\nA* Search. A* search is a widely used algorithm that balances exploration and exploitation using a\ncomposite cost function. It evaluates states based on the cumulative cost to reach the state G(s), and\nthe estimated cost to reach the goal H(s). In our case, each step has a uniform cost, meaning the\ncumulative cost G(s) represents the number of steps from the start state so to the state s. Formally,\nfor each step, A* maintains a priority queue of frontier nodes, which is the set of all states that have\nbeen generated but not yet expanded, sorted by the summed cost F(s) = G(s) + H(s). At each step,\nthe algorithm selects the node s with the smallest F(s) value as s = argmin$1\u2208QF(s'). The search\ncontinues until a goal state is expanded or the search budget is reached. See Algorithm 3 for details."}, {"title": "Heuristic Evolutions", "content": "To further enhance the performance and robustness of the proposed heuristic functions, we follow Liu\net al. [2024a], Romera-Paredes et al. [2024] to include a heuristic evolution mechanism. In this\napproach, LLMs are prompted to generate initial heuristic functions, and subsequent generations of\nheuristic functions are derived iteratively. At each generation, new heuristic functions are evaluated,\nand only the top-performing ones are retained for the next round. Specifically, LLMs are prompted to\ngenerate an initial pool of b heuristic functions, denoted as H1, H2, \u2026\u2026\u2026, H. The detailed prompts\nfor each problem are provided in Appendix A.1. A small validation set consisting of approximately 10\nproblem instances, similar in size to the in-context learning examples, is used to evaluate the quality\nof each heuristic function. In generation i, given the heuristic functions H\u2081\u22121, H\u22121,\u2026\u2026\u2026, H-1\nfrom the previous generation, we use two strategies for generating new heuristic functions, including\nexploration and modification. The exploration strategy focuses on generating new heuristic functions,\nencouraging the exploration of diverse ideas. On the other hand, the modification strategy introduces\nminor variations to existing high-performing heuristic functions, such as changing parameters, to\nrefine and improve them. This combination of strategies ensures both comprehensive exploration and\nefficient refinement of the heuristic function space. Details of the evolution prompts are provided in\nAppendix A.2. The newly generated heuristic functions Hi, H, \u2026\u2026\u2026, H are then evaluated on the\nvalidation set through the heuristic-guided search process. The top-performing heuristic functions are\nselected to form the pool for the next generation. After several rounds of evolution, the best heuristic\nfunction across all generations is selected for testing. This process is performed only once, and no\nadditional heuristic function generations are required during the inference-time search. The heuristic\nevolution process is summarized in Algorithm 1. See Figure 3 for an overview of the heuristic\ndiscovery process."}, {"title": "Discussions", "content": "Unlike existing methods that rely on LLM self-verification [Yao et al., 2024, Hao et al., 2023] or\nguidance from external models [Kambhampati et al., 2024], our approach uses LLMs to propose\nheuristic functions explicitly, which can guide the search process during inference. Additionally, while\nToT requires invoking LLMs to evaluate at every intermediate step during inference, our framework\neliminates this overhead. Once the best heuristic function is identified on the validation set, no further\nheuristic function generation is necessary during inference. See Figure 1 for a comparison between\nour proposed method and existing approaches.\nPrevious works point out that an LLM can itself be treated as a heuristic. For instance, ToT [Yao\net al., 2024] considers its own framework as a heuristic search algorithm, where the LLM serves as\nthe heuristic by evaluating and ranking nodes during the search. However, prior methods [Stechly\net al., 2024, Huang et al., 2023] demonstrate that LLMs are inherently unable to self-verify, making\nthem unreliable for providing robust heuristic values. Additionally, using the LLM itself to evaluate\nstates lacks interpretability. This is because the LLM acts as a \u201cblack-box\" evaluator, providing no\ninsight into why certain states are preferred over others. As a result, the reasoning process becomes\nobscured, making it difficult to understand the rationale behind the LLM's decisions. In contrast,\nAutoHD enhances transparency by asking the LLM to generate these heuristic functions explicitly.\nThis approach provides insight into the decision-making process, enabling a clearer understanding of\nwhy specific states are prioritized.\nAdditionally, some studies have explored process reward models (PRMs) [Luo et al., 2024, Li et al.,\n2022, Lightman et al., 2023], which enhance LLMs by evaluating and optimizing intermediate steps of\nreasoning or decision-making processes. It is worth noting that in planning tasks, heuristic functions\nand PRMs serve a similar purpose, as both can guide the search. However, training PRMs often\ndemands a large and diverse dataset of labeled intermediate states and their corresponding rewards,\nwhich can be challenging and costly to acquire, particularly for tasks with sparse data availability or\nwhere domain expertise is required to label the data [Lightman et al., 2023]. In contrast, AutoHD\nshows that LLMs can generate heuristic functions on the fly, taking advantage of their pre-trained\nknowledge without requiring additional training or manual data collection. This efficiency makes\nLLM-proposed heuristic functions a desirable alternative to traditional PRMs in many applications.\""}, {"title": "Experiments", "content": "In this section, we evaluate AutoHD on three real-world planning tasks, including Blocksworld,\nGame of 24, and Rubik's Cube. The experimental results demonstrate that AutoHD significantly\noutperforms various baseline approaches. Additionally, we present an extensive ablation study in\nSection 4.4 to analyze the contributions of individual components. More experimental results can be\nfound in Appendix B."}, {"title": "Blocksworld", "content": "Blocksworld [Valmeekam et al., 2022] is a classic planning benchmark. It involves a set of blocks,\neach with unique identifiers, which can be moved or stacked according to specific rules. The goal\nis to transform an initial configuration of blocks into a specified target configuration using a series\nof predefined actions. Each action manipulates block positions while adhering to constraints such\nas only moving one block at a time and ensuring stability of stacks. States represent the current\narrangement of blocks, while transitions occur as actions are applied, updating the state to reflect\nchanges in block positions. LLMs need to demonstrate spatial reasoning capabilities to understand\nthe physical interactions and constraints inherent in the task.\nWe compare the proposed method with the following baseline methods, including (1) IO, which uses\na standard input-output prompt; (2) CoT [Wei et al., 2022], a method that solves problems through\na linear sequence of intermediate steps; (3) CoT-SC [Wang et al., 2022], an extension of CoT that\nselects answers via majority voting across multiple paths; (4) ToT [Yao et al., 2024], which use\ntree search to explore and expand intermediate steps; (5) RAP [Hao et al., 2023], which integrates\nMonte Carlo Tree Search (MCTS) with the LLM as a world model, rewarding intermediate steps\nand guiding tree growth toward the correct answer; (6) LLM-Modulo [Kambhampati et al., 2024],\nwhich uses external critics, verifiers, and human input to ensure correctness of generated plans; and\n(7) AoT [Sel et al., 2023], which provides sequences of intermediate steps as in-context examples.\nNote that, we use 5 iterations for self-consistency. For O1 mini, we evaluate the model on a subset of\nBlockworld by randomly sampling 20 instances per step. We evaluate all methods using accuracy as\nthe metric, which measures whether the action trace generated by LLMs successfully solves the task,\ni.e., rearranges the blocks to achieve the specified goal configuration.\nThe results presented in Table 2 demonstrate that AutoHD consistently outperforms all baseline\napproaches across various LLMs, including GPT 40-mini, GPT 40, and Llama 3.1 70B, achieving\naccuracies of 42.4%, 71.5%, and 59.1%, respectively. Notably, AutoHD achieves approximately\ntwice the accuracy of the second-best baseline on GPT 4o and GPT 40-mini. Additionally, while\nsome baselines exhibit significant performance variance across different LLMs, AutoHD maintains\nrobust and consistent results, highlighting the generalization ability of our approach. It is also worth\nnoting that while Ol demonstrates near-perfect accuracy as an oracle model, AutoHD outperforms\nO1 mini, a state-of-the-art large reasoning model (LRM) [Valmeekam et al., 2024], further validating\nits effectiveness."}, {"title": "Game of 24", "content": "The Game of 24 is a classic mathematical puzzle. It involves a set of four integers, typically drawn\nfrom a standard deck of playing cards, which can be combined using basic arithmetic operations,\nincluding addition, subtraction, multiplication, and division. The goal is to manipulate these numbers\nthrough a sequence of valid operations to produce the target value of 24. Each state in the game\nrepresents the current set of intermediate results, while transitions occur as operations are applied,\nreducing the number of operands and updating the state. Constraints include the correct application\nof operations and adherence to mathematical rules, such as division by non-zero numbers. The Game\nof 24 is an NP-Complete problem and requires LLMs to use strong arithmetic reasoning to solve.\nWe compare AutoHD with the following baseline methods, including IO, CoT [Wei et al., 2022],\nCoT-SC [Wang et al., 2022], ToT [Yao et al., 2024], and AoT [Sel et al., 2023]. Note that, we use 5"}, {"title": "Rubik's Cube", "content": "Rubik's cube [Ding et al., 2023] is a well-known puzzle-solving benchmark requiring multi-step\nspatial planning. It involves a cube with six faces, each subdivided into four smaller squares of distinct\ncolors. The goal is to transform an initial scrambled configuration into a target state where each\nface of the cube is uniformly colored. This is achieved through a sequence of predefined rotational\nactions applied to the cube's faces. Each action corresponds to rotating one of the cube's layers along\na specified axis, which alters the arrangement of colored squares adhering to the cube's structural\nconstraints. States represent the current color arrangement of the cube, while transitions occur as\nrotations are executed, updating the state to reflect the new configuration. Note that Rubik's cube is\nan NP-complete problem and the maximum number of steps required to optimally solve the cube is\nfour in this dataset.\nWe compare our AutoHD with the following baselines, including IO, CoT [Wei et al., 2022], CoT-\nSC [Wang et al., 2022], ToT [Yao et al., 2024], and XoT [Ding et al., 2023]. XoT uses reinforcement\nlearning and MCTS to incorporate external domain knowledge by training an extra policy model\non 1,000 training samples. We randomly select 15 examples from the training dataset in Ding\net al. [2023] to form the validation set used in the heuristic evolution process. We follow previous\nmethods Ding et al. [2023], Yu et al. [2024] to use a fixed state transition function instead of using\nLLM to update the states. We evaluate all methods using accuracy as the metric, which measures\nwhether the action trace generated by LLMs successfully solves the task, i.e., transforms the cube\ninto the goal state where each face is uniformly colored."}, {"title": "Ablation Studies", "content": "In this subsection, we conduct extensive ablation studies to analyze the contributions of different\ncomponents.\nComparing Search Algorithms. We begin by evaluating the performance of different search methods\nwithin our proposed framework. As discussed in Section 3.2, in this work we study two search\nalgorithms, including A* and greedy BFS. Experiments are conducted using GPT-40 mini on three\ntasks, namely Blocksworld, the Game of 24, and the Rubik's Cube. For Blocksworld, the dataset\nis divided into subsets based on the minimum number of actions required to solve each test case.\nSpecifically, we evaluate different search methods on a subset corresponding to 2-step problems. The\nresults, presented in Table 5, demonstrate that both A* and greedy BFS perform effectively within\nour framework, achieving comparable outcomes across different tasks. These findings imply that\nLLM-generated heuristics are effective in guiding the search process during inference, highlighting\nthe potential for future work to explore additional search algorithms.\nDifferent LLMs for action generation and state transition. We investigate the impact of using\ndifferent LLMs for action generation and state transition on the performance of the proposed AutoHD.\nSpecifically, we conduct experiments where heuristic functions are generated by a smaller model,\nGPT 40-mini, while action generation and state transition are handled by a larger model, GPT-40. In\nother words, the action space Ae and transition function To are from GPT-40. The results in Table 6\nshow that AutoHD achieves significant improvements when using more powerful LLM for action\ngeneration and state transition. Moreover, our findings suggest that even smaller models, such as\nGPT-40 Mini, can generate effective heuristic functions. This observation implies that the primary\nlimitation of LLMs in planning tasks may lie in their ability to generate actions and predict state\ntransitions rather than in their capacity to produce heuristics.\nHeuristic Evolutions. We also conduct experiments to analyze the evolution of heuristic functions.\nFor each generation, we select the heuristic functions with the highest validation accuracy and\nevaluate their performance on the test dataset using GPT 40-mini. The experiments are conducted\non the Rubik's Cube dataset. The results in Figure 4 show that both validation and test accuracies\ninitially improve significantly during the early generations. Furthermore, performance eventually\nplateaus, indicating a saturation point in the evolution process. These suggest that the evolutionary"}, {"title": "Conclusions", "content": "In this paper, we propose AutoHD, a novel framework that enables LLMs to explicitly generate\nheuristic functions for guiding inference-time search. Moreover, the heuristic evolution process\nfurther refines these functions, enhancing their robustness and effectiveness. The proposed AutoHD\nrequires no additional model training or fine-tuning, making it adaptable to various tasks. The explicit\nheuristic functions generated by LLMs provide valuable insights into the reasoning process, making\nAutoHD a transparent solution for complex decision-making. Extensive experimentation across\ndiverse benchmarks has validated the efficacy of our approach, showing substantial performance\nimprovements over multiple baselines. These results firmly establish AutoHD as a reliable and\ninterpretable solution for addressing complex planning tasks."}, {"title": "Prompts", "content": "In this section, we provide the detailed prompts used in our experiments."}, {"title": "Prompts for Heuristic Function Proposal", "content": "In this subsection, we show the prompts we used to get the initial heuristics For Blocksworld, Game\nof 24, and Rubik's Cube."}, {"title": "Prompts for Heuristic Evolution", "content": ""}, {"title": "More Experimental Results", "content": "In this section, we present additional experimental results. For the Blocksworld dataset, we divide\nthe data into subsets based on the minimum number of actions required to solve each test case. The\nresults are summarized in Table 8."}, {"title": "Choice of Heuristic Functions", "content": "In this subsection, we analyze the impact of heuristic function selection on performance. Specifically,\nwe compare two strategies, including selecting the best heuristic functions across all generations and\nselecting the best heuristic functions from the final generation. Experiments are conducted using GPT\n40-mini. The results, presented in Table 9, indicate that LLMs explore various heuristic functions\nduring the evolution process. Selecting the best heuristic function across all generations leads to more\nrobust and stable performance compared to relying solely on the final generation."}, {"title": "Datasets", "content": "Rubik's Cube [Ding et al., 2023] is a well-known puzzle-solving benchmark requiring multi-step\nspatial planning. The cube consists of six faces, each divided into four smaller squares, with each\nsquare assigned one of six distinct colors. The objective is to manipulate the cube from an initial\nscrambled state to a solved state, where each face is uniformly colored. This is accomplished through\na sequence of predefined rotational moves applied to individual layers of the cube. Figure 5 provides\na visual representation of the Rubik's Cube dataset used in this work. The left subfigure shows a\nscrambled cube configuration, representing an initial state in the dataset. The arrangement of colored\nsquares encodes the current state of the puzzle, which requires a sequence of moves to reach the\nsolved state. The right subfigure shows a goal state, where each face of the cube is uniformly colored.\nThe dataset consists of cube states that are at most four moves away from the solved configuration."}, {"title": "Search Algorithm", "content": "In this section, we present the details of the search algorithms, specifically Greedy BFS and A*, as\noutlined in Algorithm 2 and Algorithm 3, respectively."}, {"title": "Heuristic Functions", "content": "In this section, we show some examples of heuristic functions generated by the LLMs. Table 10\nshows some three representative examples generated by GPT 40-mini. In the Blocksworld, the\nheuristic function estimates the effort required to transform an initial block configuration into a target\nconfiguration. It operates by first identifying the number of misplaced blocks that are not in their\ncorrect positions in the goal state. Additionally, it accounts for the cumulative positional difference,\nwhich measures how far each misplaced block is from its correct position. The final heuristic value is\ncomputed as the sum of these two terms. For the Game of 24, the heuristic function evaluates the\nproximity of a given set of numbers to the target value of 24. Given an input list of numbers, the"}]}