{"title": "EMOTION: Expressive Motion Sequence Generation for Humanoid Robots with In-Context Learning", "authors": ["Peide Huang", "Yuhan Hu", "Nataliya Nechyporenko", "Daehwa Kim", "Walter Talbott", "Jian Zhang"], "abstract": "This paper introduces a framework, called EMOTION, for generating expressive motion sequences in humanoid robots, enhancing their ability to engage in human-like non-verbal communication. Non-verbal cues such as facial expressions, gestures, and body movements play a crucial role in effective interpersonal interactions. Despite the advancements in robotic behaviors, existing methods often fall short in mimicking the diversity and subtlety of human non-verbal communication. To address this gap, our approach leverages the in-context learning capability of large language models (LLMs) to dynamically generate socially appropriate gesture motion sequences for human-robot interaction. We use this framework to generate 10 different expressive gestures and conduct online user studies comparing the naturalness and understandability of the motions generated by EMOTION and its human-feedback version, EMOTION++, against those by human operators. The results demonstrate that our approach either matches or surpasses human performance in generating understandable and natural robot motions under certain scenarios. We also provide design implications for future research to consider a set of variables when generating expressive robotic gestures.", "sections": [{"title": "I. INTRODUCTION", "content": "People use non-verbal communication to interact with others all the time. Facial expressions, gestures, and body movements are key components of effective interpersonal communication [1], [2]. These unspoken elements can convey emotions and intentions more profoundly than verbal means alone. As such, integrating non-verbal communication capabilities into robots can potentially lead to more natural and pleasant human-robot interactions.\nPrevious research has highlighted the importance of expressive behaviors in robots and investigated different methods for creating these behaviors across a range of applications and settings. Studies have shown that robots capable of displaying human-like emotions and reactions can improve user satisfaction and engagement [3], [4]. Techniques ranging from pre-defined sequences to dynamic, sensor-driven behaviors have been implemented [5], yet they often lack the adaptability required for real-world interactions that mimic human diversity and subtlety.\nAs humanoid is the frontier embodiment of general-purpose robots, it is critical to enable humanoid robots to generate expressive gesture motions. These motions need to be contextually appropriate and vary dynamically with the social environment and interaction goals. Given the unlimited"}, {"title": "II. RELATED WORK", "content": "Expressive Human-Robot Interactions. Human communication encompasses a broad range of non-verbal cues. Translating these cues into robot design can facilitate more natural, engaging, and accessible interactions between robots and humans [1], [2]. Previous research has shown that robots can effectively utilize non-verbal cues, such as body language [4], [9], proxemics [10], facial expressions, and eye gaze [11], to enhance human-robot interactions. These expressive behaviors can improve communication, foster empathy, increase user interest, and enhance the perceived intelligence and friendliness of robots [3], [4], [12].\nTraditionally, robot designers and animators have collaborated to manually craft expressive and dynamic robot behaviors [6], [7]. Prior works use heuristic approaches to map human motions from video footage, demonstrations, or animated characters, translating these behaviors to the robot's embodiment [5]. However, this process demands tremendous efforts and requires a high level of expertise in different embodiments.\nRecently, generative models have been utilized to create or modify expressive robot behaviors from smaller sets of demonstrated samples [13], [14]. These models can significantly reduce the effort required from human demonstrators and have the potential to produce a more diverse range of expressive behaviors, leading to greater efficiency and adaptability.\nRecent advancements in LLMs have significantly improved the generation of natural and expressive behaviors in human-robot interactions [15], [16]. Traditional rule-based and template-based methods, which struggle to produce complex, multimodal behaviors and require extensive human intervention or specialized datasets. In contrast, LLMs allow for more adaptive and diverse behavior generation with minimal input [17], as demonstrated by recent work in conversational interactions with robots during social [18], [19] and collaborative tasks [20], [21]. Mahadevan et al. [22] present GenEM, a framework for generating control code with LLMs, utilizing a pre-defined robot skill library, such as \u201cmove head\" and \"change light\". GenEM is implemented on a mobile robot with a head-neck embodiment-primarily expressing through head movements, light changes, and directional motion, while EMOTION operates on a humanoid platform capable of more sophisticated expressions, including human-like arm movements and individual finger articulations. This humanoid embodiment allows for richer and more nuanced behaviors, combining hand gestures and body language to enhance expressiveness. Furthermore, unlike GenEM, which focuses on arranging sequences of high-level pre-defined skills, EMOTION leverages LLMs to directly produce complex hand and finger trajectories with minimal examples. Additionally, qualitative results from our user study offer insights into the impact of movement patterns, hand, and finger positioning on the human perception in terms of naturalness and understandability.\nLLMs for robotic sequence optimization. Recent studies have demonstrated that pretrained LLMs possess the capability to autoregressively generate complex numerical sequences applicable to robotics. Mirchandani et al. [23] show these models' capabilities in tasks ranging from the extrapolation of simple motion sequences to the enhancement of reward-conditioned trajectories in elementary control tasks such as CartPole. Similarly, Wang et al. [21] illustrates that LLMs could generate low-level control commands enabling quadrupedal robots to walk by improving joint trajectories with historical data. Extending these capabilities to vision-based tasks, Palo et al. [24] employs Vision Transformers to extract 3D keypoint tokens from images, and utilize expert demonstrations as contexts for few-shot imitation learning with LLMs.\nDistinct from prior works targeting at generating manipulation or locomotion policies, EMOTION focus on the domain of human-robot interaction. In addition, EMOTION is designed to generate novel sequences unlike approaches aimed at few-shot adaptation from the single-task expert demonstrations [21], [24]. Furthermore, we explore the potential use of natural language feedback from humans to directly refine these sequences, thereby broadening the practical applications of LLMs in robotics."}, {"title": "III. EXPRESSIVE MOTION SEQUENCE GENERATION", "content": "Overview. As illustrated in Figure 1, EMOTION takes user language instruction robot image observation $o_{in} \\in O$ and/or $l_{in} \\in L$ as input and outputs a continuous-valued motion sequence for the humanoid robot, in the form of $\\tau = {s_1,..., s_T}$. We use T = 10 in this work. EMOTION also incorporates human feedback. At iteration $i \\in {1,2,..., I_{max}}$, human feedback $f^{(i)}$ can be provided to improve the current motion sequence $\\tau^{(i)}$ and produce a new motion sequence $\\tau^{(i+1)}$. The motion sequence is executed on the humanoid robot with inverse kinematics and trajectory interpolation and tracking.\nMotion Sequence Representation. The state at each timestep $s_t$ in T is represented by 22 real values. We use six quantities to parameterize the motion sequence: left/right-hand Cartesian position (3D), orientation represented by Euler angles (3D), and opening/closing of each finger (5D), as shown in Figure 2.\nSocial Context Analysis. The inputs, including robot image observation $o_{in}$ and/or user language instruction $l_{in}$, provide important social context to which the robot needs to respond. For example, $o_{in}$ could be an image of a person solving a math problem on a whiteboard. The prompt is then to ask the LLM agent to describe what it sees and how to respond with only gestures. Input could also be as simple as a language instruction, such as \u201cExpress confusion with only gestures\". The inputs are fed to the first VLM agent $f_1(o_{in}, l_{in})$ with chain-of-thought (CoT) prompting to obtain the social context analysis $l_a$, and the corresponding gesture $(g)$. Formally, $[l_a, (g)] = f_1(o_{in}, l_{in})$. For example, $l_a$ could be \"I see a person excitingly solving a complicated math problem on a whiteboard. I would communicate approval and encouragement for the person's achievement\u201d, and $(g)$ could thereby be \"<thumbs-up)\".\nHuman Demonstrations. We maintain a set of human demonstrations $D = {((g_{demo}), l_{demo}, \\tau_{demo})_d}_{d=1}^{|D|}$, where \u3008g\u3009 is the gesture name, $l_{demo} \\in L$ is the one-line description of the gesture, and $\\tau_{demo}$ is the motion sequence of the demonstrated gesture. Specifically, we find that with just two human demonstrations \"idle\" and \"right-hand wave\u201d (D = 2), it provides enough information for the downstream generation tasks. We use Apple Vision Pro to collect human demonstrations and transform them into the same representation as the one that LLMs are required to generate.\nLLMs as a Motion Sequence Generator using In-Context Learning. We denote the second LLM agent as $f_2$. $f_2$ takes in the gesture (g) together with instructions about the coordinate definition, motion sequence representation, as well as CoT prompts to facilitate the generation. Conditioned on the human demonstrations D, it then outputs the initial motion sequence $\\tau^{(1)}$ in the aforementioned representation. Formally, $\\tau^{(1)} = f_2((g); D)$.\nIterative Improvements from Human Feedback. EMOTION incorporates human feedback (HF) in a natural language format. We denote the HF version as EMOTION++. The third LLM agent takes in the history of the generated motion sequences $\\tau$ and human feedback $l_f$ and outputs the new version of the motion sequences. Formally, $\\tau^{(i+1)} = f_3 (\\tau^{(1)}, l_f^{(1)}, \\tau^{(2)}, l_f^{(2)},..., \\tau^{(i)}, l_f^{(i)})$, where $i \\in {1,2,..., I_{max}}$. For example, this $l_f$ could either command an explicit modification in the current motion, such as \u201cput your hands higher\u201d, or suggest a high-level improvement, such as \u201cadd some random motion\".\nThe motion sequences for EMOTION and EMOTION++ are generated by sampling OpenAI's GPT-40 (gpt-40-2024-05-13) APIs for text completion [25]. We include all the prompts in the supplementary material."}, {"title": "IV. USER STUDIES", "content": "As describe in Section III, EMOTION has two main steps: social context analysis and motion sequence generation. Since the social context analysis capability has been explored in the existing work [22] and to eliminate unnecessary randomness during the LLM inference time, we focus on investigating the motion sequence generation capability of LLMs and fix the set of gestures to be investigated as detailed in Section IV-B. To understand the effectiveness of the proposed method, we aim to study the following research questions:\n1) RQ1: How well are the generative behaviors (EMOTION) perceived when compared to the human oracle behaviors?\n2) RQ1: Whether and to what extent does adding the human feedback for EMOTION++ improve the perception of the gestures, compared to EMOTION?\nTherefore, our hypotheses are:\n1) H1: the perception of the EMOTION behaviors would not differ significantly from the oracle human behaviors.\n2) H2: the EMOTION++ behaviors would be perceived as more natural and understandable compared to the EMOTION.\nTo generate behaviors for evaluation and comparison between models, we used the GR-1 humanoid robotic hardware from Fourier [26]. Expressive gestures were either generated by models or recorded by a human operator, then verified and selected by a human researcher within the robot's simulation platform. For EMOTION++, a human researcher provided feedback during this stage until the simulated behaviors aligned with the expected outcomes with a maximum feedback iteration $I_{max}$ = 5. Finally, the gestures were deployed on the humanoid robot for recording, during which we minimized the difference in lighting and background.\nFollowing the established categorization of non-verbal communication [27], we investigate four types of expressive gestures: (1) emblems, non-verbal gestures that can generally be translated directly into words. (2) illustrators, motions that complement verbal communication by describing, accenting, or reinforcing the speech. (3) affect displays, motions that"}, {"title": "V. RESULTS", "content": "We perform quantitative analysis on users' self-reported ratings of naturalness and understandability for each generated behavior. Figure 4 plots the ratings of perceived naturalness (top) and understandability (bottom) for individual gestures, and for three different conditions: Human oracle (blue), EMOTION generated behaviors (light green), and EMOTION++ generated behaviors (dark green).\nTo test H1, we run the one-way ANOVA test to compare each and combined gestures between perceived human oracle behaviors and EMOTION generated behaviors. Combining all the gestures, EMOTION behaviors does not statistically differentiate from human oracle behaviors (Naturalness: p = 0.267, Understandability: p = 0.528). Thus, H1 is supported.\nTo test H2, we use one-way ANOVA analysis to compare the perceived naturalness and understandability between EMOTION and EMOTION++. Combining the gestures altogether, EMOTION++ is rated as significantly more natural and understandable than EMOTION (naturalness: p = 0.0014, understandability: p = 0.019). Looking at the individual gesture, the significance of the differences between EMOTION++ and EMOTION exist in the gestures \"fist pump\", \"jazz hand\", and \u201cokay\u201d (only naturalness). Thus, H2 is supported.\nTo further understand the quality of the generated motions integrating human feedback, we compare EMOTION++ to human oracle with one-way ANOVA tests. Combining all the gestures, EMOTION++ behaviors received significantly higher ratings for understandability than human oracle behaviors (p = 0.003). Perceived naturalness of EMOTION++ behaviors are also rated higher without statistical significance revealed. For individual gestures, EMOTION++ generated behaviors for \"air quotes\u201d, \u201ccome closer\", \"thumbs-up\" are perceived significantly more natural than human oracle ones, among which \"air quotes\" is also perceived as significantly more understandable. On the other hand, human oracle gesture \u201cv-sign\" received significantly higher ratings in both naturalness and understandability than EMOTION++ generated gestures. The results above suggested that the generated motions can not only match the performance of human oracle expressions, but may surpass them in many gestures, especially after incorporating human feedback.\nTo understand the effect of participants' backgrounds on their perceptions of robotic behaviors, we conducted a linear regression to examine the correlations between perception metrics (understandability, naturalness) and background variables, including age, gender, general empathy towards others, intent to make friends with robots, frequency of using hand gestures, unease with robotic emotions, and nervousness about interacting with robots in general, as shown in Figure 5. We found that participants' self-rated empathy levels were significantly positively correlated with the perceived understandability and naturalness of the robot's generated expressions. The higher the empathy level, the more natural and understandable the gestures were perceived by the participants (Naturalness: Oracle: p = 0.0006, EMOTION: p = 0.0007, EMOTION++: p = 0.023; Understandability: Oracle: p = 0.00027, EMOTION: p = 0.011, EMOTION++: p = 0.022). This finding aligns with psychological literature, which suggests that high empathy levels are associated with enhanced narrative comprehension [28]. On the other hand, participants' age was negatively correlated with the perceived naturalness of the behaviors (Naturalness: Oracle: p = 0.033, EMOTION: p = 0.039, EMOTION++: p = 0.109; Understandability: Oracle: p = 0.237, EMOTION: p = 0.193, EMOTION++: p = 0.344). We did not find any statistically significant correlations for the other variables.\nTo further reveal the insights of the perceived differences between the generative models and human oracle for individual gestures, we perform qualitative analysis of participants' reasoning comparing the three behaviors for each gesture. To identify the cluster of ideas that affect participants' perception, we use thematic analysis [29] to code participants' quotations, and identify clusters of codes to formulate themes and summarize key findings under each theme. We identify three main themes and twelve sub-themes, including robot variables, human perception, and contextual factors.\""}, {"title": "1) Robot variables:", "content": "We identified 114 quotations discussing how robotic variables affected participants' perceptions. The quotations cluster under five sub-themes:\na) Position of hands: For \u201cv-sign\u201d gesture, seven quotations expressed that a human oracle has a better hand position, where the robot raises the hand up and with the elbow angle resembling a human's. For other gestures, there are similar comments on that the higher hand position is more perceivable and natural. For \u201clistening\u201d gestures, 10 quotations mentioned hand position affecting the interpretation, with participants preferring the hands to be up close to the ears. Several said EMOTION++ has the best hand position in this term. Incorrect hand positions could confuse the gesture interpretation. For example, for the \u201clistening\u201d gesture, participants commented that the positions of the hands are not close enough to the ears, and could be interpreted as gestures like \u201csurrender\" or \"stop\".\nBesides position, the orientation of the hands also affect naturalness and engagement. For \u201ccome closer\" gesture, P1 mentioned that the human oracle gesture \"is more toward me, it feels more engaging\". For \u201cspread-hands\u201d gesture, participants prefer the hands to face upward. For gestures that engage two hands, the configuration of the two hands could affect how it is perceived, including the distance, whether they are symmetrical, and the synchronization of the motions. For example, P17 commented that the robot could \u201cwiden the hands more to look more natural\u201d for \u201cspread-hands", "pattern": 24, "taking the shortest and most direct path\" which is similar to human, among which three participants mentioned they preferred EMOTION++ in the": "kay", "fist pump\\\", \u201cjazz-hands": "and \u201cthumbs-up\u201d, the participants preferred some subtle movements. P3 mentioned liking the subtle hand shaking at the end for the EMOTION++ gesture. For \"thumbs up\", P13 and P18 liked the subtle motions added by the human oracle at the end of the trajectory that", "emphasis": "o the expression. The trajectory path and the movement pattern affect perceived naturalness. For example, for the", "stopping": "esture, both P10 and P14 commented that the Oracle has a clear push forward motion at the end of the gesture, which"}, {"stopping": "P18 mentioned that the movement of the human oracle for the", "pump": "ppears more affirmative than others.\nSeveral participants mentioned that the jerkiness of the motion affected the naturalness, especially for the", "thumbs-up": "esture. Most participants mentioned that human oracle gestures are more jerky. The timing of the movement and the coordination of the motions also affected how realistic the gesture is perceived. For the \u201cokay\u201d and \u201cv-sign\u201d gestures, participants commented that", "position.": "P11) P17 mentioned for the \u201cstopping", "shoulder": 13, "come closer": "esture. While seven of the quotes liked the EMOTION++ making arm motions along with the fingers, which makes it more \u201chuman-like\u201d, and \u201chelps to convey to come closer\u201d; three quotes found the arm motion was", "unnecessary": "and \u201cdistracting", "stop": "esture, participants discussed that the angle of arm extension affects the interpretation, since the arm was not fully extended outward, thus difficult to interpret.\nd) Finger pose: 22 quotations mentioned the problems with finger pose especially with Human Oracle, where some fingers fail to bend or make contact with each other. For \"okay\" gesture, participants commented that it is essential to make two fingers contacting each other and forming the", "O": "hape in the hand for delivering the meaning, while the robot had hardware limitations thus struggling to do so. For gesture \"v-shape\", participants wished the fingers to be more apart to resemble the", "v": "hape. Failing to do so may confuse the ges-"}, {"title": "VI. ADDITIONAL EXPERIMENTS", "content": "Computation Time. The computation time of EMOTION mainly depends on two factors: model size and number of generated tokens. Since we are using OpenAI APIs to generate the response, it is also related to internet connection speed and available server resources at the moment. Therefore, the measured time could have significant variation. As shown in Table I, EMOTION achieves an average computation time of 26.8s for the initial motion sequence generation and 21.2s for a single-round sequence generation after human feedback under our lab conditions. The computation time is still considered too long for real-time conversation. This issue could be mitigated by distilling a smaller LLM specialized in motion sequence generation and running inference locally instead of through APIs, which is an active research topic but not the main focus of this paper.\nHuman Feedback. One of the findings of this work is that it is possible to use natural language feedback to directly refine the motion sequence. For EMOTION++, the human provide 1.9 iteration of feedback averaged over 10 gestures. 21% of the feedbacks are high-level commands about the subtle motion, such as \"add some nuanced motion\" or \"make the hands movement a little bit more exaggerated\". The rest are about the position of the hands, such as \"make both hands lower\u201d.\nFailure Mode. The main failure mode is the feasibility to solve for collision-free trajectories with inverse kinematics. Since LLMs are not explicitly aware of the workspace, the motion sequence generated can occasionally be infeasible to track. In this case, we either regenerate the sequence or provide feedback to steer the trajectory. We experimented with joint-space representation but found that it is difficult for LLMs to understand the demonstrations and generate in the joint space."}, {"title": "VII. DISCUSSION AND CONCLUSION", "content": "In this paper, we present EMOTION and EMOTION++, an LLM-based generative framework (incorporating human feedback) for generating expressive gestures for a humanoid robot, particularly focusing on hand and arm trajectories. Through an online user study, the behaviors generated with human feedback (EMOTION++) outperform the human-oracle behaviors in perceived naturalness and understandability and well as generated behaviors without human feedback (EMOTION). Variation remains across different gestures, and further analysis provides insights into differences in preferences for various gestures and robotic parameters.\nQualitative results indicate that factors such as a humanoid robot's hand positioning, movement patterns, arm and shoulder trajectories, finger poses, and movement speed all contribute significantly to the understandability and naturalness of expressive gestures. Future researches could benefit from a focus on these variables when creating demonstrations and designing prompts for behavior generation, considering the following design implications derived from the findings:\nThe robot's hand positioning, orientation, and the physical configuration between the hands can convey gesture meaning. Generative models might improve behavior by explicitly articulating hand positions and two-hand spatial relationship that reflect human poses."}, {"title": "*", "content": "While users generally prefer direct, efficient movements, adding subtle motion can enhance naturalness and engagement, though preferences vary. Generative models must balance subtle motions with the primary gesture trajectory to optimize interaction, potentially adapting to user feedback for more personalized expressions.\n Finger poses play a crucial role, especially in emblematic gestures where specific shapes convey meaning, such as an \u201cokay\u201d or \u201cV-sign\u201d. Future models should pay close attention to finger positioning for expressive accuracy while also considering the robot's hardware constraints.\nThe timing and coordination of movements impact the human-likeness of gestures. Future models should not only focus on trajectory paths for hands and arms but also on temporal coordination to better mimic humans.\nThe quantitative analysis revealed a correlation between participants' age, self-reported empathy levels and their ratings of understandability and naturalness. Future research on generative expression may consider participants' background variables when evaluating the perception of generated behaviors.\nWe acknowledge certain limitations in our experiment, which we plan to address in future studies. First, hardware constraints may restrict the robot's expressive capabilities. For instance, the robot's fingers lack subtle movements, which may limit the replication of certain human gestures. This limitation may have influenced participants' perceptions, and results could vary with alternative hardware. Second, the computation time is still considered too long for real-time usage. This issue could be mitigated by distilling a smaller LLM specialized in motion sequence generation and running inference locally instead of through APIs. Third, human intervention during the prompting, feedback provision, and validation stages may introduce potential biases. To mitigate this, we provide all prompts and human feedback used in this work for transparency. Future work might assess model variance and reduce biases introduced by human preferences."}]}