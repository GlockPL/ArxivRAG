{"title": "Disentangling Visual Transformers: Patch-level Interpretability for Image Classification", "authors": ["Guillaume Jeanneret", "Lo\u00efc Simon", "Fr\u00e9d\u00e9ric Jurie"], "abstract": "Visual transformers have achieved remarkable performance in image classification tasks, but this performance gain has come at the cost of interpretability. One of the main obstacles to the interpretation of transformers is the self-attention mechanism, which mixes visual information across the whole image in a complex way. In this paper, we propose Hindered Transformer (HiT), a novel interpretable by design architecture inspired by visual transformers. Our proposed architecture rethinks the design of transformers to better disentangle patch influences at the classification stage. Ultimately, HiT can be interpreted as a linear combination of patch-level information. We show that the advantages of our approach in terms of explicability come with a reasonable trade-off in performance, making it an attractive alternative for applications where interpretability is paramount.", "sections": [{"title": "1. Introduction", "content": "Deep learning architectures have achieved remarkable breakthroughs in domains such text [19], vision [20, 46], or multimodal tasks [37], prompting widespread interest in their application to real-world problems [34]. However, as these models are increasingly used in high-stakes scenarios, understanding their decision-making process becomes crucial to ensure their decisions are grounded in meaningful variables rather than spurious correlations [4, 28]. This necessity has driven the development of trustworthy and interpretable architectures.\nInterpretable-by-Design (ID) architectures [14] aim to inherently explain their decision-making process, eliminating the need for external interpretability tools. Ideally, these architectures maintain comparable performance to traditional black-box methods while providing insights into their internal workings. Despite their promise, existing ID architectures largely rely on convolutional neural networks\n(CNNs) as feature extractors [7, 32, 45, 65]. Given the recent success of transformer-based models [18], transitioning to interpretable transformers is a logical next step.\nAlthough Vision Transformers (ViTs) [18] have demonstrated superior performance in computer vision tasks, the literature on their explainability and interpretability remains sparse. While attention maps are sometimes considered interpretable, many studies [26, 54] argue that they provide little to no insight into the actual decision-making process. We concur that attention maps offer only partial and insufficient cues, and instead route towards ID architectures.\nIn this paper, we address this gap by introducing a novel interpretable transformer-like architecture. Our proposed model, the Hindered Transformer (HiT), advances the understanding of ViTs by analyzing the flow of individual image patches and decomposing the classification token (CLS) into contributions from each individual token. By constructing predictions as the sum of these token contributions, HiT achieves interpretability without relying on external methods [3] or gradient-based approaches [57], classifying it as an ID method.\nWe summarize our contributions as follows: i) We propose the Hindered Transformer (HiT) backbone, a variant of vision transformers that is inherently interpretable. ii) We empirically validate HiT on six datasets \u2013 ImageNet [16], CUB 2011 [64], Stanford Dogs [30], Stanford Cars [33], FGVC-Aircraft [40], and Oxford-IIIT Pets [47] - demonstrating its interpretability both quantitatively and qualitatively, outperforming recent ID transformer-based models and post-hoc methods.\nTo encourage future research, we will release our code and pretrained weights upon publication."}, {"title": "2. Related Work", "content": "The performance of neural networks on computer vision tasks is well-established, but the complexity of these models can make them difficult to understand. This lack of transparency is problematic and has motivated the scientific community to develop methods for making neural net-works more interpretable. There are two main approaches to this problem: post-hoc methods, which seek to analyze an already-trained model, and interpretable by design architectures, which aim to create models whose decision-making processes are inherently transparent.\nMany post-hoc methods have been proposed in the literature, including counterfactual explanations [4, 29, 71], saliency maps [27, 49, 53, 67], and model distillation [22, 58]. Closer to our work, a few attempts have been made to explain a ViT architecture via post-hoc algorithms. For instance, Abnar and Zuidema [1] proposed to compute a score per token by recursively propagating the attention maps in a top-bottom approach. In addition, some methods extended the LRP [6] paradigm to include the attention heads [12, 13]. The previous methods leverage the mechanisms of transformers to estimate the individual contribution of each token. While these approaches provide insights into trained models, they often rely on external tools or post-processing, which can limit their generalizability and integration into the model itself.\nThe field of interpretable by design architectures is diverse, as there is no single approach to explaining the complex behaviors neural networks. While many methods have been proposed, there has been a recent focus on prototypical part networks, such as ProtoPNets introduced by Chen et al. [14]. ProtoPNets computes class predictions based on the distances between patches of the final feature map and some prototypes, which can be visualized. Additionally, there have been many variations of ProtoPNets proposed in the literature [8, 11, 15, 17, 23, 42\u201344, 52, 61, 65, 66]. These variations aim to refine ProtoPNets to enhance performance and interpretability but primarily focus on convolutional neural networks (CNNs) [24, 56], limiting their application to modern transformer architectures.\nIn addition to ProtoPNets, there are other methods that use alternative forms of prototypes. For example, PDiscoNet [62] automatically detects parts of objects and uses them for the final classification. Similarly, BagNet [9] mimics the Bag-of-Features approach to understand the decision-making process. Concept Bottleneck Models [32, 45] use concepts to explain their decisions. Finally, a family of networks propose interpretable layers, such as B-cos networks [7], that learns an easily interpretable input-dependent linear transformation. The work of Zhang et al. [72] proposed convolutional interpretable layers. Finally, some works use some variation of decoupled networks [35, 36, 55] to highlight what a filter is looking at.\nConcerning visual transformers, several recent works have attempted to make transformers more interpretable by modifying their architecture. Some papers try to push the boundaries to include transformer-based heads [25, 31, 48, 51] for prototype interpretability. However, they only include a single self-attention mechanism on top of a CNN backbone. ProtoPFormer [68] suggests including a ViT backbone in the prototype setup. To do this, ProtoPFormer includes a prototype layer on top of both the classification and image tokens. However, this architecture does not guarantee that the tokens contain purely local information, especially since computing a classification on top of image tokens increases the receptive fields of the tokens [50]. While innovative, this architecture does not ensure that the tokens contain purely local information due to the increased receptive fields of tokens when combining classification and image tokens [50]. B-cos networks V2 [10] use the same rational as the original B-cos [7] approach, but they extend it to transformers. In a few words, B-cos networks summarize their inner workings as a single linear function, creating the attribution map by simply multiplying the input and the summarized network. However, this extension still relies on convolutional layers as the initial feature extraction stage, limiting its interpretability scope. Finally, A-ViT [69] was originally designed for faster inference by removing tokens at certain layers. Despite this, this mechanism serves as an interpretable system, as the most important tokens are retained until the last layer. This approach, however, focuses on efficiency rather than interpretability as its primary goal.\nIn contrast with this literature, we propose an architecture interpretable by design, by adapting the main building blocks of vision transformers to disentangle the contributions of each image patch. This enables us to compute the salient regions of the image without the need for any non-traditional training or invasive methods. Unlike other approaches, we avoid relying on the attention mechanism of the multi-head attention (MHA) block to produce saliency maps. Instead, our network inherently generates these maps as part of its decision-making process."}, {"title": "3. Methodology", "content": "In this section, we present our proposed approach and the rationale behind it. First, in \u00a73.1, we present the preliminaries for the multi-head attention mechanism and ViTs. Next, in \u00a73.2 we will show that attention and multi-head attention output can be decomposed into the individual contributions of the inputs. Finally, in \u00a73.3 we present our novel architecture, the Hindered Transformer (HiT). The core of our method is to minimise the mixing of patch-level information, which allows us to express the classification token (CLS) in ViTs as the sum of individual tokens, a direct result of \u00a73.2. In other words, this simplification allows us to check the contribution of each token."}, {"title": "3.1. Preliminaries: Transformers, ViTs and Notations", "content": "The transformer architecture is built upon the Scaled Dot-Product Attention operation [63], commonly referred to as the attention. Given a query token sequence $x_q \\in \\mathbb{R}^{L_q \\times d_{model}}$ and a target sequence (or key-value sequence) $x_t \\in \\mathbb{R}^{L_t \\times d_{model}}$, where $L_q$ and $L_t$ are their respective sequence lengths and $d_{model}$ is the token dimension, the attention mechanism is computed as follows:\n$Q = x_q W_Q +b_Q$\n$K = x_t W_K + b_K$\n$V = x_t W_V + b_V$\n$A(x_q, x_t) = \\text{softmax} \\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V$    (1)\nwhere the output is a sequence of the same length as $x_q$, $d_k$ is the dimension of the linear transformations, and $W_i \\in [\\mathbb{R}^{d_{model} \\times d_k}$ and $b_i \\in \\mathbb{R}^{d_k}$ are the weights of the linear projection $i \\in \\{Q, K, V\\}$. In addition, Vaswani et al. [63] proposed to compute the attention mechanism $h$ times in parallel, setting $d_k = d_{model} / h$ for each individual attention operation. The resulting vectors of each individual attention, formally called heads, are concatenated and linearly post-processed to obtain the final result. This operation is called multi-head attention, and it is described as follows:\n$MHA(x_q, x_t) = [A^1(x_q, x_t); ...; A^h(x_q, x_t)] W_o + b_o,$    (2)\nwith $A^i$ being the $i$th attention mechanism in the MHA, and $W_o \\in \\mathbb{R}^{d_{model} \\times d_{model}}$ and $b_o \\in \\mathbb{R}^{d_{model}}$ the linear transformation parameters.\nIn computer vision, to incorporate image data into this sequence-based formulation, the ViT first partitions the input image into $N^2$ equal-sized patches and linearly projects them to create the patch token sequence. Additionally, following standard practice, a learnable classification token CLS is prepended to the patch sequence. Furthermore, each patch token is summed with a positional embedding to encode its spatial location within the image. For the remainder of the paper, the sequence $x \\in [\\mathbb{R}^{(N^2+1) \\times d_{model}}$ denotes the concatenation of the patch tokens and the CLS token, where $x[0]$ corresponds to the CLS token.\nThe main ViT block builds on the MHA operation, followed by a token-wise MLP block, as in text-based transformers. Formally, given a set of patches $x_l$ at layer $l$, the ViT block first computes a globalized set of tokens using the MHA block. The resulting output is summed with a skip connection. Then, the output is fed into a token-wise MLP to post-process each token, followed, again, by a skip connection. This block is summarized as follows\n$x_l' = x_l + MHA(x_l, x_l)$\n$x_{l+1} = x_l' + MLP(x_l')$    (3)\nNote that before the MHA and MLP blocks, a LayerNorm [5] operation is applied to the data sequence, but for simplicity, we omit this operation. Finally, the CLS token is fed into a LayerNorm followed by a linear classifier to produce the logits of the classification task."}, {"title": "3.2. Multi-Head Attention and Patch Mixing in Transformers", "content": "In this section, we aim to decompose the MHA operation to demonstrate that it is possible to retrieve the individual contributions of each token. In this way, we aim to lay the foundation for our architecture, which is described in the next section.\nLet's start by focusing on the attention operation (Eq. 1). Since we will focus on the CLS token later, and to simplify the analysis, let's assume that the query sequence has length $L_q = 1$. Consequently, the attention mechanism can be rewritten as\n$A(x_q, x_t) = \\sum_{v \\in x_t} a(v, x_q, x_t) (v W_V + b_V),$    (4)\nwhere $a(v, x_q, x_t)$ is the attention of a single token $v \\in x_t$. Here, Eq. 4 shows that we can decompose the attention mechanism into separately processed patches - each patch $v$ in $x_t$ adds $a(v, x_q, x_t)(v W_V +b_V)$. Accordingly, if $x_t$ contains purely local information, the output of the attention is a sum of local data.\nTo continue, we incorporate the previous observation into multi-head attention and verify that we can still unroll this operation into a sum of separate vectors. One might be concerned that the concatenation-linear operation will mix each token. However, we argue that the result is still valid, since concatenating and linearly transforming the resulting vector is equivalent to linearly transforming each head and adding them together. Formally, by denoting $W_i^i$ and $b_v^i$ as the weights of the linear transformation generating the value sequence of $i$th head, and breaking apart $W_o$ into $h$ separate matrices, $W_o = [W_o^1;W_o^2; ...; W_o^h]$, with $W_o^i \\in \\mathbb{R}^{d_k \\times d_{model}}$, then, the MHA becomes\n$MHA(x_q, x_t) = b_o + \\sum_{v \\in x_t} v'(v)$    (5)\nwhere $v'(v) = \\sum_{i=1}^h a(v, x_q, x_t)(v W_v^i + b_v^i)W_o^i$.\nThe previous result implies that we can still decompose the MHA result as the sum of vector patches, regardless of the number of heads in the MHA. So the same conclusion holds as in Eq. 4: if the content in $x_t$ is local, then we can unravel the MHA mechanisms into local contributions."}, {"title": "3.3. Untangling Visual Transformers", "content": "Unlike single MHA layers, ViTs operate on global features. To integrate local information, these architectures use two mechanisms: the MHA layers, which spread the information within tokens, and the nonlinear MLPs, which introduce complex correlations even when applied to a linear combination of local contributions. For better explainability, it would be ideal if the classifier's decision could be expressed as a combination of information from individual patches, allowing a more interpretable understanding of how local information contributes to global predictions.\nIn this section, we describe our proposed architecture: Hindered Transformer (HiT). By constraining the image tokens to contain only local information along all inference blocks, and by avoiding mixing the CLS token, our novel method is able to partition the CLS token into each individual patch, a direct outcome of the previous section. Fig. 1 shows the difference between the ViT block, and our block.\nThe first challenge is then constraining the data flow between patches. To do so, we create an intermediate architecture that uses CLS token $x_l[0]$ as the query in the MHA operation, and the rest of the sequence $x_l$ as the key-value input. So, the output from the MHA is a single token that is summed to $x_l[0]$. Then, as in ViTs, we will post-process each token in the sequence with the MLP. Thus, the ViT update function in Eq. 3 is transformed to\n$x_l'[0] = x_l[0] + MHA(x_l[0], x_l)$\n$x_l'[1 :] = x_l[1 :]$\n$x_{l+1} = x_l' + MLP(x_l').$  (6)\nThe previous model solves one problem by limiting the merging of data in local patches. However, processing the CLS token through the MLP mixes the local information provided by the MHA block, as well as the value and output operations. Since our goal is to disentangle the data flow into individual contributions, we need to further constrain this processing. To do this, we simply avoid updating the CLS token through the MLP and passing it to the target sequence. So, our block inference is\n$x_{l+1}[0] = x_l[0] + MHA(x_l[0], x_l[1 :])$\n$x_{l+1}[1 :] = x_l[1 :] + MLP(x_l[1 :])$   (7)\nWe call the final architecture the Hindered Transformer (HiT), as we hinder the connections of the ViT. In a nutshell, HiT only updates the CLS token via the MHA, while the MLP blocks update the image patches. These restrictions help to preserve purely local information in each token, while allowing the CLS token to be unrolled.\nSince the classification token is not post-processed with MLP or MHA, the final image classification is the sum of the individual tokens in all layers, as shown in \u00a73.2. Therefore, the CLS in the last layer is\n$x_L[0] = x_0[0] + \\sum_{l=0}^{L-1} MHA(x_l[0], x_l[1 :])$\n$=\\sum_{l=0}^{L-1} \\sum_{v \\in x_l[1:]} v'_l(v) + x_0[0]$\n$=\\sum_{l=0}^{L-1} \\sum_{v \\in x_l[1:]} (\\nu'_l(v) + \\frac{1}{L N^2} b_o) + x_0[0]$   (8)\nPlease note that we distribute the biases $b_o^i$ of the projection operation in the MHA head evenly to each patch $v'_l(v)$. In a similar fashion, we spread $x_0[0]$ into all tokens for all layers.\nOne advantage of this architecture is that we can easily compute saliency maps, as shown in Fig. 2. The double sum in Eq. 8 can be decomposed as a tensor $\\mathbb{R}^{L \\times N^2 \\times d_{model}}$,"}, {"title": "3.4. Token Pooling", "content": "Token pooling [41], which involves downsampling the number of tokens as one progresses through the layers, is commonly used to improve the computational efficiency of standard transformers. This pooling technique effectively addresses the issue of representation power (as empirically demonstrated in \u00a7 4.7) by expanding the receptive field of the image tokens in the deeper layers. We choose to adopt this approach due to its significant advantages.\nTo achieve this, we first reorganize the tokens into their spatial layout and then perform the pooling operation. However, we need to adapt the explanation generation approach to accommodate the pooled tokens. Typically, this involves using the backward operation of the pooling operator. In our case, since we rely on average pooling, which is linear, the backward operation is simply the transposed operator, which replicates each output token across the associated 2 \u00d7 2 block and divides by 4. In other words, we distribute the importance of the pooling step equally among the contributing tokens."}, {"title": "4. Experiments", "content": "In Fig. 1, we compare the normalized insertion-deletion curves obtained from HiT and other baseline models using the blur strategy, as established in the literature. The baselines assessed include A-ViT [69], B-cos [10], and DeiT-B [59]. For B-cos, the saliency maps are intrinsically generated, while for DeiT-B, we rely on post-hoc extraction methods. Specifically, we use the Rollout Matrix [1] and GradCAM [53], referred to as DeiT-R and DeiT-GC, respectively. To create A-ViT saliency map, we utilized the layer where tokens were discarded, as described in their paper. Additionally, to mitigate the bias of the sorting algorithm when selecting the most important tokens (which tends to favor top-left corner tokens first), we added a small amount of random noise to the saliency map.\nAdditionally, we evaluate ProtoPFormer [68] on the CUB-2011 dataset. ProtoPFormer employs a Rollout Matrix to filter out irrelevant tokens for its final computation, and we used this Rollout Matrix to define the salient regions for the insertion-deletion analysis.\nThe results of this experiment are presented in Fig. 3a for ImageNet and Fig. 3b for CUB-2011. The nAUC scores and the profiles of the curves indicate that HiT outperforms other ID methods in terms of interpretability.\nTo further enhance our understanding of HiT\u2019s interpretability, we analyze the unnormalized insertion-deletion metrics of traditional post-hoc methods compared to HiT in Tab. 1. Specifically, we compare the saliency maps extracted from HiT with those generated by GradCAM [53] and an adapted Rollout Matrix [1], both computed on HiT. Overall, the inherent saliency maps of HiT demonstrate superior performance compared to the post-hoc methods for both insertion and deletion metrics. This finding suggests that these post-hoc algorithms do not consistently identify the regions used for classification.\nFinally, in Tab. 2, we present the accuracy performance of our proposed architecture compared to both non-interpretable and interpretable alternatives. As expected, all interpretable architectures, with the exception of ProtoP-Former, show a decrease in performance relative to the non-interpretable baseline. However, our Hindered Transformer maintains clear advantages in interpretability without a significant drop in performance."}, {"title": "4.4. Qualitative Evaluation", "content": "In the next part of our study, we show in Fig. 4 a qualitative comparison between the saliency maps generated by HiT and those computed with GradCAM and the Rollout Matrix when applied to HiT. We make three main conclusions. First, we found that our method focuses on certain parts of objects, regardless of whether the prediction is accurate or not. To quantify our claim, we computed the center of mass for each explanation and checked whether it fell within the object\u2019s bounding box in the CUB test set. We achieved 93.4% accuracy, which strongly supports our claim. Second, our qualitative analysis suggests that misclassification generally occurs due to similar features between image classes. However, since our method generates saliency maps, HiT inherently adopts their weaknesses: it shows where the decision was made, but not which features were used. Third, Rollout and GradCAM produce noisy maps. For example, the former shows edges and highlights the general shape of the object, while the latter produces misaligned coarse maps with our base model."}, {"title": "4.5. Layer-wise Contribution", "content": "Another advantage of HiT is that we can compute the contribution of each layer. Similar to computing the saliency maps spatially, we can create the layer-wise output tokens and look for their individual contributions. To this end, we show the results in Fig. 5a for four tested dataset. Without any surprise, we can see that most of the discriminative features are in the final layers.\nTo ensure that our results are valid, we tested several ablations of our trained model on the ImageNet dataset, shown in Fig.5b. For instance, we tested the accuracy drop by removing or adding a layer of choice (Excluding/Exclusive Layer in the figure). Similarly, we check the performance loss by removing/adding layers in a cascaded manner, dubbed cumulative removed/inserted layer. The results corroborate our previous conclusions: our novel architecture is capable of showing the contribution of each individual layer without relying on external methods, such as Linear Probing [3], to understand the basic functioning of their inner layers."}, {"title": "4.6. Sanity Check", "content": "Adebayo et al. [2] highlight that certain maps, such as edge detectors, may appear visually coherent, although they are actually unrelated to the model\u2019s decision. In order to address this issue, they proposed sanity check, which involves the iterative randomisation of the parameters of the network layers. This process begins with the deepest layers and proceeds step-by-step to the shallower ones. The resulting saliency map produced at each randomization step is then compared with the original map. In our study, we adopt the same methodology as Adebayo et al. [2] and compute the absolute rank correlation between the original saliency map and the one generated after randomization, as shown in Fig. 6a. Additionally, we include the absolute Pearson correlation coefficient in Fig. 6b, and some examples in Fig. 6c.\nForemost, we noticed that the rank correlation has a steeper slope than the Pearson correlation for the linear classification layer. This shows that there is a greater similarity with the Pearson correlation. However, the values are relatively low (less than 0.5), indicating large variations. Secondly, both metrics reach a plateau for the subsequent randomized models. This low similarity suggests that the salient regions highlighted by our model are indeed what the model sees. Finally, Fig. 6c shows some qualitative examples produced by the randomization of all blocks, showing that, effectively, the weights\u2019 randomization reflect a large variation in the produced saliency."}, {"title": "4.7. Ablating HiT", "content": "In \u00a73.4, we suggested that token pooling layers will increase the representation power of HiT. We hypothesize that the lack of inter-token connections would downgrade greatly the performance. Thus, in this section, we empirically validate that the inclusion of token pooling layers increases the performance of the model. In addition, we implemented a more powerful pooling strategy used by IdentityFormer [70]: a 3x3 convolution with a stride of 2. We focus on this architecture because it shares similar characteristics with HiT, where each token contains its own information. Lastly, we theorize that optimizing HiT architectures is challenging. To address this, we adopt an approach similar to DeiT3 [60], training our model for 300 and 600 epochs. Finally, we observed that using binary cross-entropy adversely affects the model\u2019s performance, contrary to its effect on DeiT3.\nWe present the results in Table 3. The findings align with our suspicions: the lack of transferred information between patches significantly reduces the model\u2019s accuracy."}, {"title": "5. Conclusions", "content": "This paper proposed a novel interpretable transformer-like architecture: Hindered Transformer. HiT enhances interpretability by decoupling contributions from individual image patches, enabling the extraction of saliency maps without external tools. Extensive experiments across multiple datasets demonstrated the improved interpretability benefits from HiT with a reasonable drop in performance. HiT presents a promising approach, offering favorable trade-offs for applications where interpretability is critical.\nEven though our proposed architecture has many advantages in the terms of interpretability, HiT has limiting factors: slow convergence and the potential challenges in capturing complex dependencies. Regarding the former, we believe that a more comprehensive hyperparameter search, which we were unable to conduct due to limited computational resources, could significantly reduce the training time. Regarding the latter, HiT has some token interactions during the self-attention mechanism, yet, it is indeed weaker than standard ViTs. This is troublesome for spatial tasks and would require substantial modifications, opening opportunities for future research."}]}