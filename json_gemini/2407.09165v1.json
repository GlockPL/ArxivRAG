{"title": "Robust Yet Efficient Conformal Prediction Sets", "authors": ["Soroush H. Zargarbashi", "Mohammad Sadegh Akhondzadeh", "Aleksandar Bojchevski"], "abstract": "Conformal prediction (CP) can convert any model's output into prediction sets guaranteed to include the true label with any user-specified probability. However, same as the model itself, CP is vulnerable to adversarial test examples (evasion) and perturbed calibration data (poisoning). We derive provably robust sets by bounding the worst-case change in conformity scores. Our tighter bounds lead to more efficient sets. We cover both continuous and discrete (sparse) data and our guarantees work both for evasion and poisoning attacks (on both features and labels).", "sections": [{"title": "1. Introduction", "content": "Uncertainty quantification (UQ) is crucial for deploying models, especially in safety-critical domains. The predicted probability is not a reliable source for UQ as it is often un-calibrated (Guo et al., 2017). Most methods do not provide any guarantees and require retraining or modifications in the model architecture (Abdar et al., 2021). Instead, conformal prediction (CP) returns prediction sets with a distribution-free guarantee to cover the true label. It only requires black-box access to the model and assumes exchangeable data (a weaker assumption than i.i.d.). This makes CP flexible we can apply it to image classification, segmentation (Angelopoulos et al., 2023), question answering (Angelopoulos et al., 2022), and node classification (Huang et al., 2023).\nMost models suffer a significant performance drop when fed noisy or manipulated data, even for indistinguishable (label-preserving) perturbations (Silva & Najafirad, 2020). Adversaries can exploit this vulnerability by perturbing the training data (poisoning) or the test data (evasion). CP's performance is also sensitive to the same attacks. One goal of the adversary is to break the guarantee \u2013 reducing the probability to cover the true label by perturbing the test inputs (evasion) or poisoning the calibration data. In all settings, the perturbations are limited according to a threat model, e.g. a ball of a given radius around the clean input (see \u00a7 2). Unlike heuristic defenses which are easily overcome by new attacks (Athalye et al., 2018; Mujkanovic et al., 2022), certificates provide worst-case guarantees that the prediction does not change. How can we extend robustness certificates to conformal prediction sets?\nGiven calibration data and a score functions : X \u00d7 Y \u2192R capturing conformity (agreement) between inputs and all potential labels, CP finds a calibrated threshold qa, and defines prediction sets Ca(x) = {y : s(x,y) \u2265 qa} that include all labels with scores above it. CP guarantees that Pr [ytrue \u2208 Ca(x)] \u2265 1 \u2212 a for a clean \u00e6, exchangeable with the calibration data, and any user-specfied a. To certify robustness, we can define conservative sets that ensure the coverage remains above 1 a even under perturbation.\nTo this end, Gendler et al. (2021) leverage the fact that the randomly smoothed scores Es~N(0,021) [s(x + \u03b4,y)] change slowly around the input to compute an upper bound on the worst-case score. Their randomly smoothed conformal prediction (RSCP) method has 4 limitations: (i) It considers only the mean of randomized scores resulting in a looser bound and thus larger sets; (ii) It only certifies evasion but not poisoning attacks; (iii) It only supports L2-bounded perturbations of continuous data, ignoring discrete and sparse data such as graphs; (iv) It does not correct for finite-sample approximation errors. We address all of these limitations.\nOur key insight is that we can use the cumulative distribution (CDF) of smooth scores to obtain tighter upper bounds. The resulting CDF-aware sets are smaller while maintaining the same robustness guarantee. For continuous data we reuse Kumar et al. (2020)'s bound developed to certify confidence, while for discrete/graph data we extend the bounds of Bojchevski et al. (2020).\u00b9 We then propose an approach for finite sample correction. Different from Yan et al. (2024), we bound calibration points instead of test points. In addition to being significantly faster (especially for large datasets like ImageNet), our calibration-time algorithm also leads to smaller sets when correcting for finite samples.\n\u00b9Both of these methods do not provide sets or CP guarantees."}, {"title": "2. Background", "content": "Conformal prediction. Given a holdout calibration set Dcal = {(xi, Yi)}=1 exchangeably sampled from the data distribution (or a finite dataset) with labels unseen by the model (during training), and a user-specified coverage probability 1-a, for any test point xn+1, CP defines a prediction set Ca(xn+1) Y that is guaranteed to cover the true label Yn+1 with the predetermined probability.\nTheorem 2.1 (Vovk et al. (2005)). If Dcal = {(xi, Yi)}=1, and (xn+1, Yn+1) are exchangeable, for any continuous score function s:X \u00d7 Y \u2194 R capturing the agreement between x, and y, and user-specified a \u2208 (0,1), the pre-diction set defined as Ca(xn+1) = {y: s(Xn+1,Y) \u2265 qa} has coverage probability\nPr [yn+1 \u2208 Ca(Xn+1)] \u2265 1 - \u03b1\nwhere qa := Quant (a; {s(xi, Yi)}=1) is the x-quantile of the true scores in the calibration set.\nThis theorem was extended to graphs (Zargarbashi et al., 2023; Huang et al., 2023) showing that the same guarantee holds for node classification. Although the coverage is guaranteed regardless of the choice of score function, a good choice is reflected in the size of the prediction sets (also called efficiency), the proportion of singleton sets covering the true label, and other metrics. A simple score function known as threshold prediction sets (TPS) directly considers the model's output s(x, y) = \u03c0(x, y) where \u03c0 are the class probability (softmax) estimates (Sadinle et al., 2018). TPS tends to over-cover easy examples and under-cover hard ones (Angelopoulos & Bates, 2021). This is reme-died by the commonly used adaptive prediction sets (APS) score defined as s(x, y) := (p(x, y) + u\u00b7 \u03c0(x)y). Here \u03c1(x, y) := \u03a3\u03b51\u03c0(X)c1 [\u03c0(x)c > \u03c0(x)y] is the sum of all classes predicted as more likely than y, and u \u2208 [0, 1] is a uniform random value that breaks the ties between different scores to allow exact 1 a coverage (Romano et al., 2020).\nAdversarial attacks. We define the threat model - the set of all possible perturbations the adversary can apply \u2013 by a ball centered around a clean input x. For continuous x we consider the 12 ball of radius r around the input Br(x) = {x \u2208 X : ||x \u2212 x||2 \u2264 r}. For binary data, we define the ball w.r.t. the number of flipped bits: Bra,ra (x) = {x \u2208 X :\n\u03a3=11[x = xi - 1] < rd, \u2211a=11[xi = xi + 1] < ra} where rd and ra are the numbers of deleted and added bits respectively. This distinction accounts for sparsity as shown by Bojchevski et al. (2020). We discuss categorical data in \u00a7 C, extensions to other threat models are simple.\nEvasion attacks. For a given input x and the model f, the adversary's usual goal is to find a perturbed input x such that f(x) \u2260 f(x) (Yuan et al., 2019; Madry et al., 2017). In CP, the goal changes to excluding the true label from the prediction set Ca(x) which breaks the guarantee in Eq. 1. Here we assume that CP is calibrated with clean calibration points.\nPoisoning attacks. The adversary can perturb the training data to e.g. decrease accuracy. However, since CP is model-agnostic, the guarantee holds regardless of the model's accuracy. Instead, here the goal of the adversary is to perturb the calibration set in order to decrease the empirical coverage \u2013 breaking the guarantee (see formal definition in \u00a7 3.2)."}, {"title": "3. Robust Prediction Sets", "content": "3.1. Robustness to Evasion Attacks\nDefinition 3.1 (Robust coverage). The prediction sets Ca() have adversarially robust 1 a coverage if for any (Xn+1, Yn+1) exchangeable with Deal\nPr [yn+1 \u2208 Ca(xn+1) | Xn+1 \u2208 B(xn+1)] \u2265 1 \u2212\u03b1\nwhere B(x) can be the 12 ball Br(x), the binary ball Bra,ra, or any other threat model. Gendler et al. (2021) define a score Srscp(x, y) = \u03a6\u00af\u00b9(\u0395\u03b4\u223cN(0,021)[s(x + d, y)]) based on Gaussian smoothing (Cohen et al., 2019) where \u0424-1(.) is the inverse CDF of N (0, 1). Since the smooth score is bounded, Srscp (x, y) \u2264 Srscp(x, y) +, \u2200x \u2208 B\u2084(x) they shift the quantile qa = qa - to ensure robustness. Instead of shifting the quantile we directly bound the conformal scores which is a slight generalization.\nProposition 3.1. Define s(x, y) as the upper bound for {s(x, y) : x \u2208 B(x)}. With qa as the x-quantile of the true (clean) calibration scores, let Ca(x) = {y : s(x, y) \u2265 qa}. For all xn+1 \u2208 B(xn+1), if (xn+1, Yn+1) is exchangeable with Deal then we have Pr [yn+1 \u2208 Ca(n+1)] \u2265 1 \u2013 a.\nAll omitted proofs are in \u00a7 D.1. We summarize our notation in \u00a7 K. In short, the conservative set for any x \u2208 B(x) includes the labels of the vanilla prediction set for x. Thus, the coverage guarantee also applies for the perturbed points.\nRSCP is a special case with 3(x,y) = Srscp(x,y) + . We can equivalently rewrite RSCP as an upper bound on E[s(,)] instead of P-1(E[s(\u00b7,\u00b7)]) which matches the bound from Kumar et al. (2020) (see \u00a7 F.1). In \u00a7 4 we signif-icantly improve the bound using the CDF. Tighter bounds result in smaller (more efficient) sets."}, {"title": "3.2. Robustness to Feature Poisoning Attacks", "content": "We assume that the adversary can modify at most k in-stances, 0 \u2264 k \u2264 n = |Dcal, whose features can be per-turbed in a (continuous or discrete) ball B around the clean features. We define the threat model at dataset-level:\nBk,B(D) = {D : \u010e = {(xi, Yi) : (xi, Yi) \u2208 D,Xi \u2208 B(xi), 1[xj \u2260 xj] < k}}\nnj=1\nLet qa be the a-quantile of the clean calibration scores. To decrease coverage the adversary aims to find a per-turbed calibration set Dcal \u2208 Bk,B(Dcal) that moves the quantile \u011fa = Quant(a;\u010ecal) as right as possible com-pared to qa.2 This shift increases the probability of reject-ing true labels, resulting in a lower coverage. Namely, for \u00e3 = Quant\u00af\u00b9(qa; Dcal), the quantile inverse of the poi-soned threshold q w.r.t. the clean calibration set, the poi-soned calibration set results in near 1 \u00e3 coverage where by definition 1 \u00e3 \u2264 1 \u2013 a. Given a potentially poisoned calibration set Dcal we certify the prediction sets via the following optimization problem:\nLa\nmin Quant (a; {s(zi, Yi)}=1)\nzi Ex\ns.t. \u2200(xi, Yi) \u2208 Dcal : zi \u2208 B(xi)\n\u22111[zi \u2260 Xi] \u2264 k\ni<n\nThe problem in Eq. 3 finds the most conservative quantile qa and it holds that qa \u2264 qa since for any perturbed Deal by definition it holds Dcal \u2208 Bk,B(\u010ecal). We show that the minimizer of problem Eq. 3 certifies at least 1 a coverage.\nProposition 3.2. Let qa to be the solution to the optimiza-tion problem in Eq. 3. With the conservative prediction sets\nTa(xn+1) = {Yi : S(Xn+1, Yi) \u2265 qa}\nfor any (Xn+1, Yn+1) exchangeable with (clean) Dcal we have Pr [yn+1 \u2208 Ta(Xn+1)] \u2265 1 \u2212 \u03b1.\n2Our setup works with conformity score capturing the agree-ment between \u00e6 and y. With a non-conformity score, the goal is to equivalently shift the quantile to the left (see \u00a7 A)."}, {"title": "3.3. Robustness to Label Poisoning Attacks", "content": "In the label poisoning setup, the adversary can flip the labels of at most k datapoints in the calibration set, again aiming to shift the quantile to the right. As before, we can find the most conservative quantile by solving the problem:\nLa\n(a; {S(Xi, Zi) : (Xi, Yi) \u2208 Dcal})\nmin Quant (; {(x,\ns.t. 1[zi \u2260 Yi] < k\ni<n\nSimilar to \u00a7 3.2, since qa \u2264 qa, prediction sets defined as in Eq. 4 maintain \u2265 1 \u2013 a coverage even under worst-case label perturbation. We can solve both problems (Eq. 3 and Eq. 5) by writing them as mixed-interger linear programs (MILPs). We present the technical details in \u00a7 G.\nInterestingly, our evasion-aware sets can easily be combined with our poisoning-aware threshold to obtain prediction sets that are robust to both types of attacks. Similarly, we can easily combine the feature and label poisoning constraints in a single problem. We discuss these extensions in \u00a7 H."}, {"title": "4. Randomized Smoothing Bounds", "content": "To instantiate the conservative sets Ca(\u00b7) defined in \u00a7 3 we need bounds on the worst-case change in conformity scores under perturbation. There is a rich literature on robustness certificates for standard classification (Li et al., 2023) that we can lean on, since they often need to compute similar bounds as a byproduct. We focus on methods based on the randomized smoothing framework (Cohen et al., 2019) given their high flexibility and black-box nature. This couples well with the flexibility of CP, ensuring that our final robust CP method can be broadly applied.\nSmooth scores. A smoothing scheme \u00a7 : X \u2192 X is a function that maps the input \u00e6 to a nearby random point. Given an arbitrary score s(\u00b7,\u00b7), we compute the expected (smooth) conformal scores as \u015d(x, y) := E[s(f(x), y)]. Following Cohen et al. (2019) for Gaussian smoothing, we add isotropic noise where the scale o\u00b2 determines the amount of smoothing \u015d(x, y) = Es~N(0,021)[s(x + d, y)]. For binary data, we use sparse smoothing (Bojchevski et al., 2020) and flip zeros and ones with probabilities po and p\u2081 respectively: \u015d(x, y) = E[s(x \u2295 \u03b4, y)], where \u2295 is the XOR and each entry d[i] ~ Bernoulli (p = P\u00e6[i]). See \u00a7 C for more details. Our approach works with other smoothing schemes such as uniform noise for l\u2081 threat models (Levine & Feizi, 2021), but we focus on these two due to their popularity. Gaussian smoothing preserves exchangeability (Gendler et al., 2021). Similar argument applies to sparse smoothing and other methods that are symmetric w.r.t. xn+1 and Deal.\nThe goal is to bound the smooth score \u015d(x, y) of any adversarial x \u2208 B(x). Since the base score function s(, ) often depends on a complex model such as a neural network, even computing the expected score \u015d(\u00b7, \u00b7) is challenging, let alone finding the worst-case x. Therefore, we follow the general recipe of relaxing the problem by searching over the space of all possible score functions h(\u00b7, \u00b7) \u2208 H. We focus on upper bounds, but the entire discussion equivalently applies to lower bounds by switching from max to min. By definition we have s(\u00b7,\u00b7) \u2208 H, therefore it holds that:\nmax E[s(f(x), y)] \u2264 max EB(x),h\u2208H E[h(f(x), y)]\nThe solution to Eq. 6 is trivial unless we add additional con-straints to the functions h(\u00b7, \u00b7) \u2208 H that capture information about the actual score function s(\u00b7, \u00b7). The tightness of the re-sulting bound is directly controlled by the constraints. First, we describe a baseline bound that only captures information about the mean of s(,). This is exactly the bound used by RSCP. Then, we describe a second bound that leverages information about the entire distribution of scores via the CDF. In both cases, we only need black-box access to the score function and the underlying classifier, and we assume that s(, ) \u2208 [a, b] is bounded (w.l.o.g. a = 0, b = 1).\nCanonical view. It turns out that for both Gaussian and sparse smoothing it is sufficient to derive a so-called point-wise bound for a given (x, x) pair since it can be shown that the maximum in Eq. 6 is always attained at a canonical 2 which is on the sphere of the respective ball. Namely, for the continuous B(x) we have the canonical vectors X = 0,x = [r,0,0,...] that completely specify the problem. For the binary Bra,rd we have the canonical x = [1, . . ., 1, 0, . . ., 0] and x = 1 x where ||x||o = rd and ||x||o = ra. Intuitively, the reason is due to the symme-try of the smoothing distributions and the balls (see \u00a7 C).\nBaseline bound. A straightforward approach only incorpo-rates the expected smoothed score (mean) for the given input x. Let p = E[s(g(x), y)] for simplicity. With x \u2208 B(x) the baseline upper-bound for \u015d(x, y) = E[s(f(x), y)] is deter-mined by the following problem:\nhEH\nSmean(x, y) = max E[h(f(x), y)]\ns.t. E[h(f(x), y)] = p\nThis bound discards a lot of information about the distri-bution of scores around the given \u00e6. To remedy this, we incorporate the information from the CDF of the scores.\nCDF-based bound. Let a = b\u2081 < b2 < < bm-1 < bm = b be m real numbers that partition the output space. Let pi = Pr [s(f(x), y) \u2264 bi]. We define the problem:\nScdf (x, y) = max E[h(f(x), y)]\nhEH\ns.t. Vbi: Pr [h(f(x), y) \u2264 bi] = Pi\nThe key insight for solving Eq. 8 is to upper bound the mean of h via the CDF. Intuitively, we compute the probability of each bin [bj, bj+1] and choose the upper end of the bin to get an upper bound. This can be rewritten in terms of the CDF. Let Fh (bj) = Pr [h(x, y) \u2264 bj], for any function h\nm\nE[h(x)] \u2264 bj [(Fn(bj) \u2013 Fh (bj-1)]\nj=2\nm-1\n= bm - \u2211 Fh (bj). (bj+1 - bj)\nj=2\nNext, we show how to solve both problems for the two different smoothing schemes. For Gaussian smoothing, both problems in Eq. 7 and Eq. 8 have closed-form solutions as shown by Kumar et al. (2020). For sparse smoothing, Bo-jchevski et al. (2020) provides an efficient algorithm to solve Eq. 7. We extend their approach to also solve Eq. 8 which is a novel contribution of potentially independent interest, e.g. to certify graph neural networks with regression tasks.\nIn practice, Scdf is tighter than Smean, and the improvement depends on the distribution of random scores. While we can easily combine both mean and CDF constraints to get a provably tighter bound, we focus only on CDF constraints.\nBounds for Gaussian smoothing. For any perturbed \u00e6 with ||x - x||2 \u2264 r we have the baseline bound \u015d(x, y) \u2264 Smean (x, y) = \u03a6. (\u0424-1 (p) + r) where \u03a6, is the CDF of N(0, \u03c3\u00b2) and p = E8~N(0,021)[s(x + \u03b4, y)] is the clean expected score. We can get the lower bound by flipping the sign of r. The CDF bound is \u015d(x, y) \u2264 5cdf(x, y) with\nm-1\nScdf = bm - \u03a3 \u03a6 (\u03a6\u00af\u00b9 (pj) - r) (bj+1 - bj)\nj=2\nwhere pj = Pr8~N(0,021)[s(x + \u03b4,y) \u2264 bj]. The corre-sponding lower bound and derivations are in \u00a7 D.2.\nBounds for sparse smoothing. To solve both optimiza-tion problems, we apply the same approach as Bo-jchevski et al. (2020), dividing the input space into re-gions of constant likelihood ratio X = UR\u2081 where R\u2081 = {z: Pr [f(x) = z] /Pr [f(x) = z] = ci}. For the mean variant, we greedily distribute the p mass to each region (from the highest to the lowest ratio) until the con-straint is satisfied. For the CDF variant, we instead distribute"}, {"title": "5. CAS: CDF-Aware Sets", "content": "We use the CDF-based bounds to obtain conservative predic-tion sets for evasion and conservative thresholds for poison-ing attacks. We summarize our approach with the pseudo-code in Algorithm 1 that works with any score function\u00b3.\nAlgorithm 1 CDF-Aware Sets (CAS, Evasion)\nqa = Quant (a; {$(x,y))(x,y)\u2208Dcal } \u25b7 Clean quantile\nCompute Scdf (x, y), e.g. with Eq. 10 \u25b7 Upper bound\nReturn Ca = {y : Scdf(x, y) \u2265 qa} \u25b7 Conservative set\nCalibration-time variant. For evasion we need to compute 3(x, y) via solving Eq. 8 (or Eq. 7) for each test point and each class. This can be computationally costly if we have many classes (e.g. ImageNet has 1000) at deployment. We define an alternative approach that instead needs only a lower bound s(x, y) for each x \u2208 Dcal and the true y. The key insight is that we can directly compare the smooth test score \u015d(xn+1, y) against a conservative (lower) quantile.\nProposition 5.1. For Xn+1 \u2208 B(xn+1) and (xn+1, Yn+1) exchangeable with Dcal, define\nqa = Quant (a; {S(Xi, Yi) : (Xi, Yi) \u2208 Dcal})\nFor prediction sets Ta(xn+1) = {y : \u015d(Xn+1,Y) \u2265 qa} we have Pr[yn+1 \u2208 Ta(xn+1)] \u2265 1 \u2212 a. Moreover, the vanilla CP covers the true label with probability > 1 \u2013 \u1e9e for\n\u03b2 = Quant\u00af\u00b9 (qa; {S(Xi, Yi) : (Xi, Yi) \u2208 Dcal})\nand Quant\u00af\u00b9 (t; A) = min {r' : Quant (r'; A) \u2265 t}.\n\u00b3Our code and experiments are in the github repository soroushzargar/CAS.\nWith Prop. 5.1 we need only |Dcal | certified bounds as a pre-processing step. At test time we directly plug in \u015d(xn+1,y) and not its upper bound. Since |Dcal| is often significantly smaller than the test set the computational savings are sub-stantial (see Table 3). With Eq. 12 we can compute a lower bound on the coverage of vanilla (non-robust) CP under perturbation, where by definition 1 \u2013 \u03b2 < 1 \u2013 a. This is a generalization of Theorem 2 in Gendler et al. (2021).\nPoisoning. For poisoning attacks we simply use the conser-vative threshold qa from Eq. 3 or Eq. 5 where we use the CDF-bounds in the constraints (see \u00a7 3.2). If the test exam-ples are assumed clean we return Ca = {y : \u015d(x, y) \u2265 qa}. Since robustness to evasion and poisoning are independent, we can achieve simultaneous robustness to both evasion and poisoning via Ca = {y : 5cdf(x, y) \u2265 qa}.\nTo solve the two poisoning optimization problems we rewrite them as mixed-integer linear programs and solve them with an off-the-shelf solver. We only need 2. Dcal binary variables for Eq. 3 and |Dcal|\u00d7 |Y| binary variables for Eq. 5. See \u00a7 G for technical details. Since the calibration set is relatively small we can solve the MILPs in just a few minutes. Thus, our guarantees are practically feasible."}, {"title": "6. Finite Sample Correction", "content": "Solving Eq. 7, or Eq. 8 requires the true mean or CDF. Since exact computation is intractable, we use Monte-Carlo (MC) samples. To ensure a valid certificate, we bound the exact statistics via concentration inequalities. The resulting confidence intervals are valid together with adjustable 1 \u2013 \u03b7 probability. To account for this we calibrate with a' = \u03b1-\u03b7 so that the final sets still have 1 \u2013 a coverage (see \u00a7 E). RSCP did not include such finite-sample correction, and the resulting sets are only asymptotically valid without it.\nYan et al. (2024) incorporates the correction directly in the conformity scores, leveraging exchangeability between MC-estimated calibration scores and clean test scores. We discuss this in \u00a7 E and propose another approach built on Prop. 5.1. Our correction results in smaller sets for CAS with the same guarantee; and similar results for RSCP (see \u00a7 7).\nProposition 6.1. Let Scdf+(xi, Yi) \u2264 Scdf(xi, Yi) hold with 1 \u03b7/(2|Dcal) probability for each (xi, Yi) \u2208 Dcal, and s+(Xn+1,Yn+1) \u2265 \u00a7(Xn+1, Yn+1) hold with 1 \u03b7/(2||) probability. Define the conservative quantile La+ = Quant (a \u2013 n; {$cdf+(Xi, Yi) : (Xi, Yi) \u2208 Dcal }) and Ca+(xn+1) = {y : \u015d+(Xn+1,Y) \u2265 La+}. Then\nPr[yn+1 \u2208 Ca+(n+1)] \u2265 1 \u2212 a\nWe compute Scdf+(xi, Yi) by solving the minimization variant of Eq. 8 with CDF error correction through the Dvoretzky-Kiefer-Wolfowitz inequality (Dvoretzky et al.,"}, {"title": "A. More On Conformal Prediction", "content": "Conformity vs. non-conformity scores. As mentioned in \u00a7 2, for CP we need to define a score function that quantifies the agreement between the input and each label. Equivalently, one can define CP with a non-conformity score function that captures disagreement instead. In this case, the conformal threshold is the 1 a quantile of the calibration true scores. Similarly, in the test time, labels with score less than the threshold are included in the prediction set. Both approaches are equivalent up to a change in the sign of the scores. The latter setup is used in (Gendler et al., 2021) and is equivalent to our implementation that uses conformity scores. Our choice of agreement score is due to simplicity.\nScore function. In \u00a7 2 we mentioned that conformal prediction returns guaranteed sets regardless of the score function employed. Specifically, any score function maintaining the exchangeability (between calibration and test) is viable. In brief, the exchangeability of random variable Z1, . . ., Zn means that the joint distribution of the variables is insensitive to the order/index. In other words for any permutation function 4 : [n] \u2192 [n] we have Pr [Z1, \u2026 \u2026 \u2026, Zn] = Pr [\u0396\u03c8(1),..., \u0396\u03c8(n)]. Assuming the calibration set to be exchangeably sampled from the data distribution, any permutation equivariant trans-formation on the data still preserves the exchangeability. Conclusively, the smooth scores from Gendler et al. (2021) and Bojchevski et al. (2020) are both permutation equivariant (the smoothing applies similarly to all calibration and test points regardless of their order). Therefore, smoothing scores maintains exchangeability.\nWhile any score function preserving the exchangeability maintains the conformal guarantee, better scores result in better performance with respect to the metric of interest. For instance, even a function that returns uniform conformity scores at random provides a valid guarantee, although the prediction sets will be large.\nVarious score functions are proposed in the literature of conformal classification ranging from simple softmax function on top of model's result (Sadinle et al., 2018), to more complex functions leveraging information from embedding spaces of the model (Teng et al., 2023), or from the confidence of adjacent datapoints within a network structure (Zargarbashi et al., 2023). The expected score within the smoothing scheme around an input is no exception as it only involves the datapoint itself and applies symmetrically to all datapoints. Similar conditions hold for any approximation of that expectation e.g. the mean of Monte-Carlo samples. See \u00a7B in Yan et al. (2024) for a longer discussion.\nEffect of the calibration set size. With a calibration set exchangeably sampled from the data distribution (infinite samples), conformal prediction provides a marginal coverage of at least 1 a (Eq. 1). This probability is also upper bounded by 1 \u2212 a + 1/(n + 1). Precisely, the coverage is distributed as Beta(n + 1 \u2212 1, 1) with l = [(n + 1)a].\nFor a finite set of points and an exchangeably sampled calibration subset, e.g. transductive node-classification, Huang et al. (2023) show that the coverage probability, Cov(D) = (1/|D|) \u2211(xi,yi)\u2208D1[yi \u2208 C(xi)] is distributed as\nPr [Cov(D) < t] = 1 - \u03a6HG([a(n + 1)\u300d \u2013 1; M + N, N, [Mt] + [a(n + 1)])\nWhere M = |D|, N = |Dcal| is the size of the calibration set, and \u0424HG(P, p, K) is the CDF function of hypergeometric distribution of population P, sample size p, and K successful samples within the population.\nThis means that the coverage probability on standard CP is concentrated around 1 a. It also means that the variance around 1 a decrease as the size of Dcal increases. When moving the threshold from qa to any other value q' within the domain of the score function (as in poisoning), the new threshold will correspond to another quantile \u03b2 = Quant\u00af\u00b9 (q'; Dcal) and the coverage will be similarly concentrated around 1 \u03b2.\nAccess to a large calibration set (e.g. 1000 points) is unrealistic. Even with a large set of labeled points, there is an open question of whether to use a portion of it for training the model toward better accuracy which can help even in the efficiency of CP. While we ran our experiments with the sparse labeled setting, increasing the size of the calibration set will result in similar values on average but the results will be more concentrated following the distribution of conformal probability.\nConservative coverage. Both RSCP and CAS result in an empirical coverage higher than 1 \u2013 a for clean data. This is since the vanilla prediction set is a subset of their conservative prediction set. The empirical coverage for RSCP is even higher compared to CAS since it uses looser bounds on the score and the prediction sets are unnecessarily more conservative. Higher empirical coverage is gained by larger prediction sets; therefore the goal of Robust CP is to find conservative sets that cover the worst-case perturbed input with higher than 1 a probability but not by increasing the set size significantly.\nOne-sided robust guarantee. Although CP comes with a two-sided coverage guarantee (upper and lower bound on the coverage probability), our robust coverage guarantee is one-sided we only guarantee that the coverage is larger than 1 \u2013 \u03b1. The standard two-sided guarantee relies on exchangeability. However, since the adversary might perturb each point"}, {"title": "A.1. Impelementation Details", "content": "We based our implementation on PyTorch (Paszke et al., 2019) and Pytorch Geometric (Fey & Lenssen, 2019). We run all our experiments both on CPU (Intel(R) Xeon(R) Platinum 8368 CPU @ 2.40GHz) and, and on GPU (NVIDIA A100-SXM4-40GB)."}, {"title": "B. Faster Evasion-Robustness via Calibration-time Bound", "content": "The evasion-robust CP algorithm (see \u00a7 5) requires an estimation of the expected smooth score for (i) the true class for all calibration points", "ways": "i) instead of test points (which are larger in number)", "with{": "itle", "Robust Yet Efficient Conformal Prediction Sets": "authors\": [\"Soroush H. Zargarbashi", "Mohammad Sadegh Akhondzadeh": "Aleksandar Bojchevski", "abstract": "Conformal prediction (CP) can convert any model's output into prediction sets guaranteed to include the true label with any user-specified probability. However, same as the model itself, CP is vulnerable to adversarial test examples (evasion) and perturbed calibration data (poisoning). We derive provably robust sets by bounding the worst-case change in conformity scores. Our tighter bounds lead to more efficient sets. We cover both continuous and discrete (sparse) data and our guarantees work both for evasion and poisoning attacks (on both features and labels).", "sections": [{"title": "1. Introduction", "content": "Uncertainty quantification (UQ) is crucial for deploying models, especially in safety-critical domains. The predicted probability is not a reliable source for UQ as it is often un-calibrated (Guo et al., 2017). Most methods do not provide any guarantees and require retraining or modifications in the model architecture (Abdar et al., 2021). Instead, conformal prediction (CP) returns prediction sets with a distribution-free guarantee to cover the true label. It only requires black-box access to the model and assumes exchangeable data (a weaker assumption than i.i.d.). This makes CP flexible we can apply it to image classification, segmentation (Angelopoulos et al., 2023), question answering (Angelopoulos et al., 2022), and node classification (Huang et al., 2023).\nMost models suffer a significant performance drop when fed noisy or manipulated data, even for indistinguishable (label-preserving) perturbations (Silva & Najafirad, 2020). Adversaries can exploit this vulnerability by perturbing the training data (poisoning) or the test data (evasion). CP's performance is also sensitive to the same attacks. One goal of the adversary is to break the guarantee \u2013 reducing the probability to cover the true label by perturbing the test inputs (evasion) or poisoning the calibration data. In all settings, the perturbations are limited according to a threat model, e.g. a ball of a given radius around the clean input (see \u00a7 2). Unlike heuristic defenses which are easily overcome by new attacks (Athalye et al., 2018; Mujkanovic et al., 2022), certificates provide worst-case guarantees that the prediction does not change. How can we extend robustness certificates to conformal prediction sets?\nGiven calibration data and a score functions : X \u00d7 Y \u2192R capturing conformity (agreement) between inputs and all potential labels, CP finds a calibrated threshold qa, and defines prediction sets Ca(x) = {y : s(x,y) \u2265 qa} that include all labels with scores above it. CP guarantees that Pr [ytrue \u2208 Ca(x)] \u2265 1 \u2212 a for a clean \u00e6, exchangeable with the calibration data, and any user-specfied a. To certify robustness, we can define conservative sets that ensure the coverage remains above 1 a even under perturbation.\nTo this end, Gendler et al. (2021) leverage the fact that the randomly smoothed scores Es~N(0,021) [s(x + \u03b4,y)] change slowly around the input to compute an upper bound on the worst-case score. Their randomly smoothed conformal prediction (RSCP) method has 4 limitations: (i) It considers only the mean of randomized scores resulting in a looser bound and thus larger sets; (ii) It only certifies evasion but not poisoning attacks; (iii) It only supports L2-bounded perturbations of continuous data, ignoring discrete and sparse data such as graphs; (iv) It does not correct for finite-sample approximation errors. We address all of these limitations.\nOur key insight is that we can use the cumulative distribution (CDF) of smooth scores to obtain tighter upper bounds. The resulting CDF-aware sets are smaller while maintaining the same robustness guarantee. For continuous data we reuse Kumar et al. (2020)'s bound developed to certify confidence, while for discrete/graph data we extend the bounds of Bojchevski et al. (2020).\u00b9 We then propose an approach for finite sample correction. Different from Yan et al. (2024), we bound calibration points instead of test points. In addition to being significantly faster (especially for large datasets like ImageNet), our calibration-time algorithm also leads to smaller sets when correcting for finite samples.\n\u00b9Both of these methods do not provide sets or CP guarantees."}, {"title": "2. Background", "content": "Conformal prediction. Given a holdout calibration set Dcal = {(xi, Yi)}=1 exchangeably sampled from the data distribution (or a finite dataset) with labels unseen by the model (during training), and a user-specified coverage probability 1-a, for any test point xn+1, CP defines a prediction set Ca(xn+1) Y that is guaranteed to cover the true label Yn+1 with the predetermined probability.\nTheorem 2.1 (Vovk et al. (2005)). If Dcal = {(xi, Yi)}=1, and (xn+1, Yn+1) are exchangeable, for any continuous score function s:X \u00d7 Y \u2194 R capturing the agreement between x, and y, and user-specified a \u2208 (0,1), the pre-diction set defined as Ca(xn+1) = {y: s(Xn+1,Y) \u2265 qa} has coverage probability\nPr [yn+1 \u2208 Ca(Xn+1)] \u2265 1 - \u03b1\nwhere qa := Quant (a; {s(xi, Yi)}=1) is the x-quantile of the true scores in the calibration set.\nThis theorem was extended to graphs (Zargarbashi et al., 2023; Huang et al., 2023) showing that the same guarantee holds for node classification. Although the coverage is guaranteed regardless of the choice of score function, a good choice is reflected in the size of the prediction sets (also called efficiency), the proportion of singleton sets covering the true label, and other metrics. A simple score function known as threshold prediction sets (TPS) directly considers the model's output s(x, y) = \u03c0(x, y) where \u03c0 are the class probability (softmax) estimates (Sadinle et al., 2018). TPS tends to over-cover easy examples and under-cover hard ones (Angelopoulos & Bates, 2021). This is reme-died by the commonly used adaptive prediction sets (APS) score defined as s(x, y) := (p(x, y) + u\u00b7 \u03c0(x)y). Here \u03c1(x, y) := \u03a3\u03b51\u03c0(X)c1 [\u03c0(x)c > \u03c0(x)y] is the sum of all classes predicted as more likely than y, and u \u2208 [0, 1] is a uniform random value that breaks the ties between different scores to allow exact 1 a coverage (Romano et al., 2020).\nAdversarial attacks. We define the threat model - the set of all possible perturbations the adversary can apply \u2013 by a ball centered around a clean input x. For continuous x we consider the 12 ball of radius r around the input Br(x) = {x \u2208 X : ||x \u2212 x||2 \u2264 r}. For binary data, we define the ball w.r.t. the number of flipped bits: Bra,ra (x) = {x \u2208 X :\n\u03a3=11[x = xi - 1] < rd, \u2211a=11[xi = xi + 1] < ra} where rd and ra are the numbers of deleted and added bits respectively. This distinction accounts for sparsity as shown by Bojchevski et al. (2020). We discuss categorical data in \u00a7 C, extensions to other threat models are simple.\nEvasion attacks. For a given input x and the model f, the adversary's usual goal is to find a perturbed input x such that f(x) \u2260 f(x) (Yuan et al., 2019; Madry et al., 2017). In CP, the goal changes to excluding the true label from the prediction set Ca(x) which breaks the guarantee in Eq. 1. Here we assume that CP is calibrated with clean calibration points.\nPoisoning attacks. The adversary can perturb the training data to e.g. decrease accuracy. However, since CP is model-agnostic, the guarantee holds regardless of the model's accuracy. Instead, here the goal of the adversary is to perturb the calibration set in order to decrease the empirical coverage \u2013 breaking the guarantee (see formal definition in \u00a7 3.2)."}, {"title": "3. Robust Prediction Sets", "content": "3.1. Robustness to Evasion Attacks\nDefinition 3.1 (Robust coverage). The prediction sets Ca() have adversarially robust 1 a coverage if for any (Xn+1, Yn+1) exchangeable with Deal\nPr [yn+1 \u2208 Ca(xn+1) | Xn+1 \u2208 B(xn+1)] \u2265 1 \u2212\u03b1\nwhere B(x) can be the 12 ball Br(x), the binary ball Bra,ra, or any other threat model. Gendler et al. (2021) define a score Srscp(x, y) = \u03a6\u00af\u00b9(\u0395\u03b4\u223cN(0,021)[s(x + d, y)]) based on Gaussian smoothing (Cohen et al., 2019) where \u0424-1(.) is the inverse CDF of N (0, 1). Since the smooth score is bounded, Srscp (x, y) \u2264 Srscp(x, y) +, \u2200x \u2208 B\u2084(x) they shift the quantile qa = qa - to ensure robustness. Instead of shifting the quantile we directly bound the conformal scores which is a slight generalization.\nProposition 3.1. Define s(x, y) as the upper bound for {s(x, y) : x \u2208 B(x)}. With qa as the x-quantile of the true (clean) calibration scores, let Ca(x) = {y : s(x, y) \u2265 qa}. For all xn+1 \u2208 B(xn+1), if (xn+1, Yn+1) is exchangeable with Deal then we have Pr [yn+1 \u2208 Ca(n+1)] \u2265 1 \u2013 a.\nAll omitted proofs are in \u00a7 D.1. We summarize our notation in \u00a7 K. In short, the conservative set for any x \u2208 B(x) includes the labels of the vanilla prediction set for x. Thus, the coverage guarantee also applies for the perturbed points.\nRSCP is a special case with 3(x,y) = Srscp(x,y) + . We can equivalently rewrite RSCP as an upper bound on E[s(,)] instead of P-1(E[s(\u00b7,\u00b7)]) which matches the bound from Kumar et al. (2020) (see \u00a7 F.1). In \u00a7 4 we signif-icantly improve the bound using the CDF. Tighter bounds result in smaller (more efficient) sets."}, {"title": "3.2. Robustness to Feature Poisoning Attacks", "content": "We assume that the adversary can modify at most k in-stances, 0 \u2264 k \u2264 n = |Dcal, whose features can be per-turbed in a (continuous or discrete) ball B around the clean features. We define the threat model at dataset-level:\nBk,B(D) = {D : \u010e = {(xi, Yi) : (xi, Yi) \u2208 D,Xi \u2208 B(xi), 1[xj \u2260 xj] < k}}\nnj=1\nLet qa be the a-quantile of the clean calibration scores. To decrease coverage the adversary aims to find a per-turbed calibration set Dcal \u2208 Bk,B(Dcal) that moves the quantile \u011fa = Quant(a;\u010ecal) as right as possible com-pared to qa.2 This shift increases the probability of reject-ing true labels, resulting in a lower coverage. Namely, for \u00e3 = Quant\u00af\u00b9(qa; Dcal), the quantile inverse of the poi-soned threshold q w.r.t. the clean calibration set, the poi-soned calibration set results in near 1 \u00e3 coverage where by definition 1 \u00e3 \u2264 1 \u2013 a. Given a potentially poisoned calibration set Dcal we certify the prediction sets via the following optimization problem:\nLa\nmin Quant (a; {s(zi, Yi)}=1)\nzi Ex\ns.t. \u2200(xi, Yi) \u2208 Dcal : zi \u2208 B(xi)\n\u22111[zi \u2260 Xi] \u2264 k\ni<n\nThe problem in Eq. 3 finds the most conservative quantile qa and it holds that qa \u2264 qa since for any perturbed Deal by definition it holds Dcal \u2208 Bk,B(\u010ecal). We show that the minimizer of problem Eq. 3 certifies at least 1 a coverage.\nProposition 3.2. Let qa to be the solution to the optimiza-tion problem in Eq. 3. With the conservative prediction sets\nTa(xn+1) = {Yi : S(Xn+1, Yi) \u2265 qa}\nfor any (Xn+1, Yn+1) exchangeable with (clean) Dcal we have Pr [yn+1 \u2208 Ta(Xn+1)] \u2265 1 \u2212 \u03b1.\n2Our setup works with conformity score capturing the agree-ment between \u00e6 and y. With a non-conformity score, the goal is to equivalently shift the quantile to the left (see \u00a7 A)."}, {"title": "3.3. Robustness to Label Poisoning Attacks", "content": "In the label poisoning setup, the adversary can flip the labels of at most k datapoints in the calibration set, again aiming to shift the quantile to the right. As before, we can find the most conservative quantile by solving the problem:\nLa\n(a; {S(Xi, Zi) : (Xi, Yi) \u2208 Dcal})\nmin Quant (; {(x,\ns.t. 1[zi \u2260 Yi] < k\ni<n\nSimilar to \u00a7 3.2, since qa \u2264 qa, prediction sets defined as in Eq. 4 maintain \u2265 1 \u2013 a coverage even under worst-case label perturbation. We can solve both problems (Eq. 3 and Eq. 5) by writing them as mixed-interger linear programs (MILPs). We present the technical details in \u00a7 G.\nInterestingly, our evasion-aware sets can easily be combined with our poisoning-aware threshold to obtain prediction sets that are robust to both types of attacks. Similarly, we can easily combine the feature and label poisoning constraints in a single problem. We discuss these extensions in \u00a7 H."}, {"title": "4. Randomized Smoothing Bounds", "content": "To instantiate the conservative sets Ca(\u00b7) defined in \u00a7 3 we need bounds on the worst-case change in conformity scores under perturbation. There is a rich literature on robustness certificates for standard classification (Li et al., 2023) that we can lean on, since they often need to compute similar bounds as a byproduct. We focus on methods based on the randomized smoothing framework (Cohen et al., 2019) given their high flexibility and black-box nature. This couples well with the flexibility of CP, ensuring that our final robust CP method can be broadly applied.\nSmooth scores. A smoothing scheme \u00a7 : X \u2192 X is a function that maps the input \u00e6 to a nearby random point. Given an arbitrary score s(\u00b7,\u00b7), we compute the expected (smooth) conformal scores as \u015d(x, y) := E[s(f(x), y)]. Following Cohen et al. (2019) for Gaussian smoothing, we add isotropic noise where the scale o\u00b2 determines the amount of smoothing \u015d(x, y) = Es~N(0,021)[s(x + d, y)]. For binary data, we use sparse smoothing (Bojchevski et al., 2020) and flip zeros and ones with probabilities po and p\u2081 respectively: \u015d(x, y) = E[s(x \u2295 \u03b4, y)], where \u2295 is the XOR and each entry d[i] ~ Bernoulli (p = P\u00e6[i]). See \u00a7 C for more details. Our approach works with other smoothing schemes such as uniform noise for l\u2081 threat models (Levine & Feizi, 2021), but we focus on these two due to their popularity. Gaussian smoothing preserves exchangeability (Gendler et al., 2021). Similar argument applies to sparse smoothing and other methods that are symmetric w.r.t. xn+1 and Deal.\nThe goal is to bound the smooth score \u015d(x, y) of any adversarial x \u2208 B(x). Since the base score function s(, ) often depends on a complex model such as a neural network, even computing the expected score \u015d(\u00b7, \u00b7) is challenging, let alone finding the worst-case x. Therefore, we follow the general recipe of relaxing the problem by searching over the space of all possible score functions h(\u00b7, \u00b7) \u2208 H. We focus on upper bounds, but the entire discussion equivalently applies to lower bounds by switching from max to min. By definition we have s(\u00b7,\u00b7) \u2208 H, therefore it holds that:\nmax E[s(f(x), y)] \u2264 max EB(x),h\u2208H E[h(f(x), y)]\nThe solution to Eq. 6 is trivial unless we add additional con-straints to the functions h(\u00b7, \u00b7) \u2208 H that capture information about the actual score function s(\u00b7, \u00b7). The tightness of the re-sulting bound is directly controlled by the constraints. First, we describe a baseline bound that only captures information about the mean of s(,). This is exactly the bound used by RSCP. Then, we describe a second bound that leverages information about the entire distribution of scores via the CDF. In both cases, we only need black-box access to the score function and the underlying classifier, and we assume that s(, ) \u2208 [a, b] is bounded (w.l.o.g. a = 0, b = 1).\nCanonical view. It turns out that for both Gaussian and sparse smoothing it is sufficient to derive a so-called point-wise bound for a given (x, x) pair since it can be shown that the maximum in Eq. 6 is always attained at a canonical 2 which is on the sphere of the respective ball. Namely, for the continuous B(x) we have the canonical vectors X = 0,x = [r,0,0,...] that completely specify the problem. For the binary Bra,rd we have the canonical x = [1, . . ., 1, 0, . . ., 0] and x = 1 x where ||x||o = rd and ||x||o = ra. Intuitively, the reason is due to the symme-try of the smoothing distributions and the balls (see \u00a7 C).\nBaseline bound. A straightforward approach only incorpo-rates the expected smoothed score (mean) for the given input x. Let p = E[s(g(x), y)] for simplicity. With x \u2208 B(x) the baseline upper-bound for \u015d(x, y) = E[s(f(x), y)] is deter-mined by the following problem:\nhEH\nSmean(x, y) = max E[h(f(x), y)]\ns.t. E[h(f(x), y)] = p\nThis bound discards a lot of information about the distri-bution of scores around the given \u00e6. To remedy this, we incorporate the information from the CDF of the scores.\nCDF-based bound. Let a = b\u2081 < b2 < < bm-1 < bm = b be m real numbers that partition the output space. Let pi = Pr [s(f(x), y) \u2264 bi]. We define the problem:\nScdf (x, y) = max E[h(f(x), y)]\nhEH\ns.t. Vbi: Pr [h(f(x), y) \u2264 bi] = Pi\nThe key insight for solving Eq. 8 is to upper bound the mean of h via the CDF. Intuitively, we compute the probability of each bin [bj, bj+1] and choose the upper end of the bin to get an upper bound. This can be rewritten in terms of the CDF. Let Fh (bj) = Pr [h(x, y) \u2264 bj], for any function h\nm\nE[h(x)] \u2264 bj [(Fn(bj) \u2013 Fh (bj-1)]\nj=2\nm-1\n= bm - \u2211 Fh (bj). (bj+1 - bj)\nj=2\nNext, we show how to solve both problems for the two different smoothing schemes. For Gaussian smoothing, both problems in Eq. 7 and Eq. 8 have closed-form solutions as shown by Kumar et al. (2020). For sparse smoothing, Bo-jchevski et al. (2020) provides an efficient algorithm to solve Eq. 7. We extend their approach to also solve Eq. 8 which is a novel contribution of potentially independent interest, e.g. to certify graph neural networks with regression tasks.\nIn practice, Scdf is tighter than Smean, and the improvement depends on the distribution of random scores. While we can easily combine both mean and CDF constraints to get a provably tighter bound, we focus only on CDF constraints."}, {"title": "5. CAS: CDF-Aware Sets", "content": "We use the CDF-based bounds to obtain conservative predic-tion sets for evasion and conservative thresholds for poison-ing attacks. We summarize our approach with the pseudo-code in Algorithm 1 that works with any score function\u00b3.\nAlgorithm 1 CDF-Aware Sets (CAS, Evasion)\nqa = Quant (a; {$(x,y))(x,y)\u2208Dcal } \u25b7 Clean quantile\nCompute Scdf (x, y), e.g. with Eq. 10 \u25b7 Upper bound\nReturn Ca = {y : Scdf(x, y) \u2265 qa} \u25b7 Conservative set\nCalibration-time variant. For evasion we need to compute 3(x, y) via solving Eq. 8 (or Eq. 7) for each test point and each class. This can be computationally costly if we have many classes (e.g. ImageNet has 1000) at deployment. We define an alternative approach that instead needs only a lower bound s(x, y) for each x \u2208 Dcal and the true y. The key insight is that we can directly compare the smooth test score \u015d(xn+1, y) against a conservative (lower) quantile.\nProposition 5.1. For Xn+1 \u2208 B(xn+1) and (xn+1, Yn+1) exchangeable with Dcal, define\nqa = Quant (a; {S(Xi, Yi) : (Xi, Yi) \u2208 Dcal})\nFor prediction sets Ta(xn+1) = {y : \u015d(Xn+1,Y) \u2265 qa} we have Pr[yn+1 \u2208 Ta(xn+1)] \u2265 1 \u2212 a. Moreover, the vanilla CP covers the true label with probability > 1 \u2013 \u1e9e for\n\u03b2 = Quant\u00af\u00b9 (qa; {S(Xi, Yi) : (Xi, Yi) \u2208 Dcal})\nand Quant\u00af\u00b9 (t; A) = min {r' : Quant (r'; A) \u2265 t}.\n\u00b3Our code and experiments are in the github repository soroushzargar/CAS.\nWith Prop. 5.1 we need only |Dcal | certified bounds as a pre-processing step. At test time we directly plug in \u015d(xn+1,y) and not its upper bound. Since |Dcal| is often significantly smaller than the test set the computational savings are sub-stantial (see Table 3). With Eq. 12 we can compute a lower bound on the coverage of vanilla (non-robust) CP under perturbation, where by definition 1 \u2013 \u03b2 < 1 \u2013 a. This is a generalization of Theorem 2 in Gendler et al. (2021).\nPoisoning. For poisoning attacks we simply use the conser-vative threshold qa from Eq. 3 or Eq. 5 where we use the CDF-bounds in the constraints (see \u00a7 3.2). If the test exam-ples are assumed clean we return Ca = {y : \u015d(x, y) \u2265 qa}. Since robustness to evasion and poisoning are independent, we can achieve simultaneous robustness to both evasion and poisoning via Ca = {y : 5cdf(x, y) \u2265 qa}.\nTo solve the two poisoning optimization problems we rewrite them as mixed-integer linear programs and solve them with an off-the-shelf solver. We only need 2. Dcal binary variables for Eq. 3 and |Dcal|\u00d7 |Y| binary variables for Eq. 5. See \u00a7 G for technical details. Since the calibration set is relatively small we can solve the MILPs in just a few minutes. Thus, our guarantees are practically feasible."}, {"title": "6. Finite Sample Correction", "content": "Solving Eq. 7, or Eq. 8 requires the true mean or CDF. Since exact computation is intractable, we use Monte-Carlo (MC) samples. To ensure a valid certificate, we bound the exact statistics via concentration inequalities. The resulting confidence intervals are valid together with adjustable 1 \u2013 \u03b7 probability. To account for this we calibrate with a' = \u03b1-\u03b7 so that the final sets still have 1 \u2013 a coverage (see \u00a7 E). RSCP did not include such finite-sample correction, and the resulting sets are only asymptotically valid without it.\nYan et al. (2024) incorporates the correction directly in the conformity scores, leveraging exchangeability between MC-estimated calibration scores and clean test scores. We discuss this in \u00a7 E and propose another approach built on Prop. 5.1. Our correction results in smaller sets for CAS with the same guarantee; and similar results for RSCP (see \u00a7 7).\nProposition 6.1. Let Scdf+(xi, Yi) \u2264 Scdf(xi, Yi) hold with 1 \u03b7/(2|Dcal) probability for each (xi, Yi) \u2208 Dcal, and s+(Xn+1,Yn+1) \u2265 \u00a7(Xn+1, Yn+1) hold with 1 \u03b7/(2||) probability. Define the conservative quantile La+ = Quant (a \u2013 n; {$cdf+(Xi, Yi) : (Xi, Yi) \u2208 Dcal }) and Ca+(xn+1) = {y : \u015d+(Xn+1,Y) \u2265 La+}. Then\nPr[yn+1 \u2208 Ca+(n+1)] \u2265 1 \u2212 a\nWe compute Scdf+(xi, Yi) by solving the minimization variant of Eq. 8 with CDF error correction through the Dvoretzky-Kiefer-Wolfowitz inequality (Dvoretzky et al,"}, {"title": "A. More On Conformal Prediction", "content": "Conformity vs. non-conformity scores. As mentioned in \u00a7 2, for CP we need to define a score function that quantifies the agreement between the input and each label. Equivalently, one can define CP with a non-conformity score function that captures disagreement instead. In this case, the conformal threshold is the 1 a quantile of the calibration true scores. Similarly, in the test time, labels with score less than the threshold are included in the prediction set. Both approaches are equivalent up to a change in the sign of the scores. The latter setup is used in (Gendler et al., 2021) and is equivalent to our implementation that uses conformity scores. Our choice of agreement score is due to simplicity.\nScore function. In \u00a7 2 we mentioned that conformal prediction returns guaranteed sets regardless of the score function employed. Specifically, any score function maintaining the exchangeability (between calibration and test) is viable. In brief, the exchangeability of random variable Z1, . . ., Zn means that the joint distribution of the variables is insensitive to the order/index. In other words for any permutation function 4 : [n] \u2192 [n] we have Pr [Z1, \u2026 \u2026 \u2026, Zn] = Pr [\u0396\u03c8(1),..., \u0396\u03c8(n)]. Assuming the calibration set to be exchangeably sampled from the data distribution, any permutation equivariant trans-formation on the data still preserves the exchangeability. Conclusively, the smooth scores from Gendler et al. (2021) and Bojchevski et al. (2020) are both permutation equivariant (the smoothing applies similarly to all calibration and test points regardless of their order). Therefore, smoothing scores maintains exchangeability.\nWhile any score function preserving the exchangeability maintains the conformal guarantee, better scores result in better performance with respect to the metric of interest. For instance, even a function that returns uniform conformity scores at random provides a valid guarantee, although the prediction sets will be large.\nVarious score functions are proposed in the literature of conformal classification ranging from simple softmax function on top of model's result (Sadinle et al., 2018), to more complex functions leveraging information from embedding spaces of the model (Teng et al., 2023), or from the confidence of adjacent datapoints within a network structure (Zargarbashi et al., 2023). The expected score within the smoothing scheme around an input is no exception as it only involves the datapoint itself and applies symmetrically to all datapoints. Similar conditions hold for any approximation of that expectation e.g. the mean of Monte-Carlo samples. See \u00a7B in Yan et al. (2024) for a longer discussion.\nEffect of the calibration set size. With a calibration set exchangeably sampled from the data distribution (infinite samples), conformal prediction provides a marginal coverage of at least 1 a (Eq. 1). This probability is also upper bounded by 1 \u2212 a + 1/(n + 1). Precisely, the coverage is distributed as Beta(n + 1 \u2212 1, 1) with l = [(n + 1)a].\nFor a finite set of points and an exchangeably sampled calibration subset, e.g. transductive node-classification, Huang et al. (2023) show that the coverage probability, Cov(D) = (1/|D|) \u2211(xi,yi)\u2208D1[yi \u2208 C(xi)] is distributed as\nPr [Cov(D) < t] = 1 - \u03a6HG([a(n + 1)\u300d \u2013 1; M + N, N, [Mt] + [a(n + 1)])\nWhere M = |D|, N = |Dcal| is the size of the calibration set, and \u0424HG(P, p, K) is the CDF function of hypergeometric distribution of population P, sample size p, and K successful samples within the population.\nThis means that the coverage probability on standard CP is concentrated around 1 a. It also means that the variance around 1 a decrease as the size of Dcal increases. When moving the threshold from qa to any other value q' within the domain of the score function (as in poisoning), the new threshold will correspond to another quantile \u03b2 = Quant\u00af\u00b9 (q'; Dcal) and the coverage will be similarly concentrated around 1 \u03b2.\nAccess to a large calibration set (e.g. 1000 points) is unrealistic. Even with a large set of labeled points, there is an open question of whether to use a portion of it for training the model toward better accuracy which can help even in the efficiency of CP. While we ran our experiments with the sparse labeled setting, increasing the size of the calibration set will result in similar values on average but the results will be more concentrated following the distribution of conformal probability.\nConservative coverage. Both RSCP and CAS result in an empirical coverage higher than 1 \u2013 a for clean data. This is since the vanilla prediction set is a subset of their conservative prediction set. The empirical coverage for RSCP is even higher compared to CAS since it uses looser bounds on the score and the prediction sets are unnecessarily more conservative. Higher empirical coverage is gained by larger prediction sets; therefore the goal of Robust CP is to find conservative sets that cover the worst-case perturbed input with higher than 1 a probability but not by increasing the set size significantly.\nOne-sided robust guarantee. Although CP comes with a two-sided coverage guarantee (upper and lower bound on the coverage probability), our robust coverage guarantee is one-sided we only guarantee that the coverage is larger than 1 \u2013 \u03b1. The standard two-sided guarantee relies on exchangeability. However, since the adversary might perturb each point"}, {"title": "A.1. Impelementation Details", "content": "We based our implementation on PyTorch (Paszke et al., 2019) and Pytorch Geometric (Fey & Lenssen, 2019). We run all our experiments both on CPU (Intel(R) Xeon(R) Platinum 8368 CPU @ 2.40GHz) and, and on GPU (NVIDIA A100-SXM4-40GB)."}, {"title": "B. Faster Evasion-Robustness via Calibration-time Bound", "content": "The evasion-robust CP algorithm (see \u00a7 5) requires an estimation of the expected smooth score for (i) the true class for all calibration points, (ii) and all classes for each test point. Moreover, for the standard evasion-aware robustness, we need to additionally compute adversarial upper bounds (solutions to Eq. 8) within the threat model for all classes of all test points. This upper bound has a closed-form for continuous data, and an efficient algorithm for binary/discrete data (see \u00a7 C). Nonetheless, it can be beneficial to reduce the overall runtime. Let thound be the time complexity for the upper bound computation for a single (x, y) and tmc be the time complexity of approximating the expected smooth score with M Monte-Carlo samples. With n calibration points and c classes, we need O(n \u00d7 tmc) time for calibration, including the quantile computation. Then, for each test point we need O(c \u00d7 tMC \u00d7tbound) time.\nWe define a computationally more efficient and robust alternative built upon Prop. 5.1 in which we offload the computational overhead from the test set to the calibration set. Prop. 5.1 gives a worst-case coverage lower bound for vanilla CP even if we evaluate vanilla CP with smooth (but not upper bounded) scores. Alternatively, we can find a conservative quantile that results in a certified 1 a coverage probability for the worst case input. We call this approach the faster evasion method.\nThis method of producing prediction sets significantly reduces the computation in two ways: (i) instead of test points (which are larger in number), we compute the upper bounds on calibration points, (ii) instead of computing the upper bound for all classes, we only compute it for the true class. Thus, we need O(n \u00d7 tMC \u00d7 tbound) for calibration, and O(c\u00d7 tMC) for each test point. In practical scenarios where the test set (during deployment) is larger than the calibration set, the computational savings of the faster approach become significant, especially for tasks with"}]}]}