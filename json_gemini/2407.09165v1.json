{"title": "Robust Yet Efficient Conformal Prediction Sets", "authors": ["Soroush H. Zargarbashi", "Mohammad Sadegh Akhondzadeh", "Aleksandar Bojchevski"], "abstract": "Conformal prediction (CP) can convert any model's output into prediction sets guaranteed to include the true label with any user-specified probability. However, same as the model itself, CP is vulnerable to adversarial test examples (evasion) and perturbed calibration data (poisoning). We derive provably robust sets by bounding the worst-case change in conformity scores. Our tighter bounds lead to more efficient sets. We cover both continuous and discrete (sparse) data and our guarantees work both for evasion and poisoning attacks (on both features and labels).", "sections": [{"title": "1. Introduction", "content": "Uncertainty quantification (UQ) is crucial for deploying models, especially in safety-critical domains. The predicted probability is not a reliable source for UQ as it is often un-calibrated (Guo et al., 2017). Most methods do not provide any guarantees and require retraining or modifications in the model architecture (Abdar et al., 2021). Instead, conformal prediction (CP) returns prediction sets with a distribution-free guarantee to cover the true label. It only requires black-box access to the model and assumes exchangeable data (a weaker assumption than i.i.d.). This makes CP flexible we can apply it to image classification, segmentation (Angelopoulos et al., 2023), question answering (Angelopoulos et al., 2022), and node classification (Huang et al., 2023).\nMost models suffer a significant performance drop when fed noisy or manipulated data, even for indistinguishable (label-preserving) perturbations (Silva & Najafirad, 2020). Adversaries can exploit this vulnerability by perturbing the training data (poisoning) or the test data (evasion). CP's performance is also sensitive to the same attacks. One goal of the adversary is to break the guarantee \u2013 reducing the probability to cover the true label by perturbing the test inputs (evasion) or poisoning the calibration data. In all settings, the perturbations are limited according to a threat model, e.g. a ball of a given radius around the clean input (see \u00a7 2). Unlike heuristic defenses which are easily overcome by new attacks (Athalye et al., 2018; Mujkanovic et al., 2022), certificates provide worst-case guarantees that the prediction does not change. How can we extend robustness certificates to conformal prediction sets?\nGiven calibration data and a score functions : X \u00d7 Y \u2192R capturing conformity (agreement) between inputs and all potential labels, CP finds a calibrated threshold qa, and defines prediction sets Ca(x) = {y : s(x,y) \u2265 qa} that include all labels with scores above it. CP guarantees that Pr [ytrue \u2208 Ca(x)] \u2265 1 \u2212 a for a clean \u00e6, exchangeable with the calibration data, and any user-specfied a. To certify robustness, we can define conservative sets that ensure the coverage remains above 1 a even under perturbation.\nTo this end, Gendler et al. (2021) leverage the fact that the randomly smoothed scores Es~N(0,021) [s(x + \u03b4,y)] change slowly around the input to compute an upper bound on the worst-case score. Their randomly smoothed conformal prediction (RSCP) method has 4 limitations: (i) It considers only the mean of randomized scores resulting in a looser bound and thus larger sets; (ii) It only certifies evasion but not poisoning attacks; (iii) It only supports L2-bounded perturbations of continuous data, ignoring discrete and sparse data such as graphs; (iv) It does not correct for finite-sample approximation errors. We address all of these limitations.\nOur key insight is that we can use the cumulative distribution (CDF) of smooth scores to obtain tighter upper bounds. The resulting CDF-aware sets are smaller while maintaining the same robustness guarantee. For continuous data we reuse Kumar et al. (2020)'s bound developed to certify confidence, while for discrete/graph data we extend the bounds of Bojchevski et al. (2020).\u00b9 We then propose an ap-proach for finite sample correction. Different from Yan et al. (2024), we bound calibration points instead of test points. In addition to being significantly faster (especially for large datasets like ImageNet), our calibration-time algorithm also leads to smaller sets when correcting for finite samples."}, {"title": "2. Background", "content": "Conformal prediction. Given a holdout calibration set Dcal = {(xi, Yi)}=1 exchangeably sampled from the data distribution (or a finite dataset) with labels unseen by the model (during training), and a user-specified coverage prob-ability 1-a, for any test point xn+1, CP defines a prediction set Ca(xn+1) Y that is guaranteed to cover the true label Yn+1 with the predetermined probability.\nTheorem 2.1 (Vovk et al. (2005)). If Dcal = {(xi, Yi)}=1, and (xn+1, Yn+1) are exchangeable, for any continuous score function s:X \u00d7 Y \u2194 R capturing the agreement between x, and y, and user-specified a \u2208 (0,1), the pre-diction set defined as Ca(xn+1) = {y: s(Xn+1,Y) \u2265 qa} has coverage probability\nPr [yn+1 \u2208 Ca(Xn+1)] \u2265 1 - \u03b1\nwhere qa := Quant (a; {s(xi, Yi)}=1) is the x-quantile of the true scores in the calibration set.\nThis theorem was extended to graphs (Zargarbashi et al., 2023; Huang et al., 2023) showing that the same guarantee holds for node classification. Although the coverage is guar-anteed regardless of the choice of score function, a good choice is reflected in the size of the prediction sets (also called efficiency), the proportion of singleton sets covering the true label, and other metrics. A simple score function known as threshold prediction sets (TPS) directly consid-ers the model's output s(x, y) = \u03c0(x, y) where \u03c0 are the class probability (softmax) estimates (Sadinle et al., 2018). TPS tends to over-cover easy examples and under-cover hard ones (Angelopoulos & Bates, 2021). This is reme-died by the commonly used adaptive prediction sets (APS) score defined as s(x, y) := (p(x, y) + u\u00b7 \u03c0(x)y). Here \u03c1(x, y) := \u03a3\u03b51\u03c0(X)c1 [\u03c0(x)c > \u03c0(x)y] is the sum of all classes predicted as more likely than y, and u \u2208 [0, 1] is a uniform random value that breaks the ties between different scores to allow exact 1 a coverage (Romano et al., 2020).\nAdversarial attacks. We define the threat model - the set of all possible perturbations the adversary can apply \u2013 by a ball centered around a clean input x. For continuous x we consider the 12 ball of radius r around the input Br(x) = {x \u2208 X : ||x \u2212 x||2 \u2264 r}. For binary data, we define the ball w.r.t. the number of flipped bits: Bra,ra (x) = {x \u2208 X : \u03a3=11[x = xi - 1] < rd, \u2211a=11[xi = xi + 1] < ra} where rd and ra are the numbers of deleted and added bits respectively. This distinction accounts for sparsity as shown by Bojchevski et al. (2020). We discuss categorical data in \u00a7 C, extensions to other threat models are simple.\nEvasion attacks. For a given input x and the model f, the adversary's usual goal is to find a perturbed input x such that f(x) \u2260 f(x) (Yuan et al., 2019; Madry et al., 2017). In CP, the goal changes to excluding the true label from the predic-tion set Ca(x) which breaks the guarantee in Eq. 1. Here we assume that CP is calibrated with clean calibration points.\nPoisoning attacks. The adversary can perturb the training data to e.g. decrease accuracy. However, since CP is model-agnostic, the guarantee holds regardless of the model's accu-racy. Instead, here the goal of the adversary is to perturb the calibration set in order to decrease the empirical coverage \u2013 breaking the guarantee (see formal definition in \u00a7 3.2)."}, {"title": "3. Robust Prediction Sets", "content": "3.1. Robustness to Evasion Attacks\nDefinition 3.1 (Robust coverage). The prediction sets Ca() have adversarially robust 1 a coverage if for any (Xn+1, Yn+1) exchangeable with Deal\nPr [yn+1 \u2208 Ca(xn+1) | Xn+1 \u2208 B(xn+1)] \u2265 1 \u2212\u03b1\nwhere B(x) can be the 12 ball Br(x), the binary ball Bra,ra, or any other threat model. Gendler et al. (2021) define a score Srscp(x, y) = \u03a6\u00af\u00b9(\u0395\u03b4\u223cN(0,021)[s(x + d, y)]) based on Gaussian smoothing (Cohen et al., 2019) where \u0424-1(.) is the inverse CDF of N (0, 1). Since the smooth score is bounded, Srscp (x, y) \u2264 Srscp(x, y) +, \u2200x \u2208 B\u2084(x) they shift the quantile qa = qa - to ensure robustness. Instead of shifting the quantile we directly bound the conformal scores which is a slight generalization.\nProposition 3.1. Define s(x, y) as the upper bound for {s(x, y) : x \u2208 B(x)}. With qa as the x-quantile of the true (clean) calibration scores, let Ca(x) = {y : s(x, y) \u2265 qa}. For all xn+1 \u2208 B(xn+1), if (xn+1, Yn+1) is exchangeable with Deal then we have Pr [yn+1 \u2208 Ca(n+1)] \u2265 1 \u2013 a.\nAll omitted proofs are in \u00a7 D.1. We summarize our notation in \u00a7 K. In short, the conservative set for any x \u2208 B(x) includes the labels of the vanilla prediction set for x. Thus, the coverage guarantee also applies for the perturbed points.\nRSCP is a special case with 3(x,y) = Srscp(x,y) + . We can equivalently rewrite RSCP as an upper bound on E[s(,)] instead of P-1(E[s(\u00b7,\u00b7)]) which matches the bound from Kumar et al. (2020) (see \u00a7 F.1). In \u00a7 4 we signif-icantly improve the bound using the CDF. Tighter bounds result in smaller (more efficient) sets."}, {"title": "3.2. Robustness to Feature Poisoning Attacks", "content": "We assume that the adversary can modify at most k in-stances, 0 \u2264 k \u2264 n = |Dcal|, whose features can be per-turbed in a (continuous or discrete) ball B around the clean features. We define the threat model at dataset-level:\nBk,B(D) = {D : \u010e = {(xi, Yi) : (xi, Yi) \u2208 D, Xi \u2208 B(xi), \u22111[xj \u2260 xj] < k}}\nLet qa be the a-quantile of the clean calibration scores. To decrease coverage the adversary aims to find a per-turbed calibration set Dcal \u2208 Bk,B(Dcal) that moves the quantile \u011fa = Quant(a;\u010ecal) as right as possible com-pared to qa.\u00b2 This shift increases the probability of reject-ing true labels, resulting in a lower coverage. Namely, for \u00e3 = Quant\u00af\u00b9(qa; Dcal), the quantile inverse of the poi-soned threshold q w.r.t. the clean calibration set, the poi-soned calibration set results in near 1 \u00e3 coverage where by definition 1 \u00e3 \u2264 1 \u2013 a. Given a potentially poisoned calibration set Dcal we certify the prediction sets via the following optimization problem:\nla\nmin Quant (a; {s(zi, Yi)}=1)\nzi Ex\ns.t. \u2200(xi, Yi) \u2208 Dcal : zi \u2208 B(xi)\n\u22111[zi \u2260 Xi] \u2264 k\ni<n\nThe problem in Eq. 3 finds the most conservative quantile qa and it holds that qa \u2264 qa since for any perturbed Deal by definition it holds Dcal \u2208 Bk,B(\u010ecal). We show that the minimizer of problem Eq. 3 certifies at least 1 a coverage.\nProposition 3.2. Let qa to be the solution to the optimiza-tion problem in Eq. 3. With the conservative prediction sets\nTa(xn+1) = {Yi : S(Xn+1, Yi) \u2265 qa}\nfor any (Xn+1, Yn+1) exchangeable with (clean) Dcal we have Pr [yn+1 \u2208 Ta(Xn+1)] \u2265 1 \u2212 \u03b1."}, {"title": "3.3. Robustness to Label Poisoning Attacks", "content": "In the label poisoning setup, the adversary can flip the labels of at most k datapoints in the calibration set, again aiming to shift the quantile to the right. As before, we can find the most conservative quantile by solving the problem:\nLa\n(a; {S(Xi, Zi) : (Xi, Yi) \u2208 Dcal}) \u010ecal\nmin Quant (; {(x,zi:\ns.t. \u22111[zi \u2260 Yi] < k\ni<n\nSimilar to \u00a7 3.2, since qa \u2264 qa, prediction sets defined as in Eq. 4 maintain \u2265 1 \u2013 a coverage even under worst-case label perturbation. We can solve both problems (Eq. 3 and Eq. 5) by writing them as mixed-interger linear programs (MILPs). We present the technical details in \u00a7 G.\nInterestingly, our evasion-aware sets can easily be combined with our poisoning-aware threshold to obtain prediction sets that are robust to both types of attacks. Similarly, we can easily combine the feature and label poisoning constraints in a single problem. We discuss these extensions in \u00a7 H."}, {"title": "4. Randomized Smoothing Bounds", "content": "To instantiate the conservative sets Ca(\u00b7) defined in \u00a7 3 we need bounds on the worst-case change in conformity scores under perturbation. There is a rich literature on robustness certificates for standard classification (Li et al., 2023) that we can lean on, since they often need to compute similar bounds as a byproduct. We focus on methods based on the randomized smoothing framework (Cohen et al., 2019) given their high flexibility and black-box nature. This cou-ples well with the flexibility of CP, ensuring that our final robust CP method can be broadly applied.\nSmooth scores. A smoothing scheme \u00a7 : X \u2192 X is a function that maps the input \u00e6 to a nearby random point. Given an arbitrary score s(\u00b7,\u00b7), we compute the expected (smooth) conformal scores as \u015d(x, y) := E[s(f(x), y)]. Fol-lowing Cohen et al. (2019) for Gaussian smoothing, we add isotropic noise where the scale o\u00b2 determines the amount of smoothing \u015d(x, y) = Es~N(0,021)[s(x + d, y)]. For binary data, we use sparse smoothing (Bojchevski et al., 2020) and flip zeros and ones with probabilities po and p\u2081 respectively: \u015d(x, y) = E[s(x \u2295 \u03b4, y)], where \u2295 is the XOR and each entry d[i] ~ Bernoulli (p = P\u00e6[i]). See \u00a7 C for more details. Our approach works with other smoothing schemes such as uniform noise for l\u2081 threat models (Levine & Feizi, 2021), but we focus on these two due to their popularity. Gaussian smoothing preserves exchangeability (Gendler et al., 2021). Similar argument applies to sparse smoothing and other methods that are symmetric w.r.t. xn+1 and Deal.\nThe goal is to bound the smooth score \u015d(x, y) of any adver-sarial x \u2208 B(x). Since the base score function s(, ) often depends on a complex model such as a neural network, even computing the expected score \u015d(\u00b7, \u00b7) is challenging, let alone finding the worst-case x. Therefore, we follow the general recipe of relaxing the problem by searching over the space of all possible score functions h(\u00b7, \u00b7) \u2208 H. We focus on up-per bounds, but the entire discussion equivalently applies to lower bounds by switching from max to min. By definition we have s(\u00b7,\u00b7) \u2208 H, therefore it holds that:\nmax E[s(f(x), y)] \u2264maxEB(x),h\u2208H E[h(f(x), y)]\nThe solution to Eq. 6 is trivial unless we add additional con-straints to the functions h(\u00b7, \u00b7) \u2208 H that capture information about the actual score function s(\u00b7, \u00b7). The tightness of the re-sulting bound is directly controlled by the constraints. First, we describe a baseline bound that only captures information about the mean of s(,). This is exactly the bound used by RSCP. Then, we describe a second bound that leverages information about the entire distribution of scores via the CDF. In both cases, we only need black-box access to the score function and the underlying classifier, and we assume that s(, ) \u2208 [a, b] is bounded (w.l.o.g. a = 0, b = 1).\nCanonical view. It turns out that for both Gaussian and sparse smoothing it is sufficient to derive a so-called point-wise bound for a given (x, x) pair since it can be shown that the maximum in Eq. 6 is always attained at a canonical 2 which is on the sphere of the respective ball. Namely, for the continuous B(x) we have the canonical vectors X = 0,x = [r,0,0,...] that completely specify the problem. For the binary Bra,rd we have the canonical x = [1, . . ., 1, 0, . . ., 0] and x = 1 x where ||x||o = rd and ||x||o = ra. Intuitively, the reason is due to the symme-try of the smoothing distributions and the balls (see \u00a7 C).\nBaseline bound. A straightforward approach only incorpo-rates the expected smoothed score (mean) for the given input x. Let p = E[s(g(x), y)] for simplicity. With x \u2208 B(x) the baseline upper-bound for \u015d(x, y) = E[s(f(x), y)] is deter-mined by the following problem:\nhEH\nSmean(x, y) = max E[h(f(x), y)]\ns.t. E[h(f(x), y)] = p\nThis bound discards a lot of information about the distri-bution of scores around the given \u00e6. To remedy this, we incorporate the information from the CDF of the scores.\nCDF-based bound. Let a = b\u2081 < b2 < < bm-1 < bm = b be m real numbers that partition the output space. Let pi = Pr [s(f(x), y) \u2264 bi]. We define the problem:\nhEH\nScdf (x, y) = max E[h(f(x), y)]\ns.t. Vbi: Pr [h(f(x), y) \u2264 bi] = Pi\nThe key insight for solving Eq. 8 is to upper bound the mean of h via the CDF. Intuitively, we compute the probability of each bin [bj, bj+1] and choose the upper end of the bin to get an upper bound. This can be rewritten in terms of the CDF. Let Fh (bj) = Pr [h(x, y) \u2264 bj], for any function h\nm\nE[h(x)] \u2264 bj [(Fn(bj) \u2013 Fh (bj-1)]\nj=2\nm-1\n= bm - \u2211 Fh (bj). (bj+1 - bj)\nj=2\nNext, we show how to solve both problems for the two different smoothing schemes. For Gaussian smoothing, both problems in Eq. 7 and Eq. 8 have closed-form solutions as shown by Kumar et al. (2020). For sparse smoothing, Bo-jchevski et al. (2020) provides an efficient algorithm to solve Eq. 7. We extend their approach to also solve Eq. 8 which is a novel contribution of potentially independent interest, e.g. to certify graph neural networks with regression tasks.\nIn practice, Scdf is tighter than Smean, and the improvement depends on the distribution of random scores. While we can easily combine both mean and CDF constraints to get a provably tighter bound, we focus only on CDF constraints."}, {"title": "5. CAS: CDF-Aware Sets", "content": "We use the CDF-based bounds to obtain conservative predic-tion sets for evasion and conservative thresholds for poison-ing attacks. We summarize our approach with the pseudo-code in Algorithm 1 that works with any score function\u00b3.\nAlgorithm 1 CDF-Aware Sets (CAS, Evasion)\nqa = Quant (a; {$(x,y))(x,y)\u2208Dcal } \u25b7 Clean quantile\nCompute Scdf (x, y), e.g. with Eq. 10 \u25b7 Upper bound\nReturn Ca = {y : Scdf(x, y) \u2265 qa} \u25b7 Conservative set\nCalibration-time variant. For evasion we need to compute 3(x, y) via solving Eq. 8 (or Eq. 7) for each test point and each class. This can be computationally costly if we have many classes (e.g. ImageNet has 1000) at deployment. We define an alternative approach that instead needs only a lower bound s(x, y) for each x \u2208 Dcal and the true y. The key insight is that we can directly compare the smooth test score \u015d(xn+1, y) against a conservative (lower) quantile.\nProposition 5.1. For Xn+1 \u2208 B(xn+1) and (xn+1, Yn+1) exchangeable with Dcal, define\nqa = Quant (a; {S(Xi, Yi) : (Xi, Yi) \u2208 Dcal})\nFor prediction sets Ta(xn+1) = {y : \u015d(Xn+1,Y) \u2265 qa} we have Pr[yn+1 \u2208 Ta(xn+1)] \u2265 1 \u2212 a. Moreover, the vanilla CP covers the true label with probability > 1 \u2013 \u1e9e for\n\u03b2 = Quant\u00af\u00b9 (qa; {S(Xi, Yi) : (Xi, Yi) \u2208 Dcal}) and Quant\u00af\u00b9 (t; A) = min {r' : Quant (r'; A) \u2265 t}."}, {"title": "6. Finite Sample Correction", "content": "Solving Eq. 7, or Eq. 8 requires the true mean or CDF. Since exact computation is intractable, we use Monte-Carlo (MC) samples. To ensure a valid certificate, we bound the exact statistics via concentration inequalities. The resulting confidence intervals are valid together with adjustable 1 \u2013 \u03b7 probability. To account for this we calibrate with a' = \u03b1-\u03b7 so that the final sets still have 1 \u2013 a coverage (see \u00a7 E). RSCP did not include such finite-sample correction, and the resulting sets are only asymptotically valid without it.\nYan et al. (2024) incorporates the correction directly in the conformity scores, leveraging exchangeability between MC-estimated calibration scores and clean test scores. We discuss this in \u00a7 E and propose another approach built on Prop. 5.1. Our correction results in smaller sets for CAS with the same guarantee; and similar results for RSCP (see \u00a7 7).\nProposition 6.1. Let Scdf+(xi, Yi) \u2264 Scdf(xi, Yi) hold with 1 \u03b7/(2|Dcal) probability for each (xi, Yi) \u2208 Dcal, and s+(Xn+1,Yn+1) \u2265 \u00a7(Xn+1, Yn+1) hold with 1 \u03b7/(2||) probability. Define the conservative La+ = Quant (a \u2013 n; {$cdf+(Xi, Yi) : (Xi, Yi) \u2208 Dcal }) and Ca+(xn+1) = {y : \u015d+(Xn+1,Y) \u2265 1+}. Then\nPr[yn+1 \u2208 Ca+(n+1)] \u2265 1 \u2212 a\nWe compute Scdf+(xi, Yi) by solving the minimization variant of Eq. 8 with CDF error correction through the Dvoretzky-Kiefer-Wolfowitz inequality (Dvoretzky et al.,"}, {"title": "7. Experiments", "content": "For evasion, we compare CAS with RSCP (Gendler et al., 2021). Even though the original RSCP is not able to handle sparse or discrete data, we extend it and use it as an addi-tional baseline (see \u00a7 C). There are no baselines for poison-ing. Since both RSCP and CAS have the same guaranteed coverage we focus on two main metrics: the average size of prediction sets (or efficiency) and the empirical coverage. Ideally, we want the coverage to be concentrated around the nominal 1 a. Higher coverage costs larger prediction sets. In \u00a7 J we report additional experiments including the singleton hits ratio metric. We also consider the maximum perturbation radius such that robust CP has the same set size as standard CP (averaged across test points). This size-preserving r is the largest certified radius which we can get \"for free\". On all metrics CAS outperforms RSCP.\nSetup. We evaluate our method on two image datasets: CIFAR-10 (Krizhevsky, 2009) and ImageNet (Deng et al., 2009), and one node-classification (graph) dataset Cora-ML (McCallum et al., 2004). We used ResNet-110 and ResNet-50 pretrained on CIFAR-10 and ImageNet with noisy data augmentation from Cohen et al. (2019). We trained a GCN model (Kipf & Welling, 2017) for node classification. All models are trained on data augmented with noise. The GNN is trained with 20 nodes per class with stratified sampling as the training set and similarly sampled validation set. The size of the calibration set is between 100 and 150 (sparsely labeled setting). We use APS as the main score function.\nFor each dataset, we pick a number of test points at random (900 for CIFAR-10, 400 for ImageNet, and 2480 nodes for Cora). We estimate the expected smooth scores with 104 Monte-Carlo samples. All results are an average of 100 runs with exchangeable calibration sampling (details in \u00a7 J).\nEvasion Certificate. The conservative robust sets are nec-essarily larger than non-robust sets. Consequently, on Fig. 1 (left) we observe a higher empirical coverage on clean data compared to the nominal 1 a. The coverage on perturbed inputs which we find with a PGD attack (Madry et al., 2017) is above 1 a verifying our theory. In Fig. 1 (right) we see that the empirical coverage increases with the certified radius r and is 1 a for r = 0. CAS is less needlessly conservative (grows slower with r) than RSCP while still providing the same guarantee. This leads to improved effi-ciency (smaller sets) as shown in Fig. 1 (middle). The set size is slightly higher for perturbed inputs.\nIn Fig. 2 we see that CAS's results in smaller prediction sets, across all radii, and all nominal 1 a values, and as in Fig. 3 (left) all scores. The improvement is substantial and also grows with r \u2013 for larger radii it is doubled or even tripled, especially on ImageNet and Cora-ML. Similarly, Fig. 3 (middle) shows that with CAS we can consistently certify a larger maximum radius \"for free\".\nCalibration-time evasion. Following Prop. 5.1 if we use vanilla (non-robust) CP, in the adversarial setup we can certify a lower bound 1 \u1e9e on the worst-case robust cov-erage. In Fig. 4 (left) we see that the certificate based on CAS leads to a better (higher) lower bound. At the same time, Prop. 5.1 implies that we can avoid computing upper bounds for the test points and instead account for the ef-fect of the adversary by choosing a conservative conformal threshold (qa) via the lower bound on the calibration scores. Fig. 4 (middle) show the set size distribution for test-time vs. calibration-time evasion. The results for RSCP are com-parable. CAS shows smaller sets for the calibration-time certificate. This approach is also computationally faster es-pecially for datasets with a high number of classes, which is discussed in \u00a7 B.\nAblation study. In Fig. 3 (right) we study the effect of the smoothing strength as controlled by o in N(0, \u03c32\u0399). For all values and all radiir we get the same 1 \u2013 \u03b1 coverage guarantee, however, there is a clear trade-off for choosing \u03c3. A smaller amount of smoothing results in a smaller set size in the beginning, but the set sizes grows rapidly by increasing the certified radius. In all cases, CAS is better than RSCP. Here for each o we use the model that is pretrained on the same noise augmentation.\nFinite sample correction. The previous results were with-out error correction since RSCP did not account for finite-sample errors when estimating the smooth scores with Monte-Carlo samples. The sets are still asymptotically valid without correction, as confirmed by Fig. 1 (left); however, correction is necessary for a valid certificate as argued by Yan et al. (2024). In Fig. 4 (right) we see that the size for RSCP quickly explodes, reaching almost all classes (|| = 10) for large radii, while CAS maintains low average size. Moreover, CAS has smaller standard deviation across test inputs. CAS uses calibration-time correction (see \u00a7 E).\nLabel poisoning. Next, we study label poisoning where now the attacker can perturb the ground-truth labels of the calibration points. In Table 1 we see that increasing the bud-get k leads to predictably larger set size and larger empirical coverage. The difference to the clean calibration set (k = 0) is minor, showing that provable label robustness comes al-most for free for small k. While Einbinder et al. (2022) show that standard CP is already naturally robust to random (non-worst case) label noise, Table 1 shows that adversarial label noise can break the guarantee even for small budget k.\nFeature poisoning. Since there are no baselines that provide robust coverage guarantees under poisoning we can only study the behaviour of CAS. First, we consider feature poi-soning where the attacker is allowed to change k calibration points which we refer to as the budget, each of which can be perturbed in a given ball Br(x) (see Eq. 3). In Fig. 5 (left) we show that the coverage can slightly decrease via poison-ing the features with a limited budget. This drop becomes significant when the adversary can perturb all the calibra-tion points. To poison the data, we run the PGD attack on all calibration points and decide which point to perturb by solving Eq. 3 (specifically Eq. 24) with maximization goal. Fig. 5 (middle) shows the robustness of CAS even under an infinite budget which verifies Prop. 3.2. We also show the set size of robust CP in Fig. 5 (right). We see that as expected a smaller budget k leads to less conservative sets which translates to smaller set sizes. Interestingly, for small"}, {"title": "A. More On Conformal Prediction", "content": "Conformity vs. non-conformity scores. As mentioned in \u00a7 2", "4": ["n"], "n": "we have Pr [Z1, \u2026 \u2026 \u2026, Zn", "\u0396\u03c8(n)": ".", "1)a": ".", "C(xi)": "is distributed as\nPr [Cov(D) < t", "Mt": ["a(n + 1)"]}]}