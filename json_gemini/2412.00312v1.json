{"title": "Raw Audio Classification with Cosine Convolutional Neural Network (CosCovNN)", "authors": ["Kazi Nazmul Haque", "Rajib Rana", "Tasnim Jarin", "Bj\u00f6rn W. Schuller, Jr."], "abstract": "This study explores the field of audio classification from raw waveform using Convolutional Neural Networks (CNNs), a method that eliminates the need for extracting specialised features in the pre-processing step. Unlike recent trends in literature, which often focuses on designing frontends or filters for only the initial layers of CNNs, our research introduces the Cosine Convolutional Neural Network (CosCovNN) replacing the traditional CNN filters with Cosine filters. The CosCovNN surpasses the accuracy of the equivalent CNN architectures with approximately 77% less parameters. Our research further progresses with the development of an augmented CosCovNN named Vector Quantised Cosine Convolutional Neural Network with Memory (VQCCM), incorporating a memory and vector quantisation layer VQCCM achieves state-of-the-art (SOTA) performance across five different datasets in comparison with existing literature. Our findings show that cosine filters can greatly improve the efficiency and accuracy of CNNs in raw audio classification.", "sections": [{"title": "1. Introduction", "content": "Convolutional Neural Networks have achieved remarkable success in computer vision, mainly due to their capability to model directly from raw data, thereby removing the need for handcrafted features [1]. Similarly, in the field of audio classification, there is a growing interest in direct raw waveform modelling, which presents unique challenges. The high dimensionality and complex temporal dependencies in audio data require advanced, computationally robust CNN architectures [2]. Directly modelling from raw waveforms eliminates the need for extensive pre-processing, aligning with deep learning's data-driven approach [3]. Unlike traditional spectrogram-based CNNs, limited to specific frequencies, CNNs processing raw waveforms can detect a broader range of frequency responses, increasing their effectiveness as data availability grows [4].\nIn response to these advancements, researchers have explored various modifications of CNN architectures to handle raw audio waveforms more effectively [7, 5, 6, 8]. Much of this work has centred on designing front-end modules and filters, significantly enhancing the initial layers of waveform-based CNNs. This focus stems from the idea that the initial layer plays a crucial role by directly interacting with the raw data. Notable contributions include the SincNet filter by Ravanelli et al. [9], tailored for initial CNN layers, and Zeghidour et al.'s LEAF [10], a flexible front-end that adapts to various neural networks. These advances shift away from handcrafted features, instead introducing learnable filters or front-ends in early processing stages, though they still rely on traditional CNN architecture in later layers.\nExpanding on this direction, we propose a novel approach: CosCovNN (Cosine Convolutional Neural Network), which incorporates a cosine filter into the CNN framework. This filter, replacing traditional CNN kernels, draws inspiration from the Discrete Cosine Transform (DCT) and the real parts of the Fourier Transform. The DCT is widely recognised for representing signals through a summation of cosine functions at varying frequencies, effectively capturing audio signals' spectral characteristics [11, 12, 13]. Additionally, the Fourier Transform's real parts, composed of cosine elements, emphasise symmetrical components within a signal's frequency range-fundamental in audio processing, particularly in identifying rhythmic and harmonic structures [14, 15, 16]. Our cosine-based filter choice aligns with traditional signal processing principles, enhancing CNN's ability to interpret complex audio patterns.\nTo further extend CosCovNN's capabilities, we introduce a Vector Quantisation layer immediately following the first convolutional layer, guiding the model to focus on extracting significant features from the audio waveform [17]. Additionally, a memory module allows important information captured by the initial layers to be effectively retained and passed through to later"}, {"title": "2. BACKGROUND AND RELATED WORK", "content": "The field of audio classification has been fundamentally transformed by deep learning, transitioning from traditional signal processing techniques\u2014such as Support Vector Machines (SVM), K-Nearest Neighbors (KNN), and Hidden Markov Models (HMM) [21, 22, 23] to advanced neural network architectures [24, 25, 26, 27, 28]. Early models relied heavily on hand-crafted features, but with the introduction of deep learning, particularly Convolutional Neural Networks (CNNs) [29, 30, 31] and Recurrent Neural Networks (RNNs) [32, 33, 34], audio classification entered a new era. This evolution was driven by the need to capture complex, high-dimensional patterns in audio data, for which deep learning models are well-suited due to their capacity to learn hierarchical representations directly from raw data [35].\nDespite these advancements, a substantial number of deep learning models for audio classification still rely on two-dimensional (2D) representations of audio data, such as Mel-Frequency Cepstral Coefficients (MFCC), filter banks (FBANK), and spectrograms [37, 36, 38, 39]. While effective, these representations are rooted in hand-crafted processing and may not fully harness the learning potential of deep models. Ravanelli and Bengio [9] argued that such features, while perceptually motivated, may not be optimal for all tasks. To address this, they introduced SincNet, a model that learns band-pass filters directly from the raw waveform, allowing for efficient feature learning and faster convergence. SincNet's approach, with parameterised sine"}, {"title": "3. PROPOSED RESEARCH METHOD", "content": "This section presents a detailed overview of the Cosine Convolutional Neural Network architecture and its integration into the Vector Quantised Cosine Convolutional Neural Network with Memory (VQCCM) model. The VQCCM model is constructed using the CosCovNN layer as a fundamental building block."}, {"title": "3.1. Background Knowledge", "content": "This section presents a detailed overview of the Cosine Convolutional Neural Network architecture and its integration into the Vector Quantised Cosine Convolutional Neural Network with Memory (VQCCM) model. The VQCCM model is constructed using the CosCovNN layer as a fundamental building block."}, {"title": "3.1.1. Convolution in Convolutional Neural Network", "content": "1D convolutional neural networks usually consist of convolutional layers, maxpool, and fully connected layers. For any particular layer, if the input is x[n], the convolutional operation can be defined as follows,\n$$o[n] = x[n] * h[n] = \\sum_{l=0}^{L-1} x[l] h[n-l]$$ (1)\nwhere, h[n] is the filter with length L and o[n] is the output of that layer. During training, the aim is to learn the L parameters of the filters. Each layer of the convolutional neural network is comprised of multiple filters. We"}, {"title": "3.1.2. Vector Quantisation", "content": "The idea behind vector quantisation (VQ) is to represent n set of vectors, V \u2208 {V1, V2, . . . Un} by a finite set of m representative vectors from a code-book, C\u2208 {C1, C2, . . . Cm}. Here, each vector vi and c; has an equal dimension of D where i \u2208 {1,2,3...n} and j\u2208 {1,2,3...m} [50]. The goal of VQ is to find the closest representative vector of vi in C and represent vi as cj through the mapping function G, which can be formulated as follows,\n$$G(v_i) = argmin_j||V_i \u2013 C_j||_2$$ (2)\nwhere $$||V_i - C_j ||_2$$ represents the squared Euclidean distance between the input vector point vi and the representative vector cj.\nVQ representation can be a very powerful layer in neural networks; however, the challenge lies in the computation of the gradient for $$argmin_j||v_i- C_j||_2$$. In the VQ-VAE paper [50], authors have addressed this issue by using the gradient of V, L to update the vector vi, where L is the loss of any neural network. In this paper, the authors have used the VQ layer in their Autoencoder Network to learn the discrete representation cj for vi = E(xi), where E is the Encoder, and x\u2081 is the input data. Decoder, D takes the VQ representation cj to reconstruct xi. Here, the reconstruction objective is log $$D(x_i \u2248 x_i|c_j)$$. As the dimension of cj is equal to vi, the gradient calculated for the ci can be used to update the weights of E. This way, the Autoencoder is trained end-to-end with the back-propagation algorithm.\nHere, the authors have the following loss function to train the VQ-VAE,\n$$L = log D(x_i \u2248 x_i|C_j) + ||sg[v_i] - C_j ||_2 + \\beta||v_i - sg[c_i]||_2,$$ (3)\nWhere, stop gradient, sg stops the flow of gradient during the back-propagation through a particular layer in a neural network and \u03b2 is the hyperparameter. Here, $$\\beta||v_i - sg[c_i]||_2$$ part forces the Encoder, E to learn vi close to c; and $$||sg[v_i] \u2013 c_j||_2$$ part makes sure that cj does not deviate much from vi."}, {"title": "3.2. Architecture of the CosCo\u03c5NN", "content": "The 1D Convolutional Neural Network (CNN) and 1D Cosine Convolutional Neural Network (CosCovNN) differ primarily in the filter of their convolutional layers. A CNN requires learning L parameters for each particular"}, {"title": "3.2.1. Convolutional Layer", "content": "For the convolutional layer, we generate the filter from a periodic cosine function. A cosine function can be represented as follows,\n$$y[n] = A cos(\\frac{2\\pi}{\u03bb}n)$$\n(4)"}, {"title": "3.2.2. Pooling Layer", "content": "We have used 1D max-pooling layer for down-sampling between layers. This layer selects the most salient features within a window of size k, thereby enhancing the significance of the features obtained from the convolutional layer.[51]"}, {"title": "3.2.3. Activation Layer", "content": "The activation function employed in CosCovNN is a crucial component of the network's architecture. As the values of the filters in CosCoVNN are periodic, ranging from \u22121 to 1, it is imperative to maintain this range throughout the output of each layer of the network. To ensure consistency in the range of output values, we utilise the tanh activation function."}, {"title": "3.2.4. Classification Layer", "content": "In classification tasks involving Z classes, the conventional approach is to employ a fully connected layer of size Z at the end of the network. However, this results in a substantial increase in the number of model parameters. To address this issue, we propose utilising Z Cosine Convolutional Layers with global average pooling [52] in the classification layer, which enables us to significantly reduce the parameter count."}, {"title": "3.2.5. Dropout", "content": "To enhance the resilience of the network and prevent overfitting, we incorporate 1D spatial dropout [49]. Unlike conventional dropout methods that randomly discard individual elements, spatial dropout removes entire 1D feature maps, thereby enabling the network to learn more robust and generalised features."}, {"title": "3.3. Architecture of the VQCCM", "content": "The VQCCM model is an extension of the CosCovNN that incorporates Vector Quantization (VQ) and Memory Layers to improve its performance. In this model, the VQ layer is used in the first layer, and every layer has a memory"}, {"title": "3.3.1. Vector Quantisation Layer", "content": "In the VQ layer, an embedding matrix or codebook E \u2208 $$R^{d,k}$$ is used, where k represents the number of embedding vectors and d is the sequence length of the vector. It is noteworthy that d is identical to the sequence length of the incoming feature.\nDuring the forward pass, the audio is passed through the Cosine Convolutional Neural Network (CosCovNN) and max-pooling layer, yielding a feature representation denoted as F\u2208 $$R^{b,c,d}$$, where b and c denote the number of batches and channels, respectively. Specifically, for each batch and channel,"}, {"title": "3.3.2. Memory Layer", "content": "The memory layer is composed of three key components: Memory (MEM), Memory Writer (MW), and Reader (MR). The Memory vector MEM is a vector with dimensions $$R^{1\u00d7M}$$, where M represents the size of the Memory vector. During each layer, the Reader, MR reads the Memory vector MEM and multiplies it by the current layers feature vector fi to get f\u0131, where l refers to the index of the layer of the VQCCM. Now, the Memory Writer, MW takes the feature vector fi and sums its intermediate representation with MEM to produce a new Memory vector, which is utilised by the subsequent layers Reading operation. By utilising these distinct components, the memory layer facilitates the efficient flow of information across the VQCCM network. Rather than initiating the memory with zero, we learn the MEM during the training and used this learned memory during the test time as the start memory. \nMemory Reader. The MR takes the memory, MEM \u2208 $$R^{B,M}$$ repeated over the batch of size B. The initial read operation is conducted by a Feed Forward Network, Fr. The output size of F is equal to the sequence length, S of the current layer feature f\u0131 \u2208 $$R^{B,S,C}$$, where C is the number of channels. Then the output is passed through C number of CosCovNN layers (Cosine Convolutional Layer, CCL) to get the memory, MEM \u2208 $$R^{B,S,C}$$. Now, the MEM is multiplied with the feature fi to get the feature f' to pass through the next layer and the Memory Writer. After both FR and CCL layers, we use the activation function tanh. The whole read operation can be summarised as follows,"}, {"title": "", "content": "$$f_i = f_i \u2297 tanh(CCL(tanh(F_R(MEM))))$$ (8)\nMemory Writer. The Memory Writer, MW takes the feature f' and passes it through the CCL layer. Then the output is passed through Global Average Pooling, GAP to remove the dimension C from fi. Now, this is passed through the Feed Forward Network Fw to get the intermediate feature of size (B, M). Finally, to write and create a new memory, the intermediate feature is added with the memory, MEM. This MEM is used in the subsequent layers read operation. Similar to MR, we use tanh activation after each layer. The whole write operation can be expressed as follows,"}, {"title": "", "content": "MEM = MEM + tanh(Fw(GAP(tanh(CCL(f'))))) (9)"}, {"title": "3.3.3. Training Objective", "content": "As both of the networks are evaluated on classification tasks, we used cross-entropy loss during the training. However, for VQCCM, we have an extra loss for the VQ layer. The total loss, L is computed as follows,\n$$L = - \\sum_{i=1}^Z y_i log(y_i) + ||sg[F_i] \u2013 E_j||^3 + \\beta||F_i-sg[E_j]||_2$$ (10)\nWhere Z is the number of classes in the classification task."}, {"title": "4. DATASET", "content": "CosCovNN and VQCCM are evaluated on five datasets from both speech and non-speech audio domains. The datasets used in this study are as follows:"}, {"title": "4.1. Speech Command Classification", "content": "Speech Command Dataset is consisted of 105,829 utterances of length one second and there are 35 words from 2,618 speakers [53]. An audio digit classification dataset named S09, is created from this speech command dataset where it consists of utterances for different digit categories from zero to nine. Specifically, we have used this dataset for expensive experiments."}, {"title": "4.2. Speech Emotion Classification", "content": "In our study, we utilised the IEMOCAP dataset for emotion classification. This dataset comprises a total of 12 hours of audio data, consisting of five sessions featuring two distinct speakers (one male and one female) for each session. To ensure consistency with prior research, we focused on four primary emotional states - namely angry, neutral, sad, and happy (with the excitement category consolidated with happy) [54]."}, {"title": "4.3. Speaker Identification", "content": "We employed the VoxCeleb dataset [55] for the task of speaker identification/classification. This dataset consists of over 100,000 utterances (1000 hours of audio recordings) from 1251 speakers."}, {"title": "4.4. Acoustic Scenes Classification", "content": "We have chosen the TUT Urban Acoustic Scenes 2018 dataset for our acoustic scenes classification task. The dataset comprises 24 hours of audio, which is divided into 8640 segments of 10 seconds each. The audio belongs to ten different classes, including 'Airport', 'Shopping mall', 'Metro station', 'Pedestrian street', 'Street with medium level of traffic', 'Travelling by a tram', 'Travelling by a bus', 'Travelling by an underground metro', and 'Urban park'. [56]"}, {"title": "4.5. Musical Instrument Classification", "content": "We utilised The Nsynth audio dataset to evaluate our models for the musical instrument classification task. This dataset comprises of 305,979 musical notes with a duration of four seconds, representing ten different instruments such as 'brass', 'flute', 'keyboard', 'guitar', 'mallet', 'organ', 'reed', 'string', and 'synth lead', along with one vocal class [57]."}, {"title": "5. EXPERIMENTAL SETUP, RESULTS AND DISCUSSION", "content": "We performed experiments to assess the effectiveness of both CosCovNN and VQCCM. Our evaluation of CosCovNN involved identifying an appropriate baseline architecture to compare its performance with similar CNN architectures. Additionally, we established an experimental setup to analyse the performance of VQCCM relative to state-of-the-art literature."}, {"title": "5.1. Model Architecture Search for CosC\u03bf\u03c5NN", "content": "To evaluate the performance of cosine convolutional filters, we need to first find a benchmark architecture for the model. This will allow us to evaluate its performance and computational complexity with similar CNN architecture."}, {"title": "5.1.1. Experimental Setup", "content": "Like any typical CNN model, finding a suitable architecture for CosCovNN is the most challenging part. It is very common to tune the architecture (eg. change the size of kernels, number of layers, and number of filters) of any CNN model according to the datasets. However, tuning the proposed CosCov\u039d\u039d for different datasets is out of the scope of this research work. Therefore, we want to search for an optimal architecture for any single dataset, then measure its performance on different datasets by changing only the number"}, {"title": "5.1.2. Results", "content": "The results presented in Tables. 2, we observed that a gradual reduction in filter size from layer 1 to 5 yielded better performance. This is likely due to a decrease in feature size resulting from maxpooling at each layer, which allows for better capturing of key features with smaller filter sizes.\nFor maxpooling window sizes, we identified optimal values of 10, 8, 4, and 4 for layers 1 to 4, respectively. Our analysis indicates that larger pooling sizes can lead to significantly improved accuracy. However, balancing pooling sizes across different layers is essential to avoid performance degradation. These results provide valuable insights into optimising the architecture of the CosCovNN model for improved accuracy."}, {"title": "5.2. Comparison between CosCovNN and CNN", "content": "We have evaluated the CosCovNN on the five datasets, where all of these datasets comes with test data except IEMOCAP data. For IEMOCAP, we have calculated the accuracy based on the five fold cross validation (each fold is a session). To get a fare comparison of the performance of CosCovNN, we have used CNN with similar architecture and assessed the number of parameters for both models. Moreover, we have also compared the results with related literature Time-Domain Filterbanks (TD-filterbank) [42], SincNet [9] and LEAF [10]. Our objective is to surpass the accuracy of the CNN with our CosCovNN model while achieving accuracy levels close to those of the related literature. Additionally, visualise the filters for comparison.\nTo accommodate audio signals of varying lengths, an additional layer has been incorporated at the beginning of the architecture. This layer serves to adjust the feature size to match that of a 16KHz sample rate audio signal with a duration of one second. For instance, to process audio signals with a duration of 10 seconds, a layer with a pooling size of 10 has been added. We collected the accuracy of the TD-filterbank and SincNet from the research work of Neil et al. [42], and to keep the experiment fair, we have followed the exact experimental setup from this research work. In this work, IEMOCAP and S09 dataset was not used; therefore, we have trained time-domain filterbanks, SincNet and LEAF on these datasets to collect the accuracy."}, {"title": "5.2.1. Experimental Setup", "content": "We have evaluated the CosCovNN on the five datasets, where all of these datasets comes with test data except IEMOCAP data. For IEMOCAP, we have calculated the accuracy based on the five fold cross validation (each fold is a session). To get a fare comparison of the performance of CosCovNN, we have used CNN with similar architecture and assessed the number of parameters for both models. Moreover, we have also compared the results with related literature Time-Domain Filterbanks (TD-filterbank) [42], SincNet [9] and LEAF [10]. Our objective is to surpass the accuracy of the CNN with our CosCovNN model while achieving accuracy levels close to those of the related literature. Additionally, visualise the filters for comparison.\nTo accommodate audio signals of varying lengths, an additional layer has been incorporated at the beginning of the architecture. This layer serves to adjust the feature size to match that of a 16KHz sample rate audio signal with a duration of one second. For instance, to process audio signals with a duration of 10 seconds, a layer with a pooling size of 10 has been added. We collected the accuracy of the TD-filterbank and SincNet from the research work of Neil et al. [42], and to keep the experiment fair, we have followed the exact experimental setup from this research work. In this work, IEMOCAP and S09 dataset was not used; therefore, we have trained time-domain filterbanks, SincNet and LEAF on these datasets to collect the accuracy."}, {"title": "5.2.2. Results", "content": "The results of our experiments are presented in Table 4. It is observed that CosCovNN outperforms CNN for all the tasks. Moreover, CosCovNN performs better than TD-fbanks and SincNet for all the tasks except for Acoustic Scenes and Speaker Id classification, respectively. For Acoustic Scenes classification, TD-fbanks outperforms CosCovNN. Since SincNet is explicitly designed for speaker classification, it is reasonable that it achieves better classification accuracy than CosCovNN in this regard. However, CosCovNN could not outperform LEAF in any of the tasks. This suggests that CosCovNN needs some architectural changes to surpass the best-performing model, but it can still achieve close to SOTA results.\nNow, we can calculate the number of parameters for the CosCovNN as (1\u00d732\u00d72) + (32 \u00d7 64 \u00d7 2) + (64 \u00d7 128 \u00d7 2) + (128 \u00d7 256 \u00d7 2) + (256 \u00d7 10 \u00d7 2) = 91,200 and for CNN as (1 \u00d7 32 \u00d7 100) + (32 \u00d7 64 \u00d7 50) + (64 \u00d7 128 \u00d7 12) + (128 \u00d7 256 \u00d7 6) + (256 \u00d7 10 \u00d7 3) = 4,08,192. Notably, the CosCovNN"}, {"title": "5.3. Comparison of VQCCM with literature", "content": "Based on our previous experiments, we found that the CosCovNN architecture with cosine filters is more efficient than raw CNN filters. However, it is still being determined whether this approach can be used to develop"}, {"title": "5.3.1. Experimental Setup", "content": "Based on our previous experiments, we found that the CosCovNN architecture with cosine filters is more efficient than raw CNN filters. However, it is still being determined whether this approach can be used to develop"}, {"title": "5.3.2. Results", "content": "As shown in Table. 4, VQCCM has outperformed LEAF for all tasks. However, neither LEAF nor VQCCM could exceed TD-Fbanks performance in acoustic scene classification. VQCCM and LEAF achieved similar accuracy of 99.1%, but VQCCM has a lower standard deviation than LEAF. Furthermore, as we have only tuned VQCCM with the S09 dataset, there is an opportunity for researchers to explore and fine-tune VQCCM for each problem separately. These results demonstrate that the Cosine Convolution filter can be a solid alternative to CNN filters for raw audio classification."}, {"title": "5.4. Impact of Memory and VQ size", "content": "The VQCCM model is designed to enhance information propagation from the lower layers to the classifier layer by utilising its memory component, where the VQ layer is responsible for learning representations from specific embedding vectors. The number of vectors in the VQ layer and the size of the memory layer are two crucial factors that significantly influence the performance of VQCCM. To identify the appropriate memory size and VQ embedding numbers, we conducted separate training experiments by integrating the memory and VQ layer into the CosCovNN architecture. Initially, we planned to utilise the S09 dataset for this experimentation. However, as the performance of VQCCM and CosCovNN was found to be very similar on this dataset, the impact of the memory and VQ layer might be clear from the comparison. As a result, we expanded our experiments to include the IEMOCAP dataset. Here, we assessed the maximum accuracy based on five runs and plotted it on a graph to gain insights into its behaviour."}, {"title": "5.4.1. Experimental Setup", "content": "The VQCCM model is designed to enhance information propagation from the lower layers to the classifier layer by utilising its memory component, where the VQ layer is responsible for learning representations from specific embedding vectors. The number of vectors in the VQ layer and the size of the memory layer are two crucial factors that significantly influence the performance of VQCCM. To identify the appropriate memory size and VQ embedding numbers, we conducted separate training experiments by integrating the memory and VQ layer into the CosCovNN architecture. Initially, we planned to utilise the S09 dataset for this experimentation. However, as the performance of VQCCM and CosCovNN was found to be very similar on this dataset, the impact of the memory and VQ layer might be clear from the comparison. As a result, we expanded our experiments to include the IEMOCAP dataset. Here, we assessed the maximum accuracy based on five runs and plotted it on a graph to gain insights into its behaviour."}, {"title": "5.4.2. Results", "content": "The experimental outcomes are illustrated in Fig. 7. Our results demonstrate that the model's performance can be significantly enhanced by inte-"}, {"title": "5.5. Ablation Study for VQCCM", "content": "In order to investigate the role of the Memory and VQ layer in the VQCCM model, we conducted an ablation study. This involved adding each component separately to the CosCovNN and observing the impact on model performance. The ablation study is equivalent to removing each component from VQCCM individually. Specifically, we performed two experiments: the first involved adding the Memory layer to the CosCovNN, resulting in a model referred to as VQCCM - VQ, and the second involved adding the VQ layer to the CosCovNN, resulting in a model referred to as VQCCM - Memory. The results of these experiments are presented in Table. 5.\nOur findings indicate that adding the Memory layer to the CosCovNN results in improved performance and stability compared to CosCovNN. However, when only the VQ layer is added to the CosCovNN, improvements are not consistently observed across all experiments. While Memory is a valuable addition to CosCovNN, it is even more effective when combined with the VQ layer. The VQ layer enforces the use of a fixed number of vectors, making it difficult for the model to learn an effective representation. However, by adding the Memory layer, the model is forced to use its memory to pass important information that cannot be learned through the VQ layer alone. As a result, the presence of the VQ layer compels the model to utilise the Memory layer, leading to better results."}, {"title": "6. Conclusion", "content": "In this work, we introduce cosine filters as an innovative alternative to conventional filters in Convolutional Neural Network (CNN) models, specifically for classifying audio directly from raw waveforms. Cosine filters offer a significant advantage in computational efficiency, as each filter requires the learning of only two parameters. This is in contrast to the typically higher and more variable parameter counts associated with traditional CNN filters. We developed the CosCovNN model to implement this approach, integrating cosine filters into the CNN framework.\nComparative analyses of CosCovNN and standard CNN architectures on Speech and Non-Speech datasets demonstrate that CosCovNN serves as an effective alternative for audio classification from raw waveforms. This study details the modifications necessary to incorporate cosine filters into CNN"}, {"title": "", "content": "structures, providing practical guidelines for researchers looking to adapt existing CNN models. This adaptation not only reduces the parameter count but also has the potential to enhance performance, opening new avenues for developing CNN architectures that utilize cosine filters for audio processing.\nFurthermore, we propose an advanced model, VQCCM, which builds on the CosCovNN framework by integrating Vector Quantization (VQ) and Memory layers. The classification performance of the VQCCM model was evaluated across five datasets\u2014Speech Command, Speech Emotion, Acoustic Scenes, Musical Instrument, and Speaker Identification\u2014where it achieved state-of-the-art results and outperformed benchmarks in certain cases. The integration of VQ and Memory layers significantly enhances the performance of CosCovNN, encouraging other researchers to incorporate these elements into various CNN architectures.\nAlthough our findings are based on only five datasets, which presents a limitation, they highlight broader opportunities for future research. Future work could explore the applications of VQCCM in other domains such as speaker diarization, speech-to-text conversion, music mood identification, and audio event classification. Additionally, research may delve into cross-domain adaptation, integration with advanced machine learning techniques like transformers, and optimization for real-time audio processing and hardware deployment. By pursuing these directions, researchers can extend the applicability and impact of the VQCCM model, advancing the field of audio classification and contributing to the development of versatile and sophisticated audio analysis tools."}]}