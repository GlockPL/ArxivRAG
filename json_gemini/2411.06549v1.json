{"title": "In-Context Learning for Preserving Patient Privacy: A Framework for Synthesizing Realistic Patient Portal Messages", "authors": ["Joseph Gatto", "Parker Seegmiller", "Timothy E. Burdick", "Sarah M. Preum"], "abstract": "Since the COVID-19 pandemic, clinicians have seen a large and sustained influx in patient portal messages, significantly contributing to clinician burnout. To the best of our knowledge, there are no large-scale public patient portal messages corpora researchers can use to build tools to optimize clinician portal workflows. Informed by our ongoing work with a regional hospital, this study introduces an LLM-powered framework for configurable and realistic patient portal message generation. Our approach leverages few-shot grounded text generation, requiring only a small number of de-identified patient portal messages to help LLMs better match the true style and tone of real data. Clinical experts in our team deem this framework as HIPAA-friendly, unlike existing privacy-preserving approaches to synthetic text generation which cannot guarantee all sensitive attributes will be protected. Through extensive quantitative and human evaluation, we show that our framework produces data of higher quality than comparable generation methods as well as all related datasets. We believe this work provides a path forward for (i) the release of large-scale synthetic patient message datasets that are stylistically similar to ground-truth samples and (ii) HIPAA-friendly data generation which requires minimal human de-identification efforts.", "sections": [{"title": "1. Introduction", "content": "Have you sent a message to your doctor recently? Because in the past few years, there has been a significant increase in use of patient-facing healthcare applications which allow patients to send textual messages to their provider Hansen et al. (2023). Electronic health record (EHR) applications such as Epic's MyChart, for example, reportedly had 200 million active users as of early 2021, with 80 million joining within the previous 12 months ThisWeekHealth (2021). Unfortunately, this shift in doctor-patient communication paradigms has contributed significantly to doctor burnout as clinicians have not been provided with additional resources to handle the portal message surge Stillman (2023). To help reduce clinician workload, recent studies have explored the use of AI tools to optimize cognitively demanding tasks such as portal message triage Si et al. (2020); Gatto et al. (2022); Mermin-Bunnell et al. (2023), routing, Harzand et al. (2023), and response writing Nov et al. (2023); Kozaily et al. (2023); Athavale et al. (2023). However, most of these studies use sensitive data that cannot be released to the community. Some prior work have explored patient messaging through the lens of data sourced from public medical Q&A forums Gatto et al. (2022). However, such platforms operate assuming that the physician who will respond is unfamiliar with the patient. This produces patient messages that are highly dissimilar to actual portal message data, where personal relationships with clinicians and historical EHR data lead to patient messages that are stylistically different, often containing implicit references to EHR data and prior encounters with providers. See Appendix A for an illustrating example of this issue.\nGiven the sensitive nature of patient message data, recent advancements in language modeling may be useful in generating realistic synthetic datasets which can be made public. For example, one could fine-tune a language model on data from the true distribution and then generate realistic synthetic data. Unfortunately, this paradigm may generate samples that leak sensitive patient information learned during training. Even privacy-preserving mechanisms such as Differential Privacy (DP) Wutschitz et al. (2022) cannot provide a risk-free guarantee that no sensitive patient attributes would be released during generation. This is because DP language models are trained not to leak full training data instances, but there remains a risk of generating a sensitive token (e.g., a patient's last name) violating the patient's privacy. Thus, a solution to this problem must generate data that reflects true patient portal conversational style, semantics, and structure, all while ensuring synthetic samples pose zero risk of protected health information (PHI) leakage.\nThis issue can perhaps be solved via LLM prompting. LLMs have tremendous capacity to generate texts while following a set of instructions Lou et al. (2024), potentially removing the need for sensitive training data. One could thus prompt an LLM to generate a patient portal message containing a pre-defined set of details. Unfortunately, as discussed in prior work Wang et al. (2024); Gupta et al. (2024); Liu et al. (2024), LLMs are biased, highly-formal, and struggle to capture the natural voice of people from different identity groups. Thus, off-the-shelf applications of LLMs to this task are ill-suited to generate data that matches real patient message style.\nTo address these challenges, we propose PortalGen, a two-stage, HIPAA-friendly, LLM-powered framework for the configurable generation of realistic patient message data. In stage 1, PortalGen uses few-shot prompting of LLMs to transform codes from healthcare databases into portal message prompts. This provides a means of generating diverse large-scale message corpora covering a wide variety of health situations. In stage 2, we use grounded generation with a small number of de-identified patient messages. Grounded generation Veselovsky et al. (2023) is a technique that includes samples from the target distribution in the prompt, encouraging LLM outputs to be more stylistically and semantically faithful to the nuances of real training samples. PortalGen performs grounding with just 10 de-identified patient messages, providing a framework for researchers and institutions to release realistic synthetic patient message data without requiring large-scale de-identification efforts. Our results demonstrate that PortalGen produces data that is highly similar to real data, outperforming baseline data synthesis techniques and showing strong contrast with related public medical Q&A datasets."}, {"title": "2. Dataset Overview", "content": "Our patient portal messages are sourced from a large-scale dataset of 610k patient messages collected from a large academic medical system in the United States. All messages were sent between 1/2020 - 9/2024. The dataset contains 10,526 unique patients, where the population is 29% male, 51% female, and 20% unknown gender, with ages between 17 - 81.\nThe PortalGen framework leverages only 10 real patient message examples. We choose n=10 as it provides sufficient context to the LLM while minimizing the de-identification efforts required for community data release. The authors manually selected 10 representative messages with varying lengths and health scenarios. Each message was manually de-identified by a human annotator who has been trained on how to handle sensitive patient data and how to de-identify portal messages. All removed PHI elements were randomly replaced using viable substitutes for each PHI category (e.g. \"My name is Jane\" \u2192 \"My name is Victoria\"). All other data used for training and evaluation of PortalGen is sampled from the remaining population of patient messages."}, {"title": "3. Methods", "content": "In this section, we describe our proposed two-stage framework, PortalGen.\nStage # 1: In effort to generate a large number of patient portal messages covering a diverse range of health conditions, we develop an LLM-driven framework for converting ICD-9 codes into message prompts. ICD-9 codes are a widely used standardized mapping of numeric codes to health conditions. Our framework allows us to use public ICD-9 databases 1 to source various health conditions and convert them into patient portal message prompts. Each ICD-9 code in the database comes with a brief description, which we use as the input in our framework.\nWe map ICD-9 descriptions to portal message prompts using GPT-3.5 2 with few-shot learning (k=4). To illustrate, consider an example we provide in context. (ICD-9 Code Description: Shoulder joint replacement \u2192 Message Prompt: Patient heard a snap while trying to lift heavy boxes after shoulder surgery, and is experiencing pain.) As you can see, we instruct the LLM to extrapolate the ICD-9 description into a realistic situation that could be the context of a patient message for someone with this ICD-9 code in their medical chart. We use GPT-3.5 due to it's popularity, low cost, instruction following ability, and fast inference speeds. The full prompt used in Stage #1 as well as additional details can be found in Appendix C.\nWe use the described framework to generate 1000 message prompts using descriptions from 1000 randomly sampled ICD-9 codes, which are used throughout the remainder of this study.\nStage #2: To transform patient message prompts into synthetic patient message data, we employ grounded generation Veselovsky et al. (2023). Specifically, we include 10 real de-identified patient portal messages in the prompt when asking the model to synthesize a new message. This guides the LLM towards matching the style and prose of samples from the target distribution. For each real sample, we create a ground truth prompt that the LLM can use to learn how to map prompts to messages. This allows us to formulate our synthetic data generation as a 10-shot prompt, using 10 (prompt, message) pairs as in-context examples. The prompt used for Stage #2 can be found in Appendix B."}, {"title": "4. Experimental Setup", "content": "We compare PortalGen to a variety of relevant baselines.\nGPT-2 Fine-Tuned on Real Patient Data: We fine-tune GPT-2 on 1,000 real patient portal messages as a ground truth baseline. We then generate 1,000 messages using this model. This experiment showcases performance when privacy preservation is not a concern.\nGPT-2 Fine-Tuned with Differential Privacy: We fine-tune GPT-2 with differential privacy Wutschitz et al. (2022) on 1,000 real patient portal messages. Due to the sensitive nature of this data type, we use a privacy budget of $\\epsilon$ = 1. We then generate 1,000 messages using this model. This experiment compares our approach to an industry-standard privacy-preserving generation technique.\nZero-Shot LLM Prompting: We generate 1,000 synthetic messages using our ICD-9-based prompts with two different LLMS, GPT-3.5-Turbo and Mixtral 8x7b Jiang et al. (2024). This experiment showcases data quality without grounding.\nPortalGen: We evaluate our framework, PortalGen, on Mixtral 8x7b. Note that this study was performed on a secure computing platform with limited GPU resources and no access to the internet, limiting our ability to explore many larger API-based LLMs. Mixtral 8x7b is the largest LLM we are able to explore in this study. Additional relevant configurations and parameters for all LLM-based experiments can be found in Appendix \u0412.\nBaseline Datasets: We additionally compare our synthetic data to two open-source corpora sourced from online medical Q&A forums, ICliniq.com HuggingFace (2023b) and HealthCareMagic.com HuggingFace (2023a). We explore the semantic and stylistic differences between our synthetic corpora and 1,000 randomly sampled messages from each of these two datasets."}, {"title": "4.1. Evaluation", "content": "We explore three different evaluation metrics in this study. (i) Perplexity Analysis: We train a GPT-2 model from scratch using a given synthetic dataset. We then compute the mean perplexity of this model on a hold-out test set of 5,000 real patient messages. This experiment tells us whether an LLM trained with synthetic data is confused or surprised by real patient messages. We report 1 - the normalized perplexity in Figure 1 for ease of comparison across metrics. (ii) Semantic Similarity: We use statistical depth Seegmiller and Preum (2023), a measure of how semantically similar two corpora are, to determine if the synthetic dataset is semantically similar to our set of 5,000 test messages. This experiment tells us if the semantic structure of our synthetic data is similar to real messages. We report the normalized statistical depth Q-value (higher=more similar) in Figure 1. (iii) Human evaluation: We additionally have two human annotators with significant experience reviewing patient portal messages perform qualitative evaluation. Specifically, we randomly select 10 prompts used in the LLM-based experiments and have each annotator blindly rank the outputs from GPT-3.5, Mixtral 8x7b, and PortalGen from best to worst based on their interpretation of how the messages read in terms of stylistic and semantic similarity to real patient messages. We report the mean ranking of each model after this analysis."}, {"title": "5. Results", "content": "Quantitative Evaluation: In Figure 1 we show the results of our quantitative experiments. On perplexity, we find that as expected, the two models trained on real patient message data achieve the best performance. When we compare PortalGen to LLM Prompting, we find a significant difference in perplexity, with LLM-prompting achieving the worst score across all experiments. This result provides evidence for our claim that LLMs without grounding struggle to generate data which stylistically and/or structurally matches real patient data. Additionally, we find that PortalGen data shows better perplexity than both Medical Q&A datasets, highlighting the difference between the two data sources.\nWhen comparing each corpus using statistical depth, we again find that the GPT-2 model trained on real data achieves the highest score as expected. However, on this metric the privacy preserving generation begins to fail, as there is a known trade-off in generated content quality when training with privacy-preserving algorithms Wutschitz et al. (2022). We find that LLM-prompting scores second highest on statistical depth, with PortalGen achieving the third highest score. This follows our intuition that LLMs mostly struggle to mimic style and prose, not following instructions to include certain pre-defined semantic details. Interestingly, both Med Q&A datasets have extremely low semantic similarity compared to the ground truth data. This again provides quantitative evidence for the stark difference in medical Q&A forum data and real patient messages. In summary, we find that PortalGen achieves the best balance of depth and perplexity across all generation strategies.\nQualitative Evaluation: Our human evaluation tasked annotators with blindly ranking the outputs (1=best, 3=worst) from three different generation strategies. We find that PortalGen outputs had an average rank of 1.55, GPT-3.5 had an average rank of 2.2, and Mixtral 8x7b had an average rank of 2.25. This indicates that human annotators most often find PortalGen to generate the most realistic patient portal messages. Qualitatively, we note that the gap between PortalGen performance and baseline LLMs is smaller when the prompt is more vague. E.g. a lower ranking PortalGen output has a prompt containing \"Patient is seeking advice on managing symptoms and coping mechanisms...\". Conversely, one of the unanimous high ranking PortalGen outputs contains a more detailed prompt \"Patient is experiencing swelling and stiffness...\". We share an example model output in Appendix A.2."}, {"title": "6. Discussion, Limitations, and Future Work", "content": "In this paper, we propose the use of LLM-grounded generation for the synthesis of patient portal messages. Our framework provides researchers with access to portal messages a realistic path towards large-scale data release. Our framework promotes the generation of diverse and realistic messages that may be used in the development of patient portal tools which optimize clinical workflows and reduce clinician burnout. A limitation of this study is that we cannot evaluate our framework on LLMs larger than 50 billion parameters due to the computational restrictions of our HIPAA-compliant workstation. Additionally, our work is limited in that we only explore portal messages from one healthcare system. Future works in this space may explore varying the number of samples used for grounding, as well as how to generate realistic message data without reliance on in-domain samples."}, {"title": "Appendix A. Example Data", "content": "A.1. Comparing Patient Portal Messages to Online Medical Q&A Data\nIn this section, we provide an illustrating example of how real patient portal message data differs from related public datasets. First, consider the following example from the HealthCareMagic HuggingFace (2023a) dataset:\nI am a 35 year old male and own a landscape business for 2 years, for the last 6 weeks It has become difficult for me to get through half the day without becoming exhausted (low energy that comes on fairly sudden, slightly shakey, foggy mind kinda like im in a dream) I have cut my hours back and have had employees make up the difference, thinking maby I am working to hard, energy still cuts out half way thru the day. I have had some life changes (seperation, moving to new house, extra debt to support expanding business) but nothing I would consider unmanageable. Any advice? I have setup a health exam with my doctor in September? Could this be physical or phsychological? Any advice from past experiencees? Thanks!\nWe notice how the patient provides demographic information (i.e. age) in the body of the message, which does not typically occur in real data as there is an implicit understanding that the clinician has the patients medical chart available when reading their message. One can imagine how using NLP to classify how urgent or worrisome this patient's symptoms are, for example, may heavily depend on demographics, as fatigue may be considered more or less expected depending on the patients age. Developing a synthetic dataset for use with real messages must reflect the fact these portal message classification tasks are often multi-modal with many pieces of information needing to be pulled from the EHR. Additionally, the style and tone of this message reflect the fact that there is no relationship between the patient and provider, as the message contains significant details about the patient's personal and professional life. Conversely, real patient message data often contains personal anecdotes, references to past conversations, and friendly language given the personal relationship the patient has with their provider. Such style and prose should be reflected in synthetic datasets.\nNow consider a synthetic patient portal message:\nHey Dr. James,\nThe pain is back again. I don't know what the next steps should be but the pain medication you gave me is NOT working. Is"}, {"title": "A.2. Example Model Outputs", "content": "In this section, we show the difference in generation performance between PortalGen and comparable LLM-prompting approaches.\nPrompt: Patient is experiencing a diabetic coma and is unsure of what steps to take to regain control.\nGPT-3.5 Generation\nHey Doc, I'm in a bit of a tough spot right now. I'm experiencing a diabetic coma and I'm not sure how to regain control. I am dizzy, disoriented, and my blood sugar levels are all over. Can you give me some guidance on how to get back on track?\nMixtral 8x7b Generation\nHi Dr. I hope you are doing well,\nI am very worried as I think I'm experiencing a diabetic coma. I have been feeling extremely dizzy and confused. I am also having difficulty staying conscious.\nI'm not sure what to do and I am very scared. My blood sugar levels, which I have been monitoring, have been extremely high. My insulin does not seem to be helping.\nWhat should I do? Should I go to the hospital or wait this out? I dont want to overreact, but I also don't know if I should ignore my symptoms.\nThank you\nPortalGen Generation\nHi Dr,\nI am really scared. I just woke up, on the ground, and have no idea how I got there. I think I may have been in a diabetic coma. I am very nauseous and don't know what to do. I am worried I might slip into one again. Do I need to go to a hospital? My blood sugar has been very difficult to control.\nThanks,\nQualitative Analysis: We notice that in this example, the PortalGen output is much more natural and patient-like. Mixtral-8x7b's generation, conversely, is very formal and unrealistic for a patient experiencing this symptom. ChatGPT seems to attempt use of casual language, but also re-states the prompt in the message, producing an unnatural sounding output.\nOne interesting limitation of Mixtral-8x7b is, across numerous prompts, Mixtral-8x7b relies on a fixed message format where there is a (i) message header (ii) description of symptom (iii) how they have been managing it (iv) request for help. While this can sometimes be appropriate, the models inability to generate messages of varying formats is not optimal."}, {"title": "Appendix B. Experimental Configurations", "content": "B.1. GPT-2 Fine-Tuned on Real Patient Data\nWe use the GPT-2 fine-tuning script provided by Wutschitz et al. (2022) 3 to run our experiment. We fine-tune GPT-2 for 3 epochs on 1,000 real patient messages. We use the prefix 'Patient Message:' before all messages to provide a prompt the model can use for downstream data generation. We use the set of default hyperparameters for standard GPT-2 training from the cited repository.\nWe then generate 1000 random patient messages using this prefix. Each synthetic message is a maximum of 256 tokens long. We use a temperature of 0.75 during generation."}, {"title": "B.2. GPT-2 Fine-Tuned with Differential Privacy (DP)", "content": "We perform the same exact steps as for GPT-2 Fine-Tuned on Real Patient Data, but using the differential privacy training algorithm implementation from Wutschitz et al. (2022). We use the set of default hyperparameters for DP-based GPT-2 training from the cited repository."}, {"title": "B.3. Zero-Shot LLM Prompting", "content": "We load each LLM in 4-bit quantized mode using BitsAndBytes 4. We use Huggingface's 5 text generation pipeline with temperature = 0.75 and max_new_tokens set to 256 for each generation. The prompt used is as follows:\nPretend you are a medical patient. Write a message to your doctor using the prompt:\n### Rules ###\nAssume the doctor you are messaging has been your physician for years. It is permissible to speak informally when appropriate.\nDo not restate the prompt in the message.\nYou may add additional health context (e.g. symptoms or medications) to the message as needed.\nPrompt: [prompt]\nPatient Message:"}, {"title": "B.4. PortalGen", "content": "PortalGen uses the same prompting strategy and hyperparameters, but with 10 real messages in-context. We additionally write prompts for each of the 10 messages that could have in-theory generated them to teach the model how to map from prompt to target message. To promote output diversity, we shuffle the 10 messages in the prompt before each generation. We additionally include a special \"### End Of Message ###' token at the end of each exemplar prompt to encourage the model to write a single message and not a sequence of messages."}, {"title": "B.5. Training GPT-2 From Scratch", "content": "For our perplexity-based experiments, we train a GPT-2 model from scratch using the 1000 synthetic messages from a given generation method. We then compute the perplexity of this model in the presence of 5000 real patient messages. In this setting, we train for GPT-2 for 10 epochs using the same hyperparameters as our standard GPT-2 fine-tuning experiment detailed above."}, {"title": "B.6. Additional Evaluation Details", "content": "We note that throughout all LLM-based experiments, each model uses the exact same set of prompts to ensure fairness. Additionally, all 5,000 test messages used in quantitative evaluation are ensured to have character lengths between 500-1500 characters to ensure the message has sufficient content."}, {"title": "Appendix C. Stage #1 Prompt Generation", "content": "We use few-shot prompting to convert ICD-9 code descriptions into portal message prompts. The prompt used for this task is shown below:\nGiven an ICD9 code for a given patient, write a short description of a message that a patient might send to their doctor which may or may not be related to the code. Here are examples.\nExample Code: [code description]\nExample Message Description:\nTo obtain ICD-9 codes for generating patient message descriptions using this prompt, we randomly sample from a binned distribution of ICD-9 codes presented by patients in our dataset described in Section 2. We bin the distribution of real ICD-9 codes into ICD-9 chapters (e.g. a single chapter would be ICD-9 codes 460-519: diseases of the respiratory system) to generally match a distribution of real patient health problems, then we sample individual codes (e.g. a single ICD-9 code would be 463: tonsillitis, acute) from each chapter uniformly at random. See the code supplement for the full prompt and sampling strategy used for this task."}]}