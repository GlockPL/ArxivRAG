{"title": "RLPF: Reinforcement Learning from Prediction Feedback for User Summarization with LLMs", "authors": ["Jiaxing Wu", "Lin Ning", "Luyang Liu", "Harrison Lee", "Neo Wu", "Chao Wang", "Sushant Prakash", "Shawn O'Banion", "Bradley Green", "Jun Xie"], "abstract": "LLM-powered personalization agent systems employ Large Language Models (LLMs) to predict users' behavior from their past activities. However, their effectiveness often hinges on the ability to effectively leverage extensive, long user historical data due to its inherent noise and length of such data. Existing pretrained LLMs may generate summaries that are concise but lack the necessary context for downstream tasks, hindering their utility in personalization systems. To address these challenges, we introduce Reinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to generate concise, human-readable user summaries that are optimized for downstream task performance. By maximizing the usefulness of the generated summaries, RLPF effectively distills extensive user history data while preserving essential information for downstream tasks. Our empirical evaluation demonstrates significant improvements in both extrinsic downstream task utility and intrinsic summary quality, surpassing baseline methods by up to 22% on downstream task performance and achieving an up to 84.59% win rate on Factuality, Abstractiveness, and Readability. RLPF also achieves a remarkable 74% reduction in context length while improving performance on 16 out of 19 unseen tasks and/or datasets, showcasing its generalizability. This approach offers a promising solution for enhancing LLM personalization by effectively transforming long, noisy user histories into informative and human-readable representations.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have shown great promise for personalized prediction by leveraging historical activity data (Liu et al. 2023; Lyu et al. 2024; Li et al. 2023). However, the inherent noise and length of user history data pose obstacles to their effective utilization in LLM-powered agent systems.\nNatural language user summaries offer several advantages over using raw user activity data. First, they improve inference efficiency over using raw user data due to their compact nature. Second, they offer the potential to improve performance on downstream tasks by distilling user activities and reducing noise. Representing user context through natural language also offers several advantages over embedding-based representations. User representations in the natural"}, {"title": "2 Methodology", "content": "Problem Statement\nConsider a set of users $\\mathcal{U} = {u_i}_{i=1}^{M_1}$, where each user i has an associated chronologically ordered sequence of interactions, denoted as ${v_1, v_2, ..., v_N}$. Each $v_j$ within this sequence (where $1 \\leq j \\leq N$) comprises one or more textual features that describe a specific item, such as the titles or ratings of movies watched by the user. For each user i, we concatenate all of their interactions ${v_j}_{j=1}^{N}$ into a single string to form the user context $u_i$.\nA summarizer model $\\pi_{\\theta}$ takes as input the user context $u_i$ and generates a summary $s_i = \\pi_{\\theta}(u_i)$. The summary is then provided to off-the-shelf LLM to produce a prediction $\\hat{y_i} = P(s_i)$ for a specific downstream task. We optimize $\\pi_{\\theta}$ to generate summaries ${s_i}_{i=1}^{M_1}$ that minimize the expected error between the predictions ${\\hat{y_i}}_{i=1}^{M_1}$ and the ground truth task labels ${y_i}_{i=1}^{M_1}$.\nReinforcement Learning from Prediction Feedback\nIn the context of RL, we formulate summary generation as a Contextual Markov Decision Process (CMDP). In this framework, the state encompasses both the input text and"}, {"title": "3 Experimental Details", "content": "Dataset\nWe conduct experiments on four public datasets grounded in real-world user interactions, encompassing product reviews, movie watching behavior, and location data. We perform training on Amazon Books (He and McAuley 2016), Google Local Review (Yan et al. 2022), MovieLens 2015(Harper and Konstan 2015). Additionally, we utilized another four Amazon Review datasets with different product categories, as well as MovieLens 2003, which features distinct users and movie catalogs compared to MovieLens 2015). See appendix C for dataset details.\nData Generation For each user's interaction data, presented as a chronologically ordered list of activities $u_i \\in \\mathcal{U}$, we randomly select one item as the target for future activity prediction, denoted as $y_i$. We utilize the N activities preceding this target as the past activities ${v_j}_{j=1}^{N}$. $v_j$ represents an item name and rating pair, where item name correspond to movie title for MovieLens, product name for Amazon Review, and place name + city name for Google Local Review, respectively. As previously mentioned, we concatenate ${v_j}_{j=1}^{N}$ to construct the user context $u_i$. To prevent label leakage, the last item in each user's data is reserved as the target item in the test set. Unless otherwise specified, we set N = 50 in our experiments.\nEvaluation\nWe evaluate user summaries along the following three dimensions:\nPredictiveness We gauge the predictiveness of the summaries based on their prediction performance in various downstream tasks. Extending beyond Future Activity Prediction which is used as feedback during training, we incorporated additional tasks of various types to gauge the transferability and generalization capabilities of the generated summaries. These included 19 tasks include user interest reasoning, history activity retrieval, rating prediction, user demographic prediction and open text review generation. Please refer to Appendix G for detailed task definitions as well as their abbreviation used in the paper.\nA frozen instruction tuned Gemini 1.0 Pro model was employed to generate predictions for all downstream tasks. Each summary $s_i$ was fed into the model, and the resulting predictions were evaluated against ground truth labels.\nFactuality, Abstractiveness, and Readability To further assess the intrinsic quality of the generated summaries, we utilize automated evaluation to compare summaries before and after training. This assessment focuses on aspects not explicitly covered by downstream task performance, including Factuality, Abstractiveness, Readability and Overall quality. For each criterion and overall quality, the Auto Rater compares a pair of summaries, with their relative positions"}, {"title": "4 Results", "content": "Target Task Performance\nFigure 2 compares RLPF performance on the Future Activity Prediction task. Across all three datasets, RLPF demonstrates superior or comparable performance to various summarizers, including crafted prompting, a larger summarizer model, and RLAIF. Overall, RLPF outperforms Nano-2 zero-shot summaries by +13.4% improvement, and outperforms RLAIF by +22% on average. Compared to utilizing the full user context (all activities), RLPF achieves an average context length compression of -73.8% while still exhibiting a +12.4% performance gain. Further comparisons with other baselines are provided in the Appendix F, underscoring exceptional capability of RLPF summaries to capture both short-term and long-term user context information.\nFor comparison, we conducted supervised fine-tuning of a Gemini 1.0 Pro model on the same task, reaching 94% accuracy. However, this fine-tuned model exhibited zero performance on other tasks, highlighting its overfitting to"}, {"title": "Transferability and Generalization", "content": "the specific training task. Conversely, RLPF showcased remarkable transferability and generalization capabilities, as demonstrated in the subsequent section.\nTo evaluate the generalizability and adaptability of RLPF for various personalization agent systems, we conducted a comprehensive transferability assessment across a diverse set of unseen tasks and datasets. As shown in Table 1, RLPF summaries consistently exhibited superior transferability compared to zero-shot and RLAIF baselines, demonstrating improvements in 16 and 14 out of 19 total evaluation cases, respectively. These results highlight RLPF's exceptional transferability and its potential to be effectively applied to a wide range of personalization scenarios, particularly when training data is scarce.\nTask Transfer RLPF summaries demonstrated a slight improvement on an unseen retrieval task, common city retrieval on Google Local Review, and performed on par with zero-shot summary on an unseen personalized text generation task, review generation on Amazon Books.\nDataset and Domain Transfer We also evaluated whether an RLPF trained model can generalize to an unseen dataset, either in same domain or a different domain. We used the policy model trained with MovieLens 2015 to generate summaries on MovieLens 2003 and Amazon Movies&TVs dataset and evaluated future movie prediction with the generated summaries. From the results, RLPF model trained on MovieLens 2015, showed improvements on both unseen datasets. Furthermore, the model trained on Amazon Books achieved significant performance gains on"}, {"title": "Intrinsic Evaluation", "content": "Table 2 demonstrates that RLPF summaries consistently outperform zero-shot summaries on all three datasets, as evaluated by the automated rater across all criteria: Factuality, Abstractiveness, and Readability, as well as in the Overall evaluation."}, {"title": "Analysis", "content": "Alternative Policy Model Additionally, we applied RLPF to a policy model initialized from the PaLM-2 XS model, with results presented in Table 3. Mirroring the observations with Gemini 1.0 Nano-2, RLPF summaries based on PaLM-2 XS also exhibited improvements in both the training task (future activity prediction) and the unseen task (favorite genre/category prediction) across all three datasets. A slight drop in performance was noted for favorite genre prediction on the MovieLens 2015 dataset.\nRobustness to Model that Uses Summaries To further ensure that RLPF summaries are not overly tailored to the specific reward model used during training, we employed an additional evaluator model PaLM-2 S to assess their performance. As in previous experiments, RLPF summaries were trained using reward signals derived from Gemini 1.0 Pro. Table 5 demonstrates that the improvements achieved with RLPF summaries transfer effectively to these different evaluator models, highlighting the generalizability of RLPF summaries across various LLM-powered systems.\nImpact of Summary Length Figure 3 illustrates our experiments on MovieLens 2015, where we varied the target length(L) in the length reward term. Generally, longer summaries led to improved task performance but decreased scores in automated evaluation metrics, suggesting a tradeoff between extrinsic utility and intrinsic qualities."}, {"title": "5 Discussion", "content": "Responsible Deployment While RLPF shows promise for enhancing personalization, its use of user data raises privacy and data leakage concerns. Offline training of user summaries and employing a frozen LLM for online serving can mitigate some risks. However, a thorough analysis of potential vulnerabilities is crucial before real-world deployment."}, {"title": "6 Related Work", "content": "Text Summarization\nLeveraging language models to summarize extensive documents has become a prominent area. Two major challenges in this field are the availability of high-quality reference summaries and effective summary evaluation. Traditional summarization approaches often involve supervised fine-tuning of a model using reference summaries as prediction labels (Cohan et al. 2018; He et al. 2022, 2023; Kry\u015bci\u0144ski et al. 2021). Generated summaries are then evaluated against these references through lexical matching (Lin 2004) or embedding-based similarity metrics (Zhang et al. 2020). However, these methods are inapplicable when reference summaries are unavailable or of subpar quality, a common issue in many popular summarization datasets.\nPrevious work also explored metrics in the absence of reliable reference summaries. This work leveraged question-answering(QA) (Durmus, He, and Diab 2020; Fabbri et al. 2022; Deutsch, Bedrax-Weiss, and Roth 2021), pre-trained model (Kryscinski et al. 2020; Goyal and Durrett 2020), and human eval (Goyal, Li, and Durrett 2023) to assess factuality, coherence, and abstractiveness. However, these methods exhibited low correlation with human judgments (Fabbri et al. 2021) or were prohibitively expensive.\nThere is also prior work that leverages RL to enhance the summary quality by designing reward signals aligned with specific summarization objectives. Existing RL methods have explored using similarity scores between answers generated from reference and generated summaries as reward signals (Gunasekara et al. 2021), or directly incorporating human annotation as feedback (Stiennon et al. 2020). Yet, these approaches remain reliant on either reference summaries or human labeling. Other techniques have aimed to improve specific summary criteria, like factuality, through specialized reward models (Roit et al. 2023). It is worth pointing out these methods often involve a supervised fine-tuning stage with reference summaries, thus not being entirely reference-free.\nOur work diverges from traditional text summarization by focusing on generating abstractive and factual user insights for downstream personalization tasks. We leverage LLMs to produce novel summaries beyond simple extraction from user history.\nUser Modeling\nUser modeling has benefited significantly from LLM-driven advancements, notably in personalized ranking and retrieval. Many existing LLM approaches employ prompting or fine-tuning for specific tasks (Bao et al. 2023; Wu et al. 2024b; Liu et al. 2023; Lyu et al. 2024; Li et al. 2023; Salemi et al. 2024). However, the length and complexity of user interaction data, such as extended histories, can introduce noise and hinder processing efficiency. To address the issue, some work has focused on using pre-trained or co-trained user embeddings to condense user history information (Ning et al. 2024; Doddapaneni et al. 2024). In contrast to these embedding-based representations, our method employs concise, natural language-based user summaries. These summaries are human-readable, promoting better interpretability, explainability, and reusability across diverse LLMs.\nUser summaries are essentially user profiles extracted from extensive activity history, capturing user personality and preferences. Prior studies have primarily focused on investigating different prompting techniques (Rao, Leung, and Miao 2023; Ji et al. 2023; Wu et al. 2024a), evaluating performance directly against ground truth user profile labels. While our work focuses on generating comprehensive summaries that benefit a broad range of downstream tasks, rather than a single user characteristic.\nReinforcement Learning from AI Feedback\nRL from Human Feedback (RLHF) (Ouyang et al. 2022) aligns language models with human values through reinforcement learning guided by extensive human feedback. To address the reliance on high-quality human labels in RLHF, recently, RL from AI Feedback(RLAIF) (Bai et al. 2022) and its variants (Yang et al. 2023) have emerged as promising alternatives. These approaches levarage a powerful off-the-shelf LLM to generate preferences in lieu of human annotator and achieve superior performance to RLHF on tasks such as summarization (Lee et al. 2024). Along this direction, there is also previous work (Kwon et al. 2023) that demonstrated the use of a frozen LLM as a proxy reward function for few-shot in-context learning to fine-tune a non-LLM-based RL agent."}, {"title": "7 Conclusions", "content": "We introduced RLPF, a novel method for generating concise and human-readable user summaries from raw activity data. RLPF leverages readily available LLMs and downstream task performance as reward signals, overcoming challenges in traditional summarization approaches. Our experiments demonstrate superior performance, context compression, and generalizability across unseen tasks. Future work will extend RLPF to more complex scenarios, incorporate additional feedback mechanisms, and explore its application to other LLM capabilities."}]}