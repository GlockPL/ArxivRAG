{"title": "DEEP TREE TENSOR NETWORKS FOR IMAGE RECOGNITION", "authors": ["Chang Nie", "Junfang Chen", "Yajie Chen"], "abstract": "Originating in quantum physics, tensor networks (TNs) have been widely adopted as exponential machines and parameter decomposers for recognition tasks. Typical TN models, such as Matrix Product States (MPS), have not yet achieved successful application in natural image processing. When employed, they primarily serve to compress parameters within off-the-shelf networks, thus losing their distinctive capability to enhance exponential-order feature interactions. This paper introduces a novel architecture named Deep Tree Tensor Network (DTTN), which captures 2-order multiplicative interactions across features through multilinear operations, while essentially unfolding into a tree-like TN topology with the parameter-sharing property. DTTN is stacked with multiple antisymmetric interacting modules (AIMs), and this design facilitates efficient implementation. Moreover, we theoretically reveal the equivalency among quantum-inspired TN models and polynomial and multilinear networks under certain conditions, and we believe that DTTN can inspire more interpretable studies in this field. We evaluate the proposed model against a series of benchmarks and achieve excellent performance compared to its peers and cutting-edge architectures. Our code will soon be publicly available.", "sections": [{"title": "1 INTRODUCTION", "content": "The wavefunction of a quantum many-body system typically resides in an extremely high-dimensional Hilbert space, with its complexity increasing exponentially as the particle count grows [Jiang et al., 2008, Zhao et al., 2010]. For example, consider a system consisting of N spin- particles; the dimensionality of the corresponding Hilbert space would be 2N. Tensor networks (TNs) offer powerful numerical techniques for tackling the \"Curse of Dimensionality\" [Cichocki et al., 2016]. By leveraging the local entanglement properties of quantum states, TNs represent complex wavefunctions into multilinear representations of multiple low-dimensional cores, thereby significantly reducing computational and storage requirements\u00b2 [Cichocki, 2014, Ran et al., 2020]. This class of methods allows for an accurate representation of quantum states while mitigating the exponential growth in complexity, making it feasible to simulate large-scale quantum systems [Jaschke et al., 2018, Ran et al., 2020].\nRecently, TN-based interpretable and quantum-inspired white-box machine learning has attracted the attention of researchers. It holds the potential to generate novel schemes that can run on quantum hardware [Huggins et al., 2019, Ran and Su, 2023]. Typical TN models, including Matrix Product States [Cirac et al., 2021] (MPS\u00b3), Tree Tensor Network (TTN) [Cheng et al., 2019], and Multi-Scale Entanglement Renormalization Ansatz (MERA) [Reyes and Stoudenmire, 2020] are skillfully applied in image classification. Routine practice is to map each pixel or local patch of an image to a d-dimensional vector by a local map function $(\u00b7), and then use the tensor product to obtain a joint feature map \u03a6(x) of dN dimensionality (see Fig.1 left). The TN model used for image classification can be expressed as follows:\n\nf(x) = arg max Wm * \u0424(x).\n\nm\nHere, Wm represents a (N + 1)-th order tensor with an output index m; f(\u00b7) : Rwhc \u2192 Rm denotes a multilinear function. In principle, mapping samples into exponential dimensional space to achieve linear separability instead of adopting activation functions is the essence of TNs [Selvan and Dam, 2020, Ran and Su, 2023, Patra et al., 2024]. However, existing methods are limited to simple tasks, e.g., MNIST and Fashion MNIST [Cheng et al., 2019, Ran and Su, 2023], and we reveal that is mainly due to 1) low computational efficiency and 2) lack of feature self-interaction capability (see section 3.3 for more details). Hence, we aim to address the above challenges and apply TNs to complex benchmarks, thus bridging the gap between TNs and advanced architectures.\nOn the other hand, TNs are popularly employed to parameterize existing network models for acceleration. For example, networks' variational parameters can directly decompose into TN formats, including convolutional kernels [Nie et al., 2023], fully-connected layers [Li et al., 2023, Nie et al., 2022], and attention blocks [Liang et al., 2024], to name a few. Retaining or merging the parameter-wise TN structure during the model inference is free. However, such techniques are devoid of probabilistic interpretation and feature multiplicative interaction. Notably, advanced deep learning architectures and TNs have unexplored striking similarities, including MLP-Mixer [Tolstikhin et al., 2021], polynomial and multilinear networks [Chrysos et al., 2021, 2023, Cheng et al., 2024]. They have already achieved excellent results across complex visual tasks. As shown in Fig.2, we visualize different architectures from the TN perspective for comparison. We observe that modern architectures differ from existing TNs in several key aspects, such as the use of nonlinear activations and instance normalization. Therefore, our goal is to transfer the advantages of advanced architectures to DTTN through equivalence analysis. Thus break through the limitations of TNs for complex benchmarks.\nConcretely, we introduce a novel class of TN architectures, named Deep Tree Tensor Network (DTTN), which uses multilinear operations to capture exponential-order interactions among features and predict targets without activation functions and attention components. DTTN is sequentially stacked by multiple simple antisymmetric interacting modules (AIM) and inherently unfolds into a tree-like TN structure to reach on-par performance compared to advanced architectures, with better interpretability and understanding. Overall, the contributions of this paper are summarized as follows:\n\u2022 We introduce DTTN, a simple yet effective architecture constructed by sequentially stacking AIMs. DTTN captures feature 2 multiplicative feature interactions without activation functions and achieves state-of-the-art performance compared to other polynomial and multilinear networks with faster convergence (see Fig. 2).\n\u2022 We provide a theoretical analysis of the equivalency between DTTN and other architectures under specific conditions. Without instance normalization, DTTN essentially transforms into a tree-topology TN, thereby overcoming the limitations of TN-based Born machines that excel mainly in simple tasks.\n\u2022 We comprehensively evaluate the performance and broader impact of the proposed model across multiple benchmarks, demonstrating that DTTN performs on par with well-designed advanced architectures."}, {"title": "2 RELATED WORK", "content": "Quantum-inspired tensor networks (TNs) enhance the performance of classical algorithms by emulating quantum computational characteristics [Huggins et al., 2019]. These methods map inputs into a Hilbert space with exponential dimensions, achieving linear separability through local mappings and tensor products, while employing multiple low-order cores to parameterize coefficients, significantly reducing computational and storage complexity [Selvan and Dam, 2020, Stoudenmire and Schwab, 2016]. The avoidance of activation functions, alongside the theoretical underpinnings rooted in many-body physics, contributes to the interpretability of TNs [Ran and Su, 2023]. Recently, numerous studies have successfully applied TNs to tasks such as image classification [Stoudenmire and Schwab, 2016], generation [Cheng et al., 2019], and segmentation [Selvan et al., 2021], effectively integrating established neural network techniques like residual connection [Meng et al., 2023], multiscale structure [Liu et al., 2019], and normalization [Selvan and Dam, 2020] into TN frameworks. However, current TNs are predominantly suited for simpler tasks and face limitations in terms of computational efficiency and expressive power."}, {"title": "2.2 Advanced Modern Networks.", "content": "In the modern deep learning field, the design of network architectures has become increasingly complex and diverse, each showcasing unique advantages. Advanced models such as Convolutional Neural Networks (CNNs) [He et al., 2016, Hou et al., 2024] with efficient feature extraction, Transformers with powerful context understanding [Vaswani, 2017], MLP-architectures [Tolstikhin et al., 2021] with elegant yet effective designs, and Mamba [Gu and Dao, 2023] with linear complexity all play crucial roles in various applications. These networks rely on various types of activation functions [Apicella et al., 2021] to introduce nonlinear characteristics, but this reliance also limits their application in the fields of security and encryption, e.g., Leveled Fully Homomorphic Encryption supports solely addition and multiplication as operations [Brakerski et al., 2014, Cheng et al., 2024]."}, {"title": "2.3 Polynomial and Multilinear Networks.", "content": "Polynomial and multilinear networks employ addition and multiplication operations to construct intricate network representation [Chrysos et al., 2021, 2023, Cheng et al., 2024]. Specifically, the pioneering polynomial network (PN) [Chrysos et al., 2021] modularly constructs higher-order expanded polynomial forms of inputs and then training them end-to-end. It has shown good results in image recognition and generation tasks. The study [Chrysos et al., 2023] introduces regularization terms for PN, incorporating techniques like data augmentation, instance normalization, and increased feature interaction orders to enhance model performance. Cheng et al. drew inspiration from contemporary architectural designs to propose MONet [Cheng et al., 2024], aiming to narrow the gap between multilinear networks and state-of-the-art architectures. It is important to note that both polynomial and multilinear networks can capture interactions of features at exponential orders, though the primary distinction lies in their structural flexibility: polynomial networks maintain an unfoldable structure, whereas multilinear networks lose this property when instance normalization is applied, as\n\nAz * Bz = vec(z \u2297 z)(AT \u2297 BT),\n\nLN(Az) * (Bz) \u2260 vec(z \u2297 z)LN(AT \u2297 BT).\n\nHere, LN represents layer normalization, while A and B denote learnable matrixes. Our objectives include re-establishing the polynomial expansion form and analyzing the distinctions between DTTN and quantum-inspired TNs. Additionally, by integrating layer normalization, we aim to develop a multilinear DTTN\u2020 that surpasses the performance of current high-performing multilinear networks [Cheng et al., 2024]"}, {"title": "3 METHOD", "content": "In this section, we provide a detailed description of the Deep Tree Tensor Network (DTTN). We aim to construct a tree topology network by sequentially stacking AIM blocks, which involve solely multilinear operations. For an input image x, similar to other methodologies, we use a vanilla linear projection, also referred to as patch embedding [Tolstikhin et al., 2021], to derive the feature map (x, \u039b\u2084) \u2208 RW\u00d7H\u00d7C. Here A\u00a2 \u2208 RS2\u00d7C represents a learnable matrix, where S, C\u2208 N denote the local patch size and the number of output channels, respectively. This procedure corresponds to the local mapping illustrated in Fig.1, with its output serving as the input for the DTTN. It should be noted that batch normalization (BN) operations following the linear layer have been omitted for brevity. This omission does not affect the conclusions drawn in this paper, as these operations can be integrated with the nearest linear layer during inference through structural re-parametrization techniques [Ding et al., 2021]."}, {"title": "3.1 Antisymmetric Interaction Module", "content": "The antisymmetric interaction module (AIM) is the core of DTTN. As illustrqated in Fig. 2(d), for the l-th block input feature map X\u00b9 \u2208 RW\u0131\u00d7H\u2081\u00d7C\u0131, we utilize an antisymmetric two-branch structure to capture the linear interactions of the input features separately. Both branches, denoted as f1, f2, incorperate a depthwise convolution layer and a linear layer but apply them in reverse order. These layers are designed to capture spatial locality and channel interactions, respectively, effectively leveraging the advantages of CNN and MLP architectures [He et al., 2016, Tolstikhin et al., 2021]. Furthermore, the antisymmetric design aims to reduce the complexity of AIM. The ratio of parameters and FLOPs between the two branches is given by:\n\nRPara =\n\nrexpk2C\u0131 + rexpC?\n\nrexpk2C1 + rexpC?\n\n~\n\n1\n\nTexp\n\nRFlops =\n\nrexpk2WiHiCi + rexpWiHiC?\n\nrexpk2WiHiC\u0131 + r\u00b2xpWiHiC?\n\n~\n\n1\n\nTexp,\n\nwhere rexp \u2208 N+ represents the expansion ratio in the inverted bottleneck design, typically set to 3, k \u2208 N+ denotes the kernel size, and C\u03b9, W\u03b9, \u0397\u2081 are the number of channels, width and height of the l-th block input feature map respectively. We employ the hardware-friendly Hadamard product to capture the second-order interactions of the branch outputs, where * symbolizes an element-wise product. Following this, an optional LN and linear projection layers are sequentially applied to the computation results. Finally, a shortcut connection is used to preserve the input signal and accelerate training. (see Fig.2). Overall, the AIM forward calculation can be expressed as\n\nx\u00b2+1 = x\u00b2 + Pro (f1(x') * f(x')), for l\u2208 KL.\n\nIn summary, AIM captures second-order multiplicative interactions among input elements through multilinear operations without employing nonlinear activations. In contrast to blocks inside other architectures, such as the Basic Block in R-PolyNets [Chrysos et al., 2023] and Mu-layer\u2074 in MONet [Cheng et al., 2024], AIM employs an antisymmetric design with only one shortcut connection."}, {"title": "3.2 NETWORK ARCHITECTURE", "content": "Our proposed architecture involves stacking L AIM blocks sequentially. The final output is derived through average pooling and a fully-connected layer (refer to Fig.3). The DTTN architecture is multi-stage, similar to other existing architectures [Chrysos et al., 2021, Hou et al., 2022, Cheng et al., 2024]. By varying the hidden-sizes and the number of blocks in each stage, we have designed three variants of DTTN: DTTN-T, DTTN-S, and DTTN-L (see Table 1), each with distinct parameters to facilitate a comparative analysis. At a high level, we assert that the DTTN possesses the following characteristic:\nProposition 1. The DTTN has the capability to capture 2\u0139 multiplicative interactions among input elements, which can be represented in the format of Equation 1 as P(x) =\n\n\n2 (x, A\u2084). Consequently, the elements of f(x) are\n\nhomogeneous polynomials of degree 2L over the feature map (x, A4).\nIt should be noted, however, that this property does not hold when LN is applied within the network, introducing second-order statistical information, as seen in models like MONet and DTTN\u2020. In such cases, the network reverts to functioning as an ordinary multilinear network.\nUnfolding Topology. As illustrated in Fig. 3, the AIM is equivalent to a binary tree node without LN. At this time, AIM forward computation can be regarded as a TN contraction, which can be represented as\n\nx\u00b2+1 =x\u00b2 + B\u00b2 ((A\u2081x') * (A\u2082x'))\n\n=x\u00b2 + Reshape (B'(AA)) x (x\u00b2x')\n\n1,2\n\n\n=Cx(x\u00b2 \u00d72,3x')\n\nHere x' = vec(X') \u2208 RWiHiC\u0131, C\u0131 signifies a 3-th order tensor that is structural re-parametrization with the learnable matrixes A, A and B\u00b9. Conclusively, the AIM captures the second-order multiplicative interactions among input elements via a tensor product and structures the 3-order tensor utilizing PyTorch operators. Thus, a DTTN comprised of L AIMs can essentially be unfolded into a tree network with 2 leaf nodes, as illustrated in Fig.1."}, {"title": "3.3 DTTN vs. Other Architectures", "content": "DTTN & Polynomial Networks. DTTN shares the same polynomial expansion form as [[-Net [Chrysos et al., 2021], which can be formulated as\n\nf(x) =\n\n\n2L (W[l] \u03c6('$'(x))) + \u03b2,\n\nwhere \u03b2\u2208 Rm represents a bias term, and W[] is a (l + 1)-th order learnable parameter tensor. It is worth noting that equations (1) and (6) become equivalent when a bias term is introduced for the elements of input x. The networks differ in the structured representation of the coefficients W[l], l \u2208 K21. Specifically, these two networks feature distinct network blocks and computational graphs.\nDTTN & Multilinear Networks. We remark that networks that solely involve multilinear operations, such as MONet and DTTN\u2020 and R-PolyNets, but lose the polynomial expansion formalism can be categorized as multilinear networks. Additionally, both Polynomial Networks and TNs fall under the category of multilinear networks.\nDTTN & Quantum-inspired TNs. The primary advantages of DTTN over Quantum-inspired TNs are twofold. (1) The alternative of the local map function. Existing TNs utilized trigonometric functions for local mapping, which resulted in each term of the network's unfolded form lacking higher-order (\u2265 2) terms of the input elements, thus losing the capability of feature self-interaction. (2) The higher bond dimension induced by parameter-sharing properties. Quantum-inspired TNs parallelize the contraction of shared indexes of among N cores within the same layer, but limited memory resources restrict the bond dimension to small values. We further explore the connection between DTTN and TNs in the following theorem.\nTheorem 1. Given the local mapping function $\u00b2\u00b9 (x1) = [x,\uff65\uff65\uff65, x\u00b2]T, a polynomial network with the expansion form of Equation 6) can be transformed into a quantum-inspired TN model with finite bond dimension.\nConsidering that the internal cores of a TN can be decomposed into a \"core&diagonal factor&core\" form via higher-order Singular Value Decomposition (HOSVD) [Kolda and Bader, 2009] and merged with connectivity cores, we ignore the structural difference between DTTN and Quantum-inspired TNs. We believe this theorem not only establishes equivalence between quantum-inspired TNs and modern architectures but also provides guidance for the further development of interpretable and high-performance TNs."}, {"title": "4 EXPERIMENTS", "content": "In this section, we provide a comprehensive assessment of the DTTN's effectiveness. Specifically, in Section 4.1, we perform experiments on a series of image classification benchmarks to validate the model's superiority over other multilinear and TN architectures. In Section 4.2, we demonstrate the broader impact of DTTN's feature interactions across two downstream tasks. Section 4.3 presents ablation studies to further dissect the design choices. We conclude with a discussion of the model's limitations."}, {"title": "4.1 Image Classification Benchmarks", "content": "Setup and Training Details. A series of benchmarks with different types, scales, and resolutions is employed for the experiments, including CIFAR-10 [Krizhevsky et al., 2009], Tiny ImageNet [Le and Yang, 2015], ImageNet-100 [Yan et al., 2021], ImageNet-1k [Russakovsky et al., 2015], MNIST, and Fashion-MNIST [Xiao, 2017]. A detailed description of the benchmarks and training configurations is provided below.\n\u2022 CIFAR-10 [Krizhevsky et al., 2009] consists of 60K color images of 32 \u00d7 32 resolution across 10 classes, with 50K images for training and 10K for testing. The model is trained using the SGD optimizer with a batch size of 128 for 160 epochs. The MultiStepLR strategy is applied to adjust the learning rate, and data augmentation settings are in accordance with [Chrysos et al., 2023].\n\u2022 Tiny ImageNet [Le and Yang, 2015], ImageNet-100 [Yan et al., 2021], and ImageNet-1k [Russakovsky et al., 2015] contain 100k, 100k, and 1.2M color-annotated training images with resolutions of 64 \u00d7 64, 224 \u00d7 224, 224 \u00d7 224 pixels, respectively. These datasets are commonly used as standard benchmarks for image recognition tasks. During training, we optimized our model using a configuration consistent with [Cheng et al., 2024, Tolstikhin et al., 2021]5. Specifically, we utilized the AdamW optimizer [Loshchilov, 2017] alongside a cosine decay schedule for learning rate tuning, and applied data augmentations including label smoothing, Cut-Mix, Mix-Up, and AutoAugment. Note that we did not employ additional large-scale datasets such as JFT-300M for pre-training, nor did we use un- or semi-supervised methods to optimize our model. All experiments were conducted on 8 GeForce RTX 3090 GPUs using native PyTorch.\n\u2022 Both MNIST and Fashion-MNIST [Xiao, 2017] contain 60,000 grayscale images for training and 10,000 for validation, with each image sized at 28 \u00d7 28. We conducted comparison experiments between DTTN and other TN models on these two benchmarks, employing training configurations consistent with those used for CIFAR-10."}, {"title": "4.2 Broader Impact", "content": "Image Segmentation. We employ the Semantic FPN framework [Kirillov et al., 2019]6 to perform semantic segmentation on the ADE20K dataset, which includes 20,000 training images and 2,000 validation images. DTTN\u2020-S and DTTN\u2020-L were initialized using pre-trained ImageNet-1k weights before being integrated as the backbone of the framework. Additionally, all newly added layer weights were initialized using Xavier initialization [Glorot and Bengio, 2010]. Table 5 presents the outcomes of the DTTN model trained for 12 epochs using the AdamW optimizer, evaluated by Mean Intersection over Union (mIoU). Some experimental data are derived from [Cheng et al., 2024]. Notably, DTTN\u2020-L achieves the highest performance among all multilinear networks, underscoring the efficacy of our simply designed AIM.\nFeature Interactions in Recommendations. To further illustrate the advantage of AIM in enhancing feature interactions, we selected two recommendation-related datasets, Criteo and Avazu, given the importance of feature interactions in recommender systems [Lian et al., 2018]. AIM was incorporated as a pluggable module to enhance existing Click-Through Rate (CTR) modeling capabilities, including those of DeepFM, FiBiNet [Huang et al., 2019], and DCN-V2 [Wang et al., 2021]7. In this implementation, AIM replaced all linear layers in the target models; internal convolution operations were removed, leaving only linear and normalized layers. The experimental validation results, reported in Table 6 and evaluated by AUC (Area Under the ROC Curve), show that AIM consistently improves performance across both datasets for all CTR models. Specifically, FiBiNet achieved a 0.55% improvement on the Criteo dataset."}, {"title": "4.3 Ablation Study", "content": "Here, we conduct ablation studies to evaluate the effectiveness of various design choices, including network depth and width, the antisymmetric design of AIM, and layer normalization. We utilize ImageNet-100 as our test benchmark, a representative subset of ImageNet-1K suitable for model validation and hyperparameter tuning within resource constraints. Throughout these experiments, all hyperparameters were kept consistent with those of the final model, except for the variable under investigation, aligning with the configuration detailed in Section 4.1.\nDepth and Width. To control the depth and width of DTTN, we vary the number of AIM blocks and the hidden-size, denoted by L and d, respectively, for brevity. For ease of comparison, d was set constant across different stages. As illustrated in Table 7, we first set L to {8, 16, 24, 32} with d = 256, and subsequently varied d to {64, 128, 256, 512} while fixing L at 32. This approach allows us to explore the impact of network depth and width on model performance. Our observations indicate that model performance sharply degrades with decreases in both L and d, particularly with respect to width; Top-1 accuracy drops by 24.5% when d decreases from 512 to 64. Furthermore, model parameters exhibit linear growth with L and quadratic growth with d. These findings indirectly validate the efficacy of the multi-stage design of DTTN.\nAntisymmetrical Design of AIM. We evaluate the effectiveness of the antisymmetric design of AIM on the DTTN\u2020-T and DTTN-S architectures. As shown in Table 8, we compare the impact of symmetric versus antisymmetric designs on model performance. In this context, SIM-Conv and SIM-Linear denote the Symmetric Interaction Module (SIM) with convolutional- and linear prioritized layers, respectively, used as replacements for AIM. It is observed that DTTN-T achieves the best results among its counterparts, while DTTN\u2020-S performs only 0.1% lower than SIM-Conv but has nearly 20% fewer parameters. Furthermore, the reduction in the number of parameters aligns with the theoretical predictions corresponding to Equation 3.\nImportance of LN. Figures 2(d) and 3 demonstrate the critical role of the Layer Normalization (LN) layer in facilitating the unfoldable nature and expressiveness of the network a key technique for enhancing the performance of multilinear networks [Chrysos et al., 2023, Cheng et al., 2024]. Table 9 reports the improvements in DTTN variants after incorporating LN, showing gains of 1.8%, 0.4%, and 0.5%, respectively. Moreover, by combining the data from Figure 2(e) and Table 2, it can be seen that DTTN-S exhibits a Top-1 accuracy that is 5.6% and 2.2% lower than DTTN-S after training for 90 and 300 epochs, respectively. This further underscores the importance of LN in boosting the performance and convergence of our proposed architecture.\nLimitations. Although we have conducted an extensive study of the effectiveness and advantages of DTTN, limitations in computational resources and time constraints prevented us from performing additional experiments. These include pre-training on larger datasets such as JFT-300M, integrating physics-informed networks with AIM, and assessing model robustness. Future work will involve theoretical analysis of the proposed model and the exploration of multilinear transformer architectures that achieve linear complexity. We believe this novel architecture has significant potential for societal benefit through enhanced effectiveness, interpretability, and energy efficiency."}, {"title": "5 CONCLUSION", "content": "This paper introduces a multilinear network architecture named DTTN, which bridges the gap between quantum-inspired TN models and advanced architectures, uniquely achieving this without activation functions. Specifically, DTTN employs a modular stacking design to capture exponential interactions among input features, essentially unfolding into a tree-structured TN. We conducted extensive experiments to showcase the effectiveness of DTTNs, including comparisons with polynomial and multilinear architectures, modern neural networks, and TNs across various recognition benchmarks. Notably, this is the first validation of TNs' effectiveness on large-scale benchmarks, yielding competitive results compared to advanced architectures. Additionally, we explored the broader applicability of DTTNs in segmentation tasks and as a plug-and-play module within recommendation systems. In summary, DTTNs leverage exponential feature interactions to achieve superior performance across multiple domains, entirely without the need for activation functions or attention mechanisms."}, {"title": "APPENDIX", "content": "In Algorithm 1, we provide a PyTorch-style implementation of AIM. The primary distinction between the training and inference stages is structural re-parameterization, specifically, the integration of the Batch Normalization (BN) layers with the adjacent convolutional or linear layers. During inference, the propagation process of AIM simplifies, aligning well with the schematic overview depicted in Fig. 2(d)."}, {"title": "B Complexity Analysis", "content": "Here we analyze the complexity of DTTN including storage and computation. Without loss of generality, we consider the multi-stage DTTN with hidden-size d for each stage, and the total depth is L. We then consider the complexity of the main components of the network, including the local mapping, the core blocks, and the classification head separately.\nIn this section, we analyze the complexity of a DTTN, focusing on both storage requirements and computational cost. Without loss of generality, we consider a multi-stage DTTN where each stage has a hidden size of d, and the overall depth is L. We then examine the complexity associated with the main components of the network: the local mapping function, the core blocks, and the classification head.\nLocal mapping. The local mapping function \u03c6(\u00b7) consists of two consecutive convolutional layers, each with a kernel size and stride of 2. This function is applied to the input image x to produce a feature map \u03a6(x, \u039b\u2084) \u2208 RW\u00d7H\u00d7C Here, A \u2208 RS2\u00d7C represents a learnable matrix, where S,C = d \u2208 N denote the local patch size and the number of output channels, respectively. The parameters and floating-point operations (FLOPs) involved in this process are given by:\n\nParamL = O(14 \u00b7 d + 4 \u00b7 d\u00b2),\n\nFLOPSL = O(48 \u00b7 d \u2022 WH + 4 \u00b7 d\u00b2 \u00b7 WH).\n\nCore blocks. we analyze the complexity associated with the core AIM blocks within the DTTN architecture. Assuming that each of the four stages contains an equal number of blocks, the corresponding feature map sizes are W \u00d7H,, \u00d7, and \u00d71, respectively. Our analysis focuses on the parameters and FLOPs involved in the three linear layers and two convolutional layers of AIM. The total parameters and FLOPs can be expressed as follows:\n\nParamsAIM =\n\n\n3)\u3002\n\ns=0\n\n22\u00b7rexd+ (2\u00b7rexd + reap) \u00b7 d\u00b2 + d) \u00b7\n\nL\n4,\n\n= O ((22\u00b7 rexp + 2\u00b7 (rexp + rexp)\u00b7d + 1) \u00b7 d \u00b7 L),\n\nFLOPSAIM =\n\nO (\n\n\n3\u03a3\n\n(\n\n18. rexpd.\n\n+\n\n+(2 rexp+rexp) d\u00b2 .\n\n)\n\n4,\n\ns=0\n\nWH\n4s\n\n(\n\n765\n128\n\nTexp +\n\n85\n\nrexp)\n\nd.d. WH I \u00b7 L\n\n)).\n\nClassification head. For a classification head with m classes, we receive a feature map of size \u00d7 \u00d7 d, which is then processed through an average pooling layer followed by a fully-connected layer to output an m-dimensional vector. The parameters and FLOPs involved in this process are given by:\n\nParamsH = O(d\u00b7 (m + 1)),\n\nFLOPSH = O( d+m\u00b7d).\n\n64\nIn summary, our conclusions regarding the number of parameters and FLOPs for the DTTN architecture can be expressed as:\n\nParams = ParamsL + ParamsAIM + ParamsH,\n\nFLOPS = FLOPSL + FLOPSAIM + FLOPSH.\n\nIt can be observed that the number of parameters in the DTTN architecture remains fixed, while the computational complexity exhibits a linear relationship with the input image scale. This characteristic represents a significant advantage of the DTTN over modern MLP and Transformer architectures, which typically exhibit quadratic complexity."}]}