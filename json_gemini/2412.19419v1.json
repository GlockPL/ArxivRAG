{"title": "Introduction to Graph Neural Networks: A Starting Point for Machine Learning Engineers", "authors": ["James H. Tanis", "Chris Giannella", "Adrian V. Mariano"], "abstract": "Graph neural networks are deep neural networks designed for graphs\nwith attributes attached to nodes or edges. The number of research\npapers in the literature concerning these models is growing rapidly\ndue to their impressive performance on a broad range of tasks. This\nsurvey introduces graph neural networks through the encoder-decoder\nframework and provides examples of decoders for a range of graph ana-\nlytic tasks. It uses theory and numerous experiments on homogeneous\ngraphs to illustrate the behavior of graph neural networks for different\ntraining sizes and degrees of graph complexity.", "sections": [{"title": "1 Introduction", "content": "Relationships within data are important for everyday tasks like internet search and\nroad map navigation as well as for scientific research in fields like bioinformatics.\nSuch relationships can be described using graphs with real vectors as attributes\nassociated with the graph's nodes or edges; however, traditional machine learning\nmodels operate on arrays, so they cannot directly exploit the relationships. This\nreport surveys Graph Neural Networks (GNNs), which jointly learn from both edge\nand node feature information, and often produce more accurate models. These\narchitectures have become popular due to their impressive performance on graph\nanalysis tasks. Consequently, the number of research papers on GNNs is growing\nrapidly, and many surveys exist.\nSome surveys discuss graph neural networks in the context of broad families\nsuch as graph networks, graph representation learning and geometric deep learning\n[1, 2, 3, 4, 5]. Other surveys categorize GNNs by abstracting their distinguishing\nproperties into functional relationships [6, 7, 8, 3, 9]. Although useful for organiza-\ntional purposes, generality and abstraction can be difficult to understand for those\nnew to the field. Other surveys have a narrow focus, for example to discuss efforts\nto improve a specific weakness in GNN architectures [10], or to survey GNN work on\na particular task, such as fake news detection or product recommendation [11, 12].\nWhile valuable for those interested in the task, they provide little background in\nGNNs and therefore assume the reader already has that knowledge.\nFor this reason, a concrete and concise introduction to GNNs is missing. We\nbegin by introducing GNNs as encoder-decoder architectures. To provide perspec-\ntive on the ways GNNs are used, we discuss common GNN applications along with\nexamples of task-specific decoders for turning features into predictions. We think\nthat studying a few important examples of GNNs well will help the reader develop\na feeling for the subject that would be difficult to achieve otherwise. We there-\nfore focus on three convolutional and attentional networks, GCN, GraphSAGE,\nand GATv2, which are commonly used both as benchmarks and as components in\nother GNN architectures. We conduct numerous experiments with these GNNs at"}, {"title": "2 Common applications", "content": "Graph neural networks are suited to a variety of graph tasks.\n1. Node classification\nThis task concerns categorizing nodes of a graph. There are several appli-\ncations within the space of social networks, such as assigning roles or in-\nterests to individuals or predicting whether individuals are members of a\ngroup [13, 14]. Node classification tasks also include classifying documents,\nvideos or webpages into different categories [15, 16]. There are also important\napplications in bioinformatics, such as classifying the biological function of\nproteins (nodes) and their interactions (edges) with other proteins [17].\n2. Link prediction\nLink prediction is a classification task on pairs on nodes in a graph. Most\noften, this is a binary classification problem, where the task is to predict\nwhether an edge exists between two nodes, e.g. one to predict that an edge\nis present and zero to predict that it is absent. Link prediction also exists for\ngraphs with multiple edge types, so edges are predicted to be one of several\ntypes [18].\nLink prediction can predict the presence of a relationship (edge) between two\nindividuals (nodes) in a social network, either presently or in the near future\n[19]. Recommendation systems try to recommend products to customers; this\ntask is a link prediction problem, where one seeks edges between two differ-\nent types of nodes, the product nodes and the customer nodes [20, 21]. Link\nprediction for entity resolution predicts links between different records in a\ndataset that refer to the same object [22, 23]. For example, we want to link\na record describing \"John Smith\" with another record for the same person\nwritten \"Smith, John\". In bioinformatics, link prediction can predict rela-\ntionships between drugs and diseases [24] and the similarity between diseases\n[25]. Link prediction also includes finding new relationships between nodes\nin knowledge graphs, a task called knowledge graph completion [26, 27].\n3. Community detection"}, {"title": "3 Introduction to encoder-decoder models", "content": "An attributed graph has a set of nodes, N, as well as edges that define how the\nnodes relate to each other. To simplify the discussion, we restrict our attention to\nundirected graphs, so the edges are represented by a weighted, symmetric adjacency\nmatrix, A = (Aij) where i, j \u2208 N. An entry Aij is non-zero if an edge connects\nnode i to node j and zero otherwise. Each node, i \u2208 N has an attribute, xi \u2208 Re\nfor some l\u2208 N. Encoder-decoder models on graphs is a class of machine learning"}, {"title": "3.1 Encoder-decoder framework", "content": "Many machine learning models adhere to an encoder-decoder framework shown in\nFigure 1. The encoder is a function\nEnc: N\u2192 Re\n(1)"}, {"title": "3.2 Shallow embedding examples", "content": "We now present several representative examples of models that produce embedding\nlookups for nodes that were seen during the training process. These examples\nwill illustrate the encoder-decoder framework and at the end we will note their\nshortcomings, which Hamilton et al. [55, 2] describes. This will lead us to more\ncomplicated encoder-decoder models called GNNs in the next section.\nFor each example, the input is a fixed matrix that provides a similarity statistic\nbetween any two nodes in N such as a weighted adjacency matrix. The output\nof these algorithms is a real vector (a feature vector) for each node describing the\nnode's neighborhood structure, and taken together, they support some downstream\nmachine learning task.\nThe Laplacian eigenmaps algorithm is an early and successful nonlinear dimen-\nsionality reduction algorithm [60]. Given a user-defined parameter t > 0, a weighted\nadjacency matrix, W = (Wij)i,j\u2208N, can be defined by\nWij =\nW_{ij} (t) =\\begin{cases}\nexp\\bigg(-\\frac{|| X_{i} - X_{j} ||}{t}\\bigg) & \\text{ if } A_{ij} = 1,\\\\\n0 & \\text{ otherwise.}\n\\end{cases}\n(4)\nIn practice, the above weighted adjacency matrix is typically the input to the Lapla-\ncian eigenmaps algorithm, but a simple adjacency matrix or a k-nearest neighbor\nmatrix may alternatively be inputs.\nThe Laplacian eigenmaps algorithm can be reformulated in terms of the encoder-\ndecoder framework [55, 2]. Define the ground truth, decoder and loss functions by\nGt: N\u00d7N\u2192 R,Gt(i, j) = Wij,\n(5)\nDec : Re \u00d7 Re \u2192 R+,\nDec(w, z) = ||w \u2013 z||2,\n(6)\nL\n: R \u00d7 R+ \u2192 R,L(q, r) = qr.\n(7)\nThen the goal is to to find the (non-constant) encoder Enc\nzi = Enc(i) \u2208 R,\nfor i \u2208 N\n(8)\nthat minimizes the model's loss L\u2208 R+ up to a scaling factor, where that loss is\nL = \u2211 L(Gt(i, j), Dec(zi, zj))\ni,jEN\n= \u2211 Wij || zi - 2j || 2,\ni,jEN\n(9)\nwhere the minimization is subject to a constraint that prevents the solution from\ncollapsing to a lower dimension (i.e. ZT DZ = I, where Z = (zi)i\u2208N). Notice that\nW_{ij} \u2265 0 is larger when i and j are adjacent. Then the above equation means that\nthe model is punished during training for having node attributes of adjacent nodes"}, {"title": "4 Graph Neural Networks", "content": "Graph neural networks have several desirable properties. They jointly use node\nattributes and edge information for training, trained models can be applied to\nunseen graphs, and the number of parameters of a GNN is independent and sub-\nlinear in the number of nodes. Moreover, they apply naturally to both undirected\nand directed graphs."}, {"title": "4.1 Encoder layers", "content": "A typical GNN encoder can have three classes of layers: pre-processing layers,\nmessage-passing layers and post-processing layers. The pre- and post-processing\nlayers are optional.\nIn general, a single layer feedforward neural network has the form\n: Rd \u2192 Rd,,\nxi :\u2192 \u03c3(Wxi + b),\n(20)\nwhere for positive integers d and \u00e3, W : Rd \u2192 Rd is a matrix and b\u2208 Rd is\na vector, both with of trainable parameters, and o is an element-wise non-linear\nfunction (e.g. element-wise ReLU). Pre-processing layers are a stack of one or more\nof these networks\nPre-Proc : Rm \u2192 Rm, Pre-Proc(xi) = xi,\n(21)\nthat maps each node attribute vector xi to a node feature vector \u00eei in a computation\nthat does not involve the edges of the graph.\nThese node features feed into the message-passing layers, which are the most\nimportant layers for the GNNs performance [68]. If A is a graph with a matrix of\nnode features X = (xi)i\u2208N, then a message-passing layer is a map\nMessage-Passing : (X, A) \u2192 (H, A)"}, {"title": "4.2 Decoder and loss functions", "content": "Below are minimalist examples of these components for several graph tasks from\nSection 2. More sophisticated examples appear in the literature. For an integer"}, {"title": "4.3 Learning paradigms", "content": "Inductive and transductive learning are the two common paradigms for reasoning\nwith graph neural networks. In inductive learning, no test data is available during"}, {"title": "5 Experiments", "content": "This section complements the previous theoretical sections with experimental re-\nsults. The goal is to describe the behavior of GNNs under several training and\ndataset conditions. Our experiments focus on GCN, GATv2, and GraphSAGE be-\ncause they are commonly used as benchmarks and many GNN architectures are\nbuilt on top of them, for example [97, 98, 99, 56, 50, 100]. Table 1 summarizes\nimportant properties of these GNNs. Our experiments include two other graph\nmodels: Multilayer Perceptron (MLP), which only uses node features, and Deep-\nWalk, which only uses edges. All experiments are in the transductive setting. Two\nlimitations are that none of our datasets are large and we only consider the node\nclassification task.\nThirteen open-source datasets are used: seven high homophily datasets and six\nlow homophily ones. The high homophily datasets are citation networks (Cora,\nPubMed, CiteSeer, DBPL [101, 102]), co-purchase networks (AmazonComputers,\nAmazonPhoto [96]), and a coauthor network (CoauthorCS [96]). The low ho-\nmophily datasets are webpage-webpage networks (WikipediaSquirrel, Wikipedi-\naChameleon, WikipediaCrocodile, Cornell, Wisconsin [41, 103]) and a co-occurrence\nnetwork (Actor [103]). All datasets are homogeneous graphs, which means they\nhave a single node type (e.g. \"article\") and a single edge type (e.g. \"is cited by\").\nThe Squirrel, Chameleon and Crocodile datasets are node regression datasets, so we\ntransform them into node classification networks by partitioning the range of values\ninto five parts, where each part defines a class. The remaining ten are natively node\nclassification datasets.\nEdge homophily and the signal-to-noise ratio (SNR) of node features are two\nmeasures of complexity in an attributed graph. Edge homophily is the fraction of\nedges that connects two nodes of the same class [71]. SNR is a measure of node\nfeatures, where roughly speaking, it is the squared distance between the mean of\neach class compared to the variance within each class. Specifically, let C be the\nnode classes, and for each class i \u2208 C, let F; be the node features of class i. Define"}, {"title": "5.1 Baseline node classification performance", "content": "The baseline GCN, GATv2 and GraphSAGE architectures are two layer message-\npassing networks with 16 hidden dimensions and no pre-processing or post-processing\nlayers, Deep Walk also has 16 hidden dimensions, MLP is a three-layer fully con-\nnected network where the first layer has 128 hidden dimensions and the second one\nhas 64. In the literature, GAT is a benchmark more often than GATv2, but we\nuse GATv2 because it consistently outperforms GAT in our experiments [78]. In\nall cases, the models are trained for at most 200 epochs with a learning rate of 0.1\nand a train/val/test split. We use two training sizes, 80% or 1%, and the labels not\nused for training are divided evenly among the validation and test sets."}, {"title": "5.2 Hyperparameters and node classification accuracy", "content": "In this section we analyze the effect of hyperparameters on node classification ac-\ncuracy, which we do one hyperparameter at a time. We analyze hyperparameters\none at a time, and we study their effect separately for different training and dataset\nconditions. Existing studies show which hyperparameters were most influential in\nbuilding the best model, but they do not show the practical effect of selecting the\nright hyperparameter options in terms of the evaluation metric [104, 105, 106] - it\ncould be that modifying the hyperparameters makes very little to no difference in\nperformance. We find that under \"easy\" and \"hard\" training and conditions (see\nSection 5.2.1), modifying the hyperparameters makes little difference beyond tuning\nthe number of hidden dimensions. While automated hyperparameter tuning tools\ncan be helpful for finding good hyperparameter configurations in a given dataset,\nthey are not useful for this study, so we do not use them here."}, {"title": "5.2.1 Adjusting the number of hidden dimensions", "content": "Table 6 shows the average improvement in node classification accuracy over the\nbaseline GNN designs for a range of hidden dimensions. While increasing the hid-\nden dimensions tends to significantly improve performance for the high homophily\ngraphs, it does not on the low homophily one. At first sight, this is surprising\nbecause in principle, larger hidden dimensions should enable modeling of more\ncomplex relationships at the possible expense of overfitting due to the additional\nparameters. But in practice, we see that overfitting is not a problem even for very\nsmall datasets like Wisconsin and Cornell. It is likely that getting exposure to\nthe test features during training is helping to avoid overfitting we expect that\noverfitting would be a bigger problem for inductive inference. Next, using more"}, {"title": "5.2.2 Adjusting the number of training epochs", "content": "We use 128 hidden dimensions for each GNN and plot their performance over the\nnumber of training epochs. Figure 3 shows that on high homophily graphs, node\nclassification accuracy generally stabilizes and improves over 400 epochs, which\nis in-line with previous research [104]. In contrast, performance falls sharply for\nseveral low homophily graphs after 25 training epochs. Message-passing layers"}, {"title": "5.2.3 Adjusting the number of layers and other hyperparameters", "content": "GNNs have many design variables in addition to their hidden dimensions. We use\nthe results of You et al. [104] to decide which design variables to focus on. Our\nfixed design variables are listed in Table 7, and we investigate how performance\nchanges by tuning the other six, given in Table 8. You et al. [104] provide per-\nformance rankings of hyperparameter configurations but no information on their\ncontributions to improving the evaluation metric score, so we address this question\nfor node classification."}, {"title": "5.3 Qualitative description of GNN learning", "content": "Recall from Section 4.1 that each layer of the GNN transforms the node feature\nvectors into new feature vectors that are inputs to the next layer. Figure 6 describes\nthe energy of the signal and noise (defined in Equation (58)) at each epoch in the\nfinal hidden layer of each model. As the number of training epochs gets larger,\nthe energy of the noise stays flat while that of the signal gets larger, which means\nthat node feature vectors generally separate between classes while their within-class\nvariance stays the same. In contrast, experiments show that MLP performance does\nnot improve after around 100 epochs, as indicated in Figures 6c and 6d.\nDi Giovanni et al. [107] mathematically analyze an energy potential as it passes\nthrough the message-passing layers, where the potential concerns the energies of the\nnode feature signal and noise. Figures 7a and 7b experimentally illustrate signal\nand noise energies as node features pass through all layer types. The noise drops\nsignificantly in the pre-processing layers. In the high homophily case, the noise\ndrops again in the message-passing layers. This is intuitive because the message-\npassing layers aggregate information from their neighboring nodes, which tend to\nhave the same class label and similar node features. This brings together similar\nnode features, reducing the node feature noise. Figures 7c and 7d show what\nhappens when the message-passing layer is removed. Here, there are MLP models\nwith a comparable number of layers to what we have in tuned GNNs, but the noise\nof the node features is flat.\nGNNs do not get the same advantage on low homophily graphs because neigh-\nboring nodes tend to be of different classes so their features have different infor-"}, {"title": "6 Conclusion", "content": "A decade ago deep convolutional neural networks for image classification initiated\na revolution where feature learning was integrated into the training process of a\nneural network, and this was subsequently extended to data structures like irregu-\nlar graphs. The encoder-decoder framework neatly describes these models, and the\nshortcomings of simpler encoder-decoder models motivates the use of more com-\nplicated Graph Neural Networks (GNNs). Graph neural networks have attracted\nconsiderable attention due to state-of-the-art results on a range of graph analysis"}, {"title": "A Open-source GNN libraries", "content": "To our knowledge, PyTorch Geometric and Deep Graph Library are the largest and\nmost widely used libraries.\n\u2022 PyTorch Geometric\nThis library is built on PyTorch and its design aims to stay close to usual Py-\nTorch [108]. It provides well-documented examples, and benchmark datasets\nand most state-of-the art GNN models are available here. Many GNNs from\nthe literature are implemented, and it supports multi-GPU processing.\n\u2022 Deep Graph Library\nThis library is sponsored by AWS, NSF, NVIDIA and Intel [109]. It supports\nmulti-GPU processing and the PyTorch, TensorFlow and Apache MXNet\nframeworks. They have well-documented examples and example code for\nmany state-of-the art models.\n\u2022 GeometricFlux.jl\nThis is a Julia library for geometric deep learning [110], as described in [4].\nIt supports deep learning in a range of settings: Graphs and sets; grids and\nEuclidean spaces; groups and homogeneous spaces; geodesics and manifolds;\ngauges and bundles. It also offers GPU support and has integration with\nGNN benchmark datasets. It supports both graph network architectures,\nwhich are more general graph models than graph neural networks [1], and\nmessage-passing architectures.\n\u2022 Spektral\nThis library is built on TensorFlow 2 and Keras [111]. It intends to feel close\nto the Keras API and to be flexible and easy to use. It provides code for the\nstandard components of GNNs as well as example implementations of GNNS\non specific datasets.\n\u2022 Jraph\nThis is a library written in JAX, which is a language that enables automatic\ndifferentiation of python and numpy. It is created by DeepMind and inherits\nsome design properties from its earlier library, Graph Nets. Like Graph Nets,\nit supports building graph networks and is a lightweight library with utilities\nfor working with graphs. Unlike Graph Nets, it has a model zoo of graph\nneural network models."}, {"title": "B Results on Individual Datasets", "content": "All tables report results with 95% confidence intervals."}]}