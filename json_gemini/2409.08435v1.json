{"title": "When Context Leads but Parametric Memory Follows in Large Language Models", "authors": ["Yufei Tao", "Adam Hiatt", "Erik Haake", "Antonie J. Jetter", "Ameeta Agrawal"], "abstract": "Large language models (LLMs) have demonstrated remarkable progress in leveraging diverse knowledge sources. This study investigates how nine widely used LLMs allocate knowledge between local context and global parameters when answering open-ended questions in knowledge-consistent scenarios. We introduce a novel dataset, WikiAtomic\u00b9, and systematically vary context sizes to analyze how LLMs prioritize and utilize the provided information and their parametric knowledge in knowledge-consistent scenarios. Additionally, we also study their tendency to hallucinate under varying context sizes. Our findings reveal consistent patterns across models, including a consistent reliance on both contextual (around 70%) and parametric (around 30%) knowledge, and a decrease in hallucinations with increasing context. These insights highlight the importance of more effective context organization and developing models that use input more deterministically for robust performance.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have significantly advanced the capabilities of natural language processing. When generating responses, LLMs can use the contextual information provided in a prompt along with or instead of the parametric knowledge embedded during pretraining (Petroni et al., 2019; Brown et al., 2020; Heinzerling and Inui, 2021).\nIn order to generate accurate and coherent responses, LLMs need to effectively combine their parametric knowledge with provided contextual information, and understanding the balance between these two sources of information is crucial (Neeman et al., 2022; Li et al., 2024). Most prior work"}, {"title": "2 Task and Terminology", "content": "We structure our investigation as an open-ended question answering task to gain deeper insights into how LLMs utilize and integrate contextual knowledge with their parametric knowledge. By incrementally increasing the amount of context, we analyze the models' behavior, focusing on their ability to prioritize different types of information and their tendency toward factual hallucination in knowledge-consistent scenarios.\nWe next introduce some key terminology used throughout the paper.\nTopic: This is the title of the Wikipedia article. The WikiAtomic dataset includes 200 distinct articles, and questions are asked about these topics.\nAtomic Sentence: This is a sentence that contains a single piece of information that cannot be broken down into simpler components without losing its meaning (Liu et al., 2023).\nContext: For each topic, a context consists of k atomic sentences provided in a prompt. In our experiments, 2 \u2264 k \u2264 50, where contexts consist of increments of 2 sentences from 2 to 30 (e.g., 2, 4, 6, etc.), followed by increments of 5 sentences until reaching a total of 50 sentences. For each topic, this results in contexts of 20 different sizes, creating a total of 4,000 topic-context instances.\nResponse: Given the context, the model generates a response that includes some degree of both contextual and parametric knowledge. This response is then atomized. A sentence directly derived from the provided context is considered as contextual (local) knowledge. In contrast, a sentence that is not entailed from the context is consid-"}, {"title": "3 WikiAtomic Dataset", "content": "We created a novel dataset - WikiAtomic - consisting of 200 articles from Wikipedia. We selected Wikipedia as the basis for creating our knowledge-consistent dataset because its extensive data has historically been integral to the training of numerous LLMs, explicitly in models like GPT-2, GPT-3, BERT, T5, and BLOOM, and implicitly in large-scale aggregate corpora such as The Pile (Gao et al., 2020) and Common Crawl (Luccioni and Viviano, 2021). This extensive use allows us to reasonably assume that information from Wikipedia, particularly from older articles, is present in the pretraining data of several LLMs we study.\nExtracting Wikipedia Articles We selected 200 high-quality articles from Wikipedia\u00b2, each over 1000 words, covering diverse topics from science and technology to history, culture, and prominent figures. After manually removing low-quality texts, we ensured a diverse collection for our open-ended question answering task.\nConverting to Atomic Sentences To precisely control the number of contexts in our question, all articles are decomposed into a set of atomic sentences (Liu et al., 2023). Sentences containing multiple pieces of information can complicate evaluations, particularly those involving entailment (Kim et al., 2024). By breaking down sentences into atomic sentences, we ensure that each unit presents a single, unambiguous piece of information which enhances the accuracy of entailment assessments. Following Min et al. (2023), we use GPT-40 to extract atomic sentences by providing the definition of atomic facts and instructing the model to perform multiple passes on an article to break each sentence down to individual atomic sen-"}, {"title": "4 Experiments and Evaluation", "content": "This section outlines the experiment setup, evaluation metrics, and the models studied.\n4.1 Experiments\nFigure 2 illustrates a sample instance of our question answering task that includes the input (a list of atomic sentences as context along with a question prompt) and the output which is a model's response further atomized.\nQuestion Formulation We prompt the models to answer a question given the contexts using a semi-restrict format: \"With this information, tell me about {Topic}\"3. The effects of varying the question format are further explored in Section 5.5.\nResponse Generation The responses generated by the LLMs are converted into atomic sentences using the same method described earlier in Section 34.\nUltimately, we obtain two lists for each question-answer pair: atomic contexts (atomic sentences from the context) and atomic responses (atomic sentences from the response), allowing us to directly compare them and minimize discrepancies.\n4.2 Evaluation\nHere, we describe the metrics for detecting contextual and parametric knowledge, as well as model hallucination, focusing on evaluating the faithfulness and factual accuracy of responses.\nContextual vs. parametric knowledge detection"}, {"title": "5 Results and Analysis", "content": "Now, we present and discuss the results of our experiments.\n5.1 In knowledge-consistent setting, how do models prioritize sources of knowledge?\nFigure 4 shows how four models prioritize sources of knowledge (similar results were obtained from other models, plots included in Appendix D). The plots display changes in the average number of local, global, and total atomic sentences in responses as context increases. Somewhat surprisingly, we found that all models (except Phi-3 in certain cases)"}, {"title": "5.2 Which parts of context are used?", "content": "The results of the previous experiment intrigued us, leading us to the next question: which parts of the provided context do LLMs use in their responses? Using the INFUSE framework, we reversed the input position to give each atomic sentence in the"}, {"title": "5.3 How similar are various types of knowledge?", "content": "Our analysis so far suggests that LLMs consistently add parametric knowledge regardless of the amount of context provided in the question. This prompted us to investigate the relationships between the different types of knowledge. The graphs were generated as follows: For each Wikipedia topic containing 20 questions (up to 50 contexts), we obtained 20 responses. Each response contains sentences marked as contextual or parametric knowledge.\nLocal vs. Local We perform a pairwise comparison of contextual knowledge in responses across various context sizes (Figure 7a, full set of results in Appendix G). We observe that the local context remains moderately similar in smaller contexts but becomes increasingly similar with larger contexts. This pattern suggests that models tend to focus on certain types of context, often the earlier parts.\nGlobal vs. Global Let us now turn to Figure 7b (full set of results in Appendix H) which shows pairwise similarity between parametric knowledge in each response. The darker plots near the diagonal line indicate that the model's parametric knowledge remains similar when context sizes are close, such as context size 8's parametric knowledge being most similar to that of sizes 6 or 10. Initially, with minimal context, models provide a larger variety of information. As context size increases, some models show increased similarity across broader ranges, suggesting responses align more closely with the overall theme of the context. This indicates that models tend to repeatedly add certain pieces of information from a small pool of"}, {"title": "5.4 In knowledge-consistent setting, (how much) do models hallucinate?", "content": "In Figure 8, we present the FActScore hallucination scores across all models. For smaller contexts, models have higher hallucination rate, which improves with additional context and converges with as little as 10 sentences in context. Larger models (GPT-40, Claude Opus, Claude Sonnet, Claude Haiku, Llama 3 70B, and Mistral 8x22B) consistently show higher FActScores, indicating they are less prone to hallucinations and benefit significantly from additional context. Among smaller models, Mistral 7B and Llama 3 8B perform well, showing significant improvement with increased context, while Phi-3 tends to hallucinate more, although it also improves with context. Larger models generally stabilize at higher FActScores with fewer fluctuations, indicating a stronger ability to leverage additional context to minimize hallucinations. Smaller models show a more gradual improvement, with some like Phi-3, displaying more variability and a lower overall FActScore, suggesting they are more context-sensitive but can still improve with more information (for detailed results of false para metric knowledge for each model, see Appendix J)."}, {"title": "5.5 Further Analyses", "content": "(Potentially) Unseen Knowledge In knowledge-consistent scenarios, models rely on a mix of contextual and parametric knowledge. But in unseen knowledge scenarios, where the context contains new information that the models have potentially not seen, how do models respond? To explore this, we create a recent example around the pro-Palestinian university protests of April 2024 and prompt the models for information on this topic. Without context, most models stated they lacked information on the topic and provided a general background on protests related to Palestine (see Figure 9, with additional results in Appendix K). However, some models, Claude Haiku, Llama 8B, and Mixtral 22x8B, all confidently respond to the prompt as if they have seen this knowledge in their pre-training data. We hypothesize that this may be due to similar events that have occurred in the past that the model has seen in its pretraining data. With just two contexts, all models referenced the context but some included facts not in the context.\nPrompt Sensitivity Our naturalistic semi-restrict question prompt (\"With this information, tell me about [topic].\") encourages the models to consider the provided contexts while still allowing them to use their parametric knowledge. Considering that LLMs are sensitive to prompts (Lu et al., 2021), we tested two alternative phrasings: a no-restrict prompt which allows the models complete freedom to draw on their parametric and contextual knowledge as they see fit (\"Tell me about [topic]\") and a strict prompt which tries to restrict the models to use only the contexts provided (\u201cUsing the provided context only, tell me about [topic].\").\nTo conduct this ablation study, we randomly selected 20 topics from our dataset, resulting in a total of 400 instances of varying context sizes, and obtained responses different prompts. While the no-restrict plots look similar to the semi-restrict prompts presented earlier, the strict plots show that the models are entirely focused on contextual knowledge with very little parametric knowledge (supporting graphs in Appendix L). These results confirm model sensitivity to prompts and highlight that, with simple prompt adjustments, models can"}, {"title": "6 Discussion", "content": "Understanding how LLMs utilize contextual and parametric knowledge is important for real-world knowledge-consistent use cases. We summarize our key findings and their implications:\n\u2022 Models process context very similarly suggesting a standardized approach to context processing across all selected models.\n\u2022 Context is never fully utilized and some parametric knowledge is always included highlighting the need for developing models that utilize input contexts more deterministically.\n\u2022 Earlier information is prioritized in longer contexts, with responses following the order of presented information highlighting the importance of organizing context effectively.\n\u2022 Responses tend to include similar contextual knowledge but supplemental parametric knowledge.\n\u2022 Hallucinations decrease as context increases implying that more context helps models generate more accurate responses, reducing reliance on potentially incorrect parametric knowledge."}, {"title": "7 Related Work", "content": "Contextual Grounding A persistent challenge for LLMs lies in reconciling contradictory or superseded information between the provided context and their internal knowledge base (Li et al., 2022; Zhou et al., 2023a), with most recent work exploring this interplay between local and global knowledge in knowledge-conflict scenarios using counterfactual datasets (Si et al., 2022; Qian et al.,"}, {"title": "8 Conclusion", "content": "Understanding how LLMs handle different context sizes is crucial for developing robust models. Our evaluation of nine widely used LLMs with our WikiAtomic dataset showed that all models process context similarly, balancing contextual and parametric knowledge, while adjusting response lengths consistently. They never use all provided contexts, always include some parametric knowledge, and prioritize information sequentially. As context increases, while contextual knowledge remains similar, parametric knowledge becomes more aligned with the context, and hallucination rates decrease. These insights highlight the importance of organizing context effectively and developing models that utilize input more deterministically."}, {"title": "Limitations", "content": "Our work, while yielding some interesting findings, is not without limitations. Our method of extracting atomic sentences from Wikipedia articles often split the source sentences into multiple, occasionally creating sentences that begin with an indirect reference to a subject. Because of the natural flow of presenting information in such a format, we did not randomize the order. Further work could look at how the order of contexts used by models compared when the provided contexts are shuffled.\nWe utilized the INFUSE method to classify model response contexts into 'contextual' and 'parametric'. We empirically set the threshold for determining the categorization based on manual verification. Further work could look at more sophisticated methods to set this threshold for even better results. A limitation of the metric we adopt for hallucination detection is that it relies on a single knowledge source Wikipedia as its knowledge source. Information that is factually accurate, but not present in the knowledge source could be incorrectly classified as a hallucination."}]}