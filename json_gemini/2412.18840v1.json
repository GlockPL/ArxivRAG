{"title": "Implicit factorized transformer approach to fast prediction of turbulent channel flows", "authors": ["Huiyu Yang", "Yunpeng Wang", "Jianchun Wang"], "abstract": "Transformer neural operators have recently become an effective approach for surrogate modeling of nonlinear systems governed by partial differential equations (PDEs). In this paper, we introduce a modified implicit factorized transformer (IFactFormer-m) model which replaces the original chained factorized attention with parallel factorized attention. The IFactFormer-m model successfully performs long-term predictions for turbulent channel flow, whereas the original IFactFormer (IFactFormer-o), Fourier neural operator (FNO), and implicit Fourier neural operator (IFNO) exhibit a poor performance. Turbulent channel flows are simulated by direct numerical simulation using fine grids at friction Reynolds numbers $Re_\\tau \\approx 180, 395, 590$, and filtered to coarse grids for training neural operator. The neural operator takes the current flow field as input and predicts the flow field at the next time step, and long-term prediction is achieved in the posterior through an autoregressive approach. The prediction results show that IFactFormer-m, compared to other neural operators and the traditional large eddy simulation (LES) methods including dynamic Smagorinsky model (DSM) and the wall-adapted local eddy-viscosity (WALE) model, reduces prediction errors in the short term, and achieves stable and accurate long-term prediction of various statistical properties and flow structures, including the energy spectrum, mean streamwise velocity, root mean square (rms) values of fluctuating velocities, Reynolds shear stress, and spatial structures of instantaneous velocity. Moreover, the trained IFactFormer-m is much faster than traditional LES methods.", "sections": [{"title": "1 Introduction", "content": "Turbulence simulation is an important research area with significant applications in aerospace, energy, and many engineering fields [1-4]. Common methods for turbulence simulation include direct numerical simulation (DNS), large eddy simulation (LES), and Reynolds-averaged Navier-Stokes (RANS) method. DNS directly solves the Navier-Stokes equations, offering the high accuracy, but it is difficult to use for complex problems with high Reynolds numbers [5, 6]. LES resolves the large-scale flow structures and models the effects of small-scale ones using a subgrid-scale (SGS) model [7, 8], achieving a balance between accuracy and efficiency. RANS method merely solves the averaged flow field with the modeling of the effects of fluctuating flow field, and is widely used for practical engineering problems [9, 10]. These traditional turbulence simulation methods generally have high computational costs, even the less expensive RANS method, which greatly limits their application.\nThe introduction of machine learning (ML) techniques is expected to solve this problem [11]. Neural operator (NO) models are considered as an effective method for simulating physical systems governed by partial differential equations (PDEs), due to their theoretical foundation [12]. The trained NO model can make efficient and fast predictions, serving as a lightweight surrogate model. As a pioneer of neural operators, Lu et al. [13] introduced deep operator network (Deep-ONet), which for the first time employed neural networks to learn operators. Li et al. [14] proposed the Fourier Neural Operator (FNO), which leverages discrete Fourier transforms to perform feature fusion in the frequency domain, significantly enhancing both the model's speed and accuracy. Subsequent works have made a series of improvements based on the two models mentioned above [15-23].\nIn recent years, transformer neural operators have been gradually developed [24-30]. Li et al. [24] applied an attention-based encoder-decoder structure to predict tasks related to systems governed by partial differential equations. Hao et al. [25] proposed a general neural operator transformer that encodes information including initial conditions, boundary conditions, and equation coefficients into the neural network, while extracting the relevant physical fields and their correlations. The transformer neural operators developed in the aforementioned works suffer from high computational costs and excessive memory usage. As a result, a series of subsequent studies focused on investigating the performance of lightweight transformer neural operators. Li et al. [26] proposed a low-rank transformer operator for uniform grids, which significantly reduces computational cost and memory usage by employing axial decomposition techniques to sequentially update features in each direction. Wu et al. [27] proposed an efficient transformer neural operator for handling non-uniform grids, which projects spatial points onto a small number of slices and facilitates information exchange between the slices. Chen et al. [28] proposed a transformer neural operator that relies solely on coordinate representations. By eliminating the need for complex function values as inputs, this approach significantly improves efficiency.\nIn evaluating the effectiveness of a NO for turbulence simulation, in addition to comparing prediction accuracy over short time periods, another important criterion is its ability to achieve long-term stable predictions. Specifically, this involves assessing whether statistical quantities and structures remain consistent with high-precision numerical simulation results over extended periods. Li et al. [31] showed that the FNO [14] exhibited explosive behavior over time when predicting chaotic dissipative systems, and successfully predicted the long-term statistical behavior of dissipative chaotic systems by introducing dissipation regularization. Li et al. [32] designed an implicit U-Net enhanced FNO (IU-FNO), which achieved accurate and stable long-term predictions in isotropic turbulence, free shear turbulence, and decaying turbulence. Oommen et al. [33] applied a diffusion model to correct the long-term prediction results of the FNO, significantly improving the consistency between the predicted energy spectrum and the true distribution.\nHowever, most existing transformer neural operators tend to focus solely on reducing the single-step error and delaying the accumulation of errors in time integration as much as possible, while overlooking the ability to make long-term stable predictions. Li et al. [34] introduced the transformer-based neural operator (TNO), which combines a linear attention with spectral regression, achieving long-term stable predictions in both isotropic turbulence and free shear flow. However, the attention for each points often incurs significant computational overhead. Yang et al. [35] fouced on the long-term prediction capability of low-rank transformer neural operators in turbulence simulation. In tests on three-dimensional isotropic turbulence, they showed that the original factorized transformer (FactFormer) [26] did not diverge over extended periods, but the predicted high-wavenumber energy spectrum was inconsistent with the true value. By designing an implicit factorized transformer (IFactFormer), they successfully achieved long-term stable and accurate predictions.\nIn this paper, we investigate the long-term prediction capability of the IFactFormer model for turbulent channel flows at three friction Reynolds numbers $Re_\\tau \\approx 180, 395$, and 590. We show that the original IFactFormer (IFactFormer-o) model in previous work [35], fails to achieve long-term stable predictions of turbulent channel flows. We identify potential causes for this failure and propose solutions. By making appropriate adjustments to the network architecture, specifically replacing the chained factorized attention with parallel factorized attention, we introduce a modified version, IFactFormer-m. While only adding minimal computational time and memory usage, the IFactFormer-m model significantly outperforms IFactFormer-o in terms of single-step prediction accuracy, and achieves precise long-term predictions of statistical quantities and flow structures. The IFactFormer-m model demonstrates more accurate and stable long-term predictions compared to neural operators including FNO [8], implicit FNO (IFNO) [36], and IFactFormer-o [35], as well as traditional LES models including the dynamic Smagorinsky model (DSM) and the wall-adapted local eddy-viscosity model (WALE).\nThis paper consists of five sections. Section 2 presents the problem statement, introducing the Navier-Stokes (N-S) equations, the LES method, and the learning objectives of NO for this task. Section 3 presents several transformer neural operators and discusses modifications to the original IFactFormer model. Section 4 compares several machine learning models with traditional LES models in turbulent channel flows. Section 5 concludes the paper with a summary."}, {"title": "2 Problem statement", "content": "In this section, we first provide a brief introduction to the N-S equations and the LES method, followed by a discussion on the role of NO models and their learning objectives."}, {"title": "2.1 Navier-Stokes equations", "content": "Turbulence is widespread in nature and is a highly nonlinear, multiscale system. It is generally believed that its dynamics are governed by the N-S equations. The incompressible form of the N-S equations is as follows [1]:\n$\\frac{\\partial u_i}{\\partial x_i} = 0,$\n$\\frac{\\partial u_i}{\\partial t} + \\frac{\\partial (u_i u_j)}{\\partial x_j} = - \\frac{\\partial p}{\\partial x_i} + v \\frac{\\partial^2 u_i}{\\partial x_j \\partial x_j} + F_i,$\nwhere $u_i$ denotes the velocity component along the i-th coordinate axis, p represents the pressure normalized by the constant density $\\rho$, v is the kinematic viscosity, and $F_i$ refers to the forcing term acting in the i-th direction. Consider the turbulent channel flows with the lower and upper walls located at y = 0 and y = 2$\\delta$, respectively.\nConsidering that the velocities in the three coordinate directions are $(u, v, w) = (u_1, u_2, u_3)$ with fluctuations $(u', v', w') = (u_1', u_2', u_3')$, the total shear stress is given by [1]:\n$\\tau(y) = \\rho v \\frac{\\partial \\langle u \\rangle}{\\partial y} - \\rho \\langle u'v' \\rangle ,$\nwhere $\\langle \\rangle$ represents the spatial average over the homogeneous streamwise and spanwise directions, and $\\langle u'v' \\rangle$ is the Reynolds shear stress. At the wall, the boundary condition $u(x, t) = 0$, so the wall shear stress is\n$\\tau_w = \\rho v \\frac{\\partial \\langle u \\rangle}{\\partial y} |_{y=0}.$\nThe friction velocity $u_\\tau$ and viscous lengthscale $\\delta_v$ are defined by [1]:\n$u_\\tau = \\sqrt{\\frac{\\tau_w}{\\rho}}, \\quad \\delta_v = \\frac{\\nu}{u_\\tau}.$\nTherefore, the definition of the friction Reynolds number $Re_\\tau$ is given by:\n$Re_\\tau = \\frac{u_\\tau \\delta}{\\nu}$"}, {"title": "2.2 Large eddy simulation", "content": "DNS involves directly solving the N-S equations on a fine mesh to fully resolve all the scales of the flow. However, the inherent multiscale nature of turbulence limits the applicability of DNS to turbulence problems at high Reynolds numbers due to its high computational cost. LES aims to resolve the large-scale turbulent structures on a coarse grid, thus reducing computational cost. Consider the spatial filtering operation as described below [1]:\n$\\bar{f(x)} = \\int_\\Omega G(r, x; \\Delta) f(x - r) dr,$\nwhere G is the grid filter, $\\Delta$ is the filter width and f is a physical quantity distributed over the spatial domain $\\Omega$. The filtered N-S equations can be obtained by applying Eq. (7) to Eq. (1) and Eq. (2), as follows [1]:\n$\\frac{\\partial \\bar{u_i}}{\\partial x_i} = 0,$\n$\\frac{\\partial \\bar{u_i}}{\\partial t} + \\frac{\\partial \\bar{u_i} \\bar{u_j}}{\\partial x_j} = - \\frac{\\partial \\bar{p}}{\\partial x_i} + v \\frac{\\partial^2 \\bar{u_i}}{\\partial x_j \\partial x_j} + F_i - \\frac{\\partial \\tau_{ij}}{\\partial x_i}.$\nUnlike the N-S equations, the filtered N-S equations are unclosed due to the introduction of unclosed SGS stress $\\tau_{ij}$, which is defined as:\n$\\tau_{ij} = \\bar{u_i u_j} - \\bar{u_i} \\bar{u_j}.$\nA series of SGS models have been developed to approximate the SGS stresses [37-41]. The Smagorinsky model (SM) is given by [37]:\n$\\tau_{ij} - \\frac{1}{3} \\tau_{kk} \\delta_{ij} = -2 C_s^2 \\Delta^2 |\\bar{S}| \\bar{S_{ij}},$\nwhere $\\delta_{ij}$ is the Kronecker delta function, $C_s$ is an empirical coefficient. The strain rate of the filtered velocity $\\bar{S_{ij}}$ and characteristic filtered strain rate $|\\bar{S}|$ are given by:\n$\\bar{S_{ij}} = \\frac{\\partial \\bar{u_i} / \\partial x_j + \\partial \\bar{u_j} / \\partial x_i}{2}, \\quad |\\bar{S}| = \\sqrt{2 \\bar{S_{ij}} \\bar{S_{ij}}}.$\nFor the dynamic Smagorinsky model (DSM) [38], the empirical coefficient $C$ in Eq. (11) can be derived as:\n$C = \\frac{\\langle L_{ij} M_{ij} \\rangle}{\\langle M_{ij} M_{ij} \\rangle},$\nwhere\n$L_{ij} = \\bar{\\bar{u_i} \\bar{u_j}} - \\hat{\\bar{u_i}} \\hat{\\bar{u_j}}, \\quad M_{ij} = \\bar{a_{ij}} - \\bar{b_{ij}},$\nwith\n$\\bar{a_{ij}} = 2 \\Delta^2 |\\bar{S}| \\bar{S_{ij}}, \\quad \\bar{b_{ij}} = 2 \\hat{\\Delta}^2 |\\hat{\\bar{S}}| \\hat{\\bar{S_{ij}}}.$\nIn these expressions, the overbar denotes filtering at scale $\\Delta$, while the hat represents a test filtering operation at a doubled scale $\\hat{\\Delta} = 2 \\Delta$."}, {"title": "3 Transformer neural operator", "content": "In this section, the first part explains how attention-based neural operators approximate the true operator by leveraging a parameterized integral transform. The second part discusses the drawback of the chained factorized attention, and proposes the parallel factorized attention. The final part introduces the overall architecture of the modified implicit factorized transformer."}, {"title": "3.1 Factorized attention", "content": "The self-attention mechanism [42] dynamically weights the input by computing the correlations between different positions in the input vector, thereby capturing the dependencies among various positions. The perspectives of previous work [27, 43, 44] demonstrated that the standard attention mechanism can be viewed as a Monte Carlo approximation of an integral operator. Considering an input vector $u_i \\in \\mathbb{R}^{1\\times d_{in}}$ with $d_{in}$ channels in N points (1 \u2264 i \u2264 N), query $q_i \\in \\mathbb{R}^{1\\times d}$, key $k_i \\in \\mathbb{R}^{1\\times d}$ and value $v_i \\in \\mathbb{R}^{1\\times d}$ vectors with d channels are first generated through linear transformations as follows:\n$q_i = u_i W_q, k_i = u_i W_k, v_i = u_i W_v,$\nwhere ${W_q, W_k, W_v} \\in \\mathbb{R}^{d_{in} \\times d}$. Subsequently, the attention weights $a_{ij}$ are computed for q and k using the following equation:\n$a_{ij} = \\frac{exp[g(q_i, k_j)]}{\\sum_{s=1}^{N} exp[g(q_i, k_s)]},$\nwhere g is a scaled dot-product as follows:\n$g(q_i, k_j) = \\frac{q_i \\cdot k_j}{\\sqrt{d}}.$\nFinally, the attention weights $a_{ij}$ are applied to the value vectors $v_j$ to capture the dependencies between different positions in the sequence as follows:\n$z_i = \\sum_{j=1}^{N} a_{ij} v_j \\approx \\int_{\\Omega} \\kappa(x_i, \\psi) v_j(\\psi) d\\psi.$\nHere, the i-th row vector $\\kappa(x_i, \\psi)$ is regarded as the global kernel function $\\kappa(x_i, \\psi)$ of the approximate integral operator for point $x_i$."}, {"title": "3.2 Attention-based integral neural operator", "content": "The standard self-attention mechanism is often criticized for its quadratic computational complexity. Li et al. [26] proposed the factorized attention, which alleviates this issue. Due to the need for chained integration along each axis, we refer to this as the chained factorized attention, as illustrated in Figure 1(a). Specifically, for points on a Cartesian grid with $N_1 \\times N_2 \\times N_3 = N$ points in an three-dimensional space $\\Omega_1 \\times \\Omega_2 \\times \\Omega_3$, the chained factorized attention decomposes the kernel function in Eq. (25) into three separate kernel functions along each axis ${\\kappa^{(1)}, \\kappa^{(2)}, \\kappa^{(3)}}: \\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}$, and performs the integral transformation in the following manner [26]:\n$\\gamma(x_1, x_2, x_3) = \\int_{\\Omega_1} \\int_{\\Omega_2} \\int_{\\Omega_3} \\kappa^{(1)}(x_1, \\psi_1) \\kappa^{(2)}(x_2, \\psi_2) \\kappa^{(3)}(x_3, \\psi_3) v(\\psi_1, \\psi_2, \\psi_3) d\\psi_1 d\\psi_2 d\\psi_3,$\nwhere each kernel is obtained through a learnable projection followed by Eq. (22) and Eq. (23). The goal of the learnable projection is to compress the original input vector onto each axis. The calculation formulas for the three axes are as follows:\n$\\phi^{(1)}(\\gamma^{(1)}(x_1)) = h^{(1)} \\big( w^{(1)} \\int_{\\Omega_2} \\int_{\\Omega_3} \\gamma u(x_1, \\psi_2, \\psi_3) d\\psi_2 d\\psi_3 \\big),$\n$\\phi^{(2)}(\\gamma^{(2)}(x_2)) = h^{(2)} \\big( w^{(2)} \\int_{\\Omega_1} \\int_{\\Omega_3} \\gamma^{(2)} u(\\psi_1, x_2, \\psi_3) d\\psi_1 d\\psi_3 \\big),$\n$\\phi^{(3)}(\\gamma^{(3)}(x_3)) = h^{(3)} \\big( w^{(3)} \\int_{\\Omega_1} \\int_{\\Omega_2} \\gamma^{(3)} u(\\psi_1, \\psi_2, x_3) d\\psi_1 d\\psi_2 \\big).$\nHere, $w^{(s)} = N/N_s$ is a constant, ${h^{(1)}, h^{(2)}, h^{(3)}}$ are multilayer perceptron (MLP) and ${\\gamma^{(1)}, \\gamma^{(2)}, \\gamma^{(3)}}$ are linear transformation.\nThe above scheme has a drawback: all kernel functions ${\\kappa^{(1)}, \\kappa^{(2)}, \\kappa^{(3)}}$ are derived from the original input function u. These kernel functions are often effective at capturing the dependencies between different positions within the current function. However, for $\\kappa^{(2)}$ and $\\kappa^{(3)}$, the dependencies need to be evaluated on two new function\n$\\gamma^{(1)}(\\psi_2, \\psi_3) = \\int_{\\Omega_1} \\kappa^{(1)}(x_1, \\psi_1) v(\\psi_1, \\psi_2, \\psi_3) d\\psi_1,$\n$\\kappa^{(2)}(x_2, \\psi_2) = \\int_{\\Omega_1} \\kappa^{(1)}(x_1, \\psi_1) v(\\psi_1, \\psi_2, \\psi_3) d\\psi_1 d\\psi_2.$\nConsidering that these two functions mentioned above are obtained through a series of complex computations involving the input function u and parameters of neural network, and since the parameters are unknown to the kernel functions $\\kappa^{(2)}$ and $\\kappa^{(3)}$, they are tasked with evaluating the dependencies on an unknown system. This undoubtedly presents a significant challenge. In the works of Li et al. [26] and Yang et al. [35], the FactFormer and IFactFormer models, which are based on chained factorized attention, achieved promising test results in certain two-dimensional flows and three-dimensional isotropic turbulence. This success is likely due to the fact that, for isotropic problems, the learned dependencies in different axes are consistent, making the evaluation relatively easier. This, to some extent, obscures the underlying issue.\nBased on the above analysis, we propose parallel factorized attention, as illustrated in Figure 1(b). Compared to Eq. (26), the form of the integral transformation is modified as follows:\n$w^{(s)} = \\int_{\\Omega_s} \\kappa^{(s)}(x^{(s)}, \\psi_s) v(\\psi_1, \\psi_2, \\psi_3) d\\psi_s,$\n$w = \\text{Concat} (w^{(1)}, w^{(2)}, w^{(3)}),$\n$z = \\text{Linear} (w).$\nHere, s = 1, 2, 3, \u201cLinear\u201d is a linear transformation from $\\mathbb{R}^{3d}$ to $\\mathbb{R}^d$, and \u201cConcat\u201d means that concatenate the input function on channel dimension. The above simple modification allows each kernel function to focus on learning the dependencies along different axes of the current function."}, {"title": "3.3 Implicit factorized transformer", "content": "By utilizing the designed parallel factorized attention, we propose the modified implicit factorized transformer model (IFactFormer-m), as illustrated in Figure 2. The IFactFormer-m model consists of three components: the input layer $I : \\mathbb{R}^{d_{in}} \\rightarrow \\mathbb{R}^d$, parallel axial integration layer (PAI-layer) $P : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^d$, and the output layer $O : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d_{out}}$. For the temporal prediction of three-dimensional incompressible turbulence, it is assumed that $d_{in} = d_{out} = 3$. The input and output layers are three-layer MLPs, used to map the input function to a high-dimensional space and project the function from the high-dimensional space back to the low-dimensional space. The PAI-layer is a global nonlinear operator, used to approximate the integral transformation in the high-dimensional space to update the input function. Consistent with previous works [32,35,36,45], we adopt an implicit iteration strategy, where the parameters are shared across each PAI-layer. This approach effectively enhances the stability of the model in long-term turbulent flow predictions. Therefore, the overall operator of the L-layer IFactFormer-m model can be expressed as $O \\circ P \\circ \\cdots \\circ P \\circ I$.\n$\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad L$\nConsider the input function $u \\in \\mathbb{R}^d$ discretized into the vector $u_i \\in \\mathbb{R}^{1 \\times d}$ at points ${x_i}_{i=1}^{N}$. The computation formula for the PAI-layer is given as follows:\n$u_i^{'} = u_i + \\frac{1}{L} \\text{MLP} (z_i)$\n$= u_i + \\frac{1}{L} \\text{MLP} (\\text{P-Fact-Attn} (u_i)),$\nwhere \"MLP\" is three-layer, \u201cP-Fact-Attn\u201d denotes the parallel factorized attention and the output vector $u_i^{'}$ is the discretized representation of the updated function $u^{'}$. The factor of 1/L performs scale compression, ensuring that the final scale remains consistent for any given number of implicit iterations L."}, {"title": "4 Numerical Results", "content": "In this section, the first part discusses the construction of the dataset of turbulent channel flows. The second part compares the IFactFormer-m with other ML models, including FNO, IFNO and IFactFormer-o, as well as traditional LES methods including DSM and WALE."}, {"title": "4.1 Dataset of turbulent channel flows", "content": "We compute turbulent channel flows at three friction Reynolds numbers $Re_\\tau \\approx 180, 395$, and 590 on fine grids using DNS, employing an open-source framework Xcompact3D [46, 47]. We perform LES calculations and train ML models on a coarse grid. Table 1 presents the relevant parameters, with all simulations conducted in a cuboid domain of size $[4\\pi, 2, 4\\pi/3]$, where the X-direction represents the streamwise direction, the Y-direction the wall-normal direction, and the Z-direction the spanwise direction. $\\Delta X^+$ and $\\Delta Z^+$ represent the normalized grid spacings in the streamwise and spanwise directions, respectively, while $\\Delta Y^+$ indicates the distance from the wall to the first grid point. The superscript \u201c+\u201d denotes a distance that has been non-dimensionalized by the viscous lengthscale $\\delta_v$, e.g., $y^+ = y/\\delta_v$.\nWe perform filtering and interpolation on the DNS data to obtain filtered DNS (fDNS) data on the LES grid. The fDNS data is then used for training and testing the ML models. The DNS time step is set to 0.005, while the time step of the ML model is 200 times larger. Considering the wall viscous time $\\tau_\\nu = \\delta_v / v$, the wall viscous time steps of the ML model are 7.5$\\tau_\\nu$, 14.6$\\tau_\\nu$, and 20.7$\\tau_\\nu$ for the three Reynolds numbers $Re_\\tau \\approx 180, 395$, and 590, respectively. A total of 21 fDNS datasets are generated, each retaining data from 400 snapshots. Among the first 20 datasets, 80% and 20% are randomly selected for training and testing the model, respectively, with the final dataset reserved for post-analysis. For all ML models, the previous frame snapshot of velocity field $u^{(T)}$ is used to predict the next frame snapshot $u^{(T+1)}$."}, {"title": "4.2 Results comparision", "content": "The first comparison focuses on several different ML models, with an emphasis on both the accuracy of short-term predictions and the stability of long-term predictions. To ensure a fair comparison, the training parameters for all models are kept consistent across the same dataset. The AdamW optimizer [48] is employed, with an initial learning rate of 0.0005. The step learning rate scheduler is multiplied by a factor of 0.7 every 5 epochs, and the batch size is 2. Detailed hyperparameter settings for various models are provided in Table 2. Here, \"Layer\" refers to the number of layers for the FNO model, while for the other three models, it represents the number of implicit iterations. \u201cModes\u201d refers to the number of frequencies retained in the frequency domain, \u201cHeads\u201d refers to the number of multi-head attention and \"Dim\" refers to the number of channels in the latent space. The relative $L_2$ error is used as the loss function for both training and testing as follows:\n$L_2 = \\frac{|| \\hat{u} - u ||}{||u||},$\nwhere $\\hat{u}$ represents the predicted velocity field and u is the ground truth of velocity field.\nFigure 3 shows the test loss curves for four ML models in turbulent channel flows with different friction Reynolds numbers $Re_\\tau$. Both the IFactFormer-o and IFactFormer-m models outperform FNO and IFNO in terms of convergence speed and accuracy. The IFactFormer-m model achieves a higher accuracy after just one training step, surpassing the accuracy of FNO and IFNO at convergence. This result demonstrates the powerful fitting capability of the transformer-based model, enabling high-precision predictions in a short time frame. The accuracy of IFactFormer-m is significantly higher than that of IFactFormer-o, which demonstrates the effectiveness of the parallel factorized attention. Additionally, we observe that as the Reynolds number increases, the test error at convergence for all models tends to rise, indicating that the learning difficulty increases with the growing nonlinearity of the system.\nWe utilize a group of data that is excluded from both the training and test sets, and perform long-term forecasting using four different ML models through an autoregressive approach. The total number of forecasted time steps is 400, which means that the fluid passes through the channel approximately 21.25 times in a physical sense. The total wall viscous time spans are 3000$\\tau_\\nu$, 5840$\\tau_\\nu$, and 8280$\\tau_\\nu$ for the three Reynolds numbers, respectively. By analyzing these predictions, we can compare the long-term forecasting capabilities of the different models.\nPearson correlation coefficient is used to measure the degree of linear correlation between two variables a and b, and its formula is given as follows:\n$r = \\frac{\\sum_{i=1}^{n} (a_i - \\bar{a}) (b_i - \\bar{b})}{\\sqrt{\\sum_{i=1}^{n} (a_i - \\bar{a})^2 \\sum_{i=1}^{n} (b_i - \\bar{b})^2}}.$\nHere n is the number of grids, ( ) representes the mean values over the spatial grid. This coefficient closer to 1 indicates a stronger correlation. Figure 4 shows the Pearson correlation coefficients among the predicted streamwise velocity from four models, two LES methods, and fDNS at each forecasted time step. The correlation of FNO and IFNO sharply declines within very short time steps, even dropping below zero at $Re_\\tau \\approx 395$, and 590, followed by divergent behavior that prevents further predictions in these three cases. Although the IFactFormer-o model is capable of making 400-step predictions, the correlation coefficient gradually decreases over time. Among the four ML models, only the IFactFormer-m model is able to maintain a correlation coefficient around 0.9 over the 400-step prediction. In the traditional LES methods, the WALE model diverges at $Re_\\tau \\approx 590$. For the other two Reynolds numbers $Re_\\tau \\approx 180$, and 395, the correlation coefficients of the WALE model surpass those of the DSM model, but are slightly lower than those of the IFactFormer-m model.\nFigures 5-7 present cross-sectional snapshots of the streamwise velocity fields in an x-y plane predicted by the four ML models at different Reynolds numbers in the 10th, 50th, 200th, and 400th time steps. As the Reynolds number increases, the turbulent channel flow exhibits more small-scale features. Comparing the images in the first column of Figure 6 and 7, it can be observed that FNO and IFNO have lost a significant amount of small-scale structures in their predictions at the tenth time step, only capturing the relatively larger-scale structures. In contrast, both IFactFormer-o and IFactFormer-m are able to retain the small-scale structures. This may be attributed to the high-frequency truncation in the frequency domain required by FNO and IFNO. As time progresses, the IFactFormer-o model begins to predict an increasing number of \u201cnon-physical\" states. In contrast, the IFactFormer-m model significantly alleviates this issue, accurately maintaining multi-scale structures of turbulent channel flows even after 400 time steps.\nAmong the four ML models mentioned above, only the IFactFormer-m model achieves stable long-term predictions. Therefore, subsequent comparisons are made only between the traditional LES method and the predicted results of IFactFormer-m. Due to the divergent behavior of the WALE model in simulating channel turbulence at $Re_\\tau \\approx 590$ on the grid used in this study, the DSM model is the only one considered as the representative LES model in this case. All comparative results presented below are time-averaged statistics obtained by averaging 400 time steps.\nFigure 8 compares the streamwise and spanwise energy spectra of the IFactFormer-m model and the traditional LES model at various Reynolds numbers. At different Reynolds numbers, the energy spectra predicted by the IFactFormer-m model are closer to the fDNS spectra than those calculated by the DSM and WALE traditional LES models. We observe that as the Reynolds number increases, the high-frequency portion of the streamwise energy spectrum predicted by the IFactFormer-m model is relatively smaller than that of the fDNS, indicating that as time steps progress, the error in the IFactFormer model accumulates at the small scales. However, this does not severely impact the prediction of IFactFormer-m of the large scales. For the energy spectrum in the non-dominant direction, while IFactFormer-m outperforms the DSM and WALE models, there is still significant room for improvement. An effective approach could be to use a diffusion model to correct the model's predictions, thereby reducing the errors in the energy spectrum [33].\""}, {"title": "5 Conclusions", "content": "In this paper, we propose a modified implicit factorized transformer model (IFactFormer-m), which significantly enhances model performance by modifying the original chained factorized attention to parallel factorized attention. Compared to FNO, IFNO, and the original IFactFormer (IFactFormer-o), the IFactFormer-m model achieves more accurate short-term predictions of the flow fields and more stable long-term predictions of statistical quantities in turbulent channel flows at various Reynolds numbers. We further compare the IFactFormer-m model with traditional LES methods (DSM and WALE), using a range of time-averaged statistical quantities, including the energy spectrum, mean streamwise velocity, rms fluctuating velocities, and shear Reynolds stress. The results demonstrate that the trained IFactFormer-m model is capable of rapidly achieving accurate long-term predictions of statistical quantities, highlighting the potential of ML methods as a substitute for traditional LES approaches.\nOf course, current ML models also face a number of challenges, including but not limited to: 1. The training process still requires a large amount of data; 2. The relationship between the stability of long-term predictions and factors including model architecture and training strategies remains unclear; 3. The generalization ability of the model is inferior to that of traditional methods.\nLooking ahead, developing a physics-informed machine learning model with lower data or no data requirements that can provide stable long-term predictions of turbulence warrants further investigation [49-56]. Additionally, it would be highly meaningful to develop a machine learning model capable of generalizing across Reynolds numbers and geometries."}]}