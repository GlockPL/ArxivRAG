{"title": "BANER: Boundary-Aware LLMs\nfor Few-Shot Named Entity Recognition", "authors": ["Quanjiang Guo", "Yihong Dong", "Ling Tian", "Zhao Kang", "Yu Zhang", "Sijie Wang"], "abstract": "Despite the recent success of two-stage prototypical networks in few-shot named entity recognition (NER), challenges such as over/under-detected false spans in the span detection stage and unaligned entity prototypes in the type classification stage persist. Additionally, LLMs have not proven to be effective few-shot information extractors in general.\nIn this paper, we propose an approach called Boundary-Aware LLMs for Few-Shot Named Entity Recognition (BANER) to address these issues. We introduce a boundary-aware contrastive learning strategy to enhance the LLM's ability to perceive entity boundaries for generalized entity spans. Additionally, we utilize LoRAHub to align information from the target domain to the source domain, thereby enhancing adaptive cross-domain classification capabilities. Extensive experiments across various benchmarks demonstrate that our BANER framework outperforms prior methods, validating its effectiveness. In particular, the proposed strategies demonstrate effectiveness across a range of LLM architectures.", "sections": [{"title": "1 Introduction", "content": "Named Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP) that aims to detect the entity spans of text and classify them into pre-defined set of entity types. When there are sufficient labeled data, deep learning-based methods (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016) have achieved impressive performance. However, in practical applications, it is desirable to recognize new entity types that have not been seen in the source domain. It is time-consuming and labor-expensive to collect extra labeled data for these new types. Consequently, few-shot NER (Fritzler et al., 2019; Yang and Katiyar, 2020), which involves identifying unseen entity types based on only a few labeled samples for each class (also known as support samples) in the target domain, has attracted a lot of attention in recent years.\nPreviously end-to-end metric learning based methods (Yang and Katiyar, 2020; Das et al., 2022) dominate the field of few-shot NER. These approaches are designed to learn the intricate structure that includes both entity boundaries and entity types. However, their performance may degrade significantly when encountering a substantial domain gap. This degradation is primarily due to the challenge of understanding such complex structural information with only a few support examples for domain adaptation. Consequently, these methods often suffer from inadequate perception of boundary information, resulting in frequent misclassification of entity spans. Though LLMs have made remarkable success in various tasks, they have not proven to be effective few-shot information extractors in general (Ma et al., 2023; Zhang et al., 2024b)."}, {"title": "2 Related Work", "content": "Recent works demonstrate that adopting two-stage prototypical networks (Wang et al., 2022; Ma et al., 2022b; Li et al., 2023) can be effective to address aforementioned issue, which decompose NER task into two distinct stages: entity span detection and entity type classification tasks, executing each task sequentially. Since decomposed methods only need to locate the spans of named entities and are class-agnostic in the first stage, they can identify more accurate entity boundaries and achieve better performance than end-to-end approaches.\nWhile these two-stage prototypical methods have shown promising progress, they also present two additional challenges. Firstly, at the entity span detection stage, these decomposed approaches merely detect possible spans, often overlooking the boundary-related semantic information of named entities. For instance, following entity span detection, the sentence in Figure 1(a) illustrates that the span for \"Barack Obama\u201d is inadequately detected, resulting in \u201cObama\u201d being identified while \"Barack\u201d is overlooked. Conversely, the span for \"1961\" is excessively detected as \u201cin 1961\u201d. These inaccuracies propagate errors into the subsequent entity type classification stage.\nSecondly, in decomposed methods, prototypical networks aim to learn a type-related metric similarity function from test samples to classify entities based on their distance to prototypes. However, since the obtained prototypes are independently trained relative to the first stage, they may overlook entity type knowledge from the prior source domain. This can lead to difficulties in aligning the distribution of the same class across different domains. For example, in Figure 1(c), the entity types in the target domain exist independently of those in the source domain, leading to misaligned prototypes for the same entity type. This misalignment can severely impact the cross-domain performance of few-shot NER during the entity type classification stage.\nTo this end, we propose an approach called Boundary-Aware LLMs for Few-Shot Named Entity Recognition (BANER). Our approach adopts the two-stage framework of the decomposed method but advances two steps further to effectively address the aforementioned challenges. For entity span detection, we design a boundary-aware contrastive learning strategy to reduce the gap between span embeddings of entities and their corresponding types using LLM. This strategy enhances the boundary perception capabilities of LLM, particularly for generalized entity spans. For entity type classification, we draw upon domain adaptation principles to construct refined prototypes that retain and align entity type knowledge from the source domain. This approach involves joint pre-training in the source domain and adaptive alignment across diverse target domains within the same LLM framework, facilitated by LoRAHub (Huang et al., 2023).\nIn summary, our contributions are as follows:\n(1) We introduce a novel Few-Shot NER approach, BANER, which employs boundary-aware contrastive learning to enhance an LLM's ability to perceive entity boundaries. To our knowledge, this is the first integration of LLM with contrastive learning for few-shot NER tasks.\n(2) Leveraging an LLM pretrained on the source domain, we utilize LoRAHub to align information from target domains to enhance adaptive cross-domain classification capabilities.\n(3) Experimental results on multiple few-shot NER datasets demonstrate that BANER achieves superior performance compared to previous state-of-the-art two-stage decomposed methods. Furthermore, we validate the generalizability of our strategies across various LLM architectures."}, {"title": "3 Methodology", "content": "Figure 2 depicts the overall framework of our BANER. Like other two-stage methods, it comprises entity span detection and entity type classification. Notably, our approach incorporates boundary-aware contrastive learning and adaptive domain alignment strategies at these respective stages.\nGiven a sequence $X = \\{x_i\\}_{i=1}^L$ with L tokens, NER aims to assign each token $x_i$ to its corresponding label $y_i \\in Y \\cup O$, where Y is the pre-defined entity type set and O denotes non-entities. For few-shot NER, the NER model is first pretrained on data-sufficient source domain(s) $D_s = \\{(S_s, Q_s, Y_s)\\}$ and then fine-tuned in target domain(s) $D_t = \\{(S_t, Qt, Yt)\\}$ with only a few labeled samples. We adhere to the standard N-way K-shot setting as outlined in (Ding et al., 2021), where $S_{s/t} = \\{(x_i, Y_i)\\}_{i=1}^{N \\times K}$ denotes the support set, $Q_{s/t} = \\{(x_j, y_j)\\}_{j=1}^{NK'}$ denotes the query set, $|Y| = N$ and $|Y_t| = N'$. Our task is to recognize entities in the query set Qt from the target domain after adapting the model using its support set St. It is noteworthy that Ys and Y\u2081 exhibit little to no overlap.\n\n\nFormally, we denote the LLM as $f_{LLM}$ and input instruction as I. The output (generated) token sequence is denoted as $Y = f_{LLM}(X) = \\{Y_t\\}_{t=1}^L$. For the classic auto-regressive generative model, the sampling probability of the model generating Y is formalized as follows:\n$P(Y | I, X) = \\prod_{t=1}^L P(y_t | I, X, Y_{<t}),$ (1)\nwhere $y_t$ is the t-th token of the y, $y_{<t}$ represents the tokens before $y_t$. Utilizing generative language models for information extraction typically involves providing a prompt as input and generating results according to a specified format. In BANER, we adopt the default template for LORA fine-tuning. The prompt is fed into the LLM to perform entity span detection. An example of such a prompt is illustrated in Figure 5 in Appendix A.1. According to the LLM's token generation rule, the objective loss for auto-regressively generating Y is as follows:\n$L_{g} = -\\sum_{(X, y) \\in D_{s}} \\sum_{t=1}^{L}log P_{\\theta + \\theta_{L1}}(Y_t | I, X, Y_{<t}),$ (2)\nwhere \u03b8 is the original parameters of LLM, \u03b8\u2081 is the LoRA parameters. Note that we only update LORA parameters during the training process.\nWe enumerate all m spans $S = \\{s_1, s_2, ..., s_m\\}$ for sequence X. For example, for sentence \u201cBarack Obama was born in 1961\", span indices (begin, end) of two entities are {(0,2), (5,6)}. We use $b_i$ and $e_i$ to denote the begin- and end- index representation of the span $s_i$ in constructed prompt, respectively.\nTo enhance the LLM's ability to perceive entity boundaries, we employ the concept of contrastive learning (Khosla et al., 2020). We utilize two types of boundary-aware index representations, as illustrated in Figure 2(a), to construct positive and negative samples for each entity mention and its corresponding entity type. Specifically, the positive sample $pos_i$ of entity span is calculated by concatenating $h_{b_i}$ and $h_{e_i-1}$ as $pos_i = [h_{b_i}, h_{e_i-1}]$, where $h(\u00b7) = embedding(\u00b7)$ is the pre-trained tokenizer of LlaMA-2-7B. The negative sample $neg_i$ of entity boundary is $neg_i = [h_{b_i-1}, h_{b_i-2}, h_{e_i}, h_{e_i+1}]$. The original entity type representation $o_i$ is the (begin, end) indices of entity type from constructed prompt in the same way.\nTo learn better boundary-aware feature space, we extract entity type embedding $e_o$, entity token embedding $e_{pos}$, and $e_{neg}$, from outputs $H \u2208 R^{B\u00d7L\u00d7D}$ of 25th hidden states layer in LlaMA-2, where B is the batch size and D is the hidden dimension. The calculation formula are:\n$eo_{i} = gather(H, o_{i}) \\in R^{B \\times 1\\times D},$ (3)\n$epos_{i} = gather(H, pos_{i}) \\in R^{B \\times 2\\times D}$ (4)\n$eneg_{i} = gather(H, neg_{i}) \\in R^{B \\times 4 \\times D},$ (5)\nwhere gather() is a tensor operation commonly used in deep learning frameworks (e.g., PyTorch), which allows for the selection and extraction of specific elements from a higher-dimensional tensor H based on specified indices. Then, we can calculate the boundary-aware contrastive loss by:\n$L_{cl} = -\\frac{1}{B}\\sum_{i=1}^B log (\\sigma(sim(o_i, pos_i) - sim(o_i, neg_i))),$ (6)\n$sim(o, pos_{i}) = \\frac{eo*epos_{i}}{\\|eo\\|_2 \\|epos_{i}\\|_2} \\in RB,$ (7)\n$sim(o, neg_{i}) = \\frac{eo*eneg_{i}}{\\|eo\\|_2 \\|eneg_{i}\\|_2} \\in RB,$ (8)\nwhere \u03c3() is the sigmoid function.\nWe introduce instruction tuning to effectively and efficiently align the LLM with the span detection task. Following the standard supervised fine-tuning method, we minimize the auto-regressive loss calculated between the ground truth and the LLM output. In our approach, we mask the loss positions corresponding to the prompt part. Specific prompt formats, task-specific instructions, and ground truth details are provided in the Appendix A.1. However, directly fine-tuning the entire model can be computationally intensive and time-consuming. To address this, we propose a lightweight fine-tuning strategy using LoRA. This method involves freezing the pre-trained model parameters and introducing trainable rank decomposition matrices into each layer of the Transformer architecture. This approach facilitates lightweight fine-tuning while reducing GPU memory consumption. The final learning objective is computed as follows:\n$L_{span} = min(L_{g} + \\lambda L_{cl}),$ (9)\nwhere $\u03b8_{L1}$ is the LoRA parameters at the span detection stage and \u03bb is set to 0.001.\nSubsequently, we assign a specific entity class to each span identified during the entity span detection stage.\nAs previously mentioned, a predefined (candidate) list of entity types must be input as the schema into the LLM to trigger type generation. Figure 6 in Appendix A.1 illustrates an example of the prompt used for this stage. Using this prompt, the model constructs a prototype for each given entity type, which is then used to assign the correct type to each detected entity span.\nTo achieve this, we construct prototypical networks (ProtoNet) as the backbone, utilizing LoRA tuning across different domains. To leverage the knowledge from support examples in the target domain and align it with the source domain, we propose enhancing ProtoNet on the LLM with domain adaptation. This approach aims to create a more representative embedding space where text spans from different entity classes are more distinguishable.\nLet $Sk = \\{Z_1, Z_2, ..., Z_n\\}$ denote the set of entity type spans in the constructed prompt, which is contained in a given support set St belonging to the entity class $Yk \u2208 Y$. We compute the prototype $p_k$ for each yk by averaging the span representations of all $z_i \u2208 S_k$:\n$p_k(St) = \\frac{1}{|S_k|} \\sum_{i=1}^{S_k} Z_i.$ (10)\nGiven a training episode $D_t$, we first utilize the support set St to compute prototypes for all entity classes in Yt using Eq. 10. Subsequently, for each span $s_i$ in the query set Qt, we calculate the probability that $s_i$ belongs to an entity class $y_k$ based on the distance between its span representation and the prototype of yk:\n$P (Y_k; zi) = \\frac{exp \\{-d (p_k (St), Si)\\}}{\\sum_{y_i \\in Y} exp \\{-d (pi (St), si)\\}}$, (11)\nTo compute the distance function d (\u00b7, \u00b7), we define it as follows:\n$d (pk/i (St), Si) = \\frac{Si * Pk/i (St)}{\\|Pk/i (St) \\|_2 \\|Si\\|_2} \\in [-1,1].$ (12)\nOur goal is to minimize the cross entropy loss for each LoRA module in its corresponding target domain:\n$L_{t1} = min (-\\sum_{zi \\in Q_t} log P_{\\theta{LORA}}(Y_k; zi)),$ (13)\nwhere $\u03b8_{L\u2082}$ is the LoRA parameters at the type classification stage.\nAs depicted in Figure 2(b), we initially fine-tuned LORA modules across various target domains. Specifically, for M distinct domains, we fine-tune M separate LoRA modules, each denoted as $m_i$ for the domain $D_{t_i} \u2208 D_t$. Each mi can be defined as the product $A_iB_i$, where $A_i \u2208 R^{d\u00d7r}$ and $B_i \u2208 R^{r\u00d7k}$ are trainable low-rank matrices, with the rank r being significantly smaller than the dimensions d and k. The combined LoRA module m can be obtained by:\n$m = (W_1A_1 + \u2026 + W_NAN) (W_1B_1 + \u22ef + W_NBN).$ (14)"}, {"title": "4 Experiments", "content": "To find the optimal w, the optimization process is guided by the cross-entropy loss to identify the best set $W_1, W_2,...,w_n$ that minimizes the loss $L_{t\u2081}$ on the target domain. Additionally, we incorporate L1 regularization to penalize the sum of the absolute values of w, helping to prevent extreme values. Consequently, the final objective of LoRAHub is to minimize $L_{t\u2081} + \u03b1 \u00b7 \u03a3_{i=1}|w_i|$, where \u03b1 serves as a hyperparameter.\nAs illustrated in Figure 2(c), during target domain inference, we first extract candidate spans from query sentences and then classify these spans into specific entity types to obtain the final results. After training the LLM with boundary-aware contrastive learning, we generate candidate entity spans from a given sentence X as follows:\n$P(S|X;\u03b8 + \u03b8_{L\u2081}) = \\prod_{i=1}^N P(yi|Y_{<t}, X; \u03b8 + \u03b8_{L\u2081}).$ (15)\nNext, we obtain the candidate span set Sspan, which includes all potential spans to be assigned entity types during the entity type classification stage. For these candidate spans, the entity types are classified as follows:\n$P(C\\X, S;\u03b8+\u03b8_{L2}) = \\prod_{i=1}^N P(yi|Y_{<t}, X, S; \u03b8+\u03b8_{L\u2082}).$ (16)\nFinally, we combine the results of span detection and type classification to determine the predicted labels for a sentence X as follows:\n$P(Y|S, C; \u03b8) = P(S|X;\u03b8 + \u03b8_{L\u2081}) \u00b7 P(C|X, S; \u03b8 + \u03b8_{L\u2082}).$ (17)\nTwo tasks are considered for this dataset: (1) intra, where all entities in the train/dev/test splits belong to different coarse-grained types, and (2) inter, where the train/dev/test splits may share coarse-grained types but have mutually exclusive fine-grained entity types.\nTo evaluate cross domain adaptation, we follow Das et al. (2022) and take OntoNotes 5.0 (General) (Weischedel et al., 2013) as our source domain, and evaluate few-shot domain adaptation performances on I2B2'14 (Medical) (Stubbs and Uzuner, 2015), CoNLL'03 (News) (Tjong Kim Sang and De Meulder, 2003), WNUT'17 (Social) (Derczynski et al., 2017), and GUM (Wiki) (Zeldes, 2017) datasets."}, {"title": "4.3 Ablation Study", "content": "To validate the effectiveness of the main components in BANER, we introduce the following variant baselines for the ablation study:\nBANER w/o Boundary-Aware Span Detection (BASD): This variant removes the boundary-aware contrastive learning at the span detection stage and directly extracts entity spans using LLMs.\nBANER w/o Domain-Adaptation LoRAHub (DAL): This variant removes the composition of different LoRA modules at the type classification stage, using a single LoRA module to classify entities instead.\nBANER w/o Span Detection Fine-Tuning (SDF): This variant skips the fine-tuning on the support set of the target domain at the span detection stage.\nBANER w/o Type Classification Fine-Tuning (TCF): This variant skips the fine-tuning on the support set of the target domain at the type classification stage.\nBANER w/o ALL: This variant performs the few-shot NER task using the original LLMs (e.g., LlaMA-2-7B) without any of the enhancements provided by BANER.\nFrom Table 5, we observe the following:\n1) The removal of the boundary-aware contrastive learning strategy results in a performance decline across most cases, particularly in entity-sparse datasets like I2B2, where many spans are falsely detected.\n2) Omitting the domain-aware LoRAHub leads to a significant performance decrease. This indicates that our model effectively aligns a better prototype space for entity types, which is crucial in cross-domain scenarios."}, {"title": "4.4 Examination of other LLMs", "content": "To evaluate the generalizability of our enhanced entity boundary perception, we extend BANER to other mainstream open-source LLMs under the GUM 5-shot setting, including Mistral-7B (Jiang et al., 2023) and LlaMA-3-8B."}, {"title": "5 Conclusion", "content": "In this paper, we propose the BANER framework for few-shot named entity recognition (NER), addressing entity span detection and entity type classification in two stages. For entity span detection, we introduce a boundary-aware contrastive learning strategy to minimize the distance between span embeddings of entities and their corresponding types using LLMs. Building on this, we employ domain adaptation with LoRAHub to construct more accurate prototypes that preserve and align entity type knowledge from the source domain during the entity classification stage. Extensive experiments demonstrate that BANER outperforms previous state-of-the-art methods and is applicable to various LLMs."}, {"title": "Limitations", "content": "Our work has two main limitations: 1) BANER employs a single, specific prompt template for each stage, utilizing descriptive task instructions and limited answer options. However, there exist numerous alternative templates for generative language models. This limitation suggests the potential for future research to explore various prompt templates to enhance entity boundary detection and entity type understanding. 2) Limited access to high-performance computing facilities has prevented us from evaluating our approach on large LLMs, such as LlaMA-3-70B. This limitation highlights the potential for future work to investigate different model architectures for improved few-shot NER performance."}, {"title": "A Appendix", "content": "Figures 5 and 6 provide examples of the prompts used in the two stages of our method. To tailor these prompts to our task, we design a specific output format for the LLM. Each output starts with <im_start> and ends with <im_end>. For instances involving multiple entity spans and types, we encapsulate them together using <<<>>>.\n1) one-stage methods:\n\u2022 ProtoBERT (Snell et al., 2017)is a popular few-shot method built on prototypical networks, utilizing BERT as its backbone;\n\u2022 NNShot (Wiseman and Stratos, 2019) is a straightforward approach that utilizes token-level nearest neighbor classification;\n\u2022 StructShot (Yang and Katiyar, 2020) adopts an additional Viterbi decoder on top of NNShot;\n\u2022 CONTaiNER (Das et al., 2022) leverages contrastive learning to infer the distributional distance between Gaussian embeddings of entities;\n\u2022 MANNER (Fang et al., 2023) uses a memory module and optimal transport to adapt source domain information for few-shot tasks in the target domain.\n2) two-stage methods:\n\u2022 ESD (Wang et al., 2022) enhances prototypical networks with inter- and cross-span attention, and introduces multiple prototypes for the O label;\n\u2022 DecomposedMetaNER (Ma et al., 2022b) integrates model-agnostic meta-learning into prototypical networks to more effectively leverage the support set;\n\u2022 TadNER (Li et al., 2023) employs type-aware contrastive learning and span filtering to construct precise prototypes and eliminate false spans;\n\u2022 TSFNER (Ji and Kong, 2024) incorporates a teacher span recognizer for generating soft labels, a student span recognizer, and a prompt-based entity classifier;\n\u2022 BDCP (Xue et al., 2024) introduces an entity boundary discriminative module for span detection and refines entity-context correlations to mitigate textual adversarial attacks.\nFollowing (Li et al., 2023), we substitute the original dataset labels with their corresponding natural-language forms of type names employed in our prompt. Tables 6 and 7 present the detailed conversions for various datasets."}]}