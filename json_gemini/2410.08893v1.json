{"title": "Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and Parameter Efficient", "authors": ["Wenlong Wang", "Ivana Dusparic", "Yucheng Shi", "Ke Zhang", "Vinny Cahill"], "abstract": "Model-based reinforcement learning (RL) offers a solution to the data\ninefficiency that plagues most model-free RL algorithms. However, learn-\ning a robust world model often demands complex and deep architectures,\nwhich are expensive to compute and train. Within the world model, dy-\nnamics models are particularly crucial for accurate predictions, and vari-\nous dynamics-model architectures have been explored, each with its own\nset of challenges. Currently, recurrent neural network (RNN) based world\nmodels face issues such as vanishing gradients and difficulty in capturing\nlong-term dependencies effectively. In contrast, use of transformers suf-\nfers from the well-known issues of self-attention mechanisms, where both\nmemory and computational complexity scale as O(n\u00b2), with n represent-\ning the sequence length.\nTo address these challenges we propose a state space model (SSM)\nbased world model, specifically based on Mamba, that achieves O(n)\nmemory and computational complexity while effectively capturing long-\nterm dependencies and facilitating the use of longer training sequences\nefficiently. We also introduce a novel sampling method to mitigate the\nsuboptimality caused by an incorrect world model in the early stages of\ntraining, combining it with the aforementioned technique to achieve a\nnormalised score comparable to other state-of-the-art model-based RL al-\ngorithms using only a 7 million trainable parameter world model. This\nmodel is accessible and can be trained on an off-the-shelf laptop. Our\ncode is available at https://github.com/realwenlongwang/drama.git.", "sections": [{"title": "Introduction", "content": "Deep Reinforcement Learning (RL) has achieved remarkable success in various\nchallenging applications, such as Go [Silver et al., 2016, 2017], Dota [Berner"}, {"title": "Method", "content": "We describe the problem as a Partially Observable Markov Decision Process\n(POMDP), where at each discrete time step t, the agent observes a high-\ndimensional image Ot\u2208 rather than the true state st \u2208 S with the conditional\nobservation probability given by p(Ot st). The agent selects actions from a dis-\ncrete action set at \u2208 A = {0,1,...,n}. After executing an action at, the agent\nreceives a scalar reward rt \u2208 R, a termination flag et \u2208 [0,1], and the next\nobservation Ot+1. The dynamics of the MDP is described by the transition\nprobability p(st+1,rt|st,at). The behaviour of the agent is determined by a pol-\nicy f(0t; 0), parameterised by 0, where f: \u25cb \u2192 A maps the observation space\nto the action space. The goal of this policy is to maximise the expected sum of\ndiscounted rewards E\u2211ytrt, given that y is a predefined discount factor.\nt\nUnlike model-free RL, model-based RL does not rely directly on real experi-\nences to improve the policy f(0t; 0) [Sutton and Barto, 1998]. Instead, it learns\na world model f(0t, at; w) from actual experiences to capture the dynamics of\nthe POMDP. The actual experiences are stored in a replay buffer, allowing them\nto be repeatedly sampled for training the world model. The world model consists\nof a variational autoencoder (VAE) [Kingma and Welling, 2013, Hafner et al.,\n2021], a dynamics model, and linear heads to predict rewards and termination\nflags. The details of our world model are discussed in Section 2.2.\nEach time the world model has been updated, a batch of experiences is sam-\npled from the replay buffer to initiate a process called 'imagination'. Starting\nfrom an actual initial observation and using an action generated by the current\nbehaviour policy, the dynamics model generates the next latent state. This pro-\ncess is repeated until the agent collects enough imagined samples to improve\nthe policy. We explain this process in detail in Section 2.3."}, {"title": "State Space Modelling with Mamba", "content": "State space models (SSMs) are mathematical constructs inspired by control\ntheory to represent the complete state of a system at a given point in time. These\nmodels map an input sequence to an output sequence x \u2208 R\u00b9 \u2192 y \u2208 R\u00b9, where\nI denotes the sequence length. In structured SSMs, a hidden state H\u2208R(n,l) is\nused to track the sequence dynamics, as described by the following equations:\nHt = AHt-1 + Bxt\n(1)\nYt = CT Ht\nwhere A \u2208 R(n,n), B\u2208 R(n,1), C\u2208 R(n,1) and Ht \u2208 R(n,1), of which on repre-\nsents the predefined dimension of the hidden state that remains invariant to\nthe sequence length. To efficiently compute the hidden states, it is common to\nstructure A as a diagonal matrix, as discussed in [Gu et al., 2022a, Gupta et al.,\n2022, Smith et al., 2023, Gu and Dao, 2024]. Additionally, selective SSMs, such\nas Mamba-1, extend the matrices (A, B, C) to be time-varying, introducing"}, {"title": null, "content": "an extra dimension corresponding to the sequence length. The shapes of these\ntime-varying matrices are A \u2208 R(T,N,N), B \u2208 R(T,N), and C\u2208 R(T,\u00d1) 1.\nDao and Gu [2024] introduced the concept of structured state space duality\n(SSD), which further restricts the diagonal matrix A to be a scalar multiple of\nthe identity matrix, forcing all diagonal elements to be identical. To compensate\nfor the resulting reduced expressive power, Mamba-2 introduces a multi-head\ntechnique, akin to attention, by treating each input channel as p independent\nsequences. Unlike Mamba-1, which computes SSMs as a recurrence, Mamba-2\napproaches the sequence transformation problem through matrix multiplication,\nwhich is more GPU-efficient:\nCH\nYt\nt\nYt = \u2211C At: Bili\n(2)\ni=0\nwhere At:i is At At-1... Ai+1. This allows the SSM to be formulated as a\nmatrix transformation:\ny = SSM(x; A, B, C) = Mx\nJCAt:iBi if j \u2265 i\nMj,i :=\n(3)\n{\u00b0 if j < i\nMamba-2 reformulates the state-space equations as a single matrix multipli-\ncation by utilising semi-separable matrices [Vandebril et al., 2005, Dao and Gu,\n2024], which is well known in computational linear algebra as shown by Figure\n1. The matrix M can also be written as:\nM = L\u00b0 CBT ER(T,T)\n1\n\u04301\na2\nL =\na201\n1\n(4)\n:\naT-1\n\u0430\u0442-1... 1 \u0430\u0442-1... 2\nwhere at \u2208 [0, 1] is a input-dependent scalar. The matrix L connects the SSM\nmechanism with the causal self-attention mechanism by removing the softmax\nfunction and applying a mask matrix L to the 'attention-like' matrix. It is, in\nfact, equivalent to causal linear attention when all at = 1.\nAs a result, Mamba-2 achieves 2-8 times faster training speeds than Mamba-\n1, while maintaining linear scaling with sequence length."}, {"title": "World Model Learning", "content": "Our world model has two main components: an auto-encoder and a dynamics\nmodel. Additionally it includes two MLP heads for reward and termination\npredictions. The architecture of the world model is illustrated in Figure 1."}, {"title": "Discrete Variational Auto-encoder", "content": "The autoencoder extends the standard variational autoencoder (VAE) architec-\nture [Kingma and Welling, 2013] by incorporating a fully-connected layer to dis-\ncretise the latent embeddings, consistent with previous approaches [Hafner et al.,\n2021, Robine et al., 2023, Zhang et al., 2023]. The raw observation is a three-\ndimensional image, Ot \u2208 [0,255](h,w,c), at time step t. The encoder compresses\nthe observation into a vector of discrete numbers, denoted as zt ~ p(zt|0t).\nThe decoder reconstructs the raw image, \u00d4t, from zt. Gradients are passed\ndirectly from the decoder to the encoder using the straight-through estimator,\nbypassing the sampling operation during backpropagation [Bengio et al., 2013]."}, {"title": "Dynamics Model", "content": "The dynamics model simulates the environment in the latent variable space,\nzt, using a deterministic state variable, dt. Since we are employing SSMs like\nMamba-1 and Mamba-2, this should not be confused with the hidden states\ntypically used by SSMs to track dynamics. At each time step t, the next token\nin the sequence is determined by both the current latent variable, zt and the\ncurrent action at. To integrate these, we first concatenate them and project the\nresult using a fully-connected layer before passing it to the dynamics model.\nGiven a sequence length 1, the deterministic state is derived from all previous\nlatent variables and actions. The dynamics model can be expressed as:\ndt = f(zt-l:t, at-l:t; w)\nDynamics model:\n(5)\nLatent variable predictor:\n2t+1~P(2t+1 dt; w)\nWe implement the dynamics model with Mamba-2 [Dao and Gu, 2024].\nSpecifically, each time a batch of samples, denoted as 0 \u2208 [0,255] (6,1,h,w,c),\nis drawn from the experience buffer E, where b represents batch size,l the\nsequence length, and i, w, c the image height, width, and channel dimension re-\nspectively. After encoding, the batch will be compressed to Z \u2208 R(b,1,d) whered\nis the dimension of the latent variable. The latent variable passes through a lin-\near layer with the action to produce the input X \u2208 R(b,l,d) of the Mamba blocks.\nTo fully leverage the parallel computational capabilities of GPUs, the training\nprocess must not be strictly sequential. As a result, the targets of the dynamic\nmodel are independent of its outputs, which contrasts with the approach used\nin Dreamer V3.\nMamba-1 first transforms the input tensor Xb,:l,d into a sequence of hidden\nstates \u0397 \u2208 R(6,1-1,n), which are then mapped back to the deterministic state"}, {"title": null, "content": "sequence Db,:1,d using time-varying parameters. Since the hidden states oper-\nate in a fixed dimension n (unlike standard attention mechanisms, where the\nstate scales with the sequence length), Mamba-1 achieves linear computational\ncomplexity with respect to sequence length.\nMamba-2 applies a similar transformation but leverages matrix multiplica-\ntion. The input tensor X's dimension d is first split into d/p heads, which\nare processed independently. The transformation matrix is a specially designed\nsemiseparable lower triangular matrix, which can be decomposed into q \u00d7 q\nblocks. Different types of blocks are designed for specific purposes, such as\nhandling causal attention over short ranges and transforming the hidden states."}, {"title": "Behaviour Policy Learning", "content": "The behaviour policy is trained within the 'imagination', an auto-generative\nprocess driven by the dynamics model. Specifically, a batch of shape (bimg, limg)\nis sampled from the replay buffer, where b starting points are sampled and limg\nconsecutive steps are selected starting from each. Since the Mamba dynamics\nmodel is efficient at handling long sequences, we can leverage actual experiences\nto estimate a more informative hidden state for the 'imagination' process. The\nrollout begins from the last transition in the sequence, limg, and continues for\nh steps. Notably, the rollout does not stop when an episode ends, unlike the\nprior SSM-based meta-RL model [Lu et al., 2023] where the hidden state must\nbe manually reset, as the Mamba-based dynamics model automatically resets\nthe state at episode boundaries [Gu and Dao, 2024].\nA key difference between Mamba-based and transformer-based world mod-\nels in the 'imagination' process is that Mamba updates inference parameters\nindependent of sequence length. This independence is crucial for accelerating\nthe 'imagination' process, a significantly time-consuming component in model-\nbased RL. The behaviour policy's state concatenates the prior discrete variable\n2t with the deterministic variable ht to exploit the temporal information. While\nthe behaviour policy utilises a standard actor-critic architecture, other on-policy\nalgorithms can also be applied. In this work, we adopt the recommendations\nfrom [Andrychowicz et al., 2020] and adjust the loss functions and value nor-\nmalisation techniques as described in [Hafner et al., 2024]."}, {"title": "Dynamic Frequency-Based Sampling (DFS)", "content": "In model-based RL, the behaviour model often underestimates rewards due\nto inaccuracies in the world model, impeding exploration and error correction\n[Sutton and Barto, 1998]. These inaccuracies are particularly common early in\ntraining when the model relies on limited data. Thus, we propose a sample-\nefficient method to address this issue, i.e., Dynamic Frequency-based Sampling\n(DFS).\nThe primary objective is to sample transitions that the world model has suf-\nficiently learned to initiate 'imagination'. To accomplish this, we introduce two\nvectors during training, each matching the length of the transition buffer |E|."}, {"title": null, "content": "For the world model, v = (v1, v2, ..., \u03c5|\u03b5|), where vi \u2208 Z+ for i \u2208 {1, 2, . . ., |E|},\ntracks the number of the transition has been used to improve the world model.\nThe consequencing sampling probability is denoted as, (P1,P2,...,P\\\u03b5|) =\nsoftmax(-v). For 'imagination', b = (b1,b2,..., b\u025b]), where b\u00bf \u2208 Z+ for i \u2208\n{1,2,...,|E|}, counts the times of transition has been used to improve\nthe behaviour policy. The resulting sampling probability is denoted as,\n(P1,P2,...,\u03a1\u03b5) = softmax(f(v, b)), where f (v,b) = v \u2212 b - max(0, v \u2013 b).\nNote that for behaviour policy training, DFS employs balanced sampling\nsimilar to [Robine et al., 2023]. During training, two cases arise: 1) \u2203i \u2208 |E|,\nVi \u2265 bi, f(vi, bi) = 0, In this case, the transition has been trained more\nfrequently with the world model than with the behaviour policy, suggesting\nthat the world model is likely capable of making accurate predictions from this\ntransition. 2) \u2203i \u2208 |E|, vi < bi, f (Vi, bi) = vi - bi, indicating that the transition is\neither under-trained as a starting point for the world model generation process\nor has been over-fitted to the behaviour policy. Consequently, the probability\nof selecting this transition for behaviour policy training decreases. These two\nmechanisms ensure that 'imagination' sampling favors transitions learned by\nthe world model, while avoiding excessive determinism."}, {"title": "Experiments", "content": "In this work, the proposed DRAMA framework is implemented on top of the\nSTORM infrastructure [Zhang et al., 2023]. We evaluate the model using the\nAtari100k benchmark [Kaiser et al., 2020], which is widely used for assess-\nsing the sample efficiency of RL algorithms. Atari100k limits interactions with\nthe environment to 100,000 steps (equivalent to 400,000 frames with 4-frame\nskipping). We present the benchmark and analyse our results in Section 3.1.\nAblation experiments and their analysis are provided in Section 3.2."}, {"title": "Results", "content": "We compare our model against several benchmarks across 26 Atari games. In\nTable 1, the 'Normalised Mean' refers to the average normalised score, calculated\nas: (evaluated_score-random_score)/(human_score-random_score). For\neach game, we train DRAMA with 5 different seeds and track training perfor-\nmance using a running average of 5 episodes, as recommended by Machado et al.\n[2018], a practice also followed in related work [Hafner et al., 2024].\nDespite utilising an extra-small world model (7M parameters, referred to\nas the XS model), we achieve performance comparable to IRIS and TWM.\nFurthermore, by employing a stronger auto-encoder and a larger SSM hidden\nstate dimension (10M parameters, referred to as the S model), we demonstrate\nimproved results in ablation experiments on a reduced set of games. However,\nwe emphasise that our goal is not to achieve the highest benchmark ranking,\nbut to illustrate that Mamba can serve as a solid foundation for the dynamics\nmodel in model-based RL."}, {"title": "Ablation experiments", "content": "We selected a representative subset of games for our ablation experiments.\nKrull is a multi-scene game with dense rewards, while Boxing is a single-scene\ngame featuring an AI-controlled opponent. Freeway is a sparse reward game\nthat requires exploration, and Kangaroo demands multitasking and object iden-\ntification for different actions."}, {"title": "Dynamic Frequency-Based Sampling", "content": "In this experiment, we compare DFS with the uniform sampling method in a\nMamba-2 based Drama. As shown in Figure 2, DFS demonstrates a significant\nadvantage over uniform sampling in the games Freeway and Kangaroo, with\na smaller advantage observed in Krull and Boxing. The ablation results fur-\nther highlight the effectiveness of DFS in mitigating the suboptimality of the\nbehaviour policy when learning within a flawed world model. This is especially\nevident in Freeway, where agents often become trapped in a passive policy,\nwaiting for the game to end without taking any meaningful action."}, {"title": "Mamba-1 vs. Mamba-2", "content": "As mentioned in Sec 2.1, Mamba-2 imposes restrictions on A for efficiency. How-\never, it remains an open question whether these constraints negatively affect the\nperformance of SSMs, as previous studies have not offered comprehensive the-\noratical or empirical evidence on the matter [Dao and Gu, 2024]. In response\nto this gap, we compare Mamba-2 and Mamba-1 as the backbone of the world\nmodel in model-based RL. Ablation experiments were conducted using DFS,\nwith both Mamba-1 and Mamba-2 configured with the same default hyperpa-\nrameters.\nFigure 3 illustrates that Mamba-2 outperforms Mamba-1 in games Krull,\nBoxing and Freeway. In Krull, the player navigates through different scenes\nand solves various tasks. In the later stages, rescuing the princess while avoiding\nhits results in a significant score boost, while failure leads to a plateau in score.\nAs shown, Mamba-1 experiences a score plateau in Krull, whereas Mamba-2\nsuccessfully overcomes this challenge, leading to higher performance. Note that\nFreeway is a sparse reward game requiring high-quality exploration. A positive\ntraining effect is achieved only by combining DFS with Mamba-2 without any\nadditional configuration."}, {"title": "More trainable parameters", "content": "As model-based RL agents consist of multiple trainable components, tuning the\nhyperparameters for each part can be resource-intensive and is not the primary\nfocus of this research. Previous work has demonstrated that increasing the\nneural network's size often leads to stronger performance on benchmarks Hafner\net al. [2024]. In Figure 4, we demonstrate that Drama achieves overall better\nperformance when using a more robust auto-encoder and a larger SSM hidden\nstate dimension n. Notably, the S model exhibits significantly improved results\nin games like Breakout and BankHeist, where pixel-level information plays a\ncrucial role."}, {"title": "Related work", "content": ""}, {"title": "Model-based RL", "content": "The origin of model-based RL can be traced back to the Dyna architecture in-\ntroduced by Sutton and Barto [1998], although Dyna selects actions through\nplanning rather than learning. Notably, Sutton and Barto [1998] also high-\nlighted the suboptimality that arises when the world model is flawed, especially\nas the environment improves. The concept of learning in 'imagination' was\nfirst proposed by Ha and Schmidhuber [2018], where a world model predicts\nthe dynamics of the environment. Later, SimPLe [Kaiser et al., 2020] applied\nMBRL to Atari games, demonstrating improved sample efficiency compared to\nstate-of-the-art model-free algorithms. Beginning with Hafner et al. [2019], the\nDreamer series introduced a GRU-powered world model to solve a diverse range\nof tasks, such as Mujoco, Atari, Minecraft, and others [Hafner et al., 2020,\n2021, 2024]. More recently, inspired by the success of transformers in NLP,\nmany MBRL studies have adopted transformer architectures for their dynamics\nmodels. For instance, IRIS [Micheli et al., 2023] encodes game frames as sets of\ntokens using VQ-VAE [Oord et al., 2018] and learns sequence dependencies with\na transformer. In IRIS, the behavior policy operates on raw images, requiring an\nimage reconstruction during the 'imagination' process and an additional CNN-\nLSTM structure to extract information. TWM [Robine et al., 2023], another\ntransformer-based world model, uses a different structure. It stacks grayscale\nframes and does not activate the dynamics model during actual interaction\nphases. However, its behaviour policy only has access to short-term temporal\ninformation, raising questions about whether learning from tokens that already\ninclude this short-term information could be detrimental to the dynamics model.\nSTORM [Zhang et al., 2023], closely following DreamerV3, replaces the GRU\nwith a vanilla transformer. Additionally, it incorporates a demonstration tech-\nnique, populating the buffer with expert knowledge, which has shown to be\nparticularly beneficial in the game Freeway."}, {"title": "Structure State space model based RL", "content": "Structured SSMs were originally introduced to tackle long-range dependency\nchallenges, complementing the transformer architecture [Gu et al., 2022b, Gupta\net al., 2022]. However, Mamba and its successor, Mamba-2, have emerged as\npowerful alternatives, now competing directly with transformers [Gu and Dao,\n2024, Dao and Gu, 2024]. Deng et al. [2023] implemented an SSM-based world\nmodel, comparing it against RNN-based and transformer-based models across\nvarious prediction tasks. Despite this, SSM-based world models have yet to\nbe tested in the context of model-based RL, including Mamba-1 and Mamba-\n2. Mamba-1 has recently been applied to offline RL, either with a standard\nMamba-1 block [Ota, 2024] or a Mamba-attention hybrid model [Huang et al.,\n2024]. Lu et al. [2023] proposed applying modified SSMs to meta-RL, where\nhidden states are manually reset at episode boundaries. Since both Mamba-1\nand Mamba-2 are input-dependent, such resets are unnecessary."}, {"title": "Conclusion", "content": "In conclusion, DRAMA, our proposed Mamba-based world model, addresses key\nchallenges faced by RNN and transformer-based world models in model-based\nRL. By achieving O(n) memory and computational complexity, our approach\nenables the use of longer training sequences. Furthermore, our novel sampling\nmethod effectively mitigates suboptimality during the early stages of training,\ncontributing to a model that is both lightweight, with only 7 million train-\nable parameter world model, and accessible, being trainable on standard hard-\nware. Overall, our method achieves a normalised score competitive with other\nstate-of-the-art RL algorithms, offering a practical and efficient alternative for\nmodel-based RL systems. Although Drama enables longer training and infer-\nence sequences, it does not demonstrate a decisive advantage that would allow\nit to dominate other world models on the Atari100k benchmark. An interest-\ning direction for future work is to explore specific tasks where longer sequences\ndrive superior performance in model-based RL. Despite advances in world mod-\nels, model-based RL still faces several challenges, such as long-horizon behaviour\nplanning and learning, informed exploration, and the dynamics of jointly train-\ning the world model and behaviour policy. Another promising future direction\nis to investigate to what extent Mamba can help address these challenges."}]}