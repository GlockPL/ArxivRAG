{"title": "Large Investment Model", "authors": ["Jian Guo", "Heung-Yeung Shum"], "abstract": "Traditional quantitative investment research is encountering diminishing returns alongside rising labor and time costs. To overcome these challenges, we introduce the Large Investment Model (LIM), a novel research paradigm designed to enhance both performance and efficiency at scale. LIM employs end-to-end learning and universal modeling to create an upstream foundation model capable of autonomously learning comprehensive signal patterns from diverse financial data spanning multiple exchanges, instruments, and frequencies. These \"global patterns\" are subsequently transferred to downstream strategy modeling, optimizing performance for specific tasks. We detail the system architecture design of LIM, address the technical challenges inherent in this approach, and outline potential directions for future research. The advantages of LIM are demonstrated through a series of numerical experiments on cross-instrument prediction for commodity futures trading, leveraging insights from stock markets.", "sections": [{"title": "1. Introduction", "content": "Quantitative investment (quant) involves financial in-vestment strategies driven by mathematical, statistical, ormachine learning models, and it uses powerful computersexecute trading instructions derived from quant models atspeeds and frequencies unattainable by human traders. Inparticular, deep learning techniques are widely applied inquant modeling, such as stock/futures trend prediction [1, 2,3, 4], stock selection [5, 6, 7], portfolio optimization [8, 9,10, 11, 12] and algorithmic trading [13, 14, 15, 16, 17].\nThe traditional quantitative research paradigm is fraughtwith several limitations. First, it adheres to a comprehen-sive pipeline that includes data processing, factor mining,machine learning, portfolio optimization, and algorithmictrading. Each of these steps demands significant researchresources, including intensive labor and substantial time toidentify effective \"alphas.\" Furthermore, the optimizationobjectives across these pipeline stages often lack consis-tency, leading to suboptimal outcomes for the final tradingstrategy. Additionally, traditional task-specific quantitativemodeling relies heavily on pre-defined scenarios, strategytasks, and associated data, making it difficult to transferthese models directly to other strategy tasks. This relianceon \"local\" data not only limits the model's potential butalso exacerbates research costs, as quants are compelled todevelop a distinct model for each strategy.\nIn recent years, the rapid advancements in artificial gen-eral intelligence (AGI) have provided a unique opportunityto transform the quantitative research paradigm. Specifically,we discuss the shift toward a new modeling paradigm aimedat enhancing the efficiency and effectiveness of quantitativefinance research. First, there is a clear transition from tra-ditional multifactor modeling to state-of-the-art end-to-endmodeling. Unlike multifactor modeling, which builds trad-ing strategies incrementally through a research pipeline, end-to-end modeling seeks to directly generate the final tradingstrategy, bypassing intermediate steps such as factor mining,and producing predicted alphas, optimal positions, or evenalgorithmic trading orders. This approach has the potentialto eliminate the labor-intensive factor mining process andsignificantly enhance the efficiency of quantitative research.Second, the shift from traditional task-specific modelingto universal modeling, akin to the \"pretrained foundationmodel + fine-tuned task model\" approach commonly used inlarge language models, is becoming increasingly prominentin quantitative investment. The foundation model, typicallya universal model trained on a broad and diverse dataset(e.g., data spanning various countries, security markets,and trading assets), can be fine-tuned to optimize specifictrading strategies. By combining the strengths of end-to-endmodeling and universal modeling, we propose the Large In-vestment Model (LIM), a novel methodological frameworkfor quantitative investment research. \nThe remainder of this article is organized as follows.Section 2 provides a brief review of the data, strategies,and research pipeline in quantitative investment. Section 3introduces the LIM framework and the underlying con-cepts. Details of the upstream foundation modeling anddownstream strategy modeling within LIM are presentedin Sections 4 and 5, respectively. Section 6 discusses thearchitecture design for automated strategy generation andtrading using LIM. Section 7 proposes several new researchdirections, and Section 8 offers concluding remarks. Finally,the appendix in Section 9 presents the numerical experi-ments conducted to demonstrate the performance of LIM."}, {"title": "2. Review on Quantitative Investment", "content": "Quantitative Investment relies on automated strategiesbuilt on various data to trade different instruments suchas stocks, futures, bonds and options. This section brieflyintroduces common quantitative investment strategies anddiverse data used for building these strategies. In addition,we introduce the classic multi-factor modeling which is wideused in quantitative investment."}, {"title": "2.1. Quant Strategies", "content": "A quantitative strategy is a systematic function or tradingmethodology used for trading financial instruments, suchas stocks, options, and futures, in financial markets. Thesestrategies are based on either predefined rules or trainedmodels for making trading decisions and are typically thecore intellectual property of a trading firm. A standardquantitative strategy should specify several configurations,such as the universe of financial instruments to be traded, theaverage holding period, and the trading frequency. Addition-ally, it should define the type of strategies employed.\n\u2022 Directional trading is a strategy used in financial marketsthat involves taking a position based on the anticipateddirection of a security's price movement and profitingfrom these price changes by buying or selling securitiesaccordingly. Popular directional trading strategies includetrend following (identifying and following the directionof an existing trend, taking long positions in uptrends andshort positions in downtrends), breakout trading (takingpositions when the price decisively moves beyond sup-port or resistance levels, with long positions on bullishbreakouts and short positions on bearish breakouts), andcontrarian trading (trading against the market trend byidentifying overbought or oversold conditions and takingreverse positions).\n\u2022 Long-short trading, commonly used in hedge funds, is aninvestment strategy that involves taking both long andshort positions in different securities to exclude marketvolatility effects (the \"Beta\" return) from the overall returnand to profit from the \"Alpha\" return. A popular exampleis stock long-short selection, which predicts the \"best\"stocks to buy (long) and the \"worst\u201d stocks to sell (short)at each cross-section over time.\n\u2022 Arbitrage trading is a strategy that exploits price discrep-ancies between different markets or financial instrumentsTo achieve risk-free profits. Common arbitrage strategiesinclude cross-exchange arbitrage (profiting from price dis-crepancies of a security across various exchanges), trian-gle arbitrage (typically used in forex and cryptocurrencymarkets to exploit exchange rate discrepancies by tradingthree different currencies), calendar spread arbitrage (forfutures), convertible arbitrage (taking a long position in aconvertible bond while shorting its underlying stock), andstatistical arbitrage (trading pairs of correlated securitiesby longing the underpriced one and shorting the over-priced one).\n\u2022 Market making trading is a family of high-frequencystrategies that provide liquidity to financial markets bycontinuously quoting both buy (bid) and sell (ask) pricesfor financial instruments, aiming to profit from the spreadbetween these prices. For instance, a market maker mightquote a bid price of $100 and an ask price of $100.10 for astock. The market maker buys shares from a trader willingto sell at $100 and sells them to another trader willing tobuy at $100.10, thereby profiting from the spread.\nAdditionally, trading frequency defines the duration forwhich assets are held in a portfolio and how often trades areexecuted. High-frequency trading typically involves hold-ing positions for a few minutes or seconds, whereas low-frequency trading may involve holding assets for severalmonths or years. The significant difference in holding pe-riods between high-frequency and low-frequency tradingleads to distinct considerations in strategy design. For exam-ple, asset capacity limitations and trading costs are criticalissues in high-frequency trading, while managing drawdownrisk is a primary concern in low-frequency trading."}, {"title": "2.2. Data Diversity in Quant Modeling", "content": "Modern quantitative investment harnesses a diverse ar-ray of data to develop statistical and machine learning strate-gies aimed at profitable trading. categorizes various types of financial data along two orthogonal dimensions:data depth and data breadth. Data depth refers to the gran-ularity of data, which can span from several years at themacro level to mere nanoseconds at the micro level. Databreadth, on the other hand, indicates the diversity of thedata, encompassing quote data (such as price/volume, limitorder book (LOB) data, and market order flow), fundamentaldata (including financial statements, investment researchreports, company announcements, and analysts' opinions),and a broad range of alternative data (such as e-commercetransactions, credit card/e-payment transactions, news andsocial media comments, satellite imagery, foot traffic, andsupply chain data).\nDifferent investment strategies rely on different types offinancial data. For instance, high-frequency market-makingstrategies [18] focus on data depth by modeling granu-lar LOB data to predict price movements over very shorthorizons. Horizontal spread arbitrage strategies [19] uti-lize minute-bar quote data, seeking to capitalize on pricediscrepancies in derivatives contracts (such as options orfutures) with varying expiration dates. Stock technical trad-ing strategies [20] employ candlestick data (open, high,low, close prices) and trading volumes to generate buy/sellsignals. Meanwhile, stock fundamental investing strategies[21] analyze financial statements, analyst reports, and newsdata to evaluate the fundamental health and intrinsic valueof public companies.\nThe rapid growth of internet and mobile technologiesover the past decade has led to an explosion in the accu-mulation of big data. Financial institutions are increasinglyintegrating alternative data, such as credit card transactiondata, web traffic data, and geolocation data, into their funda-mental analysis and value investing strategies [22, 23]. Thisshift has significantly expanded the data breadth available forquantitative investment, enabling more comprehensive andnuanced analyses."}, {"title": "2.3. Multifactor Quant Modeling", "content": "The quantitative research pipeline comprises severalcritical stages, including data processing, factor mining,alpha modeling, portfolio position optimization, and orderexecution optimization. Among these stages, factor mining is particularlyvital, as the quality of the factors significantly influencesthe performance of the alpha model, which subsequentlyimpacts the overall returns of the final portfolio.\nFactors are typically mathematical formulas or functionsthat capture signals predictive of trends in various financialinstruments, such as stocks, futures, and foreign exchange.These factors can be derived from a wide range of datasources, including: (1) financial quote data, such as price,volume, and limit order book (LOB) information; (2) fun-damental data, such as financial statements and researchreports; and (3) alternative data sources [24], includingcredit card transactions, social media commentary, productreviews, satellite imagery, geolocation and climate data,shipping trackers, mobile app usage data, and news feeds."}, {"title": "3. Large Investment Models", "content": "The rapid advancement of artificial general intelligence(AGI) [26] in recent years has prompted a re-evaluation ofthe future trajectory of quantitative investment technology.Two pivotal aspects of AGI are end-to-end modeling anduniversal modeling. Notably, the success of GPT-like mod-els [27, 28] has highlighted the extraordinary potential ofself-supervised generative learning [29], which is groundedin a universal \u201cnext-token prediction\u201d task. In this paradigm,a general-purpose foundation model is pre-trained on exten-sive datasets, and through appropriate fine-tuning [30] forspecific tasks, the model's predictive power is significantlyenhanced, often surpassing models that were specializedfrom the outset for those tasks.\nThis success invites an intriguing question: could the\"pre-training + fine-tuning\" paradigm be effectively ap-plied to quantitative strategy research? If a robust founda-tional model [31] for quantitative investment can discovertransferable trading patterns and investment logic acrossvarious instruments and markets, then quantitative strategyresearch might be reconceived as a fine-tuning task tailoredto specific strategy requirements and investment scenarios.Such a paradigm shift could dramatically increase researchefficiency in the field."}, {"title": "3.1. End-to-End Modeling for LIM", "content": "Since factors are essentially \"features\" that characterizeinstruments, their information is entirely derived from theoriginal data. A natural question arises: can we build apredictive model without explicitly creating factors? The ad-vent of deep learning and the end-to-end training paradigmpresents a plausible technical route. End-to-end trainingrefers to a modeling approach that directly learns the com-plex function linking raw inputs to final outputs, encompass-ing all intermediate stages. Figure 4 illustrates three types ofend-to-end modeling, each starting from the original meta-data (raw data with standardized and simple preprocessing)and leading to different outputs: alpha predictions (e.g.,predicted returns over a future horizon), portfolio positions(e.g., the optimal position size at the next trading point), ortrade orders (e.g., the optimal order size in the next secondfor trading).\nConsider the example of data-to-position modeling,where the tasks of alpha generation and optimal positiondetermination are integrated within a single deep learningmodel. There are several methods available for directly out-putting position sizes. One such method involves calculatingMarkowitz-optimal position sizes [32] based on ground-truth returns and using these computed values as labels fortraining the deep learning model. Another approach framesthe reinforcement learning reward function in terms of theportfolio's Sharpe ratio, which is derived from the predictedposition sizes. This subsection explores a third technicalapproach: designing a novel loss function for deep learning,based on the predicted portfolio Sharpe ratio, to directlydetermine the optimal position sizes. This method enablesthe deep learning model to learn the optimal portfolioallocation by optimizing a loss function that directly reflectsthe trade-off between return and risk, as quantified by theSharpe ratio. Mathematically, suppose we have a universe of mstocks, each with a record of n time points. Let $r_{t,s}$ and $w_{t,s}$$(1 \\leq t \\leq T \\text { and } 1\\leq s \\leq S)$ denote the return and positionsize of stock $s$ at time point $t$, respectively. The portfolioreturn at time point $t$ is $r_t=\\Sigma_{s=1}^S w_{t,s}r_{t,s}$. Let $r = \\{r_t\\}_{t=1}^n$represent a sequence of $n$ portfolio returns over time, thenthe negative logarithmic Sharpe ratio of the portfolio can bedefined as:\n$L(X | w, \\Theta)=- \\log (E(r))+\\log (\\sigma(r))$  (1)\nThis serves as the loss for the end-to-end deep learningquant model. Note that $r_t$ depends on $r_{t,s}$, which is a func-tion of the input data sequence $X = \\{x_t\\}_{t=1}^T$, position$w = \\{w_1,w_2, ..., w_m\\}$ ($m$ is the number of stocks in theuniverse), and deep neural network parameters $\\Theta$. Since$L(X|w,\\Theta)$ is a differentiable function of $w$ and $\\Theta$, we canbuild an end-to-end neural network with this loss functionto output the optimal portfolio positions corresponding toan optimal Sharpe ratio on the training data.\nRecent literature has seen a growing interest in thisarea, with notable contributions such as Deep InceptionNetworks (DINs) [12] and End-to-End Active Investment(E2EAI)[33]. DINs introduce end-to-end systematic tradingstrategies by extracting time-series and cross-sectional fea-tures directly from daily price returns, outputting positionsizes by optimizing the Sharpe ratio of the entire portfo-lio during training. Similarly, E2EAI presents an end-to-end neural network model that spans the entire quantitativeresearch process, from factor selection and combination tostock selection and portfolio construction. Both approachesbypass traditional factor mining and alpha prediction steps,directly outputting optimal positions and enabling tradersto compute the difference between new and old positionsto determine subsequent orders. Another significant con-tribution is DeepLOB [34], which constructs a large-scaledeep learning model to predict price movements directlyfrom limit order book (LOB) data of cash equities. Notably,the authors emphasize that DeepLOB generalizes well toinstruments not included in the training set, demonstrat-ing the model's ability to extract universal features. Theteam further extends deep learning models to more granularmicro-structure data in [35], concluding that an ensemble ofMBO (market by order) and LOB data enhances forecastingaccuracy. Unlike the straightforward end-to-end modelingin [34] and [35], the work in [36] proposes an upstreampretraining framework to extract alphas from order flow data,applicable across various granularities and scenarios. Thisapproach also inspires the large investment model proposedin this paper.\nCompared to traditional quant research pipeline (bluepart of Figure 4), end-to-end modeling offers several ad-vantages: 1) in traditional quantitative research, the opti-mization goals of individual modules are usually incon-sistent. For instance, each factor is evaluated and selectedbased on criteria that primarily concern the factor itself,rather than its interaction with other factors. As a result, a\"good\" factor with a high information coefficient (IC) [37]or Sharpe ratio [38] may negatively impact an alpha modeldue to complex interactions with other factors, while a \"bad\"factor might significantly contribute to the model. 2) Theformulaic nature and operator space of factors can limittheir representational capability. Almost all operators (e.g.,rank(\u00b7), ts_max(\u00b7), etc.) that define formulaic factors aresimple algebraic functions, and the representational powerof their combinations is difficult to compare with deep neuralnetworks. Therefore, with sufficient sample size, end-to-endmodeling has a higher ceiling than traditional multi-factormodeling. 3) Factor mining is a labor-intensive and time-consuming process, especially for building and selectingfactors by hand. On the other hand, end-to-end modelingthrows these \"dirty work\" to deep learning algorithms andmay reduce the cost significantly.\nCompared to the traditional quantitative research pipeline(illustrated in the blue section of Figure 4), end-to-endmodeling presents several significant advantages. First, inthe traditional approach, the optimization goals of individualmodules often lack consistency. For instance, factors aretypically evaluated and selected based on criteria that focusprimarily on the factor itself, rather than on its interactionwith other factors. This can lead to situations where a\"good\" factor, characterized by a high information coef-ficient (IC) [37] or Sharpe ratio [38], may inadvertentlyhave a detrimental effect on an alpha model due to complexinteractions with other factors. Conversely, a \"bad\" factorcould unexpectedly enhance model performance. Second,the formulaic nature and operator space of traditional factorslimit their representational capacity. Most operators (e.g.,rank(\u00b7), ts_max(\u00b7)) that define formulaic factors are simplealgebraic functions. The representational power of thesecombinations is difficult to compare with that of deep neuralnetworks. Thus, with a sufficiently large sample size, end-to-end modeling has a higher potential for performance thantraditional multi-factor modeling. Third, factor mining isa labor-intensive and time-consuming process, particularlywhen factors are manually constructed and selected. Incontrast, end-to-end modeling delegates these \"dirty tasks\"to deep learning algorithms, potentially reducing the overallcost and effort significantly."}, {"title": "3.2. Universal Modeling for LIM", "content": "The universality of LIM should encompass at least thefollowing three aspects:\n1. Cross-instrument universality. Given quote data, can amachine-mined pattern for predicting stock trends beapplied to predict trends in futures or bonds? Logically,many trading patterns reflect traders' intentions and be-haviors, and it is natural for some common patterns tobe shared across various instruments. Empirically, manytechnical or price/volume factors are useful not only forstock prediction but also for bonds, futures, and evencryptocurrencies. This suggests the feasibility of traininga general-purpose upstream model with data from variousinstruments and fine-tuning this model with specific datafor each instrument.\n2. Cross-exchange universality. Stock trading across differ-ent exchanges may exhibit common patterns or signals,especially for technical indicators or strategies based onquote data (and sometimes news data). This observationmotivates the development of a \"universal\u201d model usingdata from multiple exchanges, which can then be appliedto trade equities in specific markets. Similarly, many otherinstruments (e.g., futures, bonds) share cross-exchangepatterns, making them suitable for pretraining the foun-dational quant model.\n3. Cross-frequency universality. Patterns often persist acrossdifferent data frequencies, such as 1-second, 15-second,1-minute, 20-minute, 1-hour, and daily candlesticks.Training on data from the same instrument across variousfrequencies can significantly enhance the sample size,which is crucial for improving the performance of deeplearning models.\nAs shown in Figure 5, the architecture follows a typicalpretraining-finetuning structure. The upstream foundationmodel is a self-supervised generative model pretrained ondiverse data, acquiring financial analysis, prediction capa-bilities, and data generation skills through exposure to awide array of data, corpora, and knowledge. The down-stream process involves building specific quantitative in-vestment strategies by fine-tuning the upstream model withtask-specific data and outputting a prediction model for thequantitative trading strategy. Analogous to GPT for naturallanguage processing, LIM aims to serve as an artificialgeneral intelligence system for quantitative investment. First,the upstream model acts as a generative foundation forquantitative finance, simplifying strategy formulation into aunified framework akin to a \u201cnext-token prediction\" problem[39]. Utilizing self-supervised learning, the model efficientlylearns representations from various financial data across dif-ferent exchanges and instruments, capturing nuanced marketpatterns and relationships. This predictive approach stream-lines strategy development by transforming traditional task-specific modeling into a more generalized sequence pre-diction problem. Consequently, the model can infer futuremarket conditions based on historical data, similar to howlanguage models predict the next word in a sentence, therebyunlocking new potential for quant strategy research, algo-rithmic trading, risk assessment, and portfolio managementin a more automated and scalable manner. To maximizethe universality of the upstream model, we define it as asingle-instrument processor. Therefore, strategies dependenton multiple trading instruments (such as pairwise arbitrage[40] or cross-sectional stock alpha strategies [41]) will bemodeled during the downstream fine-tuning phase. Sec-ond, the downstream model fine-tunes the upstream modelaccording to specific task requirements to develop quan-titative trading strategies, including momentum strategies,mean-reversion strategies, pairs trading strategies, triangulararbitrage strategies, calendar spread arbitrage, and cross-sectional hedging strategies. Given the diverse specificationsand configurations of different tasks, downstream modelingemploys a range of approaches. In addition to investmentstrategies, downstream tasks can also include risk models orstochastic simulators (see Section 7 for details) to simulatevarious market scenarios."}, {"title": "4. Upstream Foundation Model", "content": "As illustrated in Figure 6, the upstream modeling focuseson developing a universal foundation model for quantitativeinvestment. The goal of the upstream pretraining foundationmodel is to be as general as possible, enabling it to address abroad spectrum of financial time-series prediction problems."}, {"title": "4.1. Problem Formulation", "content": "We formulate the foundation model in the upstream pre-training stage as follows. Suppose we have a dataset ofM multivariate time series $D = \\{X^{(m)}\\}_{m=1}^M$, where eachmultivariate time series $X^{(m)} \\in [R^{T(m) \\times p}$ has a length $T(m)$and dimension $p$. For each time point $t (1 \\leq t \\leq T)$,we define two sliding windows: a look-back context win-dow of length $L_x$ that defines the input $X_{t-L_x+1:t}$, and a look-forward horizon window of length $L_y$ that definesthe output $X_{t+1:t+L_y}$. The foundation model is a function$f: R^{L_x \\times p} \\rightarrow R^{L_y \\times p}$. Given parameters $\\Theta$, we aim tosatisfy the relationship $X_{t+1:t+L_y} \\approx f(X_{t-L_x+1:t} | \\Theta)$.\nTo estimate the mapping function $f$, various deep learn-ing models can be utilized by minimizing the loss function:\n$\\frac{1}{M}\\sum_{m=1}^M \\frac{1}{T(m)}\\sum_{t=1}^{T(m)} L(X_{t+1:t+L_y}, f(X_{t-L_x+1:t} | \\Theta))$(2)"}, {"title": "4.2. Modeling Principle", "content": "To build a foundation model for quantitative investment,several conditions must be carefully considered:\n1. Single-instrument time series: To accommodate a widerange of trading strategies and investment scenarios,the upstream model should focus on single financialinstrument time series (note that the time series maybe multivariate for a single financial instrument). Modelinginvolving multiple financial instruments (e.g., cross-sectional stock selection for the S&P 500) should bereserved for downstream tasks.\n2. Trading cost and rules: Significant differences in tradingrules and transaction costs across exchanges exist. Forexample, some stock exchanges operate on a T+1 tradingbasis, where stocks bought today cannot be sold beforethe next trading day, while others use a T+0 system,allowing for same-day trading. These differences affectthe timing of cash flows, market patterns, and overallstrategy design and execution. Since the LIM foundationmodel is expected to learn common market patterns,these differences across exchanges are ignored, treatingthe problem as a pure time-series prediction task.\n3. Patching and masking: A time-series patch serves as ananalogue to a token in language models, and patching hasbeen shown to improve performance. This approach alsoenhances inference speed by reducing the number of to-kens fed into the transformer by a factor equivalent to thepatch length. Additionally, employing a random maskingstrategy can induce adaptive window lengths, allowingthe model to experience all possible context lengths,ranging from 1 to the maximum context length, ensuringrobust performance across various context lengths.\n4. Data choice: Financial data for investment can be broadlycategorized as quote data, fundamental data, and alterna-tive data. Among these, quote data at various frequenciesare used to pre-train the foundation model because theyare regularly sampled and can be collected across differ-ent exchanges. Fundamental and alternative data are usedin downstream tasks to build strategy models."}, {"title": "4.3. Design of Foundation Model", "content": "illustrates the construction of an upstream foun-dation model for LIM. This model is designed to predictfuture tokens within a Y-window (horizon) based on datafrom an X-window (context) that covers a fixed historicalperiod. The X-window data are fed into the modeling mod-ule, serving as the input for the backbone deep learningmodel. This model is trained on financial quote time-seriesdata, incorporating various variables (meta-features) suchas closing price, bid-ask imbalance, returns, and tradingvolume, with the aim of predicting the same set of variableswithin the Y-window. To enhance computational efficiency,time-series segmentation techniques such as patching [43]are applied within the windows. Additionally, patch maskingstrategies [44] are employed to improve the quality of self-supervised learning and increase the flexibility of windowlength during model training.\nAn example architecture of the deep neural network isillustrated in Figure 6. For each variable in the time series,the input data from the X-window are first transformedinto input patch vectors via an input projection, which thengenerates patch embedding vectors. These embeddings areconcatenated with a time encoding vector to capture tem-poral information and a feature encoding vector to identifythe specific variable being used. After processing through adeep neural network (e.g., a Transformer), the model outputspatches corresponding to each variable. These outputs arethen merged and converted back to the original time-seriesgranularity through an output projection, ultimately generat-ing predictions for the variables in the Y-window."}, {"title": "5. Downstream Task Model", "content": "The downstream workflow bridges the foundation modelfrom the upstream process with the final strategy develop-ment task. Unlike the foundation model, which primarilyrelies on quote data, downstream modeling can incorporatea wide variety of task-specific data sources, including news,supply chain information, satellite imagery, earnings calltranscripts and so on. These diverse data types can be cat-egorized into graph data, textual data, image data, numer-ical data, audio data, and video data. To effectively utilizethese additional inputs, we employ specialized embeddingtechniques tailored to each data type, enabling the modelto integrate and leverage the unique information containedwithin these varied structures."}, {"title": "5.1. Data Alignment and Standardization", "content": "Handling diverse data types is crucial for developing ro-bust quant models. Fundamental data, such as financial state-ments, and alternative data, such as social media sentimentand satellite imagery, often exhibit irregularities in their timeseries. This irregularity poses significant challenges for datapreprocessing, necessitating a methodical approach to aligndata accurately with corresponding time points and financialinstruments.\nProper alignment is essential to ensure that the input dataused for model training and evaluation is coherent, consis-tent, and reflective of true market conditions. First, a fun-damental step in data preprocessing is temporal alignment.Since fundamental and alternative data points do not alwayscoincide with regular intervals, it's necessary to synchronizethese data points to a common timeline that matches the trad-ing dates or specific events relevant to the investment strat-egy. Techniques such as interpolation or nearest-neighbormethods can be employed to estimate missing values andalign the data with the appropriate time stamps. This align-ment ensures that the models receive continuous and accu-rate inputs, thereby enhancing their predictive accuracy andreliability. Second, instrument alignment is equally criticalin the preprocessing phase. Given that different financial in-struments (e.g., stocks, bonds, derivatives) may have uniquecharacteristics and response patterns to various data inputs,aligning data to the correct instrument is imperative. Thisinvolves mapping the fundamental and alternative data to thespecific instruments they relate to, ensuring that each datapoint is correctly attributed. For instance, corporate earningsreports must be matched to the corresponding company'sstock, while industry-wide metrics should be appropriatelylinked to all relevant securities within that industry. Thisprecise mapping enhances the granularity and relevance ofthe data, allowing for more targeted and effective modeling.\nFurthermore, aligning data across time points and in-struments also involves normalization and standardizationprocesses. These processes adjust data to a common scale,which is crucial when combining disparate data sources.Normalization mitigates the impact of outliers and scalesdifferences, while standardization ensures that all data inputshave a consistent distribution, facilitating more efficienttraining of machine learning models. By incorporating thesepreprocessing steps, the data fed into AI models becomesmore robust, reducing noise and improving the overall qual-ity of the predictions."}, {"title": "5.2. Model Fine-tuning", "content": "Fine-tuning the LIM foundation model, which is initiallypre-trained with quote time series, is a critical step in en-hancing its applicability to broader quantitative investmentstrategies. The foundation model, built primarily on histori-cal price data, needs to be adapted to incorporate additionallayers of information to better understand market dynam-ics. By integrating more fundamental and alternative dataalongside the quote data, the fine-tuning process enriches themodel with a comprehensive dataset, making it more adeptat predicting market movements and refining investmentstrategies. The use of advanced fine-tuning methods andoptimization techniques ensures that the model not onlyretains its foundational strengths but also evolves to meetcurrent market demands. These enhancements enable thedevelopment of more accurate, reliable, and sophisticatedinvestment strategies, ultimately contributing to improvedperformance in quantitative investment.\nThe fine-tuning process involves adding various newdata types, including numerical data such as financial ratiosand technical indicators, graph data representing relation-ships between entities (e.g., corporate networks and supplychains), and unstructured data like images, videos, and au-dio. For instance, satellite imagery can offer insights intoindustrial activities, while social media sentiment analysiscan provide contextual understanding of market sentiments.This diverse data integration allows the model to capture awider array of market signals and develop more sophisti-cated strategies.\nAdapting the foundation model to meet current strategydevelopment demands involves employing several commonfine-tuning methods appropriate for quantitative investment.Transfer learning [45] is a pivotal technique where the pre-trained model is fine-tuned on a new, specific dataset, allow-ing it to retain its learned knowledge while becoming morespecialized. Feature-based Transfer Learning [46] adjustsonly the last few layers, leveraging the previously learnedfeatures, while Fine-tuning Entire Models [47] retrains theentire network to adapt to the new domain. Layer-wise Fine-Tuning [48] is a technique where different layers of themodel are fine-tuned at different rates, typically starting fromthe last layers and progressively fine-tuning earlier layers.Knowledge Distillation [49] is another approach, where as smaller model (student) is fine-tuned using the outputs of alarger pre-trained model (teacher) as soft targets. Parameter-Efficient Tuning methods, such as Low-Rank Adaptation(LoRA) [50] and Adapter modules [51], introduce a smallnumber of trainable parameters to existing pre-trained mod-els, allowing for efficient fine-tuning with fewer resources.These algorithms are chosen based on the nature of the task,the available data, and computational resources, balancingthe need for adaptation with the risk of overfitting."}, {"title": "5.3. Various Types of Downstream Tasks", "content": "Quantitative investment encompasses a broad range ofstrategy types. Given that the upstream foundation modelprimarily serves as a predictor for single time-series data, thedownstream process becomes crucial for strategies that in-volve multiple instrument time-series. These include strate-gies such as cross-sectional stock selection, pairs trading,and various complex arbitrage approaches. Below, we out-line several typical strategy scenarios and describe the cor-responding downstream processing tasks:\n\u2022 Fundamental Investing. Fundamental investing, which of-ten involves low-frequency trading, relies heavily on fun-damental data sources such as analysts' reports, finan-cial statements, news articles, and other alternative datarelated to company performance and operations. In thedownstream process, these data types are combined withthe embeddings generated by the pretrained model tofine-tune a new prediction model focused on fundamentalanalysis. Due to the typically small sample size in low-frequency trading, the downstream model is often limitedto predicting the next alpha signal. Subsequently, portfoliopositions are optimized using a Markowitz optimizer, andorder execution strategies are derived from algorithmictrading techniques.\n\u2022 Statistical Arbitrage. Statistical arbitrage [52] strategiesinvolve trading two or more historically correlated as-sets based on deviations from their mean or expectedrelationship. For instance, in pairs trading, when oneasset outperforms its counterpart, the strategy may involveselling or shorting the overvalued asset while buying orgoing long on the undervalued asset, with the expectationthat the spread will revert to its historical average. In thiscontext, the upstream foundation model embeds the assetsused in pairs trading into latent vectors. These vectorsare then processed by the downstream model to predictoptimal trading times.\n\u2022 Lead-Lag Strategy. The lead-lag strategy [53] involves trading two assets where one asset (the \"leading\" asset)is anticipated to influence the performance of the other(the \"lagging\" asset). Unlike pairs trading, where assetsare traded in opposite directions, the lead-lag strategyinvolves trading only the lagging asset based on the trendof the leading asset. In this scenario, the downstreammodel is fine-tuned to take embeddings of both the leadand lag time series as input, and outputs predictions forthe lagging series.\n\u2022 Cross-Sectional Strategy. Cross-sectional strategies [54]differ from time-series approaches in that they involvetrading a broad universe of assets simultaneously basedon predicted alphas for the same time horizon. Common cross-sectional strategies include stock selection andlong-short hedging. During the downstream process, theentire cross-section of assets is input into the fine-tuningmodel as a single sample. Multiple cross-sectional sam-ples from different time points are used to train the down-stream model, ultimately guiding the selection of stocksfor buy/sell or long/short trades based on the predictedhorizons."}, {"title": "6. System Architecture for LIM", "content": "This section outlines the"}]}