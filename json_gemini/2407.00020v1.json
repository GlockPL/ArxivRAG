{"title": "Visual Language Model based Cross-modal Semantic Communication Systems", "authors": ["Feibo Jiang", "Chuanguo Tang", "Li Dong", "Kezhi Wang", "Kun Yang", "Cunhua Pan"], "abstract": "Semantic Communication (SC) has emerged as a novel communication paradigm in recent years, successfully transcending the Shannon physical capacity limits through innovative semantic transmission concepts. Nevertheless, extant Image Semantic Communication (ISC) systems face several challenges in dynamic environments, including low semantic density, catastrophic forgetting, and uncertain Signal-to-Noise Ratio (SNR). To address these challenges, we propose a novel Vision-Language Model-based Cross-modal Semantic Communication (VLM-CSC) system. The VLM-CSC comprises three novel components: (1) Cross-modal Knowledge Base (CKB) is used to extract high-density textual semantics from the semantically sparse image at the transmitter and reconstruct the original image based on textual semantics at the receiver. The transmission of high-density semantics contributes to alleviating bandwidth pressure. (2) Memory-assisted Encoder and Decoder (MED) employ a hybrid long/short-term memory mechanism, enabling the semantic encoder and decoder to overcome catastrophic forgetting in dynamic environments when there is a drift in the distribution of semantic features. (3) Noise Attention Module (NAM) employs attention mechanisms to adaptively adjust the semantic coding and the channel coding based on SNR, ensuring the robustness of the CSC system. The experimental simulations validate the effectiveness, adaptability, and robustness of the CSC system.", "sections": [{"title": "I. INTRODUCTION", "content": "As mobile communication technology has evolved from the first generation to the fifth generation, there has been a significant increase in transmission rates, approaching system capacities close to their limits [1]. In recent years, various emerging applications, such as the metaverse and virtual reality, have introduced substantial data streams [2]. Furthermore, these applications necessitate extensive connectivity over limited spectrum resources while demanding lower latency, posing significant challenges to conventional source-channel coding. Semantic Communication (SC) operates in the semantic domain by extracting the inherent meaning of data, eliminating redundant information, and achieving data compression while preserving its essential semantic content [3].\nWith the rapid development of deep learning, many researchers have begun to explore end-to-end Image Semantic Communication (ISC) systems based on deep neural networks. For instance, ISC systems constructed using deep learning approaches such as Convolutional Neural Networks (CNN), Vision Transformers (ViT), and others have surpassed traditional solutions. Despite the significant achievements in the research of ISC based on deep learning, there remain some challenges:\n1) Low semantic density: Images are natural signals with heavy spatial redundancy [4]. Traditional ISC systems directly encode the entire image, focusing on extracting low-level semantic information at the pixel level. However, text is a human-invented signal that possesses high semantic and information density. Summarizing image information through text can surpass the low-level pixel-level semantics and achieve a more sophisticated high-level semantic understanding of objects and scenarios. Moreover, traditional ISC systems lack the ability to leverage the interpretability of knowledge bases (KBs), resulting in a black-box model based on deep learning for the semantic encoder and decoder with limited explain- ability of semantics.\n2) Catastrophic forgetting: ISC systems often operate in dynamic environments, leading to a drift in the feature distribution of transmitted image data and channel state over time. Consequently, the real data distribution becomes inconsistent with the distribution during training, resulting in a decline in the performance of the semantic encoder and decoder. Continual learning of the semantic encoder and decoder is necessary to improve the performance of the ISC system. However, during continual learning, the existing knowledge of the encoder and decoder may be disrupted or overwritten by new knowledge, leading to catastrophic forgetting in the learning process [5]. As a result, it becomes unable to adapt to semantic transmission in dynamic environments.\n3) Uncertain Signal-to-Noise Ratio (SNR): In wireless communications, traditional deep learning-based ISC systems typically consider a few discrete SNR conditions during the training phase, which cannot cover all possible SNR scenarios. As a result, the performance may severely degrade when there is a mismatch between the channel conditions during training and inference phases [6]. Training the semantic/channel"}, {"title": "II. RELATED WORK", "content": "A. Deep learning enabled ISC systems\nDeep learning techniques are commonly employed in the construction of encoders and decoders for ISC systems. In [9], a comprehensive SC system based on CNNs was initially introduced, showcasing superior performance in Peak Signal- to-Noise Ratio (PSNR) when compared to traditional compression algorithms. In [10], a novel Nonlinear Transform Source- Channel Coding (NTSCC) for SC systems was proposed, which leveraged a Variational AutoEncoder (VAE) to map the source signal to the latent space, and executed nonlinear transformation and channel coding in the space. Addition- ally, [11] presented an innovative SC system incorporating Semantic Slice-Models (SeSM) to facilitate adaptable model resemblance under diverse requirements. Furthermore, [12] introduced a Reinforcement Learning-based Adaptive Seman- tic Coding (RL-ASC) for image data. RL-ASC utilized a combination of VAE, RL, and generative adversarial networks (GANs) to encode, allocate, and decode semantic concepts.\nAlthough convolutional and ViT-based autoencoders have shown promising results, their feature extraction capabilities are limited compared to state-of-the-art VLMs. This limitation arises from constraints posed by model parameters and the availability of training data.\nB. Vision language models\nVLMs are a class of large AI models capable of simultane- ously processing both image and text information [13]. They find extensive application across various visual language tasks, encompassing image description, visual question answering, text-to-image generation, and other multimodal tasks. In [14], a contrastive loss function was utilized to train both image en- coders and text encoders. This loss function aimed to minimize the feature space distance between matching image-text pairs, enabling the learning of semantically relevant visual language features while reducing the dependence on large amounts of annotated data. In [15], images were treated as prefixes in language models. They were decomposed into multiple blocks, concatenated with text sequences as input, and used to predict the subsequent parts of the text sequences. Furthermore, in [16], a cross-attention mechanism was employed to integrate visual and language features. This mechanism allowed the two modalities to reference and enhance each other, facilitating the learning of more comprehensive and refined visual language features. The approach demonstrated applicability to various downstream tasks.\nVLMs aim to understand the correlation between images and text, enabling accurate visual description or image gen- eration. Future research involves deep integration of self- supervised pre-training techniques and VLMs. This integration"}, {"title": "III. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "The considered CSC system consists of three components: a transmitter, a receiver, and a physical channel. The physical channel ensures the correct exchange of semantic information over the transmission medium with dynamic SNR.\nA. Transmitter\nThe input to the transmitter is an image represented by the matrix x \u2208 \\mathbb{R}^{H\u00d7W\u00d7C}, whose size is H(height) \u00d7 W(weight) \u00d7 C(channel). In the transmitter, the input image x is mapped to symbols y for transmission over the physical channel. The transmitter consists of three independent components: a CKB for cross-modal semantic extraction, a semantic encoder, and a channel encoder. The CKB is used to extract semantic information from the image and represent it as the corresponding textual information. The semantic encoder and channel encoder are responsible for semantic coding, and channel coding and modulation, ensuring that the encoded semantic information can be smoothly transmitted over the physical channel. The encoded symbol sequence y can be represented as:\n$$y = C_\\beta(S_\\alpha(K_\\theta(x), \\mu), \\mu)$$\nB. Wireless channel\nThe transmitter sends encoded symbols y, which is transmitted through the physical channel to the receiver. The channel output sequence \u0177 at the receiver can be expressed as:\n$$\\hat{y} = hy + n$$\nwhere h represents the channel gain, and n is Additive White Gaussian Noise (AWGN).\nC. Recevier\nSimilar to the transmitter, the receiver consists of three components: a channel decoder, a semantic decoder, and a cross-modal knowledge base for semantic reconstruction. The semantic decoder and channel decoder are used to decode textual information from received symbols, while the cross- modal knowledge base is employed for image reconstruction based on the corresponding textual information. The decoded image can be represented as:\n$$\\hat{x} = K_{\\theta'}^{-1}(S_{\\delta}^{-1}(C_{\\gamma}^{-1}(\\hat{y}, \\mu), \\mu))$$\nwhere $C_{\\gamma}^{-1}(\\cdot)$ is the channel decoder with the parameter set \u03b3, $S_{\\delta}^{-1}(\\cdot)$ is the semantic decoder with the parameter set \u03b4 and $K_{\\theta'}^{-1}(\\cdot)$ is the cross-modal knowledge base with the parameter set \u03b8'.\nFor the purpose of reconstructing image information from the semantic level, maintaining the consistency of textual semantics between s and \u015d is crucial. Here, s = K\u03b8(x) represents the extracted textual semantic information from the image, and \u015d = $S_{\\delta}^{-1}(C_{\\gamma}^{-1}(\\hat{y}, \\mu), \\mu)$ represents the recovered textual semantic information after decoding. We utilize Cross- Entropy (CE) as the loss function:\n$$L_{CE}(s, \\hat{s}) = - \\sum_{l=1}^{L} q(w_l)log(p(w_l)) + (1 - q(w_l))log(1 - p(w_l))$$\nwhere q(wl) denotes the real probability of the appearance of the l-th word wl in the sentence s, and p(wl) represents the predicted probability of the appearance of the l-th word wl in the sentence \u015d. CE is employed to measure the difference between two probability distributions. By minimizing the CE loss, the semantic encoder and decoder can learn the word distribution q(wl) in the source sentence s, which represents the meaning of words in terms of grammar, phrases, and contextual information. Hence, the goal pf the CSC system is to determine the parameters of the semantic/channel encoder and decoder \u03b1*, \u03b2*, \u03b4* and \u03b3* that minimize the expected distortion as follows:\n$$(\\alpha^*, \\beta^*, \\delta^*, \\gamma^*) = \\arg \\min_{\\alpha, \\beta, \\delta, \\gamma} E_{p(\\mu)}E_{p(s,\\hat{s})}[L_{CE}(s, \\hat{s})]$$"}, {"title": "IV. THE VLM-CSC SYSTEM", "content": "Compared to traditional KBs based on Deep Neural Net- works (DNNs), Knowledge Graphs (KGs), and other ap- proaches, utilizing VLMs to construct KBs has several ad- vantages: (1) VLMs are large AI models with billions of parameters and powerful cognitive abilities concerning world knowledge. They excel in tasks related to understanding, expressing, and generating both visual and natural language data from the semantic level. (2) Unlike traditional methods that rely on manual rules or structure definitions to describe knowledge, VLMs have the ability to automatically learn and extract knowledge from data. This enables them to generate appropriate semantic information, reducing the risk of infor- mation loss or ambiguity. (3) In SC systems, the process of understanding and interpreting the generated results is crucial. VLMs have the ability to generate semantic information in a manner that is understandable to humans, enabling both parties in communication to have a more accurate understanding and interpretation of each other's intentions and expressions.\nIn this section, we will provide the implementation details of the proposed VLM-CSC system.\n1) Textual semantic extraction: To enhance the semantic density and interpretability of SC, a VLM called BLIP is em- ployed at the transmitter to construct the CKB. The CKB en- compasses a series of visual and language-related knowledge components. We employ the image encoder and text decoder from this CKB to perform cross-modal semantic extraction, thereby transforming the original image with low semantic density into a corresponding textual description with high semantic density.\n2) Semantic encoder and decoder: The generated textual information from the CKB then proceeds to the semantic encoder. The semantic encoder consists of alternating trans- former encoder layers and NAMs. The transformer encoder layers analyze and transform the textual information into a compact semantic representation. NAMs allow the semantic encoder to optimize the encoding process and maintain reliable semantic transmission, even in the presence of varying channel conditions. At the receiver, the semantic decoder is composed of alternating transformer decoder layers and NAMs, with a structure opposite to that of the semantic encoder, aimed at reversing the semantic encoding process to recover the original textual information.\n3) Channel encoder and decoder: The encoded semantic features are passed through the channel encoder to undergo channel encoding and modulation, ensuring the effective trans- mission of semantic information over the physical channel. Similarly, the channel encoder also consists of alternating FeedForward (FF) layers and NAMs. At the receiver, the transmitted information through the physical channel is received and decoded using the channel decoder. To maintain infor- mation consistency, the channel decoder employs a structure opposite to that of the channel encoder.\n4) Image reconstruction: To facilitate a better understand- ing of the received textual information, we design a CKB for image reconstruction using a VLM called SD. The CKB encompasses a series of visual and language-related knowl- edge components. We employ the text encoder, the denoising U-Net and the image decoder from this CKB to perform image reconstruction. Specifically, the textual information is first transformed into a conditional vector by the text encoder. Then, the denoising U-Net transforms the noisy image to a latent image feature vector aligning with the conditional vector. Finally, the latent image feature vector is processed by the image decoder to generate the final reconstructed image.\n5) Memory-assisted continual learning: During the training phase of the VLM-CSC system, the latest samples are stored in an STM. When the STM becomes full, a kernel method is employed to select representative short-term samples to be transferred to an LTM. Then, the STM is emptied to buffer new samples in the next round. The encoder and decoder sample from both STM and LTM during the training stage, thereby avoiding catastrophic forgetting. This approach ensures that the semantic encoder and decoder can access both recent and past information, allowing for continual learning and retention of previously learned knowledge.\n6) Training process of the VLM-CSC system: Remarkably, BLIP and SD-based CKBs are pretrained VLMs that do not need to be trained specifically for the CSC system. The training process unfolds as follows:\n\u2022 Joint training of channel encoder and decoder with NAMs: The channel encoder/decoder and NAMs are ini- tially trained together by MED. This involves optimizing the parameters of these modules by minimizing the mu- tual information, which eliminates noise or fading effects"}, {"title": "V. NUMERICAL RESULTS", "content": "In this section, we evaluate the performance of the proposed VLM-CSC system by comparing it with other SC systems."}, {"title": "VI. CONCLUSION", "content": "This paper introduces a novel VLM-CSC system capable of converting images into text descriptions for transmission over wireless channels, and reconstructing the image at the receiver. The system includes three main contributions: CKB for image-to-text and text-to-image conversion, MED for continual learn- ing in dynamic environments, and NAM for joint semantic and channel encoding based on SNR. Corresponding performance metrics are designed to evaluate the VLM-CSC system from both image and text perspectives. Experimental validations are conducted under various image datasets. Results demonstrate the effectiveness and robustness of the VLM-CSC system in preserving semantic similarity between the image and text, as well as its adaptability to dynamic environments."}, {"title": "A. BLIP-based CKB for semantic extraction", "content": "The BLIP model, introduced by Salesforce AI Research, is a sophisticated VLM designed for understanding and generating content that involves both visual and textual elements [22]. The BLIP model possesses rich visual-linguistic knowledge and utilizes multiple knowledge components such as text encoders, image encoders, and image-grounded text decoders and decoders to perform various visual-linguistic tasks, such as image captioning, visual question answering, and multimodal classification. At the transmitter, we employ the BLIP model to construct the CKB and utilize the image encoder and image-grounded text decoder (abbreviated as text decoder) in the CKB to transform original image data into detailed textual descriptions containing image semantic information."}, {"title": "B. SD-based CKB for image reconstruction", "content": "The SD model is an elaborate VLM collaboratively devel- oped by Stability AI, which possesses rich visual-linguistic knowledge and is applicable to diverse tasks such as text-to- image and image-to-image generation [26]. At the receiver, we use the SD to construct the CKB and utilize the text- to-image components in the CKB to reconstruct images. The semantic reconstructor is composed of a text encoder, a feature generator, and an image decoder."}, {"title": "C. Memory-assisted encoder and decoder", "content": "In dynamic environments, both the distribution of the trans- mitted contents and channel states will change over time. This necessitates that the CSC system continuously adjusts based on new input data and channel states to adapt to the evolving data distribution. However, such adjustments may lead to parameter updates in the encoder and decoder of the CSC system, potentially causing the catastrophic forgetting issue where old parameter values are overwritten or ignored [5]. Hence, continual learning diminishes the robustness of the encoder and decoder in the CSC system.\nThe memory-based learning strategy addresses the catas- trophic forgetting problem in continual learning by diversify- ing the memorized content [27]. We design a MED method with STM and LTM for both semantic encoder and decoder.\nWe denote $M_{stm} = \\{s_{stm}^i\\}_{n_{stm}}$ and $M_{ltm} = \\{s_{ltm}^j\\}_{n_{ltm}}$ as the sets representing dynamic samples stored in STM and LTM. $s_{stm}^i$ denotes the i-th sample in STM, and $s_{ltm}^j$ represents the j-th sample in LTM. Nstm and nltm denote the current number of samples, respectively. When the STM pool becomes full, it is necessary to select representative samples from it and transfer them to the LTM. Hence, let $n_{stm}^{max}$ represent the maximum number of samples that can be stored in $M_{stm}$. The sample selection process can be illustrated in Fig. 5 and described as follows:\n1) Relevance evaluation: During the inference phase of the CSC system, new samples being processed are continuously added to the STM. When the number of samples in the STM exceeds the specified maximum, an evaluation action is executed. The primary objective of this stage is to assess the relevance of samples. We evaluate the distance between two samples stored in STM and LTM using a Radial Basis Function (RBF) kernel:\n$$RBF(s_{stm}^i, s_{ltm}^j) = exp(-\\frac{||v_{stm}^i - v_{ltm}^j||^2}{2\\tau^2})$$\nwhere $v_{stm}^i$ and $v_{ltm}^j$ are feature vectors extracted by the semantic encoder from samples $s_{stm}^i$ and $s_{ltm}^j$, respectively. \u03c4 is the scale hyperparameter for the kernel function, and we set \u03c4 = 10 to ensure that the output of RBF(\u00b7, \u00b7) is within [0, 1]. Eq. (13) can be further accelerated through matrix operations, expressed as:\n$$F_{exp}(-(B^{stm}(-B^{ltm})^T) (B^{stm}(-B^{ltm})^T)/2\\tau^2)$$\nwhere Bstm and Bltm are feature matrices corresponding to Mstm and Mltm, respectively.\n2) Sample selection: The primary objective of this stage is to select samples from STM that are significantly different from those in LTM, ensuring diversity in the memory. We calculate the average similarity score between sample $s_{stm}^i$ and each sample in LTM using RBF kernel:\n$$R(s_{stm}^i) = \\frac{1}{N_{ltm}}\\sum_{k=1}^{N_{ltm}} RBF(s_{stm}^i, s_{ltm}^k)$$"}, {"title": "D. Noise attention module", "content": "Inspired by the feature attention module in [7], we propose a NAM based on SNR values. The NAM leverages a new noise attention network to determine the importance of each feature vector during the process of encoding and decoding, assigning weights to semantic coding and channel coding. This allows for achieving integrated encoding of both semantic and channel information according to the current SNR.\nSpecifically, in unfavorable channel conditions, higher weights are allocated to the channel encoder and lower weights are allocated to the semantic encoder for the same source in- formation. This allocation strategy enhances robustness in the channel encoder to mitigate the effects of severe channel noise. Conversely, in favorable channel conditions, lower weights are assigned to the channel encoder and higher weights are assigned to the semantic encoder for the same source infor- mation. This increased allocation of weights to the semantic encoder aims to enhance semantic quality.\n1) SNR projection: Firstly, the SNR projection module extends the SNR values to the same dimension as feature vectors in the encoder and decoder. The module is a fully connected network comprising three FF layers. The first two FF layers employ the ReLU activation function, while the third FF layer utilizes the Sigmoid activation function. It transforms the input SNR value r to a vector v. The mapping process from r to v is as follows:\n$$v' = ReLU(W_{n2}ReLU(W_{n1}r + b_{n1})+b_{n2})$$\n$$v = Sigmoid(W_{n3}v' + b_{n3})$$\nwhere ReLU and Sigmoid denote the activation functions, and Wni and bni are the weights and biases of FF layers, respectively.\n2) Feature scaling: Subsequently, we combine the input features with the projected SNR to obtain a scaling factor K, which records the importance of each intermediate feature vector for semantic/channel encoder and decoder as follows:\n$$K = Sigmoid(ev)$$\nwhere the Sigmoid activation function is used to constrain the output to the interval (0, 1). The e is the output of the intermediate feature vectors G after passing through the fourth FF layer as follows:\n$$e = W_{n4}G + b_{n4}$$\nwhere Wn4 and bn4 are the weights and biases of the fourth FF layer.\nFinally, the intermediate feature vector G are multiplied by the scaling factor K to obtain the calibrated vector A as follows:\n$$A_i = K_i G_i$$"}]}