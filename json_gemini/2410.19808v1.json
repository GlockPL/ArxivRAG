{"title": "LocateBench: Evaluating the Locating Ability of Vision Language Models", "authors": ["Ting-Rui Chiang", "Joshua Robinson", "Xinyan Velocity Yu", "Dani Yogatama"], "abstract": "The ability to locate an object in an image according to natural language instructions is crucial for many real-world applications. In this work we propose LocateBench, a high-quality benchmark dedicated to evaluating this ability. We experiment with multiple prompting approaches, and measure the accuracy of several large vision language models. We find that even the accuracy of the strongest model, GPT-40, lags behind human accuracy by more than 10%.", "sections": [{"title": "1 Introduction", "content": "Locating an object in an image is an essential part of many real-world tasks. For example, in web page navigation tasks (Deng et al., 2023; Yao et al., 2022; Zhou et al., 2023), the agent needs to locate buttons or other HTML elements before deciding the next action to take, and in robotics tasks (Shridhar et al., 2020; Szot et al., 2021; Li et al., 2023a), the agent needs to locate a specific object based on the grounded input. This ability also contributes to many downstream tasks such as visual question answering and image captioning. Despite numerous studies of the performance of vision language models (VLMs) on these downstream tasks (Liu et al., 2023; Li et al., 2023b; Zhang et al., 2023; OpenAI, 2023; Yu et al., 2024; Zhu et al., 2024), there is no direct measurement of the locating ability of VLMs, an upstream ability that greatly affects downstream task performance.\nTo address this, we propose LocateBench, a benchmark that requires VLMs to select the correct bounding box out of four candidate boxes in each image based on natural language questions in English (Figure 1). The multiple choice setup of LocateBench allows for evaluation of VLMs"}, {"title": "2 LocateBench", "content": null}, {"title": "2.1 Dataset Formulation", "content": "LocateBench is a multiple choice question dataset. Each sample includes an image, a description formulated as a question, and four bounding boxes representing candidate answers to the question. VLMs are tasked with choosing the bounding box that best answers the question."}, {"title": "2.2 Dataset Construction", "content": "LocateBench is constructed based on the Ref-COCO series datasets. These datasets contain descriptions of objects in images from the COCO dataset (Lin et al., 2014). We utilize these descriptions to construct our LocateBench dataset. Through manual inspection, we find the descriptions in RefCOCO-g (Mao et al., 2016) are more specific and detailed than their counterparts in RefCOCO (Kazemzadeh et al., 2014) or RefCOCO+ (Yu et al., 2016). Therefore, we prioritize using the object descriptions in RefCOCO-g where possible.\nWe construct LocateBench with the following steps:\n1. We discard objects with no descriptions found in the RefCOCO series dataset.\n2. Based on the super-category and bounding box information provided in the COCO dataset, we filter the COCO dataset and keep only the images that contain at least four objects in the same category. To ensure that there is no ambiguity, we only consider the sets of objects whose bounding boxes do not significantly overlap with each other. In particular, we ensure that the width and height of the overlapping area are no more than 10 pixels each. We further discard objects whose size in the image is too small (i.e., objects whose bounding box width or height is less than 75 pixels). We have 1317 examples after filtering. This size is comparable with the test set size of GSM8k (Cobbe et al., 2021), which is a commonly used benchmark dataset for math reasoning capability.\n3. Next, two authors manually inspect and edit the descriptions. From our inspection, we find that in RefCOCO and RefCOCO+, the descriptions are sometimes not specific enough"}, {"title": "3 Experiments on VLMs", "content": null}, {"title": "3.1 Evaluating Methods", "content": "To isolate the effect of prompt formats and precisely estimate the locating capability of LLMs, we prompt VLMs in the following formats:\nMulti-choice by alphabet letters (ABCD) We draw the four candidate bounding boxes in red. Each box is assigned a letter (either A, B, C, or D), and this letter is placed in the top left corner of the box. We prompt the model with the template:\nHere {question} is a placeholder for the which-question in our dataset (e.g, \u201cWhich contains the"}, {"title": "3.2 Results and Discussion", "content": "The results are in Table 1. In general, GPT-40 performs the best under all settings for LocateBench. Gemini-1.5-pro is the second-best-performing model on LocateBench despite being the best-performing model on Pointing QA. Claude-3 Opus and Llava-1.6 lag behing on both tasks.\nOverall, multi-choice by alphabet letters led to the highest accuracies. Gemini-1.5-pro is most sensitive to prompt methods, showing a difference in performance between the best and worst settings of 43.9%. Claude-3 Opus is the least sensitive model, with a difference of 16.8%. We plot Venn diagrams for model mistakes in Figures 3 and 4. The accuracy of GPT-40 still greatly lags behind the human accuracy of 95%. Current proprietary LLMs still have room for improvement when it comes to object locating. We include some hard examples where all models fail in Figure 5."}, {"title": "4 Comparison with Pointing QA", "content": "Although Pointing QA (Zhu et al., 2016) has the same objective as our LocateBench dataset, we"}, {"title": "Limitations", "content": "In this work, we focus on multi-choice problems with only four candidates. This may not fully reflect the complexity of some real-world tasks. We leave more challenging setups for future work.\nAdditionally, due to budget constraints, we make a few design choices when constructing the benchmark. For example, we only use a single LLM (Reka Core) to convert descriptions to English questions. Besides, our dataset is based on a compilation of existing datasets. This follows the common practice of repurposing existing resources for LLM evaluation. For example, HotpotQA (Yang et al., 2018) and StrategyQA (Geva et al., 2021) are based on Wikipedia articles. Just like how VLMs may have been exposed to COCO data, many LLMs have been exposed to Wikipedia in their training data. Critically, these datasets' challenge comes in its addition of questions on top of Wikipedia. Analogously, we contribute questions and bounding-box-to-label mappings that are not in VLM training data. It is evident that our aforementioned contributions make for a real challenge to VLMs (even if they've been exposed to COCO data) as there is still a sizable gap between best VLM and human performance."}, {"title": "5 Related Work", "content": null}, {"title": "5.1 Benchmarking Vision Language Models", "content": "In addition to the Pointing QA dataset from Visual7W (Zhu et al., 2016), recent benchmarks that involve explicit visual reference include the VCR (Zellers et al., 2019) and Pointer QA (Mani et al., 2020) datasets, which require models to reason about a specified point in an image. Other benchmarks evaluate VLM capabilities more generally (Hendrycks et al., 2021; Zhang et al., 2023; Fu et al., 2023; Yu et al., 2024; Fu et al., 2024)."}, {"title": "5.2 Grouding Vision Language Models", "content": "There have many works aimed at equipping VLMs with the ability to reference and ground objects in images (Lai et al., 2023; Yang et al., 2023; Zhao et al., 2023; Wang et al., 2023a,b; Pi et al., 2023; Chen et al., 2023; Xu et al., 2023; Peng et al., 2024; You et al., 2024; Zhang et al., 2024a; Rasheed et al., 2024; Zhang et al., 2024b). They extend VLMs to enable them to take in regions of an image specified by segmentation masks as a part of their input, and to generate segmentation masks as part of their output. Most of these models are based on existing pre-trained models, such as CLIP-ViT-L (Radford et al., 2021), ViT-H SAM (Kirillov et al., 2023), Vicuna (Chiang et al., 2023), LLaVA (Liu et al., 2023), and Alpaca (Taori et al., 2023). They extend the backbone models and conduct further instruction tuning. We discuss the source of the instruction-tuning data in \u00a7A."}, {"title": "6 Conclusion", "content": "In this work, we propose a new benchmark, LocateBench, which evaluates VLMs' ability to locate objects specified by natural language descriptions. We experiment with a set of advanced proprietary models and with a diverse set of prompting methods, and we show that even the best model still significantly lags behind human performance. Our work provides an easy-to-use, high-quality playground for future VLM developers looking to test their models' locating ability and improve the interpretability of the model performance, as the performance on LocateBench dissects the behavior on downstream tasks."}, {"title": "A Related Work: Datasets for Instruction Tuning", "content": "Most works derive instruction-tuning data from existing datasets. For example, Lai et al. (2023) utilizes image segmentation datasets such as ADE20K (Zhou et al., 2017), COCO-stuff (Caesar et al., 2018), LVIS (Gupta et al., 2019) and referring expression datasets such as RefCOCO (Kazemzadeh et al., 2014), RefCOCO+ (Yu et al., 2016), RefCOCO-g (Mao et al., 2016) and convert them into question-answer pairs with templates. LISA++ (Yang et al., 2023) further utilizes GPT-4v to generate question-answer pairs where the answer refers to multiple objects in the images. You et al. (2024) propose the GRIT dataset, which combines the RefCOCO series datasets, Visual Genome (Krishna et al., 2017), Object365 (Shao"}]}