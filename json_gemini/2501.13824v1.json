{"title": "Hallucinations Can Improve Large Language Models in Drug Discovery", "authors": ["Shuzhou Yuan", "Michael F\u00e4rber"], "abstract": "Concerns about hallucinations in Large Language Models (LLMs) have been raised by researchers, yet their potential in areas where creativity is vital, such as drug discovery, merits exploration. In this paper, we come up with the hypothesis that hallucinations can improve LLMs in drug discovery. To verify this hypothesis, we use LLMs to describe the SMILES string of molecules in natural language and then incorporate these descriptions as part of the prompt to address specific tasks in drug discovery. Evaluated on seven LLMs and five classification tasks, our findings confirm the hypothesis: LLMs can achieve better performance with text containing hallucinations as part of the input compared to prompts without hallucination or with reference description text. Notably, Llama-3.1-8B achieves an 18.35% gain in ROC-AUC compared to the baseline without hallucination. Furthermore, hallucinations generated by GPT-40 provide the most consistent improvements across models. Additionally, we conduct empirical analyses and a case study to investigate key factors affecting performance and the underlying reasons. Our research sheds light on the potential use of hallucinations for LLMs and offers new perspectives for future research leveraging LLMs in drug discovery.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have not only been extensively applied in daily life to address real-world tasks [Ge et al., 2023; Yao et al., 2024], but also increasingly utilized as tools or agents in scientific domains, including materials science [Zhang et al., 2024b], biology [Madani et al., 2023] and chemistry [Boiko et al., 2023; Zhang et al., 2024a]. However, LLMs often generate plausible yet incorrect factual information or unrelated content in reference to the source text [Huang et al., 2023; Bang et al., 2023; Yuan and Faerber, 2023]. The issue of hallucination has raised major concerns in the NLP community regarding their reliability and applicability [Rawte et al., 2023]. While various methods have been proposed to mitigate hallucinations in LLMs [Dhuliawala et al., 2024; Ji et al., 2023], a novel perspective suggests that hallucinations may contribute to fostering creativity [Lee, 2023; Wang, 2024]. As creativity extends beyond merely retrieving information to include recombining and expanding existing knowledge, it closely resembles hallucination [Ye et al., 2023]. This creativity is vital in drug discovery, where exploring vast chemical spaces and devising innovative solutions to complex biological challenges are indispensable.\nThe process of drug discovery involves evaluating a vast number of potential drugs and requires long-term experimental trials, making it highly time-consuming and expensive [Gaudelet et al., 2021]. Prior work has leveraged machine learning to assist with this process [Rifaioglu et al., 2018; Mak et al., 2024], and generative models have also been utilized to facilitate drug discovery [Chakraborty et al., 2023; Murakumo et al., 2023]. Additionally, researchers have discovered that textual descriptions in chemistry can improve models' generalization ability [Liu et al., 2023]. As discovering new drugs requires domain-specific knowledge and the creativity to identify new patterns, hallucination becomes a potential strength for discovering molecules with high-level functions and abstract properties [Edwards et al., 2024a]. Researchers have started exploring the potential of LLMs"}, {"title": "3 Prompt Design for Hallucination Generation", "content": "Translating molecules into natural language has been shown in prior research to offer significant benefits across a range of scientific domains [Edwards et al., 2022]. As illustrated in Figure 2: Hallucination Generation, we translate molecules represented as SMILES strings into natural language. To generate a description for a given molecule, we provide the model with a SMILES string and ask it to generate descriptive text using the same prompt template across all datasets. Additionally, we define the system role as an \u201cexpert in drug discovery\":"}, {"title": "4 Task Formulation for Drug Discovery", "content": "We formulate the drug discovery task as a classification task to predict whether a molecule possesses a specific property or ability relevant to a target disease or function, e.g., the ability to inhibit HIV virus replication. To better align with the requirements of LLMs, we cast the classification task as a next-token prediction problem.\nGiven a molecule represented as a SMILES string [SMILES] and its description in natural language [Description], we construct a prompt template with the task instruction [Instruct]:\nAs shown in Figure 2: Label Prediction, the LLM processes the input and generates the next token as the predicted label \u0177. We constrain the output vocabulary space by instructing the model to \"only answer 'Yes' or 'No' \" for binary classification tasks. The predicted label is determined by selecting the token with the highest probability:\n$\\hat{y} = \\arg \\max_{y \\in V} P(y)$\nwhere $P(y)$ represents the generative probability from the LLM, and V is the vocabulary space for the labels, defined as V = {Yes, No} in binary classification tasks. Details of the different settings for [Description] are elaborated in \u00a75."}, {"title": "5 Experiment Design", "content": "We investigate how hallucinations from different LLMS affect model performance. The prompt template for the user remains consistent with \u00a74, represented as [SMILES][Description][Instruct], while varying the [Description] component under different settings. We establish the following baselines:\nSMILES: The [Description] is set to e, an empty string, so the model makes predictions solely based on the SMILES string of the molecule.\nMolT5: The [Description] is set to the description of the molecule generated by MolT5, allowing the LLM to access both the molecular structure and a reference description of the molecule in natural language.\nLLMS: Beyond the baselines, we evaluate how hallucinations generated by different LLMs influence performance. For each LLM, we generate a textual description of the molecule and replace [Description] with text containing hallucinations generated from itself or other LLMs. Specifically, [Description] is replaced with the hallucinated text generated by the same LLM for the label prediction, or with hallucinated text generated by other LLMs under the same conditions. This setup enables us to assess how performance changes with hallucinations produced by various models."}, {"title": "5.1 Dataset", "content": "We select five datasets from the MoleculeNet benchmark [Wu et al., 2018]. The selected datasets are related to classifying and inferring the ability of molecules regarding biophysical and physiological features. They focus on determining if the input molecule exhibits certain properties in the human body. HIV is a dataset containing molecules experimentally measured for their ability to inhibit HIV replication. BBBP includes binary labels indicating the blood-brain barrier penetration (permeability) of molecules. Clintox labels drugs based on whether they failed clinical trials due to toxicity. SIDER, called the Side Effect Resource, is a database of marketed drugs and adverse drug reactions, grouping drug side effects into 27 system organ classes. Tox21 measures the toxicity of compounds on 12 different targets.\nData Split We follow the split strategies outlined by Wu et al. [2018] to divide the datasets. HIV and BBBP are split using scaffold splitting, which partitions the samples based on their two-dimensional structures [Bemis and Murcko, 1996]. Clintox, SIDER, and Tox21 are split randomly. For all datasets, we allocate 10% of the entire dataset as the test set.\nLabel Selection and Evaluation We use the original labels of HIV, BBBP, and Clintox, as these are binary classification tasks that measure whether the input molecule possesses a specific ability or not. To follow the same setting, we select the label with the most balanced distribution in the test set for SIDER and Tox21, as they contain multiple labels. For SIDER, we test whether the molecule causes the side effect of reproductive system and breast disorders. For Tox21, we test whether the molecule shows evidence of activity against matrix metalloproteinases (SR-MMP). We evaluate model performance using ROC-AUC, following Wu et al. [2018]."}, {"title": "5.2 Model", "content": "We select seven commonly used and recently developed instruction-tuned LLMs, which fall into three categories: general open-source LLMs, domain-specific open-source LLMs, and OpenAI LLMs. Except for the OpenAI LLMs, we choose the open-source LLMs with a similar size of 7-8 billion parameters. The general open-source LLMs include Llama-3-8B, Llama-3.1-8B, Ministral-8B, and Falcon3-Mamba-7B. We use ChemLLM-7B as the domain-specific LLM for comparison with other general-purpose LLMs. For OpenAI models, we select GPT-3.5-turbo and GPT-40. Unless otherwise specified, we use default hyperparameter settings across all LLMs for generating hallucinations, with a temperature of 0.6 and a maximum token limit of 256 for new text generation."}, {"title": "6 Main Results and Analysis", "content": "We conduct our main experiments to investigate the following:\ni) whether adding hallucinations to the prompt improves the performance of LLMs compared to prompts containing only the SMILES string or reference descriptive text and\nii) which LLM-generated hallucinations lead to the greatest improvement.\nCan Hallucination Improve LLMs? We report the main results in Table 1, showing the performance of the two baselines and one hallucinated text that brings the most improvement for each LLM."}, {"title": "7 Empirical Analysis", "content": "Building on the main results, which demonstrate that hallucinations can improve LLMs in drug discovery, we conduct additional experiments to empirically analyze the factors that may influence LLM performance when using hallucinations. Specifically, we investigate the impact of model size, the temperature used for generating hallucinations, and the language of the hallucinations. As highlighted in \u00a76, Llama-3.1-8B achieved the highest performance with hallucinations, so we use Llama-3.1-8B as the base model in this section for the most cases."}, {"title": "7.1 Model Size", "content": "Model size is a critical factor influencing the performance of LLMs [Dubey et al., 2024]. We evaluate four sizes of instruction-tuned LLMs, namely 1B, 3B, 8B and 70B from Llama-3 models and compare their performance.\nEach model is evaluated using hallucinations generated by all other models discussed in \u00a76. We calculate the average improvement across all hallucinations relative to the baselines SMILES and MolT5. As shown in Figure 4, while all models outperform the SMILES baseline, Llama-3.2-1B is the only model that fails to surpass the MolT5 baseline. A clear trend emerges: as model size increases, performance also improves gradually from the 1B to the 8B models when compared to the MolT5 baseline. Although the 70B model does not outperform the 8B model, it still achieves better results than the smaller 1B and 3B models.\nIn contrast, comparisons with the SMILES baseline do not reveal a similar trend; all models outperform it when hallucinations are included in the prompt. In summary, increasing model size enhances the influence of hallucinations, but this effect appears to plateau at model sizes around 8B."}, {"title": "7.2 Generation Temperature", "content": "Previous research has investigated the role of temperature in LLMs [Peeperkorn et al., 2024; Renze, 2024]. Temperature is a key parameter that influences the generated text, controlling the randomness of the model's output [Van Koevering and Kleinberg, 2024]. We use Llama-3.1-8B to generate molecule descriptions under different temperature settings: {0.1, 0.3, 0.5, 0.7, 0.9}.\nFirst, we evaluate the generated text using the hallucination score. As illustrated by the line chart in Figure 5, the average hallucination score across the five datasets shows a clear trend: higher temperatures result in lower factual consistency and, consequently, higher hallucination.\nThe model's performance across different temperatures reveals a general pattern where higher hallucination scores correlate with worse performance. However, there is an exception at a temperature of 0.3, where the model does not perform as well compared to the temperature 0.5 and 0.7. Despite these variations, the overall performance with different temperatures does not differ significantly, ranging from 54.20% at a temperature of 0.9 to 57.10% at 0.1. This may be due to the relatively small differences in hallucination scores across temperatures.\nAs shown in Table 1, without LLM-generated hallucinations, Llama-3.1-8B achieves only 41.71% and 46.28% average ROC-AUC for the SMILES and MolT5 baselines, respectively. These scores are lower than the ROC-AUC achieved at any temperature setting. This further supports our finding that hallucinations can improve LLM performance in drug discovery."}, {"title": "7.3 Language of Hallucination", "content": "We further investigate whether the language of hallucination can affect the model's performance. Taking Llama-3.1-8B as the base model, we modify the [Instruct] template to prompt the LLM to generate molecule descriptions in six different languages: four languages included in the pretrained language list of Llama (English, German, French, Spanish) and two unseen languages (Chinese and Japanese). Prior research indicates that LLMs have multilingual capabilities, enabling them to understand languages even if they were not encountered during pretraining [Nie et al., 2024].\nFigure 6 presents the performance of Llama-3.1-8B with hallucinations in different languages across five datasets."}, {"title": "8 Case Study", "content": "To investigate why hallucinated text generated by LLMS can improve their performance in drug discovery, we conduct a case study using an example text generated by GPT-3.5 to describe the molecule. The analysis is performed with Llama-3.1-8B, as it achieves the best performance with the hallucinated text generated by GPT-3.5. We take the example molecule as SMILES string: CC1(Br)C(=O)NC(=O)N(C2CC(O)C(CO)O2)C1N=[N+]=[N-]."}, {"title": "9 Conclusion", "content": "In this work, we investigate whether hallucination can help LLMs in drug discovery. Coming up with the hypothesis that hallucinations can improve LLMs in drug discovery, we evaluate seven LLMs across five drug discovery datasets by incorporating LLM-generated hallucinations into the prompt. The experimental results confirm our hypothesis: hallucinations enhance the performance of LLMs compared to when no hallucination is provided. Nearly all the evaluated LLMs demonstrate better performance with hallucinations compared to without them.\nWe further explore factors that may influence hallucinations and the performance of LLMs. As model size increases, the improvement in LLM performance with the same hallucination shows a general trend of growth. While generation temperature impacts the hallucination score, it has minimal effect on the model's performance. Most notably, hallucinations in Chinese yield the highest average improvement for Llama-3.1-8B, despite Chinese not being a pre-trained language for the model. Meanwhile, we conduct a case study to investigate why text containing hallucinations can enhance LLM performance. We hypothesize that unrelated yet faithful information may contribute to this improvement.\nOur work provides a new perspective on leveraging hallucinations for LLMs and highlights their potential for fostering creativity in AI. Future research could build on these findings to further investigate the effects of hallucinations and explore the underlying mechanisms in depth."}, {"title": "A Details of the Models", "content": "In Table 2, we list the full names and the links of the LLMs used in the work. All the open-source LLMs can be applied directly using Transformers library by Huggingface."}, {"title": "B Details of the Datasets", "content": "The details, including the number of samples and the count of positive labels in each dataset are reported in Table 3."}, {"title": "C Prompt Template for Different Tasks", "content": "For each task, we use different instruction as they aim for predicting different properties of molecules."}, {"title": "C.1 HIV", "content": "System: You are an expert in drug discovery.\nUser: [SMILES] [Description]\nDoes the molecule have the ability to inhibit HIV replication? Only answer Yes or No:"}, {"title": "C.2 BBBP", "content": "System: You are an expert in drug discovery.\nUser: [SMILES] [Description]\nDoes the molecule have the ability to penetrate the blood-brain barrier? Only answer Yes or No:"}, {"title": "C.3 Clintox", "content": "System: You are an expert in drug discovery.\nUser: [SMILES] [Description]\nDid the molecule fail clinical trials due to toxicity? Only answer Yes or No:"}, {"title": "C.4 SIDER", "content": "System: You are an expert in drug discovery.\nUser: [SMILES] [Description]\nDoes the molecule cause side effects on the reproductive system or breast? Only answer Yes or No:"}, {"title": "C.5 Tox21", "content": "System: You are an expert in drug discovery.\nUser: [SMILES] [Description]\nDoes the molecule have the potential toxicity affecting mitochondrial membrane potential (SR-MMP)? Only answer Yes or No:"}, {"title": "D Hallucination Score for LLMs Generated Description", "content": "We evaluate the LLMs generated description using HHM-2.1-Open and report the results in Table 4."}, {"title": "E Full Results of Main Experiments", "content": "The full results for all LLMs used in our study are presented in Table 5. Each LLM is evaluated using at least nine prompt templates: two from the baselines and seven incorporating hallucinations generated by other models."}, {"title": "F Full Results for Model Size Experiments", "content": "The full results for the model size experiments are shown in Table 6. Additionally, the hallucination scores for each model size are provided in Table 7."}, {"title": "G Full Results for Experiments on Generation Temperature", "content": "Table 8 presents the complete results of Llama-3.1-8B performance when using hallucinations generated at different temperatures. Additionally, the corresponding hallucination scores are shown in Table 9."}, {"title": "H Prompt Template for Generating Hallucination in Different Languages", "content": "Algorithm 1 Generate Molecule Descriptions in Multiple Languages\nRequire: SMILES string\nlist of languages languages\n\n\n1: for all lang \u2208 languages do\n2: prompt \u2190 [SMILES] Describe the molecule in lar\n3: Generate description using LLM with prompt\n4: end for"}, {"title": "I Full Results for Experiments on Language of Hallucination", "content": "Table 10 presents the complete results for Llama-3.1-8B when using hallucinations generated by itself in various languages."}, {"title": "JExamples of Hallucination in Different Languages", "content": "We present an example of a molecule alongside the textual descriptions generated by MolT5 and Llama-3.1-8B in various languages in Table 11."}]}