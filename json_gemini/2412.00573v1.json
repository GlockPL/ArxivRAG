{"title": "Opus\nA Large Work Model for Complex Workflow Generation", "authors": ["Th\u00e9o Fagnoni", "Bellinda Mesbah", "Mahsun Altin", "Phillip Kingston"], "abstract": "This paper introduces Opus, a novel framework for generating and optimizing Workflows tai-\nlored to complex Business Process Outsourcing (BPO) use cases, focusing on cost reduction\nand quality enhancement while adhering to established industry processes and operational con-\nstraints. Our approach generates executable Workflows from Intention, defined as the align-\nment of Client Input, Client Output, and Process Context. These Workflows are represented\nas Directed Acyclic Graphs (DAGs), with nodes as Tasks consisting of sequences of executable\nInstructions, including tools and human expert reviews. We adopt a two-phase methodology:\nWorkflow Generation and Workflow Optimization. In the Generation phase, Workflows are gen-\nerated using a Large Work Model (LWM) informed by a Work Knowledge Graph (WKG) that\nencodes domain-specific procedural and operational knowledge. In the Optimization phase,\nWorkflows are transformed into Workflow Graphs (WFGs), where optimal Workflows are de-\ntermined through path optimization. Our experiments demonstrate that state-of-the-art Large\nLanguage Models (LLMs) face challenges in reliably retrieving detailed process data as well as\ngenerating industry-compliant workflows. The key contributions of this paper include:\n1.  The integration of a Work Knowledge Graph (WKG) into a Large Work Model (LWM),\nenabling the generation of context-aware, semantically aligned, structured and auditable\nWorkflows.\n2.  A two-phase approach that combines Workflow Generation from Intention with graph-\nbased Workflow Optimization.\n3.  Opus Alpha 1 Large and Opus Alpha 1 Small, models that outperform state-of-the-art\nLLMs by 38% and 29% respectively in Workflow Generation for a Medical Coding use\ncase.", "sections": [{"title": "Introduction", "content": "In the Business Process Outsourcing (BPO) industry, achieving efficiency and consistent quality in\nwork execution is a challenge, particularly when dealing with complex Workflows that transform a\nClient's Input into a defined Output under specific constraints such as time and cost. Most BPO\nwork can be modeled as Workflows, where a series of tasks are executed to transform the Input into\nthe Output. The quality of the Output, as well as adherence to constraints, is ultimately judged\nby the Client.\nIn this paper, we represent Workflows as Directed Acyclic Graphs (DAGs). Each node is a Task\n(t), each workflow has single entry and exit Tasks. The entry Task takes as input the Client's Input\n(I), and the exit Task outputs the final Output (O) to the Client. The edges represent the flow\nof execution of the Tasks. Tasks consist of sequences of executable Instructions, which can range\nfrom simple code to predefined tools, as well as human expert reviews at various stages to ensure\nquality.\nA significant challenge in the modernization of BPO lies in the closed and proprietary nature of\nWorkflow process data, which hinders the ability to explicitly define Workflows and Tasks, and\ntherefore achieving the desired levels of complexity and quality in Workflows for specific use cases.\nOur goal is to automate and improve BPO processes, reducing costs and enhancing quality, by\ngenerating Workflows that can efficiently transform Input into Output. This includes not only\ngenerating Tasks and selecting appropriate Instructions but also deciding when human intervention\nis necessary. Additionally, in this paper, we address the optimization of Workflows post-generation\nby refining Tasks, edges, Instructions, and human expert reviews to meet time, accuracy, and cost\nconstraints. This approach is adopted in the Opus AI Knowledge Work Platform that allows\norganizations who would otherwise use BPOs to reduce costs and increase operational flexibility by\ncombining efficient AI Workflows with human expert reviews.\nDefinitions Our solution is based on the following concepts:\nWorkflow Intention is defined as the structured alignment of the Client's Input (I), potential\nProcess Context, and the expected or actual Client's Output (O).\nThe Work Knowledge Graph (WKG) is our proprietary graph representing Workflows collected\nfrom various use cases across industries, encapsulating domain-specific procedural and operational\nknowledge.\nWorkflow Generation is defined as the process of leveraging the Intention representation to\nidentify relevant areas of the WKG in order to generate Workflows as sequences of Tasks using a\nLarge Work Model (LWM).\nThe Large Work Model (LWM) is our fine-tuned model informed by the WKG and the Intention,\nconstrained to transform Input into Output. The generation phase concludes by combining the\nWorkflows generated by the LWM into a graph, the Workflow Graph (WFG).\nThe Workflow Graph (WFG) is defined as a directed graph representing multiple connected\nWorkflows from Input (I) to Output (O).\nWorkflow Optimization is defined as solving a path optimization problem on the WFG, where\nthe output Workflow is represented as a path p* that minimizes a cost function C.\nWorkflow Execution is not covered in this article."}, {"title": "Background", "content": "The generation and optimization of workflows through Large Language Models (LLMs) is a multi-\nfaceted challenge that involves a deep understanding of task decomposition, Intention capture, and\nknowledge injection. This section reviews key advancements in LLM-based workflow generation,\nfocusing on the techniques, specialized models, and frameworks that contribute to more accurate and\nefficient workflows. We first examine foundational techniques for generating workflows using LLMS,\nfollowed by specialized systems that enhance task complexity handling, the role of knowledge graphs,\nand methods for optimizing workflows. We conclude by evaluating benchmarks and addressing the\nneed for safeguards to assess the performance of these systems.\nLarge Language Models for Workflow Generation Chain of Thought [1] is a prompting\ntechnique that enhances LLMs' ability to handle complex reasoning queries by generating a se-\nries of logical steps. ReAct [2] combines reasoning, action planning, and external interactions to\nimprove interpretability and reduce errors like hallucinations in tasks such as question-answering\nand fact-verification. The study [3] explores how LLMs structure proof steps through in-context\nlearning, improving workflow generation by incorporating learned steps within the input context.\nThinking LLMs [4] is a training method that equips LLMs with reasoning and planning capabilities\nthrough iterative search and preference optimization, using a judge model to evaluate thought can-\ndidates and refine responses for complex workflow generation across diverse domains. Specialized\nsystems employ LLMs as agents to perform more complex workflows. MegaAgent [5] provides an\nautonomous framework for large-scale multi-agent systems, enabling dynamic agent generation and\ntask management. Gorilla [6] is a fine-tuned LLaMA-based model that improves LLMs' accuracy\nin making API calls, a critical function for integrating external data into workflows. The XLAM\nseries [7] offers open-source Large Action Models, ranging from 1B to 8x22B parameters, trained for\nagent-specific tasks. These models enhance generalizability, making them well-suited for complex\nworkflow generation across diverse domains.\nThese studies underscore a fundamental limitation of LLM-based systems: their dependence on\ntraining data constrains their ability to retrieve or infer process context, either due to its absence or\nbecause of poorly defined Intentions. To enable effective Workflow Generation, three key priorities\nemerge: accurately capturing user Intentions, incorporating process-specific knowledge, and clearly\ndelineating Workflows, Tasks, and Instructions.\nIntention Capture Previous work has explored strategies for intention capture in AI-generated\nworkflows. Intention is All You Need [8] highlights the complexities of using generative AI to discern\nuser intentions in natural language, emphasizing the challenge of \"bridging the gap between lan-\nguage space and the physical world\", particularly when diverse intentions risk being homogenized\nby LLMs. Lumos [9], a multi-modal question-answering system, captures user intent by integrating\ntext and visual inputs, offering a robust framework for understanding intent in multi-modal con-\ntexts. Shankar [10] introduces DocETL, a declarative framework for unstructured data processing\nusing LLMs, focusing on accuracy and task optimization through an agent-based system that re-\nfines document processing workflows through \"rewrite directives\" and incorporates an evaluation\nmechanism to orchestrate task-specific validation. While these works address intention capture\nthrough generative AI, multi-modal integration, or agent-based refinement, they often rely on im-\nplicit techniques such as prompt engineering. In contrast, we explicitly define an Intention Layer"}, {"title": "System Architecture", "content": "This section outlines the Workflow Generation and Optimization layers of the Opus system, repre-\nsented in Figure 1.\nThe Opus system consists of four main layers: Intention Encoding, Workflow Graph (WFG) Gener-\nation, Workflow Optimization, and Workflow Execution, with the latter not covered in this article.\nThese layers are supported by two key components: the Work Knowledge Graph (WKG),\nwhich organizes domain-specific knowledge, and the Large Work Model (LWM), designed to\ninterpret and produce workflows. Our approach can be classified as a knowledge graph Retrieval-\nAugmented Workflow Generation method. The main parts are described in the following sections."}, {"title": "Workflow Generation and Optimization", "content": ""}, {"title": "Work Knowledge Graph (WKG)", "content": "The WKG is a Directed Graph whose Nodes and Edges are derived from Workflows collected\nacross industries and Opus usage. The following paragraphs describe some of the Node and Edge\nfeatures, which we leverage for different purposes across the Workflow Generation, Optimization\nand Execution. This paper serves as an illustrative example of our methodology. Only the features\nleveraged for Workflow Generation will be described, and we omit how we designed and manage\nthe WKG.\nPreliminaries Let w be a Workflow, we define iw a Workflow Implementation of w into a com-\nputer executable script and Zw the set of Workflow Implementations of w. Similarly, it denotes an\nimplementation of a Task t and in an implementation of an Instruction x.\nA Workflow Implementation iu is partially composed of a sequence of Task Implementations (it,k)k.\nA Task Implementation it is partially composed of a sequence of Instruction Implementations\n(ix,k)k, which we will not cover for this version of Workflow Generation and Optimization.\nEach historical Implementation (Workflow, Task, Instruction) is stored along with historical data\ninforming on costs (compute, time, etc.) and success rate which are leveraged for Workflow Opti-\nmization.\nWe denote by W the set of all Workflows and Iw = Uwew Iw the set of all historical Workflow\nImplementations.\nNodes NWKG: Tasks In the WKG, a Node is a Task. It has Semantic Features as well as a set\nof historical Task Implementations. For this version of Workflow Generation, only the Semantic\nFeatures of the Tasks are used, which includes Title, Description, Industry, formatted strings of\nTask Implementations which sequenced Instructions semantic features, etc.\nEdges EWKG: Historical Workflow Implementations Two Tasks are connected in the WKG\nif there exists a historical Workflow Implementation where Implementations of these Tasks are\nconsecutive. This establishes the presence of an Edge and its direction. Only this information is\nleveraged for the Workflow Generation solution described in this paper."}, {"title": "Intention", "content": "The Intention Capture problem involves extracting both process-oriented and result-driven features\nfrom system inputs, specifically Client Input, Client Output, and Process Context, to generate\neffective Workflows, with the Intention Encoder (using attention networks) ensuring that missing\ncomponents whether process or output-related are resolved to align the Workflow with the\ndesired outcome.\nMulti-modal Input Pre-processing The Opus system supports multiple modalities to capture\nas much information as possible on the Process Context and the expected Client Output given\nClient Inputs, i.e. the Intention. Text, images, audio, and video Inputs (Im)m are preprocessed\nindependently into a default format:\n\u00cem = fpreprocessing (Im), m \u2208 {text, image, audio, video}\nMulti-modal Input Encoding An encoder fencode is developed for each modality m in order\nto get a common vector representation of the preprocessed input \u00cem.\nIm = fencode (\u00cem)\nwhere Im denotes the encoded features of each input of modality m in a common high-dimensional\nspace, enabling cohesive interpretation across modalities in subsequent stages. Text data is encoded\nthrough tokenization and attention layers, capturing semantic meaning. Image data goes through\nOCR for text encoding and convolutional encoding to extract spatial features.\nIntention Encoding, Routing and Decoding We define the Intention Capture problem as\nextracting both process-oriented and result-driven features from the system input to generate effec-\ntive Workflows. The problem is formalized with three system input components: Client Input (e.g.\nan input file initiating the Workflow), Client Output (e.g. the expected output file), and Process\nContext (e.g. training documents defining the Workflow, usually designed to train BPO workers).\nAt least the Client Input, alongside either the Client Output or Process Context, is mandatory\nfor Workflow Generation. We introduce Intention Encoder and Decoder as attention networks, as\nthe problem formulated above closely align with the query-key-value structure [25]. The Client\nInput acts as the \"query\", driving the Workflow Generation by seeking either a defined outcome\n(Client Output) or procedural details (Process Context). Attention layers compute relevance be-\ntween query-key pairs to produce the value, which represents the missing information necessary\nto complete the Intention. If either the Process Context or Client Output is missing, the value\nprovides the missing component. When both Process Context and Client Output are available, the\nvalue aligns with the Output, ensuring the Workflow adheres to the desired outcome.\nFor Intention Encoding, an additional encoder fIntention is developed to create a joint representation\nI of all the encoded input modalities,\n\u0393 = fIntention({\u0393m}m)\nFor Intention Routing, the goal is to utilize the Intention encoding to identify areas of the WKG,\nwhich will be leveraged later to generate process compliant Workflows. Each Node from the WKG is"}, {"title": "Large Work Model (LWM)", "content": "This section outlines the role of the LWM for Workflow Generation.\nIncentives for the LWM The LWM operates at a semantic level to create Workflows as Task\nsequences, transforming the Client's Input into the desired Output. It is guided by two primary\nincentives for generating Workflows:\n1.  being results-driven: the generated Workflow needs to go from the Client Input to the Client\nOutput, described in the decoded Intention.\n2.  being process-driven: the system needs to leverage insights from the WKG to determine how\nthe Workflow should be constructed.\nCore functionality and inference The LWM is currently a fine-tuned Large Language Model\n(LLM) trained to generate structured text representing Task sequences. For inference, the Client's\nIntention is decoded in natural language and each relevant sub-graph of the WKG is converted\ninto text (detailed in next section). The LWM is prompted with these textual descriptions to\nproduce Workflows, one for each sub-graph, detailing each Task's semantic features including Title,\nDescription, and a structured sequence of Instructions.\nThe specifics of the LWM model, including its architecture and training details, are omitted."}, {"title": "Workflow Graph (WFG)", "content": "In this section we describe the last step of Workflow Generation, the generation of a Workflow\nGraph (WFG) with the LWM, the WKG, the selected nodes V, and the decoded Intention. The\ngoal of the WFG generation is to generate a graph connecting the Client Input (I) to the Client\nOutput (O), from which an Optimal Path can be computed (see next section).\nThe first step is to compute a set of sub-graphs of the WKG:\n\u0398 {SWKG}\nWe start by splitting V into neighborhoods by computing node embeddings and running a KNN\nalgorithm to get the sets of neighbor nodes.\nV = \u222aN"}, {"title": "Workflow Optimization", "content": "The final step of the Opus system presented in this paper is the Workflow Optimization, which is\neffectively picking a Directed Acyclic Graph, or a sequence of Tasks to simplify, going from Input\nto Output, from the WFG. It can be expressed as an optimization problem,\np* = arg min C(p)\nPCWFG\nwhere C(p) is a cost function associated with each potential path p within the WFG, accounting for\nfactors such as computational resources, processing time, and specific model usage costs. Though\nwe will not expand on the cost model in this paper, we adopt a linear model over these three\ndimensions:\np* = arg min (\u03b1. Ccompute (p) + \u03b2\u00b7 Ctime(P) + \u03b3\u00b7 Cmodel(p))\np\nwhere Ccompute (p) represents the computational cost, Ctime(p) denotes the temporal cost, Cmodel (P)\nreflects the model usage cost, and \u03b1, \u03b2, and y are weight factors that prioritize the relative impor-\ntance of each cost component. In this version, we further model the path cost C(p) as the sum of\nthe costs of all tasks t within the path p: C(p) = \u2211tep C(t), following the previously described cost\nmodel. Consequently, we employ a modified Dijkstra algorithm to determine an efficient path from\nthe Input node to the Output node, adapting the approach to account for node-associated costs\nrather than edge-associated costs."}, {"title": "Empirical results: an application to Medical Coding", "content": ""}, {"title": "CPT\u00ae Evaluation / Management Medical Coding", "content": "CPT\u00ae Evaluation / Management (E/M) is a widely adopted and semantically complex work in\nhospital Medical Coding. It involves numerous specific definitions of entities that must be identified\nwithin multi-modal inputs. This process is predominantly carried out by medical coders, who\nadhere to established guidelines such as those issued by the American Medical Association (AMA).\nHowever, hospitals often implement customized variations of these guidelines, introducing additional\nintricacies that are difficult to formalize and capture. In collaboration with hospital Healthpoint\nAbu Dhabi, we mapped outpatient Medical Coding Workflows and best practices, as performed on\nthousands of cases every day. Medical Coding occurs at the level of a medical encounter, defined\nas an interaction where a patient visits the hospital to see a doctor. Documentation related to an\noutpatient encounter typically includes:\nConsultation Notes: the primary document, detailing the physician's notes and description\nof the encounter.\nOrder List: a record of tests or medications prescribed during the encounter.\nReports: ancillary documents such as MRI or X-Ray results reviewed during the encounter.\nMedical coders perform three primary tasks on these documents:\nData Points: identifying specific factual entities in the input documents.\nProblem Points: highlighting clinically significant issues.\nLevel of Risk: assessing the overall risk associated with the encounter.\nThe E/M code is deterministically derived from classifications made in these three tasks. Each\ntask requires semantic searches across the input documents to identify and count entities, applying\na series of logical operations to synthesize the results. These operations involve numerous non-\ntrivial conditions and intricate rules. In the context of our Workflow-based approach, each of these\nthree tasks is represented as a succession of Tasks within the overall Workflow. In the Appendix we\nprovide a high-level schema that outlines the reference Workflow used in our experiments, alongside\na simplified semantic description of the Tasks involved."}, {"title": "Experiments", "content": "We conducted experiments comparing two versions of the Opus system, appliedai-opus-1alpha-\nlarge and appliedai-opus-1alpha-small, against state-of-the-art LLMs openai-o1-preview-2024-09-12,\nopenai-gpt-40-2024-08-06, google-gemini-1.5-pro, google-gemini-1.5-flash, and anthropic-claude-3.5-\nsonnet. Opus Alpha 1 Large version is supported by larger WKG and LWM (70B versus 8B in the\nSmall version).\nThe evaluation focuses on two core metric classes: Semantic Fidelity and Structural Fidelity. To\nestablish a reliable benchmark, senior professional medical coders manually created a ground truth\nreference Workflow in the same Task sequence format as the Workflow Generation outputs (de-\nscribed in the Appendix). Each model was prompted to generate a Workflow in the form of a"}, {"title": "Results", "content": "The results are summarized in the following table, with models ranked based on their overall\nperformance across the metrics, computed as the total covered area of the pentagon formed with\nthe five metric values and equal angles.\nOpus Alpha 1 Large was the best overall performer followed by Opus Alpha 1 Small and Anthropic\nClaude 3.5 Sonnet. On average across all metrics evaluated, Opus Alpha 1 Large and Opus Alpha\n1 Small outperformed Anthropic Claude 3.5 Sonnet by 38% and 29% respectively."}, {"title": "Variability between Opus Alpha 1 Large and Opus Alpha 1 Small", "content": "Interestingly, the\nsmall variant of the Opus system achieves a higher coverage ratio (0.746) compared to its larger\ncounterpart (0.721), but the larger model demonstrates better performance on certain metrics, such\nas Kendall's Tau (0.498 vs. 0.083) and BLEU Score (0.361 vs. 0.235). This indicates that while the\nsmaller model excels in Task coverage possibly due to reduced noise in the process of identifying\nareas of a smaller WKG the larger system generates Workflows with better sequential coherence\nand linguistic similarity to the reference. This holds true only when the knowledge relevant to\nthe use case is contained within the smaller WKG. Despite these discrepancies, both Opus models\noutperform the other models by at least a factor of two across most metrics, underscoring their\ndominance in generating accurate Workflows."}, {"title": "Limitations in Existing Models", "content": "The results reveal that state-of-the-art models, such as openai-\ngpt-40-2024-08-06 and google-gemini-1.5-pro, fail to deliver the complexity of Workflows adequately.\nFor example, their coverage ratios are as low as 0.208 and 0.108, respectively, and their BLEU\nscores, which indicate linguistic alignment, are negligible. These findings suggest that without\nsignificant data injection tailored to Workflow Generation, these models struggle to represent the\nintricate structures required for practical Workflow applications. They are unable to optimize over\na Workflow path nor evaluate an unseen Workflow that was not part of their training data.\nResults show that standard RAG approaches and state-of-the-art LLMs fail to meet the complex-\nity and precision needed for unambiguous Workflow Generation. These models lack mechanisms\nfor capturing user Intentions and frameworks for storing and leveraging domain-specific knowledge\nabsent from training data. Our integration of Intention Capture and Workflow Knowledge Graph\n(WKG) to a Large Work Model (LWM) overcomes these limitations: Intention Capture ensures\na deep understanding of user inputs, while the WKG provides a scalable, structured repository\nof domain knowledge. We achieve the generation of detailed, rigorously structured, and logically\ncomposed workflows, as demonstrated by superior performance in our use case evaluations. By\nexplicitly defining Instructions, Tasks, and Workflow Layers, our LWM trained in this framework\ncan generate Workflows that address both content gaps and structural deficiencies. While im-\nproved prompting may offer minor gains, the challenges of Intention capture, knowledge storage\nand retrieval, and Workflow composition are only effectively addressed by Opus."}, {"title": "Conclusion", "content": "In this article, we presented the Workflow Generation framework of Opus, designed to address the\nchallenges of generating and optimizing complex, industry-level Workflows. Central to our approach\nis the adoption of a unified framework for representing Workflows, Tasks, and Instructions. A key\ncontribution of our work is the introduction of the concept of Workflow Intention, defined as a\nthree-dimensional alignment of Client Input, Client Output, and Process Context. We proposed\na novel methodology for generating Workflows as Directed Acyclic Graphs (DAGs), establishing\nthe importance of Workflows being both process-driven and results-driven. This is achieved by\nleveraging the concept of Intention, along with the Work Knowledge Graph (WKG), which en-\ncapsulates domain-specific procedural knowledge. These components collectively feed the Large\nWork Model (LWM), enabling it to generate context-aware, semantically aligned, structured and\nauditable Workflows that can be used in regulated industries. Opus Alpha 1 Large and Opus Alpha\n1 Small outperformed state-of-the-art LLMs by 38% and 29% respectively in Workflow Generation\non a real-world use case in Medical Coding."}, {"title": "Acronyms and Definitions", "content": ""}, {"title": "Background", "content": "Guardrails for Scaled Production Generative Systems with Human as Input For gen-\nerative systems, particularly those involving LLMs, it is essential to implement guardrails to ensure\nsafety, efficiency, and accountability, especially when human input is involved. These guardrails ad-\ndress potential risks such as constraint hallucinations, reduced interpretability, scaling issues, and\nsecurity vulnerabilities. The Agent-as-a-Judge framework [26] addresses the challenge of constraint\nhallucinations by enabling agents to evaluate each other throughout the generation process. This\nself-assessment mechanism fosters continuous feedback, promoting more dynamic and scalable im-\nprovements in agent performance and ensuring that the system adheres to the intended constraints\nduring workflow generation.\nTo enhance the interpretability of multi-agent systems, Triantafyllou et al. (2024) [27] introduces a\ncausal explanation model that attributes counterfactual effects to agents and state variables. This\nmodel aids in understanding how decisions are made within the system, increasing transparency\nand trustworthiness, which are essential when human involvement is integrated into the process.\nWhen scaling multi-agent LLM systems, frameworks such as Optima [28] optimize communication\nand efficiency by refining task selection and training processes, achieving significant performance\ngains up to 2.8x and reducing token usage by 10 percent in information-heavy tasks. This\nensures that agents can collaborate effectively without overwhelming the system resources.\nSecurity is also a critical consideration in scaled systems. Prompt Infection [29] identifies vulner-\nbilities in multi-agent LLM environments, where malicious prompts can propagate like viruses,\nposing risks such as data theft and misinformation. The introduction of LLM Tagging mitigates\nthis threat by marking potentially dangerous interactions, reinforcing the need for robust security\nmeasures in complex agent-based systems.\nIn addition, studies such as Lermen et al. (2024) [30] highlight safety gaps, revealing how models\nsuch as Llama 3.1 can still perform harmful tasks despite safeguards. This issue is addressed by the\nSafe Agent Benchmark, which evaluates and improves the security of agentic systems, underscoring\nthe need for more rigorous safety mechanisms to protect against unintended harmful actions."}, {"title": "Workflow Evaluation and Benchmark", "content": "Evaluating the performance of agents in workflow\ngeneration is critical for ensuring the effectiveness and scalability of these systems, yet current\nbenchmarks often fail to capture the complexity and real-world applicability of generated workflows.\nMost existing benchmarks are results-driven, focusing primarily on whether the workflow produces\nthe correct answer. For example, Zhuge et al. (2024) [24] introduces a utility function that includes\nvarious cost dimensions, but the benchmark still largely measures the ability to generate the right\nanswer, often in simple scenarios. Similarly, ChainBuddy [31] parameterizes metrics such as the\nnumber and types of nodes in a workflow, but does not go far enough in capturing the full scope of\nworkflow execution costs and real-world constraints.\nCurrent benchmarks generally fail to evaluate workflows based on the costs associated with their\nexecution. To our knowledge, no existing framework thoroughly examines how efficiently and\neffectively a workflow is executed in practice, especially when considering resource consumption,\ntime constraints, or the need for human intervention. This is a significant gap, as workflow execution"}, {"title": "Reinforcement Learning for Workflow Generation and Optimization", "content": "While we acknowl-\nedge recent advancements in leveraging reinforcement learning (RL) for task-oriented agents, we do\nnot apply RL in this version of our workflow generation approach. This decision is driven by the\ncomplexity of workflow environments, where the action space is not well-defined or simple enough\nfor traditional RL methods to be directly applicable. In complex workflows, actions are often nu-\nanced and context-dependent, making it challenging to model the environment in a way that would\nallow for effective RL-based decision-making. Instead, we focus on approaches that better handle\nthe intricate structures of real-world workflows. Reflexion [36] proposes a framework for reinforce-\nment learning for language agents that replaces traditional weight updates with linguistic feedback.\nAgents reflect on task feedback, storing it in episodic memory to improve future decision-making.\nFrom Words to Actions [11], mentioned before, presents a hierarchical RL model for LLMs, where\na Planner generates sub-goals for the Actor to execute. The Planner uses Bayesian aggregated\nimitation learning (BAIL) and an e-greedy exploration strategy to reduce regret. This approach\nenables effective decision-making in partially observable environments and supports multi-agent\ncoordination through the Planner acting as a world model."}, {"title": "Workflow Manual Builder systems", "content": "AutoGen Studio [37] and ChainBuddy [31] are systems de-\nsigned to facilitate the creation and evaluation of multi-agent workflows. AutoGen Studio, built on\nthe AutoGen framework, allows users to prototype, debug, and evaluate workflows using a no-code\ninterface. It enables the specification of LLM-enabled agents through a declarative JSON-based for-\nmat and offers tools for interactive evaluation and debugging. ChainBuddy, part of the ChainForge\nplatform, assists users in generating and evaluating LLM pipelines for specific tasks. It simplifies\nthe process of setting up and evaluating workflows, aiming to reduce the \"blank page\" problem."}]}