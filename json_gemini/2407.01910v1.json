{"title": "MG-Verilog: Multi-grained Dataset Towards\nEnhanced LLM-assisted Verilog Generation", "authors": ["Yongan Zhang", "Zhongzhi Yu", "Yonggan Fu", "Cheng Wan", "Yingyan (Celine) Lin"], "abstract": "Large Language Models (LLMs) have recently\nshown promise in streamlining hardware design processes by\nencapsulating vast amounts of domain-specific data. In addition,\nthey allow users to interact with the design processes through\nnatural language instructions, thus making hardware design\nmore accessible to developers. However, effectively leveraging\nLLMs in hardware design necessitates providing domain-specific\ndata during inference (e.g., through in-context learning), fine-\ntuning, or pre-training. Unfortunately, existing publicly available\nhardware datasets are often limited in size, complexity, or detail,\nwhich hinders the effectiveness of LLMs in hardware design\ntasks. To address this issue, we first propose a set of criteria\nfor creating high-quality hardware datasets that can effectively\nenhance LLM-assisted hardware design. Based on these criteria,\nwe propose a Multi-Grained-Verilog (MG-Verilog) dataset, which\nencompasses descriptions at various levels of detail and corre-\nsponding code samples. To benefit the broader hardware design\ncommunity, we have developed an open-source infrastructure\nthat facilitates easy access, integration, and extension of the\ndataset to meet specific project needs. Furthermore, to fully\nexploit the potential of the MG-Verilog dataset, which varies\nin complexity and detail, we introduce a balanced fine-tuning\nscheme. This scheme serves as a unique use case to leverage\nthe diverse levels of detail provided by the dataset. Extensive\nexperiments demonstrate that the proposed dataset and fine-\ntuning scheme consistently improve the performance of LLMs\nin hardware design tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have recently emerged\nas a promising approach to streamline hardware design pro-\ncesses [1], [4], [5], [8], [16], [17]. By encapsulating vast\namounts of domain-specific data and enabling users to interact\nwith the design processes through natural language prompts,\nLLMs have the potential to make hardware design more\naccessible to a broader range of developers. This increased\naccessibility can foster innovation and accelerate the develop-\nment of new hardware solutions, as it allows developers with\nvarying levels of expertise to contribute to design processes.\nDespite the great potential of LLMs, existing state-of-the-\nart (SOTA) general LLMs, e.g., OpenAI's GPT-4 [11], are still\nlimited in their ability to generate practical hardware designs.\nFor example, they might generate non-synthesizable or non-\nfunctional hardware source code [4]. To address this limitation,\nrecent studies suggest that incorporating additional domain-\nspecific data is crucial for enhancing LLMs' performance in\nhardware design tasks, using techniques across the scopes\nof LLM inference, fine-tuning, or pre-training. Specifically,\none approach to improve LLMs' hardware design capabilities"}, {"title": "II. CRITERIA FOR DATASETS IN LLM-ASSISTED\nHARDWARE DESIGN", "content": "To create a high-quality dataset for LLM-assisted hardware\ndesign, we first establish design criteria to guide the develop-\nment of the MG-Verilog dataset.\nSufficient dataset size. This is crucial for both training\n(i.e., domain-specific pre-training or fine-tuning) and inference\n(i.e., in-context learning) of LLMs. A larger dataset provides\ndiverse examples for improved generalization performance\nduring training [7], [8] and enables effective techniques such\nas Retrieval-Augmented-Generation (RAG) for enhanced gen-\neration quality during inference [4].\nAccurate code-description pairs. Each code sample needs\nto be correct, functional, and associated with a precise descrip-\ntion of its functionality. Inaccuracies or ambiguity can mislead\nLLMs during fine-tuning or pre-training and lead to erroneous\ncode generation during inference.\nVaried description detail levels. They are necessary to\naddress two challenges. Datasets with only high-level de-\nscriptions may not provide sufficient detail for accurate code\ngeneration or effective LLM training (i.e., fine-tuning or pre-\ntraining), especially for complex designs. Conversely, datasets\ndominated by detailed descriptions may limit practical utility,\nas LLMs trained on such datasets might require users to\nprovide elaborated prompts, which can be as labor-intensive as\ncoding from scratch. Hence, an effective dataset should incor-\nporate both high-level and detailed descriptions in a proper\nbalance. In particular, high-level descriptions can facilitate\nuser-friendly LLM interactions, while detailed descriptions are\ncrucial for enabling LLMs to create complex designs, offering\nin-depth guidance for LLMs during training, or serving as a\ncomprehensive reference during inference.\nExtensibility and integrability for future development.\nA high-quality hardware dataset should be designed with the"}, {"title": "III. THE PROPOSED MG-VERILOG DATASET", "content": "The MG-Verilog dataset consists of over 11,000 Verilog\ncode samples and their corresponding natural language de-\nscriptions, serving as the desired outputs and test inputs for\nvarious LLM-assisted hardware design tasks, such as Verilog\ncode generation."}, {"title": "B. Dataset Construction", "content": "The construction of the MG-Verilog dataset involves several\nsteps to ensure the quality and usability of the data.\n1) Data Collection and Preprocessing: Raw source code\nfrom open-source repositories is collected and preprocessed\nto ensure correctness. Adapting from VerilogEval [8], we use\nPyverilog [14] to parse the raw Verilog code and exclude code\nsamples containing syntax errors. Deduplication techniques\nare applied to remove redundant code samples. Additionally,\ndependencies of the code samples are extracted, i.e., sub-\nmodules of multi-module code samples are identified and\nrecorded as metadata to facilitate research on techniques such\nas few-shot learning and RAG for generating multi-module\nVerilog code.\n2) Description Generation: Natural language descriptions\nare appended to the code samples using an approach similar\nto VerilogEval [8], leveraging LLMs' superior natural lan-\nguage generation capabilities. In addition to simple high-level"}, {"title": "C. Multi-grained Dataset Structure", "content": "To strike a balance between design generation accuracy\nand user-friendliness, we adopt a multi-grained data structure,\nwhich encompasses descriptions at various levels of detail in\norder to satisfy the third criterion in Sec. II. As depicted in\nFig. 1, this structure organizes hardware code descriptions,\nranging from high-level summaries to detailed, line-by-line\ncomments. The multi-grained structure is designed to mimic\nthe learning and design processes of human designers. The\nobjective is to simplify the learning curve for using the dataset\nand, as demonstrated later, to better leverage the strengths of\nLLMs for enhanced description generation accuracy. Specifi-\ncally, the multi-grained structure mirrors the typical two phases\nexperienced by human designers. In the learning phase, a hard-\nware designer starts with the basic syntax and semantics of the\ndesign language, gradually advancing to apply this knowledge\nto design higher-level hardware modules. Conversely, in the\ndesign phase, the process begins with high-level architectural\nplanning for the entire design, followed by a detailed, step-by-step implementation."}, {"title": "D. Detailed Statistics of the Dataset", "content": "Fig. 2 presents detailed statistics of the MG-Verilog dataset,\nillustrating the distribution of token length for both the code\nand varying levels of descriptions. The complexity of the code\nsamples is also reflected in the distribution of the number of\nmodule instances. The dataset shows a wide range of natural\nlanguage description details and code complexities, making it\nsuitable for diverse LLM-assisted hardware design tasks."}, {"title": "E. Dataset Access and Extension Instructions", "content": "The MG-Verilog dataset is publicly available and packaged\nin the standard HuggingFace Datasets format [6] for easy ac-\ncess and integration. Each dataset entry contains the following\nfields: code, high-level summaries, detailed summaries, block-\nlevel summaries, line-by-line comments, and metadata. The\nmetadata field currently includes the module dependencies of"}, {"title": "IV. DATASET UNIQUE USE CASE: A BALANCED\nFINE-TUNING SCHEME", "content": "In this section, we show a unique use case of our proposed\nMG-Verilog dataset. Specifically, we introduce a balanced\nfine-tuning scheme to fully harness the diverse levels of detail\nprovided by our MG-Verilog dataset.\nThe challenge to address. The ultimate goal of fine-tuning\nis to generate hardware code solely from high-level design\ndescriptions. However, challenges arise when determining the\ntype of descriptions to be used for fine-tuning. On the one\nhand, fine-tuning with only simple high-level descriptions may\nnot provide LLMs with sufficient information to generate code\nfor complex designs. On the other hand, exclusively relying\non detailed descriptions could hinder LLMs' ability to respond\nto more high-level user instructions.\nOur balanced fine-tuning scheme. To tackle the aforemen-\ntioned challenge, we present a balanced fine-tuning scheme\nthat randomly selects training samples with varying levels of\ndescriptions from the MG-Verilog dataset in each fine-tuning\niteration. The aim is to achieve a balance when imparting\nknowledge of both global and local code semantics to LLMs."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "The primary model for generating\ndescriptions is LLaMA2-70B-Chat. GPT-3.5-turbo serves as\nan automated backup for scenarios where the maximum token\nlimit is exceeded. Based on empirical testing, we set the\ntemperature to 0.7 and top_p to 0.95, maintaining other\nhyperparameters at their default values for the best quality.\nFine-tuning and inference. CodeLLaMA-7B-Instruct is\nchosen as the primary model for hardware code generation\ndue to its superior coding performance and small model size.\nFor fine-tuning it on our dataset, the fine-tuning approach is\nbased on QLoRA [3], using its default training settings to\ndemonstrate our delivered dataset's effectiveness. The fine-\ntuned model is evaluated using 143 Verilog coding questions\nfrom the benchmark in [8], excluded from the training set.\nHardware evaluation and metrics. The validity of each\ngenerated design is tested by compiling it and checking against\nits RTL simulation results in pre-defined testbench cases.\nWe employ unbiased pass@1, pass@5, and pass@10 metrics,\ncalculated from 20 generation runs, as established in [8]."}, {"title": "B. Ablation Study on Different Evaluation Settings", "content": "In this section, we explore the performance of fine-tuned\nmodels using varying data formats in both the training and\nevaluation phases. Although high-level global summaries are\nthe most user-friendly data format, their ambiguity often\nresults in a lack of detailed information necessary for precise"}, {"title": "C. Ablations on the Number of Training Samples", "content": "We further examine how the quantity of training samples\naffects the performance of models fine-tuned for RTL code\ngeneration tasks. As illustrated in Fig. 3, there is a clear trend"}, {"title": "VI. RELATED WORK", "content": "LLMs have been applied in various stages of the hardware\ndesign process, including verification [13], security flaw de-\ntection [12], and code generation [2], [4], [8], [16]. However,\ntheir performance is still limited due to insufficient exposure\nto hardware data during pretraining [2], [4]. Some studies [8],\n[9], [16] have tried to rectify this by supplying more hardware\ncode samples and fine-tuning the LLMs. Yet, the datasets used\nare still either too small [9] or overly simplistic [8], [16],\nwhich hinder effective fine-tuning of LLMs. Our MG-Verilog\ndataset addresses this issue by providing an open-sourced,\nhigh-quality dataset, essential for optimizing LLM fine-tuning\nand in-context learning."}, {"title": "VII. CONCLUSION", "content": "In this work, we aim to mitigate the limitations of exist-\ning datasets for LLM-assisted hardware design by proposing\nthe open-sourced Multi-Grained-Verilog (MG-Verilog) dataset.\nThe MG-Verilog dataset features hardware descriptions at\ndifferent levels of detail and their corresponding Verilog code\nsamples for more generic use cases. We have demonstrated\nthe effectiveness of the dataset through a balanced fine-tuning\nscheme. Extensive experiments show that LLMs fine-tuned\nwith the MG-Verilog dataset outperform those trained on other\ndatasets in terms of Verilog code generation accuracy."}]}