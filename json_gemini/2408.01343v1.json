{"title": "StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal Semantic Segmentation", "authors": ["Bingyu Li", "Da Zhang", "Zhiyuan Zhao", "Junyu Gao", "Xuelong Li"], "abstract": "Multimodal semantic segmentation shows significant potential for enhancing segmentation accuracy in complex scenes. However, current methods often incorporate specialized feature fusion modules tailored to specific modalities, thereby restricting input flexibility and increasing the number of training parameters. To address these challenges, we propose StitchFusion, a straightforward yet effective modal fusion framework that integrates large-scale pre-trained models directly as encoders and feature fusers. This approach facilitates comprehensive multi-modal and multi-scale feature fusion, accommodating any visual modal inputs. Specifically, Our framework achieves modal integration during encoding by sharing multi-modal visual information. To enhance information exchange across modalities, we introduce a multi-directional adapter module (MultiAdapter) to enable cross-modal information transfer during encoding. By leveraging MultiAdapter to propagate multi-scale information across pre-trained encoders during the encoding process, StitchFusion achieves multi-modal visual information integration during encoding. Extensive comparative experiments demonstrate that our model achieves state-of-the-art performance on four multi-modal segmentation datasets with minimal additional parameters. Furthermore, the experimental integration of MultiAdapter with existing Feature Fusion Modules (FFMs) highlights their complementary nature. Our code is available at StitchFusion_repo.", "sections": [{"title": "Introduction", "content": "Semantic segmentation is a critical vision processing technique extensively applied in scene understanding, change detection, and autonomous driving (Chen et al. 2017). This technique enables systems to accurately recognize and interpret the surrounding environment by analyzing the semantic information of each image pixel (Chen et al. 2018). Given its importance and wide range of applications, numerous researchers have focused on semantic segmentation tasks and achieved significant results with models such as FCN (Long, Shelhamer, and Darrell 2015), and Deeplab (Chen et al. 2021). Despite the success of these models, most related research has primarily focused on single visual features (RGB), limiting their effectiveness in complex scenes and special environments, such as nighttime.\nAs application demands increase, processing a single modality is no longer sufficient to meet the requirements (He 2024). Consequently, the community is increasingly focusing on multimodal semantic segmentation(Joze et al. 2020). Unlike traditional segmentation tasks that rely solely on RGB modalities, multimodal semantic segmentation leverages the complementary features of multiple visual modalities. Each modality can provide unique information: RGB captures color and texture, TIR captures thermal properties useful in low-light conditions, and depth sensors provide spatial and structural information (Zhang et al. 2023a). By fusing these complementary modalities, segmentation models can achieve higher accuracy and robustness. This approach improves the limitations of a single modality in challenging scenes and enhances the ability to capture scene-specific features.\nGiven the complementary nature of multimodal information, achieving the effective fusion of multiple visual modalities is crucial. Previous researchers have designed numerous feature fusion modules (Zhang et al. 2023a; Fu et al. 2019), the framework can be summarized as Fig. 1(a). Essentially, these methods remap features extracted by encoders into a new subspace to achieve modality fusion (Zhou et al. 2023). Although these methods have achieved certain levels of effectiveness, they typically encounter the following two limitations: 1) the complexity of feature extraction and fusion modules introduces a large number of additional parameters, thereby increasing training costs; and 2) existing FFMs are designed for specific modalities, and thus, are not effective for arbitrary modality combinations.\nThese limitations prompt us to rethink the design and training of an effective multimodal fusion layer to address the aforementioned problems. Given the extensive generalizability and capability of pre-trained encoders to capture both global and local vision features, much-related work utilizes pre-trained models as feature extractors (Zhang et al. 2023a; Chen et al. 2017). However, the potential of these pre-trained models as feature fusion models for semantic segmentation remains underexplored.\nInspired by these considerations, we propose a new feature fusion paradigm (StitchFusion) that utilizes pre-trained models as a feature fusion layer to demonstrate potential benefits. To achieve this target, we employ a simple Multidirectional MLP layer, named MultiAdapter, which shares and synchronizes modality-specific multi-scale information throughout the encoding process. This MultiAdapter ensures effective feature fusion during encoding, as illustrated in Fig. 1(b). This method leverages the encoder's inherent multi-scale visual feature modeling capabilities, requiring fewer additional parameters for cross-modal fusion. Experimental results on 4 datasets confirm that this new fusion paradigm not only outperforms traditional feature fusion methods but also enhances segmentation efficacy when combined.\nOur contributions are summarized as follows:\n\u2022 We introduce a multimodal feature fusion framework called StitchFusion, which achieves cross-modal integration by enabling modality sharing during the encoding process.\n\u2022 We designed a multidirectional MLP layer called Multi-Adapter, which enables cross-modal information sharing. By this, our framework leverages the encoder's feature extraction capabilities to achieve effective feature fusion.\n\u2022 Experiments on various multimodal semantic segmentation datasets with the StitchFusion module for ViT-based encoder surpass previous state-of-the-art results. Comprehensive ablation studies suggest potential optimal placements and quantities for integrating the Stitch-Fusion module to maximize modal fusion efficacy.\n\u2022 The StitchFusion module and existing approaches based on additional FFMs can complement each other in terms of design and application, and we demonstrate their complementary nature through extensive experiments."}, {"title": "Related Work", "content": "Semantic Segmentation\nSemantic segmentation, a critical task in computer vision, has evolved significantly by developing various methods and models, particularly those leveraging convolutional neural networks (CNNs) and more recently, transformers (Xie et al. 2021). Early breakthroughs were achieved through fully convolutional networks (FCNs) (Long, Shelhamer, and Darrell 2015), which enabled end-to-end pixel-wise predictions. Subsequent architectures, such as SegNet (Badrinarayanan, Kendall, and Cipolla 2017) and U-Net (Ronneberger, Fischer, and Brox 2015), utilized encoder-decoder structures to capture both low-level and high-level features. The DeepLab series (Chen et al. 2017, 2018) introduced atrous convolutions and spatial pyramid pooling to enhance multi-scale context perception, while PSPNet (Zhao et al. 2017) aggregated context from different regions. Vision Transformer (ViT) models (Dosovitskiy et al. 2020) leveraged self-attention mechanisms to capture long-range dependencies, with subsequent adaptations like SETR (Zheng et al. 2021) and Swin Transformer (Liu et al. 2021) improving computational efficiency and scalability. While single-modality data has seen substantial progress, multimodal semantic segmentation, integrating data such as RGB with other vision modalities, has been increasingly explored. The existing literature has proposed numerous feature fusion approaches (Hazirbas et al. 2016). Building on these advancements, we propose the StitchFusion model which introduces a novel feature fusion paradigm (StitchFusion) using Multidirectional MLP layer (MultiAdapter) for effective multi-modal integration.\nVision Multimodal Fusion\nIn classical visual tasks, single visual modalities often struggle to handle challenges in complex environments adaptively (Fu et al. 2019; Zhang et al. 2023a). Consequently, an increasing number of researchers are turning to multiple visual modalities, making the fusion of these modalities crucial. Some researchers have used fine-tuned pre-trained models to fuse multiple visual modalities, but this approach can lead to catastrophic forgetting. To mitigate this problem, many researchers have opted to freeze the pre-trained models, as seen in the work (He 2024; Cao et al. 2024) proposing a prompt-based method for multimodal fusion. Another paradigm utilizes multi-scale information and designs various FFMs to integrate the multi-scale information from each"}, {"title": "Method", "content": "This section introduces a new multimodal feature fusion framework for semantic segmentation, named StitchFusion. Furthermore, we propose a multidirectional Adapter for cross-modal information sharing.\nFeature Extraction And Encoding\nUnlike traditional CNNs that often struggle to capture long-range dependencies within images, we chose SegFormer (Xie et al. 2021) as our frozen feature encoder in this paper. SegFormer is based on the Vision Transformer (ViT) architecture. Its self-attention layers provide a comprehensive view of the input data. Integrating a Feature Pyramid Network (FPN) within SegFormer's architecture further enhances its ability to discern multi-scale features, ensuring that fine-grained details and broader contextual features are effectively encoded. The model's end-to-end training approach eliminates the need for post-processing steps and allows for a seamless transition from raw image data to precise features, which makes it effective for our feature fusion.\nBased on SegFormer's ViT architecture, we devise the StitchFusion framework by viewing the ViT block as the encoder and feature fuser shown in Fig. 2.\nStitchFusion Framework\nWe employ the MultiAdapter Layer as an information exchanger for the ViT encoder to enable the frozen pre-trained model to function as a modality fuser. The specific data flow process is as follows.\nThe attention mechanism independently processes input feature map $x_i$ of i-th modality. This step leverages the self-attention mechanism $F_{Attn}$ to capture dependencies and relationships within the feature map $x_i$ of i-th modality:\n$z_{Attn} = x_i + DropPath(F_{Attn}(LN_1(x_i)))$.\nThe MultiAdapter $F_{Ada1}$ is then used to influence the feature maps between the i-th and the j-th modalities, which allows information to flow from one modality to another:\n$z^{Attn}_j = z_{Attn} + DropPath(F_{Ada1}(LN_1(x_i)))$, for $i \\neq j$. Subsequently, each input is independently processed through the $F_{MLP}$. This processing step involves non-linear transformations that further refine the features, preparing them for subsequent stages of the model:\n$z^{MLP} = z^{Attn} + DropPath(F_{MLP}(LN_2(z^{Attn})))$.\nThe MultiAdapter $F_{Ada2}$ is then applied again for the final cross-modal influence.\n$z^{MLP}_j = z^{MLP} + DropPath(F_{Ada2}(LN_2(z^{Attn})))$, for $i \\neq j$. This second application of the MultiAdapter ensures that the refined features from different modalities are effectively integrated.\nThe specific structure and the sequence of operations are illustrated in Fig. 2, which provides a visual representation of the StitchFusion framework.\nMultiAdapter\nwe design a simple MultiAdapter $F_{Ada}$ based on the linear module which performs the following operations during the information-sharing process: Downscaling, Processing and Upscaling.\nMathematically, these operations are defined as follows:\nGiven an input feature vector $x \\in R^d$, the downscaling transformation is defined as:\n$x_{down} = W_{down}x + b_{down}$, where $W_{down} \\in R^{d \\times r}$ is the weight matrix and $b_{down} \\in R^{d \\times r}$ is the bias vector."}, {"title": null, "content": "The downscaled features are then processed using a non-linear activation function (GELU) and dropout regularization:\n$x_{mid} = Dropout(GELU(W_{mid}x_{down} + b_{mid}))$, where $W_{mid} \\in R^{r \\times r}$ is the weight matrix and $b_{mid} \\in R^{r \\times r}$ is the bias vector.\nFinally, the processed features are upscaled back to the original dimension:\n$x_{up} = W_{up}x_{mid} + b_{up}$, where $W_{up} \\in R^{r \\times d}$ is the weight matrix and $b_{up} \\in R^{r \\times d}$ is the bias vector.\nMultiAdapter At Different Density Levels\nThis section extends the concept to support multiple modalities and different levels of connection density, providing a detailed analysis of various configurations and their implications for model performance.\nShared MultiAdapter for All Modalities. In this configuration (Fig. 3(a)), all modalities share the same MultiAdapter (named sMultiAdapter). This means that the same set of weights and biases are used for the transformations between any pair of modalities. This approach promotes consistency and reduces the overall number of parameters, making the model more efficient and easier to train. The transformation for any modality pair (i, j) uses the same weights and biases:\n$y_{ij} = W_{shared}X_i + b_{shared}$,\n$y_{ji} = W_{shared}X_j + b_{shared}$.\nUsing a shared set of weights, the sMultiAdapter ensures that the transformations are uniform across all modality pairs, which can be beneficial in scenarios where the modalities have similar feature distributions.\nIndependent MultiAdapter for Each Pair of Modalities. In this configuration (Fig. 3(b)), each pair of modalities has its own bi-directional MultiAdapters (named obMultiAdapter). For M modalities, there are $C_M^2$ MultiAdapters in total, where each pair of modalities (i, j) is assigned a unique set of weight matrices and biases. Let $x_i$ and $x_j$ be the feature vectors for modalities i and j respectively, the transformation is defined as:\n$y_{ij} = W_{i \\leftrightarrow j}X_i + b_{i \\leftrightarrow j}$,\n$y_{ji} = W_{i \\leftrightarrow j}X_j + b_{i \\leftrightarrow j}$.\nEach pair (i, j) has a unique set of weight matrices and biases. This configuration allows for more specialized transformations tailored to the specific characteristics of each modality pair. This is especially useful when the modalities have significantly different feature distributions, as it allows for more precise adaptations."}, {"title": null, "content": "Parallel MultiAdapters for Each Pair of Modalities. In this configuration (Fig. 3(c)), each pair of modalities shares two uni-directional MultiAdapter Modules (named tuMultiAdapter). This means that there are separate weight matrices and biases for the transformations in each direction between two modalities, allowing for asymmetric information exchange. The transformation is:\n$y_{ij} = W_{i \\rightarrow j}x_i + b_{i \\rightarrow j}$,\n$y_{ji} = W_{j \\rightarrow i}x_j + b_{j \\rightarrow i}$, where each pair (i, j) has a shared set of weight matrices and biases. This setup can capture directional dependencies and interactions more effectively, as it can learn distinct transformations for each direction.\nEquivalence of Configurations for Two Modalities. When the number of modalities M = 2, the configurations of a sMultiAdapter and an obMultiAdapter are equivalent. In both cases, the transformation involves a single set of weights and biases:\n$y_{1 \\rightarrow 2} = Wx_1 + b$,\n$y_{2 \\rightarrow 1} = Wx_2 + b$.\nThus, for M = 2:\n$W_{1 \\leftrightarrow 2} = W_{shared}, b_{1 \\leftrightarrow 2} = b_{shared}$.\nIn the remainder of the text, we will refer to the framework using sMultiAdapter as sStitchFusion, the framework using obMultiAdapter as obStitchFusion, and the framework using tuMultiAdapter as tuStitchFusion. Each of these frameworks offers distinct advantages depending on the number of modalities and the specific requirements of the application, providing flexible options for multimodal integration.\nExperimental Result\nExperimental Details\nWe test our model on 5 datasets. The FFMs in the paper is configed as the module from (Kaykobad and et al. 2023). We set the intermediate dimention for MultiAdatper tor = 8. the learning rate is 1.2 \u00d7 10-4 for the FMB dataset and 6 \u00d7 10-5 for others. Furthermore, a warm-up technology is implemented for the initial 10 epochs, followed by a learning rate decay factor of 0.01. For additional training details, please refer to the code implementation.\nDataSet\nMCubeS Dataset. (Liang et al. 2022) The MCubeS dataset includes RGB, Near-Infrared (NIR), Degree of Linear Polarization (DoLP), and Angle of Linear Polarization (AoLP) image pairs for semantic material segmentation across 20 categories. It consists of 302/96/102 image pairs for training/validation/testing, all sized at 1024x1024.\nFMB Dataset. (Liu et al. 2023) The FMB dataset is designed for image fusion and segmentation, containing 1,500 infrared and visible image pairs annotated with 15 pixel-level categories. The training set has 1,220 pairs, and the test set has 280 pairs."}, {"title": null, "content": "MFNet Dataset. (Ha et al. 2017) The MFNet dataset consists of 1,569 RGB-Thermal image pairs (640\u00d7480 pixels) with 8 classes. It includes 820 daytime and 749 nighttime image pairs.\nDeLiVER Dataset. (Zhang et al. 2023b) The DeLiVER dataset includes six orthogonal views with data from Depth, LiDAR, Views, Event, and RGB sensors (1024\u00d71024 pixels). DeLiVER contains 47,310 frames, with 7,885 annotated front-view samples.\nPST900 Dataset. (Shivakumar et al. 2020) The PST900 dataset contains 894 synchronized RGB-Thermal image pairs with per-pixel ground truth annotations for five classes, divided into training and test sets.\nExperimental Results on Datasets\nResults On McubeS Dataset. In the MCubeS dataset (Table 1, the StitchFusion models demonstrate superior performance across different modality combinations. The highest mIoU of 53.92% is achieved by StitchFusion+FFMs using RGB-A-D-N modalities. The models consistently outperform other methods, showcasing the effectiveness of the StitchFusion approach in integrating multiple visual modalities to enhance segmentation accuracy. The improvements are significant across various combinations such as RGB-N, RGB-D, and RGB-A, highlighting the robustness and adaptability of the proposed method."}, {"title": null, "content": "Results On DeLiVER Dataset. The comparative analysis of semantic segmentation methods on the DeLiVER dataset reveals that StitchFusion consistently outperforms its counterparts across various modalities (Table 2. With a notable 2.12% increase in mIoU over CMNeXt in RGB-Depth, and an impressive 1.88% lead in the comprehensive RGB-DEL setup, StitchFusion demonstrates its robust feature integration capabilities. Particularly striking is its 1.15% improvement in the challenging RGB-D-LiDAR modality, showcasing its adept handling of multimodality data. These results underscore StitchFusion's potential as a leading approach in the realm of advanced image segmentation.\nResults On FMB Dataset. For the FMB dataset (Table 3, the StitchFusion models exhibit exceptional performance, with the StitchFusion+FFMs achieving the highest mIoU of 64.32%. This substantial improvement over other models, including GMNet(Zhou et al. 2021) and MMSFormer(Kaykobad and et al. 2023). The significant increase in segmentation accuracy demonstrates the effectiveness of the proposed fusion techniques in handling diverse and challenging visual environments. It is evident that combining StitchFusion with FFMs results in a higher mIoU than using either method alone, highlighting the complementary nature of StitchFusion and FFMs.\nResults On MFNet Dataset. In the MFNet dataset (Table 3(c)), the StitchFusion model variants continue to excel, with StitchFusion+FFMs achieving the highest mIoU of 57.91%. This performance surpasses other prominent models such as BiSeNet and SegHRNet, reinforcing the model's generalizability and robustness in multimodal semantic segmentation tasks. The consistently high performance across multiple datasets indicates the versatility and effectiveness of the StitchFusion approach."}, {"title": "Parameter Number Efficiency", "content": "To highlight the efficiency of the StitchFusion module in terms of parameter number, we present the following formula for calculating the module's parameter number P:\n$P=\\sum_i\\left(\\left(2 r d_i + r^2\\right) \\cdot 2 \\cdot C_m^2 \\cdot \\sum_j\\left(depth_{i, j}\\right)\\right)$,\nwhere $d$: denotes the feature dimensions of input and output for the i-th stage, r is a hyperparameter that adjusts StitchFusion's downsampling dimensions (set to 8 in this paper), m represents the number of modalities, and $depth_{i,j}$ indicates the depth of the backbone network for the i-th stage. $C_m^2$ represents the number of combinations when choosing 2 encoders to perform information-sharing out of m distinct modalities."}, {"title": "Ablation Experiment", "content": "Comparison of different modal combinations. The Table 5 provides a mIoU comparison for various modality combinations on the MCubeS dataset. As the number of modalities increases, the model's segmentation performance gradually improves. The StitchFusion method showed a mIoU of 51.25% with the RGB-D modality, 52.08% with the RGB-A modality, and 52.52% with the RGB-A-D modality. Integrating StitchFusion with FFMs leads to even higher mIoU scores, with notable improvements across all tested modality groups. When StitchFusion was combined with FFMs, performance improved across all modality combinations.\nExploration of connection density for different Stitch-Fusion Table 6 show that well-timed StitchFusion configurations, particularly when combined with FFMs, significantly enhance performance. In the MCubeS dataset, StitchFusion+FFMs achieves the highest mIoU of 53.92%, outperforming simpler configurations like StitchFusion(shared) and StitchFusion(ADD). Similarly, in the FMB dataset, StitchFusion(TWO, after)+FFMs leads with a mIoU of 64.85%, demonstrating the importance of both density and timing in feature fusion for optimal multimodal semantic segmentation performance.\nStitchFusion is compatible with existing Feature Fusion Modules. The combination (Fig. 4 and Table 7) of StitchFusion and FFMs consistently improves performance across various experimental settings, demonstrating their complementary strengths in enhancing semantic segmentation. For"}, {"title": "Conclusion", "content": "This paper introduces StitchFusion, a novel approach designed to enhance feature fusion and alignment between multiple visual features for semantic segmentation. Our method progressively selects and fuses relevant features, improving model accuracy and robustness across various image recognition scenarios. Through extensive experiments, we demonstrate that StitchFusion outperforms existing methods, highlighting its capability to effectively handle complex image recognition challenges. However, the MultiAdapter is designed as a simple MLP architecture, which may not be efficient enough for fine-grained feature processing and sharing. In the future, the MultiAdapter requires a more suitable and fine-grained design."}, {"title": "Appendices", "content": "Supplementary Visualization of StitchFusion\nIn this appendix, we provide a comprehensive set of visualizations and analyses to elucidate the performance and capabilities of the StitchFusion method. Fig. 7 offers a detailed visualization of the segmentation results on the DeLiVER dataset, showcasing the method's precision in delineating boundaries and preserving fine details. To further underscore the method's feature extraction ability, Fig. 8 employs t-SNE to project the high-dimensional feature representations onto a two-dimensional plane. The distinct clusters in the t-SNE plots indicate the method's ability to capture and separate the nuances within the data. Fig. 10 extends the qualitative assessment of the Mcubes dataset by presenting additional segmentation results, thereby demonstrating the model's adaptability across varied data environments. Finally, Fig. 9 provides more visualizations of StitchFusion's segmentation outcomes on the DeLiVER dataset, offering a broader perspective on the model's performance. These additional images may include comparative analyses with other methods or highlight the method's efficacy in handling complex or challenging segments within the dataset.\nPer-class Comparision with State-of-the-art Models\nIn our analysis, we present a comparative study of segmentation performance across different categories and datasets, focusing on the DeLiVER dataset and various modality combinations shown on the left side of Fig. 11. Additionally, we compare the per-class mIOU performance on the Mcubes (Fig. 11 right), FBM (Fig. 8), and MFNet (Fig. 9) datasets. The results reveal distinct advantages and limitations of each method in handling diverse categories. For instance, while some methods excel in categories such as \"Sky\" and \"Truck,\" they may underperform in more challenging categories like \"Human\" and \"Plaster.\u201d Our proposed method, StitchFusion+FFMs, demonstrates balanced and robust performance across categories, achieving high mIOU scores in previously challenging areas. This comprehensive analysis not only highlights the strengths and weaknesses of each method but also provides valuable insights into their applicability across different datasets and categories, guiding further improvements in segmentation techniques."}]}