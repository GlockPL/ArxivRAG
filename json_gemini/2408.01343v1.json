{"title": "StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal Semantic Segmentation", "authors": ["Bingyu Li", "Da Zhang", "Zhiyuan Zhao", "Junyu Gao", "Xuelong Li"], "abstract": "Multimodal semantic segmentation shows significant potential for enhancing segmentation accuracy in complex scenes. However, current methods often incorporate specialized feature fusion modules tailored to specific modalities, thereby restricting input flexibility and increasing the number of training parameters. To address these challenges, we propose StitchFusion, a straightforward yet effective modal fusion framework that integrates large-scale pre-trained models directly as encoders and feature fusers. This approach facilitates comprehensive multi-modal and multi-scale feature fusion, accommodating any visual modal inputs. Specifically, Our framework achieves modal integration during encoding by sharing multi-modal visual information. To enhance information exchange across modalities, we introduce a multi-directional adapter module (MultiAdapter) to enable cross-modal information transfer during encoding. By leveraging MultiAdapter to propagate multi-scale information across pre-trained encoders during the encoding process, StitchFusion achieves multi-modal visual information integration during encoding. Extensive comparative experiments demonstrate that our model achieves state-of-the-art performance on four multi-modal segmentation datasets with minimal additional parameters. Furthermore, the experimental integration of MultiAdapter with existing Feature Fusion Modules (FFMs) highlights their complementary nature. Our code is available at StitchFusion_repo.", "sections": [{"title": "Introduction", "content": "Semantic segmentation is a critical vision processing technique extensively applied in scene understanding, change detection, and autonomous driving (Chen et al. 2017). This technique enables systems to accurately recognize and interpret the surrounding environment by analyzing the semantic information of each image pixel (Chen et al. 2018). Given its importance and wide range of applications, numerous researchers have focused on semantic segmentation tasks and achieved significant results with models such as FCN (Long, Shelhamer, and Darrell 2015), and Deeplab (Chen et al. 2021). Despite the success of these models, most related research has primarily focused on single visual features (RGB), limiting their effectiveness in complex scenes and special environments, such as nighttime.\nAs application demands increase, processing a single modality is no longer sufficient to meet the requirements (He 2024). Consequently, the community is increasingly focusing on multimodal semantic segmentation(Joze et al. 2020). Unlike traditional segmentation tasks that rely solely on RGB modalities, multimodal semantic segmentation leverages the complementary features of multiple visual modalities. Each modality can provide unique information: RGB captures color and texture, TIR captures thermal properties useful in low-light conditions, and depth sensors provide spatial and structural information (Zhang et al. 2023a). By fusing these complementary modalities, segmentation models can achieve higher accuracy and robustness. This approach improves the limitations of a single modality in challenging scenes and enhances the ability to capture scene-specific features.\nGiven the complementary nature of multimodal information, achieving the effective fusion of multiple visual modalities is crucial. Previous researchers have designed numerous feature fusion modules (Zhang et al. 2023a; Fu et al. 2019), the framework can be summarized as Fig. 1(a). Essentially, these methods remap features extracted by encoders into a new subspace to achieve modality fusion (Zhou et al. 2023). Although these methods have achieved certain levels of effectiveness, they typically encounter the following two limitations: 1) the complexity of feature extraction and fusion modules introduces a large number of additional parameters, thereby increasing training costs; and 2) existing FFMs are designed for specific modalities, and thus, are not effective for arbitrary modality combinations.\nThese limitations prompt us to rethink the design and training of an effective multimodal fusion layer to address the aforementioned problems. Given the extensive generalizability and capability of pre-trained encoders to capture both global and local vision features, much-related work utilizes pre-trained models as feature extractors (Zhang et al. 2023a; Chen et al. 2017). However, the potential of these pre-trained models as feature fusion models for semantic segmentation remains underexplored.\nInspired by these considerations, we propose a new feature fusion paradigm (StitchFusion) that utilizes pre-trained models as a feature fusion layer to demonstrate potential benefits. To achieve this target, we employ a simple Multidirectional MLP layer, named MultiAdapter, which shares and synchronizes modality-specific multi-scale information throughout the encoding process. This MultiAdapter ensures effective feature fusion during encoding, as illustrated in Fig. 1(b). This method leverages the encoder's inherent multi-scale visual feature modeling capabilities, requiring fewer additional parameters for cross-modal fusion. Experimental results on 4 datasets confirm that this new fusion paradigm not only outperforms traditional feature fusion methods but also enhances segmentation efficacy when combined.\nOur contributions are summarized as follows:\n\u2022 We introduce a multimodal feature fusion framework called StitchFusion, which achieves cross-modal integration by enabling modality sharing during the encoding process.\n\u2022 We designed a multidirectional MLP layer called MultiAdapter, which enables cross-modal information sharing. By this, our framework leverages the encoder's feature extraction capabilities to achieve effective feature fusion.\n\u2022 Experiments on various multimodal semantic segmentation datasets with the StitchFusion module for ViT-based encoder surpass previous state-of-the-art results. Comprehensive ablation studies suggest potential optimal placements and quantities for integrating the Stitch-Fusion module to maximize modal fusion efficacy.\n\u2022 The StitchFusion module and existing approaches based on additional FFMs can complement each other in terms of design and application, and we demonstrate their complementary nature through extensive experiments."}, {"title": "Related Work", "content": "Semantic Segmentation\nSemantic segmentation, a critical task in computer vision, has evolved significantly by developing various methods and models, particularly those leveraging convolutional neural networks (CNNs) and more recently, transformers (Xie et al. 2021). Early breakthroughs were achieved through fully convolutional networks (FCNs) (Long, Shelhamer, and Darrell 2015), which enabled end-to-end pixel-wise predictions. Subsequent architectures, such as SegNet (Badrinarayanan, Kendall, and Cipolla 2017) and U-Net (Ronneberger, Fischer, and Brox 2015), utilized encoder-decoder structures to capture both low-level and high-level features. The DeepLab series (Chen et al. 2017, 2018) introduced atrous convolutions and spatial pyramid pooling to enhance multi-scale context perception, while PSPNet (Zhao et al. 2017) aggregated context from different regions. Vision Transformer (ViT) models (Dosovitskiy et al. 2020) leveraged self-attention mechanisms to capture long-range dependencies, with subsequent adaptations like SETR (Zheng et al. 2021) and Swin Transformer (Liu et al. 2021) improving computational efficiency and scalability. While single-modality data has seen substantial progress, multimodal semantic segmentation, integrating data such as RGB with other vision modalities, has been increasingly explored. The existing literature has proposed numerous feature fusion approaches (Hazirbas et al. 2016). Building on these advancements, we propose the StitchFusion model which introduces a novel feature fusion paradigm (StitchFusion) using Multidirectional MLP layer (MultiAdapter) for effective multi-modal integration.\nVision Multimodal Fusion\nIn classical visual tasks, single visual modalities often struggle to handle challenges in complex environments adaptively (Fu et al. 2019; Zhang et al. 2023a). Consequently, an increasing number of researchers are turning to multiple visual modalities, making the fusion of these modalities crucial. Some researchers have used fine-tuned pre-trained models to fuse multiple visual modalities, but this approach can lead to catastrophic forgetting. To mitigate this problem, many researchers have opted to freeze the pre-trained models, as seen in the work (He 2024; Cao et al. 2024) proposing a prompt-based method for multimodal fusion. Another paradigm utilizes multi-scale information and designs various FFMs to integrate the multi-scale information from each modality (Zhang et al. 2023b; Dong et al. 2023; Huang et al. 2019). Although existing FFMs have achieved notable experimental results, they introduce excessive additional parameters and are often limited in the number of modality data they can handle. Therefore, in this paper, we introduce StitchFusion, a simple but effective feature fusion framework incorporating the plug-and-play MultiAdapter module, which enables modality sharing with fewer parameters, thereby achieving feature fusion during the encoding process."}, {"title": "Method", "content": "This section introduces a new multimodal feature fusion framework for semantic segmentation, named StitchFusion. Furthermore, we propose a multidirectional Adapter for cross-modal information sharing.\nFeature Extraction And Encoding\nUnlike traditional CNNs that often struggle to capture long-range dependencies within images, we chose SegFormer (Xie et al. 2021) as our frozen feature encoder in this paper. SegFormer is based on the Vision Transformer (ViT) architecture. Its self-attention layers provide a comprehensive view of the input data. Integrating a Feature Pyramid Network (FPN) within SegFormer's architecture further enhances its ability to discern multi-scale features, ensuring that fine-grained details and broader contextual features are effectively encoded. The model's end-to-end training approach eliminates the need for post-processing steps and allows for a seamless transition from raw image data to precise features, which makes it effective for our feature fusion.\nBased on SegFormer's ViT architecture, we devise the StitchFusion framework by viewing the ViT block as the encoder and feature fuser shown in Fig. 2.\nStitchFusion Framework\nWe employ the MultiAdapter Layer as an information exchanger for the ViT encoder to enable the frozen pre-trained model to function as a modality fuser. The specific data flow process is as follows.\nThe attention mechanism independently processes input feature map xi of i-th modality. This step leverages the self-attention mechanism FAttn to capture dependencies and relationships within the feature map x\u2081 of i-th modality:\n$$z^{Attn}_i = x_i + DropPath(F_{Attn}(LN_1(x_i))).$$\nThe MultiAdapter FAda1 is then used to influence the feature maps between the i-th and the j-th modalities, which allows information to flow from one modality to another:\n$$z^{Attn}_{ij} = z^{Attn}_i + DropPath(F_{Ada1} (LN_1(x_i))),$$\nfor i \u2260 j. Subsequently, each input is independently processed through the FMLP. This processing step involves non-linear transformations that further refine the features, preparing them for subsequent stages of the model:\n$$z^{MLP}_i = z^{Attn}_i + DropPath(F_{MLP} (LN_2(z^{Attn}_i))).$$\nThe MultiAdapter FAda2 is then applied again for the final cross-modal influence.\n$$z^{MLP}_{ij} = z^{MLP}_i + DropPath(F_{Ada2} (LN_2(z^{Attn}_i))),$$\nfor i \u2260 j. This second application of the MultiAdapter ensures that the refined features from different modalities are effectively integrated.\nThe specific structure and the sequence of operations are illustrated in Fig. 2, which provides a visual representation of the StitchFusion framework.\nMultiAdapter\nwe design a simple MultiAdapter FAda based on the linear module which performs the following operations during the information-sharing process: Downscaling, Processing and Upscaling.\nMathematically, these operations are defined as follows:\nGiven an input feature vector x \u2208 Rd, the downscaling transformation is defined as:\n$$X_{down} = W_{down}x + b_{down},$$\nwhere Wdown \u2208 Rdxr is the weight matrix and bdown \u2208 Rdxr is the bias vector."}, {"title": "MultiAdapter At Different Density Levels", "content": "The downscaled features are then processed using a non-linear activation function (GELU) and dropout regularization:\n$$X_{mid} = Dropout(GELU(W_{mid}x_{down} + b_{mid})),$$\nwhere Wmid \u2208 Rrxr is the weight matrix and bmid \u2208 Rrxr is the bias vector.\nFinally, the processed features are upscaled back to the original dimension:\n$$X_{up} = W_{up}X_{mid} + b_{up},$$\nwhere Wup \u2208 Rr\u00d7d is the weight matrix and bup \u2208 Rr\u00d7d is the bias vector.\nMultiAdapter At Different Density Levels\nThis section extends the concept to support multiple modalities and different levels of connection density, providing a detailed analysis of various configurations and their implications for model performance.\nShared MultiAdapter for All Modalities. In this configuration (Fig. 3(a)), all modalities share the same MultiAdapter (named sMultiAdapter). This means that the same set of weights and biases are used for the transformations between any pair of modalities. This approach promotes consistency and reduces the overall number of parameters, making the model more efficient and easier to train. The transformation for any modality pair (i, j) uses the same weights and biases:\n$$y_{ij} = W_{shared}X_i + b_{shared},$$\n$$y_{ji} = W_{shared}X_j + b_{shared}.$$\nUsing a shared set of weights, the sMultiAdapter ensures that the transformations are uniform across all modality pairs, which can be beneficial in scenarios where the modalities have similar feature distributions.\nIndependent MultiAdapter for Each Pair of Modalities. In this configuration (Fig. 3(b)), each pair of modalities has its own bi-directional MultiAdapters (named obMultiAdapter). For M modalities, there are $C_M^2$ MultiAdapters in total, where each pair of modalities (i, j) is assigned a unique set of weight matrices and biases. Let xi and xj be the feature vectors for modalities i and j respectively, the transformation is defined as:\n$$y_{ij} = W_{i \\leftrightarrow j}X_i + b_{i \\leftrightarrow j},$$\n$$y_{ji} = W_{i \\leftrightarrow j}X_j + b_{i \\leftrightarrow j}.$$\nEach pair (i, j) has a unique set of weight matrices and biases. This configuration allows for more specialized transformations tailored to the specific characteristics of each modality pair. This is especially useful when the modalities have significantly different feature distributions, as it allows for more precise adaptations.\nParallel MultiAdapters for Each Pair of Modalities. In this configuration (Fig. 3(c)), each pair of modalities shares two uni-directional MultiAdapter Modules (named tuMultiAdapter). This means that there are separate weight matrices and biases for the transformations in each direction between two modalities, allowing for asymmetric information exchange. The transformation is:\n$$y_{ij} = W_{i \\rightarrow j}x_i + b_{i \\rightarrow j},$$\n$$y_{ji} = W_{j \\rightarrow i}x_j + b_{j \\rightarrow i},$$\nwhere each pair (i, j) has a shared set of weight matrices and biases. This setup can capture directional dependencies and interactions more effectively, as it can learn distinct transformations for each direction.\nEquivalence of Configurations for Two Modalities. When the number of modalities M = 2, the configurations of a sMultiAdapter and an obMultiAdapter are equivalent. In both cases, the transformation involves a single set of weights and biases:\n$$y_{1\\rightarrow 2} = Wx_1 + b,$$\n$$y_{2\\rightarrow 1} = Wx_2 + b.$$\nThus, for M = 2:\n$$W_{1+2} = W_{shared}, b_{1+2} = b_{shared}.$$\nIn the remainder of the text, we will refer to the framework using sMultiAdapter as sStitchFusion, the framework using obMultiAdapter as obStitchFusion, and the framework using tuMultiAdapter as tuStitchFusion. Each of these frameworks offers distinct advantages depending on the number of modalities and the specific requirements of the application, providing flexible options for multimodal integration."}, {"title": "Experimental Result", "content": "Experimental Details\nWe test our model on 5 datasets. The FFMs in the paper is configed as the module from (Kaykobad and et al. 2023). We set the intermediate dimention for MultiAdatper tor = 8. the learning rate is 1.2 \u00d7 10-4 for the FMB dataset and 6 \u00d7 10-5 for others. Furthermore, a warm-up technology is implemented for the initial 10 epochs, followed by a learning rate decay factor of 0.01. For additional training details, please refer to the code implementation."}, {"title": "Parameter Number Efficiency", "content": "To highlight the efficiency of the StitchFusion module in terms of parameter number, we present the following formula for calculating the module's parameter number P:\n$$P = \\sum_{i=1}^{D} ((2rd_i + r^2) \\cdot 2 \\cdot C_m^2) + \\sum_{j=1}^S (depth_{i,j})),$$,\nwhere di denotes the feature dimensions of input and output for the i-th stage, r is a hyperparameter that adjusts StitchFusion's downsampling dimensions (set to 8 in this paper), m represents the number of modalities, and depthi,j indicates the depth of the backbone network for the i-th stage. $C_m^2$ represents the number of combinations when choosing 2 encoders to perform information-sharing out of m distinct modalities.\nTable 4 demonstrates that StitchFusion achieves significant accuracy improvements with minimal increases in parameters compared to state-of-the-art methods on the DeLiVER and Mcubes datasets. For the DeLiVER dataset, StitchFusion models show accuracy gains ranging from 8.55% to 10.83% with only a 0.14M to 0.71M increase in parameters. For instance, StitchFusion-RGB-DEL achieves a 10.83 accuracy improvement with just a 2.75% increase in parameters. Similarly, on the Mcubes dataset, accuracy improvements range from 0.81% to 1.38%, while parameter increases are between 0.40M and 2.37M."}, {"title": "Ablation Experiment", "content": "Comparison of different modal combinations. The Table 5 provides a mIoU comparison for various modality combinations on the MCubeS dataset. As the number of modalities increases, the model's segmentation performance gradually improves. The StitchFusion method showed a mIoU of 51.25% with the RGB-D modality, 52.08% with the RGB-A modality, and 52.52% with the RGB-A-D modality. Integrating StitchFusion with FFMs leads to even higher mIoU scores, with notable improvements across all tested modality groups. When StitchFusion was combined with FFMs, performance improved across all modality combinations.\nStitchFusion is compatible with existing Feature Fusion Modules. The combination (Fig. 4 and Table 7) of StitchFusion and FFMs consistently improves performance across various experimental settings, demonstrating their complementary strengths in enhancing semantic segmentation. For instance, on the FMB dataset, the integration of tuStitchFusion* with FFMs achieves a mIoU score of 64.85%, compared to 62.64% with tuStitchFusion* alone. Similarly, on the MFNet dataset, the combination of StitchFusion and FFMs results in a mIoU of 57.91%, slightly higher than the 57.8% from using StitchFusion alone. These specific improvements illustrate that the combined approach leverages the unique advantages of both methods to effectively handle multimodal information."}, {"title": "Conclusion", "content": "This paper introduces StitchFusion, a novel approach designed to enhance feature fusion and alignment between multiple visual features for semantic segmentation. Our method progressively selects and fuses relevant features, improving model accuracy and robustness across various image recognition scenarios. Through extensive experiments, we demonstrate that StitchFusion outperforms existing methods, highlighting its capability to effectively handle complex image recognition challenges. However, the MultiAdapter is designed as a simple MLP architecture, which may not be efficient enough for fine-grained feature processing and sharing. In the future, the MultiAdapter requires a more suitable and fine-grained design."}, {"title": "Appendices", "content": "Supplementary Visualization of StitchFusion\nIn this appendix, we provide a comprehensive set of visualizations and analyses to elucidate the performance and capabilities of the StitchFusion method. Fig. 7 offers a detailed visualization of the segmentation results on the DeLiVER dataset, showcasing the method's precision in delineating boundaries and preserving fine details. To further underscore the method's feature extraction ability, Fig. 8 employs t-SNE to project the high-dimensional feature representations onto a two-dimensional plane. The distinct clusters in the t-SNE plots indicate the method's ability to capture and separate the nuances within the data. Fig. 10 extends the qualitative assessment of the Mcubes dataset by presenting additional segmentation results, thereby demonstrating the model's adaptability across varied data environments. Finally, Fig. 9 provides more visualizations of StitchFusion's segmentation outcomes on the DeLiVER dataset, offering a broader perspective on the model's performance. These additional images may include comparative analyses with other methods or highlight the method's efficacy in handling complex or challenging segments within the dataset.\nPer-class Comparision with State-of-the-art Models\nIn our analysis, we present a comparative study of segmentation performance across different categories and datasets, focusing on the DeLiVER dataset and various modality combinations shown on the left side of Fig. 11. Additionally, we compare the per-class mIOU performance on the Mcubes (Fig. 11 right), FBM (Fig. 8), and MFNet (Fig. 9) datasets. The results reveal distinct advantages and limitations of each method in handling diverse categories. For instance, while some methods excel in categories such as \"Sky\" and \"Truck,\" they may underperform in more challenging categories like \"Human\" and \"Plaster.\u201d Our proposed method, StitchFusion+FFMs, demonstrates balanced and robust performance across categories, achieving high mIOU scores in previously challenging areas. This comprehensive analysis not only highlights the strengths and weaknesses of each method but also provides valuable insights into their applicability across different datasets and categories, guiding further improvements in segmentation techniques."}]}