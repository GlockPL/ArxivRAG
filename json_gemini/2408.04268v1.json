{"title": "Evaluating Modern Approaches in 3D Scene Reconstruction: NeRF vs Gaussian-Based Methods", "authors": ["Yiming Zhou", "Zixuan Zeng", "Andi Chen", "Xiaofan Zhou", "Haowei Ni", "Shiyao Zhang", "Panfeng Li", "Liangxi Liu", "Mengyao Zheng", "Xupeng Chen"], "abstract": "Exploring the capabilities of Neural Radiance Fields (NeRF) and Gaussian-based methods in the context of 3D scene reconstruction, this study contrasts these modern approaches with traditional Simultaneous Localization and Mapping (SLAM) systems. Utilizing datasets such as Replica and ScanNet, we assess performance based on tracking accuracy, mapping fidelity, and view synthesis. Findings reveal that NeRF excels in view synthesis, offering unique capabilities in generating new perspectives from existing data, albeit at slower processing speeds. Conversely, Gaussian-based methods provide rapid processing and significant expressiveness but lack comprehensive scene completion. Enhanced by global optimization and loop closure techniques, newer methods like NICE-SLAM and SplaTAM not only surpass older frameworks such as ORB-SLAM2 in terms of robustness but also demonstrate superior performance in dynamic and complex environments. This comparative analysis bridges theoretical research with practical implications, shedding light on future developments in robust 3D scene reconstruction across various real-world applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advancements in Simultaneous Localization and Mapping (SLAM) have significantly leveraged deep learning to enhance the robustness and accuracy of 3D reconstruction and camera tracking. The use of neural implicit representations, as demonstrated by NICE-SLAM [1], allows for detailed reconstructions of large-scale indoor scenes through a hierarchical grid-based encoding that efficiently manages local updates and optimizes scene representations within viewing frustums [1-8]. Additionally, the comprehensive evaluation in [9] shows that the proposed method outperforms other state-of-the-art approaches, providing a novel solution for automated GI tract segmentation through the integration of specialized architectures.\nAnother critical advancement in SLAM is the introduction of global optimization techniques. GO-SLAM [10], for example, integrates loop closure and full bundle adjustment in real-time, addressing limitations of previous works that perform these optimizations post-process [10, 11]. The continual integration of deep learning into SLAM processes not only enhances the adaptability of these systems but also significantly reduces the latency in processing complex datasets.\nDespite these innovations, 3D reconstruction remains a complex challenge, with existing algorithms often excelling in one aspect while compromising in others. This paper focuses on comparing two state-of-the-art algorithms in the context of 3D reconstruction. We analyze Neural Radiance Fields (NeRF) and Gaussian-based methods, highlighting their respective advantages and disadvantages. NeRF, though slower, enables novel view synthesis, offering unprecedented capabilities in generating new perspectives from existing data. In contrast, Gaussian-based methods are noted for their speed and expressiveness but fall short in tasks like scene completion, which are crucial for comprehensive environmental understanding."}, {"title": "II. METHODS", "content": "A. Data Collection\n1) Replica [13]: The Replica dataset features 18 photo-realistic 3D indoor scenes, encompassing a diverse array of environments such as offices, hotel rooms, and apartments. These scenes are richly detailed, showcasing dense meshes, high dynamic range (HDR) textures, semantic layers, and reflective properties. Created using a custom-built RGB-D capture rig that synchronizes IMU, RGB, and IR data, this dataset excels in providing comprehensive spatial and textural data, ideal for testing the nuanced rendering capabilities of NeRF-based models.\n2) ScanNet [14]: ScanNet encompasses over 2.5 million views from 1,513 3D scans of varied indoor locations, equipped with detailed annotations. The dataset uses structural sensors connected to handheld devices such as iPads, capturing extensive environmental data. The offline processing phase of ScanNet enhances these captures into comprehensive 3D scene reconstructions that include precise 6-DoF camera poses and semantic labeling.\nB. Model Training\n1) NeRF Based:\na) NICE-SLAM: NICE-SLAM [1] employs a structured approach to 3D scene reconstruction, leveraging multi-level voxel grids to enhance detail capture and scalability. This hierarchical grid structure is pivotal in addressing common issues such as the over-smoothing of scene features. By updating only the grid features that are visible, NICE-SLAM significantly improves optimization precision and operational efficiency, contrasting sharply with methods like iMAP which rely on global updates and may suffer from inefficiency and error propagation. The algorithm typically utilizes a set of predefined parameters for constructing the voxel grids [15, 16]. These include setting the grid resolutions at different hierarchy levels to balance detail and computational efficiency. For instance, coarse grids capture basic structural outlines, while finer grids focus on detailed textures and objects.\nHowever, NICE-SLAM is not without its drawbacks. Its predictive performance is bounded by the resolution of the coarsest grid, which may limit its applicability in scenarios requiring high precision over large scales [17, 18].\nb) Point-SLAM: Point-SLAM [19] introduces a dynamic neural point cloud approach that adapts density based on the detailed requirements driven by the data, significantly enhancing memory efficiency. By utilizing per-pixel image gradients, the model smartly modulates point density, concentrating on areas requiring more detail and simplifying regions with less complexity. As exploration progresses, Point-SLAM expands the point cloud, optimizing spatial usage by focusing point density and compressing points in less detailed regions, thereby maintaining computational efficiency in real-time scene reconstruction [20-30].\nIn Point-SLAM, the model's parameterization is crucial for fine-tuning its dynamic neural point cloud. Typical parameters include the resolution of the point cloud, which adjusts dynamically to match the density required by the scene complexity. The model also leverages parameters related to the gradient thresholds that control point density based on per-pixel image gradients [31]. Additionally, parameters for space optimization ensure efficient memory use by managing how points are distributed and compressed in areas of lesser detail. These parameters are key to maintaining the balance between detail fidelity and computational efficiency in real-time applications.\n2) Gaussian Based:\na) SplaTAM: SplaTAM [32] is a robust framework for dense RGB-D SLAM that utilizes advanced splatting techniques to achieve efficient tracking and mapping in 3D environments. SplaTAM leverages the representation of the environment using 3D Gaussian splats, which provide a continuous and smooth density function for effective integration of sensory data, enhancing the quality of mapping in complex scenes.\nThe first step in SplaTAM involves data acquisition from RGB-D cameras, where both color and depth information are collected. The depth data is filtered and aligned with the RGB frames using methods from [33], ensuring consistency in representation. Subsequently, the algorithm constructs 3D Gaussians, known as \"splats,\" from the incoming depth data. Each splat is represented by its mean, covariance, and color, enabling a rich representation of the scene's geometry and appearance [34]. For tracking, SplaTAM employs an optimized keyframe selection technique, enabling significant computational savings while maintaining accurate localization. The"}, {"title": "III. EVALUATION", "content": "A. Evaluation Metrics\nIn the context of 3D reconstruction, the performance is rigorously evaluated across three primary domains: tracking, mapping, and view synthesis. Tracking assesses the system's ability to accurately follow the trajectory of the camera through space, crucial for maintaining alignment with the 3D model. Mapping focuses on the accuracy and completeness of the 3D structure generated from the captured data, emphasizing the need for precise and comprehensive environmental details. View synthesis, on the other hand, examines the system's capability to create new viewpoints from the reconstructed 3D model, testing the model's utility for applications like virtual reality, where users may explore the environment from angles not originally captured by the data. Each domain leverages specific metrics tailored to assess distinct aspects of the reconstruction process, ensuring a holistic evaluation of the system's performance. Furthermore, leveraging Zhang's method [9], our proposal incorporates adaptive learning techniques to further enhance the data analysis process, aiming to optimize outcomes in future planning.\n1) Tracking:\n\u2022 Absolute Trajectory Error (ATE): Measures the discrepancy between the estimated camera trajectory and the ground truth trajectory. This metric is essential for assessing the precision with which the SLAM system can track the movement through the environment.\n2) Mapping:\n\u2022 Precision: Indicates the accuracy of the points that are positively identified as part of the structure. It assesses how many reconstructed points are true positives.\n\u2022 Recall: Measures the algorithm's ability to identify all relevant instances of the structure in the scene. It evaluates the completeness of the reconstruction.\n\u2022 Depth L1 Error: Calculates the absolute difference between the predicted depth values and the actual depth values in the ground truth. This metric is crucial for understanding the depth accuracy of the model.\n3) View Synthesis:\n\u2022 Peak Signal to Noise Ratio (PSNR): Evaluates the quality of reconstructed views by comparing the synthesized images against the original views, focusing on the fidelity and clarity of the visual outputs.\nB. Comparative Analysis\nThis section evaluates and compares the performance of four state-of-the-art 3D scene reconstruction algorithms: NICE-SLAM, Point-SLAM, SplaTAM, and Gaussian Splatting SLAM, utilizing two main performance metrics: rendering quality and tracking accuracy. The data from these evaluations are derived from experiments conducted on the Replica dataset.\n1) Rendering Performance: Rendering performance is quantified through three metrics: PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity Index), and LPIPS (Learned Perceptual Image Patch Similarity). Higher values in PSNR and SSIM indicate better image reconstruction quality, while a lower LPIPS suggests a closer perceptual resemblance to the ground truth.\n\u2022 NICE-SLAM shows moderate performance with average PSNR, SSIM, and LPIPS values of 24.42 dB, 0.81, and 0.23 respectively. It performs particularly well in office scenarios but struggles in highly textured environments, indicating a potential limitation in handling complex textures or lighting conditions.\n\u2022 Point-SLAM exhibits superior rendering quality with the highest average PSNR and SSIM scores of 35.17 dB and 0.98, respectively, and a low LPIPS of 0.14. Its performance highlights its robustness in both detailed and varied environmental conditions.\n\u2022 SplaTAM also demonstrates strong rendering capabilities, with a comparable PSNR to Point-SLAM at 34.11 dB and a slightly lower SSIM of 0.97. Its average LPIPS of 0.10 signifies a high perceptual quality, making it effective in diverse settings.\n\u2022 Gaussian Splatting SLAM leads in PSNR at 37.50 dB and shows a good balance in SSIM (0.96) and LPIPS (0.07), reflecting its efficiency in creating high-fidelity reconstructions across different scenes.\n2) Tracking Performance: Tracking accuracy is evaluated using the ATE RMSE metric, where lower values indicate more precise trajectory tracking.\n\u2022 NICE-SLAM records an average ATE RMSE of 1.06 cm, showing reliable tracking but with some inconsistencies across different environments.\n\u2022 Point-SLAM, with an average ATE RMSE of 0.52 cm, demonstrates exceptional tracking precision, outperforming other models particularly in complex indoor environments."}, {"title": "IV. CONCLUSIONS", "content": "This study provides a comprehensive evaluation of four leading 3D scene reconstruction algorithms: NICE-SLAM, Point-SLAM, SplaTAM, and Gaussian Splatting SLAM, using the detailed and varied Replica dataset. The findings demonstrate that while each algorithm exhibits unique strengths, Point-SLAM and SplaTAM generally outperform in terms of rendering and tracking accuracy. These algorithms provide robust solutions for precise 3D scene reconstruction, demonstrating their applicability across various real-world environments and challenges. Despite the thorough analysis presented, this study acknowledges several limitations. The computational efficiency and real-time processing capabilities of the algorithms were not exhaustively quantified, which are crucial factors for applications in robotics and augmented reality. The study also did not account for the variability in hardware performance, which can significantly influence the effectiveness of each algorithm. Future research should aim to address the noted limitations by expanding the evaluation framework to include outdoor environments and more dynamic scenes. By continuing to refine these algorithms, future studies can pave the way for broader applications and improvements in both existing and emerging fields."}]}