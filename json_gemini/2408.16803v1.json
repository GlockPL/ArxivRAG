{"title": "HLogformer: A Hierarchical Transformer for Representing Log Data", "authors": ["Zhichao Hou", "Mina Ghashami", "Mikhail Kuznetsov", "MohamadAli Torkamani"], "abstract": "Transformers have gained widespread acclaim for their versatility in handling diverse data structures, yet their application to log data remains underexplored. Log data, characterized by its hierarchical, dictionary-like structure, poses unique challenges when processed using conventional transformer models. Traditional methods often rely on manually crafted templates for parsing logs, a process that is labor-intensive and lacks generalizability. Additionally, the linear treatment of log sequences by standard transformers neglects the rich, nested relationships within log entries, leading to suboptimal representations and excessive memory usage. To address these issues, we introduce HLogformer, a novel hierarchical transformer framework specifically designed for log data. HLogformer leverages the hierarchical structure of log entries to significantly reduce memory costs and enhance representation learning. Unlike traditional models that treat log data as flat sequences, our framework processes log entries in a manner that respects their inherent hierarchical organization. This approach ensures comprehensive encoding of both fine-grained details and broader contextual relationships. Our contributions are threefold: First, HLogformer is the first framework to design a dynamic hierarchical transformer tailored for dictionary-like log data. Second, it dramatically reduces memory costs associated with processing extensive log sequences. Third, comprehensive experiments demonstrate that HLogformer more effectively encodes hierarchical contextual information, proving to be highly effective for downstream tasks such as synthetic anomaly detection and product recommendation.", "sections": [{"title": "1. Introduction", "content": "In recent years, transformers have garnered significant attention due to their versatility in handling various data structures, including images, text, graphs, tabular data, and temporal graphs (Vaswani et al. 2017; Dosovitskiy et al. 2020; Veli\u010dkovi\u0107 et al. 2017; Huang et al. 2020; Wu et al. 2024; Hou et al. 2024b). Despite their widespread application, there remains a notable gap in research focused on log data. Log data inherently possesses a hierarchical, dictionary-like structure, where each log entry is composed of nested fields and attributes. For instance, a single log entry might include metadata like timestamps, user IDs, and event types at the top level, while containing nested details such as specific actions taken, resources affected, and additional contextual information. Examples of log data include Amazon EC2 logs, IAM logs, and web server access logs.\nTraditional methods for processing log data often involve manually applying templates to parse the logs before utilizing existing transformers. These templates are predefined rules or patterns designed to extract structured information from unstructured log messages. While this approach can be effective for certain types of logs, it has several limitations. Template-based methods can be labor-intensive, requiring domain-specific knowledge to create and maintain the templates. Additionally, they may not generalize well to diverse or evolving log formats, leading to incomplete or inaccurate parsing.\nWhen lengthy log sequences are input into transformers for representation learning and downstream tasks, several challenges arise. Firstly, the memory requirements become excessive due to the sheer volume of log data, making it difficult to process efficiently. Secondly, capturing the necessary contextual information demands larger and more complex transformer models, which can be computationally expensive and resource-intensive. Lastly, there is a tendency to treat log data as linear sequences, which neglects the hierarchical and structured nature of log entries. This linear treatment fails to leverage the rich, nested relationships inherent in log data, resulting in sub-optimal representation and analysis.\nTo address these challenges, researchers have proposed several approaches aimed at extending context length and reducing memory costs. Sparse transformers (Child et al. 2019) leverage predefined patterns to limit the number of attention connections each token has. Local attention restricts the attention mechanism to a fixed-size window around each token, ensuring that only nearby tokens are considered. This approach is efficient for capturing local dependencies and reduces the overall computational burden. Strided attention extends this idea by allowing tokens to attend to other tokens at fixed intervals, further reducing the number of attention connections while maintaining the ability to capture broader context across the sequence. Other methods, such as the ones proposed by (Roy et al. 2021) and (Kitaev, Kaiser, and Levskaya 2020), take this concept further by making the sparsity pattern learnable."}, {"title": "2. Related Works", "content": "The related work in this area can be categorized into 2 main groups: efficient transformers including incorporating global memory tokens, sparse attention mechanisms, segment-based recurrence methods, and hierarchical architectures. Each category offers distinct approaches to addressing the challenges of processing long sequences with transformers."}, {"title": "2.1 Efficient Transformers", "content": "Global Memory Tokens in Transformers. Models like Longformer (Beltagy, Peters, and Cohan 2020), ETC (Extended Transformer Construction) (Ainslie et al. 2020), and Big Bird (Zaheer et al. 2020) introduce global memory tokens to address the limitations of traditional transformers with long sequences. These tokens maintain attention connections to all other tokens in the sequence, allowing the models to capture broader contextual understanding while avoiding the quadratic memory and computational overhead of standard self-attention mechanisms.\nSparse Attention Mechanisms. Sparse transformers (Child et al. 2019) employ fixed patterns with local and strided attention to address the inefficiencies of traditional transformers in processing long sequences. Other methods, such as those proposed by (Roy et al. 2021) and (Kitaev, Kaiser, and Levskaya 2020), enhance this concept by making the sparsity pattern learnable. These approaches adapt the attention patterns during training to better capture the data structure.\nSegment-based Recurrence. Segment-based recurrence methods, such as Transformer-XL (Dai et al. 2019) and Compressive Transformer (Rae et al. 2019), introduce mechanisms to maintain and leverage contextual information across segments, significantly reducing memory and computational costs.\nDespite their effectiveness, these approaches are not specifically tailored to the unique characteristics of log data, which often exhibit a hierarchical, dictionary-like structure. This gap underscores the need for models designed to capture and leverage the intrinsic structure of log data."}, {"title": "2.2 Hierarchical Architectures", "content": "Existing hierarchical transformer architectures (Nawrot et al. 2021; Pappagari et al. 2019; Pan et al. 2021; Liu et al. 2021) that primarily focus on compressing or encoding fine-grained information and decoding it back to the original size if necessary. For example, Hourglass (Nawrot et al. 2021) utilizes downsampling and upsampling techniques to create hierarchical and efficient transformers. Pappagari et al. (2019) design hierarchical transformers by segmenting the input into smaller chunks and feeding each chunk into the base model, effectively managing long documents. Swin Transformer (Liu et al. 2021) employs a shifted windows scheme to design an efficient hierarchical architecture. sentence-level information in text data. However, these architectures often prioritize compression and encoding efficiency over accurately representing the hierarchical nature of data. They focus on reducing the size of the data for efficient processing and storage, and then decoding it back when needed. These approaches do not fully align with the unique characteristics of log data, which require capturing and leveraging their inherent hierarchical structure."}, {"title": "3. Proposed Methodology", "content": "In this section, we discuss the hierarchical structure inherent in log data and introduce our novel model, HLogformer, designed to leverage this structure."}, {"title": "3.1 Hierarchical Structure of Log Data", "content": "As illustrated in Figure 1 log data, such as AWS CloudTrail Logs, can be represented in two distinct ways: as a linear sequence (Figure 1 (a)) or as a hierarchical tree (Figure 1 (b)).\nWhen log data is represented as a sequence (Figure 1 (a)), each log entry is treated as a part of a continuous stream. This sequential representation allows for the application of traditional language modeling techniques, where each log entry is analogous to a token in a sentence. By leveraging vanilla language models it is possible to derive meaningful representations of the log data.\nHowever, treating log data as a sequence can oversimplify the complex, nested relationships inherent in the logs. Each log entry in systems like CloudTrail contains multiple fields and attributes organized in a hierarchical structure, reflecting the nested nature of the recorded events. For example, user identity as a log entry contains nested attributes such as account Id, username, session context, principal Id, where session context itself has a nested structure and contains attributes such as session issuer, session arn, etc. Representing this data as a flat sequence can obscure these relationships and result in a loss of critical contextual information.\nRepresenting log data as a hierarchical tree (Figure 1 (b)) acknowledges and preserves the nested structure of the log entries. In this representation, each node in the tree corresponds to a component of the log entry, with parent-child relationships reflecting the inherent hierarchy. This approach captures the multi-level dependencies and relationships within the data more effectively, allowing for a richer and more accurate representation."}, {"title": "3.2 HLogformer: A Hierarchical Log Transformer", "content": "To fully leverage the hierarchical structure inherent in log data, we introduce a novel architecture called HLogformer, illustrated in Figure 2. This architecture is inspired by context compression techniques (Chevalier et al. 2023), but unlike them, HLogformer segments log data according to its hierarchical tree structure. This segmentation process progresses systematically from low-level details to high-level summaries, mirroring the natural organization of the data. Each segment corresponds to a distinct level of the hierarchical structure, ensuring that the model respects and utilizes the nested relationships within the log entries.\nWe can first represent the log data as a directed graph G = (V, E) where si denotes the text in node vi \u2208 V while eij = (Vi, vj) \u2208 E denotes the parent-child relationship in the log data. For step i, we concatenate all the child nodes' text of node i as the segment S\u2081 = Concat[{sj : lij \u2208 G}].\nThe processing pipeline of HLogformer operates step-by-step as shown in Figure 2 (right), beginning with the most granular details of the log data. At each step, the architecture processes a segment of the log data, extracting and summarizing the relevant information. These summary vectors encapsulate the essential context and dependencies at the current level of the hierarchy. Once processed, these summary vectors are passed to the next step, where higher-level segments are processed similarly. At each step i, the segment Si is processed along with the summary vector from the previous step oi-1. This process ensures that the hierarchical context is preserved and progressively refined as we move through the log data. The following equation formalizes this process, where the log data segment Si and the summary vector from the previous step oi-1are combined and processed by the language model LM:\n\\begin{equation}\n\u0396\u03af, \u03c3\u03af = LM([Si, oi-1])\n\\end{equation}\nIn this equation, LM represents the language model that generates the new summary vector \u03c3\u03b5 and the intermediate representation Zi, capturing both the current segment's information and the accumulated context from previous segments.\nBidirectional Hierarchical Compression Paradigm. In the primary architecture described above, summary vectors are passed exclusively from low-level to high-level segments. This allows high-level tokens to access low-level information through the summary vectors, but it may result in low-level tokens missing out some corresponding high-level context. To address this limitation, we propose a bidirectional summary passing technique. This involves initially passing the summary from low-level to high-level, and then reversing the process to ensure that low-level tokens can also benefit from high-level information.\nComplexity Analysis. Our HLogformer provides an efficient framework for handling long context in log data. Assume the entire sequence has a length of L and is split into M equal-sized segments. Then the vanilla transformer has a memory complexity of O(L2), while HLogformer reduces this to O(L2/M).\nAdvantages. This progressive approach offers several key advantages: (1) By segmenting the log data according to its hierarchical structure, HLogformer captures both fine-grained details and broader contextual relationships, building a comprehensive and layered representation at each step;"}, {"title": "3.3 Training Strategy", "content": "After building the hierarchical log transformer, we need to adopt an appropriate training strategy to obtain informative representations and perform downstream tasks. Given that log data typically lack labels, we propose a self-supervised learning approach using masked language modeling loss and volume hypersphere minimization loss\n\\begin{equation}\nLMLM = \\frac{1}{M} \\sum_{i=1}^{M} ymask_{i} log \\hat{y}mask_{i},\n\\end{equation}\nwhere M is the number of masked tokens, Ymask; represents the actual token at the i-th masked position, and \u0177maski is the predicted token at the same position. This loss function encourages the model to accurately predict the masked tokens, thereby forcing it to learn the underlying patterns and dependencies in the log data.\n\\begin{equation}\nLVHM = \\frac{1}{N}\\sum_{i=1}^{N} ||Si - cll,\n\\end{equation}\nwhere N is the number of data points, Si represents the accumulated summary vector of i-th data point, and"}, {"title": "4. Experiments", "content": "In this section, we will begin by evaluating the effectiveness of our HLogformer model in masked language modeling tasks. This will involve assessing its ability to accurately predict masked tokens within a sequence, thereby demonstrating its understanding of the underlying log data. Following this, we will apply our model to several downstream tasks to further validate its utility and performance. These tasks include fake log detection, where the model will be tested on its capability to identify fraudulent or synthetic log entries, and visualization analysis, where we will leverage the model's outputs to generate insightful visual representations of the log data."}, {"title": "4.1 Experimental Setting", "content": "In this section, we detail the dataset utilized for our experiments, the backbone architecture underpinning our models, and the hyperparameters selected to optimize performance."}, {"title": "4.1.1 Datasets", "content": "We use the following datasets in our experiments:\n(1) CloudTrail Logs Dataset: It is an anonymized public log data from flaws.could that covers over 3.5 years of data and 1,939,207 number of events.\n(2) OKTA: This log data is a private dataset which monitors and audits authentication activity to an internal system.\n(3) TrailDiscover: It is an evolving repository of CloudTrail events with detailed descriptions, MITRE ATT&CK insights, real-world incidents, references and security implications.\n(4) Amazon Reviews (Hou et al. 2024a): It is collected in 2023 by McAuley Lab. We use 9 categories of Item Metadata in Amazon Reviews including All Beauty, Amazon Fashion, Appliances, Arts Crafts and Sewing, Automotive, CDs and Vinyl, Digital Music, Health and Personal Care, and Magazine Subscriptions."}, {"title": "4.1.2 Backbone Architectures", "content": "Since our HLogformer is designed as a versatile plugin capable of integrating with any transformer architecture, we will experiment with a variety of backbone models to demonstrate its adaptability and effectiveness. Specifically, we will employ several transformer architectures, including the vanilla Transformer (Devlin et al. 2018) with random initialization, pretrained Transformer (Devlin et al. 2018) with pretrained parameters in bert-base-uncase and four efficient transformers: Linear Transformer (Katharopoulos et al. 2020), Reformer (Kitaev, Kaiser, and Levskaya 2020), Routing Transformer (Roy et al. 2021), and Sparse Transformer (Child et al. 2019)."}, {"title": "4.1.3 Hyperparameters", "content": "We use 8 transformer blocks for backbone models and 1 block for our HLogformer. We set the number of training epochs to 100, the masking rate to 0.2, and the length of the summary vector to 10 tokens. We use Adam optimizer with the learning rate of {0.01,0.005, 0.001 }, the Adam weight decay of {0.01, 0.001,0.0001}, Adam B\u2081 of {0.3,0.6, 0.9}, and Adam \u1e9e2 of { 0.9, 0.99, 0.999}. For each dataset, the training/validation/testing ratio is set as 5:1:1."}, {"title": "4.2 Self-Supervised Learning Task", "content": "To demonstrate the effectiveness and efficiency of our HLogformer, we train the models with masked language modeling loss and present the loss and the number of parameters in the transformer block . From these tables, we can make the following observations:\n\u2022 Our hierarchical framework is a highly effective plugin module that significantly and consistently reduces masked language modeling loss and therefore improves the ability to capture contextual information."}, {"title": "4.3 Supervised Learning Task on TrailDiscover", "content": "In addition to self-supervised learning, we also perform experiments on a supervised classification task. We utilize the TrailDiscover dataset which contains two features for each data point: \"usedInWild\" which is a binary feature and takes two values of True or False, and \"MITRE Attack Tactics\" which is a feature that takes ten different values of attack type."}, {"title": "4.4 Synthetic Anomaly Detection", "content": "After conducting the self-supervised training, we obtain the representation of log data as well as the summary vector. Since we assume the model is trained with real data, fake data is likely to exhibit a different distribution or representation pattern compared to real data. Motivated by this, we can utilize the representations and summary vectors to perform fake data detection. To construct the fake dataset, we mismatch the key-value pairs in the real data with a probability of p = 0.2. In this section, we divide the fake detection into three parts: (1) detection by loss, (2) detection by fake rate, and (3) detection by visualization."}, {"title": "4.4.1 Detection by loss", "content": "In this experiment, we train the model with the total loss as LMLM + 0.1 LVHM. As we train with the real data, we expect the MLM and VHM losses for real and fake data to show significant differences."}, {"title": "4.4.2 Detection by fake rate", "content": "For each masked token i, we obtain an output probability \u0177maski. We then construct a candidate set Candidate\u017c with the top T highest likelihoods. If the real value Xmaski \u2208 Candidate\u017c, we consider token i as normal; otherwise, it is considered fake. Therefore, the fake rate can be calculated as:\nFake Rate = \\frac{number of fake tokens}{number of all masked tokens} \u00d7 100%."}, {"title": "4.4.3 Detection by visualization", "content": "With VHM loss, we expect the summary vector of real data to be closely mapped to the center of the hypersphere. Consequently, the representations of real and fake data should exhibit significantly different patterns. To validate this, we use locally linear embedding (LLE) (Roweis and Saul 2000), principal component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE) (Van der Maaten and Hinton 2008) to perform the dimension reduction and visualization for the summary vectors obtained from real and fake data."}, {"title": "4.5 Product Recommendation Task", "content": "To further demonstrate the effectiveness and advantages of our proposed HLogformer, we conduct a product recommendation downstream task using the pretrained representations on Amazon Reviews dataset (Hou et al. 2024a). Specifically, we select 200 users with the highest number of purchased items and collect pretrained embeddings for all these items. For each user, the last 10 items purchased are treated as positive samples, while 10 items randomly selected from the available item repository are treated as negative samples. The average embedding of the remaining items is computed to represent the user's embedding. We then calculate the cosine similarity between each item's embedding (both positive and negative) and the user's embedding to generate a score list. This score list is sorted, and precision at K"}, {"title": "4.6 Ablation Studies", "content": "To evaluate the effectiveness of our HLogformer, we conduct ablation studies on all of the components."}, {"title": "5. Conclusion", "content": "In this paper, we propose a novel and efficient hierarchical log transformer for dictionary-like log data. Our hierarchical transformers are specifically designed for log data such as CloudTrail and employ an adaptively recursive architecture tailored to this data. Our hierarchical framework is universal, making it orthogonal and compatible with various transformer backbones to further enhance performance and efficiency. Furthermore, our preliminary experiments show that the hierarchical representation learned through self-supervised learning exhibits great potential for encoding log data from events to groups and for various downstream tasks."}, {"title": "Appendix", "content": "Summary vector visualization\nWith VHM loss, we expect the summary vector representations for the real and fake data exhibit significantly different patterns. We visualize the learned summary vector representations using LLE, PCA and t-SNE on OKTA and TrailDis-cover in Figure 5 and Figure 6."}]}