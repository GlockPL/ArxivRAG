{"title": "NeuroSym-BioCAT: Leveraging Neuro-Symbolic Methods for Biomedical Scholarly Document Categorization and Question Answering", "authors": ["Parvez Zamil", "Gollam Rabby", "Md. Sadekur Rahman", "S\u00f6ren Auer"], "abstract": "The growing volume of biomedical scholarly document abstracts presents an increasing challenge in efficiently retrieving\naccurate and relevant information. To address this, we introduce a novel approach that integrates an optimized topic\nmodelling framework, OVB-LDA, with the BI-POP CMA-ES optimization technique for enhanced scholarly document\nabstract categorization. Complementing this, we employ the distilled MiniLM model, fine-tuned on domain-specific\ndata, for high-precision answer extraction. Our approach is evaluated across three configurations: scholarly document\nabstract retrieval, gold-standard scholarly documents abstract, and gold-standard snippets, consistently outperforming\nestablished methods such as RYGH and bio-answer finder. Notably, we demonstrate that extracting answers from\nscholarly documents abstracts alone can yield high accuracy, underscoring the sufficiency of abstracts for many\nbiomedical queries. Despite its compact size, MiniLM exhibits competitive performance, challenging the prevailing\nnotion that only large, resource-intensive models can handle such complex tasks. Our results, validated across various\nquestion types and evaluation batches, highlight the robustness and adaptability of our method in real-world biomedical\napplications. While our approach shows promise, we identify challenges in handling complex list-type questions\nand inconsistencies in evaluation metrics. Future work will focus on refining the topic model with more extensive\ndomain-specific datasets, further optimizing MiniLM and utilizing large language models (LLM) to improve both precision\nand efficiency in biomedical question answering.", "sections": [{"title": "Introduction", "content": "With around 2.5 million new research contributions every year\u00b9 specifically the rapidly growing biomedical research,\nthe need for efficient and accurate information retrieval methods has become increasingly critical. The total volume of\nscholarly documents and the complexity of biomedical queries present substantial challenges in extracting relevant\nanswers from vast repositories of knowledge. As the field advances, researchers are often faced with the daunting task\nof sifting through an extensive array of documents to obtain precise information. This highlights the necessity for robust\nanswer extraction and document categorization methods that can enhance the accessibility of vital information. To\naddress these challenges, this research proposes a neuro-symbolic approach that combines optimized topic modelling\nwith advanced machine learning techniques, effectively integrating symbolic reasoning with neural representations for\nenhanced document retrieval and answer extraction. Specifically, we explore the efficacy of our method through three\ndistinct configurations: the utilization of scholarly document abstract retrieval methods, golden scholarly documents\nabstract, and golden snippets. Our method evaluations demonstrate that our topic model-based document categorization\noutperforms existing methods, such as RYGH and bio-answer finder, which utilize a complex blend of techniques\nlike BM252, ElasticSearch\u00b3, and various transformer models\u2074. This suggests that a simpler yet fine-tuned approach\ncan lead to more effective and cost-efficient solutions for biomedical information retrieval. The primary research\nquestion in this investigation was: How can optimized scholarly document abstract categorization and answer\nextraction methodologies improve the accuracy and efficiency of information retrieval in the biomedical domain?\nAddressing this question is vital, as it not only enhances the precision of answer extraction but also reduces the cognitive\nload on researchers seeking relevant information. Furthermore, our findings reveal that even distilled smaller language\nmodels like MiniLM5 can effectively extract answers when fine-tuned on domain-specific data, particularly when\nfocused on scholarly document abstracts rather than complete documents. While the comparison with the use of Large\nLanguage Models instead of the smaller MiniLM is also an interesting research avenue, it is deemed out of the scope\nof this research. Overall our promising results suggest a potential shift in focus for future biomedical information\nretrieval methods, advocating for strategies that emphasize the utility of concise scholarly abstracts. In summary, the\ncontributions of this research comprise:\n1. A novel neuro-symbolic answer extraction methodology that combines optimized topic modelling and advanced\nmachine learning techniques, effectively addressing the challenges in biomedical information retrieval, particularly\nin extracting answers from an expanding corpus of scholarly documents abstract.\n2. A novel answer extraction methodology that combines optimized topic modelling and advanced machine learning\ntechniques, effectively addressing the challenges in biomedical information retrieval, particularly in extracting\nanswers from an expanding corpus of scholarly documents abstract.\n3. A comprehensive evaluation of the proposed method across three configurations\u2014 scholarly document abstract\nretrieval, golden scholarly documents abstract, and golden snippets\u2014demonstrating its superior performance\nover existing methods like RYGH and bio-answer finder, while highlighting the advantages of simplicity and\ndomain-specific fine-tuning.\n4. Insights into the effective use of distilled models, specifically MiniLM, for accurate answer extraction when\nfined-tuned on domain-specific data, along with recommendations for future biomedical information retrieval\nmethods to focus on concise scholarly document abstracts for enhanced efficiency and accuracy."}, {"title": "Literature Review", "content": "Latent Dirichlet Allocation (LDA)7 is a foundational generative probabilistic model introduced by Blei, Ng, and Jordan,\ndesigned for analyzing collections of discrete data, such as text corpora, by uncovering latent topics within documents.\nAs a three-level hierarchical Bayesian model, LDA represents each document as a mixture over an underlying set of\ntopics, where each topic itself is modelled as a mixture over topic probabilities, thus providing an explicit representation\nof the document's thematic structure. The model's primary advantage lies in its ability to capture complex intra-\ndocument statistical relationships while remaining computationally efficient through the use of variational inference\ntechniques and an Expectation-Maximization (EM) algorithm for parameter estimation. This approach outperforms\ntraditional dimensionality reduction techniques like Latent Semantic Indexing (LSI) and probabilistic LSI (pLSI)10,\nwhich lack a fully generative model for documents and suffer from issues like overfitting and limited scalability to\nunseen data\u00b9\u00b9. Building on this foundation, Hoffman et al. developed an online variational Bayes (VB) algorithm for\nLDA12, optimizing its application for large-scale and streaming data scenarios. This advancement leverages online\nstochastic optimization to enable efficient topic modelling on massive datasets, significantly reducing computational\ncosts compared to traditional batch algorithms while maintaining or even enhancing model accuracy. The online LDA\nalgorithm's ability to handle document collections in a streaming fashion without the need for iterative data storage\npositions it as a crucial tool for real-time text analysis in various applications, making it highly relevant for contemporary\nmachine learning and natural language processing tasks13.\nThe BI-population Covariance Matrix Adaptation Evolution Strategy (CMA-ES)14, employs a dual restart strategy\nwith increasing and varying small population sizes, making it an effective optimization technique for complex search\nlandscapes. Benchmarked on the BBOB-2009 noiseless testbed, it successfully solved up to 23 out of 24 functions\nin different dimensions within a constrained budget of evaluations15. While demonstrating strong performance on\nunimodal and multimodal functions, its limitations on smooth, regular, and separable functions highlight areas for\nimprovement, suggesting that refining its restart schedule could further enhance its optimization capabilities. In Task\n10b of the BioASQ challenge, various approaches were employed in biomedical question answering and document\ncategorization. A total of 70 methods were submitted across different research groups, showcasing diverse techniques\nin solving the tasks16. The challenge was divided into two phases: Phase A, which focused on information retrieval, and\nPhase B, which addressed question answering. Below is an overview of the key methodologies and advancements in\nthese fields. The method developed by UCSD, bio-answerfinder, evolved from earlier work, introducing improvements\nin tokenization and query expansion17. In Phase A, UCSD employed a relaxation mechanism in the ranked keyword-\nbased document retrieval process, using BM25 retrieval and ranking keywords with a cascade of LSTM layers1618. In\nPhase B, they shifted towards abstractive summarization using T5, marking a transition from extractive to generative\napproaches in summarization tasks16. The University of Aveiro leveraged their transformer-UPWM model19, extending\nit with the PARADE-CNN architecture20. Throughout both phases, they utilized the PubMedBERT transformer model,\ndemonstrating the effectiveness of transformers in biomedical text representation. In Phase B, they introduced a classifier\nfor handling yes/no questions, indicating an efficient approach to binary classification16. Mohamed I University's\nLaRSA method combined ElasticSearch with BM25 for retrieval, alongside fine-tuning Roberta-base and BioBERT\nmodels for various question types16. Their use of cross-encoders and re-rankers, trained on the MS Marco Passage\nRanking task, enhanced their retrieval accuracy, while BART models were employed for generating ideal answers in\nPhase B, demonstrating an integrated approach to question answering16. The ELECTROBERT method, designed by\nBSRC Alexander Fleming, merged the strengths of ELECTRA and ALBERT, focusing on document retrieval tasks.\nPre-training on PubMed abstracts and fine-tuning on the BioASQ9 dataset emphasized the relevance of combining\nlarge-scale transformer models with domain-specific data for better document ranking21. RYGH used a multi-stage\nretrieval method combining BM25 with BioBERT, PubMedBERT, and SciBERT, integrating both traditional and modern\nretrieval techniques to enhance biomedical question answering16. This hybrid approach highlights the importance of\ncombining transformer models with established retrieval algorithms. Google's gsl methods applied a zero-shot hybrid\nmodel that integrated retrieval and re-ranking stages. Their approach combined BM25 for document retrieval with\na dual-encoder and cross-attention re-ranking model, reflecting the power of multi-stage architectures in optimizing\nbiomedical document retrieval16. TU Kaiserslautern introduced the BioNIR method, based on sentence-BERT (sBERT),\nwhich encoded queries and abstracts sentence-by-sentence and used distance metrics for retrieval, underscoring the\nutility of sentence-level ranking in biomedical text processing16. KU methods focused on augmenting their training data\nin Phase B using a BioBERT backbone. They applied question-generation techniques to increase the diversity of their\ntraining dataset, exemplifying the use of data augmentation in improving model generalization for question-answering\ntasks16. Macquarie University's two teams used different models for different tasks. The MQ method used DistilBERT\nfor ideal answers, while MQU employed BART-based abstractive summarization methods, showcasing a dual strategy\nof lightweight and abstractive models to address various question types effectively16. Fudan University's Ir_sys\nmethods employed a combination of BERT, BioBERT, and BART models across multiple question-answering tasks.\nThis comprehensive approach across factoid, list, and summarization tasks indicates the flexibility of transformer\nmodels in biomedical question answering16. The University of Delaware's UDEL-LAB method focused on fine-tuning\ntransformer-based models like BioM-ALBERT and BioM-ELECTRA. Their emphasis on hyperparameter tuning and"}, {"title": "Methods", "content": "Our research employed two distinct approaches to extract answers for factoid and list-type questions from biomedical\nscholarly documents abstracts. The methodology consisted of two different phases: scholarly document abstract\ncategorization and answer extraction. Subsequent sections provide comprehensive explanations and detailed procedures\nfor each phase. Figure 1 illustrates an overview of the overall methodology."}, {"title": "Corpora Details", "content": "In this experiment, we employed the BioASQ10 dataset16 as our primary data source. The dataset is divided into two\nsections: the \"BioASQ-training10b\" training dataset and the \"Task10BGoldenEnriched\" test dataset. The \"BioASQ-\ntraining10b\" dataset is comprehensive, containing 4,234 questions along with an extensive collection of 36,844 unique\nbiomedical scholarly document abstracts. These questions are categorized into four types: factoid, list, summary,\nand binary. However, our experiment focused exclusively on factoid (1,252) and list-type (816) questions, as these\nare particularly aligned with our research objectives. The \"Task10BGoldenEnriched\" dataset, used for evaluation,"}, {"title": "Data Preprocessing", "content": "We begin by enhancing the dataset by fetching PubMed22 abstracts corresponding to the PubMed IDs provided in\nthe BioASQ10 dataset. Following this, we conducted several essential preprocessing steps to ensure the quality and\nconsistency of the data for our experiments. We removed stop words\u2014commonly occurring words in natural language\nthat carry minimal semantic value23. This step effectively reduced noise in the dataset, improving the efficiency of\nsubsequent analyses. In addition to stopping word removal, we applied a comprehensive text-cleaning process, which\nincluded eliminating punctuation and special characters. By removing these extraneous symbols, we created a more\ncoherent and standardized dataset, ensuring uniformity across the data. Finally, we applied a stemming process to\nreduce words to their root forms, which allowed for more accurate and meaningful analyses by grouping related terms\nunder a common base form. These preprocessing steps collectively contributed to the creation of a cleaner, more\nstructured dataset that was better suited for the analytical tasks in our experiment."}, {"title": "Document Categorization", "content": "To achieve robust results for document categorization, we employed an advanced approach that integrates state-of-\nthe-art techniques and optimization strategies. For the categorization of scholarly documents abstract, we applied the\nOnline Variational Bayes for Latent Dirichlet Allocation (OVB-LDA)12, a topic modelling method well-suited for\nidentifying latent topics within large corpora. OVB-LDA's ability to uncover hidden structures in the dataset allowed us\nto categorize documents based on their underlying topic effectively. To further enhance the performance of OVB-LDA,\nwe employed the Bimodal Population Covariance Matrix Adaptation Evolution Strategy (BI-POP CMA-ES)14, a\ncutting-edge optimization method known for effectiveness in exploring high-dimensional parameter spaces. This\nstrategy was critical in fine-tuning OVB-LDA, significantly improving both the accuracy and efficiency of the document\ncategorization process. The combination of these advanced techniques allowed us to build a highly accurate model for\nthe retrieval of scholarly document abstracts."}, {"title": "Feature Engineering", "content": "For feature engineering, we utilized the Bag-of-Words (BoW)24 method, a well-established and widely-used technique\nin natural language processing. The BoW approach generates a representation of each document by capturing the\noccurrence and frequency of individual words within the scholarly document's abstract. This transformation created\nhigh-dimensional vectors, with each vector encapsulating the unique linguistic characteristics and content nuances\nof its respective document. These feature vectors formed the primary input for our OVB-LDA method, facilitating\nthe categorization of scholarly document abstracts based on their latent topics. By converting unstructured abstracts\ninto a structured and quantifiable format, BoW played a crucial role in empowering OVB-LDA to effectively process\nand categorize scholarly document abstracts. This feature engineering step was essential in linking our raw data with\nthe advanced categorization techniques, ensuring that the data was in a suitable form for robust topic modelling and\nsubsequent analysis."}, {"title": "Document Categorization using OVB-LDA", "content": "Building upon the feature engineering step, where we transformed scholarly document abstracts into structured vectors\nusing the BoW method, our research employed the Online Variational Bayes for Latent Dirichlet Allocation (OVB-\nLDA)25 as a robust and data-efficient method for scholarly documents abstract categorization and information retrieval.\nOVB-LDA's strength lies in its ability to efficiently handle large-scale collections of scholarly documents abstract by\nprocessing data in manageable mini-batches, making it well-suited for our extensive dataset. OVB-LDA extracting\nscholarly documents abstract-topic distributions from the scholarly documents abstracts. Each scholarly document\nabstract in our corpus was associated with a unique topic distribution, reflecting its underlying thematic structure.\nSimultaneously, queries were transformed into topic distributions using the same LDA approach, allowing for a\nconsistent representation of both documents and queries. For the scholarly documents abstract retrieval process, we"}, {"title": "Optimization Method for Scholarly Document Categorization", "content": "Following the implementation of the OVB-LDA method for scholarly document abstract categorization, we introduced\nan advanced optimization approach to further enhance the performance of our scholarly document abstract retrieval\nmethod. Specifically, we utilized the Bimodal Population Covariance Matrix Adaptation Evolution Strategy (BI-POP\nCMA-ES)27, a cutting-edge optimization technique designed to fine-tune the parameters of OVB-LDA for improved\ncategorization accuracy. The primary goal of applying BI-POP CMA-ES in our experiment was to maximize the retrieval\nperformance of OVB-LDA by iteratively adjusting its parameters. This process involved evaluating the performance of\nvarious parameter configurations across a diverse set of queries, allowing for precise calibration based on the specific\ncharacteristics of both the dataset and the query corpus. By continuously refining these parameters, BI-POP CMA-ES\nenabled OVB-LDA to perform more effectively in retrieving relevant scholarly document abstracts. One of the unique\nadvantages of BI-POP CMA-ES is its dynamic adaptation of population size during the optimization process28. This\nfeature allowed the method to navigate the complex optimization landscape efficiently, making it particularly suited\nfor the intricate task of biomedical scholarly document abstract categorization. Through the application of BI-POP\nCMA-ES, we achieved optimal parameter configurations for OVB-LDA, ultimately enhancing both the accuracy and\nefficiency of our scholarly document abstract categorization and retrieval method. The final optimized parameters are\ngiven in Table 2. The detailed steps of this process are methodically presented in Algorithm 1."}, {"title": "Answer Extraction", "content": "The answer extraction phase in our method focuses on retrieving precise answers from the categorized biomedical\nscholarly documents abstract, building upon the scholarly document abstract categorization process discussed in\nprevious sections. For this task, we employed the Transformer architecture, which is powered by the self-attention\nmechanism29. This architecture enabled the efficient extraction of relevant information from the large-scale biomedical\nscholarly document corpus. To enhance efficiency and maintain performance, we integrated the MiniLM model30, a distilled version of the larger Transformer model, which emphasizes the use of deep self-attention layers while\nmaintaining computational efficiency. To adapt the model specifically for answering biomedical factoid and list-type\nquestions, we fine-tuned the MiniLM model on a domain-specific dataset of biomedical scholarly document abstracts.\nThis fine-tuning step allowed the model to understand better the specialized language and entities found in biomedical\nliterature, ensuring more accurate and relevant answer extraction31."}, {"title": "Lexical Synonyms Extraction", "content": "In handling factoid and list-type queries, we further enriched the answer extraction process by incorporating lexical\nsynonym extraction. This process was vital for enhancing the model's ability to identify different variations of\nbiomedical terms and entities. For this purpose, we utilized the WordNet lexical database32 33, which is widely used\nfor identifying synonyms across various linguistic contexts. By applying WordNet to the biomedical dataset, we\nmethodically expanded the synonym coverage for key biomedical entities found in the scholarly document abstracts34.\nThis enriched the dataset with semantically related terms and improved the method's capacity to match queries with\nrelevant answers, even when different terminology or synonyms were used. This synonym extraction approach was\napplied to both factoid and list-type queries from our dataset, significantly improving the quality and comprehensiveness\nof the data."}, {"title": "Input Representation for Fine-tuning", "content": "In our research, we employed the Transformer architecture, specifically leveraging the MiniLM model and its variant,\n\"minilm-uncased-squad2\"35. This model was used for the tokenization and was essential for the effective fine-tuning of\nour answer extraction process. To maintain a robust connection between the tokenized data and the original scholarly\ndocuments abstract, we established a sample_mapping structure. This mapping allowed us to trace back each token\nto its corresponding document, ensuring that the context of the extracted answers was preserved. To facilitate the\nalignment of tokens with their respective character positions within the scholarly document abstracts, we utilized offset\nmapping. This approach allowed us to accurately represent the position of each token in the original text, which is\ncrucial for determining the start and end positions of the answers extracted from the scholarly documents abstract.\nFurthermore, we ensured that each tokenized scholarly document abstract was labelled with start and end positions that\ndelineate the exact span of the answer within the text. For instances where no answer was present, we labelled these\ncases using the CLS token index. This methodatic labelling process was vital for training the MiniLM model, enabling\nit to learn effectively from both positive (answer-present) and negative (answer-absent) examples. By establishing\nthis structured input representation, we connected the answer extraction phase with the previous stages of scholarly\ndocument abstract categorization and lexical synonym extraction, ultimately enhancing the model's performance in\nretrieving accurate and relevant answers from biomedical literature."}, {"title": "Fine-tuning", "content": "For the QA task, our primary objective was to predict the start and end positions of the answer span within scholarly\ndocument abstracts. In this context, let \\(I\\) denote a scholarly document abstract, \\(M\\) represent the pre-trained model, \\(c\\)\nbe the tokenized context, and \\(q\\) signify the query question. The predicted span \\(a\\) is computed as:\n\\[a = M (c,q)\\]\nTo evaluate the performance of the model, we calculate the loss \\(L\\) using the cross-entropy loss function:\n\\[L = CrossEntropyLoss(a)\\]\nThe learning rate is denoted by \\(a\\) and gradients are denoted by \\(\\nabla L\\), the updated model weights are obtained\nthrough the following update rule:\n\\[\u039c \u2013 M \u2013 a\u2207L\\]"}, {"title": "Answers Extraction", "content": "To extract relevant answers from the categorized scholarly documents abstract, we utilized a specialized reader from the\nHaystack library36. This extraction process begins by fragmenting each scholarly document abstract into manageable\nsamples, especially if the scholarly documents abstract exceed the model's maximum sequence length. Each sample,\nalong with the corresponding question, is then tokenized and passed through the fine-tuned model to obtain word\nvectors. These word vectors are crucial as they are transformed into logits, which indicate potential answer spans within\nthe scholarly document abstract. Subsequently, we categorize these logits to determine the most likely answer span or\nconclude that no answer exists within the scholarly document abstract. This methodatic approach to answer extraction\nbuilds on the previous sections detailing the model fine-tuning and input representation processes. By leveraging the\ncapabilities of the Haystack library and the optimized model, we aim to achieve precise and relevant answer retrieval\nfrom biomedical scholarly document abstracts, thereby enhancing the overall effectiveness of our QA method. The\ndetailed steps of this process are methodically presented in Algorithm 3."}, {"title": "Evaluation Metrics", "content": "We also explores the evaluation metrics used for Scholarly document abstract categorization and answer extraction,\nparticularly for list-type and factoid questions. Precision, recall, and F1-score are employed to assess categorization\nand entity identification accuracy in list questions, while strict and lenient accuracy is applied to factoid questions37 38.\nAdditionally, the Mean Reciprocal Rank (MRR) serves as the official metric for ranking the correctness and relevance\nof factoid answers."}, {"title": "Scholarly Document Categorization and List Questions", "content": "In the context of Scholarly document abstract categorization and answer extraction for list-type questions, we utilized\nthe following computation metrics:\n\u2022 Precision (P): For Scholarly document abstract categorization, precision measures the proportion of correctly\nidentified scholarly document abstracts, and for list-type questions, entities in the method's answer list relative to\nthe total number of entity names in the golden list. It can be quantified as:\n\\[P=\\frac{TP}{TP+FP}\\]\nHere, TP represents the count of true positives, and FP represents the count of false positives.\n\u2022 Recall (R): Recall measures the proportion of correctly identified Scholarly document abstracts in the case of\nretrieval and entity names in the case of list questions in the method's response relative to golden Scholarly\ndocument abstracts and lists. It is defined as:\n\\[R =\\frac{TP}{TP+FN}\\]\nWhere TP represents the count of true positives, and FN represents the count of false negatives.\n\u2022 F-measure (F1): The F1-score is the harmonic mean of precision and recall, offering a balanced measure of both\nmetrics. It can be calculated as:\n\\[F1 = 2 \u00d7 \\frac{PXR}{P+R}\\]"}, {"title": "Factoid Questions", "content": "In the context of factoid questions, methods are tasked with returning up to five entity names, ranked by confidence. To\nassess the performance of these methods, the BioASQ team provides a golden entity name for each question along with\nits potential synonyms. Evaluation involves the use of two key accuracy metrics:\n\u2022 Strict Accuracy (SAcc): A question is considered correctly answered if the golden entity name or one of its\nsynonyms is the topmost element in the list of returned entities. It can be quantified as:\n\\[SAcc =\\frac{c1}{n}\\]\nWhere c1 represents the count of questions correctly answered with strict accuracy, and n is the total number of\nquestions.\n\u2022 Lenient Accuracy (LAcc): This metric allows for a more flexible evaluation criterion. A question is considered\ncorrectly answered if the golden entity name or one of its synonyms appears anywhere in the list of returned\nentities. Its calculation is given by:\n\\[LAcc =\\frac{c5}{n}\\]\nHere, c5 represents the count of questions correctly answered with lenient accuracy, and n is again the total\nnumber of questions."}, {"title": "Results and Discussion", "content": "In this section, we present a comprehensive evaluation of our proposed method for answer extraction and scholarly\ndocument abstract categorization within the biomedical domain. We assess the performance across three distinct\nconfigurations: scholarly document abstract retrieval, golden scholarly documents abstract, and golden snippets. The\nevaluation is conducted over multiple batches of scholarly document abstracts, and the results highlight significant\nimprovements in precision, recall, and F-measure across various question types. Notably, our method demonstrates a\nstrong ability to retrieve accurate information, with golden snippets consistently achieving the highest performance,\nparticularly in terms of MRR. These findings emphasize the versatility and robustness of the proposed approach across\ndiverse datasets and evaluation metrics."}, {"title": "Predictive Performance", "content": "The proposed method exhibits strong predictive performance, achieving high precision and recall across multiple\nbatches, particularly in scholarly document abstract retrieval and golden scholarly document abstract configurations.\nFor answer extraction, the method excels when leveraging golden snippets, yielding superior MRR and F-measure\nscores, especially in factoid and list-type questions. This highlights the method's capability to retrieve and extract\naccurate biomedical information effectively across diverse query types."}, {"title": "Document Retrieval", "content": "In a comprehensive evaluation of scholarly document abstract categorization conducted across six different batches, the\nassessment was grounded on three distinct metrics: mean precision, recall, and the F-measure, which amalgamates\nboth precision and recall to provide a holistic view of performance (as shown in Table 3). The scores are presented\nin a sorted order based on F-measures. In the initial batch, the proposed method exhibited a mean precision of 0.305,\na recall of 0.540, and an F-measure of 0.312. While the precision was surpassed by a few other methods, notably\nthe \"bio-answerfinder,\" the recall metric was second only to the RYGH-1 method16, indicating a robust ability of the\nproposed method to retrieve relevant scholarly documents abstract. The subsequent batch saw the proposed method\nachieve a precision of 0.292, a recall of 0.523, and an F-measure of 0.282. In the third batch, the proposed method\ndemonstrated a precision of 0.294, a recall of 0.576, and an F-measure of 0.303. For the fourth batch, the proposed\nmethod achieved a precision of 0.307, a recall of 0.531, and an F-measure of 0.309. The method's performance\nremained consistent, maintaining its position among the top contenders. In the fifth batch, the proposed method attained\na precision of 0.332, while the recall and F-measure were 0.474 and 0.317, respectively. Notably, in the final batch, the\nproposed method outperformed all other methods in precision with a score of 0.559. The recall and F-measure in this\nbatch were 0.423 and 0.433, respectively, underscoring the method's balanced performance."}, {"title": "Answer Extraction Performance", "content": "In this section, we conduct an exhaustive analysis of our novel answer extraction method, methodically evaluating\nits performance under various conditions and across different batches. The proposed method is assessed under three\ndistinct configurations: first, employing outputs from the scholarly document abstract categorization, denoted as\nproposed(document_retrieval); second, utilizing golden scholarly documents abstract sourced from the test dataset,\nlabelled as proposed(golden_documents); and third, leveraging golden snippets from the same dataset, signified as\nproposed(golden_snippets). The comprehensive evaluation results, encompassing MRR and F-measure, are meticulously\npresented in Table 4. The table is thoughtfully organized by descending MRR and ascending F-measure, offering\nvaluable insights into the method's performance nuances across diverse conditions and settings. From the analysis\nof Table 4, it is evident that the proposed method demonstrates varying performance levels across the different\nconfigurations. For instance, proposed(golden_snippets) consistently achieves higher MRR values compared to other\nconfigurations, suggesting that leveraging precise snippets enhances the retrieval effectiveness significantly. Notably,\nthe results indicate that while the F-measure values for proposed(golden_documents) and proposed(document_retrieval)\nare competitive, they exhibit slightly lower MRR scores, indicating a potential trade-off between precision and recall.\nMoreover, Batch 1 exhibits the highest MRR across the configurations, suggesting that the nature of the dataset and the\ninherent complexity of the tasks influence the performance metrics significantly."}, {"title": "Evaluation of Answer Extraction Configurations", "content": "In the preceding sections, we introduced our novel answer extraction method and outlined its fundamental configurations.\nIn this subsection, we delve into the detailed performance analysis of these configurations, including scholarly document\nabstract categorization, golden documents, and golden snippets, evaluating their respective efficiencies across different\nbatches of questions."}, {"title": "Document Categorization", "content": "Concerning the comprehensive assessment of answer extraction performance, our most noteworthy results were obtained\nunder the proposed(document_retrieval) configuration, particularly in Batch 1. This configuration exhibited exceptional\nperformance for factoid questions, boasting a strict accuracy of 0.382, a lenient accuracy of 0.676, and an impressive\nMRR score of 0.482. Similarly, list-type questions displayed commendable results with a mean precision of 0.238,\na recall of 0.638, and an F-measure of 0.379, affirming its prowess in diverse question types and emphasizing its\nversatility in Batch 1."}, {"title": "Golden Documents", "content": "The proposed(golden_documents) configuration showcased superior performance in Batch 2. For factoid-type questions,\nit achieved an outstanding strict accuracy of 0.735, a lenient accuracy of 0.852, and an impressive MRR score of 0.765.\nRegarding list-type questions, the method continued to shine with a mean precision of 0.449, a recall of 0.592, and a\nsubstantial F-measure of 0.514. These scores bear significant importance as they represent the maximum potential of\nour proposed method when operating with ideal, meticulously curated scholarly documents abstract. This underscores\nour proposed approach's efficacy, particularly when interacting with precise, high-quality scholarly knowledge."}, {"title": "Golden Snippets", "content": "The proposed(golden_snippets) configuration reached its pinnacle in Batch 5, showcasing remarkable performance.\nFactoid-type questions achieved a strict accuracy of 0.620, a lenient accuracy of 0.896, and an impressive MRR score\nof 0.723. Meanwhile, for list-type questions, the configuration demonstrated a mean precision of 0.683, a recall score\nof 0.571, and a substantial F-measure of 0.596. These outstanding results underscore the remarkable effectiveness of\nour proposed answer extraction methods, particularly when they operate in tandem with the most precise and relevant\ntext snippets available."}, {"title": "Discussion", "content": "Our study offers valuable insights into the efficacy of machine learning methods within the biomedical domain,\nspecifically in answer extraction and scholarly document abstract categorization. The findings from our evaluation\nilluminate several key aspects that merit further discussion.\nFirstly, our results indicate that our meticulously optimized topic model-based scholarly document abstract catego-\nrization outperforms existing methods such as RYGH and bio-answer finder, which utilize a blend of sophisticated\ntechniques, including BM25, ElasticSearch, BioBERT, PubMedBERT, T5, BERTMeSH, and SciBERT16. This outcome\nunderscores the notion that simplicity, coupled with domain-specific fine-tuning, can sometimes surpass more complex\napproaches, providing a more efficient and cost-effective solution for biomedical knowledge retrieval. This is partic-\nlarly relevant in the context of our previous sections, where we detailed our approach's robustness against various\nconfigurations and settings.\nSecondly, our study challenges the prevailing belief that the biomedical domain necessitates large, computationally\nintensive models for accurate knowledge retrieval. Our findings illustrate that even compact methods, such as MiniLM,\ncan effectively extract answers from scholarly document abstracts when fine-tuned on domain-specific data and provided\nwith precise context. Notably, our approach focused exclusively on scholarly document abstracts rather than entire\nscholarly documents. This observation suggests that scholarly document abstracts often contain sufficient knowledge\nfor the precise extraction of answers.\nThis discovery carries substantial implications for the design of future biomedical knowledge retrieval methods,\nadvocating that a concentrated effort on scholarly document abstracts could represent a viable strategy for efficient and\neffective answer extraction. Our results reveal a promising avenue for enhancing information retrieval methodologies\nin the biomedical field, emphasizing the importance of optimizing processes without the necessity of relying on vast\ncomputational resources."}, {"title": "Limitations", "content": "While our research has yielded valuable insights, it is important to acknowledge several limitations. Firstly, despite the\nnoteworthy performance of our scholarly document abstract categorization method, particularly in integrating it with full\nscholarly documents. Although our proposed method has shown robust performance in earlier evaluations, it encounters\nchallenges when addressing more intricate query types, such as list-based questions. Even the finely-tuned MiniLM\nmodel struggles in these scenarios, indicating potential areas for enhancement. In the fine-tuning process for the\nMiniLM model required data in a specific format. Given the labour-intensive nature of manual annotations, we devised\nan automated method, primarily annotating answers based on syntactic matches within related passages. We leveraged\nWordNet to identify potential synonyms to address the absence of exact word matches. However, the complexities of\\"}]}