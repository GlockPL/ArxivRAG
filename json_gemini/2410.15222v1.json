{"title": "AutoFLUKA: A Large Language Model Based Framework for Automating Monte Carlo Simulations in FLUKA", "authors": ["Zavier Ndum Ndum", "Jian Tao", "John Ford", "Yang Liu"], "abstract": "Monte Carlo (MC) simulations, particularly using FLUKA, are essential for replicating real-world scenarios across scientific and engineering fields. Despite the robustness and versatility, FLUKA faces significant limitations in automation and integration with external post-processing tools, leading to workflows with a steep learning curve, which are time-consuming and prone to human errors. Traditional methods involving the use of shell and Python scripts, MATLAB, and Microsoft Excel require extensive manual intervention and lack flexibility, adding complexity to evolving scenarios. This study explores the potential of Large Language Models (LLMs) and AI agents to address these limitations. AI agents, integrate natural language processing with autonomous reasoning for decision-making and adaptive planning, making them ideal for automation. We introduce AutoFLUKA, an AI agent application developed using the LangChain Python Framework to automate typical MC simulation workflows in FLUKA. AutoFLUKA can modify FLUKA input files, execute simulations, and efficiently process results for visualization, significantly reducing human labor and error. Our case studies demonstrate that AutoFLUKA can handle both generalized and domain-specific cases, such as Microdosimetry, with an streamlined automated workflow, showcasing its scalability and flexibility. The study also highlights the potential of Retrieval Augmentation Generation (RAG) tools to act as virtual assistants for FLUKA, further improving user experience, time and efficiency.", "sections": [{"title": "1. Introduction", "content": "Monte Carlo (MC) simulations are virtual experiments that replicate real-world scenarios using statistical methods. These simulations are crucial for accurately modeling complex physical processes, such as radiation transport and particle interactions with matter that could be otherwise expensive, time consuming or resource-intensive. Particularly, FLUKA (FLUk-tuierende Kaskade) is a versatile MC simulation package with proven applicability across various fields of science and engineering. Originally developed at the European Organization for Nuclear Research (CERN) in the 1960s, then later, jointly by CERN and the Italian National Institute of Nuclear Physics (INFN), FLUKA has evolved into a multi-purpose tool capable of simulating the interaction and transport of over 60 different particles across a wide range of energies in matter (Ahdida et al., 2022; B\u00f6hlen et al., 2014; Fass\u00f2 et al., 2012)\nFLUKA simulations have been extensively used to advance research and innovations in nuclear science and engineering. For example, Zhao and colleagues coupled FLUKA with OpenMC for the physics calculations of higher-energy spallation (p,n) reactions in accelerator-driven sub-critical reactor systems (ADS), which are important for nuclear energy research (Zhao et al., 2019), while Polanski and colleagues coupled FLUKA with MCNPX to study the production of neutrons in heavy extended targets by electrons of energy from 15 to 1000 MeV (Pola\u0144ski et al., 2015). Bazo and colleagues performed neutron activation analysis, particularly in studying the interaction of neutron beams with different materials using the RP-10 reactor. Here, FLUKA was employed to simulate neutron activation in silicon and germanium (Bazo et al., 2018). FLUKA has also been used (either independently or coupled with other MC codes like MCNP and GEANT4) to either study or benchmark experimental results of novel materials as candidates for nuclear radia-"}, {"title": "2. MATERIALS AND METHODS", "content": ""}, {"title": "2.1. From LLMs to AI Agents: Using LLMs Programmatically", "content": "Large language models (LLMs) can be defined as a group of artificial intelligence (AI) or transformer neural networks containing a substantial number of parameters, often reaching tens or hundreds of billions (Zhao et al., 2023). The higher the number of parameters, the better the performance. These models are trained on extensive corpora, enabling them to efficiently perform a wide variety of natural language processing (NLP) tasks like text generation, summarizing, translation, general coding, question answering and even acting as chatbots.\nSince the stunning debut of ChatGPT in November 2022, several other significant AI models and tools have been released like Gemini from Google DeepMind (2024), LLaMA from Meta (2023), Claude by Anthropic (2023),"}, {"title": "2.2. Building Agentic Applications on LangChain", "content": "LangChain is an open-source framework designed for developing applications powered by LLMs. It was introduced by Harrison Chase in 2023 (H. Chase, 2023), aiming to make the integration of LLMs into AI-powered applications more accessible, especially for tasks requiring complex, scalable workflows or multi-step reasoning.\nLangChain was chosen for this project because of (i)-its wide acceptance in the AI community with a proven history of applications build to automate tasks spanning from customer service (Pandya and Holia, 2023), architecture (Jeong, 2023) to health care(Singh et al., 2024) and; (ii)-its versatility and a rich library of components and methods like; Schemas which refer to the data structures (text, ChatMessages and documents) used across LLM platforms; LLM Wrappers for connection to LLM models like those from OpenAI, Google DeepMind and Hugging Face; Prompt Templates for structuring and optimizing input prompts, thus reducing the labor in-"}, {"title": "3. AutoFLUKA's CUSTOM TOOLS", "content": "It is important to note that each block of the FLUKA simulation workflow shown in Figure 1 involves a series of sub-tasks. For instance, creating the FLUKA input files requires loading the template file, modifying it with the user-defined parameters before making multiple versions with varying seeds (statistically independent cycles). For this reason, different custom tools were"}, {"title": "3.1. Creating the Template File.", "content": "The template file must be manually created from a valid FLUKA input file; valid here means that the input file must be running correctly and producing the expected results. FLUKA uses a text-based input file system. These input files are structured with specific keywords related to the so-called input cards which are used to configure different aspects of the simulation related to the geometry, sources, materials, physics or particle generation and transport and scoring (like energy deposition, dose, particle fluence, current, yield, etc). A version of this file will be made in our Git Up repository together with the template. Our template version has been configured to allow the user to externally modify parameters through variables without"}, {"title": "3.2. Executing FLUKA and Decrypting the Generated Binary Files", "content": "Once the input files have been created, the fluka_executer_tool then lunches and monitors the simulations, capturing the standard input and outputs (stdin/stdout) and any errors generated by FLUKA itself (stderr). A bash script written in SLURM (Simple Linux Utility for Resource Management) for scheduling and submitting the jobs on the command line was incorporated within this tool to be executed using Python's subprocess.\nIn FLUKA, the user can select any Fortran logical unit \u201cxx\u201d between 21 and 99 (17 by default for the DETECT card) for their detector(s) which can also be any scoring card as shown in Table 4. FLUKA then generates results for every cycle in binary format, automatically following a naming convention. For example, if the base file was named \u201cexample.inp\", and the user selected the logical unit (BIN 46) to score the boundary crossing fluence using the USRBDX card, then the generated results from running multiple cycles will appear as example_01001_fort.46, example_02001_fort.46, etc. Even though FLAIR provides a straightforward way to decrypt and process or merge these data, users running FLUKA through the command line and on clusters with limited access to FLAIR must do this manually for each cycle using the appropriate FLUKA's in-built post-processing utilities as shown in Table 4. Though these utilities work in an interactive pattern, the user must type respective file names for all cycles in the simulation and then specify a file name in which the output (decrypted data) will be stored. This process, requiring a lot of writing is time-consuming, monotonous and could be subject to typographical errors.\nThrough our custom data decrypting tool (fluka_data_decrypter_tool), this entire process has been automated. Since we only know which utility works for the various cards as presented in Table 4, BUT cannot determine which card was employed by the user or what quantity was scored from the\""}, {"title": "3.3. Single Agent and Multi-Agent Workflows", "content": "In this project, both the single and multi-agent approaches were exploited to automate the simulation workflow. For the single-agent approach, the simple_llm_agent model developed earlier in section 2.1 was augmented by adding more tools, fine-tuning the prompt templates (system messages) and giving it memory capabilities for retaining the history of previous runs (particularly useful for the FLUKA Assistant RAG tool). The Agent Executor in this case acts both as a supervisor, selecting the appropriate tool for each task and monitoring the workflow until all steps have been completed. On the other hand, the multi-agent approach incorporated each custom tool with its own LLM agent having a specific system message. All the agent nodes were then connected using the LangGraph method from LangChain. In this approach, the actions of these individual agents (also known as workers) are coordinated by a supervisor, which, itself is an agent with a specific system message. Figure 3(a) illustrates these differences, while Figure 3(b) shows a graph representation of the multi-agent framework.\nA comparison between these two revealed that despite the inherent flexibility of the multi-agent approach, in that each agent or worker can be fine-tuned with a detailed (long) prompt template (system message and description) on how to accomplish tasks, it constantly hallucinated. Here, the agent supervisor either calls the wrong worker for a particular action (what we call worker-action mismatch) or tries to invoke a non-existent worker. Detailed crafting of the user query and exclusion of passive tools (workers not directly involved in the task) were applied to mitigate this effect. On the other hand, the single-agent approach, because of its inherent simplicity saw less hallucinations. The major limitation being the context length of tool descriptions which currently is 128,000 tokens for gpt-40.\nThe human in the loop currently has two functions: (i)- logs into the computer system and initiates the action. This is the sole responsibility of the human for security reasons, and (ii)-checks the accuracy of the simulations, from ensuring that the input files are created correctly (step 3) to evaluating the generated plots (step 9). Step 3 is quite tricky since it involves ensuring that the generated input files respect Fortran's column-based syntax as described earlier in section 3.1. This process is still time-consuming. Therefore, investigating how this could be achieved automatically through the so-called few-shot and chain-of-thought approaches, especially with the more flexible multi-agent approach is currently under study."}, {"title": "4. RESULTS AND DISCUSSIONS", "content": "In this section, we present the results from applying AutoFLUKA on two used cases; the general example treated in the FLUKA manual (Ferrari et al., 2024) and the domain-specific case of detector development in Micro-dosimetry based on this ongoing research (Ndum et al., 2024). These results were obtained from the Single-Agent framework."}, {"title": "4.1. AutoFLUKA on the General Example", "content": "This FLUKA example scores the pion fluence inside and around a proton-irradiated Beryllium (Be) target. The geometry of the setup is shown in Figure 4. It consists of a mono-energetic proton beam of momentum 50 GeV/c (gigaelectronvolts per unit of the speed of light) at 50 cm from two slabs of Be, labelled regBe3 and regBe4 standing for region 3 and region 4 respectively. The entire system is placed in a vacuum (region 2 or regVa2 as shown). The beam impinges from the left and crosses to the right, while interacting and generating secondary particles (pions) within each region. The boundary crossing fluence and current from region 3 (regBe3) to region 4 (regBe4), the pion fluence in and around the target and other quantities are scored as represented in Table 5. In this case, AutoFLUKA was asked to create five (05) input files corresponding to five statistically independent input cycles (i.e., formatted with the same user-defined parameters) but with different seeds. The complete execution sequence of the workflow is reported in Tables 6, 7, 8 and Figure 9.\nFirst, as shown in Table 6, the Input is made up of two parts; (i)-\u201cSystem\u201d or System Message which has been hard-coded into the agent's initialization scheme using LangChain's \u201cChatPromptTemplate\" method; and (ii)-\u201cHuman\" or Human Message (user query) which in this case is a prompt containing the instructions given to AutoFLUKA to complete the task. It is important to note that certain keywords and action phrases can significantly alter the agent's behaviour, thus, careful crafting using prompt engineering techniques was done to arrive at this current working version of the prompt. A stepwise approach with keys like complete file directories and file names was implemented to reduce hallucinations. Also, through this, AutoFLUKA can easily be prompted to go back and repeat certain steps in the workflow depending on its current observation of the output from other tools."}, {"title": "4.2. AutoFLUKA's RAG Tool as a Virtual Assistant", "content": "In this section, we demonstrate AutoFLUKA's ability (using our inbuilt RAG tool) to respond correctly to domain-specific questions, thus helping the FLUKA user to resolve errors within their code (input file) and to correctly post-process the simulation results. This domain-specific case is the \u201cdetector design and optimization for radiation quality measurements in Microdosimtry\u201d, which will also be presented in the next section.\nThe RAG tool developed here is scalable and can take multiple PDFs. Specifically designed to handle scientific documents with mathematical expressions, this tool combines the PyMuPDF and PyPDFLoader Libraries to extract text from documents. It also leverages Optical Character Recognition (OCR) via the PyTesseract library to extract text from images that might be"}, {"title": "4.2.1. Spikes in the DETECT Card Spectra", "content": "This error resulted during the early stages of the detector development research due to wrong settings of the Physics (production and transport) cards. After exchanges with the FLUKA expert which took several days, the correct settings were implemented. Also, cards not necessary for the simulation were removed from the code and MULSOFT card which is needed to activate single scattering in thin layers (low density gases) was also included resulting to good results as shown in Figure 7. AutoFLUKA was able"}, {"title": "4.3. AutoFLUKA on Microdosimetry", "content": "In this case study, we demonstrated AutoFLUKA's ability to automate the simulation workflow related to the \u201cDesign and optimization of an Avalanche-Confinement Tissue Equivalent Proportional Counter (AcTEPC)\u201d.\nThe motivation behind this research stems from the fact that the same absorbed dose from different fields of ionizing radiation (IR) can produce different effects on biological targets due to the random nature of radiation interaction with matter. Such interactions are currently considered to be the starting point of radiation-induced damage (Rucinski et al., 2021), which, if left unchecked can become carcinogenic. The same interactions are also exploited to treat cancer and other illnesses in radiation therapy. Therefore, to ensure efficacy while reducing side effects in radiation therapy as well as designing better radiation protection and monitoring systems, radiation detectors capable of precisely measuring such interactions, (i.e. the local energy depositions and ionization clusters in subcellular tissue targets like DNA strands and chromatin fibres) which are important inputs to calculating the radiation quality are needed. Unfortunately, most conventional dosimeters can only measure the absorbed dose, which is a macroscopic averaged quantity that disregards these stochastic effects.\nThrough a series of numerical studies, mathematical formulations and MC simulations, we demonstrated that our detector which employs an un-"}, {"title": "5. CONCLUSION", "content": "In this study, we successfully demonstrated the application of AI agents to automate Monte Carlo simulation workflows in FLUKA. Traditionally, the complexity of Monte Carlo simulations involves numerous parameters, a steep learning curve, and extensive manual intervention. These factors, in turn, lead to cumbersome processes that are time-consuming, prone to human error, and often require a significant investment of both time and computational resources. AutoFLUKA addresses these challenges by streamlining and automating key aspects of the workflow, thereby mitigating common issues such as human error, loss of execution time, and the inefficient use of computational resources.\nThrough a carefully crafted sequence of prompts, AutoFLUKA enables users to modify FLUKA input files, execute simulations, and effectively manage post-processing tasks, all with a single command. This not only saves time but also ensures the precision of the simulation by reducing the possibility for manual errors. Furthermore, the incorporation of tools to decrypt and store output data in structured JSON format facilitates easier downstream analysis and integration, enhancing both the usability and utility of the results.\nOne of the significant achievements of AutoFLUKA is its capability to automate the generation of multiple statistically independent input files. This feature is particularly valuable for scenarios where reducing simulation time is crucial while maintaining good statistical quality. AutoFLUKA also provides users with flexibility in modifying critical parameters, ensuring that specific configurations can be tested efficiently without manual file duplication and editing.\nMoreover, the introduction of the FLUKA assistant tool based on RAG shows promise in transforming user experience. By augmenting the AI model with domain-specific data, such as the FLUKA user manual and expert discussions, AutoFLUKA can assist users by addressing straightforward queries related to simulation setup, output analysis, and error troubleshooting. This feature can significantly reduce the dependency on the FLUKA user forum, potentially saving both time for users and workload for FLUKA experts.\nFuture work will focus on refining the capabilities of AutoFLUKA by extending the RAG tool to handle a broader range of data, including figures and graphs, which are often crucial for technical documentation. Further improvements in the multi-agent framework are also envisioned, potentially automating more complex human interventions such as ensuring adherence to input file formatting requirements. With these advancements, AutoFLUKA could serve as a versatile tool for any simulation software that relies on text-based input files, opening new avenues for research and development in nuclear science and engineering. This approach can be applied to any simulation software with a text-based input file system, such as the Monte Carlo N-Particle (MCNP) code, Particle and Heavy Ion Transport Code System (PHITS), etc."}, {"title": "Step 1: Calculate the Average Energy and Uncertainty from DETECT Card", "content": "Use the recorded spectra to calculate the average energy. Ensure an appropriate Eminimum is set. Calculate the average energy (E) and its uncertainty \u03c3\u03b5\u00b7"}, {"title": "2. Convert to Lineal Energy", "content": "Lineal energy is the energy deposited per unit length. Calculate lineal energy (y) as:\ny = E / \u00ce"}, {"title": "3. Propagate the Uncertainty", "content": "The uncertainty in lineal energy \u03c3y is propagated as:\n\u03c3\u03c5 = \u03c3\u03b5 / \u00ee"}, {"title": "Follow-up Question", "content": "What formula is used to calculate the average energy above?"}, {"title": "AutoFLUKA's Response", "content": "The formula for average energy (E) is:\n(E) = (\u03a3i=1N Ei Ci) / (\u03a3i=1N C)\nWhere: Ei is the midpoint of the i-th energy bin. Ci is the count in\nthe i-th energy bin. N is the total number of bins.\nSteps to Calculate Average Energy:\nDetermine the Midpoint of Each Energy Bin:\nEi = (LBi + UBi) / 2\n2. Calculate the Weighted Sum of Midpoints:\n\u03a3i=1N Ei Ci\n3. Calculate Total Counts:\n\u03a3i=1N Ci\n4. Compute the Average Energy:\n(E) = (\u03a3i=1N Ei Ci) / (\u03a3i=1N Ci)"}, {"title": "1. Calculate Midpoints", "content": "E\u2081 = (0.0+ 0.1) / 2 = 0.05\nE\u2082 = (0.1 +0.2) / 2 = 0.15"}, {"title": "2. Calculate the Weighted Sum of Midpoints", "content": "\u03a3i=1N Ei \u00b7 C\u2081 = (0.05\u00b710) + (0.15 \u00b7 20) = 0.5 + 3.0 = 3.5"}, {"title": "3. Calculate Total Counts", "content": "\u03a3i=1N Ci = 10 + 20 = 30"}, {"title": "4. Compute the Average Energy", "content": "(E) = 3.5 / 30 = 0.1167"}, {"title": "QICRU Radiation Quality Factor (QICRU)", "content": "ICRU Radiation Quality Factor (QICRU):\nQICRU(y) = 5.60 \u00d7 10\u207b\u2075y\u00b2 (1 \u2013 e\u207b\u2070.\u2075\u00d7\u00b9\u2070\u207b\u2074y \u2013 2 x 10\u207b\u2076y)"}, {"title": "Mean Quality Factor (QICRU, avg)", "content": "QICRU, avg = (\u222b QICRU(y)d(y)) / (\u222b d(y))"}, {"title": "Frequency Distribution f(y) and Dose Distribution d(y)", "content": "Frequency Distribution f(y):\nf(y) = Ci / (\u0394y \u03a3 Ci)\n2. Dose Distribution d(y):\nd(y) = (yf (y)) / YF"}, {"title": "Mean Lineal Energies", "content": "Average Lineal Energy:\ny = \u222byf(y)dy"}, {"title": "Dose-Mean Lineal Energy", "content": "YD = (\u222by\u00b2 f(y)dy) / (\u222byf(y)dy)"}, {"title": "Follow-up Question", "content": "Now, assuming that the average quality is calculated as a summation rather than an integral like so:\nQICRU, avg = (np.sum(Qi * di)) / (np.sum(di))\nand your knowledge about uncertainty propagation, GIVEN IN THE DOCUMENTS: C:/Users/.../PDFs_Temporal, please derive the expression for the uncertainty of the average quality factor.\nPlease note the following: The ONLY uncertainties present are that from the raw counts Ci and the frequency-mean lineal energy yF. To do this, first start from the known uncertainties (\u03c3\u03c4\u2081 and \u03c3yr) then: 1) Calculate the uncertainty of the frequency distribution (of\u2081). 2) Propagate it to the uncertainty of the dose distribution (\u03c3\u03b1\u2081). The lineal energy (y) has NO uncertainty. 3) Finally, propagate this to the average quality factor uncertainty.\nWhen done, please make a summary and provide a python script to automatically calculate this, given that oc; and oyf are given."}, {"title": "AutoFLUKA's Response", "content": "To derive the expression for the uncertainty of the average quality factor, we will follow the steps outlined:\nStep 1: Calculate the Uncertainty of the Frequency Distribution (of\u2081) The frequency distribution f(y) is given by:\nf(y) = Ci / (\u0394y \u03a3i Ci)\nThe uncertainty in f(y), denoted as of\u2081, can be calculated using the propagation of uncertainty formula:\n\u03c3\u03b5\u2081 = f(y)\u221a(\u03c3\u03b5 / Ci)\u00b2 + (\u03a3\u03c3\u03b5 / \u03a3C)\u00b2\nSince \u03c3\u03b5\u2081\u2081 is the combined uncertainty of all counts:\n\u03a3 = \u221a\u03a3\u2081 (\u03c3\u03b5)\u00b2"}, {"title": "Step 2: Propagate to the Uncertainty of the Dose Distribution (\u03c3\u03b1\u2081)", "content": "The dose distribution d(y) is given by:\nd(y) = (yf (y)) / YF\nThe uncertainty in d(y), denoted as \u03c3\u03b1\u2081, can be calculated using the propagation of uncertainty formula:\n\u03c3 d = d(y) \u221a\u221a(\u03c3f\u2081 / f\u2081)\u00b2 + (\u03c3y / y)\u00b2"}, {"title": "Step 3: Propagate to the Average Quality Factor Uncertainty", "content": "The average quality factor QICRU, avg is given by:\nQICRU, avg = (\u03a3i Qi di) / (\u03a3i di)\nThe uncertainty in QICRU, avg, denoted as OQICRU, avg, can be calculated using the propagation of uncertainty formula:\n\u03c3 QICRU, avg = \u221a\u03a3\u03b9 (\u2202 QICRU, avg / \u2202 di)\u00b2 \u03c3di"}]}