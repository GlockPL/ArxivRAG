{"title": "Can LVLMs and Automatic Metrics Capture Underlying Preferences of Blind and Low-Vision Individuals for Navigational Aid?", "authors": ["Na Min An", "Eunki Kim", "Wan Ju Kang", "Sangryul Kim", "Hyunjung Shim", "James Thorne"], "abstract": "Vision is a primary means of how humans perceive the environment, but Blind and Low-Vision (BLV) people need assistance understanding their surroundings, especially in unfamiliar environments. The emergence of semantic-based systems as assistance tools for BLV users has motivated many researchers to explore responses from Large Vision-Language Models (LVLMs). However, it has yet been studied preferences of BLV users on diverse types/styles of responses from LVLMs, specifically for navigational aid. To fill this gap, we first construct EYE4B dataset, consisting of human-validated 1.1k curated outdoor/indoor scenes with 5-10 relevant requests per scene. Then, we conduct an in-depth user study with eight BLV users to evaluate their preferences on six LVLMs from five perspectives: Afraidness, Nonactionability, Sufficiency, and Conciseness. Finally, we introduce EYE4B benchmark for evaluating alignment between widely used model-based image-text metrics and our collected BLV preferences. Our work can be set as a guideline for developing BLV-aware LVLMS towards a Barrier-Free AI system.", "sections": [{"title": "1 Introduction", "content": "While vision is a primary sensory modality for humans to perceive their environment for mobility or navigation, language is an essential medium for approximately 200 million Blind and Low-Vision (BLV) people worldwide. Although BLV users resort to other mediums, such as canes and guide dogs, there is an increasing demand for assistive AI technologies that can provide language-based descriptions. Whereas traditional detection-based AI systems help BLV users to detect and avoid specific obstacles/objects, they often fall short in providing the deeper context needed to navigate environments effectively. For instance, the user might want to not only be aware of surrounding obstacles, but they might also want to be provided with context-aware natural language-based description that conveys spatial information of objects or landmarks and other directional cues for navigation. To address this need, systems such as Be My Eyes, Aira, SeeingAI, and Sullivan A utilize human support or generative models to describe the scenes. Despite improvements in Large Visual Language Models (LVLMs), BLV navigation using LVLM-based systems remains challenging to be directly applied for practice use, integrated with external devices (e.g., GPS, touch sensor). To build reliable LVLMs for BLV individuals, the intuitive step is collecting extensive training data aligned with BLV preferences. However, due to the cost and fatigue of human experiments, it is challenging to collect large-scale data verified by BLV subjects. An alternative way is having an automatic BLV-preference-aligned metric that can be used as a criterion. Yet, there is a limited number of works exploring the preferences of unconventional groups of users, such as BLV, on different LVLM responses and whether existing widely used automatic metrics are sufficient in capturing the BLV preferences. Hence, our motivations are:\nMotivation 1: Necessity of unveiling BLV preferences on context-aware descriptions. Unlike past works that test a single LVLM response for the navigation or object finding task, we focus on exploring what types/styles of responses from different LVLMs that BLV users prefer. Since each LVLM produces uniquely styled responses, we employ five LVLMs shown to be capable of in-context learning and GPT-40 mini for response generation. To systematically analyze BLV preferences, we evaluate multiple in-context learnable LVLMs in five aspects spanning Afraidness, Non-actionability, Sufficiency, Conciseness, and Overall. Our analysis reveals the implicit preferences of BLV users on LVLMs, providing insight for designing real-time dynamic interactive systems.\nMotivation 2: Comprehensive benchmarking automatic metrics on BLV-aware datasets. Many metric-related works generally investigate how much CLIP/BLIP-based metrics align with human preferences on image-text pairs, assessing which image or text aligns better with the given reference text or image. While Context-Aware CLIP and IIT-DAS are metrics built in the perspectives of the BLV individuals, no prior work has been conducted on the model-based metric assessment of BLV preferences due to the lack of fine-grained BLV preference datasets. Our EYE4B benchmark fills this gap by introducing a novel evaluation dimension to a standard human correlation benchmark designed to test model-based metrics based on BLV user preferences.\nOur study advances BLV accessibility research by presenting the EYE4B benchmark from model perspective (Section 5) using our EYE4B dataset containing image-request-response (Sections 3) and BLV preferences (Section 4)."}, {"title": "2 Related Works", "content": "Prior datasets related to BLV individuals, such as VizWiz and BIV-Priv-Seg collect images taken from BLV users. While this approach provides valuable insights, these images are often low quality, degrading test performance in models. We build upon high-quality existing outdoor sidewalk, and indoor scene datasets. Although  are relevant, we mainly focus on collecting datasets with images taken in South Korea due to the BLV recruitment challenges.\nCompared to detection-based AI systems, focusing on vision-centric tasks like object detection, semantic segmentation, depth estimation, or surface masking, there are limited number of semantic-based systems. Our EYE4B dataset contributes to the collection of semantic-based datasets by extending the previous datasets with additional metadata of possible BLV user requests. However, while semantic-based datasets for BLV individuals deal with visual question-answering tasks, where responses are either 'correct' or 'incorrect,' our dataset differs by collecting fine-grained preferences of BLV users."}, {"title": "2.1 BLV-Aware Datasets", "content": "Large language models (LLMs) have expanded their capabilities beyond natural language to multiple modalities, bringing significant advancements in LVLMs. LVLMs, enhanced with in-context learning with prompting methods, demonstrate applicability to be integrated with applications for BLV users. Be My Eyes is the first BLV-aware application in collaboration with OpenAI, and Zhang and Ochiai, 2024 introduces an interface for BLV users to access object information using LVLMs. The most related work, WalkVLM, is designed to support the BLV user navigation in cities such as Beijing ."}, {"title": "2.2 LVLMs as BLV Assistance", "content": "Before extracting BLV preferences from LVLM responses and exploring how much model-based metrics align with BLV preferences, we first curate description-based BLV-aware datasets. We first collect a number of visual scenes (corresponding to BLV viewpoints) and possible requests. This stage is illustrated as step 1 in Figure 2, and more details can be found in Appendix A.1."}, {"title": "3 EYE4B Dataset Construction", "content": "We curate a diverse collection of indoor and outdoor visual scenes from the existing SideGuide and SideWalk datasets. Since SideWalk is a video-based dataset, we randomly sample one picture from each video. We repeat the process of randomly sampling and manually filtering until we reach more than 250 images from 350k and 100k images with bounding box annotations and polygon masks from SideGuide, and 1.2k outdoor and 296 indoor scenes from SideWalk. Specifically, we only leave images that (1) are not excessively blurry and too dark that sighted humans cannot perceive, (2) contain at least more than five detectable objects, and (3) are taken from perspectives of humans on the sidewalk, not in car vehicles, discarding many pictures showing highways from SideWalk. The two authors iterate through four rounds of this filtering process until both agree on using the images, resulting in approximately 1,150 visual scenes that include 300 images of bounding boxes and polygon masks independently, and 271 outdoor and 281 indoor images."}, {"title": "3.1 Visual Scene Collection and Filtering", "content": "We generate a list of possible requests for filtered visual scenes using GPT-40 mini, and validate the correctness of generated requests with sighted human participants, similar to Merchant et al., 2024. Specifically, we prompt GPT with 3-shot examples, requesting the model to generate 5 to 10 requests or requests that are related to the navigation of BLV users (more details in Appendix A.2). All the generated requests are then reviewed by 24 sighted human annotators. The reason why we have used sighted annotators is to check the relevancy of the request on the visual scene images to ensure their validity in terms of two perspectives: (1) quality itself and (2) relevance with the corresponding visual scene images. For each image-request pair, each annotator decides whether each request (out of 5-10 generated requests) is valid. For example, the annotator has to exclude requests that are not aligned with navigational goals, such as \"Explore the bushes\" and \"Check the shelf.\" If they answer more than three no's, the annotator proposes their requests. All the image-request pairs are evaluated with two annotators, reaching the agreement level of 62.33% and 68.97% ratio of yes/no per annotator. This process results in 4,979 requests with an average of 4.32 (STD: 1.42) requests per image."}, {"title": "3.2 Request Generation and Preprocessing", "content": "The final step of EYE4B data construction is to generate responses for the 4,979 image-request pairs. We use five open-source LVLMs, including LLaVA-1.6 , Qwen-VL/Qwen-VL-chat, InterNLM-x2, OpenFlamingo that are known to exhibit in-context learning ability and one close-source model, GPT-40 mini. We first generate responses with a 3-shot prompting method on open-source 7B models. Then, the responses generated by 7B models are refined using GPT-40 mini based on the assumption that large models could improve the accuracy of responses, but later, we find that even the refined responses sometimes still include hallucinations, such as non-actionable actions (e.g., look up) or inaccurate directions. Since our goal is to collect the preferences of BLV users over diverse styles/structures of responses generated by different models, we consider all these six models in preparation for the BLV user study."}, {"title": "3.3 LVLM Response Generation", "content": "In this section, we investigate the underlying preferences on our constructed EYE4B dataset. We conduct two rounds of experiments to explore the preferences of BLV users on LVLM responses with different styles. The second round is performed based on feedback from the first round. The details regarding the human experiment participants and protocols are explained in Appendix A.2."}, {"title": "4 BLV Preferences on LVLM Responses", "content": "To evaluate various model responses with BLV users, we ask six BLV participants to assess two types of generated responses - one from 7B models and the other from GPT-40 mini. To ensure the interview quality, we manually discard any low-quality responses and provide only responses aligned with paired images and requests. This is because we observe that even responses of GPT-40 mini often contain hallucinated objects and inaccurate spatial cues (e.g., direction and depth) despite detailed instructions. Hence, our experiment focuses on finding the BLV preferences over different LVLM-generated responses. Based on previous works related to mobility tasks for BLV individuals, we collect evaluation scores in terms of five perspectives: 1) Afraidness, 2) Non-actionability, 3) Sufficiency, 4) Conciseness, and 5) Overall for each response as illustrated in step c of Figure 2. It should be noted that the lower scores indicate better ratings for the first two categories and vice versa for the last three categories.\nTo ensure the safe navigation of BLV users, we set Afraidness and Nonactionability as core indicators of the benchmark. Given that the responses serve as actionable guidelines for navigation, Sufficiency, and Conciseness are chosen to capture the balance of informativeness and brevity. Finally, the Overall category reflects the user's general satisfaction. The collected BLV preferences extend our EYE4B dataset (from Section 3) to be further used for EYE4B benchmark in Section 5."}, {"title": "4.1 Study Objective and Design", "content": "To ensure that each BLV user justifies their scoring instead of randomly scoring and to allow them to become accustomed to the experiments, our user study consists of two stages, inspired by the sectional analysis framework developed by Fereday and Muir-Cochrane, 2006: (1) an interview procedure where each user needs to justify their assessments on every sub-question and (2) an annotation process where each user only needs to assess, without commenting justifications. We evaluate 10 image-request pairs (one from 7B models and the other from GPT-40 mini) for the first and 10 to 13 for the second stages for each annotator.\nFigure 3a illustrates the score of the five criteria, where we observe low scores for Afraidness and Nonactionability and high scores for Sufficiency, Conciseness, and Overall. Compared to the first two categories, the variances of scores in the Sufficiency and Conciseness are high, indicating that each BLV user has different perspectives on whether the response is sufficient or concise. Additionally, Figure 3c shows Pearson's correlations between all combinations of categories. Notably, Afraidness shows a relatively high correlation with Nonactionability, suggesting that the exclusiveness of non-actionable contexts can lessen the Afraidness level. Sufficiency and Nonactionability are the first and second crucial factors in determining the overall scores, showing correlation coefficients of 0.33 and 0.31. While these quantitative analyses give clues to the preferences of BLV users, our interviews with BLV participants offer deeper, qualitative insights. We summarize the results in four points.\n(1) Structured Response for Clarity Several participants emphasized the importance of structured responses such as template formatting. P1 highlighted that \"responses with the format of initial general overview followed by detailed guidance have been particularly helpful for understanding complex requests; however, it would be more helpful if the response has clear, structured templates.\", and P5 similarly argued \"We need consistent formats for the response.\" Most BLV users denoted that the inconsistency of the response formats across samples made it difficult to follow some responses, which motivated the authors to adopt a unified template across samples with diverse contexts in the following round of the BLV experiment."}, {"title": "4.2 The First Round of BLV Experiment", "content": "Current LVLMs lack in providing fully satisfactory responses to BLV users.\n(2) Concrete Directional and Distance Cues Since our task focuses on actionable and navigational tasks, directional and distance cues are the essential elements. P2 emphasized that \"incorporating concrete directional cues, such as moving to the 1 o'clock position and indicating specific distance metrics, such as steps or meters, are very helpful.\" However, while participants largely agreed on the importance of including concrete directional and distance cues for better guidance, preferences varied regarding the terminology used for measurement, such as step-wise vs. meter-based instructions. While P2 preferred the more intuitive \"steps,\" P5 preferred the more objective \"meters.\" This feedback underscores the need to integrate the different needs of BLV users since they do not always agree on the preferred response (Figure 3b), encouraging us to use either measurement appropriately. Later in our final generation process, we adopt meters for distance measurements in scene descriptions and step-wise instructions for the step-by-step action guidance. We discuss more improvements in providing accurate directional cues for LVLMs in Appendix A.4."}, {"title": "4.3 The Second Round of BLV Experiment", "content": "To incorporate the main feedback from the first experiment conducted with BLV users, which is to make the response with consistent format and concise, we perform another round of the BLV user experiment with a new set of responses generated using different prompts and few-shot samples. We perform the second BLV experiment involving two annotators (P1 & P2) from the first round and two new annotators (P7 & P8) to avoid result overfitting to the BLV annotators from the first round. We distribute 48 image-request pairs (one from the preferred responses - before and the other from newly generated responses - after, all from the 7B models). Different from the first stage, we ask the users to mark only their preferences (either before or after) and add a brief explanation of the reason for their choice.\nUnlike our expectation, the responses created based on feedback from the BLV users in the first experiment are not always preferred in the subsequent round, as summarized in Table 1. Although the new responses are preferred by P2 with 91.56% out of the 24 questions, the remaining three annotators either show a slightly higher preference for the new contexts than the old contexts (P8) or prefer the old responses (P1 & P7). We delve into the qualitative reasons from three perspectives:\n(1) Trade-off between Detailed and Formatted Responses One of the main reasons why participants prefer the response B (before) over A (after) is that the former is explained in more detail (P2 & P7) using \"easy language\" (P7 & P1). However, the formatted responses generated with the new prompt based on feedback from the first round resulted in a lack of fine-grained details. P1 also argues that \"Response B is more detailed than A, but it is difficult to trust because it contains conflicting statements within the description.\" In addition, P8 states that \"There is too much unnecessary information in response A.\", which is not illustrated when describing response A. Hence, we conclude that although the response B might be preferred over A for its descriptive property, it can also negatively affect the users."}, {"title": "5 EYE4B Benchmark on Alignment of Metrics and BLV Preferences", "content": "After evaluating LVLMs regarding BLV preferences/judgments on our EYE4B dataset, we present EYE4B benchmark. We investigate how much various automatic metrics can capture BLV judgments. Specifically, we evaluate to what extent various pre-trained automatic image-text evaluation metrics that show high correlations with general human judgments on general image captions can correlate with BLV judgments.\nMost automatic metrics show relatively strong correlation performances across general datasets such as PASCAL, FOIL, Flickr-Exp/CF, and Polaris that include fine-grained human judgments. To construct an image-to-text dataset with characteristics more similar to our EYE4B dataset but in a general domain, we construct Polaris* (i.e., Preference-based Polaris) and OID by extracting positive or negative texts per image based on the annotated alignment scores within a 0-1 range (scores below 0.5 are categorized as negative, while those 0.5 or higher are labeled as positive).\nOur EYE4B benchmark extends the general benchmark of human judgment by including BLV preference judgments on LVLM responses given the visual scene image and request. Based on the feedback from BLV users, which reveals that selecting a clear preference between responses is often challenging, and due to the limits in the score-annotated test data size (n = 98), we select Kendall \\(T_c\\) for the evaluation metric.\nExisting metrics are not tuned to align with BLV judgments."}, {"title": "6 Discussion", "content": "AI technologies have the potential to broaden the accessibility for BLV individuals. However, current AI-based assistance tools primarily function as simple QA systems, lacking the capability to generate structured guidance essential for BLV navigation. It is also important that AI-based assistance tools adapt to user needs. Unlike conventional navigation datasets, often dominated by road-centric images or simple QA captions irrelevant for pedestrian navigation, our dataset is specifically designed to capture the BLV preferences on natural language-based descriptions for assistance in mobility scenarios. By fully integrating BLV perspectives into dataset construction, we introduce a new paradigm for AI-driven accessibility research, setting a foundation for the LVLMs that generate actionable, contextually relevant guidance rather than listing isolated fact-based descriptions."}, {"title": "BLV Perspectives on AI Technology", "content": "Despite the remarkable generative capabilities of LVLMs, their reliability in producing accurate and logically structured navigational instructions remains a critical limitation. Based on our interviews with BLV individuals, most participants stated that they could not solely rely on AI technology due to its lack of ability to provide consistently accurate descriptions. Furthermore, these models fail to reflect the BLV preferences, leading to responses that lack real-world applicability. To address these shortcomings, our EYE4B dataset consists of verified requests from human annotators for constructing relevant scenarios on paired images and evaluation of multiple LVLM responses for capturing the implicit preferences of BLV users."}, {"title": "Reliability of LVLMs", "content": "One of the reasons why current CLIP/BLIP-based metrics might not be sufficient to incorporate BLV user judgments is that these models are not tuned to capture the instructions. Since each text sample (either request or response) in our dataset usually contains more than 248 tokens, these metrics cannot properly encode the long contexts and catch the subtle differences between BLV-preferred and BLV-non-preferred responses unlike the datasets consisting of sighted-human judgments on image-text pairs. Our EYE4B benchmark raises critical questions regarding the adequacy of model-based metrics and underscores the need for developing context-aware automatic evaluation methods tailored to both general and BLV audiences. We leave future work to develop metrics that can capture the instruction-driven nature of BLV-aware datasets."}, {"title": "BLV-Aware Metrics", "content": "This paper addresses the critical need for reliable visual context generation tailored for BLV individuals due to the limitations of current LVLMs in this domain. Our EYE4B benchmark evaluates LVLM performances, addressing a significant gap between automatic evaluation metrics and BLV preferences. This suggests the importance of incorporating BLV users to build barrier-free LVLMs. As a future study, we plan to develop automatic metrics that can better predict both sighted and BLV human preferences that can be used for training barrier-free LVLMs. We believe our work can advance the field toward enhancing AI usability to tailor the needs of BLV individuals."}, {"title": "7 Conclusion", "content": "While our current study focuses mainly on image-based visual scenes, we could extend the work to generating LVLM responses on video-based visual scenes. The limited number of evaluation data sizes and mobility scenes from a single country is due to the difficulty in recruiting a number of international BLV users. Exploring how we can provide LVLM responses, either with one-way or conversational auditory cues, is another challenging part that we leave as future work to deal with."}, {"title": "8 Limitation", "content": "The proposed dataset contains responses from GPT-40 mini, which could have unintentional potential risks in the initial stage of the data construction process. However, human users have confirmed and validated all the released data."}, {"title": "9 Ethical Statement", "content": "We clarify that all the datasets we use are open-source and for training/evaluation model purposes, and the corresponding citations are included in the Reference section."}, {"title": "A Appendix", "content": "We outline the details of the construction of our final 4,979 request datasets. First, we prompt GPT-40 mini ($0.15/1M input tokens) using the prompt stated in Table 3. We also provide the model with 3 few-shot examples provided in Table 4 for outdoor and Table 5 for indoor environmental visual scenes. We set the temperature to 0.0, the maximum tokens to 300, and the response format to 'list' for the hyperparameters. This stage results in a total of 8,149 requests for 1,150 images, resulting in 7.09 requests per image. Next, we filter these raw requests using 24 human annotators, further illustrated in Appendix A.2, resulting in 4,979 image-request pairs."}, {"title": "A.1 Dataset Construction Details", "content": "The few-shot samples we use for prompting the five-open source LVLMs are in Table 7 for outdoor and Table 8 for indoor visual scenes. After the generation using these 7B models, we prompt GPT-40 mini to enhance the response using the system prompt (before) in Table 6 and few-shot examples in Table 9 for outdoor and Table 10 for indoor scenes. Then, a randomly sampled response from one of 7B models and GPT-40 mini construct a pair for use in the BLV user evaluation (Appendix A.2). All the system prompts, few-shot examples, and generated responses for our second prompts described in Section 4.3 are in Tables 6, 7 and 8, and 11."}, {"title": "A.2 Human Experiment Details", "content": "To filter the requests generated by GPT-40 mini and verify their relevancy with the corresponding images, we recruited 24 sighted human annotators from the school community after the study design had been approved by the Institutional Review Board (IRB). We distributed the instructions (Table 12) to all the annotators who consented to participate in our task and agreed on the 50k KRW compensation (~ 34 USD in January 2025). Depending on the participant, the human experiment took < 1 to 2 hours (leading time in Figure 4).\nThe number of newly added captions and the proportion of 'yes' selected over all the samples depend on each participant (the second and third plots in Figure 4). We make sure that the shorter leading time does not indicate a lower number of new captions but is correlated with the 'yes' ratio (Figure 5), showing that most human annotators followed our guideline. The survey is distributed using Label Studio Interface (Tkachenko et al., 2020-2022) (sample screenshot in Figure 7). The resulting valid request proportions per set (a total of 4,265) are illustrated in Figure 6.\nFor the postprocessing, the authors go through two more rounds of the verification process: (1) filtering requests that include only one 'yes' annotation (Figure 8) and (2) filtering captions added by the human annotators (Figure 9), where all these stages need to be verified with a consensus of both two authors. 74 (out of 137; 54%) and 578 (out of 935; %62) requests are selected, resulting in 652 (out of 1072; 60%) newly added verified requests. In addition to these 652 requests, the authors additionally go through one more process with 12 images with no requests, resulting in the addition of 62 requests. Thus, the total number of verified requests that form our EYE4B Benchmark is 4,979 (= 4,265 + 652 + 62).\nThe first round of the BLV user experiment lasts 1 to 1 hour and 30 minutes for each participant (sample screenshot in Figure Figure 10). All eight participants (demographic information listed in Table 13 are compensated with 50k KRW (~ 34 USD in January 2025). The lasting hours and the compensation for the second round of the BLV user experiment are the same as the first. The difference between the two rounds is whether the experiment is offline, conducted with an interview, or online using a screen reader. Since we evaluate using the subquestions for each image-request pair in the first round, the averaged score refers to the evaluation score averaged across four categories-Afraidness, Nonactionability, Sufficiency, and Conciseness, except for Overall (Figure 11). The overall scores for each participant are illustrated in Figure 12."}, {"title": "A.3 Additional Related Works", "content": "Automatic evaluation metrics for image-text pairs can be divided into reference-based and reference-free approaches. The reference-based approaches require ground-truth texts (i.e., references) corresponding to the images, unlike reference-free metrics. Reference-based approaches often show better correlations with human judgments than reference-free methods, with the cost of additional annotation of text (or caption) on images. For example, RefCLIP-S, RefPAC-S, and Polos not only calculate the similarity between image and generated text, but they also utilize the ground-truth text (i.e., reference).\nIf references are unavailable or do not exist, reference-free metrics become the only viable options (e.g., CLIP-S, PAC-S and Reference-free version of Polos (without ROBERTa) (Wada et al., 2024)). Since BLIP-based metrics often correlate better with human judgments, BLIP-based metrics could also be opted. is trained to rank the quality of the generated images as similar to the human ratings. In the case of our study, we postulate that there is no single ground-truth text for BLV users corresponding to the image.\nThere are also a few image-text metrics that are tuned to BLV-related tasks. Context-Aware CLIP captures how much the description is related to the context (webpage in this work) and image without a context (i.e., image - context). IIT-DAS aims to assign higher scores to image-description than image-caption pairs by fine-tuning CLIP on the Concadia dataset. Similarly to our work, Zur et al., 2024 claims that description (or response in our case) helps the imaginability of the BLV users more than the caption with the role of complementing the image."}, {"title": "A.4 Additional Discussion Point", "content": "Although clearly instructed as in Table 6, we notice LVLMs tend to understand the clockwise direction in the perspective of the image itself, not the viewpoint. For example, although we instruct them to choose one of the options of 9 to 3 o'clock for the direction, LVLMs sometimes provide \"8 o'clock\", which corresponds to \"10 o'clock\" from an image viewpoint perspective. This suggests that LVLMs are primarily trained on image-caption datasets that describe positional relationships within the image itself, rather than recognizing depth and directional cues from a specific viewpoint. This limitation underscores the critical need for datasets designed to prompt models to interpret and generate descriptions from a defined user perspective."}, {"title": "Direction Accuracy of Generated Responses", "content": "When building ImgTxtREW-S in Table 2, we use the filtered training, validation, and test sets of the Polaris datasets, which consist of a human score ranging from 0 to 1 (0.00, 0.25, 0.50, 0.75, 1.00) for an image and candidate text, along with five reference texts. The filtering process involves selecting candidates that score less than 0.5 for the assigned human score, resulting in data sizes of 22,803, 30,461, and 38,076 for training, validation, and test datasets. Due to the extensive computational cost of training from scratch, we fine-tune ImgREW-S, setting the hyperparameters as follows: 1 epoch, batch size of 64, accumulation steps of 4, learning rate of 1e-05, learning rate decay style as cosine, and model parameter fix rate of 0.7 (hyperparameter tuning in Table 14). The model checkpoint with the lowest validation loss is saved as the best. The major difference between ImgREW-S and ImgTxtREW-S is how we encode the pair: While ImgREW-S uses positive and negative image candidates per text, ImgTxtREW-S uses positive and negative text candidates per image. Thus, our final loss objective is as follows:\n\\(L(0) = \u2212E_{(I,t_i,t_j)\\sim D} [log (\u03c3 (f_\u03b8(I, t_i) \u2013 f_\u03b8(I,t_j)))]\\) (1)\nIn the above equation, I, t, D, \\(f_\u03b8\\) indicate image, positive/negative text, batch, and preference model. This loss is optimized to learn a reward, a difference between the preference model encoded (image, positive text) and (image, negative text)."}, {"title": "A.5 Training and Implementation Details of New Metrics", "content": "You are a request writer. Given an image, your task is to generate 5 to 10 requests related to actions that blind or low-vision (BLV) people can perform. Each request must describe specific, actionable tasks in a detailed and structured manner. The focus should be on mobility, particularly actions related to safe movement, object manipulation, or accessing information that BLV individuals can perform within the context of the scene. The requests should not overlap with each other but be diverse, detailed, and read recognized texts. Please do not mention an object or person not detected in the image, and refrain from using unclear or useless verbs such as organize, explore, navigate, locate, feel, check, and gather information. Do not include color and auditory information. The output should be properly formatted as a list containing 5 to 10 requests."}, {"title": "B Visualization Credits", "content": "Table 3: System prompts we use to prompt GPT-40 mini for request generation\nTxtBLIP-S is built similarly to ImgTxtREW-S except for how we encode the image-text pairs. Whereas ImgTxtREW-S uses a preference model that inputs image embeddings as the encoder hidden states, TxtBLIP-S is trained using image and text projection layers. In other words, the function f in Equation 1 equals the cosine similarity between image and text features extracted from image and text projection layers. Also, we observe high overfitting when developing TxtBLIP-S; hence, we lower the learning rate to 5e-7. In addition, unlike ImgTxtREW-S, TxtBLIP-S is fine-tuned on pre-trained BLIP-S, not ImgREW-S. We show the effect of the learning rate on two datasets when building TxtBLIP-S in Table 14. Although TxtBLIP-S trained with the learning rate of le-5 shows higher performances than BLIP-S in several datasets, such as Polaris* and ImgREW, it shows unstable performances among other datasets, including PASCAL, FOIL, FlickrExp, and Polaris, which is the reason why we show the results of TxtBLIP-S with the learning rate of 5e-7 in Table 2.\nWe train Eye4B-S using our training dataset generated by the filtered image-request pairs. We construct the training dataset using positive texts from LLaVA-1.6 , Qwen-VL, and InterNLM-x2 responses, and negative text from GPT-40 mini, based on the BLV preference results obtained from the first round of the BLV experiment (Appendix A.2). The resulting training, validation, and test data sizes are 11.2k, 1.4k, and 1.4k. We emphasize that there is no overlapping between the test data we used for evaluation during training and the Eye4B shown in Table 2. The training configurations and procedures are the same as building TxtBLIP-S. All the training and evaluation experiments are conducted using a single NVIDIA RTX A6000 and A4000, respectively.\nWhile non-reward-based models, such as CLIP-S and BLIP-S output the similarity value as the cosine similarity between normalized image and text embeddings, the final similarity score for the reward model based metrics - ImgREW-S , ImgTxtREW-S, and Eye4B-S is calculated using the scalar value of reward. This reward is an output value of the text features encoded with the final multilayer perception. The final score is the normalized reward value using the fixed mean and standard deviations (std)."}, {"title": "A.Labeling Guidelines for Requests of Blind or Low-Vision (BLV) Mobility", "content": "Introduction: The goal of this labeling task is to create a dataset that provides detailed and actionable descriptions of mobility requests related to blind or low-vision (BLV) users. The requests gathered will ultimately support BLV users in safely navigating and interacting"}]}