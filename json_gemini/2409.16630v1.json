{"title": "Stochastic Subsampling With Average Pooling", "authors": ["Bum Jun Kim", "Sang Woo Kim"], "abstract": "Regularization of deep neural networks has been an important issue to achieve higher generalization performance without overfitting problems. Although the popular method of Dropout provides a regularization effect, it causes inconsistent properties in the output, which may degrade the performance of deep neural networks. In this study, we propose a new module called stochastic average pooling, which incorporates Dropout-like stochasticity in pooling. We describe the properties of stochastic subsampling and average pooling and leverage them to design a module without any inconsistency problem. The stochastic average pooling achieves a regularization effect without any potential performance degradation due to the inconsistency issue and can easily be plugged into existing architectures of deep neural networks. Experiments demonstrate that replacing existing average pooling with stochastic average pooling yields consistent improvements across a variety of tasks, datasets, and models.", "sections": [{"title": "Introduction", "content": "Deep neural networks have demonstrated remarkable capabilities in a variety of fields, such as computer vision and natural language processing benchmarks [4, 20, 1]. With a large number of parameters, they are able to represent abstract and high-level patterns in data, which has led to significant improvements in modeling abilities. Despite their successes, the large number of parameters often becomes over-parameterized, which causes overfitting problems to the training dataset and thereby degrades the generalization performance [14, 21]. To avoid the overfitting problem, current practices in deep learning necessitate sufficient regularization methods, such as weight decay [40], normalization layers [16], and Dropout [34].\nDropout is a famous regularization method adopted for deep neural networks. Dropout turns off arbitrary neurons within a neural network during the training phase, which enables training of a subnetwork that is randomly sampled. During the test phase, the whole network is used for inference, which becomes an ensemble of all possible subnetworks. This ensemble behavior yields a regularization effect on the deep neural network, which alleviates overfitting problems.\nHowever, the recent practice of using batch normalization [16] raises a side effect from Dropout. Batch normalization expects consistent mean and variance in its input, whereas Dropout causes inconsistent variance during training and test phases [22]. In other words, the use of Dropout breaks the fundamental assumption of batch normalization, which degrades performance when used together. Reference [17] proved that this inconsistency problem cannot be avoided for any variant of the Dropout scheme but could be partially mitigated by adopting indirect means such as choosing a proper position for Dropout.\nIn this study, we explore a form of alternative module that achieves a Dropout-like regularization effect without introducing the side effect of the inconsistency issue. Here, we extend and generalize the recent operation of PatchDropout [26, 11, 24] as stochastic subsampling and merge it into average pooling with an adequate scaling factor. To this end, we propose stochastic average pooling,"}, {"title": "Method", "content": "The full list of mathematical notations used in this study is summarized in Table 1. Let x = [x_1, x_2,..., x_n] be a vector of length n. With a keep probability p \u2208 (0,1), Dropout randomly drops certain elements from the vector during the training phase, whereas it passes intact elements without any drop during the test phase [34]. This standard definition of Dropout can be described as\nDropout_{train}(x) := \\frac{1}{p}Mx,                                                                  (1)\nDropout_{test}(x) := x,                                                                                       (2)\nwhere M is an n\u00d7n diagonal matrix with m_{ij} = 0 for i \u2260 j and m_{ij} ~ Bernoulli(p) for i = j. Note that Dropout randomly converts certain elements in the vector into zeros, whereas others are left with 1/p scaling, yielding a vector such as [0, x_2/p,\u00b7\u00b7\u00b7, 0]. A Dropout-applied neural network behaves as an ensemble of its subnetworks, which acquires a regularization effect to avoid an overfitting problem. Since the introduction of Dropout, a keep probability such as p = 0.5 or p = 0.8 has been preferred in the research community [34, 33, 35, 3].\nInspired by Dropout, recent studies on vision transformers have employed its variant called Patch-Dropout [26, 11, 24]. In vision transformers [8], a 2D image is partitioned into patches to represent it as a sequence of features on patches, which is referred to as patch embeddings. PatchDropout randomly subsamples certain patches of patch embeddings in the early stage of the vision transformer during the training phase. Removing other unchosen patches saves computational resources, thereby speeding up the training of the vision transformer. During the test phase, PatchDropout passes the patch embeddings without any subsampling. Indeed, several pieces of literature [26, 11, 24] reported that by adopting a low keep probability such as p = 0.5 or p = 0.25, applying PatchDropout into a vision transformer leads to two to three times faster training speeds while maintaining accuracy."}, {"title": null, "content": "However, PatchDropout has only been used in this limited scenario of subsampling the patch embeddings of a vision transformer. To extend and generalize the usage of PatchDropout for general purposes, here we define stochastic subsampling (SS) as\nSS_{train}(x) := [x_i]_{i\u2208S_p},                                                                          (3)\nSS_{test}(x) := x,                                                                                           (4)\nwhere a set S_p consists of element indices of a randomly selected subset from [1, 2, \u2026\u2026, n] in length np without duplication. The random indices change every time.\nAlthough PatchDropout\u2014or stochastic subsampling-may seem similar to Dropout, in fact, they differ in several points. For Dropout, the dropped elements are regarded as zeros, whereas PatchDropout simply removes them by subsampling. In a strict sense, PatchDropout is not a Dropout. To clarify the difference between them and generalize PatchDropout beyond its current usage for patch embedding, in this study, we use the term stochastic subsampling.\nIn the Dropout scheme, to alleviate decreased mean due to zeroed elements, a scaling factor of 1/p is employed during the training phase. This scaling ensures mean consistency during training and test phases: E[Mx] = E[x]. Nevertheless, the problem we claim is that zeroed elements by Dropout affect the mean of the squares for an output, i.e., the second moment:\nE[\\frac{1}{p^2} m_i^2 x_i^2] = \\frac{1}{p} E[x_i^2] > E[x_i^2],                                                                         (5)\nthereby causing increased variance during the training phase compared with variance during the test phase [22]. Reference [17] proved that Dropout cannot simultaneously satisfy both mean and variance consistency, even for other possible variants such as different choices for the distribution of M. The inconsistency in mean or variance breaks the underlying assumption of subsequent batch normalization: Though batch normalization expects to receive features of the same mean and variance during training and test phases, Dropout causes inconsistency in mean or variance. Owing to this problem, the use of Dropout with batch normalization may degrade performance and has been avoided in the research community [22, 16, 12]. Even though Reference [17] found that applying Dropout at a specific position partially resolves the inconsistency, they concluded that the inconsistency issue always exists in the Dropout scheme.\nIn contrast, we find that stochastic subsampling does not introduce zeroed elements (Figure 1), which guarantees consistency in both the mean and variance during training and test phases:\nE[SS_{train}(x)] = E[SS_{test}(x)],                                                                   (6)\nVar [SS_{train}(x)] = Var [SS_{test}(x)].                                                                (7)"}, {"title": null, "content": "These consistencies in mean and variance hold when sampling a subset from a vector with a sufficiently large size, such as a feature map in deep neural networks. Considering the consistencies in mean and variance, stochastic subsampling is a much safer choice to be deployed for deep neural networks with batch normalization, compared with Dropout.\nHowever, stochastic subsampling reduces the size of the vector from n into np during the training phase, whereas it maintains the vector size n during the test phase. Nevertheless, the architecture of a deep neural network may require a consistent size for an intermediate feature map. For example, a fully connected (FC) layer requires a fixed size of input vector, whereas stochastic subsampling changes the vector size during training and test phases. Therefore, stochastic subsampling cannot be deployed with an FC layer. Owing to its size reduction, stochastic subsampling has only been used in the limited scenario of the early stage of vision transformers as PatchDropout, and its general usage has been rarely studied up to now."}, {"title": "Proposed Method: Stochastic Average Pooling", "content": "The objective of this study is to design a module using stochastic subsampling to achieve a Dropout-like regularization effect as well as consistent vector properties such as variance and size. To cope with the reduced size of a vector, we exploit the current practice of using average pooling, which behaves as downsampling for image features. Here, we investigate a form of new module using average pooling that incorporates stochastic subsampling.\nWe first start with the standard definition of average pooling. Given a vector x = [x_1, x_2,\u00b7\u00b7\u00b7, x_n], the jth element from r-size 1D average pooling with strider is\nAP^r(x) := \\frac{1}{r} \\sum x_i,                                                                     (8)\nwhere i \u2208 {r(j \u2212 1) + 1, r(j \u2212 1) + 2,\u2026\u2026\u2026, r(j \u2013 1) + r} to cover the specified pooling size. Here, we find that average pooling conserves the mean but not the variance, owing to the decreased second moment\nE[AP(x)^2] \\backsim \\frac{1}{r^2} \\sum x_i^2 = \\frac{1}{r} E[x^2].                                                                    (9)\nIn other words, applying r-size 1D average pooling with strider decreases the second moment by 1/r, approximately. This behavior will be empirically verified in Section 2.3. Global average pooling (GAP) [25] corresponds to its special form of r = n, which decreases the second moment by a factor of 1/n. Similarly, (r \u00d7 r)-size 2D average pooling with strider \u00d7 r, denoted by AP^{r\u00d7r}, decreases the second moment by 1/r^2. In summary, pooling size r determines the decreased second moment after average pooling, which should be considered when incorporating stochastic subsampling.\nNow, we combine average pooling and stochastic subsampling. Firstly, we define a vanilla r-size 1D average pooling with strider, which is going to be used during the test phase. This average pooling reduces the size of a vector from n to n/r. For use during the training phase, we consider stochastic subsampling that is followed by another average pooling operation. Because the stochastic subsampling reduces the size of a vector from n to np, here we employ (rp)-size 1D average pooling with stride rp that reduces the size of a vector from np to n/r. Note that, though stochastic subsampling conserves the second moment, the subsequent average pooling affects the second moment. Specifically, (rp)-size 1D average pooling with stride rp decreases the second moment by a factor of 1/rp during the training phase, whereas r-size 1D average pooling with strider causes decreased second moment by a factor of 1/r during the test phase. To match second moments that differ by a factor of p, we claim to apply \u221ap scaling during the training phase. In summary, we propose a new module, stochastic average pooling (SAP), as\nSAP_{train}^r(x) := \\sqrt{p} AP^{rp}(SS_{train}(x)),                                                       (10)\nSAP_{test}^r(x) := AP^r (x),                                                                           (11)\nwhere p \u2208 (0, 1) corresponds to the keep probability of inner stochastic subsampling. Given a vector x with size n, this definition of stochastic average pooling consistently outputs a vector with a size of n/r during both training and test phases. Furthermore, the \u221ap scaling calibrates the decreased second moment due to different average poolings, thereby matching them during both training and"}, {"title": "Empirical Observation", "content": "Here, we empirically validate that applying \u221ap scaling in stochastic average pooling guarantees consistency in the second moment. We simulate a scenario of applying 2D global average pooling to a feature map x that is randomly sampled from N(0, 1). We used x \u2208 R^{N\u00d7C\u00d7H\u00d7W}, where we set the mini-batch size to N = 64, the number of channels to C = 256, and the spatial size to (H \u00d7 W) \u2208 {2^2, 4^2, 8^2, 16^2, 32^2, 64^2, 128^2, 256^2}. To obtain the behavior of global average pooling, the pooling size is set to be equal to the spatial size r \u00d7 r = H \u00d7 W. The keep probability is set to p = 0.5 here, and other different choices will be studied later. We measured the second moment after applying (1) stochastic average pooling during the test phase, i.e., global average pooling from Eq. 11, (2) stochastic average pooling during the training phase from Eq. 10, and (3) stochastic average pooling during the training phase from Eq. 10 omitting \u221ap scaling.\nThe results are summarized in Figure 5. We observed that the second moment after stochastic average pooling from Eq. 10 matched suitably with that after global average pooling from Eq. 11. If \u221ap scaling is omitted, then the second moment differed from that of global average pooling, which verifies the necessity of applying the \u221ap scaling. We emphasize that the different second moments arise from the distinct pooling sizes of the average poolings, whereas the stochastic subsampling conserves the second moment. We also observed that the second moment after global average pooling was in inverse proportion to the spatial size HW, which validates the decreased second moment after average pooling in Eq. 9."}, {"title": "Experiments", "content": "In this section, we experiment with numerous deep neural networks before and after replacing average pooling with stochastic average pooling. We target a variety of datasets, tasks, and models"}, {"title": "Replace GAP in Classifier Head", "content": "Firstly, we examine the performance differences when replacing the existing global average pooling in a classifier head with stochastic average pooling. We trained ResNet-{50, 110} [12] on a multi-class classification task using the CIFAR-{10, 100} dataset [19]. The CIFAR-{10, 100} dataset consists of 60K images of {10, 100} classes. For data augmentation, we used 32 \u00d7 32 random cropping with 4-pixel padding, a random horizontal flip with a probability of 0.5, and mean-std normalization using dataset statistics. For training, the number of epochs of 164, stochastic gradient descent with a momentum of 0.9, learning rate of 0.1, learning rate decay of 0.1 at {81, 122} epochs, weight decay of 10-4, and mini-batch size of 128 were used. The training was conducted on a single GPU machine. We examined different choices of keep probability p \u2208 {0.1,0.2,\u2026\u2026,0.9}, and an average of ten runs with different random seeds was reported for each result (Tables 3, 4).\nWe observed that replacing global average pooling with stochastic average pooling improved classification accuracy when using an adequate keep probability. Although certain extreme cases of choosing p = 0.9 exhibited little difference and p = 0.1 degraded performance, others with moderate choices of keep probability successfully improved the accuracy. The best performance was found at the middle point p = 0.5.\nNote that PatchDropout opted for lower keep probability such as p = 0.5 or p = 0.25 [26, 11, 24]. In contrast, for Dropout, a keep probability of p = 0.5 or p = 0.8 has been preferred, while the original study on Dropout opted for p = 0.5 [34]. Recent studies [35, 3] reported that sufficient regularization"}, {"title": "Replace GAP in SE Block", "content": "Besides the classifier head, global average pooling has been used in channel-wise attention modules, such as the SE block. The SE block is a pipeline of [GAP-FC\u2013ReLU-FC-Sigmoid], and we compare the performance before and after replacing its global average pooling with stochastic average pooling. We used SE-ResNet-{50, 101} [15], which are variants of ResNet [12] adopting an SE block in each residual block.\nWe targeted a multi-class classification task on the Oxford-IIIT Pet [28], Caltech-101 [9], and Stanford Cars [18] datasets. The Oxford-IIIT Pet dataset contains 7K pet images from 37 classes; the Caltech-101 dataset includes 9K object images from 101 classes with a background category; and the Stanford Cars dataset includes 16K car images from 196 classes. These datasets are available on their official websites. Each dataset was split into training, validation, and test sets in a ratio of 70:15:15. All experiments were conducted at a resolution of 2242 using standard data augmentation, including random resized cropping to 256 pixels, random rotations within 15 degrees, color jitter with a factor of 0.4, random horizontal flip with a probability of 0.5, center cropping with 224-pixel windows, and mean-std normalization based on ImageNet statistics [7].\nFor training, stochastic gradient descent with a momentum of 0.9, learning rate of 0.1, cosine annealing schedule [27] with 200 iterations, weight decay of 10-3, and mini-batch size of 128 were used. These hyperparameters were obtained based on the accuracy of the validation set. The model with the highest validation accuracy was obtained for 200 training epochs, and we report the accuracy on the validation and test sets. The training was conducted on a single GPU machine. An average of three runs with different random seeds was reported for each result.\nTable 5 summarizes the results. Compared with the baseline using global average pooling in the SE block, the use of stochastic average pooling improved the performance. These improvements were consistently observed across three datasets and two SE-ResNets, which advocates the use of stochastic average pooling within the SE block for the representation of aggregated information on a feature map."}, {"title": "Replace Average Pooling in Semantic Segmentation Networks", "content": "Now, we examine the use of stochastic average pooling on another task. Here we investigate semantic segmentation, which performs pixel-wise classification of images. We targeted PSPNet [42] and UPerNet [38], which are representative models in the semantic segmentation task. They include the pyramid pooling module, which uses four 2D average poolings with pooling sizes rxr\u2208 {12, 22, 32, 62} to aggregate multi-level feature maps on different contexts. We replaced each"}, {"title": "Replace GAP in Object Detection Networks", "content": "Now, we target an object detection task, which aims to locate bounding boxes for objects within images. We examined the recent model of DyHead [6], which uses its own attention mechanism that contains global average pooling. Specifically, a DyHead neck consists of multiple DyHeadBlocks, and each DyHeadBlock contains global average pooling to aggregate information on a feature map. We investigated the performance of DyHead when replacing global average pooling with stochastic average pooling.\nFor training and testing, we used the COCO 2017 dataset [26], which consists of 118K training images, 5K validation images, and 41K test images. Following the common practice for object detection of the COCO 2017 dataset, we applied mean-std normalization, a resize operation using a resize scale of (1333, 800) pixels, and a random flipping with a probability of 0.5. A training recipe from MMDetection was employed. For training, stochastic gradient descent with momentum 0.9, weight decay 10-4, and learning rate 2 \u00d7 10-2 with multi-step decay using an 1\u00d7 scheduler were used. ResNet-50 [12] pretrained on ImageNet [7] was employed as the backbone. The training was performed on a 4\u00d7GPU machine, and SyncBN [41] was used for distributed training. Average precision and its variants, which are commonly used indices, were measured (Table 8).\nWe observed that the DyHead with stochastic average pooling outperformed that with global average pooling. All these results demonstrate that adopting stochastic average pooling benefits performance compared with vanilla average pooling."}, {"title": "Discussion", "content": "As mentioned in Section 2.2, different ways for subsampling may be allowed. In PatchDropout, a channel-shared random pattern is opted for to delete semantic information on the selected patches, whereas a Dropout-like mask applies a fully random pattern that independently subsamples features with respect to the channel dimension (Figure 7). Although the different choice of subsampling pattern does not affect our objectives such as matching second moments, it may influence other factors such as preserved information on an image.\nFurthermore, recent studies such as DropBlock [30] and AutoDropout [10] reported that adopting specific spatial patterns for Dropout improves performance. In contrast to Dropout, which uses a random pattern without any restriction, the study of DropBlock proposed to drop contiguous features in grouped blocks. They reported that the block pattern readily removes semantic information such as objects or hues in images, which enhances the effect of Dropout in computer vision tasks. The study of AutoDropout employed a reinforcement learning model to search for a novel drop pattern, where they found that a grid pattern worked suitably. The grid pattern facilitates capturing the holistic structure on data, which benefits the effect of Dropout in vision tasks."}, {"title": null, "content": "Considering these practices for Dropout, here we study adopting a spatial pattern in stochastic subsampling. With the keep probability p = 0.5 for stochastic subsampling, we consider possible patterns to subsample half of the elements in a feature map. To this end, given an input size l \u00d7 1 and a factor s x s, four spatial patterns are examined"}, {"title": null, "content": "After generating a subsampling pattern from the above list, we additionally apply random circular shifts in vertical and horizontal directions to allow variation in the pattern, preventing fixed partitioning.\nNow, we compare the performance of stochastic subsampling when adopting the above patterns. We experimented with the classification task using CIFAR-10 and ResNet-50. The training recipe was the same as the experiment performed in Section 3.1. For this task, stochastic average pooling was applied to the last feature map that exhibits a spatial size of 8 \u00d7 8, i.e., l \u00d7 1 = 82.\nTable 9 summarizes the experimental results. Firstly, we observed that the channel-independent pattern exhibited lower performance compared with the channel-shared pattern. Thus, the channel-shared pattern is more advantageous for removing semantic information from an image to enhance its effect. We also measured performance with standard Dropout, which slightly degraded performance."}, {"title": null, "content": "Secondly, we measured the performance of using different spatial patterns. In fact, all four patterns failed to exceed the baseline performance of stochastic average pooling that did not apply restriction on the subsampling pattern. Specifically, strong restrictions such as the block pattern with s = 4 severely degraded accuracy. Rather, weak restrictions such as the uniform pattern with s = 4 exhibited reasonable performance but still underperformed compared with using a subsampling pattern without restriction. In short, stochastic average pooling worked suitably when allowing sufficient degree of freedom for randomness in the subsampling pattern.\nIndeed, because stochastic subsampling follows average pooling in our module, subsampling a unit after average pooling drops all elements within the pooling size. Thus, the effective subsampling pattern with respect to the input already exhibits a grid pattern in stochastic average pooling, which explains why introducing an additional pattern in stochastic subsampling did not improve the performance here. In other words, adopting a spatial pattern causes excessive restrictions on the effective subsampling pattern, which raises another side effect on stochastic average pooling and degrades performance. Based on these observations, we opt for allowing degree of freedom for the randomness of the subsampling pattern in stochastic subsampling, ensuring no restrictions on the spatial pattern."}, {"title": "Conclusion", "content": "This research studied preventing an overfitting problem of deep neural networks by module-level regularization. We pointed out that existing regularization methods such as Dropout and PatchDropout cause inconsistent properties such as second moment and size during training and test phase, which makes them difficult to be used in practical scenarios. To address this issue, we proposed a new module called stochastic average pooling. Stochastic average pooling\u2014which combines stochastic subsampling, average pooling, and \u221ap scaling\u2014ensures consistent properties in output, solving the aforementioned issue. Similarly to Dropout, stochastic average pooling achieves a regularization effect through the ensemble behavior of possible subnetworks. Furthermore, our design for this module enables it to seamlessly replace the existing average pooling architecture. Comprehensive evaluations showed that replacing average pooling with stochastic average pooling consistently improved performance of deep neural networks, which demonstrates its wide applicability."}]}