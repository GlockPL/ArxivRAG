{"title": "SURVEY AND EVALUATION OF CONVERGING ARCHITECTURE\nIN LLMS BASED ON FOOTSTEPS OF OPERATIONS", "authors": ["Seongho Kim", "Jihyun Moon", "Juntaek Oh", "Insu Choi", "Joon-Sung Yang"], "abstract": "Large Language Models (LLMs), emerging from advancements in Natural Language Processing\n(NLP) tasks, allow chatbots to provide more sophisticated and human-like text generation by leverag-\ning their vast model sizes, often exceeding billions of parameters, to provide deep knowledge across\nvarious domains. Hence, they are becoming more integrated into our daily lives, serving as personal\nassistants or even as experts across various domains. Initial language models relied on rule-based\nsystems and early neural networks like Recurrent Neural Networks (RNNs). However, it remained an\nissue to handle long-term dependency and to understand context over extended conversation.\nThe advent of the Attention mechanism and Transformer architecture enables contextually natural\ntext generation and compresses the burden of processing entire source information into singular\nvectors. Based on these two main ideas, model sizes gradually increases to accommodate more\nprecise and comprehensive information, leading to the current state-of-the-art LLMs being very large,\nwith parameters around 70 billion. As the model sizes are growing, the demand for substantial storage\nand computational capacity increases. This leads to the development of high-bandwidth memory and\naccelerators, as well as a variety of model architectures designed to meet these requirements. We\nnote that LLM architectures have increasingly converged. This paper analyzes how these converged\narchitectures perform in terms of layer configurations, operational mechanisms, and model sizes,\nconsidering various hyperparameter settings.\nIn this paper, we conduct a concise survey of the history of LLMs by tracing the evolution of their\noperational improvements. Furthermore, we summarize the performance trends of LLMs under\nvarious hyperparameter settings using the RTX 6000, which features the state-of-the-art Ada Lovelace\narchitecture. We conclude that even the same model can exhibit different behaviors depending on the\nhyperparameters or whether it is deployed in server or edge environments.", "sections": [{"title": "Introduction", "content": "In the past few years, since the release of the transformer model [13], AI models have evolved rapidly. Among these\nmodels, LLMs have garnered significant interest and made substantial progress, particularly due to the popularity of\nChatGPT. There are a variety of model options available on the market, with open-source LLMs being particularly\npopular due to their cost-effectiveness and extensive customizability. One notable advantage of open-source LLMs is\ntheir cost-efficiency, offering free access to high-performance models. Furthermore, these models provide the flexibility\nof customization and optimization, allowing users to adapt the models to their particular requirements. Notably, Meta's\nLlama and Google's Gemma have attracted considerable interest.\nLlama [22] was unveiled with the initial model available in four versions, ranging from 6.7B to 65.2B parameters.\nBuilding on this, Llama 2 [23] was released in three versions: 7B, 13B, and 70B, with a 40% increase in the size of\nits pre-training data. The sequence length was doubled over the previous model, and the 70B version was the first to\nincorporate Grouped Query Attention (GQA), demonstrating much stronger performance. Additionally, Meta released\nCode Llama based on Llama 2 and announced Llama 3.\nMeanwhile, Google DeepMind introduced their open-source LLM, Gemma [27]. This lightweight model was released\nin two sizes with 7B and 2B parameters, targeting on-device applications such as desktops or mobile devices. Following\nthis, Google DeepMind updated Gemma to version 1.1, maintaining the model size while enhancing performance.\nThey have continuously developed and updated the Gemma family, including the release of RecurrentGemma and\nCodeGemma.\nEfforts to accelerate these models have also been significant. One notable development is the release of TensorRT-LLM,\nan open-source library that accelerates the inference performance on NVIDIA GPUs. As LLMs have evolved and\ngrown in size, their computational cost has increased significantly, making them challenging to operate without modern\ntechniques. To address this, TensorRT-LLM [25] provides a comprehensive library that can be compiled and optimized\nfor inference. The library includes a number of optimization techniques, such as kernel fusion and quantization, as well\nas runtime optimizations such as C++ implementations, KV caching, continuous flight batching, paged Attentions, and\nan intuitive Python API for defining and building new models. It also supports multi-GPU and multi-node inference,\nand is constantly being updated for the latest models. Furthermore, it is compatible with the latest NVIDIA GPU\narchitecture.\nHowever, since the announcement of Llama, the structural development of these models has gradually converged, and\nthe differences between models often depend on the dataset and the specific operations used. Significant changes\nhave occurred in computational methods, Attention types, and activation functions. As a result, LLMs have been\ndifferentiated into server AI, edge AI, and on-device AI, depending on the purpose of the model and the resources that\ncan be allocated. Therefore, we analyze the latest open-source LLMs to understand their current technical limitations\nand provide directions for developing better performance and efficiency models. Among the three focuses, model\nperformance, learning time, and inference time, we selected the inference time as the primary target.\nIn this paper, we survey the evolution of language models from Recurrent Neural Networks (RNNs) to LLMs, focusing\non their key operations and architectures. In addition, we analyze the architecture of modern open-source LLMs such\nas Gemma and Llama. Specifically, we examine the inference process of these models on high-performance GPUs,\nanalyze the weight of each kernel, and identify the bottlenecks in current hardware."}, {"title": "Development of Architecture", "content": "In this section, we provide a overview of the development process of previous language models targeting NLP tasks,\nwith a particular focus on architecture (Figure 1). The major milestones from RNNs to modern LLMs are summarized."}, {"title": "Recurrent Neural Networks", "content": "In earlier stages, linguistic processing techniques were limited to rule-based or statistical approaches due to issues such\nas complexity or language specificity. Past neural networks had a structure in which only perceptrons were simply\nplaced, and therefore the context of language could not be considered. However, with the advent of RNNs that are\ncapable of identifying temporal correlations by their circular structure, neural networks have begun to be actively\nutilized in the field of language. In particular, Long Short-Term Memory (LSTM) [1], which has improved the structure\nof existing RNNs that are prone to gradient vanishing problems in the long term, has been extensively adopted in\nvarious early language models."}, {"title": "Encoder-decoder Architecture", "content": "[3] proposes an encoder-decoder architecture composed of two LSTMs to address the limitations of the existing model,\nwhich could only accommodate the fixed-length input. The encoder LSTM sequentially encodes the input to create a\nlong fixed-dimensional vector, and the decoder LSTM decodes this to produce the output. This structure responds to\nvarying-length input by receiving input tokens sequentially, and also improves performance in NLP tasks due to the\nlong memory of LSTM."}, {"title": "Attention Mechanism", "content": "The encoder-decoder structure converts the entire input sentence into a fixed-length vector, thus creating the bottleneck\nissue. Performance degradation occurs as the input sentences become longer. In order to address this issue, [4] proposes\nthe Attention technique that focuses on the highly relevant part of the context word generated by the encoder for each\noutput from the decoder. This alleviates the burden that arises when the model compresses the entire source information\ninto a singular vector and enhances comprehension of the contextual nuances."}, {"title": "Transformer Architecture", "content": "RNNs are inherently sequential, precluding parallelized computation within training datasets. They also generate the\nmemory burden as the sequence becomes longer, limiting batching across examples. The transformer structure proposed\nin [13] maintains the encoder-decoder structure and replaces all RNNs with attention mechanism and fully connected\nfeed-forward network. By removing recurrence, the model facilitates much higher levels of parallelization, improving\nspeed and performance simultaneously in training and inference. The transformer has become the base model for\nmodern language models."}, {"title": "Pre-trained Language Models", "content": "GPT-1 [14] and BERT [15] define the model structure with only a decoder or an encoder, respectively, and further\nincrease internal parameters such as the number of layers and dimension size. They also strengthen pre-training and\nfine-tuning techniques, such as integrating unsupervised pre-training and supervised fine-tuning, or incorporating the\n\"Masked Language Model\" and \"Next Sentence Prediction\" tasks. Therefore, the model's context understanding ability\nwas greatly improved, achieving the State-of-the-Art (SOTA) on benchmarks such as General Language Understanding\nEvaluation (GLUE)."}, {"title": "Large Language Models", "content": "Facebook AI team discovers that BERT is significantly under-trained by the previous methods, and reveals RoBERTa [18]\nwith improved performance. The team upgrades the model with several modifications, such as removing previous\nprediction tasks and expanding input sequence, batch sizes, training dataset and duration. Likewise, Google AI\nLanguage team reveals GPT-3 [19], by removing fine-tuning, expanding parameter to 175 billion and adopting few-shot\nlearning on a vast amount of datasets. Therefore, GPT-3 could achieve strong performance on various NLP tasks. As a\nresult of these previous studies, various language models improve performance by increasing parameters and datasets,\nleading to the development of modern LLMs. Recent high-performance LLMs follow a transformer-based decoder-only\narchitecture."}, {"title": "Development of Operation", "content": "This section explains the primary operations performed in each layer of modern models. It also provides definitions of\nthe techniques used in each layer and describes their development processes."}, {"title": "Feed-forward Layer", "content": "All neural networks begin with a component known as a perceptron. The perceptron generates an output by adding bias\nto the cumulative sum of multiple inputs multiplied by each weight and then passing it through the activation function.\nA Fully Connected layer (FC layer) is a form in which the perceptrons are aligned in a line. Upon analysis in a vertical\ndirection, it can be observed that it consists of a linear transformation and an activation function. The transformer model\ndefines a Feed-Forward Network (FFN) in which an activation function exists between two linear transformations.\nGated Linear Unit (GLU) [12] is proposed to resolve the gradient vanishing problem and improve prediction accuracy.\nGLU is a structure that converts the input into two linear transformations and performs a component-wise product, one\nof which is passed through an activation function. Furthermore, [20] introduces GLU variants that apply several modern\nactivation functions to GLU, as well as defining FFN variants (Feed-forward GLU) that apply GLU to the existing\ntransformer FFN. The operations are described in Figure 2, and also expressed as:\n$FC(x, W, b) = Act(xW + b)$\n$FFN (x, W, W2, b, b2) = Act(xW + b)W2 + b2$\n$GLU(x, W, V, b, c) = Act(xW + b) \\odot (xV + c)$\n$FFN_{GLU} (x, W, V, W2)$ \n$= (Act(xW) \\odot xV)W2$\nwhere x is the input vector, W, V, W2 are weight matrices, and b, c, b2 are bias vectors. Act(x) is a generalized form\nof the activation function. Note that the bias terms are omitted in feed-forward GLU."}, {"title": "Attention Layer", "content": "The attention mechanism is a method that focuses on and refers to highly relevant parts of the entire input sentence of the\nencoder during each decoder output process. It computes the similarity between the input Query (Q) and all Keys (K),\nthen applies this similarity to the corresponding Value (V) for each key, and cumulatively adds these results to return\nthe final outcome. Scaled Dot-Product Attention (SDPA), a representative form among variations of Attention, goes\nthrough the following process: after dot-producing the Q and K, SPDA calculates the \"Attention score\" by multiplying\nit by the scaling factor, $1/\\sqrt{d_k}$. Next, SDPA takes the softmax, calculates the \"Attention distribution\", and products it\nwith V to obtain the \"Attention value\".\nWhen performing a linear projection on a multi-head rather than a single Attention, the overall performance is improved\nsince each head is able to comprehend the context in subspaces of different positions and complement each other.\nThis method is referred to as Multi-head Attention (MHA), and the transformer model employs SDPA and MHA\nsimultaneously. Although [17] acknowledges the performance of MHA, it proposes Multi-Query Attention (MQA)\nby focusing on the memory burden that arises when repeatedly loading K and V, which are large tensors. MQA can\nsignificantly reduce memory bandwidth by sharing a single KV head with all Q heads, however, it has the disadvantage\nof deteriorating model performance. Based on this, [24] proposes Grouped Query Attention (GQA), which comprises\nMHA and MQA by allocating a single KV head at each Q head group."}, {"title": "Normalization Layer", "content": "During training, the phenomenon called the \"internal covariate shift\" occurs whenever the parameter is converted,\nleading to a decrease in learning performance. Normalization techniques are proposed to address this issue. Batch\nNormalization (BatchNorm) [6] normalizes the input by calculating the average and variance of multiple mini batches.\nAlthough batch normalization is effective for CNNs, it is not suitable for language models where the length of the\nsequence varies. Therefore, Layer Normalization (LayerNorm) [10], which normalizes within a single batch, is\npresented. Afterwards, [16] claims that the process of re-centering and re-scaling of layer normalization is unnecessary.\nIt proposes Root Mean Square Normalization (RMSNorm) which normalizes based on RMS without re-centering and\nre-scaling of layer normalization. The three different normalization techniques introduced are expressed as:\n$x_{BatchNorm} = \\frac{x - E_{mini-batch}(x)}{\\sqrt{Var_{mini-batch}(x) + \\epsilon}} \\cdot \\gamma + \\beta$\n$x_{LayerNorm} = \\frac{x - E_{features}(x)}{\\sqrt{Var_{features}(x) + \\epsilon}} \\cdot \\gamma + \\beta$\n$x_{RMSNorm} = \\frac{x}{\\sqrt{E_{features} (x^2) + \\epsilon}}$\nwhere $E_{mini-batch}(x)$ and $Var_{mini-batch}(x)$ are the mean and variance, computed per feature over the mini-batch, and $\\epsilon$ is\nthe constant for numerical stability. $\\gamma$ and $\\beta$ are scaling and shifting learnable parameters, respectively. $E_{features}(x)$ and\n$Var_{features}(x)$ are the mean and variance, computed over the feature dimension."}, {"title": "Activation Function", "content": "The activation function has also been developed to enhance performance along with the structure. Sigmoid is an\nactivation function that is commonly utilized in probability representation tasks. It exhibits a distinctive profile,\ncomprising a high slope in the center and a gradual decline toward both sides. However, a gradient vanishing problem\nemerges during back propagation, in which learning performance deteriorates as the distance from the output layer\nincreases. In contrast, Rectified Linear Unit (ReLU) [2] has a formula that rectifies the negative input of a linear function.\nIn the case of positive input during back propagation, the gradient vanishing problem is eliminated, yet the update is\nnot performed for negative input, resulting in the \"dying ReLU\" phenomenon in which most parameters become 0.\nConsequently, several activation functions that give a small gradient to the negative input have been proposed, including\nExponential Linear Unit (ELU) [5], Gaussian error Linear Unit (GeLU) [7], and Swish [11]. The five activation\nfunctions above are plotted in Figure 4. They sacrifice the computational efficiency of ReLU, and take improved\nperformance. Several activation functions are expressed as:\n$sigmoid(x) = \\frac{1}{1+ e^{-x}}$\n$ReLU(x) = max(0, x)$\n$ELU_{\\alpha=1}(x) = \\begin{cases}\nx & \\text{for } x \\ge 0 \\\\\ne^x-1 & \\text{for } x < 0\n\\end{cases}$\n$GeLU(x) = x \\cdot \\Phi(x) = x \\cdot \\frac{1}{2} [1 + erf(x/\\sqrt{2})]$\n$Swish_{\\beta=1}(x) = \\frac{x}{1+ e^{-x}}$"}, {"content": "where both ELU and Swish are described under the assumption of $\\alpha=1$ and $\\beta=1$. In addition, $erf(x)$ inside GeLU\nis defined as $erf(x) = \\int_0^x e^{-t^2} dt$.\n$softmax (x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_i}}$\nThe softmax function transforms input values into a set of output values, allowing classification of the input into one\nof multiple classes. The properties of applying the exponential function to inputs and normalizing them enable a\nprobabilistic interpretation. Therefore, the softmax function is generally placed in the last layer of models and plays a\nrole in determining the output token."}, {"title": "Residual Connection", "content": "When training a neural network, it is commonly expected that models with a higher number of layers, the so-called\n\"deep\" models, would yield better performance. However, there is a phenomenon where the accuracy decreases as\nthe depth of the model increases. To address this challenge, the concept of a \"deep residual learning framework\" is\nintroduced in [9]. This framework significantly improves the degradation problem by introducing \"short connections\"\nor \"residual connections,\" which skip one or more layers in the existing structure. This solution has become a crucial\ncomponent in various neural network architectures, including CNNs and language models, and remains essential to\ndate."}, {"title": "Embedding and Encoding", "content": "Embedding refers to the process of converting input text into a vector. Great deal of research has been conducted since\nmodel performance is significantly influenced by the manner in which words are expressed. As the dataset becomes\nvast, the one-hot encoding method is abandoned and the learned embedding method, in which semantic relationships\nare learned through language models, is primarily employed. From the transformer model, the concept of positional\nembedding is initiated by including positional encoding in embedding. This is because when replacing RNNs, the\npositional information of the token within the sequence must be included. Sinusoidal position encoding, used here, is a\nmethod of encoding absolute positions expressed as trigonometric functions of different frequencies into an embedding\nvector. [26] presents Rotary Position Embedding (RoPE), which encodes absolute position in embedding with a rotation\nmatrix and adds relative position dependency in the self-Attention equation. This method is widely known to be efficient\nbecause it only performs a series of rotation operations. Furthermore, the relative distance and angle between tokens are\npreserved, allowing the model to easily learn relative positional information."}, {"title": "Architecture of Modern LLMs", "content": "In this section, we provide a snapshot of modern open-source LLMs by analyzing the architecture (Section 4.1) and\nlayers (Section 4.2) of Llama and Gemma. We use Llama2-7B, Llama3-8B, Gemma-2B, and Gemma-7B models\n(Gemma 1.0 version), therefore, we could analyze MHA, GQA, and MQA in the same environment. Table 1 provides a\nsummary of the parameters for Llama and Gemma utilized in the experiments conducted in this paper."}, {"title": "Architecture Analysis", "content": "As shown in Figure 5, the model Llama architecture follows decoder-only transformer-based architecture. It applies\npre-normalization of RMSNorm, feed-forward SwiGLU, and RoPE. GQA is applied in the Llama 3-8B model. The\narchitecture of Gemma model is mostly consistent with Llama, except that GeGLU is used instead of SwiGLU for\nactivation function in feed-forward layer. MQA is also applied in the case of the Gemma-2B model. In Figure 5, the\ncolors are applied with respect to the types of kernels generated when each layer is executed, as observed in Figure\n7 of the subsequent section. The layers colored in light blue (summarization stage) and dark blue (generation stage)\nrepresent Matrix Multiplication layers, with the GEMM or GEMV operations. The attention layer, which is critical for\nthe advancement and differentiation of models, is marked in red."}, {"title": "Layer Analysis", "content": "Through the analysis of open-source models, we can summarize the variations in the torch size of each layer. The torch\nsize of the summarization, the generation stage, and the symbols can be organized as Table 2. After the embedding\nprocess, computations are performed using the predetermined dimension size. In the attention block, the torch is split\naccording to the number of heads appropriate for the structure, and it is recombined to its original configuration once the\nAttention block is complete. In the generation stage, the KV caching size, C, is updated to L after the attention block.\nTherefore, as multiple attention blocks are processed, the size of C continuously increases in each output execution."}, {"title": "Experimental Results", "content": "In this section, two major experiments we conducted and the analysis details are described. The experimental\nenvironment, the summarization and generation stage ratio, and the kernel analysis are addressed in Section 5.1, Section\n5.2, and Section 5.3, respectively."}, {"title": "Experiment Environment", "content": "We perform experiments using four LLM models, by sweeping input length, output length, and batch size. As LLMs\nrequire highly computational and memory-intensive operations, we utilize a high-performance GPU and detailed\nenvironment settings are listed in Table 3. To maintain a consistent experimental environment across four additional\nLLM configurations, we employ the ccdv/cnn_dailymail dataset throughout the experiments [8], which is provided by\nthe TensorRT-LLM library. In addition to analyzing the output results, we also perform GPU profiling to analyze the\nexecution time and kernel configuration."}, {"title": "Summarization and Generation Stage Ratio", "content": "Figure 6 illustrates the proportion of the generation stage execution time to the total execution time. It shows the\nresults of sweeping the input lengths from 1 to 256 and the output lengths from 4 to 1024 with batch sizes of 1 and 8.\nFurthermore, TensorRT-LLM is employed so that all models can be tested in the same environment as possible. The\nexperiment is conducted on a basic TensorRT-LLM environment, including flash attention, and the data type is set to\nbfloat16. The heatmap analysis yields four major implications: Input sweep, output sweep, batch sweep and attention\ntechniques, respectively.\nInput Sweep As illustrated in all heatmaps in Figure 6, a consistent trend is observed where the proportion of the\ngeneration stage decreases as the input size increases. This is because the arithmetic intensity of GEMM operations\nincreases proportionally to the size of the sequence length (S) in the summarization stage. This trend becomes more\nevident as the length of the output sequence decreases, resulting in fewer instances of the generation stage.\nOutput Sweep As the output length increases, the number of generation stage executions increases, resulting in an\nanticipated rise in the proportion of generation stage time relative to the total execution time. This trend is observed\nthroughout the results, where the proportion of the generation stage increases consistently with the length of the output.\nNotably, when the input length is 1 and the output length is 4, the proportion of the generation stage is approximately\n75% in all cases. As shown in Table 2, which presents the dimensions calculated, the arithmetic intensity between\nthe generation stage and the summarization stage remains nearly identical when the sequence length (S) is set to 1.\nTherefore, the generation stage accounts for approximately three-fourths of the total runtime.\nBatch Sweep However, the proportion of the generation stage reduces more rapidly with a larger batch size as the\ninput size increases. This phenomenon arises because, with a large batch size, the GPU processes operations with an\ninput size scaled by the batch size, leading to variations in sequence length and an increased number of operations\nfor both summarization and generation stages. In contrast to the generation stage, which transitions from GEMV to\nGEMM operations, the summarization stage intensifies arithmetic operations, making it more susceptible to the effects\nof increased input size.\nConcerning internal operations, when the batch size is 1, the generation stage typically has operations per byte\npredominantly in the single-digit range. When the operations per byte are small, the operations tend to be memory-\nbound rather than compute-bound. However, as the batch size grows, operations per byte gradually increase, leading\nto a shift towards compute-bound operations. Conversely, in the summarization stage, even with a batch size of\n1, the operations per byte are significantly greater than 10, rendering all matrix multiplication operations nearly\ncompute-bound.\nAttention Techniques For models such as Llama2-7B and Gemma-7B, comparable results are observed due to the\nsimilar number of parameters. However, it appears that the generation stage ratio increases when transitioning from the\ntwo models using MHA to the Llama3-8B model using GQA and the Gemma-2B model using MQA. This is attributed"}, {"title": "Kernel Analysis", "content": "Figure 7a illustrates the proportion of each kernel in the overall execution time when the output length is 8, 64, and 512,\nwhile keeping the batch size at 1 and the input length at 64. Figure 7b illustrates the proportion of kernels with the same\noutput length variations (8, 64, and 512), but with a batch size of 64 and an input length of 64. Both figures analyze\nfour different models within the same kernel categories, as listed below. The analyzed kernels account for more than\n95% of the total execution time and they are categorized into five groups: Generation stage GEneral Matrix-Vector\nmultiplication (Gen GEMV), Summarization stage GEMV (Sum GEMV), Attention blocks (ATTN), Generation stage\nGEneral Matrix-Matrix multiplication (Gen GEMM), and Summarization stage GEMM (Sum GEMM).\nKernel Characteristics When the batch size is 1, the proportion of summarization and generation stages based on input\nand output sweeps reveals that GEMM operations dominate the summarization stage, while GEMV operations dominate\nthe generation stage across all four models. However, when the batch size is 64, the input sequences are multiplied by\n64, resulting in GEMM operations becoming the primary kernels for both the summarization and generation stages\nacross all models. The execution times of the attention mechanism in both the generation and summarization stages are\ncombined and represented as the ATTN component, as shown in Figure 7.\nSingle Batch Regardless of the batch size, the graph demonstrates that the execution time of the generation stage\nincreases with the output length. This graph illustrates the kernel composition and its impact on changes in execution\ntime. The experiments are conducted with two different batch sizes. In edge environments, where it is common to\nprocess a single execution at a time, the scenario with a batch size of 1 can be considered representative.\nIn operations with a batch size of 1, GEMV operations account for more than 80% of the total execution time for all\nmodels and even exceed 95% when the output is over 64. Consequently, a hardware configuration optimized for GEMV"}, {"content": "operations is essential when the output length is greater than 64. The GPU utilization of the GEMV kernels reveals that\nthe memory bandwidth usage is relatively low compared to the computational capacity of the kernels, indicating that\nthe operations are memory-bound.\nLarge Batch In environments such as servers, where multiple inputs are processed simultaneously, a larger batch size\ncan be utilized to handle inputs concurrently. When the batch size is 64, the calculation is performed by multiplying the\nsequence length by the batch size, resulting in the GEMM kernels occupying the dominant part of the execution time.\nIn contrast to the case where the batch size is 1, it can be observed that the proportion of attention in the total execution\ntime increases significantly. As the size of the KV increases, the generation stage becomes more computationally\nintensive, leading to a greater demand for both computation and memory resources. This is evidenced by the observation\nthat as the batch size increases from 1 to 64, the amount of computation required to generate an output token increases.\nAttention Techniques The rate of increase varies depending on which attention mechanism is employed. As previously\nstated in 5.2, the low-memory attention mechanism exhibits a relatively high operation per byte ratio in attention blocks.\nConsequently, it can be observed that in both Llama and Gemma models, the trend of increasing the ratio of attention\nexecution time in GQA and MQA is relatively modest."}, {"title": "Conclusion", "content": "The rationale for employing TensorRT-LLM in this analysis is that it leverages a plethora of existing acceleration\ntechniques, enabling consistent acceleration and analysis of the model. In particular, on the issue of attention, the\ndifference in kernel execution time is significant before and after the application of kernel fusion and attention\nacceleration techniques. Before this, attention blocks were not well suited to GPUs, resulting in occupying an\nexcessively large portion of the total execution time. Therefore, there is considerable discussion on how to accelerate\nattention. In this analysis, we employ flash attention, a feature included in the TensorRT-LLM backend, to achieve the\ndesired outcome in a consistent environment.\nThe preceding analysis has revealed which models exhibit the desired behavior in specific hyperparameter settings and\nhow they function in such contexts. It is important to note that the types of kernels used are completely different in small\nbatch size of edge environments compared to large batch size of server environments. Furthermore, the predominant\nkernel also varies significantly depending on the required output length.\nIn the context of edge AI, the GEMV operation represents a significant proportion of the total execution time. In\ncontrast, for server-based LLM environments, the most significant challenge has been reducing the execution time of\nthe Attention mechanism. To address this, various attention architectures and mechanisms are employed to program and\nreduce the amount of memory derived from an attention part. Our evaluations indicate that by employing acceleration\ntechniques and advanced attention methods including GQA and MQA, we can mitigate the computational burden of\nattention to a certain extent.\nConsequently, our survey and evaluation demonstrate that different types of hardware, distinct from the previously\nutilized GPU, are required depending on the specified hyperparameters."}]}