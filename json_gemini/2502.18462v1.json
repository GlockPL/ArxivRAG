{"title": "Scalable Equilibrium Sampling with Sequential Boltzmann Generators", "authors": ["Charlie B. Tan", "Avishek Joey Bose", "Chen Lin", "Leon Klein", "Michael M. Bronstein", "Alexander Tong"], "abstract": "Scalable sampling of molecular states in thermo- dynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann generators tackle this problem by pairing powerful nor- malizing flows with importance sampling to obtain statistically independent samples under the target distribution. In this paper, we extend the Boltzmann generator framework and introduce SEQUENTIAL BOLTZMANN GENERATORS (SBG) with two key improvements. The first is a highly efficient non-equivariant Transformer- based normalizing flow operating directly on all-atom Cartesian coordinates. In contrast to equivariant continuous flows of prior methods, we leverage exactly invertible non-equivariant ar- chitectures which are highly efficient both during sample generation and likelihood computation. As a result, this unlocks more sophisticated inference strategies beyond standard importance sampling. More precisely, as a second key improvement we perform inference-time scaling of flow samples using annealed Langevin dynam- ics which transports samples toward the target distribution leading to lower variance (annealed) importance weights which enable higher fidelity resampling with sequential Monte Carlo. SBG achieves state-of-the-art performance w.r.t. all metrics on molecular systems, demonstrating the first equilibrium sampling in Cartesian coordi- nates of tri, tetra, and hexapeptides that were so far intractable for prior Boltzmann generators.", "sections": [{"title": "1. Introduction", "content": "The simulation of molecular systems at the all-atom resolu- tion is of central interest in understanding complex natural processes. These include important biophysical processes such as protein-folding (No\u00e9 et al., 2009; Lindorff-Larsen et al., 2011), protein-ligand binding (Buch et al., 2011), and formation of crystal structures (Parrinello & Rahman, 1980; Matsumoto et al., 2002), whose understanding can aid in problems that range from long-standing global health challenges to efficient energy storage (Deringer, 2020).\nThe dominant paradigm for molecular simulation involves running Markov Chain Monte Carlo (MCMC) or Molecular Dynamics (MD) whereby the equations of motion are integrated with finely discretized time steps. However, such molecular systems often exist in thermodynamic equilibrium by remaining for long time horizons in metastable states before rapidly transitioning to another metastable state. Such metastable states are captured in the minima of a complex energy landscape, associated with the molecular system's equilibrium (Boltzmann) distribution at a given temperature. Unfortunately, drawing uncorrelated samples from such metastable states via traditional MD or MCMC methods is prohibitively computationally expensive, requiring long simulation steps with small updates on the order of femtoseconds $1fs = 10^{-15}s$, as transitions are rare events due to the presence of high-energy barriers between well-separated metastable states (Wirnsberger et al., 2020).\nAn alternative approach is to enhance sampling efficiency by leveraging powerful generative models such as normal- izing flows (Dinh et al., 2017; Rezende & Mohamed, 2015) trained on existing biased datasets, to produce approximate samples which can then be reweighted via importance sampling to follow the desired Boltzmann distribution. Such models, called Boltzmann generators (BG) (No\u00e9 et al., 2019), allow faster sampling through amortization as generation is significantly cheaper computationally than running MD or MCMC. Despite their appeal, it remains challenging for existing BGs to generate uncorrelated samples in their native Cartesian coordinates from the energy modes of the Boltzmann distribution for larger molecular systems at the scale of small peptides (2 amino acids) (Klein et al., 2023b; Midgley et al., 2023a). The prin- cipal drawback inhibiting scalability stems from the lack of expressive equivariant architectures that are also exactly invertible (Bose et al., 2021; Midgley et al., 2023a), or the present over-reliance on simple E(n)-GNN's (Satorras et al., 2021) based equivariant vector fields used in the design of continuous-time normalizing flows (Chen et al., 2018). As a result, even the most performant BGs suffer from low overlap with the target Boltzmann distribution, leading to poor sampling efficiency during importance sampling.\nPresent work. In this paper, we introduce SEQUENTIAL BOLTZMANN GENERATORS (SBG) a novel extension to the existing Boltzmann generator framework. SBG makes progress on the scalability of Boltzmann generators in Cartesian coordinates along two complementary axes: (1) scalable pre-training of softly SE(3)-equivariant proposal normalizing flows in BGs; and (2) inference time scaling of proposal flow samples and their importance weights under fast non-equilibrium processes, e.g. such as Langevin dynamics. The final result yields higher quality generated samples with reduced weight redundancy through SMC reweighting and thus allowing for the computation of important observable quantities such as free energy differences between metastable states of $\u00b5_{target}(x)$.\nOur proposed approach SBG scales up proposal nor- malizing flows in BG's by following recent advances in atomistic generative modeling, e.g. AlphaFold 3 (Abramson et al., 2024). In particular, we opt to remove the rigid SE(3)-equivariance as an explicit architectural inductive bias in favor of softly enforcing it through simpler and more efficient data augmentation. To further improve samples and their importance weights\u2014a crucial step in the real-world application of BGs\u2014we perform inference scaling by designing a target-informed non-equilibrium process. More precisely, we define an interpolation between the proposal"}, {"title": "2. Background and preliminaries", "content": "We are interested in drawing statistically independent samples from the target Boltzmann distribution $\u00b5_{target}$, with partition function Z, defined over $R^{n\u00d73}$:\n$\u00b5_{target}(x) = \\frac{1}{Z} exp(\\frac{-E(x)}{k_BT}) , Z = \\int_{R^{d}} exp(\\frac{-E(x)}{k_BT}) dx$.\nThe Boltzmann distribution is defined for a system and includes the Boltzmann constant $k_B$, and is specified for a given temperature $T$. Additionally, the potential energy of the system $E: R^{n\u00d73} \u2192 R$ and its gradient $\u2207E$ can be eval- uated at any point $x \u2208 R^{n\u00d73}$, but the exact density $\u017f_{target}(x)$ is not available as the partition function $Z$ associated to the Boltzmann distribution in general is intractable to evaluate.\nIn this paper, unlike pure sampling-based settings, we are afforded access to a small biased dataset of $N$ samples"}, {"title": "2.1. Normalizing Flows", "content": "D = {$x_i$}$_{i=1}^N$, provided as an empirical distribution $p_D$. Consequently, it is possible to perform an initial learning phase that fits a generative model $p_\u03b8$, with parameters $\u03b8$, to $p_D$\u2014e.g. by minimizing the forward KL $D_{KL}(p_D||p_\u03b8)$ to act as a proposal distribution that can be corrected.\nA key desirable property needed for the correction of a trained generative model $p_\u03b8$ on a biased dataset $D$ is the ability to extract an exact likelihood $p_\u03b8(x)$. Normalizing flows (Dinh et al., 2017; Rezende & Mohamed, 2015) rep- resent exactly such a model class as they learn to transform an easy-to-sample base density to a desired target density using a parametrized diffeomorphism. More formally, given a sample from a (prior) base density $x_0 \u223c p_0$ and a diffeomorphism $f_\u03b8 : R^{n\u00d73} \u2192 R^{n\u00d73}$ that maps the initial sample to $x_1 = f_\u03b8(x_0)$. We can obtain an expression for the log density of $x_1$ via the classical change of variables,\n$log p_1 (x_1) = log p_0 (x_0) - log det(\\frac{df_\u03b8(x_0)}{\u2202x_0})$\nIn Eq. 1 above the $log det||$ term corresponds to the Jacobian determinant of $f_\u03b8$ evaluated at $x_0$. Optimizing Eq. 1 is the maximum likelihood objective for training normalizing flows and results in $f_\u03b8$ learning $p_1 \u2248 P_{data}$.\nThere are multiple ways to construct the (flow) map $f_\u03b8$. Perhaps the most popular approach is to consider the flow to be a composition of a finite number of ele- mentary diffeomorphisms $f_\u03b8 = f_M \u25e6 f_{M\u22121}\u2026\u2026\u25e6 f_1$, resulting in the change in log density to be:\n$log p_1 (x_1) = log p_0 (x_0) - \\sum_{i=1}^{M} log|df_{i,\u03b8}(x_{i\u22121})/dx_{i-1}|$.\nWe note that the construction of each $f_{i,\u03b8}, i \u2208 [M]$ is motivated such that both the inverse $f_{inv}(x)$ and Jacobian $df_{i,\u03b8}(x)/dx$ are computationally cheap to compute.\nContinuous Normalizing Flows. In the limit of the infinite elementary diffeomorphisms, a normalizing transforms into a continuous normalizing flow (CNF) (Chen et al., 2018). Formally, a flow is a one-parameter time- dependent diffeomorphism $V_t : [0, 1] \u00d7 R^{n\u00d73} \u2192 R^{n\u00d73}$ that is the solution to the following ordinary differential equation (ODE): $\u03c8_t(x) = u_t(V_t(x))$, with initial conditions $\u03c8_0(x_0) = x_0$, for a time-dependent vector field $u_t : [0, 1] \u00d7 R^{n\u00d73} \u2192 R^{n\u00d73}$. It is often desirable to construct the target flow by associating it to a designated probability path $p_t : [0, 1] \u00d7 P(R^{n\u00d73}) \u2192 P(R^{n\u00d73})$ which is a time-indexed interpolation in probability space between two distributions $p_0, p_1 \u2208 P(R^{n\u00d73})$. In such cases, the flow $V_t$ is said to generate $p_t$ if it pushes forward $p_0$ to $p_1$ by following $u_t$\u2014i.e. $p_t = [V_t]#(p_0)$. As $V_t$ is a valid flow and satisfies an ODE the change in log density can be computed using the instantaneous change of variables:\n$log p_t (x_1) = log p_0 (x_0) - \\int_0^1 \u2207u_t(x_t)dt,$\nwhere $x_t = V_t(x_0)$ and $\u2207$ is the divergence operator.\nA CNF can then be viewed as a neural flow that seeks to learn a designated target flow $V_t$ for all time $t \u2208 [0, 1]$.\nThe most scalable way to train CNFs is to employ a flow-matching learning framework (Liu, 2022; Albergo & Vanden-Eijnden, 2023; Lipman et al., 2023; Tong et al., 2023). Specifically, flow-matching regresses a learnable vector field of a CNF $f_{t,\u03b8}(t, \u00b7) : [0, 1] \u00d7 R^{n\u00d73} \u2192 R^{n\u00d73}$ to the target vector field $u_t(x_t)$ associated to the flow $V_t$. In practice, it is considerably easier to regress against a target conditional vector field $u_t(x_t|z)$\u2014which generates the con- ditional probability path $p_t(x_t|z)$\u2014as we do not have closed form access to the (marginal) vector field $u_t$ which gener- ates $p_t$. The conditional flow-matching (CFM) objective can then be stated as a simple simulation-free regression,\n$L_{CFM}(\u03b8) = E_{t,q(z),p_t(x_t|z)} ||f_{t,\u03b8} (t, x_t) - u_t(x_t|z)||^2$.\nThe conditioning distribution $q(z)$ can be chosen from any valid coupling, for instance, the independent coupling $q(z) = p(x_0)p(x_1)$. We highlight that Eq. 3 allows for greater flexibility in $f_{t,\u03b8}$ as there is no exact invertibility constraint. To generate samples and their corresponding log density according to the CNF we may solve the following flow ODE numerically with initial conditions $x_0 = \u03c8_0(x_0)$ and $c = log p_0 (x_0)$, which is the log density under the prior:\n$\\frac{d}{dt} \\begin{bmatrix} \u03c8_{t,\u03b8}(x_t) \\\\ log p_t (x_t) \\end{bmatrix} = \\begin{bmatrix} f_{t,\u03b8}(t, x_t) \\\\ \u2207u_t(x_t) \\end{bmatrix}$"}, {"title": "2.2. Boltzmann generators", "content": "A Boltzmann generator (No\u00e9 et al., 2019) $\u00b5_\u03b8$ pairs a normalizing flow as the proposal generative model $p_\u03b8$, which is then corrected to obtain i.i.d. samples under $\u00b5_{target}$ using importance sampling. More precisely, as normalizing flows are exact likelihood models, BG's first draw $K$ independent samples $x^i \u223c p_\u03b8(x)$, $i \u2208 [K]$ and compute the corresponding importance weights for each sample\n$w(x^i) = exp(\\frac{-E(x^i)}{k_BT}) / p_\u03b8(x^i)$. Leveraging the collection of importance weights we can compute a Monte-Carlo ap- proximation to any test function $f(x)$ of interest under $\u00b5_{target}$ using self-normalized importance sampling as follows:\n$E_{\u00b5_{target}(x)} [\u03c6(x)] = E_{p_\u03b8}[\u03c6(x)W(x)] \u2248 \\frac{\u2211_{i=1}^{K} w(x^i)\u03c6(x^i)}{\u2211_{i=1}^{K} w(x^i)}$\nIn addition, computing importance weights also enables resampling the pool of samples according to the collection of normalized importance weights $W = {w(x^i)}^K_{i=1}$."}, {"title": "3. SEQUENTIAL BOLTZMANN GENERATORS", "content": "We now present SBG which extends and improves over classical Boltzmann generators by adding a non-equilibrium sampling process that leads to higher-quality samples with reduced redundancy. We begin by identifying the key limitation in current BG's as importance sampling with a suboptimal proposal. Indeed, while the self-normalized importance sampling estimator is consistent, its' fidelity is highly dependent on the quality of the actual proposal $p_\u03b8$. In fact, the optimal proposal distribution is proportional to the minimizer of the variance of $\u03c6(x^i)\u00b5_{target}(x^i)$ (Owen, 2013). Unfortunately, since $p_\u03b8$ within a BG framework is trained on a biased dataset $D$ the importance weights computed typically exhibit large variance\u2014resulting in a small effective sample size (ESS).\nWe address the need for more flexible proposals in \u00a73.1 with modernized scalable training recipes for normalizing flows. In \u00a73.2 we outline our novel application of non-equilibrium sampling with Sequential Monte Carlo (Doucet et al., 2001) that powers our inference scaling algorithm to transport proposal samples and their importance weight towards the metastable states of $E(x)$. We term the overall process of combining a pre-trained Boltzmann generator with inference scaling through annealing: SEQUENTIAL BOLTZMANN GENERATORS.\nSymmetries of molecular systems. The energy function $E(x)$ in a molecular system using classical force fields is known to be invariant under global rotations and translation, which corresponds to the group $SE(3) \u2243 SO(3) \u00d7 (R^3, +)$. Unfortunately, $SE(3)$ is a non-compact group which does not allow for defining a prior density $p_0(x_0)$ on $R^{n\u00d73}$. Equivariant generative models circumvent this issue by defining a mean-free prior which is a projection of a Gaussian prior $N(0, I)$ onto the subspace $R^{(n\u22121)\u00d73}$ (Garcia Satorras et al., 2021). Thus pushing forward a mean free prior with an equivariant flow provably leads to an invariant proposal $p_1(x_1)$ (K\u00f6hler et al., 2020; Bose et al., 2021). We next build BG's by departing from exactly equivariant maps by instead considering soft-equivariance which opens up the usage of more scalable and efficient architectures."}, {"title": "3.1. Scaling training of Boltzmann generators", "content": "To improve proposal flows in SBG we favor scalable architectural choices that are more expressive than exactly equivariant ones. We motivate this choice by highlighting that many classes of normalizing flow models are known to be universal density approximators (Teshima et al., 2020; Lee et al., 2021). Thus, expressive enough non-equivariant flows can learn to approximate any equivariant map.\nSoft equivariance. We instantiate SBG with a state-of-the- art TarFlow (Zhai et al., 2024) which is based on Blockwise Masked Autoregressive Flow (Papamakarios et al., 2017) based on a causal Vision Transformer (ViT) (Alexey, 2021) modified for molecular systems where patches are over the particle dimension. Since the data comes mean-free we further normalize the data to a standard deviation of one. Combined, this allows us to scale both the depth and width of the models stably as there is no tension between a hard equivariance constraint and the invertibility of the network.\nWe include a series of strategies to improve training of non-equivariant flows by softly enforcing SE(3)- equivariance. First, we softly enforce equivariance to global rotations through data augmentation by sampling random rotations $R \u2208 SO(3)$ and applying them to data samples $Ro x_1 \u223c p_1(x_1)$. Secondly, as the data is mean-free and has (n - 1) \u00d7 d degrees of freedom we lift the data dimen- sionality back to n by adding noise to the center of mass. This allows us to easily train with a non-equivariant prior distribution such as the standard normal $p_0 = N(0, I)$. The next proposition outlines the family of permissible noise.\nProposition 1. Given an SE(3)-invariant $p_{target}(X)$ and the noise-adjusted distribution $\u00b5'_{target}(x)$. Consider the decomposition of a data sample into its constituent mean-free component, $x$ and center of mass $c \u2208 R^3$, $x = x+c$, where $c \u223c \u00b5(c)$ and $\u00b5(c)$ is SO(3)-invariant. Then $\u00b5_{target}(x) = \u00b5'_{target}(x)$ if $\u00b5'_{target}(x) = \u03bc(x)\u03bc(||c||)$.\nWe prove Proposition 1 in \u00a7B.1, which tells us that any noise distribution that acts on the norm of the center of mass does not operationally change the target. As a result, we choose to add small amounts of Gaussian noise $c \u223c N(0, \u03c3^2)$ to the center of mass of a given data sample. The impact of this noise is that during reweighting we must account for $\u03bc(||c||)$ which follows a \u03c7(3) distribution. Consequently, we must adjust the model energy to account for the impact of CoM noise during reweighting as follows:\n$log p(x) = log p_\u03b8(\\frac{x-c}{\u03c3}) + \\frac{||c^2||}{2\u03c3^3} + log \\frac{\u03bc(||c||)}{p_o(x)} + C ,$\nwhere $C = \u2013 log (\u221a2\u0393(\\frac{3}{2}))$ and $\u0393$ is the gamma function."}, {"title": "3.2. Inference time scaling of Boltzmann generators", "content": "Given a trained BG with proposal flow $p_\u03b8$, the simple importance sampling estimator suffers from a large variance of importance weights as the dimensionality and complexity of $\u00b5_{target}(x)$ grows in large molecular systems. We aim to address this bottleneck by proposing an inference time scaling algorithm that anneals samples $x^i \u223c p_\u03b8(x)$\u2014and corresponding unnormalized importance weights $w(x^i)$\u2014in a continuous manner towards $\u00b5_{target}$.\nImproving samples through non-equilibrium trans- port. We leverage a class of methods that fall under non-equilibrium sampling to improve the base proposal flow samples. One of the simplest instantiations of this idea is to use annealed Langevin dynamics with reweighting through a continuous-time variant of Annealed Importance Sampling (AIS) (Neal, 2001). Concretely, we consider the following SDE that drives proposal samples towards metastable states of the Boltzmann target:\n$dx_\u03c4 = \u2212\u03f5_\u03c4\u2207E_\u03c4(x_\u03c4)d\u03c4 + \u221a2\u03f5_\u03c4dW_\u03c4,$\nwhere $\u03f5_\u03c4 \u2265 0$ is a time-dependent diffusion coefficient and $W$ is the standard Wiener process. We distinguish $\u03c4$, from $t$ used in the context of training $p_\u03b8$, as the time variable that evolves initial proposal samples at $\u03c4 = 0$ towards the target at $\u03c4 = 1$. The energy interpolation $E_\u03c4$ is a design choice, and we opt for a simple linear interpolant $E_\u03c4 = (1 \u2212 \u03c4)E_0 + \u03c4E_1$, and set $E_0(x) = log p_\u03b8 (x)$. We highlight that unlike past work in pure sampling (M\u00e1t\u00e9 & Fleuret, 2023; Albergo & Vanden-Eijnden, 2024) which use the prior energy $E_0(x) = log p_0(x)$, our design affords the significantly more infor- mative proposal given by the pre-trained normalizing flow $p_\u03b8$. As such, there is often no need for additional learning during this step which we view as extending the inference capabilities of the original Boltzmann generator $\u00b5_\u03b8(x)$.\nTo compute test functions for the transported samples, and thus reweighting, we use a well-known and celebrated result known as Jarzynski's equality that enables the calculation of equilibrium statistics from non-equilibrium processes. We recall the main result, originally derived in Jarzynski (1997), and recently re-derived in continuous-time in the context of learning to sample by Vargas et al. (2024); Albergo & Vanden-Eijnden (2024) that makes explicit the time evolution of the new importance weights.\nProposition 2 (Albergo & Vanden-Eijnden (2024)). Let $(x_\u03c4, w_\u03c4)$ solve the coupled system of SDE / ODE\n$dx_\u03c4 = \u2212\u03f5_\u03c4\u2207E_\u03c4(x_\u03c4)d\u03c4 + \u221a2\u03f5_\u03c4dW\\$\n$d log w_\u03c4 = \u2212\u2202_\u03c4E_\u03c4(x_\u03c4)d\u03c4$ with $x_0 \u223c p_\u03b8, w_0 = 0$\nthen for any test function $\u03c6 : R^d \u2192 R$ we have\n$\\int \u03c6(x)\u00b5_{target}(x)dx = \\frac{E[w_\u03c4 \u03c6(x_\u03c4)]}{E[w_\u03c4]}$\nand\n$log \\frac{Z_\u03c4}{Z_0} = E[e^r] (Jarzynski's Equality)$.\nThe final samples $x_{\u03c4=1}$ can then be reweighted according to final importance weights $w_{\u03c4=1}$ that have lower variance than simple importance sampling in conventional BGs. It is crucial to highlight that through inference-time scaling, we never need to utilize the high-variance importance weights under the prior $p_0(x_0)$, and instead the proposal $p_\u03b8 (x_0)$ acts as a new prior for the annealing process. It is precisely this learned proposal distribution that $dlog w$ accounts for within the parlance of Annealed Importance Sampling. To evolve the Langevin SDE we require,\n$\u2207E_\u03c4(x_\u03c4) = (1 \u2212 \u03c4)\u2207(\u2212 log p_\u03b8(x_\u03c4)) + \u03c4\u2207(\\frac{E(x)}{k_BT}),$\nwhich requires efficient gradient computation through the log-likelihood estimation under the normalizing flow $p_\u03b8$ as given by Eq. 1. This presents the first point of distinction between finite flows and CNF's. The former class of flows trained using Eq. 1 gives fast exact likelihoods-especially for our scalable non-equivariant TarFlow model. In contrast, CNF's must simulate Eq. 4 and differentiate through an ODE solver to compute $\u2207log p_\u03b8(x_\u03c4)$ for each step of the Langevin SDE in Eq. 6. As a result, a TarFlow proposal is considerably cheaper to simulate and reweight with AIS than a CNF. In \u00a7A we present an alternate interpolant that does not require the proposal distribution during sampling which is appealing when only samples are needed but at the cost of more expensive computation of log weights. These paths are of interest in the setting of Boltzmann emulators and other generative models and are of independent interest but are not considered further in the context of SBG.\nTo further enable a reduced computational footprint we propose a strategy that eliminates the forward evolution of the initial proposal that already obtain high energy. Specifically, we can simulate a large number of samples via Eq. 11 and threshold using an energy threshold $\u03b3 > 0$, and evaluate the log weights of promising samples. We jus- tify our strategy by first remarking a lower bound to the log partition function of target using a Monte Carlo estimate,\n$log Z = log E_{p_\u03b8(x)}[exp(\\frac{-E(x)}{k_BT}) \\frac{p_\u03b8(x)}{p_o(x)}] \\ge  E_{x\u223cp_\u03b8(x)} [\\frac{exp(-\\frac{E(x)}{k_BT})}{p_o(x)}] = log Z\u0302.$\nPlugging this estimate in the definition of the target Boltzmann distribution we get an upper bound,\n$log \u00b5_{target}(x) \\le log (exp(\\frac{-E(x)}{k_BT})) - log Z\u0302$.\nAn upper bound on $\u00b5_{target}(x)$ allows us to threshold samples using the energy function, $E(x) > \u03b3$, of the target. Formally, this corresponds to truncating the target distribution $\u03bc\u0302_{target}(x) := IP (E(x) < \u03b3)$ which places zero mass on high energy conformations. Correcting flow samples with respect to this truncated target introduces an additional bias into the self-normalized importance sampling estimate, which precisely corresponds to the"}, {"title": "4. Experiments", "content": "We evaluate SBG on small peptides using classical force-fields as the energy function with exact experimental setups described in \u00a7E. To generate samples and their cor- responding weights we follow Algorithm 1 with resampling (lines 6-9) which is run on initial proposal samples and is equivalent to performing SMC (Doucet et al., 2001).\nDatasets. We consider small peptides composed of varying numbers of alanine amino acids, with some systems additionally incorporating an acetyl group and an N-methyl group. We investigate alanine systems of up to 6 amino acids. All datasets are generated from a single MD simulation in implicit solvent using a classical force field. For each system, the first 100ps is used for training, the next 20ps for validation, and the remainder serves as the test set. Therefore, some metastable states may not be represented in the training set. An exception is alanine dipeptide, for which we use the dataset from Klein & No\u00e9 (2024). In addition to the alanine systems, we also investigate the significantly larger protein Chignolin, consisting of 10 amino acids generated with the Anton supercomputer in Lindorff-Larsen et al. (2011). We provide additional dataset details in \u00a7D.\nBaselines. For baselines, we train prior state-of-the-art equivariant Boltzmann generators. Specifically, we use SE(3)-augmented coupling flow (Midgley et al., 2023a) as the exactly invertible and equivariant architecture and the equivariant ECNF employed in Transferrable Boltzmann Generators (Klein & No\u00e9, 2024). We also include an improved equivariant CNF (ECNF++) as a stronger baseline, see \u00a7E.3 for full details, which uses improved flow matching loss, improved data normalization, a larger network, improved learning rate schedule, and optimizer. We note that ECNF is equivariant to E(3) and hence generates samples of both global chiralities, which we resolve by applying a flip transformation as in Klein & No\u00e9 (2024), for further details see \u00a7F.\nMetrics. We report interatomic distances as a normalized density between the ground truth data, and initial proposal samples, as well as the energy histogram of the system for ground truth, initial proposal samples, transported samples, and the reweighted energy histogram. We also include Ramachandran plots (Ramachandran et al., 1963) for each molecular system studied that visualizes dihedral angles' distribution for the ground truth data distribution and the generated samples. We include additional quantitative met- rics that provide a finer grained evaluation of each method. Concretely, we compute the ESS, Wasserstein-1 distance on the energy distribution, and the Wasserstein-2 distance of the dihedral angles used in the Ramachandran plot."}, {"title": "4.1. Results", "content": "We evaluate SBG and our chosen baselines on alanine dipeptide (ALDP), trialanine (AL3), alanine tetrapeptide (AL4), and hexaalanine (AL6) with quantitative metrics summarized in Table 2 and Table 3 and generated samples In SBG @ 10k we generate 10k samples and directly report metrics on these samples. In SBG @100k we generate 100k samples and subsample to 10k after SMC to computl directly comparable metrics. For SE(3)-EACF we retrain this baseline on our more challenging version of ALDP and observe that performance degrades substantially at the selected 0.2% weight clipping threshold (c.f. \u00a7F for higher clipping thresholds). Furthermore, we find that on ALDP, our improved ECNF++ baseline obtains a 177% relative improvement in ESS over the previous SOTA ECNF from Klein & No\u00e9 (2024). Importantly, we observe SBG is the best method on the Wasserstein-1 energy distance E-W1 and Wasserstein-2 distance on dihedreal angles T-W2. As SBG involves resamples on a finite set of points, we observe that the higher number of particles (100k) results in consistently improved E-W\u2081 and T-W2. These results are further substantiated in Figure 3 which depicts the energy histograms of SBG in relation to the ground truth energy of the system and depicts a near perfect overlap.\nFor tripeptides, tetrapeptides, and hexapeptides we remark that the SE(3)-EACF baseline is too computationally ex- pensive and thus does not scale (c.f. Table 5). Consequently, we report metrics for our improved ECNF, ECNF++, and SBG. We observe that ECNF fails to learn effectively on the tri and tetrapeptides with E-W\u2081 exploding over 104, while our improved ECNF++ is orders of magnitude better. We highlight that SBG is the best method across all metrics, and in particular, we highlight that the improvements are more prominently driven by inference time scaling of proposal samples as observed in Figure 5, Figure 6, and Figure 7. As reweighted samples under SBG show extremely high over- lap with the ground truth target $\u00b5_{target}(x)$, we argue that SBG suc- cessfully solves these molecular systems in comparison to prior BG's. We also report in \u00a7F.2 the Ramachandran plots for each method. Finally, we include additional ablations such as the utility of CoM energy adjustment in Appendix F.\nInference scaling. To illustrate the scalability of SBG in relation to other methods we plot in Figure 4 the log-scale inference time for each dataset. In particular, for inference, we include the time to generate and reweight 10k samples. We observe an almost exponential scaling of ECNF as the size of the peptide grows, while SBG is dramatically faster at inference. We also plot T-W2 metric on AL3 as a function of Langevin timestep granularity which shows a monotonic decrease as the number of samples increase for the center of mass adjusted energy, demonstrating a dimension for inference-time scaling of SBG not present in standard Boltzmann generators. Furthermore, the standard energy function does not give monotonic improvement with increasing granularity, demonstrating the improvement achieved by the center of mass adjusted energy."}, {"title": "4.2. Scaling to decapeptides", "content": "We now apply SBG to a decapeptide in Chignolin. As no other baseline can scale to this molecule we report energy histograms and distance plots for SBG only in Figure 8. We observe success of SBG at matching the interatomic distance distribution. We additionally observe imperfect but reasonable proposal samples under the energy distribution whose energies are greatly improved after reweighting. Evidently the equilibrium distribution is not perfectly sampled as evidenced by the lack of density at the lowest energy states and large peaks in the reweighted distributions. Nevertheless, our application of SBG to Chignolin represents a significant step towards the scalability of BG's that previously struggled on even tripeptides like AL3 as found in the ECNF E-W\u2081 result in Table 3."}, {"title": "5. Related work", "content": "Boltzmann generators (BGs) (No\u00e9 et al., 2019) have been applied to both free energy estimation (Wirnsberger et al., 2020; Rizzi et al., 2023; Schebek et al., 2024) and molecular sampling. Initially, BGs relied on system-specific representations, such as internal coordinates, to achieve relevant sampling efficiencies (No\u00e9 et al., 2019; K\u00f6hler et al., 2021; Midgley et al., 2023b; K\u00f6hler et al., 2023; Dibak et al., 2022). However, these representations are generally not transferable across different systems, leading to the development of BGs in Cartesian coordinates (Klein et al., 2023b; Midgley et al., 2023a; Klein & No\u00e9, 2024). While this improves transferability, they are currently limited in scalability, struggling to extend beyond dipeptides. Scaling to larger systems typically requires sacrificing exact sampling from the target distribution (Jing et al., 2022; Abdin & Kim, 2023; Jing et al., 2024a; Lewis et al., 2024), which often includes coarse-graining. An alternative to direct sampling from $\u00b5_{target}(x)$ is to"}, {"title": "6. Conclusion", "content": "In this paper, we introduce SBG an extension to the Boltz- mann generator framework that scales inference through the use of non-equilibrium transport. Unlike past BG's in SBG, we scale training using a non-equivariant transformer-based TarFlow architecture with soft equivariance penalties to 6 peptides. In terms of limitations, using non-equilibrium sampling as presented in SBG does not enjoy easy applica- tion to CNFs due to expensive simulation, which limits the use of modern flow matching methods in a SBG context. Considering hybrid approaches that mix CNFs through distillation to an invertible architecture or consistency-based objectives is thus a natural direction for future work. Finally, considering other classes of scalable generative models such as autoregressive ones which also permit exact likelihoods is also a ripe direction orf future work."}, {"title": "7. Impact Statement", "content": "This work studies amortized sampling from Boltzmann den-"}]}