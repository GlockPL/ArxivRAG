{"title": "Lucy: Think and Reason to Solve Text-to-SQL", "authors": ["Nina Narodytska", "Shay Vargaftik"], "abstract": "Large Language Models (LLMs) have made significant progress in assisting\nusers to query databases in natural language. While LLM-based techniques\nprovide state-of-the-art results on many standard benchmarks, their perfor-\nmance significantly drops when applied to large enterprise databases. The\nreason is that these databases have a large number of tables with complex\nrelationships that are challenging for LLMs to reason about. We analyze\nchallenges that LLMs face in these settings and propose a new solution that\ncombines the power of LLMs in understanding questions with automated\nreasoning techniques to handle complex database constraints. Based on\nthese ideas, we have developed a new framework that outperforms state-of-\nthe-art techniques in zero-shot text-to-SQL on complex benchmarks.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have significantly enhanced AI agents' capacity to assist\nhumans in a variety of important tasks, including co-pilot programming [Chen et al., 2021,\nGitHub, Inc., 2021], program verification [Wu et al., 2024, Chakraborty et al., 2023], and\nmath problem solving [Zhou et al., 2024]. One of the fastest-growing areas in this space\nis the development of LLM-based assistants for querying SQL databases. In this task, a\nuser poses a question to a database in natural language. The agent's goal is to generate\nan SQL query that, when executed against the database, answers the user's question. Such\nassistance enables users with different levels of expertise to effectively analyze their data.\nRecently, LLM-based solutions have made significant progress in addressing the text-to-SQL\nproblem [Gao et al., 2024, Li et al., 2024a]. While GPT-based methods have quickly reached\nnear-human performance on academic benchmarks, like Spider [Yu et al., 2018], they strug-\ngle to provide high-quality user assistance on large industrial databases [Sequeda et al., 2023,\nLi et al., 2023]. One of the core challenges is that industrial databases model many objects\nwith complex relationships between them. To transform a natural language question into\nan SQL query, the LLM must effectively reason about these intricate relationships, which\nis highly non-trivial for LLM models. Interestingly, we found that GPT4 can even indicate\nin some cases that it needs help with logical reasoning on complex databases. Here is a com-\nmon GPT4 output message on a question that requires multiple joins from ACME insurance\ndatabase [Sequeda et al., 2023]: 'This join may need adjustment based on the actual logic\nof relating claims to policy coverage details.'. While we do provide the database schema as\npart of the input, it is still challenging for LLMs to formally reason about database logic.\nIn this work, we propose a new text-to-SQL framework, LUCY, designed for large databases\nwith complex relationships between objects. Our main underlying idea is to combine the\nability of LLM models to effectively relate user questions to database objects with the\npower of automated reasoning to analyze relationships between these objects. The LUCY\nworkflow consists of three high-level steps. First, upon receiving a user's question, we\nidentify the relevant objects and their attributes in the target database. In the second step,"}, {"title": "Motivation", "content": "To provide high-quality user assistance in text-to-SQL tasks, we face two types of challenges.\nThe first type of challenge comes from the formulation of the user's question. A question\ncan be poorly specified, ambiguous, or require additional knowledge that is not present in\nthe question. For example, the user might ask to list clients eligible for a loan; however,\nthe eligibility criteria are not present in the question [Li et al., 2023, 2024b]. The second\nclass is related to the complexity of the queried database that can have a large number\nof tables with complex relations between them [Sequeda et al., 2023, Li et al., 2023]. In\nthis work, we focus on the second class. One approach to deal with complex relationships\nis to introduce an intermediate layer, like a knowledge graph or ontology structure, that\ncontains rich information about the underlying database. Then, LLMs generate queries\nto this knowledge graph using specialized languages, e.g., SPARQL, [Sequeda et al., 2023].\nIn turn, these queries can be automatically translated to SQL. While this approach does\nshow promise, it does not alleviate the core issue: an LLM is still expected to reason\nabout complex relations between objects in this intermediate representation. Moreover,\nsuch a rich intermediate layer, like an ontology, might not be easy to obtain for a database.\nOther standard techniques, like additional training, multi-shot or fine-tuning, also rely on\nLLMs to perform constrained reasoning steps [Gao et al., 2023, Pourreza and Rafiei, 2024,"}, {"title": "Database description", "content": "Gao et al., 2024]. To the best of our knowledge, dealing with complex relationships in\ntext-to-SQL remains an open problem. In order to isolate the underlying challenges in this\nproblem, we created an example database that covers standard relationship patterns adopted\nin industry and academia. We identified a set of simple and clearly formulated questions\nand demonstrated that even on this simplified schema and clear questions, state-of-the-art\nLLMs struggle to assist the user.\nWe describe a minimal example database schema that contains basic relations, like 1:1\nand 1:m, and more advanced relationship patterns, like m:m and STAR, and analyze the\nperformance of LLMs on this schema (See Appendix A for relational database definitions).\nSuppose a business sells cloud compute resources to customers and uses a database, DDO,\nto manage its Day-to-Day Operations. Figure 1 shows objects' corresponding tables, their\nrelationships, and a subset of attributes. In particular, each table has a primary key, e.g.,\nLOCATION.id, and might have foreign keys to refer to another table, e.g., CLIENT refers\nto LOCATION using CLIENT.loc_id. All attributes relevant to our examples are shown in\nFigure 1 with self-explanatory names. DDO manages payments (PAYMENT) and marketing\nretention strategies (RETENTION) for clients (CLIENT) and resources (RESOURCEPOOL) in\ndatacenters (DATACENTER). This example is in part inspired by the VMware vSphere\ndata model (discussed in Section 5). The full data model contains hundreds of types of\nresources that form deep tree-like structures [Managed Object, 2024]. Next, we consider\nhow relationships between objects are modeled in DDO. Figure 1 already defines basic\nrelationships, including 1:1 (dotted edges) and 1:m (solid edges).\nMany-to-many (m:m). CLIENT and RESOURCEPOOL are related via a m:m relationship\n(the dashed edge) meaning that a client might use multiple resource pools and one resource\npool can serve multiple clients. The table RSPOOL2CLIENT models this relation.\nStar. A STAR pattern is a type of database schema composed of a single, central fact\ntable surrounded by dimension tables. There are two groups of objects connected in a\nSTAR patterns in our example. STAR A keeps track of retention marketing strategies for\neach client that can be either GIFT or/and BONUS. STAR B records clients' payments\n(PAYMENT). Payments' amounts are stored in the PAYAMOUNT table. Each amount can be\nexactly one of three types: TAX, SUPERCHARGE, and INCOME.\nSnowflake. A SNOWFLAKE schema consists of one fact table connected to many dimension\ntables, which can be connected to other dimension tables through a many-to-one relationship.\nIn DDO, database resource pools are modeled using the snowflake pattern. Each resource\npool has configurations (CONFIG) and snapshots of the current usage (RUNTIME). CONFIG\nand RUNTIME have two children nodes each to define CPU and memory properties.\nLookup. A LOOKUP table is a table that contains descriptions and code values used by\nmultiple tables, e.g., zip codes, country names. etc. In DDO, LOCATION is a lookup table\nthat stores geo-location related data for quick access."}, {"title": "User questions", "content": "We consider three simple questions to DDO that are well formulated: outputs are explicitly\nspecified, so no additional information is needed to answer them. We use GPT4 (\u2018gpt-4-0125-\npreview'), and PROMPTB [Sequeda et al., 2023] for these questions. For each question, we\npresent a ground truth answer and a GPT answer. Table 1 presents both questions (Q3 is\npresented in Appendix C.1).\nQuestion Q1 is 'List customers who use datacenters with names starting with 'dev'. Out-\nput clients and datacenters names'. The user asks for information that relates clients and\ndatacenters. Consider GPT's answer. GPT misses the core logic of the database: clients\nand datacenter resources are related via a m:m relation (modeled with RSPOOL2CLIENT).\nGPT outputs clients and datacenters that share the same location, which is incorrect.\nQuestion Q2 is 'List resource pool names with CPU overhead limit greater than runtime\noverall usage by 100'. Here the user asks about resource pool properties. However, the\nGPT answer ignores the database's primary/foreign relations. It performs an inner"}, {"title": "Framework design", "content": "In this section, we present our framework LUCY. Figure 2 illustrates the workflow dia-\ngram, and Algorithm 1 shows the main steps of the workflow. There are two inputs to\nthe framework. The first input is a user question Q. The second input is DBMODEL,\nwhich is a description of the database schema that we discuss in the next section (Sec-\ntion 3.1). The workflow consists of three sequential subtasks: MatchTables, GenerateView,\nand QueryView. MatchTables identifies the relevant tables and their attributes related to\nthe user question (Section 3.2). GenerateView finds a combined view of relevant tables\ntaking into account database constraints (Section 3.3). The third phase, QueryView, takes\nV and the user question Q and produces an SQL query Q for V (Section 3.4). To simplify\nnotations, we assume that DBMODEL is a global variable in Algorithm 1."}, {"title": "Database model (dbModel)", "content": "We start with DBMODEL, or DBM for short. DBM is a data structure that contains aggre-\ngated information about the database, maintained as a JSON structure. DBM should be\nconstructed once for a database as the structure of the database is relatively stable. DBM\ncan always be extended if the database requires modifications. Here are the two main blocks\nof DBM:\nDatabase schema. The schema is written using the SQL Data Definition Language (CRE-\nATE TABLE statements). It includes table names, names and types of columns in each table,\nand database constraints such as primary and foreign keys. It can also contain optional user\ncomments associated with each table and column. We refer to tables and constraints as"}, {"title": "The MatchTables phase", "content": "Algorithm 1 shows MatchTables in lines 2-12. First, the algorithm focuses on tables that are\nnot inner tables of any patterns. We refer to such tables as core tables (core_tables in line 3).\nFor example, Figure 3 shows core tables for DDO. Next, we ask LLM to find relevant tables\namong these core tables using PROMPTA in line 5. (Appendix D.2.1 shows a PROMPTA\nwith a few examples.) As a result, we obtain a set of relevant core tables. We explore\nthem one by one in the loop in line 7. If it is a root table of a pattern, we perform a search\ninside the corresponding pattern to find more relevant tables using a breadth-first deepening\nprocedure, ITERATIVEPROMPTING, in line 10 (Algorithm 2 shows ITERATIVEPROMPTING'S"}, {"title": "The GenerateView phase", "content": "The MatchTables phase identifies a set of relevant tables and their attributes. Next, we\nconstruct a view table that combines relevant tables and attributes into a single table.\nWe build an abstract schema graph G which provides a graph view of DBM, and define a\nCSP over this graph. For each table ti in DBM.tables, we introduce a node in G. We use\nthe names ti to refer to the corresponding nodes. For each pair of tables ti and tj, s.t.\n(ti, tj) \u2208 DBM.constraints, we introduce an edge that connects them. We denote V the set of\nnodes in G and E its edges. Figure 3 illustrates a part of the graph (core tables) for DDO.\nAlgorithm 1 shows three main steps of this phase: build an abstract graph representation G\nof the schema (line 15); formulate and solve CSP to obtain a path P (line 17); and perform\njoins along this path to obtain the designed view V (line 19). Next, we describe these steps.\nProblem Formulation. Let T = tables(RT) be a set of relevant tables returned by\nMatchTables. We formulate the problem of finding a path Pin G that visits a set of nodes\nT and satisfies a set of database constraints.\n(C1) P must be a valid path in G. This ensures that we follow primary/foreign keys\nrelationships, i.e., 1:1, 1:m, and build a valid view.\n(C2) P visits all relevant tables T. This ensures combining all relevant tables to a view.\n(C3) Consider (ti, tj, tk) \u2208 DBM.m:m. If ti \u2208 P and tj \u2208 P then te must occur in P once\nbetween ti and tj. These constraints enforce m:m relationships.\n(C4) If t\u2208 Pand t \u2208 DBM.lookup then t's predecessor equals its successor in P. This\nensures that a lookup table serves as a look-up function for each table individually.\n(C5) Cost function: we minimize the number of occurrences of tables outside of T in\nP. A shorter path that focuses on the tables in T allows us to build more succinct\nviews.\n(C1)-(C5) are common constraints that we encounter in the benchmark sets. In general, the\nuser can specify more constraints to capture the logical relationships of the modeled data.\nConstraint satisfaction problem (CSP). We define a CSP formulation S of constraints\n(C1)-(C5). We start with a basic formulation. Let n be the maximum length of the path P.\nFor each node ti in G and step r, where r \u2208 [1, n], we introduce a Boolean variable $b_{i}^{r}$. $b_{i}^{r}$ is\ntrue iff ti is the rth node in P. We also introduce a sink-node Boolean variable $b_{y}$ for each"}, {"title": "The QueryView phase", "content": "layer to model paths that are shorter than n. S contains the following logical constraints:\n(C5):\n$minimize \\sum_{i,t.i \\notin T} occ_{i}$\n(1)\n\u2200i.ti \u2208 V\n$occ_{i} = b_{i}^{1} + ... + b_{i}^{n}$\n(2)\n(C1):\n\u2200i.ti \u2208 V, r \u2208 [1, n \u2212 1]\n$b_{i}^{r} \u21d2 (\\bigvee_{(Vj.(t_{i},t_{j})\\in E} b_{j}^{r+1}) \u2228 b_{y}^{r+1})$\n(3)\n(C2):\n\u2200i.ti \u2208 T\n$occ_{i} \u2265 1$\n(4)\n(C3):\n\u2200k.(ti, tj, tk) \u2208 DBM.m:m\n$occ_{k} = 1$\n(5)\n(C3):\n\u2200k.(ti, tj, tk) \u2208 DBM.m:m, r \u2208 [2, n-1] $b_{k}^{r} \u21d2 ((\\neg b_{i}^{r-1} \u2227 b_{i}^{r+1}) \u2228 (\\neg b_{j}^{r-1} \u2227 b_{j}^{r+1}))$\n(6)\n(C4):\n\u2200i.ti \u2208 DBM.lookup, r \u2208 [2, n \u2212 1]\n$b_{i}^{r} \u21d2 (b_{i}^{r-1} \u21d4 b_{i}^{r+1})$\n(7)\n\u2200r \u2208 [1, n]\n$b_{1}^{r} + ... + b_{[v]}^{r} = 1$\n(8)\n\u2200r\u2208 [1, n - 1]\n$b_{y}^{r}\u21d2 b_{y}^{r+1}$\n(9)\nConsider the encoding S. Equations 2 specify integer variables, occi, for i \u2208 [1,n], that\ncount the occurrences of each table in the path. Equations 8 encode that only one node\nbelongs to a path at each step. Equations 9 encode that if the path visits the sink node,\nthen it must stay there. Other equations encode constraints (C1)-(C5). By construction,\nEquations 1-9 generate a valid path in G that satisfies the constraints (C1)-(C5).\nExample 3.2. For Q1, solving S gives the green path between DATACENTER and CLIENT\nin Figure 3. S rules out the red path as we enforce constraint (C4) and optimization (C5).\nImprovements of CSP. Our basic model S can be improved to take advantage of STAR\nand SNOWFLAKE patterns. Namely, we can leverage the decomposition of G and find a path\nPamong core tables only. Then, for each core table in P that is a pattern root, and for\neach inner relevant table in this pattern, we build a path P' along the corresponding branch.\nFor example, Figure 1 shows two paths from RESOURCEPOOL to CCPU (an orange path)\nand RCPU (a blue path). We use left join to combine tables along each such branch.\nFinally, we combine P and P's into a single view.\nSummary view. Given a path P in a graph, we join tables along the path using their\nprimary and foreign key relations. We keep the same set of attributes that MatchTables\nidentified. An example of the V for Q1 that corresponds to the green path in Figure 3 is\nshown in the listing in Table 7 in Appendix D.3.1.\nQueryView takes the summary view V along with the user question, and prompts an LLM\nto obtain the final SQL using PROMPTC (line 21 in Algorithm 1). PROMPTC is defined in\nAppendix D.4.1. The listing in Table 7 shows an SQL Q to answer Q1 (Appendix D.3.1)."}, {"title": "Discussion on strengths and limitations", "content": "LUCY is designed based on the principle of separation of responsibilities between\ngenerative tasks and automated reasoning tasks: each step focuses on either an NLP-related\nsubproblem or a constraint reasoning subproblem. This separation allows us to support a\nnumber of unique capabilities. First, LUCY shifts the burden of complex reasoning from\nLLMs to constraint solvers. Second, we support reasoning on complex relationships, like\nm:m, LOOKUP, STAR or SNOWFLAKE. Third, our framework is flexible and extensible as\nit is easy to incorporate domain-specific constraints as soon as they can be expressed by\nconstraint modeling language. This assumes that the user has a data analytics role and\nunderstands the logic of the database. Such formal reasoning capability is important, as it\nis hard to control LLMs via prompts when non-trivial reasoning is required. Fourth, we can\nevaluate each phase and diagnose LUCY failure modes. For example, if MatchTables misses\nrelevant tables, this indicates that we need to provide more information about the schema\nto an LLM. Fifth, based on our evaluation, LUCY can support complex queries that include"}, {"title": "Experimental evaluation", "content": "multiple filtering operators and aggregators, e.g. average or sum. This capability follows\nfrom the QueryView phase as the final call to an LLM is performed on a single view table.\nLimitations. The first limitation is that we cannot guarantee that the SQL query answers\nthe user's question. Given the current state of the art, providing such guarantees is beyond\nthe reach of any copilot method that takes natural language descriptions and outputs struc-\ntured text, like code or SQL. However, our solution does guarantee that V satisfies database\nconstraints, which is a step forward in this direction. Second, we do not support questions\nthat require union operators in the GenerateView phase. In fact, there are no benchmarks\navailable that require the union operator to answer questions. Supporting union would\nrequire an extension of MatchTables and GenerateView. Third, we observed experimen-\ntally that LUCY struggles with certain types of queries that involve a particular interleaving\nordering of filtering and aggregate operators or question-specific table dependencies, like a\nlookup table that has to be used multiple times to answer the user's question. We further\ndiscuss such questions in our experiments.\nIn our experimental evaluation, we aim to answer the main questions:\n\u2022\nIs LUCY competitive with existing LLM-based approaches?\n\u2022\n\u2022\nCan we debug LUCY to gain insights about failure modes?\nCan LUCY handle complex questions?\nSetup. We compare with the following zero-shot baselines: GPT4, NSQL, and CHAT2QUERY\n(C2Q for short). GPT4 and c2Q methods are the best zero-shot techniques according to\nthe BIRD leadership board that are accessible for evaluation [Li et al., 2024b]. NSQL is\nthe best open-source large foundation model designed specifically for the SQL generation\ntask [Labs, 2023b]. CHAT2QUERY is closed-source but the authors kindly extended their\nAPI that we can run experiments with GPT4. We provide all benchmarks and frameworks'\nresults in the supplementary materials. For GPT4 and LUCY, we use the \u2018gpt-4-0125-preview'\nAPI without fine-tuning. We use OR-Tools as a constraint solver [Perron and Didier, 2024]\n(Appendix E.1 provides full details of the experimental setup).\nEvaluation metrics. We use the standard Execution Accuracy (ex) [Li et al., 2023]. In\naddition, we consider a relaxation of this metric. We noticed that frameworks often add ad-\nditional attributes to the output as the exact format of the output is rarely specified. Hence,\nwe extend ex to esx metrics that check if the output of a framework contains the ground\ntruth outputs. To better understand performance characteristics and possible failure modes,\nwe consider the coverage metric that captures whether a framework correctly identified a\nsubset of relevant tables and attributes. Let sqlg be the ground truth answer and sqlF be a\ngenerated query. Then we assess the percentage of the ground truth content slqF captures:\n$cou_{T} = \\frac{|tables(slqF) \\cap tables(slqG)|}{|tables(slqG)|}$\n(10)\n$cou_{a} = \\frac{|attributes(slqF) \\cap attributes(slqG)|}{|attributes(slqG)|}$\nwhere tables () and attributes () are functions that return a set of tables and attributes.\nACME insurance. We consider the ACME insurance dataset that was recently pub-\nlished [Sequeda et al., 2023]. The dataset represents an enterprise relational database\nschema in the insurance domain. The authors focused on a subset of 13 tables out of\n200 tables and proposed a set of 45 challenging questions. We identified two STAR patterns\nin this database. The authors showed that their method (DW) solved 24 out of 45 problems\nusing intermediate representation of a knowledge graph, while GPT4 solved only 8 problems."}, {"title": "ACME insurance", "content": "Cloud resources. Next, we propose a new benchmark based on the vSphere API data\nmodel [VMware, Inc., 2024]. We experimented with this publicly available data model of\nan industrial product, as it is well-documented and easily accessible via a web interface. It\ndescribes the state of the system as well as its configuration parameters. States are stored\nin a database and queried by customers to keep track of performance, maintenance, and\ndata analysis. We extracted the descriptions of main objects in Managed Object [2024],\nincluding data centers, resource pools, hosts, and virtual machines and their properties, and\nbuilt a database that captures these relationships using 52 tables. Overall, we have two\nSTARS, five SNOWFLAKES and two m:ms patterns. For each table and an attribute, we\nget descriptions from [Managed Object, 2024]. As these can be a lengthy description, we"}, {"title": "Cloud resources", "content": "Cloud resources. Next, we propose a new benchmark based on the vSphere API data\nmodel [VMware, Inc., 2024]. We experimented with this publicly available data model of\nan industrial product, as it is well-documented and easily accessible via a web interface. It\ndescribes the state of the system as well as its configuration parameters. States are stored\nin a database and queried by customers to keep track of performance, maintenance, and\ndata analysis. We extracted the descriptions of main objects in Managed Object [2024],\nincluding data centers, resource pools, hosts, and virtual machines and their properties, and\nbuilt a database that captures these relationships using 52 tables. Overall, we have two\nSTARS, five SNOWFLAKES and two m:ms patterns. For each table and an attribute, we\nget descriptions from [Managed Object, 2024]. As these can be a lengthy description, we"}]}