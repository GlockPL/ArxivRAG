{"title": "Lucy: Think and Reason to Solve Text-to-SQL", "authors": ["Nina Narodytska", "Shay Vargaftik"], "abstract": "Large Language Models (LLMs) have made significant progress in assisting\nusers to query databases in natural language. While LLM-based techniques\nprovide state-of-the-art results on many standard benchmarks, their perfor-\nmance significantly drops when applied to large enterprise databases. The\nreason is that these databases have a large number of tables with complex\nrelationships that are challenging for LLMs to reason about. We analyze\nchallenges that LLMs face in these settings and propose a new solution that\ncombines the power of LLMs in understanding questions with automated\nreasoning techniques to handle complex database constraints. Based on\nthese ideas, we have developed a new framework that outperforms state-of-\nthe-art techniques in zero-shot text-to-SQL on complex benchmarks.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have significantly enhanced AI agents' capacity to assist\nhumans in a variety of important tasks, including co-pilot programming [Chen et al., 2021,\nGitHub, Inc., 2021], program verification [Wu et al., 2024, Chakraborty et al., 2023], and\nmath problem solving [Zhou et al., 2024]. One of the fastest-growing areas in this space\nis the development of LLM-based assistants for querying SQL databases. In this task, a\nuser poses a question to a database in natural language. The agent's goal is to generate\nan SQL query that, when executed against the database, answers the user's question. Such\nassistance enables users with different levels of expertise to effectively analyze their data.\nRecently, LLM-based solutions have made significant progress in addressing the text-to-SQL\nproblem [Gao et al., 2024, Li et al., 2024a]. While GPT-based methods have quickly reached\nnear-human performance on academic benchmarks, like Spider [Yu et al., 2018], they strug-\ngle to provide high-quality user assistance on large industrial databases [Sequeda et al., 2023,\nLi et al., 2023]. One of the core challenges is that industrial databases model many objects\nwith complex relationships between them. To transform a natural language question into\nan SQL query, the LLM must effectively reason about these intricate relationships, which\nis highly non-trivial for LLM models. Interestingly, we found that GPT4 can even indicate\nin some cases that it needs help with logical reasoning on complex databases. Here is a com-\nmon GPT4 output message on a question that requires multiple joins from ACME insurance\ndatabase [Sequeda et al., 2023]: 'This join may need adjustment based on the actual logic\nof relating claims to policy coverage details.'. While we do provide the database schema as\npart of the input, it is still challenging for LLMs to formally reason about database logic.\nIn this work, we propose a new text-to-SQL framework, LUCY, designed for large databases\nwith complex relationships between objects. Our main underlying idea is to combine the\nability of LLM models to effectively relate user questions to database objects with the\npower of automated reasoning to analyze relationships between these objects. The LUCY\nworkflow consists of three high-level steps. First, upon receiving a user's question, we\nidentify the relevant objects and their attributes in the target database. In the second step,"}, {"title": "Motivation", "content": "To provide high-quality user assistance in text-to-SQL tasks, we face two types of challenges.\nThe first type of challenge comes from the formulation of the user's question. A question\ncan be poorly specified, ambiguous, or require additional knowledge that is not present in\nthe question. For example, the user might ask to list clients eligible for a loan; however,\nthe eligibility criteria are not present in the question [Li et al., 2023, 2024b]. The second\nclass is related to the complexity of the queried database that can have a large number\nof tables with complex relations between them [Sequeda et al., 2023, Li et al., 2023]. In\nthis work, we focus on the second class. One approach to deal with complex relationships\nis to introduce an intermediate layer, like a knowledge graph or ontology structure, that\ncontains rich information about the underlying database. Then, LLMs generate queries\nto this knowledge graph using specialized languages, e.g., SPARQL, [Sequeda et al., 2023].\nIn turn, these queries can be automatically translated to SQL. While this approach does\nshow promise, it does not alleviate the core issue: an LLM is still expected to reason\nabout complex relations between objects in this intermediate representation. Moreover,\nsuch a rich intermediate layer, like an ontology, might not be easy to obtain for a database.\nOther standard techniques, like additional training, multi-shot or fine-tuning, also rely on\nLLMs to perform constrained reasoning steps [Gao et al., 2023, Pourreza and Rafiei, 2024,"}, {"title": "Database description", "content": "We describe a minimal example database schema that contains basic relations, like 1:1\nand 1:m, and more advanced relationship patterns, like mim and STAR, and analyze the\nperformance of LLMs on this schema (See Appendix A for relational database definitions).\nSuppose a business sells cloud compute resources to customers and uses a database, DDO,\nto manage its Day-to-Day Operations. Figure 1 shows objects' corresponding tables, their\nrelationships, and a subset of attributes. In particular, each table has a primary key, e.g.,\nLOCATION.id, and might have foreign keys to refer to another table, e.g., CLIENT refers\nto LOCATION using CLIENT.loc_id. All attributes relevant to our examples are shown in\nFigure 1 with self-explanatory names. DDO manages payments (PAYMENT) and marketing\nretention strategies (RETENTION) for clients (CLIENT) and resources (RESOURCEPOOL) in\ndatacenters (DATACENTER). This example is in part inspired by the VMware vSphere\ndata model (discussed in Section 5). The full data model contains hundreds of types of\nresources that form deep tree-like structures [Managed Object, 2024]. Next, we consider\nhow relationships between objects are modeled in DDO. Figure 1 already defines basic\nrelationships, including 1:1 (dotted edges) and 1:m (solid edges).\nMany-to-many (m:m). CLIENT and RESOURCEPOOL are related via a m:m relationship\n(the dashed edge) meaning that a client might use multiple resource pools and one resource\npool can serve multiple clients. The table RSPOOL2CLIENT models this relation.\nStar. A STAR pattern is a type of database schema composed of a single, central fact\ntable surrounded by dimension tables. There are two groups of objects connected in a\nSTAR patterns in our example. STAR A keeps track of retention marketing strategies for\neach client that can be either GIFT or/and BONUS. STAR B records clients' payments\n(PAYMENT). Payments' amounts are stored in the PAYAMOUNT table. Each amount can be\nexactly one of three types: TAX, SUPERCHARGE, and INCOME.\nSnowflake. A SNOWFLAKE schema consists of one fact table connected to many dimension\ntables, which can be connected to other dimension tables through a many-to-one relationship.\nIn DDO, database resource pools are modeled using the snowflake pattern. Each resource\npool has configurations (CONFIG) and snapshots of the current usage (RUNTIME). CONFIG\nand RUNTIME have two children nodes each to define CPU and memory properties.\nLookup. A LOOKUP table is a table that contains descriptions and code values used by\nmultiple tables, e.g., zip codes, country names. etc. In DDO, LOCATION is a lookup table\nthat stores geo-location related data for quick access."}, {"title": "User questions", "content": "We consider three simple questions to DDO that are well formulated: outputs are explicitly\nspecified, so no additional information is needed to answer them. We use GPT4 (\u2018gpt-4-0125-\npreview'), and PROMPTB [Sequeda et al., 2023] for these questions. For each question, we\npresent a ground truth answer and a GPT answer. Table 1 presents both questions (Q3 is\npresented in Appendix C.1).\nQuestion Q1 is 'List customers who use datacenters with names starting with 'dev'. Out-\nput clients and datacenters names'. The user asks for information that relates clients and\ndatacenters. Consider GPT's answer. GPT misses the core logic of the database: clients\nand datacenter resources are related via a mim relation (modeled with RSPOOL2CLIENT).\nGPT outputs clients and datacenters that share the same location, which is incorrect.\nQuestion Q2 is 'List resource pool names with CPU overhead limit greater than runtime\noverall usage by 100'. Here the user asks about resource pool properties. However, the\nGPT answer ignores the database's primary/foreign relations. It performs an inner"}, {"title": "Framework design", "content": "In this section, we present our framework LUCY. Figure 2 illustrates the workflow dia-\ngram, and Algorithm 1 shows the main steps of the workflow. There are two inputs to\nthe framework. The first input is a user question Q. The second input is DBMODEL,\nwhich is a description of the database schema that we discuss in the next section (Sec-\ntion 3.1). The workflow consists of three sequential subtasks: MatchTables, GenerateView,\nand QueryView. MatchTables identifies the relevant tables and their attributes related to\nthe user question (Section 3.2). GenerateView finds a combined view of relevant tables\ntaking into account database constraints (Section 3.3). The third phase, QueryView, takes\nV and the user question Q and produces an SQL query Q for V (Section 3.4). To simplify\nnotations, we assume that DBMODEL is a global variable in Algorithm 1."}, {"title": "Database model (dbModel)", "content": "We start with DBMODEL, or DBM for short. DBM is a data structure that contains aggre-\ngated information about the database, maintained as a JSON structure. DBM should be\nconstructed once for a database as the structure of the database is relatively stable. DBM\ncan always be extended if the database requires modifications. Here are the two main blocks\nof DBM:\nDatabase schema. The schema is written using the SQL Data Definition Language (CRE-\nATE TABLE statements). It includes table names, names and types of columns in each table,\nand database constraints such as primary and foreign keys. It can also contain optional user\ncomments associated with each table and column. We refer to tables and constraints as"}, {"title": "The MatchTables phase", "content": "The first phase, MatchTables, needs to find relevant tables and their attributes to the\nuser question. One approach to achieve that can be to provide the schema and a question\nto an LLM and ask for this information. However, one of the distinguishing features of\nreal-world databases is their large number of tables and attributes. Hence, feeding all of\nthem along with their descriptions to the prompt might not be feasible for many LLM\nmodels. Therefore, we build an iterative procedure that takes advantage of database tree-\nlike patterns. In general, this procedure can be customized to best support the structure of\na database."}, {"title": "The GenerateView phase", "content": "The MatchTables phase identifies a set of relevant tables and their attributes. Next, we\nconstruct a view table that combines relevant tables and attributes into a single table.\nWe build an abstract schema graph G which provides a graph view of DBM, and define a\nCSP over this graph. For each table ti in DBM.tables, we introduce a node in G. We use\nthe names ti to refer to the corresponding nodes. For each pair of tables ti and tj, s.t.\n(ti, tj) \u2208 DBM.constraints, we introduce an edge that connects them. We denote V the set of\nnodes in G and E its edges. Figure 3 illustrates a part of the graph (core tables) for DDO.\nAlgorithm 1 shows three main steps of this phase: build an abstract graph representation G\nof the schema (line 15); formulate and solve CSP to obtain a path P (line 17); and perform\njoins along this path to obtain the designed view V (line 19). Next, we describe these steps.\nProblem Formulation. Let T = tables(RT) be a set of relevant tables returned by\nMatchTables. We formulate the problem of finding a path Pin G that visits a set of nodes\nT and satisfies a set of database constraints.\n(C1) P must be a valid path in G. This ensures that we follow primary/foreign keys\nrelationships, i.e., 1:1, 1:m, and build a valid view.\n(C2) P visits all relevant tables T. This ensures combining all relevant tables to a view.\n(C3) Consider (ti, tj, tk) \u2208 DBM.m:m. If ti \u2208 P and tj \u2208 P then te must occur in P once\nbetween ti and tj. These constraints enforce m:m relationships.\n(C4) If t\u2208 Pand t \u2208 DBM.lookup then t's predecessor equals its successor in P. This\nensures that a lookup table serves as a look-up function for each table individually.\n(C5) Cost function: we minimize the number of occurrences of tables outside of T in\nP. A shorter path that focuses on the tables in T allows us to build more succinct\nviews.\n(C1)-(C5) are common constraints that we encounter in the benchmark sets. In general, the\nuser can specify more constraints to capture the logical relationships of the modeled data.\nConstraint satisfaction problem (CSP). We define a CSP formulation S of constraints\n(C1)-(C5). We start with a basic formulation. Let n be the maximum length of the path P.\nFor each node ti in G and step r, where r \u2208 [1, n], we introduce a Boolean variable b. b is\ntrue iff ti is the rth node in P. We also introduce a sink-node Boolean variable by for each"}, {"title": "The QueryView phase.", "content": "QueryView takes the summary view V along with the user question, and prompts an LLM\nto obtain the final SQL using PROMPTC (line 21 in Algorithm 1). PROMPTC is defined in\nAppendix D.4.1. The listing in Table 7 shows an SQL Q to answer Q1 (Appendix D.3.1)."}, {"title": "Discussion on strengths and limitations", "content": "Strengths. LUCY is designed based on the principle of separation of responsibilities between\ngenerative tasks and automated reasoning tasks: each step focuses on either an NLP-related\nsubproblem or a constraint reasoning subproblem. This separation allows us to support a\nnumber of unique capabilities. First, LUCY shifts the burden of complex reasoning from\nLLMs to constraint solvers. Second, we support reasoning on complex relationships, like\nm:m, LOOKUP, STAR or SNOWFLAKE. Third, our framework is flexible and extensible as\nit is easy to incorporate domain-specific constraints as soon as they can be expressed by\nconstraint modeling language. This assumes that the user has a data analytics role and\nunderstands the logic of the database. Such formal reasoning capability is important, as it\nis hard to control LLMs via prompts when non-trivial reasoning is required. Fourth, we can\nevaluate each phase and diagnose LUCY failure modes. For example, if MatchTables misses\nrelevant tables, this indicates that we need to provide more information about the schema\nto an LLM. Fifth, based on our evaluation, LUCY can support complex queries that include"}, {"title": "Experimental evaluation", "content": "In our experimental evaluation, we aim to answer the main questions:\n\u2022 Is LUCY competitive with existing LLM-based approaches?\n\u2022 Can we debug LUCY to gain insights about failure modes?\n\u2022 Can LUCY handle complex questions?\nSetup. We compare with the following zero-shot baselines: GPT4, NSQL, and CHAT2QUERY\n(C2Q for short). GPT4 and c2Q methods are the best zero-shot techniques according to\nthe BIRD leadership board that are accessible for evaluation [Li et al., 2024b]. NSQL is\nthe best open-source large foundation model designed specifically for the SQL generation\ntask [Labs, 2023b]. CHAT2QUERY is closed-source but the authors kindly extended their\nAPI that we can run experiments with GPT4. We provide all benchmarks and frameworks'\nresults in the supplementary materials. For GPT4 and LUCY, we use the \u2018gpt-4-0125-preview'\nAPI without fine-tuning. We use OR-Tools as a constraint solver [Perron and Didier, 2024]\n(Appendix E.1 provides full details of the experimental setup).\nEvaluation metrics. We use the standard Execution Accuracy (ex) [Li et al., 2023]. In\naddition, we consider a relaxation of this metric. We noticed that frameworks often add ad-\nditional attributes to the output as the exact format of the output is rarely specified. Hence,\nwe extend ex to esx metrics that check if the output of a framework contains the ground\ntruth outputs. To better understand performance characteristics and possible failure modes,\nwe consider the coverage metric that captures whether a framework correctly identified a\nsubset of relevant tables and attributes. Let sqlg be the ground truth answer and sqlF be a\ngenerated query. Then we assess the percentage of the ground truth content slqF captures:\n$cou_{tb} = \\frac{|tables(slq_F) \\cap tables(slq_G)|}{|tables(slq_G)|}$, $cou_a = \\frac{|attributes(slq_F) \\cap attributes(slq_G)|}{|attributes(slq_G)|}$     (10)\nwhere tables () and attributes () are functions that return a set of tables and attributes.\nACME insurance. We consider the ACME insurance dataset that was recently pub-\nlished [Sequeda et al., 2023]. The dataset represents an enterprise relational database\nschema in the insurance domain. The authors focused on a subset of 13 tables out of\n200 tables and proposed a set of 45 challenging questions. We identified two STAR patterns\nin this database. The authors showed that their method (DW) solved 24 out of 45 problems\nusing intermediate representation of a knowledge graph, while GPT4 solved only 8 problems."}]}