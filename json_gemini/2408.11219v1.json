{"title": "CoDi: Conversational Distillation for Grounded Question Answering", "authors": ["Patrick Huber", "Arash Einolghozati", "Rylan Conway", "Kanika Narang", "Matt Smith", "Waqar Nayyar", "Adithya Sagar", "Ahmed Aly", "Akshat Shrivastava"], "abstract": "Distilling conversational skills into Small Language Models (SLMs) with approximately 1 billion parameters presents significant challenges. Firstly, SLMs have limited capacity in their model parameters to learn extensive knowledge compared to larger models. Secondly, high-quality conversational datasets are often scarce, small, and domain-specific. Addressing these challenges, we introduce a novel data distillation framework named CoDi (short for Conversational Distillation, pronounced \"Cody\"), allowing us to synthesize large-scale, assistant-style datasets in a steerable and diverse manner. Specifically, while our framework is task agnostic at its core, we explore and evaluate the potential of CoDi on the task of conversational grounded reasoning for question answering. This is a typical on-device scenario for specialist SLMs, allowing for open-domain model responses, without requiring the model to \"memorize\" world knowledge in its limited capacity. Our evaluations show that SLMs trained with CoDi-synthesized data achieve performance comparable to models trained on human-annotated data in standard metrics. Additionally, when using our framework to generate larger datasets from web data, our models surpass larger, instruction-tuned models in zero-shot conversational grounded reasoning tasks.", "sections": [{"title": "Introduction", "content": "Small Language Models (SLMs), defined here as having approximately 1 billion parameters, do not exhibit the same level of generalization and emergent abilities as Large Language Models (LLMs) (Fu et al., 2023). For instance, when the model size is reduced from 70 billion to 7 billion and further to 1.3 billion parameters, performance on the \"Massive Multitask Language Understanding\" (MMLU) benchmark decreases by 23.6% and 43.2% respectively (Touvron et al., 2023; Xia et al., 2023). These sharply declining results of general language understanding across model sizes highlight the limitations of SLMs as generalist language models. As a result, we believe that smaller language models are more suited as task specialists, solving a small subset of tasks.\nWithin this space of task specialist models, conversational abilities of SLMs play a crucial role for their deployment on edge devices, such as mobile phones and wearables. Despite the benefits of on-device models (e.g., latency and availability), enabling SLMs to be conversational agents is a difficult problem due to (1) the limited capacity to learn and retain extensive knowledge in the parameters of SLMs and (2) the lack of large-scale, high quality, conversational datasets, which are often limited to narrow domains (Duan et al., 2023).\nThe latter point is thereby largely attributed to the notoriously difficult human annotation of multi-turn data. Manually creating conversational multi-turn datasets is resource intensive, since every sample requires a valid and meaningful conversational history. Hence, annotating multi-turn datasets is either significantly more resource-intensive (at the same scale) or results in much smaller datasets using the same resources, e.g. CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), or OpenAssistant (K\u00f6pf et al., 2023). This, in turn, leads to a shortage of suitable training resources for conversational models in terms of volume and diversity."}, {"title": "Method", "content": "In order to enrich Small Language Models (SLMs), a popular approach has been to \u201cblack-box\u201d distill capabilities from large language models (Elmadany et al., 2023; Hsieh et al., 2023; Gunasekar et al., 2023) through data augmentation or synthesis. Following this intuition, we propose a new distillation methodology to enrich small language models with conversational grounded reasoning abilities. In comparison to existing distillation paradigms mostly focusing on synthesized data in the instruction-tuning domain (e.g. WizardLM (Xu et al., 2023), Alpagasus (Chen et al.), and Alpaca (Taori et al., 2023)), we focus on synthesizing true multi-turn conversations using a \"turn-by-turn\" generation paradigm to target the shortcoming of current LLMs when used at the conversation level. This way, we aim to increase the distillation performace along two dimensions: diversity and steerability."}, {"title": "Distilling Conversational Abilities", "content": "Enabling conversational abilities in SLMs through supervised fine-tuning is a data intensive process requiring careful data curation at scale. Multiple lines of research, e.g. Hsieh et al. (2023); Mukherjee et al. (2023); Gunasekar et al. (2023); Zhou et al. (2023) recently found that data quality, besides scale, is imperative when aiming to effectively distill language models. Building on top of these insights, we present a distillation method to explicitly enrich SLMs with conversational reasoning abilities by introducing three new concepts to generate diverse, engaging and fluent conversations: conversational graphs, turn based prompt augmentations, and explicit linguistic features."}, {"title": "Conversational Graph Generation", "content": "Synthesizing diverse, yet naturally flowing conversations is imperative when imitating human interactions. Generating valid, diverse and coherent conversation is hence a key challenge for our multi-turn synthesis framework. To to able to make some guarantees regarding conversation validity, we propose a conversational graph generation approach inspired by Markov Chains. An example conversation graph is shown in Figure 1. While specific instances of the conversational graph vary based on the synthesis task at hand, the general structure defines a set of \u201cconversation links\" (visualized as vertices), representing blueprints for prompting conversational turns, connected by transition probabilities (here: edges) used to sample a \"conversational chain\", a sequence of conversation links representing a valid and natural multi-turn conversation from the graph.\nIn general, for a specific task (e.g. grounded question answering), a conversational graph G = (V, E) with vertices V = (0, L1, L2, L3) (defining \u201cconversation links\") and edges E = (W\u00f8,1, W1,2, W2,1, ...) (representing transition probabilities) is defined. To generate a question-answering conversation using G, we sample from the set of valid edges (e.g. {W0,1, W0,2 and w0,3} if V = \u00d8) and instantiate the sampled target link (e.g. L2 in Figure 1). In the next step, we now sample from {w2,1, W2,3}, since V = L2). Once we reach the defined conversation length n (e.g. 4 in Figure 1), we end the graph traversal and return the valid and diverse conversational blueprint for teacher model synthesis.\""}, {"title": "Conversational Links", "content": "Once a valid conversational chain is sampled from the graph according to the transition probabilities, the links are executed in-order (e.g., in Figure 1: L2 \u2192 L1 \u2192 L2 \u2192 L3), with an optional context prepended to the sequence for grounded tasks (e.g. grounded reasoning). While the conversational graph defines the \"macro\" level conversation structure, the individual links define the conversational turns themselves. Each link thereby contains: (1) A link-specific prompt to steer the conversation and (2) potential seed data for the current step (see Figure 3). The prompt is mandatory in every link and should ideally utilize methods to enhance the distillation quality, such as \"Chain-of-Thought\" reasoning steps (CoT, Wei et al. (2023), not shown in Figure 3). The prompt can optionally (depending on the link itself) utilize additional seed data samples to support diversity in the conversational chain or, in cases where auxiliary data is required, directly use external information (e.g., context). The role of the prompt and seed are configurable depending on the link and present the primary means to enable explicit conversational phenomena in the conversation.\nFigure 2 shows the flow of data in a single generation step, where the prompt template, originating in the Link, is filled in by the conversational context and diversified by an optional data point. It is then used to generate a new conversational turn through the teacher \"DataGen LLM\". The newly generated conversation turn is then added to the stored conversation and the process is started over with the next link in the conversational chain."}, {"title": "Linguistic Phenomena", "content": "Up to this point, our generations could still result in a sequence of independent, single turn utterances. Inspired by everyday conversations between humans, we use explicit linguistic phenomena to naturally tie turns together. Figure 4 visualizes this core principle behind the approach for our running example using coreference to refer back to a prior conversational turn. This way, we explicitly tie together entity mentions in the context and synthesize semantic follow-ups. Figure 5 shows an example of a synthesized multi-turn conversation based on a CoQA context document.\nThis results in our final, steerable and diverse conversation synthesis framework proposed in this paper."}, {"title": "Small Scale Conversational Specialists", "content": "To enable Small Language Models (SLMs) to acquire conversational abilities, we introduce a new data format, which adds additional flexibility to the otherwise limited set of roles in common chat templates, such as the Llama format (Touvron et al., 2023). Specifically, we allow for an arbitrary number of conversational roles. In the most basic case, a conversation contains two roles, a USER and an AGENT (where even this naming convention is flexible) as shown below:\n[USER] What color is the sky? [/USER]\n[AGENT] The sky is blue. [/AGENT]\nBesides the core roles of USER and AGENT, the CONTEXT role is a common third role, sometimes also referred to as the SYSTEM role. The CONTEXT represents any contextual information the model requires to reason over, which can range anywhere from an instruction to external knowledge in the form of documents, (external) conversations, structured representations, or additional modalities (e.g., images). The CONTEXT generally plays a crucial role as the interface between on-device information and the model, as shown below:\n[CONTEXT] <USER MESSAGE INBOX> [/CONTEXT]\n[USER] Do I have any messages? [/USER]\n[AGENT] You have 2 new messages from Alex. He asks about your weekend plans. Reply? [/AGENT]\nThe second advantage to our approach is Role Weighting, which refers to the ability to adjust the training loss depending on the role. For example, the primary responsibility of an assistant-style model is to generate meaningful AGENT turns based on a context. To emphasize this behavior, learning the AGENT role should be the primary objective during model training. In general, having the ability to control the loss based on the underlying role allows us to gain better control over the learning process, leading to more well defined models."}, {"title": "Experiments", "content": "To evaluate our distillation approach for small-scale conversational agents we choose the task of grounded reasoning for a variety of reasons. First, targeting smaller, specialist language models at on-device size, a natural focus is put on the models interaction with on-device context (e.g. summarize notes on-device). Thus, a knowledge grounded task is a natural choice. Second, given on-device limitations (which restrict usage to SLMs), we can not expect the model to reliably retain large amounts of information. This is evident when evaluating SLMs on knowledge intensive tasks, e.g., MMLU (Touvron et al., 2023; Xia et al., 2023). To this end, a grounded agent offers a more attainable goal. We therefore focus on question answering centered scenarios, where users interact with the system and expect truthful (i.e. grounded) responses to their requests."}, {"title": "Models", "content": "In this paper, we use two main models:\nThe teacher model (LLM) is the 70B Llama3 instruction tuned checkpoint, one of the most capable open-source, instruction tuned large language models to date.\nThe student model (SLM) is a pre-trained 1.4B Llama2-style model.\nWe use additional model sizes and architectures for ablation experiments and comparisons. Specifically, we further employ the 70B Llama2 instruction tuned checkpoint as an alternative teacher model (Touvron et al., 2023). As an alternative student model, we use a pre-trained 500M Llama2-style model. As instruction-tuned baselines, we explore 7B (Touvron et al., 2023), 1.4B and 500M Llama2-style models as well as Phi-3 (Abdin et al., 2024). Further baselines are taken from the literature and directly cited in the relevant sections."}, {"title": "Datasets", "content": "We utilize two sets of datasets in this work: First, to generate diverse data for our CoDi distillation approach, we present our synthesis datasets in section 3.3.1. Subsequently, we describe our evaluation datasets to compare our distilled models against supervised and zero-shot baselines in section 3.3.2. Lastly, section 3.3.3 describes the dataset metrics used for our comparisons."}, {"title": "Synthesis Datasets", "content": "To evaluate the grounded reasoning abilities of our CoDi distillation approach, we explore two synthesis scenarios: intra-domain and zero-shot."}, {"title": "Distillation Scale Comparison", "content": "This experiment targets one of the most important properties of our CoDi distillation framework: the ability to synthesize data at scale. While small-scale, human-annotated datasets exist, producing them is resource-intensive. Using CoDi, we can synthesize large amounts of diverse conversational data across multiple orders of magnitude. In the ablation experiment in Table 4 we show the influence of the number of synthesized conversation on the model performance. The trend across orders of magnitude is clear: larger synthesis scales improve the performance near linearly up to 1 million samples."}, {"title": "Student Model Size Comparison", "content": "This ablation explores the impact of different student model sizes on the conversational question answering performance. We compare the Llama2-like 1.4B student checkpoint used in the main results (using the full dataset) with a Llama2-like 500M student checkpoint. In Table 5 we observe (as expected) a clear quality regression when moving from 1.4B parameters to the 500M size. However, compared to the human-annotated training datasets (see \u201cHuman multi turn\"), our distilled zero-shot model still only slightly under performs the model trained on human multi-turn data.\""}, {"title": "Teacher Model Comparison", "content": "This experiment explores the impact of the teacher model synthesis quality on the final distillation performance. We compare the 70B instruction-tuned Llama2 and Llama3 models. In Table 6 we see a clear quality improvement from using the Llama3 teacher model, showing a 2%+ absolute performance improvement of the distilled student models when trained on Llama3 synthesized conversations."}, {"title": "Per-Turn Performance Comparison", "content": "In this exploration, we cover an important dimension to better understand our conversational distillation system for grounded reasoning: The per-turn performance on the CoQA evaluation dataset. Specifically, we compare the per-turn test set performance of the gold single-turn, gold multi-turn and our full (1M samples) trained models. In Figure 7 we find that our CoDi Web model is on par with the gold multi-turn trained baseline for short conversations (three or less turns). For longer conversations, our approach near-consistently outperforms the multi-turn baseline. The single-turn baseline performs consistently worse, with the performance gap generally increasing in later turns."}, {"title": "Language Modeling Evaluations", "content": "We further evaluate our final model (based on the full 1M training process) on a commonly used subset of language model evaluations (LM evals) for small language models (e.g. used in Liu et al. (2024a); Allal et al. (2024)). Namely, we evalute the performance for ARC-easy and -challenge (Clark et al., 2018), BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), Hellaswag (Zellers et al., 2019), OBQA (Mihaylov et al., 2018) and Winogrande (Sakaguchi et al., 2021). The goal of this evaluation is to show the performance of our grounded reasoning based distillation approach on this diverse set of language understanding tasks compared to pre-trained and instruction-tuned model alternatives. As shown in Table 7, the CoDi Web trained model on average still under performs instruction-tuned versions of the same base model, however, can improve over the pre-trained checkpoint, without any prompt adjustments during the evaluation. We believe that not regressing, yet even slightly improving the language model evaluation, shows promise of grounded reasoning tasks to act as rather generalist models."}, {"title": "Small-Scale Human Evaluation", "content": "In this section, we present a small-scale human evaluation to confirm the validity of our evaluation metrics. We ask the human reviewers to rank three responses in the context of a textual document and the prior conversation (randomly taken from the CoQA test set). From the response ranking, we retrieve three binary win rates: (1) between our CoDi Web predictions and gold labels, (2) comparing the human multi-turn trained baseline and gold labels, and (3) selecting our CoDi Web predictions or the human multi-turn trained baseline. Table 8 shows the results of the human evaluation2 on 20 distinct documents containing 284 question answer rankings. We can see that there is a large overlap of equally good responses between our CoDi distilled models and both, the multi-turn baseline and the gold answers. The results of the small-scale human evaluations further validate our findings using traditional metrics."}, {"title": "Zero-shot Summarization Performance", "content": "Lastly, we explore another zero-shot scenario in the realm of grounded reasoning tasks: abstractive summarization on CNN/DM (Nallapati et al., 2016) and XSum (Narayan et al., 2018). Table 9 shows the zero-shot results obtained from instruction-tuned 1.4B and 7B Llama2-style baseline models, as well as the 3.8B Phi-3 checkpoint compared to our full CoDi Web distilled model. We further show two supervised fine-tuned model comparisons to put the zero-shot performance into perspective, a fully fine-tuned 1.4B Llama2-style model and a competitive BART-Large model (Lewis et al., 2019). Looking at the results, we find that CoDi Web outperforms both Llama2-style instruction tuned baselines, at the 1.4B and 7B scale. While the Phi-3 checkpoint outperforms our model on the CNN-DM dataset, the delta is small given the significant size difference between the models. Comparing the models on the XSum dataset, CoDi Web outperforms all other models, despite their significant size advantage."}, {"title": "Related Work", "content": "High Quality Data Distillation Recently, there has been a large push towards curating high quality datasets for training small language models. With the intuition that small models are more sensitive to low-quality data, recent research (1) filtered datasets based on quality, (2) rewrites data samples to improve quality, and (3) synthesizes new and diverse samples to teach the model the desired behavior. For example, in their seminal work, Gunasekar et al. (2023) use code data from the web and refine it to \u201ctextbook-style\" samples to pre-train small scale-decoder only models. Similarly, Zhou et al. (2023) argue for the need of high-quality datasets, even for alignment purposes. Along similar lines, Wei et al. (2022); Longpre et al. (2023) show that dataset diversity along the task axis plays a crucial role for model training. In the creative writing domain, Ravi et al. (2024) show that small language models are able to learn difficult concepts, such as humor, when distilled in an interactive manner.\nConversational Question Answering Conversational question answering has been explored extensively in a variety of works given the importance of the task. For example, Liu et al. (2024b) propose a family of conversational question answering models at large scales by adding a dense retrieval module. Feng et al. (2020) propose a new method to create conversational datasets using discourse units, while Anantha et al. (2021) proposes a question rewriting method in the conversational context. Adlakha et al. (2022) publish a new dataset for conversational question answering focusing on topic switches. Compared to these approaches, our new conversational synthesis approach is more scalable, while maintaining data diversity and steerability.\nPrompting Paradigms In black-box LLM distillation, the human curated prompt plays a major role for the downstream performance. While many different approaches have been proposed in the past, so called \"Chain-of-Thought\" (CoT) prompting is one of the most popular black-box LLM distillation paradigms to achieve high-quality results (Wei et al., 2023). To this end, we follow the approach taken in in Wei et al. (2023) and prompt our per-turn conversational links using a flavor for CoT prompting, asking the model to produce a reasoning trace in CoT style.\nSmall Language Models Given the strong generalist performance of LLMs, such as the GPT (OpenAI et al., 2024) and the Llama (Touvron et al., 2023) series, the question on how much these abilities can be distilled into SLMs has become important research question. For example, in the Orca work, Mukherjee et al. (2023) show promising performance at the 13B model scale when distilling data from GPT-4 using explanation traces. Similarly, the Phi series (Gunasekar et al., 2023) shows strong performance of even smaller language models when trained on code data. Lastly, OpenELM (Mehta et al., 2024) shows similar results."}, {"title": "Conclusion", "content": "In this paper we show a novel conversational distillation method applied to the challenging task of conversational grounded reasoning for question answering. We show that using our framework to generate diverse, steerable and conversational question answer traces can significantly close the intra-domain performance gap compared to human curated multi-turn conversations. Furthermore, we show that our synthesis approach can improve zero-shot question answering and summarization performance compared to similar sized instruction-tuned models, and even outperform models of significantly larger size. With these promising results, we make a compelling case for using the CoDi framework to synthesize data from diverse seeds instead of going through the resource-intensive human annotation process or scaling up the model size."}, {"title": "Contributions", "content": ""}]}