{"title": "Addressing Imbalance for Class Incremental Learning in Medical Image Classification", "authors": ["Xuze Hao", "Wenqian Ni", "Xuhao Jiang", "Weimin Tan", "Bo Yan"], "abstract": "Deep convolutional neural networks have made significant break-throughs in medical image classification, under the assumption that training samples from all classes are simultaneously available. However, in real-world medical scenarios, there's a common need to continuously learn about new diseases, leading to the emerging field of class incremental learning (CIL) in the medical domain. Typically, CIL suffers from catastrophic forgetting when trained on new classes. This phenomenon is mainly caused by the imbalance between old and new classes, and it becomes even more challenging with imbalanced medical datasets. In this work, we introduce two simple yet effective plug-in methods to mitigate the adverse effects of the imbalance. First, we propose a CIL-balanced classification loss to mitigate the classifier bias toward majority classes via logit adjustment. Second, we propose a distribution margin loss that not only alleviates the inter-class overlap in embedding space but also enforces the intra-class compactness. We evaluate the effectiveness of our method with extensive experiments on three benchmark datasets (CCH5000, HAM10000, and EyePACS). The results demonstrate that our approach outperforms state-of-the-art methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Nowadays, deep learning has emerged as a powerful tool across various fields [7, 18, 23, 24], including the medical domain [2, 33, 36]. However, traditional deep learning methods often make assumptions about stationary and independent data distributions, which may be impractical in real-world scenarios. Most trained diagnosis models would be fixed once developed, while in real clinical practice, the distribution of medical data frequently undergoes shifts over time, primarily due to the continuous emergence of new diseases, treatment protocols, and patient data [6, 41]. Under such circumstances, the model needs to incorporate new class knowledge incrementally instead of retraining the model with all data available [28]. Therefore, in this work, we focus on class incremental learning in the medical domain.\nFig. 1 illustrates the setting of class incremental learning in medical Image classification. Taking the EyePACS dataset [8] as an example, the model is initially trained to classify three classes (i.e., No DR, Mild, and Moderate). Subsequently, incremental classes (e.g., Severe, and Proliferative DR) arrive in sequential steps to update the model. The classes introduced in different steps are disjoint, and the model must be able to predict all classes seen over time. However, when updating the model with only new classes, new data tends to erase previous knowledge. This phenomenon is known as catastrophic forgetting [16, 26].\nFor class incremental learning, imbalanced data between old and new classes is one of the primary reasons for catastrophic forgetting [27, 54]. To this end, numerous approaches have been proposed to store a small proportion of previous training data in memory and rehearse them when learning new classes [1, 20, 49]. However, the limited size of memory can also lead to an imbalance between old and new classes [32, 38]. Under this circumstance, the class imbalance will lead to (i) a classifier biased towards the new and majority classes; and (ii) the embeddings of new classes inevitably overlap with the old ones in the feature space (i.e. the ambiguities problem). In addition to the class incremental learning imbalance, many real-life medical datasets exhibit significant class imbalance [36], with some classes having notably higher instances in training samples than others, e.g., HAM10000 [42], and EyePACS [8], which further aggravate the catastrophic forgetting. Therefore, addressing data imbalance is crucial for class incremental learning in medical image classification.\nIn this paper, we propose two simple yet effective plug-in loss objectives to tackle two challenges caused by imbalance in class incremental learning. First, we propose a CIL-balanced classification loss instead of the traditional cross-entropy (CE) loss to avoid the issue of classifier bias. Specifically, we first adjust the logits based on the category frequency to place more emphasis on rare classes and then introduce a scale factor to further achieve a balance between old and new classes. Second, to alleviate the overlap of classes in the feature space, we propose a distribution margin loss, a novel improved margin loss, which not only facilitates to push away the distributions of old and new classes but also obtains the compact intra-class clustering. Extensive experiments on benchmark datasets under various settings verify the superiority of our method.\nTo summarize, the main contributions of this paper are:\n\u2022 To reduce the classifier bias towards new and majority classes, we propose a CIL-balanced classification loss that emphasizes rare ones via logit adjustment.\n\u2022 We introduce a novel distribution margin loss that can effectively separate the distributions of old and new classes to avoid ambiguities and realize the optimization of the intra-class compactness.\n\u2022 Extensive experiments demonstrate that our method can effectively address the issue of data imbalance with the state-of-the-art performance achieved on three benchmark datasets: CCH5000, HAM10000, and EyePACS."}, {"title": "2 RELATED WORK", "content": "2.1 Class Incremental Learning\nClass incremental learning aims to train a model from a sequence of classes, ensuring its performance across all the classes. Existing class incremental learning methods can be roughly divided into three groups: regularization-based, structure-based, and memory-based.\nRegularization-based methods [11, 14, 17, 20, 40, 52] apply additional constraints to prevent the existing model from forgetting previous knowledge. LUCIR [20] constrains the orientation of the features to preserve the geometric configuration of old classes. PODNet [14] introduces a novel spatial distillation not only for the outputs of the final layer but also for the intermediate features to mitigate representation forgetting. However, regularization-based methods still suffer from feature degradation of old knowledge due to the limited access to old data [50].\nStructure-based methods [15, 21, 31, 44, 50] aim to preserve the learned parameters associated with old classes while incrementally creating modules to enhance the model's capacity to acquire new knowledge. Recently, DER [50] adds a new feature extractor at each step and then concatenates the extracted features for classification. DyTox [15] applies transformer [12] to incremental learning and dynamically expands task tokens when learning new classes. Nevertheless, dynamically adding new modules will lead to an explosion in the number of parameters and an increase in the independence between each feature extractor to harm performance in new classes [44].\nMemory-based methods [3, 4, 38, 46, 49, 51] address the challenge of forgetting by storing a limited number of representative samples from old classes in a memory buffer. iCaRL [38] learns the exemplar-based data representation and makes predictions using a nearest-mean-of-exemplars classifier. GEM [4] uses exemplars for gradient projection to overcome forgetting. Additionally, some approaches employ generative models to synthesize old class samples for data rehearsal [35, 39, 45] while other works consider saving feature embeddings instead of raw images [22]. In our work, we follow the memory-based approach to directly store a small subset of old class data for rehearsal.\n2.2 Class Imbalance\nClass imbalance is a key challenge for class incremental learning [20]. Due to the only access to the classes of the current step, the classifier is severely biased, and there is an inevitable overlap and confusion between the feature space representations of old and new classes [27]. Even with the limited size of the memory buffer, the biased optimization by imbalanced data between old and new classes is still a crucial problem that causes catastrophic forgetting [32, 38]. To cope with it, SS-IL [1] isolates the computation of the softmax probabilities on old and new classes for bias compensation. BiC [49] introduces a bias correction layer to address the bias in the last fully connected layer.\nIn real-world medical scenarios, most existing datasets contain highly imbalanced numbers of samples [36], which leads to a more severe forgetting. To the best of our knowledge, LO2LN [6] is the first attempt to address the problem of class incremental learning in medical image classification. First, they utilize the class-balanced focal loss [9] to avoid the classifier bias. However, the class-balanced focal loss is not specialized and efficient for incremental learning. Second, they introduce the margin ranking loss [20] to separate old and new classes. We argue that this constraint may not be sufficiently robust, resulting in large clusters within classes (intra-class) and potential overlaps between classes (inter-class). By contrast, in this paper, we propose two simple yet effective plug-in loss objectives: (i) a CIL-balanced classification loss to alleviate prediction bias by adjusting the logits, and (ii) a distribution margin loss that can push the distributions of old and new classes away and provide more compact intra-class clustering simultaneously."}, {"title": "3 METHOD", "content": "In this section, we first outline the setting of class incremental learning in medical image classification (Sec. 3.1). Then, we provide a detailed description of the two proposed loss objectives: CIL-balanced classification loss (Sec. 3.2) and distribution margin loss (Sec. 3.3).\n3.1 Setting and Notation\nClass incremental learning aims to train a model from a sequence of data incrementally. Specifically, we denote the sequence of tasks as  D = {D_1, D_2, D_3, ..., D_\u2020} , where  D_t = (X_t, Y_t) = {(x,y)_i}^(n_t)  represents the training set from step t with nt instances. Here, x \u2208 X_t is a sample and  y \u2208 Y_t  is the corresponding label. The label space of the model is all seen classes  Y_1:t = U_(i=1)^t Y_i , where  Y_t \u2229 Y_(t') = 0  for all  t != t' . Inspired by memory-based methods [3, 38, 49], our method consistently samples m representative instances from each old class and store them in a memory buffer  M_t , which is updated after the training step t is completed. It should be mentioned that only data from  D'_t = D_t \u222a M_(t\u22121)  is available for training during the t-th step.\nClassically, the model at step t can be written as the composition of two functions:  f_t = f_\u03b8^c \u2022 f_\u03b8^(fe)(\u00b7) , where f_\u03b8^(fe) represents a feature extractor, and  f_\u03b8^c represents a classifier. For an input sample  x_i , its feature representation is denoted as  h_i = f_\u03b8^(fe)(x_i) . We employ cosine normalization [20] as the classifier  f_\u03b8^c. Consequently, the predicted logit  p_(ic)  for class c at step t can be calculated from h as:\n\\(p_{ic} = \u03b7 \u22c5 (h, w_c),\\)   (1)\nwhere  w_c  are the weights for class c in the classifier layer, \u03b7 is a learnable scalar, and  (,)  denotes the cosine similarity between two vectors.\n3.2 CIL-Balanced Classification Loss\nAs claimed in previous works [32, 36], the inherent imbalance in medical datasets and the imbalance in class incremental learning can lead to a biased classifier. Inspired by [34], we aim to mitigate this issue by adjusting the logits according to category frequency. However, for a memory-based method in class incremental learning, only the data from Dt is available at step t, which consists of the memory buffer  M_(t\u22121)  and the training set  D_t . Hence, we define the category frequency as follows:\n\\(r_c =  \\begin{cases}   \\frac{m}{q_c} & \\text{if } c \u2208 Y_{1:t-1}, \\\\   \\frac{1}{q_c} & \\text{if } c \u2208 Y_t,  \\end{cases}\\)   (2)\nwhere  q_c  is the number of training samples for class c, and |\u00b7| is the cardinality of a given set. After that, we add  log r_c  to the output logits during training. Thus, the logit-balanced classification loss can be formulated as:\n\\(L_{lbc} = \\frac{1}{|D_t|} \\sum_{i \u2208 D_t} log \\frac{exp (p_{i, y_i} + log r_{y_i})}{\\sum_{j \u2208 Y_{1:t} } exp (p_{ij} + log r_j)}\\)   (3)\nTo explain how our method works, we reformulate Eq. 3 into Eq. 4 by introducing  v_(yi,j) := log r_j \u2212 log r_(y_i) , which are defined as follows:\n\\(L_{lbc} = \\frac{1}{|D_t|} \\sum_{i \u2208 D_t} log \\frac{exp (p_{i, y_i})}{\\sum_{j \u2208 Y_{1:t} } exp (p_{ij} + v_{y_i, j})}\\)   (4)\nwhere:\n\\(v_{y_i, j} =  \\begin{cases}   log(\\frac{q_j}{m}), & \\text{if } y_i \u2208 Y_{1:t-1}, j \u2208 Y_t, \\\\   log(\\frac{m}{m} = 0), & \\text{if } y_i \u2208 Y_{1:t-1}, j \u2208 Y_{1:t-1}, \\\\   log(\\frac{m}{q_{y_i}}), & \\text{if } y_i \u2208 Y_t, j \u2208 Y_{1:t-1}, \\\\   log(\\frac{q_j}{q_{y_i}}), & \\text{if } y_i \u2208 Y_t, j \u2208 Y_t.  \\end{cases}\\)   (5)\nIt is known that traditional softmax loss necessitates  p_(iy_i) > p_(ij) for the accurate classification of sample xi. In order to prioritize the learning of old and rare classes, we employ the following logit adjustment strategy. Specifically, when  y_i \u2208 Y_(1:t\u22121)  and  j \u2208 Y_t  (the first line in Eq. 5), we instead require  p_(i, y_i) > p_(ij) + log (q_j/m) [> 0] . Hence, it is clear that we require a larger  p_(iy_i) , which makes the training process place more emphasis on old class yi than previously. However, if both  y_i  and j are within  Y_(1:t\u22121)  (the second line in Eq. 5), the logit remains unchanged, since they are both old classes with the same memory size.\nFor the other two cases when  y_i \u2208 Y_t . If  j \u2208 Y_(1:t\u22121)  (the third line in Eq. 5), the term  log (m/q_(y_i)) < 0  suggests that old class j receives more emphasis. If  j \u2208 Y_t  (the fourth line in Eq. 5), more emphasis is placed on the class yi when it has fewer instances, and conversely, the focus is on class j when the size qj is smaller. Therefore, the logit-balanced classification loss can effectively reduce the bias towards new and frequent classes.\nTo further control the balance between the old and new classes, we introduce a scale factor \u03b3:\n\\(\u03b3_c =  \\begin{cases}   \u03b1, & \\text{if } c \u2208 Y_{1:t-1}, \\\\   1, & \\text{if } c \u2208 Y_t,  \\end{cases}\\)   (6)\nwhere  \u03b1 \u2208 [0, 1]  is a trade-off coefficient for each dataset. With the help of this scale factor, the CIL-balanced classification loss can be written as:\n\\(L_{clbc} = \\frac{1}{|D_t|} \\sum_{i \u2208 D_t} log \\frac{\u03b3_{y_i} exp (p_{i, y_i})}{\\sum_{j \u2208 Y_{1:t} } \u03b3_j exp (p_{ij} )}\\)   (7)\nwhich reduces the output values for the old classes while maintaining the outputs for the new classes unchanged, thereby encouraging the model to produce larger logits for these old ones. Consequently, this scaling strategy further mitigates the issue of imbalance in class incremental learning. In this context, although a decrease in  \u03b1  improves the significance of old classes, it may affect the model's learning of new ones. Thus, determining the optimal  \u03b1  becomes crucial for achieving a better trade-off (see Sec. 4.4). Notably, when  \u03b1  is assigned a value of 1, the current CIL-balanced classification loss degrades to the logit-balanced classification loss (Eq. 4).\n3.3 Distribution Margin Loss\nIn class incremental learning, the representations of the old and new classes would be easily overlapped in the deep feature space [53]. To address this issue, margin loss [5] is introduced to avoid the ambiguities between old and new classes. In detail, the vanilla margin loss aims to ensure that the distance from the anchor to the positive (embedding of the ground-truth old class) is less than the distance of the anchor from the negative (embedding of the new class) to meet a predefined margin m, which can be computed as:\n\\(L_m = \\sum_{i \u2208 M_{t-1}} \\sum_{c \u2208 Y_t} max {0, (\\langle h, w_c \\rangle - \\langle h, w_{y_i} \\rangle + m)}, \\)   (8)\nwhere \u27e8\u00b7,\u00b7\u27e9 denotes the cosine similarity and the margin m is set to 0.4 for all experiments.\nHowever, the vanilla margin loss exhibits two limitations. First, it only focuses on the triplet: anchor, positive, and negative embeddings. Even if the distance from the anchor to the negative exceeds that to the positive by a margin m, their distributions may remain close or even overlap, thereby introducing potential ambiguities in classification (shown in Fig. 2a). Second, while the vanilla margin loss aims to separate the ground-truth old class from new classes (maximizing inter-class distance), it fails to adequately address the minimization of intra-class distance, often leading to large intra-class clustering (shown in Fig. 2c).\nTo address the above limitations, we try to restore the class distribution and design a novel distribution margin loss that contains two loss terms. The first term optimizes the triples by ensuring that the distance from the anchor to the positive embedding is less than its distance to the negative class distributions by the margin m, rather than merely to the negative embeddings. By optimizing this term, the distribution margin loss can push the samples of old classes away from the new class distributions to facilitate the inter-class separation (shown in Fig. 2b). The second term attempts to maintain the anchor embedding within the distribution range of its corresponding class, thus obtaining compact intra-class clustering (shown in Fig. 2d). Accordingly, the distribution margin loss can be formulated as:\n\\(L_{dm} = \\sum_{i \u2208 M_{t-1}} \\sum_{c \u2208 Y_t} max {0, (\\langle h, w_c \\rangle - \\langle h, w_{y_i} \\rangle + m)} + \\sum_{i \u2208 M_{t-1}} max {0, (\\langle w_{y_i}, w^n_{y_i} \\rangle - \\langle h, w_{y_i} \\rangle)},\\)   (9)\nwhere  w^n_c  represents the distribution range of class c. Specifically, we model the data distribution of each class in the feature space by applying a Gaussian distribution around their centroids. However, due to the imbalanced number of samples across different classes, the features of classes with limited instances may get squeezed into a narrow area in the feature space [47]. As a result, we assign a larger distribution range to the majority classes and a more restricted range to the minority classes:\n\\(w_c^n = w_c + \u03b7 * f_c, r_c = \\frac{q_c}{\\sum_{c' \u2208 Y_{1:t} } q_{c'}}\\)   (10)\nwhere  r_c  represents the inherent ratio of class c among all seen classes, and  \u03b7 \u223c N (0, 1)  is a Gaussian noise which has the same dimension as the classifier weight.\nTo prevent forgetting and maintain the discrimination ability, we also apply knowledge distillation loss [19] to build a mapping between the old and the current model:\n\\(L_{kd} = \\frac{1}{|D_t|} \\sum_{i \u2208 D_t} \\sum_{c \u2208 Y_{1:t-1}} || p'_{i.c} - p_{i.c} ||_2,\\)   (11)\nTherefore, the overall loss is defined as:\n\\(L_{all} = L_{clbc} + \u03bb_d L_{dm} + \u03bb_k L_{kd},\\)   (12)\nwhere  \u03bb_d  and  \u03bb_k  are the hyper-parameters for balancing the importance of each loss. We show the guideline of our method at incremental step t in Alg. 1."}, {"title": "4 EXPERIMENTS", "content": "4.1 Experimental Setups\nDatasets. Following the benchmark setting [6], we evaluate the performance on CCH5000 [25], HAM10000 [42], and EyePACS [8].\n\u2022 CCH5000: consists of histological images in human colorectal cancer. This dataset contains 8 different classes with 625 images per class: tumor, stroma, complex, lympho, debris, mucosa, adipose, and empty.\n\u2022 HAM10000: consists of 10,015 skin cancer images, including seven types of skin lesions: melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis, benign keratosis, dermatofibroma, and vascular lesions. The distribution ratios for each type are as follows: 3.27%, 5.13%, 10.97%, 1.15%, 11.11%, 66.95%, and 1.42%, which indicates a severe class imbalance.\n\u2022 EyePACS: is commonly used for the task of diabetic retinopathy (DR) classification. EyePACS dataset contains 35,126 retina images for training, which are categorized into five stages of DR. Specifically, there are 25,810 images labeled as no DR, 2,443 as mild DR, 5,292 as moderate DR, 873 as severe DR, and 708 as proliferative DR images. It is worth noting that this dataset is also highly imbalanced.\nEvaluation protocols. Following the experimental protocols in [6], we evaluate our method for different scenarios, such as 4-1, 4-2, 3-1, and 3-2. In each scenario, the numbers indicate the number of base and new classes, respectively. For example, considering the HAM10000 dataset with 7 classes, the scenario of 3-2 corresponds to learning 3 classes at the initial step and subsequently adding 2 new classes at each incremental step, requiring a total of 3 training steps.\nMetrics. Following previous work [6], we evaluate our method based on two standard metrics: Average Accuracy (Acc) and Average Forgetting (Fgt). Let  a_(t,i)  be the accuracy of the model evaluated on the test set of classes in  Y_i  after training on the first t steps. The Average Accuracy is defined as:\n\\(Acc = \\frac{1}{T} \\sum_{t=1}^T (\\frac{1}{|Y_t|} \\sum_{i=1}^{|Y_t|} a_{t,i})\\)   (13)\nwhich measures the average classification accuracy of the model until step T. The Average Forgetting is defined as:\n\\(Fgt = \\frac{1}{T} \\sum_{i=1}^T (\\frac{1}{|Y_i|} \\sum_{j \u2208 [1,t-1]} max (a_{j,i} - a_{t,i})\\)   (14)\nwhich measures an estimate of how much the model forgets by averaging the decline in accuracy from the peak performance to its current performance.\nCompared methods. To demonstrate the superiority of our method, we first compare it to classical incremental learning approaches: iCaRL [38], UCIR [20], PODNet [14], and DER [50]. Besides, we also compare to the current state-of-the-art method: LO2LN [6].\nImplementation details. As in [6], we adopt a cosine normalization classifier with a ResNet-18 [18] backbone pre-trained on the ImageNet [10]. Our method is implemented in PyTorch [37], and we employ SGD with a momentum value of 0.9 and weight decay of 0.0005 for optimization. During training, the batch size is set to 32 for the CCH5000 and HAM10000 datasets and 128 for the EyePACS dataset in each learning step. Note that, for a fair comparison, we use the same memory setting for every compared method, i.e., a fixed number of 20 training examples per class are selected via the herding selection strategy [48] and stored in memory M. Furthermore, we conduct all experiments on three different class orders and report the means \u00b1 standard deviations over three runs.\n4.2 Performance Comparison\nAs shown in Tab. 1, 2, and 3, we report the experimental results on three benchmark datasets: CCH5000, HAM10000, and EyePACS, respectively.\nCCH5000. We can see that our method achieves state-of-the-art performance in terms of Acc and Fgt on both settings. Specifically, our method surpasses LO2LN by 1.7% on the 4-2 setting and 1.8% on the 4-1 setting in terms of Fgt, indicating the effectiveness of our method in overcoming forgetting.\nHAM 10000. Different from the CCH5000 dataset, the HAM10000 dataset is a highly imbalanced dermoscopy dataset. Experimental results demonstrate that our method significantly improves the performance on the HAM10000 dataset, benefiting from the strong ability to address class imbalance. To be more specific, compared with the SOTA method, we improve the accuracy from 82.0% to 85.0% on the 3-2 setting. On the 3-1 setting, we achieve an overall performance of 80.9%, which is 2.8% higher than LO2LN's 78.1%. Moreover, the average forgetting is also reduced by 4.8% (3-2 setting) and 4.9% (3-1 setting).\nEyePACS. Furthermore, we present a comparison of different methods on the challenging EyePACS dataset. Our proposed method not only demonstrates significantly higher average accuracy but also achieves lower average forgetting than the other baselines. Notably, it surpasses LO2LN by 0.9% in terms of Acc and outperforms PODNet and DER by substantial margins of 19.5% and 24.1%, respectively.\n4.3 Analysis of Incremental Performance\nAccuracy. As shown in Fig. 3, we display the average incremental performance of each step for three datasets. According to these curves, it is evident that the performances of all methods are similar in the first step, but the baselines suffer from a significant drop as the learning steps increase. In contrast, our method effectively slows down the drop, leading to an increasing gap between the baselines and our method over time. This demonstrates that our method benefits class incremental learning in medical image classification and outperforms prior works.\nForgetting. Fig. 4 depicts the average forgetting across each incremental step for three datasets. The forgetting of most methods increases rapidly as new classes arrive, while our method consistently outperforms the SOTA methods, indicating improved resilience to catastrophic forgetting.\n4.4 Ablation Study\nImpact of each component. In Tab. 4, we present an ablation analysis on the HAM10000 3-2 setting to evaluate the effect of each proposed component. The first row refers to the baseline, which is trained with the cross-entropy loss (CE) and the knowledge distillation loss Lkd. Firstly, we observe that the distribution margin loss Ldm brings a significant contribution when applied alone, improving the performance by 12.6% in terms of Acc. Secondly, when we replace CE with the CIL-balanced classification loss Lcbc, the average accuracy is improved from 67.6% to a notable 83.6%. Finally, the combination of Ldm and Lcbc further improves the performance, demonstrating the effect of both proposed components.\nEffect of CIL-Balanced Classification Loss. We investigate the impact of different classification losses on the HAM10000 3-2 setting when combined with the knowledge distillation loss Lkd and our distribution margin loss Ldm. As shown in Tab. 5, we present results of using cross-entropy loss (CE), focal loss (Focal) [29], class-balanced focal loss (CB Focal) [9], and our proposed methods (logit-balanced and CIL-balanced). It can be observed that both of our proposed methods consistently outperform the other classification loss objectives, indicating the effectiveness of them to address the imbalance issue. Furthermore, the CIL-balanced classification loss (Eq. 7) achieves an additional 1.9% improvement compared to the logit-balanced classification loss (Eq. 4), benefiting from the scale factor  \u03b3  to strengthen the learning of old classes.\nEffect of Distribution Margin Loss. To verify the effectiveness of our distribution margin loss objectives, we conduct experiments on the HAM10000 3-2 setting combined with the knowledge distillation loss Lkd and our CIL-balanced classification loss Lcbc. The results presented in Tab. 6 demonstrate that our distribution margin loss brings significant improvements compared to cases without the margin loss and with the margin ranking loss [20]. To further illustrate the advantages of our method, we present t-SNE [43] visualizations of feature distributions with different margin loss objectives for the CCH5000 dataset, as shown in Fig. 5. In the absence of margin loss, we observe large intra-class clusters (blue rectangle) and significant inter-class overlap in feature space (red circle). When employing the margin ranking loss, the issue of overlap is mitigated to some extent (red circle) compared to the method without margin loss. Finally, by optimizing our distribution margin loss, we achieve a more pronounced separation between the distributions of old and new classes (red circle), while simultaneously ensuring that the representations of old classes become more compact (blue rectangle).\nAbility to integrate with other existing methods. Our proposed methods can be easily integrated with other existing CIL methods. To demonstrate this, we conduct experiments utilizing iCaRL [38] and UCIR [20] on the HAM10000 3-1 setting. Specifically, we replace the classification loss in each baseline with our CIL-balanced classification loss and incorporate our distribution margin loss. As shown in Tab. 7, the accuracy (Acc) for both baselines can be improved by about 3% with the integration of our methods. More notably, our approaches effectively reduce forgetting (Fgt) by 5.7% and 14.8% for iCaRL and UCIR, respectively.\nSensitivity study of hyper-parameters. In this paper, there are three hyper-parameters during training: the weight of the distribution margin loss  \u03bb_d , the weight of the knowledge distillation loss  \u03bb_k , and the trade-off coefficient  \u03b1 . We first conduct experiments to explore the impacts of different  \u03bb_d  and  \u03bb_k  on the HAM10000 3-2 setting. As shown in Fig. 6a, we vary  \u03bb_d  within the range of {0.1, 0.2, 0.3, 0.4, 0.5}, and  \u03bb_k  within the range of {0.1, 0.25, 0.5, 0.75, 1.0}, resulting in a total of 25 compared results. From the results, we consistently observe satisfactory performance from our model, demonstrating its robustness to the selection of  \u03bb_d  and  \u03bb_k .\nTo investigate the impact of different values of  \u03b1  on addressing the imbalance between the old and new classes, we evaluate the accuracy by varying  \u03b1  from {0.1, 0.3, 0.5, 0.7, 0.9} on three benchmark datasets. As shown in Fig. 6b, the results indicate that the accuracy gradually improves as  \u03b1  grows larger initially, while it starts to decline when  \u03b1  is close to 1. Since the data distribution differs across datasets, the selection of the trade-off coefficient  \u03b1  also varies. Specifically, the optimal values of  \u03b1  for the three datasets are 0.7, 0.5, and 0.3, respectively.\nLonger incremental learning. In class incremental learning, a key challenge is catastrophic forgetting, which becomes more pronounced as the number of learning classes increases [13, 20, 38]. To quantify the robustness of our method in overcoming catastrophic forgetting, we evaluate it on two longer-step protocols: 50-10 (6 steps) and 50-5 (11 steps), employing the more challenging CIFAR100 dataset. Following the experimental protocol outlined in [30], we conduct experiments under both conventional (Conv) and long-tail (LT) scenarios. In the conventional scenario, each class has 500 training samples for training. Conversely, the long-tailed scenario follows an exponential decay in sample sizes across classes, where the ratio between the least and the most frequent class is 0.01. As illustrated in Tab. 8, our method achieves superior results in all settings. Specifically, we observe a more significant improvement in the long-tail scenario, further validating the effectiveness of our method in addressing the class imbalance problem in class incremental learning. Furthermore, we present the dynamic performance changes during the incremental learning process in Fig. 7. It is evident that with more learning steps, the gap between the baselines and our method widens, and our method's performance remains superior across different scenarios (conventional and long-tailed) throughout most of the learning steps."}, {"title": "5 CONCLUSION", "content": "In this paper, we propose two simple yet effective plug-in loss functions for class incremental learning in medical image classification. First, to address the challenge of classifier bias caused by class imbalance, we introduce a CIL-balanced classification loss via logit adjustment. Second, we propose a novel distribution margin loss that aims to enforce inter-class discrepancy and intra-class compactness simultaneously. Our extensive experimental evaluation demonstrates the state-of-the-art performance of our method across various scenarios on medical image datasets: CCH5000, HAM10000, and EyePACS."}]}