{"title": "Addressing Imbalance for Class Incremental Learning in Medical Image Classification", "authors": ["Xuze Hao", "Wenqian Ni", "Xuhao Jiang", "Weimin Tan", "Bo Yan"], "abstract": "Deep convolutional neural networks have made significant breakthroughs in medical image classification, under the assumption that training samples from all classes are simultaneously available. However, in real-world medical scenarios, there's a common need to continuously learn about new diseases, leading to the emerging field of class incremental learning (CIL) in the medical domain. Typically, CIL suffers from catastrophic forgetting when trained on new classes. This phenomenon is mainly caused by the imbalance between old and new classes, and it becomes even more challenging with imbalanced medical datasets. In this work, we introduce two simple yet effective plug-in methods to mitigate the adverse effects of the imbalance. First, we propose a CIL-balanced classification loss to mitigate the classifier bias toward majority classes via logit adjustment. Second, we propose a distribution margin loss that not only alleviates the inter-class overlap in embedding space but also enforces the intra-class compactness. We evaluate the effectiveness of our method with extensive experiments on three benchmark datasets (CCH5000, HAM10000, and EyePACS). The results demonstrate that our approach outperforms state-of-the-art methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Nowadays, deep learning has emerged as a powerful tool across various fields [7, 18, 23, 24], including the medical domain [2, 33, 36]. However, traditional deep learning methods often make assumptions about stationary and independent data distributions, which may be impractical in real-world scenarios. Most trained diagnosis models would be fixed once developed, while in real clinical practice, the distribution of medical data frequently undergoes shifts over time, primarily due to the continuous emergence of new diseases, treatment protocols, and patient data [6, 41]. Under such circumstances, the model needs to incorporate new class knowledge incrementally instead of retraining the model with all data available [28]. Therefore, in this work, we focus on class incremental learning in the medical domain.\nFig. 1 illustrates the setting of class incremental learning in medical Image classification. Taking the EyePACS dataset [8] as an example, the model is initially trained to classify three classes (i.e., No DR, Mild, and Moderate). Subsequently, incremental classes (e.g., Severe, and Proliferative DR) arrive in sequential steps to update the model. The classes introduced in different steps are disjoint, and the model must be able to predict all classes seen over time. However, when updating the model with only new classes, new data tends to erase previous knowledge. This phenomenon is known as catastrophic forgetting [16, 26].\nFor class incremental learning, imbalanced data between old and new classes is one of the primary reasons for catastrophic forgetting [27, 54]. To this end, numerous approaches have been proposed to store a small proportion of previous training data in memory and rehearse them when learning new classes [1, 20, 49]. However, the limited size of memory can also lead to an imbalance between old and new classes [32, 38]. Under this circumstance, the class imbalance will lead to (i) a classifier biased towards the new and majority classes; and (ii) the embeddings of new classes inevitably overlap with the old ones in the feature space (i.e. the ambiguities problem). In addition to the class incremental learning imbalance, many real-life medical datasets exhibit significant class imbalance [36], with some classes having notably higher instances in training samples than others, e.g., HAM10000 [42], and EyePACS [8], which further aggravate the catastrophic forgetting. Therefore, addressing data imbalance is crucial for class incremental learning in medical image classification.\nIn this paper, we propose two simple yet effective plug-in loss objectives to tackle two challenges caused by imbalance in class incremental learning. First, we propose a CIL-balanced classification loss instead of the traditional cross-entropy (CE) loss to avoid the issue of classifier bias. Specifically, we first adjust the logits based on the category frequency to place more emphasis on rare classes and then introduce a scale factor to further achieve a balance between old and new classes. Second, to alleviate the overlap of classes in the feature space, we propose a distribution margin loss, a novel improved margin loss, which not only facilitates to push away the distributions of old and new classes but also obtains the compact intra-class clustering. Extensive experiments on benchmark datasets under various settings verify the superiority of our method.\nTo summarize, the main contributions of this paper are:\n\u2022 To reduce the classifier bias towards new and majority classes, we propose a CIL-balanced classification loss that emphasizes rare ones via logit adjustment.\n\u2022 We introduce a novel distribution margin loss that can effectively separate the distributions of old and new classes to avoid ambiguities and realize the optimization of the intra-class compactness.\n\u2022 Extensive experiments demonstrate that our method can effectively address the issue of data imbalance with the state-of-the-art performance achieved on three benchmark datasets: CCH5000, HAM10000, and EyePACS."}, {"title": "2 RELATED WORK", "content": "Class incremental learning aims to train a model from a sequence of classes, ensuring its performance across all the classes. Existing class incremental learning methods can be roughly divided into three groups: regularization-based, structure-based, and memory-based.\nRegularization-based methods [11, 14, 17, 20, 40, 52] apply additional constraints to prevent the existing model from forgetting previous knowledge. LUCIR [20] constrains the orientation of the features to preserve the geometric configuration of old classes. PODNet [14] introduces a novel spatial distillation not only for the outputs of the final layer but also for the intermediate features to mitigate representation forgetting. However, regularization-based methods still suffer from feature degradation of old knowledge due to the limited access to old data [50].\nStructure-based methods [15, 21, 31, 44, 50] aim to preserve the learned parameters associated with old classes while incrementally creating modules to enhance the model's capacity to acquire new knowledge. Recently, DER [50] adds a new feature extractor at each step and then concatenates the extracted features for classification. DyTox [15] applies transformer [12] to incremental learning and dynamically expands task tokens when learning new classes. Nevertheless, dynamically adding new modules will lead to an explosion in the number of parameters and an increase in the independence between each feature extractor to harm performance in new classes [44].\nMemory-based methods [3, 4, 38, 46, 49, 51] address the challenge of forgetting by storing a limited number of representative samples from old classes in a memory buffer. iCaRL [38] learns the exemplar-based data representation and makes predictions using a nearest-mean-of-exemplars classifier. GEM [4] uses exemplars for gradient projection to overcome forgetting. Additionally, some approaches employ generative models to synthesize old class samples for data rehearsal [35, 39, 45] while other works consider saving feature embeddings instead of raw images [22]. In our work, we follow the memory-based approach to directly store a small subset of old class data for rehearsal."}, {"title": "2.2 Class Imbalance", "content": "Class imbalance is a key challenge for class incremental learning [20]. Due to the only access to the classes of the current step, the classifier is severely biased, and there is an inevitable overlap and confusion between the feature space representations of old and new classes [27]. Even with the limited size of the memory buffer, the biased optimization by imbalanced data between old and new classes is still a crucial problem that causes catastrophic forgetting [32, 38]. To cope with it, SS-IL [1] isolates the computation of the softmax probabilities on old and new classes for bias compensation. BiC [49] introduces a bias correction layer to address the bias in the last fully connected layer.\nIn real-world medical scenarios, most existing datasets contain highly imbalanced numbers of samples [36], which leads to a more severe forgetting. To the best of our knowledge, LO2LN [6] is the first attempt to address the problem of class incremental learning in medical image classification. First, they utilize the class-balanced focal loss [9] to avoid the classifier bias. However, the class-balanced focal loss is not specialized and efficient for incremental learning. Second, they introduce the margin ranking loss [20] to separate old and new classes. We argue that this constraint may not be sufficiently robust, resulting in large clusters within classes (intra-class) and potential overlaps between classes (inter-class). By contrast, in this paper, we propose two simple yet effective plug-in loss objectives: (i) a CIL-balanced classification loss to alleviate prediction bias by adjusting the logits, and (ii) a distribution margin loss that can push the distributions of old and new classes away and provide more compact intra-class clustering simultaneously."}, {"title": "3 METHOD", "content": "In this section, we first outline the setting of class incremental learning in medical image classification (Sec. 3.1). Then, we provide a detailed description of the two proposed loss objectives: CIL-balanced classification loss (Sec. 3.2) and distribution margin loss (Sec. 3.3)."}, {"title": "3.1 Setting and Notation", "content": "Class incremental learning aims to train a model from a sequence of data incrementally. Specifically, we denote the sequence of tasks as D = {D1, D2, D3, ..., D\u2020}, where Dt = (Xt, Yt) = {(xi,yi)}^nt_{i=1} represents the training set from step t with nt instances. Here, x\u2208 Xt is a sample and y \u2208 Y\u0165 is the corresponding label. The label space of the model is all seen classes Y1:t = U^t_{i=1} Yi, where Yt\u2229Yt' = \u2205 for all t\u2260 t'. Inspired by memory-based methods [3, 38, 49], our method consistently samples m representative instances from each old class and store them in a memory buffer Mt, which is updated after the training step t is completed. It should be mentioned that only data from D\u2081 = Dt \u222a Mt\u22121 is available for training during the t-th step.\nClassically, the model at step t can be written as the composition of two functions: ft = f_\u03b8^c \u2022 f_\u03b8^f(\u00b7), where f_\u03b8^f represents a feature extractor, and f_\u03b8^c represents a classifier. For an input sample xi, its feature representation is denoted as hi = f_\u03b8^f(xi). We employ cosine normalization [20] as the classifier f_\u03b8^c. Consequently, the predicted logit p for class c at step t can be calculated from h as:\n$$p_{i,c} = \\eta \\cdot \\langle h_i, w_c \\rangle,$$"}, {"title": "3.2 CIL-Balanced Classification Loss", "content": "As claimed in previous works [32, 36], the inherent imbalance in medical datasets and the imbalance in class incremental learning can lead to a biased classifier. Inspired by [34], we aim to mitigate this issue by adjusting the logits according to category frequency. However, for a memory-based method in class incremental learning, only the data from D\u2081 is available at step t, which consists of the memory buffer Mt-1 and the training set Dr. Hence, we define the category frequency as follows:\n$$r_c = \\begin{cases}\n\\frac{m}{q_c}, & \\text{if } c \\in \\mathcal{Y}_{1:t-1}, \\\\\n1, & \\text{if } c \\in \\mathcal{Y}_{t},\n\\end{cases}$$\nwhere qe is the number of training samples for class c, and || is the cardinality of a given set. After that, we add log re to the output logits during training. Thus, the logit-balanced classification loss can be formulated as:\n$$L_{lbc} = \\frac{1}{\\mathcal{D}_t} \\sum_{i \\in \\mathcal{D}_t} - \\log \\frac{\\exp (p_{i,y_i} + \\log r_{y_i})}{\\sum_{j \\in \\mathcal{Y}_{1:t}} \\exp (p_{i,j} + \\log r_j)}.$$\nTo explain how our method works, we reformulate Eq. 3 into Eq. 4 by introducing \u03c8yi,j := log rj \u2013 log ryi, which are defined as follows:\n$$L_{lbc} = \\frac{1}{\\mathcal{D}_t} \\sum_{i \\in \\mathcal{D}_t} - \\log \\frac{\\exp (p_{i,y_i})}{\\sum_{j \\in \\mathcal{Y}_{1:t}} \\exp (p_{i,j} + \\psi_{y_i, j})},$$"}, {"title": "3.3 Distribution Margin Loss", "content": "In class incremental learning, the representations of the old and new classes would be easily overlapped in the deep feature space [53]. To address this issue, margin loss [5] is introduced to avoid the ambiguities between old and new classes. In detail, the vanilla margin loss aims to ensure that the distance from the anchor to the positive (embedding of the ground-truth old class) is less than the distance of the anchor from the negative (embedding of the new class) to meet a predefined margin m, which can be computed as:\n$$L_m = \\sum_{i \\in M_{t-1}} \\sum_{c \\in \\mathcal{Y}_t} \\max \\{0, \\langle h_i, w_c \\rangle - \\langle h_i, w_{y_i} \\rangle + m \\},$$"}, {"title": "4 EXPERIMENTS", "content": "Datasets. Following the benchmark setting [6], we evaluate the performance on CCH5000 [25], HAM10000 [42], and EyePACS [8].\n\u2022 CCH5000: consists of histological images in human colorectal cancer. This dataset contains 8 different classes with 625 images per class: tumor, stroma, complex, lympho, debris, mucosa, adipose, and empty.\n\u2022 HAM10000: consists of 10,015 skin cancer images, including seven types of skin lesions: melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis, benign keratosis, dermatofibroma, and vascular lesions. The distribution ratios for each type are as follows: 3.27%, 5.13%, 10.97%, 1.15%, 11.11%, 66.95%, and 1.42%, which indicates a severe class imbalance.\n\u2022 EyePACS: is commonly used for the task of diabetic retinopathy (DR) classification. EyePACS dataset contains 35,126 retina images for training, which are categorized into five stages of DR. Specifically, there are 25,810 images labeled as no DR, 2,443 as mild DR, 5,292 as moderate DR, 873 as severe DR, and 708 as proliferative DR images. It is worth noting that this dataset is also highly imbalanced.\nEvaluation protocols. Following the experimental protocols in [6], we evaluate our method for different scenarios, such as 4-1, 4-2, 3-1, and 3-2. In each scenario, the numbers indicate the number of base and new classes, respectively. For example, considering the HAM10000 dataset with 7 classes, the scenario of 3-2 corresponds to learning 3 classes at the initial step and subsequently adding 2 new classes at each incremental step, requiring a total of 3 training steps.\nMetrics. Following previous work [6], we evaluate our method based on two standard metrics: Average Accuracy (Acc) and Average Forgetting (Fgt). Let at,i be the accuracy of the model evaluated on the test set of classes in Yi after training on the first t steps. The Average Accuracy is defined as:\n$$Acc = \\frac{1}{T} \\sum_{t=1}^T (\\frac{1}{i} \\sum_{i=1}^i a_{t,i})$$\nwhich measures the average classification accuracy of the model until step T. The Average Forgetting is defined as:\n$$Fgt = \\frac{1}{T} \\sum_{i=1}^T (\\frac{1}{j} \\sum_{j \\in [1, t-1]} \\max (a_{j,i} - a_{t,i}))$$"}, {"title": "4.3 Analysis of Incremental Performance", "content": "Accuracy. As shown in Fig. 3, we display the average incremental performance of each step for three datasets. According to these curves, it is evident that the performances of all methods are similar in the first step, but the baselines suffer from a significant drop as the learning steps increase. In contrast, our method effectively slows down the drop, leading to an increasing gap between the baselines and our method over time. This demonstrates that our method benefits class incremental learning in medical image classification and outperforms prior works.\nForgetting. Fig. 4 depicts the average forgetting across each incremental step for three datasets. The forgetting of most methods increases rapidly as new classes arrive, while our method consistently outperforms the SOTA methods, indicating improved resilience to catastrophic forgetting."}, {"title": "4.4 Ablation Study", "content": "Impact of each component. In Tab. 4, we present an ablation analysis on the HAM10000 3-2 setting to evaluate the effect of each proposed component. The first row refers to the baseline, which is trained with the cross-entropy loss (CE) and the knowledge distillation loss Lkd. Firstly, we observe that the distribution margin loss Ldm brings a significant contribution when applied alone, improving the performance by 12.6% in terms of Acc. Secondly, when we replace CE with the CIL-balanced classification loss Lebe, the average accuracy is improved from 67.6% to a notable 83.6%. Finally, the combination of Ldm and Lcbc further improves the performance, demonstrating the effect of both proposed components.\nEffect of CIL-Balanced Classification Loss. We investigate the impact of different classification losses on the HAM10000 3-2 setting when combined with the knowledge distillation loss Lkd and our distribution margin loss Ldm. As shown in Tab. 5, we present results of using cross-entropy loss (CE), focal loss (Focal) [29], class-balanced focal loss (CB Focal) [9], and our proposed methods (logit-balanced and CIL-balanced). It can be observed that both of our proposed methods consistently outperform the other classification loss objectives, indicating the effectiveness of them to address the imbalance issue. Furthermore, the CIL-balanced classification loss (Eq. 7) achieves an additional 1.9% improvement compared to the logit-balanced classification loss (Eq. 4), benefiting from the scale factor \u03b3 to strengthen the learning of old classes.\nEffect of Distribution Margin Loss. To verify the effectiveness of our distribution margin loss objectives, we conduct experiments on the HAM10000 3-2 setting combined with the knowledge distillation loss Lkd and our CIL-balanced classification loss Lcbc. The results presented in Tab. 6 demonstrate that our distribution margin loss brings significant improvements compared to cases without the margin loss and with the margin ranking loss [20]. To further illustrate the advantages of our method, we present t-SNE [43] visualizations of feature distributions with different margin loss objectives for the CCH5000 dataset, as shown in Fig. 5. In the absence of margin loss, we observe large intra-class clusters (blue rectangle) and significant inter-class overlap in feature space (red circle). When employing the margin ranking loss, the issue of overlap is mitigated to some extent (red circle) compared to the method without margin loss. Finally, by optimizing our distribution margin loss, we achieve a more pronounced separation between the distributions of old and new classes (red circle), while simultaneously ensuring that the representations of old classes become more compact (blue rectangle).\nAbility to integrate with other existing methods. Our proposed methods can be easily integrated with other existing CIL methods. To demonstrate this, we conduct experiments utilizing iCaRL [38] and UCIR [20] on the HAM10000 3-1 setting. Specifically, we replace the classification loss in each baseline with our CIL-balanced classification loss and incorporate our distribution margin loss. As shown in Tab. 7, the accuracy (Acc) for both baselines can be improved by about 3% with the integration of our methods. More notably, our approaches effectively reduce forgetting (Fgt) by 5.7% and 14.8% for iCaRL and UCIR, respectively.\nSensitivity study of hyper-parameters. In this paper, there are three hyper-parameters during training: the weight of the distribution margin loss \u03bbd, the weight of the knowledge distillation loss \u03bb\u03ba, and the trade-off coefficient \u03b1. We first conduct experiments to explore the impacts of different \u03bbd and \u03bbk on the HAM10000 3-2 setting. As shown in Fig. 6a, we vary dd within the range of {0.1, 0.2, 0.3, 0.4, 0.5}, and Ak within the range of {0.1, 0.25, 0.5, 0.75, 1.0}, resulting in a total of 25 compared results. From the results, we consistently observe satisfactory performance from our model, demonstrating its robustness to the selection of Ad and \u03bb\u03ba.\nTo investigate the impact of different values of a on addressing the imbalance between the old and new classes, we evaluate the accuracy by varying a from {0.1, 0.3, 0.5, 0.7, 0.9} on three benchmark datasets. As shown in Fig. 6b, the results indicate that the accuracy gradually improves as a grows larger initially, while it starts to decline when a is close to 1. Since the data distribution differs across datasets, the selection of the trade-off coefficient a also varies. Specifically, the optimal values of a for the three datasets are 0.7, 0.5, and 0.3, respectively.\nLonger incremental learning. In class incremental learning, a key challenge is catastrophic forgetting, which becomes more pronounced as the number of learning classes increases [13, 20, 38]. To quantify the robustness of our method in overcoming catastrophic forgetting, we evaluate it on two longer-step protocols: 50-10 (6 steps) and 50-5 (11 steps), employing the more challenging CIFAR100 dataset. Following the experimental protocol outlined in [30], we conduct experiments under both conventional (Conv) and long-tail (LT) scenarios. In the conventional scenario, each class has 500 training samples for training. Conversely, the long-tailed scenario follows an exponential decay in sample sizes across classes, where the ratio between the least and the most frequent class is 0.01. As illustrated in Tab. 8, our method achieves superior results in all settings. Specifically, we observe a more significant improvement in the long-tail scenario, further validating the effectiveness of our method in addressing the class imbalance problem in class incremental learning. Furthermore, we present the dynamic performance changes during the incremental learning process in Fig. 7. It is evident that with more learning steps, the gap between the baselines and our method widens, and our method's performance remains superior across different scenarios (conventional and long-tailed) throughout most of the learning steps."}, {"title": "5 CONCLUSION", "content": "In this paper, we propose two simple yet effective plug-in loss functions for class incremental learning in medical image classification. First, to address the challenge of classifier bias caused by class imbalance, we introduce a CIL-balanced classification loss via logit adjustment. Second, we propose a novel distribution margin loss that aims to enforce inter-class discrepancy and intra-class compactness simultaneously. Our extensive experimental evaluation demonstrates the state-of-the-art performance of our method across various scenarios on medical image datasets: CCH5000, HAM10000, and EyePACS."}]}