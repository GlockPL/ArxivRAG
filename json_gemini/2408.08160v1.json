{"title": "General-purpose Clothes Manipulation with Semantic Keypoints", "authors": ["Yuhong Deng", "David Hsu"], "abstract": "We have seen much recent progress in task-specific\nclothes manipulation, but generalizable clothes manipulation\nis still a challenge. Clothes manipulation requires sequential\nactions, making it challenging to generalize to unseen tasks.\nBesides, a general clothes state representation method is crucial.\nIn this paper, we adopt language instructions to specify and\ndecompose clothes manipulation tasks, and propose a large\nlanguage model based hierarchical learning method to enhance\ngeneralization. For state representation, we use semantic key-\npoints to capture the geometry of clothes and outline their\nmanipulation methods. Simulation experiments show that\nthe proposed method outperforms the baseline method in terms of\nsuccess rate and generalization for clothes manipulation tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "People have long anticipated that an intelligent household\nrobot can free them from the tedium of organizing and\nstoring clothes. Toward this goal, the robot should perform\na broad range of clothes manipulation tasks, such as \"fold\nthe T-shirt for storage\" and \"hang the skirt on the hanger\"\n(Fig. 1). Recently, learning task-specific clothes manipulation\nskills has been widely investigated [1], [2]. However, these\nmethods often fail to generalize to unseen tasks with new\nobject categories or new requirements such as different\nfolding direction, position, and times. For example, it's\ndifficult to transfer the skill from a T-shirt to a skirt or from\nfolding a towel in half once to folding it twice. However,\ngeneralizable clothes manipulation poses two challenges.\nClothes manipulation tasks typically require sequential ac-\ntions, where the action order is crucial for task completion.\nThus, generalizing to unseen tasks poses higher requirements\nfor task planning. Besides, clothes are characterized by high\ndimensionality within their state space [3]. Moreover, the\ngeometric structures of different clothes vary significantly.\nThus, an effective and general state representation method is\ncrucial.\nFor generalizable clothes manipulation, we develop\na hierarchical learning method and decompose clothes manip-\nulation into three levels of hierarchy: planning, grounding,\nand action (Fig. 2). In the planning layer, we adopt language\ninstructions to specify clothes manipulation tasks and use a\nlarge language model (LLM) for task planning. Compared\nto goal images, language instructions provide a more in-\ntuitive and flexible way of specifying tasks. Furthermore,\nlarge language models can provide commonsense knowl-\nedge for task planning and enhance the generalization [4].\nSpecifically, we prompt the LLM to decompose the given\nlanguage instruction into sequential sub-tasks. Each sub-task"}, {"title": "II. RELATED WORK", "content": "Learning for Deformable Object Manipulation. Learn-\ning methods have been used to equip the robot with task-\nspecific deformable object manipulation abilities such as\nrope rearrangement [8], [9], cloth folding [1], [10], cloth\nflattening [11], [12], and bag opening [13], [14]. Some goal-\nconditioned approaches use goal images to specify different\ntasks for multi-task learning of deformable object manipula-\ntion [15], [16]. However, the task diversity is still limited and\nit's difficult to generalize to new goals. Unlike previous work,\nwe adopt language instructions to specify and decompose\ndifferent tasks for generalizable clothes manipulation.\nLanguage-conditioned object manipulation. Language\nprovides an intuitive interface in human-robot interaction and\ncan explicitly capture the transferable concepts between dif-\nferent manipulation tasks. Thus, language-conditioned object\nmanipulation has been widely investigated. Early work fo-\ncuses on how to make the robot understand language instruc-\ntions and perform manipulation tasks [17], [18]. Recently,\nlarge language models have been employed in language-\nconditioned manipulation to enhance generalization [19]\u2013\n[21]. However, previous methods are limited to rigid objects.\nIn this paper, we extend language-conditioned manipulation's\napplication scenarios to deformable objects.\nState representation of deformable objects. Given the\nhigh-dimensional state of deformable objects, an effec-\ntive state representation method is necessary. To simu-\nlate deformable objects, particles and mesh representations\nhave been explored [22]-[24]. Compared with particles and\nmeshes, keypoints representation has lower dimensionality,\nleading to more effective policy learning [25]. Keypoints\nrepresentation is also suitable for clothes, which has a pre-\ndefined structure with significant geometric features. In this\npaper, we explore how to detect effective semantic keypoints\nas the state representation of clothes."}, {"title": "III. METHOD", "content": "In this work, we propose a hierarchical learning method\n(Fig. 2) that formulates the problem of generating trajectories\n${T_i}$ for clothes manipulation task specified by a given\nlanguage instruction s into three levels of hierarchy: (1) Task\nplanning \u2013 inferring a sequence of sub-task ${w_i}$ conditioned"}, {"title": "A. Task Planning", "content": "LLMs are utilized to enhance robot task planning due\nto their powerful commonsense knowledge from extensive\ninternet-scale training data. However, previous work is lim-\nited to action primitives such as picking and placing, moving,\nand opening. Such action primitives are not sufficient for\ngeneralizable clothes manipulation. Thus, we utilize the\nLLM with a chain of thought prompting [26] to define\naction primitives. The LLM is prompted to (1) provide\nexamples of clothes manipulation tasks; (2) decompose these\nexamples into basic actions; (3) summarize the actions used\nin step (2) and identify action primitives. In this way, we\nidentify action primitives, including grasp, release, moveto,\nrotate, press, and pull. These action primitives reflect LLM's\ncommonsense knowledge, enhancing LLM's task planning.\nTo generate sub-tasks, we then prompt the LLM with some\nexamples consisting of language instructions paired with\ndesirable sub-tasks sequences."}, {"title": "B. Visual Grounding", "content": "Upon obtaining a sub-task $w_i = a_i(d_i)$ from task plan-\nning, our visual grounding layer will ground the contact\npoint description $d_i$ conditioned on current observation $I_i$.\nFor sim-to-real transferring, we utilize depth images as\nobservation. Given that clothes has a predefined structure\nwith significant geometric features, such as sleeves and\ncollars, identifying semantic keypoints of these features can\neffectively capture the clothes' geometry and outline possible\nmanipulation methods. Thus, our visual grounding layer is\nbased on semantic keypoints detection. We first leverage a\nmasked autoencoder as a spatiotemporal learner to establish"}, {"title": "C. Action Generation", "content": "After grounding the contact point description $d_i$ to its\nposition $c_i$, our action model generates an action trajectory\n$T_i$ conditioned on the action primitive $a_i$. The action model\nis based on manually designed rules."}, {"title": "IV. EXPERIMENTS", "content": "To evaluate the proposed hierarchical learning method\non generalizable clothes manipulation, we compared the\nproposed method with the end-to-end baseline method on a\nset of clothes manipulation tasks in simulation environment."}, {"title": "A. Simulation Experiment Setup", "content": "We choose CLIPORT [18] as the baseline method, which\nrepresents the typical end-to-end algorithm for language-\nconditioned manipulation policy learning. CLIPORT relies\non a pre-trained vision-language model.\nWe extended SoftGym benchmark to 30 common clothes\nmanipulation tasks. These tasks can be mainly divided into:\n(1) folding clothes in different way (corner folding, half fold-\ning, and diagonal folding); (2) folding clothes for storage; (3)\nflattening crumpled clothes; (4) hanging clothes on a hanger;\n(5) placing clothes and storing them. The clothes categories\ninclude T-shirts, trousers, skirts, and towels. Each category"}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a hierarchical learning method\nfor generalizable clothes manipulation, where language in-\nstructions and a LLM are used for task specification and\nplanning. To represent the s clothes effectively, we use\na masked autoencoder to detect semantic keypoints under\nocclusion. Semantic keypoints are used to ground contact\npoint of manipulation tasks. Simulation experiment results\nshow that proposed hierarchical learning method outperforms\nthe baseline method in success rate and generalization.\nProposed method can generalize to unseen tasks with new\nobject categories or new requirements. For future work, we\nwill explore the generalization on object instances and close-\nloop task planning of clothes manipulation."}]}