{"title": "General-purpose Clothes Manipulation with Semantic Keypoints", "authors": ["Yuhong Deng", "David Hsu"], "abstract": "We have seen much recent progress in task-specific\nclothes manipulation, but generalizable clothes manipulation\nis still a challenge. Clothes manipulation requires sequential\nactions, making it challenging to generalize to unseen tasks.\nBesides, a general clothes state representation method is crucial.\nIn this paper, we adopt language instructions to specify and\ndecompose clothes manipulation tasks, and propose a large\nlanguage model based hierarchical learning method to enhance\ngeneralization. For state representation, we use semantic key-\npoints to capture the geometry of clothes and outline their\nmanipulation methods. Simulation experiments show that\nthe proposed method outperforms the baseline method in terms of\nsuccess rate and generalization for clothes manipulation tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "People have long anticipated that an intelligent household\nrobot can free them from the tedium of organizing and\nstoring clothes. Toward this goal, the robot should perform\na broad range of clothes manipulation tasks, such as \"fold\nthe T-shirt for storage\" and \"hang the skirt on the hanger\"\n(Fig. 1). Recently, learning task-specific clothes manipulation\nskills has been widely investigated [1], [2]. However, these\nmethods often fail to generalize to unseen tasks with new\nobject categories or new requirements such as different\nfolding direction, position, and times. For example, it's\ndifficult to transfer the skill from a T-shirt to a skirt or from\nfolding a towel in half once to folding it twice. However,\ngeneralizable clothes manipulation poses two challenges.\nClothes manipulation tasks typically require sequential ac-\ntions, where the action order is crucial for task completion.\nThus, generalizing to unseen tasks poses higher requirements\nfor task planning. Besides, clothes are characterized by high\ndimensionality within their state space [3]. Moreover, the\ngeometric structures of different clothes vary significantly.\nThus, an effective and general state representation method is\ncrucial.\nFor generalizable clothes manipulation, we develop\na hierarchical learning method and decompose clothes manip-\nulation into three levels of hierarchy: planning, grounding,\nand action (Fig. 2). In the planning layer, we adopt language\ninstructions to specify clothes manipulation tasks and use a\nlarge language model (LLM) for task planning. Compared\nto goal images, language instructions provide a more in-\ntuitive and flexible way of specifying tasks. Furthermore,\nlarge language models can provide commonsense knowl-\nedge for task planning and enhance the generalization [4].\nSpecifically, we prompt the LLM to decompose the given\nlanguage instruction into sequential sub-tasks. Each sub-task"}, {"title": "A. Task Planning", "content": "LLMs are utilized to enhance robot task planning due\nto their powerful commonsense knowledge from extensive\ninternet-scale training data. However, previous work is lim-\nited to action primitives such as picking and placing, moving,\nand opening. Such action primitives are not sufficient for\ngeneralizable clothes manipulation. Thus, we utilize the\nLLM with a chain of thought prompting [26] to define\naction primitives. The LLM is prompted to (1) provide\nexamples of clothes manipulation tasks; (2) decompose these\nexamples into basic actions; (3) summarize the actions used\nin step (2) and identify action primitives. In this way, we\nidentify action primitives, including grasp, release, moveto,\nrotate, press, and pull. These action primitives reflect LLM's\ncommonsense knowledge, enhancing LLM's task planning.\nTo generate sub-tasks, we then prompt the LLM with some\nexamples consisting of language instructions paired with\ndesirable sub-tasks sequences."}, {"title": "B. Visual Grounding", "content": "Upon obtaining a sub-task \\(w_i = a_i(d_i)\\) from task plan-\nning, our visual grounding layer will ground the contact\npoint description \\(d_i\\) conditioned on current observation \\(I_i\\).\nFor sim-to-real transferring, we utilize depth images as\nobservation. Given that clothes has a predefined structure\nwith significant geometric features, such as sleeves and\ncolars, identifying semantic keypoints of these features can\neffectively capture the clothes' geometry and outline possible\nmanipulation methods. Thus, our visual grounding layer is\nbased on semantic keypoints detection. We first leverage a\nmasked autoencoder as a spatiotemporal learner to establish"}, {"title": "C. Action Generation", "content": "After grounding the contact point description \\(d_i\\) to its\nposition \\(c_i\\), our action model generates an action trajectory\n\\(T_i\\) conditioned on the action primitive \\(a_i\\). The action model\nis based on manually designed rules."}, {"title": "IV. EXPERIMENTS", "content": "To evaluate the proposed hierarchical learning method\non generalizable clothes manipulation, we compared the\nproposed method with the end-to-end baseline method on a\nset of clothes manipulation tasks in simulation environment."}, {"title": "A. Simulation Experiment Setup", "content": "We choose CLIPORT [18] as the baseline method, which\nrepresents the typical end-to-end algorithm for language-\nconditioned manipulation policy learning. CLIPORT relies\non a pre-trained vision-language model.\nWe extended SoftGym benchmark to 30 common clothes\nmanipulation tasks. These tasks can be mainly divided into:\n(1) folding clothes in different way (corner folding, half fold-\ning, and diagonal folding); (2) folding clothes for storage; (3)\nflattening crumpled clothes; (4) hanging clothes on a hanger;\n(5) placing clothes and storing them. The clothes categories\ninclude T-shirts, trousers, skirts, and towels. Each category"}, {"title": "B. Simulation Experiment Results", "content": "The experiment results are shown in TABLE I. Over-\nall, our method outperforms CLIPORT in seen and un-\nseen tasks, especially when the task complexity increases.\nCLIPORT can generalize to unseen easy tasks and some\nunseen medium tasks. The pre-trained vision-language model\nenable CLIPORT to capture the similarity between different\ntasks (e.g.\"hang the T-shirt\" and \"hang the skirt\"). But it's\ndifficult to learn action sequences of hard tasks in an end-\nto-end manner. The learned policies are not generalizable.\nIn contrast, hierarchical learning can learn transferable lan-\nguage and visual concepts across clothes manipulation tasks.\nLLM can complete the task planning of unseen tasks and\ndecompose unseen tasks to predefined action primitives.\nAdditionally, semantic keypoints are independent of specific\ntasks, which can be utilized to ground contact points of\nunseen manipulation tasks."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a hierarchical learning method\nfor generalizable clothes manipulation, where language in-\nstructions and a LLM are used for task specification and\nplanning. To represent the s clothes effectively, we use\na masked autoencoder to detect semantic keypoints under\nocclusion. Semantic keypoints are used to ground contact\npoint of manipulation tasks. Simulation experiment results\nshow that proposed hierarchical learning method outperforms\nthe baseline method in success rate and generalization.\nProposed method can generalize to unseen tasks with new\nobject categories or new requirements. For future work, we\nwill explore the generalization on object instances and close-\nloop task planning of clothes manipulation."}, {"title": "II. RELATED WORK", "content": "Learning for Deformable Object Manipulation. Learning methods have been used to equip the robot with task-specific deformable object manipulation abilities such as rope rearrangement [8], [9], cloth folding [1], [10], cloth flattening [11], [12], and bag opening [13], [14]. Some goal-conditioned approaches use goal images to specify different tasks for multi-task learning of deformable object manipula-tion [15], [16]. However, the task diversity is still limited and it's difficult to generalize to new goals. Unlike previous work, we adopt language instructions to specify and decompose different tasks for generalizable clothes manipulation.\nLanguage-conditioned object manipulation. Language provides an intuitive interface in human-robot interaction and can explicitly capture the transferable concepts between dif-ferent manipulation tasks. Thus, language-conditioned object manipulation has been widely investigated. Early work fo-cuses on how to make the robot understand language instruc-tions and perform manipulation tasks [17], [18]. Recently, large language models have been employed in language-conditioned manipulation to enhance generalization [19]\u2013[21]. However, previous methods are limited to rigid objects. In this paper, we extend language-conditioned manipulation's application scenarios to deformable objects.\nState representation of deformable objects. Given the high-dimensional state of deformable objects, an effec-tive state representation method is necessary. To simu-late deformable objects, particles and mesh representations have been explored [22]-[24]. Compared with particles and meshes, keypoints representation has lower dimensionality, leading to more effective policy learning [25]. Keypoints representation is also suitable for clothes, which has a pre-defined structure with significant geometric features. In this paper, we explore how to detect effective semantic keypoints as the state representation of clothes."}, {"title": "III. METHOD", "content": "In this work, we propose a hierarchical learning method\n(Fig. 2) that formulates the problem of generating trajectories\n\\({T_i}\\) for clothes manipulation task specified by a given\nlanguage instruction s into three levels of hierarchy: (1) Task\nplanning \u2013 inferring a sequence of sub-task \\({\\{w_i\\}}\\) conditioned\non the language instruction s, \\(w_i = a_i(d_i)\\), \\(a_i\\) refers to the\naction primitive and \\(d_i\\) refers to the language description of\nthe contact point. (2) Visual grounding - for each sub-task\n\\(w_i\\), detecting keypoints \\(P_i\\) from observation \\(I_i\\) as the state\nrepresentation, and grounding contact point \\(c_i\\) conditioned\non keypoints \\(P_i\\) and contact point description \\(d_i\\). (3) Action\ngeneration - for each sub-task \\(w_i\\), generating a trajectory \\(T_i\\)\nconditioned on the action primitive \\(a_i\\) and the contact point\n\\(c_i\\). We make two assumptions: the clothes manipulation is\nquasi-static and not long-horizon."}]}