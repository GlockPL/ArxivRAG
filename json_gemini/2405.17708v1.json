{"title": "OPERA: Automatic Offline Policy Evaluation with Re-weighted Aggregates of Multiple Estimators", "authors": ["Allen Nie", "Yash Chandak", "Christina J. Yuan", "Anirudhan Badrinath", "Yannis Flet-Berliac", "Emma Brunskill"], "abstract": "Offline policy evaluation (OPE) allows us to evaluate and estimate a new sequential decision-making policy's performance by leveraging historical interaction data collected from other policies. Evaluating a new policy online without a confident estimate of its performance can lead to costly, unsafe, or hazardous outcomes, especially in education and healthcare. Several OPE estimators have been proposed in the last decade, many of which have hyperparameters and require training. Unfortunately, choosing the best OPE algorithm for each task and domain is still unclear. In this paper, we propose a new algorithm that adaptively blends a set of OPE estimators given a dataset without relying on an explicit selection using a statistical procedure. We prove that our estimator is consistent and satisfies several desirable properties for policy evaluation. Additionally, we demonstrate that when compared to alternative approaches, our estimator can be used to select higher-performing policies in healthcare and robotics. Our work contributes to improving ease of use for a general-purpose, estimator-agnostic, off-policy evaluation framework for offline RL.", "sections": [{"title": "1 Introduction", "content": "Offline reinforcement learning (RL) involves learning better sequential decision policies from logged historical data, such as learning a personalized policy for math education software (Mandel et al., 2014; Ruan et al., 2024), providing treatment recommendations in the ICU (Komorowski et al., 2018) or learning new controllers for robotics(Kumar et al., 2020; Yu et al., 2020). Offline policy evaluation (OPE), in which the performance  J(\u03c0e)  of a new evaluation policy  \u03c0e  is estimated given historical data, is a common subroutine in offline RL for policy selection, and can be particularly important when deciding whether to deploy a new decision policy that might be unsafe or costly. Offline policy evaluation methods estimate the performance of an evaluation policy  \u03c0e  given data collected by a behavior policy  \u03c0\u044c. There are many existing OPE algorithms, including those that create importance sampling-based estimators (IS) (Precup, 2000), value-based estimators (FQE) (Le et al., 2019), model-based estimators (Paduraru, 2013; Liu et al., 2018b; Dong et al., 2023), doubly robust estimators Jiang and Li (2016); Thomas and Brunskill (2016), and minimax-style estimators (Liu et al., 2018a; Nachum et al., 2019; Yang et al., 2020).\nThis raises an important practical question: given a set of different OPE methods, each producing a particular value estimate for an evaluation policy, what value estimate should be returned? A simple approach is to avoid the problem and pick only one OPE algorithm or look at the direction of a set of OPE algorithms' scores as a coarse agreement measure. Voloshin et al. (2021) offered heuristics based"}, {"title": "2 Related Work", "content": "Offline policy evaluation Most commonly used offline policy estimators can be divided into a few categories depending on the algorithm. An important family of estimators focuses on using importance sampling (IS) and weighted importance sampling (WIS) to reweigh the reward from the behavior policy (Precup, 2000). These estimators are known to produce an unbiased estimate but have a high variance when the dataset size is small. For a fully observed Markov Decision Process (MDP), a model-free estimator, such as fitted Q evaluation (FQE), is proposed by Le et al. (2019), and one can also learn a model given the data to produce a model-based (MB) estimate (P\u0103duraru, 2007; Fu et al., 2021; Gao et al., 2023). When the behavior policy's probability distribution over action is unknown, a minimax style optimization estimator (DualDICE) can jointly estimate the distribution ratio and the policy performance (Nachum et al., 2019). For a partial observable MDP (POMDP), many of these methods have been extended to account for unobserved confounding, such as minimax style estimation (Shi et al., 2022), value-based estimation (Tennenholtz et al., 2020; Nair and Jiang, 2021), uses sensitivity analysis to bound policy value (Kallus and Zhou, 2020; Namkoong et al., 2020; Zhang and Bareinboim, 2021), or learns useful representation over latent space (Chang et al., 2022).\nOPE with multiple estimators Choosing the right estimators has become an issue when there are many proposals even under the same task setup and assumptions. Voloshin et al. (2021) proposed"}, {"title": "3 Notation and Problem Setup", "content": "We define a stochastic Decision Process  M = (S, A, T, r, \u03b3) , where  S  is a set of states;  A  is a set of actions;  T  is the transition dynamics;  r  is the reward function; and  \u03b3 \u2208 (0, 1)  is the discount factor. Let  Dn = {\u03c4i}i=1n = {si, ai, si', ri}i=0n  be the trajectories sampled from  \u03c0  on  M. We denote the true performance of a policy  \u03c0  as its expected discounted return  J(\u03c0) = E\u03c4\u223c\u03c1\u03c0[G(\u03c4)]  where  G(\u03c4) = \u2211t=0\u221e\u03b3trt  and  \u03c1\u03c0  is the distribution of  \u03c4  under policy  \u03c0. In an off-policy policy evaluation problem, we take a dataset  Dn , which can be collected by one or a group of policies which we refer to as the behavior policy  \u03c0b  on the decision process  M. An OPE estimator takes in a policy  \u03c0e  and a dataset  Dn  and returns an estimate of its performance, where we mark it as  V : \u03a0 \u00d7 D \u2192 R. We focus on estimating the performance of a single policy  \u03c0. We define the true performance of the policy  V\u03c0 = J(\u03c0) , and multiple OPE estimates of its performance as  Vi\u03c0(Dn) = Vi(\u03c0, Dn)  for the  i -th OPE's estimate."}, {"title": "4 OPERA", "content": "In this section, we consider combining results from multiple estimators  {Vi\u03c0}i=1k  to obtain a better estimate for  V\u03c0. Towards this goal, given  {Vi\u03c0}i=1k , we propose estimating a set of weights  \u03b1 \u2208 Rk  such that  V\u03b1 := \u2211i=1k \u03b1iVi\u03c0 \u2208 R  has the lowest mean squared error (MSE) towards estimating  V\u03c0. Formally, let  V\u0302 \u2208 Rk\u00d71  be a vector whose elements correspond to values from different estimators, and let  V  \u2208  Rk\u00d71  correspond to a vector where each element is the same and corresponds to  V\u03c0 . Let  \u03b1* \u2208 Rk\u00d71  be a vector with values of all  \u03b1 's and let  \u03b1 \u2208 Rk\u00d71  be an estimate of  \u03b1* . For any estimator  V\u03b1 , the mean-squared error is denoted by,\nMSE(V\u03b1) := EDn[(V\u03b1(Dn) \u2212 V\u03c0)2] \u2208 R,\nwhere we make  V\u03b1  explicitly depend on  Dn  to indicate that the expectation is over the random variables  V\u03b1  which depend on the sampled data  Dn . With this formulation, estimating  \u03b1*  can be elicited as a solution to the following constrained optimization problem.\nRemark 1. Let  \u2211i\u03b1i = 1 , then\n\u03b1\u2217 \u2208 argmin\u03b1\u2208Rk\u00d71\u03b1T A\u03b1 s.t.\u2211i\u03b1i = 1, then,\n$\\alpha^* \\in \\underset{\\alpha \\in \\mathbb{R}^{k \\times 1}}{argmin} \\alpha^T A \\alpha s.t. \\sum_i \\alpha_i = 1$\nwhere  A := E[(V \u2212 V\u0304)(V \u2212 V\u0304)T] \u2208 Rk\u00d7k ."}, {"title": "4.1 Properties of OPERA", "content": "For  A  obtained from the bootstrap procedure in equation 4 to be an asymptotically accurate estimate of  A , (a) a consistent estimator of  V  is required, and (b) the estimators  V  need to be smooth. We discuss these points in more detail in Appendix A.3. In the following, we theoretically establish the properties of OPERA on performance improvement and consistency. We also demonstrate how OPERA allows us to interpret each estimator's quality. Further, in Section 6, we empirically study the effectiveness of OPERA even when we do not have any consistent base estimators, or  V  is constructed using deep neural networks."}, {"title": "4.2 Finite Sample Analysis of OPERA", "content": "Without loss of generality, let  \u2200n \u2208 \u03a0, |J(\u03c0)| \u2264 1, such that we can always consider  \u2200i, |V\u03b1| \u2264 1 (this can be trivially achieved by normalizing each estimator's output by  |Vmax| ). Let  V\u03b1  be a weighted sum of  Vi\u03b1  with  \u03b1\u2217 , where the total number of estimators in the ensemble is  k ."}, {"title": "5 Experiment", "content": "We now evaluate OPERA on a number of domains commonly used for offline policy evaluation. Experimental details, when omitted, are presented in the appendix."}, {"title": "5.1 Task/Domains", "content": "Contextual Bandit. We validate the performance of OPERA on the synthetic bandit domain with a 10-dimensional feature space proposed in SLOPE (Su et al., 2020). This domain illustrates how OPERA compares to an estimator-selection algorithm (SLOPE) that assumes a special structure between the estimators. The true reward is a non-linear neural network function. The reward estimators are parametrized by kernels and the bandwidths are the main hyperparameters. As in their paper, we ran 180 configurations of this simulated environment with different parametrization of the environment. Each configuration is replicated 30 times.\nSepsis. This domain is based on a simulator that allows us to model learning treatment options for sepsis patients in ICU (Oberst and Sontag, 2019). There are 8 actions and a +1/-1/0 reward at the episode end. We experiment with two settings: Sepsis-MDP and Sepsis-POMDP, where some crucial states have been masked. We evaluate 7 different policies: an optimal policy and 6 noised suboptimal policies, which we obtain by adding uniform noise to the optimal policy.\nGraph. Voloshin et al. (2019) introduced a ToyGraph environment with a horizon length T and an absorbing state  xabs = 2T. Rewards can be deterministic or stochastic, with +1 for odd states, -1 for even states plus one based on the penultimate state. We evaluate the considered methods on a short horizon H=4, varying stochasticity of the reward and transitions, and MDP/POMDP settings.\nD4RL-Gym. D4RL (Fu et al., 2020) is an offline RL standardized benchmark designed and commonly used to evaluate the progress of offline RL algorithms. We use 6 datasets (200k samples each) from three Gym environments: Hopper, HalfCheetah, and Walker2d. We use two datasets from each: the medium-replay dataset, which consists of samples from the experience replay buffer, and the medium dataset, which consists of samples collected by the medium-quality policy. We use conservative Q-learning (CQL) (Kumar et al., 2020), implicit Q-learning (IQL) (Kostrikov et al., 2021), and TD3 (Fujimoto et al., 2018). We train 6 policies from these three algorithms with 2 different hyperparameters for the neural network. We selected 2 FQE hyperparameters for each task and picked 2 checkpoints (one early, one late) to obtain 4 estimators to build the OPE ensemble."}, {"title": "5.2 Baseline Ensemble OPE Methods", "content": "We compare to using single OPE estimators as well as two new baseline algorithms that combine OPE estimates together. AvgOPE: We can compute a simple average estimator that just outputs the average of all underlying OPE estimates. If an estimator in the ensemble outputs an arbitrarily bad value, this estimator has no implicit mechanism to ignore such an adversarial estimator. BestOPE: We select the OPE estimator that has the smallest estimated MSE. This estimator can be better than AvgOPE as it can ignore bad estimators. In addition, in different domains, we compare to other OPE strategies such as BVFT (Batch Value Function Tournament): making pariwise comparisons between different Q-function estimators with the BVFT-loss (Xie and Jiang, 2021; Zhang and Jiang, 2021).\nSLOPE: an estimator selection method that based on Lepski's method, assuming the estimators forming an order of decreasing variance and increasing bias (Yuan et al., 2021). DR (Doubly Robust):"}, {"title": "5.3 Results", "content": "Contextual Bandit We report the result in Figure 2. Figure 2a shows that as the dataset size grows, the bootstrapping procedure employed by OPERA can quickly estimate the performance each estimator and compute a weighted score that is better than a single estimator. In the ultra-small data regime, OPERA is worse than single-estimator selection style algorithms, mainly because OPERA does not explicitly reject estimators. We can add an additional procedure to reject bad estimators and then combine the rest with OPERA, using a rejection algorithm by Lee et al. (2022).\nSepsis We report the results in Table 1. In this domain, OPERA is able to produce an estimate, on average, across many policies with different degrees of optimality, that matches and exceeds the best estimator in the ensemble. Even though in three out of four tasks, OPERA MSE is close to the MSE of the best estimator in the ensemble, in the MDP (N=200) setup, OPERA is able to get a significantly lower MSE than any of the estimators in the ensemble, suggesting a future direction of carefully choosing a set of weak estimators to put in the ensemble to obtain a strong estimator.\nGraph We report the graph domain result in Appendix A.9 and in Table 5. We find a similar result to the Sepsis domain. OPERA is able to outperform AvgOPE and BestOPE in different setups.\nD4RL We report the results in Table 2. We choose this domain because, in continuous control tasks, the horizon is often very long. Many OPE estimators that rely on short-horizon or discrete actions will not be able to extend to this domain. A popular OPE choice is FQE with function approximation, but it is difficult to determine hyperparameters like early stopping, network architecture, and learning rate. We can see that even though FQE used in D4RL is not a consistent estimator and does not satisfy OPERA's theoretical assumption, we are still able to combine the estimations to reach an aggregate estimate with lower MSE."}, {"title": "6 Discussion: Different MSE Estimation Strategies", "content": "Part of our algorithm implicitly involves estimating the MSE of each OPE estimator. In our algorithm we do this using bootstrapping but other alternatives are possible. For example, prior work by (Thomas and Brunskill, 2016) provided a way to estimate the bias and variance of an OPE estimator are computed through per-trajectory OPE scores and used this as part of their MAGIC estimator. However, this method cannot estimate the MSE of self-normalizing estimators (such as WIS) or minimax-style estimators (such as any estimator in the DICE family (Yang et al., 2020)). We denote this estimator as MSEMAGIC(V\u03c0) and now explore how our approach of using boostrapping compares to this method in an illustrative setting."}, {"title": "6.2 Variants of OPERA with Different Strategies", "content": "We now explore two alternative strategies to estimate the MSE of each estimator. The first strategy is, instead of using the estimator's own score as the centering variable  V , we use a consistent and unbiased estimator's score as  V . We call this OPERA-IS. Another strategy is to use the idea from (Thomas and Brunskill, 2016)'s MAGIC algorithm, where the bias estimate of each estimator compares the estimand to the upper or lower confidence bound of a weighted importance sampling estimator, as above. We call this OPERA-MAGIC. These are two new variants of our OPERA algorithm that will may lead to learning different  \u03b1  weights and producing different linearly stacked estimates. We use these two new methods, and compute the true MSE of the resulting stacked estimate, compared to OPERA and other baseline estimates. We use the Sepsis domains to illustrate the results and use as input IS, WIS and FQE OPE estimates.\nThe true MSE of the resulting estimates are presented in Table 4. While using an unbiased consistent estimator as the centering variable can help further improve OPERA's estimate, sometimes it also hurts the performance (MDP N=1000 setting). OPERA-MAGIC however almost always performs worse than the best estimator in the ensemble. This suggests that when combining OPE scores this bound on the bias, which will provide a distorted estimate of the estimator bias especially in low data regimes, can lead to learning less effective weightings of the input OPE estimands. OPERA remains a solid option across all settings presented in the table."}, {"title": "7 Conclusion", "content": "We propose a novel offline policy evaluation algorithm, OPERA, that leverages ideas from stack generalization to combine many OPE estimators to produce a single estimate that achieves a lower MSE. Though such stacked generalization / meta-learning has been frequently used to create better estimates from ensembles of input methods in supervised learning, to our knowledge this is the first time it has been explored in offline reinforcement learning. One challenge is that unlike in supervised learning, we do not have ground truth labels for offline policy learning. OPERA uses bootstrapping to estimate the MSE for each OPE estimator in order to find a set of weights to blend each OPE's estimate. We provide a finite sample analysis of OPERA's performance under mild assumptions, and demonstrate that OPERA provides notably more accurate offline policy evaluation estimates compared to prior methods in benchmark bandit tasks and offline RL tasks, including a Sepsis simulator and the D4RL settings. There are many interesting directions for future work, including using more complicated meta-aggregators."}]}