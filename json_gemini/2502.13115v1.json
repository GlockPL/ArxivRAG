{"title": "Near-Optimal Private Learning in Linear Contextual Bandits", "authors": ["Fan Chen", "Jiachun Li", "Alexander Rakhlin", "David Simchi-Levi"], "abstract": "We analyze the problem of private learning in generalized linear contextual bandits. Our approach\nis based on a novel method of re-weighted regression, yielding an efficient algorithm with regret of\n\u00d5(d\u00b2\u221aT+d5/2) and \u00f5(\u221aT/a) in the joint and local model of privacy, respectively. Further, we\nprovide near-optimal private procedures that achieve dimension-independent rates in private linear mod-\nels and linear contextual bandits. In particular, our results imply that joint privacy is almost \"for free\"\nin all the settings we consider, partially addressing the open problem posed by Azize and Basu [2024].", "sections": [{"title": "Introduction", "content": "Contextual bandits provide a natural framework for interactive decision making, applicable to numerous\nreal-world domains. In this setting, the decision maker (or, the algorithm) sequentially observes a context,\nselects an action, and receives a reward [Abbasi-Yadkori et al., 2011, Auer et al., 2002, Simchi-Levi and\nXu, 2020, Foster and Rakhlin, 2020]. The central challenge is to balance exploration (learning the reward\nstructure) and exploitation (maximizing the cumulative rewards). Further, in many applications, there\nare additional privacy concerns, as the contexts often involve sensitive personal information such as past\npurchase histories, credit scores, or physcial data-information not meant for public disclosure [Dwork et al.,\n2014, Lei et al., 2024, Chen et al., 2022]. Despite extensive research on interactive decision making, it\nis not yet well-understood how to achieve the optimal privacy-utility trade-offs even in the fundamental\nsetting of contextual bandits-arguably one of the simplest and most commonly considered models of online\nlearning [Shariff and Sheffet, 2018, Azize and Basu, 2024].\nFormally, in the setting of contextual bandits, the learner observes a context xt \u2208 X at each step t \u2208 [T],\ndrawn stochastically as xt ~ P. Based on the context xt (and the history up to step t), the learner selects\nan action at \u2208 A and observes a reward rt \u2208 [-1,1] with expected value E[rt xt, at] = f*(xt, at). Here,\nf* : X \u00d7 A \u2192 [-1,1] is the underlying mean reward function. The performance of the learner is typically\nmeasured by its regret, defined as\nReg := E\\sum_{t=1}^{T} max_{a \\in A} f^{*}(x_{t}, a) - f^{*}(x_{t}, a_{t})]\nwhich measures the gap between the learner's cumulative rewards and that of an optimal policy with the\nfull knowledge of f*. In generalized linear contextual bandits, a widely studied model, the ground-truth f*\nis assumed to take the form\nf*(x, \u03b1) = \u03bd((\u00a2(x, \u03b1), \u03b8*)), \u221a(x, a) \u2208 X \u00d7 A,\nwhere v: [-1,1] \u2192 [-1,1] is a known link function, \u00a2 : X \u00d7 A \u2192 Bd(1) is a known feature map, and\n0* \u2208 Bd(1) is the unknown underlying parameter.\nEven in linear contextual bandits (where v(t) = t is the identity function and f* is linear), there is limited\nunderstanding of how to design privacy-preserving procedures that attain optimal regret. Under the joint"}, {"title": "Preliminaries", "content": "Differential privacy (DP) is a widely adopted framework for ensuring privacy in data analysis. In this section,\nwe will introduce the definition of joint DP and local DP algorithms for both offline and interactive settings.\nWe first review the notion of differentially private (DP) channels.\nDefinition 1 (DP channel) For the latent observation space Z and the noisy observation space O, a\nchannel Qis a measurable map Z \u2192 \u2206(O). A channel Q is (a,\u03b2)-DP if for z, z' \u2208 Z, any measurable set\nECO,\nQ(E|z) \u2264 e^{\u03b1}Q(E|z') + \u03b2.\nIn this paper, we focus on the regime a, \u03b2\u03b5 (0,1).\nThe Gaussian channel is a standard example of the (\u03b1, \u03b2)-DP channels (see e.g., [Balle and Wang, 2018]).\nDefinition 2 (Gaussian channel) Suppose that \u03b1, \u03b2\u2208 (0,1), and denote \u03c3\u03b1,\u03b2= 2/log(1.25/\u03b2) For any\ngiven function F : Z \u2192 Rd, we let \u2206(F) := supz,z/ ||F(z) \u2013 F(z')||2. Then, for any \u25b3 > \u25b3(F), the channel\nQ(\u00b7|z) = N(F(z), \u03c3\u03b1,\u03b2\u2206\u00b2) is a (\u03b1, \u03b2)-DP channels.\nIn the following, we denote by Priv\u25b3(v) = N(v, \u03c3\u03b1,\u03b2\u22062) the Gaussian channel with sensitivity \u2206. It is\nguaranteed that for any function F with ||F(z)|| \u2264 \u2206, the channel z \u2192 Priv\u25b3(F(z)) is (\u03b1, \u03b2)-DP. Further,\nfor any symmetric matrix V \u2208 Rd\u00d7d, we denote SymPriv\u25b3(V) to be the distribution of V + Z, where Z is a\nsymmetric Gaussian random matrix, i.e., Zij = Zji ~ N(0, \u03c3\u03b1,\u03b2\u22062) independently.\nJoint DP. We first recall the definition of (joint) DP algorithms for non-interactive problems. In this\nsetting, an algorithm maps the dataset D = {\u22481,\u2026,zT} \u2208 ZT to a distribution over the output decision\n\u03a0. The two dataset D = (\u22481,\u2026, zT), D' = (z\u2081,\u00b7\u00b7\u00b7, z'\u00fd) \u2208 ZT are neighbored if there is at most one index i\nsuch that zi \u2260 2.\nDefinition 3 (JDP for non-interactive algorithms) An algorithm Alg preserves (a, \u03b2)-JDP if for any\nneighbored dataset D,D' and any measurable set E \u2286 \u2206(II),\nAlg(E|D) \u2264 e^{\u03b1} Alg(E|D') + \u03b2."}, {"title": "Motivation", "content": "We start by reviewing why most existing private algorithms for linear contextual bandits fail to achieve an\noptimal regret rate without the strong explorability condition Amin > 0. Such insufficiency motivates the\nconfidence intervals-based approach of Li et al. [2024] discussed in Section 3.2.\n3.1 Insufficiency of standard regression\nIn general, the existing algorithmic principles for learning contextual bandits mostly rely (either explicitly\nor implicitly) on the regression subroutines that, given a sequence of observation {(xt, at, rt)}t\u2208[N], produce\nan reward estimation f with bounded mean-square error:\nE_{(x,a)~D}(f(x, a) - f^{*}(x,a))^{2} \u2264 E(N)^{2}.\nFor non-private linear contextual bandits, it is well-known that regression-based estimators achieve the\noptimal rate of E(N)\u00b2 = 1/N, and such a convergence guarantee of N-1-rate is essential in the regret analysis\nof the classical LinUCB algorithm and its variants [Abbasi-Yadkori et al., 2011, Li et al., 2019, Bastani and\nBayati, 2020]. Further, for contextual bandits with a general reward function class, the recent regression-\noracle based algorithms [Simchi-Levi and Xu, 2020, Foster and Rakhlin, 2020] achieve regret bounds scaling\nwith \u00d5(TE(T)), and hence a T-1-rate of convergence under L2-error metric is also crucial to obtain a\nregret of order \u00d5(\u221aT).\nTherefore, for regression-based algorithms, achieving rate-optimal regret essentially relies on the L2-error\nguarantee of the regression subroutine. However, it is known that in linear models, privacy leads to slower\nconvergence under the L2-error if the covariate distribution is ill-conditioned, as the following folklore lemma\nindicates.\nProposition 3.1 (Lower bounds for ill-conditioned linear regression) Suppose that T > 1, a \u2208 (0, 1],\n\u03bb\u2208 [0, 1], d = 1, and the link function v(t) = t is identity. Let the covariate distribution p\u2208\u2206([-1,1]) be\nknown and given by p(1) = \u03bb,p(0) = 1 \u2212 \u03bb. Then\n(1) For any T-round (a, \u03b2)-JDP algorithm Alg with output ), it holds that\nsup_{\\theta^{*} \\in [-1,1]} E_{\\phi \\sim p} [E_{\\phi \\sim p} (\\phi, \\hat{\\theta} - \\theta^{*})^{2}] \\geq \\min {\\frac{\\lambda}{T}, \\frac{1}{(\\alpha + \\beta^{2})T^{2}}}.\n(2) For any T-round (\u03b1, \u03b2)-LDP algorithm Alg with output \u00ea, as long as \u1e9e < \u221a\\lambda, it holds that\nsup_{\\theta^{*} \\in [-1,1]} E_{\\phi \\sim p} [E_{\\phi \\sim p} (\\phi, \\hat{\\theta} - \\theta^{*})^{2}] \\geq \\min {\\frac{\\lambda}{T}, \\frac{1}{\\sqrt{T}/\\alpha}}.\nNote that in the above construction, E\u03c6\u223cp[\u0444\u0444\u0f0b] = \u03bb. Hence, in linear contextual bandits, the oracle-based\n1\nregret bounds described above will scale with \u00d5(\u221aT+ \u03b1) under joint DP model, and \u00d5(\u221aT/(Amina))\nunder the local DP model, where Amin is the minimum eigenvalue over any policy that the algorithm may\nplay. Further, if there is not a lower bound on \u5165 > 0, then Proposition 3.1 provides the worst-case lower\n1                                                                                                1\nbounds of ( \u221aT+ a) and ( \u221aT/(min)) for L2-regression under the JDP model and LDP model, respectively,\nimplying significant degradation under privacy."}, {"title": "Alternative approach: Regression with confidence intervals", "content": "As an alternative to the standard regression based approach [Foster et al., 2018, Foster and Rakhlin, 2020],\nLi et al. [2024] propose an action elimination framework based on regression with L\u2081-error guarantee and\nthe additional confidence interval structures. The key observation is that, while the negative results (Propo-\nsition 3.1) do rule out the regression oracles with O(1/T) convergence rate under L2-error, such oracles are\nnot necessary for designing algorithm. Specifically, Li et al. [2024] consider the regression subroutine with\nconfidence intervals (L1-regression for short), which is defined as following:\nDefinition 6 (L\u2081-regression oracle) Let N > 1, \u03b4 \u2208 (0, 1). In contextual bandits, a L\u2081-regression oracle\nis a N-round algorithm Alg that outputs an estimate of reward function f : X \u00d7 A \u2192 [-1,1] and an\nconfidence bound b : X \u00d7 A \u2192 R>o, such that for any fixed policy \u03c0 : X \u2192 \u2206(A), given data (xt,at, rt)\ngenerated independently as\nXt ~ P, at ~ \u03c0(xt), E[rt xt, at] = f*(xt, at), t\u2208 [N]\nthe following holds with probability at least 1 \u2013 \u03b4:\n(1) (Valid confidence interval) | f(x, a) - f*(x, a) | \u2264 b(x, a) for all (x, a) \u2208 X \u00d7 A.\n(2) (L1-performance bound) Ex~P,a~\u03c0(x) [b(x, a)] \u2264 Es(N).\nThe above conditions on L\u2081-regression oracle only imply that the L\u2081-error is bounded as Elf(x, a) -\nf*(x, a)| \u2264 E(N). This is arguably weaker than the mean-square (L2) convergence E(f(x, a) \u2013 f*(x, a))\u00b2 <\nE(N)2 by the common L2-regression.\nWith a private L\u2081-regression oracle, Li et al. [2024] adopt an algorithm based on action elimination that\nachieves a regret of \u00d5(T\u00b7 E1/T(T)) (for details, see also Appendix D.2). Therefore, the framework opens the\ndoor for a VT-regret by developing L\u2081-regression oracle with Es(T) = \u00d5(1/\u221aT). However, the L\u2081-regression\noracle of Li et al. [2024] is based on iterative private PCA and layered private linear regression, achieving\nthe rate Es(T) < \u00d5(\\frac{log^{d}(T)}{\u03b1\\sqrt{T}}), and hence leading to a loga(T)\u221aT-regret that is exponential of the dimension\nd. This regret bound is meaningful only when the dimension d = O(1) is of constant order.\nOn the other hand, Chen and Rakhlin [2025] provides a significantly improved regret of \u221ad3T/a. While the\nDecision-Estimation Coefficient (DEC) approach is much different from the aforementioned ones, the way\nthey upper bound the DEC implicitly utilizes the confidence interval bounds with L\u2081 guarantee. Indeed,\nfor linear regression, Chen and Rakhlin [2025] also provide a near-optimal T-1/2-rate under L\u2081-error. For\nachieving such guarantees, they propose a novel normalization method based on a re-weighting matrix U\n(detailed discussion in Section 4). However, this method is introduced purely for upper bounding the DEC,\nand it is unknown whether it provides a more efficient algorithm.\nInspired by the insights from Li et al. [2024] and Chen and Rakhlin [2025], in Section 4, we develop an efficient\nand near-optimal L\u2081-regression procedure that applies to both joint DP and local DP settings. Then, in\nSection 5, we adopt the proposed L\u2081-regression oracle in the action elimination framework of Li et al. [2024]\nto provide rate-optimal private regret bounds."}, {"title": "Private Reweighted Regression", "content": "In this section, we build upon the techniques of Chen and Rakhlin [2025] to provide an optimal estimation\nguarantee under L\u2081-error. For linear regression, Chen and Rakhlin [2025] provides a near-optimal convergence"}, {"title": "Key idea: Reweighting based on normalization matrix", "content": "For generalized linear models with link function v, we can consider the following loss objective,\nL_{GLM}(\\theta) := E_{(\\phi,y)~M^{*}} [l_{v}((\\phi, \\theta), y)],\nwhere the integral loss l, associated with v is defined as l\u2081(t,y) := \u2212yt + \u222bov(s)ds. The basic property\nof Lgum is that \u2207LGLM(0) = E(\u00a2,y)~M+ [(\u03bd((\u00a2, 0)) \u2013 y) \u00b7 \u03c6], and hence \u2207LGLM(0*) = 0, i.e., 0* is a global\nminimizer of LGLM.\nReweighted objective. Given a normalization matrix U and a parameter together satisfying Eq. (2), we\nmay reweigh and regularize the objective function LGLM according to\nL_{weighted} (\\theta) := E_{(\\phi,y)~M^{*}} [\\frac{l_{v}((\\phi, \\theta), y)}{||U\\phi||}]+ \\frac{L_{v}\\lambda}{2}||\\theta||^{2}.\nBy Assumption 1 and Eq. (2), we know that \u03bc\u00b7U-2 < \u22072Lweighted(0) \u2264 2Lv.U-2 for any ||0|| \u2264 1.\nTherefore, the objective Lweighted is well-conditioned after suitable linear transformation 0 = Uw. Specifically,\nwe define\nL_{U} (w) := E_{(\\phi,y)~M^{*}} [\\frac{l_{v}((U\\phi, w), y)}{||U\\phi||}]+\\frac{L_{v}\\lambda}{2}||w||^{2}\nand then Lu is (2L)-smooth and (\u00b5\u2081/2)-strongly convex over the domain Wu := {w \u2208 Rd : ||Uw|| \u2264 1}.\nFurther, the gradient of Lu can be derived as\n\\nabla L_{U} (w) = E_{(\\phi,y)~M^{*}} [\\frac{U \\phi}{\\|U\\phi\\|} \\nabla l_{v}((U\\phi, w)) - y) ]+ \\lambda L_{v} Uw.\nThe following lemma indicates that, any approximate minimizer of Lu is provides a good approximation of\nthe ground truth parameter (under the L\u2081-error).\nLemma 4.1 Suppose that (U, \u03bb) satisfies Eq. (2). We let wt := arg minw:||Uw||\u22641 Lu (w) and w* := U\u221210*.\nThen it holds that ||w - w*|| \u2264 4\u03bb. Further, the following holds:\n(1) Estimation error: for any w \u2208 W, we have\nE_{\\phi~p}|\\nu((\\phi, Uw)) - \\nu((\\phi,\\theta^{*}))| \\leq L_{v}E_{\\phi~p}| (\\phi, Uw - \\theta^{*})| < 2\\sqrt{dL_{v}} ||w - w^{*}|| \n(2) Confidence interval: \u2200\u00a2 \u2208 Rd, |\u00a2,Uw) \u2013 (\u03c6, \u03b8*>| < ||U\u00a2|| \u00b7 ||w \u2013 w*||, and E\u3085~p[||U\u00a2||] \u2264 2d."}, {"title": "Learning normalization matrix privately", "content": "We start by describing how we can learn the normalization matrix U satisfying Eq. (2) from data privately.\nIndeed, even when the covariate distribution p is known, it is not clear how to compute a normalization\nmatrix U, and Chen and Rakhlin [2025] have to invoke a fixed-point argument (Brouwer's theorem) to prove\nits existence. Our key observation is that the following spectral iterates converge to solution to Eq. (1):\n\\Sigma_{(k)} =E_{\\phi~p} [\\frac{U_{(k)}\\phi\\phi^{T}U_{(k)}}{||U_{(k)}\\phi||}]+ \\lambda U_{(k)}, U_{(k+1)} = sym((\\Sigma_{(k)})/2U_{(k)}),\nwith the initial point U(0) = I. Specifically, it holds that\n\\lambda_{min} (\\Sigma_{(k)})^{1/2} \\leq \\lambda_{min} (\\Sigma_{(k+1)}) \\leq \\lambda_{max}(\\Sigma_{(k+1)}) \\leq \\lambda_{max}(\\Sigma_{(k)})^{1/2}.\nTherefore, if the iteration (5) is exact, the matrix \u2211(k) converges to the identity matrix at a quadratic rate,\nimplying that O(log log(1/1)) iterations are enough to achieve sufficient accuracy.\nApproximate spectral iteration. In general, the covariate distribution p is also not known, and we\nhave to approximately implement the update rule (5). To this end, we propose Algorithm 1, which privately\napproximates Eq. (5) by Eq. (6) with batched samples.\nAlgorithm 1 preserves (\u03b1, \u03b2)-JDP by the composition property of joint DP mechanisms, as proved in Ap-\npendix C.2.\nLemma 4.2 Subroutine JDP_Learning_Normalization (Algorithm 1) preserves (\u03b1, \u03b2)-JDP.\nWe demonstrate that the iterates of Algorithm 1 converge to a solution of Eq. (2), as follows. The details\nare deferred to Appendix C.3.\nProposition 4.3 Let T > 1, \u039a \u2265 1, \u03b4 \u2208 (0,1), and \u025bn := C_{0} (\\sqrt{\\frac{\\sigma_{\\alpha,\\beta d+\\log(K/\\delta)}}{\\frac{\\log(K/\\delta)}{N} + \\sqrt{N}) Where Co\nis an absolute constant chosen according to Lemma C.2. Suppose that Algorithm 1 is instantiated with"}, {"title": "Private regression with reweighting", "content": "In the following, we present Algorithm 2 for private L\u2081-regression, which is based on (1) first learning the\nnormalization (U,A) by the subroutine JDP_Learning_Normalization (Algorithm 1), and then (2) running\nthe private batched SGD subroutine (Algorithm 7) on the reweighted objective Lu defined in Eq. (3). The\nbatched SGD subroutine is standard and hence deferred to Appendix C.4.\nTheorem 4.4 (Generalized linear regression with JDP) Let T > 1,\u03b4 \u2208 (0,1), and the subroutines\nof Algorithm 2 are suitably instantiated according to Appendix C.4. Then, under generalized linear model,\nAlgorithm 2 preserves (\u03b1, \u03b2)-JDP, and the following holds:\n(1) With probability at least 1 \u2013 8, the normalization (U, A) satisfies Eq. (2), and the returned estimator \u0175\nsatisfies Lv ||\u0175 \u2013 w || \u2264 \u20ac, where\n\\lambda := \\lambda(T, \\delta) = O( \\sqrt{\\frac{d\\log(1/\\delta)}{T}} + \\sigma_{\\alpha,\\beta} \\frac{\\sqrt{d\\log(1/\\delta)}}{T})\n\\epsilon := \\epsilon(T, \\delta) = O(\\kappa^{3/2} \\frac{\\sqrt{d\\log(1/\\delta)}}{T} + \\sigma_{\\alpha,\\beta}(\\kappa^{3/2} + \\kappa \\sqrt{\\frac{\\log(1/\\delta)}{T}}),\nare defined in Proposition 4.3 and Proposition C.6, respectively, and \u00d5(\u00b7) hides polynomial factors of log(T).\nThe overall estimation error\u0113 is defined as \u0113(T, \u03b4) := 4L,(\u03a4, \u03b4) + \u20ac(\u03a4, \u03b4).\n(2) In particular, under the success event of (1), we have (by Lemma 4.1)\nE_{\\phi~p}|\\nu((\\phi, \\hat{\\theta})) - \\nu((\\phi,\\theta^{*}))| \\leq L_{v}E_{\\phi~p}| (\\phi, \\hat{\\theta} - \\theta^{*})| \\leq 2\\sqrt{d}. \\epsilon,\nand for all \u00a2 \u2208 Rd, the confidence bound holds: |\u03bd((\u03c6, \u03b8)) \u2013 \u03bd((\u03c6, \u03b8*))| \u2264 ||U4|| \u20ac. \nTherefore, Algorithm 2 is an L\u2081-regression oracle in the sense of Definition 6. In particular, for linear models,\nwe have k\u2081 = 1, and hence the convergence rate can further be simplified to \u00d5\\sqrt{\\frac{d}{T}} + \\frac{1}{aT} . The first term\nmatches the minimax-optimal convergence rate of non-private L\u2081-regression (given by the least squares),\nand the second term (\"price-of-privacy\") is of lower order compared to the first term.\nExtension: Local DP. As an LDP extension of Algorithm 2, we propose Algorithm 8 that preserves local\nDP while achieving near-optimal L\u2081-regression guarantee.\nTheorem 4.5 (Generalized linear regression with LDP) Let T > 1,\u03b4\u2208 (0,1). Algorithm 8 (de-\nscribed in Appendix C.5) preserves (\u03b1, \u03b2)-LDP, and it returns estimatore such that with probability at"}, {"title": "Private Learning in Generalized Linear Contextual Bandits", "content": "In this section, we use our L\u2081-regression method from Section 4 as a subroutine for the action elimination\nframework [Li et al., 2024], providing rate-optimal private regret bounds for generalized linear contextual\nb bandits.\nOur algorithm (Algorithm 3) is epoch-based (with a given epoch schedule 1 = To < T\u2081 < T2 < \uff65\uff65\uff65 < Tj = T),\nand it iteratively builds estimations of the ground truth reward function f* and plans according to the\nestimations. The algorithm, which consists of an estimation procedure and a planning procedure described\nas follows, is similar to that of Li et al. [2024].\nEstimation procedure. For jth epoch, the estimate reward function f(i) and the confidence radius\n\u0125(i) are produced by the subroutine JDP_L1_Regression. Then, by Theorem 4.4, it holds that with high\nprobability\nf*(x,a) \u2208 [f(i) (x, a) \u2013 (i) (x, a), f(i) (x, a) + (i) (x, a)], \u2200x \u2208 X, a \u2208 A.\nPlanning procedure. The policy (i) of the jth epoch is built upon the confidence intervals (7) given\nby the estimations (f(0), (0)),\u2026\u2026\u2026, (f(i-1), (i-1)) from previous epochs. Given the estimations, subroutine\nConfidence_based_Planning (Algorithm 4) eliminates the sub-optimal arms for each context x \u2208 X according\nto Eq. (8), and output the \u03c0(1)(x) based on a spanner of the remaining actions. This procedure implicitly\nencourages exploration, as it uses optimistic estimation (UCB) of the value of each arm."}, {"title": "Dimension-free Linear Regression", "content": "In Section 4 and Section 5, we provided a somewhat satisfactory picture of the optimal rates of private\nlearning in contextual bandits with generalized linear models, when the dimension d is bounded. In this\nsection, we turn our focus to the setting where the dimension d is prohibitively large or unbounded, e.g.,\nwhen the linear function parametrization is in fact given by a Reproducing Kernel Hilbert Space (RKHS).\nThis setting is fundamentally more challenging, as the following lower bounds indicates. The proof can be\nfound in [Chen and Rakhlin, 2025, Appendix C].\nProposition 6.1 Let d > 1, covariate space C = Sd\u22121 be the d-dimensional unit sphere. For each \u03b8 \u2208 Sd\u22121,\nwe consider the linear model Me:\n(\u03c6, \u0443) ~ \u041c\u0473 :\n\u03c6 = 0, y = 1.\nFor any parameter R\u2208 [1, c\u221ad] (c > 0 is a small absolute constant), the following holds:\n(a) Suppose that Alg is a T-round (a,0)-JDP algorithm with output ||0|| < R. Then it holds that\nsup_{\\theta^{*} \\in S^{d-1}} E_{M^{*}, Alg}|<\\theta, \\theta^{*}> - 1| \\geq 1,\nunless T \\geq \\frac{d}{R^{2}}\\alpha .\n(b) Suppose that Alg is a T-round (a, \u03b2)-LDP algorithm with output ||6|| < R. Then it holds that\nsup_{\\theta^{*} \\in S^{d-1}} E_{M^{*}, Alg}|<\\theta^{*}, \\hat{\\theta}> - 1| \\geq 1,\nunless T \\geq \\min {\\frac{d}{R^{2}}, \\frac{1}{R^{2}}\\frac{\\alpha^{2}}{\\beta}}.\nNote that for each linear model Pe, the covariance matrix Ep.\u0444\u0444 = 00\u2122 is of rank 1. Therefore, Propo-\nsition 6.1 has two implications for pure JDP (and also LDP) regression with unbounded dimension d and\nunknown covariate distribution:\n(1) Estimating the covariance matrix requires \u03a9(d) samples, even when the covariance matrix is known to\nhave rank 1.\n(2) Proper estimator of the parameter 0* also requires \u03a9(d) samples to achieve a non-trivial error. Con-\nversely, any non-trivial estimator \u00ea must have norm ||0|| \u2265 \u03a9(\u221ad/T) (with non-trivial probability).\nTherefore, with unbounded dimension d, to achieved estimation guarantees, the estimator in consideration\nhas to be highly improper, and it also cannot rely on estimating the covariance matrix.\nBased on the observations above, in this section, we develop improper private procedures with dimension-free\nbounds in private linear regression. We then apply the proposed methods to provide dimension-free regret\nbounds in private linear contextual bandits."}, {"title": "Private improper batched SGD", "content": "We begin with the joint DP setting. For any non-private estimator 6 with sensitivity s, it is well-known\nthat the estimator \u03b8' = 0 + \u03da ensures (\u03b1, \u03b2)-JDP with noise \u03da ~ N(0, \u03b5\u00b2I) and parameter \u03b5:= s\u00b7\u03c3\u03b1,\u03b2. The\nkey idea is that, while ||5|| = \u025b\u221ad, we have ||5||\u2211 \u03a3\u03b5 with high probability, where \u2211 := E\u03c6~p\u0444\u0444\u00ae_is the\ncovariance matrix. Therefore, to ensure JDP, it is sufficient to privatize a non-private estimator with low\nsensitivity.\nBased on these observations, we consider the projected batched SGD on the standard square-loss:\nL_{sq} (\\theta) = E_{(\\phi,y)~M^{*}} [((>,0) - y)^{2}].\nBy directly privatizing its last iterate (Algorithm 5), we can achieve a near-optimal convergence rate (detailed\nin Appendix E.1).\nTheorem 6.2 (Dimension-free JDP regression) Let T > 1,\u03b4 \u2208 (0,1). Algorithm 5 preserves (a, \u03b2)-\nJDP, and with a suitably chosen parameter K, it ensures that\nE_{\\phi~p}(\\phi, \\hat{\\theta} - \\theta^{*})^{2} = ||\\hat{\\theta} - \\theta^{*}|| \\leq  \\frac{\\sqrt{\\log T\\log(1/\\delta)}}{\\sqrt{T}} + \\frac{\\sigma_{\\alpha,\\beta}\\sqrt{d\\log(1/\\delta)}}{T} + (\\frac{1}{\\alpha T})^{2/3}.\nTherefore, the estimator \u00ea achieves the dimension-independent convergence rate of 1/ \u221aT +\nIn non-\nprivate linear models, the T-1/2-rate of convergence is known to be minimax-optimal and can be achieved\nby vanilla gradient descent.\nExtension: Local DP. The situation under the local DP model is much more subtle. Indeed, one may\nexpect that Algorithm 5 naturally extends to this setting. However, to ensure local privacy in (batched)\nSGD, for every step, the gradient estimator has to be privatized. To this end, it is typically necessary to\nadd a noise vector \u03da that has norm scaling with \u03a9(\u221ad) (e.g., when \u0121 is the Gaussian noise). In other words,\nthe privatized gradient estimator has norm scaling with \u221ad. Hence, after a single step of gradient descent,\nthe iterate falls outside the unit ball Bd(1), and projection back to Bd(1) can lead to large bias. Therefore,\nthe method of projected gradient descent may not be applied here.\nInstead, we consider performing privatized batch SGD directly, replacing the projection operation with a\ncareful clipping on the gradient estimator (Algorithm 6). This is based on extending the aforementioned"}, {"title": "Application: Linear contextual bandits with dimension-free regret", "content": "As an application, we use the dimension-free procedures developed in Section 6.1 as subroutines for learning\nlinear contextual bandits. We invoke the SquareCB algorithm [Abe and Long, 1999, Foster and Rakhlin, 2020,\nSimchi-Levi and Xu, 2020], which has a regret guarantee given any offline regression oracle with L2-error\nbound. In particular, by instantiating the regression oracle as JDP_Improper_BatchSGD (Algorithm 5) or\nLDP_Improper_BatchSGD (Algorithm 6), we obtain the following private regret bounds. The details are\npresented in the Appendix E.3."}, {"title": "Conclusion", "content": "In this work, we propose a novel method of private re-weighted regression for private (generalized) linear\nregression, achieving near-optimal convergence rates under L\u2081-error. Based on this method, we provide\nefficient algorithms for (generalized) linear contextual bandits with near-optimal regret bounds in both the\njoint and local model of differential privacy. Furthermore, we also develop the improper private procedures\nwith near-optimal, dimension-independent rates in linear models and linear contextual bandits."}]}