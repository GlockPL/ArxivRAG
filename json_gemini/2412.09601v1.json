{"title": "TIMEREFINE: Temporal Grounding with Time Refining Video LLM", "authors": ["Xizi Wang", "Feng Cheng", "Ziyang Wang", "Huiyu Wang", "Md Mohaiminul Islam", "Lorenzo Torresani", "Mohit Bansal", "Gedas Bertasius", "David Crandall"], "abstract": "Video temporal grounding aims to localize relevant temporal boundaries in a video given a textual prompt. Recent work has focused on enabling Video LLMs to perform video temporal grounding via next-token prediction of temporal timestamps. However, accurately localizing timestamps in videos remains challenging for Video LLMs when relying solely on temporal token prediction. Our proposed TIMEREFINE addresses this challenge in two ways. First, instead of directly predicting the start and end timestamps, we reformulate the temporal grounding task as a temporal refining task: the model first makes rough predictions and then refines them by predicting offsets to the target segment. This refining process is repeated multiple times, through which the model progressively self-improves its temporal localization accuracy. Second, to enhance the model's temporal perception capabilities, we incorporate an auxiliary prediction head that penalizes the model more if a predicted segment deviates further from the ground truth, thus encouraging the model to make closer and more accurate predictions. Our plug-and-play method can be integrated into most LLM-based temporal grounding approaches. The experimental results demonstrate that TIMEREFINE achieves 3.6% and 5.0% mIoU improvements on the ActivityNet and Charades-STA datasets, respectively. Code and pretrained models will be released.", "sections": [{"title": "1 Introduction", "content": "Video Temporal Grounding (VTG) Yuan et al. [2019a], Zhang et al. [2019], Yuan et al. [2019b], Zhang et al. [2020a], Ghosh et al. [2019], Cheng and Bertasius [2022], Lin et al. [2023c], Yu et al. [2019], Zhou et al. [2018], Lei et al. [2021a], Zhang et al. [2020b], Zheng et al. [2025], Chen et al. [2020, 2018], Liu et al. [2022] is a foundational task in video understanding that aims to localize relevant temporal boundaries in a video given a textual prompt (e.g., \"during which time does the person stir the soup?\"). VTG has numerous real-world applications, including anomaly detection Yao et al. [2020], sports analytics, security and surveillance, consumer video retrieval, and education."}, {"title": "2 Related Work", "content": "Video Large Language Models. With the advancement of image-based Large Language Models (LLMs) [Li et al., 2023a, Liu et al., 2024a], recent research [Zhang et al., 2023b, Wang et al., 2024c, Li et al., 2023b, Lin et al., 2023a, Zhang et al., 2023a, Wang et al., 2024e] has focused on enhancing the video understanding capabilities of LLMs. Existing models have achieved significant success in various video understanding tasks, such as video question-answering and video captioning. However, they often face challenges with more detailed temporal understanding [Liu et al., 2024b]. Most existing methods focuses on building high-quality instructional tuning datasets or making architectural changes, instead, TIMEREFINE focuses on modifying the learning objectives to better accommodate the temporal grounding task within the Video LLM framework.\nVideo Temporal Grounding. Video Temporal Grounding (VTG) is crucial to video understanding, as it aims to accurately identify event timestamps within a given video [Lin et al., 2023b]. Traditional task-specific models have been developed to address this task [Lei et al., 2021b, Moon et al., 2023a,b, Zeng et al., 2024b, Zala et al., 2023], typically framing it as a timestamp regression task based on video inputs and user queries. Despite their early success, these models struggle in zero-shot settings, are limited to handling a single task per model, and often require additional fine-tuning for various downstream tasks and datasets. Recent methods Huang et al. [2024a], Wang et al. [2024d], Li et al. [2024c], Huang et al. [2024b], Zeng et al. [2024a], Guo et al. [2024b], Wang et al. [2024a], Qu et al. [2024], Qian et al. [2024] leverage the capabilities of powerful Video Large Language Models to perform LLM-based temporal grounding. For instance, VTimeLLM [Huang et al., 2024a] directly fine-tunes Video LLMs on VTG datasets. LITA [Huang et al., 2024b] incorporates SlowFast visual tokens and time tokens into LLM tokenizers to enhance temporal localization capabilities. Momentor [Qian et al., 2024] addresses time token quantization errors by implementing a time encoder, thus improving fine-grained temporal reasoning. VTG-LLM [Guo et al., 2024a] integrates specialized time tokens and temporal position embeddings to enhance video LLMs' understanding of timestamps. However, these approaches mainly emphasize the development of mechanisms for encoding visual or temporal features and data collection, often neglecting the fundamental challenges of temporal grounding for Video LLMs. In contrast, TIMEREFINE employs a step-by-step refinement schema to enhance the temporal understanding ability, providing a more effective training objective for Video LLMs.\nRefinement-based Learning. The concept of refinement \u2013 initially making a rough prediction and subsequently refining it for greater accuracy \u2013 has been extensively explored across various computer vision domains. For instance, two-stage object detectors Ren et al. [2016], He et al. [2017], Girshick et al. [2014] utilize a region proposal network to generate candidate bounding boxes, which are then refined by another network to produce final predictions. Similarly, Iterative Error Feedback is commonly applied in tasks that require precise, step-by-step refinement, such as human pose estimation Carreira et al. [2016]. Diffusion models Ho et al. [2020], Dhariwal and Nichol [2021], Cheng et al. [2025] also adopt a step-by-step denoising approach. Inspired by the previous works, TIMEREFINE proposes a refinement paradigm for the temporal grounding task, which iteratively refines its predictions through next-token prediction. To our knowledge, TIMEREFINE is the first work to explore the refinement paradigm for LLM-based temporal localization."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Problem Formulation", "content": "Given a video $V \\in \\mathbb{R}^{T \\times H \\times W \\times C}$ and a textual query $Q$ (e.g., \u201cWhen does the man have breakfast in the video?\"), the model is required to generate a corresponding answer $A$ with localized timestamps (e.g., \"20s to 90s\").\nExisting approaches Huang et al. [2024a], Guo et al. [2024a], Qian et al. [2024], Guo et al. [2024b], Ren et al. [2024] incorporate video temporal grounding tasks within the Video LLM framework using a next-token prediction scheme, the log-likelihood of the predicted answer conditioned on the textual query and video input could be formulated as:\n$1 = \\sum_{i=1}^{m}logp(a_i | H_v, H_q, a_{j<i})$\nwhere $H_v = \\phi(V)$ represents the video tokens encoded by a pre-trained video encoder $\\phi$ (e.g., CLIP Radford et al. [2021]), $H_q = \\{q_1, q_2,..., q_n\\}$ are the tokenized elements of the query, and $H_a = \\{a_1, a_2,..., a_m\\}$ are the tokenized elements of the answer. The encoded tokens of timestamps (e.g., \"20s\") are denoted as temporal tokens, which can be tokenized as standard text tokens (e.g., VTimeLLM Huang et al. [2024a]) or some special tokens (e.g., VTG-LLM Guo et al. [2024a] uses <TIME-TWO>).\nWe identify two key issues in this standard Video LLM-based temporal localization framework:\n\u2022 Directly predicting temporal timestamps in a long video is challenging. Additionally, treating the timestamp prediction task as a token prediction task within the LLM framework results in sparse supervision, leading to inferior temporal localization accuracy. The model receives supervision signals from only two temporal tokens (e.g., a start and end token for each segment) while processing hundreds of visual and textual tokens. In comparison, traditional temporal grounding models (e.g., ActionFormer Zhang et al. [2022], TallFormer Cheng"}, {"title": "3.2 TIMEREFINE", "content": "To address abovementioned issues, we introduce TIMEREFINE, which proposes a step-by-step refinement paradigm to the LLM-based temporal grounding task. This iterative refinement strategy helps the model learn to self-correct its errors in boundary perception, thus enhancing the Video LLM's temporal understanding ability.\nFirst, we reformulate the temporal token prediction task into an iterative time refinement task. Given a video and a user prompt, the model is required to make multiple rounds of predictions. In each round, it makes an initial rough prediction and then learns to self-correct the errors of this rough prediction. This scheme not only provides a stronger supervision signal but also transforms the problem from directly predicting precise timestamps to making coarse-to-fine predictions, akin to how humans localize moments in videos. Second, we introduce an auxiliary time prediction head to enhance the Video LLM's temporal perception capability. This auxiliary head is optimized using L1 loss, which helps the model learn that closer predictions are preferable in temporal grounding tasks."}, {"title": "3.2.1 Architecture", "content": "Our method is agnostic to the exact model architecture and can be applied to any LLM-based VTG method that follows the formulation described above. For clarity, we visualize a common architecture of Video LLMs in Fig. 2. This architecture consists of a visual encoder, a visual adapter, a text tokenizer, and a large language model (LLM). The visual encoder is typically a CLIP-like model Radford et al. [2021] used to extract visual features. The visual adapter projects these visual features into the language space using MLPs Rumelhart et al. [1986] or Q-Formers Li et al. [2023a]. The LLM takes the visual embeddings of the video and the embedded text tokens of user prompts as input to output the answers. We retain the core architecture unchanged but modify the learning objectives to include 1) iterative refinement of temporal segments and 2) an auxiliary segment prediction head, which we discuss below."}, {"title": "3.2.2 Iterative Time Refinement", "content": "We modify the direct timestamp prediction scheme in two ways. First, instead of predicting the timestamp directly, which is challenging for Video LLMs, we employ a coarse-to-fine approach. The model first makes a rough prediction and then predicts the offsets to the target segment, encouraging the model to learn to self-correct its errors. Second, making accurate predictions in a single step is difficult, so we implement the coarse-to-fine prediction scheme iteratively. This allows the model multiple opportunities to correct its errors, thereby enhancing its temporal localization accuracy.\nSpecifically, for the start and end timestamp in a segment $S = (s, e)$ that the model needs to predict, we convert it into an iterative time refinement prediction sequence:\n$S' = ((s_0, e_0, o^s_0, o^e_0), ..., (s_k, e_k, o^s_k, o^e_k))$\nwhere $(s_0, e_0, o^s_0, o^e_0)$ represents a single coarse-to-fine prediction step, $s = s_k + o^s_k, e = e_k + o^e_k$, and $o^s_k$ and $o^e_k$ are offsets to the target segment. $K$ denotes the number of refinement steps. During training, we format $S'$ into a sequence with additional control tokens:"}, {"title": "3.2.3 Temporal Perception", "content": "The Cross-Entropy (CE) loss Goodfellow [2016], commonly used in LLM-based video temporal grounding methods, is less suitable for temporal timestamp predictions, which are continuous variables. For instance, for a ground truth timestamp at 20s, CE loss assigns the same penalties for the following two predictions: (i) p(t = 20s) = 0.1,p(t = 21s) = 0.9 and (ii) p(t = 20s) = 0.1,p(t = 100s) = 0.9. This approach is counterintuitive, as the latter prediction is significantly more inaccurate.\nTo address this issue, we propose adding an auxiliary prediction head attached to the  token to predict the segment start and end:\n$\\hat{S} = Linear(h[< refine >])$\nwhere $h[< refine >]$ is the embedding of the  token from the model's last layer. The output $\\hat{S}$ is supervised by the L1 loss between predictions and ground truth: $L_1 = |\\hat{S} \u2013 S|$. Unlike Cross-Entropy loss, L1 loss penalizes predictions more heavily when they are further from ground truth, thus enhancing the model's temporal perception ability. To maintain the integrity of the Video LLM, we retain the original Cross-Entropy loss and add this L1 loss as an auxiliary supervision signal. We experimented with other loss functions, such as GIoU Rezatofighi et al. [2019] loss, but did not observe improvements. See Sec. 4.4.3 for more design choices."}, {"title": "3.3 Training and Inference", "content": "Training. The total loss is the sum of the original Cross-Entropy loss and the L1 loss of the auxiliary linear branch, as defined in Sec. 3.2.3.\n$L = \\frac{1}{m}\\sum_{i=1}^{m}cross\\_entropy(a_i, \\hat{a_i}) + \\lambda \\frac{1}{|S|}|S - \\hat{S}|$\nwhere $\\hat{a_i}$ and $a_i$ are the predicted and ground-truth answer tokens (including temporal tokens), $|S|$ is the number of segments in the answer, and $\\lambda$ is a hyperparameter to balance these two losses.\nInference. Our answer generation process follows the next-token prediction scheme in Video LLMs. The auxiliary prediction head (Sec. 3.2.3) is discarded during inferenceFor the prediction of the temporal segment $S$, we use the predictions from the final refinement step, $s_k + o^s_k$ and $e_k + o^e_k$, as the predicted start and end timestamps."}, {"title": "3.4 Implementation Details", "content": "The proposed TIMEREFINE is architecture-agnostic and can be applied to most LLM-based VTG methods. We first introduce the hyperparameters of our TIMEREFINE, and then explain how we apply TIMEREFINE to other VTG methods (i.e., VTimeLLM Huang et al. [2024a] and VTG-LLM Guo et al. [2024a])."}, {"title": "3.4.1 \u03a4\u0399\u039cMEREFINE", "content": "Iterative Time Refinement Scheme. We reformat the training data by replacing the direct start and end timestamps in grounding-related QA pairs with time-refinement sequences, while leaving non-grounding QA pairs unchanged. To construct the iterative time refinement sequence, we choose the number of refinement steps of each time segment as $K = 4$. The $K$ Gaussian distributions have fixed standard deviations sorted in decreasing order: $\\sigma^2 = \\{5,3,1,0\\}$ (in seconds). Such a design will progressively add less noise in the refinement process to approximate the ground truth, which can simulate a coarse-to-fine refinement process.\nTemporal Perception. In addition to the Cross-Entropy loss, we add the L1 loss with a weight of $\\lambda = 10$ to balance the losses. We show ablations of loss choices in Sec. 4.4.3."}, {"title": "3.4.2 Integrating with other VTG methods", "content": "VTimeLLM. To enable direct comparison with VTimeLLM Huang et al. [2024a], we use the same training settings and data as VTimeLLM. We initialize TIMEREFINE from VTimeLLM's stage one checkpoint so that visual features are already aligned with LLM's semantic space. We then train our model on the stage two and stage three training data for 4000 steps and 1000 steps, respectively, with a learning rate of 1 \u00d7 10\u22124 on a single 8-A5000 (24G) machine. The LoRA rank is set to 64 and alpha to 128.\nVTG-LLM. We also apply TIMEREFINE on VTG-LLM, and train the model on the same training data of VTG-LLM, i.e., VTG-IT-120k Guo et al. [2024a] and a randomly sampled subset (97k) from the Valley dataset Luo et al. [2023]. All the hyperparameters (batch size, learning rate, optimizers, etc.) are kept the same as the original VTG-LLM."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Evaluation Setups", "content": "Evaluation Datasets. Following prior work Huang et al. [2024a], Guo et al. [2024a], we employ ActivityNet Captions Krishna et al. [2017] and Charades-STA dataset Gao et al. [2017] as evaluation datasets. We measure the Intersection over Union (IoU) Everingham et al. [2010] between the predicted time segments and the ground truth time segments. We report the Recall@1 at IoU"}, {"title": "4.2 Main Results", "content": "In Table 1, we apply the proposed TIMEREFINE on VTimeLLM Huang et al. [2024a] and evaluate the performance on ActivityNet Captions Krishna et al. [2017] and Charades-STA Gao et al. [2017]. With the same training settings and data, it surpasses VTimeLLM-7B by 3.2% in R@0.7 and 3.6% in mIoU on the ActivityNet Captions dataset. Though not our purpose, we can also observe a small performance increase on the dense video captioning task, showing that the proposed method not only strengthens temporal grounding ability but also benefits other grounding-related tasks.\nWe further compare the results with existing Video LLMs. It is important to note that different Video LLMs are trained on varying datasets, which complicates fair comparisons. Nonetheless, TIMEREFINE achieves the best performance on most metrics among all 7B Video LLMs on both the ActivityNet Captions and Charades-STA datasets.\nTo demonstrate the plug-and-play adaptability of the proposed TIMEREFINE with various Video LLMs, we further evaluate its application on VTG-LLM Guo et al. [2024a] with results in Table 1. For zero-shot temporal grounding on Charades-STA, TIMEREFINE outperforms VTG-LLM by 4.6% on R@0.3, 1.3% on R@0.5 and 1.0% on R@0.7. While VTG-LLM improves temporal grounding accuracy by integrating knowledge about timestamps, TIMEREFINE further boosts this capability through iterative time refinement strategy and auxiliary temporal perception supervision."}, {"title": "4.3 Qualitative Evaluation", "content": "In Fig. 3, we compare the output of VTimeLLM Huang et al. [2024a], VTG-LLM Guo et al. [2024a] and TIMEREFINE on the Charades-STA dataset in a zero-shot setting. VTimeLLM and VTG-LLM directly predict one segment, resulting in IoU of 0.46 and 0.65. In contrast, VTimeLLM-TIMEREFINE estimates a rough boundary of the segment at first with IoU = 0.46. Then it progressively refines this estimation over 3 additional refinement steps, providing a new segment and offsets at each refinement step. The final prediction of TIMEREFINE achieves the highest IoU of 0.95, showing the effectiveness of iterative time refinement strategy and auxiliary supervision signal."}, {"title": "4.4 Design Choices", "content": "In this section, we examine the design choices of TIMEREFINE and provide detailed ablations about the iterative time refinement strategy and the auxiliary prediction head for temporal perception. We begin by analyzing the refinement task design, specifically, the refinement goal. Next, we explore the multi-step refinement sequence generation strategy, such as the number of refinement steps and the appropriate amount of noise to add. Finally, we investigate the use of auxiliary prediction branches to enhance the model's temporal perception capabilities and the method for decoding target timestamps from our predictions. Unless otherwise specified, all results are based on the evaluation of TIMEREFINE on VTimeLLM-7B Huang et al. [2024a] for the temporal grounding task of ActivityNet Captions."}, {"title": "4.4.1 Time Refinement Task Design", "content": "The first question is how to design a refinement task that helps the Video LLM better learn temporal grounding. In Table 2, we explore three options:"}, {"title": "4.4.2 Refinement Sequence Generation Strategy", "content": "In this section, we investigate how to generate the refinement sequence via $K$ Gaussian distributions. We show the results in Table 3.\nHow many refinement steps $K$ does the model need? In the iterative time refinement process, we investigate how varying the number of refinement steps $K$, from 2 to 8, affects the model's temporal prediction performance. We found out that setting the number to 4 brings the most improvement, with performance gains plateauing beyond this point. This shows that predicting and refining multiple temporal segments is beneficial to temporal grounding task.\nHow much noise does the model need? For the $K = 4$ Gaussian distributions with fixed standard deviations, we tune the amount of noise to be added. Starting from standard deviations of {5, 3, 1,0}, we adjust the noise level by factors of 0.4, 1, 2 and 4 to try different scale of noise. Experiment results demonstrates that adding smaller amount of noise is more effective. Whereas larger scale of noise can disrupt the model's learning process.\nDoes the noise need to be adaptive to the video duration? We validate this by setting the standard deviations to be adaptive to the length of the video, adding stronger noise for longer videos. Instead of using fixed standard deviations, we adopt adaptive standard deviations of {0.2, 0.1, 0.05, 0} and {0.1, 0.05, 0.01, 0}, where each value represents a fraction of the video duration (e.g., 0.2 corresponds to 0.2x video duration). In this way, we add larger noise to longer videos. The bottom part of Table 3 shows that using adaptive standard deviations results in a 0.6% lower mIoU compared to using fixed standard deviations. This outcome is reasonable, as the temporal dynamics within a video are independent of its length, suggesting that a consistent noise distribution should be applied to both short and long videos.\nFrom now on, we use $K = 4$ refinement steps with std. {5,3,1,0}."}, {"title": "4.4.3 Temporal Perception", "content": "Does an auxiliary prediction branch help temporal perception? In Table 4, adding an auxiliary branch optimized with L1 loss leads to a 1.6% higher mIoU comparing to no auxiliary loss. This proves that an auxiliary supervision signal can complement the original Cross-Entropy loss and further enhance the model's temporal perception capability.\nWhich loss can better help temporal prediction tasks? We employ L1 loss, L2 loss and GIOU loss as the auxiliary supervision signal in addition to the Cross-Entropy loss, since these losses are commonly used in detection and grounding tasks Ren et al. [2016], Redmon [2018], Bochkovskiy et al. [2020], Rezatofighi et al. [2019], Tan et al. [2020], Zhu et al. [2020]. In Table 4, experiments show that L1 loss is the most effective loss, while further adding GIoU loss doesn't bring much improvement."}, {"title": "4.4.4 Segment Decoding", "content": "Each refinement process involves multiple steps, allowing us to select the segment predicted at either the initial or final step. Alternatively, we can use the segment predicted by the auxiliary head or combine it with the sequence-based prediction. Table 5 presents the results for these decoding strategies. We can observe that 1) evaluating using segments predicted at the final step significantly outperforms using the segment from the initial step. This is consistent with our sequential refinement design, where the final segment is intended to match the target; and 2) utilizing predictions from either the auxiliary prediction head, last step of sequence prediction or combining the two gives similar performance. This means that the auxiliary head can be discarded after training, making our method compatible with existing token generation frameworks."}, {"title": "5 Conclusion", "content": "In this work, we propose TIMEREFINE to enhance the capability of Video LLMs in performing temporal grounding. Unlike previous approaches that focus on data curation or architectural enhancements, we focus on refining the learning objective to better suit temporal grounding within the Video LLM framework. We transform the timestamp prediction task into an iterative error refinement task. Additionally, we complement the next-token prediction with an auxiliary L1 prediction head, encouraging the model to make closer and more accurate predictions. Our method is architecture-agnostic, allowing us to apply it to two recent state-of-the-art LLM-based VTG methods, VTimeLLM and VTG-LLM, demonstrating promising performance improvements with minimal training cost overhead. One limitation of our method is the need to predict several times more temporal-related tokens compared to prior methods. However, this drawback is mitigated when users are interested in both temporal grounding and question-answering tasks, where the model generates significantly more textual tokens compared to temporal tokens. Future research directions include further refinement of task and sequence designs and exploring how temporal grounding can enhance video question-answering tasks."}]}