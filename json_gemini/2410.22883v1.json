{"title": "Adaptive Paradigm Synergy: Can a Cross-Paradigm Objective Enhance Long-Tailed Learning?", "authors": ["Haowen Xiao", "Guanghui Liu", "Xinyi Gao", "Yang Li", "Fengmao Lv", "Jielei Chu"], "abstract": "Self-supervised learning (SSL) has achieved impressive results across several computer vision tasks, even rivaling supervised methods. However, its performance degrades on real-world datasets with long-tailed distributions due to difficulties in capturing inherent class imbalances. Although supervised long-tailed learning offers significant insights, the absence of labels in SSL prevents direct transfer of these strategies. To bridge this gap, we introduce Adaptive Paradigm Synergy (APS), a cross-paradigm objective that seeks to unify the strengths of both paradigms. Our approach reexamines contrastive learning from a spatial structure perspective, dynamically adjusting the uniformity of latent space structure through adaptive temperature tuning. Furthermore, we draw on a re-weighting strategy from supervised learning to compensate for the shortcomings of temperature adjustment in explicit quantity perception. Extensive experiments on commonly used long-tailed datasets demonstrate that APS improves performance effectively and efficiently. Our findings reveal the potential for deeper integration between supervised and self-supervised learning, paving the way for robust models that handle real-world class imbalance.", "sections": [{"title": "1. Introduction", "content": "Self-supervised learning (SSL) [18, 19] that aims to learn a generic robust feature extractor from unlabeled data has been consistently explored in recent year, achieving results comparable to those of supervised pre-training [4, 8, 47]. Contrastive learning, a leading SSL paradigm, seeks to learn discriminative representations by encouraging the similarity of augmented view pairs, and has achieved remarkable success in computer vision (CV), natural language processing (NLP) and graph learning (GL) [16, 48]. Nevertheless, most contrastive learning methods are con-"}, {"title": "2. Related Work", "content": "Supervised Long-tailed Recognition. Classical supervised long-tail recognition can be categorized in three directions: category-level re-balancing, information augmentation and module improvement [50]. Related to our work, exploration of category-level re-balancing mainly focuses on re-sampling, re-weighting and logit adjustment [25, 26, 44, 55]. Decoupling [25] explores impact of different sampling strategies, and finds that square-root sampling [33] and progressively-balanced sampling are better suited long-tailed datasets. LOCE [14] proposes a memory-agumented feature sampling to dynamically monitor the prediction score of each classes to drive re-sampling rate distribution. Oriented towards supervised contrastive learning paradigm, BCL [55] develops a novel loss for balanced training, which considers the averaging of negative class gradients and the complement of mini-batch tail classes. Logit adjustment [34] utilises training label frequencies to adjust prediction logit. VS loss [26] conducts a comprehensive analysis on impact of additive and multiplicative logit-adjusted losses, and further formulates a novel strategy that combines the best of both worlds.\nExplorations on information augmentation seek to introduce additional information into training process, mainly containing transfer learning and data augmentation [15, 17, 29, 35, 46, 51]. SSP [46] systematically analyzes the benefits of combining semi-supervised and self-supervised manners to standard learning for balanced feature space learning. SSD [29] develops a soft labels generator driven by self-supervised distillation, producing a robust auxiliary information to transfer knowledge from long-tailed distribution. DeiT-LT [35] aims to tackle the problems of high efficiency training from scratch, introducing an effective distillation construct from a flat CNN teacher via CLS token and distillation DIST token to support a head expert and a tail expert, respectively. Remix [9] benefits from re-balanced data mixup to boost long-tail learning. SBCL [21] employs subclass-balancing adaptive clustering and bi-granularity contrastive loss to facilitate the implicit mining of sub-classes and optimise the balance of sub-classes.\nModule improvement can be divided into classifier design and ensemble learning [11, 23, 45, 49, 51]. Paco [11] introduces a set of parametric class-wise learnable centers to adaptively enhance the optimisation of latent space. RIDE [45] leverages strategy of multiple experts driven by KL-divergence based loss to benefit hard example learning. SADE [49] explores a multi-expert scheme to handle test-agnostic long-tailed recognition, innovating diversity-promoting expertise-guided losses to adaptively aggregate experts for handling unknown distribution.\nSelf-supervised Long-tailed Recognition expects to automatically correct potential category bias without ground-truth labels and data distributions. There exists several"}, {"title": "3. Method", "content": "3.1. Preliminary\nFor a given dataset D, every instance is denoted by (x, y) \u2208 D,where y is corresponding class label of input x. Contrastive learning expects to close similarity of positive pairs (xi, xi) produced by a random augmentation and repel similarity of negative pairs (xi, xj). The contrastive loss for instance i is defined by:\n$l_i = -log \\frac{S(v_i, v_{i'}, \\tau)}{S(v_i, v_{i'}, \\tau) + \\sum_{v_j \\in B_i}S(v_i, v_j, \\tau)}$\n(1)\n$S(v_i, v_j, \\tau) = exp(sim(v_i, v_j)/\\tau)$\n(2)\nWhere sim(\u00b7) denotes the cosine similarity, B\u2081 denotes a set of negative samples of xi and vi = g(f(xi)) is embedding of xi. f(.) and g(\u00b7) are encoder and projection head network, respectively.\n3.2. Temperature Properties for Latent Structure\nLatent spatial structure has a direct impact on capability of transferring pre-trained model to other downstream tasks. In recent years, research has been conducted to examine the structure of the latent space for contrastive learning in greater detail [37,43]. Several evidence from both quantitative and qualitative experiments has demonstrated that temperature parameters exert a profound influence on the uniformity and tolerance of the spatial structure. Previous work [43] calls these two opposing properties as uniformity-tolerance dilemma. To be specific, for Eq. (1) and Eq. (2), given a small T, the penalty will be dominated by hard negative samples, and therefore more attention will be paid to push the more similar samples farther away, i.e., the tolerance of this feature point is small. Accordingly, when the tolerance of all the points is small, the latent space"}, {"title": "3.3. Structure Separability and Category Uniformity", "content": "In this section, we give a comprehensive analysis about the expectation of the semantic spacial structure and how dynamic temperature strategy could help reach this ideal structure.\nIntuitively, we desire a separable semantic feature to facilitate migration to more downstream tasks [43]. Nevertheless, when we train on a long-tail dataset, due to the potential imbalance of the data, we need a more reasonable structure as target than balanced learning. We describe it as structure separability and category uniformity. As mentioned above, the temperature parameter can control different tolerances for each sample, i.e., in long-tailed learning, we can give different samples different temperatures to form category-level semantic clusters, thereby obtaining a better separable structure. Moreover, due to the inherent imbalance, we also noticed that category uniformity can also help sample to combat tail class neglect and learn robust representation [53]. Specifically, forcing categories with more samples to occupy a similar latent space as categories with fewer samples can effectively combat category representation disparity.\nFrom the perspective of temperature setting, a smaller temperature can make the sample pay more attention to strong negative examples and pursue better separation. On the other hand, stronger uniformity can make the latent space occupied by the category larger, which is more suitable for categories with fewer samples. Conversely, we hope to set a higher temperature for categories with more samples. To this end, we develop a temporary feature clustering module to assign dynamic soft labels as auxiliary information to guide the assignment of temperature. There"}, {"title": "3.4. Explicit Quantity Awareness", "content": "In Sec. 3.3, we describe how we use dynamic temperature tuning strategy to optimize spatial structure. However, we find that in order to increase the stability of training and prevent overfitting, the temperature setting has a certain limit range. At the same time, adjusting the temperature parameter according to different datasets is computationally expensive, so a fixed temperature range is a common choice. This means that when facing real-world datasets with large imbalance factors, our method may not achieve the desired correction effect.\nThis under-optimization is mainly manifested in the destruction of category-level uniformity due to temperature restrictions. However, this category-level uniformity information can be directly obtained from the sample quantity distribution. Therefore, we introduce a classic supervised re-weighting strategy into self-supervised learning paradigm to assist category homogenization. This supervision can work together with temperature adjustment. At the same time, in order to avoid over-correction, we use square root inverse proportional correction instead of general inverse proportional correction."}, {"title": "3.5. Training with Hybrid Cross-paradigm Loss", "content": "In this section, we introduce our design of loss function feature and clustering component.\n$l_i = - log \\frac{S(v_i, v_{i'}, \\tau_i)}{S(v_i, v_{i'}, \\tau_i) + \\sum_{v_j \\in D_j} S(v_i, v_j, \\tau_i)}$\n(5)\nFirst, we give our hybrid cross-paradigm loss as Eq. (5), where Ti denotes a dynamic temperature assigned by temporary labels, and |Dj| denotes the number of the samples that is allocated to the same class as vj.\nOur training is divided into two stages: warming for B epochs and cluster balance training for the rest epochs. In the warming phase, we utilize original SimCLR to seek a good cluster initialization. After B epochs, we use a progressive temperature factor until S epochs.\nThe pseudo-code is summarized as Sec. 3.5"}, {"title": "3.6. More Discussion", "content": "Computational Complexity and Storage Consumption.\nOur method maintains a temporary list of pseudo-labels with length equal to the number of samples and performs a cross-batch clustering operation once in a certain number of rounds. our method requires only a small increase in storage consumption and computational overheads."}, {"title": "4. Experiment", "content": "4.1. Datasets\nCIFAR10-LT and CIFAR100-LT [12] are long-tailed versions of the CIFAR10 and CIFAR100 datasets, respectively. The original CIFAR10 [27] dataset comprises of 32\u00d732 images from 10 classes, with 50k images allocated for training and 10k images for testing. The imbalance factor is defined as the number of the head classes divided by the tail classes. Following SDCLR [22] and TS [28], we set the imbalance factor to 100. Each class contains images from 4500 to 45 according to the Pareto distribution, a total of 11165 images for training and test set is remained at 10k images.\nSimilarly, the CIFAR100 dataset has 100 classes of images with 50k images for training and 10k for testing. The number of images per class for training in CIFAR100-LT varies from 450 to 4, with a total of 9,754 images. we average all of our results three times over five splits to reduce randomness to the greatest extent possible..\nImageNet100-LT is a subset of ImageNet100 [40] dataset, consisting of 12.21k images from 100 classes for training and 5k images for testing. The images of each class in the training set range from 1280 to 5 according to the Pareto distribution.\n4.2. Experimental Setup\nEvaluation. The previous exploration, TS [28] provides a comprehensive validation that employs K nearest neighbours [10] (KNN) to verify the latent space structure by directly evaluating the learned local representations, and two linear probing (LP) methods to validate the generalization and linear separability [22] of learned embedding, which we call the balanced minimum sampling linear probing (MS LP) and the long-tailed linear probing (LT LP). We follow their validation methods, and in addition, we provide two"}, {"title": "4.3. Experimental Results", "content": "In this section, we summarize the comprehensive experimental results by comparing our APS with recent methods across a range of benchmarks, and offer insights into the observed outcomes.\nIn the CIFAR10-LT and CIFAR100-LT datasets, we conduct comparison experiments across all six benchmarks previously mentioned. As shown in Tab. 1 and Tab. 2, for comparison of latent space structure with the ResNet18 back-"}, {"title": "4.4. Analysis of sensitivity", "content": "We verify the hyperparameters sensitivity of our APS. Firstly, as we simply utilize K-Means [32] for assigning the dynamic labels, the number of clusters K should be given carefully consideration, it is challenging to ascertain the total number of image categories without ground truth labels, and finding the optimal parameters is computationally expensive and time-consuming working on large-scale datasets. However, the strong robustness exhibited by proposed APS will alleviate this problem, as shown in Tab. 5. We validate performance of various K on CIFAR100-LT dataset, the best item is indicated by bold text and underline for the worst one. Additionally, we report the accuracy range of each benchmark, spanning from 0.27% to 1.23%. Our method maintains competitive results even with K=200, and the high robustness of K helps us to find the optimal parameters more efficiently in the face of unknown datasets."}, {"title": "4.5. Ablation study", "content": "To verify the importance of each component of proposed APS, we conduct ablation experiments for proposed component, as shown in Tab. 3. Our approach has adaptive temperature awareness and quantity-aware re-weighting, both of which utilise the temporary labels obtained from clustering to correct the neglect of tail classes.\nCompared to SimCLR, our method maintains a consistent gain from clustering information. APS improves the performances on all the benchmarks ranging from 3.97% to 9.26%, with an average improvement of 6.75%. Even only utilising the re-weighting strategy, APS can also have an average of 3.90% improvement.\nIn contrast to the TS using temperature cosine variation strategy, APS still improves accuracy by about 0.4% on each benchmark when only using temperature adaptation. These results corroborate the effectiveness of our proposed clustering enhancement approach."}, {"title": "4.6. T-SNE visualization", "content": "T-SNE provide a qualitative perspective into the learned discriminative representation. As shown in Fig. 3, we select samples from all 10 classes of CIFAR10-LT to generate plots. Compared to the SDCLR, Our APS exhibits tighter clusters and more clearly distinguishing boundaries. The results demonstrate that our method is effective in producing high-quality auxiliary supervision when facing long-tailed distributions."}, {"title": "5. Conclusion", "content": "In this paper, we proposed a novel method for self-supervised long-tailed recognition called APS. A cross-paradigm collaborative loss function is devised to facilitate the synergy between adaptive temperature adjustment and re-weighting techniques, thereby mitigating the issue of neglecting tail classes and enabling the acquisition of more generalized embeddings. Our approach demonstrates that the experience of supervised long-tail learning can be migrated to self-supervised long-tail learning with low overhead, in contrast to previous work that proposed non-generic solutions due to the absence of labels."}]}