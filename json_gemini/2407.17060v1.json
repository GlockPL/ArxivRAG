{"title": "High Efficiency Image Compression for Large Visual-Language Models", "authors": ["Binzhe Li", "Shurun Wang", "Shiqi Wang", "Yan Ye"], "abstract": "In recent years, large visual language models (LVLMs) have shown impressive performance and promising generalization capability in multi-modal tasks, thus replacing humans as receivers of visual information in various application scenarios. In this paper, we pioneer to propose a variable bitrate image compression framework consisting of a pre-editing module and an end-to-end codec to achieve promising rate-accuracy performance for different LVLMs. In particular, instead of optimizing an adaptive pre-editing network towards a particular task or several representative tasks, we propose a new optimization strategy tailored for LVLMs, which is designed based on the representation and discrimination capability with token-level distortion and rank. The pre-editing module and the variable bitrate end-to-end image codec are jointly trained by the losses based on semantic tokens of the large model, which introduce enhanced generalization capability for various data and tasks. Experimental results demonstrate that the proposed framework could efficiently achieve much better rate-accuracy performance compared to the state-of-the-art coding standard, Versatile Video Coding. Meanwhile, experiments with multi-modal tasks have revealed the robustness and generalization capability of the proposed framework.", "sections": [{"title": "I. INTRODUCTION", "content": "LARGE visual-language models (LVLMs) have shown impressive success in a variety of multi-modal application domains. Images, which are typically featured with high data volume, are typically compressed for transmission before feeding to the LVLMs at the cloud end. Instead of supporting only a single task, LVLMs typically support multi-tasks simultaneously, which brings unprecedented challenges to image coding for machines [1].\nIn the past decades, as the default visual data communication solutions, existing image and video standards have been developed and facilitated to improve rate-distortion (RD) performance, such as H.264/AVC [2], H.265/HEVC [3], H.266/VVC [4], and AVS [5]. Inspired by the rapid development of deep neural networks, many learning-based image and video codecs are proposed [6]-[10], which have achieved comparable and even better RD performance compared with VVC [11], [12]. Treating the human visual system (HVS) as the consumer of the visual signals is the default assumption for most traditional and learning-based codecs. However, when the receiver is replaced by machine vision, these codecs usually struggle to obtain satisfactory results. One apparent reason is that machine tasks typically require different semantic features, such that features for signal reconstruction toward human vision are not dedicated to machine vision. Therefore, a robust machine-oriented framework for efficiently compressing visual information is in a high demand, to adapt to the growing trend of employing a variety of artificial intelligence (AI) analytical models as the ultimate consumers of data.\nTo facilitate downstream machine tasks with limited bandwidth, various efficient compression methods have been proposed to improve coding performance. These methods are mainly divided into three categories. The first category of methods applies the image/video enhancement networks as the pre-processing or post-processing modules [13]\u2013[15] without modifying the existing codecs and downstream analysis tasks. These enhancement networks are designed to remove unnecessary background information to save bitrate consumption or enhance the semantic features to achieve better accuracy for machine tasks. The second category involves improving traditional or end-to-end codecs to optimize bitrate allocation and semantic feature representation [16]\u2013[21], thereby enhancing machine analysis performance. The codec is designed to reconstruct the machine task-friendly visual signal at the decoder side. The visual signal preserves the task-relevant semantic features, resulting in better rate-accuracy (RA) performance. Regarding the third research direction, the codecs are designed to transmit and reconstruct the compact feature representation of the machine vision task instead of the visual signal as an input to the downstream task [22]\u2013[26]. The compression of compact features reduces inter-feature redundancy and lowers bitrate consumption. However, reconstructed intermediate features are specific to a limited number of task networks, lacking the generalization capability.\nMost of the current approaches presume specific tasks as downstream tasks after receiving the reconstructed signals/features, such as image classification [27], object detection [28], instance segmentation [29]. However, with the development of transformer-based large models [30]\u2013[35], multi-modal tasks completed with LVLMs have been widely studied and deployed. Regarding video coding for machines, when the machine transitions from handling specific tasks to analyzing scenes in a more general manner, the compression algorithms need to be redesigned. Compared to specific tasks, multi-modal tasks are not limited to particular object categories but can concentrate on richer semantic information. For example, in detection and segmentation tasks, the color of a target is not semantically significant. By contrast, in visual grounding or image-text retrieval tasks, the color of the target may be present in the textual description and serves as a pivotal piece of task-relevant semantic information, significantly aiding in the alignment of visual and textual modalities. Therefore, it is crucial to develop a compression scheme to overcome the limitations of existing methods with enhanced generalization capability.\nIn this paper, we propose a dedicated image codec for LVLMs, as shown in Fig. 1. The primary idea is developing a token-based pre-editing framework and training the preprocessing network and codec jointly by incorporating a semantically-oriented optimization function. As such, we can preserve and enhance generic semantic information throughout the compression process, leading to an efficient image compression scheme for LVLMs. Extensive experimental results show that the proposed framework achieves efficient and robust compression performance on different multi-modal tasks with different LVLMs. The main contributions of this paper are summarized as follows,\n\u2022 We propose an image compression scheme for LVLMs, consisting of the semantically driven pre-editor and the codec. This unique scheme is tailored to the growing trend of increasingly powerful artificial intelligence, and the whole process is fully optimized by the semantic information instead of fitting a particular analytical task. Experimental results show that the proposed scheme achieves more than 50% bitrate savings at the same accuracy level when applying LVLMs as the ultimate receiver, and stronger generalization capability is exhibited in a variety of applications.\n\u2022 We develop the pre-editing module guided by the large model tokens. The proposed pre-editing network is designed to preserve critical semantic information to maintain the machine task performance while discarding semantically irrelevant information to minimize bitrate consumption. This is accomplished by leveraging semantic tokens at different spatial levels, such that the characteristics of LVLMs can be fully exploited.\n\u2022 We enhance the semantic consistency within the compression process by imposing supervision with the rank of large model tokens. The design of this loss function is based on the assumption that the rank of tokens reflects the semantic abundance. By attempting to maintain the rank of tokens in the reconstructed visual signals, the performance of machine tasks with LVLMs can be further optimized."}, {"title": "II. RELATED WORKS", "content": "A. Visual Signal Compression\nVisual signal compression plays a fundamental role in various image/video based applications, typically targeting at better perception quality with lower representation expense. Motivated by this, the compression of visual signals has been elaborately investigated in terms of signal processing in the past decades, especially in various image/video compression standards. With the unprecedented development of deep learning in recent years, the efficiency of visual signal compression has also been explored by means of a variety of deep learning techniques.\nImage/video compression standards. Over the past decades, visual signal compression has been driven by the development of image/video compression standards. For image compression, JPEG [36] has been widely deployed since 1992. JPEG 2000 [37] was further standardized for higher compression efficiency. Motivated by the new requirements of image compression on websites and mobile devices, multiple image codecs were further developed, such as WebP [38] and BPG [39], achieving obvious compression efficiency improvement. For video compression standards, there is a continuous evolution from H.262/MPEG-2 [40], H.264/MPEG-4 AVC [2] to H.265/HEVC [3]. Based on this, H.266/VVC [41] was further developed and published in 2020, which is the state-of-the-art video compression standard.\nDeep learning based visual signal compression. Inspired by the development of deep learning algorithms and hardware, deep learning-based visual signal compression has achieved remarkable progress, leveraging the powerful representation capability of artificial neural networks. Pioneering works have been proposed to compress images with a recurrent neural network (RNN) [42]. A block-based convolutional neural network (CNN) image compression model [43] was further proposed, achieving superior performance with JPEG. Moreover, a series of end-to-end image compression codecs were proposed, including [44], [45], and [46], which are featured with generalized divisive normalization (GDN) [47], variational hyper-prior structure and joint autoregressive hierarchical prior, respectively. Subsequently, the image compression performance is further improved with a discretized Gaussian Mixture Likelihoods [48], which could parameterize the latent code distribution and improve the accuracy of the entropy model, achieving comparable performance compared with the still picture profile of VVC. Meanwhile, noticeable progress has been made in end-to-end video compression. An end-to-end video compression framework [6] was first proposed and achieved better performance compared with H.264/AVC. A feature-space video compression framework [49] was further proposed, achieving higher compression efficiency compared with HEVC. It is worth mentioning that the neural video compression with diverse contexts [50] could achieve superior performance compared with VVC under low delay configuration."}, {"title": "B. Visual Signal Compression for Machine Vision", "content": "With the successes of AI-based visual signal analysis in various machine vision applications, there is a shift for the ultimate consumer of visual signals from human perception to machine vision. Targeting to improve the coding efficiency for machine vision, a variety of techniques have been proposed, which could be categorized into codec improvement, feature compression, and processing for machine vision, depending on the reconstructed information at the decoder side.\nVisual signal codec for machine vision. Recently, obvious progress has been made in deep learning-based visual signal codec for machine vision. To be specific, regarding the neural network structure for machine vision, the compression efficiency for machine vision is improved by means of a latent space masking network (LSMnet) [20] and spatial-channel attention-based variable bitrate structure [21]. In order to further improve the machine vision compression performance, the machine-oriented loss function is also investigated by means of the combination of signal level distortion and task loss function directly [51]. The visual feature map distortion was further proposed to replace the task loss function in [52], avoiding the dependency of machine task labels. Moreover, the optimal weight parameter of various distortions in the joint loss function was explored in [53] with an iterative optimization algorithm, achieving obvious performance improvement for machine vision.\nFeature compression. Machine vision is operated upon visual features instead of textures. As such, a common paradigm is to extract and compress the visual features for transmission rather than compressing the textures. Generally speaking, the visual features are typically more compact than textures, facilitating the simultaneous large-scale visual information transmission in front-end devices. To improve the visual search accuracy and efficiency, two standards, Compact Descriptors for Visual Search (CDVS) [54], and Compact Descriptors for Video Analysis (CDVA) [55], were developed for image and video search respectively. In recent years, a number of machine vision algorithms have been proposed for various machine analysis tasks, such as object detection [29], instance segmentation [29], and object tracking [56]. To facilitate the aforementioned machine vision tasks, various algorithms have been proposed for the compact representation of deep learning features [57]\u2013[60]. Moreover, the compression of deep learning features for machine vision is in the standardization process for VCM in MPEG [61].\nProcessing toward machine vision. It is a straightforward pipeline to process the visual signal toward machine vision without modifying the existing codec. The coding efficiency of image/video coding standards is boosted with various pre-post processing techniques, including the pre-processing [13], [62], spatial resampling [63], temporal resampling [64] and post-processing [65]. Moreover, a task-switchable pre-processor was designed in [14] to preserve important semantic information based on the specific characteristics of various downstream tasks, which supports the demand for multi-task applications in the real world."}, {"title": "C. Large Visual-Language Models", "content": "Having witnessed the success of transformer-based models in natural language processing (NLP) tasks, numerous efforts have been made to apply the transformer structure in vision-language tasks. Parmar et al. [66] increased the image size that can be handled by the model by restricting self-attention in local neighborhoods. Sparse Transformer [67] employs scalable approximations of global self-attention by introducing sparse factorizations of the attention matrix to avoid quadratically growing of the time and memory with the sequence length. Subsequently, Dosovitskiy et al. [68] discarded the reliance on CNNs and proposed the Vision Transformer (ViT) with excellent results. To improve the performance with visual modality, efficient pre-training methods [69], [70] were developed. Based on these advanced results, numerous works [30], [31], [33], [34] achieved superior performance in vision-language tasks by jointly learning multi-modal data."}, {"title": "III. THE PROPOSED IMAGE COMPRESSION SCHEME", "content": "A. Framework in a Nutshell\nThis subsection introduces the image compression framework for LVLMs, which is composed of the pre-editing module and the end-to-end codec. As shown in Fig. 1, the pre-editing module includes the semantic tokens extractor and the pre-editing network, and the end-to-end codec is composed of the encoder and decoder for compressing and reconstructing the preprocessed image. The LVLMs are regarded as the ultimate receivers of the reconstructed images. The text information for different tasks is also fed into LVLMs. In detail, the pre-editing module takes the original image \\(I_o\\) and compression ratio index \\(Q\\) as input and outputs the preprocessed image \\(I_p\\). The \\(Q\\) also corresponds to the index of the Lagrange multiplier parameters in the loss function. Subsequently, the \\(I_p\\) is compressed as bitstream \\(x\\) by the encoder Enc(\u00b7). On the decoder side, the decoder Dec(\u00b7) reconstruct the decoded image \\(I_d\\). Finally, the large models use the \\(I_d\\) and other modality information in different multi-modal tasks. In particular, the pre-editing module consists of the tokens exactor \\(F_{tk}(\\cdot)\\) and the pre-editing network \\(F_{pre}(\\cdot)\\). Given \\(I_o\\), the \\(F_{tk}(\\cdot)\\) outputs the tokens \\(T_{gt}\\) for semantic information extraction. Based on the \\(T_{gt}\\) and \\(I_o\\), the pre-editing network \\(F_{pre}(\\cdot)\\) generates the \\(I_p\\) for better rate-accuracy performance."}, {"title": "B. Pre-editing Based on Semantic Tokens", "content": "In this subsection, we incorporate the semantic tokens into the pre-editing network, accommodating the unique characteristics of LVLMs. Instead of the task-wise optimization in the traditional coding for machine, we formulate the problem with the general semantic information extraction and optimization pipeline, leading to a more robust, effective, and generic solution. In particular, given the outstanding performance of visual transformer-based models in various tasks [71], [72], it is reasonable to assume that the tokens used as internal features could provide accurate and generalized semantics. To enhance semantic information and remove irrelevant features, we introduce tokens from visual transformers as semantic prompts for the image editing network. As shown in Fig. 2, we combine U-Net [73] and token information \\(T_{gt}\\) to create a pre-editing network. By utilizing semantic tokens at multiple scales, we can achieve better RA performance across a range of tasks.\nMore specifically, the proposed pre-editing network consists of three parts: semantic token refinement, down-sampling, and up-sampling branches. The semantic tokens refinement branch uses the token block to extract semantic features \\(F_{ik}\\) from the \\(T_{gt}\\), where the token blocks refine the semantic feature representation in different scales. These features are fed into the down-sampling and up-sampling branches. The down-sampling branch combines the image features \\(F_d\\) extracted from the input image and \\(F_{ik}\\), and the fusion block further extracts intermediate features \\(F_d^{+1}\\). Subsequently, \\(F_d^a\\) is down-sampled to \\(F_d^{+1}\\) by the maximum pooling layer with a 2 \u00d7 2 kernel. \\(F_d^{\\prime}\\) is fed to the up-sampling branch by skip connections. Analogously, the up-sampling branch also combines the intermediate features \\(F_u^i\\), \\(F_{ik}\\) as well as \\(F_a^u\\) and employs the fusion modules to obtain the features \\(F_u^{\\prime}\\). The output image is obtained based on \\(F_u^{\\prime}\\) by the convolution layer.\nFurthermore, as shown in Fig. 2, the token block consists of the resize step, the convolution layer with the 1 \u00d7 1 kernel, the normalization layer [74], and the base block. To adapt to images of different scales, the resize step reshapes the tokens from [\\(T_{dim}\\), \\(T_{num}\\)] to [\\(T_{dim}\\), \\(\\sqrt{T_{num}}\\), \\(\\sqrt{T_{num}}\\)] and interpolates the tokens to the size of \\(F_d\\). The base block follows the one in [75] with the channel-wise attention mechanism [76]. Moreover, the fusion block is composed of the convolution layer for reducing the channel size, the base block for feature enhancement, and the adaption processing. Given the index \\(Q\\), the adaption layer predicts the feature weighting vector which is channel-wisely multiplied with the input feature \\(F_{in}\\) to output modulated feature \\(F_{out}\\), ultimately adapting the compression ratio."}, {"title": "C. Variable Bitrate Codec", "content": "This subsection introduces the end-to-end variable bitrate codec that compresses the input image \\(I_p\\) and reconstructs the \\(I_d\\) with the compression ratio index \\(Q\\). To remove the redundancy when considering the ultimate receivers are LVLMs, we develop a codec using the variational autoencoders-based codec [45], [77] as the foundation, as shown in Fig. 3. Furthermore, the ResBlcok and compression ratio adaption layers are also introduced to achieve better variable RA performance. In particular, the codec includes the \\(g_{enc}\\), \\(g_{dec}\\), \\(h_{enc}\\), and \\(h_{dec}\\). The \\(g_{enc}\\) and \\(g_{dec}\\), which are composed of convolutions and the ResBlocks, form the image autoencoder structure to compress the image. Meanwhile, the \\(h_{enc}\\) and \\(h_{dec}\\) consist of the convolutions and adaption layers to implement the hyperprior autoencoder for transmitting the spatial distribution of standard deviations \\(\u03b4\\). Moreover, the \\(Q_n\\) is the quantization, and AE and AD represent the arithmetic encoder and arithmetic decoder, respectively. Besides, the ResBlocks include convolutions, adapt layers, and GDN/IGDN activation functions. For the convolution layer, the parameters N\u00d73\u00d73 represent the number of channels and the kernel size. Herein, /2 and *2 indicate downsampling or upsampling with the convolution stride and pixel shuffle function, respectively. Finally, the adaption layer is consistent with the adaption layer of the pre-editing network."}, {"title": "D. Loss Functions", "content": "The training of a traditional codec can be typically casted into a classical rate-distortion optimization (RDO) problem. In general, the training loss function typically consists of the number of coding bits and the distortion between the input and reconstructed signals. Moreover, to address the problem of coding for machine, the loss from the machine analytics task is also included in the loss function. However, such loss functions cannot be straightforwardly employed in the context of LVLMs, as LVLMs can complete a series of tasks simultaneously. By contrast, the loss function of one or several particular tasks focuses only on task-related semantics, ignoring irrelevant semantic information that might be meaningful for other tasks. Therefore, task-based loss functions in training make it challenging to encompass the semantics of the entire image comprehensively. To address this challenge, the loss functions based on tokens instead of tasks are incorporated to improve the generalization capability and RA performance of the LVLMs. Consequently, the loss function of the proposed framework is formulated as follows,\n\\[ L = \\lambda_R L_R + \\lambda_D L_D + \\lambda_T L_T, \\]\nwhere \\(L_R\\), \\(L_D\\), and \\(L_T\\) are the coding bitrate loss, image distortion loss, and token-based loss, respectively. Meanwhile, the \\(\\lambda_R\\), \\(\\lambda_D\\), and \\(\\lambda_T\\) are the hyper-parameters of the weights for the corresponding loss. The \\(L_R\\) is the bitrate loss obtained by the proposed codec. The image distortion loss \\(L_D\\) is the MSE loss between the \\(I_o\\) and \\(I_d\\).\nMore specifically, the token-based loss includes the mean squared error (MSE) loss \\(L_{tk}\\) and the rank loss \\(L_{rk}\\).\n\\[ L_T = \\lambda_{tk} L_{tk} + \\lambda_{rk} L_{rk}, \\]\nThe \\(L_{tk}\\) is also the MSE loss between the ground truth tokens \\(T_{gt}\\) and tokens \\(T_d\\) of reconstructed image \\(I_d\\). The MSE loss of tokens facilitates the reduction of compression distortion at the semantic level. Moreover, the feature collapse phenomenon implies the relevance of the rank of the tokens to the semantic information [32], [78]. The larger the rank of the tokens, the more likely that more semantics are included. As such, the rank loss is proposed to measure the loss of the rank of the tokens during compression and is used to optimize information loss at the semantics level. The rank of the matrix is discontinuous, such that we use the trace instead of the rank. In detail, the rank loss \\(L_{rk}\\) measures MSE loss between the rank of the ground truth tokens and the rank of reconstructed tokens, which is formulated as,\n\\[ L_{rk} = D(tr(Sig(\\Sigma_{gt})), tr(Sig(\\Sigma_d))), \\]\nwhere the \\(\\Sigma_{gt}\\) and \\(\\Sigma_d\\) are the eigenvalue matrix of \\(T_{gt}\\) and \\(T_d\\), respectively. The Sig(\u00b7) and tr(\u00b7) mean the sigmoid function and the trace of the input matrix, respectively. The function D(\u00b7,\u00b7) defines the MSE loss between the rank of ground truth tokens and the reconstructed tokens."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "A. Experimental Setup\n1) Datasets and Task Models: The LVLMS OFA [33] and OP [34] are treated as the ultimate consumers of the decoded images. In training, the COCO dataset [79] is used to train the proposed variable bitrate framework, and semantic tokens in training and testing are extracted by the backbone of the ViTDet [71]. To evaluate the influence of compression on cross-modal downstream tasks, the MSCOCO dataset [79]\u2013[81] and RefCOCO/RefCOCO+ [82], [83] are used as testing datasets. For both the RefCOCO and RefCOCO+ datasets, val/testA/testB are regarded as different testing splits. In testing, the OFA [33] is taken as the task network for the visual grounding task and image captioning task, and OP [34] is used for the visual grounding task and image-text retrieval task. Moreover, we use the Acc@0.5, R@1, and CIDEr [84] as the evaluation metrics on visual grounding, image-text retrieval, and image captioning tasks, respectively. Furthermore, the Bjontegaard Delta Bit Rate (BD-Rate) [85] is used to measure the percentage of saved bitrate at the same task accuracy. In addition, for the nonmonotonic curves, the pareto front curve is created to present the performance [86].\n2) Implementation Details: The Adaptive Moment Estimation (Adam) [87] optimizer with the parameters (\u03b2\u2081 = 0.5 and B2 = 0.999) is set as the optimizer. The model training is conducted on the NVIDIA TESLA A100 GPU with a batch size of 8. To achieve better performance, we divide the training process of the scheme into three stages. In the first stage, the variable bitrate codec is trained without token-related loss, which is taken as the pre-trained codec. For this stage, the initial learning rate is set to 1 \u00d7 10\u207b\u2074 with 200K iterations. In the second stage, the pre-editing network is trained with the pre-trained codec, and the initial learning rate is set to 1 \u00d7 10\u207b\u2074 and reduced to 1 \u00d7 10\u207b\u2076 with the cosine annealing schedule [88] in 150K iterations. After that, the pre-trained codec and pre-editing network are fine-tuned in the third stage. Herein, the initial learning rate is set as 1 \u00d7 10\u207b\u2075 to the codec and 1 \u00d7 10\u207b\u2076 to the pre-editing network with 150K iterations."}, {"title": "B. Performance Evaluations", "content": "In this subsection, we conduct a series of experiments to study the generalization capability and efficiency of the proposed framework. First, the RA performance of the proposed framework is compared with the baseline across different vision-language tasks and corresponding LVLMs. Subsequently, we conduct the ablation studies to investigate the effect of different modules in the proposed framework.\n1) Overall Performance Comparisons: To verify the efficiency and the generalization capability of the proposed image compression framework, the RA performance of the proposed method and VVC anchor are compared in different vision-language tasks and LVLMs. For the VVC anchor, the VTM-22.2 [89] is deployed as the VVC platform with coding performed in the YUV420 format in these experiments, which is the state-of-the-art coding standard. The QPs in testing are set as [32, 35, 37, 40, 43, 47] to achieve a similar bits-per-pixel (bpp) range with the proposed framework. In addition, the BD-Rate is used for performance evaluations.\nAs shown in Table I, the proposed framework performs better than the anchor in these experiments. Moreover, in Fig. 4, we compare RA curves for four different tasks with two large models, respectively. We can observe that the proposed framework achieves promising improvements compared to the anchor method across different datasets and tasks. Meanwhile, these proposed curves are close to performance obtained by the analysis of the original images. These experimental results demonstrate the proposed framework surpasses the anchor method, maintaining the performance of multi-modal tasks while significantly saving bit rates. Furthermore, Fig. 6 shows two examples of reconstructed images and image captioning results. It is obvious that the proposed compression framework could achieve accurate image captioning results with less bitrate consumption than VVC. Meanwhile, we enlarge the patch of the decoded image to show the details. As shown in Image 1 of Fig. 6, the proposed method does not introduce any undesired textures. In Image 2 of Fig. 6, the proposed method results in clearer object boundaries compared to the baseline. In conclusion, these experiments demonstrate that the proposed framework has a promising generalization capability and efficiency toward different vision-language tasks and datasets.\n2) Ablation Studies: Herein, the ablation experiments are conducted in three multi-modal tasks to demonstrate the influences of different modules. In particular, we first adopt the pre-trained codec instead of the codec fine-tuned with token-related loss, denoted as the Proposed (w/o Codec Training). Second, we remove the model of the pre-editing such that the given images are coded directly, denoted as the Proposed (w/o Pre-editing). Moreover, the proposed framework is retrained and evaluated without the rank loss, denoted as Proposed (w/o Rank Loss), to study the performance of the proposed rank loss. Finally, the pre-editing module of the proposed framework removes the tokens inputs, denoted as Proposed (w/o Tokens), to evaluate the performance of semantic information guidance. Based on the results in Table II and Fig. 5, we can draw some conclusions. Firstly, the proposed method achieves the best performance, which demonstrates the rationality and effectiveness of this design. Compared with the Proposed (w/o Lrk), the proposed scheme achieves 2.01%, 2.82%, and 3.33% BD-Rate savings in three tasks, respectively, illustrating the effectiveness of rank loss. Finally, the Proposed (w/o Pre-editing) achieves better performance than the Proposed (w/o Codec Training), which may be because the codec can adapt to the feature distribution rather than just enhancing features and eliminating redundancy.\n3) Complexity Study: When deploying compression frameworks in real-world scenarios, computational complexity plays a critical role. Therefore, complexity study experiments are conducted to measure the number of parameters, inference time, and floating-point operations per second (FLOPS) of four networks in the framework. Experimental results are shown"}, {"title": "V. CONCLUSION", "content": "In this paper, we have proposed a new solution for image compression when LVLMs are treated as visual data consumers. The novelty of this paper lies in that the learning process is fully guided by semantically meaningful tokens, which enables the reconstructed images to preserve semantic information in a limited bandwidth environment. Moreover, a dedicatedly designed pre-processing model is proposed for RA performance improvement by discarding the irrelevant semantic information in images. The proposed scheme could adapt to diverse multi-modal tasks with LVLMs. The framework proposed in the experiment achieves superior RA performance compared to the state-of-the-art coding standard. Meanwhile, the experiments on different multi-modal tasks and datasets verify the generalization ability of this framework, demonstrating its potential in practical applications. The proposed framework validates the potential of video coding for large models and provides a reference for the compression research of multi-modal data for large models. In the future, the proposed scheme can be expanded to multi-modal data, various scenarios, and diverse large model tasks. Meanwhile, the non-monotonous RA curve also indicates the requirement for research on the robustness of a large model in real-world applications."}]}