{"title": "INTERCEPTING UNAUTHORIZED AERIAL ROBOTS IN CONTROLLED AIRSPACE USING REINFORCEMENT LEARNING", "authors": ["Francisco Giral", "Ignacio Gomez", "Soledad Le Clainche"], "abstract": "The proliferation of unmanned aerial vehicles (UAVs) in controlled airspace presents significant risks, including potential collisions, disruptions to air traffic, and security threats. Ensuring the safe and efficient operation of airspace, particularly in urban environments and near critical infrastructure, necessitates effective methods to intercept unauthorized or non-cooperative UAVs. This work addresses the critical need for robust, adaptive systems capable of managing such threats through the use of Reinforcement Learning (RL). We present a novel approach utilizing RL to train fixed-wing UAV pursuer agents for intercepting dynamic evader targets. Our methodology explores both model-based and model-free RL algorithms, specifically DreamerV3, Truncated Quantile Critics (TQC), and Soft Actor-Critic (SAC). The training and evaluation of these algorithms were conducted under diverse scenarios, including unseen evasion strategies and environmental perturbations. Our approach leverages high-fidelity flight dynamics simulations to create realistic training environments. This research underscores the importance of developing intelligent, adaptive control systems for UAV interception, significantly contributing to the advancement of secure and efficient airspace management. It demonstrates the potential of RL to train systems capable of autonomously achieving these critical tasks.", "sections": [{"title": "1 Introduction", "content": "Autonomous vehicles and robotics have gained significant attention in recent years due to their potential to address various contemporary issues. Specifically, aerial robotics and Unmanned Aerial Vehicles (UAVs) are utilized in both civil and military applications, including search and rescue, surveillance and reconnaissance, boundary control, critical installations protection, and autonomous transportation, among others. Currently, these applications face several limitations because existing methods do not generalize well or adapt to environmental changes, thereby hindering full autonomy.\nMachine Learning, and particularly Reinforcement Learning (RL), has proven effective at overcoming these limitations. RL has been successfully applied in various fields such as video games [1, 2], mathematical discoveries [3], and robotics [4, 5]. In robotics and autonomous vehicles, its potential is particularly promising, providing capabilities for embodied systems to make intelligent decisions despite environmental uncertainties or high-dimensional observations that are challenging for traditional methods. In the domain of autonomous aerial vehicles, Reinforcement Learning has achieved state-of-the-art results in tasks such as drone racing competitions [6], autonomous dogfighting [7, 8], and robust control of multiple suspended loads [9].\nThe proliferation of aerial vehicles, both manned and unmanned, has led to an increased presence of flying robots in controlled airspace, such as the vicinity of airports. Unauthorized UAVs in these areas pose significant risks, including potential accidents and disruptions to air traffic operations, which can result in financial losses, flight delays, or complete operational shutdowns. Looking ahead to urban air mobility, where an Unmanned Traffic Management (UTM) system will enable piloted and autonomous aerial vehicles to coexist in urban airspace, controlling both authorized and unauthorized UAVs is crucial for maintaining operational safety.\nAddressing the need to capture or intercept unauthorized UAVs in specific airspace zones, whether urban or natural, is essential for ensuring the safety of aerial operations and air traffic. One effective solution involves deploying an intelligent system capable of detecting and capturing unauthorized UAVs, which requires robustness and adaptability to varying target movements and maneuvers. Autonomous aerial robots are well-suited for this task.\nThis research aims to tackle the challenge of intercepting a non-cooperative dynamic target (a UAV) with a pursuing agent. We employ Reinforcement Learning to train a pursuer agent with the objective of intercepting the target swiftly. Previous studies have utilized RL for similar tasks; some have approached the problem using Multi-Agent Reinforcement Learning (MARL), training both pursuers and evaders in adversarial settings with multiple quadcopters in complex environments [10, 11, 12]. Other works have focused on training the evader to escape attackers [13, 14]. In single-agent RL settings, researchers have trained quadcopters to follow targets using visual sensors and discrete control actions [15] or decomposed the problem to manage system dynamics at a low level [16]. Applications have extended to fixed-wing UAVs, intercepting moving targets with fixed trajectories [17] or training RL agents to follow control setpoints for pursuing targets [18, 19].\nThis work presents a training framework for a fixed-wing UAV pursuer agent using Single-Agent Reinforcement Learning, with the goal of capturing a dynamic evader. The objective is to train a pursuer UAV capable of handling various evading strategies, beyond those encountered during training. We propose a training setup where the pursuer must intercept a dynamic UAV target that employs different evasion strategies, tested by manually controlling the evader. All simulations are conducted using JSBSim [20], a high-fidelity Flight Dynamics Model (6 DoF), ensuring both pursuer and evader share the same physics model. Unlike most previous works, we utilize a model-based RL algorithm, DreamerV3 [21], which leverages imagined trajectories in a world model to train the control policy in a continuous action space, aiming for robust task representation. We compare Dreamer's performance against two well-known model-free RL algorithms, Truncated Quantile Critics (TQC) [22] and Soft-Actor-Critic (SAC) [23], both of which have shown excellent results in continuous control problems. The robustness and generalization capabilities of the trained agents are evaluated under various scenarios, including wind disturbances and sensor noise.\nThe contributions and advantages of this paper are as follows:\n\u2022 We propose a Reinforcement Learning-based training framework to design a pursuer agent in a single-agent setup, capable of intercepting an evader regardless of its strategy. This approach aims to create a pursuer capable of generalizing to different evading strategies, avoiding the complexity of multi-agent RL setups, and allowing exploration of various algorithmic approaches without relying on low-level or setpoint tracking controllers.\n\u2022 We compare model-based and model-free RL approaches. Unlike many other studies that focus solely on model-free algorithms, we evaluate the performance of both methods.\n\u2022 We validate the system's robustness by introducing sensor noise and wind gust perturbations post-training, exposing the agent to previously unseen scenarios.\nThe paper is organized as follows: Section 2 summarizes the methodology, defining the problem, describing the simulation environment, and detailing the reinforcement learning training framework, including the Markov Decision Process (MDP) definition, reward design, and training parameters. Section 3 presents the training and validation results under various scenarios. Finally, we conclude with our findings and future directions."}, {"title": "2 Methodology", "content": "This section introduces the proposed method. First, we provide a brief introduction to the problem we aim to address. Next, we detail the simulation environment and the Flight Dynamics Model used to test our methodology. Following that, we describe the algorithms used in this work, with a particular focus on the model-based RL algorithm, DreamerV3."}, {"title": "2.1 Problem Formulation", "content": "Ensuring the safety and efficiency of airspace operations, especially in controlled environments like airports, necessitates addressing the challenge posed by unauthorized or non-cooperative UAVs. These UAVs can disrupt air traffic, cause accidents, and compromise security. Therefore, developing an intelligent system capable of intercepting dynamic flying targets is crucial for maintaining operational integrity and safety. This problem is particularly significant in the context of advancing urban air mobility, where both piloted and autonomous aerial vehicles need to coexist safely.\nThe goal is to develop an intelligent system capable of intercepting a dynamic flying target as quickly as possible, anticipating the target's movements regardless of its evading actions. We refer to the agent as the pursuer and the dynamic target as the evader. Our objective is to train the pursuer to complete this task using Reinforcement Learning.\nThe result of training the agent is the trained pursuer's policy, \\(\\pi_{\\rho}(a_t|s_t)\\), which serves as a controller, mapping the dynamics of the pursuer and the position of the evader to the optimal actions for the control surfaces and motors."}, {"title": "2.2 Simulation Environment", "content": "We utilize the JSBSim Flight Dynamics Model (FDM) as our physics simulator. JSBSim is an open-source software written in C++ and is considered a high-fidelity simulator for the dynamics of fixed-wing air vehicles.\nJSBSim simulates the dynamics of an aircraft using a non-linear flight dynamics model based on the differential system of the form \\(\\dot{x} = f(x, u)\\). The equations governing this system are shown in Equations 1 to 4, where m is the mass of the aircraft, and I is the moment of inertia around each axis. X, Y, and Z are the total forces on the x-axis, y-axis, and z-axis of the plane, respectively, expressed in the body frame. L, M, and N represent moments similarly.\nSolving this system of equations allows the calculation of the aircraft's state, given by its position (\\(x_e, y_e, z_e\\)), velocity (u, v, w), Euler angles (\\(\\phi, \\theta, \\psi\\)), and angular rates (p, q, r). For this purpose, JSBSim uses numerical integrators such as Runge-Kutta or Adams-Bashforth.\n\n\n\n\n\n\n\n\n\n\nIn addition to these fixed-wing aircraft equations, JSBSim also models atmospheric disturbances, ground reactions, geodetic modeling, and rotational effects, among others, to enhance simulation realism. Of particular relevance to this work is the turbulence/gusts model used by JSBSim, the Dryden Gusts or Dryden Wind Turbulence model. This model is commonly used in aeronautics due to its suitability for representing atmospheric turbulence effects on aircraft.\nThe Dryden model equations are given by:\n\n(5)\n\n(6)\n\n(7)\nIn these equations, \\(\\Phi_u(\\omega)\\), \\(\\Phi_v(\\omega)\\), and \\(\\Phi_w(\\omega)\\) are the power spectral densities for the longitudinal, lateral, and vertical turbulence components, respectively, where w is the frequency; \\(\\sigma_u\\), \\(\\sigma_v\\), and \\(\\sigma_w\\) are the root mean square (RMS) turbulence velocities, and \\(L_u\\), \\(L_v\\), and \\(L_w\\) are the scale lengths for these components."}, {"title": "2.3 Reinforcement Learning Algorithms Overview", "content": "Reinforcement Learning (RL) is one of the three primary Machine Learning (ML) paradigms, focused on maximizing cumulative reward through trial and error. Although there are various approaches within RL, they can be broadly categorized into model-free and model-based algorithms."}, {"title": "2.3.1 Model-free RL", "content": "Model-free RL is a widely used approach where the learning algorithm directly interacts with the environment to learn an optimal policy that maps observations to actions. This method does not involve learning a model of the environment's dynamics. One common family of model-free algorithms is the Actor-Critic (AC) methods, which combine the benefits of both value-based and policy-based approaches.\nIn Actor-Critic algorithms, the actor updates the policy \\(\\pi(a|s; \\theta)\\) based on feedback from the critic, which evaluates the action by estimating the value function \\(V(s; w)\\) or the action-value function \\(Q(s, a; w)\\). The objective is to minimize the loss function for the critic, such as:\n\n(9)\nand update the actor by optimizing the expected return:\n\n(10)\nDifferent algorithms apply diverse loss functions and update methods to improve either speed, sample efficiency, exploration capabilities, or allow continuous or discrete actions. Model-free algorithms can be further classified into on-policy and off-policy methods. On-policy methods, like PPO (Proximal Policy Optimization) [24], update the policy based on the actions actually taken, whereas off-policy methods, such as SAC (Soft Actor-Critic) [23] and TQC (Truncated Quantile Critics) [22], learn the value of the optimal policy independently of the agent's actions by using experiences from a replay buffer. The latter approach is more sample efficient, making these algorithms a better approach for continuous control tasks in some cases."}, {"title": "2.3.2 Model-based RL", "content": "Model-based RL, in contrast, involves learning a model of the environment's dynamics and exploiting it to select the best option. The model typically consists of a transition function \\(\\hat{T}(s'|s, a)\\) and a reward function \\(\\hat{R}(s, a)\\). By simulating the environment using this learned model, the agent can predict future states and rewards, allowing for more efficient learning of an actor network or planning.\nA prominent example of a model-based algorithm is Dreamer, specifically the most recent version DreamerV3, developed by Danijar Hafner [21]. Dreamer is capable of learning long-horizon behaviors purely by latent imagination, meaning that it learns an embedded representation of the real environment and uses this embedded representation to learn the optimal policy. Moreover, it is an off-policy algorithm, so it learns from previous experiences gathered with a different policy than the one the agent is trying to learn."}, {"title": "2.4 Markov Decision Process Formulation of the Problem", "content": "Reinforcement Learning relies on the Markov Decision Process (MDP) formulation, making it necessary to design the problem at hand as a sequence of states (S), actions (A), and rewards (R).\nWe define the problem as a fully observable MDP during training, assuming all necessary states are known and observable in the state space. This way, the agent can completely observe its own state and the necessary states of the target to determine its relative position."}, {"title": "2.4.1 State Space Formulation", "content": "The agent's state space, defined as S in the MDP, comprises all necessary observations to be aware of its attitude, velocity, and position, which define the complete state of an aircraft, as well as the observations needed to know the relative position and velocity of the target, allowing it to determine the target's movement."}, {"title": "2.4.2 Action Space Formulation", "content": "As mentioned in Section 2.2, the UAV model used in JSBSim for the simulations includes a Fly-By-Wire system model which maps the pilot's commanded inputs to the control surface deflections and throttle. Thus, the RL agent needs to take the same actions as a human pilot would. The action space is defined as follows:\n\\(A = [\\delta_e, \\delta_a, \\delta_r, \\delta_\\tau] \\in [-1,1]^3 \\times [0, 1]\\),"}, {"title": "2.4.3 Reward Function Definition", "content": "A goal-oriented reward function is defined for this problem, where the goal is the event when the dynamic target is reached by the agent, i.e., the agent is within a goal distance (\\(d_{goal}\\)) of the evader.\nGiven the problems of sample inefficiency derived from the goal-oriented reward-since the agent only receives a reward when the objective is reached a shaping reward/penalization is added to help the agent understand the task objective in fewer time steps. This shaping penalization function can be observed in Fig. 4 for different shaping parameters k. We found that using a parameter k = 1 improves the sample efficiency of agents during training for this task.\nAdditionally, a penalization is added when the pursuer UAV is below 2000 ft of altitude to avoid simulation errors in the FDM due to ground contact. This condition is also an episode termination condition, which will be detailed in the next section.\nFinally, the piecewise reward function for the pursuer agent is defined as shown in Equation 18.\n\n(18)"}, {"title": "2.4.4 Training Framework", "content": "Training is designed based on the Gymnasium framework architecture. The framework is divided into the simulation part and the task-related part. The simulation part is carried out by the Flight Dynamics Model, handling the transfer of information with the task-related part to provide the dynamic values calculated in the simulator. The task-related part handles all the training setup, episode initialization, evasion strategies of the evader, and the calls to the running FDM simulation of the evader, which is independent of the pursuer agent.\nAt the beginning of each episode, the training is set up as follows:\n\u2022 The target UAV is placed in a randomly sampled position given by its latitude (\\(\\phi\\)), longitude (\\(\\lambda\\)), and altitude (h).\n\u2022 The agent UAV, the pursuer, is placed randomly but depends on the previously placed evader's position. Table 2 shows the range of values added to the target's position to calculate the position of the pursuer. The latitude and longitude of the pursuer based on the distance and bearing to the evader are calculated using Equation 19.\n\u2022 At the start of the episode, an evasion strategy is randomly selected for the evader.\n\nWhere:\n\u2022 \\(\\phi_1\\): Target's latitude in radians.\n\u2022 \\(\\lambda_1\\): Target's longitude in radians.\n\u2022 \\(\\theta\\): Relative bearing in radians (direction to move in, measured clockwise from north).\n\u2022 d: Distance between the two UAVs.\n\u2022 R: Radius of the Earth.\nTo control the evader UAV, a policy trained through Reinforcement Learning is used as a controller. This policy maps from the concatenation of the aircraft's internal state and the goal state to the pilot-related commands as mentioned in Section 2.4.2 for the pursuer agent. This goal state is given by the commanded setpoint values in altitude (href), heading (\\(\\Psi_{ref}\\)), and airspeed (\\(V_{Tref}\\)), meaning that the objective of the controller policy is to minimize the deviations between the current values and the setpoint values. Fig. 5 showcases an example of how this evader control policy tracks the reference values for href, \\(\\Psi_{ref}\\), and \\(V_{Tref}\\) simultaneously.\nEvasion strategies are defined by controlling the dynamics of the evader through commands in these three reference values, href, \\(\\Psi_{ref}\\), and \\(V_{Tref}\\), partially based on the dynamics of the pursuer, even though both UAV dynamics run on different simulations.\nDuring training, the commanded setpoints which define the evasion maneuvers are selected based on two strategies.\nFirst evasion strategy: This evasion strategy is based on time, changing the commanded setpoints to the evader at a fixed number of time steps. For the particular case of the training in this work, the commanded references to the evader change every 50 time steps. Commanded heading (\\(\\Psi_{ref}\\)) and altitude (href) are chosen based on the corresponding values of the pursuer at that time step, adding a given quantity randomly sampled from a range of possible values. The commanded airspeed (\\(V_{Tref}\\)) is independent of the pursuer's airspeed and is also randomly sampled from a range.\nSecond evasion strategy: Unlike the first strategy, this one is based on the distance to the pursuer, producing a change in the commanded values when the pursuer is closer than a limit distance to the evader. In this work, this distance is chosen to be 2 km. This way, the evader UAV's dynamics are changed at every time step while the pursuer is in the distance range. Commanded values are selected in the same way as in the first strategy. Table 4 shows the range of values that can be added to the pursuer's dynamics to define the commanded values for both strategies.\nTraining is set up with the following configuration:\n\u2022 The maximum length of episodes is 2000 steps.\n\u2022 The simulation integration frequency is 60 Hz.\n\u2022 The control frequency is 12 Hz (5 simulator steps per agent action).\n\u2022 The maximum training budget is 1M steps for each algorithm.\nEpisodes termination conditions are defined as:\n\u2022 Episode is truncated at the maximum length of 2000 steps, without penalization.\n\u2022 Episode is terminated with penalization if the agent is lower than 2000 ft, to avoid ground contact errors in the simulation.\n\u2022 Episode is terminated with reward if the target is caught."}, {"title": "2.4.5 Validation Scenarios", "content": "To assess the capabilities of the trained agents to act against unseen evader behaviors during training, two different evasion strategies are included during validation:\n\u2022 Random evasion: A randomly selected setpoints strategy is defined to test the agents against unpredictable and irrational target movements.\n\u2022 User-controlled evasion: During validation, the possibility for a user to control the commanded values in the evader through href, \\(\\Psi_{ref}\\), and \\(V_{Tref}\\) is defined to test the generalization capabilities of the trained agents to new evasion maneuvers.\nAdditionally, two different test scenarios are defined to evaluate the robustness of the trained agents:\n\u2022 Wind gust perturbations: Wind external perturbations are added during validation to test robustness, using the Dryden Gusts model defined in Section 2.2. The wind gust magnitudes introduced to the simulation can be seen in Table 5. These wind gusts directly affect the dynamics of the UAV calculated in the FDM, and the agent is unaware of these changes in the dynamics in a direct way.\n\u2022 Sensor noise perturbations: Sensor noise is added to the observations seen by the agent during validation to assess robustness against instabilities in the observations. The noise added for validation follows a Gaussian distribution with a mean of zero and standard deviation of 0.25 (N(0, 0.25)), considering that the observations are already normalized."}, {"title": "3 Results", "content": "In this section, we present the results obtained during this work, separating between the training results and the assessment of the trained algorithms during validation in different scenarios presented in Section 2.4.5."}, {"title": "3.1 Training Results", "content": "Our goal was to train and compare model-based and model-free RL algorithms, both in terms of performance and sample efficiency during training. To assess the latter, we set a predefined training budget both for hyperparameter tuning and for final training. These training budgets are 10M steps for hyperparameter tuning and 1M time steps for final training. All algorithms were trained using the same set of random seeds.\nWe consider an algorithm to have accomplished the task if, within the given budget, the agent is able to learn the final goal of catching the moving target before the episode ends. Given that the only positive reward is received upon catching the target, the theoretical maximum episode return is 2000, indicating that any episode return greater than zero means the target was caught.\nSeveral algorithms were tried during this work, including well-known RL algorithms such as PPO (Proximal Policy Optimization) [24], DDPG (Deep Deterministic Policy Gradient) [27], and TD3 (Twin Delayed DDPG) [28]. However, these three algorithms were unable to learn the task within the given budget (i.e., they did not surpass zero episode return), failing to find the correct set of hyperparameters to accomplish the task. This behavior is expected given that the task at hand is complex and goal-oriented, producing sparse rewards during training despite a shaping reward being added to help with sample efficiency. Considering that PPO is an on-policy method (and overall less sample efficient), and DDPG and TD3 are very sensitive to hyperparameters (a common problem in RL), it is expected that these algorithms could not be trained successfully under the given conditions and task.\nNevertheless, we found three algorithms that were able to accomplish the task under the given conditions. On the model-based side, DreamerV3 was chosen due to its excellent performance across a wide set of tasks, being state-of-the-art in some [21]. On the model-free side, SAC (Soft Actor-Critic) [23] and TQC (Truncated Quantile Critics) [22], which is inspired by SAC, were able to learn the task within the given budget.\nHyperparameter tuning was applied to both model-free algorithms, TQC and SAC. After tuning, the SAC algorithm was trained using both its own found hyperparameters and those from TQC, with better results observed using the latter. These hyperparameters are shown in Table 6. The model-based algorithm, DreamerV3, did not require hyperparameter tuning (as stated in the original paper), so default hyperparameters were used. For these hyperparameters, please refer to [21].\nIt should be noted that the SAC algorithm was not trained under the same conditions as DreamerV3 and TQC. Since SAC was unable to learn the task starting from the established training setup, a manual curriculum was added to SAC training, pre-training the algorithm for 500k time steps on a fixed line trajectory target strategy to facilitate learning. After that pre-training, regular training was applied as for the other two algorithms. This implies SAC was trained for a total of 1.5M time steps."}, {"title": "3.2 Validation Results", "content": "As mentioned in Section 2.4.5, the algorithms have been tested not only against the same evasion strategies used for the evader during training but also against two different strategies: completely random and human-controlled. A validation across all these circumstances was studied for the three algorithms, DreamerV3, TQC, and SAC.\nThis study consists of evaluating the performance of each algorithm for each evasion strategy separately, measuring the performance in mean episode return and standard deviation across all of them. The same random seeds were used for the three algorithms to handle randomness in the environment. Evasion strategies were tested across 100 episodes.\nFig. 7 shows the results of the validation for each algorithm across all evasion strategies, showing performance both in mean and standard deviation. As can be seen in the results, Dreamer and TQC achieve similar performance in scenarios 1 and 2, which are the training evasion scenarios, with Dreamer showcasing more stability in the decision-making process given the lower standard deviation. Both algorithms obtain excellent results in these scenarios, considering that the maximum possible reward is 2000, as mentioned before.\nHowever, when the algorithms are tested using the two validation evasion strategies, scenarios 3 and 4, it can be seen how TQC reduces its performance and increases its standard deviation across episodes considerably, whereas Dreamer is consistent with its performance, achieving similar results as for the training strategies. This showcases Dreamer's ability to generalize well to different target behaviors beyond those seen during training. We assume that the imagination-based training framework and latent state-space of Dreamer enable it to form a more robust representation and prediction of the target.\nThe SAC algorithm, on the other hand, shows far worse results than Dreamer and TQC, being unable to catch the evader with a random strategy and only able to reach the target inefficiently for the other scenarios.\nIn Fig. 8, an example of an episode trajectory can be seen, comparing the trajectory of the Dreamer agent with the TQC agent. In this case, the evader has no evasion strategy, resulting in a fixed line trajectory, which was not seen during training either. This example shows the same episode for both agents, starting from the same initial conditions. Fig. 8a\nshows the trajectory of both agents and the target in three dimensions, represented by latitude, longitude, and altitude. Dreamer's trajectory is shown in green, with the interception point marked in red. For TQC, the agent's trajectory is shown in blue and the target's in black. It can be seen that Dreamer catches the target faster despite departing from the same point, showcasing a more efficient sequence of decisions in controlling the aircraft to reach the goal. Fig. 8b shows the same trajectories in two dimensions, with altitude represented as a color map. Here, it is clear that the trajectory generated by the Dreamer agent's control is more optimal than that of the TQC agent.\nIn Figs. 8c and 8d, another episode example is displayed in a similar manner, but in this case, the evader follows a random evasion strategy (scenario 3). Initial conditions and the evader's actions and trajectory are the same for both algorithms (since in this case, the evader's decisions do not rely on the pursuer's position). The red point marks the intersection point of Dreamer with the target, and the black point marks that of TQC. In this case, Dreamer catches"}, {"title": "3.2.1 Robustness Validation Against Perturbations", "content": "Algorithms have been tested against wind-gust perturbations to assess robustness. This validation was done following the setup explained in Section 2.4.5, but in this case, only the Dreamer and TQC agents were studied, given the poor results of SAC in validation under no perturbation conditions.\nSimilar to before, validation was done across all scenarios for each algorithm. Fig. 10a shows the results of the study against wind for the Dreamer and TQC algorithms across each scenario. These results show that both algorithms have similar performance on average, maintaining results comparable to the cases without wind perturbations. However, whereas Dreamer is stable across episodes, as seen in the standard deviation results, TQC increases its variance considerably compared to validation without wind gusts.\nAdditionally, the same study was done but evaluating agents' responses against sensor noise instead of wind. This sensor noise is represented as Gaussian noise in the observations, adding a zero-mean Gaussian noise with a 25% standard deviation for each value in the observation, which translates to high values of noise added in some cases. This is a particularly hard test for reinforcement learning algorithms trained in a fully observable MDP, given that the noise converts the process into a POMDP (Partially Observable Markov Decision Process) [29]. Fig. 10b shows the results of this study, again showcasing that the Dreamer agent seems to be more robust against perturbations, being able to reach the target for all evasion strategies, although with reduced performance compared to before. The TQC agent's performance is reduced compared to the other studies, especially for random evasion and human-controlled evasion, being unable to capture the evader when it uses a random evasion strategy. The latent state space of Dreamer, which is composed of current observation and recurrent observations, allows it to handle POMDP.\nFinally, both test perturbations were combined to assess the agent in these particular situations. Table 7 provides a summary of all the results obtained during the studies.\nGiven these results, we can assume Dreamer is a more robust algorithm than TQC for these particular scenarios, showcasing stable behavior under perturbations and generalizing to other target behaviors.\nIn Fig. 11, a comparison of the trajectories followed by the Dreamer agent for the cases with wind-gust and sensor-noise perturbations and without perturbations is shown. The target is following a fixed line trajectory. The green line shows"}, {"title": "4 Conclusions", "content": "This paper presents a comprehensive study on training fixed-wing UAV pursuer agents using Reinforcement Learning (RL) to intercept dynamic evader targets. We compared the performance of model-based and model-free RL algo-rithms, specifically DreamerV3, TQC, and SAC, under a variety of scenarios, including unseen evasion strategies and perturbations such as wind gusts and sensor noise."}]}