{"title": "Visions of Destruction: Exploring a Potential of Generative AI in Interactive Art", "authors": ["Mar Canet Sola", "Varvara Guljajeva"], "abstract": "This paper explores the potential of generative Al within interactive art, employing a practice-based research approach. It presents the interactive artwork \"Visions of Destruction\" as a detailed case study, highlighting its innovative use of generative AI to create a dynamic, audience-responsive experience. This artwork applies gaze-based interaction to dynamically alter digital landscapes, symbolizing the impact of human activities on the environment by generating contemporary collages created with AI, trained on data about human damage to nature, and guided by audience interaction. The transformation of pristine natural scenes into human-made and industrialized landscapes through viewer interaction serves as a stark reminder of environmental degradation. The paper thoroughly explores the technical challenges and artistic innovations involved in creating such an interactive art installation, emphasizing the potential of generative AI to revolutionize artistic expression, audience engagement, and especially the opportunities for the interactive art field. It offers insights into the conceptual framework behind the artwork, aiming to evoke a deeper understanding and reflection on the Anthropocene era and human-induced climate change. This study contributes significantly to the field of creative AI and interactive art, blending technology and environmental consciousness in a compelling, thought-provoking manner.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid advancements in deep learning (DL) have significantly broadened the capabilities of generative AI, which radically transformed the creative methodologies used by creators in conceptualizing ideas and transitioning them into production [10]. GANS, first introduced in 2014 [15], initially produced outputs with limited diversity, resembling their training datasets. Our previous practice-based research confirms this statement and also underlines that not harmonious datasets produce rather abstract results [19].\nThis limitation was partly due to the use of smaller dataset sizes. For instance, StyleGAN3 [27] was released in late 2021 with three pre-trained datasets, each featuring different content types: the\nFFHQ dataset includes 70,000 high-quality images for human faces,\nAFHQv2 1 offers around 12,200 diverse animal face images, and\nMetFaces 2, designed explicitly for artistic face generation, contains\na collection of only 1,336 images. These datasets are still consid-erably smaller compared to those used by diffusion models. For example, Stable Diffusion released in 2022 was trained on the extensive LAION-5B dataset 3, which comprises over 5 billion images with mixed content type, showcasing the evolution in dataset scale for generative models. However, advancements in GAN architectures, such as StyleGAN-XL [37] and StyleGAN-T [36], demonstrate significant improvements and show that the model also can work with large diverse datasets like ImageNet\u2074 of 14 million. Moreover, advancements in methods for navigating the latent space of AI models, such as image-to-text techniques [29, 34] and image-to-image inpainting methods [35] using diffusion models, bring new opportunities for interactive art. These innovations enable more precise and diverse manipulations within the latent space for image generation, enhancing the creative potential and application scope of generative models.\nThe study by Miyazaki et al. explores public perception towards generative AI, particularly in creative professions [30]. According to their findings, artists and other creative professionals often harbor negative sentiments towards generative AI technologies [30]. This apprehension predominantly stems from concerns regarding the training datasets used by these Al systems, which frequently contain copyrighted material from artists [23] and other creatives. Such issues raise important ethical and legal questions about the sourcing and use of data in AI development, along with potential harms to artists [26]. However, it is noteworthy that many artists have embraced DL tools within their creative workflows for content generation [8]. Concurrently, the concept of collaborative co-creation with AI tools is making inroads into educational settings [11].\nThis article aims to shed light on the exciting opportunities that generative Al presents within the field of interactive art, especially for alternative forms of real-time content generation guided by the users' interactions"}, {"title": "1.1 Generative AI and Interactivity", "content": "Advancements in AI, particularly in the domain of computer vision, have significantly revolutionized the field of interactive art. This genre, which dynamically responds to the participant's movements and actions, traditionally relied on interfaces comprising cameras, lights and crafting custom-made software. However, the integration of DL techniques has simplified and enhanced the tracking mechanisms in these installations. Previously, these systems required complex and expensive setups with precise calibrations, often utilizing specialized equipment like infrared cameras with controlled lighting, thermal cameras, or depth cameras to accurately capture body silhouettes or detect specific body parts such as the face, hands, or eyes. DL models trained on large datasets have streamlined these processes, simplifying the setups in many cases to standard RGB webcams, reducing the complexity of calibration and improving the tracking accuracy. This evolution makes the technology more accessible to artists and opens new avenues for creative expression, where the interaction between the participant and the artwork becomes more fluid and natural.\nMoreover, the advent of embedding techniques like ResNet (short for Residual Network) [22] and OpenAI's CLIP (Contrastive Language-Image Pre-training) [32] has added depth and sophistication to the interactions possible in interactive art. Embeddings represent unstructured, high-dimensional data-such as images, audio, or text-in numeric form within a lower-dimensional space, typically a matrix. This process allows for a more nuanced understanding of media, facilitating tasks such as image processing and extracting contextual information.\nIn the context of interactive art, these techniques enable a more detailed and sensitive interpretation of participants' actions and behaviors. For instance, face embeddings, which encode facial features into a compact numerical representation, can be employed in various sophisticated ways. Additionally, these embeddings can adeptly interpret different modalities like speech-to-text inputs or various camera inputs. When used in interactive art, such capabilities contribute to new forms of interaction.\nAs an example, CLIP can be used to create installations that respond to user input as text by finding semantically meaningful images through embeddings generated for both text and images. This strategy was employed in the interactive art installation \"Dream-painter\" (2021) by Varvara & Mar [4], which used CLIP to transform speech input from the audience into text and create interpretations of their dreams drawn by a robotic arm. As this example demonstrates, the technology enriches audience interaction and blurs the lines between the artwork and the audience, fostering a unique, co-creative experience where the artwork evolves in real-time in response to the viewer's input."}, {"title": "1.2 Reference Artworks", "content": "This research delves deeper into the realm of interactive art, particularly highlighting the advent of generative Al models in this field. A pivotal moment in this evolution was marked by the introduction of the GAN-based pix2pix model [24] in 2017. Pix2pix showcased an innovative image-to-image translation interface, fundamentally transforming the landscape of interactive installations. It enabled real-time transformation of images, whether captured by a camera or drawn by users, into distinct and creative outputs. Despite being limited to lower resolutions and achieving only modest frame rates, primarily due to the GPU capabilities available then, pix2pix represented a significant breakthrough in the application of interactive AI [1]. This development demonstrated the potential of DL in artistic contexts while also opening new possibilities for artists and creators to explore real-time, Al-driven interactivity in their work.\nA known example of using the pix2pix model in an interactive artwork is 'Learning to See' (2017) by Memo Akten [1]. To be more precise, this work can be described as a closed-circuit installation that \"self-feeds and self-imagines\" [41]: the real-time camera image composed of everyday objects manipulable by the audience is transformed into an abstract landscape by using generative AI.\nAnother relevant artwork is Circuit Training (2019) by Mario Klingemann 5. This installation, based on GAN architecture, features a capture station, a screen that visualizes the dataset, and a curation system equipped with small touch screens. These screens are used to gather audience feedback on the outputs, enabling the system to learn and adapt to the style most favored by visitors. Participants wishing to be included in the training dataset of the piece must first give their consent by pressing a button on the touchscreen. This work is particularly fascinating as it unveils to the visitors the various components of AI architecture and exposes the often-unseen efforts required to construct Al systems, allowing spectators to engage as co-creators.\nThe \"AI Portraits PRO\" (2019) by Mauro Martino 6 in collaboration with Luca Stornaiuolo was an experimental interactive art net-art piece. Using a GAN trained with a dataset of 45,000 Renascence portraits from museum collections allowed users to transform themselves into classical art portraits through a website using a webcam [39]. Since the algorithm was trained on Renaissance portraiture, which has its own stylistic and cultural biases, the work did not accurately represent modern or diverse facial features and expressions. For instance, smiling or laughing faces were not rendered correctly, reflecting the historical bias in portrait styles where such expressions were uncommon. Due to its immense popularity and the ensuing discussions about its biased outputs related to the dataset, the app was only available for a short span of a few days. This interactive piece holds significant relevance in demonstrating how an interactive system can illuminate new dimensions of meaning within a vast corpus of art through novel forms of inquiry and highlights the broader issue of bias in Al systems, where the characteristics of the training data heavily influence the output. Additionally, the work succeeded in actively engaging the public in a unique experience to explore, in this instance, the classical portraits housed in museums.\nDream Painter (2021) is an interactive robotic art installation by Varvara & Mar that skillfully connects robotics and interface design to generative Al and lets the audience experience real-time navigation in the latent space of the model by sharing a dream in voice [4, 18]. The sketch produced is interesting because it also allows the audience to complete the interpretation since it is between abstract and figurative, allowing a more open range of meanings from the person who interacted and also from the others. The authors analyzed the corpus of outputs from an exhibition in their study. They conducted an interesting classification of the results, focusing on the interactions between the participants and the algorithm. This analysis helped to understand how the models influenced the aesthetic qualities of the outputs [21].\nIn addition, there are other interactive artworks that apply generative AI, such as 'Ray' (2021) by Zhang Weidi\u2077, and 'Unsupervised' (2022) by Refik Anadol\u2078. The latter does not demonstrate direct audience interaction but rather interaction with the system itself [16]. As highlighted previously, the artworks that deploy co-creative generative AI tend to produce more meaningful results than closed"}, {"title": "1.3 Historical and theoretical overview of the gaze", "content": "The concept of \"the gaze\" has been a pivotal subject in critical, historical, and cultural theoretical discussions, particularly in the fields of art, literature, psychology, and human-computer interaction (HCI). Historically, the gaze has been explored extensively in the works of theorists such as Michel Foucault and Laura Mulvey. Foucault's analysis of the gaze in \"Discipline and Punish\" highlights how visual observation serves as a means of exerting power and control [13]. Mulvey's seminal essay \"Visual Pleasure and Narrative Cinema\" introduces the idea of the male gaze as a critique of commercial films, explaining how visual media often frames women as objects for male pleasure [31]. This theoretical framework underscores the power dynamics inherent in the act of looking and being looked at, revealing how the gaze can reinforce societal structures and gender roles.\nCulturally, the gaze has been interpreted in various ways across different societies and eras. In Western art, for instance, the portrayal of subjects gazing directly at the viewer often served to establish a connection or assert dominance, while in Eastern art, the gaze might convey different symbolic meanings. These cultural interpretations of the gaze influence how viewers engage with and interpret visual works, making it a critical element in the creation and reception of art [3].\nFrom a scientific perspective, studying perception, vision, and gaze involves understanding the complex processes through which we interpret visual stimuli. Saccades, rapid movements of the eye that occur as we shift our focus from one point to another, play a crucial role in how we perceive our surroundings [6]. These eye movements are not random but are guided by our cognitive processes and prior knowledge [12]. Research in vision science reveals that saccades help gather detailed visual information, which the brain then processes to form a coherent understanding of the environment [14].\nIn HCI domain, eye-tracking technology has become a vital tool for understanding user behavior and optimizing interface design. Eye-tracking studies provide insights into how users interact with digital interfaces, revealing patterns of attention and focus. For example, Jacob and Karn's comprehensive review on eye-tracking in HCI outlines its applications and the valuable data it provides for improving user experience [25]. Duchowski's extensive work on eye-tracking methodology and its applications in HCI further emphasizes its importance in understanding user interaction and enhancing the usability of interfaces [9]. By leveraging eye-tracking technology, researchers can design interfaces that align more closely with natural human visual behaviors, ultimately improving the efficiency and meaningfulness of user interactions [7].\nIntegrating the critical, historical, and cultural discussions of the gaze with the scientific background of perception and vision, as well as the practical applications of eye-tracking in HCI, provides a comprehensive framework for understanding the impact of the gaze in visual art and digital interfaces. This approach not only highlights the theoretical significance of the gaze but also grounds it in empirical research and practical applications, offering a deeper insight into the viewer's experience and the thematic depth of the work."}, {"title": "2 VISION OF DESTRUCTION", "content": "\"Visions of Destruction\" is an immersive, interactive digital artwork that encapsulates human activities' transformative and often detrimental impact on Earth's natural landscapes [5]. This piece serves as a poignant commentary on the Anthropocene - the current geological age, viewed as the period during which human activity has dominated climate and the environment. The artwork begins by displaying a series of breathtakingly beautiful natural landscapes, epitomizing the pristine and unblemished state of nature before significant human interference. These landscapes are vivid, dynamic representations of various ecosystems - from lush forests and tranquil lakes to majestic mountains and expansive deserts, all untouched by civilization. As soon as a spectator engages with the artwork by directing their gaze towards it, the scenery begins to transform. This change is gradual yet inexorable, introducing artificial elements into the natural setting. What starts as a small path may slowly morph into a sprawling road and then into a bustling cityscape. Tranquil rivers may turn into industrial canals, and serene skies may become clouded with the smoke of factories. This metamorphosis is a powerful symbol of the degradation of nature due to industrialization and environmental catastrophe.\nThe transformation is driven by the audience's interaction. The direction of their gaze acts as a catalyst for change, making each experience unique. By using gaze-tracking technology, the artwork creates a dynamic and responsive environment where the spectator's attention directly influences the evolution of the landscape."}, {"title": "2.1 Method", "content": "\"Visions of Destruction\" can be seen as a collage of generative AI. The spectator's gaze indicates where a change will appear on a digital canvas. The regions of gaze focus are constructed using aggregated spectator gaze data collected through high-resolution eye-tracking technology. For this purpose, we are utilizing the Tobii Eye Tracker 5 device \u00b9\u00b3, specifically designed for gaming and interactive experiences. The device operates effectively without requiring individual calibration for each user, making it highly suitable for art installations. The primary consideration is that users must be close to the screen and within its eye-tracking range. Due to varying heights among spectators, the system functions optimally when the audience is seated, reducing height differences by nearly half. Additionally, the use of adjustable chairs further enhances convenience and effectiveness. Eye-tracking is not affected by the room's light conditions since it operates using infrared light. The sensor is installed at the lower part of the screen. For a better understanding of the installation setup and its involved components.\nIt's noteworthy to note that glasses can interfere with the eye-tracking system, particularly when reflections occur. However, even with glasses, the system generally functions well, though there may be brief moments where tracking is momentarily lost. In this installation, if there are no spectators for a few seconds, the landscape automatically regenerates into a new one. The regeneration process in the installation serves as a metaphor, symbolizing how nature, given sufficient time, can also recover and heal from the damages inflicted upon it.\nThe artwork begins with an AI-created depiction of idyllic beautiful landscapes generated through Stable Diffusion [35], offering a romanticized view of nature based on collective human memories and trained on the LAION-5B dataset [5]. In a technical twist, the data from the eye-tracker is used to craft a mask that alters specific areas of the image where the audience's gaze lingers. This involves applying an array of artist-designed prompts about the climate emergency to perform inpainting with Stable Diffusion, such as \"mining\" or \"catastrophe\". The result is a dynamic experience for viewers witnessing a seamless morphing of nature as if the gaze is a weapon of mass destruction.\nThe system employs a recursive approach, periodically iterating over eye-tracking data to create evolving landscapes. This process incorporates aesthetic accidents as part of the piece's aesthetics, as some prompts yield unexpected outcomes. Initially, the landscapes undergo slower transformations due to drastic changes requested by the prompts, leading to more rapid changes with continued interaction. Technical challenges arose during development, especially related to image degradation occurring with repeated in-painting. The solution involved using a masked area for inpainting and merging only this part with the original image, leaving the rest"}, {"title": "3 DISCUSSION", "content": "In interactive AI art, the artist's role is pivotal in shaping the audience's experience, much like in other interactive art forms. However, a unique aspect of generative AI art is its inherent unpredictability. Artists must adapt to and incorporate the spontaneous nature of Al-generated content, viewing these unexpected outcomes not as limitations but as integral, dynamic components of artistic expression. This approach marks a distinct shift in art creation and experience, blending creativity with the unforeseen capabilities of Al to generate unique and individualized content.\nInteractive artworks utilizing Al require open-source models due to their real-time nature and the need for custom software, ideally running on a local machine. For instance, this project runs on a gaming machine with an NVIDIA RTX 4090. While APIs could be a viable alternative in some scenarios, they are subject to network lag times, usage fees, and the risks associated with the provider's changing commercial strategies, potentially leading to closure or restricted access. API request fees can lead to substantial costs, particularly in exhibitions with high visitor traffic, making it challenging for artists to manage, as we have observed in our experience with art institutions. From a preservation standpoint, it is more advantageous to have a system running entirely on one or more local machines using open-source libraries, ensuring long-term sustainability. Rafael Lozano-Hemmer highlights in his guide to the preservation of media art both the significance and the risks associated with using closed-source products that may eventually be discontinued, necessitating the re-engineering of artworks with open-source tools [28]."}, {"title": "4 CONCLUSIONS", "content": "In conclusion, the study effectively demonstrates the transformative power of generative AI in interactive art. Through the innovative use of gaze-tracking technology as an interface for AI, the artwork vividly symbolizes the environmental impacts of human activities. This research highlights the shift from traditional artistic methods to prompt-guided creation using diffusion models like Stable Diffusion, marking a significant evolution in interactive art. Furthermore, it underscores the importance of open-source models for real-time interaction and sustainability in art installations. The study contributes to a deeper understanding of AI's role in artistic expression and audience engagement, bridging technology and environmental awareness in a profound and impactful way."}]}