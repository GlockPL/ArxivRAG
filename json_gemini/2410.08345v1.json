{"title": "LARGE LEGISLATIVE MODELS: TOWARDS EFFICIENT AI POLICYMAKING IN ECONOMIC SIMULATIONS", "authors": ["Henry Gasztowtt", "Benjamin Smith", "Vincent Zhu", "Qinxun Bai", "Edwin Zhang"], "abstract": "The improvement of economic policymaking presents an opportunity for broad societal benefit, a notion that has inspired research towards AI-driven policymaking tools. Al policymaking holds the potential to surpass human performance through the ability to process data quickly at scale. However, existing RL-based methods exhibit sample inefficiency, and are further limited by an inability to flexibly incorporate nuanced information into their decision-making processes. Thus, we propose a novel method in which we instead utilize pre-trained Large Language Models (LLMs), as sample-efficient policymakers in socially complex multi-agent reinforcement learning (MARL) scenarios. We demonstrate significant efficiency gains, outperforming existing methods across three environments. Our code is available at https://github.com/hegasz/large-legislative-models.", "sections": [{"title": "1 INTRODUCTION", "content": "Economic policy-making is a field rife with uncertainty (Al-Thaqeb & Algharabali, 2019), high stakes (Persson & Tabellini, 2004), and complexity (Mueller, 2020). Human policy-makers are often faced with overwhelming amounts of data (Huh et al., 2018) and the influence of vested interests (Elliott, 1997), complicating effective and equitable decision-making. AI-driven tooling, with the ability to avoid self-centered bias and parse large amounts of data quickly, could offer significant assistance."}, {"title": "2 PRELIMINARIES", "content": "We follow Zhang et al. (2024) in using Stackelberg-Markov games as the foundational concept from which to construct formalizations of economic policy-making.\nDefinition 2.1. A Partially Observable Markov Game (POMG) M with n agents is a tuple $(S, A, T, r, \\Omega, O,\\gamma, \\mu_0)$, where S is a state space, $A = \\text{X}_{i \\in [n]} A_i$ is the joint action space, $T:S\\times A \\leftrightarrow \\triangle(S)$ is a stochastic transition function, $r : S \\times A \\rightarrow R^n$ is the reward function"}, {"title": "3 ANALYSIS OF PRIOR METHODS", "content": "In this section, we formalize AI policymaking as finding a Stackelberg-Markov equilibrium in order to perform a principled analysis of Zheng et al. (2022)'s AI Economist and its limitations through an ablation of their policy generator inputs. We additionally ablate MetaGrad Yang et al. (2022), a related method that differs only in the optimization process of the economic policy generator. Detailed explanations of AI Economist and MetaGrad can be found in subsection A.1 and subsection A.2 respectively.\nWe now begin by describing our generalized framework, where we model simulated environments as a POMG. For each principal action, we draw follower equilibria for the induced POMG from an oracle, an abstraction that allows for a clear theoretical separation of principal and follower learning problems."}, {"title": "4 EFFICIENT AI POLICYMAKING IN ECONOMIC SIMULATIONS", "content": "Motivated by our ablations, we propose a simpler method using Large Language Models as economic policymakers to directly output economic policies (e.g. tax rates). This also enables us to flexibly take in a wide variety of inputs beyond the current POMG state, of which in this paper we focus on contextualization and historical observations. We propose using the In-Context Learning (ICL) Dong et al. (2024) capabilities of the LLM to learn the optimal economic policy without updating any weights through leveraging contextualization and historical observation. We define contextualizations as natural language descriptions of the problem setting to which our method is applied (Sodhani et al., 2021). Historical data we define by extending induced POMGs $M^{\\phi}$ in Stackelberg-Markov_games to $(S, A^{\\phi}, T^{\\phi},r^{\\phi},\\Omega,\\Omega_p, O, O_p, \\gamma^{\\phi}, \\mu_0)$, augmented with a non-parameterizable principal observation space $\\Omega_p$ and stochastic observation function $O_p$, distinct from the follower observation space and observation function $O_F$ and $O_F$ respectively. At the end of an episode of $M^{\\phi}$, the principal POMG observation trajectory $t_p \\in \\Omega := \\bigcup_{t>0} \\Omega_p$ is summarized into \"historical observations\" to inform the next choice of principal action $\\phi'$. Note that this is in contrast to the AI Economist's use of historical observations to inform choices of $\\phi$ via the loss and generator network optimization step. For more detail on the extended POMG, please refer to Appendix D, where we provide detailed groundwork to assist future work in incorporating a larger variety of information into automated policy-making methods, which adds further detail to our above extension.\nOur LLM principal method, as illustrated in Figure 3, queries an LLM for an initial action choice conditioned on a contextualization of the problem setting and iteratively adds to this contextualization, appending successive action choices and their corresponding payoffs and historical observations. Additionally, to further increase the difficulty of our environments for GPT-40 mini, our strongest LLM, we occasionally include deliberate irrelevant information in contextualizations and historical observations received to test its ability to parse these effectively; the exact prompts we use are in Appendix H. Overall, the prompt structure for our method is as follows:\n<CONTEXT>\n<ACTION1><PAYOFF\u2081><HISTORICAL-OBSERVATION1>"}, {"title": "5 EXPERIMENTAL RESULTS", "content": "To demonstrate the effectiveness of our approach, we conduct experiments across three environments. Each environment was chosen to showcase social dilemmas where agents without external influence achieve low social welfare. We rigorously validate the following baselines: Zheng et al. (2022)'s AI Economist, Yang et al. (2022)'s MetaGrad, and three prevalent bandit algorithms: UCB (Auer et al., 2002), Thompson sampling (Thompson, 1933), and e-greedy (Sutton & Barto, 2018). Our LLM method is tested across two models: GPT-40 mini and Gemini-1.5 flash. In the following subsections, we evaluate our method against these baselines and explore the degree to which contextualizations affect performance. We had mixed success in ablating historical observations from our method; results and discussion are given in Appendix B. MetaGrad, and AI Economist were run with and without historical observations on all environments; we use each method's best performance in our main comparison in subsection 5.2, and ablate the effect of these in Appendix B. The LLM uses historical observations on all environments. subsection 5.3 ablates the effect of contextualization on the LLM's performance; we use contextualization in all final LLM experiments.\nConsistent with standard economic models (Smith, 1776), our aim is to assess policymaking methods in environments where agents are assumed to be rational. Therefore, in Harvest and Clean Up we use pre-trained follower agents reset each principal action as implementations of oracles, as set out in Algorithm 1, and across all environments we allow agents to converge within $M^{\\phi}$ over multiple episodes for each action. The latter serves to fine-tune agent policies and ensure the oracles do not violate Theorem 1 of Gerstgrasser & Parkes (2023). To fairly evaluate each method, we fix the number of agent convergence episodes performed for all methods on a per-environment basis. These numbers were chosen according to static tests as the minimal amount needed for agents to exhibit certain rational behaviors; namely, overharvesting under a free market environment in Harvest and Cleanup and successfully opening the door under the known global optimum incentives in CER. Principal actions $\\phi$ are then evaluated against converged agent policies in validation episodes; as our"}, {"title": "5.1 ENVIRONMENTS", "content": "We employ three environments for our main experimental results: Harvest, Clean Up, and Contextual Escape Room. We focus on Stackelberg-Markov games with easily-interpretable principal action spaces. In the following subsections, we give an brief overview of our each environment further detail can be found in subsection D.2.\nCommons Harvest Open (Harvest). In Harvest (Agapiou et al., 2023), the environment contains several patches of apples, and agents i \u2208 [7] are given reward +1 for harvesting an apple. Harvested apples regrow stochastically at a rate proportional to the number of nearby unharvested apples never regrowing once a patch has been exhausted. To sustainably accrue reward, agents must refrain from overharvesting and allow apples time to regrow. Without external influence, the environment succumbs to tragedy of the commons. In Harvest, the principal outputs a three-tiered tax rate to be applied to the reward signal of agents collecting apples, where the applicable tax rate is determined by the number of apples an agent has collected in the past 10 timesteps. Episodes are terminated after 1000 timesteps.\nClean Up. Clean Up (Jaques et al., 2019; Agapiou et al., 2023) also involves seven agents harvesting apples, but features a fundamentally different social dilemma. In this environment, a river builds pollution at a constant rate, and apples grow in a single, large patch at a rate inversely proportional to the river's pollution level. Note that apple regrowth is no longer affected by overharvesting. Agents can clean river pollution, but must leave the apple patch to do so. Harvesting apples yields a small intrinsic reward of +0.1 and cleaning a harsh penalty of -1. Under unmodified environmental rewards, agents remain in the apple patch without cleaning, even when pollution stops regrowth entirely. In this environment, the principal incentivizes three subsets of the agent action space, adjusting the reward signal for agents that harvest, clean, or do other actions. Episodes are terminated after 1000 timesteps.\nContextual Escape Room (CER). A (n, m, L) Contextual Escape Room environment is an extension of Yang et al. (2020)'s Escape Room environment. This simple environment has L + 2 states, consisting of L lever states, a door state, and a start state. Agents i \u2208 [n] begin an episode in the start state, and draw actions from a space equivalent to the state space \u2013 determining where they move to next. In each episode, one lever state l at random is chosen to be \u201cactivated\"; if m agents are at lever l, the door \"opens\" and all agents at the door state receive a reward of +10. Moving to any other state, regardless of the door's status, incurs a penalty of -1, unless the door is closed and agents have not moved since the last step, in which case they receive 0. The social dilemma is that self-interested agents have no reason to incur a penalty for visiting lever states, and thus without incentivization the door is never opened. A (n, m, L) CER environment has known global optimum of 10 \u00b7 (n \u2212 m) \u00b7 m total agent reward per step. We employ a (5, 2, 3) CER environment in which the principal observes the active lever and outputs five incentives to be added to the agent reward signal, corresponding to each state. Episodes are terminated after 5 timesteps."}, {"title": "5.2 COMPARISON OF METHODS", "content": "Here we present the direct comparison across all methods and baselines shown in Figure 4. Because of the high cost of LLM APIs for frontier models, we terminate LLM runs after several dozen steps of convergence \u2013 totaling 90 generated actions on Harvest, 60 on Clean Up, and 100 on CER. Note that the LLM and MetaGrad methods have continuous output spaces, whilst all others use discretized output spaces. We use a normal-inverse-gamma conjugate prior for Thompson Sampling initialised to high variance estimates, that in practice mean it begins by pulling a large proportion of arms, and initialise the UCB algorithm by one pull on each bandit arm following standard convention. The sharp increase in performance for UCB across several environments occurs at the end of this exploration phase. To allow for a reasonable comparison, therefore, we decrease the number of arms for these two methods by using a coarser discretization. This decreases the cost of initial exploration, and we verified experimentally that optimal arms were not removed. Details of the discretization used along with all other hyperparameters can be found in subsection E.3; in all cases, the discretization rate we use is either equal to or more generous than that used by the original AI Economist method.\nIn Table 1, we provide an analysis of the results pictured in Figure 4. We show superior sample efficiency on all three environments, converging an order of magnitude faster than AI Economist on Harvest and Cleanup, and two orders of magnitude faster on CER, with similar or greater payoffs."}, {"title": "5.3 CONTEXTUALIZATION ABLATION", "content": "In this section, we conduct an ablation on our method's performance with and without contextualization of the problem setting. In the contextualization case, the LLM received a detailed description of the environment and task at hand, including an explanation of what its outputs are used for. Without contextualization, the LLM is asked to produce outputs to maximize an unspecified function, relying purely on in-context learning. Results, shown in Figure 6, demonstrate that LLM performance significantly improves when contextualization is included - consistently across all environments and"}, {"title": "6 RELATED WORK", "content": "While the exploration of LLMs specifically concerning economic policymaking in MARL environments is limited beyond previously outlined methods, we now outline research within similar fields.\nContextual Attention-based Representation Learning (CARE) (Sodhani et al., 2021) identifies the difficulty of achieving strong multi-task performance for RL-based autonomous agents. Sod-hani et al. (2021) attribute this to an inability to leverage auxiliary information between tasks and propose the usage of a pretrained LM to encode task descriptions, used to condition the policy of an RL agent. CARE employs multiple encoders, each focusing on a specific component of a task. While CARE achieved impressive results in several robotic manipulation tasks, using an end-to-end LLM approach allows our method to leverage more fine-grained auxiliary information without being restrained by a set number of encoders.\nReward Design with Language Models (Kwon et al., 2023) attempts to mitigate the human expense of reward design within RL. Rather than directly providing expert demonstrations or a specific reward function, Kwon et al. (2023) provide a natural language description of desired agent behavior to an LLM. The LLM is then tasked with evaluating agent behavior and subsequently producing a reward signal. Unlike our work, rewards are reactively assigned based on agent action.\nSimulating Human Behavior aims to instantiate generative agents as realistic human proxies. One such approach (Park et al., 2023) does so in a sandbox environment, using extended LLMs with memory banks to store experiences unique to each agent. Vinitsky et al. (2023) introduces MARL environments with 'public sanctions', in which agents equipped with a novel architecture learn social norms and punish transgressors, facilitating agent cooperation in socially complex environments. Research towards this end indicates the possibility of future AI-policymaker tests that better evaluate real-world performance."}, {"title": "7 CONCLUSION", "content": "In this work, we standardize and evaluate existing methodologies in the field of AI-driven economic policy design. We further contribute a novel method, leveraging LLMs for a more generalizable and tractable approach that outperforms baselines. The relative infancy of this field engenders a lengthy research agenda; importantly, one such direction for future work is increasing the realism and complexity of the economic environments being tested. Although we show that prior methods are overly complex for existing environments and directly outputting a static tax rate is enough, in the real-world, a tax rate generator may be necessary and more complex environments should be developed to investigate this further."}, {"title": "7.1 LIMITATIONS", "content": "As simulated environments increase in complexity, prompts given to instantiations of LLM policymakers will presumably increase as well. This calls into question the scalability of prompt design, as well as the memory constraints imposed by the context windows of LLMs. Furthermore, we found in our experiments that LLMs can demonstrate a tendency to trap themselves in local minima, choosing to make small adjustments to past outputs rather than attempting a more dramatic shift."}, {"title": "7.2 ETHICS", "content": "Though we strongly believe that research in this field has the potential to benefit society broadly, we also realize that the development of these systems comes with risk. Though that this field is still years"}, {"title": "A METHOD OVERVIEW", "content": null}, {"title": "\u0391.1 \u0391\u0399 ECONOMIST", "content": "AI Economist is an application of deep policy-based model-free RL to the economic policymaking problem: the economic policymaker is modelled as an additional agent in the environment, at every timestep receiving an observation, sampling an action from a policy network and receiving a reward. The policy network, serving as an economic policy generator, has a separate discrete action head for each tax bracket, each a categorical distribution over a discretized output range with an additional NO-OP action. The economic policy generator network outputs at timesteps corresponding to the start of a tax period are set as tax rates in the environment, and all other actions are masked to NO-OPs. In GTB, rewards are given for each episode timestep as the difference in social welfare between that timestep and the last; in our environments, since only one action is reinforced per episode, we give each a reward according to the total social welfare achieved in that entire episode."}, {"title": "\u0391.2 \u039c\u0395\u03a4AGRAD", "content": "Yang et al. (2022)'s MetaGrad method train an economic policy generator with meta-gradients flowed back through player agent parameter updates. Their method is conceptually similar to MAML (Finn et al., 2017) and we describe it here. A detailed flowchart to supplement our explanation is shown in Figure 7.\nThe environment we ablate their method on is named Escape Room; this was the basis for our Contextual Escape Room, and is the same but with just one lever state and no \u201cactivated levers\". At each step, their principal, termed an Incentive Designer (ID), observes both the positions of all agents and their most recent joint action and decides an incentive for visiting each state at that timestep. This incentive is decided using a forward pass through a neural network, termed the incentive function. When the agents subsequently act, their base environmental rewards have the correct incentives added \u2013 and crucially these modified rewards maintain gradient flow back to the incentive function's parameters. The player agents act for one episode of the game collecting rewards, and then form a normal RL loss from these rewards \u2013 for example, a policy gradient loss or a PPO (Schulman et al., 2017) loss through advantage calculations that maintain gradient flow. This loss is therefore still a differentiable function of the incentive function parameters. At this point, for agent parameters $\\theta$, incentive function parameters $\\eta$ and trajectory $\\tau$, we have a loss $L(\\tau, \\theta, \\eta)$. We step the agent nets by SGD $\\theta' \\leftarrow \\theta\u2212 \u03b1\u2207L(\\tau, \u03b8, \u03b7)$ or more generally any update rule $\u03b8' \\leftarrow \u03b8 + f(\\tau, \u03b8, \u03b7)$. These updated agent policy networks \u2013 with differentiable parameters $\u03b8' (\u03b7)$ are used to collect a second trajectory $\\tau'$, this time with no gradient flow back through rewards to the incentive function parameters. In this trajectory, the ID receives a reward at each timestep according"}, {"title": "\u0391.3 \u039c\u0395\u03a4AGRAD PERFORMANCE IN OUR ENVIRONMENTS", "content": "Here, we discuss MetaGrad's shortcomings on our test environments. Figure 8 shows copies of our main results for Harvest and CER counting all environment timesteps, to demonstrate our choice of using validation episode timesteps does not disguise significantly better MetaGrad performance.\nIn CER, even with the lever indicator fixed for all episodes instead of varying randomly, MetaGrad failed to successfully incentivize agents to open the door. Consistently, the incentive function increased both the correct lever incentive and the door state incentive to their largest allowable values. Clearly, the meta-gradients carry a consistently positive signal for the door state \u2013 and in fact we found that MetaGrad also fails on the original Escape Room environment without an incentivization cost loss as used in the original paper. We managed to get MetaGrad to work on fixed-indicator CER by adding an incentivization cost and carefully tuning its learning rate to catch the door state incentive increase \u2013 that has a gradient of subtly smaller magnitude than that of the correct lever incentive - but when lever randomization was reintroduced all incentivization cost learning rates we tried lead to all incentives going to zero before MetaGrad could learn to adapt to the changing indicator.\nIn Clean Up, all hyperparameter sets we considered failed completely. Sigmoid final layer activations mean the incentive function initially outputs values around 1.5 for each of the three incentives \u2013 including the third \"other actions\" incentive. Under most learning rates, this third incentive was decreased too slowly, leading to agents freezing completely in the initial phase of training. Once agents have frozen and no longer collect any reward, differentiating through agent policy network updates carries little information back to incentive function parameters, and incentives thereon only minorly oscillate with no distinguishable pattern. All learning rates we tried that were large enough to avoid this issue lead to highly unstable learning.\nTo verify whether the issue was our use of a policy gradient loss, or the fact that we have principals acting just once per episode, we tried allowing MetaGrad to act at every timestep, with a full critic network and a PPO validation episode loss. This was not successful either. Overall, the hyperparameters \u2013 specifically the principal learning rate were very difficult to tune; perhaps we did not consider the right sets in our grid searches."}, {"title": "D ENVIRONMENT FORMALIZATIONS", "content": null}, {"title": "D.1 DEFINITION", "content": "To formalize efficiently-informed economic policymaking, we extend Stackelberg-Markov games to allow for principal actions to be conditioned on observations. This is separate to the effects of state-space observations on the outputs of principal actions within POMGs, and we thus term these outer-level observations macro-observations. We use these to condition principal action choice on summarized information from the previous episodes POMGs and contextualizations of problem settings, though the macro-observation space can be richer.\nDefinition D.1. A Hierarchical Observation Stackelberg-Markov Game (HOSMG) S = (\u03b7, \u039f, \u03a6, \u03c8, \u03a1, I, u, f) is a n-follower online Stackelberg-Markov Game, where O is the principal macro-observation space; I is the principal action space; \u03c8 : \u039f \u2194 is a principal policy; P : \u0424 \u2194 M\u00ba is a policy implementation map from principal actions & \u2208 \u03a6 to parameterized POMGs M$ = (S, A\u00ba,T$,r\u00ba,\u03a9,\u03a9\u03a1, \u039f, Op, \u03b3\u03a6, \u03bc\u03cc) augmented with a non-parameterizable principal observation space Op and stochastic observation function Op, distinct from the follower observation space and observation function OF and OF respectively; II = Xi\u2208[n] Ii is the followers' joint policy space; u = (ui)=0 holds the payoff functions ui : \u0424 \u00d7 \u220f \u21a6 R; f : \u2229\u266d \u2192 O maps POMG principal observation trajectories in Np := U\u00bf>0 p to macro-observations.\nPrincipal actions & are now produced by a policy & and macro-observations, and we augment induced POMGS M\u00ba with an additional observation space Op and stochastic observation function Op for the principal. At the end of each episode, a trajectory of principal POMG observations of \u2208 Op in M\u00ba can be mapped by f to a macro-observation in O, the domain of 4 \u2013 and thus the next choice of $ can be formally conditioned on a previous episode's events. Worked examples of this formalization are given in subsection D.2.\nFor an RL-based approach such as AI Economist, & is the optimizer step used to determine the next set of weights 4. The HOSMG formalization explicitly clarifies our view that these weights & should be chosen informed by macro-observations drawn from a rich space O including, but not limited to, summarized state-space observations from the previous episode's POMG and contextualizing game descriptions. Sodhani et al. (2021) discuss the latter in detail, but, overall, it is not clear how one would go about incorporating large amounts of data in potentially many modalities efficiently into gradient-based optimizer steps. We take a different approach entirely, but tackling this problem directly represents a potential avenue for future work."}, {"title": "D.2 ENVIRONMENT FORMALIZATIONS", "content": "Here, we formalize each of our environments as HOSMGs.\nHarvest. We augment this environment with a principal equipped with action space \u0424 = [0,1]3. Principal actions $ = (R1, R2, R3) are interpreted as three-tiered tax rates, corresponding to three tax brackets {[Tb, Tb+1] : b = 1,2} \u222a {[73,\u221e]}. Actions \u222e parameterize M = (S, A, T, r\u00ba, \u03a9\u0391, \u03a9\u03a1, \u039f\u0391, \u039f\u03c1, \u03b3, \u03bc\u03bf), $r(s,a) = r_{raw} (s, a) + \\frac{1}{\\Omega} (1- tax_i (s, a) + \\Sigma_{j=1}^3 tax_j (s, a))$, where $tax_i$ computes an amount of tax owed at each episode timestep by agent i, and we redistribute the total tax collected evenly. We tax the raw environmental reward collected by each agent at that timestep at a rate determined by their cumulative\u2074 raw reward in the past H steps: $taxi,t = a \\cdot R_{bracket(i,t)}^{raw}$, bracket (x) = \u2211b=13b \u00b7 1\u03c4b\u2264x3, where a is a scalar multiplier allowing rewards to be made negative. Principal payoff is calculated as the mean end-of-episode cumulative endowment over all agents, equivalently pre-tax or post-tax. The principal POMG observation function Op : S \u00d7 A \u2194 \u2206 (\u03a9p) deterministically maps states to the number of apples the environmental state they represent contains; f : \u03a9 \u2192 O for the LLM maps to the number of apples remaining at the end of a game, and for AI Economist and MetaGrad to trajectories of remaining apples.\nClean Up. We augment this environment following Yang et al. (2022), with a principal equipped with action space \u0424 = [0,2]3. Principal actions $ = (I1, I2, I3) are interpreted as incentives to be added at each timestep to the raw environmental rewards of agents that respectively: harvest an apple, perform a cleaning action, perform any other action. Actions & parameterize M$ = (S, A, T,r', \u03a9\u0391, \u03a9p, OA, OP, \u03b3, \u03bc\u03bf), where r(s, a) = raw (s, a) + I\u2081 \u00b7 1{a\u2081=\u201charvest\u201d} + I2 \u00b7 1{a\u2081=\u201cclean\u201d} + I3 \u00b7 1{a\u2081=\u201celse\u201d}. Principal payoff is the mean number of apples harvested during an episode. The principal POMG observation function Op : S \u00d7 A \u2192 \u2206 (\u03a9p) deterministically maps state-action pairs (s, a) to observations containing: the number of apples in the environmental state s represents; the number of harvesting actions \u2211i=11{a\u2081=\u201charvest\u201d} in joint action a; the number of cleaning actions \u2211i=11{a\u2081=\u201cclean\u201d} in joint action a. For the LLM, the function f : p \u2194 O then maps the two former to the total number of apples that regrew in an episode and the latter to the total number of cleaning actions that occurred in an episode. For AI Economist and MetaGrad, f maps to downsampled trajectories of these two quantities.\nCER. We augment this environment with a principal that incentivizes the action of visiting each state. Principal actions \u00a2 \u2208 ML\u00d7(L+2) ([0,5]) are matrices used to produce incentive sets (I1,..., IL+2) that are added to raw environmental rewards as in Clean Up above. The POMG state space S contains an indicator of which lever l is activated, and the set of L + 2 incentives to be used for a reward r\u00ba (s, a) is extracted from & as ef $. In practice, given this lever indicator is fixed throughout each episode, we can equivalently query the principal for the relevant set of incentives only by providing the lever incentive to be used in advance. Principal payoff is calculated as the agents' cumulative joint raw reward collected each episode, averaged over each timestep. The principal POMG observation function Op (s, a) = (sdoor, a) deterministically keeps joint actions and whether the door is open or closed in a state s; f : \u2229p \u2192 O then summarizes the door's status, the active lever, and the average number of agents moving to each state per timestep of an episode."}, {"title": "E EXPERIMENTAL SETUP", "content": null}, {"title": "E.1 AGENT ARCHITECTURES AND PRETRAINING", "content": "In Commons Harvest Open and Clean up, we train player agents with parameter sharing, PPO Schulman et al. (2017) and GAE Schulman et al. (2016). Agent actor and critic networks share three convolutional layers followed by one fully-connected layer, with separate single linear layer heads. Flattened output from the convolutional layers is concatenated to a one-hot player indicator and the vector of current tax rates / incentives before the actor and critic heads. Agents are pretrained and reset to their initial parameters at each principal step, after which they are allowed to converge"}, {"title": "E.2 PRINCIPAL ARCHITECTURES", "content": "AI Economist. For all environments, we use actor-critic MLP networks with two hidden layers, trained with PPO and GAE. The principal economic policy generator has a discrete action head for each tax bracket / incentive. In Commons Harvest Open and Clean Up we discretize these into 21 points with an additional no-op action, and in Contextual Escape Room only 6 for this method to remain tractable.\nMetaGrad. For all environments, the MetaGrad incentive function is a two layer MLP with a sigmoid final layer activation, scaled to the desired incentive range. Meta-gradients are flowed back from a policy gradient loss in validation episodes, using agent log-probabilities and principal rewards calculated as the difference in social welfare between each timestep. A running mean baseline is used to reduce variance.\nBandit algorithms. We discretize the tax rates and incentives to produce bandit arms. In CER, we use a separate set of arms for each lever indicator, and coarsen discretization to account for the increase in number of arms."}, {"title": "E.3 HYPERPARAMETERS", "content": "We used Cartesian grid searches to tune hyperparameters for each baseline. Agent hyperparameters, including learning rate, were chosen according to a grid search on static tests and fixed for all methods as part of our oracle abstraction \u2013 though we grid searched over agent learning rate for MetaGrad to attempt to improve its performance as much as possible. The hyperparameter ranges used for tuning are as follows. Each hyperparameter set was evaluated on 3 seeds, except e-greedy which was tested on 8."}, {"title": "\u0415.4 \u0421\u043e\u043cMONS HARVEST OPEN TAX MULTIPLIER", "content": "Our implementation of taxation in the Commons Harvest Open environment uses a fixed scalar multiplier a on each agent's due tax before taxation and redistribution. Agent i's final reward post-tax, writing b\u2081 as shorthand for the tax bracket agent i falls into, is:\n$T_{i,t}^{raw} = r_{i,t}^{raw} - \\alpha \\cdot R_{b_{i,t}} + \\frac{1}{\\Omega}  \\Sigma_{j=1}^7  \\Sigma_{i=1}^{N_{ag}}  \\alpha \\cdot R_{b_{i,t}}^{raw}$\nThe motivation for this scaling is that, due to tax redistribution and our environment containing only 7 player agents, even under maximal taxation [1, 1, 1] agents heavily overharvesting still receive a reward of + for collecting an apple. To allow the principals a more expressive range of reward modifiers, we scale collected taxes to allow for effective rewards to be made negative. We fixed this"}, {"title": "F COMPUTATIONAL RESOURCES", "content": "Experiments were primarily run using RTX 4090 GPUs sourced from RunPod. A small subset of experiments were run using a Titan Xp GPU and an RTX 3090 GPU. Throughout our research, we spent roughly $4,000 USD on RunPod, correlating to approximately 2,500 GPU hours, and $100 USD between the OpenAI and Gemini APIs."}, {"title": "G IMAGES OF THE ENVIRONMENTS", "content": "Images of our environments are shown in Figure 11, Figure 12 and Figure 13."}, {"title": "H PROMPTING", "content": "This section gives examples of prompt-response pairs across each model and each of our environments. For readability, we mark the initial context with blue, the history with green, and the query with brown. Experiments ablating contextualization use the same prompt for both GPT-40 mini and Gemini-1.5 flash."}, {"title": "H.1 GPT-40 MINI", "content": null}, {"title": "\u041d.1.1 \u0421\u043eMMONS HARVEST OPEN", "content": "Prompt:\nYou are overseeing agents in a MARL environment. These agents populate an environment that contains apples, and are rewarded when they harvest an apple. However, if all of the apples are harvested before they can regrow, the environment will be barren for the rest of the episode and the agents will be unable to increase their return. Without any tax rates, the agents overharvest, leaving none to regrow and getting a low mean return. You are charged with creating tax rates that will affect the reward signal for an agent harvesting an apple. Each 'taxed' reward will be redistributed evenly amongst all of the agents. Your goal is to maximize the mean return of the agents. The following history of your previous attempts tells you the attempt number, the three-tiered tax rate you previously generated for that attempt, and the approximate mean income that the tax rate resulted in, as well as other data that might be pertinent."}, {"title": "H.1.2 CLEAN UP", "content": "Prompt:\nYou are overseeing agents in a MARL environment. These agents populate an environment that contains apples, and receive a reward of 0.1 when they harvest an apple. There is also a river in the environment that builds pollution at a steady rate. The apples can regrow, but do so more slowly if the river is polluted. You will produce 3 modifiers to be implemented in an episode of this environment. Agents that harvest an apple will see a reward signal of (0.1 + modifier1), agents that clean the pollution will see a reward signal of (-1 + modifier2), and agents that do some other action will see a reward signal of (0 + modifier3). Your goal is to maximize the mean number of apples harvested."}, {"title": "H.1.3 CONTEXTUAL ESCAPE ROOM", "content": "Prompt:\nYou are overseeing agents within a MARL environment. There are 5 states in this environment three levers, door, and start. You will be providing 5 incentives, between 0 and 5, corresponding to each lever, the start, and the door.\nNote that agents are already given a +10 incentive for being at the door \u2013 but only if it opens. At the beginning of the episode, the door is closed and gives no rewards to the agents. Your goal is to maximize the total environmental rewards collected by the agents. The only thing in the environment that changes between episodes is the location of a light. Below, you are provided with a history of your attempts:"}, {"title": "H.2 GEMINI-1.5 FLASH", "content": "In the following section, the"}]}