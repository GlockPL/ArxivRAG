{"title": "Are you sure? Analysing Uncertainty Quantification Approaches for Real-world Speech Emotion Recognition", "authors": ["Oliver Schr\u00fcfer", "Manuel Milling", "Felix Burkhardt", "Florian Eyben", "Bj\u00f6rn Schuller"], "abstract": "Uncertainty Quantification (UQ) is an important building block for the reliable use of neural networks in real-world scenarios, as it can be a useful tool in identifying faulty predictions. Speech emotion recognition (SER) models can suffer from particularly many sources of uncertainty, such as the ambiguity of emotions, Out-of-Distribution (OOD) data or, in general, poor recording conditions. Reliable UQ methods are thus of particular interest as in many SER applications no prediction is better than a faulty prediction. While the effects of label ambiguity on uncertainty are well documented in the literature, we focus our work on an evaluation of UQ methods for SER under common challenges in real-world application, such as corrupted signals, and the absence of speech. We show that simple UQ methods can already give an indication of the uncertainty of a prediction and that training with additional OOD data can greatly improve the identification of such signals.", "sections": [{"title": "1. Introduction", "content": "Intelligent audio analysis is becoming more and more important in our lives, whether in voice assistant systems or assistive robots, e. g., in the care sector. While the field of automatic speech recognition (ASR) has seen major advances through Deep Learning (DL), enabling robust real-world applications, speech emotion recognition (SER) still sees some major challenges after more than two decades of research [1]. A particular challenge hereby is the ambiguity, subjectivity [2] or even untypical expression [3] of emotions and thus the lack of ground truth labels [4]. Even though a rough intuition of emotions can be considered common sense, a wide range of emotion definitions and an almost arbitrary granularity of emotions make it already challenging to compare models between datasets [5]. A common approach to mitigate these challenges, which we also apply in this work, is to model only a subset of the available emotions, which tend to be available in most datasets and have a high agreement amongst annotators.\nStill, robust cross-corpus SER models are hard to develop and they struggle when confronted with OOD data. Beyond this, the paralinguistic information necessary to detect emotions is easily corrupted by real-world challenges, such as poor recording conditions or background noise [6].\nIn line with the subjectivity of emotions, many application scenarios of emotional context do not require a pin-point accurate estimation of emotion. Instead, they rather follow the philosophy of no (or a neutral) emotion prediction being preferable to a faulty emotion prediction. An expressive example of this is the frequent discussion of confusion matrices when analysing SER performance, where for instance mistaking happiness for anger is considered worse than the confusion with neutral. SER systems could thus benefit vastly from knowing when an emotion prediction is accurate and when it is not. A key approach to put this idea into practice is the quantification of uncertainty.\nUncertainties have been studied extensively in the literature and can usually be divided into two or three different categories [7, 8]: aleatoric uncertainty, caused by uncertainty in the data, on the one hand and epistemic uncertainty (model uncertainty), as well as distributional uncertainty on the other hand. In the context of SER, aleatoric uncertainty arises for instance through the ambiguity of the emotions themselves or through corrupted data. Epistemic uncertainty arises from the model itself and describes the inability of the model to precisely model the underlying distribution. One common reason for this is a lack of variability in the training data [9], for instance unbalanced gender or age distribution or a mismatch of the language between training data and real-world application. Distributional uncertainty arises, as the name suggests, from OOD samples. In the case of SER, this could be emotions which were not included in the training set or recordings that do not contain speech at all, e. g., music, silence or ambient sounds. This is also often referred to as Out-of-Domain samples [10, 11], which for the sake of simplicity we will summarise with the mathematical term Out-of-Distribution under the abbreviation OOD.\nPopular methods aiming to quantify these effects of uncertainty on a sample level can generally be split into four different types: single deterministic methods, Bayesian methods, ensemble methods and test-time augmentation methods [8]. The latter three all rely on multiple forward passes through a network for an uncertainty estimation and thus add significant computational cost at inference time, making efficient deployment challenging. Single deterministic methods, however, only add minimal computational overhead and are thus promising candidates for real-time real-world uncertainty estimation in SER applications. Apart from being used directly by an application, UQ can also help to leverage more sophisticated training methods, such as active [12] or weakly-supervised learning.\nIn this work, we therefore aim to investigate the behaviour of several common (mostly single deterministic) UQ methods under simulated challenges of real-world SER scenarios, in particular when confronted with OOD data and different noise levels.\nTo our knowledge, this is the first systematic investigation to evaluate the most prominent UQ methods on multiple SER datasets under realistic challenges of real-world application."}, {"title": "2. Uncertainty Tests", "content": "In order to obtain a good picture of our SER models with respect to different contexts, we measure the models' behaviour with respect to four different uncertainty tests.\n2.1. Rater Agreement\nA rater agreement test is a straight-forward approach to evaluate aleatoric uncertainty in the context of SER [13]. It calculates the sample-level correlation between the uncertainty of the model and the level of agreement of different annotators during the labelling process. We thus obtain an impression of whether the sources of uncertainty are similar for a SER model and human annotators.\n2.2. Unknown Emotions\nFor uncertainty quantification in classification tasks, one of the main questions is, how the UQ method reacts to data points from classes not included during the training, in general aiming for higher uncertainties for unknown classes. Accordingly, we will test our SER models for samples with emotion labels that are unknown to the model. It is important to note, however, that the expressiveness of this test suffers from limitations in the SER case, as the ambiguity and potential overlaps of emotions may not be properly accounted for in the datasets[14].\n2.3. Non Speech Data\nA more clearly interpretable test is to expose the model to OOD data, which does not contain speech at all, such as \"background music\" or environmental sounds. In this case, it would be desirable for the UQ measures to have high scores, in order to avoid incorrect predictions. Most real-world SER applications apply Voice Activity Detection to prevent this type of faulty predictions, thus adding additional computational resources.\n2.4. Corrupted Signals\nOur final test aims to simulate bad recording quality, one of the major challenges of real-world applications. For that purpose, we augment data with varying levels of white noise or environmental sounds and investigate the interplay of model performance and uncertainty predictions for increasing levels of noise."}, {"title": "3. Uncertainty Quantification", "content": "With respect to the above discussed tests, we analyse four UQ methods, three of which belong to the category of single deterministic UQ methods, with only Monte Carlo (MC) Dropout needing several forward passes through the model.\n3.1. Entropy\nOne of the easiest ways to get insights about the uncertainty of a categorical task is to calculate the entropy [15] over logits. The entropy is calculated as H = \\sum_{i=1}^{N} p_i \\log(p_i) for N classes, where \\(p_i\\) is the probability to predict class i. We consider this method our baseline as it can be calculated a posteriori for each classification model trained with a standard softmax layer and (weighted) cross-entropy loss.\n3.2. MC Dropout\nMany State-of-the-art (SOTA) models like PANNS [16] or wav2vec2.0 [17] contain dropout layers to prevent overfitting during training. This again enables an out-of-the-box use of our second baseline, MC dropout [18], which works by reactivating the dropout layers during inference to obtain multiple outputs for the same input sample. The UQ is then calculated as the variance over the different outputs. For classification tasks, it is alternatively possible to use the mean probability per class to calculate the entropy, as in section 3.1.\n3.3. Evidential Deep Learning\nEvidential Deep Learning (EDL) is a more sophisticated approach of UQ and formulates learning as an evidence acquisition process [19, 20], where each of the training samples adds to an evidential distribution. It is based on the Dempster-Shafer theory of belief functions [21]. For application, Sensoy et al. [19] specify non-negative belief masses \\(b_k\\) for all K classes and an uncertainty mass \\(u\\), which all together add up to one\n\\[u + \\sum_{k=1}^{K} b_k = 1.\\]\nThe model outputs K values, describing the evidence per class \\(e_k\\). The evidences are thereby modelled by a Dirichlet distribution with the parameters \\(\\alpha = (\\alpha_1, .., \\alpha_K)\\), as \\(\\alpha_k = e_k + 1\\), which are then used to calculate the uncertainty and belief masses.\n3.4. Prior Networks\nAnother approach specifically tailored to detect OOD samples are Prior Networks (PN)s [22]. Similar to EDL, PNs formulate the ground truth via an underlying Dirichlet distribution (see Figure 1). This further allows for a distinction between data uncertainty and OOD uncertainty. By defining sharp distributions towards the respective classes, the network is trained for its classification task (Figure 1 a)). By specifying a flat distribution (Figure 1 c)) it is possible to train on OOD data without the need for an additional OOD class. For a 4-class-problem for instance, one would use, instead of the one hot encoding (1,0,0,0), a Dirichlet distribution with \\(\\alpha = (101, 1, 1, 1)\\) and for OOD data \\(\\alpha = (1, 1, 1, 1)\\) with the Kullback-Leibler (KL) divergence as the loss function. For PNs it is possible to differentiate between data uncertainty and OOD data by calculating the precision (sharpness) of the distribution. For the sake of simplicity, however, we will only refer to the combined uncertainty in this paper, which is once more calculated via the entropy."}, {"title": "4. Experiments", "content": "4.1. Model Architecture\nWe run the experiments for a SOTA wav2vec2.0 pre-trained model for SER as described in Wagner et al. [23] followed by a classification head. We use the first 11 transformer layers of the pre-trained 'facebook/wav2vec2-large-robust' model [24] followed by 2 dense layers with a tanh activation function as classification head, which adds up to 153M parameters.\n4.2. Datasets\nWe combine three datasets for categorical emotional speech, the two English datasets CRowd-sourced Emotional Mutimodal Ac-"}, {"title": "5. Conclusions and Outlook", "content": "We tested the behaviour of different UQ methods under realistic sources of uncertainty for SER applications, such as OOD emotions, OOD data or corrupted signals. We find that entropy-based UQ predictions on models with a standard cross-entropy training pipeline can already give good intuition on whether an emotion prediction can be considered trustworthy, thus reducing the importance of additional gatekeepers such as SNR models or voice activity detection systems. Nevertheless, the reliability of UQ can greatly be increased by incorporating the UQ already in the network design, in particular through Prior Networks and by exposing the model to OOD data during training. In this regard, further improvement might be expected through methods that artificially create the OOD data [32] or calibration [33]. However, a closer investigation about the separability of data uncertainty and OOD data through PNs and other UQs in the context of SER seems necessary."}]}