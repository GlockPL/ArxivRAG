{"title": "Are you sure? Analysing Uncertainty Quantification Approaches for Real-world Speech Emotion Recognition", "authors": ["Oliver Schr\u00fcfer", "Manuel Milling", "Felix Burkhardt", "Florian Eyben", "Bj\u00f6rn Schuller"], "abstract": "Uncertainty Quantification (UQ) is an important building block for the reliable use of neural networks in real-world scenarios, as it can be a useful tool in identifying faulty predictions. Speech emotion recognition (SER) models can suffer from particularly many sources of uncertainty, such as the ambiguity of emotions, Out-of-Distribution (OOD) data or, in general, poor recording conditions. Reliable UQ methods are thus of particular interest as in many SER applications no prediction is better than a faulty prediction. While the effects of label ambiguity on uncertainty are well documented in the literature, we focus our work on an evaluation of UQ methods for SER under common challenges in real-world application, such as corrupted signals, and the absence of speech. We show that simple UQ methods can already give an indication of the uncertainty of a prediction and that training with additional OOD data can greatly improve the identification of such signals.", "sections": [{"title": "1. Introduction", "content": "Intelligent audio analysis is becoming more and more important in our lives, whether in voice assistant systems or assistive robots, e. g., in the care sector. While the field of automatic speech recognition (ASR) has seen major advances through Deep Learning (DL), enabling robust real-world applications, speech emotion recognition (SER) still sees some major challenges after more than two decades of research [1]. A particular challenge hereby is the ambiguity, subjectivity [2] or even untypical expression [3] of emotions and thus the lack of ground truth labels [4]. Even though a rough intuition of emotions can be considered common sense, a wide range of emotion definitions and an almost arbitrary granularity of emotions make it already challenging to compare models between datasets [5]. A common approach to mitigate these challenges, which we also apply in this work, is to model only a subset of the available emotions, which tend to be available in most datasets and have a high agreement amongst annotators.\nStill, robust cross-corpus SER models are hard to develop and they struggle when confronted with OOD data. Beyond this, the paralinguistic information necessary to detect emotions is easily corrupted by real-world challenges, such as poor recording conditions or background noise [6].\nIn line with the subjectivity of emotions, many application scenarios of emotional context do not require a pin-point accurate estimation of emotion. Instead, they rather follow the philosophy of no (or a neutral) emotion prediction being preferable to a faulty emotion prediction. An expressive example of this is the frequent discussion of confusion matrices when analysing SER performance, where for instance mistaking happiness for anger is considered worse than the confusion with neutral. SER systems could thus benefit vastly from knowing when an emotion prediction is accurate and when it is not. A key approach to put this idea into practice is the quantification of uncertainty.\nUncertainties have been studied extensively in the literature and can usually be divided into two or three different categories [7, 8]: aleatoric uncertainty, caused by uncertainty in the data, on the one hand and epistemic uncertainty (model uncertainty), as well as distributional uncertainty on the other hand. In the context of SER, aleatoric uncertainty arises for instance through the ambiguity of the emotions themselves or through corrupted data. Epistemic uncertainty arises from the model itself and describes the inability of the model to precisely model the underlying distribution. One common reason for this is a lack of variability in the training data [9], for instance unbalanced gender or age distribution or a mismatch of the language between training data and real-world application. Distributional uncertainty arises, as the name suggests, from OOD samples. In the case of SER, this could be emotions which were not included in the training set or recordings that do not contain speech at all, e. g., music, silence or ambient sounds. This is also often referred to as Out-of-Domain samples [10, 11], which for the sake of simplicity we will summarise with the mathematical term Out-of-Distribution under the abbreviation OOD.\nPopular methods aiming to quantify these effects of uncertainty on a sample level can generally be split into four different types: single deterministic methods, Bayesian methods, ensemble methods and test-time augmentation methods [8]. The latter three all rely on multiple forward passes through a network for an uncertainty estimation and thus add significant computational cost at inference time, making efficient deployment challenging. Single deterministic methods, however, only add minimal computational overhead and are thus promising candidates for real-time real-world uncertainty estimation in SER applications. Apart from being used directly by an application, UQ can also help to leverage more sophisticated training methods, such as active [12] or weakly-supervised learning.\nIn this work, we therefore aim to investigate the behaviour of several common (mostly single deterministic) UQ methods under simulated challenges of real-world SER scenarios, in particular when confronted with OOD data and different noise levels.\nTo our knowledge, this is the first systematic investigation to evaluate the most prominent UQ methods on multiple SER datasets under realistic challenges of real-world application."}, {"title": "2. Uncertainty Tests", "content": "In order to obtain a good picture of our SER models with respect to different contexts, we measure the models' behaviour with respect to four different uncertainty tests.\n2.1. Rater Agreement\nA rater agreement test is a straight-forward approach to evaluate aleatoric uncertainty in the context of SER [13]. It calculates the sample-level correlation between the uncertainty of the model and the level of agreement of different annotators during the labelling process. We thus obtain an impression of whether the sources of uncertainty are similar for a SER model and human annotators.\n2.2. Unknown Emotions\nFor uncertainty quantification in classification tasks, one of the main questions is, how the UQ method reacts to data points from classes not included during the training, in general aiming for higher uncertainties for unknown classes. Accordingly, we will test our SER models for samples with emotion labels that are unknown to the model. It is important to note, however, that the expressiveness of this test suffers from limitations in the SER case, as the ambiguity and potential overlaps of emotions may not be properly accounted for in the datasets[14].\n2.3. Non Speech Data\nA more clearly interpretable test is to expose the model to OOD data, which does not contain speech at all, such as \"background music\" or environmental sounds. In this case, it would be desirable for the UQ measures to have high scores, in order to avoid incorrect predictions. Most real-world SER applications apply Voice Activity Detection to prevent this type of faulty predictions, thus adding additional computational resources.\n2.4. Corrupted Signals\nOur final test aims to simulate bad recording quality, one of the major challenges of real-world applications. For that purpose, we augment data with varying levels of white noise or environmental sounds and investigate the interplay of model performance and uncertainty predictions for increasing levels of noise."}, {"title": "3. Uncertainty Quantification", "content": "With respect to the above discussed tests, we analyse four UQ methods, three of which belong to the category of single deterministic UQ methods, with only Monte Carlo (MC) Dropout needing several forward passes through the model.\n3.1. Entropy\nOne of the easiest ways to get insights about the uncertainty of a categorical task is to calculate the entropy [15] over logits. The entropy is calculated as \\(H = \\sum_{i=1}^{N} P_i log(p_i)\\) for N classes, where \\(p_i\\) is the probability to predict class i. We consider this method our baseline as it can be calculated a posteriori for each classification model trained with a standard softmax layer and (weighted) cross-entropy loss.\n3.2. MC Dropout\nMany State-of-the-art (SOTA) models like PANNS [16] or wav2vec2.0 [17] contain dropout layers to prevent overfitting during training. This again enables an out-of-the-box use of our second baseline, MC dropout [18], which works by reactivating the dropout layers during inference to obtain multiple outputs for the same input sample. The UQ is then calculated as the variance over the different outputs. For classification tasks, it is alternatively possible to use the mean probability per class to calculate the entropy, as in section 3.1.\n3.3. Evidential Deep Learning\nEvidential Deep Learning (EDL) is a more sophisticated approach of UQ and formulates learning as an evidence acquisition process [19, 20], where each of the training samples adds to an evidential distribution. It is based on the Dempster-Shafer theory of belief functions [21]. For application, Sensoy et al. [19] specify non-negative belief masses \\(b_k\\) for all K classes and an uncertainty mass u, which all together add up to one\n\\(u + \\sum_{k=1}^{K} b_k = 1\\). The model outputs K values, describing the evidence per class \\(e_k\\). The evidences are thereby modelled by a Dirichlet distribution with the parameters \\(\\alpha = (\\alpha_1, .., \\alpha_K)\\), as \\(\\alpha_k = e_k + 1\\), which are then used to calculate the uncertainty and belief masses.\n3.4. Prior Networks\nAnother approach specifically tailored to detect OOD samples are Prior Networks (PN)s [22]. Similar to EDL, PNs formulate the ground truth via an underlying Dirichlet distribution (see Figure 1). This further allows for a distinction between data uncertainty and OOD uncertainty. By defining sharp distributions towards the respective classes, the network is trained for its classification task (Figure 1 a)). By specifying a flat distribution (Figure 1 c)) it is possible to train on OOD data without the need for an additional OOD class. For a 4-class-problem for instance, one would use, instead of the one hot encoding (1,0,0,0), a Dirichlet distribution with a = (101, 1, 1, 1) and for OOD data a = (1, 1, 1, 1) with the Kullback-Leibler (KL) divergence as the loss function. For PNs it is possible to differentiate between data uncertainty and OOD data by calculating the precision (sharpness) of the distribution. For the sake of simplicity, however, we will only refer to the combined uncertainty in this paper, which is once more calculated via the entropy."}, {"title": "4. Experiments", "content": "4.1. Model Architecture\nWe run the experiments for a SOTA wav2vec2.0 pre-trained model for SER as described in Wagner et al. [23] followed by a classification head. We use the first 11 transformer layers of the pre-trained 'facebook/wav2vec2-large-robust' model [24] followed by 2 dense layers with a tanh activation function as classification head, which adds up to 153M parameters.\n4.2. Datasets\nWe combine three datasets for categorical emotional speech, the two English datasets CRowd-sourced Emotional Mutimodal Ac-"}, {"title": "4.4. Correctness", "content": "The performances on the test sets are listed in Table 1. For MC Dropout, we use the mean over all forward passes. All models have comparable performance.\ndiscrepancy between correct and false predictions with the help of CDF plots, similar to [31] (see first row of Figure 2). Ideally, we would expect a considerably larger area under the curve (AUC) for the correct predictions versus the false predictions in the CDF graph. While to a certain extent true for all UQ, in particular, our baselines CE and MC show very small differences, thus apparently limiting their reliability to detect in-domain (ID) misclassifications."}, {"title": "4.5. Correlation with rater agreement", "content": "We further determine the Pearson Correlation Coefficient (PCC) between the UQ predicitons for all models and the utterance-level rater agreement over all three SER datasets. The latter can be calculated as the ratio percentage of raters that chose the majority class as their annotation (see Table 2).\nEven though a weak negative correlation can be observed in all cases, indicating that lower rater agreement also leads to higher uncertainty, the effect is minimal. A reason for the weak correlation might be that the model only has to distinguish between 4 emotions, while the raters had a larger amount of emotions to discriminate. Beyond, even samples with very high annotator disagreement are presented with a \"ground truth\" emotion during training, thus potentially not sensitising the model for rater disagreements."}, {"title": "4.6. Out-of-Distribution Emotions", "content": "Table 3 shows the mean uncertainty for emotions included during training vs the emotions omitted during training on the EmoDB test data. As expected, all methods have greater uncertainty for unseen emotions than for known ones."}, {"title": "4.7. Out-of-Domain Data", "content": "In order to test how the UQ behave under non-speech OOD data, we compare the CDF plots of the SER datasets to those of the other audio databases cochlscene (acoustic ambient noise), musan (speech/music/noise), and additionally, artificially generated \"white noise\" (45 samples from near silence with gradually increasing Gaussian white noise) in the second row of Figure 2. As the latter type of data is completely unfit for the SER task, we expect lower AUC for their graphs (red/grey) compared to those of the speech datasets (blue). At first glance, it stands out that PN(out) has by far the largest separation between ID and OOD data, as the speech and noise datasets are clearly separated in the CDF plot. A small separation can also be observed for EDL, while any of the other models' UQs fail to produce a visible distinction. It is important to note at this point that the PN(out) model is the only one that has been trained on some OOD due to its design allowing for this in a straightforward manner. However, it has only seen OOD data in the form of ambient noise from cochlscene, and it manages to generalise well to other types of OOD data. Regarding the individual databases, EmoDB has almost always the lowest uncertainty, which aligns with models showing the highest performance on it."}, {"title": "4.8. Sensor Uncertainty", "content": "Our final test aims to simulate sensor uncertainty by augmenting the EmoDB dataset with different levels of white noise, starting with an Signal to Noise Ratio (SNR) of 30 (original utterance is 30 dB louder than the noise) and increasing noise levels up to an SNR of -10. The last row of Figure 2 shows the mean uncertainty for correct and false predictions over the different SNRs. The plot also contains the UAR, which, as expected, gets progressively worse as the noise level increases. For an SNR of 30 dB, the uncertainties for the correct and false predictions are clearly separated for all UQ methods, even more prominently than in the top row, as we only evaluate on the 'easiest' dataset EmoDB.\nFor increasing noise levels, we would expect an increasing uncertainty for the correct predictions as more and more samples are correctly classified by accident, which we can observe to varying degrees for all methods. However, with (Cross) Entropy and MC Dropout, the uncertainty of the misclassified samples also decreases, which makes it impossible to reliably distinguish between false and correct predictions based on the UQ at this point. For EDL however, we see exactly the opposite behaviour, as all predictions become more uncertain with decreasing SNR, which is a promising behaviour in order to prevent faulty classifications based on UQ. Still, the fact that the uncertainty increases much quicker than UAR indicates that the UQ is too sensitive to noise. Interestingly, for EDL we see higher uncertainties for speech samples with added noise than for pure white noise in the middle row of Figure 2. The most ideal behaviour is shown by the two PNs with the uncertainty of false predictions initially remaining stable, but the uncertainty of correct predictions steadily increasing."}, {"title": "5. Conclusions and Outlook", "content": "We tested the behaviour of different UQ methods under realistic sources of uncertainty for SER applications, such as OOD emotions, OOD data or corrupted signals. We find that entropy-based UQ predictions on models with a standard cross-entropy training pipeline can already give good intuition on whether an emotion prediction can be considered trustworthy, thus reducing the importance of additional gatekeepers such as SNR models or voice activity detection systems. Nevertheless, the reliability of UQ can greatly be increased by incorporating the UQ already in the network design, in particular through Prior Networks and by exposing the model to OOD data during training. In this regard, further improvement might be expected through methods that artificially create the OOD data [32] or calibration [33]. However, a closer investigation about the separability of data uncertainty and OOD data through PNs and other UQs in the context of SER seems necessary."}]}