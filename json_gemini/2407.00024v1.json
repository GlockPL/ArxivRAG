{"title": "LMVD: A Large-Scale Multimodal Vlog Dataset for Depression Detection in the Wild", "authors": ["Lang He", "Kai Chen", "Junnan Zhao", "Yimeng Wang", "Ercheng Pei", "Haifeng Chen", "Jiewei Jiang", "Shiqing Zhang", "Jie Zhang", "Zhongmin Wang", "Tao He", "Prayag Tiwari"], "abstract": "Depression can significantly impact many aspects of an individual's life, including their personal and social functioning, academic and work performance, and overall quality of life. Many researchers within the field of affective computing are adopting deep learning technology to explore potential patterns related to the detection of depression. However, because of subjects' privacy protection concerns, that data in this area is still scarce, presenting a challenge for the deep discriminative models used in detecting depression. To navigate these obstacles, a large-scale multimodal vlog dataset (LMVD), for depression recognition in the wild is built. In LMVD, which has 1823 samples with 214 hours of the 1475 participants captured from four multimedia platforms (Sina Weibo, Bilibili, Tiktok, and YouTube). A novel architecture termed MDDformer to learn the non-verbal behaviors of individuals is proposed. Extensive validations are performed on the LMVD dataset, demonstrating superior performance for depression detection. We anticipate that the LMVD will contribute a valuable function to the depression detection community. The data and code will released at the link: https://github.com/helang818/LMVD/.", "sections": [{"title": "I. INTRODUCTION", "content": "MAJOR depression disorder (MDD) has been projected to a primary mental illness by 2030. A comprehensive survey and meta-analysis unveiled that from a study of 41,531 individuals, 33.7% of them experienced depression during the COVID-19 pandemic [1]. Normally, depression can affect various aspects of daily activities, including the progress of one's careers, studies, and families, etc [2]. In some severe cases, individuals suffering from depression may contemplate or attempt suicide [3]. In spite of the many attempts to identify depression, recent findings suggest that the prevalence of depression may be increasing in the younger individuals group.\nDepressed subjects often express several different non-verbal behaviors, e.g., facial expressions, body gestures, and smile. Diagnostic and Statistical Manual of Mental Disorders (DSM-V) outlines symptoms such as agitation (e.g., the inability to sit still, pacing, and hand-wringing) or retardation (e.g., slowed speech and body movements and increased pauses before answering) that may be displayed by depressed subjects. Current approaches of treating depression are mainly based on assessment by clinicians or the description of depressed subjects, both of which may be subjective. With the fast advancement of computer vision methods, a list of methods are explored to study the depression severity. Present methods of recognising depression can be considered as hand-crafted [4]\u2013[7] and deep learning-based [8]\u2013[12] approaches. From the modality perspective, methods for depression recognition can be classified into audiovisual cues, social media data (Weibo, Twitter), and physiological and non-physiological signals [13] such as skin conductance, electroencephalography (EEG), magnetoencephalography (MEG), electrocardiography (ECG), heart sounds, respiration, and pulse signals. Despite promising performances achieved by current depression recognition methods, several challenges remain in effectively recognizing depression via multimodal signals. This is especially prevalent in the field of deep learning community, where a substantial quantity of data samples is necessary to train the available models. By reviewing the depression databases from recent decades [2], one observes small samples of publicly accessible data because of privacy protection. Furthermore, most of the available databases are collected in controlled laboratory environments via doctor-patient interactions; therefore behavioral patterns of the depressed subjects outside of the laboratory environment are missed [14]. Although, Yoon et al. [15] introduced a D-vlog depression dataset containing 961 samples and providing facial landmarks and low level descriptors (LLDs).\nHence, to mitigate the above-mentioned major issues, a large-scale multimodal vlog database (LMVD) is built. In general, these vlogs are recorded and uploaded spontaneously by users, which motivate us to release this novel dataset to capture the potential patterns embedded in the vlogs of individuals navigating their daily lives. First, we obtain the"}, {"title": "II. RELATED WORKS", "content": "In this section, we offer a detailed explanation of the collected dataset and its relevance to multimodal depression detection by reviewing related works.\nA. The Depression Dataset\nBecause of the privacy protection related to studies on depression, data collection is very complicated. Consequently, various research teams have endeavored to record their own databases for depression estimation. This section examines a total of 21 databases, with only nine being accessible to the public. \nBased on Table I, the following observations are considered:\n1) Data Accessibility: Eight databases on depression detection are publicly available. However, most of the databases provide the extracted hand-crafted and deep-learning features, without providing the raw audio and video signals. Most importantly, our collected dataset has more samples for multimodal depression detection among individuals navigating their daily lives.\n2) Sample Size: In terms of the number of data samples, one can see that only the D-vlog [15] holds the previous record with 961 samples. LMVD boasts a significantly larger collection of 1,823 samples (214 hours) from 1,475 participants. However, the databases AVEC2013 and AVEC2014 are the most frequently used by researchers, although AVEC2014 only contains 292 subjects with 300 samples.\n3) Geographic Diversity: Most of the databases are located in the EU, with only two collected in China. Based on the concepts of depression, the depressed subjects represent different behaviors in different countries.\n4) Modality Coverage: Most of the datasets contain audio and video modalities, while only a few contain the text modality for depression detection. Similar to existing datasets, LMVD primarily focuses on audio and video modalities. However, it offers a wider range of visual features compared to some existing datasets, including FAUs, landmarks, eye gaze, and head pose features, which can provide richer insights. As indicated in Table I, 12 databases only have unimodal, and 50% of the them have the audio modality, because this metric is easily recorded. There are only eight (45%) multimodal databases available.\nOverall, LMVD offers several advantages for researchers in the field of multimodal depression detection. Its larger sample size, diverse participant pool, raw data access, and broader range of features make it a valuable resource for advancing research efforts.\nB. Multimodal Depression Detection\nA series of studies, which utilize cues from audio, video, and text modalities, have been suggested to accurately evaluate"}, {"title": "III. LMVD DEPRESSION DATASET", "content": "This section introduces the LMVD dataset, a large-scale multimodal dataset built \u201cin the wild\" for depression detection. We elaborate on the following aspects: (1) the motivation of this study, (2) the procedure of collecting the Chinese vlogs from the four media platforms, (3) the step for annotating the vlogs, (4) the step of prepossessing the details, and (5) the extraction of audio and visual features.\nA. Motivation\nAs reported in Table I, the limitations of the available datasets motivated us to build a large-scale dataset for depression detection in individuals navigating their daily lives. This large-scale dataset offers several advantages:\n1) Promotes Research and Applications: the collection of LMVD will boost both research endeavors and clinical scenarios in the community of automatic depression detection.\n2) Benefits Various Stakeholders: the developed prototype system offers an efficient solution applicable to various sectors, including government agencies, hospitals, and universities.\nB. Data Collection\nOur goal is to build a large-scale dataset for multimodal depression detection in individuals navigating their daily lives. To achieve this, we aim to collect depression and non-depression vlogs from different platforms with similar content distribution. Therefore, we collect the vlog videos from three Chinese multimedia platforms (Sina Weibo, Bilibili, and Tiktok). Data was collected from 1st Jan 2019 to 30th October 2023 using the following keywords (in Chinese):\n1) Depressed category: depression, my depressed life, and depressed vlog.\n2) Non-depressed category: daily Life, daily vlog, my vlog.\nIn addition to the Chinese platforms, we collect vlogs from YouTube using the same keywords in the same period. The collection process followed a similar approach. By collecting data from both Chinese and English platforms, we aimed to enhance the diversity and generalized ability of the LMVD dataset.\nC. Data Annotation\nTo perform the annotation for the depression and health vlogs, four master and ten undergraduate students are recruited. First, we ask the ten undergraduate students to check whether the vlogs have faces and if the audio is synthesized. As shown in Table III, we obtained 1823 data samples for the LMVD. For the Depressed and Non-depressed categories, the number of vlogs is 908 and 915, respectively. Secondly, we ask the four master's students to recheck the vlogs to ensure the quality is sufficient for training the deep models for multimodal depression detection. Then we assign the 1823 vlogs to the ten undergraduate students who assigned them to either the Depressed or Non-depressed categories. Finally, the master students check the labels assigned by the undergraduate students.\nD. Multimodal Feature Extraction\nTo establish a foundational benchmark for the field of depression recognition, we employ the primary features commonly used.\nFor audio features, the pre-trained VGGish [41] model is adopted. This is because the traditional hand-crafted features have the following limitations: (1) professional knowledge is often needed to design the discriminative features, and (2) additional valuable patterns may be lost in developing the deep features.\nFor visual features, FAU, facial landmarks, eye gaze, and head pose features are adopted.\n1) Facial Action Units (FAUs): We focus on a subset of 17 AUs (AU01, AU02, AU04, AU45) that have been linked to emotional expression. These features represent specific muscle movements in the face that can provide insights into a person's emotional state.\n2) Facial Landmarks: We extract facial landmarks (see Fig. 1) to represent the key points of the participant's facial structure. Facial landmarks are robust for capturing facial muscle movements, making them valuable features for tasks like emotion analysis, depression detection, and facial action unit detection."}, {"title": "IV. METHODS", "content": "In this section, we describe the pipeline for multimodal depression detection using the LMVD. We first provide a brief overview of the pipeline, followed by a detailed description of the multimodal depression detection method.\nA. Architecture Overview\nFig. 2 illustrates the proposed MDDformer. First, the audio features are extracted by the VGGish architecture. For the visual cue, AUs, head pose, landmarks, and eye gaze features are extracted by TCN architecture. Then, the CFformer can adopt the advantages of cross fusion for learning behaviors from the audiovisual cues. Finally, two fully connected layers and the softmax function are performed to predicting the depression.\nB. Multimodal Depression Detection\n1) Baseline Model: Over the past decades, deep learning methods have gathered significant attention across various tasks. Consequently, to establish a benchmark in the field of depression detection, we adopt both traditional machine learning methods and several deep learning techniques. Specifically, K-nearest neighbor (KNN) [42] is employed to address two category problems, serving as the baseline model for multimodal depression detection. More information on KNN can be found in [42].\n2) Structure of MDDformer: To effectively detect the discriminative patterns within audiovisual cues, MDDformer is proposed. Let's define the audio feature as \\(X^{a} \\in \\mathbb{R}^{N\\times D_a}\\), and the video feature as \\(X^{v} \\in \\mathbb{R}^{N\\times D_v}\\) before inputting into the MDDformer. Here, \\(D_a\\) represents the dimension of audio, \\(D_v\\) represents the dimension of video, and N denotes the length of sequences.\nInitially, the transformer architecture [43] is proposed to model therelationships in natural language processing (NLP) tasks, which consists of an encoder-decoder structure. Both the encoder and decoder are composed of multiple identical layers, each containing two main sub-modules,i.e., multi-head self-attention and position-wise feed-forward networks. In our task, to fuse the patterns from audio and video branch, an MDDformer is proposed.\nThe input audio feature \\(X^{a} \\in \\mathbb{R}^{N\\times D_a}\\) is mapped to three matrices by three linear transformations, i.e., key \\(K_a\\), query \\(Q_a\\), and value \\(V_a\\).\n\\[Q_a = X^{a}W^{Q_a}, K_a = X^{a}W^{K_a},V_a = X^{a}W^{V_a}\\]\nwhere \\(W^{Q_a}\\), \\(W^{K_a}\\), and \\(W^{V_a}\\) denotes the weights of linear transformation. The video feature \\(X^{v} \\in \\mathbb{R}^{N\\times D_v}\\) can be mapped to three matrices by three linear transformations, i.e., key \\(K_v\\), query \\(Q_v\\), value \\(V_v\\).\n\\[Q_v = X^{v}W^{Q_v}, K_v = X^{v}W^{K_v}, V_v = X^{v}W^{V_v}\\]\nwhere \\(W^{Q_v}\\), \\(W^{K_v}\\), and \\(W^{V_v}\\) denotes the weights of linear transformation.\nNext, \\(Q_a\\) is multiplied with \\(K^T_a\\) to generate the feature \\(F_a = Q_aK^T_a\\) and \\(Q_v\\) multiplied with \\(K^T_v\\) to generate the feature \\(F_v = Q_vK^T_v\\). Then we concatenate \\(F_v\\) and \\(F_a\\) to generate the feature map, i.e., \\(F_{av}\\):\n\\[F_{av} = concat(F_a, F_v)\\]\nThe self attention of the audio branch can be expressed as:\n\\[Attention\\_a (Q_a, K_a, V_a) = softmax(\\frac{F_{av}}{\\sqrt{d_k}})V_a\\]\nwhere \\(d_k\\) is the dimension of the \\(F_{av}\\) matrix.\nThe self attention of the visual branch can be expressed as:\n\\[Attention\\_v (Q_v, K_v, V_v) = softmax(\\frac{F_{av}}{\\sqrt{d_k}})V_v\\]\nwhere \\(d_k\\) is the dimension of the \\(F_{av}\\) matrices.\nThen, we concatenate the outputs of each head and reshape them to add with the feature \\(X^{a} \\in \\mathbb{R}^{N\\times D_a}\\),generating the fusion feature.\n\\[F_f=Concat((Concat ( head_{1a}, head_{2a},...,head_{h_a})W_a +X^{a})+(Concat ( head_{1v}, head_{2v},..., head_{h_v})W_v + X^{v}))\\]\nwhere h is the heads of the multi-head self-attention and \\(W_a\\) and \\(W_v\\) are weight matrices.\nFollowing the concatenate operation, \\(F_f\\) is then input to the feed-forward network, adopting the add/norm operation to generate the feature \\(F_n\\):\n\\[F_n = Norm(FFN(F_f) + F_f)\\]\nwhere Norm is the normalization operation."}, {"title": "V. RESULTS", "content": "We elaborate on the details the experimental setup and describe the MDDformer performances in this section.\nA. Experimental Setup\nWe implement and trained the MDDformer model using the PyTorch deep learning toolkit. A 10-fold cross-validation is performed for validating the efficiency of MDDformer. The Adam optimizer is set to \u03b2 = (0.9,0.999) and 6 = 1e8 with the batch size of 4. The initial training learning rate size is 0.00001 and then updated with CosineAnnealingLR decay. To overcome the overfitting problem, a dropout of 0.2 is adopted in the linear layers and the total epochs is set to 300. Our architecture is evaluated on four NVIDIA Tesla V100-DGX with 32GB.\nB. Evaluation Metrics\nIn general, the classification performance is mainly evaluated using five metrics for binary classification problems: accuracy, precision, recall (also known as sensitivity), specificity, and F1-score. Here, true positive (TP) indicates samples with positive labels that are correctly predicted as positive. Similarly, true negative (TN), false positive (FP), and false negative (FN) respectively represent samples correctly predicted as negative, incorrectly predicted as positive, and incorrectly predicted as negative, respectively. The formula can be expressed as:\n\\[Accuracy = \\frac{TP+TN}{TP+FP+TN+FN}\\]\n\\[Precision = \\frac{TP}{TP + FP}\\]\n\\[Recall = \\frac{TP}{TP+FN}\\]\n\\[F1\\ score = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\\]\n1) Performances of Baseline Methods: To further validate the performances of the MDDformer, several machine learning and deep learning architectures are adopted i.e., KNN, SVM, LR, RF, Xception, ViT, BiLSTM, and SEResnet. To make a fair comparison, the weighted accuracy, precision, recall, and F1-score are adopted. As shown in Table V, the MDDformer obtains the best performance in term of the evaluation metrics. The terms \"add\" and \"concat\" represent the addition and concatenate operation, respectively. In our task, we list the models in ascending order according of accuracy. One can"}, {"title": "VI. CONCLUSION", "content": "In this article, we collect a large-scale vlog dataset, i.e., the LMVD, for depression recognition among individuals navigating their daily lives based on audiovisual cues. The proposed dataset has 1823 samples (214 hours) from 1475 participants. To performance with the MDDformer, i.e., KNN, SVM, LR, RF, Xception, BiLSTM, SEResnet, and ViT. More importantly, our LMVD is the largest dataset for audiovisual depression recognition in individuals navigating their daily lives, which is a positive contribution to the affective computing field. In the future, we will augment the dataset and explore non-verbal behaviors for multimodal depression recognition."}]}