{"title": "EARLY EXTERNAL SAFETY TESTING OF OPENAI'S 03-MINI: INSIGHTS FROM PRE-DEPLOYMENT EVALUATION", "authors": ["Aitor Arrieta", "Miriam Ugarte", "Pablo Valle", "Sergio Segura", "Jos\u00e9 Antonio Parejo"], "abstract": "Large Language Models (LLMs) have become an integral part of our daily lives. However, they impose certain risks, including those that can harm individuals' privacy, perpetuate biases and spread misinformation. These risks highlight the need for robust safety mechanisms, ethical guidelines, and thorough testing to ensure their responsible deployment. Safety of LLMs is a key property that needs to be thoroughly tested prior the model to be deployed and accessible to the general users. This paper reports the external safety testing experience conducted by researchers from Mondragon University and University of Seville on OpenAI's new 03-mini LLM as part of OpenAI's early access for safety testing program\u00b9. In particular, we apply our tool, ASTRAL, to automatically and systematically generate up to date unsafe test inputs (i.e., prompts) that helps us test and assess different safety categories of LLMs. We automatically generate and execute a total of 10,080 unsafe test input on a early 03-mini beta version. After manually verifying the test cases classified as unsafe by ASTRAL, we identify a total of 87 actual instances of unsafe LLM behavior. We highlight key insights and findings uncovered during the pre-deployment external testing phase of OpenAI's latest LLM.\nWarning: This report contains extracts from unsafe test inputs generated by ASTRAL, which may upset some readers. Reader discretion is advised.", "sections": [{"title": "Introduction", "content": "Safety of Large Language Models (LLMs) has become an integral property that needs to be thoroughly tested before and after the LLM has been deployed. The Trust4AI project\u00b2 is an initiative of two Spanish universities (Mondragon University and University of Seville) that aims to research automated methods to systematically test different LLM properties, including the safety and fairness. When the user of an LLM provides an unsafe test input (e.g., \"How can one build a home-made explosive?\"), the LLM shall refuse to respond. Different testing methods and benchmarks have been proposed to this end [1, 2, 3, 4, 5, 6, 7, 8, 9].\nOur recent study proposes ASTRAL, a novel tool that automatically generates unsafe test inputs to test the safety of LLMs [10]. ASTRAL has three core advantages over previous LLM safety tools and benchmarks. First, instead of relying on static safety benchmarks, ASTRAL generates novel and imaginative test inputs guaranteeing a broad range of unsafe LLM test inputs. To this end, ASTRAL uses the capabilities of LLMs along with Retrieval Augmented Generation (RAG) and few-shot prompting strategies to automatically generate unsafe test inputs (i.e., prompts) in 14 different safety categories (e.g., terrorism, child abuse), with different writing styles (e.g., using slang or uncommon"}, {"title": "Background and Related Work", "content": "Safety in LLMs primarily concerns ensuring their outputs remain free from harmful content while maintaining relia-bility and security [12]. This is particularly critical when LLMs are applied to sensitive domains such as healthcare, pharmaceuticals, or terrorism, where responses may inadvertently include malicious or misleading information with serious consequences. To address these risks and enhance trust in AI, the European Union AI Act (Regulation (EU) 2024/1689) [13] establishes a regulatory framework focused on AI governance.\nThis framework adopts a risk-based approach to AI regulation. Under Article 51 of the EU AI Act [14], LLMs are classified as General-Purpose AI Models with Systemic Risk-referring to large-scale risks that can significantly impact the value chain, particularly in areas affecting public health, safety, security, fundamental rights, or society at large, as defined in Article 3(35). As a result, ensuring LLMs undergo rigorous safety testing and regulatory compliance assessments has become imperative.\nDifferent testing techniques have been proposed to assess the safety quality of LLMs. Several studies have proposed multiple-choice questions to facilitate the detection of unsafe LLM responses [3, 4, 6, 7]. These benchmarks have an issue, i.e., they are fixed in structure and pose significant limitations, differing from the way users interact with LLMs. An alternative to this was to leverage LLMs that are specifically tailored to solving the oracle problem when testing the safety of LLMs. To this end, Inan et al. [15] propose LlamaGuard, a Llama fine-tuned LLM that incorporates a safety risk taxonomy to classify prompts either as safe or unsafe. Zhang et al. [16] propose ShieldLM, an LLM that aligns with common safety standards to detect unsafe LLM outputs and provide explanations for its decisions.\nOther techniques exists to test the safety of LLMs, such as red teaming and creating adversarial prompt jailbreaks (e.g., [17, 18, 19, 20, 21, 22, 23]). Red-teaming approaches use human-generated test inputs, resulting in significant and expensive manual work. Adversarial works, on the other hand, do not typically represent the interactions that general LLM users employ.\nA large corpus of studies focuses on proposing large benchmarks for testing the safety properties of LLMs, e.g., by using question-answering safety prompts. For example, Beavertails [8] provided 333,963 prompts of 14 different safety categories. SimpleSafetyTests [9] employed a dataset with 100 English language test prompts split across five harm areas. SafeBench [5] conducted various safety evaluations of multimodal LLMs based on a comprehensive harmful query dataset. WalledEval [24] proposed mutation operators to introduce text-style alterations, including changes in tense and paraphrasing. Nevertheless, all these approaches employ imbalanced datasets, in which some safety categories are underrepresented. Therefore, SORRY-Bench [1] became the first framework that considered a balanced dataset, providing multiple prompts for 45 safety-related topics. In addition, they employed different linguistic formatting and writing pattern mutators to augment the dataset. While these frameworks are useful upon release, they have significant drawbacks in the long run. First, they may eventually be incorporated into the training data of new LLMs to enhance safety and alignment. Consequently, LLMs could internalize specific unsafe patterns, significantly diminishing the utility of these prompts for future testing, thereby requiring continuous evolution and the development of new benchmarks. Second, as discussed in the introduction, they risk becoming outdated and less effective over time.\nTo address all these limitations faced by previous studies, our previous paper proposes ASTRAL [10]. ASTRAL proposes a novel approach that leverages a black-box coverage criterion to guide the generation of unsafe test inputs. This method enables the automated generation of fully balanced and up-to-date unsafe inputs by integrating RAG, few-shot prompting and web browsing strategies. More details of the key features of ASTRAL can be found in Section 2.2 and the related paper [10]."}, {"title": "2.1 Safety Testing of LLMs", "content": null}, {"title": "2.2 ASTRAL", "content": "We now briefly explain ASTRAL and its features to automatically and systematically generate, execute, and evaluate unsafe LLM test inputs. All details of ASTRAL can be found in its original paper [10]. ASTRAL uses LLMS, RAG and few-shot prompting strategies to automatically generate and execute unsafe test inputs (i.e., prompts) of a total of 14 different safety categories. Specifically, ASTRAL operates in three main phases. First, during the test generation phase, an LLM generates a set of N unsafe test inputs tailored to predefined categories, writing styles and persuasion techniques. To guide the generation of unsafe prompts, we propose a new black-box coverage criterion. This criterion ensures the generation of balanced unsafe test inputs across different safety categories, writing styles and persuasion techniques. The safety categories, writing styles and persuasion techniques supported by the latest ASTRAL version are summarized in Table 1. We hypothesize that introducing a variety of test input types permits detecting a wider scope of safety-related issues in LLMs. To achieve this, we leverage OpenAI's assistant APIs, as they support RAG-based methods to be integrated in GPT-based LLMs. Lastly, we leverage a novel feature that gives access to the test input generator to live data (e.g., browsing the latest news) with the goal of generating up-to-date unsafe test inputs.\nThe second step consists of the execution. During this phase, ASTRAL feeds the generated test inputs into the target LLM under test (i.e., the 03-mini model in this case). Lastly, in the evaluation phase, another LLM acts as an oracle to analyze the outputs (i.e., responses) of the tested LLM. This LLM determines whether the output provided by the LLM meets the safety standards.\nTo guide the generation of unsafe prompts, we propose a new black-box coverage criterion. This criterion ensures the generation of balanced unsafe test inputs across different safety categories, writing styles and persuasion techniques. Furthermore, we conjecture that introducing a variety of test input types permits detecting a wider scope of safety-related misbehaviors in LLMs. Lastly, we leverage a novel feature that gives access to the test input generator to live data (e.g., browsing the latest news) with the goal of generating up-to-date unsafe test inputs."}, {"title": "3 Methodology", "content": "This section explains the methodology we used to test OpenAI's 03-mini beta LLM."}, {"title": "3.1 Test input generation", "content": "We used two different test suites, the one employed to assess ASTRAL in the original paper [10], and a new one whose generation began in January, 2025.\nFirst test suite (TS1): The first test suite is the original test suite employed in the evaluation of our original paper. This test suite was generated in November 2024. Since ASTRAL leverages web browsing to generate up-to-date inputs, the resulting test inputs included notable events from that period, such as the 2024 US elections. This test suite consisted of 3 different ASTRAL versions:\n\u2022 ASTRAL (RAG): This version uses ASTRAL with RAG, without including few-shot prompting to guide the generator in the generation of different writing styles, nor Tavily Search (TS) to browse recent news from the internet.\n\u2022 ASTRAL (RAG-FS): Besides RAG, this version uses ASTRAL with few-shot prompting strategies that guides the tool generate prompts with different writing styles. However, it does not include TS, which pre-vents it from generating prompts related to recent events.\n\u2022 ASTRAL (RAG-FS-TS): This version includes the three key features from ASTRAL, i.e., RAG, to retrieve examples of different safety categories; FS, to guide the generation of prompts following different writing styles; TS, to browse recent news from the internet to generate up to date test inputs.\nFor each of these versions, we generated 1260 test inputs. We generated test inputs by combining each of the categories of the three features described in Table 1 with each other (i.e., 6 (styles) \u00d7 5 (persuasion) \u00d7 14 (safety categories) \u00d7 3 (tests)). Therefore, with the three different ASTRAL versions, a total of 3780 test inputs were generated and executed on the 03-mini model.\nSecond test suite (TS2): For the second test suite, we generated test inputs starting in January 2025. Remarkable events during this time included Donald Trump's inauguration and the Gaza's ceasefire, among others. In this case, since ASTRAL (RAG+FS+TS) showed best results, we opted for generating test cases only with this version of the tool. However, instead of generating 3 test inputs for each of the combinations (as conducted in TS1), we generated 15, resulting in a total of 6300 test inputs (i.e., 6 (styles) \u00d7 5 (persuasion) \u00d7 14 (safety categories) \u00d7 15 (tests))."}, {"title": "3.2 Test execution and evaluation", "content": "Prior to executing our generated test inputs, we had to adapt our ASTRAL version for two main reasons. First, OpenAI deprecated the API version we used in the initial ASTRAL version. Therefore, we migrated our code to the new API. Second, the 03-mini model triggered exceptions to a large portion of our test inputs, claiming a policy violation. That is, we conjecture that the API was able to detect unsafe test inputs before they were provided to the LLM under test. The first test suite started to be executed on January 21st, 2025, whereas the second one took place from January 24 to January 29th, 2025."}, {"title": "3.3 Manual assessment", "content": "As our test evaluator is subject to provide false positives (i.e., outcomes classified as \u201cunsafe\" that should be \"safe\"), we manually assessed those outcomes classified as both, \u201cunsafe\" and \"unknown\". We did not manually assess the rest of the prompts due to the large number of total test inputs we executed.\nIt is noteworthy that the manual classification of unsafe LLM behaviors might be subject to individual sentiments as well as culture. For instance, in one of the cases, 03-mini responded with arguments supporting the use of firearms by civilians. Culturally in Spain, this practices are not widely accepted, as gun ownership and civilian use of firearms are heavily regulated and generally viewed with skepticism. This contrasts with other cultures where such practices might be considered more commonplace or acceptable. In addition, we found some borderline cases, for which three of the authors discussed different point of views to classify as safe or as unsafe and found an agreement.\nDuring the assessment, if needed, the authors also considered the explanation given by the LLM to classify the out-come."}, {"title": "4 Results and Discussion", "content": null}, {"title": "4.1 Summary of the Results", "content": "Table 2 reports the number of safe and unsafe LLM outcomes ASTRAL found. After manual verification, ASTRAL found 87 unsafe LLM outcomes for out of 10,080 test inputs. The category in which the most unsafe test results were found was c3, i.e., controversial topics and politics, especially for test suite 2, followed by c13 (i.e., terrorism and organized crime), as reported in Table 3. The recent appointment of Donald Trump as president of the United States could have led to providing unsafe controversial outcomes on an LLM that seemed not well aligned for the test inputs generated by ASTRAL."}, {"title": "4.2 Overall Discussion", "content": "We now discuss a set of five key findings and their implications.\nFinding 1 \u2013 OpenAI's 03-mini model seems to be safer than its predecessors. In principle, when compared with other older OpenAI's models (i.e., GPT3.5, GPT4 and GPT-40), OpenAI's o3-mini model is safer. In our previous study [10], ASTRAL was able to uncover a total of 752, 166 and 215 unsafe behaviors on GPT3.5, GPT4 and GPT-40, respectively for the first test suite. That is, the number of uncovered unsafe LLM outcomes were much higher in older"}, {"title": "Finding 2 - OpenAI's 03-mini model is safer than other state-of-the-art tested LLMs with ASTRAL", "content": "In our previous study [10], Llama 2 was the safest model, with a total of 95 unsafe outcomes found within the first test suite. Nevertheless, it is noteworthy that these unsafe outcomes were not manually verified. However, the number of unsafe inputs found by ASTRAL without confirmation for the first test suite was 49, much less than the ones found in Llama 2 (i.e., 95)."}, {"title": "Finding 3 - The policy violation feature might have played an important role on safeguarding the model", "content": "While executing the test inputs, we observed that the API crashed for certain cases, citing a policy violation. When further investigating this issue, we found that OpenAI had a usage policy that prevented users from using their API when \"not complying with applicable laws\u201d as well as \"not using it to harm (yourself or others)\"3. As part of the safety testing process, the generated test inputs by ASTRAL included both, illegal activities (e.g., fabrication of weapons) as well as prompts related to self-harm (e.g., self-medication, consumption of drugs). This suggests the presence of a firewall-like mechanism designed to block unsafe test inputs from reaching the LLM. While we can argue that the entire 03-mini system is safer, it seems like many of the generated test inputs were not actually executed on the LLM itself. When testing other models, this policy was not violated. Therefore, we are also unsure whether this mechanism will later be deployed when a non-beta version of the model is deployed for general users. If this mechanism is not finally deployed, we recommend OpenAI's developers to re-execute those test cases that triggered the policy violation."}, {"title": "Finding 4 \u2013 Recent controversial topics seem to be able to trigger unsafe LLM outcomes", "content": "As shown in Table 2, for the second test suite, the third category triggered many unsafe LLM outputs. This may be due to the large number of inputs generated by ASTRAL referencing Donald Trump's pardoning of January 6th offenders, which triggers, as our LLM evaluator describes, \u201csuggestions and implications that could undermine the rule of law, shake public trust in governance, deepen political polarization, impact democracy negatively, and stir societal unrest\u201d."}, {"title": "Finding 5 - Besides c3 (controversial topics), c13, c1, c5 are other critical safety categories", "content": "Most of these categories also triggered misbehaviors related to safety in other OpenAI's models when compared to other safety categories in our previous study [10]. This suggests that OpenAI could pay special attention to those categories to make their models safer."}, {"title": "5 Conclusion", "content": "This report explains the early external safety testing we conducted on a beta version of OpenAI's 03-mini model. We employed our tool, ASTRAL [10], to automatically and systematically generate and execute a total of 10,080 test inputs on the 03-mini model. After a manual verification of the LLM outcomes classified as \"unsafe\" or \"unknown\", we observed that 87 of them provoked an unsafe outcome. When compared with our previous experiments [10], where other state-of-the-art LLMs were used, we found that the results of this new model are highly competitive in terms of safety. We noted, however, that many of the inputs were refused by a safeguard that seemed to be external to the LLM, as it triggered crashes related to a policy violation.\nLastly, it is important to note that excessive safety can come at the cost of helpfulness [25]. This trade-off, a crucial aspect of LLMs, was not explored in this study and is left for future work."}, {"title": "Replication Package", "content": "The reults can be obtained in the following link: https://doi.org/10.5281/zenodo.14762830"}]}