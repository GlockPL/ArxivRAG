{"title": "Face-MakeUp: Multimodal Facial Prompts for Text-to-Image Generation", "authors": ["Dawei Dai", "Mingming Jia", "Yinxiu Zhou", "Hang Xing", "Chenghang Li"], "abstract": "Facial images have extensive practical applications. Although the current large-scale text-image diffusion models exhibit strong generation capabilities, it is challenging to generate the desired facial images using only text prompt. Image prompts are a logical choice. However, current methods of this type generally focus on general domain. In this paper, we aim to optimize image makeup techniques to generate the desired facial images. Specifically, (1) we built a dataset of 4 million high-quality face image-text pairs (FaceCaptionHQ-4M) based on LAION-Face to train our Face-MakeUp model; (2) to maintain consistency with the reference facial image, we extract/learn multi-scale content features and pose features for the facial image, integrating these into the diffusion model to enhance the preservation of facial identity features for diffusion models. Validation on two face-related test datasets demonstrates that our Face-MakeUp can achieve the best comprehensive performance. All codes are available at: Face-MakeUp", "sections": [{"title": "I. INTRODUCTION", "content": "Image generation has made remarkable strides with the success of recent large text-to-image diffusion models like DALL-E 2 [1], Imagen [2], and Stable Diffusion (SD) [3]. Stable Diffusion, by performing diffusion in latent image space, not only revolutionized the generation process but also significantly reduced computational costs, making it one of the dominant methods in recent years. Not only do they exhibit a remarkable capacity for generating high-quality, intricate, and imaginative images, but their practical applications in fields such as artistic creation, advertising design, and interactive entertainment have proven that they can significantly enhance work efficiency and open up new possibilities for innovation.\nIn practical applications, facial image generation have broad application across various aspects [4], [5]. However, crafting effective text prompts to produce desired face image can be challenging. Two fundamental reasons can be summarized as: (1) text alone may not adequately convey complex scenes or concepts. It is difficult for people to describe highly complex geometric and textural information using text, resulting in generated images that deviate from expectations. (2) The absence of large-scale image-text pair datasets with sufficiently detailed text results in models having insufficient capability to generate detailed images.\nConsidering these limitations of text prompts, image prompts are a logical choice, as they can convey more details and nuances than text. For example, IP-Adapter [6] extracts image features through CLIP [7] and fuse the multimodal information using cross-attention modules to guide the diffusion process, generating images that align with the reference image. Photomaker [8] combines facial embeddings that projected into the CLIP space with text tokens to produce joint embeddings as a conditional input to guide the diffusion process. InstantID [9] introduces an additional ID & Landmark ControlNet [10], further improving the model's control efficiency, while it significantly boosts ID similarity, it sacrifices some editing capability and flexibility. PuLID [11] combines contrastive loss and precise ID loss to effectively minimize interference with the reference while ensuring a high level of ID fidelity.\nAlthough researchers have conducted a series of important studies and achieved certain successes in image-prompt text-to-image generation, there is a significant need for specialized optimization for facial image generation due to its wide range of applications. First, we constructed a large-scale facial image-text dataset (FaceCaptionHQ-4M) to train our dedicated facial model (Face-MakeUp). Second, we utilized both general and specialized facial visual encoders to extract multi-scale features from facial regions, and also employed a structure extractor to learn the structural (pose) information, integrating them into the pretained diffusion model, ensuring consistency between the facial regions of the generated images and the references. Experiments demonstrate that our Face-MakeUp achieves the best competitive outcomes across a broad range of evaluation metrics. Our contributions can be summarized as:\n(1) FaceCaptionHQ-4M. For facial image generation task, we constructed a large-scale and high-quality facial image-text dataset, where text describe facial features.\n(2) Face-MakeUp Model. We developed and optimized the image-prompt Text-to-Image diffusion model for facial image generation, and achieve the best consistency of facial image.\n(3) Open-source. To facilitate research, we will release the following assets to the public: All the image-text pairs data, the model checkpoints, and the codebase for model training."}, {"title": "II. METHOD", "content": "A. Constructing FaceCaptionHQ-4M\nLAION-Face [12] is a large-scale facial image-text dataset. However, the quality of facial images in this dataset varies significantly. In this study, we cleaned the LAION-Face to construct a high-quality facial image-text data for image generation task. We utilize the information of FaceCaption-15M [13] (each image in FaceCaption-15M corresponds one image in LAION-Face) to clean the LAION-Face data efficiently. Specifically: (1) We sorted the images in FaceCaption-15M by resolution, and selected the top 10M images from LAION-Face; (2) We removed black-and-white images by checking whether the mean of the standard deviation exceeds a set threshold to ensure the selection of color images; (3) We employed an OCR text detection model [14] to eliminate images containing a large amount of text; (4) We removed the group photos containing multiple faces by using the yolov5-face model [15]; (5) We eliminated cartoon-style images using a cascade classifier based on LBP [16] to detect anime-style faces within the images. Finally, we obtained a 4.2M high-quality human-scene images.\nAfter finishing the selection of candidate images, we further processed the human photos. Specifically, (1) we proportionally expanded the width and height of the face region (as indicated by the bounding box that provided in FaceCaption-15M) by 1.5 times outward to encompass the upper body. We then cropped facial region into a square based on the smaller one (width or height); (2) we used Qwen2-VL [17] to generate more concise short text descriptions for the image.\nB. Architecture of Face-MakeUp\nOur purpose is to generate the image that aligns with the input text while maintaining a high degree of consistency with the reference image. In this study, we integrate fine-grained features of face into the feature space of the diffusion model (SD) while preserving posture, ensuring that the generated image reflects the identity characteristics and overall facial morphology of the reference portrait.\nSpecifically, (1) the inputs of Face-MakeUp include a reference facial image, a pose map that extracted from the reference image, and text prompt; (2) facial features extraction modules, which includes general and specialized visual encoders as well as a learning module for pose map; (3) a pre-trained text-to-image diffusion model; and (4) a cross-attention module is designed to learn the joint representation of facial image (reference) and text prompts. In addition, embeddings of pose map are integrated through an additive way (Fig. 2(b)). This final embeddings are then incorporated into the feature space of the diffusion model through an overlay method, which enriches the feature space of the diffusion model with more information of the reference facial image, thereby ensuring consistency between the generatacial image and the reference image.\nC. Extracting Representations for Facial Image-Text\nWe need to extract or learn the embeddings of the prompt image and text. We utilize CLIP's Image-Encoder and Text-Encoder as general encoders to extract the representations of facial images and their corresponding texts, respectively. This is not enough. To enhance the fine-grained facial features and individual identity characteristics, we introduce ArcFace model [18] as FaceEncoder. ArcFace is designed specifically for facial identity verification, generating highly discriminative and consistent facial embeddings in high-dimensional feature space. We combine the facial features extracted by ArcFace and CLIP's visual encoder through a classical cross-attention module (projector), thereby enhancing the representation capability of facial images.\nD. Learning Representations of Facial Pose\nTo precisely maintain the structure consistency between the generated facial image and the prompt image, we incorporate facial pose information into the feature space of the text-to-image diffusion model. Specifically: (1) We first employ a pose detector [19] to obtain the pose map for the reference facial image; (2) We design a learnable module, named PoseNet, to learn the representations (embeddings) of the facial pose. To ensure that the feature vectors learned by this module can be seamlessly integrated into the corresponding layers of the diffusion model, the architecture of PoseNet is kept identical to that of the diffusion model.\nE. Fusion Strategy\nOur Face-MakeUp framework incorporates three additional embeddings into the diffusion model to enhance the consistency between the generated image and the reference. As shown in Fig. 2(b): (1) We introduce a new cross-attention module (Face-ID Cross-Attn) in the diffusion model to fuse the visual features of the reference facial image with the feature space of the diffusion model; (2) The fused features of the reference facial image are further combined with the text features via addition, and then passed through a feedforward layer to achieve the feature fusion between the reference and the text; (3) the pose features of the reference are added to (2), enabling multi-modal features fusion of the face. Using this approach, additional modalities of features can also be integrated, allowing for even more precise facial image generation."}, {"title": "III. EXPERIMENTS", "content": "A. Implementation Details\nTraining process. We adopt stable-diffusion-v1-5 [3] as our pre-trained Text-to-Image model. Correspondingly, the training data resolution is adjusted to 512x512. We use CLIP Vit-H/14 [20] and buffalo_1 [19] models as our Image-Encoder and Face-Encoder, respectively. The overall framework is trained using the Adam optimizer, with the learning rate set to le-4, over 500,000 steps on an 8*H800 GPU (94G), where the batch size per GPU is 10. Additionally, we apply 50% random dropout to the image features extracted by CLIP. During inference, we use DDIM [21] as the sampler, with the number of steps set to 50 and the guidance scale set to 7.5.\nMetric. (1) Some popular metrics, such as DINO [22], CLIP-I [23], CLIP-T and FID [24] are commonly employed to evaluate the generated image. (2) We also employ FaceSim [25] to further evaluate facial similarity. (3) To score the realism, we introduce Qwen2VL [17] to evaluate the models VLM-score. In addition, we employ a detection model [26] to recognize the attributes of the generated facial image, and the total count of detected attributes (Attr_c) is used as a basis for fine-grained evaluation of models.\nDatasets. We evaluate the models on two dataset, in which each dataset contain 50 reference images. We design 20 different text prompts for each reference image. The first dataset is the Unsplash-Face selected from the People category of the Unsplash\u00b9 website, collected in October. The second one comes from the test set of our FaceCaptionHQ-4M. By modifying the prompt texts (as shown in Table II), each model under evaluation generate 20 images for each reference image, resulting in a total of 1,000 image-text pairs per test dataset.\nB. Comparisons with Existing Methods\nWe present the comparisons in Table I and Fig. 1. We can make the main observations as follows:\n(1) In terms of the realism for generated facial images (VLM-score), our proposed Face-MakeUp significantly outperforms other models, indicating that our model"}, {"title": "D. Ablation Study", "content": "(1) Effectiveness of FaceCaptionHQ-4M. Since the data used by many other models have not been made publicly available, we train the classic IPAdapter model from scratch on our FaceCaptionHQ-4M. The evaluations on Unsplash-Face are shown in Table III. It can be observed that the IPAdapter-FaceID trained on FaceCaptionHQ-4M achieves significant improvements in the quality of generated facial images (CLIP-I, DINO, FaceSim, FID, and VLM-score). This demonstrates that FaceCaptionHQ-4M is beneficial for generating high-quality facial images.\n(2) Effectiveness of Facial Pose: In the inference, we eliminate facial features one by one. The results on Unsplash-Face are shown in Table IV. We observe that removing facial pose information leads to a decrease in the quality of the generated facial imagess. This demonstrates that pose is also beneficial for Face-MakeUp."}, {"title": "E. Other Applications", "content": "Identity mixing. When users provide images with different identities as input, Face-MakeUp can combine the characteristics from each identity to create a new one. As shown in Fig. 4, due to the lack of metrics, we rely solely on visual observation, new identity preserves the characteristics of the two input images.\nStylization. Fig. 5 showcases the stylization capabilities of our method. Illustrations demonstrate that Face-MakeUp not only maintains strong face fidelity but also effectively incorporates the style information from the input prompt. This highlights the potential of Face-MakeUp to enable a wide applications."}, {"title": "IV. CONCLUSION", "content": "General text-to-image models often lack realism and unpredictability when generating facial images. In this work, we have specifically optimized for facial image generation to improve the quality of the generated images. We have presented Face-MakeUp, an efficient method for personalized text-to-image generation, specifically aimed at creating realistic facial images. Experimental results have shown that Face-MakeUp achieves both high-quality and diverse image generation while maintaining strong facial ID fidelity of references. Additionally, we've discovered that our method enables various intriguing applications, such as identity mixing and stylization."}]}