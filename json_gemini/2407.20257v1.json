{"title": "Causal Understanding For Video Question Answering", "authors": ["Bhanu Prakash Reddy Guda", "Tanmay Kulkarni", "Adithya Sampath", "Swarnashree Mysore Sathyendra"], "abstract": "Video Question Answering is a challenging task, which requires the model to reason over multiple frames and understand the interaction between different objects to answer questions based on the context provided within the video, especially in datasets like NEXT-QA (Xiao et al., 2021a) which emphasize on causal and temporal questions. Previous approaches leverage either sub-sampled information or causal intervention techniques along with complete video features to tackle the NEXT-QA task. In this work we elicit the limitations of these approaches and propose solutions along four novel directions of improvements on the NEXT-QA dataset. Our approaches attempts to compensate for the shortcomings in the previous works by systematically attacking each of these problems by smartly sampling frames, explicitly encoding actions and creating interventions that challenge the understanding of the model. Overall, for both single-frame (+6.3%) and complete-video (+1.1%) based approaches, we obtain the state-of-the-art results on NEXT-QA dataset.", "sections": [{"title": "1. Introduction", "content": "Understanding video data brings us a step closer towards real world embodied agents. Previous challenges such as Visual Question Answering attempted to answer questions based on the information present in a single image or a scene. Videos add additional challenges with respect to understanding interactions across multiple frames, so as to reason about the situation being described. Not only do the agents need to understand events that have happened in previous frames, but they also need to reason about how the events in the previous frames affect the events happening in the current and/or future frames.\nCurrent state of the art models such as VGT (Xiao et al., 2022) attempt to connect relationships through associations between the video and text features through convoluted modules, which are prone to learning spurious correlations, and are often hard to debug. Another line of work (Li et al., 2022c;a) attempts to take advantage of the invariance of the non-causal frames and equivariance of the causal frames as additional sources of supervision for enhancing the predictions on the data. However, both of these approaches seem to be lacking in generalization power, as they are only able to get test performance of \u2248 50% while human performance is around 90% (Xiao et al., 2021a).\nOn the other hand, approaches like ClipBERT (Lei et al., 2021c) promote methods which rely on a subsampled information from the entire video to achieve compute tractability for VideoQA task. To this end, recently (Buch et al., 2022b) proposed the Atemporal Probe (ATP) approach for sampling the single most relevant frame for answering video-question pairs of VideoQA datasets MSR-VTT-MC (Xu et al., 2016), VALUE-How2QA (Li et al., 2020; 2021b). However, to achieve state-of-the-art performance in datasets like NEXT (Xiao et al., 2021a), they concluded that entire video is required as the dataset predominantly consists of causal and temporal questions which require answering why . happened or when happened in a given video.\nIn this work, we broadly categorize the prior works into single-frame based (ClipBERT, ATP) and complete-video based (IGV, EIGV, VGT) methods, and attempt to solve some of the severe limitations present in both categories of models. Motivated by the extensive analysis performed in our previous mid-term report, we propose to attempt the NEXT-QA video question answering task, containing causal, temporal and descriptive video question answer pairs, from four novel research directions. Extending prior art, we finally formulate the following four research questions (ideas), one for each direction, and aim to answer these through our proposed approaches.\nRQ1: Instead of using either of the extremes, can we leverage a smart aggregation of the sub-sampled information to improve low compute approaches like ATP?\nRQ2: Can we improve the robustness of causal component based answering models like EIGV through hard-negative mining approaches, which could lead to performance boost?\nRQ3: Can we aim to improve grounding of video representations with the questions by extracting suitable information"}, {"title": "2. Related Work", "content": "Invariant Learning (Leen, 1994; Sohn & Lee, 2012; Xiao et al., 2021b; Benton et al., 2020; Misra & Maaten, 2020) show invariant learning(IL) as a paradigm to generalize better to data outside training distribution. For multi-modal tasks such as VideoQA, IL as means of visual grounding(Li et al., 2022c), equivariant grounding when changes are made to casual/temporal scenes of videos(Li et al., 2022b) and improving intrinsic interpreatibility have been explored on datasets like NEXT-QA(Xiao et al., 2021a).\nContrastive Learning for better Interventions We look at using constrastive loss for learning better intervention mechanisms. Current constrastive losses can be instance based - InfoNCE (Oord et al., 2018), UberNCE (Han et al., 2020), MIL-NCE (Miech et al., 2020); clustering based - CentreNCE (Diba et al., 2021). (Dave et al., 2022) propose a contrastive learning approach to preserve temporally varying video features using non-overlapping clips of the same video as negative samples. VQA tasks often rely on spurious relationships between language and images(Agrawal et al., 2016; Zhang et al., 2016; Johnson et al., 2017). This can be alleviated with contrastive loss approach proposed in (Li et al., 2022b), which we modify to provide a more robust approach for the VQA task.\nGrounded Video Representation and Aggregation Prior approaches (Storks et al., 2019; Wang et al., 2020; Fang et al., 2020) attempt to extract causal knowledge using natural language. In visual domain, causal reasoning has been used for downstream applications such as image classification (Lopez-Paz et al., 2017), visual dialog (Qi et al., 2020), image captioning (Zhou et al., 2020) and identifying hidden actions in videos (Fire & Zhu, 2017). Temporal reasoning has also been attempted within visual domain for understanding events in videos (Zhou et al., 2018; Lin et al., 2019; Arnab et al., 2021). Multimodal approaches for this include Uniter (Chen et al., 2020), Vilbert (Lu et al., 2019), Videobert (Sun et al., 2019) that use supervised/weakly-supervised methods of encoding visual-semantic knowledge into representations.\nTo the best of our knowledge, our proposed approaches of using cross modal grounded aggregation of frames from videos to minimize information loss in single-frame based approaches, enriching the intervention of videos by hard mining similar scenes, grounding videos by using salient action recognition and description modules, and smart-sampling of frames/clips from videos to maximize information gain are the first attempts towards enhancing VideoQA models through multiframe grounded approaches."}, {"title": "3. Proposed Approach", "content": "In this section, we start by briefly describing the VideoQA task setup and then proceed to elaborate on various components of the proposed approaches, which we use towards answering our four research questions. Given an input video V, and a question Q, and answer choices C = [Ci]=1, the objective is to minimize expected risk of using a multimodal framework F in predicting answer A e C i.e.,\nmin $L_{ERM}(F(E_v(V); E_t(Q); E_t(C)), A)$ (1)\nwhere Ev, Et, Et are the encoders that generate the representations for the V, Q, and C respectively. Often, we represent each of the choices individually i.e., E\u2081(C) = [&(C\u2081)]=1, and the encoders for the question and choices are same, as the modality of information for both of them is text (hence Et).\n3.1. Pairwise Cross Modal Aggregation (PCMA)\nAs highlighted in Section 1, we can briefly categorize the VideoQA models into frame-based and clip-based methods. The components that we describe now are targeted at improving the less-compute frame-based methods. Following (Buch et al., 2022b), we use N frames, which are sampled uniformly or at random as inputs for VideoQA. While ATP uses all N frames as input, it makes use of a"}, {"title": "3.2. Multimodal Action Grounding (MAG)", "content": "In Section 2, we summarized the prior art on video comprehension literature. Amongst these, action recognition in the videos is a non-trivial task which requires comprehending entire video information through a small number of abstract concepts. In this module, we emphasize particularly on action recognition and video description task as the NExT-QA dataset predominantly consists of descriptive, causal, and temporal questions. The proposed Multimodal Action Grounding (MAG) module is also inspired by our human-in-the-box error analysis of causal and temporal questions, where we highlighted the importance of capturing actions taken by each entity in the video. The baseline models lack the strength for identifying when and where certain actions took place in the video, and we propose to provide this missing context. To achieve this goal, we propose an action grounding pipeline (reported in Figure 2-left), whose components are described here.\nAction Recognition: The first stage of the pipeline is to detect the different actions that are performed by the entities in the video. Following (Bertasius et al., 2021), we also model the action recognition as a high level video classification task. While the granularity of the predicted actions might be restricted at an high-level interaction as shown in examples like Figure 7, this under-representation is solved in the next stage of description generation.\nVideo Description: By leveraging pretrained object detection and interaction based description generation models (Lin et al., 2022; Contributors, 2020), for each video in the NEXT-QA dataset, we obtain the video descriptions and supplement with the action label for grounding the video in the language domain.\nVideo Grounding: In the next stage of MAG, we concatenate the action labels with video descriptions to feed as language input for multimodal video grounding. In particular, we propose to leverage salient moment detector models (Lei et al., 2021a;b) to extract the keypoints of the video by grounding the video using the actions and descriptions in the language domain to enhance the performance in question categories like when and why. The video along with the keypoint information are then streamed to the next stage for generating the final grounded representations.\nFrame Selection and Video Feature Extraction: In addition to predicting the key moments from the video, the saliency detector models also outputs the saliency scores for different frames of the video. As we also aim to reduce the compute cost and time in our research direction RQ1, we propose to directly sample the salient frames. With a pre-fixed sampling rate N, we split the video into N clips at the top N salient points, and randomly sample one/many frames from the clips. Finally, we obtain the video representations by using any standard image encoding model like CLIP (Radford et al., 2021b).\nThe output representations from the MAG model are then replaced with clip features of any previously proposed VideoQA backbone methods like EIGV (Li et al., 2022b) or HGA (Jiang & Han, 2020) models to predict the final answer using multimodal approaches. However, one should note that the final N representations are still generated using the entire video content at different stages of the MAG module and hence we classify this as full-video based approach."}, {"title": "3.3. Multimodal Robust Intervener (MRI)", "content": "(Li et al., 2022b) proposed the Equivariant and Invariant grounding approach (EIGV) to enhance the VideoQA task performance by guiding the model to leverage only the causal moments of the video (C'), where the causal component for a video V is defined by the corresponding question Q. The core principle of this approach is that any intervention to the non-causal parts(T) of the video, shouldn't result in any change in the final predicted answer \u00c2, and any change in the original question (Q) and/or changes to the causal components C of V should warrant for a change. The former principle is the concept of Invariance to non-causal frames, which is first proposed in (Li et al., 2022c), and later enhanced with the latter principle of Equivariance to casual frames and questions in (Li et al., 2022b). More concretely, this approach introduces an equivariant transformation Te to C and Q, and expects proportionate change in prediction \u00c2, and invariant transformation T\u2081 on T shouldn't trigger any variation in \u00c2.\n$T_t(\\hat{A}) = F_A(T_t(C), T_t(Q))$\n$\\hat{A} = F_A(T_i(T), Q)$\nIntervention Pipeline: Similar to the interventions performed in (Li et al., 2022b), we perform a linear mixup of two videos (V and V') generating a final video V* and perform perturbations on the mixed video, question and answer datapoints. To achieve the equivariance-invariance principle, first a Scene Intervener creates the tuple (Q*, A*,C*,T*) with a similar linear mixup principle for questions, answers, causal, and non-causal components from the original videos V and V', and their questions and answers. In the next step, a Causal Disruptor generates additional A+ and A\u00af examples using the mixup data to use a Contrastive Learning objective for pushing A+ toward A and pulling A\u00af away from A. The positive and negative examples are generated by perturbing T* and C* components of V* respectively using random clips from other videos stored in a memory"}, {"title": "3.4. Smart Sub-part Sampler (S3)", "content": "In addition to the smart frame sampling technique used in the MAG approach, we also propose two more ways to perform a smart sampling of the entire video.\nTeacher-Student Sampler: Given a question Q, we score (student model) each of the frame from pool of input video frames and select only the top N frames to feed as input to the PCMA model (teacher model), and design the following custom loss for training the student model using teacher loss.\n$L_t = L_{ce} = ce \\text{ loss of the teacher model}$\n$L_f =\\begin{cases}P_s(f|V, Q) \\times L_s, & \\text{if } f \\in \\text{top-N frames} \\\\P(f/V, Q) \\times \\lambda, & \\text{otherwise}\\end{cases}$\n$L = \\text{Batch Mean}(\\frac{1}{\\vert f \\vert} \\sum_f L_f)$\nwhere Ls = Lce is the loss obtained from the teacher PCMA on a single datapoint, Lf is the loss for a single frame in the video V and L, is the final aggregated loss of the student model. As evident from the loss function, this model is targeted at improving the PCMA component by specifying what frames to be chosen from the video as the input for the cross modal attention and aggregation procedure.\nRL for VideoQA: On the contrary, instead of using a multi-stage approach, we also explore the possibility of reformulating the VideoQA as an end-to-end reinforcement learning (RL) problem. In this setting, an agent is designed to select the most relevant frames from the video given the question. Thus, at every state, the agent picks an action to select or not-select a frame. Based on the frames selected by the agent and the input question, a prediction backbone attempts to predict the correct answer. Hence, we design the reward for the RL agent as the negative loss obtained at the end of each episode. To control the size of the video being used at the end, we also penalize the agent actions proportional to the number of frames selected by the agent as relevant. An episode is defined as the agent being able to view all the frames in the video. Since the reward is very sparse, we also condition the agent to differentiate between next frame and a random frame as the intermediate dense reward."}, {"title": "4. Experimental Setup", "content": "In this section, we elaborate on the dataset, inputs, architectural, and hyperparameter choices for the proposed novel components PCMA, MAG, MRI, and S3. We also briefly describe our various experimental settings which we designed to the four research directions through the four questions (RQs) mentioned in Section 1. More nuanced changes to the experiments and detailed analysis of the effectiveness of these components are presented in the Results section 5.\n4.1. Dataset\nFor this work, we run all the approaches on the NEXT-QA (Xiao et al., 2021a) dataset. It consists of 5,440 videos with an average duration length of 44s. There are a total of 52,000 question answer pairs divided into causal, temporal and descriptive types respectively. This dataset is challenging due to the fact that the model needs to perform both causal and temporal reasoning on the video frames in order to answer the questions. The input modalities for the task are the visual modality from the videos and text modality from questions and answer choices.\n4.2. Multimodal Baselines\nNow, we briefly describe three baseline methods which are the most recent state-of-the-art approaches that attempted to solve NEXT-QA task through different hypotheses.\nAtemporal Probe: (Buch et al., 2022a) It investigates the need for multiple frames to solve a task. This model attempts to pick the best frame from the video V for solving the question given the information about the questions Q and answer choices C.\nEIGV: (Li et al., 2022a) This model takes a causal approach towards solving the problem by predicting the relevant (C) and irrelevant parts (T) of the video V and replacing them with clips from other videos to make sure that the model is invariant to a change in the irrelevant part and equivariant to a change in the relevant part.\nVGT: (Xiao et al., 2022) The most recently proposed VGT model identifies the relationship between objects obtained with the help of object detection. They then model the interactions by constructing a graph of objects and the spatial relations between them. Finally, they perform late multimodal fusion on this aggregated representation with the"}, {"title": "4.3. Proposed Approach", "content": "For train, test and validation, we use the same splits as those provided by the dataset. In addition, the dataset also contains pre-computed features corresponding to video, questions and answers. The video features are obtained by first dividing the video into 16 clips of equal length and then running through 3D ResNeXt-101 (Hara et al., 2017) to obtain embeddings of dimension 16 x 4096. For text modality, the dataset also provides BERT (Devlin et al., 2018) embeddings of Question-Answer pairs of the form QUESTION (SEP) ANSWER which (Xiao et al., 2021a) have fine-tuned on the NEXT dataset to generate embeddings of dimension 5 x 37 x 768. For all our experiments, we follow a similar process of splitting the video into equal length clips and select one frame for single-frame methods and aggregate all frames for complete-video based methods. Unless stated otherwise, the Ev, Et are CLIP for ATP and PCMA, and ResNeXt-101, BERT for EIGV based methods. The details on encoding clips for VGT model are presented in Appendix A.2. The specific architecture choices made for the PCMA, MAG, MRI and S3 modules are as follows.\nPCMA: We sample the first frame from each of the clips, which is treated as the representative frame of the entire clip (Buch et al., 2022b). The 512 dimensional CLIP (Radford et al., 2021a) representations of the frames and the question, answer choices are used as the inputs in the architecture shown in Figure 1. For the cross modal and self modal attention and aggregation mechanisms, we leverage the Multiheaded Transformer (Vaswani et al., 2017) units. For final prediction, we simply align answer choice representation with the aggregated video representation through cosine similarity.\nMAG: The input video is processed at 1FPS due to computation limitations and fed as inputs for action recognition and textual description models. In this work, we use the pretrained TimeSformer (Bertasius et al., 2021) model trained on the Kinetics-400 dataset (Kay et al., 2017) for action recognition, and SwinBERT (Lin et al., 2022) model, pretrained on the VATEX dataset (Wang et al., 2019) to get video descriptions. We use the pretrained models and implementations from MMAction2 (Contributors, 2020) for both the tasks. For further grounding the video we first concatenate the embeddings corresponding to ACTION \u00d7 [SEP] \u00d7 DESCRIPTION X [SEP] \u00d7 QUESTION \u00d7 (SEP) x ANSWER to obtain 768 dimensional BERT (Devlin et al., 2018) features. The generated textual representation along with the sampled"}, {"title": "5. Results and Discussion", "content": "In this section, we report the effectiveness of the proposed approach through several quantitative, qualitative, and ablation studies. The comprehensive set of results are reported in Table 1. Overall, the proposed approach of using PCMA in single-frame based models outperforms ATP by huge margin, and the MAR + MRI improve the performance of complete-video based baseline EIGV by 1.12%.\n5.1. Effectiveness of PCMA Aggregation\nIn Table 1, the first part showcases the effectiveness of using PCMA aggregation over ATP's single-frame hypothesis. To reiterate, the improvements of PCMA over ATP are two-fold (1) explicit cross modal attention based residual learning to enhance uni-modal information, and (2) question conditioned video information aggregation instead of single-frame sampling. Though the PCMA uses an aggregation of N = 16 frames and ATP selects one out of 16, the compute resources are almost similar for both approaches since ATP also uses 16 frames to arrive at single frame. The performance boost of ~4.4% over ATP advocates for the use of PCMA2.\nMitigating Sampling Bias As mentioned in Section 4, the ATP and vanilla PCMA uses the first frame of each clip to represent the entire clip. However, this approach leads to sampling bias and hence lower generalization to test environment. We mitigate this by splitting the video into a higher number N = 80 clips, randomly sample one frame for each clip, and at each training iteration, sub-sample N ="}, {"title": "5.2. MAR Module Contribution", "content": "As the MAR module simply generates N = 16 length representations from a video V, we need a backbone model to perform the downstream we replace the CLIP representations used in the PCMA model and incorporate the MAR module features. As observed from middle two rows of Table 1, PCMA+MAR consistently improves over vanilla PCMA (except when using pseudo 80 frames). MAR-16 and 32 correspond to two sampling strategies for summarizing V through actions as described in Section 4."}, {"title": "5.3. Improvement in Robustness through MRI", "content": "To reiterate, the key principle of EIGV method is to correctly identify the causal and non-causal/complement scenes through do(.) calculus operations like intervention and thereby enhance the overall performance of VideoQA task. However, in the baseline EIGV method, the do(.) operations are performed using random clips, causing inconsistencies in the videos. Table 2 reports the results from replacing the random do(.) with MNSE-do(.). Here F1, F2, F3 corresponds to the three types of MSNE as described in Section 4.3. We observe a consistent drop in performance with the three types of MSNE-do(.) interventions.\nTo understand this effect, we further experimented with"}, {"title": "5.4. Joining the components together", "content": "Previously, in the MAR analysis part, we have discussed about the improvements contributed by the MAR representations over standard CLIP visual representations for the frames. Now we try to combine the MAR module with the vanilla and MNSE based MRI intervention methods. As observed from Table 1, the best performing method is the MAR-32 module with EIGV baseline as as the VideoQA backbone, in the presence of robust interventions from the proposed MNSE algorithm. Note that we do not perform any joint modeling approaches between the PCMA and MRI because the intervention mechanism of the EIGV is not compatible with the cross modal and uni modal self attention interactions, and the changes that would make it possible are not trivial. However, since the MAG is just a novel method of generating action grounded video representations, it is compatible with both MRI and PCMA models. As mentioned several times earlier, the EIGV is a complete video based approach and ATP is a single-frame based approach. Following this notion, we have ordered the different models"}, {"title": "5.5. Issues with the VGT model", "content": "Although VGT is the most recent best performing baseline proposed by (Xiao et al., 2022), we refrain from using it in our experiments and instead choose the next best baseline method EIGV (Li et al., 2022b). Being easily transferable, we modified the multimodal fusion layers of VGT with those of the PCMA model, and to our surprise, we have observed a drastic drop in performance across all question types (by at least 6%). Upon further experimentation, we found that removing the answer component from the PCMA's cross modal fusion led to a slight improvement of 0.22% in the overall test accuracy (Figure 5).\nTo understand this weird phenomenon, we have performed"}, {"title": "6. Conclusion and Future Work", "content": "In this work, we established the need for solving the proposed four novel research questions and through extensive experiments, we have arrived at the answers for each of them. We showcased the fusion power of the PCMA modules for the NEXT dataset and leveraged the action grounded video representations to enhance both the PCMA and EIGV+MRI models in single-frame and complete-video based methods. In addition to improved robustness, we have also observed that the MRI approach helps in achieving state-of-the-art performance on the NEXT dataset.\nIn future we intend to extend these components to the other VideoQA datasets. Our ablation study on the VGT baseline method advocates for the need of such extensive analyses in future to eliminate any spurious correlations. Our teacher-student model also acts as the proof-of-concept for the future research threads on developing smart sampling techniques."}, {"title": "A. Appendix", "content": "A.1. EIGV scene intervener and causal disruptor modules\nThe scene intervener creates the q*, a*, c* and t* which are the casual factors - Question, Answer, Causal scene and Complement scene respectively and c* and t* are from video v. They are obtained as linear interpolation between two data points on their causal factor - C, Q and A with different mixing ratios \u03bf ~Beta(\u03b1, \u03b1) for causal scene mixup and \u2081 ~U(0,1) for complement scene mixup as shown below.\nc* = \u03bb\u03bf\u0109 + (1 \u2013 \u03bb\u03bf)\u03b5' (2)\nq* = \u03bb\u03bf\u011d + (1 \u2013 \u03bb\u03bf) (3)\n\u03b1* = \u03bb\u03bf\u1fb6 + (1 \u2013 \u03bb\u03bf) \u03b1' (4)\nt* = \u03bb\u2081\u0142 + (1 \u2212 1)\u0390' (5)\nwhere \u0109', \u011d', \u00e2' and t' come from a second data sample (V') used for the mixup. The causal disruptor combines both these elements into a contrastive loss where a positive video V+ is obtained by disrupting V* on the complement by substituting with an item randomly sampled from a memory bank \u03c0. Negative counterpart V- obtained using E-intervention on V* by substituting question-critical causal part. It also creates linguistic alternatives as negative sample by disrupting (V*, q*) to (V*, qr) where qr is a random sample question.\na = F\u2081(V*, q*), (6)\na+ = F(V+, q*), (7)\na\u00af = F\u2081[concat((V\u00af, q*), (V*, qr))] (8)\nThe contrastive loss is then given by InfoNCE as:\n$L_{CL} = -log(\\frac{exp(a^Ta^+)}{exp(a^Ta^+)+ \\sum_{n=1}^{N} exp(a^Ta^n)})$ (9)\nwhere N is the number of negative samples, and a denotes answer generated by one negative sample. This combined with ERM objective, we have:\n$L_{EIGV} = E_{(v,q,a)\\in O} L_{ERM} + \\beta L_{CL}$ (10)\nwhere O is set of training instances, \u03b2 is a hyperparameter to balance CL strength.\nA.2. Video Graph Transformer\nContrary to the single frame/single clip approaches such as (Li et al., 2021a; Buch et al., 2022b), or identifying holistic regions of the video for answering questions (Yu et al., 2018; Zhu & Yang, 2020; Lei et al., 2018), (Xiao et al., 2022) proposes to use the objects and their spatio-temporal relationships, which vary across time, in addition to using the video, questions and answer choices as the modalities of information. Towards this end they proposed a Dynamic Graph Transformer based VideoQA model (VGT) with emphasis on capturing the dynamics of objects and relations for better reasoning, and separate transformers for text and video modalities to better encode the information from the corresponding modalities. The different components of the model are as follows - video graph representation: they split a video into n clips and sample evenly distributed frames for each clip to identify the objects and the spatial locations of the objects in each frame of a clip. The consistency of objects across frames is obtained through IoU overlap and similarity of the objects. Once the objects are fixed for a clip (across frames), the graph is constructed at each frame by computing edge scores as\n$E_{i,j} = \\sigma(W_{ak}(F_{Oi}) W_{ar} (F_{Oj}))$ (11)\nwhere i, j are the objects, Fo\u2081 is the encoding of object i, which is made up of ROI pooling of visual and spatial locations, and the & matrices are the transformations learnt in the model. A temporal graph transformer modifies the edge weights as the node representations propagate through transformer layers, i.e., the object representations enhance by attending to the other objects and the edge weights are updated accordingly. A spatial graph convolution operation uses the graph attention convolution (Kipf & Welling, 2016) to encode spatial relations, and a final hierarchical aggregation layer, which is a weighted representation of the nodes in the graphs of the frames. Finally a mean pooling layer aggregates the clips to obtain the final video relational representation.\nAfter encoding text modality using the Eq, Ec modules, the cross modal interactions are captured through additional attention modules. The final answer is generated based on the multimodal qv and answer representations.\n$E_{qv} = E_v + \\sum_{m=1}^{M} \\beta_m E_{qm} \\text{ where } \\beta = \\sigma(E_v \u00a9 E_q)$ (12)\n$S_i = F_{qv} (E_{qv}) F_C(E_C(C_i)) \\text{ for } i \\in [1, 5]$ (13)\nwhere E is the video representation obtained from graph transformer and Eqm is the representation for mth token in the question, Fqv, Fc are transformations of question-video and candidate answer representations. The final objective is a simple ERM softmax loss as defined in Equation 1.\nA.3. Action Grounding Ablations\nIn Table 4, list of acronyms:\nN: Number of Nearest Neighbors\nNumFrames: Number of Frames"}]}