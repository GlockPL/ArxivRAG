{"title": "Causal Understanding For Video Question Answering", "authors": ["Bhanu Prakash Reddy Guda", "Tanmay Kulkarni", "Adithya Sampath", "Swarnashree Mysore Sathyendra"], "abstract": "Video Question Answering is a challenging task, which requires the model to reason over multiple frames and understand the interaction between different objects to answer questions based on the context provided within the video, especially in datasets like NEXT-QA (Xiao et al., 2021a) which emphasize on causal and temporal questions. Previous approaches leverage either sub-sampled information or causal intervention techniques along with complete video features to tackle the NEXT-QA task. In this work we elicit the limitations of these approaches and propose solutions along four novel directions of improvements on the NEXT-QA dataset. Our approaches attempts to compensate for the shortcomings in the previous works by systematically attacking each of these problems by smartly sampling frames, explicitly encoding actions and creating interventions that challenge the understanding of the model. Overall, for both single-frame (+6.3%) and complete-video (+1.1%) based approaches, we obtain the state-of-the-art results on NEXT-QA dataset.", "sections": [{"title": "1. Introduction", "content": "Understanding video data brings us a step closer towards real world embodied agents. Previous challenges such as Visual Question Answering attempted to answer questions based on the information present in a single image or a scene. Videos add additional challenges with respect to understanding interactions across multiple frames, so as to reason about the situation being described. Not only do the agents need to understand events that have happened in previous frames, but they also need to reason about how the events in the previous frames affect the events happening in the current and/or future frames.\n\nCurrent state of the art models such as VGT (Xiao et al., 2022) attempt to connect relationships through associations between the video and text features through convoluted modules, which are prone to learning spurious correlations, and are often hard to debug. Another line of work (Li et al., 2022c;a) attempts to take advantage of the invariance of the non-causal frames and equivariance of the causal frames as additional sources of supervision for enhancing the predictions on the data. However, both of these approaches seem to be lacking in generalization power, as they are only able to get test performance of \u2248 50% while human performance is around 90% (Xiao et al., 2021a).\n\nOn the other hand, approaches like ClipBERT (Lei et al., 2021c) promote methods which rely on a subsampled information from the entire video to achieve compute tractability for VideoQA task. To this end, recently (Buch et al., 2022b) proposed the Atemporal Probe (ATP) approach for sampling the single most relevant frame for answering video-question pairs of VideoQA datasets MSR-VTT-MC (Xu et al., 2016), VALUE-How2QA (Li et al., 2020; 2021b). However, to achieve state-of-the-art performance in datasets like NEXT (Xiao et al., 2021a), they concluded that entire video is required as the dataset predominantly consists of causal and temporal questions which require answering why . happened or when . happened in a given video.\n\nIn this work, we broadly categorize the prior works into single-frame based (ClipBERT, ATP) and complete-video based (IGV, EIGV, VGT) methods, and attempt to solve some of the severe limitations present in both categories of models. Motivated by the extensive analysis performed in our previous mid-term report, we propose to attempt the NEXT-QA video question answering task, containing causal, temporal and descriptive video question answer pairs, from four novel research directions. Extending prior art, we finally formulate the following four research questions (ideas), one for each direction, and aim to answer these through our proposed approaches.\n\nRQ1: Instead of using either of the extremes, can we leverage a smart aggregation of the sub-sampled information to improve low compute approaches like ATP?\nRQ2: Can we improve the robustness of causal component based answering models like EIGV through hard-negative mining approaches, which could lead to performance boost?\nRQ3: Can we aim to improve grounding of video representations with the questions by extracting suitable information from videos like actions and descriptions?\nRQ4: Can we smartly identify the right frames/clips that need to be sampled in single-frame and alike low compute approaches for avoiding loss of information?\n\nIn the quest of finding answers for these questions, we contribute the following to the research on VideoQA task.\n\n1.  Using only single-frame level compute, we propose PCMA model to bridge the gap between ATP and VGT baselines. In general, proposed PCMA is easy to transfer and replace multimodal fusion layers of any model.\n\n2.  We propose to enhance video features by extracting and grounding with salient action and description information through MAR pipeline. These video features can be used with EIGV, ATP models with minimal changes.\n\n3.  We highlight the drawbacks of using random clip based do(.) for robustness and propose MRI pipeline with MNSE algorithm to generate hard-contrastive examples. The MRI also contributed to our final SOTA model.\n\nIn addition to the obvious contributions, we also summarize the effectiveness of reinforcement learning and teacher-student paradigms for smartly sampling content without loss of information towards solving RQ4. Overall we achieve a ~6.3% improvement in test accuracy for the single-frame models using a hybrid of the proposed PCMA and MAR techniques, and also achieve the state-of-the-art test performance (+1.1%) on the NEXT-QA dataset through the MAR and MRI techniques in the complete-video based approaches."}, {"title": "2. Related Work", "content": "Invariant Learning (Leen, 1994; Sohn & Lee, 2012; Xiao et al., 2021b; Benton et al., 2020; Misra & Maaten, 2020) show invariant learning(IL) as a paradigm to generalize better to data outside training distribution. For multi-modal tasks such as VideoQA, IL as means of visual grounding(Li et al., 2022c), equivariant grounding when changes are made to casual/temporal scenes of videos(Li et al., 2022b) and improving intrinsic interpreatibility have been explored on datasets like NEXT-QA(Xiao et al., 2021a).\n\nContrastive Learning for better Interventions We look at using constrastive loss for learning better intervention mechanisms. Current constrastive losses can be instance based - InfoNCE (Oord et al., 2018), UberNCE (Han et al., 2020), MIL-NCE (Miech et al., 2020); clustering based - CentreNCE (Diba et al., 2021). (Dave et al., 2022) propose a contrastive learning approach to preserve temporally varying video features using non-overlapping clips of the same video as negative samples. VQA tasks often rely on spurious relationships between language and images(Agrawal et al., 2016; Zhang et al., 2016; Johnson et al., 2017). This can be alleviated with contrastive loss approach proposed in (Li et al., 2022b), which we modify to provide a more robust approach for the VQA task.\n\nGrounded Video Representation and Aggregation Prior approaches (Storks et al., 2019; Wang et al., 2020; Fang et al., 2020) attempt to extract causal knowledge using natural language. In visual domain, causal reasoning has been used for downstream applications such as image classification (Lopez-Paz et al., 2017), visual dialog (Qi et al., 2020), image captioning (Zhou et al., 2020) and identifying hidden actions in videos (Fire & Zhu, 2017). Temporal reasoning has also been attempted within visual domain for understanding events in videos (Zhou et al., 2018; Lin et al., 2019; Arnab et al., 2021). Multimodal approaches for this include Uniter (Chen et al., 2020), Vilbert (Lu et al., 2019), Videobert (Sun et al., 2019) that use supervised/weakly-supervised methods of encoding visual-semantic knowledge into representations.\n\nTo the best of our knowledge, our proposed approaches of using cross modal grounded aggregation of frames from videos to minimize information loss in single-frame based approaches, enriching the intervention of videos by hard mining similar scenes, grounding videos by using salient action recognition and description modules, and smart-sampling of frames/clips from videos to maximize information gain are the first attempts towards enhancing VideoQA models through multiframe grounded approaches."}, {"title": "3. Proposed Approach", "content": "In this section, we start by briefly describing the VideoQA task setup and then proceed to elaborate on various components of the proposed approaches, which we use towards answering our four research questions. Given an input video V, and a question Q, and answer choices C = [Ci]=1, the objective is to minimize expected risk of using a multimodal framework F in predicting answer A e C i.e.,\n\nmin $L_{ERM}(F(\\E_{V}(V); E_{t}(Q); E_{t}(C)), A)$ (1)\n\nwhere Ev, Et, Et are the encoders that generate the representations for the V, Q, and C respectively. Often, we represent each of the choices individually i.e., E\u2081(C) = [&(C\u2081)]=1, and the encoders for the question and choices are same, as the modality of information for both of them is text (hence Et).\n\n3.1. Pairwise Cross Modal Aggregation (PCMA)\n\nAs highlighted in Section 1, we can briefly categorize the VideoQA models into frame-based and clip-based methods. The components that we describe now are targeted at improving the less-compute frame-based methods. Following (Buch et al., 2022b), we use N frames, which are sampled uniformly or at random as inputs for VideoQA. While ATP uses all N frames as input, it makes use of a"}, {"title": "3.2. Multimodal Action Grounding (MAG)", "content": "In Section 2, we summarized the prior art on video comprehension literature. Amongst these, action recognition in the videos is a non-trivial task which requires comprehending entire video information through a small number of abstract concepts. In this module, we emphasize particularly on action recognition and video description task as the NEXT-QA dataset predominantly consists of descriptive, causal, and temporal questions. The proposed Multimodal Action Grounding (MAG) module is also inspired by our human-in-the-box error analysis of causal and temporal questions, where we highlighted the importance of capturing actions taken by each entity in the video. The baseline models lack the strength for identifying when and where certain actions took place in the video, and we propose to provide this missing context. To achieve this goal, we propose an action grounding pipeline (reported in Figure 2-left), whose components are described here.\n\nAction Recognition: The first stage of the pipeline is to detect the different actions that are performed by the entities in the video. Following (Bertasius et al., 2021), we also model the action recognition as a high level video classification task. While the granularity of the predicted actions might be restricted at an high-level interaction as shown in examples like Figure 7, this under-representation is solved in the next stage of description generation.\nVideo Description: By leveraging pretrained object detection and interaction based description generation models (Lin et al., 2022; Contributors, 2020), for each video in the NEXT-QA dataset, we obtain the video descriptions and supplement with the action label for grounding the video in the language domain.\nVideo Grounding: In the next stage of MAG, we concatenate the action labels with video descriptions to feed as language input for multimodal video grounding. In particular, we propose to leverage salient moment detector models (Lei et al., 2021a;b) to extract the keypoints of the video by grounding the video using the actions and descriptions in the language domain to enhance the performance in question categories like when and why. The video along with the keypoint information are then streamed to the next stage for generating the final grounded representations.\nFrame Selection and Video Feature Extraction: In addition to predicting the key moments from the video, the saliency detector models also outputs the saliency scores for different frames of the video. As we also aim to reduce the compute cost and time in our research direction RQ1, we propose to directly sample the salient frames. With a pre-fixed sampling rate N, we split the video into N clips at the top N salient points, and randomly sample one/many frames from the clips. Finally, we obtain the video representations by using any standard image encoding model like CLIP (Radford et al., 2021b).\n\nThe output representations from the MAG model are then replaced with clip features of any previously proposed VideoQA backbone methods like EIGV (Li et al., 2022b) or HGA (Jiang & Han, 2020) models to predict the final answer using multimodal approaches. However, one should note that the final N representations are still generated using the entire video content at different stages of the MAG module and hence we classify this as full-video based approach."}, {"title": "3.3. Multimodal Robust Intervener (MRI)", "content": "(Li et al., 2022b) proposed the Equivariant and Invariant grounding approach (EIGV) to enhance the VideoQA task performance by guiding the model to leverage only the causal moments of the video (C'), where the causal component for a video V is defined by the corresponding question Q. The core principle of this approach is that any intervention to the non-causal parts(T) of the video, shouldn't result in any change in the final predicted answer \u00c2, and any change in the original question (Q) and/or changes to the causal components C of V should warrant for a change. The former principle is the concept of Invariance to non-causal frames, which is first proposed in (Li et al., 2022c), and later enhanced with the latter principle of Equivariance to casual frames and questions in (Li et al., 2022b). More concretely, this approach introduces an equivariant transformation Te to C and Q, and expects proportionate change in prediction \u00c2, and invariant transformation T\u2081 on T shouldn't trigger any variation in \u00c2.\n\n$T_{e}(\\hat{A}) = F_{A}(T_{e}(C), T_{e}(Q))$\n\n$\\hat{A} = F_{A}(T_{i}(T), Q)$\n\nIntervention Pipeline: Similar to the interventions performed in (Li et al., 2022b), we perform a linear mixup of two videos (V and V') generating a final video V* and perform perturbations on the mixed video, question and answer datapoints. To achieve the equivariance-invariance principle, first a Scene Intervener creates the tuple (Q*, A*,C*,T*) with a similar linear mixup principle for questions, answers, causal, and non-causal components from the original videos V and V', and their questions and answers. In the next step, a Causal Disruptor generates additional A+ and A\u00af examples using the mixup data to use a Contrastive Learning objective for pushing A+ toward A and pulling A\u00af away from A. The positive and negative examples are generated by perturbing T* and C* components of V* respectively using random clips from other videos stored in a memory"}, {"title": "3.4. Smart Sub-part Sampler (S3)", "content": "In addition to the smart frame sampling technique used in the MAG approach, we also propose two more ways to perform a smart sampling of the entire video.\n\nTeacher-Student Sampler: Given a question Q, we score (student model) each of the frame from pool of input video frames and select only the top N frames to feed as input to the PCMA model (teacher model), and design the following custom loss for training the student model using teacher loss.\n\n$L_{t} = L_{ce} = ce loss of the teacher model$\n\n$L_{f} = \\begin{cases}P_{s}(f|V, Q) \\times L_{s}, \\text{if } f \\in \\text{top-N frames} \\\\ P(f/V, Q) \\times \\lambda, \\text{otherwise} \\end{cases}$\n\n$L = Batch Mean(\\frac{1}{\\sum f} L_{f})$\n\nwhere Ls = Lce is the loss obtained from the teacher PCMA on a single datapoint, Lf is the loss for a single frame in the video V and L, is the final aggregated loss of the student model. As evident from the loss function, this model is targeted at improving the PCMA component by specifying what frames to be chosen from the video as the input for the cross modal attention and aggregation procedure.\n\nRL for VideoQA: On the contrary, instead of using a multi-stage approach, we also explore the possibility of reformulating the VideoQA as an end-to-end reinforcement learning (RL) problem. In this setting, an agent is designed to select the most relevant frames from the video given the question. Thus, at every state, the agent picks an action to select or not-select a frame. Based on the frames selected by the agent and the input question, a prediction backbone attempts to predict the correct answer. Hence, we design the reward for the RL agent as the negative loss obtained at the end of each episode. To control the size of the video being used at the end, we also penalize the agent actions proportional to the number of frames selected by the agent as relevant. An episode is defined as the agent being able to view all the frames in the video. Since the reward is very sparse, we also condition the agent to differentiate between next frame and a random frame as the intermediate dense reward."}, {"title": "4. Experimental Setup", "content": "In this section, we elaborate on the dataset, inputs, architectural, and hyperparameter choices for the proposed novel components PCMA, MAG, MRI, and S3. We also briefly describe our various experimental settings which we designed to the four research directions through the four questions (RQs) mentioned in Section 1. More nuanced changes to the experiments and detailed analysis of the effectiveness of these components are presented in the Results section 5.\n\n4.1. Dataset\n\nFor this work, we run all the approaches on the NEXT-QA (Xiao et al., 2021a) dataset. It consists of 5,440 videos with an average duration length of 44s. There are a total of 52,000 question answer pairs divided into causal, temporal and descriptive types respectively. This dataset is challenging due to the fact that the model needs to perform both causal and temporal reasoning on the video frames in order to answer the questions. The input modalities for the task are the visual modality from the videos and text modality from questions and answer choices.\n\n4.2. Multimodal Baselines\n\nNow, we briefly describe three baseline methods which are the most recent state-of-the-art approaches that attempted to solve NEXT-QA task through different hypotheses.\n\nAtemporal Probe: (Buch et al., 2022a) It investigates the need for multiple frames to solve a task. This model attempts to pick the best frame from the video V for solving the question given the information about the questions Q and answer choices C.\n\nEIGV: (Li et al., 2022a) This model takes a causal approach towards solving the problem by predicting the relevant (C) and irrelevant parts (T) of the video V and replacing them with clips from other videos to make sure that the model is invariant to a change in the irrelevant part and equivariant to a change in the relevant part.\n\nVGT: (Xiao et al., 2022) The most recently proposed VGT model identifies the relationship between objects obtained with the help of object detection. They then model the interactions by constructing a graph of objects and the spatial relations between them. Finally, they perform late multimodal fusion on this aggregated representation with the"}, {"title": "4.3. Proposed Approach", "content": "information from question and the answer.\n\nWe compare our proposed modules, PCMA with the ATP method (single-frame), and MAG, MRI components against EIGV and VGT models (complete-video).\n\nFor train, test and validation, we use the same splits as those provided by the dataset. In addition, the dataset also contains pre-computed features corresponding to video, questions and answers. The video features are obtained by first dividing the video into 16 clips of equal length and then running through 3D ResNeXt-101 (Hara et al., 2017) to obtain embeddings of dimension 16 x 4096. For text modality, the dataset also provides BERT (Devlin et al., 2018) embeddings of Question-Answer pairs of the form QUESTION (SEP) ANSWER which (Xiao et al., 2021a) have fine-tuned on the NEXT dataset to generate embeddings of dimension 5 x 37 x 768. For all our experiments, we follow a similar process of splitting the video into equal length clips and select one frame for single-frame methods and aggregate all frames for complete-video based methods. Unless stated otherwise, the Ev, Et are CLIP for ATP and PCMA, and ResNeXt-101, BERT for EIGV based methods. The details on encoding clips for VGT model are presented in Appendix A.2. The specific architecture choices made for the PCMA, MAG, MRI and S3 modules are as follows.\n\nPCMA: We sample the first frame from each of the clips, which is treated as the representative frame of the entire clip (Buch et al., 2022b). The 512 dimensional CLIP (Radford et al., 2021a) representations of the frames and the question, answer choices are used as the inputs in the architecture shown in Figure 1. For the cross modal and self modal attention and aggregation mechanisms, we leverage the Multiheaded Transformer (Vaswani et al., 2017) units. For final prediction, we simply align answer choice representation with the aggregated video representation through cosine similarity.\n\nMAG: The input video is processed at 1FPS due to computation limitations and fed as inputs for action recognition and textual description models. In this work, we use the pretrained TimeSformer (Bertasius et al., 2021) model trained on the Kinetics-400 dataset (Kay et al., 2017) for action recognition, and SwinBERT (Lin et al., 2022) model, pretrained on the VATEX dataset (Wang et al., 2019) to get video descriptions. We use the pretrained models and implementations from MMAction2 (Contributors, 2020) for both the tasks. For further grounding the video we first concatenate the embeddings corresponding to ACTION \u00d7 [SEP] \u00d7 DESCRIPTION X [SEP] \u00d7 QUESTION \u00d7 (SEP) x ANSWER to obtain 768 dimensional BERT (Devlin et al., 2018) features. The generated textual representation along with the sampled video are fed to the Moment DETR (Lei et al., 2021a) model pretrained on the QVHighlights dataset (Lei et al., 2021b), for detecting the salient moments in the video V. For sampling N salient frames, we experiment with two levels of sampling (1) MAR-16 - we first randomly sample 8 frames in the duration described by the moment coordinates with the highest saliency scores. Next, we divide the video into 4 equal segments, and randomly sample 2 frames from each section. We perform this additional sampling since we observed in our human-in-the-box analysis that some temporal questions ask questions related to what happens in the start/middle/end of the video (2) For MAR-32, we follow similar approach of 16 frames but with twice the values. The selected salient frames are encoded using the CLIP model.\n\nMRI: In the MNSE algorithm 2, for this work, we use a nearest neighbour approach for indexing and querying similar scenes. To handle nearest neighbour querying at large scale on the fly (for each batch), we implement the FAISS method proposed in (Johnson et al., 2019) for the NEXT-QA dataset using the Hugging Face framework\u00b9. We experiment with three different ways of incorporating the MNSE into EIGV model (1) Static FAISS (F1) - Populate the memory bank \u03c0with all the video data and query for nearest scenes (s) at the start of the training process and keep them constant (2) Dynamic FAISS (F2) - Perform a call to the MNSE algorithm at each batch to dynamically vary the scenes as the training progresses (3) Dynamic FAISS + \u03c0\u03b3* (F3) - Previous two setups append to \u03c0 for only one of V, V' (refer to Section 3.3). In this setup, we extend the MNSE to accommodate the linear mixup videos V* by populating with the corresponding mixup representations. Since we experiment with MRI on top of EIGV, the video representation dimension in the MNSE is of size 4096.\n\nS3: The teacher-student variant for the S3 component follows a similar cross modal attention transformer for sampling the frames. At each layer, we learn v + cross-attention(v, q), where v, q are the video and question representations. The final stage in the student model is a linear layer followed by softmax to predict the probabilities Ps(f|V, Q) (Section 3.4) for each frame f. In the RL method, we initialize the buffer of the agent with text embedding of the question. At each step, the state representation is obtained by taking the last state of the state model Fstate, which takes in frames selected by the model Vbuf as the input. The agent based on the state representation and its own policy Fpolicy decides to add Vframe. At the end of each episode, the sparse reward is obtained from the loss of the prediction backbone Fpred and the ratio of the selected frames to the total number of frames. The state model is defined as a simple transformer encoder, the policy"}, {"title": "5. Results and Discussion", "content": "In this section, we report the effectiveness of the proposed approach through several quantitative, qualitative, and ablation studies. The comprehensive set of results are reported in Table 1. Overall, the proposed approach of using PCMA in single-frame based models outperforms ATP by huge margin, and the MAR + MRI improve the performance of complete-video based baseline EIGV by 1.12%.\n\n5.1. Effectiveness of PCMA Aggregation\n\nIn Table 1, the first part showcases the effectiveness of using PCMA aggregation over ATP's single-frame hypothesis. To reiterate, the improvements of PCMA over ATP are two-fold (1) explicit cross modal attention based residual learning to enhance uni-modal information, and (2) question conditioned video information aggregation instead of single-frame sampling. Though the PCMA uses an aggregation of N = 16 frames and ATP selects one out of 16, the compute resources are almost similar for both approaches since ATP also uses 16 frames to arrive at single frame. The performance boost of ~4.4% over ATP advocates for the use of PCMA2.\n\nMitigating Sampling Bias As mentioned in Section 4, the ATP and vanilla PCMA uses the first frame of each clip to represent the entire clip. However, this approach leads to sampling bias and hence lower generalization to test environment. We mitigate this by splitting the video into a higher number N = 80 clips, randomly sample one frame for each clip, and at each training iteration, sub-sample N = 16 frames from these 80 frames. We can observe a ~2% improvement in the test accuracy i.e acts as a regularizer against overfitting on sampling bias.\n\nWith this analysis, we answer our first research question RQ1: RA1 While optimizing for compute resources through sub-sampling is important, we can mitigate the tradeoff in performance to some extent through enhanced multimodal grounding and information aggregation approaches.\n\n5.2. MAR Module Contribution\n\nAs the MAR module simply generates N = 16 length representations from a video V, we need a backbone model to perform the downstream we replace the CLIP representations used in the PCMA model and incorporate the MAR module features. As observed from middle two rows of Table 1, PCMA+MAR consistently improves over vanilla PCMA (except when using pseudo 80 frames). MAR-16 and 32 correspond to two sampling strategies for summarizing V through actions as described in Section 4.\n\n5.3. Improvement in Robustness through MRI\n\nTo reiterate, the key principle of EIGV method is to correctly identify the causal and non-causal/complement scenes through do(.) calculus operations like intervention and thereby enhance the overall performance of VideoQA task. However, in the baseline EIGV method, the do(.) operations are performed using random clips, causing inconsistencies in the videos. Table 2 reports the results from replacing the random do(.) with MNSE-do(.). Here F1, F2, F3 corresponds to the three types of MSNE as described in Section 4.3. We observe a consistent drop in performance with the three types of MSNE-do(.) interventions.\n\nTo understand this effect, we further experimented with"}, {"title": "5.4. Joining the components together", "content": "seen vs unseen intervention evaluation setups and reported the results in Table 3. The base models (top rows) for comparison are EIGV and EIGV + F2, i.e., MNSE-do(.) with type F2 intervention. We limit our experiments to one type of intervention due to space and compute restrictions. From here on wards, all the interventions of MNSE are by default type 2, unless specified. Middle rows report the result in case of seen interventions, where the inputs are the positive samples A+ from Causal Disruptor, and bottom rows corresponds to the results of unseen interventions, where we use MNSE-do(.) to intervene and predict the answer using vanilla EIGV model and vice-versa. Since we are changning only non-causal parts, in all the cases, the expected answer remains unchanged, and hence the results are directly comparable to the top rows.\n\nThrough these experiments, we observe that the drop in performance for vanilla EIGV on intervened instances is ~3.5% in the seen case and ~6.3% for unseen case. However, for the EIGV + F2 model, in both cases, the drop is consistent, in fact better in case of unseen intervention, which is the simple random intervention. We hypothesize that the results on simple random intervention are typically better than MSNE due to the abrupt discontinuities from random clips. Based on these results, we conclude that RA2 proposed MSNE based do(.) interventions lead to a better robustness of the EIGV model by precisely identifying causal components and to some extent mitigate the abrupt discontinuities in intervened videos, as the answer to our RQ2. We see that the MSNE based MRI helps boost generalization and reduce overfitting to a single type of intervention. Qualitative examples for MRI are presented in Appendix Figures 9&10\n\nPreviously, in the MAR analysis part, we have discussed about the improvements contributed by the MAR representations over standard CLIP visual representations for the frames. Now we try to combine the MAR module with the vanilla and MNSE based MRI intervention methods. As observed from Table 1, the best performing method is the MAR-32 module with EIGV baseline as as the VideoQA backbone, in the presence of robust interventions from the proposed MNSE algorithm. Note that we do not perform any joint modeling approaches between the PCMA and MRI because the intervention mechanism of the EIGV is not compatible with the cross modal and uni modal self attention interactions, and the changes that would make it possible are not trivial. However, since the MAG is just a novel method of generating action grounded video representations, it is compatible with both MRI and PCMA models. As mentioned several times earlier, the EIGV is a complete video based approach and ATP is a single-frame based approach. Following this notion, we have ordered the different models in Table 1 from top to down based on the model size and training time.\n\nAs observed from Table 1, there is a clear gap between the single-frame based and complete-video based methods. The proposed PCMA and MAR components helps to bridge this gap and the joint MAR +PCMA models lies in between the two extremes. The negative effect of adding MRI module to the vanilla EIGV is suppressed by the MAR module and hence led to the overall state-of-the-art result on the NEXT-QA dataset. We hypothesize that identifying causal components just from the multimodal encoders Ev, Et is difficult compared to the MAR augmented representations where the actions are clearly encoded in the representations. A qualitative example depicting the salient regions of the images for generating the final CLIP representations is shown in Figure 3.\n\nThus we answer our third question RQ3 as follows RA3 extracting information like actions, descriptions etc..., supplement the video representations with the help of grounding and hence improve the overall VideoQA task in both single-frame and complete-video based methods.\n\nNumber of nearest neighbor scenes Instead of greedily choosing a single nearest neighbor scene for replacing causal (C) and non-causal (T) parts of V, we also explored the effect of extracting a higher number of nearest neighbors and then randomly sampling one amongst them. To some extent the higher the number of nearest scenes, the harder is the intervention mechanism as randomness is added on top of nearest scene. The results are plotted in Figure 4. The key points to note here are: (1) Descriptive questions are more susceptible to the changes, and hence as the NN value increases the performance decreases in both MAR-16 and MAR-32 (2) This observation is exactly reverse for the temporal questions and (3) In causal question, the NN=3 works better than vanilla and NN=5 cases. This result showcases that it is not trivial to design a single best mechanism across all types of questions."}, {"title": "5.5. Issues with the VGT model", "content": "Although VGT is the most recent best performing baseline proposed by (Xiao et al., 2022), we refrain from using it in our experiments and instead choose the next best baseline method EIGV (Li et al., 2022b). Being easily transferable, we modified the multimodal fusion layers of VGT with those of the PCMA model, and to our surprise, we have observed a drastic drop in performance across all question types (by at least 6%). Upon further experimentation, we found that removing the answer component from the PCMA's cross modal fusion led to a slight improvement of 0.22% in the overall test accuracy (Figure 5).\n\nTo understand this weird phenomenon, we have performed"}, {"title": "6. Conclusion and Future Work", "content": "In this work, we established the need for solving the proposed four novel research questions and through extensive experiments, we have arrived at the answers for each of them. We showcased the fusion power of the PCMA modules for the NEXT dataset and leveraged the action grounded video representations to enhance both the PCMA and EIGV+MRI models in single-frame and complete-video based methods. In addition to improved robustness, we have also observed that the MRI approach helps in achieving state-of-the-art performance on the NEXT dataset.\n\nIn future we intend to extend these components to the other VideoQA datasets. Our ablation study on the VGT baseline method advocates for the need of such extensive analyses in future to eliminate any spurious correlations. Our teacher-student model also acts as the proof-of-concept for the future research threads on developing smart sampling techniques."}, {"title": "A. Appendix", "content": "A.1. EIGV scene intervener and causal disruptor modules\n\nThe scene intervener creates the q*, a*, c* and t* which are the casual factors - Question, Answer, Causal scene and Complement scene respectively and c* and t* are from video v. They are obtained as linear interpolation between two data points on their causal factor - C, Q and A with different mixing ratios \u03bf ~Beta(\u03b1, \u03b1) for causal scene mixup and \u2081 ~U(0,1) for complement scene mixup as shown below.\n\nc* = \u03bb\u03bf\u0109 + (1 \u2013 \u03bb\u03bf)\u03b5' (2"}]}