{"title": "Improving Steering and Verification in Al-Assisted Data Analysis with Interactive Task Decomposition", "authors": ["Majeed Kazemitabaar", "Jack Williams", "Ian Drosos", "Tovi Grossman", "Austin Z. Henley", "Carina Negreanu", "Advait Sarkar"], "abstract": "LLM-powered tools like ChatGPT Data Analysis, have the potential\nto help users tackle the challenging task of data analysis program-\nming, which requires expertise in data processing, programming,\nand statistics. However, our formative study (n=15) uncovered se-\nrious challenges in verifying Al-generated results and steering\nthe AI (i.e., guiding the AI system to produce the desired output).\nWe developed two contrasting approaches to address these chal-\nlenges. The first (STEPWISE) decomposes the problem into step-\nby-step subgoals with pairs of editable assumptions and code until\ntask completion, while the second (PHASEWISE) decomposes the\nentire problem into three editable, logical phases: structured in-\nput/output assumptions, execution plan, and code. A controlled,\nwithin-subjects experiment (n=18) compared these systems against\na conversational baseline. Users reported significantly greater con-\ntrol with the STEPWISE and PHASEWISE systems, and found interven-\ntion, correction, and verification easier, compared to the baseline.\nThe results suggest design guidelines and trade-offs for AI-assisted\ndata analysis tools.", "sections": [{"title": "1 INTRODUCTION", "content": "Data science often involves large datasets, source code, domain ex-\npertise, and unwritten assumptions [64]. The process of extracting\ninsights from data [90] for decision making and knowledge discov-\nery [19] has several documented challenges [7, 64]. Data scientists\nspend considerable time inspecting data, writing single-use scripts,\n\"gluing together\" data sources, cleaning messy data, and document-\ning their efforts [7, 44, 45, 64]. In fact, data scientists describe the\nneed to \"have a conversation\" with their data to understand it [64].\nRecent advancements in AI and particularly the natural language\nprocessing and code generation capabilities of Large Language\nModels (LLMs) have shown promise to facilitate data science tasks."}, {"title": "2 RELATED WORK", "content": "2.1 AI-assisted Data Analysis\nPrevious work has considered how data transformation scripts can\nbe synthesized from demonstrations (e.g., Wrangler [31, 39]), fol-\nlowing an influential line of research that synthesizes programs by\nexample in data wrangling contexts (e.g., [29]), which may include\nnatural language [30]. These can be constrained to use specific APIs\nsuch as pandas, using generator-based synthesis (e.g., AutoPandas\n[4]). Scripts can also be synthesized based on heuristics of data\nquality improvement (e.g, CoWrangler [8]), and data preparation\nheuristics can also be learned from corpora (e.g., Auto-Suggest\n[95]).\nMore recently, a number of commercialized LLM supported\ndata analysis tools have become available. These enable data sci-\nentists to access AI-powered chat assistants within their notebook\n(such as Anaconda [1], Databricks [13], and Jupyter AI [36]), and\nother alternate data-science environments (e.g., DataChat AI [14],\nSQL and file editors for Databricks, etc.). The semantic abilities of\nLLMs, coupled with a chat interface, allows conversational inter-\naction with data, follow-up questions, and highly contextualized\nresponses. Consequently, research has investigated in detail the\nchatbot paradigm for AI assistance in data analysis and visualization\n[17, 27, 34, 41, 80, 101].\nThus, early work on data wrangling script synthesis can be con-\ntrasted with current LLM-powered data analysis tools both in terms\nof the complexity of tasks being tackled, and the interaction modal-\nity (i.e., from demonstration, examples, and direct manipulation, to\nnaturalistic language prompts). In turn, this also means that gener-\nation mistakes become more common, due to underspecification of\nnatural language, assumptions that the AI is making but the user is\nnot aware of, etc. This creates new metacognitive demands for the\nuser to verify the Al's responses and then steer the Al if incorrect.\nIn our work, we try to provide new interaction modalities with\nLLMs for data analysis tasks to increase the transparency of the AI\nand the assumptions that it is making."}, {"title": "2.2 Verifying LLM Outputs and their Reliability", "content": "Data science is a challenging yet important function within soft-\nware teams. Previous research has focused on how data scientists\nengage in collaborative sensemaking, and make choices about how\nto communicate and report results [10, 47, 48, 67, 89, 100]. They\nhave found that data scientists need support in managing these\ncomplex collaborative workflows [43, 45, 91]. Consequently, re-\nsearch has explored how data scientists can manage, visualize, and\ntrace the evolution of their analysis process [32, 42, 70, 93, 94]."}, {"title": "2.3 Steering LLMs", "content": "Research on the metacognitive demands of generative AI identifies\ndecomposition and structured generation as potential aids [85]. Suh\net al. [83] explore hierarchical text generation at different abstrac-\ntion levels to assist with sensemaking and managing information\noverload from large text quantities. They also introduce structured\ngeneration [82], where user's prompt is first used to generate dimen-\nsions that make the model's responses vary, and then responses\nare generated according to those dimensions. Additionally, Mas-\nson et al. [59] propose principles of direct manipulation for LLMs:\ncontinuous representation of objects of interest, physical actions to\nlocalize prompt effects, and reusable prompts.\nSpecifically in the context of AI-assisted programming, Liu and\nSarkar et al. [56] introduce \"grounded abstraction matching,\u201d al-\nlowing users to steer LLMs by editing natural language utterances\ngrounded in each step of AI-generated code for data analysis in\nspreadsheets. Similarly, Tian et al. developed STEPS, which lets\nusers edit step-by-step explanations of AI-generated SQL code from\nnatural language queries [86]. CoLadder [97] aids experienced pro-\ngrammers in externalizing their problem-solving intentions flexibly,\nenhancing their ability to evaluate and modify code across various\nabstraction levels, from goal to final code implementation. These\nmethods enable users to edit natural language prompts grounded in\neach step of Al-generated code, providing an accessible abstraction\nlevel for reading, verifying, and editing.\nHowever, these approaches hide the Al's reasoning and decom-\nposition process, leaving users without insight into the \"how\" and\n\"why\" behind the generated code. The decision to manually infer\nthe Al's reasoning from the output and coming up with explicit\nactions to edit and refine the grounded utterances is left entirely to\nthe user. While this might be an acceptable trade-off in systems that\ngenerate short programs (e.g. typical spreadsheet formulas or SQL\nqueries), it is unclear how this approach would extend to longer\nand more complex data analysis scripts."}, {"title": "3 FORMATIVE STUDY", "content": "To understand the challenges involved in data analysis with con-\nversational Al assistants, we conducted a formative study with\n15 participants (12 male, 3 female). We used the Noteable plugin\nfor ChatGPT, providing a web-based computational notebook for\ncode execution. Users could input natural language (NL queries\ninto ChatGPT, which would generate and send code to Noteable.\nUpon execution, Noteable returned the results to ChatGPT for fur-\nther analysis and follow-up queries. At the time of study, ChatGPT\nwith the Noteable plugin was the only available tool offering fea-\ntures similar to those in ChatGPT Data Analysis (formerly Code\nInterpreter).\nParticipants (F1-F15), who were recruited from our research insti-\ntute, regularly performed data analysis tasks using computational\nnotebooks and Python data science libraries. Each participant was\nassigned to one of four tasks commonly performed by data scien-\ntists: data cleaning, merging and plotting, extracting insights, or\ntraining an ML model (see Table 1).\nStudy sessions lasted approximately 60 minutes and were con-\nducted in-person. Screen activity was recorded. Participants were\nasked to think aloud [23], and audio data was recorded and tran-\nscribed. Participant consent was obtained prior to the study and\nparticipants were each compensated with a GBP \u00a325 Amazon gift\ncard. The study protocol was approved by our institution's ethics\nand compliance review board."}, {"title": "3.1 Results", "content": "We analyzed the interactions participants had with ChatGPT and\nthe Noteable plugin from 301 total prompts. Participants used a mix\nof different actions which included (1) directing the AI to perform a\ndata analysis task, (2) exploring the dataset, (3) requesting suggested\nmethods or approaches to accomplish the task, (4) steering and\nrepairing the Al process in how it should accomplish the task, and\n(5) performing verification on the results of the task (either with or\nwithout the AI).\nSteering: Participants steered the Al's actions and methods using\ntheir NL prompts (106 prompts, 35%). Many of the steering prompts\n(n=34) were for performing data wrangling (cleaning and manip-\nulation) tasks on specific columns of the dataset. Similarly, some\nprompts (n=20) were used to explicitly add, remove, or change code\nproduced in previous steps (e.g., \"exclude the ones that are\npurely categorical\" (F5)). For repairing mistakes the AI made or\nany miscommunications between human and AI, participants fre-\nquently corrected an assumption the AI had made (36 prompts). For\nexample, after ChatGPT generated data analysis code, F8 prompted\nChatGPT that they wanted code that could \"map each row to\nmultiple classes and not just one closest class\" instead.\nData exploration: We identified 76 instances (25%) in which par-\nticipants wanted to inspect the data frames loaded into the notebook\nusing natural language filters such as displaying \"which country\nnames are inconsistent\" (F12), and \"unique values in GPA\ncolumn\" (F8). Sometimes these explorations were in the form of\nvisualizations, (e.g., requesting a \"histogram of cholesterol\nlevels\" (F6)).\nVerification: Although the most common behavior for validating\nthe Al's process was reading the AI-generated code and inspecting\nthe output, we also categorized 57 prompts (19%) as assisting with\nverification, such as: \"The USA is missing from all these\nheat maps, is it also missing from the CSV files or\nnot?\" (F11).\nCode or Logic Explanation: In 21 prompts (7%), participants used\nthe main thread of the conversation to ask the AI for explanations\nabout code they did not understand, an algorithm that was used,\nhow something was computed, or help interpreting the results.\nParticipants struggled with the linear conversational format of in-\nteraction with ChatGPT, often requesting an \"undo button\" for Al's\nmisunderstandings or errors. Without this, they tried workarounds,\nasking the AI to \"undo the last step\" (F9) or to \"ignore the\nprevious data cleansing steps and do it from scratch\"\n(F5)."}, {"title": "3.2 Design Goals and Rationale", "content": "Based on our formative study, relevant prior work, and how such\nAI tools use Chain-of-Thought prompting for task decomposition\nand execution, we identified the following design goals to improve\nuser and control over the AI-assisted data analysis process.\nDG.1 The system should visually distinguish assumptions that the\nAl is making, and contrast them from their corresponding actions.\nDG.2 The system should allow users to edit, modify, and update\nthe Al's assumptions and actions.\nDG.3 The system should provide various intervention points for\nthe user to understand and edit the Al's reasoning.\nDG.4 The system should facilitate users in efficiently retrieving\nessential information to validate responses and to inform decision-\nmaking at each intervention point.\nCurrently, LLM-based tools such as ChatGPT Plugins and Ad-\nvanced Data Analysis employ strategies like Chain-of-Thought\nprompting [92] and ReAct prompting [96]. These methods decom-\npose the input task into natural language \"reasoning\" traces (e.g.\ndataset and task assumptions), and subsequently \"act\" using code\nexecution agents that receive code as an input (which the LLM is\ncapable of generating) and return the output.\nDespite these advancements in task decomposition, a disconnect\npersists between users and the Al's reasoning, as evidenced by\nthe frequent attempts to steer and correct the AI. This disconnect\nunderscores the necessity for our design goals to focus on making\ntask decomposition more interactive and transparent. Participants\nhad to identify the assumptions behind mistakes and then attempt\nto correct them using follow-up prompts. This disrupted the data\nanalysis workflow and was not always effective, leading to frustra-\ntion. Thus, the first three design goals aim to enhance the user's\nability to visually distinguish, directly edit, and intervene in the\nAl's reasoning process.\nFurthermore, participants often engaged in iterative prompting\nto explore the dataset or intermediary results to build up assump-\ntions and steer the AI accordingly. Prior work has also showed\nthat a lack of comprehension of the code and the exact data oper-\nations impacts data analysts' confidence in their verification [28].\nHowever, due to the linearity of the conversational interface, users\nneeded to mix clarification and exploratory prompts with their\nactual analytical tasks. This resulted in a workflow that was slow,\nprone to inaccuracies, and disruptive to the primary task. Addition-\nally, the Al itself can aid the user in forming proper assumptions\nby retrieving relevant information from the dataset at intervention\npoints. This led to our fourth design goal."}, {"title": "4 SYSTEM DESIGN", "content": "We address design goals DG.1 and DG.2 through interactive task\ndecomposition. This involves: (1) Prompting the LLM to generate\nits chain-of-thought reasoning as assumptions and corresponding\nactions about the input task and dataset; and (2) parsing and ren-\ndering LLM output as structured, editable UI components, allowing"}, {"title": "4.1 Core System Features", "content": "4.1.1 Task Input. Data analysis begins with dataset(s) and a task\nspecification. Dataset Input: Data is loaded using the Input Query\ninterface (Figure 2, a). Users can then select one or more datasets\nrelevant to the task (Figure 2,6). The Python server generates\na summary of each selected dataset with sample values for all\ncolumns. This summary is passed to the LLM to build its initial set\nof assumptions for the data analysis task."}, {"title": "4.1.2 Task Decomposition: PHASEWISE System.", "content": "Using the summary\nof the dataset and the user-specified task description, the PHASE-\nWISE System decomposes the task into three phases: Input and\nOutput assumptions, Execution Plan, and Code and Output.\nA) Input and Output Assumptions: After loading a dataset (Figure\n3,a), this component displays all the columns that the AI found\nto be relevant to the task, and for each column it displays several\neditable assumptions regarding the task (Figure 3, 6). Assumptions\ncan pertain to data type, uniformity, units, sorting order, etc. Users\ncan delete columns they find unrelated to the task, or add columns\nthat the AI incorrectly did not select (Figure 3, d). Within each\ncolumn, users can edit, add, or remove assumptions for that column\n(Figure 3,6). For each column, users can \"inspect\" descriptive sta-\ntistics (Figure 3,), including a frequency table of sample values for\ncategorical columns. Additionally, the entire dataset can be viewed\nby clicking on the \"open\" button, with the selected columns\nhighlighted to help the user leverage the columns to build up as-\nsumptions. Finally, the task's output assumptions can be viewed\nand changed to edit, add, or remove assumptions to steer the final\noutput (Figure 3,6).\nB) Execution Plan: Using the assumptions, including edits, the\nsystem generates a list of natural language steps for solving the task\n(Figure 3,1). Steps are editable and the user may add or remove\nsteps. The model is also prompted to include optional steps that are\nrendered as selectable steps with a checkbox (Figure 3, 3). After\nthe user is satisfied with the plan, they can proceed to generating\nand running the code.\nC) Code and Output: Here the AI generates code to solve the\ntask based on the previous two components. The code is immedi-\nately executed and displayed in an editor to allow modification and\nre-execution (Figure 3, h). Users can inspect the dataframe and\nvariables used in the code execution (Figure 3, 1), and see the code\noutput (Figure 3, j"}, {"title": "4.1.3 Task Decomposition: STEPWISE System.", "content": "Unlike the PHASE-\nWISE System where each component reflects the entire task, the\nSTEPWISE system decomposes the task into subgoals, which have\nintermediate objectives. Each subgoal (except the first, which loads\nthe dataset (Figure 4, )), is represented as a pair of components:\nSubgoal Assumptions and Actions, and Subgoal Code and Output.\nA) Subgoal Assumptions and Actions: Each subgoal starts with a\nshort description of its objective in natural language, followed by\nseveral assumptions and actions based on the dataset or previous\nsteps (Figure 4, 6). We designed LLM prompts so that each subgoal\nwould focus on one specific objective such as pre-processing data,\nfiltering columns, performing calculations, and displaying plots.\nUsers may reorder assumptions and actions, add or remove assump-\ntions, and edit them directly. Once the user is satisfied with them,\nthey can proceed to generate the subgoal code and output.\nB) Subgoal Code and Output: Similar to Code and Output in the\nPHASEWISE System, in this component, the system generates code\nto solve the task based on the previous assumptions and actions\n(Figure 4,). The code is immediately executed and can be edited.\nOnce executed, the system generates the next subgoal to allow the\nuser to either reflect on the current subgoal or start working on\nthe next. This process continues until the task is finished and the\nrequirements have been satisfied, in which case the next Subgoal\nAssumptions and Actions will indicate completion. However, if the\nuser still wants to continue, they can add assumptions and actions\nto continue."}, {"title": "4.1.4 Editable LLM Assumptions and Actions.", "content": "We prompted the\nLLM to generate each assumption paired with its corresponding ac-\ntion in the format of <assumption> <action>. We also prompted\nthe LLM to enclose column names, variables, and keywords in"}, {"title": "4.1.5 Code Execution and Intermediary Variables.", "content": "The Python\nserver runs the AI-generated code and returns any outputs, in-\ncluding text, visualization plots, or any errors. Any variables and\ndataframes created during execution are displayed as intermediary\nvariables that the user may inspect.\nInspect Intermediary Variables: users can click on each intermedi-\nary variable to open a full-screen window for inspecting its values.\nFor dataframes, the interface includes a string matching filter to\nassist users in finding specific values."}, {"title": "4.1.6 Managing Edits.", "content": "Within each component, edits can be either\npending or submitted. A submitted edit means that the edits have\nbeen applied to generate the next component, whereas a pending\nedit has not. Pending edits can be reverted using an undo button.\nHowever, once an edit is submitted, our system introduces a new\nbranch to preserve the original, unedited version, while incorpo-\nrating the edited version in the \u201cmain\u201d branch. New branches are\ndisplayed in a tabbed ribbon at the top of the UI. To allow iteration\nwhile minimizing proliferation of branches, new branches are not\ncreated when the user edits the last generated component in the\nstream of components. Branching allows users to keep track of\nprevious edits and switch between edits as needed."}, {"title": "4.1.7 Side Conversations.", "content": "We allocated space to the right of the\nmain components for running side conversations with the system\nin three formats: Ask Question, Generate Code, and Run Side Query.\nThese features are available in all editable code execution blocks,\nwith the exception of Run Side Query, which is also accessible\nalongside the Input and Output Assumptions in the PHASEWISE AI\nsystem.\nAsk Question: This allows users to ask questions about the gen-\nerated code (See Figure 5). When a code editor is in focus or code is\nselected, the Ask Question button appears to the right of the editor.\nThe user can provide a natural language query and the system\ngenerates a response on the side. The selection allows users to ask\ntargeted questions such as \"what does this function do?\"\nGenerate Code: This feature generates code based on the selected\ncode segment and the user's query. The user can inspect the gener-\nated code and, if it is found useful, insert it into the editor. Similar to\nthe Ask Question feature, the selection here enables asking targeted\nquestions, such as updating the code to exhibit a different behavior\nbased on a natural language prompt.\nRun Side Query: This feature enables ancillary data analysis tasks\nusing natural language queries. It enables further exploration of\nthe dataset or any intermediary dataframes, and helps users vali-\ndate and refine assumptions. By clicking on the Side Query button\n(Figure 6), users can ask natural language queries about the dataset\nor the current state of the code and variables. The system generates\nand executes code in the side panel, allowing users to view outputs\nsuch as visualizations, identifying outliers, and check the data's\nconsistency."}, {"title": "4.2 CONVERSATIONAL Baseline System", "content": "We developed a CONVERSATIONAL system similar to ChatGPT's\nAdvanced Data Analysis plugin as a baseline to compare with the\nPHASEWISE and STEPWISE Systems. The CONVERSATIONAL system\ndoes not include any intervention points or editable assumptions, or\nany of the side conversation features (e.g. Ask Question or Run Side\nQuery). It decomposes the task into a bullet point of non-editable,\nnatural language assumptions and actions about the task, and then\nimmediately generates and runs non-editable code that solves the\nentire task. To interact with this system, as with ChatGPT, the\nuser needs to issue follow-up prompts. In this baseline system only\nthe prompts (and follow-up prompts) are editable. For verification,\nusers could read the code and inspect the intermediate variables,"}, {"title": "4.3 System Implementation", "content": "All three variants are built as a web application and Python server\nstack. The web application is written in TypeScript and the React\nweb framework to include the interface elements described in Sec-\ntion 4.1. The web application interacted with the Python server for\nuploading datasets, obtaining their descriptive summaries, running\ncode, and retrieving their execution results. It also called the GPT-4\nTurbo models from OpenAI. Each component in the PHASEWISE\nor STEPWISE System is represented as a node in a tree data struc-\nture inside the application. This enables tracing the path from each\nnode to the root node to prepare the context prompt for interacting\nwith the LLM and generating the next component. It also provides\nstate management for the edits that create branches and is used to\nrender the tabbed ribbon interface. The Monaco Editor is used as\nthe code editor in each of the code execution blocks and for syntax\nhighlighting the non-editable code pieces. To enable stateless code\nexecution on the Python server, and to retrieve code execution\noutputs and intermediary variables, we used the IPython kernel.\nWe used the \\%matplotlib inline command which returned all\nplots as base64 images that could be included as a response inside\nthe REST APIs."}, {"title": "4.3.1 LLM Prompt Structures.", "content": "We required complete control over\nthe format of the LLM's output to allow reliable parsing and ren-\ndering of structured components. However, few-shot learning (e.g.\nproviding specific input and output examples) would make the\nLLM overfit to the provided few-shot examples. Through informal\nexperimentation with different prompts and models, we concluded\nthat the GPT-4 and GPT-4 Turbo models are capable of following\ntemplates that only specify the format of the output with mini-\nmal specification of the content to be generated, with sufficient\nreliability for a practical evaluation. Figure 7 shows an example\nof the prompt used to select the columns relevant to the task and\ngenerate assumptions and actions about each column. Although\nthe exact format and structure is explicitly provided, the values are\nnot, which enables the system to work generally on a variety of\ninput tasks and datasets."}, {"title": "5 USER EVALUATION", "content": "To evaluate and compare the STEPWISE and PHASEWISE Systems in\nenabling users to steer the AI and verify its responses, we conducted\na within-subjects study. The study compared these systems with\nthe CONVERSATIONAL baseline and involved 18 participants who\nused all three systems to complete six data analysis tasks, with two\ntasks per system. Datasets and tasks were designed to be sufficiently\ncomplex that the AI would not automatically produce correct so-\nlutions without user involvement. They required participants to\ncarefully verify the Al's process and responses and steer the AI in\naddressing any issues.\nThe main focus of our exploratory study is on understanding the\nunique ways in which each system aids in steering and verification\nduring the AI-assisted data analysis process. We also investigated\nthe perceived utility of other various system components, and ex-\nplored the usage patterns and user preferences that emerged with\neach system."}, {"title": "5.1 Participants", "content": "We recruited 18 participants (10 men, 8 women, 0 non-binary) from\na large research university. Participants were pre-screened to ensure\nthey were proficient in writing Python code, familiar with Python\ndata science libraries, and experienced in regularly performing data\nanalysis tasks. In terms of data analysis experience, five participants\nreported having 1-2 years, seven having 3-5, and six more than five\nyears. The majority (14 participants) used Python daily, while the\nrest used it at least weekly. All reported familiarity with data science\nlibraries like numpy, matplotlib, with 15 also familiar with pandas.\nJupyter Notebooks were used daily by eight participants, weekly\nby six, monthly by two, and rarely by two. For English proficiency\nin technical contexts, 16 participants felt very comfortable, while\ntwo felt somewhat comfortable. In LLM usage for coding, seven\nreported using daily, seven weekly, and four using monthly or less."}, {"title": "5.2 Data Analysis Tasks", "content": "We designed six tasks derived from the ARCADE benchmark [98],\nwhich contains a diverse set of tasks from various datasets on Kag-\ngle [37]. These tasks included a series of natural language (NL)\nqueries written by professional data scientists with the intention\nof interacting with an Al assistant. We selected tasks to not re-\nquire specific domain knowledge, targeting for participants to solve\nthem within a 15-minute time frame. However, we also wanted\nto make sure that the tasks included additional complexities that\nwould make it difficult for the AI to correctly solve them without\nproper user verification and intervention. Therefore, we selected\nand altered tasks and their corresponding datasets with some for-\nmat inconsistencies and altered distributions. For instance, Task 2\n(Table 2) requires splitting tags and themes with commas before\ngrouping tags by themes. To increase complexity, we modified the\ntags and themes columns to have only the first theme or tag in\ncapital case, with the rest in lowercase. See Table 2 for details of the\nsix study tasks. To guarantee consistent failure of the system on"}, {"title": "5.3 Study Procedure", "content": "The order of the PHASEWISE, STEPWISE, and CONVERSATIONAL (base-\nline) systems was counterbalanced across participants and tasks\nto minimize order effects, while tasks were fixed from T1 to T6.\nEach participant began with a 10-minute introduction to their first\nassigned system, followed by a 10-minute warm-up task to famil-\niarize themselves with its features and mitigate learning effects.\nParticipants then proceeded to the main study tasks, where they\nwere given a dataset and a NL query. The researcher explained the\ndataset and relevant columns for each task. Participants proceeded\nto execute the task and were asked to think aloud throughout the\nstudy [23].\nParticipants were made aware that identifying and correcting\nmistakes made by the AI was their responsibility. They were asked\nto notify the experimenter once they believed they have achieved a\ncorrect result using the AI tool. The experimenter would then check\ntheir result and provide hints if necessary. Completion criteria for\neach task required resolving both issues listed in Table 2.\nFollowing the completion of each task, participants were asked\nabout their choice of method for steering the AI (e.g., editing the\nexecution plan versus directly editing the code) and their verifica-\ntion processes. After completing two tasks under each condition,\nparticipants completed a questionnaire including Likert items about\ntheir ability to verify, intervene and steer the AI, sense of control,\ninformation overload, frustration levels, and the utility of specific\nfeatures. Additionally, participants discussed their experience with\neach system in a 5-minute semi-structured interview. After complet-\ning all tasks using the three systems, a final questionnaire elicited\na comparative evaluation of the systems in terms of confidence,\nusability, control, and ease of verification and steering.\nThe sessions was conducted in-person, lasting approximately 2.5\nhours with a short break at the halfway point. Consent was obtained\nbefore running the study and each participant was compensated\nwith a GBP \u00a350 Amazon gift card. Our study protocol was reviewed\nand approved by our institution's ethics and compliance review\nboard."}, {"title": "5.4 Data Collection and Analysis", "content": "We recorded the audio and screen activity during each session using\nMS Teams. Audio recordings were transcribed for analysis. User\nsystem interactions and feature usage was also logged.\nThe think-aloud data was our main source of understanding how\nparticipants used the different systems and what they thought about\nthem in comparison with each other. We transcribed the think-aloud\ndata and post-condition interviews and two researchers performed\na negotiated, directed qualitative analysis. Ahead of the analysis,\nwe identified a focused set of research themes concerning steering\nand verification during the AI-assisted data analysis process, and\nreport our findings organised by these themes in Section 6. Because\nwe were interested in specific themes a priori, and were not devel-\noping a reusable coding scheme, our analysis differs from the more\ncommonly applied inductive approach [6]. We did not develop a\ncodebook, and this is not a situation in which it is appropriate to\nseek inter-rater reliability. Instead, in accordance with qualitative\ncoding best practices, the two researchers iteratively discussed their\ninterpretation of the findings and negotiated each disagreement\nuntil it was resolved [60].\nTask completion was defined as achieving a solution that cor-\nrectly resolved both issues indicated in Table 2 for each task within\nthe 15-minute time frame. For tasks that were correctly completed,\nwe recorded the number of hints provided during each task and\ncalculated approximate time on task. Task completion time was an\napproximate of when participants started the task (clicking on the\nrun query button) until they notified the experimenter that they\nfinished the task, with no remaining issues. However, our analysis\nof task time is only indicative, as think-aloud protocols interfere\nwith accurate timing.\nWe analyzed post-condition Likert responses to compare the\nthree systems and determine any statistically significant differ-\nences using a Friedman Chi Square test on the responses for each\nquestion with the system type as the independent variable. When\nsignificant differences were found (a = 0.05), a Wilcoxon signed-\nrank test identified pairwise significant comparisons, after applying\nBonferroni correction (a = 0.016)."}, {"title": "6 RESULTS", "content": "In this section, we present a comparative analysis of the STEPWISE\nand PHASEWISE AI tools versus the CONVERSATIONAL baseline. Our\nfindings are derived from study observations, log data, participant\n(P1-P18) think-aloud data, post-condition surveys, and post-study\ninterviews. In turn, we present the results regarding task completion\n(Section 6.1), steering and control (Section 6.2), and verification\n(Section 6.3)."}, {"title": "6.1 Task Completion", "content": "Successful task completion was determined as solving the task with\nno remaining issues within 15 minutes. Of the 108 task episodes\n(18 participants \u00d7 6 tasks)", "follows": "Baseline: 1", "PHASE-\nWISE": 2, "STEPWISE": 4.0}]}