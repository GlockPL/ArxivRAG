{"title": "Auction Design using Value Prediction with Hallucinations", "authors": ["Ilan Lobel", "Humberto Moreira", "Omar Mouchtaki"], "abstract": "We investigate a Bayesian mechanism design problem where a seller seeks to maximize revenue by selling an indivisible good to one of n buyers, incorporating potentially unreliable predictions (signals) of buyers' private values derived from a machine learning model. We propose a framework where these signals are sometimes reflective of buyers' true valuations but other times are hallucinations, which are uncorrelated with the buyers' true valuations. Our main contribution is a characterization of the optimal auction under this framework. Our characterization establishes a near-decomposition of how to treat types above and below the signal. For the one buyer case, the seller's optimal strategy is to post one of three fairly intuitive prices depending on the signal, which we call the \"ignore\", \"follow\" and \"cap\" actions.", "sections": [{"title": "Introduction", "content": "In this work, we study the Bayesian auction design problem where a seller aims to design a revenue- maximizing mechanism to sell an indivisible good to n buyers. In the classical version of the problem (Myerson, 1981), each buyer's private value is independently drawn from a prior distribution, which is common knowledge to all agents. A classical feature of this problem is information asymmetry: while buyers know their own private values, the seller has no direct access to this information beyond the prior. However, in many practical applications of mechanism design, such as in advertising auctions, the seller often does possess additional information about buyers' private values. In particular, sellers can train a machine learning models to predict buyers' valuations. To do so, they can often rely on a wealth of data: past interactions with the same buyer, contextual information, and even bids by similar buyers.\nHowever, sellers face significant challenges when using ML predictions to design auctions. One particular challenge is that some of the most advanced prediction systems, such as Large Language Models (LLMs) and deep neural networks more generally, often hallucinate. By hallucinate, we mean that they sometimes generate output that appears to be of high quality but that is in fact uncorrelated to the true quantity of interest. Even worse, such systems typically lack any sort of uncertainty quantification, making it difficult to decide whether the ML output should be used or discarded. They can appear confident in their predictions even when such predictions are completely erroneous.\nMotivated by the increasing importance of such hallucination-prone models in practice, we propose a novel Bayesian framework to explore the design of mechanisms that incorporate ML predictions taking into account the risk the predictions could be hallucinations. Specifically, in our framework, each buyer's private value is independently drawn from a known prior distribution, and the seller observes a signal for each buyer. This signal either equals the buyer's private value or, with some probability, is independently sampled and uncorrelated with the buyer's value. We call such uncorrelated signals hallucinations. Our framework stands out due to its ability to model signals generated by machine learning models that do not quantify uncertainty in their predictions and differs from more classical models of signals with statistical error (as discussed in Section 5.2).\nOur paper is inspired by the recent literature on learning-augmented algorithms (see Section 1.1) but our approach is quite different from prior work. Instead of the two-objective approach common in the computer science literature, we propose using a classical Bayesian framework to analyze our problem.\nFor the one buyer case, the optimal auction is easy to interpret. The signal space is broken into three segments, and the seller should post a price according to the realized segment. For intermediate signal values, the seller should follow the signal. That is, they should set the price of the item according to the signal. For low signals, the seller should ignore the signal. If the prediction is that the buyer's value is low, the seller is better off betting that the signal is a hallucination since pricing low does not help the seller. For high signals, the seller should cap the signal. Capping the signal means pricing at a value below the signal, but above the signal-ignoring monopoly price. Capping is a way of benefiting from the high signal while hedging against a hallucination.\nOur main technical contribution consists in characterizing the structure of the optimal signal-"}, {"title": "1.1 Literature Review", "content": "Mechanism design and auction theory have been very active areas of research since at least the 1960s, including the celebrated Vickrey-Clarke-Groves framework for welfare maximization (Vickrey, 1961;\nClarke, 1971; Groves, 1973). Myerson (1981) laid the foundation for the literature on revenue max- imization, proving many of the results that we build on: revelation principle, the role of the virtual value and the ironing procedure. We also build closely on Monteiro and Svaiter (2010), who devel- oped techniques for ironing virtual values in settings where the priors do not have densities. For gen- eral distributions, the complexity of the revenue-maximizing auction derived in Myerson (1981) has motivated extensive research into simple and more practical mechanisms that are easier to imple- ment while remaining near-optimal (Hartline and Roughgarden, 2009; Roughgarden and Talgam-Cohen, 2019). Our work contributes to this literature by modeling a practical setting in which the seller relies on machine learning algorithms that provide hallucination-prone predictions and by studying the design of optimal mechanisms that are robust to such predictive errors.\nOur work relates to the literature on learning-augmented algorithms, also known as algorithms with predictions/advice in which a decision-maker has access to some prediction with unknown accuracy. The standard goal in this literature is to design algorithms achieving a good trade-off between two performance metrics: consistency, which is the performance if the predictions are perfect, and robustness, which corresponds to the performance when the predictions are adver- sarial (Purohit et al., 2018; Lykouris and Vassilvitskii, 2021). This framework has been applied to"}, {"title": "The Model", "content": "A seller has one indivisible good to sell, and there are n potential buyers. Each buyer i has a private value $v_i$, which is drawn from a cumulative distribution $F_i$. The distributions $F_i$ are assumed to satisfy all of the assumptions as in Myerson (1981): they admit densities $f_i$, which are strictly positive everywhere within a support $[a_i, b_i]$. We will also assume the value distributions are regular.\nAssumption 1 (Regularity). For every i, the virtual value function $v - (1 - F_i(v))/f_i(v)$ is assumed to be non-decreasing over the support of buyer i's valuation.\nThe seller has access to a value prediction technology, which generates a signal $s_i$ for each i. The signal is a hallucination with probability $\\gamma_i \\in (0,1)$. If the signal is a hallucination, then $s_i = w_i$\nwhere $w_i$ is a random variable also drawn from distribution $F_i$ that is independent of buyer i's value $v_i$ (we discuss the case where $w_i$ drawn from a different distribution than $v_i$ in Section 3). If the signal is not a hallucination, then the signal is assumed to be accurate: $s_i = v_i$. The seller is assumed to know the values $\\gamma_i$, but not whether a given realization is a hallucination or not. We assume that the realizations of hallucinations, $v_i$ and $w_i$ are independent across buyers.\nWe will use $\\gamma$ and $s$ to represent the vectors of hallucination probabilities and signals, respec- tively. Given a signal, the seller can perform a Bayesian update to obtain what we call the posterior distribution of a buyer's value. We will denote by $F_{\\gamma,s}$ the posterior distribution of the buyers' values and by $F_{\\gamma,s,-i}$ the posterior distribution of the buyers' values excluding the ith buyer."}, {"title": "Signal-revealing direct mechanisms.", "content": "Problem (1) specifies the problem of finding the optimal signal-revealing direct mechanism. A direct mechanism is one where the seller chooses an incentive- compatible allocation and payment scheme, and asks the buyers to reveal their types. In standard mechanism design, restricting to direct mechanisms is without loss of optimality (Myerson, 1981). We define a signal-revealing mechanism to be one where the seller shares the signals alongside the allocation and payment rules. Exploring non-signal-revealing mechanisms is a potentially difficult problem, as the choice of allocation and payment rule will reveal the signals unless the seller explic- itly pools signals (i.e., chooses a mechanism that is at least partially non-responsive to signals). We leave the question of whether restricting attention to signal-revealing mechanisms is without loss of optimality open for future work. Note that by assuming the mechanism is signal-revealing we made the formulation relatively straightforward: both the objective and the IC and IR constraints use the posterior distributions given signals rather than the priors.\nOn the correctness of non-hallucinatory signals. A natural question regarding this model is why we assume that, when a signal is not a hallucination, it equals the buyer's private value. In reality, errors from deep neural network models are likely a combination of hallucinations and classical Gaussian noise. We analyze pure hallucination in this paper in order to achieve a clean characterization. If we added a Gaussian noise on top of the hallucination, the answer would"}, {"title": "Bayesian Update and Applying Myerson", "content": "In our setting, the seller obtains the signals $s_i$ prior to selecting the mechanism. After obtaining $s_i$, the seller's posterior belief about $v_i$ is given by:\n$f_{s_i}(v) = \\gamma_i f_i(v) + (1 - \\gamma_i) \\cdot \\delta_{s_i}(v)$, (2)\nwhere $\\delta_{s_i}(\\cdot)$ is the Dirac delta function that places a unit of mass at $s_i$ and zero mass everywhere else. Equivalently,\n$F_{i, s_i}(v) = \\begin{cases} \\gamma_i f_i(v) & \\text{for } v < s_i, \\\\  \\gamma_i f_i(v) + (1 - \\gamma_i) & \\text{for } v \\geq s_i. \\end{cases}$ (3)\nThe question we aim to address can thus be rephrased as what is the revenue-maximizing auction when the valuation of buyer i is drawn according to $F_{v_i,s_i}$.\nOn the distribution of hallucinations. We will assume throughout the paper that the value $v_i$ and any potential hallucination $w_i$ are drawn from the same distribution. However, if we were to assume that the value were drawn from density $g_i$ and the hallucination from density $f_i$, where these distributions are absolutely continuous with respect to each other, we could obtain a similar formula via Bayesian updating. Let $Z_i$ represent whether a hallucination occurred. The posterior density would then be given by:\n$f_{i,s_i}(v) = P(Z_i | s_i) f_{i|s_i}(v | Z_i) + P(\\text{not } Z_i | s_i) f_{i,s_i}(v | \\text{not } Z_i) = \\frac{P(Z_i | s_i) \\cdot f_i(v) + P(\\text{not } Z_i | s_i) \\cdot \\delta_{s_i}(v)}{\\gamma_i f_i(s_i) + (1 - \\gamma_i) \\cdot g_i(s_i)} =  \\frac{\\frac{\\gamma_i \\cdot f_i(s_i)}{\\gamma_i f_i(s_i) + (1 - \\gamma_i) \\cdot g_i(s_i)} f_i(v) + \\frac{(1 - \\gamma_i) \\cdot g(s_i)}{\\gamma_i f_i(s_i) + (1 - \\gamma_i) \\cdot g_i(s_i)} \\delta_{s_i}(v)}{\\gamma_i f_i(s_i) + (1 - \\gamma_i) \\cdot g_i(s_i)} = \\frac{1- \\gamma_i}{\\gamma_i} \\frac{g_i(s_i)}{f_i(s_i)} $.\nThat is, our results from the rest of the paper would apply if we replace $\\gamma_i$ with"}, {"title": "3.1 Applying Myerson", "content": "Myerson (1981) tells us that in a private values setting, the revenue-maximizing auction is given by calculating the virtual value of each agent (which might require ironing) and then allocating the item to the agent with the highest non-negative virtual value, or discarding the item if all of the virtual values are negative. Since virtual values are computed separately for each buyer, we will suppress the buyer index i from the notation whenever possible to lighten the notational burden.\nFor a given density f and cumulative distribution F, the pre-ironing virtual value function is $\\varphi_F(v) = v - (1 - F(v))/f(v)$. For the density and cumulative distributions given by Eqs. (2) and (3), we have:\n$\\varphi_{F_{\\gamma,s}}(v) = \\begin{cases} v - \\frac{1/\\gamma - F(v)}{f(v)} & \\text{for } v < s, \\\\ v - \\frac{1 - F(v)}{f(v)} & \\text{for } v > s. \\end{cases}$\nWe note that the virtual value function is not well-defined at s, but we will ignore this issue for now since that is a single point. The function $F_{\\gamma,s}$ does not need to be ironed after s since $\\varphi_{F_{\\gamma,s}}(v) = \\varphi_F(v)$ for v > s and we have assumed F is regular. Ironing could be necessary before s depending on the choice of F.\nLet's apply this to single-buyer, uniform over [0,1] case. For this particular F, we obtain:\n$\\varphi_{F_{\\gamma,s}}(v) = \\begin{cases} 2v - 1/\\gamma & \\text{for } v < s, \\\\ 2v - 1 & \\text{for } v > s. \\end{cases}$ (4)\nFor this particular distribution, ironing is not necessary before s since 2v \u2013 1/\u03b3 is an increasing function of v. Consider the special case s = 1/2 - \\epsilon and $\\gamma = \\epsilon$, for a small $\\epsilon$. Eq. (4) crosses zero at v = 1/2, implying that the optimal price is 1/2. However, this cannot be the correct optimal price.\nThe revenue generated by this price is bounded above by $\\epsilon$ since it requires s to be a hallucination as a necessary condition for a sale to occur. Meanwhile, using the signal 1/2 - $\\epsilon$ as the price would generate at least $(1 - \\epsilon) \\cdot (1/2 - \\epsilon)$ in revenue.\nIt turns out that ignoring what occurred at s, where the density $f_{\\gamma,s}$ is not well-defined, and applying Myerson's technique naively was a mistake. To obtain a correct optimal auction, we will need to use a more sophisticated characterization of optimal auctions that applies for distributions that do not admit densities."}, {"title": "Characterization of the Optimal Auction", "content": "In this section, we first introduce a slight generalization of Myerson's ironing operation, which we will need to state our results. We then present our main theorem, and demonstrate what it implies for some simple distributions. We also show that our main theorem fails if we remove the regularity assumption."}, {"title": "4.1 Truncated Myerson Ironing", "content": "Consider a distribution F supported on [a, b] and which admits a positive density on its support. In that case F is strictly increasing on [a, b] and therefore it admits an inverse function $F^{-1}$ strictly increasing on [0,1]. When the virtual value function of F defined for every x \u2208 [a,b] as $\\varphi_F(x)$ is not monotonic non-decreasing, Myerson (1981) proposes a general procedure called ironing to characterize the optimal auction. In what follows we introduce our slight generalization of Myerson's ironing operator. The only difference between the operator we introduce below and the one presented in Myerson (1981) is that we also allow for the operation to be performed only in an interval of the quantile space rather than over the entire quantile space. Hence, we call this operation the truncated Myerson ironing. If we restrict x to be equal to 1 in what follows, we would be mimic the definition of the original Myerson ironing operator.\nFor every quantile q \u2208 [0, 1], let\n$J(q) = \\int_{0}^{q} \\varphi_F(F^{-1}(r))dr$. (5)\nFurthermore, for every x \u2208 [0,1], let $G_x : [0,x] \\to R$ be the convex hull of the restriction of the function Jon [0, 1], formally defined for every q \u2208 [0, x] as,\n$G_x(q) = \\min_{\\substack{(\\lambda, r_1, r_2) \\in [0,1] \\times [0,x]^2 \\\\ s.t. \\lambda r_1 + (1-\\lambda) \\cdot r_2 = q}} \\lambda \\cdot J(r_1) + (1 - \\lambda) \\cdot J(r_2)$\nBy definition, Gr is convex on [0,x]. Therefore, it is continuously differentiable on [0, x] except at countably many points. For every q \u2208 [0,x], we define the function g as,\n$g_x(q) = \\begin{cases} G_x'(q) & \\text{if } G \\text{ is differentiable at q} \\\\ \\lim_{q' \\to q} G_x'(q') & \\text{otherwise}. \\end{cases}$"}, {"title": "4.2 Main Result", "content": "The convexity of Gr implies that gr is monotone non-decreasing. For any t \u2208 [a, b] we define the truncated ironed virtual of F on [a,t] as the mapping,\n$IRON_{[a, t]}[F] : \\begin{cases} [a, t] & \\to \\mathbb{R} \\\\ v & \\to g_{F^{-1}(t)}(F(v)). \\end{cases}$\nWe note that $IRON_{[a,b]}[F]$ corresponds to the classical notion of ironing introduced in Myerson (1981). We emphasize that when t < b, the mapping $IRON_{[a,t]}[F]$ is in general different from the restriction of $IRON_{[a,b]}[F]$ on [a,t] .\nIf the distribution F does not admit a density that is positive everywhere in the support, the classical Myerson ironing procedure is not applicable since it relies on the existence of the inverse $F^{-1}$. In this case, there exists a more general virtual value characterization developed by Monteiro and Svaiter (2010) that is still applicable. That characterization is difficult to work with because it involves generalized convex hulls, rather than the standard convexification used by Myerson. We defer the presentation and discussion of how to use this complex machinery until Section 6. We are now"}, {"title": "5 The Single Buyer Case", "content": "In this section, we first leverage Section 4.2 to study the structure of the optimal mechanism for a single buyer. We then, compare the mechanism obtained in our model of hallucination-prone signals with another model which corresponds to the classical model of Gaussian noise."}, {"title": "5.1 Optimal Mechanism for One Buyer", "content": "An important implication of Theorem 1 is the following characterization of the optimal mechanism for a single buyer. In this setting, the optimal mechanism is a posted price.\nProposition 1. Assume n = 1 and F_is regular on [a,b] with continuous density. Then, for any s \u2208 [a,b] and any \u03b3 \u2208 [0,1], there exist two thresholds $L_\\gamma$ and $U_\\gamma$ such that the optimal price satisfies:\n$p^* = \\begin{cases} p^{\\text{ignore}} & \\text{if } s < L_\\gamma, \\\\ s & \\text{if } L_\\gamma < s < U_\\gamma, \\\\ p^{\\text{cap}} & \\text{if } s > U_\\gamma, \\end{cases}$\nwhere $p^{\\text{ignore}}$ and $p^{\\text{cap}}$ satisfy:\n$\\frac{p^{\\text{ignore}} - \\frac{1 - F(p^{\\text{ignore}})}{f(p^{\\text{ignore}})}}{f(p^{\\text{ignore}})} = 0$ and $\\frac{p^{\\text{cap}} - \\frac{1/\\gamma - F(p^{\\text{cap}})}{f(p^{\\text{cap}})}}{f(p^{\\text{cap}})} = 0$."}, {"title": "5.2 Comparison to the Value-with-noise Model", "content": "We next contrast the optimal prices under our hallucination model with the ones that emerge from a more classical model where the signal corresponds to the true value plus some Gaussian noise. In this alternative model, we assume that the signal s observed by the decision-maker satisfies s = v + \u03b5, where v is the private value of the buyer and \u03b5 is a random variable independently sampled from a zero-mean Gaussian distribution with variance \u03c32.\nIn some sense, the key difference between the value-with-noise model and the hallucination- prone one is that the error is relatively local in the former, whereas it is more global for the latter. For instance, when the variance o\u00b2 is small, the signal obtained will likely be close to the true value, whereas a small hallucination probability \u03b3 still implies that when the signal is wrong it can be arbitrarily far from the true value and is completely uncorrelated to it. We compare in the optimal price for these two models."}, {"title": "Key Technical Arguments", "content": "In this section we present the key technical arguments needed to prove Theorem 1. We first describe the family of semi-infinite dimensional problems developed in Monteiro and Svaiter (2010) to characterize the ironed virtual value for arbitrary distributions. We then solve this family of problems to obtain our closed-form solution."}, {"title": "6.1 Ironing for Arbitrary Distributions", "content": "Let F be a regular distribution which admits a positive density f on its support. For any \u03b3 \u03b5 (0,1) and any s in the support of F, recall the definition of the post-signal distribution $F_{\\gamma,s}$ defined in Eq. (3). We note that the post-signal distribution does not admit a density at v = s. In this setting, the virtual value function used to iron in the Myerson sense (see Section 4.1), and which is defined for every distribution F with positive density on its support is not well-defined. In what follows, we present the formalism developed in Monteiro and Svaiter (2010) to characterize the optimal auction for general distributions. This formalism generalizes Myerson's characterization.\nFor every distribution F (which does not need to have a density), we define for every x \u2208 [a, b]\nthe function\n$H_F(x) = \\int_{a}^{x} tdF(t) - \\int_{a}^{x} (1 - F(t))dt$.\nFix t \u2208 [a, b]. For every x \u2208 [a,t], we define the generalized convex hull of HF as,\n$\\Psi_t^F(x) = \\sup_{\\alpha, \\beta \\in \\mathbb{R}} \\alpha + \\beta \\cdot F(x)$\ns.t.\n$\\alpha + \\beta \\cdot F(y) < H_F(y) \\forall y \\in [a, t]$. (7a) (7b)\nLet $\\partial \\Psi_t^F(x)$ be the generalized sub-differential of $\\Psi_t^F$ at x defined as the set of \u03b2\u2208 R such that\n$\\Psi_t^F(z) \\geq \\Psi_t^F(x) + \\beta \\cdot (F(z) - F(x)) \\text{ for every } z \\in [a,t]$. (8)\nEquivalently (see Section 2 of Monteiro and Svaiter (2010)), one has that\n$\\partial \\Psi_t^F(x) = {\\beta \\in \\mathbb{R} \\text{ s.t. there exists } \\alpha \\in \\mathbb{R} \\text{ such that } (\\alpha, \\beta) \\text{ is optimal for (7)}}$. (9)\nFurthermore, let $l_t^F(x) = \\inf \\partial \\Psi_t^F(x)$ and $s_t^F(x) = \\sup \\partial \\Psi_t^F(x)$."}, {"title": "6.2 Outline of the proof of Theorem 1", "content": "Fix a regular distribution F with positive continuous density f on its support [a, b]. The generalized convex hull of HF, is defined as,\n$\\Psi_{F_{\\gamma,s}}(x) = \\sup_{\\alpha, \\beta \\in \\mathbb{R}} \\alpha + \\beta \\cdot F_{\\gamma,s}(x)$\ns.t.\n$\\alpha + \\beta \\cdot F_{\\gamma,s}(y) \\leq H_{F_{\\gamma,s}}(y) \\forall y \\in [a, b]$.\nBy expressing $F_{\\gamma,s}$ and $H_{F_{\\gamma,s}}$ as a function of F, HF, \u03b3 and s , we obtain the following equivalent expression for $F_{\\gamma,s}$. For every x we have that,\n$\\Psi_{F_{\\gamma,s}}(x) = \\sup_{\\alpha, \\beta \\in \\mathbb{R}} \\alpha + \\beta\\cdot \\gamma \\cdot F(x) + 1{x > s} \\cdot \\beta \\cdot (1 - \\gamma)$\ns.t.\n$\\alpha + \\beta\\cdot \\gamma \\cdot F(y) < \\gamma \\cdot H_F(y) - (1 - \\gamma) \\cdot y \\forall y < s$. (11a) (11b) (11c)\n$\\alpha + \\beta \\cdot (1 - \\gamma) + \\beta \\cdot \\gamma \\cdot F(y) \\leq \\gamma \\cdot H_F(y) \\forall y \\geq 8$.\nTo prove Theorem 1, we aim to relate $F_{\\gamma,s}$ to $l_0^F$ on the interval [a, s) and $l_{F_{\\gamma,s}}$ to $l_F$ on the interval [s, b]. Then, by applying Proposition 2, we obtain the desired expression.\nKey proof technique. To establish this result, we first prove that the generalized virtual value functions we consider are well-behaved on every interval which does not include s. We prove more generally the following result on the regularity of the generalized virtual value function.\nLemma 1. Let I be an interval included in [a,b]. Assume that G admits a density g that is positive and continuous on I. Then, $l_g$ is continuous on I.\nGiven a distribution G, recall that $l_g$ is the lowest generalized sub-gradient of the function IG which is itself the generalized convex hull of the function HG. Therefore, Lemma 1 extends the statement that \"the convex hull of a differentiable function of one variable is continuously differentiable\" to our generalized notions of convexity and differentials.\nIn turn, the key argument to prove that two distributions of interest Fand G have the same virtual value function on some interval consists in first establishing the continuity of $l_F$ and $l_g$ by using Lemma 1. We then prove that $l_F$ is a generalized sub-gradient of IG on the whole interval and conclude applying the following lemma."}, {"title": "Conclusion", "content": "In this paper, we studied how Bayesian mechanism design can be adapted to address the chal- lenges posed by hallucination-prone predictions generated by modern machine learning models. By introducing a novel Bayesian framework, we modeled these imperfect signals and rigorously charac- terized the structure of optimal mechanisms, extending classical results like those of Myerson (1981)"}]}