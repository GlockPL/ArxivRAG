{"title": "EMERGENT SYMBOL-LIKE NUMBER VARIABLES IN ARTIFICIAL NEURAL NETWORKS", "authors": ["Satchel Grant", "Noah D. Goodman", "James L. McClelland"], "abstract": "There is an open question of what types of numeric representations can emerge in neural systems. To what degree do neural networks induce abstract, mutable, slot-like numeric variables, and in what situations do these representations emerge? How do these representations change over the course of learning, and how can we understand the neural implementations in ways that are unified across different models' implementations? In this work, we approach these questions by first training sequence based neural systems using Next Token Prediction (NTP) objectives on numeric tasks. We then seek to understand the neural solutions through the lens of causal abstractions or symbolic algorithms. We use a combination of causal interventions and visualization methods to find that artificial neural models do indeed develop analogs of interchangeable, mutable, latent number variables purely from the NTP objective. We then ask how variations on the tasks and model architectures affect the models' learned solutions to find that these symbol-like numeric representations do not form for every variant of the task, and transformers solve the problem in a notably different way than their recurrent counterparts. We then show how the symbol-like variables change over the course of training to find a strong correlation between the models' task performance and the alignment of their symbol-like representations. Lastly, we show that in all cases, some degree of gradience exists in these neural symbols, highlighting the difficulty of finding simple, interpretable symbolic stories of how neural networks perform numeric tasks. Taken together, our results are consistent with the view that neural networks can approximate interpretable symbolic programs of number cognition, but the particular program they approximate and the extent to which they approximate it can vary widely, depending on the network architecture, training data, extent of training, and network size.", "sections": [{"title": "1 INTRODUCTION", "content": "Both biological and artificial Neural Networks (NNs) have powerful modeling abilities. We can see an example of this in biological NNs (BNNs) from the impressive capabilities of human cognition, and we can see this in artificial NNs (ANNs) where recent advances have had such great success that ANNs have been crowned the \"gold standard\" in many machine learning communities (Alzubaidi et al., 2021). The inner workings of NNs, however, are still often opaque. This is, in part, due to their representations being highly distributed. Individual neurons can play multiple roles within a network (Rumelhart et al., 1986; McClelland et al., 1986; Smolensky, 1988; Olah et al., 2017; 2020; Elhage et al., 2022; Scherlis et al., 2023; Olah, 2023).\nSymbolic Algorithms/programs (SAs), in contrast, defined as processes that manipulate distinct, typed entities according to explicit rules and relations, can have the benefit of consistency, transparency, and generalization when compared to their neural counterparts. A concrete example of an SA is a computer program, where the variables are abstract, mutable entities, able to represent many different values, processed by well defined functions. There are many existing theories that posit the necessity of algorithmic, symbolic, processing for higher level cognition (Do & Hasselmo, 2021; Fodor & Pylyshyn, 1988; Fodor, 1975; 1987; Newell, 1980; 1982; Pylyshyn, 1980; Marcus, 2018;"}, {"title": "2 RELATED WORK", "content": "We wish to highlight the importance of using causal manipulations for interpreting neural functions in this work. Causal inference broadly refers to methods that isolate the particular effects of individual components within a larger system (Pearl, 2010). An abundance of causal interpretability variants have been used to determine what functions are being performed by the models' activations (or circuits) (Olah et al., 2018; 2020; Wang et al., 2022; Geva et al., 2023; Merrill et al., 2023; Bhaskar et al., 2024; Wu et al., 2024). Vig et al. (2020) provides an integrative review of the rationale for and utility of causal mediation in neural model analyses. We rely heavily on DAS for our analyses. This method can be thought of as a specific type of activation patching (also referred to as causal tracing) (Meng et al., 2023; Vig et al., 2020).\nMany publications explore ANNs' abilities to perform counting tasks (Di Nuovo & McClelland, 2019; Fang et al., 2018; Sabathiel et al., 2020; Kondapaneni & Perona, 2020; Nasr et al., 2019; Zhang et al., 2018; Trott et al., 2018) and closely related tasks (Csord\u00e1s et al., 2024). Our tasks and modeling paradigms differ from many of these publications in that numbers are only latent in the structure of our tasks without explicit teaching of distinct symbols for distinct numeric values. El-Naggar et al. (2023) provided a theoretical treatment of Recurrent Neural Network (RNN) solutions to a parentheses closing task, and Weiss et al. (2018) explored Long Short-Term Memory RNNs (LSTMs) (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Units (GRUs) (Cho et al., 2014) in a similar numeric equivalence task looking at the activations. These works showed correlates of a magnitude scaling solution in both theoretical and practically trained ANNs. Our work builds on their findings by using causal methods for our analyses, and by expanding the models considered. Behrens et al. (2024) explored transformer counting solutions in a task similar to ours. Our work builds on theirs by exploring positional encodings-thus avoiding explicit labels of the numeric concepts and using causal analyses."}, {"title": "3 METHODS", "content": "In this work, we train models on numeric equivalence tasks and then use interpretability methods such as Distributed Alignment Search (DAS) (Geiger et al., 2021; 2023) to understand the manner in which the models solve the task."}, {"title": "3.1 NUMERIC EQUIVALENCE TASKS", "content": "Each task we consider is defined by varying length sequences of tokens. Each sequence starts with a Beginning of Sequence (BOS) token and ends with an End of Sequence (EOS) token. Each sequence is defined by a uniformly sampled object quantity from the inclusive range of 1 to 20. The sequence is constructed as the combination of two phases. The first phase, called the demonstration phase (demo phase), starts with the BOS token and continues with a series of demo tokens equal in quantity to the sampled object quantity. Following the demo tokens is the Trigger token (T), indicating the end of the demo phase and the beginning of the response phase (resp phase). The resp phase consists of a series of resp tokens equal in number to object quantity. The EOS token follows the resp tokens, denoting the end of the sequence.\nDuring the initial model training, we include all tokens in the autoregressive loss. During model evaluation and DAS trainings, we only consider tokens in the resp phase-which are fully determined by the demo phase. During model trainings, we hold out the object quantities 4, 9, 14, and 17. A trial is considered correct when all resp tokens and the EOS token are correctly predicted by the model after the trigger. We include three variants of this task differing only in their demo and resp token types.\nMulti-Object Task: there are 3 demo token types {D1, D2, D3} with a single response token type, R. The demo tokens are uniformly sampled from the 3 possible token types. An example sequence with a object quantity of 2 could be: \"BOS D3 D1 TRREOS\"\nSingle-Object Task: there is a single demo token type, D, and a single response token type, R. An example with a object quantity of 2 is: \"BOSDDTRREOS\"\nSame-Object Task: there is a single token type, C, used by both the demo and resp phases. An example with a object quantity of 2 would be: \"BOS CCTCC EOS\".\nFor some transformer trainings, we include Variable-Length (VL) variants of each task to break count-position correlations. In these variants, each token in the demo phase has a 0.2 probability of being sampled as a unique \"void\" token type, V, that should be ignored when determining the object quantity of the sequence. The number of demo tokens will still be equal to the object quantity when the trigger token is presented. As an example, consider the possible sequence with a object quantity of 2: \"BOSVDVVDTRREOS\"."}, {"title": "3.2 MODEL ARCHITECTURES", "content": "The recurrent models in this paper consist of Gated Recurrent Units (GRUs) (Cho et al., 2014), and Long Short-Term Memory networks (LSTMs) (Hochreiter & Schmidhuber, 1997). These architectures both have a Markovian, hidden state vector that bottlenecks all predictive computations following the structure:\n$h_{t+1} = f(h_t, x_t)$ (1)\n$X_{t+1} = g(h_{t+1})$ (2)\nWhere $h_t$ is the hidden state vector at step t, $x_t$ is the input token at step t, f is the recurrent function (either a GRU or LSTM cell), and g is a multi-layer perceptron (MLP) used to make a prediction, denoted $2_{t+1}$, of the token at step t + 1. We contrast the recurrent architectures against transformer"}, {"title": "3.3 SYMBOLIC ALGORITHMS (SAS)", "content": "In this work, we examine the alignment of 3 different SAs to the models' distributed representations.\n1. Up-Down Program: uses a single numeric variable, called the Count, to track the difference between the number of demo tokens and resp tokens at each step in the sequence. It also contains a Phase variable to determine whether it is in the demo or resp phase. The program ends when the Count is equal to 0 during the resp phase.\n2. Up-Up Program: uses two numeric variables-the Demo Count and Resp Count-to track quantities at each step in the sequence. It uses a Phase variable to track which phase it is in. This program increments the Demo Count during the demo phase and increments the Resp Count during the resp phase. It ends when the Demo Count is equal to the Resp Count during the resp phase.\n3. Context Distributed (Ctx-Distr) Program: queries a history of inputs at each step in the sequence to determine when to stop rather than encoding a cumulative quantity variable. A more specific version of this program (that appears to emerge under some conditions) is is one in which the program assigns a value of 1 to each demo token and a -1 to each resp token (or visa-versa) and computes their combined sum at each step in the sequence to determine the count. This program outputs the EOS token when the sum is 0.\nIt is important to note that there are an infinite number of causally equivalent implementations of these programs. For example, the Up-Down program could immediately add and subtract 1 from the Count at every step of the task in addition to carrying out the rest of the program as previously described. We do not discriminate between programs that are causally indistinct from one another in this work."}, {"title": "3.4 DISTRIBUTED ALIGNMENT SEARCH (DAS)", "content": "DAS is a hypothesis testing framework for finding alignments between distributed systems and SAs (also referred to as causal abstractions) by performing interchange interventions (equivalently referred to as causal interventions, patches, or substitutions) (Geiger et al., 2021; 2023). freeze the model weights before performing the analysis.\nIn general, DAS measures the degree of alignment between the best subspace of a distributed model's representations with the variables from a specified SA. The method uses causal interventions to both train the alignment and to make claims about the degree of alignment. For a given variable from the SA, DAS learns an orthogonal rotation matrix, $R \u2208 R^{m\u00d7m}$, that orients a subspace of the distributed representations along a subset of the dimensions in the representation, allowing the subspace to be freely interchanged between representations. The method relies on the notion of counterfactual"}, {"title": "3.5 ADDITIONAL INTERVENTIONS", "content": "A sufficient experiment to demonstrate the lack of use of a cumulative count variable is to look for unchanged behavior after performing a full activation vector substitution on relevant hidden representations. Concretely, our main test for the Ctx-Distr strategy is to replace a full hidden state at time step t with the full hidden state at time step u from a different set of inputs. We provide further detail in Supplement A.5 as to why this experiment is sufficient for the claim of a time-distributed solution. We trivially apply these interventions on the recurrent hidden states in the RNNs, and we apply these interventions to the hidden states from Layer 1 in the transformer architectures. Results are displayed as Ctx-Distr in Figure 4. If the model is using the Ctx-Distr program, we would expect the models' subsequent token predictions to be unaffected by this intervention. We include a further DAS analysis to align the Last Value variable in the Ctx-Distr program (representing the increment value of the previous input token). These alignments are applied to the embeddings in the GRUs and to the embeddings that are projected into the k and v vectors in the Transformers. We leave the pre-query embeddings unperturbed, further demonstrating the anti-Markovian hidden states.\nIn an attempt to localize the transformers' computations to a single attention layer, we include attention interventions that directly substitute the outputs of the self-attention module from time u to time t. We perform two intervention variants and report the average of their results in Figure 3(f).\nWe also explore a direct substitution of individual artificial neuron activations in the Multi-Object trained models. In these experiments, we directly substitute the activation value of a specific neuron at time step t with the value of the same neuron at time step u from a different sequence. We include an additional, single model activation intervention on the activations of neurons 12 and 18 from the LSTM shown in Figure 2, where we substitute both values in the interventions. In all direct interventions detailed in this section, we evaluate the model's IIA on counterfactual behavior assuming a transfer of the Count."}, {"title": "4 RESULTS", "content": "4.1 SYMBOLIC ALGORITHMS\nFigure 4 shows DAS performance as a function of the SA used in the alignment. In the Multi-Object recurrent models, we see that the most aligned SA is the Up-Down program. The results are compared against the Up-Up program and the Ctx-Distr program which have significantly lower IIAs. We use this as evidence in favor of the interpretation that the recurrent models develop a count up, count down strategy to track quantities within the task."}, {"title": "4.2 TASKS", "content": "An interesting result is the impact of demonstration token type on the resulting alignment of the recurrent models with the Up-Down program. Figure 4 shows that recurrent models trained on the Same-Object task-in which the demo tokens are the same type as the resp tokens have poor alignment with any of the proposed SAs. We use this result to highlight the significance of the unified, interchangeable numeric representations found in the Multi-Object and Single-Object tasks.\nWe present a number of theoretical neural solutions to the counting task in Figure 3 as examples of possible neural solutions to each of the tasks. The Overlap Solution, shown in blue in Panel 3(a), is an example of how some solutions may fail to align with the Up-Down solution. In the Overlap Solution, we see that the Count is entangled with the phase of the trial due to the overlap of the trajectory on the vertical axis. In this model, we would be unable to distinguish between a count of n in the demo phase and a count of n + 1 in the response phase at the overlapping points in the trajectories. We do not make claims that this is how the Same-Object models are solving the task, but merely provide the theoretical models as ways that it could solve the task."}, {"title": "4.3 MODEL SIZE, LEARNING TRAJECTORIES, AND SYMBOLIC GRADIENCE", "content": "Figure 5 shows that although many model sizes can solve the Multi-Object task, increasing the number of dimensions in the hidden states of the GRUs improves IIA in alignments with the Up-Down program. We can also see in Figure 5 that the larger models tend to have less graded alignments. We examine the symbolic alignments over the course of training in Figure 5. Of note is the correlation between alignment and performance. This is especially pronounced in the larger models. And we note the relatively flat curves of the alignment trajectories after the models solve the task.\nWe now provide a deeper analysis of the symbolic alignments with neural systems, where we highlight the graded nature of the neural symbols. Figure 5 shows that the GRU models trained on the Multi-Object task have worse IIA when the quantities involved in the intervention are larger, and when the intervention quantities have a greater absolute difference. We point out that the task training data forces the models to have more experience with smaller numbers, as they necessarily interact with smaller numbers every time they interact with larger numbers. This is perhaps a causal factor for the more graded representations at larger numbers. The DAS training data suffers from a similar issue, where we use a uniform sampling of the object quantities that define the training sequences and then we uniformly sample the intervention indices from these sequences. This results in a disproportionately large number of training interventions containing smaller values."}, {"title": "5 DISCUSSION/CONCLUSION", "content": "In this work we used causal methods to demonstrate the existence of symbol-like number variables within NN solutions to numeric equivalence tasks. We showed that these numeric neural variables emerge purely from an NTP objective and represent abstract information that is only latent in the task structure. These findings are a proof of principle that neural systems do not need explicit exposure to discrete numeric symbols nor built in counting principles for symbol-like representations of number to emerge.\nWe also demonstrated differences in the high-level solutions used by different model architectures in different tasks. Namely, we showed that increasing the dimensionality of the GRUs improved their symbolic alignment, we showed that transformers solved the tasks by recomputing relevant information at each step in the sequence-contrasted against the cumulative count variables in the recurrent models and we showed that different solutions arise in the Same-Object Task compared to the Multi-Object and Single-Object variants. An interesting phenomenon in the LLM literature is the effect of model scale on performance (Brown et al., 2020; Kaplan et al., 2020). Although our scaling results are for GRUs on toy tasks, they are provocative for understanding why size might improve autoregressive results. Perhaps increased dimensionality allows the models to find more symbol-like, disentangled solutions when solving their NTP objectives. This is consistent with the early learning and strong correlation between performance and symbolic alignment demonstrated in larger models in Figure 5. We conjecture the possibility that this result can be explained by the lottery ticket hypothesis (Frankle & Carbin, 2019) combined with lazy learning dynamics (Jacot et al., 2020). Perhaps the majority of what these models learn are linear functions of their initial features,"}, {"title": "Preprint under review", "content": "and increasing the dimensionality of the model increases the number of potential pathways/features that the model can use to solve the task.\nWe are unsure if the \"stateless\", time-distributed solution exhibited by the transformers generalizes beyond the counting tasks presented in this work. It is possible that this finding is representative of a more general principle\u2014that transformers avoid solutions that use cumulative, Markovian state variables. We provide an analysis in Supplement A.4 of a one-layer transformer without positional encodings trained on a variant of the Single-Object task without a BOS token, and without a T token. We experimentally and mathematically support the idea that this model solves the task by assigning opposite numeric values to the demo and resp tokens and averaging their values at each step in the attention. From the relatively low alignment with the Last Value variable in Figure 3(c), it seems as though the Multi-Object RoPE transformers might rely, in part, on a positional readout. We managed to get a much higher alignment when using transformers trained on a variant of the task that breaks correlations between the position and Count of the sequence. We find it worth noting that the Ctx-Distr solution exhibited by the transformers lends itself to the type of solutions that might be predicted by RASP-L (Zhou et al., 2023).\nGRUs and LSTMs trained on the Same-Object Task failed to align with any of the SAs that we presented in this paper. To address this, we included Figure 3 showing the first two principal components of a Same-Object GRU model over different trial trajectories. We included theoretical models as examples of why some neural solutions might align with some SAs whereas others might not. We note that SAs that use memorization could trivially align with each of the recurrent models. One such solution might consist of a single variable that maps a tuple of the Count-Phase combination to a prediction. In this case, DAS would simply learn to transfer the complete state at each causal intervention. We are only concerned with solutions that are causally distinct from one another. We leave a more thorough, causal analysis of the Same-Object models to future work.\nAn important contribution of our work is in demonstrating the potential for misleading conclusions in the absence of causal analysis methods. We can see this in Figure 2 where a subset of the activations for the LSTM might be mistaken as sufficient causal features to change the model's count. Similarly, the PCA projections in Figure 3 might fail to provide predictions of neural alignment, and the attention weights shown in Figures 10- 13 might mislead on token value interchangeability. We wish to be clear, however, that these non-causal techniques are still fruitful as tools for scientific exploration and conceptualization, complementing causal methods.\nWe now expand upon the learning trajectories displayed in Figure 5. We can see from the performance curves that both the models' task performance and IIA begin a transition away from 0% at similar epochs and plateau at similar epochs. This result can be contrasted with an alternative result in which the alignment curves significantly lag behind the task performance of the models. Alternatively, there could have been a stronger upward slope of the IIA following the initial performance jump and plateau. In these hypothetical cases, a possible interpretation could have been that the network first develops more complex solutions or unique solutions for many different input-output pairs and subsequently unifies them over training. The pattern we observe instead is consistent with the idea that the networks are biased towards the simplest, unified strategies early in training. Perhaps our result is expected from works like Saxe et al. (2019) and Saxe et al. (2022) which show an inherent tendency for NNs trained via gradient descent to find solutions that share network pathways. This would provide a driving force towards the demo and resp phases sharing the same representation of a Count variable.\nWe demonstrated that the neural variables illuminated by DAS are not always perfectly symbolic, often exhibiting a smooth, graded influence from the content of the variables being intervened upon. We interpret these results as a reminder that representations in distributed systems exist on a continuum despite seemingly discrete, symbolic performance on tasks. These results have an analogy to children's number cognition in which children may appear to possess a symbol-like understanding of exact numbers and their associated principles, but when probed deeper, the symbol-like picture falls apart (Wynn, 1992; Davidson et al., 2012). Perhaps the graded nature of the neural variables reinforces the utility of thinking about network solutions as trajectories in a dynamical system. We use our findings as a reminder that although NNs may discover approximations to interpretable, symbol-like solutions, their representations are still ultimately graded-adding nuance to the effort of SA alignment."}, {"title": "A.1 ADDITIONAL FIGURES", "content": "Identity\n00\nEmbeddings\nAttention Outputs\nResidual Stream 0\nNorm &\nROPE &\nSingle Head Self Attn\nFFN Outputs\nNorm & t\nFeed Forward Network\nResidual Stream 1\nLayer 1\nHidden States\nIdentity\n0000000\nAttention Outputs\nResidual Stream 0\nNorm &\nROPE &\nSingle Head Self Attn\nFFN Outputs\nNorm &\nFeed Forward Network\nResidual Stream 1\nLayer 2\nHidden States\nLogits\nLinear\nFigure 6: Diagram of the main transformer architecture used in this work. The white rectangles represent activation vectors. The arrows represent model operations. Unless otherwise stated, all interchange interventions were performed on the Hidden State activations from Layer 1 or the Residual Stream 0 within Layer 1 for the key and value projections. All normalizations are Layer Norms (Ba et al., 2016)."}, {"title": "A.2 MODEL DETAILS", "content": "All artificial neural network models were implemented and trained using PyTorch (Paszke et al., 2019) on Nvidia Titan X GPUs. Unless otherwise stated, all models used an embedding and hidden state size of 20 dimensions. To make the token predictions, each model used a two layer multi-layer perceptron (MLP) with GELU nonlinearities, with a hidden layer size of 4 times the hidden state dimensionality with 50% dropout on the hidden layer. The GRU and LSTM model variants each consisted of a single recurrent cell followed by the output MLP. Unless otherwise stated, the transformer architecture consisted of two layers using Rotary positional encodings (Su et al., 2023). Each model variant used the same learning rate scheduler, which consisted of the original transformer (Vaswani et al., 2017) scheduling of warmup followed by decay. We used 100 warmup steps, a maximum learning rate of"}, {"title": "A.3 DAS TRAINING DETAILS", "content": "A.3.1 ROTATION MATRIX TRAINING\nTo train the DAS rotation matrices, we applied PyTorch's default orthogonal parametrization to a square matrix of the same size as the model's state dimensionality. PyTorch creates the orthogonal matrix as the exponential of a skew symmetric matrix. In all experiments, we selected the number of dimensions to intervene upon as half of the dimensionality of the state. We chose this value after an initial hyperparameter search that showed the number of dimensions had little impact on performance between 5-15 dimensions. We sampled 10000 sequence pairs and for each of these pairs, we uniformly sampled corresponding indices to perform the interventions. We excluded the BOS, and EOS tokens from possible intervention sample indices. When intervening upon a state in the demo phase, we uniformly sampled 0-3 steps to continue the demo phase before changing the phase by inserting the trigger token. We used a learning rate of 0.003 and a batch size of 512."}, {"title": "A.3.2 SYMBOLIC PROGRAM ALGORITHMS", "content": "A.4 SIMPLIFIED TRANSFORMER\nThe self-attention calculation for a single query $q_r \u2208 R^d$ from a response token, denoted by the subscript r, is as follows:\n$Attention(q_r, K, V) = V (softmax(\\frac{K^Tq_r}{\\sqrt{d}})) = \\frac{1}{\\sum_{j=1}^{n}e^{\\frac{q_kj}{\\sqrt{d}}}} \\sum_{i=1}^{n} e^{\\frac{q_ki}{\\sqrt{d}}} V_i = \\sum_{i=1}^{n} \\frac{e^{\\frac{q_ki}{\\sqrt{d}}}}{\\sum_{j=1}^{n}e^{\\frac{q_kj}{\\sqrt{d}}}} V_i$ (5)"}, {"title": "Algorithm 1 One sequence step of the Up-Down Program", "content": "q\u2190 Count\np \u2190 Phase\ny \u2190 input token\nif y == BOS then\nq\u21900, p-0\nreturn sample(D)\nelse if y \u2208 D then\nqq+1\nreturn sample(D)\nelse if y == T then\np\u2190 1\nelse if y == R then\nq\u2190q-1\nend if\nif (q == 0) & (p == 1) then\nreturn EOS\nend if\nreturn R"}, {"title": "Algorithm 2 One sequence step of the Up-Up Program", "content": "d\u2190 Demo Count\nr\u2190 Resp Count\np \u2190 Phase\ny\u2190 input token\nif y == BOS then\nd \u2190 0, r \u2190 0, p - 0\nreturn sample(D)\nelse if y \u2208 D then\nd-d+1\nreturn sample(D)\nelse if y == T then\np1\nelse if y == R then\nr\u2190r+1\nend if\nif (d == r) & (p == 1) then\nreturn EOS\nend if\nreturn R"}, {"title": "Algorithm 3 One sequence step of the specific Ctx-Distr Program", "content": "v \u2190 list of previous values excluding the most recent step\nl \u2190 Last Value\np\u2190 Phase\ny input token\nv.append(l)\ns - SUM(v)\nif y == BOS then\nl\u21900, p\u21900\nreturn sample(D)\nelse if s\u2264 0 and p == 1 then\nreturn EOS\nelse if y == T or y == R then\np\u2190 1\nl-1\nreturn R\nelse if y \u2208 D then\nl\u2190 1\nend if\nif p == 1 then\nreturn R\nelse\nreturn sample(D)\nend if"}, {"title": "A.5 ADDITIONAL INTERVENTIONS CONTINUED", "content": "We detail in this section why our activation transfers are sufficient to demonstrate that the transformers use a solution that re-references/recomputes the relevant information to solve the tasks at each step in the sequence. The hidden states in Layer 1 are a bottleneck at which a cumulative counting variable must exist if it were to use a strategy like the Up-Down or Up-Up programs. This is because the Attention Outputs of Layer 1 are the first activations that have had an opportunity to cross communicate between token positions. This means that the representations between the Residual Stream 1 of Layer 1 up to the Residual Stream 0 of Layer 2 cannot have read off a cumulative state from the previous token position other than reading off the positional information from the previous positional encodings. The 2-layer architecture is then limited in that it has only one more opportunity to transfer information between positions\u2014the attention mechanism in Layer 2. Thus, if a hidden state at time t were to have encoded a cumulative representation of the count that will be used by the model at time t + 1, that cumulative representation must exist in the activation vectors between the Residual Stream 1 in Layer 1 and the Residual Stream 0 of Layer 2. If it is using such a cumulative representation, then when we perform a full activation swap in the Layer 1 hidden states then the resulting predictions should be influenced by the swap. As Figures 4 and 14 indicate, the resulting transformer predictions are mostly unchanged by the intervention, demonstrating a recomputing of information at each step in the task."}, {"title": "A.6 VARIABLE-LENGTH TASK VARIANTS", "content": "Here we include additional tasks to prevent the transformers with positional encodings from learning a solution that relies on reading out positional information. We introduce Variable-Length variants of"}, {"title": "Preprint under review", "content": "each of the Multi-Object, Single-Object, and Same-Object tasks. In the Variable-Length versions, each token in the demo phase has a 0.2 probability of being sampled as a unique \"void\" token type, V, that should be ignored when determining the object quantity of the sequence. The number of demo tokens will still be equal to the object quantity when the trigger token is presented. We include these void tokens as a way to vary the length of the demo phase for a given object quantity, thus breaking correlations between positional information and object quantities. As an example, consider the possible sequence with a object quantity of 2: \"BOS V DVVDTRREOS\".\nWe show the transformer performance and the IIA for the Ctx-Distr interventions in Figure 14. Although we do not make strong claims about the manner in which these transformers solve these new tasks, we do highlight the fact that the transformers can no longer use a direct positional encoding readout to achieve 100% accuracy. These results are consistent with the hypothesis that the transformers are using the more specific, summing version of the Ctx-Distr strategy to solve these tasks, much as the no-positional encoding transformers do."}, {"title": "A.7 PRINCIPLE COMPONENTS ANALYSIS", "content": "pc0\n\u2022 BOS\n\u2022 DO\n\u2022 D1\n\u2022 D2\n\u2022 EOS\n\u2022 R\n\u2022 T\nFigure 15: Principal Components Analysis of a single GRU model seed including hidden state representations over 10 trials for each object quantity from 1 to 20 in the Multi-Object task variant. Green points indicate the start of a plotted trajectory, black points indicate an intermediate step, and red points indicate the end of a plotted trajectory. The blue line plots a single trajectory from start to finish with a object quantity of 3. Similarly, the orange and green lines follow single trajectories of 7 and 15 respectively.\nSingle-Object GRU\npc1\npc0\n\u2022 BOS\n\u2022 DO\n\u2022 EOS\n\u2022 R\n\u2022 T\nFigure 16: Principal Components Analysis of a single GRU model seed including hidden state representations over 10 trials for each object quantity from 1 to 20 in the Single Object task variant. Green points indicate the start of a plotted trajectory, black points indicate an intermediate step, and red points indicate the end of a plotted trajectory. The blue line plots a single trajectory from start to finish with a object quantity of 3. Similarly, the orange and green lines follow single trajectories of 7 and 15 respectively."}, {"title": "Preprint under review", "content": "pc0\nSame-Object GRU\n\u2022 BOS\n\u2022 C (Phase 0)\n\u2022 C (Phase 1)\n\u2022 EOS\n\u2022 T\nFigure 17: Principal Components Analysis of a single GRU model seed including hidden state representations over 10 trials for each object quantity from 1 to 20 in the Same-Object task variant. Green points indicate the start of a plotted trajectory, black points indicate an intermediate step, and red points indicate the end of a plotted trajectory. The blue line plots a single trajectory from start to finish with a object quantity of 3. Similarly, the orange and green lines follow single trajectories of 7 and 15 respectively."}]}