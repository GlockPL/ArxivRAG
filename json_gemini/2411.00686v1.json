{"title": "Latent Paraphrasing: Perturbation on Layers Improves Knowledge Injection in Language Models", "authors": ["Minki Kang", "Sung Ju Hwang", "Gibbeum Lee", "Jaewoong Cho"], "abstract": "As Large Language Models (LLMs) are increasingly deployed in specialized domains with continuously evolving knowledge, the need for timely and precise knowledge injection has become essential. Fine-tuning with paraphrased data is a common approach to enhance knowledge injection, yet it faces two significant challenges: high computational costs due to repetitive external model usage and limited sample diversity. To this end, we introduce LaPael, a latent-level paraphrasing method that applies input-dependent noise to early LLM layers. This approach enables diverse and semantically consistent augmentations directly within the model. Furthermore, it eliminates the recurring costs of paraphrase generation for each knowledge update. Our extensive experiments on question-answering benchmarks demonstrate that LaPael improves knowledge injection over standard fine-tuning and existing noise-based approaches. Additionally, combining LaPael with data-level paraphrasing further enhances performance.", "sections": [{"title": "Introduction", "content": "Pre-trained Large Language Models (LLMs) encode extensive factual information from their training data, enabling them to answer factoid questions such as \u201cWho is the director of Dune: Part Two?\" [4, 32]. However, knowledge in LLMs is static, which can lead to outdated information as real-world knowledge evolves. Additionally, LLMs often lack specificity for specialized or private domains. To address this, it is common practice to fine-tune LLMs with updated or domain-specific documents, keeping the model's knowledge up-to-date and enhancing expertise in particular domains [14, 17, 19].\nHowever, does fine-tuning LLMs on a single document allow them to fully internalize its knowledge? Even in pre-training, Kandpal et al. [20] found that LLMs cannot perfectly learn all the information in the training data, particularly long-tail knowledge that appears rarely or only once. Existing work [33] has shown that this issue persists with fine-tuning and suggested that data augmentation, such as paraphrasing, is a simple yet effective way to enhance knowledge injection. As shown in Figure 1, fine-tuning with paraphrases enhances knowledge injection, as evidenced by improved Question-Answering (QA) task performance.\nWhile data-augmented approach via paraphrasing is effective for knowledge learning, it has two main limitations: (1) High computational cost: Generating high-quality paraphrases requires significant computational resources. As shown in Figure 2, paraphrasing models such as LLMs [5, 7, 11, 58] need to repeatedly generate paraphrases for each document with the new incoming knowledge. This leads to higher costs as the number of documents being learned continually increases; and (2) Limited"}, {"title": "Related Work", "content": "Knowledge of Large Language Models Large Language Models (LLMs) store vast amounts of factual knowledge in their pre-trained parameters [36, 44]. The straightforward way to extract the knowledge of LLMs is to ask the question that requires factual knowledge [43, 58]. Through asking questions, Kandpal et al. [20] have found that LLMs cannot perfectly memorize the entire knowledge in the pre-training corpora, especially for knowledge that appears rarely or only once. To make LLMs answer the question requires under-represented or new knowledge, previous works have clustered into two different solutions. The first one is retrieval-augmented methods [26, 39, 42] that retrieve knowledge from an external knowledge base and input the retrieved knowledge alongside the question into LLMs. The second one is fine-tuning [12, 17] where the parameters of pre-trained LLMs are continually updated by fine-tuned on the document containing knowledge in an unsupervised way as in pre-training [37]. In our work, we focus on improving the fine-tuning-based solution, as storing new knowledge in the parameters of LLMs is efficient since we can reduce the length of the input prompt and do not need any extra module or memory in the deployment time [6].\nKnowledge Injection in LLMS In this work, knowledge injection in LLMs denotes fine-tuning LLMs on the set of documents to inject new or under-represented knowledge into LLMs [33, 17], different from another task of injecting symbolic knowledge (e.g., knowledge graph) into LLMs [55, 54]. Among previous works, CaMeLS [14] has introduced a meta-learning method for learnable loss scaling function that improves knowledge injection. As a concurrent work, MAC [45] has proposed using the memory of amortized context is highly effective in a knowledge injection. However, both methods have drawbacks like high computational costs for bi-level optimization or the need for additional modules and memory. Recent works [33, 58] have shown that data augmentation which paraphrases the knowledge-containing sentences helps language models memorize knowledge in a more extractable format (e.g., asking questions) after knowledge injection. Furthermore, Jiang et al. [19] has shown that the instruction-tuned model is better at learning new knowledge. Compared to previous works, we focus on developing an alternative method to data augmentation that perturbs the latent representation of LLMs for better knowledge injection.\nData Augmentation and Latent Perturbation The usefulness of data augmentations for text data was empirically observed in the literature. For instance, EDA [52] has introduced simple data augmentation method which randomly deletes, swaps, replaces, and inserts the words. Other previous works [22, 5, 30] have utilized the trained LMs to augment the text data. Recently, Maini et al. [29] has shown that adding data rephrased by LLMs into the pre-training corpus improves the performance of LM pre-training. However, those methods require additional costs in the knowledge injection as it utilize the LLMs to rephrase the text. In contrast, the latent perturbations offer an orthogonal approach to improve the robustness of neural networks, complementing data augmentation. This technique has been employed in meta-learning and out-of-distribution generalization [24, 25, 40]. For instance, NEFTune [16] demonstrated that adding noise, randomly sampled from a uniform distribution, to token embedding layers improves instruction tuning performance. Expanding on the concept of latent perturbations, our work introduces a novel approach that internalizes the effects of text paraphrasing by identifying optimal latent perturbations through training a small neural network within the LLMs."}, {"title": "Problem Formulation", "content": "In this work, we follow the knowledge injection setting outlined by Ovadia et al. [33]. We are given three resources: (1) documents $D_k$ containing knowledge that we are interested to inject; (2) question & answering dataset $D_{QA} = \\{(q^{(i)}, a^{(i)})\\}_{i=1}^{N}$ for verifying injected knowledge from $D_{k}$; and (3) a pre-trained Large Language Models (LLMs) $p_{\\theta}(\\cdot)$ parameterized by $\\theta$. Our objective is to find a transformation $F$ that could enhance the knowledge about $D_{QA}$:\n$\\theta' = F(\\theta, D_k) \\quad \\text{s.t.} \\quad S(\\theta', D_{QA}) > S(\\theta, D_{QA}),$ (1)\nwhere the score function $S$ is defined as:\n$S(\\theta, D_{QA}) := \\frac{\\sum_{i=1}^{N} \\mathbb{I}(f(p_{\\theta}(q^{(i)})) = a^{(i)})}{n},$ (2)\nand $\\mathbb{I}(\\cdot)$ and $f(\\cdot)$ denote the indicator function and a decoding function that samples a sequence of tokens from $p_{\\theta}$, respectively."}, {"title": "Proposed Method", "content": "We propose Latent Paraphrasing of Language Models (LaPael), a framework that perturbs the latent feature of LLMs, to achieve the equivalent effect of data augmentation at the latent level. Knowledge injection using LaPael consists of the following four processes: paraphrasing the set of documents to make the paraphrased data (Section 4.1), training the latent paraphrasers with paraphrased data (Section 4.2), fine-tuning LLMs with the trained latent paraphrasers on $D_k$ and evaluate the injected knowledge of LLMs on $D_{QA}$ (Section 4.3)."}, {"title": "Data Augmentation: Paraphrasing", "content": "To train the latent paraphrasers, we need a distinct set of training data $D_{train} = \\{s^{(i)}\\}_{i=1}^{N}$ which consists of documents having different knowledge with $D_k$. As a preliminary, we formulate the paraphrasing of the text in terms of the knowledge equivalence, which is a narrower concept than semantic equivalence [23] where two different sentences can contain the same knowledge. We consider that each sentence $s$ in $D_{train}$ can be decomposed into words for the object (entity or attribute) of the knowledge ($y$) and others ($x$) where both are the sequence of tokens. For instance, given the sentence \"The capital of the United States is Washington D.C.\u201d,\n$x = \\text{``The capital of the United States is''}; \\quad y = \\text{``Washington D.C.''},$\nrepresent the knowledge (United States, capital, Washington D.C.). Then, we paraphrase a sentence $(x, y)$ into a paraphrased sentence\u00b9. For the above sentence, a paraphrased sentence can be\n$x' = \\text{``In the case of the United States, the designated capital city is''}$\nwith the same $y$, which is knowledge equivalent to $(x, y)$. For each knowledge $K$, we assume that there is a set of the knowledge equivalent sentences $S(K)$ where $(x, y) \\in S(K)$. We generate $K$ paraphrased sentence via a LLM: $(x_1, y), ..., (x_K, y) \\sim P_{LLM}(x' \\text{ prompt}, x, y)$. Then, we have the set of paraphrased data $\\{\\{(x'^{(i)}_k, y^{(i)})\\}_{k=1}^{K}\\}_{i=1}^{N}$ of $D_{train}$. We define $p(x'|x) := P_{LLM}(x' \\text{ prompt}, x, y)$ which denotes the probability distribution of paraphrases given the original sentence."}, {"title": "Introducing Latent Paraphraser", "content": "Latent Paraphraser We introduce a latent paraphraser within a transformer layer [50], which augments a latent feature and is expected to paraphrase the given input text within the latent space. As illustrated in Figure 3(a), within the transformer architecture, we insert this new layer just before the Multi-layer Perceptron (MLP), using the output from the second LayerNorm as its input.\nLet $h \\in \\mathbb{R}^d$ denote the latent feature after the second LayerNorm. The latent paraphraser, denoted by $g_{\\phi}: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ and parameterized by $\\phi$, augments the latent feature as follows:\n$h \\odot g_{\\phi}(h),$ (4)\nwhere $\\odot$ is the element-wise multiplication. The function $g_{\\phi}(h)$ is given by:\n$g_{\\phi}(h) = (1 - m) \\cdot 1 + m \\cdot z,$ (5)\nwith $z \\in \\mathbb{R}^d$ and $m \\in [0, 1]$ representing a noise vector and a learnable mask, respectively. The noise vector $z$ is generated by\n$z = \\text{softplus}(MLP_z(\\alpha)), \\quad \\alpha \\sim \\mathcal{N}(\\mu, I), \\quad \\mu = W_{\\mu}h + b_{\\mu},$ (6)\nwhere $MLP_z$ is a 2-layers MLP. We use the reparameterization trick [21] to enable the back-propagation through the sampling from the Gaussian distribution: $\\alpha = \\mu + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, I)$.\nTo modulate the scale of perturbation for individual tokens, we employ a learnable mask. It is important as too much noise on key tokens (e.g., United States) might hurt the semantics of the sequence. For learnable binary mask, we use concrete distribution to approximate the sampling discrete random variable from a Bernoulli distribution using continuous relaxation [8] as follows:\n$m = \\text{sigmoid}\\left(\\frac{1}{\\tau}(\\log(u) + \\log(1 - u) + \\hat{m})\\right), \\quad \\hat{m} = W_m h + b_m,$ (7)\nwhere $u \\sim \\text{Unif}(0, 1)$, $\\tau$ is temperature, and $\\hat{m}$ is mask value in scalar.\nTraining Then, how do we train the latent paraphrasers to approximate optimal perturbation functions for estimating the distribution of the paraphrased text? We employ the dataset with paraphrases $\\{\\{(x'^{(i)}_k, y^{(i)})\\}_{k=1}^{K}\\}_{i=1}^{N}$ generated in Section 4.1. Our objective is to match two distributions for each transformer layer:\n1. the distribution of transformer layer output feature for the last token $h_{out}$ without the latent paraphraser given the data perturbation distribution $p(x'|x)$ from Section 4.1:\n$p_{\\theta}(h_{out}|x) = \\int p_{\\theta}(h_{out}|x')p(x'|x)dx';$ (8)\n2. the distribution of output feature for the last token $h_{out}$ with the latent paraphraser given $x$, $p_{\\theta, \\phi}(h_{out}|x)$. As a latent paraphraser outputs stochastic noise, we can formulate the probabilistic distribution $p_{\\theta, \\phi}(h_{out}|x)$ as follows:\n$p_{\\theta, \\phi}(h_{out}|x) = \\int p_{\\theta}(h_{out}|x, z)p_{\\theta, \\phi}(z|x)dz,$ (9)\nwhere $p_{\\theta, \\phi}(z|x)$ is the distribution for noise from the latent paraphraser in Equation (6).\nWe make the simplistic parametric assumption that both distributions are Gaussian:\n$p_{\\theta}(h_{out}|x) \\sim \\mathcal{N}(h_{out}; \\mu_{data}, \\Sigma_{data} I); \\quad p_{\\theta, \\phi}(h_{out}|x) \\sim \\mathcal{N}(h_{out}; \\mu_{latent}, \\Sigma_{latent} I).$ (10)\nTo train latent paraphrasers, we minimize the symmetric Kullback-Leibler (KL) divergence between two estimated Gaussian distributions of each layer as follows:\n$\\mathcal{L}_{KL}(x) = \\frac{1}{2}(D_{KL}(p_{\\theta}(h_{out}|x)||p_{\\theta, \\phi}(h_{out}|x)) + D_{KL}(p_{\\theta, \\phi}(h_{out}|x)||p_{\\theta}(h_{out}|x))),$ (11)\n$D_{KL}(p_{\\theta}(h_{out}|x)||p_{\\theta, \\phi}(h_{out}|x)) = \\log \\left(\\frac{\\sigma_{latent}}{\\sigma_{data}}\\right) + \\frac{(\\sigma_{data})^2 + (\\mu_{data} - \\mu_{latent})^2}{2 \\sigma_{latent}^2} - \\frac{1}{2},$ (12)"}, {"title": "Fine-tuning the LLM with the Trained Latent Paraphrasers", "content": "We fine-tune the LLM on documents containing knowledge to be injected ($D_K$) as in Equation (3). We use the trained latent paraphraser parameterized by $\\phi^*$ during LLM fine-tuning as follows:\n$\\theta^* = \\underset{\\theta}{\\text{arg min }} \\frac{1}{|D_k|} \\sum_{s \\in D_k} \\left( \\frac{1}{s} \\sum_{t=1}^{s} - \\log p_{\\theta, \\phi^*}(s_t | z^{(j)}, s_{<t})p_{\\theta, \\phi^*}(z^{(j)} | s_{<t})\\right),$ (17)\nwhere we sample $N$ noise $z^{(j)}$ by sampling multiple $\\alpha$ from Gaussian distribution as defined in Equation (6). Then, we evaluate the knowledge injected in LLMs by measuring $S(\\theta^*, D_{QA})$ as defined in Equation (2)."}, {"title": "Experiments", "content": "In experiments, we validate the effectiveness of the proposed method, LaPael, in injecting new or under-represented knowledge into Large Language Models (LLMs)."}, {"title": "Experimental Setting", "content": "To follow the experimental setup in Section 3, we need (1) documents containing knowledge $D_K$ and (2) associated QA datasets $D_{QA}$. We mainly use the test split of three QA datasets: SQUAD [38], StreamingQA [27], and ArchivalQA [51] for the source of $D_K$ and $D_{QA}$ in our main experiments. These datasets, previously used in Hu et al. [14], consist of documents paired with their corresponding QAs, making them well-suited to our experimental setup. While the questions in these datasets are of decent quality, a significant limitation lies in the documents provided. These documents are likely to have been seen by LLMs during pre-training, making it difficult to accurately assess the performance of methods on injecting new knowledge."}, {"title": "Proposed Method", "content": "We propose Latent Paraphrasing of Language Models (LaPael), a framework that perturbs the latent feature of LLMs, to achieve the equivalent effect of data augmentation at the latent level. Knowledge injection using LaPael consists of the following four processes: paraphrasing the set of documents to make the paraphrased data (Section 4.1), training the latent paraphrasers with paraphrased data (Section 4.2), fine-tuning LLMs with the trained latent paraphrasers on $D_k$ and evaluate the injected knowledge of LLMs on $D_{QA}$ (Section 4.3)."}, {"title": "Experimental Results", "content": "Experiments with Synthetic Documents In Table 2, we present the experimental results for the synthetic documents setting. Fine-tuning does improve the QA performance of LLMs, but it does not lead to near-perfect scores even though the synthetic document contains the necessary knowledge for answering the questions, as shown in Table 1.\nOur experiments show that paraphrasing documents for fine-tuning consistently improves QA per-formance across all three benchmarks. Notably, LaPael demonstrates performance comparable to fine-tuning with paraphrases on StreamingQA and even outperforms it on two other benchmarks. These findings suggest that the latent paraphrasers learn an effective noise distribution that aids knowledge injection without additional data augmentation.\nWe also compared LaPael with two other noise-based methods, FreeLB [57] and NEFTune [16], to validate that the latent-level noise generated by latent paraphrasers is more effective. As shown in Table 2, LaPael outperforms these baselines, confirming the strength of our approach.\nExperiments with Raw Documents While our method has proven effective for knowledge injection with synthetic documents, it is important to evaluate its performance on raw documents, which represent a more realistic data format. To demonstrate the applicability of our method to real-world data, we conducted experiments in which we fine-tuned LLMs on raw documents for each dataset, using latent paraphrasers trained on $D_{train}$ from SQuAD-syn.\nAs shown in Table 3, our method outperforms both fine-tuning and noise-based baselines in the context of knowledge injection with raw documents. Considering that the latent paraphrasers were trained on synthetic sentences from $D_{train}$, these results demonstrate their effectiveness on documents with a different format than those used in training.\nCross-domain Transfer Once trained, the latent paraphrasers can be applied to fine-tune LLMs on documents from any domain. To demonstrate this, we conducted cross-domain transfer experiments. Specifically, we trained latent paraphrasers on $D_{train}$ from a source domain (e.g., SQUAD) and fine-tuned LLMs with the trained latent paraphrasers on $D_k$ from a target domain (e.g., StreamingQA)."}, {"title": "Ablation Studies", "content": "Effects of the Size of $D_{train}$ LaPael needs additional data $D_{train}$ for training latent paraphrasers. Although only a small amount of data is required, it might be unclear how much is needed to make the latent paraphrasers learn the useful noise distribution. As shown in Figure 5a, LaPael works well even with 50 sentences for $D_{train}$, while increasing the size of $D_{train}$ ensures a steady performance improvement for LaPael.\nEffects of the Position of Latent Paraphrasers Our latent paraphrasers can be inserted into any layer of the LLMs. The possible question is which position and how many layers are optimal for latent paraphrasers to effectively learn noise for knowledge injection. To answer this, we analyzed the position and number of latent paraphrasers.\nIn Figure 5b, we show the QA accuracy results, varying the start position and number of latent paraphrasers. The first layer is the closest layer to the input layer, and \"start position 1\" with \"# layers = 3\" means we insert the latent paraphrasers into the first, second, and third layers of the LLM. Results show that inserting three latent paraphrasers into the early layers of the LLM is effective. This is consistent with findings in previous works [16, 57, 25] where using noisy token embeddings (the lowest layer) enhanced the generalization in LLMs. Furthermore, in Table 5, we empirically show that positioning the latent paraphraser before the MLP layer within each transformer layer is the most effective choice over other positions."}, {"title": "Conclusion", "content": "We have introduced LaPael, a method for enhancing knowledge injection in Large Language Models (LLMs) by applying learned perturbations to their layers. Unlike traditional data-level augmentations or noise-based approaches, LaPael operates at the latent level, preserving the semantic integrity of the text while introducing meaningful variability. LaPael addresses key limitations of existing methods by reducing computational costs and increasing the diversity of augmented data. Our extensive validation across diverse benchmark datasets demonstrates the superiority of our method in knowledge injection, as it significantly outperforms both standard fine-tuning and other noise-based baselines. Moreover, combining LaPael with paraphrases yields complementary benefits, further enhancing performance. We believe that LaPael, being simple yet effective, has the potential for significant practical impact and will encourage further research on applying perturbation within the latent space of LLMs."}, {"title": "Discussions & Limitations", "content": "Cost Analysis Our method requires additional costs compared to the fine-tuning baseline. Specifically, it involves two extra computational costs beyond standard fine-tuning. A comparison of the per-step computational cost (in GFLOPs) between the baseline and our proposed method is shown in Table 8, where we consider fine-tuning LLMs with 7B parameters. In detail, one forward pass of a 7B parameter LLM requires 13.21 GFLOPs, and one backward pass costs twice as much as a forward pass. The latent paraphraser model we used in the experiments consists of 5 paraphrasers, each with 4 linear layers, totaling 250M parameters, which is 3.6% of the parameter size of the LLM. The total computational costs can vary depending on the hyperparameters (e.g., $N$ in Equation (13)) and the size of the dataset used.\nWhile training the latent paraphrasers requires an initial cost, this is a one-time expense. Once trained, these can be used repeatedly for knowledge injection without additional ongoing costs. This makes the overall expense relatively low in the long term. Furthermore, incorporating latent paraphrasers during fine-tuning adds only a minimal computational overhead, as their parameter size is just 3.6% of the size of LLM.\nKnowledge Retention A common drawback of knowledge injection is the potential for LLMs to forget previously learned knowledge [17]. To assess this issue, we used the EntityQuestions dataset [41], which contains simple questions about entities. Specifically, we focused on \"place-of-birth\" questions for well-known entities (e.g., \"Where was Leonardo da Vinci born?\"), with 988 questions in total. We fine-tune the Vicuna-7b-v1.5 [56] on a synthetic SQUAD document set ($D_K$) using each method, then measure its QA performance on the EntityQuestions.\nAs in Table 9, the experimental results show that all fine-tuning approaches negatively impact knowledge retention, as observed in the previous work [9]. Additionally, we observe that improved knowledge injection often comes at the cost of greater knowledge forgetting. Although our primary focus is on enhancing knowledge injection, we acknowledge that addressing knowledge retention is crucial and should be a focus of future research.\nComparison to RAG The primary advantages of fine-tuning methods, including ours, over retrieval-based approaches like Retrieval-Augmented Generation (RAG) [26], lie in their simplicity and reduced computational cost on the inference [6]. Fine-tuning results in a self-contained model, which simplifies the system architecture by removing the need for additional components like document retrieval and ranking during inference. This reduction in complexity leads to lower computational overhead, especially in terms of GPU memory usage due to the shorter length of the prompt, making fine-tuning more suitable for an LLM deployment in resource-constrained environments.\nHowever, it is important to check the performance gap between them. Therefore, we experiment with RAG on the Events 2024 dataset with Vicuna-7b. For ours, we follow the same experimental setting with Table 3. For RAG, we use the bge-large-en-v1.5 [53] model for document and query embedding for retrieval. In Table 10, our experimental results indicate that the RAG approach outperforms fine-tuning methods including ours, as previously observed by de Luis Balaguer et al. [6]. However, our LaPael method narrows the gap between the two approaches, suggesting that there is potential for further improvements in fine-tuning strategies."}]}