{"title": "Evaluating Loss Landscapes from a Topology Perspective", "authors": ["Tiankai Xie", "Caleb Geniesse", "Jiaqing Chen", "Yaoqing Yang", "Dmitriy Morozov", "Michael W. Mahoney", "Ross Maciejewski", "Gunther H. Weber"], "abstract": "Characterizing the loss of a neural network with respect to model parameters, i.e., the loss landscape, can provide valuable insights into properties of that model. Various methods for visualizing loss landscapes have been proposed, but less emphasis has been placed on quantifying and extracting actionable and reproducible insights from these complex representations. Inspired by powerful tools from topological data analysis (TDA) for summarizing the structure of high-dimensional data, here we characterize the underlying shape (or topology) of loss landscapes, quantifying the topology to reveal new insights about neural networks. To relate our findings to the machine learning (ML) literature, we compute simple performance metrics (e.g., accuracy, error), and we characterize the local structure of loss landscapes using Hessian-based metrics (e.g., largest eigenvalue, trace, eigenvalue spectral density). Following this approach, we study established models from image pattern recognition (e.g., ResNets) and scientific ML (e.g., physics-informed neural networks), and we show how quantifying the shape of loss landscapes can provide new insights into model performance and learning dynamics.", "sections": [{"title": "1 Introduction", "content": "Given the important role that the loss function plays during learning, examining it with respect to a neural network's weights\u2014by visualizing the so-called loss landscape-can provide valuable insights into both network architecture and machine learning (ML) dynamics [Martin and Mahoney, 2021, Martin et al., 2021, Yang et al., 2022b, 2021, Zhou et al., 2023]. Indeed, the loss landscape has been essential for understanding certain aspects of deep learning, including, but not limited to, test accuracy, robustness of transfer learning [Djolonga et al., 2021], robustness to out-of-distribution"}, {"title": "2 Background on TDA", "content": "Topological data analysis (TDA) aims to reveal the global underlying structure of data. It is particularly useful for studying high-dimensional data or functions, where direct visualization is impossible. Here, we leverage ideas and algorithms from TDA to study the structure of the loss function, i.e., the so-called loss landscape. In the context of a loss function, we are interested in the number of minima (i.e., unique sets of parameters for which the loss is locally minimized) and how \u201cprominent\" they are (i.e., measuring how many other sets of neighboring parameters have a higher loss than the parameter sets that locally minimize the loss function). Such information can be obtained from the merge tree and persistence diagram (i.e., captured by the 0-dimensional persistent homology).\nA merge tree [Carr et al., 2003, Heine et al., 2016] tracks connected components of sub-level sets L\u00ae(v) = {x \u2208 D;x < v} as a threshold, v, is increased. Note, the merge tree can track either sub-level or super-level sets, but here we are interested in characterizing loss functions and their minima, so we focus on sub-level sets. In this case, as v increases, new connected components form at local minima and later merge with neighboring connected components (other local minima) at saddles. The merge tree encodes these changes in the loss landscape as nodes in a tree-like structure, where local minima are represented by degree-one nodes (connected to other local minima through a single saddle point), and the saddle points connecting different minima are represented by degree-three nodes (each connecting two local minima and one other saddle point).\nA persistence diagram represents features (i.e., branches in the merge tree) as points in a two-dimensional plane. The horizontal axis corresponds to the birth of each feature-which is the value of the minimum at which it first appears. The vertical axis corresponds to the death of each feature-which is the value of the saddle where it merges into a more persistent feature. The distance between a point and the diagonal line y = x encodes the persistence of the feature\u2014which is a measure of how long the feature lasts in the filtration (i.e., as the threshold, v, is increased). Such a metric captures information about the ruggedness of the landscape, and for example, the depth of local valleys and the height of the barriers between them. Here, we quantify the average persistence by computing the distance between each persistence pair and the diagonal and then taking the average."}, {"title": "3 Empirical Results", "content": "In this section, we provide a summary of our main empirical results. We study established models from image pattern recognition (e.g., ResNets) and scientific ML (e.g., physics-informed neural networks). We first show how removing residual connections from ResNet changes the shape of the loss landscape. We then show how the loss landscape changes for a physics-informed neural network as a physical parameter is varied and optimization begins to fail. In both experiments, we quantify the merge tree and persistence diagrams, and we relate our results to model performance and loss landscape metrics like the top Hessian eigenvalue and the Hessian trace."}, {"title": "3.1 Image Pattern Recognition", "content": "In our first experiment, we explore image pattern recognition using ResNet-20 trained on CIFAR-10 [Yao et al., 2020]. Specifically, we look at (and quantify) loss landscapes before and after adding residual connections. Recent work shows that the residual connections are related to the \"smoothness\" of the loss landscape [Li et al., 2018, Yao et al., 2020]. Here, we aim to verify this and further characterize how the residual connections in ResNet-20 change the underlying loss landscape. Note, we removed the residual connections from ResNet-20 before training. The accuracy of ResNet-20 without residual connections (90% average accuracy across four random seeds) was slightly lower than ResNet-20 with residual connections (92% average accuracy across four random seeds)."}, {"title": "3.2 Physics-Informed Neural Networks", "content": "In our second experiment, we look at a set of physics-informed neural network (PINN) models trained to solve increasingly difficult convection problems. Specifically, we consider the one-dimensional convection problem, a hyperbolic partial differential equation that is commonly used to model transport phenomena:\n$\\frac{\\partial u}{\\partial t} +\\beta \\frac{\\partial u}{\\partial x} = 0, x \\in\\Omega, t\\in [0, T]$  (1)\n$u(x,0) = h(x), x\\in \\Omega$, (2)\nwhere \u1e9e is the convection coefficient and h(x) is the initial condition. The general loss function for this problem is\n$L(\\theta) = \\frac{1}{N_u} \\sum_{i=1}^{N_u}(\\hat{u_i} - u_i)^2 + \\frac{1}{N_f} \\sum_{i=1}^{N_f} (\\frac{\\partial \\hat{u}}{\\partial t} +\\beta \\frac{\\partial \\hat{u}}{\\partial x})^2 +L_B$, (3)\nwhere \u00fb = NN(0,x,t) is the output of the NN, and LB is the boundary loss. The goal of this case study is to investigate the PINN's soft regularization and how it helps (or fails to help) the optimizer find optimal solutions to seemingly simple convection problems. As shown in Krishnapriyan et al. [2021], increasing the wave speed parameter, \u1e9e, can make it harder for the PINN to find a reasonable solution. This difficulty has been linked to changes in the loss landscape, which becomes increasingly complicated, such that optimizing the model becomes increasingly difficult.\nIn Fig. 2, we show that increasing the wave speed indeed results in a more complicated loss landscape. In the bottom row, we show the spatiotemporal patterns predicted by the PINN models. Note, increasing \u1e9e makes the problem harder to solve, and here we can see that our PINN models fail to"}, {"title": "4 Conclusion and Future Work", "content": "In this work, we study neural network loss landscapes through the lens of TDA. Specifically, we capture the underlying shape of loss landscapes using merge trees and persistence diagrams. By quantifying these topological constructs, we reveal new insights about the landscapes. Furthermore, we explore the relationship between our TDA-based metrics and relevant traditional ML metrics.\nFor ResNet models, we find that the number of saddle points is inversely related to the average persistence and to the other ML-based metrics. For PINN models, we find that the number of saddle points and average persistence increase together along with the other ML-based metrics. These relationships reflect the curvature and sharpness of the landscape, which in turn strongly impacts the model's performance and generalization abilities. Note, in this work we only show the 0-dimensional persistence diagram, which is exactly what the branches in the merge tree encode. Since here our original focus was on extracting merge trees, we decided to limit our analysis to these lower-dimensional features. We leave the analysis of higher-dimensional holes for future work.\nMoreover, since the merge tree and persistence diagram can be computed for arbitrary-dimensional spaces, our generalized and scalable approach opens up the door to studying loss landscapes in higher dimensions. In the future, we hope to extend our approach to characterize higher-dimensional loss landscapes, under the hypothesis that more information (hidden in these additional dimensions) could perhaps provide new insights into the loss function and properties relating neural network architecture to learning dynamics. One simple way to do this would be to sample along more directions. So, for example, we could construct a subspace based on the top 3 to 10 Hessian eigenvectors. While this would still be far from the potentially billions of dimensions in the true high-dimensional loss landscapes of modern ML models (with potentially billions of parameters), we expect that there exists a much lower-dimensional manifold upon which the interesting variation can be observed."}]}