{"title": "XGRAMMAR: FLEXIBLE AND EFFICIENT STRUCTURED GENERATION ENGINE FOR LARGE LANGUAGE MODELS", "authors": ["Yixin Dong", "Charlie F. Ruan", "Yaxing Cai", "Ruihang Lai", "Ziyi Xu", "Yilong Zhao", "Tianqi Chen"], "abstract": "The applications of LLM Agents are becoming increasingly complex and diverse, leading to a high demand for structured outputs that can be parsed into code, structured function calls, and embodied agent commands. These developments bring significant demands for structured generation in LLM inference. Context-free grammar is a flexible approach to enable structured generation via constrained decoding. However, executing context-free grammar requires going through several stack states over all tokens in vocabulary during runtime, bringing non-negligible overhead for structured generation. In this paper, we propose XGrammar, a flexible and efficient structure generation engine for large language models. XGrammar accelerates context-free grammar execution by dividing the vocabulary into context-independent tokens that can be prechecked and context-dependent tokens that need to be interpreted during runtime. We further build transformations to expand the grammar context and reduce the number of context-independent tokens. Additionally, we build an efficient persistent stack to accelerate the context-dependent token checks. Finally, we co-design the grammar engine with LLM inference engine to overlap grammar computation with GPU executions. Evaluation results show that XGrammar can achieve up to 100x speedup over existing solutions. Combined with an LLM inference engine, it can generate near-zero overhead structure generation in end-to-end low-LLM serving.", "sections": [{"title": "INTRODUCTION", "content": "Recent advancements in large language models (LLMs) have created new possibilities for complex applications such as code generation (Chen et al., 2021; Wang et al., 2021), debugging (Pearce et al., 2022; Mozannar et al., 2024), external tool invocation through function calling (OpenAI, 2024; LangChain, 2024), and robotic control (Liu et al., 2023). These applications bring great demand for LLM systems to perform structured generation and produce outputs that follow specific formats, such as JSON, SQL or other structures tailored to the task. The downstream applications can then organically consume the structured outputs to perform followup interactions with the system.\nConstrained decoding (Deutsch et al., 2019; Kuchnik et al., 2023) is a commonly adopted method for structured generation. At each decoding step, constrained decoding examines the vocabulary and filters out tokens that violate the specified structure by setting the probabilities of invalid tokens to zero. To support the rich structure formats arising in diverse applications, a flexible mechanism is needed to specify and check the constraints. Context-free grammar (CFG) (Chomsky, 1956; Poesia et al., 2022; Scholak et al., 2021) provides a general approach for defining structures through a set of rules. Each rule contains a sequence of characters or other rules, allowing recursive composition to represent complex structures. Compared to alternative formats such as regular expressions, CFGs offer greater flexibility by allowing recursive structures, making them suitable for describing common languages such as JSON, SQL, and domain-specific languages (DSLs).\nHowever, naively applying CFG to constrained decoding is not efficient because of its flexible nature. First, each decoding step needs to interpret CFG for every possible token in the vocabulary, which can be as large as 128k in Llama 3.1 (Dubey et al., 2024a). Additionally, CFG interpretation requires a stack state that tracks the recursive rules matched so far, making it impossible to precompute and cache all combinatorial combinations of stack patterns ahead of time. Finally, each token in the LLM generation comprises multiple characters, which may cross the boundaries of grammar elements and cause further recursion or stack pop during runtime execution. The misaligned boundaries bring the need to handle them carefully during grammar execution.\nIn this paper, we introduce XGrammar, a flexible and efficient structured generation engine for large language models to address the above challenges. XGrammar builds a byte-level pushdown automaton to represent context-free"}, {"title": "BACKGROUND", "content": ""}, {"title": "LLM Constrained Generation", "content": "Large Language Models (LLMs) like GPT-4 (OpenAI et al., 2024), Llama (Dubey et al., 2024a), and Mistral (Jiang et al., 2023) generate text in an auto-regressive manner, predicting one token at a time based on preceding sequence of tokens. The process starts with an initial prompt and continues as the model iteratively appends tokens until the response is complete. In LLMs, tokens serve as the basic input and output units. Each token represents a fixed string but may not correspond to a complete semantic unit or may break a Unicode character (Wang et al., 2019), creating challenges for structured text generation. At each step, the model produces a logits vector across its vocabulary, which is then converted into a probability distribution using the softmax function (Bridle, 1989). A sampler then selects the next token from this distribution.\nConstrained decoding guides the structure of LLM-generated text by restricting available tokens at each step, as illustrated in Figure 2. At each step, tokens that would violate the required structure are identified as invalid. Their logits are set to -\u221e, effectively assigning them zero probability after the softmax operation and preserving the relative probabilities of other valid tokens. This ensures that only valid tokens are sampled. Efficiently identifying and masking invalid tokens is essential, as it directly impacts generation speed."}, {"title": "Context-free Grammar and Pushdown Automata", "content": "Context-free grammar (CFG) (Chomsky, 1956) is widely used to define structures in structured generation. With an example shown in Figure 3, CFG contains multiple rules, each including characters or references to other rules, allowing recursive composition to define complex structures. This makes CFG suitable for languages such as JSON, SQL, and various domain-specific languages. CFG's recursive nature provides greater expressive power than simpler patterns, such as regular expressions, which are also frequently applied in LLM structured generation.\nPushdown automata (PDA) (Sch\u00fctzenberger, 1963; Evey, 1963) are typically used to recognize languages generated by CFGs, as they employ a stack to manage nested structures."}, {"title": "XGRAMMAR", "content": "As shown in Figure 1, XGrammar utilizes a byte-level pushdown automaton to interpret the context-free grammar. This byte-level design allows each character edge to include one or more bytes, handling irregular token boundaries and supporting tokens containing sub-UTF8 characters. The automaton's structure is optimized to accelerate matching, as described in \u00a73.4. In the preprocessing phase, we generate an adaptive token mask cache, as detailed in \u00a73.1, which accelerates runtime mask generation by precomputing context-independent tokens. The effectiveness of this cache is further enhanced by context extension in \u00a73.2. At runtime, the token mask cache quickly generates most of the mask, while the persistent execution stack in \u00a73.3 efficiently processes the rest context-dependent tokens. Additionally, mask generation and LLM inference are overlapped in \u00a73.5 to minimize the overhead of constrained decoding. Once the LLM generates a new token under the mask constraint, this token is then used to update the stack state of the pushdown automaton for the next mask generation."}, {"title": "Adaptive Token Mask Cache", "content": "To accelerate the generation of the token mask cache, the adaptive token cache categorizes tokens into two types (Figure 4): context-independent tokens, which constitute the vast majority and can be pre-computed, and context-dependent tokens, which require slower, on-the-fly processing but are relatively few. This token classification relates to how tokens are validated by the pushdown automaton. We found that, considering the transition of the stack state, the process of matching tokens to the automaton can be divided into three categories:\n1. The matching process expands into a child rule, pushing new elements onto the stack.\n2. The matching process advances within the current rule, updating the stack top node to a new position.\n3. The matching process reaches the end of the current rule and returns to a parent rule, popping elements from the stack.\nValidating tokens in the former two cases only relies on the stack top node, which represents the position within the current rule, so we define these tokens as context-independent tokens. The tokens in the third type, however, requires inspecting the entire running stack in validation, and are defined as context-dependent tokens. For every node of the pushdown automaton, there is a set of context-independent tokens with this node being at the top of the stack at runtime, and their validity can be determined ahead of time. Therefore, we precompute the validity of these tokens and store them in a cache with the stack top node as the key, which we refer to as the adaptive token mask cache. It also adaptively selects the most efficient storage format based on the cache's contents, as explained in the next paragraph.\nAt runtime, we retrieve the validity of context-independent tokens directly based on the top of the stack to generate the token mask. The remaining few context-dependent tokens are validated by executing the pushdown automaton with the full stack. If parallel stacks exist due to the ambiguity of the grammar, the token masks for every stack is merged into a final token mask by finding the union of the accepted tokens in each mask. The computation for the token mask is significantly reduced because our method do not need to check context-independent tokens at runtime. Experiments show that context-dependent tokens account for only a minor proportion, amounting to less than 1% (1134 out of 128k) for the Llama-3.1 model using JSON grammar.\nAdaptive storage. The token mask cache adopts an adaptive storage format to reduce memory usage, as illustrated in Figure 5. For each automaton node, the token mask cache divides the vocabulary into three parts: the accepted context-independent tokens, the rejected context-independent tokens, and the context-dependent tokens. Since these three parts together cover all tokens, it is sufficient to store only the two smaller subsets. We observe that, for a set of context-independent tokens, they tend to be either almost entirely accepted, namely accept-heavy cases, or almost entirely rejected, namely reject-heavy cases. This arises because, if wildcards can be matched from the current node, such as the wildcard `[^", "format": "n1. For accept-heavy cases, we store the rejected context-independent tokens and context-dependent tokens in two arrays.\n2. For reject-heavy cases, we store the accepted context-independent tokens and context-dependent tokens in two arrays.\n3. For rare cases where the accepted and rejected tokens are roughly equal, we store the accepted and rejected context-independent tokens and compress them into a bitset matching the vocabulary size.\nThus, in both accept-heavy and reject-heavy cases, the adaptive storage format only requires storing a small subset of tokens, significantly reducing memory usage. For Llama-3.1 model and JSON grammar, this adaptive storage method can effectively reduce the total memory usage to 0.2% (from 160 MB to 0.46 MB).\nAdditionally, when multiple parallel stacks exists, we need to merge the token masks. The merging algorithm of token masks is optimized based on storage type, as shown in Algorithm 1. For an accept-heavy mask (many accepted tokens, storing only rejected tokens), it intersects the rejected tokens with Partial Rej. For a reject-heavy mask (many rejected tokens, storing only accepted tokens), it combines accepted tokens with Partial Acc. In the final mask, the rejected tokens are the set difference `PartialRej\\Partial Acc`. This algorithm limits set operations to small token subsets, thus enhancing efficiency."}, {"title": "Context Expansion", "content": "Although the adaptive token mask cache effectively reduces the number of tokens checked at runtime, checking all context-dependent tokens remains an efficiency bottleneck at runtime. To further reduce the number of context-dependent tokens, XGrammar introduces context expansion, which leverages the grammar's context information to reject more context-dependent tokens during preprocessing, as shown in Figure 6.\nWhen validating a context-dependent token on a pushdown automaton, the matching process will reach the end of the current rule and return to parent rules to continue checking. This means a prefix of the token can be matched by the current rule, but whether the rest part can be matched remains to be determined by parent rules. However, we observed that the set of possible parent rules for each rule is limited, and the set of strings that can continue be matched after returning to parent rules is often constrained. Based on this observation, context expansion precomputes the possible suffix strings for each rule when returning to parent rules, called the expanded suffix. If a context-dependent token cannot match any string in the expanded suffix after finishing the current rule, it is marked as invalid. This filtering process effectively reduces the number of context-dependent tokens by eliminating those that would fail in higher-level rule contexts. Applied to the Llama-3.1 model and JSON grammar, this technique reduces context-dependent tokens by 90% (from 1,134 to 120), substantially improving the efficiency of generating token masks at runtime.\nAlgorithm 2 describes the context expansion process that finds the expanded suffix of each rule. For a rule R, we utilize a finite state automaton (FSA) $A_{ctx}$ (ctx is the abbreviation for context) to represent the expanded suffix, and that is extracted from the pushdown automata. We first find all edges $e = (s, t)$ in the pushdown automata that references R and belongs to rule R'. R' is not necessarily different from R. Then we find a subgraph of the automaton of rule R' starting from t to represent the possible strings that can follow R via depth-first search (DFS). However, we will not consider edges in the subgraph that reference other rules to avoid recursive references between rules, so the edges in the extracted subgraph will only have character labels. If a node has both character edges and edges referencing other rules, we will stop the search at this node. The extracted subgraph is then merged into $A_{ctx}$. This process is repeated for all rules, and the extracted $A_{ctx}$ is used to reject context-dependent tokens cannot match any string in it after finishing matching rule R.\nAlthough we do not consider rule-referencing edges when extracting the expanded context automata, this algorithm can still extract many useful context information. That is because the inlining optimization introduced in \u00a73.4 inlines fragment rules into their parent rules, reducing the need to check into child rules to reject context-dependent tokens."}, {"title": "Persistent Execution Stack", "content": "As the grammar engine still needs to handle context-dependent tokens, we need to efficiently execute the pushdown automata for these tokens. Additionally, we also need to execute the pushdown automata for preprocessing the context-independent token sets for all positions in the pushdown automata. In both cases, we need to maintain multiple parallel stacks and branch out as we match the characters in each token. To support efficient state branching, we introduce the persistent execution stack (Driscoll et al., 1989) to manage the multiple stacks and efficiently execute the pushdown automata. It can also manage the stacks from previous time points and enable the state rollback operation, effectively speeding up the execution of the pushdown automata on a set of tokens.\nAs shown in Figure 7, the persistent execution stack manages a set of stacks, which are either the parallel stacks from the current time point or the stacks from previous time points, into a single tree, and every stack is represented by a path from the root node on the tree. The stack top node is stored as a pointer to the node in the tree. Since the stacks from adjacent time points often share most of the deeper elements and only a few nodes are pushed or popped, this merging avoids memory redundancy for storing multiple stacks. When matching a new character from a token, we may need to split the stack into multiple stacks due to the ambiguity of the grammar, each corresponding to a different expansion of grammar rules. In this case, we only need to split the branch for that stack instead of copying the whole stack, which reduces the overhead of state branching.\nAdditionally, the persistent execution stack enables fast state rollback by maintaining the stack from previous time points. At runtime, a sliding window of history is maintained. To roll back to a previous state, we only need to change the current stack pointers, which requires constant time. This rollback operation is particularly useful for checking a large set of tokens, as many tokens share a common prefix with other tokens, such as read, ready, and reader all sharing the prefix read. All the checked tokens are sorted in lexicographical order to find the maximum length of the common prefixes. Then the tokens are checked one by one, and before checking each token, the state rolls back to just after the common prefix with the previous token. Therefore, we can avoid the redundant checks of these common prefixes, reducing the number of characters that need to be checked. For Llama-3.1 model and JSON grammar, this approach reduces the number of characters that need to be checked across the entire vocabulary to 30%, significantly speeding up the preprocessing stage.\nThe rollback operation enables more applications with efficient structured generation. There are many LLM applications that involve rolling back the output to a previous token. For instance, the jump-forward decoding requires retokenization, which involves rolling back some tokens in the context and then inserting new tokens. To ensure structured generation can continue after rolling back tokens, we can roll back the automaton state simultaneously with the output token rollback. There are also many LLM applications that requires LLMs generate in a tree structure, such as in Tree-of-thought (Yao et al., 2024), SGLang (Zheng et al., 2024), and the speculative model in the speculative decoding algorithm SpecInfer (Miao et al., 2024). We can maintain the automata state for every branch of the output tree, and when the output branches, we can quickly split the automaton state, maintaining separate matching states for each output branch. This branching is fast because we only need to maintain the stack top pointer on the tree for every branch. Therefore, the persistent execution stack enables us to ensure efficient structured generation for all these applications."}, {"title": "Pushdown Automata Structure Optimizations", "content": "We will perform additional optimizations to improve the structure of pushdown automata to speed up the efficiency of final execution. These optimizations draw from traditional compiler optimization concepts, but we find them particularly useful for efficient constrained decoding.\nRule inlining. There could be many fragment rules, i.e. rules with only a few elements, in the specified context-free grammar, which are then converted into small FSA in the pushdown automaton. On the one hand, this increases the ambiguity of the grammar since we need to inspect into these fragment rules and check during the execution of the pushdown automata. On the other hands, during context expansion, references to fragment rules are not considered, so the extracted context automata will be smaller. We will miss the opportunity to reject context-dependent tokens based on the structure of these fragment rules.\nTo address this issue, we introduce an automatic inlining strategy (Scheifler, 1977) for fragment rules. We iteratively pick rules that do not reference other rules and inline them into the parent rules. To avoid the explosion of the automaton size, we limit the size of the inlined rule and the size of inlined result to constants. This inlining process almost eliminated fragment rules, thereby improving the efficiency of token checking and enhancing the effectiveness of the context expansion.\nPushdown automata node merging. For pushdown automata, in many cases, the ambiguity comes from multiple outward edges of a node with the same label. When matching tokens, if we arrive at this node, and the next character just matches the label, the matching stack will be split into multiple stacks, one for each outward edge. The increase in the number of stacks increases the computation as we need to check the context-dependent tokens for each stack and merge the token masks. To reduce this kind of ambiguity, the node merging algorithm merges the subsequent nodes that satisfy: a) they are pointed to by edges with the same label originating from the same point b) they are not pointed to by other edges.\nAdditionally, the epsilon edge also increases the ambiguity of the matching process. An epsilon edge $s\\Rightarrow t$ in the automata means that the matching process can directly move from s to t without consuming any characters. If the matching process arrives at s, the execution stack will split into two stacks, one with s at the top and the other with t, both of which can continue matching. To reduce this kind of ambiguity, the node merging algorithm also merges the nodes s and t into a single node, as long as s has no other outward edge or t has no zero inward edge.\nThese two optimizations preserves the equivalence of the automaton, but reduces the number of nodes and edges. At runtime, the number of stacks and the computation required for token checking are reduced, speeding up the mask generation process."}, {"title": "Overlapping Mask Generation and LLM Inference", "content": "With the optimizations mentioned above, the token mask generation process is significantly accelerated, but it still requires CPU computation. To further eliminate the overhead of constrained decoding, we overlap the computation for mask generation with the LLM inference process, as shown in Figure 8. We observed that the mask generation process and LLM inference process can be overlapped. That is because the mask generation only requires CPU, and only depends on the previously generated tokens. The LLM inference process except the sampling stage only requires GPU, and also only depends on the previously generated tokens. Therefore, we can parallelize the mask generation process on the CPU with the LLM inference process on the GPU. We will synchronize before sampling, and the GPU will obtain the mask from the CPU and perform masked sampling to generate the new token. Additionally, the preprocessing stage can also be overlapped with the LLM prefilling stage, where the LLM processes the prompt. This orchestration between CPU and GPU ensures that the token restrictions are applied seamlessly, with almost zero overhead for LLM inference. In practice, the time for mask generation is less than the time for LLM inference, so the mask generation process will not become the bottleneck of the generation process."}, {"title": "EVALUATION", "content": "We implement XGrammar in 12,000 lines of core C++ code, and we provide Python bindings to facilitate seamless integration with LLM inference frameworks. In this section, we evaluate XGrammar to answer the following questions:\n\u2022 Can XGrammar efficiently support each step of constrained decoding? (\u00a74.1)\n\u2022 Does XGrammar achieve minimal overhead for end-to-end structured generation in LLM serving? (\u00a74.2)\n\u2022 Can XGrammar be deployed across a broader range of platforms? (\u00a74.3)"}, {"title": "Grammar Engine Efficiency", "content": "This subsection evaluates the grammar engine performance. We evaluate our method and baselines on Llama-3.1-8B-Instruct, a popular model with the ability to follow human instructions. We first evaluate the performance of JSON grammar. We apply the standard context-free grammar of JSON adopted from ECMA-404 (Ecma International, 2013) as a context-free grammar without additional constraints. We also evaluate the JSON schema, where we leverage the additional schema constraints from the dataset. We utilize the JSON-mode-eval dataset (NousResearch, 2024) for the prompts. We run the evaluation AMD Ryzen 9 7950X CPU and NVIDIA RTX 4090 GPU. For baseline comparisons, we compare three two popular implementations of structured generation engine, Outlines (Willard & Louf, 2023)(v1.0) and the builtin grammar engine of llama.cpp (Gerganov, 2023) (b3998).\nThe results are shown in Figure 9. XGrammar can achieve up to 3x speedup in the setting of JSON schema, and more than 100x speedup in the case of JSON grammar. The context-free grammar of JSON contains more complicated rules compared to the JSON schema (which is more constrained), as it can contain recursive lists and dictionaries, making it harder for grammar engines to execute it efficiently. In both cases, XGrammar can generate each token mask at in less than 40us, making it ideal for low-latency LLM inference."}, {"title": "End-to-End LLM Engine Evaluation", "content": "This section evaluates XGrammar under LLM serving setting. We integrate XGrammar into an end-to-end LLM inference framework and compare its efficiency with other LLM serving frameworks. We measure the average time to the first token (TTFT), which is primarily affected by preprocessing the constraint, and the average time per output token (TPOT), which is primarily affected by applying the constraint to each output token. We compare the efficiency with other LLM engines that support structured generation, including vLLM (Kwon et al., 2023b)(v0.6.3) integrated with Outlines, and llama.cpp with its builtin grammar engine. We conduct the evaluations on Llama-3.1-8B-Instruct under JSON grammar and JSON schema. We turn on the grammar cache for all engines to enable caching of the preprocessed grammars. The hardware used for the tests is AMD EPYC 7R13 CPU and NVIDIA H100 GPU. We evaluate multiple batch sizes settings in LLM inference tasks.\nThe experiment results are shown in Figure 10. XGrammar achieves the best TTFT and TPOT among all baselines for both CFG and JSON Schema. The computation of vLLM and llama.cpp is hindered by their grammar engines' longer preprocessing and per-token processing times. The decrease in TPOT speed in vLLM becomes particularly noticeable with larger batch sizes. This is because a larger batch size leads to higher throughput, putting greater pressure on grammar processing. Overall XGrammar engine can bring up to 80x speed output token rate compared to existing solutions. This speedup comes from the performance optimizations bought by XGrammar. We also studied the overhead of grammar processing in Table 1. The grammar process incurs nearly zero overhead in the TPOT, thanks to the token mask generation efficiency and grammar GPU overlap."}, {"title": "Cross-platform Deployment", "content": "This section explores bringing XGrammar to a wide variety of platforms. We leverage Emscripten (Zakai, 2011) to compile XGrammar into WebAssembly (Haas et al., 2017) and build a JavaScript binding. This approach enables XGrammar to run in client-side browsers on portable devices like laptops and mobile phones. We further integrate the web-binding with the in-browser LLM inference framework WebLLM (MLC team, 2023b) to enable structured generation. We evaluate the end-to-end performance with the JSON-mode-eval dataset, using 4-bit quantized models Llama-3.1-8B-Instruct (Dubey et al., 2024b) on a Macbook Pro M3 Max (macOS 14.5) with Google Chrome, and Qwen-2.5-0.5B-Instruct (Yang et al., 2024) on an iPhone 14 Pro Max (iOS 18) with Safari.\nThe results are shown in Figure 11. We compare the time to first token (TTFT) and time per output token (TPOT) between structured generation with XGrammar and non-structured generation while ensuring the number of generated tokens is the same. The results show that XGrammar brings close to zero overhead in both settings, enabling a great potential to support future on-device agents with high performance."}, {"title": "CONCLUSION", "content": "We proposed XGrammar, a flexible and efficient structured generation engine for LLMs. XGrammar separates the vocabulary into context-independent tokens and context-dependent ones. It prechecks the context-dependent tokens and stores the result in an adaptive token mask cache. We further introduce a persistent stack to speed up the execution of context-dependent checks. Finally, we co-design the grammar engine with LLM inference to overlap grammar execution with GPU computation. Our system greatly speeds up the token mask generation process in token mask and enables zero overhead structure generation in end-to-end LLM inference flows. We hope our system can enable a broader range of structure generation across platforms."}]}