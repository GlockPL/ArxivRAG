{"title": "Beyond Preferences in AI Alignment", "authors": ["Tan Zhi-Xuan", "Micah Carroll", "Matija Franklin", "Hal Ashton"], "abstract": "The dominant practice of AI alignment assumes (1) that preferences are an adequate\nrepresentation of human values, (2) that human rationality can be understood in\nterms of maximizing the satisfaction of preferences, and (3) that AI systems should\nbe aligned with the preferences of one or more humans to ensure that they behave\nsafely and in accordance with our values. Whether implicitly followed or explicitly\nendorsed, these commitments constitute what we term a preferentist approach to AI\nalignment. In this paper, we characterize and challenge the preferentist approach,\ndescribing conceptual and technical alternatives that are ripe for further research.\nWe first survey the limits of rational choice theory as a descriptive model, explaining\nhow preferences fail to capture the thick semantic content of human values, and\nhow utility representations neglect the possible incommensurability of those values.\nWe then critique the normativity of expected utility theory (EUT) for humans and\nAI, drawing upon arguments showing how rational agents need not comply with\nEUT, while highlighting how EUT is silent on which preferences are normatively\nacceptable. Finally, we argue that these limitations motivate a reframing of the\ntargets of AI alignment: Instead of alignment with the preferences of a human user,\ndeveloper, or humanity-writ-large, AI systems should be aligned with normative\nstandards appropriate to their social roles, such as the role of a general-purpose\nassistant. Furthermore, these standards should be negotiated and agreed upon by all\nrelevant stakeholders. On this alternative conception of alignment, a multiplicity of\nAl systems will be able to serve diverse ends, aligned with normative standards that\npromote mutual benefit and limit harm despite our plural and divergent values.", "sections": [{"title": "1 Introduction", "content": "Recent progress in the capabilities of AI systems, as well as their increasing adoption in society, has\nled a growing number of researchers to worry about the impact of AI systems that are misaligned\nwith human values. The roots of this concern vary, with some focused on the existential risks that\nmay come with increasingly powerful autonomous systems (Carlsmith, 2022), while others take a\nbroader view of the dangers and opportunities presented by potentially transformative AI technologies\n(Prunkl and Whittlestone, 2020; Lazar and Nelson, 2023). To address these challenges, AI alignment\nhas emerged as a field, focused on the technical project of ensuring an AI system acts reliably in\naccordance with the values of one or more humans.\nYet terms like \u201chuman values\" are notoriously imprecise, and it is unclear how to operationalize\n\"values\" in a sufficiently precise way that a machine could be aligned with them. One prominent\napproach is to define \"values\" in terms of human preferences, drawing upon the traditions of rational\nchoice theory (Mishra, 2014), statistical decision theory (Berger, 2013), and their subsequent influence\nupon automated decision-making and reinforcement learning in AI (Sutton and Barto, 2018). Whether\nexplicitly adopted, or implicitly assumed in the guise of \u201creward\u201d or \u201cutility\u201d, this preference-based\napproach dominates both the theory and practice of AI alignment. However, as proponents of\nthis approach note themselves, aligning AI with human preferences faces numerous technical and\nphilosophical challenges, including the problems of social choice, anti-social preferences, preference\nchange, and the difficulty of inferring preferences from human behavior (Russell, 2019)."}, {"title": "1.1 Overview", "content": "The rest of this paper is organized as follows: In Section 2, we examine rational choice theory as a\ndescriptive account of human decision-making. Drawing upon the tradition of revealed preferences\nin economics, rational choice theory is often taken for granted by AI researchers seeking to learn\nhuman preferences from behavior. In doing so, they assume that human behavior can be modeled as\nthe (approximate) maximization of expected utility, that human preferences can be represented as\nutility or reward functions, and that preferences are an adequate representation of human values. We\nchallenge each of these assumptions, offering alternatives that better account for resource-limited\nhuman cognition, incommensurable values, and the constructed nature of our preferences.\nDeveloping upon these ideas, in Section 3 we turn to expected utility theory (EUT) as a normative\nstandard of rationality. Even while recognizing that humans often do not comply with this standard,\nalignment researchers have traditionally assumed that sufficiently advanced AI systems will do so,\nand hence that solutions to AI alignment must be compatible with EUT. In parallel with recent\ncritiques of this view (Thornley, 2023, 2024; Bales, 2023; Petersen, 2023), we argue that EUT is\nboth unnecessary and insufficient for rational agency, and hence limited as both a design strategy and\nanalytical lens. Instead of adhering to utility theory, we can design tool-like AI systems with locally\ncoherent preferences that are not representable as a utility function. We can also go beyond EUT,\nbuilding systems that reason about preferences in accordance with deeper normative principles."}, {"title": "2 Beyond rational choice theory when modeling humans", "content": "The central tenet of rational choice theory is the assumption that humans act so as to maximize\nthe satisfaction of their preferences, and that both individual and aggregate human behavior can be\nunderstood in these terms. As far as theoretical presuppositions go, this assumption has been wildly\nsuccessful, forming the bedrock of modern economics as a discipline, and influencing a great variety\nof fields concerned with analyzing human behavior, including sociology (Boudon, 2003), law (Ulen,\n1999), and cognitive science (Chater and Oaksford, 1999; Jara-Ettinger et al., 2020).\nRevealed preferences and their representation as utility functions. In its most standard form,\nrational choice theory assumes that human preferences can be represented as a scalar-valued utility\nfunction defined over outcomes that is, in terms of a quantity that can be maximized and\nthat human choice can be modeled as selecting actions so as to maximize the expected value of\nthis function. The promise this offers is that we can directly derive what a person prefers from\nwhat they choose, and furthermore represent how much they prefer it as a scalar value. Such\npreferences are called revealed preferences, because they are supposedly revealed through what\na person chooses. This methodology is bolstered by numerous representation theorems (Savage,\n1972; Bolker, 1967; Jeffrey, 1991) showing that any preference ordering over outcomes that obeys\ncertain \u201crationality axioms\u201d can be represented in terms of a utility function, such as the famous von\nNeumann-Morgenstern (VNM) utility theorem (von Neumann and Morgenstern, 1944).\nRational choice theory in machine learning. In keeping with rational choice theory, many machine\nlearning and AI systems also assume that human preferences can be derived from human choices in a\nmore or less direct manner, and furthermore represent those preferences in terms of scalar utilities or\nrewards. This is most pronounced in the fields of inverse reinforcement learning (Ng and Russell,\n2000; Abbeel and Ng, 2004; Hadfield-Menell et al., 2016) and reinforcement learning from human\nfeedback (Christiano et al., 2017; Zhu et al., 2023), which explicitly assume that the behavior of\na human can be described as (approximately) maximizing a sum of scalar rewards over time, and\nthen tries to infer a reward function that explains the observed behavior. Similar assumptions can\nbe found in the field of recommender systems (Thorburn et al., 2022), with many papers modeling\nrecommendation as the problem of showing items to users that they are most likely to engage with,\nwhich is presumed to be the item they find the most rewarding (Li et al., 2010; Hill et al., 2017;\nMcInerney et al., 2018).\nBoltzmann models of noisily-rational choice. While these preference-based models of human\nbehavior are rooted in rational choice theory, it is worth noting that they are slightly more complex than\n\"maximize expected utility\" might imply. In particular, they allow for the fact that humans may not\nalways maximize utility, and hence are models of noisy or approximately rational choice. In machine\nlearning and AI alignment, the most common of such choice models is called Boltzmann rationality\n(after the Boltzmann distribution in statistical mechanics), which assumes that the probability of a\nchoice c is proportional to the exponential of the expected utility of taking that choice:\n$P(c) \\propto exp(\\beta E[U(c)])$"}, {"title": "2.1 Beyond noisily-rational models of human decisions", "content": "The issue with both perfect and noisily-rational models of human decision-making is that they do\nnot account for the systematic deviations from optimality that humans in fact exhibit. As a long\nline of psychological and behavioral research has shown, humans are boundedly rational at best,\nexhibiting satisficing instead of optimizing behavior, (Simon, 1957, 1979). These deviations from\noptimality include framing effects, loss aversion, anchoring biases, and mis-estimation of high and\nlow probabilities \u2013 phenomena which are better modeled by prospect theory (Kahneman and Tversky,\n1979; Tversky and Kahneman, 1992) than standard rational choice theory. More generally, many\nof the decision problems that people encounter are computationally intractable to solve optimally,\nmaking rational choice a implausible model of human behavior (van Rooij, 2008; Bossaerts et al.,\n2019; Camara, 2022). Instead, research suggests that humans make use of a variety of heuristics in\norder to approximately solve the problems they encounter (Gigerenzer, 2008).\nChallenges to modeling bounded rationality. How might AI systems that infer human preferences\nand values account for these findings? One approach might be to incorporate a sufficiently long list\nof known heuristics and biases into our models of human decision-making, thereby ensuring that\npreferences can be robustly inferred even in the presence of such biases (Evans et al., 2016; Chan\net al., 2021). However, this approach is highly contingent upon on our current state of knowledge\nabout human rationality what if we miss out important biases in our models, leading to inaccurate\npredictions and inferences? (Christiano, 2015b; Steinhardt, 2017) As a potential remedy, Shah\net al. (2019) suggest learning human biases alongside their preferences. But a conceptual difficulty\nremains: Without any inductive constraints on the types of errors humans are susceptible to, how\ncan we ensure that human biases are accurately learned? As Armstrong and Mindermann (2018)\nshow, even inductive preferences for more parsimonious models of human decision-making cannot\ndistinguish important classes of observationally-equivalent hypotheses from intuitively plausible\nones, such as the possibility that humans are acting anti-rationally by minimizing the satisfaction of\ntheir preferences."}, {"title": "2.2 Beyond reward and utility functions as representations of human preferences", "content": "While resource rationality provides a more flexible framework for modeling the relationship between\npreferences and behavior, this says little about how preferences themselves should be represented.\nFor the most part, resource rational analyses continue to represent human preferences in terms of\nscalar costs and rewards, or more generally, utility functions, with the primary innovation being the\ninclusion of costs on computation (Lieder and Griffiths, 2020; Callaway et al., 2022). Yet, there are\nmany reasons to think that reward functions and utility functions are inadequate representations of\nhuman preferences, while also tending to produce conceptual confusion about what they do represent.\nThe limited expressivity of reward functions. These issues are most easily appreciated in the case\nof (scalar, Markovian) reward functions. As noted earlier, the reward representation assumes that the\nutility of a sequence of states and actions $ \\xi = (s_1, a_1, \\dots, s_{T-1}, s_T) $ can be decomposed into a sum\nof scalar rewards over time:\n$U(\\xi) = \\sum_{t=1}^T R(s_t, a_t)$\nAdvocates of the reward representation argue that any task accomplishable by an intelligent agent can\nbe framed as a reward maximization problem (Silver et al., 2021). As Kasenberg et al. (2018) point\nout, however, this minimally requires that all historically relevant information is already included in\nthe representation of each state $ s_t $ a requirement since stated more formally by Abel et al. (2021)\nand Bowling et al. (2023). This means that without careful feature engineering, reward functions\ncannot easily express time-extended preferences like the desire to keep a promise, or the value of\nnarrative coherence. Separately, the scalar nature of the (standard) reward representation means that it\ncannot represent the existence of incomplete preferences due to multiple incommensurable scales of\nvalue (Vamplew et al., 2022; Anderson, 1995; Chang, 1997): Sometimes, the choices before us may"}, {"title": "2.3 Beyond preferences as representations of human values and reasons", "content": "Preferences are constructed, not basic. Thus far, we have proceeded as if human motivations\nand values are adequately captured by the concept of \u201cpreference\u201d as it is used in rational choice\ntheory. But as far as evaluative concepts go, this concept of \u201cpreference\u201d is an extremely thin one:\nMathematically, a \u201cpreference\u201d is just some ordering of two options, which can be interpreted as\neither a disposition to choose one option over another, subjective liking of one option over the other\nFranklin et al. (2022), or an all-things-considered judgment in favor of one of the options. Distinct as\nthese interpretations are, what they share is their highly abstract and general nature - \"preference\"\nis a thin concept because it does not encode richer semantic information beyond the bare notion of\n\u201cbetterness\u201d. Insofar as utility functions are interpreted as representations of preferences, this thinness\nis inherited by them: Utility just represents the mere preferability of some option. But why exactly\nare some options preferred over others? In virtue of what reasons do people make these preference\njudgments? Without answering these questions, we are unlikely to model how someone's preferences\ngeneralize to novel options in ways they would endorse. To do so, we must go beyond preferences\nas the fundamental unit of analysis, and understand how preferences are computed and constructed\nfrom our reasons and values (Warren et al., 2011; Lichtenstein and Slovic, 2006).\nRational choice as action on the basis of reasons. In making this point, we depart from the\ndomain of rational choice theory, and return to a more basic understanding of what it means to model\nourselves as rational agents: We are agents that take ourselves to act on the basis of reasons (Raz,\n1999; Logins, 2022). These reasons might include desires, such as an intrinsic desire to avoid pain\n(Sinhababu, 2017), evaluative judgments, such as the judgment that a movie is artistic enough to\nbe worth watching (Anderson, 1995), or even acts of will, such as the intention to pursue a specific\ncareer (Chang, 2009).\nEvaluative concepts as building blocks for reasons. What exactly is the content of these reasons?\nIn decision theory and Humean accounts of motivation (Sinhababu, 2017), only beliefs (represented\nas subjective probabilities) and desires (represented as the utility of some desired outcome) are\nconsidered as reasons for action. But even if we set aside other accounts (Anderson, 1995; Chang,\n2004; Parfit, 2018), this leaves open what a person's beliefs and desires are about. If I desire to\nbe both helpful and honest to others, what does it mean to be helpful or honest? Acting upon this\ndesire requires applying the concepts of helpfulness and honesty, which are not just any concepts, but\nevaluative concepts, or values. Importantly, most such concepts are not thin ones, like preference,\nutility or goodness; they are thick evaluative concepts - concepts that comprise both descriptive and\nnormative elements - such as beauty, humor, or health. As Blili-Hamelin and Hancox-Li (2023)\npoint out, even the concept of intelligence so central to AI is thick in this way.\nUtility functions as aggregators of distinct evaluative judgments. How should AI systems model\nsuch evaluative concepts, and their relationship to preferences and action? As a first pass, one\nmight turn the utility representation theorems on their head, viewing reward and utility functions\nas generators of human preferences, instead of mere representations of them. Indeed, as gestured\nat earlier, reward and utility functions are often interpreted in this way, with rewards, costs, and"}, {"title": "3 Beyond expected utility theory as a normative standard of rationality", "content": "In the previous section, we described how research in AI alignment often assumes approximate utility\nmaximization as a descriptive model of human behavior, then highlighted the shortcomings of this\napproach. However, this leaves open whether utility maximization is a desirable normative standard\nfor both human and machine behavior - that is, whether agents ought to maximize the satisfaction\nof their preferences as a condition of ideal rationality, regardless of whether they actually do so.\nCoherence arguments for EUT. There is a long history of debate regarding the validity of this\nnormative standard. Arguments in favor of expected utility theory (EUT) include the utility represen-\ntation theorems mentioned earlier (Samuelson, 1938; Savage, 1972; Bolker, 1967; Jeffrey, 1991; von\nNeumann and Morgenstern, 1944), which start from an axiomatization of what preferences count as\nrational, then demonstrate that any agent that acts in accordance with such preferences must act as if\nthey are an expected utility maximizer. In the AI alignment literature, these results are often treated\nas \u201ccoherence theorems\u201d about the nature of rational agency, either by taking the rationality axioms\nfor granted, or by providing arguments in defense of the axioms (Omohundro, 2007; Yudkowsky,"}, {"title": "3.1 Beyond expected utility theory as an analytical lens", "content": "Coherence is not rationally required. However, coherence arguments for expected utility theory\nare not as strong as the AI alignment literature has often presumed. The most extensive version\nof these arguments is given by Gustafsson (2022), who provides a money pump argument for\npreference completeness, and then uses completeness to derive arguments for transitivity, continuity,\nand independence. Yet, as Thornley (2023) points out, the argument for completeness depends on\nparticular assumptions about how agents are permitted to choose when offered a series of potentially\nexploitative trades, which can be avoided as long as agents do not accept offers that are less preferred\nthan options they previously turned down. Petersen (2023) formalizes this counter-argument further,\nproposing a dynamic choice rule that ensures agents with incomplete preferences are invulnerable\nto money pumps. Indeed, it is accepted by many decision theorists that preference completeness\nis not a requirement of rationality; instead, all that is required is for an agent's preferences to be\ncoherently extendible (Steele and Stef\u00e1nsson, 2020). In turn, this implies that rational agents need not\nbe representable as EU maximizers.\nCoherent EU maximization is intractable. But let us imagine that coherence arguments do go\nthrough after all. Even if this were the case, it is far from obvious that advanced intelligences would\ncomply with the axioms of utility theory (or be incentivized to do so) in the face of computational and\npractical limitations. As Bales (2023) argues, behaving as an expected utility maximizer can come\nwith considerable costs, while only providing limited benefits. In fact, as we noted in Section 2, most\nutility functions are computationally intractable to coherently maximize: Camara (2022) shows that\nwhile certain simple classes of utility functions allow for rational choice behavior to be computed\nin polynomial time, for a large class of other utility functions, agents cannot tractably compute\nchoice behavior that complies with the rationality axioms, and must instead resort to approximately\nmaximizing their utility function. Alternatively, agents may insist on complying with the rationality\naxioms, but give up on even approximate optimality with respect to their original utility functions. In\nother words, it is not always resource rational to maximize expected utility.\nCoherence alone is not informative. Suppose we could set aside these tractability worries as well.\nEven so, it is unclear what information EUT provides us. As discussed by Shah (2018), Ngo (2019),\nand Bales (2023), many kinds of behavior can trivially be described in terms of utility maximization,\nincluding an \"agent\" that does nothing at all. This means that EUT alone does not say much about\nthe kinds of goals that advanced AI systems are likely to pursue, or what they are likely to do in"}, {"title": "3.2 Beyond globally coherent agents as design targets", "content": "If agents are neither rationally required nor practically required to act as if they are expected utility\nmaximizers, this opens up the design space of (advanced) AI systems that we might hope to build\nand align. In particular, we have the option of building AI systems that do not comply with one or\nmore of the axioms of expected utility theory systems that are not globally coherent in the way\nthat expected utility maximizers are required to be.\nNon-globally coherent AI may be more faithfully and safely aligned. Why might this be desirable?\nThere are two broad reasons. One reason is faithfulness. As we discussed in Section 2, many human\npreferences may be incomplete due to incommensurable values, and we might want AI systems to\nfaithfully represent that preferential structure when making decisions (Eckersley, 2018). Otherwise,\nsuch systems might reliably take actions that promote certain outcomes over others, even though\nwe have yet to form a preference over which of those outcomes is better. Another reason is safety\n- for a wide range of (time unbounded) utility functions, expected utility maximizers have been\nshown to seek power over their environment (Turner et al., 2021), and avoid being shut down by their\ncreators (Soares et al., 2015), suggesting that sufficiently capable utility maximizers will create\nconsiderable risks if their utility functions are not compatible with human safety (Carlsmith, 2022).\nAI tools as locally coherent agents. A general class of AI systems that seem to largely satisfy\nfaithfulness and safety are what we might intuitively think of as tools. We use tools to perform tasks\nthat are context-specific - the goals we use them for vary by context as well as local we do\nnot expect or want them to reliably affect the world beyond the contexts of their use. Insofar as these\ntools can be thought of as agents, they are at best locally coherent ones. In this sense, they mimic\nthe role-specific nature of human preferences. Just as people have differing goals and obligations\ndepending on whether they are in the role of a parent or a worker (Anderson, 1995), tools take on\nthe aims and constraints of their users, whether those involve classifying images or generating code.\nWithin each context, we are typically willing to commensurate our values such that our preferences\ncan be represented as a local utility function, even if we are unwilling to do so in general.\nTool-like locality through local scope. How can we build AI systems that function as tools? The\nanswer, of course, is that we already have: Most AI systems that exist today are best thought of as\ntools. This is not due to any special care on our part as designers, but only because functioning as a\ntool is the default nature of rule-bound, computationally limited algorithms with no representation\nof their own existence in the world. Such algorithms execute a bounded amount of computation in\nresponse to some input, terminating when they find an answer or if time runs out. They exhibit no\npreference for altering the conditions of their termination, or for gaining control over more of their\nenvironment, because they cannot even represent the environment they exist in. In other words, such\nsystems are local in scope. This is the case even for systems that we might be tempted to call agents\ndue to their long horizon reasoning abilities (e.g. classical planners, theorem provers) or relative\nautonomy (e.g. self-driving cars, robot vacuums). To the extent that such systems can be represented\nas utility maximizers, they can often be viewed as having local, time-bounded utility functions, which\nprovide no incentive for continued operation beyond a certain time or resource bound (Dalrymple,\n2022). Very plausibly, we could even build highly advanced, economically transformative AI systems\nby composing these bounded tools (Drexler, 2022; Dalrymple, 2024).\nMaintaining locality despite global scope. Suppose, however, that some actors want to build\nadvanced AI systems that are not bounded in these ways. For example, many AI companies are\nkeen to develop general purpose AI assistants, which follow human instructions in a wide range\nof domains and contexts, remain operational across contexts, and possess enough understanding\nof the wider world that they can represent both themselves and their users as entities in that world\nmodel. LLMs are increasingly being used in this way, and while their reasoning capabilities remain\nunreliable and limited (Valmeekam et al., 2023b; Dziri et al., 2023; Momennejad et al., 2024), one\nmight imagine augmenting or embedding them within systems with more coherent representations\nand reasoning abilities (Parisi et al., 2022; Sumers et al., 2023). Can we ensure that such systems\ncontinue to function as tools, despite their increasingly global scope?\nContextual reward functions are insufficient for locality. We suggest that the answer may depend\non whether such systems remain local in terms of the completeness of their preferences, despite having\nglobal scope. What does it mean for preferences to be only locally complete? Consider one tempting\nbut unsuccessful way to formalize this idea: We design our system to have a context-sensitive reward\nfunction $ R(s,c) $, where s is the current state, and c is the current context (e.g. an instruction or\nprompt given to a LLM-based assistant). The hope is that users will be able to set c to whatever they\nlike, and the system will change the task it optimizes for. Within the context c, the system exhibits\nlocally coherent behavior, since its preferences are given by the reward function $ R(\\cdot, c) $. However,\nsince our system has global scope, it also cares about rewards across contexts: its utility function\nfor a trajectory $ \\xi = ((s_1, c_1), \\dots, (s_T, c_T)) $ is $ U(\\xi) = \\sum_{t=1}^T R(s_t, c_t) $. This means that the system\nwill have a context manipulation incentive, i.e. an incentive to enter and remain within contexts that\ndeliver more reward. For example, it might persuade or manipulate the user to give it instructions that\nare easier to satisfy. The reason for this is that the system's preferences are still globally complete\nthey are represented by a global utility function, despite being context-sensitive."}, {"title": "3.3 Beyond preferences as the normative basis of action", "content": "EUT does not explain when our preferences are normatively acceptable. Up to this point, we\nhave primarily critiqued the normativity of expected utility theory on formal grounds, drawing upon\narguments from decision theory and computational complexity theory. But an arguably deeper\nproblem with EUT is that it fails to ground the normativity of our preferences. EUT is a theory\nof instrumental rationality not value rationality: It tells us how to choose our actions in order to\nsatisfy our preferences, and imposes constraints on what those preferences can be, but it does not say\nanything further about where those preferences can or should come from. Yet, as we have elaborated\nin Section 2, human preferences are not fundamental, but derivative they derive from our values\nand reasons. EUT is thus woefully incomplete. It might tell us how to derive instrumental preferences\nfrom intrinsic ones, but it provides no guidance on many questions of great normative importance,\nsuch as why and how to value human and animal lives, whether and when it is permissible to give up\nequality for efficiency in a democracy, or how to judge the desirability and relevance of EUT itself.\nNormative judgments are increasingly automated. Reasoning about these normative questions\nhas traditionally been the purview of humans alone. Indeed, there are many reasons to preserve that\nstate of affairs, lest we cede our moral and political autonomy entirely to machines (van Wynsberghe\nand Robbins, 2019). But even without replacing human autonomy over normative affairs, we are\nalready building AI systems that automate normative judgments, assist us with normative reasoning,\nor operate under normative uncertainty. For example, machine learning methods are routinely used to\nmoderate content that may be regarded as toxic and offensive (Gorwa et al., 2020), or to steer LLMs\ntowards producing outputs that are less harmful (Bai et al., 2022a). More ambitiously, AI writing\nassistants are being used to draft legal arguments by mimicking certain aspects of legal reasoning\n(Iu and Wong, 2023; Lohr, 2023). If these trends continue, then increasing amounts of work will\nhave to be done to ensure that AI systems produce normatively appropriate behavior. Humans will\neither have to do work upfront a difficult task, given the combinatorially large space of situations\nthat increasingly autonomous systems might encounter or we will have to imbue AI systems with\nsome semblance of normative reasoning.\nThe need for theories of normative reasoning. What options do we have for doing this? What\nwould it look like to reason about the preferences and values one ought to have? Given the complexity\nof these questions, one might hope to sidestep the need for a formal account like EUT entirely, and\ninstead train AI systems to imitate human normative reasoning. This is exemplified by the standard"}, {"title": "4 Beyond single-principal AI alignment as preference matching", "content": "If rational choice theory is an inadequate description of human behavior and values, and expected\nutility theory is an unsatisfactory account of rational decision-making, what does this imply for the\npractice of AI alignment? Though there is growing awareness of the limits of these preferentist\nassumptions (Casper et al., 2023; Lambert et al., 2023), most applied methods for AI alignment\ncontinue to treat alignment as the problem of preference matching: Given an AI system, the goal is to\nensure that its behavior conforms with the preferences of a human user or developer.\nReward learning as alignment via preference matching. At present, the most prominent of such\nmethods is reinforcement learning from human feedback (RLHF). Similar to other reward learning\nmethods such as inverse reinforcement learning (Ng and Russell, 2000), RLHF learns an estimate of a\nuser's presumed reward function a reward model \u2013 from a dataset of their stated preferences. The\nAI system is then trained to optimize the learned reward model, with the aim of producing behavior\nthat better conforms to the user's preferences. Since the development of RLHF for classical control\nproblems (Knox and Stone, 2011; Griffith et al., 2013; Akrour et al., 2014), the method has been\nextended to train increasingly complex AI systems in increasingly open-ended domains, including\ndeep neural networks for robotic control (Christiano et al., 2017) and large language models (Ouyang\net al., 2022; Bai et al., 2022a). This latter development has led to an explosion of interest in RLHF,\ngiven the unprecedented capabilities and general purpose nature of LLMs.\nFoundational limitations of reward learning. For all its success, RLHF faces numerous technical\nchallenges (Casper et al., 2023), ranging from issues with preference elicitation (Knox et al., 2023)\nand scalable oversight (Leike et al., 2018) to over-optimization (Gao et al., 2023; Moskovitz et al.,\n2024) and stable training (Hejna et al., 2024). Our focus, however, is more foundational, and applies to\nnot just RLHF but any alignment method derived from reward learning: By committing to a reward\nrepresentation of human preferences or values, reward learning suffers from all the representational\nlimits we discussed in Section 2. Furthermore, by treating reward as something to be optimized,\nreward-based methods adopt EUT as a normative standard, with all the issues that Section 3 describes.\nThe limited scope of reward learning and preference matching. In this section, we discuss what\nit would require for AI alignment research to take these challenges seriously. Importantly, we do\nnot claim that reward-based methods are never appropriate. Rather, we argue that reward-based\nalignment and preference matching more generally is only appropriate for AI systems with"}, {"title": "4.1 Beyond alignment with scalar and acontexual rewards", "content": "Two aspects of reward functions are important for determining their role in the practice of AI\nalignment. The first is whether they are scalar. As explained in Section 2", "contextual": "Is\nthe reward function understood to be a representation of context-specific preference judgments, or of\nan individual's overall preferences?\nScalar rewards are only appropriate in narrow decision contexts. Scalar rewards are generally\ninadequate, since (as elaborated in Section 2) they assume away the possibility of incomplete human\npreferences. But as long as these rewards are also understood to be contextual, then reward-based\nalignment can be appropriate. In relatively narrow decision contexts without sharp practical or moral\ndilemmas, it is not unreasonable to assume that people are willing to commensurate their values\n(Anderson, 1995). In these contexts (e.g. buying groceries, travel planning, solving math homework)\nit is often clear to us how to weight different values against others (e.g. quality vs. cost, time vs.\ncomfort, correctness vs. verbosity), leading to a complete preference ordering that it is representable\nby scalar reward. Learning a reward function is thus not inherently problematic. If this learned\nreward function is then optimized by a bounded AI system - the kind of local, tool-like system we\ndiscussed in Section 3 then the downsides are also limited. A poorly learned reward function may\nstill result in negative outcomes (Zhuang and Hadfield-Menell, 2020), but the system will not reliably\nbring about unintended non-local effects.\nModels of context-specific preferences will not generalize across contexts. By and large, this is\nthe setting within which methods like RLHF are applied. Reward models are learned from human\npreferences, but these preferences typically represent context-specific goodness-of-a-"}]}