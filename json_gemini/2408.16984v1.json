{"title": "Beyond Preferences in AI Alignment", "authors": ["Tan Zhi-Xuan", "Micah Carroll", "Matija Franklin", "Hal Ashton"], "abstract": "The dominant practice of AI alignment assumes (1) that preferences are an adequate\nrepresentation of human values, (2) that human rationality can be understood in\nterms of maximizing the satisfaction of preferences, and (3) that AI systems should\nbe aligned with the preferences of one or more humans to ensure that they behave\nsafely and in accordance with our values. Whether implicitly followed or explicitly\nendorsed, these commitments constitute what we term a preferentist approach to AI\nalignment. In this paper, we characterize and challenge the preferentist approach,\ndescribing conceptual and technical alternatives that are ripe for further research.\nWe first survey the limits of rational choice theory as a descriptive model, explaining\nhow preferences fail to capture the thick semantic content of human values, and\nhow utility representations neglect the possible incommensurability of those values.\nWe then critique the normativity of expected utility theory (EUT) for humans and\nAI, drawing upon arguments showing how rational agents need not comply with\nEUT, while highlighting how EUT is silent on which preferences are normatively\nacceptable. Finally, we argue that these limitations motivate a reframing of the\ntargets of AI alignment: Instead of alignment with the preferences of a human user,\ndeveloper, or humanity-writ-large, AI systems should be aligned with normative\nstandards appropriate to their social roles, such as the role of a general-purpose\nassistant. Furthermore, these standards should be negotiated and agreed upon by all\nrelevant stakeholders. On this alternative conception of alignment, a multiplicity of\nAI systems will be able to serve diverse ends, aligned with normative standards that\npromote mutual benefit and limit harm despite our plural and divergent values.", "sections": [{"title": "1 Introduction", "content": "Recent progress in the capabilities of AI systems, as well as their increasing adoption in society, has\nled a growing number of researchers to worry about the impact of AI systems that are misaligned\nwith human values. The roots of this concern vary, with some focused on the existential risks that\nmay come with increasingly powerful autonomous systems (Carlsmith, 2022), while others take a\nbroader view of the dangers and opportunities presented by potentially transformative AI technologies\n(Prunkl and Whittlestone, 2020; Lazar and Nelson, 2023). To address these challenges, AI alignment\nhas emerged as a field, focused on the technical project of ensuring an AI system acts reliably in\naccordance with the values of one or more humans.\nYet terms like \u201chuman values\" are notoriously imprecise, and it is unclear how to operationalize\n\"values\" in a sufficiently precise way that a machine could be aligned with them. One prominent\napproach is to define \"values\" in terms of human preferences, drawing upon the traditions of rational\nchoice theory (Mishra, 2014), statistical decision theory (Berger, 2013), and their subsequent influence\nupon automated decision-making and reinforcement learning in AI (Sutton and Barto, 2018). Whether\nexplicitly adopted, or implicitly assumed in the guise of \u201creward\u201d or \u201cutility\u201d, this preference-based\napproach dominates both the theory and practice of AI alignment. However, as proponents of\nthis approach note themselves, aligning AI with human preferences faces numerous technical and\nphilosophical challenges, including the problems of social choice, anti-social preferences, preference\nchange, and the difficulty of inferring preferences from human behavior (Russell, 2019)."}, {"title": "1.1 Overview", "content": "The rest of this paper is organized as follows: In Section 2, we examine rational choice theory as a\ndescriptive account of human decision-making. Drawing upon the tradition of revealed preferences\nin economics, rational choice theory is often taken for granted by AI researchers seeking to learn\nhuman preferences from behavior. In doing so, they assume that human behavior can be modeled as\nthe (approximate) maximization of expected utility, that human preferences can be represented as\nutility or reward functions, and that preferences are an adequate representation of human values. We\nchallenge each of these assumptions, offering alternatives that better account for resource-limited\nhuman cognition, incommensurable values, and the constructed nature of our preferences.\nDeveloping upon these ideas, in Section 3 we turn to expected utility theory (EUT) as a normative\nstandard of rationality. Even while recognizing that humans often do not comply with this standard,\nalignment researchers have traditionally assumed that sufficiently advanced AI systems will do so,\nand hence that solutions to AI alignment must be compatible with EUT. In parallel with recent\ncritiques of this view (Thornley, 2023, 2024; Bales, 2023; Petersen, 2023), we argue that EUT is\nboth unnecessary and insufficient for rational agency, and hence limited as both a design strategy and\nanalytical lens. Instead of adhering to utility theory, we can design tool-like AI systems with locally\ncoherent preferences that are not representable as a utility function. We can also go beyond EUT,\nbuilding systems that reason about preferences in accordance with deeper normative principles."}, {"title": "2 Beyond rational choice theory when modeling humans", "content": "The central tenet of rational choice theory is the assumption that humans act so as to maximize\nthe satisfaction of their preferences, and that both individual and aggregate human behavior can be\nunderstood in these terms. As far as theoretical presuppositions go, this assumption has been wildly\nsuccessful, forming the bedrock of modern economics as a discipline, and influencing a great variety\nof fields concerned with analyzing human behavior, including sociology (Boudon, 2003), law (Ulen,\n1999), and cognitive science (Chater and Oaksford, 1999; Jara-Ettinger et al., 2020).\nRevealed preferences and their representation as utility functions. In its most standard form,\nrational choice theory assumes that human preferences can be represented as a scalar-valued utility\nfunction defined over outcomes that is, in terms of a quantity that can be maximized and\nthat human choice can be modeled as selecting actions so as to maximize the expected value of\nthis function. The promise this offers is that we can directly derive what a person prefers from\nwhat they choose, and furthermore represent how much they prefer it as a scalar value. Such\npreferences are called revealed preferences, because they are supposedly revealed through what\na person chooses. This methodology is bolstered by numerous representation theorems (Savage,\n1972; Bolker, 1967; Jeffrey, 1991) showing that any preference ordering over outcomes that obeys\ncertain \u201crationality axioms\u201d can be represented in terms of a utility function, such as the famous von\nNeumann-Morgenstern (VNM) utility theorem (von Neumann and Morgenstern, 1944).\nRational choice theory in machine learning. In keeping with rational choice theory, many machine\nlearning and AI systems also assume that human preferences can be derived from human choices in a\nmore or less direct manner, and furthermore represent those preferences in terms of scalar utilities or\nrewards. This is most pronounced in the fields of inverse reinforcement learning (Ng and Russell,\n2000; Abbeel and Ng, 2004; Hadfield-Menell et al., 2016) and reinforcement learning from human\nfeedback (Christiano et al., 2017; Zhu et al., 2023), which explicitly assume that the behavior of\na human can be described as (approximately) maximizing a sum of scalar rewards over time, and\nthen tries to infer a reward function that explains the observed behavior. Similar assumptions can\nbe found in the field of recommender systems (Thorburn et al., 2022), with many papers modeling\nrecommendation as the problem of showing items to users that they are most likely to engage with,\nwhich is presumed to be the item they find the most rewarding (Li et al., 2010; Hill et al., 2017;\nMcInerney et al., 2018).\nBoltzmann models of noisily-rational choice. While these preference-based models of human\nbehavior are rooted in rational choice theory, it is worth noting that they are slightly more complex than\n\"maximize expected utility\" might imply. In particular, they allow for the fact that humans may not\nalways maximize utility, and hence are models of noisy or approximately rational choice. In machine\nlearning and AI alignment, the most common of such choice models is called Boltzmann rationality\n(after the Boltzmann distribution in statistical mechanics), which assumes that the probability of a\nchoice c is proportional to the exponential of the expected utility of taking that choice:\n\\(P(c) \\propto exp (\\beta E[U(c)])\\)"}, {"title": "Justifications and extensions of Boltzmann rationality", "content": "This choice model exhibits a number of\npractically useful and theoretically appealing properties. For example, by varying the \u201crationality\nparameter\" \u03b2 between zero and infinity, Boltzmann rationality interpolates between completely\nrandom choice and deterministic optimal choice (Ghosal et al., 2023). As an instantiation of Luce's\nchoice axiom (Luce, 1979), it obeys independence of irrelevant alternatives. Boltzmann rationality\nhas also been justified as the maximum entropy distribution that matches certain constraints implied\nby observed behavior (Ziebart et al., 2008, 2010), or as a thermodynamically-inspired model of\nbounded rationality where agents have to spend energy investigating which choice leads to the highest\nutility (Ortega and Braun, 2013; Jarrett et al., 2021). In addition, Boltzmann rationality has been\nextended to model other aspects of human behavior besides goal-directed actions, including direct\ncomparisons between options (i.e. stated preferences) (Akrour et al., 2014; Christiano et al., 2017;\nZhu et al., 2023), explicitly stated reward functions (Hadfield-Menell et al., 2017b), entire behavior\npolicies (Laidlaw and Dragan, 2022), and linguistic utterances (Lin et al., 2022), allowing preferences\nto be inferred from multiple forms of human feedback (Jeon et al., 2020).\nLimitations of Boltzmann rationality. As useful as Boltzmann rationality may be, however, we\nbelieve it is important to seek alternatives. For one, it is not the only intuitively plausible model of\nnoisily rational choice: Random-utility models instead model choice as the result of maximization\nover randomly perturbed utility values, and are widely used in marketing research (Horowitz et al.,\n1994; Azari Soufiani et al., 2013). More crucially, noisy rationality is not enough to account for the\nfull set of ways in which humans fail to act optimally. Richer models of bounded rationality are\nnecessary to accurately infer human preferences and values from their behavior. Most fundamentally,\nthe contents of human motivation are not entirely reducible to bare preferences or utility functions.\nInstead, we need to enrich our models of human rationality to encompass all the ways in which\nhumans are guided by reasons for acting, including the thick evaluative concepts that we apply when\ndeciding between courses of action (Blili-Hamelin et al., 2024). We elaborate upon these limitations\nin the following sections.\""}, {"title": "2.1 Beyond noisily-rational models of human decisions", "content": "The issue with both perfect and noisily-rational models of human decision-making is that they do\nnot account for the systematic deviations from optimality that humans in fact exhibit. As a long\nline of psychological and behavioral research has shown, humans are boundedly rational at best,\nexhibiting satisficing instead of optimizing behavior, (Simon, 1957, 1979). These deviations from\noptimality include framing effects, loss aversion, anchoring biases, and mis-estimation of high and\nlow probabilities \u2013 phenomena which are better modeled by prospect theory (Kahneman and Tversky,\n1979; Tversky and Kahneman, 1992) than standard rational choice theory. More generally, many\nof the decision problems that people encounter are computationally intractable to solve optimally,\nmaking rational choice a implausible model of human behavior (van Rooij, 2008; Bossaerts et al.,\n2019; Camara, 2022). Instead, research suggests that humans make use of a variety of heuristics in\norder to approximately solve the problems they encounter (Gigerenzer, 2008).\nChallenges to modeling bounded rationality. How might AI systems that infer human preferences\nand values account for these findings? One approach might be to incorporate a sufficiently long list\nof known heuristics and biases into our models of human decision-making, thereby ensuring that\npreferences can be robustly inferred even in the presence of such biases (Evans et al., 2016; Chan\net al., 2021). However, this approach is highly contingent upon on our current state of knowledge\nabout human rationality what if we miss out important biases in our models, leading to inaccurate\npredictions and inferences? (Christiano, 2015b; Steinhardt, 2017) As a potential remedy, Shah\net al. (2019) suggest learning human biases alongside their preferences. But a conceptual difficulty\nremains: Without any inductive constraints on the types of errors humans are susceptible to, how\ncan we ensure that human biases are accurately learned? As Armstrong and Mindermann (2018)\nshow, even inductive preferences for more parsimonious models of human decision-making cannot\ndistinguish important classes of observationally-equivalent hypotheses from intuitively plausible\nones, such as the possibility that humans are acting anti-rationally by minimizing the satisfaction of\ntheir preferences."}, {"title": "Resource rationality as a unifying frame", "content": "To address these challenges, we suggest \u2013 in line with\nprior work \u2013 that resource rational analyses of human decision-making might provide an answer:\nInstead of treating human biases and heuristics as idiosyncratic artifacts, resource rationality posits\nthat seemingly irrational human behavior can often be understood as arising from the rational use\nof limited computational resources (Lieder and Griffiths, 2020). For example, availability biases\ntowards extreme events can be modeled as a form of resource-rational sampling (Lieder et al.,\n2018), susceptibility to sharing inaccurate information can result from a form of rational inattention\n(Pennycook et al., 2021; Sims, 2003), and habitual action can be explained as a mechanism for\navoiding costly planning under time constraints (Keramati et al., 2016). Resource rationality thus\nserves as a generative principle for hypothesizing possible deviations from standard rationality, and\nthen testing whether such deviations in fact occur in humans.\nResource rationality as an inductive bias. What does this imply for AI alignment? Most practically,\nthe assumption of resource rationality can be embedded as priors over computation time and repre-\nsentational complexity in probabilistic models of human decision-making (Zhi-Xuan et al., 2020; Ho\nand Griffiths, 2022; Berke et al., 2023; Jacob et al., 2024), enabling systems to infer human goals and\npreferences from failed plans and mistaken reasoning (Evans et al., 2016; Alanqary et al., 2021; Chan\net al., 2021), while accelerating the speed of goal inference (Zhi-Xuan et al., 2024a). Embedding\nthese priors on human resource bounds provides a strong but flexible inductive bias on the the space\nof decision procedures that humans might employ. Unlike a simplicity prior, this may avoid concerns\nabout the non-identifiability of human preferences (Armstrong and Mindermann, 2018).\nThe normative appeal of resource rationality. Indeed, the inductive bias imposed by resource\nrationality has a normative appeal over a simplicity-based approach: It tries to make sense of humans\nas rational creatures, aiming for teleological explanations of our behavior instead of reducing us to\nmere physical phenomena to be explained by the simplest causal mechanism. At the same time, it\nis a forgiving standard of rationality, allowing room for mistakes when inferring preferences from\ntheir decisions, while placing greater evidential weight on decisions made after lengthier deliberation.\nBoth of these features make resource rationality a promising framework for systems that learn our\nvalues: Rather than directly associating our behavior with our preferences, preferences are associated\nwith how we would act if we were more thoughtful, reflective, and informed."}, {"title": "2.2 Beyond reward and utility functions as representations of human preferences", "content": "While resource rationality provides a more flexible framework for modeling the relationship between\npreferences and behavior, this says little about how preferences themselves should be represented.\nFor the most part, resource rational analyses continue to represent human preferences in terms of\nscalar costs and rewards, or more generally, utility functions, with the primary innovation being the\ninclusion of costs on computation (Lieder and Griffiths, 2020; Callaway et al., 2022). Yet, there are\nmany reasons to think that reward functions and utility functions are inadequate representations of\nhuman preferences, while also tending to produce conceptual confusion about what they do represent.\nThe limited expressivity of reward functions. These issues are most easily appreciated in the case\nof (scalar, Markovian) reward functions. As noted earlier, the reward representation assumes that the\nutility of a sequence of states and actions \u00a7 = (51,A1,\u2026\u2026, AT\u22121, ST) can be decomposed into a sum\nof scalar rewards over time:\n\\(U(\\xi) = \\sum_{t=1}^{T}R(s_t, a_t)\\)\nAdvocates of the reward representation argue that any task accomplishable by an intelligent agent can\nbe framed as a reward maximization problem (Silver et al., 2021). As Kasenberg et al. (2018) point\nout, however, this minimally requires that all historically relevant information is already included in\nthe representation of each state st a requirement since stated more formally by Abel et al. (2021)\nand Bowling et al. (2023). This means that without careful feature engineering, reward functions\ncannot easily express time-extended preferences like the desire to keep a promise, or the value of\nnarrative coherence. Separately, the scalar nature of the (standard) reward representation means that it\ncannot represent the existence of incomplete preferences due to multiple incommensurable scales of\nvalue (Vamplew et al., 2022; Anderson, 1995; Chang, 1997): Sometimes, the choices before us may"}, {"title": "Confusion about what reward functions represent", "content": "Alongside these limitations in expressiveness,\nthere is often slippage among AI researchers regarding the ontological status of reward, which is\nsometimes interpreted as the intrinsic desirability of a particular state or action (Schroeder, 2004),\nor as a biological signal that promotes learning (Butlin, 2021) or evolutionary success (Singh et al.,\n2009), but is also used to define the instrumental value of a state (as in reward shaping (Ng et al.,\n1999; Booth et al., 2023)), or to demarcate goals (i.e. desired trajectories or states of affairs (Molinaro\nand Collins, 2023; Davidson et al., 2024)). While this is partly a testament to the flexibility of reward\nfunctions as a mathematical formalism, this also means that distinct normative concepts (preferences,\ngoals, intents, desires, values, etc.) get conflated or subsumed under the label of \"reward\". In\nalignment research, this manifests as the tendency to frame value alignment in terms of reward\nlearning (Hadfield-Menell et al., 2016; Leike et al., 2018), and to formalize concepts like \"goals\"\n(Di Langosco et al., 2022) and \u201cintents\" (Ouyang et al., 2022) as reward functions. This is despite\nthe existence of other useful and potentially more appropriate formalisms, such as the formalization\nof goals as logical specifications (Fikes and Nilsson, 1971), and the formalization of intentions as\n(partial) plans (Bratman, 1987; Bratman et al., 1988).\nUtility functions are more expressive, but insufficiently constrained. While not without their own\ninterpretive confusions, utility functions are considerably more general than (Markovian) reward\nfunctions. For example, they can be defined over arbitrarily long sequences of states, allowing them\nto capture time-extended preferences. However, what utility functions buy in terms of expressiveness\ncomes at a cost to both identifiability and tractability: If no constraints are placed on the structure\nof human utility functions, then given some sequence of actions (e.g. a person buying ten apples,\nthen two oranges), it is not possible to disambiguate a reasonable utility function that explains the\nactions (e.g. by assigning higher utility to an apple over an orange) from a degenerate utility function\nthat assigns a utility of one to exactly the observed sequence. In addition, many utility functions are\nintractable to coherently maximize (Camara, 2022) or even to compute. If we apply the principle of\nresource rationality here too, this makes intractable utility functions less plausible representations of\nhuman preferences. Finally, utility functions are not without their own expressivity limitations: Like\nscalar rewards, they assume away preference incompleteness due to plural and incommensurable\nvalues (Chang, 2021; Eckersley, 2018). Indeed, empirical work shows that incomplete preferences\nare not just possible, but actual (Cettolin and Riedl, 2019; Nielsen and Rigotti, 2023). This means\nthat utility functions are, at best, approximate representations of human preferences, not exact ones.\nFundamental tensions for any representation of preferences. It is worth noting that these tensions\nbetween expressivity, structure, and tractability apply to any representation of human preferences,\nnot just reward or utility functions. Thus, while it might be tempting to ensure expressivity by\ndirectly representing human preferences as a (possibly incomplete) list of comparisons over universe\ntrajectories (or a distribution over such comparisons (Dumoulin et al., 2024)), such a list would be\nextremely space-inefficient, while providing little to no action guidance in novel choice situations.\nInstead, we should recognize that part of what makes reward and utility functions so useful in practice\nis that they are typically engineered to be compact representations of preferences. Practically useful\nalternatives should maintain this property, while better capturing the richness of human preferences.\nAlternative representations can better capture temporal structure and value plurality. Fortu-\nnately, many promising options exist: Temporal logics (Kasenberg et al., 2018) and reward machines\n(Icarte et al., 2022; Davidson et al., 2024) avoid the limitations of traditional reward functions,\""}, {"title": "2.3 Beyond preferences as representations of human values and reasons", "content": "Preferences are constructed, not basic. Thus far, we have proceeded as if human motivations\nand values are adequately captured by the concept of \u201cpreference\u201d as it is used in rational choice\ntheory. But as far as evaluative concepts go, this concept of \u201cpreference\u201d is an extremely thin one:\nMathematically, a \u201cpreference\u201d is just some ordering of two options, which can be interpreted as\neither a disposition to choose one option over another, subjective liking of one option over the other\nFranklin et al. (2022), or an all-things-considered judgment in favor of one of the options. Distinct as\nthese interpretations are, what they share is their highly abstract and general nature - \"preference\"\nis a thin concept because it does not encode richer semantic information beyond the bare notion of\n\u201cbetterness\u201d. Insofar as utility functions are interpreted as representations of preferences, this thinness\nis inherited by them: Utility just represents the mere preferability of some option. But why exactly\nare some options preferred over others? In virtue of what reasons do people make these preference\njudgments? Without answering these questions, we are unlikely to model how someone's preferences\ngeneralize to novel options in ways they would endorse. To do so, we must go beyond preferences\nas the fundamental unit of analysis, and understand how preferences are computed and constructed\nfrom our reasons and values (Warren et al., 2011; Lichtenstein and Slovic, 2006).\nRational choice as action on the basis of reasons. In making this point, we depart from the\ndomain of rational choice theory, and return to a more basic understanding of what it means to model\nourselves as rational agents: We are agents that take ourselves to act on the basis of reasons (Raz,\n1999; Logins, 2022). These reasons might include desires, such as an intrinsic desire to avoid pain\n(Sinhababu, 2017), evaluative judgments, such as the judgment that a movie is artistic enough to\nbe worth watching (Anderson, 1995), or even acts of will, such as the intention to pursue a specific\ncareer (Chang, 2009).\nEvaluative concepts as building blocks for reasons. What exactly is the content of these reasons?\nIn decision theory and Humean accounts of motivation (Sinhababu, 2017), only beliefs (represented\nas subjective probabilities) and desires (represented as the utility of some desired outcome) are\nconsidered as reasons for action. But even if we set aside other accounts (Anderson, 1995; Chang,\n2004; Parfit, 2018), this leaves open what a person's beliefs and desires are about. If I desire to\nbe both helpful and honest to others, what does it mean to be helpful or honest? Acting upon this\ndesire requires applying the concepts of helpfulness and honesty, which are not just any concepts, but\nevaluative concepts, or values. Importantly, most such concepts are not thin ones, like preference,\nutility or goodness; they are thick evaluative concepts - concepts that comprise both descriptive and\nnormative elements - such as beauty, humor, or health. As Blili-Hamelin and Hancox-Li (2023)\npoint out, even the concept of intelligence so central to AI is thick in this way.\nUtility functions as aggregators of distinct evaluative judgments. How should AI systems model\nsuch evaluative concepts, and their relationship to preferences and action? As a first pass, one\nmight turn the utility representation theorems on their head, viewing reward and utility functions\nas generators of human preferences, instead of mere representations of them. Indeed, as gestured\nat earlier, reward and utility functions are often interpreted in this way, with rewards, costs, and"}, {"title": "Utility functions assume that values are always commensurable", "content": "Although there is much to be\nsaid in favor of this approach, we believe that it is not quite enough. For one, it is still subject to the\nrepresentational limits of reward and utility functions. In particular, if utility functions are used to\nrepresent aggregate value judgments, this effectively assumes that distinct human values are always\ncommensurable in some way, and that our resulting preferences are always complete. Yet, as value\npluralists argue, there are contexts where it seems hard or impossible to commensurate our values\n(Anderson, 1995), resulting in choices where our reasons run short, and we cannot say if one option\nis ultimately better than another (Chang, 1997). Even when we do commensurate our values, utility\nfunction do not provide further information on our reasons and justifications for those trade-offs.\nEvaluative judgments are not reducible to observable features. For another, by conceiving of\nevaluative concepts as \u201cfeatures\u201d, we risk over-simplifying the semantics of many evaluative domains.\nConsider, for example, the concept of whether a research paper is novel, or whether an action is helpful\nor universalizable. Applying these concepts requires a complex set of computations: novelty involves\nevaluating the contributions of a paper with respect to a broader field of established knowledge\n(Amplayo et al., 2019); helpfulness involves estimating the goals of the agent being helped, and\nthen judging whether the action aided in achieving that goal (Ullman et al., 2009); universalizability\ninvolves simulating what would happen if everyone took a particular action (Levine et al., 2020;\nKwon et al., 2023b). The structured nature of these concepts suggests the need for a suitably rich\nlanguage of thought - one that captures the compositionality and algorithmic complexity of human\nconceptual cognition (Piantadosi and Jacobs, 2016; Quilty-Dunn et al., 2023; Wong et al., 2023).\nExplicitly modeling processes of evaluation and commensuration. To begin to capture all of this\ncomplexity, we propose that human decisions can be productively modeled as a three-stage process:\nEvaluate, Commensurate, then Decide (ECD). Given some choice options, a set of evaluation\nprocedures compute valuations or rankings of the options under consideration, where each procedure\ncorresponds to a distinct evaluative concept. These valuations serve as inputs to a commensuration\nprocedure (Espeland and Stevens, 1998), which produces, where possible, a context-sensitive value\nassignment or preference ordering over the options (optionally with justifications for why certain\ntrade-offs were made), while leaving certain preferences unspecified when some options are not\ncomparable. Finally, a decision procedure computes actions and policies with respect to the (possibly\nincomplete) preference ordering induced by the evaluation and commensuration procedures, resulting\nin behavior that approximately satisfies those preferences. By explicitly modeling human decisions\nin this way, we can maintain the distinctness of the values that guide our actions, while foregrounding\nthe ways in which we commensurate our values and dynamically construct our preferences."}, {"title": "3 Beyond expected utility theory as a normative standard of rationality", "content": "In the previous section, we described how research in AI alignment often assumes approximate utility\nmaximization as a descriptive model of human behavior, then highlighted the shortcomings of this\napproach. However, this leaves open whether utility maximization is a desirable normative standard\nfor both human and machine behavior that is, whether agents ought to maximize the satisfaction\nof their preferences as a condition of ideal rationality, regardless of whether they actually do so.\nCoherence arguments for EUT. There is a long history of debate regarding the validity of this\nnormative standard. Arguments in favor of expected utility theory (EUT) include the utility represen-\ntation theorems mentioned earlier (Samuelson, 1938; Savage, 1972; Bolker, 1967; Jeffrey, 1991; von\nNeumann and Morgenstern, 1944), which start from an axiomatization of what preferences count as\nrational, then demonstrate that any agent that acts in accordance with such preferences must act as if\nthey are an expected utility maximizer. In the AI alignment literature, these results are often treated\nas \u201ccoherence theorems\u201d about the nature of rational agency, either by taking the rationality axioms\nfor granted, or by providing arguments in defense of the axioms (Omohundro, 2007; Yudkowsky,"}, {"title": "3.1 Beyond expected utility theory as an analytical lens", "content": "Coherence is not rationally required. However, coherence arguments for expected utility theory\nare not as strong as the AI alignment literature has often presumed. The most extensive version\nof these arguments is given by Gustafsson (2022), who provides a money pump argument for\npreference completeness, and then uses completeness to derive arguments for transitivity, continuity,\nand independence. Yet, as Thornley (2023) points out, the argument for completeness depends on\nparticular assumptions about how agents are permitted to choose when offered a series of potentially\nexploitative trades, which can be avoided as long as agents do not accept offers that are less preferred\nthan options they previously turned down. Petersen (2023) formalizes this counter-argument further,\nproposing a dynamic choice rule that ensures agents with incomplete preferences are invulnerable\nto money pumps. Indeed, it is accepted by many decision theorists that preference completeness\nis not a requirement of rationality; instead, all that is required is for an agent's preferences to be\ncoherently extendible (Steele and Stef\u00e1nsson, 2020). In turn, this implies that rational agents need not\nbe representable as EU maximizers.\nCoherent EU maximization is intractable. But let us imagine that coherence arguments do go\nthrough after all. Even if this were the case, it is far from obvious that advanced intelligences would\ncomply with the axioms of utility theory (or be incentivized to do so) in the face of computational and\npractical limitations. As Bales (2023) argues, behaving as an expected utility maximizer can come\nwith considerable costs, while only providing limited benefits. In fact, as we noted in Section 2, most\nutility functions are computationally intractable to coherently maximize: Camara (2022) shows that\nwhile certain simple classes of utility functions allow for rational choice behavior to be computed\nin polynomial time, for a large class of other utility functions, agents cannot tractably compute\nchoice behavior that complies with the rationality axioms, and must instead resort to approximately\nmaximizing their utility function. Alternatively, agents may insist on complying with the rationality\naxioms, but give up on even approximate optimality with respect to their original utility functions. In\nother words, it is not always resource rational to maximize expected utility.\nCoherence alone is not informative. Suppose we could set aside these tractability worries as well. \nEven so, it is unclear what information EUT provides us. As discussed by Shah (2018), Ngo (2019),\nand Bales (2023), many kinds of behavior can trivially be described in terms of utility maximization,\nincluding an \"agent\" that does nothing at all. This means that EUT alone does not say much about\nthe kinds of goals that advanced AI systems are likely to pursue, or what they are likely to do in"}, {"title": "3.2 Beyond globally coherent agents as design targets", "content": "If agents are neither rationally required nor practically required to act as if they are expected utility\nmaximizers, this opens up the design space of (advanced) AI systems that we might hope to build\nand align. In particular, we have the option of building AI systems that do not comply with one or\nmore of the axioms of expected utility theory systems that are not globally coherent in the way\nthat expected utility maximizers are required to be.\nNon-globally coherent AI may be more faithfully and safely aligned. Why might this be desirable?\nThere are two broad reasons. One reason is faithfulness. As we discussed in Section 2, many human\npreferences may be incomplete due to incommensurable values, and we might want AI systems to\nfaithfully represent that preferential structure when making decisions (Eckersley, 2018). Otherwise,\nsuch systems might reliably take actions that promote certain outcomes over others, even though\nwe have yet to form a preference over which of those outcomes is better. Another reason is safety\n- for a wide range of (time unbounded) utility functions, expected utility maximizers have been\nshown to seek power over their environment (Turner et al., 2021), and avoid being"}]}