{"title": "LEAN-GitHub: Compiling GitHub LEAN repositories\nfor a versatile LEAN prover", "authors": ["Zijian Wu", "Jiayu Wang", "Dahua Lin", "Kai Chen"], "abstract": "Recently, large language models have presented promising results in aiding formal\nmathematical reasoning. However, their performance is restricted due to the\nscarcity of formal theorem-proving data, which requires additional effort to be\nextracted from raw formal language corpora. Meanwhile, a significant amount\nof human-written formal language corpora remains underutilized. To address\nthis issue, we propose LEAN-GitHub, a dataset consisting of large-scale formal\ndata extracted from almost all Lean 4 repositories on GitHub. After fine-tuning\nInternLM-math-plus on this dataset, our model achieved accuracies of 48.8% with\na single pass and 54.5% with 64 passes on the Lean 4 miniF2F test, surpassing state-\nof-the-art method at 52%. And it also achieves state-of-the-art on two other Lean 4\nbenchmarks (ProofNet and Putnam) targeting different fields/levels of math. These\nresults demonstrate that our proposed dataset is beneficial for formal reasoning\non a wide range of math topics. We open-source our model at https://GitHub.\ncom/InternLM/InternLM-Math and our data at https://huggingface.co/\ndatasets/InternLM/Lean-GitHub.", "sections": [{"title": "1 Introduction", "content": "Theorem proving stands as a fundamental objective in mathematics. To tackle the escalating intricacy\nof proofs and identify non-trivial flaws within them, formalized mathematical systems like Lean, Isabelle, and Coq have been developed to furnish computer-verifiable proofs. However,\ncrafting formal proofs demands substantial human effort, posing challenges for further advancement\nand underscoring the necessity for automated theorem proving. Recently, large language models\n(LLMs) have shown promising results in resolving high-school level\nmath problems through interactions with formalized proof assistants. Nevertheless, their performance\nremains unsatisfactory, primarily due to data scarcity.\nFormal languages necessitate significant expertise and effort and are utilized by a relatively small\nnumber of mathematicians, leading to a shortage of formal language corpora. In addition, unlike\nconventional programming languages such as Python or Java, formal proof languages contain inter-\nmediate information not directly visible in their raw code, e.g. proof trees comprising intermediate\nstates between proof steps, making raw language corpora unsuitable for training. This scarcity of\nwell-crafted human-written formal language data persists while many valuable human-written corpora\nremain underutilized. Although auto-formalization enables the synthesis of more aligned\ndata for training, the quality and diversity of their data remain constrained and thus cannot substitute\nfor human-crafted data.\nTo address this challenge, we propose LEAN-GitHub in this paper: a large-scale Lean dataset\nthat leverages open-source Lean repositories on GitHub, serving as a crucial complement to the"}, {"title": "2 Related works", "content": "The development of modern proof assistants, including Coq, Isabelle , and Lean has\nexpanded the expressive capabilities of formal systems beyond first-order logic, thereby stimulating"}, {"title": "3 The GitHub-Lean dataset", "content": "There are numerous Lean repositories on GitHub, which contains scarce human-written theorems\nand proofs. However, the raw Lean code is unsuitable for direct training, as it has crucial runtime\ninformation that humans can access when interact with Lean environments unrevealed. Examples\ninclude the intermediate states and targets between each proof steps, and hints provided by some\ntactics. Though there has been works on extracting these information, they are restricted on Mathlib\n4, Lean's centralized formal math library. Meanwhile, hundreds of Lean repositories covering\ndiverse topics exists on GitHub without getting exploited and extracted.\nWe form a dataset for theorem proving in Lean 4, named LEAN-GitHub, built upon 147 Lean 4\nrepositories available on the web. The dataset is one of the largest theorem proving datasets in Lean\n4 formal mathematics, consisting of 28,597 theorems with formal proofs and 218,866 tactics from\n2133 files. The dataset has 0.138B tokens. We propose that, training on the dataset improves model\nperformance on theorem proving in various mathematical topics."}, {"title": "3.1 Dataset Construction", "content": "In this section, we will detailedly describe how we construct the LEAN-GitHub dataset.\nSelection of the repositories. After conducting an exhaustive search on GitHub, we identified a\ntotal of 237 Lean 4 repositories (GitHub does not differentiate between Lean 3 and Lean 4) which\nmay contain compilable theorems. By filtering for keywords such as \"theorem\" and \"lemma\", we\nestimated that there are approximately 48,091 theorems across these repositories. However, it is\nimportant to note that the presence of a keyword does not guarantee the existence of a theorem with a\nproof written in the form of tactics. The main obstacles include: a) some repositories cannot compile,\neither due to improper construction of the project or incorrect Lean files included; b) dependencies\non other repositories that are not available online; c) repositories written in older versions of Lean\nwith deprecated features that cannot be migrated to newer versions; and d) proofs of theorems not\nconstructed using tactics. We discarded 90 repositories written in deprecated Lean 4 versions.\nAmong the remaining repositories, only 61 out of 147 could be compiled as valid Lean 4 projects with-\nout requiring modifications. The remaining repositories required extra efforts to compile successfully.\nA small fraction of projects relies on non-official releases of Lean 4, others contains a significant\nnumber of isolated files. We develop automated scripts to try heuristically finding the closest official\nreleases for the former case. Solution to the latter case is described in the next paragraph.\nSource Code Compilation. We opted not to use the Lake tool provided by the Lean 4 standard\nlibrary, but instead called the underlying leanc compiler directly to compile the source code. This"}, {"title": "3.2 Dataset Statistics", "content": "Fig. 3 shows the word cloud of the set of theorem names of our dataset. The word cloud highlights\nthe most frequently occurring keywords in the theorem names, with \"Logic\", \"FirstOrder\", \"Matroid\",\nand \"Arith(mezation)\" being the most prominent, indicating that the dataset contains mathematics\nfrom various fields. Fig. 4 displays the top 30 repositories with the most theorems extracted.\nThe distribution further shows the diverse mathematical topics in the data, including cutting-edge\nmathematical fields, data structures, as well as Olympiad-level problems."}, {"title": "4 Experiments", "content": "We develop the InternLM2-StepProver model that utilizes the LEAN-GitHub. InternLM2-StepProver\nis built upon InternLM-math-plus-7B model, which is a decoder-only transformer that is\ncontinued pre-trained on a corpus comprising 200B informal and formal math-related tokens. We\nthen conduct extensive experiments on various Lean 4 datasets to test the effectiveness of InternLM2-\nStepProver on formal reasoning. We also conduct ablation studies and case studies to further validate\nthe effectiveness of LEAN-GitHub."}, {"title": "4.1 Experiment settings", "content": "Our training set mainly consists of three parts: the LEAN-GitHub, Lean's Mathlib (via the LeanDojo\ndataset), which is the common practive in training formal reasoning models in Lean, and other\nprivate synthetic theorem, which mainly comes from the autoformoalization effort of Lean Workbook.\nSeveral other models were trained with different training set settings for ablation. We follow the\nproofstep objective used by GPT-f , which generating a PROOFSTEP (a Lean tactic) given a GOAL\n(current Lean tactic state) and the current DECLARATION (the Lean theorem name to be proved): DECL\n<DECLARATION> \\nGOAL <GOAL> \\nPROOFSTEP <PROOFSTEP>\\n, as depicted in Fig. 5.\nWe used a global batch size of 512 and a learning rate of $10^{-5}$. We fine-tuned for 2 epoch to obtain\nthe SFT model. For the learning rate, we used a warm-up in the first 3% steps, followed by a cosine\nschedule decaying to zero. The whole fine-tuning process took around 6 hours on 32 A100 GPUs."}, {"title": "4.1.2\nEvaluation settings", "content": "We utilized a standard methodology that iteratively performs a best-first search to generate tactics and\nvalidate intermediate proof steps within a formal proof until the proof is either finalized or resources\nare exhausted. During each generation step, a state $S_i$ is expanded by generating S tactic candidates\nfor it, with a maximum of K expansions allowed in a single iteration. In this context, we select\nS = 32 and K = 100.\nState De-duplication One of the most prevalent issues in the best-first search process is the highly\nduplicated states. This arises from the foundational nature of the Lean language, which is rooted in\ndependent type theory. In Lean, every proposition can be viewed as a valid type, and demonstrating\nthat proposition is akin to discovering or constructing an element of that type, essentially proving that\nthe proposition is inhabited. Moreover, Lean considers two proofs of a proposition to be definitionally\nequivalent. Consequently, there is no inherent mechanism to distinguish if two (potentially partial)\nproofs lead to the same intermediate states. Given that numerous Lean tactics, such as intro\nand have, involve introducing new hypotheses with customized names, it is common for multiple\nintermediate states with essentially identical hypotheses but distinct names to emerge during the\nsearch process, as depicted in Fig. 6. This issue becomes more pronounced in extensive proof"}, {"title": "4.2 Main Results", "content": "After testing with Lean 4 benchmarks aiming\nat different levels (high-school or undergradu-\nate), difficulties (ordinary exercises or Olympiad\nproblems), and taste (problem-solving vs. con-\nstructing a math system like in textbooks), we\nconclude that InternLM2-StepProver exhibits\nversatile formal reasoning abilities compared to\nprior works by achieving state-of-the-art perfor-\nmance, proving the effectiveness of the diversity\nof LEAN-GitHub."}, {"title": "Results on miniF2F.", "content": "We first test the Lean 4\nformal reasoning ability on miniF2F-Test and\nValidation dataset. MiniF2F [38] is a standard testing dataset for evaluating the performance of\nformal provers, containing 244 validation and 244 test problems, all stated in Lean. The range of\nproblems varies from high-school competition questions to undergraduate-level theorem proofs, e.g.\nproblems sampled from the MATH dataset [10], high-school mathematical competitions (including\nAMC, AIME, and IMO) and some other manually crafted problems at the same difficulty. We use the\nversion of miniF2F in Lean 4 released by the LeanDojo [34] project, with adaptations to Lean 4.7.0.\nWe present the main experimental results in Tab. 2. From the table, we can observe that InternLM2-\nStepProver achieves a cumulative accuracy rate of 63.9% on miniF2F-Valid and 54.5% on miniF2F-\nTest, suppressing all the baselines, including DeepSeek-Prover [30] which scores 60.2% and 52.0%,\nrespectively. Specifically, InternLM2-StepProver significantly outperforms all prior tree-search\nmethods, including Hypertree Proof Search, which achieves only 58.6% on miniF2F-valid and 41.0%\non miniF2F-test, demonstrating that the potential of tree search methods still remains to be fully\nexplored and such method is comparable to generating whole proofs."}, {"title": "Results on ProofNet.", "content": "ProofNet [3] is a benchmark for testing the formal reasoning ability on\nundergraduate-level mathematics. It comprises 371 formal problems which are sourced from popular\nundergraduate pure mathematics textbooks and cover topics such as real and complex analysis, linear\nalgebra, abstract algebra, and topology. We list results in Table 3. Outperforming the benchmark's\nprior leader, ReProver [34], which had a Pass@1 rate of 13.8%, our model, InternLM2-StepProver,\nachieves a significantly higher Pass@1 rate of 18.1%(Fig. 3). We have also discovered 24 proofs in\nProofNet that currently do not have Lean proofs."}, {"title": "Results on PutnamBench.", "content": "PutnamBench [25] is a benchmark comprising 640 theorems sourced\nfrom the Putnam Mathematical Competition, a renowned math competition in North America. These\ntheorems are formalized in Lean 4, Isabelle, and partially in Coq. The benchmark is designed to\ntest models' formal reasoning abilities in solving problems at a premier undergraduate mathematics\nlevel and is meticulously curated to prevent test-set leakage. We list results in Table 3. Restricted\nto Lean 4 formalization, GPT-4, and COPRA [23] each solved one of the 640 problems, while\nReProver failed to solve any. To our knowledge, the most effective method has been DSP [12], which\noperates in Isabelle and solved 4 problems with pass@10. As shown in Fig. 3, InternLM2-StepProver"}, {"title": "4.3 Ablation Studies", "content": "Data source ablation As described in Sec. 4.1.1, InternLM2-StepProver is trained on LEAN-\nGitHub, along with synthetic data including rule-based generated equations and inequalities and Lean-\nWorkbook [35], and human-written data extracted from Mathlib. To demonstrate the effectiveness\nof the LEAN-GitHub dataset, we conducted a comparative analysis among various combinations of"}, {"title": "5 Conclusion", "content": "In this paper, we introduce LEAN-GitHub\u2014a dataset comprising a large-scale collection of formal\ndata extracted from open Lean 4 repositories on GitHub, which includes 28,597 theorems and 218,866\ntactics. We then train InternLM2-StepProver using this dataset, which is the state-of-the-art model\nperformance on Lean 4 formal reasoning. We also train various models with LEAN-GitHub to\nevaluate the formal reasoning performance that can be achieved by training on our dataset. Notably,\nwe find that models trained on LEAN-GitHub exhibit performance improvements in formal reasoning\nacross various fields and difficulty levels, demonstrating that a well-extracted, diverse dataset can\nenhance model performance on a range of reasoning tasks. We hope that by opening LEAN-GitHub,\nwe can assist the community in better exploiting the under-utilized information in raw corpora and in\nimproving mathematical reasoning capabilities."}, {"title": "A Case study", "content": "This section presents case studies to demonstrate the performance of our methods."}, {"title": "Case 1: Binomial Coefficients", "content": "Natural Language problem: Show that for positive integers n and k with k \u2264 n, we have\n${n \\choose k} = {n-1 \\choose k} + {n-1 \\choose k-1}$"}, {"title": "Case 2: Putnam 1988 B2", "content": "Natural Language problem: Prove or disprove: If x and y are real numbers with y \u2265 0 and\ny(y + 1) \u2264 (x + 1)\u00b2, then y(y \u2212 1) \u2264 x\u00b2."}, {"title": "Case 3: IMO 1964 P1(2)", "content": "Natural Language problem: Prove that there is no positive integer n for which 2\u207f + 1 is\ndivisible by 7."}, {"title": "Case 4: Pough 3.2.8", "content": "Natural Language problem: Prove that if H and K are finite subgroups of G whose orders\nare relatively prime then H\u2229 K = 1."}]}