{"title": "DRL4AOI: A DRL Framework for Semantic-aware AOI Segmentation in Location-Based Services", "authors": ["Youfang Lin", "Jinji Fu", "Haomin Wen", "Jiyuan Wang", "Zhenjie Wei", "Yuting Qiang", "Xiaowei Mao", "Lixia Wu", "Haoyuan Hu", "Yuxuan Liang", "Huaiyu Wan"], "abstract": "In Location-Based Services (LBS), such as food delivery, a fundamental task is segmenting Areas of Interest (AOIs), aiming at partitioning the urban geographical spaces into non-overlapping regions. Traditional AOI segmentation algorithms primarily rely on road networks to partition urban areas. While promising in modeling the geo-semantics, road network-based models overlooked the service-semantic goals (e.g., workload equality) in LBS service. In this paper, we point out that the AOI segmentation problem can be naturally formulated as a Markov Decision Process (MDP), which gradually chooses a nearby AOI for each grid in the current AOI's border. Based on the MDP, we present the first attempt to generalize Deep Reinforcement Learning (DRL) for AOI segmentation, leading to a novel DRL-based framework called DRL4AOI. The DRL4AOI framework introduces different service-semantic goals in a flexible way by treating them as rewards that guide the AOI generation. To evaluate the effectiveness of DRL4AOI, we develop and release an AOI segmentation system. We also present a representative implementation of DRL4AOI - TrajRL4AOI - for AOI segmentation in the logistics service. It introduces a Double Deep Q-learning Network (DDQN) to gradually optimize the AOI generation for two specific semantic goals: i) trajectory modularity, i.e., maximize tightness of the trajectory connections within an AOI and the sparsity of connections between AOIs, ii) matchness with the road network, i.e., maximizing the matchness between AOIs and the road network. Quantitative and qualitative experiments conducted on synthetic and real-world data demonstrate the effectiveness and superiority of our method. The code and system is publicly available at https://github.com/Kogler7/AoiOpt.", "sections": [{"title": "I. INTRODUCTION", "content": "Location-based services (such as food delivery, logistics, ride-sharing and spatial crowdsourcing [1], [2]) have experienced rapid growth by greatly facilitating people's lives. One fundamental spatiotemporal data management [3] behind these LBS platforms is partitioning urban geographical space into multiple small, non-overlapping Areas of Interest (AOIs). For instance, ride-sharing giants like DiDi\u00b9 and Uber\u00b2 divide the city into several AOIs to dispatch idle drivers to high-demand areas [4], [5]. Similarly, logistics platforms like Cainiao\u00b3 and JD.COM4 dispatch orders to couriers based on AOI segmentation [6]\u2013[8]. The quality of the AOI segmentation directly influences the service quality. In light of the above cases, there is a rising call for effective AOI segmentation methods to generate a well-managed set of AOIs.\nAs shown in Figure 1, an intuitive way is road network-based methods, which divide AOIs according to road networks [9], [10]. Though promising in preserving the geo-semantics (such as identifying a geographical entity, i.e., school), road network-based models are not designed to meet various service-semantics goals (e.g., workload equity [11]) in LBS. To this end, there is a rising trend of optimization-based approaches that aim to meet the service-semantic goals. For instance, E-partition [11] segments AOIs with the goal of workload equity, and RegionGen [3] aims to achieve more accurate demand prediction when generating AOIs. Though promising, those optimization-based algorithms are challenged to model abundant spatial and temporal features in the optimization process, which restricts their performance in real-life scenarios.\nTo address the above limitation, we resort to Deep Reinforcement Learning (DRL) for the AOI segmentation, for it combines the power of RL methods in non-differentiable objective optimization with deep learning models in complex feature learning. Specifically, we provide a novel perspective"}, {"title": "II. RELATED WORK", "content": "Current AOI segmentation methods can be broadly classified into three classes: 1) fixed-shape, 2) road-network-based, and 3) optimization-based methods.\nFixed-shape methods. Fixed-shape methods divide the urban space into several fixed-shape grids or hexagons [4], [12]\u2013[14]. It is a simple and effective way to output a partition that naturally covers the city, i.e., any point in the target space is associated with an AOI. However, those methods cannot capture the geographic semantic meaning of the urban space, which may significantly trim down the performance of other AOI-based tasks in the platform.\nRoad-network-based methods. Targeting at solving the shortcomings of fixed-shape methods, road-network-based methods [9], [10] capture the geographic semantics by dividing the urban space via road networks. In this way, nearby locations with the same geographic meaning (e.g., all areas in a school) can be grouped into one AOI. Though effective in most scenarios, a fundamental limitation of the road-network-based method [3], [11] is the lack of service semantic information, which is usually considered a bottleneck that restricts the overall system performance.\nOptimization-based methods. To address the above limitation, optimization-based methods have emerged in recent years. Those models introduce service-specific goals or constraints (i.e., service semantics) to guide the AOI generation based on optimization-based methods by levering various kinds of data generated in the service. For example, [15]\u2013[18], combines downstream route planning tasks and establishes a cost function based on factors like workload and distance. They employ heuristic search algorithms to solve the optimization problem and partition the regions accordingly. RegionGen [3] models the AOI generation as a multi-objective optimization problem. It first segments the city into atomic spatial elements based on road networks. Then, it clusters the above elements into different AOIs by maximizing various operation goals, i.e., clusters' average predictability and service specificity. [19] introduced an alpha-shape partitioning method based on POI data, extracting relevant data from the Web and using the alpha-shape method to infer the boundaries of imprecise AOIs. C-AOI [20] propose an image-based model that converts the AOI segmentation into an instance segmentation task on multi-channel images. Each channel represents a feature constructed from the order's location, satellite image, road networks, etc. E-partition [11] aims to cluster AOIs with equitable workload assignment. To achieve this goal, it first predicts the service time of a work given a specific AOI. Then, it clusters the AOIs into different delivery regions with the optimization goal based on the time prediction by setting the workload balance as the optimization goal.\nOptimization-based models mostly resort to traditional optimization algorithms to achieve the service-semantic goals. Though promising, traditional optimization algorithms lack the ability to model different kinds of spatial and temporal features in the optimization process. These restrict their performance in real-life scenarios. This motivates us to propose a DRL-based framework that can handle different kinds of information as well as achieve the optimization goal. Moreover, the DRL-based model is more flexible, where different service-semantic goals can be introduced as rewards in the RL learning process."}, {"title": "III. PRELIMINARIES AND PROBLEM DEFINITION", "content": "In this section, we first give a general formulation for AOI segmentation under service-semantic goals. Then, we instantiate the general formulation by defining the AOI segmentation problem for the logistics service."}, {"title": "A. A General Formulation", "content": "Without loss of generality, we first give a general form of service-semantic-aware AOI segmentation. It unifies the problem of previous efforts, where definitions and settings in previous methods can be viewed as different instantiations of the general formulation.\nDefinition 1: Geo-related Input. The geo-related data $G$ contains the graphical information of the target urban space, such as road networks or the satellite image of the target area. Incorporating geo-related data helps the model capture the geographical semantics.\nDefinition 2: Service-related Input. Massive historical data are generated in the service process. We use $S = {s_1, ...,s_m}$ to denote the service-related data, with each $s_i$ representing one type of data. For example, food orders in the online food delivery system or courier's trajectories in the logistics platform.\nDefinition 3: Service-semantic Goals. In different LBS services, there can be various service-semantic goals, such as predictability and workload equity, as introduced in Section II. We use $O = {o_1, ..., o_k}$ to denote the service semantic goals, and each $o_r$ represents a predefined goal.\nDefinition 4: AOI Segmentation. Given the geo-related data $G$ and service-related data $S$, service semantic-aware AOI segmentation learns a mapping function that can divide the target urban space into several AOIs, which aims to meet the service semantic goals $O$, formulated as:\n$\\mathcal{F}_O(G, S) \\rightarrow A := {a_1,..., a_n},$ (1)\nwhere $A$ is an segmentation result with $n$ AOIs, and $a_i$ means the $i$-th AOI.\nIn conclusion, the general formulation offers a shared understanding of the AOI segmentation problem, which recognizes the input and objective as geo/service-related information and service-semantic goals. This paves the way to create more effective models from the proposed perspectives."}, {"title": "B. AOI Segmentation for Logistics Service", "content": "Following the above general formulation, we instantiate the problem we focus on in this paper, i.e., AOI segmentation for logistics service, by specifying geo-related input, service-related input, and service semantic goals.\nThe urban space is divided into several AOIs for efficient management of logistics services. Each AOI will be assigned to at least one courier, who will then be responsible for delivering all the packages within that AOI. Typically, the courier completes the delivery of all the packages in one AOI before moving on to the next one.\nDefinition 5: AOI. To define the AOI, we first divide the geographic space into $M \\times N$ grids, which efficiently and effectively represent the spatial unit, as adopted in many previous works. Each grid is represented by a tuple $(i, j)$, which means that the grid is in the $i$-th row and the $j$-th column. An AOI is defined as a collection of nearby grids $a = {(i_1, j_1), ..., (i_m, j_m)}$, where $m$ is the number of grids contained in the AOI.\nDefinition 6: Road Network. It is the geo-related input in our setting. We also use $G$ to represent the road network to ease the presentation. It contains all roads within the region, formulated as ${r_1,...,r_s}$, where $s$ means the total number of roads. Each road $r$ is represented by a series of points, i.e., $r = {(x_1, y_1), ..., (x_l, y_l)}$, where $x$ and $y$ means the longitude and latitude, respectively. $l$ is the number of points in the road $r$.\nDefinition 7: Courier Trajectory. It is the service-related input in our setting. A courier trajectory is a sequence of coordinates and time, i.e., $r = {(x_1, y_1, t_1), ...}$, where $x$ is the longitude, $y$ is the latitude and $t$ is the time. We use $T$ to represent the trajectories of all couriers. Section V details the motivation for introducing the trajectory.\nDefinition 8: Delivery-service Semantic Goals. We envision two service-related goals for the AOI segmentation in the package delivery service.\nGoal 1: Trajectory modularity. We borrow the concept of modularity from community detection in network analysis [21]. A large modularity instructs dense connections within the same community but sparse connections between communities. Similarly, we want a large trajectory modularity to minimize switches between AOIs. Because in real situations, a courier will move to the next AOI after completing all the delivery tasks in one AOI. If the courier frequently switches between two nearby AOIs, then the two AOIs can be combined into one to form a more reasonable segmentation. The goal can be formulated as:\n$o_1: \\min_A \\sum_{\\tau \\in T} N_{switch}(A, T),$ (2)\nwhere $N_{switch}(A, T)$ is the number of switches between AOIs $A$ in the trajectory $\\tau$.\nGoal 2: Matchness with the road network. Road-network-based methods provide an initial segmentation (denoted by $A_G$) that grasps the basic geographic-semantic meaning of the urban space. To this end, it is unreasonable if the result is quite different from road-based segmentation. We use the similarity between result and road-based segmentation to describe this semantic goal. It can be formulated as:\n$o_2: \\max_A Similarity(A, A_G),$ (3)\nwhere $Similarity$ is a function that calculates the similarity between two segments, and $A$, $A_G$ are the model result and segmentation based on the road network, respectively.\nDefinition 9: AOI Segmentation Problem for Logistics Service. Given the road network $G$, and the courier's trajectory $T$, we aim to learn an AOI segmentation that can satisfy the delivery-service semantic goals $O = {o_1,o_2}$, formulated as:\n$\\mathcal{F}_{{o_1,o_2}}(G,T) \\rightarrow A := {a_1, ..., a_n}.$ (4)\nTo provide a big picture of our problem setting and the related works, we summarize their geo-related information, service-related information, and service-semantic goals in Table II."}, {"title": "IV. PROPOSED DRL4AOI FRAMEWORK", "content": "This section describes the DRL4AOI framework, a deep reinforcement learning (DRL) framework for AOI segmentation. We first introduce how we formulate the problem from the RL perspective and then present the details of the framework."}, {"title": "A. Formulation from the RL perspective", "content": "We provide a novel perspective to show that the problem can be naturally regarded as a sequential decision-making problem, which can be effectively mitigated by reinforcement learning. As illustrated in Figure 2, the core idea behind this is deciding which neighbor AOI it belongs to for each grid on AOI's border at each decision step (To ease the presentation, we call this action AOI selection).\nSpecifically, the process of making sequential decisions can be represented by a finite-horizon discounted Markov Decision Process (MDP) [22]. In this process, an AOI segmentation agent interacts with the environment over $T$ discrete time steps by the action of AOI selection. Formally, a MDP can be formulated as $\\mathcal{M} = (S,A, \\mathcal{P}, R, \\gamma)$, where $S$ is the set of states, $A$ is the set of actions, $\\mathcal{P}: S\\times A\\times S \\rightarrow \\mathbb{R}^+$ represents the transition probability, and $R : S\\times A \\rightarrow \\mathbb{R}$ represents the reward function. The initial state distribution is $s_0: S\\rightarrow \\mathbb{R}^+$, and $\\gamma$ is a discount factor whose value is between 0 and 1. Moreover, it is worth mentioning that by setting the action as adjusting the AOI's border, the spatial continuity constraint that each AOI should have inter-connected grids can be naturally and easily satisfied.\nWhen provided with a state $s_t$ at a given time $t$, the AOI segmentation agent utilizes the current policy $\\pi_\\theta$ which is a deep neural network parameterized by $\\theta$ to generate an action, i.e., selecting a nearby AOI for the grid on AOI's border. The agent then receives a reward $r_t$ defined by the service semantic goals from the environment. Then, the reward is utilized to train the agent. During the training process, the main objective is learning the best parameter $\\theta^*$ for the agent to maximize the expected cumulative reward, which is formulated as:\n$\\theta^* = \\arg \\max_\\theta \\mathbb{E}_{\\pi_\\theta} [\\sum_{t=1}^T \\gamma^t r_t],$ (5)\nwhere the discount factor $\\gamma$ controls the trade-offs between the importance of immediate and future rewards, and the total time step, denoted by $T$, is determined by the number of grids in AOI's border. The following part introduces the details of the agent, state, action, reward, and state transition probability.\nAOI Segmentation Agent: The AOI segmentation agent is responsible for learning the target function $\\mathcal{F}_O$. It accomplishes this by selecting an AOI for each grid on the AOI border with the help of a deep neural network, which will be detailed in the action introduction part. The agent follows an encoder-decoder architecture, where the encoder takes meaningful features from the current state $s$ and embeds them into hidden representations. The decoder accepts these representations as input and produces an action $a$ at each time step. Abstractly, the agent can be represented as:\n$a_t = Decoder(Encoder(s_t)),$ (6)\nwhere Encoder and Decoder represent the state encoder and action decoder of the agent, respectively.\nState: A state $s_t$ is composed of the information of current AOI segmentation and the target grid in the AOI border to be modified. All the possible segmentations make up the entire state space.\nAOI-select Action: An action $a_t \\in A_t$ is defined as selecting a nearby AOI for a grid on the AOI border. Intuitively, we can set all current AOIs as candidates in the action space, i.e., $A_t = {a_1,..., a_n}$. However, such a design introduces a large search space, which challenges the agent to learn a converged policy. To address the issue, we consider the AOI selection as merging the target grid into its neighbor AOIs that are located in different directions, as shown in Figure 2. In this way, the action space is reduced to only five actions, namely, up, down, left, right, and origin. Where up/down/left/right means merging the target grid to its up/down/left/right AOI, and origin means that the target AOI stays in its current AOI. We use a five-dimensional one-hot vector to represent an action in the implementation.\nService-semantic Reward: $r_t \\in \\mathbb{R} \\leftarrow S \\times A$: Reward design is of great importance in reinforcement learning; a well-designed reward can have a significant impact on the learning"}, {"title": "B. DRL4AOI Framework Architecutre", "content": "Based on the above formulation, we are now ready to introduce the proposed framework, DRL4AOI. Figure 3 shows the overall architecture of DRL4AOI, where the inputs are geo-related and service-semantic-related information, and the output is the result of AOI segmentation. The RL-based segmentation learning mainly includes three steps: 1) data preprocessing, 2) model training, and 3) post-processing.\nFirstly, the data preprocessing step constructs both the geo-related input and service-related input for the model. Secondly, the RL-based AOI segmentation solves the problem from the reinforcement learning perspective. Specifically, this step trains an AOI segmentation agent, which takes the state as input and outputs an AOI selection (up/down/left/right/origin) at each time step. After each action is performed, the environment will provide reward feedback that is aligned with service-semantic goals. Guided by the rewards, the agent updates its parameters and gradually adjusts the grids in AOI's border guided. In order to avoid the influence of traverse order on the partition result, the agent will traverse several times in grids. As the training progresses, the accumulated reward will gradually rise, which means a better segmentation strategy is learned regarding the service-semantic goals. At last, the post-processing further refines the segmentation results outputted in the previous step. We give a detailed introduction in the following part."}, {"title": "V. PROPOSED TRAJRL4AOI FOR LOGISTICS SERVICE", "content": "Based on the DRL4AOI framework, we further propose a model called TrajRL4AOI for logistics delivery service to demonstrate the effectiveness and generality of the framework. TrajRL4AOI incorporates two types of input - the road networks as the geo-related information and the courier's trajectory as the service-semantic input. Since they are the most common data in the realm of spatial-temporal data mining [23]\u2013[26]. TrajRL4AOI uses these to generate high-quality AOIs by setting the trajectory modularity and road network matchness as the service-semantic goals."}, {"title": "A. Motivation of Introducing the Trajectory", "content": "Before introducing TrajRL4AOI, it is worth introducing the motivation for modeling the courier's trajectory. As shown in Figure 4(a), two communities (i.e., Com1 and Com2) are surrounded by the same road network, while a fence separates them. And there is another big community (i.e., Com3) which has a small road inside. If we generate the AOI segmentation by road-network-based model (the result is shown in Figure 4(b)), it can be seen that Com1 and Com2 are assigned to the same AOI. However, it is better to identify them as two separate AOIs in the logistics system since it is more aligned with the real scenario. Similarly, we also hope that all components of Com3 can be merged into the same AOI"}, {"title": "B. Preprocessing", "content": "The data preprocessing contains three kinds of feature processing, including AOI, trajectory, and road network.\nAOI features processing. We rasterize the urban space to represent AOI segmentation. Specifically, we rasterize the map into $M \\times N$ grids to facilitate the model analysis and processing, where $M$, $N$ is the length and width after rasterization. In our problem, using a grid to represent the smallest unit in the map offers the following two merits: 1) Grid mapping is an effective way that naturally ensures coverage of all location points within a city; 2) The AOI adjustment can be straightforwardly achieved through grid aggregation or splitting, which can get more fine-grained optimization especially compared to road-network based models. Because in road network-based models, the smallest operational units are geographical objects that are divided by the road network. Some of the geographical objects may deserve further segmentation.\nTrajectory processing. To represent and mine the mobility patterns of couriers, we further build a trajectory transfer graph $T_G$ based on all couriers' trajectory data. It is a directed weight graph that describes the trajectory transition between nearby grids. $T_G$ can be formulated as $T_G = (V, E)$, where $V$ is the node set with each node corresponding to a grid in the gridded map. And $E$ is the edge set, with each edge represented by a triplet $(s, d, w)$, which means there are total $w$ trajectories from $s$-th node to $d$-th node. We flatten the grids to obtain the 1D index of 2D grids. In order to get $E$, we map each point in every trajectory into a grid and sort them in time order. So that it is converted into $\\tau = {n_1, ..., n_m}$, where $n$ means the point is mapped into the $n$-th grid, and $m$ is the total point number in trajectory $\\tau$.\nRoad Network processing. Figure 5 shows the process of obtaining road network data and converting it into an initial AOI segmentation. It can be divided into two steps:\nStep 1: Rasterizing the road network data and exporting the road network pixel matrix. We obtain the required road network data from OpenStreetMap5 [27] and import it into the ArcGIS6 software. In ArcGIS, we crop the road network information based on the desired coordinate range, remove all other elements, and only keep the road network. Then, we export it as an image format with a certain precision for further processing.\nStep 2: Performing semantic segmentation on the road network image to obtain initial AOI segmentation. We convert the map image to grayscale and perform binary thresholding to distinguish the road network from other elements clearly. The road parts in the map are marked as black, and the rest are white. Then, we find connected components7 on the binary image and obtain a label matrix containing AOI regions separated by the road network. To remove the road network itself from the AOI segmentation, we perform contour extraction and expansion [3] on the non-road regions iteratively. In each iteration, we extract contours and expand the label matrix based on the contour information. Pixels within the same contour are assigned the same label. Through multiple iterations, neighboring regions with similar features are merged into the same region.\nBy following these steps, we obtain the initial AOI segmentation of the road network, which can be used as input to the model."}, {"title": "C. Double DQN for AOI Segmentation", "content": "In this subsection, we elaborate on the proposed TrajRL4AOI by introducing three important components in the framework, including the state, reward, and agent.\nState. A state records information around the target grid, the current AOI segmentation, trajectory and road network partition. Here we use a matrix with size $M \\times N$ to describe all grids in the map. Generally, a state $s \\in \\mathbb{Z}^{M\\times N\\times C}$ is a feature matrix with $C$ channels. The following features are extracted to achieve a comprehensive representation of the current state:\n\u2022 Target grid, denoted by $x_t \\in \\mathbb{B}^{M\\times N\\times 1}$, which records the target grid (i.e., the grid to be modified) in the receptive field by a $M \\times N$ boolean matrix. The position of the grid that needs to be modified is set to 1, while the other positions are set to 0.\n\u2022 AOI features $A_t \\in \\mathbb{Z}^{M\\times N\\times 1}$. It records the current AOI segmentation (i.e., AOI ID) of all grids in the $M \\times N$ field. Understanding its neighbor AOI of a grid is crucial for the decision of AOI selection.\n\u2022 Trajectory features $M_T \\in \\mathbb{Z}^{M\\times N\\times 4}$, which records the local trajectory transfer in the $M \\times N \\times 4$ field. $a_{i,j,d}$ means the trajectory number of the grid in $i$-th row, $j$-th column and the direction $d$ (i.e., up/down/left/right). Such information illustrates how grids interact with their neighbors regarding the trajectory connection in the courier's service process [28].\n\u2022 Road segmentation features $G_t \\in \\mathbb{Z}^{M\\times N\\times 1}$. It contains the AOI segmentation results based on the road network in the $M \\times N$ field. The information on road networks could help the AOI selection avoid some abnormal divisions, such as narrow areas.\nAt last, the overall state at time step $t$ is represented by the concatenation of the above features, formulated as $s_t = (x_t, A_t, M_T, G_t)$.\nAction & State Transition. As defined in the framework, actions indicate the nearby AOI to which the target grid should belong, which is transformed from the direction indicator (up, down, left, right, origin.) in the action space. Once an AOI selection is made for the target grid, the current AOI segmentation will change, thus forming a state transition.\nReward Design. One advantage introduced by the reinforcement framework is the flexibility of the reward design, which can accommodate different service-semantic goals. In the logistics service, the two goals are trajectory modularity and road network matchness, as we introduced in Section III. In that case, the reward $r_t$ after an action is taken at step $t$ is composed of two parts in TrajRL4AOI, with each $r^i$ defined as follows:\n\u2022 Trajectory reward $r^1_t$. This reward is related to the objective $o_1$ (i.e., trajectory modularity). Recall that $N_{switch}(T, A)$ is the number AOI switch for trajectory $\\tau$ under current AOI segmentation $A$. Let $o_1(S_t) = \\sum_{\\tau \\in T} N_{switch}(\\tau, A_t)$ be the total switch times of all trajectories at state $s_t$. The trajectory reward is defined as the difference between the objective values $o_1(s_t)$ and $o_1(s_{t+1})$, i.e.,\n$r^1_t = o_1(s_t) - o_1(s_{t+1}).$ (7)\n\u2022 Similarity to road-based segmentation $r^2_t$, which is related to the second objective $o_2$, i.e., Appropriate segmentation based on the road network. Let $o_2(S_t) = Similarity(A_t, A_G)$, where $A_G$ denotes road network based division. And $Similarity$ calculates the Co-AOI rate (detailed in Equation 13), i.e., the similarity between current AOI segmentation and segmentation based on the road network. The reward $r^2_t$ is defined as the similarity change between two states, i.e.,\n$r^2_t = o_2(s_t) - o_2(s_{t-1}).$ (8)\nOverall, the total reward function is defined as:\n$r_t = k_1 * r^1_t + k_2 * r^2_t$ (9)\nwhere $k_1,k_2 > 0$ are hyper-parameters to control the weight of different rewards.\nAgent. The agent is designed to learn a policy network for AOI selection. We adopt Double Deep Q-Networks (DDQNs) [29] to implement the agent because of its strong power of combining deep neural networks with reinforcement learning, as proved in different environments such as game playing [30]\u2013[32]. As shown in Figure 3, at each step, the agent encodes the state with a Convolutional Neural Network (CNN) [33], [34]. Via multiple convolutional operations, the CNN state encoder extracts an effective representation of the target grid's neighborhood information, which will be further fed into the policy networks. The AOI-selection policy network takes the state embedding as input and scores each action candidate with a Multi-Layer Perceptron (MLP) [35], where the obtained score indicates the performance score of the action in the learned policy.\nSpecifically, DDQN is based on DQN [31], [36], whose primary idea is to approximate the Q-value function using a neural network. The network takes a state $s$ as input and produces Q-values for all possible actions. The key equation for the Q-value in DQNs is given by the Bellman equation:\n$Q(s, a) = r + \\gamma \\max_{a'} Q(s', a')$ (10)\nThe optimal action value Q is the sum of the reward and the maximum Q-values of all actions in the next state.\nHowever, DQNs tend to overestimate Q-values because they use the same network to select the best action and evaluate it. This can lead to suboptimal policies and reduced stability in training.\nDouble DQNs aim to address this overestimation bias. Instead of using a single network for both action selection and evaluation, Double DQNs use two networks: i) An online network to select the best action. ii) A target network to evaluate the Q-value of the selected action. Thus, the updated Q-value equation in Double DQNs is:\n$Q(s, a) = r + \\gamma Q_{target}(s', \\arg \\max_{a'} Q_{online}(s', a'))$ (11)"}, {"title": "D. Discussion", "content": "1) Addtional Geo-/Service-related data: In reality, additional data may be available which can contribute to AOI segmentation. For example, geo-related data such as satellite image [38], [39], POI distribution [40], [41]. In terms of the service-semantic data, different data can exist in different service scenarios, such as the courier's trajectories [42], [43], and the order information [44] in the food delivery. Considering this, we improve the flexibility of DRL4AOI from the following two aspects: i) RL combined with deep reinforcement learning. The deep neural network-based nature allows the model to accommodate abundant information from different domains. For example, the satellite image can seamlessly serve as a feature channel in the input of the CNN. ii) customzied AOI segmentation agent encoder. In our work, the input is organized into grid data. Thus, the CNN-based encoder is utilized to represent the information in the input. To allow for more types of data structure, the AOI segmentation agent encoder can be customized, such as adding the graph neural network for graph-based data.\n2) Addtional service-semantic goals: Moreover, various service-semantic goals can exist in real scenarios, mainly due to various LBS service types and the development of the service. Those service-semantic goals can be broadly classified into two classes: the first one directly relates to AOIs themself, which requires specified attributes the AOI needs to have, such as proper granularity [3], or matchness with geographic boundaries [20]. The other class is related to the service performance based on the AOI segmentation results, such as predictability [3] and workload equity [11]. In our designed framework, different goals can be considered in a plug-and-play way by converting them into different rewards. By weighing the importance of different rewards, the segmentation agent tunes its policy to set its preference for different goals. In summary, the reward mechanisms serve as a convenient way to bridge the service-semantic goals and the model training."}, {"title": "VI. EXPERIMENT", "content": "Data. We conduct extensive experiments on synthetic data: Synthetic Data. To test the performance of different methods in the ideal case, we first create a benchmark synthetic dataset called SyntheticAOI. The main idea is to generate ground-truth AOIs and then trajectories under the AOIs. The data construction mainly contains two steps: 1) AOI generation. We first generate several AOIs for $M \\times N$ grids and assign them to several couriers. Each courier is responsible for one AOI. 2) Trajectory generation. We randomly generate several trajectories for each courier based on the previously generated AOI. To generate a single trajectory, we first randomly generated several packages in his served AOIs, and generated the trajectory by solving a VRP problem [45], [46] with the minimized total distance."}, {"title": "B. Baselines", "content": "We choose several methods that belong to different types as baselines:\n\u2022 GreedySeg. A simple method to satisfy the trajectory semantic goal. We traverse each grid and merge it into one nearby AOI, which can reduce the trajectory switch to the maximum extent.\n\u2022 RoadNetwork [10", "47": ".", "48": ".", "37": "which converts the AOI segmentation into a"}]}