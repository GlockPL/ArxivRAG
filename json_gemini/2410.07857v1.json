{"title": "SNN-PAR: Energy Efficient Pedestrian Attribute Recognition via Spiking Neural Networks", "authors": ["Haiyang Wang", "Qian Zhu", "Mowen She", "Yabo Li", "Haoyu Song", "Minghe Xu", "Xiao Wang"], "abstract": "Artificial neural network based Pedestrian Attribute Recognition (PAR) has been widely studied in recent years, despite many progresses, however, the energy consumption is still high. To address this issue, in this paper, we propose a Spiking Neural Network (SNN) based framework for energy-efficient attribute recognition. Specifically, we first adopt a spiking tokenizer module to transform the given pedestrian image into spiking feature representations. Then, the output will be fed into the spiking Transformer backbone networks for energy-efficient feature extraction. We feed the enhanced spiking features into a set of feed-forward networks for pedestrian attribute recognition. In addition to the widely used binary cross-entropy loss function, we also exploit knowledge distillation from the artificial neural network to the spiking Transformer network for more accurate attribute recognition. Extensive experiments on three widely used PAR benchmark datasets fully validated the effectiveness of our proposed SNN-PAR framework. The source code of this paper will be released on https://github.com/Event-AHU/OpenPAR.", "sections": [{"title": "1 Introduction", "content": "Pedestrian Attribute Recognition (PAR) [8,57] targets describing the appearance cues of humans from an attribute set, like gender, age, hair style, wearing, etc. It is a widely studied research problem due to its important role in human-related tasks, such as person re-identification [33], detection and tracking [32], text-based retrieval [25]. With the development of deep neural networks, the research on the PAR has been widely exploited using different neural networks [8,29,50,54] and training strategies [16,35]. However, the challenging factors still influence the final results significantly including illumination, background clutters, and motion blur.\nWith the aforementioned issues in mind, we first review existing PAR models and find that the mainstream neural networks like the CNN [19], RNN [9], Transformer [12, 48, 52, 55, 71] are widely utilized for this problem. To be specific, Wang et al. [51] propose JRL, a novel framework for pedestrian attribute recognition that employs LSTM [23] to jointly learn attribute context and correlations in a recurrent manner. Transformer networks, initially introduced for natural language processing tasks, they gain adoption within the computer vision community because of their remarkable performance [12, 48, 52, 55, 71]. Several studies explore the use of Transformers in the PAR domain to capture global contextual information [8,45]. For instance, DRFormer [45] models long-range relationships between regions and attributes, while VTB [8] integrates image and language information to achieve more accurate attribute recognition. Although these works improve the PAR performance significantly, however, the inference cost is still high in the testing stage.\nRecently, Spiking Neural Networks (SNN) have drawn more and more attention due to their advantages of lower energy consumption and bio-inspired network design. Various spiking neurons (LIF [14], ALIF [44]) are developed to replace the artificial neurons (e.g., ReLU) in MLP, CNN, or Transformer networks, thus, leading to spiking versions of these models. SNN has been widely used in object detection [4], recognition [28], tracking [3,61], image enhancement and reconstruction [13,78], but few efforts are conducted on the pedestrian attribute recognition. Consequently, it makes sense to ask the subsequent question \"How can we design an energy-efficient spiking backbone network for pedestrian attribute recognition?\"\nIn this work, we propose the first spiking Transformer networks for the PAR task, as shown in Fig. 2. Given the pedestrian image, we first adopt a spiking tokenizer module to get the spiking features, which contain Conv-BN-Multistep LIF-MaxPooling-Conv-BN layers. Here, the Conv and BN are short for Convolutional and Batch Normalization layers, respectively. The Multistep LIF spiking neuron is used as the activation function. The output features will be fed into a spiking Transformer block, each block contains a core self-attention operation and residual connections. We feed the spiking features into a set of FFN (Feed Forward Networks) for attribute prediction. The BCE (Binary Cross-Entropy) loss function is used for the optimization of the whole SNN-PAR framework. To improve the final recognition performance, we further introduce the knowledge distillation strategy to guide the optimization of the SNN-PAR network. In our implementation, the VTB [8] is selected as the teacher network for knowledge distillation. We conducted extensive experiments on three PAR benchmark datasets and these results fully validated the effectiveness of our SNN-PAR framework for pedestrian attribute recognition.\nIn conclusion, we highlight the contributions of this paper in the following three areas:\n1). We propose an energy-efficient spiking Transformer network for pedestrian attribute recognition, termed SNN-PAR. To the best of our knowledge, it is the first work that exploit the SNN for the PAR task."}, {"title": "2 Related Works", "content": "2.1 Spiking Neural Networks\nSpiking Neural Networks (SNNs), hailed as the third generation of neural networks, aim to emulate the complex information processing mechanisms of the human brain. Due to their low power consumption characteristics, an increasing number of studies [42,58,59,69] and innovative Spiking Transformer architectures [67,77] have emerged. Federico et al. [38] propose a hierarchical spiking architecture for optical flow estimation, leveraging selectivity to local and global motion through Spike-Timing-Dependent Plasticity (STDP) [7]. Zhou et al. [76] utilize non-leaky Integrate-and-Fire (IF) neurons with single-spike temporal coding to train deep SNNs. Additionally, Zhou et al. combine spiking neurons with Transformer networks to introduce the Spikformer [74] for recognition tasks. Fang et al. [17] introduces an innovative method for simultaneously learning synaptic weights and membrane time constants in SNNs. SNNs are developed using two primary training techniques: conversion from ANNs and direct training. The conversion method employs pulse frequencies to emulate ReLU activation, which aids in transforming pre-trained ANNs into SNNs [6,69]. Conversely, the direct training method applies alternative gradient approaches to optimize SNNs directly [69], allowing these networks to be trained on diverse datasets and achieve competitive results within a short number of time steps. This methodology has resulted in a broad range of applications for SNNs in visual tasks, including essential areas such as image recognition, natural language processing, and robotic control [37,73,79]. This underscores the capability of SNNs to perform computationally demanding tasks efficiently, presenting a sustainable option compared to traditional ANNs.In this study, we investigate pedestrian attribute recognition using SNNs and adopt the direct training strategy to construct our model.\n2.2 Pedestrian Attribute Recognition\nPedestrian attribute recognition [57] uses predefined images to predict pedestrian attributes through various model architectures, including CNNs [1, 70], RNNs [47,49], GNNs [31,39], and Transformers [8,16]. Early works rely on CNNS for attribute analysis. Specifically, Abdulnabi et al. [1] use CNNs for attribute analysis. They introduce a multi-task learning strategy that employs several CNNs to acquire attribute-specific features, allowing for knowledge sharing between the networks. Zhang et al. introduce the PANDA [70], which combines part-aware models with CNN-based attribute classification.\nRNNs are employed to model sequential dependencies in attributes. For example, Wang et al. [49] Use Long Short-Term Memory (LSTM) to develop robust semantic connections among labels in the context of pedestrian attribute recognition. By integrating labels that have been predicted earlier, the visual features can flexibly adjust to accommodate the subsequent labels. Zhao et al. propose the GRL [47] to model attribute dependencies, addressing both intra-group exclusions and inter-group associations. Graph Neural Networks (GNNs) are introduced to model semantic relations among attributes. VC-GCN [31] and A-AOG [39] depict attribute correlations using graphical models. Li et al. [31] approach pedestrian attribute recognition as a sequence prediction task, leveraging GNNs to represent spatial and semantic relationships. Recently, Transformer models, leveraging self-attention mechanisms, have gained prominence in the PAR task. Numerous works in PAR have also been developed utilizing the Transformer network. For instance, Fan et al. [16] present a model called PAR-former, which extracts features using Transformers instead of CNNs, effectively merging both global and local perspectives. VTB [8] integrating an additional text encoder to enhance pedestrian attribute recognition. Different from these works, this paper first exploits energy-efficient PAR using spiking Transformer networks.\n2.3 Knowledge Distillation\nKnowledge Distillation is a technique for model compression that facilitates a smaller student model in learning from a larger teacher model. The student acquires knowledge by imitating various aspects of the teacher, such as its intermediate features [41], prediction logits [22], or activation boundaries [20]]. This approach was originally put forth by Hinton et al. [21] to supervise students based on the hard and soft label's output by the teacher, and nowadays there is a lot of work on using distillation for knowledge transfer to help the model get better performance. Earlier knowledge distillation (KD) techniques can be classified into three distinct categories: distillation from logits, distillation from features, and distillation based on attention. In terms of logit distillation, DIST [24] employs the Pearson correlation coefficient in place of KL divergence, combining both inter- and intra-class relationships. SRRL [62] ensures that the logits output from the teacher and the features of the student, after the teacher's linear layer, are identical. WSLD [75] examines soft labels and assigns varying weights to them based on the bias-variance trade-off. In addition to logit distillation, Several studies [5, 43, 64, 66] concentrate on transferring knowledge through intermediate features. FitNet [41] directly distills semantic information from these intermediate features. AT [68] shifts the attention from feature maps to the student model. RKD [40] extracts relationships from the feature maps. MGD [65] masks the features of the student model, compelling it to replicate the features of the teacher model. To our knowledge, AT [68] is the sole knowledge distillation method that focuses on transferring attention, defining the attention map as a spatial representation that highlights the areas of the input that the model concentrates on the most. Wang et al. propose the HDETrack [56] which employs a hierarchical knowledge distillation strategy to augment the student tracking network from multi-modal or multi-view teacher network. In this paper, we employ both logits and intermediate features for knowledge distillation, believing that the integration of these two methods can greatly enhance knowledge transfer and improve the effectiveness of the student model."}, {"title": "3 Our Proposed Approach", "content": "3.1 Preliminaries: Leaky-Integrate-and-Fire (LIF) model\nThe Leaky Integrate-and-Fire (LIF) model is a fundamental framework for simulating neuronal dynamics, effectively capturing the essential characteristics of biological neurons, as shown in Fig. 1. The LIF model effectively simulates spiking neuron dynamics by integrating incoming signals while accounting for membrane leakage. Its simplicity and biological relevance establish it as a cornerstone in computational neuroscience, allowing researchers to investigate neuronal behavior and network dynamics. Additionally, its efficiency makes it suitable for applications in spiking neural networks, where energy consumption and computational resources are critical. This model comprises two key components: leaky integration and reset behavior. The main function of the Leaky Integration is represented by the following equation:\n$\\begin{equation}\nT_m \\frac{du}{dt} = -[u(t) - U_{rest}] + RI(t)\n\\end{equation}$\nHere, u(t) indicates the membrane potential of the neuron, urest refers to the resting membrane potential, R represents the membrane resistance, and I(t) signifies the input current. The term Tm is the time constant, determining how quickly the membrane potential responds to inputs. When the neuron receives synaptic inputs, the membrane potential increases, integrating these signals over time. Nonetheless, because of the leaky characteristics of the membrane, the potential diminishes over time, indicating a gradual charge loss. Once the membrane potential hits a threshold uth, the neuron generates an action potential, resulting in the reset of the membrane potential:\n$\\begin{equation}\nu(t_f + \\delta) = u_r, \\nu(t_f) \\geq u_{th}.\n\\end{equation}$\nHere, u(tf) is the membrane potential at the firing time, and ur is the reset potential. This reset behavior mimics the firing and refractory period of biological neurons, allowing the model to represent the spiking nature of neuronal activity accurately.\n3.2 Overview\nThis paper presents a new deep learning framework called SNN-PAR, as shown in Fig. 2. The core of this framework lies in harnessing the unique spatiotemporal dynamics and low-power characteristics of Spiking Neural Networks (SNNs) to achieve fine-grained image feature extraction while minimizing power consumption. SNNs simulate the information processing strategies of biological neural networks, enabling the efficient capture of key spatiotemporal features from images while maintaining energy efficiency. In addition, we use knowledge distillation techniques to improve the efficiency of feature learning and transferability. In this process, a pre-trained teacher model is employed to guide the learning of the SNN. The teacher model typically offers rich knowledge of image features. By transferring abstract high-level information from the teacher model to the SNN, our approach enhances the richness of the feature content while also bolstering their resilience. This is particularly important for dealing with complex and variable environmental conditions and noise interference.\n3.3 Network Architecture\nIn this section, we will focus on the network architectures of our proposed SNN-PAR framework, including the Teacher and Student PAR module, Knowledge Distillation Enhanced Learning, and the loss functions used for the optimization. more details of these modules will be introduced in the subsequent paragraphs.\n\u2022 Teacher PAR Module. Current pedestrian attribute recognition models typically utilize Convolutional Neural Networks (CNNs) as the backbone, achieving remarkable performance. We adopt the VTB model [8] as our teacher model, which excels in extracting image features, providing more comprehensive and precise knowledge to the student model. This enables the student model to improve its accuracy in attribute prediction.\nGiven a pedestrian image $X \\in R^{C \\times H \\times W}$ and the corresponding annotated list of attributes $A = \\{a_1, a_2,..., a_M\\}$, the image is first partitioned into multiple patches via a projection layer. These patches are subsequently encoded by the visual encoder from the teacher model, yielding visual features $F_t$. Simultaneously, the attribute set $A$ is expanded into sentence form and processed by the text encoder to generate textual features $F_t$. VTB [8] then introduces the Multi-modal Transform Fusion Module, which is specifically designed to effectively aggregate both text and visual features. This module employs advanced techniques to seamlessly integrate and enhance multimodal representations, enabling more comprehensive and precise feature fusion, which in turn improves model performance. The fused representation is then passed through a feed-forward network to produce the attribute prediction results $P_t$. An overview of the Spikingformer pipeline is shown in Fig. 2.\n\u2022 Student PAR Module. Spiking Neural Networks (SNNs) have gained significant attention in recent years owing to their biological relevance and exceptional energy efficiency, demonstrating robust computational capabilities in processing complex temporal information. Despite these advantages, the application of SNNs to pedestrian attribute recognition remains in early stages of exploration. To advance research in this area, we adopt the Spikingformer model [74] as the student model to further investigate the specific use of SNNs for pedestrian attribute recognition. This work aims to provide deeper theoretical insights and practical guidance, contributing to the advancement and maturity of SNN applications in pedestrian attribute recognition.\nThe design of the student model, Spikingformer, comprises a Spiking Tokenizer (ST), a series of Spiking Transformer Blocks, and a Classification Head. input 2D image $I \\in R^{C \\times H \\times W}$, here C, H, W denotes the channel, height, and width of the image, respectively, the Spiking Tokenizer is applied for patch embedding. Specifically, the first layer serves as a spike encoder when processing the static images. As shown in Eq. 3 and Eq. 4, the convolutional component of ConvBN refers to a 2D convolution layer, while MP and SN denote max pooling and multi-step spiking neurons, respectively. Spiking Patch Embedding (SPE) without downsampling is based on Eq. 3, and Spiking Patch Embedding with Downsampling (SPED) utilizes Eq. 4. Ultimately, the input I divided into a sequence of image patches $X \\in R^{N \\times D}$, represented as:\n$\\begin{equation}\nX = ConvBN(SN(I))\n\\end{equation}$\n$\\begin{equation}\nX = ConvBN(MP(SN(I)))\n\\end{equation}$\nAfter passing through the Spiking Tokenizer, the spiking patches X are processed by L Spiking Transformer Blocks. In a manner similar to the standard ViT encoder block, each Spiking Transformer Block includes a Spiking Self-Attention (SSA) mechanism along with a Spiking MLP block. The SSA mechanism draws inspiration from the pure spiking self-attention design in Spikingformer. In Spikingformer [74], the spike-driven residual mechanism enhances SSA by repositioning the spiking neuron layer to avoid the multiplication of integer and floating-point weights, while replacing the LinearBN structure from Spikformer with ConvBN. Consequently, the Spiking Self-Attention (SSA) mechanism is defined as follows:\n$\\begin{equation}\nX' = SN(X),\n\\end{equation}$\n$\\begin{equation}\nQ = SN_Q(ConvBN_Q(X')),\n\\end{equation}$\n$\\begin{equation}\nK = SN_K(ConvBN_K(X')),\n\\end{equation}$\n$\\begin{equation}\nV = SN_V(ConvBN_V(X')),\n\\end{equation}$\n$\\begin{equation}\nSSA(Q, K, V) = ConvBN(SB(QKT^V * s)),\n\\end{equation}$\nwhere Q, K, and V denote pure spike data, consisting exclusively of binary values (0, 1). The scaling factors s, as described in [77], modulates the magnitude of the matrix multiplication outputs. The Spiking MLP block integrates two Spiking Perceptron Ensembles (SPEs), as outlined in Eq. 3. The spiking Transformer block serves as a fundamental component of the Spikingformer architecture. We employ a fully connected layer as the classifier after the last Spiking Transformer module. The visual features Fo output from the Spiking Transformer Block is processed through the Classification Head to obtain the final prediction results Ps. Finally, We adopt a weighted binary cross-entropy loss function to alleviate the distribution imbalance among pedestrian attributes, which is commonly employed in optimization for attribute recognition models. As a result, the for-mulation of our model's classification head is as follows:\n$\\begin{equation}\nY = FC(AvgPooling(X_L)).\n\\end{equation}$\n\u2022 Knowledge Distillation Enhanced Learning. In this study, we adopt a dual-level knowledge distillation strategy, conducting learning at both the feature and response levels. This hierarchical knowledge transfer framework ensures that the student model effectively assimilates knowledge from the teacher model, thereby enhancing its predictive performance. At the feature level, we focus on aligning the feature representations between the teacher visual features $F_t$ and the student visual features $F'_t$, carefully tuning the spiking activity of the SNN to mimic the high-level features extracted by the teacher model in intermediate layers. At the response level, we focus on achieving the alignment of the predictions $P_t$ of the teacher model and the predictions $P_s$ of the student model. By transferring classification decision information from the teacher model to the student model, we ensure the accuracy of the SNN in its final predictions. This two-tier distillation approach not only enables the student model to inherit the teacher model's strengths in feature extraction and decision-making, but also improves the model's generalization and robustness, all while maintaining low computational complexity. Additionally, this distillation method effectively mitigates issues such as gradient vanishing and overfitting, which are common in traditional SNN training, thus rendering the model more robust in processing real-world image data.\n3.4 Loss Function\nIn this study, we use a combination of cross-entropy loss $L_{CE}$ and distillation loss functions $C_{respKD}, L_{featKD}$ to optimize the SNN-PAR framework. $L_{CE}(S, y)$ is the cross-entropy loss between the output S of the student model and the labels y, which is typically defined as:\n$\\begin{equation}\nL_{CE}(S,Y) = \\sum_i  y_i log(S_i).\n\\end{equation}$\n$L_{respKD}(S,T)$ is the response distillation loss between the output from the student model S and the output from the teacher model T, typically the Kullback-Leibler divergence, defined as:\n$\\begin{equation}\nL_{respKD}(S,T) = D_{KL}(T||S) = \\sum_i T_i(log(T_i) - log(S_i)).\n\\end{equation}$\nIn this formula, $T_i$ and $S_i$ represent the ith element of the output from the teacher model and the student model, typically obtained as probability distributions through the Softmax function. $L_{featKD}$ is the feature distillation loss, defined as:\n$\\begin{equation}\nL_{featKD} = \\sum_i P(sim(F_t, F'_v)); log \\frac{P(sim(F_t, F'_v))_i}{\\sum_k P(sim(F_t, F'_v))_k}.\n\\end{equation}$\nAmong them, $F_t$ is the text feature of the teacher model, $F_v$ is the visual feature of the teacher model, and $F'_v$ is the visual feature of the student model. sim is the cosine similarity between two features.\nBased on the above loss functions, the distillation loss is attached with weight coefficients \u03b1 and \u03b2, respectively, along with the temperature coefficient T, which together form the final loss function:\n$\\begin{equation}\nL = L_{CE} + \\alpha L_{respKD} + \\beta L_{featKD}.\n\\end{equation}$"}, {"title": "4 Experiments", "content": "4.1 Datasets and Evaluation Metric\nTo evaluate the effectiveness of our proposed SNN-PAR model, we perform experiments on three publicly accessible datasets: PETA [10], PA100K [34] and RAPv1 [30].\n\u2022 PETA comprises 19,000 images of pedestrians in outdoor or indoor settings, along with 61 binary attributes. These images are divided into training, validation, and testing subsets, containing 9,500, 1,900, and 7,600 images, respectively. In line with previous studies, we choose 35 pedestrian attributes for our experiments.\n\u2022 PA100K is the most extensive for pedestrian attribute recognition, encompassing 100,000 pedestrian images with 26 binary attributes. Note that, 90,000 of these images are designated for training and validation purposes, while a separate set of 10,000 images is reserved for testing.\n\u2022 RAPv1 comprises 41,585 pedestrian images and 69 binary attributes, 33,268 images are designated for training. Typically, 51 attributes are selected for both training and evaluation purposes.\nIn our experiments, we utilize five widely recognized evaluation metrics to measure performance: mean Accuracy (mA), Accuracy (Acc), Precision (Prec), Recall and F1-score (F1), these metrics are defined as follows:\n$\\begin{equation}\nAccuracy = \\frac{TP+TN}{FP+FN+TP+TN}\n\\end{equation}$\n$\\begin{equation}\nPrecision = \\frac{TP}{FP+TP}, Recall = \\frac{TP}{TP+FN}\n\\end{equation}$\n$\\begin{equation}\nF1_{score} = \\frac{2 \\times Recall \\times Precision}{Recall + Precision}\n\\end{equation}$\nWhere TP denotes the number of samples that were correctly predicted as positive (true positives), TN represents the count of samples accurately identified as negative (true negatives). Additionally, FP refers to the number of false positives, which are samples incorrectly predicted as positive, while FN indicates the number of false negatives, or samples that were incorrectly classified as negative.\n4.2 Implementation Details\nIn the training stage, we utilize a batch size of 12 and proceed to train the model over a complete duration of 60 epochs. In the teacher model, the input to the visual encoder of the student model and teacher model is set to 256 \u00d7 128. This configuration is consistently applied across experiments on the RAPv1, PETA, and PA100K datasets. The initial learning rate is set at 8e-4, with a decay rate of le-4 as training progresses. We use the Adam optimizer [11] for our experiments. To optimize the learning process, we implement a warm-up strategy, gradually increasing the learning rate from 0 to an initial value of 1e-3 over the first 10 epochs. As the iteration count mounts, we decrease the learning rate by a multiplicative factor of 0.1. Knowledge distillation is performed with a temperature coefficient set to 2. Further details are available in our source code.\n4.3 Comparison on Public Benchmarks\nWe evaluate the proposed SNN-PAR model against state-of-the-art methods on three benchmark datasets: PA100K [34], RAPv1 [30], and PETA [10]. These datasets, renowned for their diverse pedestrian attributes and challenging scenarios, provide a robust foundation for assessing the effectiveness of attribute recognition models under real-world conditions.\nThe results on three public datasets are provided in Table 1. For the PA100K [34] dataset, our proposed SNN-PAR model achieves 73.86 in mA, 71.70 in Accuracy, 83.03 in Precision, 81.30 in Recall, and 81.67 in Fl-score. On the RAPv1 [30] dataset, it records 75.43 for mA, 63.06 for Accuracy, 74.67 for Precision, 78.28 for Recall, and 75.94 for F1-score. Lastly, the evaluation on the PETA [10] dataset results in 80.58, 73.55, 81.76, 82.79, and 81.96 for mA, Accuracy, Precision, Recall, and F1-score, respectively.\nAs shown in Table 1, while most prior methods outperform our SNN-PAR framework by approximately 4 to 5 points, this is expected given the superior performance of ANN architectures. In contrast, our model prioritizes balancing accuracy with energy efficiency, achieving comparable performance while consuming significantly less energy.\n4.4 Ablation Studies\n\u2022 Baseline Comparison: SNN vs. Transformer-based PAR Model. To assess the efficiency and effectiveness of the proposed SNN-PAR model, we perform a comparative analysis by substituting the SNN module in the student model with a Transformer (ViT) module. This comparison is designed to highlight the benefits of employing Spiking Neural Networks (SNNs) over conventional Transformer architectures, focusing on both energy efficiency and attribute recognition accuracy. SNN-PAR: Our proposed model incorporates the SNN module in the student branch. Transformer-based model: A variant of the SNN-PAR model, where the SNN module in the student branch is replaced with a Transformer (ViT) module. Both models are trained on the PA100K pedestrian attribute recognition dataset to ensure a fair and consistent evaluation. Table 2 presents the performance comparison in terms of Acc, mA, Prec, Rec, and F1.\nAs presented in Table 2, the SNN-PAR model achieves scores of 73.86, 71.70, 83.03, 81.30, and 81.67 for mA, Accuracy, Precision, Recall, and F1, respectively. In comparison, the Transformer-based model attains higher values of 80.33, 78.24, 86.48, 87.48, and 86.49 across the same metrics. While it is expected that the ViT model exhibits superior performance due to its more complex architecture and higher energy consumption, our SNN-PAR model strikes a balance by sacrificing a small degree of accuracy in favor of greater energy efficiency and a more lightweight network architecture.\n\u2022 Effects of Knowledge Distillation. At this stage, our objective is to refine the student model's acquisition of knowledge from the teacher model by employing knowledge distillation techniques, incorporating both feature-level and response-level distillation. We conduct experiments using each distillation method independently to assess their effectiveness. The results of these experiments are presented in the following section.\nSNN with feature-level distillation: We also conduct an experiment to evaluate the effectiveness of feature-level distillation (second row). As illustrated in Table 3, compared to the original SNN model (first row), the SNN model with the additional feature-level distillation strategy achieves improvements of +1.24 in mA and +0.67 in Accuracy.\nSNN with response-level distillation: As shown in Table 3, we initially apply only response-level distillation to validate its effectiveness (third row). Notably, the mA and F1 scores improve by +0.41,+0.85, respectively, compared to the baseline SNN model (first row), highlighting the importance of the proposed response-level distillation.\nSNN with response and feature Distillation: To further improve the performance of our student model, we combine the aforementioned distillation strategies. As shown in Table 3, the SNN model with both response-level and feature-level distillation (fourth row) achieves the highest performance, with scores of 74.76 in mA and 83.16 in F1. This demonstrates the effectiveness of integrating these two levels of distillation.\n4.5 Parameter Analysis\nIn this section, we present the key parameters of our SNN. Specifically, we report the total number of learnable parameters in the model. As illustrated in Table 4, Comparison with 157.54M learnable parameters for ViT-B/16 and 87.59M learnable parameters for Swin-B, our SNN-PAR model contains only 65.59M learnable parameters, making the overall network significantly more lightweight. In particular, the number of parameters in our method is more than halved compared to the ViT-B/16 method.\n4.6 Visualization\nIn this section, we present a case study highlighting successfully predicted attributes on the PA100K dataset. To offer a clearer insight into the prediction process of our model, we also include heatmap visualizations that illustrate the predicted regions of interest.\n\u2022 Attributes Predicted using Our Student Model. As depicted in Fig. 3, we showcase 10 predictions generated by our model on the PA100K dataset. It is clear that the model effectively identifies a range of pedestrian attributes, including gender, age, motion, and outfit, among others.\n\u2022 Heatmap Visualization. To deliver a more straightforward visualization of the key areas attended to by our model on the PA100K dataset when predicting pedestrian attributes, we visualize the model's prediction process using heatmaps. As shown in Fig. 4, the model accurately focuses on the relevant regions corresponding to the pedestrian attributes during prediction.\n4.7 Limitation Analysis\nAs shown in Fig. 4, while our model focuses on broad regions within pedestrian images, such as those corresponding to motion and outfit, the localization is not sufficiently precise. Moreover, as reflected in Table 1, the performance of our SNN-PAR model, while more energy-efficient and lightweight, falls short in accuracy compared to other state-of-the-art methods. In our future work, we plan to explore the design of a hybrid ANN-SNN architecture to strike a better balance between accuracy and energy efficiency."}, {"title": "5 Conclusion", "content": "In this paper, we propose a novel SNN-based pedestrian attribute recognition framework, termed SNN-PAR, leveraging Spiking Neural Networks (SNNs) to achieve more energy-efficient performance. However, general SNN-based models often struggle to deliver high accuracy. To strike a balance between accuracy and energy efficiency, we employ a teacher-student model to train our SNN student model. We incorporate two levels of distillation-response-level and feature-level, which significantly enhance the attribute recognition accuracy of our SNN-PAR model. While our framework performs well on three benchmark datasets, there remains a notable gap compared to methods using purely ANN architectures. In future research, we intend to investigate more efficient frameworks to enhance the performance of our model further."}]}