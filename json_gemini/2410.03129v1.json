{"title": "ARB-LLM: ALTERNATING REFINED BINARIZATIONS FOR LARGE LANGUAGE MODELS", "authors": ["Zhiteng Li", "Xianglong Yan", "Tianao Zhang", "Haotong Qin", "Dong Xie", "Jiang Tian", "Zhongchao Shi", "Linghe Kong", "Yulun Zhang", "Xiaokang Yang"], "abstract": "Large Language Models (LLMs) have greatly pushed forward advancements in natural language processing, yet their high memory and computational demands hinder practical deployment. Binarization, as an effective compression technique, can shrink model weights to just 1 bit, significantly reducing the high demands on computation and memory. However, current binarization methods struggle to narrow the distribution gap between binarized and full-precision weights, while also overlooking the column deviation in LLM weight distribution. To tackle these issues, we propose ARB-LLM, a novel 1-bit post-training quantization (PTQ) technique tailored for LLMs. To narrow the distribution shift between binarized and full-precision weights, we first design an alternating refined binarization (ARB) algorithm to progressively update the binarization parameters, which significantly reduces the quantization error. Moreover, considering the pivot role of calibration data and the column deviation in LLM weights, we further extend ARB to ARB-X and ARB-RC. In addition, we refine the weight partition strategy with column-group bitmap (CGB), which further enhance performance. Equipping ARB-X and ARB-RC with CGB, we obtain ARB-LLMx and ARB-LLMRC respectively, which significantly outperform state-of-the-art (SOTA) binarization methods for LLMs. As a binary PTQ method, our ARB-LLMRC is the first to surpass FP16 models of the same size. The code and models will be available at https://github.com/ZHITENGLI/ARB-LLM.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, Transformer-based (Vaswani, 2017) large language models have shown impressive performance across various natural language processing tasks. However, this unprecedented capability is largely attributed to the sheer scale of these models, which often encompass billions of parameters. For instance, open pre-trained Transformer (OPT) series (Zhang et al., 2022) includes various models, with the largest boasting 66B parameters. Similarly, the LLaMA family (Touvron et al., 2023) features variants such as LLaMA3-70B, showcasing even larger architectures. The substantial memory requirements for inference in such large models (e.g., 150 GB memory for a 70B model) pose significant challenges for their deployment on mobile devices.\nThe study of compressing LLMs can be categorized into weight quantization (Lin et al., 2024; Frantar et al., 2023), low-rank factorization (Zhang et al., 2024; Yuan et al., 2023), network pruning (Sun et al., 2024; Frantar & Alistarh, 2023), and knowledge distillation (Zhong et al., 2024; Gu et al., 2024). Among these, binarization, a specific technique within the realm of quantization, is particularly distinguished for its ability to achieve extreme memory compression, reducing storage requirements to as low as 1 bit. Given the substantial size of LLMs, some binarization methods adopt the post-training quantization (PTQ) framework to enable a rapid transition from full-precision models to compact binarized versions, requiring minimal resources (e.g., binarizing a 70B model in one 80 GB GPU)."}, {"title": "2 RELATED WORKS", "content": "Current quantization techniques for large language models mainly fall into quantization-aware training (QAT) and post-training quantization (PTQ) frameworks."}, {"title": "2.1 NETWORK BINARIZATION", "content": "Network binarization compresses the parameters to only 1 bit (\u00b11) by using the sign function. Then, the straight through estimator (STE) (Bengio et al., 2013) is used to tackle the gradient vanishing during back-propagation if training a binary network. Binary weight network (BWN) (Rastegari et al., 2016) implemented binarization on weights while maintaining full-precision activations. XNOR-Net (Rastegari et al., 2016) extended this by binarizing weights and activations. They both focus on standard first-order binarization and employ a scaling factor a to reduce quantization error. Network sketching (Guo et al., 2017) extended the first-order binarization by proposing a binary coding quantization (BCQ) to approximate the full-precision weights with multiple binary matrices. Xu et al. (2018) improved BCQ by using a binary search tree to determine the optimal code. However, both methods are tailored for scenarios involving multiple binarizations and are not applicable to standard first-order binarization processes. In another direction, OneBit (Xu et al., 2024) extended the scaling factor to both weights and activations. BinaryMoS (Jo et al., 2024) introduced several scaling experts to improve the performance. Nevertheless, training these models requires substantial resources. For example, training OneBit on LLaMA-7B takes 7 days using 8 A100-80GB GPUs."}, {"title": "2.2 LARGE LANGUAGE MODEL QUANTIZATION", "content": "Current quantization techniques for large language models mainly fall into quantization-aware training (QAT) and post-training quantization (PTQ) frameworks."}, {"title": "Quantization-Aware Training (QAT)", "content": "QAT integrates quantization into the training process, enabling the model to adapt to low-bit representations. Recent works have successfully applied QAT to LLMs. LLM-QAT (Liu et al., 2024) addressed data barrier issues in QAT training by adding data-free distillation. EfficientQAT (Chen et al., 2024) proposed an optimized QAT framework with two stages (i.e., Block-AP and E2E-QP) to reduce QAT's memory and computational overhead for LLMs. However, QAT still requires considerable computational resources, including significant GPU memory and more training time. Therefore, LLM quantization techniques such as QLoRA (Dettmers et al., 2024a) focused on parameter-efficient fine-tuning methods, which enhanced the efficiency of QAT. Nevertheless, the efficiency of the LLM quantization methods remained unsatisfactory."}, {"title": "Post-Training Quantization (PTQ)", "content": "PTQ applied quantization directly to the existing model weights instead of retraining it. Therefore, it is significantly faster and more resource-efficient than QAT. Recent studies have effectively deployed PTQ in LLMs. RTN rounds weights to the nearest quantization level in order to ensure efficient runtimes when quantizing LLMs. Works like ZerqQuant (Yao et al., 2022) and BRECQ (Li et al., 2021) enhanced quantization accuracy by adding additional grouping labels for custom quantization blocks. GPTQ (Frantar et al., 2023) utilized layer-wise quantization and reduced the quantization error by second-order error compensation. Moreover, PB-LLM (Shang et al., 2024), SpQR (Dettmers et al., 2024b), and BiLLM (Huang et al., 2024) implemented a hybrid approach by selectively quantizing salient weights with low bits while binarizing non-salient weights. In addition, Smoothquant (Xiao et al., 2023) proposed a strategy of scaling weight and activation outliers, which simplified quantization. Thereafter, AWQ (Lin et al., 2024) and OWQ (Lee et al., 2024) also proposed scale transformations of salient weights for activation features to preserve their model capacity. Our work belongs to the category of binary PTQ, achieving a significant improvement over the SOTA method BiLLM."}, {"title": "3 METHOD", "content": "Overview. As shown in Figure 4, to progressively align the distribution between binarized and full-precision weights in LLMs, we first propose a framework Alternating Refined Binarization (ARB) in Section 3.1. Based on ARB framework, we propose the Alternating Refined Binarization with calibration data (ARB-X) to enhance the usage of the calibration set, which is crucial for binary LLMs. Additionally, we introduce the Alternating Refined Binarization along row-column axes (ARB-RC) to address the column deviation challenge in LLM weights. These methods are detailed in Sections 3.2 and 3.3, respectively. Finally, we discuss our refined strategy to combine salient column bitmap and group bitmap (CGB) in Section 3.4. Our final models, ARB-LLMx and ARB-LLMRC, are obtained by equipping ARB-X and ARB-RC with CGB respectively."}, {"title": "3.1 ALTERNATING REFINED BINARIZATION (ARB)", "content": "We begin by discussing standard weight binarization in LLMs. For a full-precision weight $W \\in \\mathbb{R}^{n\\times m}$, we define the objective of binarization as (with dimension broadcasting omitted for simplicity)\n$\\arg \\min_{\\alpha,B} ||W - \\alpha B||_F^2$, where $\\overline{W} = W - \\mu$, $\\mu = \\frac{1}{m} \\sum_{j=1}^{m} W_{:,j}$,\nwhere $\\alpha \\in \\mathbb{R}^n$ denotes the row-wise scaling factor, and $B \\in \\{+1, -1\\}^{n\\times m}$ is a binary matrix.\nSince the mean of $\\overline{W}$ is not necessarily zero, a common practice is to apply a row-wise redistribution before binarization. After redistribution, the weights achieve a row-wise zero-mean distribution, which facilitates the binarization process. Under the objective of binarization (Equation (1)), the optimal solutions for $\\alpha$ and B can be solved with $\\alpha = \\frac{1}{m} \\sum_{j=1}^{m} |W_{:,j}|$ and $B = sign(\\overline{W})$ respectively. Then we can define the quantization error $L_1$ after binarization as\n$L_1 = ||\\overline{W} - \\hat{W}||_F^2$, where $\\hat{W} = \\alpha B + \\mu$.\nMoving forward, we aim to investigate how to reduce the quantization error $L_1$. We first define the residual matrix as $R = \\overline{W} - \\hat{W}$. In analyzing the residual matrix R, we observe a distribution shift in R, where the mean of R is not always zero due to inevitable errors during the binarization process (see Figure 2). To address this, we introduce a correction term $\\delta \\mu$ to the original mean $\\mu$, effectively mitigating the distribution shift. The refined mean is defined as follows:\n$\\mu_{refine} = \\mu + \\delta \\mu$, where $\\delta \\mu = \\frac{1}{m} \\sum_{j=1}^{m} R_{:,j}$.\nThis is equivalent to taking the partial derivative of $L_1$ with respect to $\\mu$ and setting it to 0, as shown in Figure 4. Since $\\mu$ has been updated to $\\mu_{refine}$, the original $\\alpha$ and B are no longer optimal solutions for quantization error $L_1$. To further minimize the quantization error, the optimal solutions for $\\alpha_{refine}$ and $B_{refine}$ can be obtained by setting $\\partial L_1/\\partial \\alpha = 0$, leading to the following expressions:\n$\\alpha_{refine} = \\frac{1}{m}diag(B^T(\\overline{W} - \\mu_{refine})), B_{refine} = sign(\\overline{W} - \\mu_{refine})$.\nAfter refining $\\mu$, $\\alpha$, and B, we can obtain the $\\hat{W}_{refine}$ as $\\hat{W}_{refine} = \\alpha_{refine} \\cdot B_{refine} + \\mu_{refine}$. We find that this parameter update strategy can be extended to an iterative algorithm.\nIn each iteration, we sequentially update $\\mu$, $\\alpha$, and B to ensure they are the optimal solutions under the current quantization error $L_1$. The pseudocode is shown in Algorithm 1, which extends ARB with group mask (a bitmap detailed in Section 3.4). Moreover, we theoretically analyze the quantization error during the ARB process and derive a specific value for the reduced quantization error after T iterations, as stated in Theorem 1. The proof is provided in supplementary file."}, {"title": "3.2 ARB WITH CALIBRATION DATA (ARB-X)", "content": "Although the ARB algorithm can effectively reduce the quantization error $L_1$, we observe that the weight matrix $W$ operates in conjunction with the input data to produce the output. It means that $L_1$ alone does not fully capture the true impact of quantization. To address this issue, we introduce calibration data X and define a new quantization error $L_2$ as $L_2 = ||WX - \\hat{W}X||_F^2$. Based on $L_2$ and the ARB algorithm, we propose an extended algorithm, naming ARB-X.\nReformulation. However, incorporating calibration data necessitates a large number of matrix multiplications when computing $L_2$, substantially increasing computational overhead, and often making the combination of calibration data impractical. To address this issue, we reformulate the error computation by decoupling the calibration data and weight matrix as:\n$L_2 = (S, R^T R)_F = Tr(RSR^T)$, where $S = \\sum_{b} X_b X_b^T, R = \\overline{W} - \\mu - \\alpha B$.\n$X \\in \\mathbb{R}^{B\\times L\\times m}$ denotes the calibration data with batch size B, sequence length L, and embedding dimension m. By compressing the high-dimensional tensor X into a 2D matrix $S \\in \\mathbb{R}^{m\\times m}$ and precomputing it, we can significantly reduce the computational overhead. To quantify the efficiency"}, {"title": "3.3 ARB ALONG ROW-COLUMN AXES (ARB-RC)", "content": "Previous binarization methods use a row-wise scaling factor $\\alpha^r$ for weight binarization. However, our analyses of the numerical distribution of the weight matrix W in LLMs reveal significant deviations across columns, with some columns exhibiting notably larger values (Figure 3). As a result, using a single row-wise scaling factor may not effectively capture the distribution characteristics of LLM parameters. Additionally, the weight distribution shows a mean close to zero, making the redistribution to zero-mean less effective in LLM binarization.\nTo address this, we propose the ARB-RC algorithm, which introduces a column-wise scaling factor $\\alpha^c$ to better handle parameter variations across columns, while eliminating the redistribution parameter $\\mu$ to enhance compression in LLMs. The row-column binarization process is performed as follows:\n$\\alpha_i^r = \\frac{1}{m} \\sum_{j=1}^{m} W_{i,j}$, $\\alpha_j^c = \\frac{1}{n} \\sum_{i=1}^{n} W_{i,j}$, $B = sign(\\overline{W})$.\nThen, we can obtain the binarized matrix as $\\hat{W} = \\alpha^r \\alpha^c B$, where removing $\\mu$ while introducing $\\alpha^c$ reduces parameters but improves model performance. However, introducing $\\alpha^c$ without adopting an alternating parameter update strategy fails to improve performance and can even increase quantization error. Thus, it is necessary to combine $\\alpha^c$ with the discussed ARB algorithm. In this approach, we optimize the parameters using the quantization error $L_1$. Although the quantization error $L_2$ is more aligned with real-world conditions, our analysis shows that incorporating X in the ARB-RC method results in parameter coupling, making optimization difficult (detailed in supplementary file). Thus, based on $L_1$, we can update $\\alpha^r$ and $\\alpha^c$ by setting $\\partial L_1/\\partial \\alpha^r = 0$ and $\\partial L_1/\\partial \\alpha^c = 0$ respectively:\n$\\alpha^r = \\frac{diag(W(B \\odot \\alpha^c)^T)}{diag((\\alpha^c \\odot B)(\\alpha^c \\odot B)^T)}, \\alpha^c = \\frac{diag(W^T(\\alpha^r \\odot B))}{diag((\\alpha^r \\odot B)(\\alpha^r \\odot B)^T)}$.\nThe first-order and second-order pseudocodes of the ARB-RC are provided in supplementary file."}, {"title": "3.4 COLUMN-GROUP BITMAP (CGB)", "content": "Inspired by BiLLM (Huang et al., 2024), we partition the entire set of weights into salient and non-salient columns, and apply higher-bit representation, i.e., second-order binarization, to the salient weights. However, different from BiLLM, we not only divide the non-salient weights into sparse and concentrated groups but also divide salient weights in a similar manner. This approach allows for more efficient use of both column bitmap and group bitmap, as shown in Figure 5.\nTo identify the sensitivity of weights, i.e., salient weights, we follow well-established PTQ methods by utilizing the Hessian matrix as a standard criterion. The sensitivity is computed as $s_i = w_i^2/[H^{-1}]_{ii}$"}, {"title": "4 EXPERIMENTS", "content": "All the experiments are conducted with PyTorch (Paszke et al., 2019b) and Huggingface (Paszke et al., 2019a) on a single NVIDIA A800-80GB GPU. We implement 15 iterations for ARB-LLMx and ARB-LLMRC to ensure the convergence of binarization parameters. Following Frantar et al. (2023) and Huang et al. (2024), we use 128 samples from C4 (Raffel et al., 2020) dataset as calibration data.\nModels and Datasets. We conduct extensive experiments on the LLaMA, LLaMA-2, and LLaMA-3 families (Touvron et al., 2023), the OPT family (Zhang et al., 2022), and instruction-tuned LLMs Vicuna (Chiang et al., 2023). To evaluate the effectiveness of our proposed ARB-LLMx (ARB-X + CGB) and ARB-LLMRC (ARB-RC + CGB), we measure the perplexity of LLM's outputs on WikiText2 (Merity et al., 2017), PTB (Marcus et al., 1994), as well as a part of the C4 (Raffel et al., 2020) data. Moreover, we also evaluate the accuracy for 7 zero-shot QA datasets: ARC-c (Clark et al., 2018), ARC-e (Clark et al., 2018), BoolQ (Clark et al., 2019), Hellaswag (Zellers et al., 2019), OBQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), and Winogrande (Sakaguchi et al., 2020)."}, {"title": "4.1 SETUP", "content": "All the experiments are conducted with PyTorch (Paszke et al., 2019b) and Huggingface (Paszke et al., 2019a) on a single NVIDIA A800-80GB GPU. We implement 15 iterations for ARB-LLMx and ARB-LLMRC to ensure the convergence of binarization parameters. Following Frantar et al. (2023) and Huang et al. (2024), we use 128 samples from C4 (Raffel et al., 2020) dataset as calibration data."}, {"title": "4.2 MAIN RESULTS", "content": "We follow BiLLM to report the average bit-width of all methods, where our methods have the same bit-width as BiLLM. Table 1 presents the perplexity comparison of the OPT family across different model sizes. It can be observed that both ARB-LLMx and ARB-LLMRC significantly outperform SOTA BiLLM, and reduce the perplexity by up to 68.7% without increasing weight bit-width. Table 2 presents the perplexity comparison on LLaMA1&2&3 families, which also suggests the superior performance of our ARB-LLM. It is noteworthy that ARB-LLMRC outperforms RTN with 3-bit quantization on some models, such as the LLaMA1&3 families, LlaMA2-70B model, as well as OPT family. Similarly, ARB-LLMRC"}, {"title": "4.3 ABLATION STUDY", "content": "To validate the effectiveness of our advanced variants ARB-LLMx and ARB-LLMRC, we compare them with the vanilla ARB algorithm in Table 4a. First, we observe that the vanilla ARB already significantly outperforms BiLLM. Furthermore, by introducing either the calibration update or the row-column update to the binarization process, performance is further improved. This demonstrates that our advanced variants, ARB-LLMx and ARB-LLMRC, can further enhance the performance of binary LLMs based on ARB.\nTo demonstrate the effectiveness of our column-group bitmap (CGB), we conduct an ablation study in Table 4b. In this study, the absence of CGB does not imply the exclusion of partitioning but rather the use of the partitioning strategy used by BiLLM. The results show that CGB further enhances the performance of both ARB-LLMx and ARB-LLMRC. Notably, even when using BiLLM's partitioning strategy, our methods significantly outperform BiLLM.\nWe use a column bitmap to differentiate between salient and non-salient weights, and a group bitmap to separate weights based on their magnitude. The combination of column and group bitmaps creates four distinct zones. As shown in Table 4c, we explore the effect of decoupling this combination by using either the column bitmap or the group bitmap individually. It is evident that using the column bitmap or group bitmap only will result in a significant performance drop. Omitting both column bitmap and group bitmap entirely (i.e., #group=1), which reduces the method to naive binarization, leads to complete failure."}, {"title": "Calibration Set Size.", "content": "Similar to other PTQ methods, our ARB-LLM requires a small calibration set of just 128 samples. We further incorporate the calibration data into the update of binarization parameters in ARB-LLMx. To explore the effect of calibration set size on performance, we compare results using different set sizes, as shown in Table 4d. It can be observed that using fewer calibration samples (e.g., 64) results in a performance drop, while increasing the calibration set size from 128 to 256 yields similar results. This indicates that our ARB-LLMx requires only a small calibration set. Even with just 64 samples, ARB-LLMx significantly outperforms the baseline BiLLM."}, {"title": "ARB Iteration Number.", "content": "We use 15 iterations for the main results (Table 1, Table 2, Table 3, and Figure 6), as all parameters have fully converged. To explore the impact of different iteration numbers, we compare results using 1, 3, and 15 iterations in Table 4e. As can be seen, regardless of the iteration number, the perplexity of ARB-LLMx and ARB-LLMRC significantly outperforms the baseline BiLLM. Increasing the iteration number further reduces perplexity, yet they can achieve superior results even with just one iteration. Additionally, we visualize the changes in the scaling factor \u03b1 throughout the alternating iterations to provide further insights in supplementary file."}, {"title": "Group Number.", "content": "Following BiLLM (Huang et al., 2024), we introduce an additional bitmap for grouping weights, which has been demonstrated to enhance performance. To explore the impact of group size, we expand the group bitmap from a 1-bit to a 2-bit system, increasing the number of groups from 2 to 4. As shown in Table 4f, increasing the number of groups leads to better performance, especially for ARB-LLMx, which outperforms ARB-LLMRC with the same number of groups. Yet, this also results in extra storage (about 0.8 GB for LLaMA-7B). In contrast, using only one group (i.e., the first row of Table 4c) results in total failure. Given the additional storage overhead, the 2-group configuration strikes a good balance between performance and memory efficiency."}, {"title": "4.4 TIME AND MEMORY ANALYSES", "content": "As a binary PTQ framework, ARB-LLM eliminates the need for fine-tuning. The alternating algorithm requires more computation to align the distribution progressively, yet this overhead is acceptable. In Table 5, ARB-LLMRC with 15 iterations requires only 21 more minutes than BiLLM, while ARB-LLMRC (without CGB) requires only 3 more minutes than BiLLM using just 1 iteration. The combination of CGB results in an increase of time overhead, due to the percentile search for optimal splitting."}, {"title": "5 CONCLUSION", "content": "In this work, we propose ARB-LLM, a series of alternating refined binarization (ARB) methods for LLMs. Through the analyses of the distribution shift between binarized and full-precision weights, we propose an alternating refinement of binarization parameters to progressively align the weight distribution. Moreover, we extend the basic ARB by equipping the calibration data and scaling along row-column axes, resulting in ARB-X and ARB-RC respectively. Additionally, we propose a refined strategy to better combine the salient column bitmap and group bitmap. Our experiments on multiple open-source LLM families show that the final models ARB-LLMx and ARB-LLMRC can further push the performance boundary from the SOTA binary PTQ methods."}, {"title": "4.1 SETUP", "content": "Effectiveness of Advanced Variants.To validate the effectiveness of our advanced variants ARB-LLMx and ARB-LLMRC, we compare them with the vanilla ARB algorithm in Table 4a. First, we observe that the vanilla ARB already significantly outperforms BiLLM. Furthermore, by introducing either the calibration update or the row-column update to the binarization process, performance is further improved. This demonstrates that our advanced variants, ARB-LLMx and ARB-LLMRC, can further enhance the performance of binary LLMs based on ARB.\nEffectiveness of CGB. To demonstrate the effectiveness of our column-group bitmap (CGB), we conduct an ablation study in Table 4b. In this study, the absence of CGB does not imply the exclusion of partitioning but rather the use of the partitioning strategy used by BiLLM. The results show that CGB further enhances the performance of both ARB-LLMx and ARB-LLMRC. Notably, even when using BiLLM's partitioning strategy, our methods significantly outperform BiLLM.\nColumn Bitmap and Group Bitmap. We use a column bitmap to differentiate between salient and non-salient weights, and a group bitmap to separate weights based on their magnitude. The combination of column and group bitmaps creates four distinct zones. As shown in Table 4c, we explore the effect of decoupling this combination by using either the column bitmap or the group bitmap individually. It is evident that using the column bitmap or group bitmap only will result in a significant performance drop. Omitting both column bitmap and group bitmap entirely (i.e., #group=1), which reduces the method to naive binarization, leads to complete failure."}]}