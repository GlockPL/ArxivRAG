{"title": "LOST IN TIME: CLOCK AND CALENDAR UNDERSTANDING CHALLENGES IN MULTIMODAL LLMS", "authors": ["Rohit Saxena", "Aryo Pradipta Gema", "Pasquale Minervini"], "abstract": "Understanding time from visual representations is a fundamental cognitive skill, yet it remains a challenge for multimodal large language models (MLLMs). In this work, we investigate the capabilities of MLLMs in interpreting time and date through analogue clocks and yearly calendars. To facilitate this, we curated a structured dataset\u00b9 comprising two subsets: (1) ClockQA, which comprises various types of clock styles-standard, black-dial, no-second-hand, Roman numeral, and arrow-hand clocks-paired with time-related questions; and (2) CalendarQA, which consists of yearly calendar images with questions ranging from commonly known dates (e.g., Christmas, New Year's Day) to computationally derived ones (e.g., the 100th or 153rd day of the year). We aim to analyse how MLLMs can perform visual recognition, numerical reasoning, and temporal inference when presented with time-related visual data. Our evaluations show that despite recent advancements, reliably understanding time remains a significant challenge for MLLMs.", "sections": [{"title": "INTRODUCTION", "content": "The ability to interpret and reason about time from visual inputs is critical for many real-world applications-ranging from event scheduling to autonomous systems. Despite advances in multimodal large language models (MLLMs), most work has focused on object detection (Wu et al., 2024), image captioning (McKinzie et al., 2024), or scene understanding (Fu et al., 2024), leaving temporal inference underexplored (Zhang et al., 2025). In particular, analogue clock reading and calendar comprehension involve intricate cognitive steps: they demand fine-grained visual recognition (e.g., clock-hand position, day-cell layout) and non-trivial numerical reasoning (e.g., calculating day offsets). In recent years, a variety of vision-language benchmarks were proposed to evaluate multi-modal reasoning on diverse tasks such as geometry, logic, coding, and advanced mathematics (Yue et al., 2024; Kazemi et al., 2024a;b; Lu et al., 2024). Additional efforts have been made to automatically read analogue clocks and other dials (Yang et al., 2022; Alexeev et al., 2020; Bao et al., 2019; Cai et al., 2020; Howells et al., 2021), showing that dial or gauge interpretation is a cognitively complex skill requiring visual-spatial understanding and arithmetic reasoning. However, clock and calendar readings remain underexplored in these existing large-scale benchmarks, and comprehensive evaluations of MLLMs on such tasks are lacking (see Appendix A for more information on related works)."}, {"title": "DATASET", "content": "We created a small dataset comprising two subsets, ClockQA and CalendarQA, each containing images paired with question-answer pairs to test the time and date reasoning of multimodal large language models (MLLMs). Figure 2 illustrates the dataset and its two subsets.\nClockQA. Given an image of an analogue clock, a multimodal LLM is asked the following question \"What time is shown on the clock in the given image?\" This requires (1) detecting the clock hand positions (hour, minute, and second) and (2) converting them into time representation. The ClockQA subset contains 62 samples of analogue clocks with varying appearances, requiring precise readings of the hour, minute, and second hands. It includes the following categories: (1) Basic clocks: standard analogue clocks; (2) Black dial clocks: featuring a darker face for contrast-based parsing;"}, {"title": "TASKS AND EXPERIMENTS", "content": "Experimental Setup. We evaluate seven multimodal LLMs in a zero-shot setting. We evaluate closed-source multimodal models, including GPT-40 (OpenAI et al., 2024), GPT-01 (OpenAI et al., 2024), Gemini 2.0 (Team et al., 2024), and Claude 3.5 Sonnet (Anthropic, 2024). We also evaluate open-source models such as Llama 3.2-11B-Vision-Instruct (Grattafiori et al., 2024), Qwen2-VL-7B-Instruct (Yang et al., 2024), and MiniCPM-V-2.6 (Yao et al., 2024). The experiment details and exact prompt template used are in Appendix D.\nClockQA Metrics. We measure performance using four metrics. Exact Match (EM) is the proportion of predicted clock readings that exactly match the ground truth. MAE (Seconds) quantifies the average absolute difference between predicted and actual times in seconds, applying a circular 12-hour wraparound (maximum error: 21,600 seconds). We also report Hour Error and Minute Error, which compute mean absolute differences for each clock hand, again with a circular wraparound. See Appendix C.1 for more details.\nCalendarQA Metrics. We adopt standard classification metrics: Accuracy (Acc) to measure correct weekday predictions, and macro-averaged Precision (P), Recall (R), and F1 across date categories. See Appendix C.2 for more details.\nImplementation Details. We conduct experiments on a shared test set of 62 clock samples (across six clock-face variants) and 10 calendar years (with six question types per year). Model prompts followed a consistent format, providing one image and one question. Responses are automatically parsed to extract time or weekday (i.e., removing explanation or conversion of short forms) and evaluated against the reference answer."}, {"title": "RESULTS AND DISCUSSION", "content": "Table 1 summarizes performance across both tasks. In ClockQA, Gemini-2.0 achieves the highest EM score (22.58%) and the lowest hour/minute errors, indicating relatively stronger clock understanding compared to other models. However, overall EM scores remain low, underscoring persistent difficulties in clock reading by MLLMS. Conversely, GPT-01 excels in CalendarQA with an accuracy of 80%, highlighting robust date arithmetic and reasoning capabilities. Other models lag substantially, indicating that date arithmetic and structured layout parsing remain challenging. Overall performance on both ClockQA and CalendarQA remains poor, except for the high performance of GPT-01 on CalendarQA. See Appendix E for a sample of generated predictions.\nClock Reading Remains Error-Prone. Across the ClockQA subset, performance was notably weaker than for the calendar questions (see Table 1). Figures 4a and 3a reveal that performance remains poor even on standard dials; some models exhibit bias toward a single \"default\" time. Roman numerals and stylized clock hands further increase the errors. Removing the second hand did not simplify reasoning, suggesting deep-seated issues with hand detection and angle interpretation.\nCalendar Reasoning Analysis By contrast, calendar tasks elicited higher success rates for certain models and question types. GPT-01 dominates the CalendarQA subset with an 80% overall accuracy (Table 1 and Figure 3b).\nClosed-source models like GPT-01 and Claude-3.5 outshine open-source ones on popular holidays, potentially reflecting memorized patterns in the training data (see Figure 4b). However, accuracy diminishes substantially for lesser-known or arithmetically demanding queries (e.g., 153rd day), indicating that performance does not transfer well to offset-based reasoning. The drop is especially evident among smaller or open-source models (MiniCPM, Qwen2-VL-7B, and Llama3.2-Vision), which exhibit near-random performance on less popular or offset-based queries."}, {"title": "CONCLUSION", "content": "In this work, we conduct a preliminary study on understanding and reasoning about time from visual inputs, which remains a significant challenge for multimodal large language models. We build a small dataset to benchmark these models for clock and calendar understanding. The experimental results highlight key shortcomings in the ability of these models to accurately interpret time from analogue clocks and yearly calendars. Our findings suggest that successful temporal reasoning requires a combination of precise visual perception, numerical computation, and structured logical inference that current MLLMs have not yet mastered. This work highlights the need for further research to improve the processing of geometric relationships in clock faces and structured calendar information in MLLMs."}, {"title": "CLOCKQA METRICS", "content": "We evaluate the clock-reading performance with the following metrics:"}, {"title": "Exact Match (EM).", "content": "The proportion of predictions that exactly match the ground truth time:\nExact Match Accuracy = $\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}_{T_{\\text{true}, i} = T_{\\text{pred}, i}}$"}, {"title": "MAE (Seconds).", "content": "The mean absolute error in seconds, with a 12-hour circular wraparound (i.e., we measure the shorter way around the clock face with a maximum error of 21,600):\nMAE = $\\frac{1}{n} \\sum_{i=1}^{n} \\min (\\|T_{\\text{true}, i} - T_{\\text{pred}, i}\\|, 43200 - \\|T_{\\text{true}, i} - T_{\\text{pred},i}\\|)$"}, {"title": "Hour and Minute Errors.", "content": "Average absolute differences for hour and minute hands, each with a circular wraparound:\nMAE$_{hours}$ = $\\frac{1}{n} \\sum_{i=1}^{n} \\min (\\|H_{\\text{true}, i} - H_{\\text{pred}, i}\\|, 12 - \\|H_{\\text{true},i} - H_{\\text{pred},i}\\|)$,\nMAE$_{minutes}$ = $\\frac{1}{n} \\sum_{i=1}^{n} \\min (\\|M_{\\text{true}, i} - M_{\\text{pred}, i}\\|, 60 - \\|M_{\\text{true},i} - M_{\\text{pred},i}\\|)$"}, {"title": "CALENDARQA METRICS", "content": "For calendar-based reasoning, we employ standard classification metrics:\nAccuracy (Acc). The fraction of correct predictions for the day of the week.\nPrecision (P), Recall (R), & F1. Macro-averaged scores across different date categories."}]}