{"title": "Combining Diverse Information for Coordinated Action: Stochastic Bandit Algorithms for Heterogeneous Agents", "authors": ["Lucia Gordon", "Esther Rolf", "Milind Tambe"], "abstract": "Stochastic multi-agent multi-armed bandits typically assume that the rewards from each arm follow a fixed distribution, regardless of which agent pulls the arm. However, in many real-world settings, rewards can depend on the sensitivity of each agent to their environment. In medical screening, disease detection rates can vary by test type; in preference matching, rewards can depend on user preferences; and in environmental sensing, observation quality can vary across sensors. Since past work does not specify how to allocate agents of heterogeneous but known sensitivity of these types in a stochastic bandit setting, we introduce a UCB-style algorithm, MIN-WIDTH, which aggregates information from diverse agents. In doing so, we address the joint challenges of (i) aggregating the rewards, which follow different distributions for each agent-arm pair, and (ii) coordinating the assignments of agents to arms. MIN-WIDTH facilitates efficient collaboration among heterogeneous agents, exploiting the known structure in the agents' reward functions to weight their rewards accordingly. We analyze the regret of MIN-WIDTH and conduct pseudo-synthetic and fully synthetic experiments to study the performance of different levels of information sharing. Our results confirm that the gains to modeling agent heterogeneity tend to be greater when the sensitivities are more varied across agents, while combining more information does not always improve performance.", "sections": [{"title": "Introduction", "content": "The setting of stochastic multi-agent multi-armed bandits (MAB) [15, 14, 19] is characterized by multiple agents taking actions simultaneously in each time step. This setting serves as a natural model for diverse domains, from COVID test allocation [3] to preference matching [8, 21] and poaching prevention [22, 23]. These real-world problems involve unknown characteristics about the environment that are learned online while the planner figures out the optimal action for each agent. The result-ing explore-exploit tradeoff lends itself well to UCB-style algorithms [2], which estimate unknown quantities optimistically with an upper confi-dence bound (UCB) in an effort to maximize cumulative reward over time.\nWe introduce a new stochastic MAB problem wherein a planner specifies actions for agents of heterogeneous but known sensitivities to their unknown environment. The \"environment\" comprises a set of arms, each of which takes on a state of 0 or 1 at each time step following a Bernoulli distribution, which models a binary outcome as in Solanki et al. [17] and Xu et al. [22]. The mean of the Bernoulli is an unknown parameter that must be learned online by the agents. The agents differ in their sensitivity, which is their probability of receiving a reward of 1 upon pulling an arm given that its state is 1. In this way, the utility of the agents' actions is a function of their sensitivity as well as the arm mean.\nSeveral key ideas help us tackle the core challenges of sequential decision-making with multiple agents with heterogeneous sensitivities to their environment. First, we address the combinatorial challenge of the many ways of allocating agents to arms by decomposing our combinatorial problem into learning the means of the individual arms. Second, we address the learning challenge by combining rewards across agents of varying sensitivity to speed up learning in a sensitivity-aware manner. Third, we address the problem of how to match heterogeneous agents with arms by assigning the highest-sensitivity agents to the arms with the highest UCBs, which we experimentally show is an effective strategy. In contrast, applying past work to our problem without these insights would either naively combine all the agents' rewards and ignore their sensitivities [5] or slowly learn the optimal assignments by approaching the problem at the coarse, super-arm level [2].\nWe introduce the MIN-WIDTH algorithm designed for this new problem (\u00a75). For each arm, MIN-WIDTH combines all the agents' rewards to generate a mean estimator with the tightest UCB, which is nontrivial since rewards are drawn from different distributions for each agent-arm pair. We derive an instance-independent O(Tlog(T)) regret upper bound for the MIN-WIDTH algorithm, where T is the time horizon and there are additional factors for the numbers of agents and their sensitivities (Theorem 2). We also evaluate MIN-WIDTH through pseudo-synthetic experiments with realistic parameter values for diverse domains including COVID test allocation, hotel recommendation, and poaching prevention along with fully synthetic experiments (\u00a78). To compare algorithms with different levels of information sharing, we introduce two sensitivity-aware baselines that we evaluate against MIN-WIDTH. We find that MIN-WIDTH outperforms classical baseline algorithms (CUCB [5] and UCB [2]) not designed for heterogeneous agents as well as our sensitivity-aware baselines in many settings. Moreover, we show experimentally that the performance of MIN-WIDTH is robust to having only approximate knowledge of the agent sensitivities."}, {"title": "Motivating Domains", "content": "Our setting of heterogeneous agents with known sensitivities is motivated by a diverse range of application domains, highlighted by the examples below. By explicitly incorporating agent heterogeneity and information-sharing, we introduce a more natural model for these domains.\nCOVID Test Allocation Consider the problem of allocating a limited number of COVID tests of varying sensitivity among floors of a college dorm with unknown virus prevalence to maximize detection of infected in-dividuals. Different floors in a dorm, which serve as our \"arms,\" will likely have different prevalence rates of the virus. The two primary types of tests to detect COVID are PCR (very sensitive) and antigen (less sensitive) tests, which serve as \"agents\" in our model. Due to limited availability, suppose a college only has enough supply to test one student on each floor per time step. Thus, when we pull a super-arm, we distribute tests (the agents) among floors (the arms) and observe the test results. Bastani et al. [3] consider a similar setup, developing a MAB system for airport COVID testing, but they do not account for varying test sensitivities.\nHotel Recommendation Consider the problem of matching customers with different preferences to hotels with limited space in order to maximize customer satisfaction, a task performed by websites such as Booking.com. Hotels, which are our \"arms,\" differ in features such as cleanliness, service, etc., which may not be known, especially for new hotels. Customers, which serve as our \"agents,\" vary in how much these features matter to them, information that booking platforms can request in a pre-recommendation survey. For instance, one customer may value cleanliness above other features and always be satisfied with their stay if the hotel was clean. Another customer may care equally about cleanliness and staff friendliness, so even if the hotel is clean they will not be satisfied if the staff are not very responsive. Assuming space in hotels is limited and some customer will have to be matched with a hotel that tends to be less clean, we maximize overall satisfaction by matching this latter customer with the less clean hotel and the former customer with the cleaner hotel on average. A similar setting has been modeled using contextual bandits in Wanigasekara et al. [21]. However, unlike their algorithm we combine data from post-stay cleanliness reviews across customers with different preferences to better match either the same customers or new ones with similar preferences in future time steps.\nPoaching Prevention Consider the problem of maximizing the detec-tion of animal traps by planning patrols for rangers with different detection rates in a protected area such as a national park. Poachers hide snares to trap animals, and rangers patrol the park for illegal activity and remove any snares they find, which can be modeled as a stochastic bandit problem [22]. Different parts of the park, which serve as our \"arms,\" are more or less likely to contain snares depending on their animal density, accessibil-ity, etc., but the poaching rates across a park are often unknown due to the vastness of the areas. Rangers, the \"agents,\" vary in terms of their expertise, tools, and vehicles, giving them varying snare detection rates, or \"sensi-tivities,\" a type of real-world heterogeneity that has not been previously modeled. Since the rangers are all patrolling the same park, we combine their observations to speed up our learning of poaching hotspots and opti-mize our assignments for rangers in the next round of patrols, accounting for the rangers' differing partial observabilities in snare detection."}, {"title": "Related Work", "content": "Auer et al. [2] introduce the UCB algorithm for the stochastic bandit problem that pulls the highest-UCB arm in every time step. Audibert et al. [1] introduce the UCB-V algorithm that builds on UCB by incorporating arms' empirical variances. Neither of these model multiple agents. Gai et al. [7] extend the UCB algorithm to the combinatorial setting where multiple arms are pulled in each time step with linear reward functions. Chen et al. [5] introduce an algorithm for possibly unknown reward functions. Both assume the reward obtained from pulling any given arm is an i.i.d. draw from a fixed distribution, an assumption that does not hold in our problem. Rejwan and Mansour [12] consider the combinatorial bandit setting with full-bandit feedback (only the sum of the rewards is observed), whereas we operate in the semi-bandit feedback setting (the reward from each pull is observed) and also have heterogeneous agents.\nExisting multi-agent bandit papers differ widely in their definition of agent heterogeneity. Some works consider agents with access to only a subset of the arms [24], differing but known communication abilities [11], or varying user preferences [8, 21]. The latter is similar to our definition, but in their case the arm context is known and the user preferences are unknown, whereas we have unknown arm means but known sensitivities. Federated combinatorial bandits [17] also have heterogeneous agents, but their agents operate in a competitive environment and are subject to privacy constraints, whereas in our setting the agents are collaborating and there is no cost to their communication.\nOur notion of sensitivity is inspired by past work that utilizes sensor models to capture imperfect observability. In Xu et al. [22], the agents do not observe the true state of each arm, though the more effort they exert, the more reliable their observations become. Their algorithm, however, assumes effort can be specified and distributed in each time step, whereas our agent sensitivities are fixed in advance. Rolf et al. [13] model a single sensor trying to pick out the environment point with the strongest signal on average given observations that include contributions from all points but are more sensitive to those nearby. Unlike this work, we have multiple sensors, and we assume the rewards from distinct arms are independent.\nPast work has explored the consequences of agent communication in different settings. In Shi et al. [16], multiple agents can be assigned to the same arm of a MAB, which results in a collision that can be used to transmit information between agents. We assume each agent pulls a distinct arm, so our agents cannot communicate in this way. In Madhushani and Leonard [11], the agents observe the actions and rewards of their neighbors with some known probability, where the agents are heterogeneous in terms of their \u201csociability,\" so some may be more likely to observe their neighbors than others. Our agents' heterogeneity is unrelated to the way in which information is shared among them. Taylor et al. [18] consider a multi-agent explore-exploit optimization problem and demonstrate the uncertainty penalty phenomenon, wherein increased teamwork under uncertainty can degrade performance relative to the agents acting alone. Unlike their setting and the one in Cesa-Bianchi et al. [4], our problem has no intrinsic spatial nature."}, {"title": "Problem Statement", "content": "We introduce a sequential decision-making problem in which a set of heterogeneous agents are allocated among a set of arms that yield stochastic rewards. There are A agents $A = \\{a\\}_{a=1}^{A}$, N arms $N = \\{n\\}_{n=1}^{N}$, and T time steps $T = \\{t\\}_{t=1}^{T}$. The state of arm $n$ at time $t$ is a random variable $X_{t,n} \\sim Bern(\\mu_n)$, so $X_{t,n} \\in \\{0, 1\\}$. The agents' heterogeneity is captured in their \"sensitivity,\" a scalar value associated with each agent. We denote the set of agent sensitivities by $S = \\{s_a\\}_{a=1}^{A}$, where $s_a$ represents the probability agent $a$ receives a reward of 1 conditional on the true state of the arm being 1. Thus, the reward $Y_{t,a,n}$ obtained when pulling arm $n$ is a random variable that depends on both the agent $a$ who pulls it and the arm's mean:\n$Y_{t,a,n} \\sim Bern(s_a \\mu_n)$.\nBy construction, $s_a = P[Y_{t,a,n} = 1 | X_{t,n} = 1]$, and in our model, $P[Y_{t,a,n} = 0 | X_{t,n} = 0] = 1$.\nAt each time step, the planner selects a super-arm assigning each agent to a distinct arm. The super-arm is chosen from the set $F = \\{f\\}_{j=1}^{\\frac{N!}{(N-A)!}}$ where $f : A \\rightarrow N$ such that $f(a) \\neq f(a')$ if $a \\neq a'$. The super-arm selected at time $t$ is denoted $f_t$, and so $f_t(a)$ is the arm to which agent $a$ is assigned at time $t$. We keep track of the number of times each arm"}, {"title": "MIN-WIDTH Algorithm", "content": "We introduce MIN-WIDTH, a UCB-style algorithm for assigning heterogeneous agents to stationary stochastic arms with Bernoulli rewards. We assume a centralized planner that knows the sensitivities of all the agents and coordinates their assignment to arms in each time step. MIN-WIDTH, outlined in Algorithm 1, revolves around an N-length vector of UCBs denoted by UCB, where $UCB_t$ represents the UCBs the planner uses to match each agent $a$ with an arm $n$ at time $t+1$. In each time step $1 < t \\leq T$, the agents are assigned to arms sequentially in descending order by sensitivity (lines 6-10) so that the highest-sensitivity agent is assigned to the arm with the highest $UCB_{t-1}$, the next-highest-sensitivity agent is then assigned to the arm with the highest $UCB_{t-1}$ out of those remaining unselected, and so on until all the agents have been assigned to distinct arms. The super-arm corresponding to this assignment is then pulled and each agent gets some reward $Y_{t,a, f_t(a)}$ (line 11). Next, the UCBs are updated $UCB_{t-1}[n] \\rightarrow UCB_t[n]$ for every arm $n$ (line 13) according to Equation 11.\nOur agent allocation strategy (lines 6-10) is inspired by the definition of the optimal super-arm in Equation 6: $f^*$ assigns the $i$th-highest-sensitivity agent to the $i$th-highest-mean arm. In practice, we do not know the true means $\\{\\mu_n\\}_{n \\in N}$. Instead, we can estimate them with some $\\{\\hat{\\mu}_n\\}_{n \\in N}$ and set some upper confidence bounds $\\{UCB_n = \\hat{\\mu}_n + e_n\\}_{n \\in N}$ on them, where $e_n$ is the width of the confidence interval around our estimate $\\hat{\\mu}_n$ of $\\mu_n$. In the standard UCB algorithm, the optimal action is to pull the arm with the highest mean, and since that is unknown, the algorithm instead pulls the arm with the highest UCB [2]. Analogously, in our setting, the optimal action is to match the $i$th-best agent with the arm with the $i$th-highest mean, but since the means are unknown, we instead match the $i$th-best agent with the arm with the $i$th-highest UCB. Another way to motivate this is to consider the infinite-data setting, in which the UCBs are equal to the true arm means. In that case, to maximize our expected reward we must match the highest-sensitivity agent to the highest-mean arm. In the finite-data setting, we optimistically estimate the means with the UCBs (which converge to the means with increasing amounts of data), which is why we match the highest-sensitivity agent with the highest-UCB arm.\nAt each time step $t$, the planner uses all the rewards collected so far to generate a new $UCB_{t,n}$ for each arm $n$, as derived in \u00a76. In particular, the planner constructs an empirical estimator $\\hat{\\mu}_{t,n}$ for the mean of each arm that combines all the agents' rewards while accounting for their heterogeneity so as to minimize the width of the confidence interval $e_{t,n}$ around that estimator. The empirical estimator of the mean of arm $n$ at time $t \\in T$ is given by\n$\\hat{\\mu}_{t,n} = \\begin{cases}\n0.5 & t \\notin T_n \\\\\n\\frac{\\sum_{a=1}^A s_a \\sum_{\\tau=1}^t 1_{f_\\tau(a)=n} Y_{\\tau, a, n}}{\\sum_{a=1}^A s_a^2 C_{t,a,n}} & t \\in T_n\n\\end{cases}$.\n(8)\nThe width of the confidence interval on $\\mu_n$ at time $t \\in T$ is\n$E_{t,n} = \\sqrt{\\frac{\\ln(2NG(T,A) / \\delta)}{2 \\sum_{a=1}^A s_a^2 C_{t,a,n}}}$\n(9)\nfor\n$G(T, A) = \\sum_{t=1}^T \\binom{t+A-1}{A-1} < (T+1)^A$.\n(10)\nThe UCB on the mean of arm $n$ at time $t \\in T$ is\n$UCB_{t,n} = \\hat{\\mu}_{t,n} + E_{t,n}$\n(11)\nand is used as the $UCB_t[n]$ in line 13: $UCB_t[n] = UCB_{t,n}$. Note that the algorithm returns a finite $UCB_t[n]$ after a single pull of arm $n$ by any agent. The distribution of the rewards can vary greatly depending on the sensitivity of the agent who collected them, but because the planner knows all the agent sensitivities, they can harness that information to appropri-ately weight the rewards from different agents in generating a shared UCB."}, {"title": "Agent Allocation Strategy", "content": "Our agent allocation strategy (lines 6-10) is inspired by the definition of the optimal super-arm in Equation 6: $f^*$ assigns the ith-highest-sensitivity agent to the ith-highest-mean arm. In practice, we do not know the true means $\\{\\mu_n\\}_{n \\in N}$. Instead, we can estimate them with some $\\{\\hat{\\mu}_n\\}_{n \\in N}$ and set some upper confidence bounds $\\{UCB_n = \\hat{\\mu}_n + e_n\\}_{n \\in N}$ on them, where $e_n$ is the width of the confidence interval around our estimate $\\hat{\\mu}_n$ of $\\mu_n$. In the standard UCB algorithm, the optimal action is to pull the arm with the highest mean, and since that is unknown, the algorithm instead pulls the arm with the highest UCB [2]. Analogously, in our setting, the optimal action is to match the ith-best agent with the arm with the ith-highest mean, but since the means are unknown, we instead match the ith-best agent with the arm with the ith-highest UCB. Another way to motivate this is to consider the infinite-data setting, in which the UCBs are equal to the true arm means. In that case, to maximize our expected reward we must match the highest-sensitivity agent to the highest-mean arm. In the finite-data setting, we optimistically estimate the means with the UCBs (which converge to the means with increasing amounts of data), which is why we match the highest-sensitivity agent with the highest-UCB arm."}, {"title": "Update Rule", "content": "At each time step $t$, the planner uses all the rewards collected so far to generate a new $UCB_{t,n}$ for each arm $n$, as derived in \u00a76. In particular, the planner constructs an empirical estimator $\\hat{\\mu}_{t,n}$ for the mean of each arm that combines all the agents' rewards while accounting for their heterogeneity so as to minimize the width of the confidence interval $e_{t,n}$ around that estimator. The empirical estimator of the mean of arm $n$ at time $t \\in T$ is given by\n$\\hat{\\mu}_{t,n} = \\begin{cases}\n0.5 & t \\notin T_n \\\\\n\\frac{\\sum_{a=1}^A s_a \\sum_{\\tau=1}^t 1_{f_\\tau(a)=n} Y_{\\tau, a, n}}{\\sum_{a=1}^A s_a^2 C_{t,a,n}} & t \\in T_n\n\\end{cases}$.\nThe width of the confidence interval on $\\mu_n$ at time $t \\in T$ is\n$E_{t,n} = \\sqrt{\\frac{\\ln(2NG(T,A) / \\delta)}{2 \\sum_{a=1}^A s_a^2 C_{t,a,n}}}$\nfor\n$G(T, A) = \\sum_{t=1}^T \\binom{t+A-1}{A-1} < (T+1)^A$.\nThe UCB on the mean of arm $n$ at time $t \\in T$ is\n$UCB_{t,n} = \\hat{\\mu}_{t,n} + E_{t,n}$\nand is used as the $UCB_t[n]$ in line 13: $UCB_t[n] = UCB_{t,n}$. Note that the algorithm returns a finite $UCB_t[n]$ after a single pull of arm $n$ by any agent. The distribution of the rewards can vary greatly depending on the sensitivity of the agent who collected them, but because the planner knows all the agent sensitivities, they can harness that information to appropri-ately weight the rewards from different agents in generating a shared UCB."}, {"title": "Theoretical Results", "content": "We provide analytical results for the MIN-WIDTH algorithm introduced in \u00a75, with complete proofs in Appendix A. Incorporating all the agents' rewards to generate a shared UCB for each arm is a complex problem due to the agents' heterogeneity. Naively, one may think we could simply divide each reward for a given arm by the sensitivity of the agent who collected it and apply the original UCB algorithm to this sequence. This is invalid, however, because while these rescaled rewards will have identical means, they will still have different variances. The UCB algorithm assumes the rewards from a given arm are i.i.d. and hence would not apply to these rescaled rewards. To resolve this, in Proposition 1 we take a more general approach by treating this as an optimization problem where we optimize over weights on the agents' rewards to get the tightest confidence interval around the arm mean estimator.\nSuppose agent $a$ pulls arm $n$ a fixed number of times, a number we denote $C_{a,n}$, where the reward from each pull is $Y_{i,a,n} \\sim Bern(s_a \\mu_n)$. Let $C_n = \\{c_{a,n}\\}_{a=1}^{A}$ contain the $c_{a,n}$ for every agent. Let $D_{c_n,n}$ be the weighted sum of the independent rewards collected by all the agents from arm $n$, expressed in terms of weights $W_{c_n,a,n}$:\n$D_{c_n,n} = \\sum_{a=1}^A \\sum_{i=1}^{C_{a,n}} W_{c_n,a,n} Y_{i,a,n} = \\sum_{a=1}^A \\sum_{i=1}^{C_{a,n}} W_{c_n,a,n} Y_{i,a,n}$.\nThen the weights $W_{c_n,a,n}$ that minimize the width of the confidence interval on $\\mu_n$ given by\n$\\Xi_{c_n,n} = \\sqrt{\\frac{\\ln(2/\\delta) \\sum_{a=1}^A W_{c_n,a,n}^2 C_{a,n}}{2}}$\nunder the constraint that the empirical estimator $D_{c_n,n}$ is unbiased are given by\n$W_{c_n,a,n} = \\frac{s_a}{\\sum_{b=1}^A s_b^2 C_{b,n}} 1_{c_{a,n}>0}$.\nNext, in Theorem 1, we show that we can use the weights derived in Proposition 1 to construct an empirical estimator for the mean of each arm, whose deviation from the true mean we bound with high probability. This bound involves the challenge of counting the number of possible pulls of each arm by each agent.\nSuppose the empirical estimator of the mean of arm $n$ at time $t \\in T$ is given by Equation 8. Then for $\\epsilon_{t,n}$ given in Equation 9, $\\hat{\\mu}_{t,n}$ satisfies\n$\\forall \\delta \\in (0,1), P[\\bigcap_{n \\in N, t \\in T} |\\hat{\\mu}_{t,n} - \\mu_n| < E_{t,n}] > 1 - \\delta$.\nFinally, in Theorem 2 we use the concentration bound on the shared empirical mean from Theorem 1 to upper bound the cumulative regret of the MIN-WIDTH algorithm.\nSuppose we act according to the MIN-WIDTH algorithm. Then $\\forall \\delta \\in (0,1)$, the cumulative regret at time T is bounded by\n$P[R_T < A(N-1) + 2 \\sqrt{2ANT \\ln(\\frac{2NG(T,A)}{ \\delta})} \\frac{\\max S}{\\min S}] > 1 - \\delta$."}, {"title": "Agent Allocation Strategy", "content": "Our agent allocation strategy (lines 6-10) is inspired by the definition of the optimal super-arm in Equation 6: $f^*$ assigns the ith-highest-sensitivity agent to the ith-highest-mean arm. In practice, we do not know the true means $\\{\\mu_n\\}_{n \\in N}$. Instead, we can estimate them with some $\\{\\hat{\\mu}_n\\}_{n \\in N}$ and set some upper confidence bounds $\\{UCB_n = \\hat{\\mu}_n + e_n\\}_{n \\in N}$ on them, where $e_n$ is the width of the confidence interval around our estimate $\\hat{\\mu}_n$ of $\\mu_n$. In the standard UCB algorithm, the optimal action is to pull the arm with the highest mean, and since that is unknown, the algorithm instead pulls the arm with the highest UCB [2]. Analogously, in our setting, the optimal action is to match the ith-best agent with the arm with the ith-highest mean, but since the means are unknown, we instead match the ith-best agent with the arm with the ith-highest UCB. Another way to motivate this is to consider the infinite-data setting, in which the UCBs are equal to the true arm means. In that case, to maximize our expected reward we must match the highest-sensitivity agent to the highest-mean arm. In the finite-data setting, we optimistically estimate the means with the UCBs (which converge to the means with increasing amounts of data), which is why we match the highest-sensitivity agent with the highest-UCB arm."}, {"title": "Update Rule", "content": "At each time step $t$, the planner uses all the rewards collected so far to generate a new $UCB_{t,n}$ for each arm $n$, as derived in \u00a76. In particular, the planner constructs an empirical estimator $\\hat{\\mu}_{t,n}$ for the mean of each arm that combines all the agents' rewards while accounting for their heterogeneity so as to minimize the width of the confidence interval $e_{t,n}$ around that estimator. The empirical estimator of the mean of arm $n$ at time $t \\in T$ is given by\n$\\hat{\\mu}_{t,n} = \\begin{cases}\n0.5 & t \\notin T_n \\\\\n\\frac{\\sum_{a=1}^A s_a \\sum_{\\tau=1}^t 1_{f_\\tau(a)=n} Y_{\\tau, a, n}}{\\sum_{a=1}^A s_a^2 C_{t,a,n}} & t \\in T_n\n\\end{cases}$.\nThe width of the confidence interval on $\\mu_n$ at time $t \\in T$ is\n$E_{t,n} = \\sqrt{\\frac{\\ln(2NG(T,A) / \\delta)}{2 \\sum_{a=1}^A s_a^2 C_{t,a,n}}}$\nfor\n$G(T, A) = \\sum_{t=1}^T \\binom{t+A-1}{A-1} < (T+1)^A$.\nThe UCB on the mean of arm $n$ at time $t \\in T$ is\n$UCB_{t,n} = \\hat{\\mu}_{t,n} + E_{t,n}$\nand is used as the $UCB_t[n]$ in line 13: $UCB_t[n] = UCB_{t,n}$. Note that the algorithm returns a finite $UCB_t[n]$ after a single pull of arm $n$ by any agent. The distribution of the rewards can vary greatly depending on the sensitivity of the agent who collected them, but because the planner knows all the agent sensitivities, they can harness that information to appropri-ately weight the rewards from different agents in generating a shared UCB."}, {"title": "Experimental Setup", "content": "We perform experiments to compare the efficacy of five algorithms: the one we design for this setting, MIN-WIDTH; two sensitivity-aware baselines we introduce, NO-SHARING and MIN-UCB; and two canonical baselines, CUCB and UCB."}, {"title": "NO-SHARING", "content": "The simplest information sharing setting is not to combine rewards across agents at all. In this NO-SHARING strategy, each agent keeps track of their own UCB for each arm relying solely on their own rewards. The empirical estimator of the mean of arm $n$ according to agent $a$ with sensitivity $s_a$ at time $t \\in T$ is\n$\\hat{\\mu}_{t,a,n} = \\begin{cases}\n0.5 & t \\notin T_{a,n} \\\\\n\\frac{\\sum_{\\tau=1}^t 1_{f_\\tau(a)=n} Y_{\\tau, a, n}}{s_a C_{t,a,n}} & t \\in T_{a,n}\n\\end{cases}$.\nLet the width of agent a's confidence interval on the mean of arm n at time $t \\in T$ for $\\delta \\in (0,1)$ be\n$E_{t,a,n} = \\frac{1}{s_a} \\sqrt{\\frac{\\ln(2ANT / \\delta)}{2 C_{t,a,n}}}$\nAgent a's UCB on the mean of arm $n$ at time $t \\in T$ is then\n$UCB_{t,a,n} = \\hat{\\mu}_{t,a,n} + E_{t,a,n}$.\nHere, each agent is almost operating in the standard UCB setting except for the assignment hierarchy, which has more sensitive agents pick which arms they want to pull before less sensitive agents. Consequently, less sensitive agents may have to pull arms that do not have the maximum $UCB_{t,a,n}$."}, {"title": "MIN-UCB", "content": "The MIN-UCB algorithm directly improves on the naive NO-SHARING strategy. Each agent still keeps track of their own UCB for each arm, but since all the agent UCBs on the mean of a given arm hold simultaneously by Proposition A.2, we can take the minimum of these UCBs to get a tighter bound. The shared UCB for arm $n$ at time $t \\in T$ is then\n$UCB_{t,n} = \\min_{a \\in A} UCB_{t,a,n}$.\nIn contrast to the NO-SHARING algorithm, now agents effectively get information about arms they have not yet pulled since $UCB_{t,n} < \\infty$ if any agent has pulled arm $n$ even if agent $a$ has not. The algorithm will match the ith-highest-sensitivity agent with the arm with the ith-highest $UCB_{t,n}$, still giving higher-sensitivity agents priority.\nWhile MIN-UCB yields a tighter UCB than NO-SHARING, it still ignores potentially valuable information by always using the UCB of one of the agents, which accounts for the rewards collected by that agent alone. If we want to tightly bound the mean on a given arm, intuitively it makes sense to use all the rewards from the arm, not just those of whichever agent happens to have the lowest UCB for the arm. This is most evident in a setting where there are two agents of high sensitivity, such as 0.9 and 0.8. Perhaps the 0.9-agent has a lower UCB for an arm, but the pulls by the 0.8-agent represent additional rewards that could be used to further shrink the UCB that MIN-UCB ignores, motivating our MIN-WIDTH algorithm that combines all the agents' rewards.\nNote, however, that both NO-SHARING and MIN-UCB have AT in the logarithm, which is smaller than MIN-WIDTH'S G(T, A) factor. This may cause MIN-WIDTH'S UCBs to be higher than those of NO-SHARING and MIN-UCB in some cases. As a result, we anticipate that MIN-WIDTH may not always outperform NO-SHARING and MIN-UCB."}, {"title": "CUCB", "content": "CUCB [5"}]}