{"title": "Combining Diverse Information for Coordinated Action: Stochastic Bandit Algorithms for Heterogeneous Agents", "authors": ["Lucia Gordon", "Esther Rolf", "Milind Tambe"], "abstract": "Stochastic multi-agent multi-armed bandits typically assume that the rewards from each arm follow a fixed distribution, regardless of which agent pulls the arm. However, in many real-world settings, rewards can depend on the sensitivity of each agent to their environment. In medical screening, disease detection rates can vary by test type; in preference matching, rewards can depend on user preferences; and in environmental sensing, observation quality can vary across sensors. Since past work does not specify how to allocate agents of heterogeneous but known sensitivity of these types in a stochastic bandit setting, we introduce a UCB-style algorithm, MIN-WIDTH, which aggregates information from diverse agents. In doing so, we address the joint challenges of (i) aggregating the rewards, which follow different distributions for each agent-arm pair, and (ii) coordinating the assignments of agents to arms. MIN-WIDTH facilitates efficient collaboration among heterogeneous agents, exploiting the known structure in the agents' reward functions to weight their rewards accordingly. We analyze the regret of MIN-WIDTH and conduct pseudo-synthetic and fully synthetic experiments to study the performance of different levels of information sharing. Our results confirm that the gains to modeling agent heterogeneity tend to be greater when the sensitivities are more varied across agents, while combining more information does not always improve performance.", "sections": [{"title": "1 Introduction", "content": "The setting of stochastic multi-agent multi-armed bandits (MAB) [15, 14, 19] is characterized by multiple agents taking actions simultaneously in each time step. This setting serves as a natural model for diverse domains, from COVID test allocation [3] to preference matching [8, 21] and poaching prevention [22, 23]. These real-world problems involve unknown characteristics about the environment that are learned online while the planner figures out the optimal action for each agent. The resulting explore-exploit tradeoff lends itself well to UCB-style algorithms [2], which estimate unknown quantities optimistically with an upper confidence bound (UCB) in an effort to maximize cumulative reward over time.\nWe introduce a new stochastic MAB problem wherein a planner specifies actions for agents of heterogeneous but known sensitivities to their unknown environment. The \"environment\" comprises a set of arms, each of which takes on a state of 0 or 1 at each time step following a Bernoulli distribution, which models a binary outcome as in Solanki et al. [17] and Xu et al. [22]. The mean of the Bernoulli is an unknown parameter that must be learned online by the agents. The agents differ in their sensitivity, which is their probability of receiving a reward of 1 upon pulling an arm given that its state is 1. In this way, the utility of the agents' actions is a function of their sensitivity as well as the arm mean.\nSeveral key ideas help us tackle the core challenges of sequential decision-making with multiple agents with heterogeneous sensitivities to their environment. First, we address the combinatorial challenge of the many ways of allocating agents to arms by decomposing our combinatorial problem into learning the means of the individual arms. Second, we address the learning challenge by combining rewards across agents of varying sensitivity to speed up learning in a sensitivity-aware manner. Third, we address the problem of how to match heterogeneous agents with arms by assigning the highest-sensitivity agents to the arms with the highest UCBs, which we experimentally show is an effective strategy. In contrast, applying past work to our problem without these insights would either naively combine all the agents' rewards and ignore their sensitivities [5] or slowly learn the optimal assignments by approaching the problem at the coarse, super-arm level [2].\nWe introduce the MIN-WIDTH algorithm designed for this new problem (\u00a75). For each arm, MIN-WIDTH combines all the agents' rewards to generate a mean estimator with the tightest UCB, which is nontrivial since rewards are drawn from different distributions for each agent-arm pair. We derive an instance-independent $O(T \\log(T))$ regret upper bound for the MIN-WIDTH algorithm, where T is the time horizon and there are additional factors for the numbers of agents and their sensitivities (Theorem 2). We also evaluate MIN-WIDTH through pseudo-synthetic experiments with realistic parameter values for diverse domains including COVID test allocation, hotel recommendation, and poaching prevention along with fully synthetic experiments (\u00a78). To compare algorithms with different levels of information sharing, we introduce two sensitivity-aware baselines that we evaluate against MIN-WIDTH. We find that MIN-WIDTH outperforms classical baseline algorithms (CUCB [5] and UCB [2]) not designed for heterogeneous agents as well as our sensitivity-aware baselines in many settings. Moreover, we show experimentally that the performance of MIN-WIDTH is robust to having only approximate knowledge of the agent sensitivities."}, {"title": "2 Motivating Domains", "content": "Our setting of heterogeneous agents with known sensitivities is motivated by a diverse range of application domains, highlighted by the examples below. By explicitly incorporating agent heterogeneity and information-sharing, we introduce a more natural model for these domains.\nCOVID Test Allocation Consider the problem of allocating a limited number of COVID tests of varying sensitivity among floors of a college dorm with unknown virus prevalence to maximize detection of infected individuals. Different floors in a dorm, which serve as our \"arms,\" will likely have different prevalence rates of the virus. The two primary types of tests to detect COVID are PCR (very sensitive) and antigen (less sensitive) tests, which serve as \"agents\" in our model. Due to limited availability, suppose a college only has enough supply to test one student on each floor per time step. Thus, when we pull a super-arm, we distribute tests (the agents) among floors (the arms) and observe the test results. Bastani et al. [3] consider a similar setup, developing a MAB system for airport COVID testing, but they do not account for varying test sensitivities.\nHotel Recommendation Consider the problem of matching customers with different preferences to hotels with limited space in order to maximize customer satisfaction, a task performed by websites such as Booking.com. Hotels, which are our \"arms,\" differ in features such as cleanliness, service, etc., which may not be known, especially for new hotels. Customers, which serve as our \"agents,\" vary in how much these features matter to them, information that booking platforms can request in a pre-recommendation survey. For instance, one customer may value cleanliness above other features and always be satisfied with their stay if the hotel was clean. Another customer may care equally about cleanliness and staff friendliness, so even if the hotel is clean they will not be satisfied if the staff are not very responsive. Assuming space in hotels is limited and some customer will have to be matched with a hotel that tends to be less clean, we maximize overall satisfaction by matching this latter customer with the less clean hotel and the former customer with the cleaner hotel on average. A similar setting has been modeled using contextual bandits in Wanigasekara et al. [21]. However, unlike their algorithm we combine data from post-stay cleanliness reviews across customers with different preferences to better match either the same customers or new ones with similar preferences in future time steps.\nPoaching Prevention Consider the problem of maximizing the detection of animal traps by planning patrols for rangers with different detection rates in a protected area such as a national park. Poachers hide snares to trap animals, and rangers patrol the park for illegal activity and remove any snares they find, which can be modeled as a stochastic bandit problem [22]. Different parts of the park, which serve as our \"arms,\" are more or less likely to contain snares depending on their animal density, accessibility, etc., but the poaching rates across a park are often unknown due to the vastness of the areas. Rangers, the \"agents,\" vary in terms of their expertise, tools, and vehicles, giving them varying snare detection rates, or \"sensitivities,\" a type of real-world heterogeneity that has not been previously modeled. Since the rangers are all patrolling the same park, we combine their observations to speed up our learning of poaching hotspots and optimize our assignments for rangers in the next round of patrols, accounting for the rangers' differing partial observabilities in snare detection."}, {"title": "3 Related Work", "content": "Auer et al. [2] introduce the UCB algorithm for the stochastic bandit problem that pulls the highest-UCB arm in every time step. Audibert et al. [1] introduce the UCB-V algorithm that builds on UCB by incorporating arms' empirical variances. Neither of these model multiple agents. Gai et al. [7] extend the UCB algorithm to the combinatorial setting where multiple arms are pulled in each time step with linear reward functions. Chen et al. [5] introduce an algorithm for possibly unknown reward functions. Both assume the reward obtained from pulling any given arm is an i.i.d. draw from a fixed distribution, an assumption that does not hold in our problem. Rejwan and Mansour [12] consider the combinatorial bandit setting with full-bandit feedback (only the sum of the rewards is observed), whereas we operate in the semi-bandit feedback setting (the reward from each pull is observed) and also have heterogeneous agents.\nExisting multi-agent bandit papers differ widely in their definition of agent heterogeneity. Some works consider agents with access to only a subset of the arms [24], differing but known communication abilities [11], or varying user preferences [8, 21]. The latter is similar to our definition, but in their case the arm context is known and the user preferences are unknown, whereas we have unknown arm means but known sensitivities. Federated combinatorial bandits [17] also have heterogeneous agents, but their agents operate in a competitive environment and are subject to privacy constraints, whereas in our setting the agents are collaborating and there is no cost to their communication.\nOur notion of sensitivity is inspired by past work that utilizes sensor models to capture imperfect observability. In Xu et al. [22], the agents do not observe the true state of each arm, though the more effort they exert, the more reliable their observations become. Their algorithm, however, assumes effort can be specified and distributed in each time step, whereas our agent sensitivities are fixed in advance. Rolf et al. [13] model a single sensor trying to pick out the environment point with the strongest signal on average given observations that include contributions from all points but are more sensitive to those nearby. Unlike this work, we have multiple sensors, and we assume the rewards from distinct arms are independent.\nPast work has explored the consequences of agent communication in different settings. In Shi et al. [16], multiple agents can be assigned to the same arm of a MAB, which results in a collision that can be used to transmit information between agents. We assume each agent pulls a distinct arm, so our agents cannot communicate in this way. In Madhushani and Leonard [11], the agents observe the actions and rewards of their neighbors with some known probability, where the agents are heterogeneous in terms of their \u201csociability,\" so some may be more likely to observe their neighbors than others. Our agents' heterogeneity is unrelated to the way in which information is shared among them. Taylor et al. [18] consider a multi-agent explore-exploit optimization problem and demonstrate the uncertainty penalty phenomenon, wherein increased teamwork under uncertainty can degrade performance relative to the agents acting alone. Unlike their setting and the one in Cesa-Bianchi et al. [4], our problem has no intrinsic spatial nature."}, {"title": "4 Problem Statement", "content": "We introduce a sequential decision-making problem in which a set of heterogeneous agents are allocated among a set of arms that yield stochastic rewards. There are A agents $A = \\{a\\}_{a=1}^A$, N arms $N = \\{n\\}_{n=1}^N$, and T time steps $T = \\{t\\}_{t=1}^T$. The state of arm n at time t is a random variable $X_{t,n} \\sim Bern(\\mu_n)$, so $X_{t,n} \\in \\{0, 1\\}$. The agents' heterogeneity is captured in their \"sensitivity,\" a scalar value associated with each agent. We denote the set of agent sensitivities by $S = \\{s_a\\}_{a=1}^A$, where $s_a$ represents the probability agent a receives a reward of 1 conditional on the true state of the arm being 1. Thus, the reward $Y_{t,a,n}$ obtained when pulling arm n is a random variable that depends on both the agent a who pulls it and the arm's mean:\n$Y_{t,a,n} \\sim Bern(s_a \\mu_n)$.\t\t\t\t\t(1)\nBy construction, $s_a = P[Y_{t,a,n} = 1 | X_{t,n} = 1]$, and in our model, $P[Y_{t,a,n} = 0 | X_{t,n} = 0] = 1$.\nAt each time step, the planner selects a super-arm assigning each agent to a distinct arm. The super-arm is chosen from the set $F = \\{f\\}_{f=1}^{\\frac{N!}{(N-A)!}}$ where $f: A \\rightarrow N$ such that $f(a) \\neq f(a')$ if $a \\neq a'$. The super-arm selected at time t is denoted $f_t$, and so $f_t(a)$ is the arm to which agent a is assigned at time t. We keep track of the number of times each arm"}, {"title": "5 MIN-WIDTH Algorithm", "content": "5.1 Algorithm Structure\nWe introduce MIN-WIDTH, a UCB-style algorithm for assigning heterogeneous agents to stationary stochastic arms with Bernoulli rewards. We assume a centralized planner that knows the sensitivities of all the agents and coordinates their assignment to arms in each time step. MIN-WIDTH, outlined in Algorithm 1, revolves around an N-length vector of UCBs denoted by UCB, where $UCB_t$ represents the UCBs the planner uses to match each agent a with an arm n at time t+1. In each time step 1 < t \u2264 T, the agents are assigned to arms sequentially in descending order by sensitivity (lines 6-10) so that the highest-sensitivity agent is assigned to the arm with the highest $UCB_{t-1}$, the next-highest-sensitivity agent is then assigned to the arm with the highest $UCB_{t-1}$ out of those remaining unselected, and so on until all the agents have been assigned to distinct arms. The super-arm corresponding to this assignment is then pulled and each agent gets some reward $Y_{t, a, f_t(a)}$ (line 11). Next, the UCBs are updated $UCB_{t-1}[n] \\rightarrow UCB_t[n]$ for every arm n (line 13) according to Equation 11.\n5.2 Agent Allocation Strategy\nOur agent allocation strategy (lines 6-10) is inspired by the definition of the optimal super-arm in Equation 6: $f^*$ assigns the ith-highest-sensitivity agent to the ith-highest-mean arm. In practice, we do not know the true means $\\{\\mu_n\\}_{n \\in N}$. Instead, we can estimate them with some $\\{\\hat{\\mu}_n\\}_{n \\in N}$\n5.3 Update Rule\nAt each time step t, the planner uses all the rewards collected so far to generate a new $UCB_{t,n}$ for each arm n, as derived in \u00a76. In particular, the planner constructs an empirical estimator $\\hat{\\mu}_{t,n}$ for the mean of each arm that combines all the agents' rewards while accounting for their heterogeneity so as to minimize the width of the confidence interval $\\epsilon_{t,n}$ around that estimator. The empirical estimator of the mean of arm n at time $t \\in T$ is given by\n$\\hat{\\mu}_{t,n} = \\begin{cases}\\frac{\\sum_{a=1}^A s_a \\sum_{\\tau=1}^t 1_{f_\\tau(a)=n} Y_{\\tau, a, n}}{\\sum_{a=1}^A s_a^2 C_{t, b, n}} & t \\in T_n\\\\0.5 & t \\notin T_n\\end{cases}$.\t\t\t\t(8)\nThe width of the confidence interval on $\\mu_n$ at time $t \\in T$ is\n$\\epsilon_{t,n} = \\sqrt{\\frac{\\ln(2 N G(T, A) / \\delta)}{2 \\sum_{a=1}^A s_a^2 C_{t, a, n}}}$.\t\t\t\t\t(9)\nfor\n$G(T, A) = \\sum_{t=1}^T \\binom{t+A-1}{A-1} < (T+1)^A$.\t\t\t\t\t(10)\nThe UCB on the mean of arm n at time $t \\in T$ is\n$UCB_{t,n} = \\hat{\\mu}_{t,n} + \\epsilon_{t,n}$\t\t\t\t\t(11)\nand is used as the $UCB+[n]$ in line 13: $UCB_t[n] = UCB_{t,n}$. Note that the algorithm returns a finite $UCB+[n]$ after a single pull of arm n by any agent. The distribution of the rewards can vary greatly depending on the sensitivity of the agent who collected them, but because the planner knows all the agent sensitivities, they can harness that information to appropriately weight the rewards from different agents in generating a shared UCB."}, {"title": "6 Theoretical Results", "content": "We provide analytical results for the MIN-WIDTH algorithm introduced in \u00a75, with complete proofs in Appendix A. Incorporating all the agents' rewards to generate a shared UCB for each arm is a complex problem due to the agents' heterogeneity. Naively, one may think we could simply divide each reward for a given arm by the sensitivity of the agent who collected it and apply the original UCB algorithm to this sequence. This is invalid, however, because while these rescaled rewards will have identical means, they will still have different variances. The UCB algorithm assumes the rewards from a given arm are i.i.d. and hence would not apply to these rescaled rewards. To resolve this, in Proposition 1 we take a more general approach by treating this as an optimization problem where we optimize over weights on the agents' rewards to get the tightest confidence interval around the arm mean estimator.\nProposition 1. (MIN-WIDTH Weights Derivation). Suppose agent a pulls arm n a fixed number of times, a number we denote $C_{a,n}$, where the reward from each pull is $Y_{i,a,n} \\sim Bern(s_a \\mu_n)$. Let $C_n = \\{c_{a,n}\\}_{a=1}^A$ contain the $c_{a,n}$ for every agent. Let $D_{c_n,n}$ be the weighted sum of the independent rewards collected by all the agents from arm n, expressed in terms of weights $W_{c_n,a,n}$:\n$D_{c_n,n} = \\sum_{a=1}^A W_{c_n,a,n} \\sum_{i=1}^{C_{a,n}} Y_{i,a,n} = \\sum_{a=1}^A \\sum_{i=1}^{C_{a,n}} W_{c_n,a,n} Y_{i,a,n}$.\t\t\t\t\t\t(12)\nThen the weights $W_{c_n,a,n}$ that minimize the width of the confidence interval on $\\mu_n$ given by\n$\\Upsilon_{c_n,n} = \\sqrt{\\frac{\\ln(2/\\delta)}{2} \\sum_{a=1}^A W_{c_n,a,n}^2 C_{a,n}}$\nunder the constraint that the empirical estimator $D_{c_n,n}$ is unbiased are given by\n$W_{c_n,a,n} = \\frac{\\frac{1}{s_a}}{\\sum_{b=1}^A \\frac{C_{b,n}}{s_b^2}}1_{c_{a,n}>0}$.\t\t\t\t\t\t(13)\nProof. Since the rewards collected by a certain agent when pulling a certain arm are i.i.d., we consider weights on such sequences of i.i.d. rewards rather than on every single reward. If $C_{a,n} = 0$, then agent a has collected no rewards for arm n, and so we set $W_{c_n,a,n} = 0$ for any such agent. Hence, we need to solve for $w_{c_n,a,n}$ only for agents with $C_{a,n} > 0$. If $D_{c_n,n}$ is to be unbiased, then we need $E[D_{c_n,n}] = \\mu_n$, which sets the constraint\n$\\sum_{a=1}^A W_{c_n,a,n} s_a C_{a,n} = 1$.\nSince $W_{c_n,a,n} Y_{i,a,n}$ is bounded by $0 < W_{c_n,a,n} Y_{i,a,n} < W_{c_n,a,n}$, Hoeffding's inequality gives\n$\\forall \\delta \\in (0, 1), P[|D_{c_n,n} - \\mu_n| < \\sqrt{\\frac{\\ln(2/\\delta)}{2} \\sum_{a=1}^A W_{c_n,a,n}^2 C_{a,n}}] > 1 - \\delta$,\t\t\t\t\t(14)\nyielding $\\Upsilon_{c_n,n} = \\sqrt{\\frac{\\ln(2/\\delta)}{2} \\sum_{a=1}^A W_{c_n,a,n}^2 C_{a,n}}$ as the width of the confidence interval on the mean of arm n for some fixed number of pulls of each arm by each agent captured in $C_n$. To make this confidence interval as tight as possible, we solve for the weights that minimize $\\Upsilon_{c_n,n}$ under the constraint that $D_{c_n,n}$ is unbiased for any non-random $C_n$. We solve this constrained optimization problem with the method of Lagrange multipliers, using Lagrangian\n$\\mathcal{L}(\\omega, \\lambda) = \\sqrt{\\frac{\\ln(2/\\delta)}{2} \\sum_{b=1}^A W_{c_n,b,n}^2 C_{b,n}} + \\lambda (\\sum_{b=1}^A W_{c_n,b,n} s_b C_{b,n} - 1)$.\nwhich results in $W_{c_n,a,n} = \\frac{\\frac{1}{s_a}}{\\sum_{b=1}^A \\frac{C_{b,n}}{s_b^2}}$, which holds for any agent a with $C_{a,n} > 0$. Since $W_{c_n.a,n} = 0$ for agents with $C_{a,n} = 0$, we get Equation 13.\nNext, in Theorem 1, we show that we can use the weights derived in Proposition 1 to construct an empirical estimator for the mean of each arm, whose deviation from the true mean we bound with high probability. This bound involves the challenge of counting the number of possible pulls of each arm by each agent.\nTheorem 1. (MIN-WIDTH Concentration Bound). Suppose the empirical estimator of the mean of arm n at time $t \\in T$ is given by Equation 8. Then for $\\epsilon_{t,n}$ given in Equation 9, $\\hat{\\mu}_{t,n}$ satisfies\n$\\forall \\delta \\in (0, 1), P[\\prod_{n \\in N, t \\in T} |\\hat{\\mu}_{t,n} - \\mu_n| < \\epsilon_{t,n}] > 1 - \\delta$.\t\t\t\t\t(15)\nProof. Applying a union bound over the arms to Equation 14 gives $\\forall \\delta \\in (0, 1)$,\n$P[\\forall n \\in N, Cn \\in H, |D_{c_n,n} - \\mu_n| < \\sqrt{\\frac{\\ln(2N \\frac{G(T, A)}{\\delta})}{2} \\sum_{a=1}^A W_{C_n,a,n}^2 C_{a,n}}] > 1 - \\delta$.\nLet H be the set of all possible instantiations of the set Cn assuming that arm n has been pulled at least once within a time horizon of T, so H is a set of sets. To apply a union bound over these sets, we determine the cardinality of H using the constraints that each element of Cn is between 0 and T and the sum of the elements in Cn is between 1 and T. We denote the resulting cardinality G(T, A), which would simply be T, as for CUCB, if all the agents were identical. We perform the union bound, use that $C_{t,n} \\in H \\forall t \\in T_n$, and plug in for $D_{c_t,n,n}$ and $W_{C_t,n,a,n}$, resulting in\n$\\forall \\delta \\in (0, 1), P[\\forall n \\in N, t \\in T_n, |\\hat{\\mu}_{t,n} - \\mu_n| < \\epsilon_{t,n}] > 1 - \\delta$.\nFor $t \\notin T_n$, Equation 8 gives $\\hat{\\mu}_{t,n} = 0.5$, and $\\epsilon_{t,n} = \\infty$ since $C_{t,n} = 0$. Equation 15 follows directly since the difference between the true mean $\\mu_n$ and 0.5 must be $< \\infty$.\nFinally, in Theorem 2 we use the concentration bound on the shared empirical mean from Theorem 1 to upper bound the cumulative regret of the MIN-WIDTH algorithm.\nTheorem 2. (MIN-WIDTH Regret Bound). Suppose we act according to the MIN-WIDTH algorithm. Then $\\forall \\delta \\in (0, 1)$, the cumulative regret at time T is bounded by\n$P[R_T < A(N - 1) + 2 \\sqrt{2ANT \\ln(\\frac{2NG(T, A)}{\\delta})} \\frac{\\max S}{\\min S}] > 1 - \\delta$.\t\t\t\t\t(16)\nProof. We bound $\\mu_n - \\hat{\\mu}_{t,n}$ by $\\mu_n - \\hat{\\mu}_{t,n} < \\epsilon_{t,n}$. Using the bound on $|\\hat{\\mu}_{t,n} - \\mu_n|$ from Equation 15 and the UCB on the mean of arm n at time $t \\in T$ from Equation 11 gives\n$\\forall \\delta \\in (0, 1), P[\\forall n \\in N, t \\in T, \\mu_n < UCB_{t,n}] > 1 - \\delta$.\t\t\t\t\t(17)\nWe split Equation 7 into terms with t<N and t>N:\n$R_T = \\sum_{t=1}^{N-1} \\sum_{a=1}^A \\delta_a (\\mu_{f^*(a)} - \\mu_{f_t(a)}) + \\sum_{t=N}^{T} \\sum_{a=1}^A \\delta_a (\\mu_{f^*(a)} - \\mu_{f_t(a)})$.\t\t\t\t\t(18)"}, {"title": "5.2 Agent Allocation Strategy", "content": "Agent a's UCB on the mean of arm n at time $t \\in T$ is then\n$UCB_{t,a,n} = \\hat{\\mu}_{t,a,n} + \\epsilon_{t,a,n}$.\t\t\t\t\t(24)\nHere, each agent is almost operating in the standard UCB setting except for the assignment hierarchy, which has more sensitive agents pick which arms they want to pull before less sensitive agents. Consequently, less sensitive agents may have to pull arms that do not have the maximum $UCB_{t,a,n}$.\n7.2 MIN-UCB\nThe MIN-UCB algorithm directly improves on the naive NO-SHARING strategy. Each agent still keeps track of their own UCB for each arm, but since all the agent UCBs on the mean of a given arm hold simultaneously by Proposition A.2, we can take the minimum of these UCBs to get a tighter bound. The shared UCB for arm n at time $t \\in T$ is then\n$UCB_{t,n} = \\min_{a \\in A} UCB_{t,a,n}$.\t\t\t\t\t(25)\nIn contrast to the NO-SHARING algorithm, now agents effectively get information about arms they have not yet pulled since $UCB_{t,n} < \\infty$ if any agent has pulled arm n even if agent a has not. The algorithm will match the ith-highest-sensitivity agent with the arm with the ith-highest $UCB_{t,n}$, still giving higher-sensitivity agents priority.\nWhile MIN-UCB yields a tighter UCB than NO-SHARING, it still ignores potentially valuable information by always using the UCB of one of the agents, which accounts for the rewards collected by that agent alone. If we want to tightly bound the mean on a given arm, intuitively it makes sense to use all the rewards from the arm, not just those of whichever agent happens to have the lowest UCB for the arm. This is most evident in a setting where there are two agents of high sensitivity, such as 0.9 and 0.8. Perhaps the 0.9-agent has a lower UCB for an arm, but the pulls by the 0.8-agent represent additional rewards that could be used to further shrink the UCB that MIN-UCB ignores, motivating our MIN-WIDTH algorithm that combines all the agents' rewards.\nNote, however, that both NO-SHARING and MIN-UCB have AT in the logarithm, which is smaller than MIN-WIDTH'S G(T, A) factor. This may cause MIN-WIDTH'S UCBs to be higher than those of No-SHARING and MIN-UCB in some cases. As a result, we anticipate that MIN-WIDTH may not always outperform NO-SHARING and MIN-UCB."}, {"title": "7.3 CUCB", "content": "CUCB [5] combines all the rewards collected from each arm to generate a UCB for the arm. The algorithm is designed for sequences of i.i.d. rewards, which is not the case in our setting because of the agent heterogeneity. This algorithm has no way of accounting for heterogeneous agents, so we naively combine observations across agents in our implementation, ignoring the fact that the i.i.d. assumption does not hold. Consequently, it may never be able to learn the optimal agent-arm assignments. For CUCB we use Equation 26 for the UCB of arm n at time t. Since this algorithm was not designed for heterogeneous agents, we randomly assign agents to the top-UCB arms at each time step.\n$UCB_{t,n} = \\frac{\\sum_{\\tau=1}^t \\sum_{a=1}^A 1_{f_\\tau(a)=n} Y_{\\tau, a, n}}{\\sum_{\\tau=1}^t \\sum_{a=1}^A 1_{f_\\tau(a)=n}} + \\sqrt{\\frac{\\ln(2Nt / \\delta)}{\\sum_{\\tau=1}^t \\sum_{a=1}^A 1_{f_\\tau(a)=n}}}$\t\t\t\t\t(26)"}, {"title": "7.4 UCB", "content": "The standard UCB algorithm [2] maintains a UCB on the mean reward of every action that can be taken at each time step. In our implementation, we treat every super-arm as an arm and apply the UCB algorithm to every super-arm. We use Equation 27 for the UCB of super-arm f at time t and pull the super-arm with the highest UCB at each time step. By treating each super-arm as an arm, the UCB algorithm is implicitly able to account for the heterogeneity among the agents. However, it does not combine any information across agents or arms, making it increasingly unsuitable as the number of agents or arms increases.\n$UCB_{t, f} = \\frac{\\sum_{\\tau=1}^t 1_{f_\\tau = f} \\sum_{a=1}^A Y_{\\tau, a, f(a)}}{\\sum_{\\tau=1}^t 1_{f_\\tau = f}} + \\sqrt{\\frac{\\ln(2 \\frac{N!}{(N-A)!}t / \\delta)}{2 \\sum_{\\tau=1}^t 1_{f_\\tau = f}}}$\t\t\t\t\t(27)"}, {"title": "7.5 Implementation", "content": "We implement MIN-WIDTH, NO-SHARING, and MIN-UCB as described in \u00a75, \u00a77.1, and \u00a77.2, respectively, setting T\u2192t in Equations 9 and 23. While using T facilitates the regret analysis, using t in our experiments allows us to assess the performance at different times with a single run and also compare across simulations. All graphs display the cumulative regret averaged over 90 trials with two standard errors and use \u03b4=0.05."}, {"title": "8 Results", "content": "We perform simulations in four domains: three pseudo-synthetic domains with parameter values inspired by real data COVID test allocation", "rates": "mu = \\{0.05", "6": "S = \\{0.8", "in": "where we see all the algorithms perform similarly for the initial time steps", "20": ".", "ratings": "mu= \\{0"}]}