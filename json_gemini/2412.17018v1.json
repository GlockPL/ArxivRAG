{"title": "GAS: Generative Auto-bidding with Post-training Search", "authors": ["Yewen Li", "Shuai Mao", "Jingtong Gao", "Nan Jiang", "Yunjian Xu", "Qingpeng Cai", "Fei Pan", "Peng Jiang", "Bo An"], "abstract": "Auto-bidding is essential in facilitating online advertising by automatically placing bids on behalf of advertisers. Generative auto-bidding, which generates bids based on an adjustable condition using models like transformers and diffusers, has recently emerged as a new trend due to its potential to learn optimal strategies directly from data and adjust flexibly to preferences. However, generative models suffer from low-quality data leading to a mismatch between condition, return to go, and true action value, especially in long sequential decision-making. Besides, the majority preference in the dataset may hinder models' generalization ability on minority advertisers' preferences. While it is possible to collect high-quality data and retrain multiple models for different preferences, the high cost makes it unaffordable, hindering the advancement of auto-bidding into the era of large foundation models. To address this, we propose a flexible and practical Generative Auto-bidding scheme using post-training Search, termed GAS, to refine a base policy model's output and adapt to various preferences. We use weak-to-strong search alignment by training small critics for different preferences and an MCTS-inspired search to refine the model's output. Specifically, a novel voting mechanism with transformer-based critics trained with policy indications could enhance search alignment performance. Additionally, utilizing the search, we provide a fine-tuning method for high-frequency preference scenarios considering computational efficiency. Extensive experiments conducted on the real-world dataset and online A/B test on the Kuaishou advertising platform demonstrate the effectiveness of GAS, achieving significant improvements, e.g., 1.554% increment of target cost.", "sections": [{"title": "1 Introduction", "content": "The rapid digitalization of commerce has significantly expanded the reach and importance of online advertising platforms, making them essential for businesses to engage target audiences and boost sales [12, 40]. With the vast number of impression opportunities, manually adjusting bid prices to optimize costs under budget and KPI constraints is impractical. To address this, ad platforms now offer auto-bidding services that automate the bidding process using advanced strategies [4, 9, 32]. These strategies consider factors from immediate or historical bidding information, such as the distribution of impression opportunities and remaining budgets [25]. Besides, according to different types of advertisers, the strategy should take their different preferences into consideration. For example, brand advertisers, who aim for long-term growth and awareness, typically aim to show their ads to as many people as possible under constraints like average cost per impression. However, performance advertisers, who focus on maximizing the value of winning impressions, aim to maximize conversions with cost-per-conversion constraints [18]. To meet these diverse needs, advertising platforms like Google, Facebook, and Alibaba provide tailored multiple bidding strategies for their customers [2, 13, 16]. Additionally, in the face of the dynamic ever-changing advertising environment, strategies must be regularly optimized to stay closely aligned with customers' preferences, thereby helping them achieve long-term commercial benefits [17].\nReinforcement learning (RL) has long been a leading method for enhancing auto-bidding strategies by training agents with an advertising simulator or offline advertising logs. However, RL approaches are predominantly based on the Markovian decision process (MDP), which assumes that future states are determined solely by the current single step's state and action. Recent statistical analyses [17] question this assumption in auto-bidding, revealing a strong correlation between the lengths of historical state sequences and subsequent states. This finding suggests that relying solely on the most recent state can lead to instability in the unpredictable online advertising environment. Moreover, the preference for the RL strategy is not easy to control. USCB [18] proposes calculating an optimal solution under multiple constraints using historical"}, {"title": "2 Preliminary", "content": ""}, {"title": "2.1 Problem Statement", "content": "During a time period, suppose there are H impression opportunities arriving sequentially and indexed by i. In a bidding platform, advertisers submit bids to compete for each impression opportunity. An advertiser wins the impression if its bid bi is higher than the bids of others. Upon winning, the advertiser incurs a cost of ci, which is typically the highest bid of the other advertisers in a second-price auction setting. During the period, the goal of an advertiser is to maximize the sum value of winning impressions $\\sum_i o_i v_i$, where vi is the value of impression i and oi is the binary indicator of whether the advertiser wins impression i. Besides, the budget and multiple KPI constraints [18] are critical for an ad advertiser to control the ad delivery performance. The budget constraint is considered as $\\sum_i o_i c_i \\leq B$, where ci is the cost of impression i and B is the budget. Other KPI constraints are more complicated and can be classified into two categories. The first category is the cost-related (CR) constraints, which restrict the unit cost of certain advertising events, such as CPC and CPA. The second category is the non-cost-related (NCR) constraints, which restrict the average advertising effect, such as CTR and CPI. For simplicity, we consider the auto-bidding with cost-related constraints, which have a unified formulation: $\\frac{\\sum_i C_{ij}o_i}{\\sum_i P_{ij}o_i} \\leq C_j$, where Cj is the upper bound of j-th constraint provided by the advertiser. pij can be any performance indicator, e.g., return or constant. cij is the cost of constraint j. Given J constraints, we have the Multi-Constrained Bidding (MCB):\nmaximize $\\sum_i o_i v_i$\ns.t. $\\sum_i O_i c_i \\leq B$ \n$\\frac{\\sum_i C_{ij}o_i}{\\sum_i P_{ij}o_i} \\leq C_j, \\forall j$\n$0_i \\in \\{0, 1\\}, \\forall i$\n(1)"}, {"title": "2.2 Decision-making Process for Auto-bidding", "content": "As the advertising environment is highly dynamic, the optimal bidding parameters should be adjusted regularly to maximize the total received value over the entire period. This leads to the formulation of the auto-bidding task as a sequential decision-making task. We consider a standard decision-making setup consisting of an auto-bidding agent interacting with advertising environment E in discrete timesteps. At each timestep t of a bidding period, the agent receives a state st \u2208 S describing the real-time advertising status and then outputs an action at \u2208 A for the final bid. The advertising environment has an underlying unknown state transition dynamic T. Under an MDP assumption, the transition dynamic T could be expressed by T : St \u00d7 at \u2192 St+1, i.e., the next state St+1 \u2208 S is determined by the current state st and action at. In this case, the agent's policy is expressed as \u03c0(at|st). Without an MDP assumption, the next state could be determined by more factors like the historical trajectory \u03c4. After transitioning to the next state, the advertising environment would emit a reward rt representing the value contributed to the objective obtained within the time period t. Repeating this process until the end of a bidding period, one day for instance, the goal of the auto-bidding agent is to maximize the total received value over the entire period as stated in Equation 1.\nA detailed description of our modeling are listed below:\n\u2022 st: the state is a collection of information that describes the advertising status from the perspective of a campaign. The information should in principle reflect time, budget consumption and KPI constraints satisfying status, such as left time, left budget, budget consumption speed, current KPI ratio for constraint j (i.e. ($\\frac{\\sum_i c_{ij}o_i}{\\sum_i p_{ij}o_i}$)/Cj), etc.\n\u2022 at: adjustment to the bidding parameters $\\lambda_j, j = 0, . . ., J$, at the time period t, and modeled as $(a_{\\lambda_0}^t,..., a_{\\lambda_J}^t)$.\n\u2022rt: at time-step t, let C be the candidate impression set between step t and t + 1, then a typical setting of the reward could be the value contributed to the objective obtained on this C within the time period. For simplicity, we will omit C in the following sections."}, {"title": "3 Generative Auto-bidding with Search", "content": "In this section, we first introduce how an MCTS-inspired post-training search process is developed to refine a base policy model's"}, {"title": "3.1 MCTS-inspired Post-training Search", "content": "As the decision transformer is widely used as a backbone in generative decision-making, we also adopt the decision transformer as our backbone for auto-bidding and the policy for generating an action could be formulated as\nat = DT(a\u2264t, a<t, R\u2264t),\n(3)\nwhere the condition Rt is the return to go at time step t, i.e.,\n$R_t = \\sum_{i=t~T} \\gamma^{i-t}r(S_i, a_i)$,\n(4)\nwhere \u03b3 is the discounting factor and r(si, ai) is a rewarding function representing the preference, e.g., it could be set as the oivi representing a preference considering only the value.\nOur purpose of the search scheme is to find a better action that could align better with the preference like a higher value. Applying the typical MCTS method to the decision-making process should involve four parts in every time step t:\n\u2022 Selection: start from a root state node st and randomly select successive valid child action nodes a within exploration budget N, i.e., i = 1 ~ N.\n\u2022 Expansion: unless the child action nodes end the bidding process, create one further child state node s with respect to the transition dynamic s ~ T.\n\u2022 Simulation: complete one rollout from node s given the policy \u03c0(a|s) until the end.\n\u2022 Backpropagation: use the result of the rollout to update value information in the nodes a given root st.\nAfter these four parts, we could choose a final action a based on some principles such as balance between exploration and exploitation. In this work, we choose the action of maximum preference value to execute given a state st with uncertainty consideration modeled as a random action selection operation. However, unlike the GO game where we can simulate all possible moves, this could be impractical in the bidding scenarios as bidding is a typical partially observable MDP (POMDP) task where other advertisers' behaviors are not predictable. Therefore, we make modifications to the typical MCTS process by approximating the expansion and simulation process together via learning an enhanced transformer-based Q-value function, without actual simulation in a simulator (which is also invalid). Now, we will introduce how we implement GAS in the following three parts, with an illustration in Figure 1(a)."}, {"title": "3.1.1 Selection", "content": "Given a decision transformer policy DTe, we generate a base action a first, and then we perpetuate it by multiplying a random factor uniformly between 90% and 110% to get N - 1 random actions {$a_i^t$}$_{i=1:N\u22121}$, expressed as\na = at * \u0454, \u0454 ~ U(90%, 110%).\n(5)\nWe also preserve the initial base action at to get the final N action proposals {$a_i^t$}$_{i=1:N}$ = {$a_i^t$}$_{i=1:N\u22121}$\u2295at. Then, the selection process is choosing an action from these action proposals."}, {"title": "3.1.2 Expansion and Simulation", "content": "As we cannot rollout in a simulator or real environment, we need to approximate this rollout process after taking an action proposal a given st. As we actually need only the result of the rollout, we could directly estimate the value of a given st using a Q-value function, expressed as\nQ\u03c6(st, a\u00bf; \u03c0) = r(st, a\u00bf) + Est+1~T,at+1~\u03c0Q\u03c6 (St+1, at+1; \u03c0).\n(6)\nWe could employ the IQL [22] method to the learning of Q\u03c6, which introduces an extra value net V\u03c8(s) to avoid the overestimation issues due to the distributional shift, expressed as\nLv (\u03c6) = E(s,a)~D [L\u2211(Q\u03c6(s, a) \u2013 V\u03c8(s))],\n(7)\nwhere L (u) = |\u03c4 - 1(u < 0)|u\u00b2 is an expectile regression loss. The value net V\u03c8(s) is used to the Q-value learning:\nLQ(\u03c6) = E(st,at,st+1)~D[r(st, at) + \u03b3V\u03c8 (St+1) \u2013 Q\u03c6 (st, at))\u00b2].\n(8)\nAs indicated in Equation 6, the Q is coupled with the underlying policy \u03c0 in the latter expectation term. However, Q\u03c6(st, at) only receives a single state-action pair without policy indication, leading to a value prediction based on the underlying policy \u03c0\u03b2 that collected the dataset actually. This causes a policy gap in the generative auto-bidding, as its policy \u03c0\u03b5 indicated by various conditions is different from \u03c0\u03b2, leading to a biased value approximation.\nRollout via QT. To address the distributional gap by representing the actual policy \u03c4\u03b5, we employ the historical trajectory for Q-value learning with a transformer utilizing its sequential modeling ability, termed QT, i.e.,\nQe (st, a\u00bf) = Qe(st, a\u00bf; \u03c0\u03b5) = QTe (st, a\u00bf; s<t, a<t).\n(9)\nThis could be beneficial with a large-scale pertaining set, which contains trajectories collected by various policies because it could empirically use the historical trajectory to predict the future trajectory, i.e., the rollout {s\u2264t, a<t} \u2192 {St+1:T, at+1:T}.\nAfter training, the QT (st, a; s<t, a<t) could return an approximation of the value of the rollout until the end of bidding."}, {"title": "3.1.3 Backpropagation", "content": "As the overestimation issue of the Q-value function is notorious, an inaccurate value estimation for the rollout could bias the backpropagation to provide an inaccurate value of action node a given root node st, leading to the execution of a bad action. To alleviate the overestimate issue, we provide a Q-voting mechanism for value assessment, which is based on a heuristic insight of the consensus.\nValue Backpropagation via Q-voting. Given the success of offline RL, where Q-value is employed to learn a policy improving over the behavior policy that collected the dataset, we could intuitively believe different randomly trained Q-value nets could achieve an agreement in giving the true best action higher value with high probability. Besides, due to the overestimation being an occasional error, there would be no agreement in giving a specific action a higher value. Formally, if we independently train M Q-value nets {Q}$_{k=1:M}$ with different random seeds and we have N action proposals {$a_i^t$}$_{i=1:N}$ with a ground-truth best action $a^*_t$, we model the"}, {"title": "4.2 Performance Comparison with Baselines", "content": "In this experiment, we conducted a performance comparison of various baseline methods under different settings, including varying datasets and budget constraints in the MCB bidding. The dataset"}, {"title": "5 Live Experiment", "content": "To verify the effectiveness of our proposed GAS, we have deployed it on Kuaishou's advertising system, see Figure 1(b). The deployment scenario is the Multiple Constraint Bidding(MCB). Under the MCB setting, the advertisers set the budget with or without the CPA/ROI constraint, and the bidding strategy aims to get as many conversions as possible under these constraints. Due to the limited online testing resources and the potential impact on the advertisers' value, we only compared GAS-infer with the baseline model, DT, which is currently in production. The experimental setup is as follows:\n\u2022 State: Budget, cost, time-based budget allocation, time-based cost speed, predicted conversion rates, real CPA or ROI states, etc.\n\u2022 Action: Adjustment to the last time bidding coefficient, \u03bbt = \u03bbt\u22121 + at, \u03bbt is the bidding coefficients in Eq. 2.\n\u2022 Post-training Search: The critic is trained with the sum value of the winning impressions, and the search is conducted under the value-first setting. The action range for the search is still sampled within \u00b110% of the base action, with 5 points being searched.\nOur online A/B testing is conducted for five full days. For each MCB campaigns, 25% budget and traffic are allocated to the baseline bidding model and GAS. The results are shown in Table 4. The experiment results show that GAS improved impressions by +0.934%, cost by +0.953%, target cost by -1.554%, and overall ROI by +0.595%. All metrics showed significant improvements."}, {"title": "6 Related Work", "content": "Auto-bidding and Offline Reinforcement Learning. To eliminate the need for manual intervention, auto-bidding autonomously manages ad placement bids on behalf of advertisers to optimize key performance indicators (KPIs) like clicks. Initially, classic control methods like PID [8] were used to optimize budget spending using predefined curves, while OnlineLP [44] adjusted bids based on traffic predictions. With increasing complexity of the online bidding environments, RL algorithms such as USCB [18], SORL [29], and MAAB [41] became crucial for decision-making. Especially offline RL methods, which learn policies from existing datasets without online interaction, have shown significant success. Notable methods include BCQ [14], which narrows the action space to"}, {"title": "A DETAILED DESCRIPTIONS OF DATASETS", "content": "The datasets consisting of AIGB-2M and AIGB-sparse, where AIGB-sparse is a sparse version of AIGB-2M with less conversions. Each dataset consists of 21 advertising delivery periods, with each period containing approximately 500000 impression opportunities, divided into 48 intervals. Detailed parameters are shown in Table 5. Each advertiser will bid for all impressions.\nEach dataset contains over 500 million records, with each recording information for multiple advertisers across various time steps and multiple periods. The data is structured in the following format:\n\u2022 deliveryPeriodIndex: The index indicting the current advertising delivery period."}, {"title": "B HYPERPARAMETERS SETTING", "content": "We conduct experiments in a simulated experimental environment resembling real advertising system, provided by Alibaba [43]. During evaluation, an episode, also referred to as an advertising delivery period, is a day divided into 48 intervals, with each interval lasting 30 minutes. Each episode contains approximately 500000 impression opportunities that arrive sequentially. An advertising delivery period involves 48 advertisers from diverse categories, with varying budgets and CPAs. Each advertiser bids for all impression opportunities during each period. In each evaluation, our well-trained model represents a designated advertiser in bidding given specific budget and CPA. In order to comprehensively evaluate the performance of the model under different advertisers, we will use different advertiser configurations and advertising periods to evaluate this model multiple times in the simulated environment, and average the results as the evaluation score."}]}