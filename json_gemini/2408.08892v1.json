{"title": "Leveraging Large Language Models for Enhanced Process Model Comprehension", "authors": ["Humam Kourani", "Alessandro Berti", "Jasmin Hennrich", "Wolfgang Kratsch", "Robin Weidlich", "Chiao-Yun Li", "Ahmad Arslan", "Wil M. P. van der Aalst", "Daniel Schuster"], "abstract": "In Business Process Management (BPM), effectively comprehending process models is crucial yet poses significant challenges, particularly as organizations scale and processes become more complex. This paper introduces a novel framework utilizing the advanced capabilities of Large Language Models (LLMs) to enhance the interpretability of complex process models. We present different methods for abstracting business process models into a format accessible to LLMs, and we implement advanced prompting strategies specifically designed to optimize LLM performance within our framework. Additionally, we present a tool, AIPA, that implements our proposed framework and allows for conversational process querying. We evaluate our framework and tool by i) an automatic evaluation comparing different LLMs, model abstractions, and prompting strategies and ii) a user study designed to assess AIPA's effectiveness comprehensively. Results demonstrate our framework's ability to improve the accessibility and interpretability of process models, pioneering new pathways for integrating AI technologies into the BPM field.", "sections": [{"title": "1. Introduction", "content": "Business Process Management (BPM) represents a management approach focusing on aligning an organization's processes with its strategic objectives. This includes process documentation, automation, integration, and continuous process improvement. Using BPM allows organizations to optimize performance, manage change, and achieve operational excellence [1].\nIn the context of BPM, process models serve as the primary artifact for depicting the flow of activities, data, events, and organizational units in a process. Process models facilitate the analysis, simulation, optimization, and automation of business processes. In today's competitive market, high-quality process models are pivotal for enterprises seeking to enhance their operational efficiency and the quality of their products and services. However, the inherent complexity of real-world business processes often results in intricate models that can be difficult to understand and manage. This complexity can lead to higher costs and more errors during the maintenance and improvement of the processes.\nThe Business Process Model and Notation (BPMN) [2] has become the de facto standard for modeling business processes and is widely used in industry. While BPMN offers a diverse range of elements and constructs, typical usage in industry centers around a limited, commonly used subset [3, 4]. However, the availability of additional, more advanced elements allows for the modeling of specialized or complex scenarios. This capability enhances BPMN's versatility but also complicate model comprehension, increasing the risk of cognitive overload for users [5, 6]. This complexity acts as a barrier, hindering users from fully comprehending BPMN models.\nIn addressing the challenge of comprehending complex process models, leveraging new technologies in AI holds promise. Among these technologies, Large Language Models (LLMs) stand out for their advanced capabilities in natural language processing and pattern recognition. Advanced LLMs, such as gpt-4 [7], are trained on vast amounts of text data, enabling them to generate human-like text and engage in natural language conversations [8]. Due to their comprehensive training data, LLMs contain a wealth of process-related domain knowledge that can facilitate process model comprehension [9]. Moreover, LLMs possess reasoning capabilities, allowing them to recognize, analyze, and make inferences from textual data in various contexts [10]. These reasoning capabilities might enable LLMs to comprehend process models, identify relationships between elements, and interpret process structures accurately.\nIn this paper, we present a novel framework that leverages the capabilities of LLMs to enhance the interpretability and accessibility of complex BPMN models. By abstracting process models into different formats and employing advanced prompt engineering techniques, we guide LLMs to comprehend the different process structures and relationships. Our framework enables users to interact with the LLMs, gaining deep insights into complex processes without requiring technical expertise in modeling languages. Furthermore, we present our tool, AIPA (AI-Powered Process Analyst), which integrates our framework with state-of-the-art LLMs. Our research is structured around the following key research questions:\n\u2022 RQ1: How does the choice of abstraction method for BPMN models affect their comprehension by LLMs?\n\u2022 RQ2: What prompting strategies can enhance the comprehension capabilities of LLMs for BPMN models?\n\u2022 RQ3: How effectively can state-of-the-art LLMs understand and interpret BPMNS models?\n\u2022 RQ4: How does the application of LLMs influence user comprehension of BPMN models in practical scenarios?\nThe remainder of the paper is structured as follows. Section 2 discusses related work. In Section 3, we introduce our framework for LLM-based process model comprehension. In Section 4, we present our tool, AIPA. We evaluate our framework on different textual abstractions and prompting strategies in Section 5, and we conduct a user study to assess the effectiveness and usability of AIPA in Section 6. Section 7 discusses the limitations of our framework and proposes ideas for future work. Finally, Section 8 concludes this paper."}, {"title": "2. Related Work", "content": "The complexity of comprehending business process models has been extensively explored. In [11], the authors conducted a comprehensive review of metrics for assessing business process complexity, identifying different dimensions of complexity. In [12], the authors empirically evaluated the cognitive difficulty associated with comprehending process models, focusing on specific process constructs. Their findings revealed that repetition and exclusive choices impose a higher cognitive load compared to concurrent and sequential tasks. The study [13] assessed the comprehension of BPMN models by healthcare associates, highlighting the challenges they face in understanding complex BPMN models. These studies underscore the need for new methods to enhance the interpretability and accessibility of process models.\nProcess model querying methods [14] play a crucial role in enhancing the accessibility and usability of business process models. These methods can be categorized into two main categories: model-specific querying [15, 16, 17, 18] and models repository querying [19, 20] (i.e., finding the models in a repository satisfying some given constraint). All the proposed approaches allow for a large number of queries. However, the model-specific approaches are limited by i) lack of domain knowledge on the underlying process; ii) prototypal implementation; iii) the user needs to learn a new querying language (either graphical or textual). These limitations highlight the need for more intuitive and accessible querying methods, which our framework addresses by leveraging LLMs to facilitate conversational process querying.\nAI systems can help non-expert analysts interpret BPMN models by automatically analyzing, extracting, and summarizing relevant information from process models [21]. In [22], the authors present a logical model for analyzing BPMN models using AI techniques. Their approach demonstrates the ability of AI systems to detect patterns, errors, and inconsistencies, thereby enhancing the analysis of BPMN models. In [23], the authors leverage conversational AI to create interactive systems that can understand and respond to natural language inputs, thus facilitating more intuitive and efficient management of business processes. These advancements underscore the potential of AI technologies to simplify the interpretation of complex process models.\nRecently, some LLM-based model querying methods have been proposed. In [24], Petri nets are textually abstracted for process description and improvement purposes. Declarative models involving the control flow and time perspectives are texutally abstracted in [25], allowing for a range of queries. In [26], a double-faced approach involving fine-tuning open-source LLMs and retrieval augmented generation techniques on a specific BPMN process model is proposed. However, the results of the assessment are not convincing, achieving low percentages on simple tasks such as tasks and flow existence. The approach described in [27] aims to explain decision points in a process exploiting both the structure of the process model and information extracted from the event log (for instance, causal relations and Shapley values). A limitation stated in the paper is that the implemented tool does not allow for iterative cycles, being limited to the initial response. Moreover, it is not possible to express questions unrelated to decision points."}, {"title": "3. LLM-Based Process Model Comprehension Framework", "content": "This section introduces our framework for LLM-based process model comprehension. We outline the high-level architecture of the framework, and we propose several methods for abstracting process models and various prompting strategies aimed at optimizing LLM performance."}, {"title": "3.1. Architecture", "content": "Figure 1 illustrates the architecture of our LLM-based process model comprehension framework. The process initiates with a user uploading a BPMN model, which is automatically abstracted into an input format that an LLM can process (cf. Section 3.2). For targeted analysis, users can optionally select specific parts of the BPMN model. Only these selected elements are then included in the generated abstraction to facilitate the analysis of specified areas of interest.\nTo aid the LLM in effectively understanding and analyzing the BPMN model, the model abstraction is augmented with tailored instructions. These incorporate various prompting strategies designed to optimize the LLM's response (cf. Section 3.3).\nThe user then poses an inquiry about the BPMN model, which is integrated into a prompt that combines the textual representation, the applied prompting strategies, and the user's inquiry. This enriched prompt is submitted to the LLM, which processes the input and generates a response. The response is then forwarded to the user. The interaction is dynamic, allowing the user to pose follow-up questions. Each follow-up question is integrated into a new prompt, maintaining the conversation history, and submitted to the LLM, which then generates subsequent responses."}, {"title": "3.2. BPMN Abstraction", "content": "The abstraction of BPMN models is a crucial component of our framework, enabling their transformation into formats that can be effectively processed by LLMs.\nThis section details the four abstraction formats supported by our framework: XML, simplified XML, JSON, and image.\nXML. BPMN Models of the 2.0 standard are typically stored in an XML format [2]. This comprehensive format encapsulates all visual and structural aspects of the process, including tasks, events, gateways, and detailed attributes such as positions, styles, and additional metadata. The Full XML abstraction retains all these details, conforming with BPMN 2.0 standard.\nSimplified XML (SXML). We designed this abstraction format to reduce the complexity of the standard XML format by deliberately omitting non-essential elements such as layout information, styling, and metadata. These elements are considered irrelevant to the core logical structure of the BPMN model. SXML retains the original XML's hierarchical structure but includes only the fundamental components such as swimlanes, tasks, gateways, and connections. This abstraction provides a concise representation that highlights the operational aspects of the BPMN model without the additional complexity of visual details.\nJSON. We designed the JSON abstraction to restructure the BPMN data into an attribute-based representation. Unlike the hierarchical structure of XML, the JSON format organizes BPMN elements into a flat list, where each element is associated with a set of attributes that encapsulate all necessary information. Similarly to the Simplified XML abstraction, the JSON abstraction retains only the essential components and excludes non-essential attributes like styling and layout information. By doing so, it enables LLMs to process the structural and operational aspects of the process model efficiently, focusing on the attributes that are most relevant for analysis.\nImage (PNG). Recent advancements in LLMs have extended their capabilities beyond text processing to include support for image inputs. This enhancement allows for the processing and analysis of non-textual data, offering a broader range of applications. We leverage these capabilities through the image abstraction of BPMN models. This abstraction involves transforming the graphical BPMN models into PNG images. This format captures the visual layout of the model, preserving the spatial arrangement of the different BPMN elements. By feeding the generated image into an advanced model, its image processing capabilities are used to interpret the BPMN model's content and structure."}, {"title": "3.3. Prompt Engineering Techniques", "content": "Prompt engineering refers to the techniques used to instruct LLMs and guide them towards desired outputs. This includes providing additional information in the prompt to better inform the LLM about the requirements of the task and the domain knowledge required to perform it. The goal is to optimize the LLM's performance and improve the relevance and quality of its outputs.\nIn our framework, we support several prompting techniques that can be combined to optimize the understanding of BPMN models by LLMs. This section outlines these techniques, and the full prompts are available at https://zenodo.org/records/12800002.\nRole Prompting\nOne problem of LLMs is their laziness, which was empirically studied in [28]. Laziness should be interpreted as the tendency to reduce the effort to accomplish a task and can be either observed in generic answers, statements without explanations, or missing important parts of the original prompt. Some strategies have been proposed to help mitigate laziness. One strategy that can be used to migrate this issue is Role prompting [29]. This strategy involves configuring the prompt to position the LLM as a domain expert [30]. As illustrated in Listing 4, our framework includes two implementations of role prompting:\n\u2022 Process Modeling Expert (S1): We assign the LLM the role of an expert in business process modeling and the BPMN 2.0 standard.\n\u2022 Process Owner (S2): We assign the LLM the role of a process expert familiar with the domain of the provided process, instructing it to use its domain knowledge to fill in any missing gaps when analyzing the process.\nNon-Technical Abstraction (S3)\nAs shown in Listing 5, we instruct the LLM to avoid technical terms and use natural language to describe the behavior of the underlying process. This technique ensures that explanations and analyses are accessible to users who may not be familiar with the specialized terminologies of the BPMN 2.0 standard.\nChain of Thoughts (S4)\nLLMs are subject to problems such as hallucinations [31] (the answer is partly unrelated to the original question) and non-determinism [32] (the answer changes in different sessions). The chain-of-thoughts technique aims to improve the interpretability and reasoning capabilities of the LLM by explicitly requesting the motivations and intermediate reasoning steps it employs to answer the question [33]. Listing 6 illustrates our implementation of the chain of thoughts method.\nKnowledge Injection (S5)\nLLMs are trained on a vast corpus of knowledge, but they might still miss context-specific information. To address this issue, fine-tuning techniques [34] revise the parameters of LLMs to include additional information about the task in question. However, due to the high computational costs associated with fine-tuning, an alternative is to engineer the prompts to include additional information. This strategy, known as knowledge injection [35], involves enriching prompts with task-specific information that the LLM may not have encountered during its initial training.\nThe BPMN standard is inherently complex, encompassing a wide range of elements and constructs. To balance the comprehensiveness of the information with the constraints of the LLM's context limit, we provide an informative summary of BPMN essential elements. The injected knowledge covers the following BPMN elements:\n\u2022 Flow objects: events, gateways, tasks, sub-processes, transactions, and call activities.\n\u2022 Connections: sequence flows, message flows, and associations.\n\u2022 Further elements: pools, lanes, data objects, groups, and annotations.\nFew-Shot Learning (S6)\nThis method leverages the LLM's ability to learn from limited examples by providing several pairs of example inputs and their expected outputs [36]. This choice over zero-shot, one-shot, and many-shot learning approaches stems from our goal to optimize the balance between model training efficiency and output accuracy. Zero-shot learning, where the model generates answers without prior related examples, often leads to less accurate or generalized responses due to the absence of context-specific training. One-shot learning provides a single example, which may not sufficiently capture the complexity or variability of the task. Many-shot learning, though potentially more effective due to a broader range of examples, requires a substantially larger dataset.\nGiven the constraints of LLM prompt size and processing capacity, we elected to utilize only one BPMN model for training, complemented with five pairs of question-and-answer examples. While we anticipate that including a broader array of examples across more models might yield more robust capabilities in BPMN comprehension, the current setup was chosen to maintain a reasonable trade-off between the prompt size and the richness of the training data. This strategic decision allows us to experiment within a feasible framework while setting the stage for potentially scaling up the number of examples or integrating more diverse models in the future to enhance the LLM's performance.\nThe BPMN model we use for implementing few-shot learning is the credit scoring process, available at https://github.com/camunda/bpmn-for-research. We crafted a set of five questions designed to challenge the LLM across various aspects of the process: starting the process, handling immediate outcomes, dealing with delays, and finalizing results. Moreover, the questions target different BPMN elements: tasks, events, pools, gateways, and message flows. The provided expected outputs for these questions were designed to not only provide accurate responses but also to use plain, accessible language that enhances comprehension for users without technical knowledge.\nNegative Prompting (S7)\nNegative prompting involves guiding the LLM by explicitly stating what should be avoided in its responses [37]. We enhance our few-shot learning demonstrations by including examples of undesired outputs"}, {"title": "4. Tool Support", "content": "In this section, we present our tool AIPA (AI-Powered Process Analyst) which integrates our LLM-based process model comprehension framework, as detailed in Section 3, with OpenAI's LLMs. This integration is flexible, allowing users to configure the tool to use any OpenAI model. The tool can be downloaded from https://zenodo.org/records/12800002 and used following the instructions contained in the README.md file.\nAIPA uses JSON as the default textual abstraction format to represent BPMN models. This choice was informed by our evaluation experiments in Section 5, which show that the JSON and Simplified XML formats provided a clearer and more effective simplification compared to the other abstraction formats we proposed in Section 3.2. The prompting strategies proposed in Section 3.3 are all implemented in AIPA and enabled by default.\nThe user can upload a BPMN model and start a chat about the uploaded model with an AI assistant. When users select specific elements within a BPMN model, the tool generates a JSON representation containing only those selected elements. This focused abstraction ensures that the LLM processes only the relevant parts of the model, enhancing both the efficiency and relevance in the analysis. Additionally, our tool supports voice interactions, allowing users to input their questions and receive responses audibly. The dynamic nature of the tool facilitates an interactive dialogue between the user and the LLM. Users can pose follow-up questions, maintaining the conversation history. Users can reset the conversation at any time to start a new chat.\nNote that before starting the conversation with the AI assistant, the user needs to configure the OpenAI connection by selecting an LLM and entering the corresponding OpenAI API key."}, {"title": "5. Evaluation", "content": "In this section, we comprehensively evaluate our framework by addressing the first three research questions we defined in Section 1:\n\u2022 Abstraction Methods (RQ1): In Section 5.2.1, we compare the different BPMN abstraction methods supported within our framework.\n\u2022 Prompting Strategies (RQ2): In Section 5.2.2, we explore the effect of various prompting strategies on the comprehension of BPMN models by LLMs.\n\u2022 State-Of-The-Art LLMs (RQ3): In Section 5.2.3, we assess the overall effectiveness of integrating state-of-the-art LLMs with our framework."}, {"title": "5.1. Setup", "content": "This section describes the experimental setup. First, we detail the dataset comprising diverse BPMN models and user queries in Section 5.1.1. Then, we outline our scoring mechanism used to evaluate LLM outputs in Section 5.1.2."}, {"title": "5.1.1. Dataset", "content": "We use three BPMN models in our experiment. First, we use a publicly available BPMN model (Healthcare Process) [38]. This model represents a standard medical process and is publicly available with a textual description. It provides a baseline for comparison but is limited to simple workflow elements.\nTo ensure an unbiased evaluation and to address the possibility that the healthcare model and its description might have been included in the training data of the LLMs, we designed two additional BPMN models specifically for this study. These models incorporate a broader range of BPMN elements such as events, data objects, and complex gateways, ensuring a comprehensive assessment of the LLMs' capabilities across various BPMN facets.\nThe following list gives an overview of the two additional BPMN models we designed for our evaluation together with the help of BPMN experts from our research team:\n\u2022 Dispatch of Goods: This process, shown in Figure 3, details the preparation and dispatch of goods, starting from the decision on shipping methods to the final packaging and shipment preparation. It includes advanced BPMN elements such as time events, data objects, and inclusive gateways.\n\u2022 Order Manufacturing: This process, shown in Figure 4, revolves around handling and fulfilling customer orders within a company, engaging multiple departments from sales to warehouse. It captures various advanced BPMN elements such as conditional flows, events (e.g., messages, compensation), and sub-processes.\nTo thoroughly evaluate the quality of answers generated by the LLMs, we generate questions that cover the various aspects These aspects were designed to cover different facets of process understanding, ranging from basic control-flow sequencing to complex data interactions and organizational roles. However, it is important to note that for the healthcare process, we do not have questions"}, {"title": "5.1.2. Outputs Scoring", "content": "In [39], evaluation criteria for LLMs' outputs on process mining tasks are proposed. These criteria include human evaluation, where a human evaluates the text provided by the LLM; automatic evaluation, which is only applicable to quantitative questions; and self-evaluation, where LLMs evaluate LLM outputs. Particularly, self-reflection methods, where the output of one session is provided to another LLM or another session of the same LLM to score the quality of the output, can be employed for self-evaluation [40].\nFor the evaluation of our framework, we decided to employ LLM self-evaluation due to the large size of the experiments. We incorporate a direct comparison of LLM outputs to ground truth answers. We utilize gpt-40 to perform a self-evaluation, assigning quality scores to LLM responses based on their alignment with the ground truth answers. To assess the robustness of our evaluation, we conducted a detailed review involving human experts; specifically, we assigned this role to the team who designed the models and the ground truth answers. We selected a sample encompassing two questions from each process, with four different LLM-generated answers for each question. Our experts were also provided with the rationale used by gpt-40 for scoring these answers, to assess both the quality and fairness of the evaluation process.\nThe feedback we received on the LLM self-evaluation has been positive in general, affirming that the quality of the evaluation is high and the reasoning behind the scores is both transparent and well articulated. Although there is an inherent level of subjectivity in any evaluation, our experts agreed that the scores assigned by gpt-40 were justified and in line with their understanding of the content. The detailed and contextually rich responses from the LLMs were particularly praised for their relevance. An oversight in detecting an error in one answer was noted; however, it did not detract significantly from the overall success of the evaluation method. In general, our experts confirmed the reliability and high accuracy of the LLM self-evaluation approach."}, {"title": "5.2. Results", "content": "The automatic evaluation is reproducible by downloading the code from https://zenodo.org/records/12800002 and executing the scripts contained in the offline_tests/questions_auto_eval.py folder. In particular, answer_questions.py is used to execute the questions and evaluate_questions.py can be used to evaluate their results. Each script can be modified with the LLM's connection parameters (API URL, name of the model, and API key) and allows the configuration of the abstraction, prompting strategies, the BPMN model, the list of questions, and the ground truth answers. The BPMN models are stored in the folder offline_tests/bpmn_models, and the questions with their ground truth answers are stored in offline_tests/data. We also collect in aipa_auto_evaluation_results.zip the evaluation results of all the experiments. For each dataset, we include the different experiments (abstractions, prompting strategies, choice of the LLM) in different folders."}, {"title": "5.2.1. Evaluating Abstraction Formats", "content": "In this section, we perform a comparative evaluation of the different abstraction formats defined in Section 3.2. We configure our framework to use gpt-40-2024-05-13, and we only enable simple prompting strategies (S1, S2, S3, and S4) that are efficient in terms of token usage, allowing us to focus on the impact of the abstraction format itself on the comprehension of BPMN models. The results of evaluating the effect of the abstraction formats on LLM comprehension are detailed in Table 5, where we report the average quality scores across various question categories, the overall average quality score, and the number of tokens required for each format.\nOur findings indicate that JSON, simplified XML (SXML), and XML generally produce similar comprehension scores, with PNG lagging behind overall. This difference becomes especially apparent for the healthcare process, where the PNG format scored very low compared to the other formats. However, PNG showed better performance in answering data-related and role-perspective questions, likely due to its visual nature, which makes artifacts and swimlanes more recognizable."}, {"title": "5.2.2. Evaluating Prompting Strategies", "content": "In this subsection, we explore the impact of the prompting strategies we defined in Section 3.3 on LLMs' comprehension of BPMN models. We employing both the JSON and simplified XML abstraction formats for each BPMN model, and we configure our framework to use gpt-40-2024-05-13. Each prompting strategy is assessed individually to determine its specific effect; however, for practical applications, combining these strategies is recommended to achieve optimal results. The average quality scores and standard deviation values for each strategy are reported in Table 6.\nThe results show that strategies S3 (non-technical restriction), S6 (few-shot learning), and S7 (negative prompting) stand out with particularly strong positive impacts across the different models and abstraction formats. For S3, restricting LLMs to use only natural language significantly enhances model comprehension, likely due to improved readability and accessibility of the information. Strategies S6 and S7 also align with expectations, showing a positive effect and confirming the usefulness of both positive and negative example-based learning for enhancing LLM comprehension.\nConversely, strategies S1 and S2, which involve role prompting, demonstrate minimal impact on the comprehension capabilities of LLMs. This outcome suggests that simple identity-based prompts do not significantly influence LLM performance in this context. Similarly, the chain-of-thoughts approach (S4) and the knowledge injection strategy (S5) show limited effects. For S4, the tendency to generate unnecessarily lengthy outputs might overwhelm the user with information that does not directly aid in model comprehension. For S5, the lack of observed improvement might be attributed to the extensive availability of information on the BPMN 2.0 standard online, which the LLM could have already encountered during its training. Future work could explore more targeted knowledge injection strategies, focusing specifically on the complex elements of the given BPMN model rather than providing general knowledge."}, {"title": "5.2.3. Evaluating State-Of-The-Art LLMs", "content": "In this section, we evaluate the performance of state-of-the-art LLMs on our framework. We configure our framework to use the Simplified XML (SXML) abstraction, and we enable the first four prompting strategies (S1, S2, S3, and S4). We have selected four LLMs for this analysis:\n\u2022 GPT-40-2024-05-13: The latest iteration of OpenAI's language models\u00b9, known for its broad and extensive training on a diverse dataset which may enhance its capability in domain-specific questions. The context window of this LLM is 128K. It also supports visual prompts.\n\u2022 GPT-4-Turbo-2024-04-09: A variant of GPT-4 optimized for speed and efficiency, potentially sacrificing some depth in exchange for faster response times. The context window of this LLM is 128K. It also supports visual prompts. On a note, this LLM is outperformed in most known benchmarks by GPT-40 in both response time and quality.\n\u2022 Microsoft WizardLM-2-8x22B 2: An open-source high-capacity model from Microsoft, designed to excel in deep contextual understanding and complex reasoning tasks. The context window of this LLM is 64K, still allowing for the full provision of our considered textual prompts.\n\u2022 Mixtral-8x22B-Instruct v0.1 3: An open-source instructive model geared towards following explicit user instructions with high precision, using fewer tokens for outputs which might affect the depth of generated content. The context window of this LLM is 64K, still allowing for the full provision of our considered textual prompts.\nWe also considered smaller LLMs (Mixtral-8x7B-Instruct v0.14, Codestral5, Mistral 7B6) in our initial experiments, but in light of significantly worse results, we discarded them. Some other LLMs were discarded for their limited context window (Llama3-70B7, Nemotron-4-340B-Instruct\u00ae).\nThe results of our comparative analysis are shown in Table 7, where we report the average quality scores for different question categories along with the number of output tokens produced by each LLM.\nAll LLMs demonstrated similar levels of performance; however, GPT-40-2024-05-13 stands out in domain-specific questions, possibly due to its more extensive training on a wider range of topics and scenarios. This suggests that a broader training corpus can significantly benefit domain-specific comprehension.\nNotably, the results show that open-source LLMs perform comparably to commercial alternatives, highlighting the advancements and accessibility in LLM technology. Interestingly, there is a consistent trend where increased token usage correlates with better scores. This suggests that more detailed responses, which utilize more"}, {"title": "6. User Study", "content": "In this section, we present a user study designed to assess the usability and effectiveness of our LLM-based framework for process model comprehension in real-world contexts. The study specifically addresses RQ4 (cf. Section 1), exploring how industry experts in the field of business process management perceive and interact with BPMN models using our tool, AIPA. It is important to note that the version of AIPA used in this study did not include the voice support functionalities, as these features were added subsequently. The user study was conducted by configuring AIPA to use gpt-4-1106-preview."}, {"title": "6.1. Setup", "content": "We conducted a total of six interviews, each lasting between 45 minutes and one hour. The interview partners were contacted via email and selected for their extensive experience with BPMN modeling in professional contexts and their background knowledge of how LLMs work. The experts were identified through the professional networks of the authors, who are well-connected within the BPM research and industry community. The selected experts represent a diverse range of professional experiences, from 0-5 years to 15-20 years in the field. All invited experts agreed to participate in the user study.\nThe user study comprised six semi-structured interviews guided by a 16-question interview guide. Conducting semi-structured expert interviews allowed us to focus on the research topic while gathering in-depth information [41].\nThe interview guide is divided into three sections. The first section consists of three questions aimed at understanding the interviewee's expertise and creating a comfortable interview atmosphere, as these questions are easy to answer.\nNext, we presented AIPA, explained its functionality, and gave the interviewees enough time to familiarize themselves with the interface and the selected BPMN model. We chose a BPMN model of low complexity to facilitate quick understanding and interaction. Once they indicated that they had understood the model, we asked the interviewees to ask the AI assistant questions about the BPMN model displayed. On average, four questions were asked by each interviewee.\nAfter the user had time to interact with the tool, we moved on to the second section of the interview guide. The first question focused on evaluating the user interface, while the remaining eleven questions were aimed at evaluating the responses. These eleven questions are structured using eleven criteria derived from the study [42] which utilized these criteria to evaluate the usability of explainable AI systems. For each question, the interviewees were first asked to provide a rating on a five-point scale about the degree of fulfillment of the criterion and then to explore the reasons for the rating. By incorporating these tested criteria, we have followed a robust evaluation framework that improves the functionality of our system and ensures its usability.\nThe following list shows the eleven criteria from [42] we used to define the questions of the second phase of the interview guide:\n(C1) Soundness: Ensures that the explanations accurately reflect the operations of the underlying model, emphasizing the truthfulness of the information presented\n(C2) Completeness: Measures whether the explanation covers enough context and cases to be useful across various scenarios, not just the specific instance it was generated for.\n(C3) Contextfullness: Provides explanations with sufficient background to allow users to understand them in relation to their broader operational environment.\n(C4) Interactiveness: Allows users to engage with the system dynamically, adjusting and querying the explanations to better suit their understanding or needs.\n(C5) Actionability: Ensures that the explanations guide users towards meaningful actions based on the insights provided.\n(C6) Chronology: Considers the timing of events or data points in explanations, highlighting the most recent or relevant information that impacts the outcome.\n(C7) Coherence: Focuses on the consistency of explanations with the user's prior knowledge and expectations, making them intuitively easier to accept.\n(C8) Novelty: Ensures that the information provided is insightful, revealing new information that can provoke thought or lead to unexpected insights.\n(C9) Complexity: Addresses the level of detail in the explanations, ensuring they are neither too simple to be trivial nor too complex to understand.\n(C10) Personalization: Tailors explanations to the background knowledge and specific needs of individual users, enhancing their relevance and accessibility.\n(C11) Parsimony: Emphasizes the brevity of explanations, ensuring they are concise and avoid overloading the user with unnecessary information."}, {"title": "6.2. Results", "content": "This section provides a comprehensive summary of the interview results, offering insights and key findings derived from the discussions.\nUser-Friendliness. The user-friendliness aspect of the tool is highly praised as it is clear and concise, making it easy for users to interact with the system (Experts 1, 4). Users appreciate that the BPMN model is visible and can be reset, but they suggest further improvements such as displaying multiple process models at the same time to be able to simultaneously analyze several business processes that are connected with each other and improving visual elements, such as adapting the size of the window that contains the model (Expert 3). The ability to drag elements of the BPMN model directly on the user interface is highlighted as positive (Expert 5). It is suggested to further integrate the possibility of enlarging the model view to allow better interaction (Experts 5, 6) and possibly implement a function that highlights in color which part of the model the AI assistant's response refers to (Expert 6).\nSoundness (C1). Regarding the question of soundness, the explanations provided by the AI assistant were generally effective and answered most questions well, although sometimes the answers could have been more precise (Experts 1, 3, 4). The AI assistant explained elements such as message flow accurately, although its impact on the workflow engine was somewhat unclear (Expert 3). It performed well in explaining the exclusive gateway and notations (Expert 3). Overall, more than 80% of the information was correct (Expert 5). However, some issues were also uncovered, such as the AI assistant confusing signals and messages, indicating the need for careful use of correct terminology where a message should be consistently referred to as a message and not a signal (Expert 4). In addition, the AI assistant initially omitted an activity when describing the minimum process and only correctly identified the \u201cdraft invoice\u201d activity after repeated requests.\nCompleteness (C2). In response to the question regarding the completeness of the answers, the feedback shows that the explanations are well-generalized and applicable to different elements of BPMN (Experts 1 - 6). The answers are effective for common processes"}]}