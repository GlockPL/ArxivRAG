{"title": "CoVis: A Collaborative Framework for Fine-grained Graphic Visual Understanding", "authors": ["Xiaoyu Deng", "Zhengjian Kang", "Xintao Li", "Yongzhe Zhang", "Tianmin Guo"], "abstract": "Graphic visual content helps in promoting information communication and inspiration divergence. However, the interpretation of visual content currently relies mainly on humans' personal knowledge background, thereby affecting the quality and efficiency of information acquisition and understanding. To improve the quality and efficiency of visual information transmission and avoid the limitation of the observer due to the information cocoon, we propose CoVis, a collaborative framework for fine-grained visual understanding. By designing and implementing a cascaded dual-layer segmentation network coupled with a large-language-model (LLM) based content generator, the framework extracts as much knowledge as possible from an image. Then, it generates visual analytics for images, assisting observers in comprehending imagery from a more holistic perspective. Quantitative experiments and qualitative experiments based on 32 human participants indicate that the CoVis has better performance than current methods in feature extraction and can generate more comprehensive and detailed visual descriptions than current general-purpose large models.", "sections": [{"title": "I. INTRODUCTION", "content": "Graphic visual content, particularly images, can convey richer information than texts. Compared to videos, it efficiently communicates concise, non-spatiotemporal information, thereby enhancing the efficiency of production and work [1]. However, an individual's understanding of visual content is often limited by complex factors such as personal life experiences and knowledge background, leading to the effect of information silos [2]. These incomplete interpretations of images may impose potential limitations on the quality of visual guidance for observers. Such limitations can restrict thought processes, confidence, and even cause misunderstandings [3]. In turn, these factors can affect the quality of visual content communication and pose obstacles to potential creation [4], design [5], and optimization [6].\nCurrent research in visual image understanding spans a wide range of domains, including image classification [7], analysis [8], retrieval [9], and description [10]. However, these methods are often constrained by the data and model scale, limiting them to solving single-dimensional image understanding problems. This limitation stems from the fact that the knowledge base of small-scale models within a singular domain has not yet reached the level of extensiveness observed in human cognition [11]. As a result, these methods struggle to provide comprehensive and objective assistance in visual perception. Consequently, observers' understanding of images largely depends on their own experiential background and cognition, which affects the objectivity and quality of information understanding.\nTo help break through the limitations of information silos and achieve a more objective and comprehensive understanding of visual semantics, this paper introduces CoVis, a collaborative framework for fine-grained visual understanding, to optimizes a cascaded visual segmentation module based on the FastSam (Fast Segment Anything Model) [12] and U-Net [13] models and bridges it with a multimodal textual content generation model based on ChatGPT 4. By combining these components with prompt word engineering, the framework generates interpretive text for the main subjects of visual images, assisting observers in understanding images more comprehensively, efficiently, and objectively, as illustrated in"}, {"title": "II. RELATED WORK", "content": "At present, research on visual understanding is extensive, encompassing various domains within computer vision such as image classification, identification, detection, description, and retrieval. For instance, Kim et al. [14] have achieved high-quality medical image classification through a transfer learning-based visual understanding solution and have validated the optimal performance of their proposed method. Gulzar et al. [15] addressed the feature recognition challenge in fruit classification with an image recognizer based on the MobileNet V2 network, demonstrating the best performance through extensive experiments. Additionally, Peng et al. [16] proposed an industrial-grade solution framework-based image detector, achieving high-quality fruit ripeness prediction in the agricultural production field, significantly enhancing the efficiency and quality of production work. Li et al. [17] presented a noise-robust image classification framework by integrating a cascaded CNN algorithm, thereby achieving optimal performance. However, these methods are not suitable for comprehensive understanding of image content, as they are primarily aimed at understanding image content in a single domain. They also suffer from the limitations of information silos when assisting visual content observers in understanding images from a more comprehensive perspective. Although some large models have shown high-quality generalization performance in general domains, these models may still have issues such as hallucinations [18] and randomly generated content [19]."}, {"title": "B. Visual Understanding in Human-Machine Collaboration", "content": "Currently, human-computer collaborative systems have significantly enhanced the quality and efficiency of human production, work, and daily life. For instance, Nardo et al. [20] designed a human-computer collaborative system under the backdrop of Industry 4.0, which has improved production efficiency and reduced resource waste in the industrial production field. Interactive systems [21] based on computer vision for human-computer collaboration can achieve low-cost fruit development quality detection in the agricultural production field, thereby significantly enhancing production efficiency and output. Moreover, the introduction of deep learning technology can make up for the potential risks of errors in manual operation and identification processes. Human-computer collaborative systems [22] that integrate CNN and LSTM technologies can accurately recognize building structures in the construction field, thereby verifying the accuracy and rationality of drawings, assisting in correcting potential problems and errors of architects, and thus reducing potential risks and enhancing the reliability of the entire construction project. Furthermore, in the field of education, Othman et al. [23] proposed a collaborative system for computer science education for college students, which not only ensures interest but also conveys subject knowledge of high quality. In the field of art, Feng et al. [24] proposed a collaborative system for creation that achieves high-quality content creation through understanding and analyzing images. In the field of bio-medical research, Sicho et al. [25] developed an interactive collaborative system that can automatically generate potential drug structures, thereby significantly improving the efficiency of drug design and greatly reducing the cycle and consumption of human resources in drug research and development."}, {"title": "III. METHODOLOGY", "content": "In this paper, we propose an approach targeting general visual content understanding through a multi-stage image segmentation and language generation method. As illustrated in Fig. 2, the proposed framework includes a coarse-grained and a fine-grained segmentation module, along with a cascaded content generator. Specifically, we employ a FastSAM-based module for coarse-grained segmentation. Afterward, a U-Net enabled fine-grained image segmentation module is imported and bridge the gap with Large Language Models (LLMs) to produce interpretive text for visual images. To enhance the quality of generated textual content, we incorporate Prompt Engineering techniques. Through cooperation with professional designers, the text of prompt words, including color, composition, connotation, and other dimensions, is deliberately designed, thereby refining the accuracy and consistency of text output."}, {"title": "B. Coarse-grained Segmentation Module", "content": "The backbone network of the coarse-grained segmentation module is FastSAM, an advanced pre-trained model optimized for swift image segmentation. It specializes in quickly identifying and segmenting primary objects within an image. FastSAM operates on the principle of feature extraction, generating masks for objects of interest and providing the foundational outline and location necessary for subsequent fine-grained segmentation. A significant advantage of FastSAM is its ability to produce segmentation results without the need for domain-specific data training. This capability allows it to segment images effectively, overcoming the limitations posed by non-gaseous components and high-temperature environments that can affect tools such as laser spectroscopy and CCD cameras.\nThe FastSAM model leverages the convolutional neural network (CNN) architecture, a cornerstone in deep learning, to extract multi-level feature information from input images through its feature extraction network. The model's architecture consists of the following components:\n\u2022 An input layer that receives the original image I.\n\u2022 A feature extraction module utilizing multiple convolutional layers to distill features.\n\u2022 An object recognition module responsible for generating bounding boxes B and masks Mgroup.\nThe feature extraction process of the FastSAM model can be articulated as:\nF = $(I) \\qquad (1)\nAmong them, the extracted feature maps are denoted as F, $ represents the output of the feature extraction network. Subsequently, the model employs the following formula to generate bounding boxes and masks:\n\u0392 = \u03b3(F) \\qquad (2)\nMgroup \u03b4(F) \\qquad (3)\n\u03b3 and 8 represent the boundary box generation function and the mask generation function, respectively. The final coarse-grained segmentation result can be represented as:\nMgroup = {(Bi, Mi) | i \u2208 [1, N]} \\qquad (4)"}, {"title": "C. Fine-grained Segmentation Module", "content": "For the fine-grained segmentation module, we have strategically adopted a U-Net architecture, which is widely recognized for its proficiency in detailed image segmentation. By synergistically combining FastSAM for coarse segmentation with the U-Net for refining the segmentation details, we significantly augment the precision of the overall segmentation process. Specifically, following the acquisition of coarse-grained segmentation, the U-Net serves as a fine-grained segmentation module, further refining the boundaries of the objects. The U-Net employs an encoder-decoder architecture, which is adept at progressively restoring the detailed parts of an image, making it particularly suitable for segmentation tasks that demand precise boundaries. By taking the segmentation output from FastSAM as the initial input, U-Net is able to accurately refine the object boundaries, yielding high-resolution segmentation outcomes.\nThe U-Net architecture is comprised of an encoder and a decoder. The encoder progressively extracts features, while the decoder utilizes these features to perform high-precision fine-grained segmentation. The input for fine-grained segmentation is the coarse-grained segmentation result Mgroup generated by FastSAM. The output result of the U-Net model can be represented as:\nMfine = fU-Net(Mgroup) \\qquad (5)\nWhere Mgroup is the fine-grained segmentation result. The fine-grained segmentation process encompasses key technical steps as follows: Firstly, feature extraction is conducted through convolutional layers to capture intrinsic image properties. Subsequently, up-sampling within the decoder progressively restores the spatial information of the image. Finally, the model generates fine-grained masks for each object with precision. Through this sequence of operations, U-Net is capable of producing more accurate fine-grained segmentation outcomes based on the initial coarse-grained segmentation."}, {"title": "D. Cascaded Content Generator", "content": "To optimize the generation of interpretive text from visual images, we have integrated a Large Language Model (LLM) enhanced with Prompt Engineering techniques. This fusion of cutting-edge technologies not only elevates segmentation accuracy but also ensures a highly efficient and structured language output during the visual-to-text transformation process. Specifically, to generate multi-dimensional, fine-grained image descriptions, we propose a 3-step systematic approach for designing prompts based on prompt engineering:\nNeeds Analysis. In the initial phase, we collaborate with professional designers to delineate the requirements for image description. This involves identifying key elements within the image, such as color, composition, and connotation, and determining how these elements can be translated into dimensions of textual description. This step is crucial as it encompasses a deep understanding and analysis of the image content, ensuring that all relevant details and features are captured.\nPrompt Design. Based on the outcomes of the needs analysis, we craft a series of prompt words that serve as inputs to the model, guiding it to generate descriptions encompassing the desired dimensions. These prompt words are meticulously constructed to effectively direct the model's attention to the critical features of the image:\n\u2022 Color-related: bright, dull, warm tones, cool tones, etc.\n\u2022 Composition-related: balanced, symmetrical, dynamic, static, etc.\n\u2022 Connotation-related: abstract, realistic, dreamlike, surreal, etc.\nPilot Experiment Evaluation. After the initial prompts are designed, we conduct a small-scale experiment to assess their effectiveness. We use these prompts to generate image descriptions and compare the generated descriptions with the actual content of the images.\nThe segmentation results from FastSAM and U-Net are encoded as feature inputs. These features are used to prompt the LLM to generate descriptive text. The process can be formalized as follows:\nFeatureInputs = Encode(FastSAM,U_Net) \\qquad (6)\nDescriptiveText = LLM(FeatureInputs) \\qquad (7)\nOutput = Prompt Engineer(LLM, Feature_Inputs) \\qquad (8)\nThis method ensures that the generated text is not only accurate but also contextually relevant, providing a comprehensive understanding of the image's content."}, {"title": "IV. EVALUATION", "content": "During the experimental process, we opted for high-performance hardware, including Intel Core i7 processors, NVIDIA GeForce RTX 3080 graphics cards, and the robust Windows 10 Pro version operating system. Code development was efficiently managed through the Jupyter platform."}, {"title": "B. Quantitative Evaluation on Image Segmentation", "content": "For quantitative analysis, we benchmarked our method against 8 established baselines: PFNet [26], UNet [27], SDTC [28], MBV3 [29], BASNet [30], HySM [31], U2Net-Tiny [32], and the proposed CoVis. Our evaluation criteria encompassed Fmax , Fmaxsure , Fweighted , MAE (Mean Absolute Error), SMeasure, and EMeasure.\nBy comparing our proposed CoVis with these 8 baselines, the results, as shown in Table I, demonstrate that, our proposed approach has achieved improvements of 1.2%, 4.7%, 8.9%, 1.7%, and 1.0% in Fmaxsure, Fweighted , MAE, SMeasure, and EMeasure metrics, respectively, over the state-of-the-art algorithms. This proves that the framework combining FastSAM with U-Net possesses robust and high-quality image segmentation performance, surpassing current mainstream advanced methods, laying a good foundation for subsequent image analysis."}, {"title": "C. Ablation Evaluation", "content": "In the ablation study, we systematically removed the SAM and U-Net detectors from our framework to evaluate their individual contributions, utilizing the same metrics as in our comparative studies. The studies were designed to meticulously assess the contribution of each integral component of the CoVis framework: the FastSAM and U-Net modules. By systematically eliminating them from the complete framework, we can evaluate the performance of the resultant models. The results revealed that the CoVis framework, incorporating both FastSAM and U-Net, consistently outperformed its counterparts with individual components removed. The results, as presented in the table below, showcasing that our proposed method secured the highest scores across a range of critical metrics, including the Fmax Measure, MAE, SMeasure, and EMeasure."}, {"title": "D. Qualitative Evaluation", "content": "For qualitative evaluation, we engaged 32 participants from North America and East Asia to rate the content generation of randomly selected images based on satisfaction, accuracy, and creativity. In order to evaluate the performance of the CoVis in practical applications, we randomly selected 6 images from the test dataset to evaluate both the advanced Chat GPT4-Mini and Chat GPT4 models in comparison with our proposed method. Subsequently, we invited 32 human participants from Asian and American regions to rate descriptions generated by the 3 approaches on satisfaction, accuracy, and creativity, using a 1-to-4 scale (with higher scores indicating better performance). The participants include experts from the art field, designers and randomly recruited users from the Internet. The participants' information and the qualitative assessment results are presented in Tables III & IV. The results indicate that our method received the highest scores across all metrics, including satisfaction, accuracy, and creativity."}, {"title": "E. Generalization Evaluation", "content": "To assess the generalization capabilities of the proposed CoVis method, we conducted experiments across multiple datasets, including DIS-VD, ImageNet-S, and PhenoBench. As shown in the results table below, our approach exhibits high efficacy and robustness across these diverse datasets. This demonstrates that the CoVis framework can effectively satisfy a wide range of visual content presentation needs, making it a versatile solution for a potentially larger audience."}, {"title": "V. CONCLUSION", "content": "This paper introduces the CoVis framework, a collaborative approach for fine-grained graphic visual understanding. The proposed method addresses existing inefficiencies in visual communication by incorporating a cascaded dual-layer segmentation network, complemented by a large-model-based content generator. This integrated framework automates the generation of visual analytics for images, facilitating a more comprehensive understanding of graphic visual contents by extracting a greater amount of information from visual data.\nThrough extensive quantitative and qualitative experiments, the manuscript demonstrates that the proposed model exhibits enhanced stability, robustness, and insightfulness. These attributes contribute to the improvement of the quality and effectiveness of human-computer interaction within the realm of Computer-Supported Cooperative Work (CSCW) community. Furthermore, this paper reports on the results of generalization experiments, indicating that the proposed framework possesses broad applicability and warrants further exploration. For instance, as a mode of man-machine cooperation for life support, the framework has the potential to significantly assist vulnerable populations, such as the visually impaired, in navigating the visual challenges they encounter in daily life.\nHowever, the current CoVis framework still has some limitations, such as the lack of personalized, targeted content generation strategies. To further improve the efficiency of computer-supported cooperative work, future work would explore incorporating user-specific preferences to enable the targeted generation of stylized visual analysis content."}]}