{"title": "Structure-prior Informed Diffusion Model for Graph Source Localization with Limited Data", "authors": ["Hongyi Chen", "Jingtao Ding", "Xiaojun Liang", "Yong Li", "Xiao-Ping Zhang"], "abstract": "The source localization problem in graph information propagation\nis crucial for managing various network disruptions, from mis-\ninformation spread to infrastructure failures. While recent deep\ngenerative approaches have shown promise in this domain, their ef-\nfectiveness is limited by the scarcity of real-world propagation data.\nThis paper introduces SIDSL (Structure-prior Informed Diffusion\nmodel for Source Localization), a novel framework that addresses\nthree key challenges in limited-data scenarios: unknown propa-\ngation patterns, complex topology-propagation relationships, and\nclass imbalance between source and non-source nodes. SIDSL incor-\nporates topology-aware priors through graph label propagation and\nemploys a propagation-enhanced conditional denoiser with a GNN-\nparameterized label propagation module (GNN-LP). Additionally,\nwe propose a structure-prior biased denoising scheme that initial-\nizes from structure-based source estimations rather than random\nnoise, effectively countering class imbalance issues. Experimental\nresults across four real-world datasets demonstrate SIDSL's supe-\nrior performance, achieving 7.5-13.3% improvements in F1 scores\ncompared to state-of-the-art methods. Notably, when pretrained\nwith simulation data of synthetic patterns, SIDSL maintains robust\nperformance with only 10% of training data, surpassing baselines\nby more than 18.8%. These results highlight SIDSL's effectiveness\nin real-world applications where labeled data is scarce.", "sections": [{"title": "Introduction", "content": "In today's highly interconnected world, graph information propa-\ngation issues, such as misinformation spread, cyber threats, and in-\nfrastructure failures, have far-reaching consequences for society [4].\nThe ability to quickly identify the source of these disruptions is crit-\nical for mitigating their impact. By analyzing snapshots of affected\nnetworks, we can trace the origin of the spread, a process essential\nfor managing crises like disease outbreaks [24], enhancing network\nsecurity [15], and preventing further damage in scenarios such as\npower grid failures [2].\nEarly methods [18, 21, 22, 26, 42, 43] for source localization\nin graphs rely on metrics or heuristics derived from the graph's\ntopology, applicable only to specific propagation patterns like the\nSusceptible-Infected (SI) or Independent Cascade (IC) models. No-\ntably, Wang et al. [34] overcome this limitation by introducing a la-\nbel propagation algorithm based on the intuition of source centrality.\nNevertheless, the approach has limitations in both expressiveness\nand scalability, as they cannot effectively encode graph topologi-\ncal information. Data-driven methods [5, 10, 31] overcome these\nlimitations as they directly learn graph neural networks (GNNs) to\ncapture the propagation process exhibited in empirical data, but still\nneglect the indeterminacy of information propagation that corre-\nsponds to the uncertain nature of source localization [20]. Recently,\ndeep generative models including variational autoencoders [20],\nnormalization flows [36] and diffusion models [11, 37] have been\nadopted for solving the source localization problem. These prob-\nabilistic generative methods can quantify the indeterminacy in\nsource localization by learning the empirical data distribution and\nadvance the state-of-the-art performance.\nHowever, collecting real-world propagation data for deep genera-\ntive methods is difficult and costly, posing significant requirements\non source localization models that can adapt to real-world envi-\nronments with limited data. This brings up three main following\nchallenges.\n\u2022 Firstly, real-world graphs typically exhibit unknown\npropagation patterns, which becomes far more chal-\nlenging to characterize when data is limited. In this\nregard, existing methods [5, 20, 31, 37] rely purely on data to\ngain an understanding of the propagation patterns, limiting\ntheir capability to generalize in unseen scenarios.\n\u2022 Secondly, complex interrelations between propagation\npatterns and graph topology are difficult to capture\nwith limited data. Existing deep learning methods rely on\na large amount of labeled data from the target network (i.e.,\nidentified source nodes from historical propagation) to ac-\ncount for the impact of structural heterogeneity on propa-\ngation patterns. Due to this heavy dependence on labeled\ndata, these models typically underperform when applied\nto networks with limited training samples, hindering their\npractical applicability.\n\u2022 Thirdly, the inherent class imbalance between source\nand non-source nodes becomes more harmful under\ndata scarcity, compromising the accuracy and reliabil-\nity of source identification. In most propagation scenarios,\nsource nodes naturally constitute a small minority of the to-\ntal graph nodes [3]. Limited training data can further amplify\nthis problem, leading to biased distribution models that tend\nto degenerate towards predicting all nodes as non-sources."}, {"title": "Related Works", "content": "As the inverse problem of information propagation on networks,\nsource localization refers to inferring the initial propagation sources\ngiven the current diffused observation, such as the states of the\nspecified sensors or a snapshot of the whole network status [27]. It\ncan be applied to tasks like rumor source identification and finding\nthe origin of rolling blackouts in intelligent power grids [27]. Early\napproaches are rule-based and rely on metrics or heuristics derived\nfrom the network's topology for source identification [25, 42, 43].\nFor example, Shah and Zaman [25] develop a rumor-centrality-\nbased maximum likelihood estimator under the Susceptible-Infected\n(SI) [16] propagation pattern. This kind of method fails to effectively\nencode topology information. Later, deep learning-based methods\ndevised for capturing the topological effect exhibited in empirical\ndata have been proposed [5, 9, 18, 21, 31, 34]. However, most of\nthem fail to model the uncertainty of the location of sources, as the\nforward propagation process is stochastic. To overcome this, deep\ngenerative models have been adopted [11, 20, 33, 36, 37]. SLVAE [20]\nutilizes the Variational Auto-Encoders (VAEs) backbone and op-\ntimizes the posterior for better prediction. However, it is difficult\nto converge when the propagation pattern is complicated due to\nthe nature of VAEs. DDMSL [37] models the Susceptible-Infected-\nRecovered (SIR) [16] infection process into the discrete Diffusion\nModel (DM) [8], and design a reversible residual block based on\nGraph Convolutional Networks (GCNs) [17]. However, it requires\nadditional intermediate propagation data and cannot be generalized\nto other propagation patterns. Our method demonstrates superior\nfunctionality and adaptability for real-world applications, requiring\nfewer input data while addressing existing limitations, thus offering\ngreater practical value. We provide a comparison of typical source\nlocalization methods in the Appendix A."}, {"title": "Typical Propagation Models", "content": "Information propagation estimation models information spread\nin networks and explains propagation sources, with applications\nin event prediction [40], adverse event detection [32], and disease\nspread prediction [29]. Two main model categories exist: infection\nmodels and influence models. Infection models like Susceptible-\nInfected (SI) and Susceptible-Infected-Susceptible (SIS) manage tran-\nsitions between susceptible and infected states in networks [13, 16].\nIn these models, infected nodes attempt to infect adjacent nodes\nwith probability \u03b2 at each iteration, while in SIS, infected nodes may\nrevert to susceptible with probability \u03bb. The Susceptible-Infected-\nRecovered (SIR) model extends this by adding a recovered state.\nIndependent Cascade (IC) and Linear Threshold (LT) [14] are\ninfluence models that examine influence spread in social and in-\nfrastructure networks. In the IC model, nodes are either active or\ninactive, starting with initial active nodes. Newly activated nodes\nget one chance to activate inactive neighbors, with activation prob-\nability based on edge weight. The LT model activates inactive nodes\nwhen accumulated neighbor influence exceeds a threshold."}, {"title": "Preliminaries", "content": "Our research problem is formulated as follows. Given an undirected\nsocial network G = (V, E) where V is the node set, E is the edge\nset, and Y = {Y1, . . ., Yv } is an infection state of all nodes in G,\nwhich describes that a subset of nodes in G have been infected. Each\nY; \u2208 {1,0} denotes the infection state of node vi \u2208 V, where Y\u2081 = 1\nindicates that vi is infected and otherwise Yi = 0 indicates it is\nuninfected. We aim to find the original propagation source X from\nthe propagated observation Y, so that the loss with the ground Truth\nsource set X* \u2208 {1,0}|V|\u00d71 is minimized, i.e. X = argminx||X \u2013\nX*||2. To account for the uncertainty in source localization, we\nneed to construct a probabilistic model P(X|Y, G), which can be\nused to sample for the final prediction."}, {"title": "Label Propagation based Source Identification", "content": "In realistic situations, the intractable propagation process does not\nhave an explicit prior, and it is also challenging to value appropri-\nate parameters for the pre-selected underlying propagation model.\nTo address this, Wang et al. [34] propose Label Propagation based\nSource Identification (LPSI). Since LPSI investigates the same prob-\nlem as our research, we use it as a baseline to compare performance.\nLPSI captures source centrality characteristics in the method design.\nThe centrality of sources shows that nodes far from the source are\nless likely to be infected than those near it [26], which can also be\nobserved in the real-world data by our analysis in the Appendix B.\nBased on these ideas, they propose to perform label propagations on\nthe observation state of the network. By setting Y[Y = 0] = -1 and\nZt=0 \u2190 Y, the iteration of label propagation and the convergence\nstates are as follows:\n$$\u0396^{t+1} = \u03b1 \u03a3 S_{ij}\u0396_{j}^{t} + (1 \u2212 \u03b1)Y_{i}$$\nj:jeN(I)\n$$Z$ finally converges to:$Z* = (1-\u03b1)(I\u2212\u03b1S)^{-1}Y$,where S = D^{-1/2}AD^{-1/2}\nis the normalized weight matrix of graph G, \u03b1 is the fraction of\nlabel information from neighbors, and N(i) stands for the neighbor\nset of the node i. After obtaining the converged label matrix Z*,\none node can identified as a source when its final label is larger\nthan its neighbors. While node labels alone cannot fully capture\ncomplex structural information, this method still effectively identi-\nfies structural patterns related to source centrality (Appendix B).\nIn our work, we leverage the idea of LPSI to generate structural\nguidance for efficient source identification and details can be found\nin Section 4."}, {"title": "SIDSL: the Proposed Method", "content": "To capture the indeterminacy of the ill-posed localization problem,\nit is essential to build a probabilistic model that can also leverage\nthe topological information in the graph structure. We consider\nusing the generative diffusion model (DM) framework to tackle this\nrequirement by modifying it as a source predictor, which classifies\neach node into two categories: source or non-source. The diffusion\nmodel aims to learn source distributions by gradually adding Gauss-\nsian noise in samples in the forward process and learning to reverse\nthis process using denoising networks conditioned on observations\nand graph structure.\nThe forward process of the diffusion model is to gradually cor-\nrupt the initial source labels (X = X) by adding Gaussian noise\nover n timesteps, which can be formulated as:\n$$p(X_{1:n}|X_o, Y, G) = \\prod_{t=1}^{n}p(X_t|X_{t-1}, Y, G)$$\n$$p(X_t|X_{t-1}, Y, G) = N(X_t; \\sqrt{1 - \u03b2_t}X_{t-1}, \u03b2_tI)$$\nwhere \u03b2t is the noise schedule, Xo represents the initial source la-\nbels, Y denotes the observations, G represents the graph structure,\nand Xt denotes the noisy features at timestep t. At the end of for-\nward process, Xn becomes pure Gaussian noise. While the forward\nprocess only requires source labels, the reverse process leverages\nobservation Y and graph structure G as conditional inputs to guide\nthe denoising.\nIn the reverse process, it gradually transforms pure Gaussian\nnoise into the original source labels, generating new samples X \u2208\n[0, 1]|V|\u00d71 representing the probability of each node being a source.\nThe final source predictions are obtained by thresholding these\nprobabilities. To enhance stability across different propagation pat-\nterns, we leverage graph structure-based source estimations as\nconditional priors. These priors Xest = Z* are obtained through\nlabel propagation (Section 3.2), which identifies potential sources\nby aggregating infection values through the graph's normalized\nweight matrix while capturing source centrality characteristics. The\nconvergence result Z* reflects each node's structural importance\nrelative to the observed infection pattern.\nThe reverse process can be formulated as:\n$$q(X_{n-1:0}|X_n, Y, G) = \\prod_{t=n}^{1}q(X_{t-1}|X_t, Y,G)$$\n$$q(X_{t-1}|X_t, Y, G) = N(X_{t-1}; \u03bc_{\u0398}(X_t, t, Y, G, X_{est}), \u03c3_tI)$$\nwhere \u03bc\u03b8 is parameterized by a denoising network fe that predicts\nthe mean of the Gaussian distribution and \u03c3\u03b5 is the predicted vari-\nance. The diffusion framework, whose iterative diffusion process\nenables fine-grained integration of prior knowledge at multiple\nscales through its progressive denoising steps, allows for elegant\nintegration of prior knowledge. By integrating topology-aware\npriors Xest as stable guidance across different propagation pat-\nterns, our diffusion framework combines structural knowledge\nwith data-driven pattern learning, enhancing generalization to new\npropagation scenarios."}, {"title": "Structure-prior biased Denoising Process", "content": "In most propagation scenarios, source nodes constitute a small\nminority of total graph nodes [3]. With limited training data, this\nsevere class imbalance often leads models to degenerate towards\npredicting all nodes as non-sources, as the model overfits to the\ndominant non-source class. To address this, inspired by Han et al.\n[7], we propose an structure-prior biased denoising scheme that set\nthe prior of the denoising process to structure-based source estima-\ntions rather than random noise (illustrated in Figure 2). This design\nleverages two key insights: (1) structure-based estimations provide\nreliable initial source candidates by capturing inherent source struc-\ntural properties like centrality, making them more informative than\nrandom noise, and (2) starting the denoising process from these es-\ntimations creates a natural bias in the trajectory space, encouraging\nthe model to explore regions with higher likelihood of containing\ntrue sources. By incorporating these topology-aware priors into\nthe initialization and denoising process, our approach effectively\nprevents model collapse while reducing the data requirements for\nlearning accurate source patterns.\nSpecifically, we first modify the mean of the diffusion endpoint\n(or reverse starting point) as the graph structure-based source esti-\nmation Xest = Xest (Y, G) instead of using standard Gaussian noise,\ni.e.:\n$$p(X_n|Y, G) = N(X_{est}(Y, G), I)$$\nAccording to the original notation in Ho et al. [8], the Markov\ntransition should be modified as:\n$$p(X_t|X_{t-1}, Y, G) = N(\\sqrt{1 - \u03b2_t}X_{t-1} + (1 - \\sqrt{1 - \u03b2_t})X_{est}, \u03b2_tI)$$\nwhich derives the closed-form distribution with arbitrary t:\n$$p(X_t|X_0, Y, G)) = N(\\sqrt{\\bar{\u03b1}_t}X_0+ (1 -\\sqrt{\\bar{\u03b1}_t})X_{est}(Y, G), (1-\\bar{\u03b1}_\u03c4)\u0399)$$\nwhere at := 1 \u2212 \u03b2t, \u1fb6t := \u03a0\u03c4 at.\nIn the reverse denoising process, the reverse Markov denoiser\nq(Xt-1 Xt, Y, G) recovers the original data. DM framework trains\nthe parameterized denoiser to fit the ground truth forward process\nposterior:\n$$p(X_{t-1}|X_t, X_0, Y, G) = p(X_{t-1}|X_t, X_0, X_{est} (Y, G))$$ \n$$= N(\u03bc(X_t, X_0, X_{est}(Y, G)), \u03b2_tI)$$\nwhere\n$$\u03bc(X_t, X_0, X_{est}(Y, G)) :=-\\frac{1}{\\sqrt{\u03b1_t}}( \\frac{\u03b2_t}{\\sqrt{1-\\bar{\u03b1}_t}})X_t+\\frac{\\sqrt{\u03b1_{t-1}}\u03b2_t}{1-\\bar{\u03b1}_{t-1}}(1+\\frac{\\sqrt{\u03b1_t} - 1}{\\sqrt{\u03b1_t}})X_0 +\\frac{1 - \u03b1_t}{1 - \\bar{\u03b1}_t}(\\sqrt{\\bar{\u03b1}_t} - \\sqrt{\\bar{\u03b1}_{t-1}})(1+\\frac{\\sqrt{\u03b1_t} - 1}{\\sqrt{\u03b1_t}}))X_{est}$$\n$$\u03b2_t := \\frac{1-\\bar{\u03b1}_{t-1}}{1 - \\bar{\u03b1}_t} \u03b2_t$$\nThe denoising network fe is set to estimate the ground truth source\nX(i.e. Xo in Equation (8)), which we empirically find more effec-\ntive. The denoise network outputs the estimated source vector\nX0 := fo (Xt, Xest, Y, G, t) to calculate the posterior for step-by-step\ndenoising. The denoising network fe can be trained by the simple\nL2 loss function [8]:\n$$L(\u0398) = E_{X_0~p(X_0),t,\\epsilon}||X_0 \u2013 f_\u0398(X_t, t, \u2022)||^2$$\nwhere represents the conditional inputs."}, {"title": "Propagtion-enhanced Conditional Denoiser", "content": "The architecture of our de-\nnoising network is shown in Figure 3.\nEncoding the noisy input and soft labels. The pre-estimated\nXest is forwarded through a multi-layer GNN to capture the hid-\nden message with graph structural information. Subsequently, it\nis added to the noisy input Xt and passed through a linear layer.\nThe final input for the GNN encoder is Ze = Linear (GNN (Xest) Xt) Emb(t), where for the denoising step t, we use the classi-\ncal sinusoidal embedding [30]. The indicates element-wise sum.\nZe is then passed through a GCN-based encoder and is smoothed\nthrough a softmax function \u03c3 and layer normalization:\n$$Zd = LN(\u03c3(GNN(Ze)))$$\nSoftmax and layer normalization operations are then used to im-\nprove the network's representational capacity and convergence per-\nformance, resulting in better performance and faster training [12].\nConditioning. Shown at the left part of the figure, a GCN-based\nmodule learns the encoding carrying the source prominence and"}, {"title": "GNN-parameterized Label Propagation(GNN-LP)", "content": "Our condi-\ntioning module takes the observed infection states as input, which\ncontain crucial coupling information between spreading dynam-\nics and network topology. To effectively extract this information,\nwe introduce the GNN-LP module. We first employ label propa-\ngation [34], which first mark infected nodes as 1 and uninfected\nas -1 in the observation state Y, resulting in Y*. The propagation\nfollows the update rule: the label of a node in the next step is a\ncombination of its original label and the sum of normalized labels\nfrom its neighbors. We can rewrite this iteration as:\n$$\u0396^{t+1} = \u03a5 + \u03c3(\\sum_{j:j\u2208N(I)} \u03c6(\u0396_{j}^{t}), S_{ij}))$$\nwhere we add non-linear transformations h() and (.) to enhance\nthe expressiveness of the propagation process. As propagation con-\ntinues, each node's label value changes according to the graph\nstructure. These changes reflect the node's importance (centrality)\nand spreading influence in the network. By observing these changes,\nwe can understand the relationship between propagation patterns\nand network structure. We then enhance this structural diffusion\nwith GNNs, which learn topology-specific message passing rules\nto capture how different local structures influence propagation pat-\nterns. We can notice that the structure of the above equation exactly\nmatches the form of the general Graph Neural Network (GNN) [6],\nand the parameterization can be achieved by using a residual block\ncombined with a graph convolutional network(GCN, [17]):\n$$g(h^{(1)}) = \u03c3(\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} \u00b7 h^{(1)} \u00b7 w)$$\n$$h(0) = Y*UT, h(l+1) = h(0) + g(h(l)).$$\nAmong them, U \u2208 RC\u00d71 is the linear transformation, o is the\nactivation operator PReLU, h(1) stands for the output hidden state\nof the 1-th layer of the GCN, \u00c3 = A + I is the adjacency matrix\nwith self-loops, and \u010e is the degree matrix of \u00c3. The final layer's\noutput h(lf) is projected back to dimension 1 and multiplied by the\ngraph's Laplacian matrix L, i.e. hout := L \u00b7 h(lf), which highlights\nthe prominent nodes with higher propagation propagated labels\namong their neighbors.\nThrough this integration, the module transforms binary infection\nstates Y into continuous representations hout that simultaneously\nencode both the spreading process and network structure informa-\ntion, enabling effective learning of their complex interactions even\nwith limited training samples."}, {"title": "Pretrain using Simulation data from Established Models", "content": "The proposed structure-prior informed diffusion framework effec-\ntively extracts topology-aware features that remain largely stable\nacross different propagation dynamics. These designs enable the\nframework to learn diverse knowledge from different propagation"}, {"title": "Experiments", "content": "The basic settings of our experiments are shown below.\nFollowing Ling et al. [20] and Huang et al. [11], we use\nfour real-world propagation data to evaluate SIDSL. The real-world\ndatasets include Digg, Twitter, Android and Christianity, in which\nthe real propagation cascades are available. For each cascade in all\nsets, we designate the infected nodes at the first 10% of the propa-\ngation time as source nodes and take the network's infection status\nat 30% of the propagation time as observation input. In the context\nof real-world applications, we often can only collect sufficient data\nfor analysis after some time has elapsed since the occurrence of the\nevent. Therefore, attempting to predict what initially happened in\nthe process when we have observed enough propagation patterns\nat a certain degree of infection time is very much in line with the\nneeds of real-world operations.\nFor each dataset, the ratio of training,\nvalidation, and testing portion is 6:1:1. For the diffusion framework\nof SIDSL, we use T = 500 maximum diffusion timestep and linear\nschedule for noise scheduling. In the denoising network, we lever-\nage a 2-layer graph convolutional network (GCN) to forward the\nLPSI estimation Xest condition. The GNN encoder and decoder com-\nprise 3-layer GCNs with a hidden dimension of 128. The residual\nGNN of the conditioning module is a 2-layer GCN, with a hidden"}, {"title": "Overall Performance on Real-world Datasets", "content": "To evaluate the real-world performance of our proposed method\nagainst baselines, we conduct direct training and testing on four\nreal-world datasets for all methods. The experimental results are\npresented in Table 1. Our proposed SIDSL outperforms all baselines\nacross nearly all metrics on all datasets. Specifically, SIDSL achieves\nF1 scores that exceed the second-best baseline by 7.5%, 11.0%, 11.8%,\nand 13.3% on Digg, Twitter, Android, Christianity, respectively,\nwhich demonstrates SIDSL's superior ability to predict source nodes\ndespite their sparsity.\nFurther, we have the following findings. First, compared to other\ngenerative methods like DDMSL and SLVAE which rely purely on\ndata-driven approaches, SIDSL demonstrates superior performance,\nvalidating the effectiveness of incorporating structural prior infor-\nmation on top of purely data-driven distribution learning. Second,\ncompared to rule-based and pure learning-based methods, genera-\ntive methods (SIDSL, DDMSL and SLVAE) show relatively strong\nperformance, highlighting the advantages of effective source distri-\nbution modeling for source localization accuracy. LPSI and other"}, {"title": "Performance in Low Data Regime", "content": "To assess how deep learning methods leverage simulation data\npretraining for few-shot and zero-shot learning in real-world source\nlocalization, we generate pretraining data using standard IC and LT\npropagation models (1:1 ratio) across four networks. These models\neffectively capture social network dynamics by representing real-\nworld interaction randomness, cumulative influence, and topology-\nbased spread patterns. For few-shot learning (P), we fine-tune the\npretrained models with limited real data (10%). We also compare\nthe performance of all the methods trained on equivalent data\nvolumes but without pretraining (NP). For zero-shot learning, all\nthe methods are directly tested after being pretrained.\n The result of few-shot learning is shown in\nTabel 2. Rule-based methods (Netsleuth and LPSI) are not shown\nhere because they are training-free. SIDSL demonstrates superior\nperformance compared to all baseline methods across all datasets,\nboth with and without pretraining, achieving improvements of\n24%~365% across all metrics. Further, we have the following two\nfindings. First, SIDSL demonstrates enhanced performance advan-\ntages over baselines when leveraging pretraining, as evidenced by\nthe improvement percentages across datasets. Among the improve-\nments (A) across different datasets and metrics, 8 out of 12 cases\nshow larger gains with P compared to NP. This consistent pattern of\nlarger improvements with pretraining demonstrates SIDSL's supe-\nrior ability to effectively leverage information from simulated data\nto real-world scenarios in limited-data settings. Second, the base-\nline methods exhibit varying capabilities in utilizing pretraining.\nWhile the non-generative method TGASI shows consistent improve-\nments with pretraining across datasets, generative approaches like\nSLVAE and DDMSL demonstrate limited and unstable gains (e.g.,\nSLVAE's F1 only increases from 0.009 to 0.036 while DDMSL's F1\nonly increases from 0.010 to 0.109 on Android). This contrast high-\nlights the generative models' sensitivity to distribution shifts be-\ntween simulated and real-world data, whereas our method achieves\nrobust and significant improvements through effective incorpora-\ntion of pattern-invariant structural prior information.\n The results of zero-shot learning are shown\nin Table 3. SIDSL significantly outperforms all baseline methods\nacross all datasets, achieving improvements of 85%364% in F1 scores.\nThe performance gap is particularly notable on Android and Chris-\ntianity datasets, where SIDSL achieves over 0.9 recall while main-\ntaining reasonable precision. In contrast, baseline methods show\nlimited zero-shot transfer capability with F1 scores mostly below 0.3."}, {"title": "Analysis of Pretraining on Simulation Data", "content": "To investigate how pretraining on simulation data affects model's\nfew-shot/zero-shot performance under different data volume", "ratio\n1": 1}]}