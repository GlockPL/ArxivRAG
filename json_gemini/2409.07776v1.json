{"title": "Training Spiking Neural Networks via Augmented Direct Feedback Alignment", "authors": ["Yongbo Zhang", "Katsuma Inoue", "Mitsumasa Nakajima", "Toshikazu Hashimoto", "Yasuo Kuniyoshi", "Kohei Nakajima"], "abstract": "Spiking neural networks (SNNs), the models inspired by the mechanisms of real neurons in the brain, transmit and represent information by employing discrete action potentials or spikes. The sparse, asynchronous properties of information processing make SNNs highly energy efficient, leading to SNNs being promising solutions for implementing neural networks in neuromorphic devices. However, the nondifferentiable nature of SNN neurons makes it a challenge to train them. The current training methods of SNNs that are based on error backpropagation (BP) and precisely designing surrogate gradient are difficult to implement and biologically implausible, hindering the implementation of SNNs on neuromorphic devices. Thus, it is important to train SNNs with a method that is both physically implementatable and biologically plausible. In this paper, we propose using augmented direct feedback alignment (aDFA), a gradient-free approach based on random projection, to train SNNs. This method requires only partial information of the forward process during training, so it is easy to implement and biologically plausible. We systematically demonstrate the feasibility of the proposed aDFA-SNNs scheme, propose its effective working range, and analyze its well-performing settings by employing genetic algorithm. We also analyze the impact of crucial features of SNNs on the scheme, thus demonstrating its superiority and stability over BP and conventional direct feedback alignment. Our scheme can achieve competitive performance without accurate prior knowledge about the utilized system, thus providing a valuable reference for physically training SNNs.", "sections": [{"title": "Introduction", "content": "Neuromorphic computing refers to a series of devices and models inspired by the real brain [1]. In the machine learning field, such biologically inspired technology is designed to simulate the learning and adaptability of the brain by utilizing hardware as accelerators to accomplish complex tasks with high accuracy and low energy consumption [1-4]. With the convergence of Moore's law and the increasing need for large-scale, low-energy neural networks, neuromorphic computing has great potential. Currently, although artificial neural networks (ANNs) have already achieved impressive performance on various tasks, the high computational complexity and energy consumption of ANNS hinder their application on neuromorphic devices [5]. Spiking neural networks (SNNs), which simulate the mechanism of real neurons in the brain, represent a solution to the application of neural networks in neuromorphic computing. Different from ANNs that use continuous scalars to represent and transfer information, SNNs communicate through streams of discrete action potentials or spikes, as shown in Fig.1a. This discrete spikes-based information processing mode makes SNN neurons consume energy only when they generate spikes, allowing SNN significantly reduce the activity times of neurons and energy demand for information transmission [6\u20138]. Taking advantage"}, {"title": "DFA: BP, gradient-free training mechnism", "content": "Considering a standard multilayer network in Fig.2, the forward propagation is expressed as $x_{n+1} = f (a_n)$, where $a_n = W_nx_n$. $X_n \\in R^{N_n}$ is the input signal from n 1-th hidden layer $H_{n-1}$ to n-th layer, $N_n$ represents number of nodes in $H_n$. $W_n \\in R^{N_n \\times N_{n-1}}$ stands for the weight for the n-th layer. f denotes the element-wise activation function, which is often ReLu or sigmoid function in conventional ANNs [27\u201330], while in SNNs, f is the non-continuous firing function [14, 31], as shown in Appendix A. In general, to train such a network, the connection matrices W need to be optimized to minimize the loss function L. The process of the BP algorithm is shown in Fig.2a, here using the optimization of $W_n$ as an example, and the gradient $e_n$ that transmitted to $H_n$ through the chain-rule of BP can be expressed as:\n\n$e_n = [W_{n+1}^Te_{n+1}] \\odot f'(a_n),$\\n\nwhere the superscript T represents precise transposition, $\\odot$ denotes Hadamard product, and f' is the exact derivative of activation function f. With this information, we can compute the gradient of $W_n$ as $dW_n = -e_n x^T$. From Eq.1, we can see that the injected error signal to current layer depends on the error information from the layers behind, and it also needs to engage several precise calculations. Thus, this scheme is not the best choice from the perspective of neuromorphic computing. Feedback alignment (FA) [32], which is one of the earliest BP-free algorithms, replaces the calculation of precise transposition in the backward process by employing fixed randomly generated matrices B, thus simplifying Eq.1 into:\n\n$e_n = [B_ne_{n+1}] \\odot f'(a_n),$\\n\nHowever, the sequential transmission mechanism still constrains the neuromorphic implementation of FA. Then, considering the solution of this mechanism, direct feedback alignment (DFA) [33\u201336], which can break the chain-rule of BP by injecting global error signal e from the final layer to each previous layer directly through B, leads to a new mechanism:\n\n$e_n = [B_ne] \\odot f'(a_n),$\\n\nNevertheless, the precise calculation of derivative f' still could not be avoided, which impedes the complete physical implementation of this method. Additionally, in the context of its application on"}, {"title": "Results", "content": "First, we demonstrate the feasibility of the aDFA-SNNs scheme, which employs a variety of ar- bitrary nonlinear functions as backward functions g in Eq.4, to check the effect of aDFA on the performance of SNNs. We use the benchmark task MNIST [41] with a simple three-layer fully con- nected SNN model. In this experiment, the model has dimensions 784 \u00d7 1000 \u00d7 10, which consist of two spiking layers with 1000 and 10 nodes, respectively. The spiking layers are composed of leaky- integrate-and-fire (LIF) neurons (see Appendix A). For making a comparison, a smoother, exact approximation of the derivative of the discontinuous functions in LIF neuron-namely an approxima- tion of the Dirac delta function-is utilized as the derivative f' of the dynamics of LIF neurons during the backward process, thus constructing both standard BP and DFA schemes (see Appendix D).\n\nFor preparing nonlinear functions g, we generate them from random Fourier series (RFS) $g (a) = \\sum_{k=1}^K[p_ksin(\\kappa\\pi a) + q_kcos(k\\pi a)]$, where $p_k$ and $q_k$ are random coefficients that are uniformly sampled from the interval [-1,1]. k is set to 4 in our case, and $p_k$ and $q_k$ are normalized by the relationship of $\\sum_{k=1}^K(|p_k|+|q_k|) = 1$. As can be seen, RFS is the sum of a series of sine and cosine functions with introduced randomness, hence possessing the theoretical capability to indefinitely approximate any function. When generating RFS, we notice that neither the exact derivative of the dynamics of the LIF neuron, nor the smoother differentiable approximation f' yields a negative value. Therefore, we introduce a shift to the vertical axis of RFS to obtain positive random Fourier series (PRFS). In fact, numerous standard RFSs are tested in this experiment; however, almost all of them proved to be ineffective. We consider that this phenomenon arises because the negative values of standard RFSs in the backward process change the updating direction of W, which affects the accumulation of membrane potential and firing of LIF neurons in the forward process, hence leading to training failure. We employ correlation coefficient \u03b7 (see Appendix E) to denote the degree of functional similarity between generated PRFS and f' so as to conduct classified investigation and analysis on the performance of many generated PRFS on the aDFA-SNNs scheme. When \u03b7 equals 1, g is the exact f', that is, the standard BP and DFA cases; when it is 0, it represents uncorrelated case; and when it equals -1, it denotes the negative correlated case. In our study, the shape of f', as indicated by the gray line in Fig.3c, is highly slender and distinctive, making it challenging to directly obtain \u03b7 with a higher value and broader range, which hinders a systematic classified analysis. Therefore, to achieve relatively higher value and wider range of degree of functional similarity with f', we incorporate the scaling factor w into PRFS to adjust its fundamental frequency. The transformed PRFS is presented as:\n\n$g'(a) = |m| + \\sum_{k=1}^K[p_ksin(\\omega k\\pi a) + q_kcos(\\omega k\\pi a)].$\\n\nwhere |m| denotes a shift toward the positive field. To obtain proper w, that is, to achieve a higher value and wider range of n, we generate PRFSs with 10,000 random seeds, calculate their correlation coefficient \u03b7 with f' at different orders of w, and investigate the distribution of them. The results are shown in Fig.3a. When w equals 0.01, the distribution of \u03b7 is approximately normal in the range [-0.6, 0.6], which represents the maximum value and widest range of \u03b7 that we can obtain. We divide this range into six intervals with a uniform size of 0.2 and randomly select five PRFSS"}, {"title": "Conclusion", "content": "In the present study, we investigated the implementation of aDFA-a learning mechanism that is easy to implement physically and that is biologically plausible-on the SNNs platform. By using PRFS- random functions with universal properties-as the backward function g to replace the meticulous designed derivative f' of LIF neurons, we systematically showed the feasibility of the aDFA-SNNS scheme. We have presented the range of the validity of the approach and utilized GA to identify the PRFS settings that yield good performance. We also analyzed the impact of crucial features of SNNs on this scheme, so that showing the superiority and stability of it. Finally, we directly adjusted the B and g of schemes with determined forms of g, thus achieving competitive performance. Compared with BP and DFA, in our experiments, the stable and competitive performance obtained by the aDFA-SNNs scheme, which leverages the simple, straightforward, and hardware-friendly learn- ing mechanism, provides a valuable reference for training SNNs. In the future, we will continue to explore the application of aDFA methods on more complex SNN models, and focus on devel- oping general and efficient methods for optimizing backward functions g to achieve competitive performance."}, {"title": "Appendix", "content": null}, {"title": "A Leaky-integrate-and-fire neuron", "content": "Numerous types of SNN neuron models have been proposed, but in the present study, we use one of the most popular mathematical neuron models called a leaky integrate and fire (LIF) neuron to construct our SNNs, which can achieve a good balance between the complexity needed to simu- late dynamics of real neurons and the simplicity needed to model them [46, 47]. Fig.1b visually illustrates the dynamics of a single LIF neuron. For a given LIF neuron, the input-driving signal is derived from the weighted sum of the output of the spike sequences from all its connected presy- napses, which is expressed as:\n\n$v(t) = \\sum_j W_{ij}a_j(t); + b_i,$\\n\nwhere $v (t)_i$ represents the input signal to a single neuron i at time t. $a (t)_j$ is an output signal from presynaptic neuron j at time t. $W_{ij}$ is the synaptic weight between neuron i and neuron j, representing the strength of the connection. $b_i$ is injected bias.\n\nThe current membrane potential of the given LIF neuron, that is, the state of that neuron, depends on its previous membrane potential as well as the current input signal. To better numerically simulate this model, we consider its variation in discrete time, which leads to the dynamics of membrane potential being represented as:\n\n$h_i(t) = (1 - \\frac{\\Delta t}{\\tau})h_i(t-1) + v_i(t) + \\eta_i(t),$\\n\nwhere $h (t)_i$ is the membrane potential of neuron i at time t. $\\Delta t$ represents the length of time step used in digital integration, and $\\tau$ is the time constant used for the decay of membrane potential, both of which constitute the leaky factor. When $h (t)_i$ exceeds the threshold value, neuron i will emit a spike to the postsynapses. The process of generating a spike output is expressed in the form of a piece-wise function as:\n\n$a_i(t) = \\begin{cases} 1 & h_i(t); \\geq h_{th} \\\\ 0 & h_i (t); < h_{th}  \\end{cases},$\\n\nwhere $h_{th}$ is the threshold value of membrane potential. After neuron i emits a spike, the membrane potential $h (t)_i$ of it will be placed to reset the value. The notation of $\\eta (t)$ in Eq.A.2 is used to describe the reset process, which can be shown as:\n\n$\\eta_i (t) = \\begin{cases} 0 & h_i(t); < h_{th} \\\\ -\\{(1 - \\frac{\\Delta t}{\\tau})h_i(t-1) + v_i(t);\\} & h_i(t) = h_{th}  \\end{cases},$\\n\nwhere, when the current membrane potential $h (t)_i$ does not exceed threshold value $h_{th}$, the mem- brane potential will continuously accumulate, so $\\eta (t)$ is placed at 0 so as not to affect the process of accumulation. When $h (t)_i$ exceeds $h_{th}$, for simplifying the simulation and making the model more generalized, we set the reset value to 0, meaning $\\eta (t)$ is placed at the negative of the current membrane potential $h (t)_i$ of neuron i. After the reset process, the neuron will enter the refractory period, during which $h (t)_i$ does not follow Eq.A.2 but remains being pinned at 0, preventing the neuron from being fired."}, {"title": "B Experimental Setup", "content": "In our study, we employ an identical LIF model and initialization techniques of weight matrices W and backward matrices B as in a previous broadcast alignment (BA) paper[37]. BA is a variant of DFA, which has been utilized to achieve good performance on SNNs. We utilize the bench- mark datasets MNIST and Fashion-MNIST for image classification to assess the performance of the proposed framework [41, 45]. The inputs are not encoded; instead, a direct mapping method is employed to continuously inject static input signals in a period of interval so as to fulfill the re- quirements of time dynamics in LIF neurons and the simplicity and universality of experiments. The duration of the time interval T is set to 100 ms and divided into two segments. The first segment is a 20 ms running period, during which we keep injecting input signals to obtain stable states of the"}, {"title": "C Initialization of matrices", "content": "In order to make comparison with the existing biologically plausible SNN training framework, the broadcast alignment (BA), we use the same initialization method for fixed random matrices B and connection matrices W [37]. This initialization method is similar to the techniques in computer science, rather than them in real brain. The biases b are initialized to a physiological value of 0.8. The weight matrix in the n-th layer is initialized as follows:\n\n$W_n = \\overline{W_n} + 2\\sqrt{3}\\sigma_w (rand - 0.5),$\\n\nwhere, rand has a uniform distribution over the range [0, 1], $\\sigma_w$ represents the standard deviation of the weights $W_n$, $\\overline{W_n}$ denotes the desired mean of the weights, and the desired second moment of weights $\\overline{W_n}$ are expressed as:\n\n$\\overline{W_n} = \\frac{(0 - 0.8)}{(\\alpha N_v)},$\n\n$\\overline{W_n} = \\frac{(\\delta + \\alpha^2(N - N^2)\\overline{W^2}\\sigma_v^2 - 1.6\\alpha N_v\\overline{W_n} - 0.64)}{(\\alpha^2 N_v)},$\n\nwhere, $\\delta$ and denote the mean value and second moment of value of input signals, respectively, with values of 8, 164. N represents the number of nodes in the n-th layer, a is constant with value of 0.066. $\\sigma_{wn}$ in Eq.A.6 can be calculated by $\\overline{W_n}$ and $\\overline{W_n}$. It should be noted that the $\\overline{W_n}$ and $\\sigma_{wn}$ also will be employed to initialize B. The values of W and B initialized in this way will be within a reasonable range i.e. not too large and not too small for the SNNs."}, {"title": "D Accurate differentiable approximation of LIF neurons", "content": "For making a comparison, a smoother, more exact approximation of the derivative of the discontin- uous functions in the LIF neuron, that is, the approximation of the Dirac delta function, is utilized as the derivative f' of the activation function during the backward process, thus constructing both standard BP and DFA frameworks. The f' is expressed as:\n\n$f'(a) = \\begin{cases} \\frac{h_{th}t_{ref} T}{a(a-h_{th}) (t_{ref}+\\tau log(\\frac{a}{a-h_{th}}))^2} & a > h_{th} \\\\ 0 & a \\leq h_{th}  \\end{cases},$\\n\nwhere the input to the function is represented by a, while the values of $h_{th}, t_{ref}$, and $T$ are given in Section B. The above experiments are also conducted on the standard BP and DFA frameworks"}, {"title": "E Correlation coefficient", "content": "We employ the correlation coefficient \u03b7 to denote the degree of functional similarity between the generated PRFS and f' so as to conduct a classified investigation and analysis on the performance of numerous generated PRFS on the aDFA-SNNs framework. The expression of n is shown as:\n\n$\\eta = \\frac{\\int {f'(a) - \\overline{f'(a)}\\} {g(a) - \\overline{g(a)}\\} da}{\\sqrt{\\int {f'(a) - \\overline{f'(a)}}^2 da \\int {g(a) - \\overline{g(a)}}^2 da}},$\\n\nwhere g (a) represents generated PRFS, the superscript mean the average, and the range of integra- tion is set as [-100, 100]. When \u03b7 equals 1, g is the same as f', that is, the standard BP and DFA cases; when it is 0, it represents the uncorrelated case; and when it equals -1, it denotes the negative correlated case."}, {"title": "F Using genetic algorithm to obtain well-performing settings", "content": "In this section, we investigate the general feature of backward nonlinear functions g that can yield good performance in the aDFA-SNNs scheme. We employ PRFSs, as illustrated in Eq.5, as the backward nonlinear functions g of the fully connected aDFA-SNNs framework with a dimension of 784 \u00d7 1000 \u00d7 10. The genetic algorithm (GA), which is a evolutionary computational technique for updating and optimizing parameters [42\u201344], is subsequently utilized to search for good PRFS pa- rameter combinations to acquire appropriate nonlinear functions that can achieve good performance.\n\nIn this experiment, we randomly generate 10 PRFSs, that is, the population is set to 10, and use the test accuracy on the MNIST and F-MNIST datasets after one epoch of training as the fitness scores to optimize the random parameters $p_k$ and $q_k$ in PRFSs. The number of generations is set to 20, and in each generation, the two highest-scoring individuals will undergo crossover and mutation processes to generate offspring that replace the worst-performing individual in the population. The evolutionary processes of PRFSs, that is, the results of the population's fitness score as the function of generation, are depicted in the Fig.A.1. The box, whisker, and orange line represent the distri- bution, maximum and minimum values, and median of the population's fitness score, respectively. The shape of PRFSs for randomly initialized and final generation are plotted in the left figures of Fig.A.1. The red line represents the smoother approximation derivative f', the gray line represents the PRFSs in the population, and the blue line represents the best-performing individual of PRFSs. As can be seen, as the number of generations rises, which indicates the evolution process, the fitness scores improve while the data dispersion decreases on both tasks; this means that the performance of the aDFA-SNNs scheme with PRFS becomes better and more stable. This observation shows the successful evolution of PRFS. Therefore, by utilizing this method of automatically updating and evolving parameters, we can obtain proper settings for PRFS that can achieve good performance. From the PRFS shapes, the initial irregular PRFSs always converge to shapes with a specific charac- teristic after 20 generations, that is, the \"bell curve\" near the peak of f'. The average test accuracy of the best-scoring individuals in the final generation through 20 epochs of training can reach 97.91% and 87.20% on the MNIST and F-MNIST datasets, respectively (the selected GA-PRFSs are used to conduct five trials). These results suggest the general feature of PRFS that can achieve good per- formance in the aDFA-SNNs scheme is possessing a \"bell curve\" shape when their input values are near the threshold value of membrane potential of the SNN neurons."}, {"title": "G Impact of the network scale", "content": "In this section, we investigate the impact of network size, one of the most fundamental and cru- cial characteristics of neural networks, on the aDFA-SNNs scheme. We employ a three-layer fully connected SNN model with the similar architecture, and same experimental settings as that in the previous experiments, and utilize the number of nodes within the hidden layer to denote the size of the network. We also examine the impact of this characteristic on standard BP and DFA methodolo- gies for making comparison. The frameworks are evaluated by using both MNIST and F-MNIST datasets. For the aDFA-SNNs scheme, we conduct experiments with five randomly selected PRFSS in the range of \u03b7 belonging to [0.4, 0.6], while for the standard BP and DFA frameworks, five trials are conducted with f'. The results of testing accuracy as a function of the number of nodes in hidden layer are shown in Fig.A.2. The box plots show the data distribution of frameworks to illustrate their stability. Whiskers, orange lines, box bodies, and dots represent the maximum and minimum values, median, data distribution, and outliers, respectively. The line chart illustrates the mean test accu- racy of examined frameworks, serving as the indicator to reflect their performance and trends, while facilitating a direct comparison. These results demonstrate that the aDFA-SNNs scheme can work stably and achieve good performance on both datasets, regardless of network size. Furthermore, as the network size increases, there is a consistent improvement in test accuracy leading to eventual convergence. The standard BP and DFA frameworks, however, exhibit significant instability and poor performance on both datasets, failing to show the dependency of test accuracy on network size. In addition, the average performances of aDFA surpass that of standard DFA on both datasets, irre- spective of the network size. Only in small network size, standard BP can achieve competitive or better test accuracy than aDFA. Therefore, the analysis and comparison of this characteristic demon- strate the exceptional stability of the aDFA-SNNs scheme, and show that aDFA is more suitable for large-scale SNNs than BP and DFA, which highlights the superiority of aDFA-SNNs scheme."}, {"title": "H Impact of the temporal dynamics", "content": "Another characteristic being analyzed is the temporal dynamics of LIF neurons. In this experi- ment, we alter the temporal dynamics of LIF neurons by changing their length of time step $\\Delta t$. By measuring the test accuracy of three-layer fully connected SNN model, which with dimension 784 \u00d7 1000 \u00d7 10, on MNIST and F-MNIST datasets as $\\Delta t$ is varied, we investigate the robust- ness of the aDFA-SNNs scheme to the impact that from changing the temporal dynamics of LIF neurons. We employ the identical approach as in sectionG to conduct experiments, wherein we utilize the same PRFSs on the aDFA-SNNs scheme, and also employ the standard BP and DFA frameworks with five 5 trials for making comparison. The results are shown in Fig.A.3. The box plots illustrate the data distribution, while the line plots depict the average test accuracy of frame- works as a function of $\\Delta t$. Here, the refractory time tref of 1ms is a critical factor that requires our attention. Specifically, when the length of time step $\\Delta t$ exceeds or equals 1ms, LIF neurons will lose their refractory period, resulting in significant alterations in their temporal dynamics. The phenomenon above is evident in our findings, where the test accuracy of all investigated frameworks exhibit significant decreases when $\\Delta t \\geq 1ms$. For standard BP-SNNs and DFA-SNNs frameworks, when the temporal dynamics are significantly altered, they are failing to achieve meaningful learn- ing. On the other hand, although the aDFA-SNNs scheme experiences a drastic reduction in test accuracy, it can still exhibit learning capabilities to a certain degree, with average accuracy of more than 90% on MNIST. When the $\\Delta t$ less than 1ms, the aDFA-SNNs framework demonstrates stable performances and achieves high accuracy, with mean accuracy of approximately 97% on MNIST dataset. In contrast, the dispersion of the test accuracy distribution observed in standard BP-SNNs and DFA-SNNs frameworks indicate unstable performances, and their mean test accuracy presented in the line graphs indicate mediocre performances. In addition, these results also demonstrate the high sensitivity of the BP-SNNs framework to changes in temporal dynamics of LIF neurons, that is, the variations of $\\Delta t$ have greater impacts on the performances of it. For DFA-SNNs and aDFA- SNNs frameworks that based on the mechanism of direct error transmission with random mappings, they are robust to non-significant changes in this characteristic. Specifically, when $\\Delta t <1ms$, the"}, {"title": "I Explorations of the performance of backward functions with fixed nonlinear form", "content": "The error transmission in the backward process of aDFA method involves two crucial relaxed com- ponents, namely the fixed random mappings and the arbitrary backward nonlinear functions, denoted as B and g in Eq.4 respectively. Unlike in standard BP method, which employs strictly exact WT and sequential transmission as the error mapping mechanism, and unlike in both BP and DFA meth- ods, which take the precise derivative of the activation function as the backward nonlinearity. The utilization of a relaxed error transmission mechanism by aDFA provides an invaluable opportunity to directly adjust the entire backward process, thereby further enhancing the performance of net- works. In other words, by using the aDFA method, the B and g in Eq.4 can be directly adjusted to improve the performance of the network, regardless of any information in the feedforward process. Based on this principle, in this section, we employ two nonlinear functions with determined form as the backward function g to construct aDFA-SNNs schemes, then directly adjust their scale factors y in the initialization of B (shown in the Eq.A.5) as well as parameters of selected nonlinear functions g, leading to achieve competitive performances with high neuromorphic hardware feasibility. The first function is the Gaussian function, commonly employed in surrogate gradient learning as an"}, {"title": "J Preliminary analysis of parameters of \u201cOpto\u201d and Gaussian based frameworks", "content": "The horizontal extension degree of the function is an important parameter, which seriously affects the scope and shape of the function in the direction of the x-axis. In our cases, we used the width of the Gaussian function and the width of the 'Opto' function in one period to represent it. For Gaussian function, the c in Eq.A.11 is proportional to the width of function, and for 'Opto' function, the win Eq.A.12 is inversely proportional to the width of the function in one period. In these experiments, we used a fully connected SNN model with dimension of 784 \u00d7 1000 \u00d7 10. We tested 5 different orders of magnitude of c and w from 10-2 to 102 and checked corresponding test accuracy of these settings on MNIST and F-MNIST tasks, to get a preliminary range of settings for widths that can achieve good performance for subsequent and detailed analysis. The upper two schematics of Fig.A.5 show the results of test accuracy. The blue line and red line represent performance on MNIST task and F-MNIST task, respectively. For Gaussian function, the best performances are achieved when c in the order of 101, with average test accuracies of 97.2% on MNIST and 86.5% on F-MNIST. For 'Opto' function, the best performances are achieved when w in the order of 10-\u00b9, with average test accuracy of 97.84% on MNIST and 85.3% on F-MNIST. In order to explain these performances, we"}, {"title": "K The comparison with the performance of existing studies.", "content": "Table.A.1 shows the results of our schemes and existing studies of full-connected SNNs. We also compare them from the perspective of neuromorphic hardware feasibility. We define neuromorphic hardware feasibility in terms of the difficulty of a fully physical implementation of the training al- gorithm. The \u201cNo\u201d implies that full physical implementation is impossible; the \"Low\" implies the existence of a layer-by-layer error propagation mechanism that is difficult to implement physically,"}]}