{"title": "Let Me DeCode You: Decoder Conditioning with Tabular Data", "authors": ["Tomasz Szczepa\u0144ski", "Michal K. Grzeszczyk", "Szymon P\u0142otka", "Arleta Adamowicz", "Piotr Fudalej", "Przemys\u0142aw Korzeniowski", "Tomasz Trzci\u0144ski", "Arkadiusz Sitek"], "abstract": "Training deep neural networks for 3D segmentation tasks can be challenging, often requiring efficient and effective strategies to improve model performance. In this study, we introduce a novel approach, De- Code, that utilizes label-derived features for model conditioning to sup- port the decoder in the reconstruction process dynamically, aiming to en- hance the efficiency of the training process. DeCode focuses on improving 3D segmentation performance through the incorporation of conditioning embedding with learned numerical representation of 3D-label shape fea- tures. Specifically, we develop an approach, where conditioning is applied during the training phase to guide the network toward robust segmenta- tion. When labels are not available during inference, our model infers the necessary conditioning embedding directly from the input data, thanks to a feed-forward network learned during the training phase. This approach is tested using synthetic data and cone-beam computed tomography (CBCT) images of teeth. For CBCT, three datasets are used: one publicly available and two in-house. Our results show that DeCode significantly outperforms traditional, unconditioned models in terms of generalization to unseen data, achieving higher accuracy at a reduced computational cost. This work represents the first of its kind to explore conditioning strategies in 3D data segmentation, offering a novel and more efficient method for leveraging annotated data.", "sections": [{"title": "1 Introduction", "content": "The annotation process in medical imaging is time-consuming, costly, and re- quires medical domain knowledge [17]. Furthermore, deep learning-based algo-"}, {"title": "2 Method", "content": "In this section, we provide a detailed description of the network and the DeCode decoder with an emphasis on the application of conditioning information. Then, we describe the process of calculating shape features that are further used for the regression task to generate their embedding for inference-time conditioning. The go-to standard for accomplishing medical imaging segmentation tasks is U-shaped architectures [10,1,15,9]. Here, we follow this approach and present lightweight architecture with an overview of our method in Fig. 1. Let X be a 3D CBCT scan $X \\in R^{1\\times H \\times W \\times D}$ of height H, width W and depth D, the U-shaped network generates multi-scale features at encoding stages $E_i$. Deeper encoder stages yield more abstract features up to the bottleneck within the deepest part of the architecture, containing compressed information from the input image. The decoding part $D_i$ aims to reconstruct the segmentation map from features extracted by the encoder with additional skip connections at consecutive stages.\nStarting with high-level features of shape 14\u00d714\u00d710 in the bottleneck through all decoder stages, we utilize learned feature embedding to condition the decoding process to improve the quality of the output mask.\nDecoder Conditioning. The first step within the decoding step is processing features from the previous stage with the convolutional layer. We add features from the encoder skip connections just before the conditioning layer to avoid leakage of low-level features without first conditioning them on shape feature embedding. The conditioning layer utilizes affine transformations to scale and shift feature maps. The transformation parameters are $a_c$ and $\u00df_c$, the products of hyper-network, which implement scale and offset, where c is the number of feature map's channels (see Conditioning Layer in Fig. 1). In contrast to FiLM conditioning, we parameterize the scale parameter to $(1 - a_c)$ to facilitate the identity transform especially at the early stage of training, and to allow the scaling parameter to be regularized as a distance from zero. For a > 0, the scaling factor inverts a feature map, highlighting features that the ReLU acti- vation would have otherwise suppressed [16]. The conditioning operation takes a normalization role, replacing the batch norm between the convolutional layer and the activation function. In addition to the possibility of conditioning itself, this operation has an additional advantage compared to Batch Norm or Layer Norm: it does not depend on batch statistics [18]. The transformed features are summed via residual connection with the processed input to the decoder. The decoding step finishes with refining and up-sampling feature maps via transposed convolution, Batch Normalization, and a ReLU activation.\nShape Features. We utilize ground-truth masks to extract rich information and shape features. We consider, e.g., sphericity, volume, and elongation features, (see Fig. 2), which aim to decode more morphologically accurate masks. Before training, we extract shape features for every segmented object separately based on the ground truth mask (up to 32 objects, teeth, in a CBCT scan). This"}, {"title": "Loss function", "content": "The multi-task loss function, which minimizes both segmenta- tion, embedding distance, and regression tasks, is defined as follows:\n$L = L_{Dice} + A_1L_{Focal} + L_1 + A_2L_{RMSE} + \\eta(||a||_2^2 + ||\\beta||_2^2),$ (1)\nwhere $A_1$ = 0.5, and $A_2$ = 0.75. The coefficients are determined based on a trial-and-error optimization. $L_{Dice}$ and $L_{Focal}$ correspond to the segmentation task. We optimize an $L_1$ distance to make encoder features embedding close to the representation of tabular shape features. During inference, we use this learned embedding to condition the decoder. We also add the helper task of shape features regression during training which we optimize based on Root Mean Square Error (RMSE). Finally, we add an $L_2$ penalty $\\eta = 0.00001$ to regularize the conditioning layer parameters a and \u1e9e, due to the high capacity of the deep network following the conditioning layer, thus reducing the risk of overfitting."}, {"title": "3 Experiments and Results", "content": "In this section, we describe implementation details and introduce the synthetic dataset, 3DeCode, where we investigate the possibility of conditioning with shape features on 3D data. Moreover, we apply DeCode to the task of 3D segmentation, utilizing CBCT datasets. We highlight the significance of DeCode key compo- nents and evaluate its ability to generalize to unseen CBCT data in comparison to lightweight 3D UNet, which lacks decoder conditioning.\nImplementation details. We implement identical models for both synthetic and clinical datasets. We use a UNet network with 4-down and 4-up sampling stages, Batch Normalization, ReLU activations, and a Sigmoid layer for final classification. The conditioning layers are placed inside decoder stages as shown in Fig. 1. We crop an ROI around the teeth based on labels with a size of 240\u00d7240\u00d7176 from the input CBCT scan. Then, we randomly crop a patch of size 224x224\u00d7160 and feed it to the network. We train the model using a batch size of 4, and the AdamW optimizer for 400 epochs. A learning rate is set to 0.001, and the weight decay is set to 0.0001. The intensity of the Hounsfield Unit is clipped to the range [0, 3500] and linearly scaled to [0, 1]. We employ geometric and intensity-related data augmentation such as random rotation, translation, or brightness and contrast adjustments throughout the training process. We im- plement our model in PyTorch 1.13.1 and MONAI 1.2.0 and train it on NVIDIA A100 80GB GPU with CUDA 11.6. We use PyRadiomics 3.1.0 to calculate bi- nary mask shape features. In case of a missing tooth, we fill its shape features\nwith a vector of zeros and finally normalize tabular data to a range of [0, 1]. We perform a paired t-test with p < 0.05 to identify significant differences.\n3DeCode dataset. We present a novel dataset inspired by CLEVR-Seg [6], extending it to 3D and generating segmentation masks based on conditioning scenario tasks. We design tasks that require conditioning based on Shape, Size, or Shapes of different Sizes (referred to as Mixed). To utilize the rich informa- tion stored as non-binary shape features, we also enrich the dataset with solids of varying shapes and sizes. Namely, we generate two additional tasks that intro- duce non-discrete variability in Size or Shape to the solids, based on a uniform distribution, e.g., to generate the varying-size solid class 'small sphere' we vary its radius by \u00b1 20%. While this approach does not reflect the full spectrum of information that shape features can store, it allows us to assess the feasibility of conditioning on 3D data in a segmentation task. The Varying Mixed task con- sists of shapes varying in size and shape, where, e.g., the base spherical shape can result in an ellipsoid and a cube in a cuboid. The generated solids are binary, as complex image feature extraction is not a concern. Tasks to be solved accu- rately require the use of the conditioning information by the network. Otherwise, accuracy is reduced to a random guess based solely on the image. The positions of the solids are drawn randomly, whereby they may overlap. We generate 300 labeled conditions for tasks of Size (small, medium, or large) or Shape (sphere, cube, cylinder), and 900 for the Mixed tasks. Data consists of condition-based 3D images with up to 18 objects in volume space of the same size as the patch size used by our model. We generate every possible conditioning combination per image to prevent the model from memorizing image-condition pairs. For evalu- ation, we split the dataset into training, validation, and testing subsets with a 60:20:20 ratio. 3DeCode samples can be found in the supplementary material - Sec.4."}, {"title": "4 Conclusions", "content": "This paper investigates the possibility of conditioning the decoder in the 3D segmentation task on the tabular data. Compared to unconditioned training,\nDeCode performs better on unseen data, requiring no extra labeling work and marginal additional training time. We evaluated our method on two external CBCT datasets, proving its enhanced generalizability. Obtained results encour- age further research in this field, allowing more efficient use of annotated data.\nThere are limitations to our method. Firstly, we train our method on a rela- tively small dataset where selecting hyperparameters is complex, and their small changes may lead to a loss of stability in embedding learning, including their collapse. We expect better stability and further segmentation improvement with the increased dataset. Secondly, the radiomics features provide information lim- ited to shape without considering objects' positions and relations between them. In the future, we plan to conduct the conditioning on features extracted au- tomatically from labels, enabling the end-to-end training of representations for improved clinical image segmentation."}]}