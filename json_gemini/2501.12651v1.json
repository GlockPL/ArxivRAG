{"title": "The potential \u2013 and the pitfalls \u2013 of using pre-trained language models as cognitive science theories", "authors": ["Raj Sanjay Shah", "Sashank Varma"], "abstract": "Many studies have evaluated the cognitive alignment of Pre-trained Language Models (PLMs), i.e., their correspondence to adult performance across a range of cognitive domains. Recently, the focus has expanded to the developmental alignment of these models: identifying phases during training where improvements in model performance track improvements in children's thinking over development. However, there are many challenges to the use of PLMs as cognitive science theories, including different architectures, different training data modalities and scales, and limited model interpretability. In this paper, we distill lessons learned from treating PLMs, not as engineering artifacts but as cognitive science and developmental science models. We review assumptions used by researchers to map measures of PLM performance to measures of human performance. We identify potential pitfalls of this approach to understanding human thinking, and we end by enumerating criteria for using PLMs as credible accounts of cognition and cognitive development.", "sections": [{"title": "1 Introduction", "content": "With the improving performance of pre-trained language models (Touvron et al., 2023; Gemini Team, 2023; OpenAI, 2023; Wei et al., 2022), researchers are increasingly advocating for their use as computational models of cognition (Piantadosi, 2023; Mahowald et al., 2024; Warstadt and Bowman, 2024; Coda-Forno et al., 2024). This is true for many domains including mathematical reasoning (Shah et al., 2023; Ahn et al., 2024), language comprehension (Warstadt et al., 2020; Li et al., 2024; Hu et al., 2024a), concept understanding (Vemuri et al., 2024), spatial reasoning (Ramakrishnan et al., 2024) and analogical reasoning (Webb et al., 2023; Hu et al., 2023). For example, Shah et al. (2023) investigated the latent number representations of PLMs, finding that they showed the distance, size, and ratio effects observed in humans and understood to be the behavioral signatures of a \u201cmental number line\" (Moyer and Landauer, 1967; Parkman, 1971; Halberda et al., 2008). To take another example, Raven's Progressive Matrices test is the standard psychometric measure of fluid reasoning. Although Raven's problems are visual, Webb et al. (2023) translated them to equivalent \"digit encodings\" and showed that PLMs perform as well as humans on this test. More generally, many works combine tests across multiple task domains to form comprehensive benchmarks that enable scientific evaluation of the alignment of ML models to human cognition (Chang et al., 2024; Zhuang et al., 2023; Shah et al., 2024; Coda-Forno et al., 2024; Wang et al., 2024; Tan et al., 2024).\nRecently, researchers have begun using PLMs to additionally model the development of cognition in children (Hosseini et al., 2022; Kosoy et al., 2023; Frank, 2023; Shah et al., 2024; Tan et al., 2024). For example, Portelance et al. (2023) suggest the use of language models to predict the age of acquisition of words in children. Wang et al. (2024) use Piaget's Theory of Cognitive Development to estimate that models like GPT-40 show cognitive abilities similar to 20-year-old humans. Instead of just looking at model end states, Shah et al. (2024) investigated developmental trajectories and path dependence: whether the performance improvements of PLMs over training track the growth of cognitive abilities in children over development. Similarly, Tan et al. (2024) explored developmental parallels by comparing the learning trajectories of vision-language models to both child and adult behavioral data. To take a final example, researchers have begun varying the exposure of PLMs to multiple languages during pre-training to understand the differential rates of bilingual language development (Evanson et al., 2023; Marian, 2023; Sharma et al., 2024)."}, {"title": "2 PLMs as theories in cognitive and developmental science", "content": "PLMs make promising candidates as theories of cognitive science due to the nature of language model training (Hardy et al., 2023). PLMs acquire a wide range of capabilities in the pre-training phase, essentially obtaining human-like behaviors \"for free\" without the need for extensive task-specific tuning or adaptations (Demszky et al., 2023; Weng, 2024; Yang et al., 2024; Minaee et al., 2024; OpenAI, 2023). For example, the probabilistic nature of PLM text generation lures researchers to draw parallels with the human decision-making process, where humans anticipate future courses of action based on their past experiences and context. These models can provide insights into how humans may handle ambiguous situations where multiple interpretations are possible (McGrath et al., 2020; Gawlikowski et al., 2023; Dong et al., 2024; Belem et al., 2024). Moving from cognitive science to developmental science, PLMs have been shown to gradually learn linguistic sensitivities to syntax, semantics, morphology, etc., from mere exposure to (i.e., masked word prediction of) large amounts of text data. This raises the possibility that their learning trajectories mimic the development trajectories of children acquiring language (Aher et al., 2023; Duan et al., 2024; Shah et al., 2024).\nThis allure of PLMs as theories of cognitive science also extends to adjacent research areas. One example is the ability to simulate personas using PLMs, which enables the proxying of human subjects (Park et al., 2022; Samuel et al., 2024; Schuller et al., 2024; Tseng et al., 2024). This is especially important when real-world experiments with human participants are complex, resource intensive, and/or pose ethical challenges (Aher et al., 2023; Dillion et al., 2023; H\u00e4m\u00e4l\u00e4inen et al., 2023). Another adjacent area is neuroscience: Functional"}, {"title": "3 Pitfalls of using PLMs as scientific theories", "content": "Although they show great promise, the assumptions made in the three-stage mapping process can lead to potential pitfalls when using PLMs for cognitive modeling. We draw on current research to enumerate these pitfalls. Many have been noted before in the literature; a new conceptual contribution is to partition them into two distinct classes, pitfalls of commission and pitfalls of omission. Pitfalls of the commission are the methodological and meta-theoretical mistakes researchers may make when comparing PLMs and humans: using relatively distal linking hypotheses, insufficient consideration of the psychological plausibility of training corpora, etc. Pitfalls of omission stem from incorrectly assuming that cognitive ability (e.g., analogical reasoning) is a \"module\" and failing to consider the high-level context in which it functions: its relation to other cognitive abilities (psychometric data), its progression over-development (cognitive development data), and its neural correlates (neuroscience data).\nPitfalls of Commission The first pitfall of commission arises because researchers must use linking hypotheses to map model performance characteristics to human performance characteristics (Hale, 2001; Levy, 2008). For example, when modeling incremental sentence processing, the log probability of the next word (given the words that come before) according to a PLM can be mapped to the time humans take to read that word (Li et al., 2024). The problem is that these links are often quite distal, i.e., there is a large difference between the performance measures extracted from PLMs and those captured from humans, leaving it unclear whether PLMs are actually \u201cexplaining\u201d cognitive science data. (See the next section for further discussion.)\nSecond, PLMs are opaque and have limited interpretability. When models and humans fail to align in their performance, the reason(s) why can be difficult to debug. This lack of interpretability is a barrier to treating these models as scientific theories (Kar et al., 2022; McGrath et al., 2023). Furthermore, commercial models often have many tuning interventions like supervised fine-tuning, instruction tuning, and RLHF/ RLAIF. While research shows that instruction tuning can more closely align PLMs to human brain-imaging data (Aw et al., 2023), lack of transparency in the tuning methods used makes it difficult to understand why they work when they do indeed work. It is true that there exist techniques to look at the mechanistic workings of a model, for example, mechanistic interpretability. However, these are often insufficient for investigating LLM misalignment to human cognition due to their focus on low-level mechanisms, which are the wrong level of analysis for capturing the emergent, contextual, and symbolic aspects of human thought.\nFinally, PLMs are trained on aggregate human data (like Wikipedia), and thus, their behavior may not always reflect the broad range of different behaviors observed across individuals. Although some research has explored fingerprinting human personas with PLMs, i.e., using prompts to elicit individualistic human-like behaviors (Park et al., 2022; Aher et al., 2023; Potter et al., 2024), there are conflicting views on whether models can simulate these personas (Mili\u010dka et al., 2024) or not (Salewski et al., 2024).\nPitfalls of Omission Moving to pitfalls of omission, the first is that human brains and PLMs are architecturally different. Referring to Marr (2010) on the three levels of analysis, the architectural difference lies primarily at the implementational level, where the human brain relies on biological neural networks composed of neurons and synapses, while PLMs operate using artificial neural networks implemented in silicon-based hardware. Recent research attempts to map different aspects of PLMs (layers, attention heads, etc.) to different brain regions, seeking correspondence between model performance and functional neuroimaging measures. However, this work is in its infancy, and its viability (which requires a correspondence between NLP software and neural hardware) remains an open question (Hosseini et al., 2022; Kauf et al., 2023).\nSecond, most studies evaluating the cognitive alignment of PLMs focus on a narrow range of cognitive abilities and overlook correlations with other abilities. This is in line with the experimental approach to human behavior \u2013 but at odds with the differential or psychometric approach, which has also proven to be important (Cronbach, 1957). While recent research is developing larger benchmarks and expanding the evaluation of PLMs to suites of tests (Coda-Forno et al., 2024; Chang et al., 2024), there has been almost no attention paid to the correlations between the various tests. By contrast, psychometric approaches to human intelligence put the focus on the correlations across tests of a broad range of cognitive abilities: mathematical, verbal, spatial, fluid, and so on (Snow et al., 1984; Schneider and McGrew, 2012). This differential view is also in contrast with unified theories of cognition, the precursors to Artificial General Intelligence, that"}, {"title": "Challenges of Development", "content": "Finally, we consider the pitfalls that come with treating PLMs as developmental science theories, i.e., of the progressions in children's thinking over time.\nFirst, PLM checkpoints are snapshots or fingerprints of the data on which they are trained. Most research only looks at final model checkpoints and evaluates cognitive alignment to adult thinking. Equally important is the question of whether, as language models observe more and more data, their performance is aligned to that of older and older children (Warstadt and Bowman, 2024; Frank, 2023; Shah et al., 2024). Developmental alignment is frequently overlooked because of the general unavailability of intermediate training checkpoints or because of resource constraints. This limits our understanding of the developmental fidelity of model training. However, recent open-source language modeling efforts to make available these checkpoints represent a promising opportunity to study developmental progressions (Biderman et al., 2023; Liu et al., 2023; Groeneveld et al., 2024).\nSecond, there are large differences in the nature of the data observed by PLMs versus those experienced by humans. PLMs are trained on magnitudes more textual data than the number of words seen by children (Huebner et al., 2021; Hosseini et al., 2022; Warstadt et al., 2023; Bhardwaj et al., 2024). That said, children learn from input from multiple senses (Smith and Gasser, 2005), whereas models are not embodied in nature (Chemero, 2023; Cuskley et al., 2024; Birhane and McGann, 2024). Here, the emergence of vision-language models offers the potential to bridge the gap between the disembodied nature of PLMs and the multisensory learning of humans. However, this approach still faces certain limitations. For example, research suggests that while models can process simple visual features like color and size, models struggle with complex spatial and numerical reasoning tasks when applied to novel contexts or objects (Yiu et al., 2024).\nFinally, for studies that have evaluated the developmental alignment of PLMs, the developmental trajectories observed in models might be artifacts of the pre-training order (Shah et al., 2024). Studies are needed to analyze training corpora and assess the impact of training curriculum design. This includes examining the sequence and nature of data presented during pre-training to distinguish genuine developmental progressions from artifacts introduced by training strategies."}, {"title": "Pitfalls", "content": "Using PLMs as cognitive models potentially brings pitfalls of commission and omission and requires attention to the challenges of development. This means that researchers should be cautious when assessing the suitability of these models as proxies for human cognition and its development."}, {"title": "4 Linking Hypotheses - Mapping Model Performance to Human Performance", "content": "The \"outputs\" of PLMs are often quite different from the behavioral measures that cognitive scientists collect in their experiments and evaluate their theories against. Researchers use various linking hypotheses to map PLM performance to human performance measures. These assumptions necessarily define \u2013 and potentially limit \u2013 the strength and validity of the alignment. It is important to critically evaluate these linking hypotheses because they structure how we interpret the models' cognitive capabilities. Below, we review different approaches to mapping indices of model behavior to human performance.\nSimilarity computations Many cognitive tasks require people to judge the similarity of two items. Examples range from Shepard's classic psychophysical studies of people's similarity judgments of perceptual stimuli (e.g., circles of differing diameters) (Shepard and Podgorny, 1978) to Griffiths' current Bayesian and neural network models of how people judge the similarity of an exemplar to a category prototype (L Griffiths et al., 2008).\nHuman similarity judgments can be directly modeled by computing the similarity between the corresponding representations in a PLM's latent space. This can be via cosine similarity or other metrics (Turney and Littman, 2005). One example of similarity computations in cognitive tasks comes from modeling the typicality effect, which is the finding that people regard some members as \"better\" examples of a category than others (Rosch, 1975; Bhatia and Richie, 2022). The typicality of an exemplar is commonly defined as the proportion"}, {"title": "Surprisal values", "content": "One way of quantifying the uncertainty of model generations is in terms of the summation of their surprisas, i.e., their negative log probabilities. A common linking hypothesis is that higher surprisal values correspond to longer human response times. For instance, studies of reading (Rambelli et al., 2024; Ivanova et al., 2024b) and categorization (Misra et al., 2021) have found evidence for this correspondence. Similarly, Shain (2024) use PLMs to demonstrate strong surprisal predictability estimates of human reading times. Research also shows that surprisal values provide a better match to human plausibility judgments than prompts (Ivanova et al., 2024a). Finally, relative surprisal has been used to distinguish grammatical and ungrammatical sentences (Warstadt et al., 2020). In this case, surprisal enables direct comparison of the right answer with all possible candidate answers in a deterministic manner because PLMS will generate sequential probabilities for all strings, whether they are grammatical or not.\nA problem with surprisal-based approaches is that they show high context sensitivity, i.e., the alignment of PLMs to humans often depends on the framing and the structure of the original prompt. For example, in mathematical tasks, models tend to achieve better alignment with human reasoning when they are prompted to generate longer texts and step-by-step reasoning chains (Jiang et al., 2024). This sensitivity to prompt structure makes it challenging to operationalize variations in PLM outputs in a way that mirrors the natural variances observed in human responses to stimuli. In some cases, surprisal is a weak proxy for human behavior: Van Schijndel and Linzen (2021) find that estimates of the time cost of word predictability derived using surprisal methods underestimate the magnitude of human garden path effects during processing of temporarily ambiguous sentences. More generally, surprisal fails to explain syntactic disambiguation difficulty (Huang et al., 2024). These findings highlight some failure modes of surprisal as a linking hypothesis and reveal the need to establish the empirical plausibility \u2013 goodness of fit score of each linking hypothesis on each task. Surprisal makes assumptions about the relevance of probability distribution shifts to human sentence parsing and the relevance of information-theoretic measures to human cognition, which may be wrong or non-neutral and might be partially due to how surprisal is framed."}, {"title": "Linking Hypothesis", "content": "Our findings reveal the need to establish the strong empirical plausibility \u2013 goodness of fit score \u2013 for each linking hypothesis on each task."}, {"title": "5 Criteria for Evaluating and Developing PLMS as Scientific Models", "content": "PLMs are increasingly being evaluated as models of cognitive and developmental science phenomena. In light of the discussion above, the review of some of the popular assumptions (linking hypotheses), and the common pitfalls outlined, we propose two sets of criteria for using PLMs for this purpose. While some may seem obvious or trivial, we believe that it is important to explicitly document them, given the evolving complexity and scope of PLMs. Explicit criteria provide a basis for interpreting results meaningfully, identifying limitations, and ensuring methodological rigor in their application as cognitive models."}, {"title": "5.1 Appropriateness", "content": "The first set concerns the appropriateness of PLMs as scientific tools for modeling cognitive and developmental phenomena:\nDesign multiple experiments to test alignment to each cognitive or developmental phenomenon PLMs may track human performance characteristics well under one linking hypothesis on one type of test. However, this alignment may just be an artifact, for example, of pre-training data contamination, i.e., the accidental inclusion of evaluation samples in the pre-training data. We recommend empirical triangulation \u2013 conducting more experiments evaluating the same cognitive/developmental phenomena \u2013 to establish stronger empirical plausibility.\nUse multiple methods to interpret PLM successes and failures PLMs lack explainability and interpretability due to their large size (McGrath et al., 2023). Some methods for PLM interpretation are often better than others. For example, in the experiments conducted by Li et al. (2024), incrementally constructed parse trees provided a better account of PLM alignment than the information from attention weights. It is typically not clear in advance which evaluation metric will be most insightful for a given set of models and cognitive tasks, necessitating a multi-pronged approach.\nTest the path-dependency of PLMs for developmental alignment The claim that the final model state of a PLM approximates adult performance leads to the question of the path by which it arrived there. Ideally, the model's performance improvements over training should also track the progression of cognitive abilities over development (Elman, 1996; Bengio et al., 2009). This would support researchers exploring the scaling of training data and model size in their investigations of human development. Inspired by (Tan et al., 2024), we encourage research into building age-aligned developmental benchmarks by sampling data aligned with children's learning trajectories.\nControl for tuning techniques PLMs are often tuned on specific data and in different ways, such as Instruction Tuning, Reinforcement Learning from Human Feedback (RLHF), etc. The technique used is not incidental. Rather, it can influence model behavior, which is important if the model's output centers around tuning goals rather than developing a representation of world knowledge. Empirical evidence suggests that tuning methods result in better aligning PLMs with human cognition (Aw et al., 2023). We suggest evaluating cognitive alignment across multiple PLMs tuned with different objectives (e.g., instruction-tuned vs. RLHF-tuned).\nRemember the linking hypothesis Adapting human experimental materials to textual counterparts that match the required modalities for PLMs requires making certain assumptions (refer to different operationalizations in section 4). These assumptions are not neutral, but rather part of why models may or may not align with human performance. For this reason, they need to be well-documented and explored in their own right. Additionally, transparency in selecting such assumptions helps establish the credibility and replicability of findings.\nEstablish task correlations Inspired by Snow et al. (1984), who study the correlations in human performance across different psychometric tests of intelligence, we propose a similar cross-task PLM evaluation paradigm. The goal will be to evaluate whether the pattern of cross-task correlations observed in humans is also produced by PLMs.\nEmbodiment and interactiveness Many researchers have advocated for the addition of more modalities in the pre-training process, such as vision, touch, etc., to emulate the learning environment of the child (Cuskley et al., 2024). While purely theoretical and speculative, the increasing \"embodiment\" of pre-training and the addition of external interactions may yield more substantial human alignment."}, {"title": "5.2 Development of PLMs for Cognitive Modeling", "content": "The second set of criteria is for guiding the development of PLMs as credible accounts of cognition and its development. This is a more open-ended task, and the following can be considered as mere suggestions to researchers:\nPre-training data may benefit from developmentally plausible corpora PLMs should be evaluated at regular intervals of pre-training to assess their potential developmental alignment. This step is often overlooked in current studies of cognitive alignment. This includes training on a curriculum based on the known developmental trajectories of knowledge and skill acquisition (Bhardwaj et al., 2024; Warstadt and Bowman, 2024; Frank, 2023; Hu et al., 2024b). For example, corpora could be ordered or sampled based on the age-of-acquisition of the words their text contains (Huebner et al., 2021; Portelance et al., 2023). Informed pre-training will allow us to better understand the developmental alignment of models.\nPLMs can first be tuned on a small number of \"core\" cognitive tasks and then evaluated on a broader range of cognitive tasks For example, typicality experiments (Vemuri et al., 2024; Misra et al., 2021) could be used to preference-tune PLMs using reinforcement learning techniques. This might result in better cognitive alignment to a large number of tasks that all rely on human-like semantic representations (V\u00e1zquez Mart\u00ednez, 2021; V\u00e1zquez Mart\u00ednez et al., 2023). This builds off the promise of generalization and transferability for model alignment, where alignment on n tasks leads to predictive utility and alignment on n + 1th task (Binz et al., 2024)."}, {"title": "6 Conclusion", "content": "This paper advocates for the use of Pre-trained Language Models (PLMs) as theoretical tools for investigating human cognition and its development through the three-stage model shown in Figure 1. In this advocacy, we are not alone (McGrath et al., 2023; Frank, 2023; Warstadt and Bowman, 2024; Mahowald et al., 2024). However, at the same time, we caution researchers toward the informed use of PLMs when making theoretical claims in the cognitive and developmental sciences. We have highlighted common pitfalls in this enterprise, reviewed the different assumptions (i.e., linking hypotheses) used by researchers to map PLM performance to human performance, and outlined criteria for evaluating and developing PLMs as credible models of cognition and cognitive development. These criteria are intended to guide researchers in designing robust experiments (particularly using the three-stage criteria as a framework for experiment design), interpreting PLM behaviors accurately, and increasing fidelity to human data. Given the constantly evolving nature of the field, we call for researchers to continuously refine and expand these guidelines to match new advancements in NLP and cognitive science."}, {"title": "7 Limitations", "content": "(1) The paper highlights common pitfalls, linking hypotheses, and evaluative criteria while using PLMs for cognitive modeling. These constitute a set of sound views to aid new researchers in the field. They do not exhaustively cover every pitfall, hypothesis, or criterion. (2) The suggestions in this work are good-to-have practices that support using PLMs for open cognitive and developmental science. No one-answer-fits-all approach is possible. NLP is a developing field, and we recommend articulating newer guidelines and practices as newer and ever-larger PLMs are trained and deployed. (3) Our work calls for language technologies for the psychological sciences and provides criteria for developing credible accounts of cognition and cognitive development. Despite providing general guidelines, our work is theoretical and does not conduct experiments or offer empirical evidence of performance comparisons or other quantitative measures. In section 5, we suggest criteria by building upon and refining prior works based on empirical observations from the field. (4) PLMs, as described in this paper, are artificially designed systems that hope to reveal mechanisms of naturally evolving ones. Thus, researchers should keep the final goal in mind and not mistake a technological tool for an absolute source of insight. (5) This paper focuses on using PLMs for behaviorally benchmarking cognitive profiles of neurotypical and developmentally typical individuals. PLMs should also be used for simulating the behaviors of impaired individuals to better support them. (6) We do not address broader ethical implications of modeling cognitive impairments, limited cultural generalizability of PLMs trained on Western-centric corpora, and other factors outside the scope of our arguments."}, {"title": "8 Ethical Considerations", "content": "There are no significant risks associated with conducting this research beyond those associated with working with PLMs. There may be risks in misinterpreting the criteria enlisted in this study. The suggestions in this study are one-way: we wish to find human performance characteristics and behaviors in PLMs to help model psychological sciences and, in the future, to aid people with cognitive impairments. We do not advocate for developing PLMs to replace humans or suggest ways to reach Artificial General Intelligence. PLMs are experimental technologies, and future work using these models should be conducted cautiously."}]}