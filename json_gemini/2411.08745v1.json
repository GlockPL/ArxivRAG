{"title": "Separating Tongue from Thought: Activation Patching Reveals\nLanguage-Agnostic Concept Representations in Transformers", "authors": ["Cl\u00e9ment Dumas", "Chris Wendler", "Veniamin Veselovsky", "Giovanni Monea", "Robert West"], "abstract": "A central question in multilingual language\nmodeling is whether large language models\n(LLMs) develop a universal concept represen-\ntation, disentangled from specific languages.\nIn this paper, we address this question by an-\nalyzing latent representations (latents) during\na word translation task in transformer-based\nLLMs. We strategically extract latents from a\nsource translation prompt and insert them into\nthe forward pass on a target translation prompt.\nBy doing so, we find that the output language\nis encoded in the latent at an earlier layer than\nthe concept to be translated. Building on this\ninsight, we conduct two key experiments. First,\nwe demonstrate that we can change the concept\nwithout changing the language and vice versa\nthrough activation patching alone. Second, we\nshow that patching with the mean over latents\nacross different languages does not impair and\ninstead improves the models' performance in\ntranslating the concept. Our results provide ev-\nidence for the existence of language-agnostic\nconcept representations within the investigated\nmodels.", "sections": [{"title": "1 Introduction", "content": "The emergence of the field of mechanistic inter-\npretability has led to the conception of powerful\ntools (Carter et al., 2019; Nostalgebraist, 2020;\nSchubert et al., 2020; Belrose et al., 2023; Cun-\nningham et al., 2023; Kram\u00e1r et al., 2024; Marks\net al., 2024; O'Neill and Bui, 2024; Tufanov et al.,\n2024) for the investigation of the inner workings of\ndeep neural networks such as large language mod-\nels (LLMs) (Vaswani et al., 2017; Radford et al.,\n2019; Touvron et al., 2023) with the ultimate goal\nof reverse engineering the algorithms encoded in\ntheir weights. As a result, researchers today are\noften able to open up a \"black box\" neural network,\nand with near surgical precision pinpoint where a\ncertain input-output behaviour comes from (Wang\net al., 2022; Conmy et al., 2023; Nanda et al., 2023;\nZhong et al., 2024; Furuta et al., 2024).\nThis provides a unique opportunity to examine\nhow multilingual concepts are represented and pro-\ncessed within LLMs, potentially revealing insights"}, {"title": "2 Background", "content": "2.1 Transformers's forward pass\nWhen an autoregressive decoder-only trans-\nformer (Vaswani et al., 2017; Touvron et al., 2023)\nwith L layers processes a sequence of input tokens\n$x_1,..., x_n \\in V$ from a vocabulary V, each token\nis initially transformed into a d-dimensional vector\n$h_i^{(0)}$ by an embedding layer. This first set of vector is\nthe beginning of the residual stream. Then, for each\ntoken position i, the layer $j \\in 1,..., L$ updates the\nresidual stream the following way:\n$h_i^{(j)} = h_i^{(j-1)} + f_j (h_1^{(j-1)},...,h_n^{(j-1)})$ (1)\nwhere $f_j$ represents the operations of the j-th layer\n(typically self-attention followed by a feedforward\nnetwork). Finally, for a m-layer transformer, the\nnext-token probabilities are obtained via a learned\nlinear layer followed by a softmax operation map-\nping $h_i^{(m)}$ to $P(x_{i+1}|x_1... x_i)$.\n2.2 Concepts\nWe use capitalization to denote an abstract concept,\n(e.g. CAT). Let C be an abstract concept, then we\ndenote $C_l$ its language-specific version. Further,\nwe define w($C_l$) as the set of words expressing the\nabstract concept C in language l. For example, if\nC = CAT and l = EN we have w($C_l$) = {\"cat\"}\nand similarly w($C_{DE}$) = {\"Katze\", \"Kater\"}.\nNote that we talk about words for the sake of\nsimplicity. However, on a technical level w\nrefers to the set of starting tokens of these words\n(e.g. \"Katze\", \"Kat\"). Therefore, when we\ntrack different sets of tokens W, (e.g. W \u2208\n{w($C_T$), w($C_H$), w($C_T$), w($C_{ZH}$), w($C_{EN}$) U\nw($C_{EN}$)} = W), we ensure that there is no token"}, {"title": "2.3 Prompt design", "content": "We use the same template as (Wendler et al., 2024):\nFor a given concept C, input language $l^{(in)}$, and out-\nput language $l^{(out)}$, we construct a few-shot trans-\nlation prompt TP($l^{(in)}$, $l^{(out)}$, C). This prompt con-\ntains examples of single-word translations from\n$l^{(in)}$ to $l^{(out)}$, concluding with the model being\ntasked to translate C from $l^{(in)}$ to $l^{(out)}$. For ex-\nample, TP(EN, FR, CLOUD) could be:\nHere the task is to translate w(CLOUDEN) =\n{\"cloud\"} into w(CLOUDFR) = {\u201cnuage\"}.\nImportantly, whether the model correctly an-\nswers the prompt is determined by its next token\nprediction. For example above, the next token pre-\ndicted should be \"nu\", the first token of \u201cnuage\".\nThus, we can track P($C^l$), i.e., the probability of\nthe concept C occurring in language l, by simply\nsumming up the probabilities of all starting tokens\nof w($C^l$) in the next-token distribution.\nWe improve upon the construction of Wendler\net al. (2024) by considering all the possible ex-\npressions of C in l using BabelNet (Navigli et al.,\n2021), instead of GPT-4 translations, when com-\nputing P($C^l$). This allows us to capture many\npossible translations, instead of one. For exam-\nple, \"commerce\u201d, \u201cmagasin\u201d and \u201cboutique\u201d are\nall valid words for SHOPFR."}, {"title": "3 Exploratory analysis with patching", "content": "Problem statement. We aim to understand\nwhether language and concept information can vary\nindependently during Llama-2's forward pass when\nprocessing a multilingual prompt. For example, a\nrepresentation of $C^l$ of the form $z_c = z_c + z_l$,\nin which $z_c \\in U$, $z_l \\in U^{\\bot}$ and $U \\oplus U^{\\bot} = R^d$\nis a decomposition of $R^d$ into a subspace U and\nits orthogonal complement $U^{\\bot}$, would allow for\nlanguage and concept information to vary inde-\npendently: language can be varied by changing\n$z_l \\in U^{\\bot}$ and concept by changing $z_c \\in U$. Con-\nversely, if language and concept information were\nentangled, a decomposition like this should not ex-\nist: varying the language would mean varying the\nconcept and vice versa."}, {"title": "3.1 Experimental design", "content": "We start our analysis with an exploratory experi-\nment on Llama 2 7B (Touvron et al., 2023). We\nuse 5-shots translation prompts to create paired\nsource S = TP($l^{(in)}$, $l^{(out)}$, $C_S$) and target prompt\nT = TP($l^{(in)}$, $l^{(out)}$, $C_T$) datasets with different\nconcept, input languages and output languages. If\nnot mentioned otherwise, $l_S$ and $l_T$ refer to the\noutput language of S and T.\nSimilar to (Variengien and Winsor, 2023), we\nwould like to infer at which layers the output lan-\nguage and the concept enter the residual stream\n$h_i^{(T)}$ respectively and whether they can vary in-\ndependently for our task. In order to investigate\nthis question, we perform the experiment depicted\nin Figure 2. For each transformer block $f_j$ we\ncreate two parallel forward passes, one process-\nsing the source prompt S = ($s_1, ..., s_{n_s}$) and one\nprocessing the target prompt T = ($t_1,...,t_{n_T}$).\nWhile doing so, we extract the residual stream\nof the last token of the source prompt after layer\nj, $h_{n_s}^{(j)}(S)$, and insert it at the same layer at posi-\ntion $n_T$ in the forward pass of the target prompt,\ni.e., by setting $h_{n_T}^{(j)}(T) = h_{n_s}^{(j)}(S)$ and subse-\nquently completing the altered forward pass. From\nthe resulting next token distribution, we compute"}, {"title": "3.2 Results", "content": "In this experiment, we perform the patching at one\nlayer at a time and report the probability that is as-\nsigned to P($C^S_l$), P($C^T_l$), P($C^S_{l'}$), and P($C^T_{l'}$).\nAs a result we obtain Figure 3 in which we report\nmeans and 95% confidence interval over 200 exam-\nples."}, {"title": "Interpretation", "content": "We observe the following pattern\nwhile patching at different layers (see Figure 3):\n\u2022 Layers 0-11: Target concept decoded in target\nlanguage, resulting in large P($C_{ZH}$).\n\u2022 Layers 12-16: Target concept decoded in\nsource language, resulting in large P($C_{IT}$).\n\u2022 Layers 16-31: Source concept decoded in\nsource language, resulting in large P($C_{IT}$).\nThis pattern suggests that the model first com-\nputes the output language: from layer 12 onwards,\nwe decode in the source output language. This in-\ndicates that up until that layer, the need to decode\nto $l^{(out)}$ is being encoded in the residual stream\nand subsequently remains unchanged. For exam-\nple, this could be achieved by the model computing\na function vector $Z_{p^{(out)}}$ (Todd et al., 2023). If this\nhypothesis is correct, patching at layer 12 would\noverwrite $Z_{p^{(out)}}$ with $Z_{p^{(out)}}$. The green spike be-\ntween layer 12 and 16 indicates that at those layer,\nthe concept is not yet represented, so the model"}, {"title": "Hypotheses", "content": "We are left with two hypotheses com-\npatible with these results, depicted in Figure 4:\n\u2022 H1: Concept and language are represented\nindependently. When doing the translation,\nthe model first computes $l^{(out)}$ from context,\nand then identifies C. In the last layers, it then\nmaps C to the first token of w($C^{l^{(out)}}$).\n\u2022 H2: The representation of a concept is always\nentangled with its language. When doing the\ntranslation, the model first computes $l^{(out)}$,\nthen computes $l^{(in)}$ and $C^{l^{(in)}}$ from its context\nand solves the language-pair-specific transla-\ntion task of mapping $C^{l^{(in)}}$ to $C^{l^{(out)}}$."}, {"title": "4 Ruling out hypotheses", "content": "Next, we run additional experiments to (1) provide\nfurther evidence that we are either in H1 or H2 and\n(2) to disambiguate whether we are in H1 or H2\n(3) to show that our findings hold for other models.\nFurther evidence experiment. In the experiments\nin Sec. 3 we did not observe source concept in tar-\nget language. However, both H1 and H2 would\nallow for that to happen via patching in the right\nway. Therefore, in this experiment, instead of over-\nwriting the residual stream at the last token of the\nprompt, we overwrite them at the last token of the\nword to be translated. Let $p_S$ and $p_T$ denote the\nposition of that token in source and target prompt\nrespectively. Since the concept information seems\nto enter via multiple layers (16-20) into the latent of\nthe last token, we overwrite the latent correspond-\ning to the token at position $p_T$ at layer j and all\nsubsequent ones. By patching in this way in both\nH1 and H2 we would expect to see large P($C^{T}_{ZH}$)."}, {"title": "Disambiguation experiment", "content": "Both H1 and H2\ncompute w($C^{T}_{ZH}$) but in different ways. In H1 one\ndecoding circuit per output language is required in\norder to compute the expression for the concept C's\nin language ly. In contrast, in H2 one translation\ncircuit per input-output language pair is required\nto map the entangled $C^{l^{(in)}}S^{l^{(in)}}$ to $C^{l^{(out)}}T^{l^{(out)}}$. Therefore,\nin order to disambiguate the two, we construct a\npatching experiment that should work under H1,\nbut fail under H2.\nIn order to do so, instead of patching the latent\ncontaining $C^{l^{(in)}}S$ from a single source forward pass,\nwe create multiple source prompts with the same\nconcept $C_S$ but in different input languages $l^{(in)}_i \\ne$\n$l^{(in)}_k$ and output languages $l^{(out)}_{S_i} \\ne l^{(out)}_{S_k}$\nand patch by setting\n$h_i^{(a)}(T) = \\frac{1}{k} \\sum^{k}_{i=1} h_{n_{S_i}}^{(a)}(S_i)$,\nfor a \u2208 j,..., m. Let $C_i = C_S$, under H1, tak-\ning the mean of several language-specific concept"}, {"title": "Results", "content": "In the first experiment we use the same\nlanguages as above and in the second one we used\nDE, NL, ZH, ES, RU as input and IT, FI, ES, RU, KO\nas output languages for the source prompts, and,\nFR to ZH for the target prompt.\nIn Figure 6 we observe, that in both experiments\nwe obtain very high probability for the source con-\ncept in the target language P($C^{ZH}_T$) from layers 0\nto 15, i.e., exactly until the latents at the last token\nstop attending to the last concept-token.\nTherefore, Figure 6 (a) supports that we are in-\ndeed either in H1 or H2, since as planned we suc-\ncessfully decode source concepts in the target lan-\nguage P($C^{ZH}_T$) from layers 0 to 15. Conversely,\nif we were not able to decode source concept in"}, {"title": "5 Conclusion", "content": "In this paper, we showed that transformers use\nlanguage-agnostic latent representations of con-\ncepts when processing word-level translation\nprompts. We achieved this by patching latents\nbetween parallel forward passes for translation\nprompts that differed in both input and output\nlanguages, as well as in the specific concepts be-\ning translated. Our main finding was that transla-\ntion performance improves when the transformer\nis forced to translate a concept representation av-\neraged across multiple languages. This finding\nspeaks for language-agnostic concept representa-\ntions. As we argued, for language-agnostic concept\nrepresentations, taking the mean representation of\na concept across languages should not impair the\nLLM's ability to translate this concept. In con-\ntrast, for language-specific ones, taking the mean\nshould result in interference between the different\nlanguage-specific versions of the concept. Thus,\nour results are consistent with findings from pre-\nvious work (Wendler et al., 2024) indicating that\nLlama-2 represents concepts in a concept space in-\ndependent of the language of the prompt. Further-\nmore, our work also provides evidence that findings\nfrom BERT models (Wu et al., 2019; Pires et al.,\n2019) generalize to a wide range of decoder-only\ntransformers."}, {"title": "Limitations", "content": "In this work, we studied how transformers repre-\nsent concepts when processing multilingual text.\nHowever, we only considered very simple transla-\ntion prompts and also probed only for the language-\nspecific words describing the concept. Further ex-\nperiments are needed to make claims about how\ntransformers process multilingual text in more gen-\neral settings. Furthermore, more fine-grained prob-\ning will be required to determine to which degree\ntransformers are able to specialize a concept to a\nlanguage and whether concepts and languages are\nentangled in more subtle ways."}, {"title": "A Patchscope experiment", "content": "We performed an additional experiment using the\npatchscope lens (Ghandeharioun et al., 2024) to\ncollect more evidence about from which layer it is\npossible to decode the source concept in Figure 7.\nThe results of this experiment corroborate the find-\nings presented in Section 3. To enable a convenient\ncomparison of the experimental results, we also\ninclude Figure 3 in Figure 7."}, {"title": "B Random prompt task experiment", "content": "In order to investigate the leftmost part of Figure 7a\nmore deeply, we performed additional experiments\nin which we explore \"random\" source prompts in-\nstead of translation source prompts.\nThe experimental setting here is similar to the\none in Sec. 3, except for the fact that instead of\npatching in latents from a translation source prompt\nwe patch latents from different \"random\" source\nprompts. For the random source prompts, we grad-\nually move away from the prompting template.\nSame template. In Figure 8a, we ran-\ndomized both input and output language as\nwell as concepts in the source prompts, re-\nsulting in prompts of the following form:\nA: \"CATDE\" - B: \"DOGIT\"\nA: \"OWLJA\" - B: \"SUNHI\"\nA: \"ICEFR\" - \u0412: \u201c\nBy doing this, the latent of the source prompt is\nsimilar in terms of prompt structure, but the model\ncannot infer a task vector specifying the output\nlanguage since the source prompt instantiates an\nimpossible task (to predict a random word in a ran-\ndom language). As shown in Figure 8a, for layers\n0-11, we observe no drop in the accuracy, which\nconfirms our hypothesis that in those layers the la-"}, {"title": "Empty context", "content": "For example, replacing the source\nprompt with an empty prompt, merely containing\nB: \" results in Figure 8b. In contrast to Figure 8a,\nthe target concept in target language probability\ndrops already starting from layer 4. We think this\nis due to the fact that until layer 4 the quotation\nmark token information which is shared among the\ntwo prompting templates \u201cdominates\u201d the latent\nrepresentation and is not yet converted to a task"}, {"title": "Modified template", "content": "Next, replacing the quotation\nmarks by \"@\" (Figure 8c) in the random prompt,\nA: @CATDE @ - B: @DOGIT @\ni.e., A: @OWLJA @ - B: @SUNHI @\nA: @ICEFR @ - B: @\nleads to a drop of performance for early layers,\nbut for layers 5\u201311, the model is not much affected\nby the patching. We postulate that at those layers,\nposition-marker tokens have been already mapped\nto a general position-marker feature that is similar\nin between source and target forward pass, even\nthough at input level different symbols have been\nused.\nShuffled tokens. Lastly, in Figure 8d we try to\ndestroy all of the shared structure in between the\nsource and the target prompt by randomly shuffling\nthe characters of the source prompts from the mod-\nified template task. As expected, the probability of\nthe target concept in target language becomes very\nlow (albeit surprisingly not zero), which shows\nthat the task cannot be solved without the position\nmarker feature."}, {"title": "C Other models", "content": "In this section, we report results for additional mod-\nels, namely, Mistral 7B (Jiang et al., 2023), Llama\n3 8B (Dubey et al., 2024), Qwen 1.5 7B (Bai et al.,\n2023) and Llama 2 70B (Touvron et al., 2023). We\nalso include Aya 23 8B (Aryabumi et al., 2024) for\nthe mean patching experiment in App C.2.\nC.1 Exploratory analysis\nThe results of the exploratory analysis outlined in\nSec. 3 are in Figure 9.\nAs can be seen in Figure 9, the target concept\nin source language spike is smaller for Llama 3,\nMistral 7B v0.3 and Qwen 1.5 7B. This hints that\nfor those models, $z_{e^{(out)}}$ and C computation overlap\nmore than for Llama-2-7B."}, {"title": "C.2 Ruling out hypotheses", "content": "In this section, we report results for the experiments\nperformed in Sec. 4.\nIn addition, instead of just patching in the mean\nover different language pairs (Figure 10c), we\nalso patch in the mean over contexts composed\nof different concept words in Figure 10b. In\nparticular, we take the mean over 5 different few-\nshot contexts from the same language pair. E.g.:\nOur results in Figure 10 show that the mean over\ncontexts does not increase P($C^T_{ZH}$), whereas the\nmean over language pairs does. This is intuitive,\nsince there may be some languages in which the\nmapping from words to concept features results in\nthe correct concept feature vector. Therefore, av-\neraging over different language pairs can increase\nthe signal about the source concept. However, hav-\ning additional random contexts stemming from the\nsame language pair does not bring in any informa-\ntion about the source concept."}]}