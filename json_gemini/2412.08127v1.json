{"title": "Evil twins are not that evil: Qualitative insights into machine-generated prompts", "authors": ["Nathana\u00ebl Carraz Rakotonirina", "Corentin Kervadec", "Francesca Franzon", "Marco Baroni"], "abstract": "It has been widely observed that language models (LMs) respond in predictable ways to algorithmically generated prompts that are seemingly unintelligible. This is both a sign that we lack a full understanding of how LMs work, and a practical challenge, because opaqueness can be exploited for harmful uses of LMs, such as jailbreaking. We present the first thorough qualitative analysis of opaque machine-generated prompts, or autoprompts, pertaining to 3 LMs of different sizes and families. We find that machine-generated prompts are characterized by a last token that is often intelligible and strongly affects the generation. A small but consistent proportion of the previous tokens are fillers that probably appear in the prompt as a by-product of the fact that the optimization process fixes the number of tokens. The remaining tokens tend to have at least a loose semantic relation with the generation, although they do not engage in well-formed syntactic relations with it. We find moreover that some of the ablations we applied to machine-generated prompts can also be applied to natural language sequences, leading to similar behavior, suggesting that autoprompts are a direct consequence of the way in which LMs process linguistic inputs in general.", "sections": [{"title": "1 Introduction", "content": "An intriguing property of language models (LMs) is that they respond in predictable ways to machine-generated prompts (henceforth, autoprompts) that are unintelligible to humans. Shin et al. (2020) first showed that autoprompts can outperform human-crafted prompts on various tasks. More worryingly, Wallace et al. (2019) and several other studies after them have shown that they can be used in adversarial attacks making models, including latest-generation aligned LMs, behave in undesirable ways (e.g., Zou et al., 2023b; Geiping et al., 2024).\nIn this paper, we present the first thorough qualitative analysis of autoprompts. We discover that, despite the superficial impression of opacity they convey, they can to a significant extent be explained in terms of a few general observations. First, in autoregressive models the last token of the prompts has a disproportionate role in generating the continuation, and this last token is both very important and often quite transparent in autoprompts. Second, several tokens contributing to the opaqueness of autoprompts act as fillers that are ignored by the model. Third, the non-final tokens that are actually influencing generation might do so in a keyword-like way, and even occasionally display a loose form compositionality, in a sense we'll make precise below. As we will see, these factors are also at play when LMs are fed natural-language sequences, suggesting that they are core properties of how LMs process linguistic strings.\nFrom a theoretical point of view, our study offers new insights into LM language processing in general. From a practical point of view, it highlights which aspects of LMs we should pay attention to, if we want to make them more robust to harmful autoprompts (or, conversely, to develop more efficient benign autoprompt generation techniques)."}, {"title": "2 Related work", "content": "Starting with the seminal work of Wallace et al. (2019) and Shin et al. (2020), many studies have revealed that, using various discrete gradient-following techniques it is possible to automatically discover prompts that, while unintelligible, let LMs generate a desired target output (e.g., Shin et al., 2020; Deng et al., 2022; Wen et al., 2023; Melamed et al., 2024),. Moreover, such prompts are at least to some degree transferable, in the sense that they can be induced using a LM, but then successfully used to prompt a different one, including much larger models (Rakotonirina et al., 2023; Zou et al., 2023b; Melamed et al., 2024).\nInitially, the interest was mainly in whether algorithmically-generated autoprompts could be used as alternatives to manually crafted prompts in knowledge-extraction tasks or other LM applications (e.g., Shin et al., 2020; Deng et al., 2022; Rakotonirina et al., 2023). With the recent astounding progress in LM ability to respond to natural language prompts, this goal has become somewhat obsolete, but autoprompts are still an important concern because they can be used for adversarial purposes, for example to bypass LM security filters in order to generate offensive or dangerous information (e.g., Zou et al., 2023b; Geiping et al., 2024). Even more importantly, the fact that several modern LMs are more likely to provide information about the star formation process when prompted with the string \"Produ bundcules cation ofstars efect\u201d than when prompted with the question \u201cWhat leads to the creation of new stars?\" suggests that there is something fundamental we still do not understand about how LMs process language.\nThere is relatively little work attempting to characterize the nature of autoprompts. Geiping et al. (2024) present a set of intriguing qualitative observations about how autoprompts support various types of attacks (e.g., by including instruction fragments in different languages), as well as an analysis of tokens commonly appearing in autoprompts. Ishibashi et al. (2023) find that autoprompts are less robust to token re-arrangement than natural prompts, whereas Rakotonirina et al. (2023) report that the autoprompts that best transfer across models contain a larger proportion of English words and, surprisingly, are less order-sensitive than autoprompts that do not transfer. Kervadec et al. (2023) analyze the activation paths of autoprompts and comparable natural sequences across the layers of a LM, finding that often they follow distinct pathways.\nMelamed et al. (2024) study, like us, what they call \"evil twins\u201d, namely autoprompts that produce continuations comparable to those of a reference natural sequence. They compare the relative robustness to token shuffling of autoprompts and natural prompts, finding that, depending on the model family, autoprompts might be more, less or comparably robust to shuffling. They also run a substitution experiment similar to the one we will describe below (but replacing tokens with a single, fixed, [UNK] token). They find that this ablation strongly affects the autoprompts: we find a more nuanced picture, by considering a large range of possible replacements."}, {"title": "3 Experimental setup", "content": "Models We use decoder-only LMs from the Pythia (Biderman et al., 2023) and OLMO (Groeneveld et al., 2024) families, as these are fully open-source models whose training data are publicly available. Specifically, in the text we discuss the results we obtained with Pythia-1.4B, and we replicate the main experiments with Pythia-6.9B and OLMo-1B in Appendix B, reporting similar results.\nData collection We sample 25k random English sequences from the WikiText-103 corpus (Merity et al., 2017), such that they contain between 35 and 80 (orthographic) tokens, and they are not interrupted by sentence boundary markers. We refer to these corpus-extracted sequences as original prompts. We also record the original continuation of these sequences in the corpus. We let moreover the LM generate a continuation of each prompt using greedy decoding. The generation process stops after a maximum of 25 tokens or when end-of-sentence punctuation (period, exclamation mark, question mark) is encountered. We filter out sequences whose generated continuation is less than 4 tokens long. As we are interested in genuine model generation, as opposed to cases where the model is simply producing a memorized corpus sequence, we compute the BLEU score (Papineni et al., 2002) between the model continuation and the original continuation, removing sequences with BLEU greater than 0.1. After completing the filtering processes, we are left with a total of 5k sequences, which we use to train autoprompts.\nPrompt optimization For each target continuation, we want to find a fixed-length autoprompt that makes the model produce that continuation."}, {"title": "4 Experiments", "content": "To achieve that, we maximize the probability of the target continuation given the prompt. More formally, if we denote the target sequence by $(t_1, ..., t_m) \\in V^m$, where V is the vocabulary, and the n-length autoprompt by $(p_1, ..., p_n) \\in V^n$ (in our case, n = 10), the optimization problem can be formulated as follows:\n$\\displaystyle\\min_{(p_1,...,p_n) \\in V^n} log P_{LLM}(t_1, ..., t_m | p_1, ..., p_n)$\nWe use a variant of Greedy Coordinate Gradient (GCG) (Zou et al., 2023b), a widely used gradient-based algorithm that iteratively updates the prompt one token at a time (Ebrahimi et al., 2018; Wallace et al., 2019; Shin et al., 2020). During each iteration, we select the top 256 tokens with the largest negative gradients for every position, then we uniformly sample 256 candidates across all positions. We then compute the loss of each candidate replacement, and select the one with the lowest loss. In our experiments, we run up to 150 iterations of this process. We discard cases in which, after this number of iterations, we have not found an autoprompt that produces the very same continuation as the original prompt.\nData-set statistics The final data-set we use for the Pythia-1.4B experiments reported in the main text consists of 2473 triples of original prompt, autoprompt and continuation. The average original prompt length is of 38.6 tokens (s.d. 11.7); that of the continuations is of 9.4 tokens (s.d. 2.7)."}, {"title": "4.1 Pruning autoprompts", "content": "We greedily prune the autoprompts in our data-set. Starting from the original sequence of n tokens, we strip each token in turn, and pick the n-1-length sequence that produces the same continuation as the original, if any (if there's more than one such sequence, we randomly pick one). We repeat the process starting from the shortened sequence, and stop where there is no shorter sequence generating the original continuation, or when we are down to a single-token prompt. It is possible to shorten the original autoprompt in a clear majority of the cases (60%), with the average pruned autoprompt having lost 1.9 tokens of 10 (s.d.: 1.1). Autoprompt-discovery algorithms fix the number of tokens as a hyperparameter. It is thus reasonable that some tokens in the final autoprompt are just there to fill all the required slots, and can consequently be pruned. The view that pruned tokens are filler-like is supported by the following observation. We roughly classified the tokens into the autoprompts into language-like and non-linguistic, such as digits, punctuation, code-fragments and non-ascii characters. We found that the proportion of non-linguistic tokens is decidedly higher among pruned tokens (32.9%) than among kept tokens (24.5%).\nTable 2 further shows tokens that are most typically kept or removed by the pruning algorithm according to the local mutual information statistics (Evert, 2005). Among the kept ones, we notice a prevalence of content words such as verbs, nouns and adjectives, whereas the typically pruned tokens are function words or word fragments.\nAs expected if they are somewhat filler-like, pruned tokens are easier to ignore when randomly interspersed into natural sequences than non-pruned tokens are. To quantify this claim, we extracted the set of tokens that are always pruned in our autoprompt corpus, as well as the set of tokens that are never pruned. We then inserted in each of the original prompts a random sample of 3 always-pruned or always-kept tokens, in random positions. We measured how this affected the continuation by computing the average BLEU scores for the continuations after the insertion, with the original continuations as reference. Not surprisingly, in both cases adding 3 new tokens does affect generation, but adding pruned tokens has a lower effect than adding kept tokens: the average pruned-token insertion BLEU is at 0.40 ( s.d. 0.39); the kept-token insertion BLEU is at 0.37 (s.d 0.39). The difference is highly significant according to a paired t-test (p < 0.001).\nImportantly, the likelihood of pruning is not equally distributed across autoprompt positions: as Fig. 1 shows, the last token of the autoprompt is extremely unlikely to be pruned, pointing to the special role it plays in generating the continuation. It seems that, when analyzing autoprompts, we can establish a 3-way distinction, in terms of importance, between pruned tokens, kept tokens in all"}, {"title": "4.2 Replacing autoprompt tokens", "content": "Working from now on with the pruned autoprompts, we replace the token in each position in turn with one of the 10k most frequent tokens from the Pile. We quantify the impact of the ablations in terms of BLEU score with respect to the original continuation. The ablation results are summarized in Fig. 3, where replacements are binned based on the impact they have on the continuation.\nFirst, we confirm that non-pruned tokens in all positions play a significant role in generating the continuation, as shown by the fact that most replacements have a strong impact on BLEU. However, for all positions except the last, we also see that a non-negligible proportion of replacements do not affect the continuation at all, and in a significant proportion of cases the continuation is only mildly affected (as the examples in Table 8 of Appendix A show, even a BLEU score around 0.2 typically corresponds to a continuation that is quite similar to the original).\nWe confirm moreover the special role of the last token, that can almost never be replaced without a catastrophic result on the continuation. The importance of the ending of the autoprompt is further shown by the fact that, as we approach the last position, it is increasingly more difficult to find replacements that do not strongly affect the continuation.\nFurthermore, by manually inspecting the cases that lead to only a moderate change in the continuation, we observe that sometimes they show a degree of \"compositionality\", in the sense that the continuation stays the same except for one or a few tokens that are replaced with new tokens that reflect the meaning of the replacement, and/or drift away from the meaning of the replaced token. Some examples are presented in Table 3.\nTo make this intuition more quantitative, we ran the following experiment. First, to facilitate automated similarity analysis, we extracted all cases where the replacement leads to the change of a single (typographic) word in the continuation (about 3% of the total cases). For these cases, we used FastText (Bojanowski et al., 2017) to measure the semantic similarity of both the original autoprompt token and its replacement to the original word in the continuation and to the changed one. We found that the original token is more similar to the new continuation word (vs. the original one) in only 37% of the cases, whereas the replacement token is more similar to the new continuation in 55% of the cases. We thus conclude that, indeed, there is a tendency for at least this type of replacement to work compositionally, with a small change in the autoprompt leading to a semantically consistent change in the continuation. This, in turn, suggests that autoprompts do not function as unanalyzable holistic wholes, but their \"meaning\u201d to the model derives, at least partially, from assembling the meaning of its parts, as with natural language sequences. As the examples show, though, this assembling looks nothing like the one performed by natural language syntax."}, {"title": "4.3 Shuffling autoprompt tokens", "content": "The picture we get from the previous studies is one where autoprompts are composed of three types of tokens. A number of tokens are fillers that, being ignored by the LM, can simply be pruned. The final token is extremely important and hard to change, because, in autoregressive prediction, it determines the exact nature of the first token of the continuation, and consequently strongly affects the rest of the continuation. The other non-prunable tokens also have an impact on the continuation, but they seem to rather work as single \"keywords\" that affect the semantic content of what follows, without forming a tight syntactic bound with each other and what follows.\nPrevious work has uncovered a somewhat mixed picture in terms of the robustness of autoprompts to token order shuffling (Ishibashi et al., 2023; Rakotonirina et al., 2023; Melamed et al., 2024). Based on what we just observe, we conjecture that the last token will be \u201crigid\u201d, as moving it around would strongly affect the continuation, whereas the preceding tokens might be more robust to order ablations. To test the conjecture, we randomly shuffled the tokens (10 repetitions per autoprompt) and measured the resulting BLEU with respect to the original continuation. We either shuffled all tokens or left the last one fixed.\nThe average BLEU when shuffling all tokens is at 0.02 (s.d. 0.03) and at 0.05 (s.d. 0.07) when leaving the last token in its slot. This difference is highly significant (paired t-test, p < 0.001). However, the low BLEU values suggest that, contrary to our conjecture, the autoprompt tokens before the last are not a bag of keywords, since their order matters as well. One possibility is that, while autoprompts as a whole do not constitute syntactically well-formed sequences, they are composed of tight sub-sequences that should not be separated. For example, given that modern tokenizers split text at the sub-word level, token-level shuffling will arbitrarily break words.\nSome support for the view that the catastrophic effect of shuffling pre-last tokens is due to short-distance dependencies comes by looking at the cases in which a bigram in an autoprompt (excluding the last position) is also attested in the Pile corpus, either in the original or in the inverted order. In 61.5% of these cases, the Pile frequency of the original bigram is larger than that of the inverted one. This suggests that there is at least some tendency towards a natural local ordering among autoprompt tokens."}, {"title": "4.4 Making human prompts more autoprompt-like", "content": "As a final piece of evidence that the dynamics we see at work in autopromts are general properties of how LMs process language, we re-ran some of the experiments above on the original corpus-extracted natural-language prompts, finding that they respond in similar ways to our ablations.\nPruning Applying the same greedy-pruning method to the original prompts, we find that more than 99% can also be pruned, with 21.9 tokens removed on average. Considering the average token length of the original prompts is 38.57, this means that, strikingly, on average 57% of the tokens can be removed without affecting the continuation. Since the prompts are long, one could think that what is removed is primarily material towards the beginning of the sequence, but actually we find that 95% of the prompts also have pruned tokens among the last 10 items.\nExamples of the latter are in Table 4. Prunable material often consists of modifiers whose removal does not affect the basic syntactic structure of the fragments (\u201cstrategic bomber\u201d, \u201csection of the pipeline\u201d, \u201creplication fork\u201d. . . ), but this is not always the case, and in many examples pruning turns well-formed sentences into seemingly unstructured token lists or telegraphic texts at best (\u201cmost section, since it\u201d, \u201cfork mobile day but", "\u2026 \u2026 \u2026 bomber Tu, / which was designed. . .": "\u2026 \u2026 \u2026 since it / was the only one...\").\""}, {"title": "5 Discussion", "content": "We show that seemingly opaque, machine-induced prompts possess, to some extent, interpretable properties, such as a strong reliance on the last token, the presence of filler tokens that are ignored by the model, and the compositional-like behavior of some keyword tokens. We further observe that some of these properties are also present in natural prompts.\nThese findings might shed some light on how LMs process language in general. They seem to rely on a simplified model of it, where not all tokens have specific syntactic and semantic functions in an abstract syntactic tree. We note that the phenomenon of relying on over-simplified representations of the data is not specific to LMs. Convolutional Neural Network classifiers of visual data have also been shown to latch onto superficial correlations in the data, leading to poor out-of-distribution generalization (Jo and Bengio, 2017; Ilyas et al., 2019; Yin et al., 2019; Geirhos et al., 2020).\nIdentifying and characterizing the features that deep learning models respond to are crucial steps in understanding their inner workings and making them more robust. In future work, besides addressing the issues discussed in the Limitations section below, we aim to extend our analysis beyond discrete tokens, focusing either on circuits through mechanistic interpretability methods or on representations using a more top-down approach, such as representation engineering (Zou et al., 2023a)."}, {"title": "Limitations", "content": "\u2022 Due to the time it takes to induce autoprompts with our computational resources, we could only experiment with 3 models, the largest of which has 6.9B parameters. We make our code available in hope that researchers with bigger resources will run similar experiments on a larger scale.\n\u2022 For analogous reasons, we only experimented with one variant of the autoprompt inducing algorithm, and we fixed the number of tokens in the induced prompt to 10. Given that all algorithms we are aware of adopt similar gradient-following methods, and based on qualitative inspections of autoprompt examples in other papers, we expect our conclusions to hold for autoprompts independently of how they are induced, but this should be verified empirically.\n\u2022 Our autoprompts most closely resemble adversarial attack where an obfuscated sequence is used to retrieve one specific piece of information from the LM. However, autoprompts might be also induced for other purposes, such as to improve factual knowledge retrieval when combined with a query sequence (Shin et al., 2020). It remains to be explored if different classes of autoprompts possess significantly different properties.\n\u2022 We have now a basic understanding of how an autoprompt determines its continuation, but we still need a better characterization of which tokens are more likely to be pruned, and of the means by which randomizing non-last tokens affects the continuation so strongly."}, {"title": "Ethics Statement", "content": "If we do not achieve a genuine understanding of how LMs process and generate text, we cannot fully control their behaviour and mitigate unintended or intentional harm. Opaque autoprompts are an indication that there are important aspects of LM prompting and generation that are still out of our control. Our investigation into the nature of this phenomenon contributes to a better understanding of how LMs work and, thus, ultimately, to make them safer and more predictable."}]}