{"title": "Piano Transcription by Hierarchical Language Modeling with Pretrained Roll-based Encoders", "authors": ["Dichucheng Li", "Yongyi Zang", "Qiuqiang Kong"], "abstract": "Automatic Music Transcription (AMT), aiming to get musical notes from raw audio, typically uses frame-level systems with piano-roll outputs or language model (LM)-based systems with note-level predictions. However, frame-level systems require manual thresholding, while the LM-based systems struggle with long sequences. In this paper, we propose a hybrid method combining pre-trained roll-based encoders with an LM decoder to leverage the strengths of both methods. Besides, our approach employs a hierarchical prediction strategy, first predicting onset and pitch, then velocity, and finally offset. The hierarchical prediction strategy reduces computational costs by breaking down long sequences into different hierarchies. Evaluated on two benchmark roll-based encoders, our method outperforms traditional piano-roll outputs 0.01 and 0.022 in onset-offset-velocity F1 score, demonstrating its potential as a performance-enhancing plug-in for arbitrary roll-based music transcription encoder. We release the code of this work at https://github.com/yongyizang/AMT_train", "sections": [{"title": "I. INTRODUCTION", "content": "Automatic music transcription (AMT) is a task of converting audio recordings into symbolic representations [1]. As a key topic in music information retrieval (MIR), AMT bridges audio-based and symbolic-based music understanding. AMT systems enable applications such as score following [2] and audio-score alignment [3].\nPiano transcription, an instrument-specific subtask of AMT, is a challenging task due to the high polyphony of piano music. Numerous methods have been utilized for piano transcription in recent decades, including Factorization-based models [4], adaptive estimation of harmonic spectra [5], and SVM-HMM structure [6]. With deep learning's rise, models like CNN [7] and CRNN [8] have been effectively applied in AMT. Onsets & Frames system [9] significantly improved note-level metrics by integrating onset and pitch detection. Kong et al. [10] further enhance the AMT system by proposing a high-resolution AMT system trained by regressing precise onset and offset times of piano notes. To minimize model size, studies [11], [12] have used prior knowledge of harmonic structures in audio representations to develop Dilated Convolutional networks. Transformer is a revolutionary model with a encoder-decoder architecture, the self-attention mechanism of which can extract global features and catch long-term relationships. Toyama et al. [13] proposed a two-level hierarchical frequency-time Transformer to catch long-term spectral and temporal dependencies to determine the precise onset and offset for each note. The above methods use piano rolls as output, which has a frame-level resolution and requires a threshold and a post-processing stage to decode it into a note sequence. Hawthorne et al. [14] revolutionize the AMT framework by treating it as a sequence-to-sequence task, where the the output is directly a sequence of note-event tokens, eliminating the need for extensive threshold-based post-processing\nFrame-level and language model (LM)-based systems have traditionally been viewed as distinct approaches in AMT. Frame-level systems utilize a compact piano-roll objective but require complex post-processing, while LM-based systems directly output note-level predictions. However, LM-based systems face challenges due to the lengthy sequences created by flattening note events that contain tokens of onset, offset, pitch, and velocity, resulting in resource-intensive training and inference processes. Furthermore, audio encoder selection has been explored in related domains such as audio captioning [15]\u2013[19] and multimodal large language models [20]-[23], yet its impact on AMT remains unexplored. This gap in research presents an opportunity to bridge the divide between frame-level and LM-based approaches.\nIn this paper, we introduce a novel approach that leverages the strengths of both roll-based systems and note-based language models for music representation. Our proposed method employs pre-trained roll-based systems as encoders and a language model as a decoder, effectively combining their respective advantages. To enhance prediction efficiency, we implement a hierarchical prediction strategy: first predicting onset and pitch, followed by velocity, and finally offset. To achieve the hierarchical prediction strategy, we trained three models with the same model architecture to predict onset and pitch, velocity, and offset, respectively. This approach significantly reduces the prediction sequence length compared to a flattened note event sequence, resulting in improved performance. We empirically evaluate the impact of different roll-based encoders and language model decoder size. Our findings reveal that the choice of encoder has a much more substantial effect on overall performance than the size of the language model. Notably, we observe that increasing the language model size does not reflect as improved model performance, corroborating observations reported in [14]. We further found that velocity modeling is more prone to overfitting compared to onset-pitch and offset. Our findings highlight the importance of encoder selection in AMT tasks, calling for further research in improving the scalability of language-model-based AMT systems.\nThis paper is structured as follows: Section II provides a detailed description of both the piano roll-based and LM-based systems. In Section III, we compare the traditional flattened token construction method with our proposed hierarchical approach. Our experimental design is outlined in Section IV, followed by Section V, which presents our findings, including results, ablation studies, and related discussions. Finally, Section VI offers concluding remarks."}, {"title": "II. ROLL-BASED AND LM-BASED AMT SYSTEMS", "content": "In roll-based AMT, a waveform x is firstly transformed to an feature in time-frequency domain $X \\in \\mathbb{R}^{T\\times F}$, where T is time frames and F is frequency bins, using short-time Fourier transform (STFT). Then, the feature is transformed to predict a piano roll $Y \\in \\{0,1\\}^{T\\times K}$, where K is 88 possible pitches, and 0 or 1 encodes absence or presence of each pitch. The neural network model $f_{\\theta}(X)$ predicts $\\hat{Y} \\in [0, 1]^{T\\times K}$, representing predicted pitch probabilities. Training typically uses binary cross-entropy loss:\n$\\mathcal{L}_{BCE} = \\frac{1}{T K} \\sum_{t=1}^{T} \\sum_{k=1}^{K} [Y_{t,k} \\log(\\hat{Y}_{t,k}) + (1 - Y_{t,k}) \\log(1 - \\hat{Y}_{t,k})]$.\nAt inference time, the continuous predictions $\\hat{Y}_{t,k}$ are typically thresholded to obtain binary predictions, which are then post-processed to extract note events with onset and offset times. While roll-based systems have shown good performance, the need for post-processing limits their use cases, and have motivated the development of LM-based systems."}, {"title": "B. LM-based AMT Systems", "content": "Language Model (LM)-based AMT systems treat music transcription as a sequence generation task. In these systems, the input audio $X \\in \\mathbb{R}^{T\\times F}$ is typically first encoded into a sequence of hidden representations $H \\in \\mathbb{R}^{T'\\times D}$, where $T'$ is the number of encoded time steps and D is the dimension of the hidden representation. The system then generates a sequence of note events $Y = (y_1, \\dots, y_N)$, where each $y_i$ represents a note event typically consisting of onset time, pitch, duration, and velocity. The neural network model in an LM-based system can be represented as a conditional language model $p_{\\theta}(Y|X)$, where $\\theta$ are the learnable parameters. This model generates the probability distribution of the next note event given the previous events and the input audio:\n$P_{\\theta}(Y|X) = \\prod_{i=1}^{N} P_{\\theta}(Y_i|Y_{<i}, X)$.\nTherefore, the training objective for LM-based systems typically uses the negative log-likelihood loss:\n$\\mathcal{L}_{NLL} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log p_{\\theta} (y_i|y_{<i}, X)$.\nAt inference time, the model generates note events autoregressively, often using beam search or other decoding strategies to improve the quality of the generated sequence. This approach allows for direct generation of note-level predictions without the need for post-processing, potentially capturing long-term dependencies in the music. While LM-based systems offer the advantage of direct note-level prediction, they are often computationally expensive due to the need for flattening of note event sequence."}, {"title": "III. FLATTENED AND HIERARCHICAL TOKEN STRUCTURE", "content": "Our system is composed of an encoder $f_{enc}$ and a LM-based decoder $f_{dec}$, as illustrated in Figure 1. The encoders $f_{enc}$ are pre-trained on piano roll objectives, and transforms the input audio $X \\in \\mathbb{R}^{T\\times F}$ into a sequence of hidden representations $H = f_{enc}(X) \\in \\mathbb{R}^{T'\\times D}$, where the LM decoder $f_{dec}$ is designed to predict note-level output $Y = (y_1, ..., y_N)$ from the frame-level feature H extracted by the encoder. We propose a hierarchical prediction strategy for note events, where $f_{dec}$ predicts note pitch, velocity, and offset sequentially, controlled by task-specific query tokens $q \\in \\{q_p, q_v, q_f\\}$ with a vocabulary size of three. The probability of generating a note event $Y_i$ can be expressed as:\n$P_{\\theta} (Y_i| Y_{<i}, X, q) = f_{dec}(y_{<i}, H, q)$,\nwhere $\\theta$ represents the learnable parameters of both the encoder and decoder. In this section, we detail how we construct the token sequence used to train our system.\nLet $H = (x_{1:T'})$ be the hidden representations extracted from the input audio sequence, and $Y = Y_{1:N}$ be the sequence of note events, where each $y_n = (o_n, p_n, v_n, d_n)$ represents a note event with onset time $o_n$, pitch $p_n$, velocity $v_n$, and offset $d_n$. For $P_{onset-pitch}$, $P_{velocity}$, and $d_{offset}$, we add <sos> to the sequence and append task-specific query tokens $q_p$, $q_v$, or $q_d$. Notes are organized by onset time (first to last), then pitch (low to high). We use <eos> to end sequences and <pad> for batching. For flattened sequences, we follow [14], organizing note onset, pitch, velocity, and offset similarly.\nDifferently than [14], which employs an encoder-decoder language model T5 [24], we use LLaMA [25], a decoder-only language model"}, {"title": "IV. EXPERIMENT DESIGN", "content": "For a comprehensive evaluation, we employ two benchmark roll-based systems, and adapt them as encoder. Specifically, we use CRNN [10] and HPPNet [11]. CRNN is comprised of convolutional layers, followed by bi-directional GRU layers and a linear readout layer. Two models of the same architecture are designed to perform note and pedal predictions. We take the embedding before the final readout layer, with 768 dimensions, and concatenate both to form a 1536-dim embedding as the representation H into the LM decoder. HPPNet combines convolutional and recurrent elements, introducing \"harmonic dilated convolution\" (HD-Conv) layers to exploit harmonic characteristics of the input constant-Q transform."}, {"title": "V. RESULTS AND DISCUSSIONS", "content": "Results are shown in Tab. I. Comparing between the three settings, we can see that the flattened sequence is slightly worse at modeling onset behavior, but drastically worse at predicting note offset; whereas the hierarchical setting consistently reports comparable or better performance. It is worth noting that the \"Roll\" approach requires setting a threshold to gate notes as posterior information, whereas the \"Hierarchy\" approach do not. We have also achieved new state-of-the-art result of the LM-based piano transcription model with the HPPNet setting on all three $F_1$ scores.\nComparing between our results and results from [14], which also applies a flattened sequence during training, we see that their results are much closer to the \"Roll\" approach. We believe this is primarily due to the influence of sequence length. In [14], each segment is 4.088 seconds, while in ours, the segment length is 10 seconds. This resulted in more notes per sequence, and with the flattened approach, sequence length grows quickly with note events, which may have hindered model performance. Also, [14] used an encoder-decoder language modeling architecture, while we use a decoder-only language model. We hypothesize that the non-autoregressive nature of the encoder-decoder architecture helped it better encode information that would be especially lost in the long, flattened sequence [28], therefore increased its performance compared to our flattened setting.\nLanguage models have been observed to show a \"scaling law\" where large models with more parameters exhibit better perfor-mance [29]. However, on the piano transcription task, [14] observed that larger model overfits rather than generalize better. To better examine this phenomenon, we conduct three scaling experiments by training hierarchical token sequence models on three settings, dubbed as \"tiny\", \"small\" and \"large\", as shown in Table II. We conduct these scaling experiments on 8 NVIDIA RTX 4090s.\nResults for the scaling experiments are shown in Table III. We observe that compared to the original setting, no significant change in any metric is observed, while all settings slightly under-perform the \"base\" setting. To examine the training process, we plot the training and validations sets' loss at different timesteps in Figure 3. We observe that while onset-pitch and offset language models quickly plateaus during training, the velocity training show over-fitting at only about 100k steps, similar to the phenomenon described in [14]. This suggests that the scaling behavior for different token types are different, and further merits our hierarchical approach."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we propose a hybrid method that combines pre-trained roll-based encoders with an LM decoder. Besides, our approach employs a hierarchical prediction strategy, first predicting onset and pitch, then velocity, and finally offset. The hierarchical prediction strategy reduces computational costs by breaking down long sequences. Evaluated on two benchmark roll-based encoders, our method outperforms traditional piano-roll outputs 0.01 and 0.022 in onset-offset-velocity F1 score, demonstrating its potential as a performance-enhancing plug-in for any roll-based encoder. Results show that encoder choice significantly impacts performance more than LM size, highlighting the importance of encoder selection in AMT. This study calls for further investigation into improving the scalability of LM-based AMT systems."}]}