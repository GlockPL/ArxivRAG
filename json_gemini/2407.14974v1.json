{"title": "Out of spuriousity: Improving robustness to spurious correla- tions without group annotations", "authors": ["Phuong Quynh Le", "J\u00f6rg Schl\u00f6tterer", "Christin Seifert"], "abstract": "Machine learning models are known to learn spurious correlations, i.e., features having strong relations with class labels but no causal relation. Relying on those correlations leads to poor performance in the data groups without these correlations and poor generalization ability. To improve the robustness of machine learning models to spurious correlations, we propose an approach to extract a subnetwork from a fully trained network that does not rely on spurious correlations. The subnetwork is found by the assumption that data points with the same spurious attribute will be close to each other in the representation space when training with ERM, then we employ supervised contrastive loss in a novel way to force models to unlearn the spurious connections. The increase in the worst-group performance of our approach contributes to strengthening the hypothesis that there exists a subnetwork in a fully trained dense network that is responsible for using only invariant features in classification tasks, therefore erasing the influence of spurious features even in the setup of multi spurious attributes and no prior knowledge of attributes labels.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks tend to learn spurious attributes, especially when spurious attributes have a positive relation with target labels and are easier to learn which is known as simplicity bias (Shah et al., 2020). This phenomenon leads to the scenario that deep neural networks under-learn invariant information and then lose generalization ability and perform poorly in the data group that does not hold spurious correlations. Spurious features may occur naturally and some of them, such as image background, can be easily detected, whereas other implicit spurious features, such as texture (Geirhos et al., 2018), superficial statistics (Wang et al., 2019) or frequency bias defined by Fourier domain (Wang et al., 2022; 2023) requires more effort. An example of a natural spurious correlation of background is the cows and camels classification task, where the background desert is spuriously correlated with the class camels and grass positively correlates with cows (Beery et al., 2018). These features seem to be easy to learn for deep networks, but relying on them makes networks perform poorly in minority groups such as camels on grass background.\nTo directly improve the performance of minority groups, mitigation strategies use group label information during training to minimize the worst-group loss (Sagawa et al., 2020a) or re-direct the classification distribution by retraining part of deep networks (Kirichenko et al., 2023). However, group labels can only be collected if one has prior knowledge of the spurious correlations within the dataset, and even in that case, the label annotation process requires much time and effort and might be impossible in a realistic scenario. For example, Wang et al. (2022) defined a frequency shortcut type based on Fourier domain. However, the detection of this type of shortcut is heavily influenced by the order of frequency removal, and their observations are limited to texture-based shortcuts (Wang et al., 2023). To overcome the limitation of collecting group labels, a scope of work leverages hard cases in the training data and defines a weighted loss for retraining models (Liu et al., 2015; Nam et al., 2020; Zhang et al., 2022; Park et al., 2023). Hard cases are those instances that are misclassified by a model trained with empirical risk minimization (ERM). This definition is based on the assumption that models that are influenced by spurious correlations fail to classify instances where the spurious correlation is not present. However, this assumption is not guaranteed to hold. Further, hard cases are not available in the case of zero training loss, i.e., when the model fully fits the training data. Then, careful early-stopping control is required to obtain a sufficient amount of hard cases without fully detoriating the ERM training.\nTo overcome the limitation of the assumption that ERM misclassified instances are hard and spurious-free cases, we propose Pruning Spurious Correlations (PruSC) that defines potential spurious groups based on the representation of instances in latent space. Our main motivation is that if the ERM models have spurious correlations, we assume that instances with the same spurious features lie close together in the representation space. That means we can identify groups of intances that have the same spurious feature by applying unsupervised clustering on the output of the penultimate layer. Consequently, to reduce the effect of spurious correlations, we distort the clusters of spurious features in feature space. To do that, we introduce a novel application of supervised contrastive loss for training a subnetwork by defining positive and negative anchors based on both target labels and their position in the feature space. By the distortion of spurious feature clusters in representation space, the subnetwork cannot rely on learning and grouping samples based on similar spurious attributes and hence is less influenced by spurious correlations.\nPruSC does not require prior knowledge about spurious correlations and is even applicable when more than one spurious attribute is present or the attributes are not trivial for human to annotate (e.g., texture or fluency bias (Geirhos et al., 2018; Wang et al., 2023)). In addition, clusters of spurious attributes form in the representation space as soon as models rely on spurious correlations, i.e., also when models have zero training loss. Hence, our approach to mitigate spurious correlations can be directly applied to a fully trained ERM model\nIn summary, our contributions are:\n\u2022 We propose a subnetwork pruning approach PruSC to mitigate the influence of spurious correlations in deep neural networks that does not require group annotations or prior knowledge of spurious features, not even in the validation set for fine-tuning hyper-parameters.\n\u2022 We propose an effective way to construct contrastive learning batches based on representation clustering.\n\u2022 Our approach can be applied directly to a trained network without any control requirements (e.g., misclassified cases defined from early stopping).\n\u2022 Our approach is robust to multiple spurious correlations.\nWe evaluate our approach on the CelebA dataset (Liu et al., 2015) with potential multiple spurious attributes and the Skin Cancer ISIC dataset (Codella et al., 2019), a realistic image collection used to detect melanoma and an artificial benchmark WaterBird (Sagawa et al., 2020a). Our method outperforms all approaches that do not require group annotations, even for tuning hyper-parameters, and is comparable with methods that require group labels during training. Our code is available at: https://github.com/pquynhle/ spurious-free-subnetwork"}, {"title": "2 Related Work", "content": "Robustness to Spurious Correlations. Approaches to mitigate spurious correlations can be broadly classified in two main categories: having prior knowledge of spurious correlations (such as group labels or human annotations) or not having that information.\nWith full information about spurious correlations on all instances, data-centric approaches like UV-DRO (Srivastava et al., 2020) and LISA (Yao et al., 2022) augment the training data with additional samples to eliminate the spurious correlation. Group-DRO (Sagawa et al., 2020a) instead utilizes the existing group labels directly to minimize the loss for worst-group performance.\nRequiring only a small subset of group labeled training data, BARACK (Sohoni et al., 2022) predicts other instances' groups and DFR (Kirichenko et al., 2023; Izmailov et al., 2022) retrains part of the model with a small, spurious-free dataset. Prior knowledge of spurious correlations has the advantage that training regimes can directly mitigate undesired attribute influence or relations in the data and consequently yields high accuracy, both on average and in the minority group. However, those group annotations or human-in-the-loop annotations are costly and hard to attain in practice. In particular complex spurious attributes, such as as fluency bias (Wang et al., 2023) or texture and shape bias (Geirhos et al., 2018) remain challenging, as they are hard to recognize for humans.\nWhen group annotations are not available, (Sohoni et al., 2020; Seo et al., 2022) and Creager et al. (2021) focus on automatically inferring appropriate groups before training a robust model (e.g., Group-DRO) with pseudo-group labels. Multiple methods follow a two stage training approach, where the first stage commonly serves to identify hard cases from ERM training. These hard cases are instances that the ERM model fails to classify, presumably because ERM relies on spurious correlations but in those instances the correlation is not present (also called bias-conflict). Approaches differ in the second stage: (Liu et al., 2021; Nam et al., 2020) upweight hard cases in an additional training run, while (Yaghoobzadeh et al., 2021) exclusively fine-tune hard cases. CNC (Zhang et al., 2022) and DCWP (Park et al., 2023) utilize the set of hard cases for contrastive learning to define contrastive batches and retrain the whole network (CNC) or to extract a subnetwork (DCWP) with contrastive loss. Different from two-stage methods, DEDIER (Tiwari et al., 2024) uses knowledge distillation to find over-confident predictions in early network layers d then leverages this signal into a graded adjustment of distillation loss on a per-instance level. SELF (LaBonte et al., 2024) finds a small set of hard cases by measuring the degree of disagreement in predictions between an ERM and an early-stopped ERM model, then applies last-layer fine-tuning with this set.\nThe effectiveness of methods that rely on ERM misclassifications for hard cases depends on both quantity and quality of misclassifications. In cases when ERM models almost perfectly fit the training data (training accuracy close to 100%), there are not enough hard cases (i.e., misclassifications) and these methods fail. To obtain a sufficiently large set of hard cases, careful control of early-stopping regularization or choice of layer d (DEDIER) is required. Hence, those methods can not directly mitigate spurious correlations of a trained ERM model unless trainers have access to the whole training process of that model.\nModular Networks. Modular networks or subnetworks have previously been investigated regarding statistics and connection clusters (Filan et al., 2020). Csord\u00e1s et al. (2020) proposes a technical training approach for subnetworks in terms of functional modular, a subnetwork highly correlated to a designed task. For example, Csord\u00e1s et al. (2020) investigates whether there exists a subnetwork responsible purely for one class. Using the same technique, Zhang et al. (2021) utilizes the functional modular to solve out-of-distribution generalization by probing subnetworks serving different functional subtasks from a pre-trained ERM."}, {"title": "3 Problem Setting and Example", "content": ""}, {"title": "3.1 Problem Setting", "content": "Assume a dataset D consisting of data samples belonging to groups g\u2208 G and that the groups have different levels of biases correlated to an input feature of x and the label of y, i.e., a spurious correlation (Sagawa et al., 2020b). We set each group g\u2208 G to be defined by the combination of the label y and a corresponding bias attribute A. Hence, g follows G = A \u00d7 Y, and we say that there exists a spurious correlation between A and Y.\nA model trained with ERM on the class labels might be able to separate the classes well but rely on spurious features to do so, resulting in a mismatch between the intended and model-learned solution (Geirhos et al., 2020). Spurious features typically appear in the majority of the dataset. Thus, by using these features, an ERM model can minimize the training loss for the expected average population but still show high errors over the minority group, which lacks these spurious features. Furthermore, in cases where the spurious features"}, {"title": "3.2 Motivating Example", "content": "To illustrate the problem and motivate our approach, we revisit a simple 2-D classification task with spurious features (Pezeshki et al., 2021). The two moons dataset D consists of input pairs (x,y), where x = (x1, x2) \u2208 R\u00b2 and y \u2208 {0,1} (cf. Fig. 1). The dataset contains the artificial direct relation between x1 and the corresponding label y.\nWe train three different models on this dataset, showing the effect of i) smaller networks and ii) our contrastive loss formulation (Sec. 5.2.1). A simple feedforward network (5 ReLu layers, 500 hidden units per layer, 100 epochs, cross-entropy loss) can separate the two classes very well (Fig. 1, left). The decision boundary is nearly linear because the cross-entropy loss optimizes for class separation and the model can rely on the correlation between the x-axis values of x and y. We train the same network architecture with random masks (50% of weights in each layer) and use the same fraction of random weights for testing, i.e., we randomly prune the network to 50% of its weights. We observe curved decision boundaries (Fig. 1, center), suggesting that a smaller network within a dense network is less influenced by spurious correlations. Combining masked training and our contrastive loss (Sec. 5.2.1, we obtain a curved decision boundary with a large margin (Fig. 1, right), suggesting that the model uses information from both, the x and y coordinate.\nImplications. The above example strengthens two hypotheses for mitigating spurious correlations.\nFirst, capacity seems to have an influence. Both, the dense network and the smaller subnetwork with randomly pruned connections perform well on the dataset, but the pruned network has better decision boundaries, i.e., depends less on the spurious feature. However, that does not suggest to train a small architecture from scratch as larger models have a higher ability to learn information and improve performance beyond the bias-variance regime (Nakkiran et al., 2021). Instead, it suggests to first train a large network with large enough capacity to encode all relevant information and then, post-training, extract a subnetwork from it that does not rely on spurious correlation.\nThe post-training approach also matches our realistic scenario that the existence of spurious correlations becomes evident only after training (and possibly later in the lifetime of a model). Thus, the extraction of a subnetwork without spurious correlations is less expensive than retraining the whole model from scratch. This observation serves as our main motivation.\nSecond, our contrastive learning scheme improves decision boundaries by learning more information from training data than just the simplicity bias features. Our batches of contrastive instances serve as guidance for models to know which feature relations should be less focused on."}, {"title": "4 Background: Subnetwork Extraction", "content": "In this section, we outline the approach for the extraction of subnetworks (Csord\u00e1s et al., 2020). Generally, we aim to extract a sparsely connected subnetwork from a dense network with a binary mask that indicates which weights to include (the subnetwork) and which to discard.\nMasking models. Given a trained neural network f, for layer I in L hidden layers of a neural net- work f, we introduce a learnable parameter \u03a0 = (\u03c0\u00b9, \u03c0\u00b2, ..., \u03c0\u3134) corresponding to each layer's weights W = (w1,w2, ..., wL). Each element of \u03c0\u00b9 is assumed to be an independent Bernoulli random variable. Each element wj,k performs as a logit which indicates the probability of keeping the corresponding weight wk. Weight wjk connects the jth-neuron from (1 \u2013 1)th-layer to kth-neuron from 1th-layer, we write each weight and its corresponding logit in short wi and \u03c0i. During training, network parameters in hidden layers become (w\u00b9 0 \u03c0\u00b9, w\u00b2 \u2299 \u03c02, ..., wL \u2299 \u03c0L).\nMask training. To train the binary mask, we freeze the model weights W = (w\u00b9, w\u00b2, ..., wL) and train only the added parameters \u03a0 = (\u03c01, \u03c02, ..., \u03c0L) with each \u03c0\u2081 equal to 0.9 initially, i.e., high keep probability. Modular loss Lmod (Csord\u00e1s et al., 2020) aims to extract a set of sparse weights (the subnetwork) that retain the performance of the original densen netwrok on the classification task.\nLmod = LCE + \u03b1 \u03a3\u03c0\u03ad\u03c2\nwhere Lce is cross-entropy loss, \u03a3\u03af\u03c0\u03af sparse regularization which keeps a logit \u03c0\u2081 small unless it is needed for the task and a is responsible for the strength of the sparse regularization.\nThe Gumbel-Sigmoid trick (Jang et al., 2017) is applied during training to the logit\nSi = \u03c3((\u03c0\u03af \u2013 log(log U\u2081/log U2)/T) with U1, U2 ~ U(0, 1),\nwhere is the temperature and o(x) is the sigmoid function. We obtain the binary mask by\nmi = [1si>y - Si]stop + Si\nwhere threshold y is 0.5 by default, 1 is the indicator function and [.] stop is the stop gradient operator.\nThe final parameters of the resulting subnetwork are defined as W\u2032 = (w\u00b9 \u2299 m\u00b9, w\u00b2 \u2299 m\u00b2, ..., wL \u2299 mL)."}, {"title": "5 Our Approach", "content": "We hypothesize that instances with the same spurious attribute are nearby in representation space, and that spurious features induce clusters (cf. illustration in Fig.3 A). We verify this hypothesis for multiple attributes on the CelebA dataset (Sec. 5.1).\nOur approach centers around two key ideas: First, we identify clusters in the latent space and move instances with the same spurious attributes away from each other and samples of the same class closer to each other. Second, we reduce the representation capacity of the network to a subnetwork that is less prone to spurious correlations.\nMore specifically, we learn a task-oriented subnetwork (Sec. 5.2) with a task-specific contrastive loss (Sec. 5.2.1) that optimizes the representation space. We detail the specific data selection and sampling procedure for negative and positive samples in the contrastive loss in Sec. 5.2.2. Finally, we present the overall training procedure in Sec. 5.3."}, {"title": "5.1 Analysis of ERM representation space for multipe spurious features", "content": "In the presence of spurious features, ERM tends to learn these as predictive features yielding decision boundaries aligned with the spurious feature (as exemplified on the blue moons data set in Fig. 1). We"}, {"title": "5.2 Task-oriented Subnetwork", "content": "To ensure that spurious features do not define data manifolds and cannot be used as discriminative features, we introduce a task-oriented subnetwork, and a contrastive loss that optimizes the representation space. More"}, {"title": "5.2.1 Contrastive Learning", "content": "We use a variant of contrastive loss (Khosla et al., 2020) to mitigate the tendency of ERM to form clusters based on spurious attributes. Different from the original work, our contrastive batch is not only defined by class labels, but also by the location of samples in representation space. Fig. 3 outlines the general idea.\nFor each contrastive batch we randomly sample a set of anchors and for each anchor a corresponding set of positive and negative samples. We define positive samples as having the same class as the anchor, but from a different cluster than the anchor. We define negative samples as samples from the same cluster (independent of their class label). For anchor (x, y) belonging to cluster c CC, the positive set of size P is {(x+,y+) | (x+, y+) \u2208 C \\ c > y+ = y} and the negative set of size N is {(x\u00af,y\u00ae) | (x\u00ae, y\u00af) \u2208 c \\ (x, y)}.\nThe contrastive loss term defined with respect to the anchor (x, y) is\nLcon(x) = - \\sum_{i=1}^P  log  \\frac{exp(z^Tz^+/\u03c4)}{\\sum_p exp(z^Tz/\u03c4) + \\sum_n exp(z^Tz^-_n/\u03c4)}"}, {"title": "5.2.2 Data", "content": "PruSC does not require prior knowledge of spurious features and is applicable in the presence of multiple spurious features. We do not assume that group annotations are available but automatically infer groups to construct the de-biasing dataset Dtask. Based on the assumption that the representation space of ERM has regions corresponding to spurious attributes (cf. Sec. 5.1, and Fig. 2), we use unsupervised clustering to construct the groups for our dataset Dtask.\nGiven an ERM model, we calculate the embedding of the training data D and apply k-means clustering on the embedding. Each sample in the training set D is assigned to a cluster c, (x, y, c) \u2208 D = X \u00d7 Y \u00d7 \u0421, with input domain X, class labels y, and clusters C. We define C = {cj|j \u2208 I} with the cluster index set j \u2208 I = {1, 2, ..., n}.\nWe differentiate clusters based on their class purity. We call a cluster c i-dominant for class i, if at least 90% of its samples belong to class i, i.e., a cluster is class-dominant, if \u2203i \u2208 Y such that |{(x,y,c)\u2208c|y=i}| > 0.9 then c is i-dominant. Otherwise, c is neutral. A cluster can be either neutral or i-dominant for some i \u2208 Y and multiple different clusters may be i-dominant for the same class i. C\u00b2 is the union of i-dominant clusters (of the same class i) and CN is the union of neutral clusters\nC\u00b2 = \u222a Cj\nj\u2208Ii\nCN = \u222a Cj,\nj\u2208IN\nwith I CI being the set index of i-dominant clusters and IN CI being the set index of neutral clusters.\nLet mi be the minority set of samples with label i assigned to clusters dominated by a different class\nm\u2081 = {(x, y, cj) | j \u2209 (I\u00bf \u222a In) \u2227 y = i}\nTo construct a class-balanced dataset Dtask for subnetwork training, we determine a fixed number p of samples for each class. We draw p samples for class i as follows: First, we take all samples from the minority set mi. Second, for each cluster in CUCN, we sample a number of p-mi IN samples having class label i. We obtain a subset of the training data Dtask which is class-balanced with p samples for each class."}, {"title": "5.3 Overall Training Procedure", "content": "Given a trained ERM neural network f on dataset D, we perform the following steps sequentially.\nRepresentation clustering and sub-sampling. First, we extract the representations learned by the ERM model as the output of the last hidden layer of f: femb. We apply k-means on femb(x) with a pre-defined k. We determine cluster labels (i-dominant and neutral) and construct the subnetwork mask-training dataset Dtask as described in Sec. 5.2.2. In our experiments, we set p, the number of samples for each class such that Dtask typically contains 10% of the samples of D.\nBinary mask training. We freeze all weights W of f and train only the mask II with the mask-training dataset Dtask (cf. Sec. 4). To construct contrastive batches, we randomly sample multiple anchors x from distinct cluster together with their corresponding positive x+ and negative x\u00af. We accumulate the per-batch constrastive loss by summing over Lcon of all anchors in the batch. The mask I = (\u03c01,\u03c02, ..., \u03c0\u3134) is updated through the final loss term\nL = LCE + a \u03a3\u03c0\u03af + \u03b2Lcon,\ni"}, {"title": "6 Experiments", "content": "We evaluate the effectiveness of PruSC on three datasets (CelebA, WaterBird, and ISIC) in comparison to state-of-the-art methods (Sec. 6.3). We further examine the impact of PruSC in the presence of multiple potential spurious correlations (Sec. 6.4) and conduct an ablation study for the main components of our approach (Sec. 6.5)."}, {"title": "6.1 Datasets", "content": "CelebA. Following prior work, our classification target is hair color (blond or non-blond hair) on the CelebA dataset (Liu et al., 2015), where the potential spurious attribute is gender (male or female). Majority groups are blond females and non-blond males with proportions of 44% and 41% of the data. Minority groups are non-blond females (14%) and blond males (1%). We extend the common setup to a multi-spuriouss dataset by incorporating additional attributes (no beard, young, heavy makeup, wearing lipstick, pale skin) which also pose potential spurious correlations based on their heavily imbalanced distributions.\nWaterBird. WaterBird was introduced by Sagawa et al. (2020a) as a standard spurious correlation benchmark. This artificially constructed dataset pastes bird segmentations from the CUB dataset (Wah et al., 2011) onto backgrounds form the Places dataset (Zhou et al., 2018). The birds labeled as either waterbirds or landbirds are placed on backgrounds of either water or land. The task is to classify bird types: y = {waterbird, landbird} and pasting 95% waterbirds on a water background and 95% landbirds on a land background results in spurious correlations in the default training set since models tend to rely on the background instead of actual bird characteristics. Accordingly, the spurious attributes are A = {water background, land background}.\nISIC. ISIC Skin is a real-world medical dataset provided by the International Skin Imaging Collaboration ISIC (Codella et al., 2019). The objective is to distinguish benign (non-cancerous) cases from malignant (cancerous) cases. Using the source code of Rieger et al. (2020), we retrieved 20, 394 images for the two classes benign (17,881) and malignant (2,513) from the ISIC Archive\u00b2. Nearly half of the benign cases (8,349) have colored patches attached to patients' skin, whereas no malignant case contains such patches. In this realistic dataset, a the group \"malignant with patches\" is missing from the training set. To create this missing group during the validation procedure, we programmatically add colored patches to a subset of malignant cases outside the area of the lesion, following Nauta et al. (2021)."}, {"title": "6.2 Baselines", "content": "Empirical Risk Minimization (ERM) represents conventional training without any procedures for improving worst-group accuracies.\nWe compare PruSC with two approaches that require group annotations: GroupDRO (Sagawa et al., 2020a) needs knowledge about spurious attributes and group annotations to train from scratch, while DFR (Kirichenko et al., 2023) requires a group-balanced subset for its retraining stage. To meet this requirement in the ISIC dataset, we create artificial samples for the missing group and use those samples during the training of"}, {"title": "6.3 Predictive Performance", "content": "Our main results on predictive performance are shown in Tab. 2. PruSC shows the best performance in terms of worst-group accuracy (WGA) among methods that do not require any group annotations and shows comparable performance in most cases (CelebA and ISIC) to state-of-the-art methods that rely on annotations."}, {"title": "6.4 Robustness to Multiple Spurious Correlations", "content": "In this section, we analyze how PruSC performs in the presence of multiple spurious correlations within a dataset. Since we do not explicitly use information about the spurious attribute during subnetwork training, we expect the representations to be agnostic to any spurious feature. We use the CelebA dataset which has a potential set of spurious attributes A (Seo et al., 2022) (for details see Appendix A.3). Specifically, we investigate six potential spurious attributes ai \u2208 A = {male, presence of beard, heavy makeup, wearing lipstick, young, pale skin}. For each attribute ai, we create a group set G = {(blond, a\u2081), (non-blond, ai), (blond, non-ai), (non-blond, non-a\u017c)}. We train an ERM model to predict hair color and evaluate performance separately for each group in G. We then train all methods for correcting spurious attributes. GroupDRO and DFR are trained on balanced groups for the attribute male.3"}, {"title": "6.4.1 Evaluation Metrics", "content": "We report worst-group accuracy (WGA) for each spurious attribute ai evaluated on groups in G. WGA is the standard metric to evaluate whether models use spurious correlations. A significant drop between average accuracy and worst-group accuracy is a strong indicator of relying on spurious correlations. To assess the impact on the group with the fewest training samples (usually the group with a spurious attribute uncorrelated to a target label), we report minority-group accuracy (MGA) for each spurious attribute ai. To assess group fairness, i.e., whether all groups have similar prediction error, we measure the unbiased accuracy gap (UAG). Unbiased accuracy (UA) is the accuracy on a balanced test set, sampled such that there is no correlation between the attribute a\u017c and a target label. UGA is the difference of the overall (official) test set accuracy and UA. A small UAG indicates that groups are treated equally, regardless of their distribution in the dataset."}, {"title": "6.4.2 Results", "content": "The ERM models usually fail for the minority group represented by target and uncorrelated attributes. For example, for the attribute gender = male, the ERM model only has 50.2% accuracy for the group (blond hair, male) since in the training set there are only 1,102 samples in the group (blond hair, male) while the group (non-blond hair, male) contains 53,483 samples. In all of these cases, the accuracy for the minority group is the worst-group accuracy evaluated by the baseline ERM model."}, {"title": "6.5 Ablation study", "content": "Tab. 5 shows an overview of our ablations: performing the pruning and fine-tuning steps, with and without our contrastive loss, respectively (ConLoss Pr, ConLoss Ft).\nContrastive Loss. Settings 1 and 2 demonstrate the impact the contrastive loss during fine-tuning (ConLoss Ft) when training a subnetwork. Similarly, settings 5 and 6 show the influence of contrastive loss while not pruning but only fine-tuning the whole network with Dtask. In both cases, using the contrastive loss in fine-tuning downgrades the performance. Setting 3 reveals that without contrastive loss in the training mask (ConLoss Pr), the WGA drops significantly (89.6% from setting 1 to 41.8% in setting 3). It shows that our final combination (only using contrastive loss during pruning) is the best combination and surpasses other combinations by large margins. In other words, our defined contrastive loss is most effective when applied to the mask training instead of directly to the model weight training. The contrastive loss forces the pruning process to retain only the connections that contribute to an optimized representation, ensuring that all spurious attributes are distributed in the representation space. Furthermore, in the pruning stage, the backpropagation process only updates the masks, which later will be binarized by Gumbel-Sigmoid, instead of directly updating the model weights.\nFine-tuning. Without fine-tuning (setting (4)) the performance drops significantly. Without fine-tuning the model on the dataset Dtask, the subnetwork's performance is similar to that before pruning. Note, that we only need to finetune 2 - 5 epochs. This finding matches with previous work on modular subnetworks (Csord\u00e1s et al., 2020; Zhang et al., 2021; Park et al., 2023) that require retraining or fine-tuning the remaining network after pruning to achieve better performance.\nSubnetwork. The three ablation experiments (settings (5) to (7)) answer the question of whether one can improve the worst-group accuracy only by fine-tuning and getting advantages from the design of a class-balanced subset. We either fine-tune the whole network (setting (5) and (6)), or only one last layer (setting (7)) for same number of epochs. The latter setting is equivalent to DFRclass, with Dtask. We observe no significant improvement when fine-tuning the whole network compared to fine-tuning only the last layer (setting 6 vs. 7), fine-tuning the whole network even results in worse WGA. As noted before, using the contrastive loss during fine-tuning again drops the group-worst accuracy."}, {"title": "6.6 Variant of contrastive learning.", "content": "We study the effectiveness of our choices for contrastive batch learning in PruSC. Results reported in Tab. 6 shows that other ablations of contrastive batch are less effective than our choice in terms of improvement worst-group accuracy. Recall our default setting: sampling a random an- chor, positives are defined by samples taking from different clusters and having the same class la- bel with the anchor, while negatives are defined by samples from the same cluster with the anchor."}, {"title": "6.7 The impact of pruning ratio.", "content": "Fig. 4 shows the average and worst-group accuracy evaluated on the CelebA dataset with respect to different pruning ratios. We consider the set up of classifying blond hair and spurious feature is male or not-male. While the average accuracy does not change much as long as the pruning ratio is under 70%, the worst-group accuracy varies more signifi- cantly. The smaller gap between the average and worst-group accuracy indicates that the models are less prone to spurious correlations. In the CelebA dataset, the most effective pruning ratio is around 40% to 50%. When pruning beyond 70% of the parameters, the models seem to fail to predict accu- rately, as both the worst-group and average accuracy drop."}, {"title": "7 Feature visualization", "content": "In this section, we discuss how PruSC affects the representation space at the end. We show in the previous part that ERM tends to learn and cluster samples based on spurious attributes during training (Fig. 2). In Fig. 5 (left), we visualize the embedding space of the non-blond hair class in the test set of the same ERM model, which reveals again ERM learns based on spurious features. Despite achieving high average test accuracy (90%), the fact that ERM learns to use spurious features for predicting leads to low worst-group accuracy (49.7%) and lack of generalization ability.\nDCWP mitigates spurious correlations by extracting a subnetwork by training a contrastive loss, and sampling positives and negatives from bias-conflict (ERM misclassified) and bias-align (ERM correctly classified) cases. From the predictive result, DCWP successfully improves the worst-group accuracy (73%), however, the model still tends to group samples by female attribute (Fig. 5, center). We hypothesize that defining bias-conflict based on ERM misclassified cases is not reliable. When a spurious attribute (female) is easy to learn and strongly correlated with a class (non-blond hair), the model easily learns and predicts correctly based on the spurious feature, even in the early stage of training.\nPruSC eliminates this reliance on spurious feature learning. While average accuracy remains high (91%) through cross-entropy loss, the use of contrastive learning within clusters makes the learning spurious process"}, {"title": "8 Conclusion", "content": "In this paper, we presented PruSC to extract a spurious-free subnetwork from a fully trained neural network. We do not require annotations of spurious features, butinstead, optimize the representation space to uncorrelated spurious attributes and class labels by forcing examples of the same class which is in different clusters (presumably due to different spurious attributes) to become more similar. PruSC achieves competitive worst-group accuracy across several benchmarks without requiring annotations of the spurious feature(s). We find that this not only reduces annotation overhead and makes the method applicable in cases where spurious attributes are not known beforehand, but also results in models that are robust to multiple spurious correlations.\nNotably, we extract the subnetwork from a fully trained network without extensively retraining the weights. This shows that even when models are heavily influenced by spurious correlations, there exists a smaller part of the network that learns the invariant features relevant to the task. This finding is a promising direction for future research.\nOne limitation of our pruning method is that it fails to explicitly explain the exact roles of the pruned connections. It is possible that the pruned connections encode some information essential for the classification task but not defined during pruning. This potential missing information could be crucial for effectively learning the WaterBird dataset, as discussed in Sec.6.3."}, {"title": "A Appendix / supplemental material", "content": ""}, {"title": "A.1 Datasets", "content": "Celeba. Tab. 7 shows the distribution of group G = {(blond, ai), (non-blond, ai), (blond, non-ai), (non-blond, non-a)} in training dataset and testing dataset. This shows the imbalanced distribution of the group (target, correlated attribute) and (target, un-correlated attribute). The models can both depend on the easier to learn features and the dominance of training data to suffer from spurious correlations. Generally, the minority group (group having smallest portion in training set) is the group having worst accuracy in test (with respect to baseline ERM)."}, {"title": "A.2 Task-oriented Subnetworks", "content": "Csord\u00e1s et al. (2020) aims to highlight sets of non-shared weights solely responsible for individual class k by training a mask with class k removed from the training dataset (Ctask = D \\ {(x,y|y = k}) and need not add any other loss term. The weights solely responsible for this class will be absent from the resulting mask. Results show that the performance of the target class drops significantly, while only a small drop in performance is observed for non-target classes. This indicates a heavy reliance on class-exclusive, non-shared weights in the feature detectors. Interestingly, Csord\u00e1s et al. (2020) also shows the mis-classification behaviors based on \"shared features\" (or possibly spurious features), e.g., when \"airplane\" is removed, images are often misclassified as \"birds\" or \"ships,\" both of which commonly feature a blue background.\nZhang et al. (2021) investigates the capability of finding a subnetwork for a specific task within a trained dense network by extracting it using a specially designed held-out dataset tailored for that task. Specifically, the authors train a dense network on the Colored MNIST dataset, which contains a high spurious correlation between digits and colors, with each digit being strongly correlated with a particular color. Using the same technique as Csord\u00e1s et al. (2020), but while pruning, Zhang et al. (2021) trains the network with a dataset balanced in terms of colors and digits to isolate subnetworks that solely predict either digits or colors. Surprisingly, even though the trained dense network suffers significantly from spurious correlations, it is still possible to extract a subnetwork that predicts only the digits with high accuracy, without extensive retraining on a balanced (or non-spurious) dataset.\nPark et al. (2023) constructs the mask training data by upweighting the complex cases (called by bias-conflict samples) defined by the misclassified cases of the ERM model, aiming to extract a set of weights more robust to those cases therefore, eliminating bias in the network."}, {"title": "A.3 Multiple spurious attributes", "content": "CelebA - Dataset contains multiple spurious attributes. We analyze 7 chosen attributes having annotations from dataset, namely: male, no beard, heavy makeup, wearing lipstick, young, pale skin, big nose. Treating each attribute independently, we show their potential of being spurious attribute by comparing the worst-group accuracy from set set G = {(blond, ai), (non-blond, ai), (blond, non-a\u2081), (non-blond, non-ai)} for each attribute ai and the ERM average accuracy. A fair model should treat all groups equally and minimize the influence of distribution. As shown in Tab. 9 (cf. ERM), we see some gaps between worst-group and average accuracy, and at the same time the unequal accuracy among groups.\nModel performance across multiple spurious attributes. Tab. 9 shows more details on the accuracies of each group and compares among more methods; namely ERM, DCWP, GroupDRO, DFR, and Ours. Interestingly, when considering the group having the worst performance by ERM, our method outperforms"}, {"title": "A.4 Feature Visualization", "content": ""}]}