{"title": "UNETVL: ENHANCING 3D MEDICAL IMAGE SEGMENTATION WITH CHEBYSHEV KAN POWERED VISION-LSTM", "authors": ["Xuhui Guo", "Tanmoy Dam", "Rohan Dhamdhere", "Gourav Modanwal", "Anant Madabhushi"], "abstract": "3D medical image segmentation has progressed considerably due to Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), yet these methods struggle to balance long-range dependency acquisition with computational efficiency. To address this challenge, we propose UNETVL (U-Net Vision-LSTM), a novel architecture that leverages recent advancements in temporal information processing. UNETVL incorporates Vision-LSTM (ViL) for improved scalability and memory functions, alongside an efficient Chebyshev Kolmogorov-Arnold Networks (KAN) to handle complex and long-range dependency patterns more effectively. We validated our method on the ACDC and AMOS2022 (post challenge Task 2) benchmark datasets, showing a significant improvement in mean Dice score compared to recent state-of-the-art approaches, especially over its predecessor, UNETR, with increases of 7.3% on ACDC and 15.6% on AMOS, respectively. Extensive ablation studies were conducted to demonstrate the impact of each component in UNETVL, providing a comprehensive understanding of its architecture. Our code is available at https://github.com/tgrex6/UNETVL, facilitating further research and applications in this domain.", "sections": [{"title": "1. INTRODUCTION", "content": "The incorporation of artificial intelligence (AI) into medical image process has profoundly revolutionized patient care, providing a more efficient and accessible approach to improving diagnostic precision and prognostic outcomes. Accurate segmentation, in particular, acts as a fundamental step in biomedical image analysis, delineating key anatomical structures or regions of interest to facilitate interpretation by clinicians and researchers [1]. Researchers have thoroughly investigated numerous deep learning-based segmentation techniques following the emergence of a new age in AI, attributed to advancements in graphical processing units (GPUs). In the past decade, Convolutional Neural Networks (CNNs), particularly U-shaped encoder-decoder architectures [2], have been fundamental to the most advanced techniques in this field. Nonetheless, although CNNs are proficient at extracting local characteristics, they have difficulties in capturing long-range dependencies due to the static nature of the receptive fields in convolutional layers [1].\nIn recent years, Vision Transformers (ViTs) have facilitated dynamic learning of relationships between distant regions in input data by employing self-attention mechanisms, thereby being able to address the challenge of modeling spatial dependencies. A notable example is the UNETR model [1], which leverages this advantage by incorporating ViT layers as the feature extractor for the encoder. This approach has demonstrated superior efficacy across several 3D medical image segmentation tasks compared to CNN-based methods. However, ViTs face significant computational challenges, particularly in high-resolution tasks, due to their quadratic complexity relative to input size. This limitation poses a considerable barrier in medical imaging applications, where high-resolution three-dimensional volumes are typically the standard. The computational burden of ViTs becomes pronounced especially when processing such large, detailed medical images.\nIn the latest development, the original team behind LSTM introduced XLSTM [3], a significant improvement that incorporates new gating mechanisms and memory structures to enhance both performance and scalability when dealing with large-scale data. Vision-LSTM (ViL) [4], built on xLSTM, consisting of stacked mLSTM blocks, which follow the bidirectional processing mechanism, is able to efficiently capture both contextual and spatial information, making it highly suited for complex image segmentation tasks, where preserving spatial relationships is essential.\nIn this work, we propose to replace the ViT layers in the UNETR model with ViL layers, introducing what we call UNET VIL (UNETVL) architecture. In addition, we modify the up and down projection layer in ViL and leverage the stronger nonlinear representation capabilities of the Kolmogorov-Arnold Network (KAN) [5] compared to Multi-Layer Perceptrons (MLP) to help capture subtle details in medical images. We validate the effectiveness of our method"}, {"title": "2. METHODOLOGY", "content": "We present the overall architecture of our UNETVL model in Fig. 1. Building upon the UNETR structure [1], UNETVL employs an encoder-decoder network that efficiently captures both local features and long-range contexts. The model processes a 3D oversampled instance with resolution (H, W, D) and C input channels, applying preprocessing and data augmentation steps from the nnUNet framework[10]. The input is divided into non-overlapping patches of shape $(P, P, P)$, which are linearly projected into N = $(\\frac{H}{P} \\times \\frac{W}{P} \\times \\frac{D}{P})$ patch tokens. Learnable positional embeddings are added to maintain spatial information. These tokens are sequentially processed in a K-dimensional embedding space through multiple Vision-LSTM (ViL) block pairs, generating intermediate feature representations at various scales (e.g., Z3, Z6, Z9, Z12), each with shape (N, K). The embedding dimension K remains constant throughout the network depth. Finally, these extracted latent representations are passed to a CNN-based decoder via skip connections, which merges the multi-scale features to produce the final segmentation output. This architecture combines the strengths of Vision Transformers for capturing global context with the efficiency of CNNs for local feature processing, resulting in a powerful model for 3D medical image segmentation tasks."}, {"title": "2.2. ViL Block", "content": "The ViL blocks in our model comprise alternating mLSTM layers that process patch tokens bidirectionally: forward (from top-left to bottom-right) and backward (from bottom-right to top-left). At the core of these blocks lies the mLSTM architecture, which enhances memory capabilities and spatial information retrieval by replacing the scalar memory cell with a d x d matrix. This modification allows the model to capture more complex data relationships and patterns within a single time step. The mLSTM design draws inspiration from the Query-Key-Value (QKV) attention mechanism in transformers but diverges in its computational approach. This adaptation enables parallelization of LSTM operations, significantly improving computational efficiency and allowing the model to scale more effectively to large datasets."}, {"title": "2.3. Chebyshev KAN", "content": "As a promising alternative to MLP, Kolmogorov-Arnold Network (KAN) [5] represents a variant of the traditional feed-forward neural network with a distinctive twist: its activation functions are learnable and are moved from the nodes to the edges. Chebyshev KAN [9] is an innovative framework that combines KAN with the efficient and intuitive Chebyshev polynomials to enhance the performance of the original B-spline approach. The core of this framework are Chebyshev polynomials, a class of orthogonal polynomials defined on the interval [-1, 1], which are well-suited for function approximation. The Chebyshev KAN layer, built on these polynomials, offers a novel alternative to original B-splines, addressing their limitations in both performance and ease of use. Integrating Chebyshev KAN into existing projects is straightforward, requiring only a single line of code to implement. Our modified ViL block incorporates Chebyshev KAN transformations to help improve the flexibility and adaptability of the architecture to approximate high-dimensional feature representation capabilities and we have conducted ablation experiment to prove the feasibility.\nFor implementation in our up and down projection layer, we just pass the feature embedding of shape (N, K) through the Chebyshev KAN layer, where the Chebyshev polynomials $T \\in \\mathbb{R}^{N \\times K \\times (degree+1)}$ is computed recursively as $T_m(x)$ are defined as $T_0(x) = 1$, $T_1(x) = x$, and $T_m(x) = 2xT_{m-1}(x) - T_{m-2}(x)$ for $m > 2$. A tensor of learnable Chebyshev coefficients $C \\in \\mathbb{R}^{K \\times output-dim \\times (degree+1)}$ is introduced, serving as trainable parameters. The layer's output $y \\in \\mathbb{R}^{N \\times output\\_dim}$ is computed using Einstein summation as\n$\\y_{no} = \\sum_{i=1}^{K} \\sum_{j=0}^{degree} T_{nij} \\cdot C_{ioj}$\nwhere n, i, o, and j index the patch, input dimensions, output dimensions, and polynomial degree, respectively."}, {"title": "3. EXPERIMENTS", "content": "We evaluate the effectiveness of our approach on two public 3D segmentation benchmarks, both involving multiple structures or multiple organs: the Automated Cardiac"}, {"title": "4. RESULTS", "content": "As shown in Table 1, when comparing the performance of the ViL architecture with Chebyshev KAN to the UNETR baseline, a significant improvement is observed. On the ACDC dataset, the Dice score improves by approximately 7.3% (from 85.34% in UNETR to 91.59% in UNETVL with KAN). Similarly, for the AMOS dataset, the Dice score increases by about 15.6% (from 76.59% in UNETR to 88.57% in UNETVL with KAN). Though UNETVL's results are not the highest when compared with other listed SOTA models on ACDC dataset, its strength lies in addressing over-segmentation issue, which is common in complex anatomical areas. The integration of Chebyshev KAN allows ViL to capture finer details, leading to more precise boundaries and cleaner segmentation. Notably, this approach achieves the highest mean Dice score on the AMOS 2022 post-challenge Task 2 dataset, demonstrating its capability in handling complex medical images with real-world clinical settings.\nTo further dissect the contributions of various components of the proposed UNETVL architecture, we conducted ablation studies (refer to Table 2). The models with and without KAN, and with varying latent dimensions, were evaluated. As expected, the use of KAN consistently improves performance, and increasing the latent dimension results in better segmentation, albeit at the cost of higher parameter counts.\nTo rigorously assess the performance of various univariate functions in the KAN layer, we conducted experiments within a lightweight CNN-based U-KAN model [16]. Using 5-fold cross-"}, {"title": "5. DISCUSSION AND CONCLUSION", "content": "The proposed UNETVL architecture, enhanced by Chebyshev KAN, demonstrates competitive performance in 3D medical image segmentation tasks. While the Dice score may not be the absolute highest, the model significantly mitigates over-segmentation, capturing finer anatomical details and providing cleaner segmentations. This capability is particularly beneficial in clinical applications where precise delineation of complex structures is critical for diagnostic and treatment planning. The Chebyshev KAN, compared to other univariate functions, excels in handling complex structures, offering a flexible and robust alternative for MLP on medical image segmentation tasks. However, the increased complexity of the Chebyshev KAN layer adds to the computational burden, particularly when scaling to larger datasets or higher-resolution images. Future work may explore optimizations to mitigate these costs."}, {"title": "6. COMPLIANCE WITH ETHICAL STANDARDS", "content": "This research study was conducted retrospectively using human subject data made available in open access by Bernard et al. [6] and Ji et al. [7]. Ethical approval was not required as confirmed by the license attached with the open access data."}]}