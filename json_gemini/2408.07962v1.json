{"title": "Meta SAC-Lag: Towards Deployable Safe Reinforcement Learning via MetaGradient-based Hyperparameter Tuning", "authors": ["Homayoun Honari", "Amir M. Soufi Enayati", "Mehran Ghafarian Tamizi", "Homayoun Najjaran"], "abstract": "Safe Reinforcement Learning (Safe RL) is one of the prevalently studied subcategories of trial-and-error-based methods with the intention to be deployed on real-world systems. In safe RL, the goal is to maximize reward performance while minimizing constraints, often achieved by setting bounds on constraint functions and utilizing the Lagrangian method. However, deploying Lagrangian-based safe RL in real-world scenarios is challenging due to the necessity of threshold fine-tuning, as imprecise adjustments may lead to suboptimal policy convergence. To mitigate this challenge, we propose a unified Lagrangian-based model-free architecture called Meta Soft Actor-Critic Lagrangian (Meta SAC-Lag). Meta SAC-Lag uses meta-gradient optimization to automatically update the safety-related hyperparameters. The proposed method is designed to address safe exploration and threshold adjustment with minimal hyperparameter tuning requirement. In our pipeline, the inner parameters are updated through the conventional formulation and the hyperparameters are adjusted using the meta-objectives which are defined based on the updated parameters. Our results show that the agent can reliably adjust the safety performance due to the relatively fast convergence rate of the safety threshold. We evaluate the performance of Meta SAC-Lag in five simulated environments against Lagrangian baselines, and the results demonstrate its capability to create synergy between parameters, yielding better or competitive results. Furthermore, we conduct a real-world experiment involving a robotic arm tasked with pouring coffee into a cup without spillage. Meta SAC-Lag is successfully trained to execute the task, while minimizing effort constraints. The success of Meta SAC-Lag in performing the experiment is intended to be a step toward practical deployment of safe RL algorithms to learn the control process of safety-critical real-world systems without explicit engineering.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement Learning (RL) is one of the most important paradigms for learning to control physical systems. However, a major shortcoming of RL is its need for exploration and extensive trial and error. For that reason, while we observe its wide success in various domains such as Energy systems [1], Video games [2], and Robotics [3], the real-world deployment of these algorithms to learn the control process poses is challenging [4] since the exploration process might lead the system to states that might damage the system and incur heavy costs to the user. To this end, safe RL methods aim to address this issue by optimizing the policy such that it is compliant with the constraints. The constraints are defined such that they aim to prevent the system from exceeding its physical limitations.\nThe most common approach in safe RL is through the Lagrangian method. Specified under the Constrained Markov Decision Process (CMDP) framework, through defining thresholds for the constraints, the multi-objective optimization problem is converted to constraint satisfaction and is solved by casting it to an unconstrained problem using the Lagrangian method. While the approach has been extensively studied in the literature [5], [6], without precise tuning and engineering of the constraint thresholds, the Lagrangian methods will suffer from convergence to suboptimal policies. For that reason, the real-world use of these algorithms is rendered to be challenging due to the iterative process of hyperparameter tuning.\nTo address these challenges, based on the Soft Actor-Critic (SAC) architecture [7], our algorithm aims to address two fundamental problems: safe exploration and tuning-free constraint adjustment. Previous attempts to automate the tuning of exploration-related hyperparameter of SAC have been mostly focused on optimizing the performance of the system [8], [9]. However, addressing the safety compliance of SAC has been limited. To this end, we propose a threshold-free safety-aware exploration optimization pipeline. In addition, our approach optimizes the safety threshold according to the overall performance of the policy. We are able to update the aforementioned hyperparameters using the metagradients w.r.t. to the meta-objectives which are computed based on the gradients of the internal learnable parameters. Finally, to assess the performance of Meta SAC-Lag, as depicted in Fig. 1, we study its performance against several baseline algorithms in five simulated robotic tasks with four different application themes. We observe that our method attains better or comparable results in terms of safety or reward performance while automatically tuning the safety-related hyperparameters. Furthermore, we present a safety benchmark test case, called Pour Coffee, which attempts to relocate and pour a coffee-filled mug into another cup. Constraint violation happens in case of collision or the spillage of the coffee. We deploy and train Meta SAC-Lag in a real-world setup using Kinova Gen3 robot. Our implementation shows that not only is Meta SAC-Lag capable of safe deployment without the iterative process of hyperparameter tuning but also, the learning process of the policy results in a smooth and jerk-free execution of the task with minimum effort imposed on the system.\nOur main contributions can be summarized as:\n\u2022\n\u2022 We propose a Lagrangian-based safe RL method able to automatically adjust the constraint bounds.\nMeta SAC-Lag addresses safe exploration through an unconstrained metagradient-based optimization pipeline.\n\u2022 We validate the applicability of Meta SAC-Lag in five simulated robotic environments against baseline algorithms.\n\u2022 A test environment, called Pour Coffee, is presented, and, with minimal prior safety-related hyperparameter tuning, Meta SAC-Lag is trained on a real-world Kinova Gen3 setup. The algorithm successfully achieves the task objective with minimized effort exerted on the robot."}, {"title": "II. RELATED WORK", "content": "The constrained Markov decision process (CMDP), is the theoretical building block of safe RL. CMDPs have been widely studied in the RL paradigm [10], [11] and are solved using Lagrangian methods [12]. In this regard, Shen et. al [13] devised the risk-sensitive policy optimization (RSPO) algorithm which sequentially decreases the Lagrangian multiplier to zero. Furthermore, Stooke et. al [14] updates the multiplier using PID control. Additionally, Reward-constrained policy optimization (RCPO) employs dual gradient descent optimization for the policy and Lagrange multiplier [15].\nIn another aspect, metagradient optimization has been explored thoroughly in RL hyperparameter tuning. Initially, model-agnostic meta-learning (MAML) [16] introduced meta-optimization of initial weights to enable fast task adaptation within a few gradient descent steps. In a different approach, Meta-Gradient RL [17] extended the concept to learn the hyperparameters of return functions online. This paradigm offered a general approach, applicable to other RL hyperparameters. Subsequently, similar techniques were applied for auto-tuning other RL hyperparameters, such as exploration thresholds [18], entropy temperature in SAC [9], auxiliary tasks and sub-goals [19], and differentiable hyperparameters of loss functions [20]. Despite these advancements, metagradient methods have not been extensively explored in constrained RL paradigms [5], with few applications focusing on ensuring safety in sensitive learning tasks, as seen in the work by Calian et al. [21]. The authors utilized meta-gradients to update the Lagrange multiplier learning rate in an off-policy RL framework.\nThe Lagrangian methods are not the sole approach taken toward solving safe RL. Thananjeyan et al. [22] trained a recovery policy in parallel to the task policy and used it whenever the task policy chose actions deemed too risky. More prominently, model-based RL and safety guarantees for risk aversion during training are proposed. Koppejan et al. [23] used the neuroevolutionary approach to exploiting domain expertise to learn safe models for model-based RL. Thomas et al. [24], in a different approach, used near-future imagination to plan safe trajectories ahead of time. Moldovan et al. [25] focused on risk aversion in MDPs using near-optimal Chernoff bounds. Lyapunov functions have also been used to guarantee safety during training [26], [27], though constructing Lyapunov functions remains a challenge due to their typically hand-crafted nature and the absence of clear principles for agent safety and performance optimization.\nIn summary, RL agent evaluation relies on human-provided rewards, but the risk of misstated human objectives is often overlooked. Also, despite significant progress in safe RL methodologies, there remains a big gap in readily deployable agents in industrial contexts. Moreover, a critical balance exists between reward and cost in RL, as each action can impact both aspects, creating a multi-dimensional problem. These challenges hinder robust and dependable RL algorithms suitable for real-world implementation. Our motivation for this work is to use metagradient optimization for self-tuning of the safety threshold. This will minimize the need for safety-related hyperparameter tuning in safe RL while improving performance. Ultimately, the self-tuning safety threshold will enable us to deploy the agent directly in the real world."}, {"title": "III. BACKGROUND", "content": "In this section, we investigate the background by exploring essential preliminary concepts that serve as the foundation for this paper. We start with the discussion of the CMDP framework. Furthermore, we delve into the formulation of the safety critic and SAC.\nCMDP comprises the tuple $< S, A, P, r, c, \\gamma_r, \\gamma_c, \\rho_0 >$ where S denotes the state space, A represents the action"}, {"title": "A. Constrained Markov Decision Process (CMDP)", "content": "space, and $r$ denotes the reward function: $r :S\\times A\\times S\\leftrightarrow \\mathbb{R}$. The transition function $P : S \\times A \\times S \\leftrightarrow [0,1]$ defines the likelihood $P(s'|s, a)$ of moving from state $s$ to $s'$ by executing action $a$. The probability distribution function $\\rho_0$: $S\\rightarrow [0, 1]$ denotes initial state distribution of the framework. Furthermore, $c(s)$ is the constraint indicator function which determines whether state $s$ violates the constraint functions specified by C: $c(s) = \\mathbb{1}[C(s)==1]$. Parameters $\\gamma_r \\in [0,1)$ and $\\gamma_c \\in [0,1)$ serve as discount factors for reward and safety critics, respectively. Ultimately, the solution to CMDP is represented by the policy $\\pi : S \\times A \\rightarrow [0,1]$ which is the probability distribution over actions. The value function associated with policy $\\pi$ for a specific state-action pair $(s, a)$ and the corresponding recursive equation, known as the Bellman equation, can be formulated as follows:\n$\\begin{aligned} &Q^{\\pi}(s, a) = \\mathbb{E}_{s_t\\sim P, a_t\\sim \\pi} [\\sum_{t=0}^{\\infty} \\gamma_r^t r(s_t, a_t)| s_0 = s, a_0 = a] \\\\ & = \\mathbb{E}_{s'}[r(s, a) + \\gamma_r V^{\\pi}(s')]\\end{aligned}$\n(1)\nAdditionally, the primary function of the safety critic is to estimate the probability of a policy failure occurring in the future, determined by the expected cumulative discounted probability of failure.\n$\\begin{aligned} &Q_c^{\\pi}(s, a) = \\mathbb{E}_{s_t\\sim P, a_t\\sim \\pi} [c(s) + (1 - c(s))\\sum_{t=1}^{\\infty}[\\gamma_c^t c(s_t)]] \\\\ & = Pr[c(s)==1] + \\gamma_c\\mathbb{E}_{s'\\sim p} [(1 - c(s))V_c^{\\pi}(s')]\\end{aligned}$\n(2)\nFinally, the main objective of an RL algorithm in a CMDP framework is to find a policy to maximize expected return while satisfying the constraints starting from the initial state $s_0$:\n$\\begin{aligned} \\pi^* = \\underset{\\pi \\in \\Pi}{\\operatorname{argmax}} \\; T^{\\pi} = \\underset{\\pi \\in \\Pi}{\\operatorname{argmax}} \\; \\mathbb{E}_{s_0\\sim \\rho_0} [\\sum_{t=0}^{\\infty} \\gamma_r^t r_t] \\\\ s.t. \\; T_c^{\\pi} = \\mathbb{E}_{s_0\\sim \\rho_0} [\\sum_{t=0}^{\\infty} \\gamma_c^t c_t] \\leq \\epsilon\\end{aligned}$\n(3)"}, {"title": "B. Soft Actor Critic (SAC)", "content": "SAC [7] optimizes a stochastic policy in an off-policy manner, utilizing two neural networks: one for estimating Q-function (critic) and another for policy updates (actor). A key feature of SAC is entropy regularization, where the policy aims to strike a balance between maximizing expected return and maximizing entropy. This balance mirrors the exploration-exploitation trade-off; higher entropy encourages greater exploration, potentially accelerating learning and prevent convergence to suboptimal solutions.\nConsidering $w_r$ and $\\phi$ as parameters representing the critic and actor networks, respectively, training these networks involves sampling a batch of samples from the replay buffer. $w_r$ is updated by taking the gradient through the mean squared error (MSE) loss between the critic output and the target value:\n$J_{w_r} = \\mathbb{E}_{(s,a,r)\\sim D}[(Q_{w_r}(s, a) \u2013 Q^{tar}(s, a))^2]$,\n(4)\nwhere $Q^{tar}$ is calculated as:\n$Q^{tar}(s, a) = \\mathbb{E}_{s'\\sim P(s,a)}[r(s, a) + \\gamma_r(Q_r(s', a') \u2013 \\alpha log(\\pi_{\\phi}(a'|s')))]$\n(5)\n$a' \\sim \\pi_{\\phi}$\nFurthermore, the policy $\\pi_{\\phi}$ is optimized by taking the gradient through the critic and the expected entropy of the policy:\n$J_{\\pi_{\\phi}} = \\mathbb{E}_{s\\sim D} [\\alpha log(\\pi_{\\phi}(a|s)) \u2013 Q_{w_r}(s, a)]$\n(6)\n$a \\sim \\pi_{\\phi}$\nFinally, it is important to note that the safety critic ($Q_{w_c}$) defined in Section III-A is trained using the same loss formulation in Eq. 4, without the entropy term."}, {"title": "IV. METHOD", "content": "In this section the process of metagradient optimization of the safety threshold $\\epsilon$ and entropy temperature $\\alpha$ is discussed.\nMetagradient optimization is the process with which we can optimize the hyperparameters that are not a part of the main loss function. Fundamentally, these meta-parameters\u00b9 dictate the dynamics of the system and direct it toward a certain behavior. In the context of metagradient reinforcement learning [17], in abstract terms, the learnable system variables are parameterized as $\\theta$. These parameters are updated to $\\theta'$ by following the rule:\n$\\theta' = \\theta + f(\\mathcal{T}, \\theta, \\eta, B)$\n(7)\nwhere $\\eta$ is the list of hyperparameters, $B$ a mini-batch of experience, and $f$ the gradient of the objective function $\\mathcal{T}$ w.r.t. $\\theta$. Furthermore, the optimization process of the meta-parameters $\\eta$ can be formulated based on the updated parameter $\\theta'$:\n$\\eta' = \\eta + \\beta_{\\eta} \\frac{\\partial \\mathcal{T}'(\\theta', \\eta, B')}{\\partial \\eta} = \\eta + \\beta_{\\eta} \\frac{\\partial \\mathcal{T}'(\\theta', \\eta, B')}{\\partial \\theta'} \\frac{\\partial \\theta'}{\\partial \\eta}$\n(8)\n\u00b9In this paper, we use hyperparameters and meta-parameters terms interchangeably."}, {"title": "A. Metagradient Optimization", "content": "where $\\mathcal{T}'$ is the meta-objective used for the optimization of the meta-parameters, $\\beta_{\\eta}$ the learning rate associated with $\\eta$, and $B'$ a resampled mini-batch validation set similar to the cross-validation method in the meta-optimization literature [28]. Finally, $\\frac{\\partial \\theta'}{\\partial \\eta}$ can be calculated as:\n$\\frac{\\partial \\theta'}{\\partial \\eta} = (I + \\frac{\\partial f(\\mathcal{T}, \\theta, \\eta, B)}{\\partial \\theta}) \\frac{\\partial \\theta}{\\partial \\eta} + \\frac{\\partial f(\\mathcal{T}, \\theta, \\eta, B)}{\\partial \\eta}$\n(9)"}, {"title": "B. SAC-Lagrangian", "content": "In the Lagrangian version of the SAC we aim to optimize the policy based on its reward objective such that it is compliant with the safety objective:\n$\\begin{aligned} \\pi^* = \\underset{\\pi_{\\phi} \\in \\Pi}{\\operatorname{max}} \\; \\mathcal{T}^{\\pi_{\\phi}} \\\\ s.t. \\; \\mathcal{I}^{\\pi_{\\phi}} = \\mathbb{E}_{s\\sim D} [Q_{w_c}(s,a)] \\leq \\epsilon \\\\ a \\sim \\pi_{\\phi} \\end{aligned}$\n(10)\nNaturally, multiple constraints can be defined for the policy to consider all of them. However, in this paper, in order to keep the formulation simple and general, we consider a single constraint signal that is the result of the superposition of all the constraint functions. In this paper, in contrast to [7], we refrain from considering $\\alpha$ as an additional constraint and aim to optimize it through metagradient optimization.\nFurthermore, the optimization process of policy in Eq. 10 is formulated by casting it as a Lagrangian loss and back-propagating through the loss:\n$\\begin{aligned} \\underset{\\pi_{\\phi} \\in \\Pi}{\\operatorname{min}} \\underset{\\nu \\geq 0}{\\operatorname{max}} \\; \\mathcal{L}(\\pi_{\\phi}, \\nu, \\epsilon, \\alpha) &= \\mathcal{T} \u2013 \\nu(\\mathcal{I}^{\\pi_{\\phi}} \u2013 \\epsilon) \\\\ &= \\mathbb{E}_{s\\sim D} [Q_{w_r}(s, a) \u2013 \\alpha log \\pi_{\\phi}(a|s) \u2013 \\nu(Q_{w_c}(s,a) \u2013 \\epsilon)] \\\\ a \\sim \\pi_{\\phi} \\end{aligned}$\n(11)\nwhere $\\nu$ is the Lagrange multiplier."}, {"title": "C. Meta SAC-Lag", "content": "Following the conventional notation in the context of gradient-based hyperparameter optimization [29], we split the parameters into inner and outer parameters. Rather than a one-shot optimization as in Eq. 7, we propose a sequential updating approach. We define and update the inner parameters as:\n$\\Theta^{inner} = \\left[\\begin{array}{c} \\nu \\\\ \\phi \\end{array}\\right] \\leftarrow \\left[\\begin{array}{c} \\nu \\\\ \\phi \\end{array}\\right] + \\left[\\begin{array}{c} -\\beta_{\\nu} \\nabla_{\\nu} \\mathcal{L}(\\pi_{\\phi}, \\nu, \\epsilon, \\alpha) \\\\ \\beta_{\\phi} \\nabla_{\\phi} \\mathcal{L}(\\pi_{\\phi}, \\nu', \\epsilon, \\alpha) \\end{array}\\right]$\n(12)\nFurthermore, in the same sequential manner, we first update $\\epsilon$ and then $\\alpha$:\n$\\Theta^{outer} = \\left[\\begin{array}{c} \\epsilon \\\\ \\alpha \\end{array}\\right] \\leftarrow \\left[\\begin{array}{c} \\epsilon \\\\ \\alpha \\end{array}\\right] + \\left[\\begin{array}{c} \\beta_{\\epsilon} \\nabla_{\\epsilon} \\mathcal{J}_{\\epsilon}(\\pi_{\\phi'}) \\\\ \\beta_{\\alpha} \\nabla_{\\alpha} \\mathcal{J}_{\\alpha}(\\pi_{\\phi'}, \\nu', \\epsilon') \\end{array}\\right]$\n(13)\nwhere $\\mathcal{J}_{\\epsilon}$ and $\\mathcal{J}_{\\alpha}$ correspond to the objective functions of $\\epsilon$ and $\\alpha$, respectively. To this end, we intended to design the objective function for $\\epsilon$ solely based on the performance of the resultant policy. Our intuition behind the aforementioned design stems from the idea that the threshold should be adjusted such that it improves the performance of the agent as a whole. For that purpose, the $\\epsilon$ objective function is proposed as:\n$\\mathcal{I}_{\\epsilon}(\\pi_{\\phi'}) = \\mathbb{E}_{s\\sim D} [V^{copy}Q_{w_c}(s, a) \u2013 Q_{w_r}(s, a)]$\n(14)\n$a \\sim \\pi_{\\phi'}$\n$\\begin{aligned} \\mathcal{I}_{\\alpha}(\\pi_{\\phi'}, \\nu', \\epsilon') = \\underset{0<a<1}{max} \\mathbb{E}_{s_0 \\sim \\rho_0} [Q_{w_{\\phi}}(s_0, a) \u2013 \\nu' (Q_{w_c}(s_0, a) \u2013 \\epsilon')] \\end{aligned}$\n(15)\n$a_{det}$\nwhere $a_{det}$ indicates the deterministic action value output by the policy. Basically, we use the expectation of the Lagrangian formulation evaluated in the initial states encoun-"}, {"title": "Algorithm 1 Meta SAC-Lag", "content": "Require:\nInitialize Policy network $\\phi^0$, Exploration rate $\\alpha$\nCritic network $w_r^0$, $w_r^{'0}$, Safety critic network $w_c^0$, $w_c^{'0}$\nLagrangian values $\\epsilon^0$, $\\nu^0$\nLearning rates $\\beta_{\\phi}$, $\\beta_{\\epsilon}$, $\\beta_{\\nu}$, $\\beta_{\\alpha}$\n1: Create Transition buffer $D$, Safety buffer $D_s$, and Initial state buffer $D_0$\n2: Randomly sample initial state $s_0 \\sim \\rho_0$ and fill $D_0$\n3: for $e=1, \\dots$ do\n4: Reset environment $s_0 \\sim \\rho_0 = env.reset()$\n5: for $t=0, \\dots, T-1$ do\n6: Sample action $a_t \\sim \\pi_{\\phi}$\n7: $s_{t+1}, r_t, c_t \\leftarrow env.step(a_t)$\n8: if $c_t==1$ then\n9: $D_s \\leftarrow D_s \\cup (s_t, a_t, c_t, s_{t+1})$\n10: else\n11: $D \\leftarrow D \\cup (s_t, a_t, r_t, c_t, s_{t+1})$\n12: Train $w_{c_1}, w_{c_2}$ on $D \\cup D_s$ (Eq. 2)\n13: Sample a batch of transitions $B= \\{(s, a, r, c, s')\\} \\in D$\n14: Train $w_{r_1}, w_{r_2}$ using $B$ (Eq. 4)\n15: $\\nu' \\leftarrow \\nu - \\beta_{\\nu} \\nabla_{\\nu} \\mathcal{L}(\\pi_{\\phi}, \\nu, \\epsilon, \\alpha)$ using $B$ (Eq. 11)\n16: $\\phi' \\leftarrow \\phi + \\beta_{\\phi} \\nabla_{\\phi} \\mathcal{L}(\\pi_{\\phi}, \\nu', \\epsilon, \\alpha)$ using $B$ (Eq. 11)\n17: Resample $B' = \\{s \\in D\\}$\n18: $\\epsilon' \\leftarrow \\epsilon + \\beta_{\\epsilon} \\nabla_{\\epsilon} \\mathcal{I}_{\\epsilon}(\\pi_{\\phi'})$ using $B'$ (Eq. 14)\n19: $\\alpha' \\leftarrow \\alpha + \\beta_{\\alpha} \\nabla_{\\alpha} \\mathcal{I}_{\\alpha}(\\pi_{\\phi'}, \\nu', \\epsilon')$ using $D_0$ (Eq. 15)\n20: $\\nu \\leftarrow \\nu', \\phi \\leftarrow \\phi', \\epsilon \\leftarrow \\epsilon', \\alpha \\leftarrow \\alpha'$\n21: if $c_t==1$ then Break\nThe objective function $\\mathcal{I}$ is consistent with the objective function of the policy. This is evident by comparing Eq. 14 with the gradient w.r.t. the policy parameters $\\phi$ in Eq. 11. The objective function for $\\epsilon$ is designed to minimize the policy objective. This design stems from the idea that $\\epsilon$ aims to capture the worst-case performance of the policy $\\pi_{\\phi'}$. Hence, by being optimized in this way, the safety region of the policy can be correctly adjusted to reflect that. It is important to note that we use $V^{copy}$ to indicate that we merely use $\\nu'$ value in the objective function and not include its gradient w.r.t. $\\epsilon$. We observed better performance by the gradient detachment in our early experiments which may be due to the injection of bias in $\\nu'$ into its optimization process. Furthermore, to optimize the exploration value $\\alpha$, [9] used $Q_{w_{\\phi}}$ as the objective function to change the value based on the performance of the policy. Therefore, in order to make the exploration rate of the Meta SAC-Lag safety compliant, we propose the objective function of $\\alpha$ as:\n$\\begin{aligned} \\mathcal{I}_{\\alpha}(\\pi_{\\phi'}, \\nu', \\epsilon') = \\underset{0<a<1}{max} \\mathbb{E}_{s_0 \\sim \\rho_0} [Q_{w_{\\phi}}(s_0, a) \u2013 \\nu' (Q_{w_c}(s_0, a) \u2013 \\epsilon')] \\end{aligned}$\n(15)\n$a_{det}$\nwhere $a_{det}$ indicates the deterministic action value output by the policy. Basically, we use the expectation of the Lagrangian formulation evaluated in the initial states encoun-"}, {"title": "V. EXPERIMENTS", "content": "In this section, we evaluate the performance of Meta SAC-Lag. Specifically, our aim is to study two questions:\n\u2022\n\u2022\nHow much does the added autonomy affect the performance of the algorithm compared to the baseline methods?\nHow capable is Meta SAC-Lag to learn optimal performance in a real-world setup while avoiding actions that might catastrophically damage the system?\nIn order to study how the proposed algorithm will perform in safety-critical robotic scenarios, we use five simulated robotic environments with four different themes:\n\u2022\nLocomotion: In this theme, the purpose of control is to move the robotic system in the forward direction. The safety constraints are violated whenever the controller's actions make the system exceed its limits, e.g., the velocity is higher than a certain threshold or the robot is falling to the ground. For that purpose, we use the Mujoco-based [32] Humanoid-Velocity environment from the Safety Gymnasium codebase [33]. It is important to note that the safety-related reward shaping of this\nenvironment is removed to have a better understanding of the safety performance of the algorithms.\n\u2022\nObstacle Avoidance: In many real-world robotic applications, there are mobile robots with manipulation capabilities. An important constraint of these systems is achieving their goal while avoiding certain regions in their surroundings. We adopt Isaac Gym-based Freight-FrankaCloseDrawer [34]. In this setup, the robot attempts to get near a drawer and close it while avoiding a red region. In addition, we use Car-Circle2 task [33] where the objective is to steer a car in a circular motion while avoiding collision with two walls.\n\u2022\nManipulation: Another important area of safety-concerned robotic applications is manipulation. For that purpose, we use two embodied scenarios. For the robotic manipulation task we use Push Topple [35], [36] environment where the robotic arm must relocate a box without toppling it. Furthermore, in the dexterous manipulation scenario, we adopt the Egg Manipulate task where the agent must rotate an egg to a specific orientation without dropping it or exerting a force of more than 20 N. For both tasks, we use the Gymnasium Robotics codebase [37].\nIt is important to note that in the training process, we treat the constraints as hard constraints and terminate the episode whenever a violation has happened in the system. Furthermore, three baseline algorithms are chosen to compare and study the performance of Meta SAC-Lag:\nSACv2-Lag: The basic form of Meta SAC-Lag which uses Eq. 11 to optimize the policy and the Lagrangian multiplier with a fixed safety threshold.\nReward Constrained Policy Optimization (RCPO-SACv2): Optimizes the policy using the Q-function formulated as $\\mathbb{E}_{\\pi}[Q(s, a) = Q_r(s, a) \u2013 \\nu Q_c(s, a)]$. The dual variable $\\nu$ is also updated using Eq. 11.\nRCPO-MetaSAC: To show the effectiveness of our safe exploration technique, we use the $Q(s, a)$ formulation in RCPO and optimize $\\alpha$ using the approach proposed in [9].\nMeta SAC-Lag Ini: Inspired by [38], we experiment with a nonlinear objective function for $\\alpha$ specified as:\n$\\mathcal{J}_{nl}^{\\epsilon}(\\pi_{\\phi'}) = \\left\\{\\begin{array}{ll} \\mathbb{E}_{s \\sim D} [Q_{w_r}(s, a) \u2013 Q_{w_c}(s, a)] & \\text{if } Q_{w_c}(s, a) < 0\\\\ a \\sim \\pi_{\\phi'} \\\\ Q_{w_r}(s, a) (1 - Q_{w_c}(s, a)) & \\text{otherwise} \\end{array}\\right.$\n(16)\nEssentially, $\\mathcal{J}_{nl}$ can have the advantage of no reliance on external parameter values, as opposed to Eq. 14 which uses $\\nu'$ in the objective formulation.\nTo have a fair comparison, we tune the values of $\\epsilon$ and $\\nu$ for SACv2-Lag and RCPO-SACv2. Also, we use the values of RCPO-SACv2 for RCPO-MetaSAC. The values are outlined in Table I. For the value of $\\alpha$, SACv2 constrains the policy entropy as $\\mathbb{E}_{s \\sim D}[-\\log(\\pi(s_t, a_t))] > H$ and defines the $\\alpha$ loss as $\\underset{\\alpha>0}{min} \\mathcal{L}(\\alpha) = \\mathbb{E}_{s \\sim D}[\\alpha(log(\\pi(s_t, a_t)) + H)]$. The authors propose the formula $H = -dim(A)$ as their target entropy. Furthermore, two important initial hyperparameter"}, {"title": "B. Simulation Results", "content": "The simulation results are depicted in Fig. 3. To this end, we also report the return and the policy episodic violation rate in Table II. The violation rate is calculated as the average number of failures over a specific window of episodes. The results not only indicate that Meta SAC-Lag provides automated tuning of the safety-related hyperparameters but also, that the convergence process of the policy incurs lower constraint violations and yields higher or comparable returns. Furthermore, the update profile of $\\alpha$ shows that as training goes on, in most cases, Meta SAC-Lag updates $\\alpha$ to values lower than SACv2. This indicates that as the policy converges to a near-safe optimal solution, $\\alpha$ is rapidly decreased to favor exploitation and prevent further constraint violations. Moreover, we can observe similar $\\alpha$ profiles in Meta SAC-Lag and RCPO-SACv2 which can be attributed to $\\alpha$ being optimized using similar objective functions. In addition, the optimization process of $\\epsilon$ shows a generally fast convergence. The fast convergence of $\\epsilon$ provides the advantage of stable optimization as other values can updated based on the optimally achieved value of $\\epsilon$. Finally, regarding the comparison between Eq. 14 and Eq. 16 we observe consistently better performance of Eq. 14 in both aspects of return and safety. In summary, the optimization outcomes of Meta SAC-Lag demonstrate that the algorithm excels across a range of embodied control tasks, proficiently learning optimal solutions, while demanding minimal hyperparameter tuning."}, {"title": "C. Real-World Deployment", "content": "Deployability can be regarded as one of the most important obstacles in using RL for learning to control real-world systems [39]. Choosing unsafe actions might lead the system to states that might damage it catastrophically, if chosen repeatedly. Therefore, using the conventional safe RL algorithms hinders their deployability since they require intensive hyperparameter tuning. In line with our purpose of assessing the deployability of a safe RL method, we propose a simple, yet important, safe RL testbench. This task, which we call Pour Coffee, is the task of moving a coffee-filled mug from a home position to a specific location and pouring the coffee into another cup. The task is executed using a Kinova Gen3 robot and its digital twin is created in the PyBullet simulation environment [40]. We define the state space $S = \\mathcal{X}_{cup} \\cup \\Theta_{cup} \\cup \\mathcal{X}_{goal} \\cup \\Theta_{goal}$ where $\\mathcal{X} = \\{x, y, z\\}$ and $\\Theta = \\{\\varphi, \\theta, \\phi\\}$ refer to the Cartesian position and the Euler angles in the Tait-Bryan ZYX intrinsic convention, respectively. Furthermore, the action of the agent maps to the velocity of the end-effector: $A = \\{ \\dot{\\mathcal{X}}_{cup}, \\dot{\\Theta}_{cup}\\}$. Moreover, we hierarchically define the reward function for reaching and pouring the coffee based on the Euclidean distance between the cup and"}, {"title": "The goal", "content": "d = $||\\mathcal{X"}, {"mathcal{X}_{goal}||_2$": "r(s,a,s') = \\left\\{\\begin{array}{ll} r_1 d + r_2 ||\\dot{\\mathcal{X}}_{cup}|| + r_3 \u00b7 \\mathbb{1}[spillage"}, 10, -2, -0.05, -1, 5, 41]}