{"title": "Meta SAC-Lag: Towards Deployable Safe Reinforcement Learning via MetaGradient-based Hyperparameter Tuning", "authors": ["Homayoun Honari", "Amir M. Soufi Enayati", "Mehran Ghafarian Tamizi", "Homayoun Najjaran"], "abstract": "Safe Reinforcement Learning (Safe RL) is one of the prevalently studied subcategories of trial-and-error-based methods with the intention to be deployed on real-world systems. In safe RL, the goal is to maximize reward performance while minimizing constraints, often achieved by setting bounds on constraint functions and utilizing the Lagrangian method. However, deploying Lagrangian-based safe RL in real-world scenarios is challenging due to the necessity of threshold fine-tuning, as imprecise adjustments may lead to suboptimal policy convergence. To mitigate this challenge, we propose a unified Lagrangian-based model-free architecture called Meta Soft Actor-Critic Lagrangian (Meta SAC-Lag). Meta SAC-Lag uses meta-gradient optimization to automatically update the safety-related hyperparameters. The proposed method is designed to address safe exploration and threshold adjustment with minimal hyperparameter tuning requirement. In our pipeline, the inner parameters are updated through the conventional formulation and the hyperparameters are adjusted using the meta-objectives which are defined based on the updated parameters. Our results show that the agent can reliably adjust the safety performance due to the relatively fast convergence rate of the safety threshold. We evaluate the performance of Meta SAC-Lag in five simulated environments against Lagrangian baselines, and the results demonstrate its capability to create synergy between parameters, yielding better or competitive results. Furthermore, we conduct a real-world experiment involving a robotic arm tasked with pouring coffee into a cup without spillage. Meta SAC-Lag is successfully trained to execute the task, while minimizing effort constraints. The success of Meta SAC-Lag in performing the experiment is intended to be a step toward practical deployment of safe RL algorithms to learn the control process of safety-critical real-world systems without explicit engineering.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement Learning (RL) is one of the most important paradigms for learning to control physical systems. However, a major shortcoming of RL is its need for exploration and extensive trial and error. For that reason, while we observe its wide success in various domains such as Energy systems [1], Video games [2], and Robotics [3], the real-world deployment of these algorithms to learn the control process poses is challenging [4] since the exploration process might lead the system to states that might damage the system and incur heavy costs to the user. To this end, safe RL methods aim to address this issue by optimizing the policy"}, {"title": "II. RELATED WORK", "content": "The constrained Markov decision process (CMDP), is the theoretical building block of safe RL. CMDPs have been widely studied in the RL paradigm [10], [11] and are solved using Lagrangian methods [12]. In this regard, Shen et. al [13] devised the risk-sensitive policy optimization (RSPO) algorithm which sequentially decreases the Lagrangian multiplier to zero. Furthermore, Stooke et. al [14] updates the multiplier using PID control. Additionally, Reward-constrained policy optimization (RCPO) employs dual gradient descent optimization for the policy and Lagrange multiplier [15].\nIn another aspect, metagradient optimization has been explored thoroughly in RL hyperparameter tuning. Initially, model-agnostic meta-learning (MAML) [16] introduced meta-optimization of initial weights to enable fast task adaptation within a few gradient descent steps. In a different approach, Meta-Gradient RL [17] extended the concept to learn the hyperparameters of return functions online. This paradigm offered a general approach, applicable to other RL hyperparameters. Subsequently, similar techniques were applied for auto-tuning other RL hyperparameters, such as exploration thresholds [18], entropy temperature in SAC [9], auxiliary tasks and sub-goals [19], and differentiable hyperparameters of loss functions [20]. Despite these advancements, metagradient methods have not been extensively explored in constrained RL paradigms [5], with few applications focusing on ensuring safety in sensitive learning tasks, as seen in the work by Calian et al. [21]. The authors utilized meta-gradients to update the Lagrange multiplier learning rate in an off-policy RL framework.\nThe Lagrangian methods are not the sole approach taken toward solving safe RL. Thananjeyan et al. [22] trained a recovery policy in parallel to the task policy and used it whenever the task policy chose actions deemed too risky. More prominently, model-based RL and safety guarantees for risk aversion during training are proposed. Koppejan et al. [23] used the neuroevolutionary approach to exploiting domain expertise to learn safe models for model-based RL. Thomas et al. [24], in a different approach, used near-future imagination to plan safe trajectories ahead of time. Moldovan et al. [25] focused on risk aversion in MDPs using near-optimal Chernoff bounds. Lyapunov functions have also been used to guarantee safety during training [26], [27], though constructing Lyapunov functions remains a challenge due to their typically hand-crafted nature and the absence of clear principles for agent safety and performance optimization.\nIn summary, RL agent evaluation relies on human-provided rewards, but the risk of misstated human objectives is often overlooked. Also, despite significant progress in safe RL methodologies, there remains a big gap in readily deployable agents in industrial contexts. Moreover, a critical balance exists between reward and cost in RL, as each action can impact both aspects, creating a multi-dimensional problem. These challenges hinder robust and dependable RL algorithms suitable for real-world implementation. Our motivation for this work is to use metagradient optimization for self-tuning of the safety threshold. This will minimize the need for safety-related hyperparameter tuning in safe RL while improving performance. Ultimately, the self-tuning safety threshold will enable us to deploy the agent directly in the real world."}, {"title": "III. BACKGROUND", "content": "In this section, we investigate the background by exploring essential preliminary concepts that serve as the foundation for this paper. We start with the discussion of the CMDP framework. Furthermore, we delve into the formulation of the safety critic and SAC.\nCMDP comprises the tuple < S, A, P, P, r, r, c, c, \\gamma_r, \\gamma_r, \\gamma_c, P_o > where S denotes the state space, A represents the action"}, {"title": "A. Constrained Markov Decision Process (CMDP)", "content": "CMDP comprises the tuple < S, A, P, P, r, r, c, c, \\gamma_r, \\gamma_r, \\gamma_c, P_o > where S denotes the state space, A represents the action"}, {"title": "B. Soft Actor Critic (SAC)", "content": "SAC [7] optimizes a stochastic policy in an off-policy manner, utilizing two neural networks: one for estimating Q-function (critic) and another for policy updates (actor). A key feature of SAC is entropy regularization, where the policy aims to strike a balance between maximizing expected return and maximizing entropy. This balance mirrors the exploration-exploitation trade-off; higher entropy encourages greater exploration, potentially accelerating learning and prevent convergence to suboptimal solutions.\nConsidering \\omega_r and \\phi as parameters representing the critic and actor networks, respectively, training these networks involves sampling a batch of samples from the replay buffer. \\omega_r is updated by taking the gradient through the mean squared error (MSE) loss between the critic output and the target value:\n$J_{\\omega_r} = E_{(s,a,r)~D}[(Q_{\\omega_r}(s, a) \u2013 Q^{tar}(s,a))^2]$,\nwhere $Q^{tar}$ is calculated as:\n$Q^{tar}(s, a) = E_{s'~P(s,a)} [r(s, a) + \\gamma_r(Q_{\\omega_r}(s', a') \u2013 \\alpha log(\\pi_{\\phi}(a'|s')))]$\nFurthermore, the policy $\\pi_{\\phi}$ is optimized by taking the gradient through the critic and the expected entropy of the policy:\n$J_{\\phi} = E_{s~D} [\\alpha log(\\pi_{\\phi}(a|s)) \u2013 Q_{\\omega_r}(s, a)]$\nFinally, it is important to note that the safety critic ($Q_{\\omega_c}$) defined in Section III-A is trained using the same loss formulation in Eq. 4, without the entropy term."}, {"title": "IV. METHOD", "content": "In this section the process of metagradient optimization of the safety threshold $\\epsilon$ and entropy temperature $\\alpha$ is discussed."}, {"title": "A. Metagradient Optimization", "content": "Metagradient optimization is the process with which we can optimize the hyperparameters that are not a part of the main loss function. Fundamentally, these meta-parameters\u00b9 dictate the dynamics of the system and direct it toward a certain behavior. In the context of metagradient reinforcement learning [17], in abstract terms, the learnable system variables are parameterized as $\\theta$. These parameters are updated to $\\theta'$ by following the rule:\n$\\theta' = \\theta + f(T, \\theta, \\eta, B)$\nwhere $\\eta$ is the list of hyperparameters, B a mini-batch of experience, and f the gradient of the objective function I w.r.t. $\\theta$. Furthermore, the optimization process of the meta-parameters $\\eta$ can be formulated based on the updated parameter $\\theta'$:\n$\\eta' = \\eta + \\beta_{\\eta} \\frac{\\partial T'(\\theta', \\eta, B')}{\\partial \\eta} = \\eta + \\beta_{\\eta} \\frac{\\partial J'(\\theta', \\eta, B')}{\\partial \\theta'} \\frac{\\partial \\theta'}{\\partial \\eta}$"}, {"title": "B. SAC-Lagrangian", "content": "In the Lagrangian version of the SAC we aim to optimize the policy based on its reward objective such that it is compliant with the safety objective:\n$\\pi^{*} = \\underset{\\pi_{\\phi}\\in \\Pi}{\\text{argmax }} J_{\\pi} = \\underset{\\pi_{\\phi}\\in \\Pi}{\\text{argmax }} E_{s\\sim D} [Q_{\\omega_r}(s, a) \u2013 \\alpha \\log \\pi_{\\phi}(s, a)]$\ns.t. $J_{\\pi}^{S} = E_{s\\sim D} [Q_{\\omega_c}(s,a)] \\leq \\epsilon$\nNaturally, multiple constraints can be defined for the policy to consider all of them. However, in this paper, in order to keep the formulation simple and general, we consider a single constraint signal that is the result of the superposition of all the constraint functions. In this paper, in contrast to [7], we refrain from considering $\\alpha$ as an additional constraint and aim to optimize it through metagradient optimization.\nFurthermore, the optimization process of policy in Eq. 10 is formulated by casting it as a Lagrangian loss and back-propagating through the loss:\n$\\underset{\\pi_{\\phi}\\in \\Pi}{min}\\ \\underset{\\nu\\geq0}{max}\\  \\mathcal{L}(\\pi_{\\phi},\\nu,\\epsilon,\\alpha) = T \u2013 \\nu(T^{S} \u2212 \\epsilon)$\n$= E_{s\\sim D} [Q_{\\omega_r}(s, a) \u2013 \\alpha log \\pi_{\\phi}(s, a) \u2013 \\nu(Q_{\\omega_c}(s,a) \u2013 \\epsilon)]$ \nwhere $\\nu$ is the Lagrange multiplier."}, {"title": "C. Meta SAC-Lag", "content": "Following the conventional notation in the context of gradient-based hyperparameter optimization [29], we split the parameters into inner and outer parameters. Rather than a one-shot optimization as in Eq. 7, we propose a sequential updating approach. We define and update the inner parameters as:\n$\\theta_{inner}^{'} = \\theta + \\beta_{\\theta}[-\\nabla_{\\nu}\\mathcal{L}(\\pi_{\\phi}, \\nu, \\epsilon, \\alpha)]\\atop [\\nabla_{\\phi}\\mathcal{L}(\\pi_{\\phi}, \\nu^{'}, \\epsilon, \\alpha)]$\nFurthermore, in the same sequential manner, we first update $\\epsilon$ and then $\\alpha$:\n$\\theta_{outer}^{'} = \\theta + \\beta_{\\theta}[\\nabla_{\\epsilon}\\mathcal{J_{\\epsilon}}(\\pi_{\\phi^{'}})]\\atop [\\nabla_{\\alpha}\\mathcal{J_{\\alpha}}(\\pi_{\\phi^{'}}, \\nu^{'}, \\epsilon^{'})]$\nwhere $J_{\\epsilon}$ and $J_{\\alpha}$ correspond to the objective functions of $\\epsilon$ and $\\alpha$, respectively. To this end, we intended to design the objective function for $\\epsilon$ solely based on the performance of the resultant policy. Our intuition behind the aforementioned design stems from the idea that the threshold should be adjusted such that it improves the performance of the agent as a whole. For that purpose, the $\\epsilon$ objective function is proposed as:\n$\\mathcal{J_{\\epsilon}}(\\pi_{\\phi^{'}}) = E_{s\\sim D} [V_{copy}Q_{\\omega_c}(s, a) \u2013 Q_{\\omega_r}(s, a)]$\nTo have a fair comparison, we tune the values of $\\epsilon$ and $\\nu$ for\nFinally, regarding\n$\\mathcal{T_{\\alpha}}(\\pi_{\\phi^{'}}, \\nu^{'}, \\epsilon^{'}) = \\underset{0<a<1}{max} E_{s_0 \\sim p_0} [Q_{\\omega_{\\phi}}(s_0, a) \u2013 \\nu^{'}(Q_{\\omega_c}(s_0, a) \u2013 \\epsilon^{'})]$\nwhere $a_{det}$ indicates the deterministic action value output by the policy. Basically, we use the expectation of the Lagrangian formulation evaluated in the initial states encoun-\nAlgorithm 1 Meta SAC-Lag\nRequire:\nInitialize Policy network $\\phi^{0}$, Exploration rate $\\alpha^{0}$\nCritic network $\\omega_{r}^{0}$, $\\omega_{r}^{0}$, Safety critic network $\\omega_{c}^{0}$, $\\omega_{c}^{0}$\nLagrangian values $\\epsilon^{0}$, $\\nu^{0}$\nLearning rates $\\beta_{\\phi}, \\beta_{\\epsilon}, \\beta_{\\nu}, \\beta_{\\alpha}$\n1: Create Transition buffer $\\mathcal{D}$, Safety buffer $\\mathcal{D_s}$, and Initial\nstate buffer $\\mathcal{D_0}$\n2: Randomly sample initial state $s_0 \\sim p_0$ and fill $\\mathcal{D_0}$\n3: for e = 1, ... do\n4: Reset environment $s_0 \\sim p_0 = env.reset()$\n5: for t = 0, ..., T \u2013 1 do\n6: Sample action $a_t \\sim \\pi_{\\phi}$\n7: $s_{t+1}, r_t, c_t \\leftarrow env.step(a_t)$\n8: if $c_t == 1$ then\n9: $\\mathcal{D_s} \\leftarrow \\mathcal{D_s} \\cup (s_t, a_t, c_t, s_{t+1})$\n10: else\n11: $\\mathcal{D} \\leftarrow \\mathcal{D} \\cup (s_t, a_t, r_t, c_t, s_{t+1})$\n12: Train $\\omega_{c_1}, \\omega_{c_2}$ on $\\mathcal{D}\\cup\\mathcal{D_s}$ (Eq. 2)\n13: Sample a batch of transitions B =\n14: $\\{(s, a, r, c, s^{'}) \\} \\in \\mathcal{D}$\n15: Train $\\omega_{r_1}, \\omega_{r_2}$ using B (Eq. 4)\n16: $\\nu^{'} \\leftarrow \\nu \u2013 \\beta_{\\nu}\\nabla_{\\nu}\\mathcal{L}(\\pi_{\\phi}, \\nu, \\epsilon, \\alpha)$ using B (Eq. 11)\n17: $\\phi^{'} \\leftarrow \\phi + \\beta_{\\phi}\\nabla_{\\phi}\\mathcal{L}(\\pi_{\\phi}, \\nu^{'}, \\epsilon, \\alpha)$ using B (Eq. 11)\n18: Resample B' = $\\{s \\in D\\}$\n19: $\\epsilon^{'} \\leftarrow \\epsilon + \\beta_{\\epsilon}\\nabla_{\\epsilon}\\mathcal{I_{\\epsilon}}(\\pi_{\\phi^{'}})$ using B' (Eq. 14)\n20: $\\alpha^{'} \\leftarrow \\alpha + \\beta_{\\alpha}\\nabla_{\\alpha}\\mathcal{I_{\\alpha}}(\\pi_{\\phi^{'}}, \\nu^{'}, \\epsilon^{'})$ using $\\mathcal{D_0}$ (Eq. 15)\n21: $\\nu \\leftarrow \\nu^{'}, \\phi \\leftarrow \\phi^{'}, \\epsilon \\leftarrow \\epsilon^{'}, \\alpha \\leftarrow \\alpha^{\u2019}$\nif $c_t == 1$ then Break\nSA Cv2-Lag and RCPO-SA Cv2. Also, we use the values of RCPO-SA Cv2 for RCPO-MetaSA C. The values are outlined"}, {"title": "V. EXPERIMENTS", "content": "In this section, we evaluate the performance of Meta SAC-Lag. Specifically, our aim is to study two questions:\n\u2022\n\u2022How much does the added autonomy affect the performance of the algorithm compared to the baseline methods?\nHow capable is Meta SAC-Lag to learn optimal performance in a real-world setup while avoiding actions that might catastrophically damage the system?\nIn order to make comparisons\nSacrifice the"}, {"title": "A. Test Benchmarks and Baselines", "content": "In order to study how the proposed algorithm will perform in safety-critical robotic scenarios, we use five simulated robotic environments with four different themes:\n\u2022\nLocomotion: In this theme, the purpose of control is to move the robotic system in the forward direction. The safety constraints are violated whenever the controller's actions make the system exceed its limits, e.g., the ve-locity is higher than a certain threshold or the robot is falling to the ground. For that purpose, we use the Mujoco-based [32] Humanoid-Velocity environment from the Safety Gymnasium codebase [33]. It is important to note that the safety-related reward shaping of this"}, {"title": "B. Simulation Results", "content": "The simulation results are depicted in Fig. 3. To this end, we also report the return and the policy episodic violation rate in Table II. The violation rate is calculated as the average number of failures over a specific window of episodes. The results not only indicate that Meta SAC-Lag provides automated tuning of the safety-related hyperparameters but also, that the convergence process of the policy incurs lower constraint violations and yields higher or comparable returns. Furthermore, the update profile of $\\alpha$ shows that as training goes on, in most cases, Meta SAC-Lag updates $\\alpha$ to values lower than SA Cv2. This indicates that as the policy converges to a near-safe optimal solution, $\\alpha$ is rapidly decreased to favor exploitation and prevent further constraint violations. Moreover, we can observe similar $\\alpha$ profiles in Meta SAC-Lag and RCPO-SA Cv2 which can be attributed to $\\alpha$ being optimized using similar objective functions. In addition, the optimization process of $\\epsilon$ shows a generally"}, {"title": "C. Real-World Deployment", "content": "Deployability can be regarded as one of the most im-portant obstacles in using RL for learning to control real-world systems [39]. Choosing unsafe actions might lead the system to states that might damage it catastrophically, if chosen repeatedly. Therefore, using the conventional safe RL algorithms hinders their deployability since they require intensive hyperparameter tuning. In line with our purpose of assessing the deployability of a safe RL method, we propose a simple, yet important, safe RL testbench. This task, which we call Pour Coffee, is the task of moving a coffee-filled mug from a home position to a specific location and pouring the coffee into another cup. The task is executed using a Kinova Gen3 robot and its digital twin is created in the PyBullet simulation environment [40]. We define the state space $S = \\{X_{cup}\\cup O_{cup} \\cup X_{goal} \\cup O_{goal} \\}$ where $X = \\{x, y, z\\}$ and $O = \\{\\phi, \\theta, \\psi\\}$ refer to the Cartesian position and the Euler angles in the Tait-Bryan ZYX intrinsic convention, respectively. Furthermore, the action of the agent maps to the velocity of the end-effector:\n$A = \\{X_{cup}, Y_{cup}, O_{cup}, \\phi_{cup}\\}$. Moreover, we hierarchically define the reward function for reaching and pouring the coffee based on the Euclidean distance between the cup and\n\u2022\nIn order to"}, {"title": "VI. CONCLUSIONS", "content": "The paper focused on the problem of automatic hyper-parameter tuning in Lagrangian safe RL methods. A novel model-free architecture called Meta SAC-Lag was proposed which addressed two inherent problems: safe exploration and constraint bound tuning. To this end, through the use of metagradient optimization, the algorithm is capable of adjusting the safety-related hyperparameters with minimal initial tuning. Furthermore, we studied the performance of our algorithm in five simulated embodied applications with the themes of locomotion, obstacle avoidance, robotic manipulation, and dexterous manipulation. We observed that the synergy created between the parameters and the hyper-parameters results in comparable or better performance of the policy in terms of reward or safety. Additionally, we conducted an experiment in a real-world setup involving a practical coffee-pouring robotic environment without any explicit safety-related reward shaping. We deployed the algorithm on the Kinova Gen3 robot and showed that the"}, {"title": "APPENDIX", "content": "In this section, the gradients of the each component of the algorithm is derived. While the automated differentiation tools for deep learning such as PyTorch and TensorFlow provide automated gradient calculation of this algorithm, the gradient analysis of Meta SAC-Lag may provide insightful information.\nStep 1: In the beginning of optimization, the gradient of the Lagrangian multiplier v is calculated using the inner loss (for ease of presentation, the expected values and the source of s are dropped in this analysis):\n$\\nabla_{\\nu}J_{\\nu} = \\nabla_{\\nu}\\mathcal{L}(\\pi_{\\phi},\\nu,\\epsilon,\\alpha)$\n$= \\nabla_{\\nu} [Q_{\\omega_r}(s, \\pi_{\\phi}(s)) \u2013 \\nu(Q_{\\omega_c}(s, \\pi_{\\phi}(s)) \u2013 \\epsilon) \u2013 \\alpha log \\pi_{\\phi}(a|s)]$\n$= \u2212[Q_{\\omega_c}(s, \\pi_{\\phi}(s)) \u2013 \\epsilon]$\nHence, v is updated as:\n${\\nu}^{'} \\leftarrow \\nu \u2013 {\\beta_{\\nu}}\\nabla_{\\nu}J_{\\nu} = \\nu + {\\beta_{\\nu}} [Q_{\\omega_c}(s, \\pi_{\\phi}(s)) \u2013 \\epsilon]$\nStep 2: Following the Lagrangian multiplier, the gradient of the actor parameters w.r.t. the Lagrangian loss is calculated as:\n$\\nabla_{\\phi}J_{\\phi} = \\nabla_{\\phi}\\mathcal{L}(\\pi_{\\phi},\\nu^{'},\\epsilon,\\alpha)$\n$= \\nabla_{\\phi} [Q_{\\omega_r}(s, \\pi_{\\phi}(s)) \u2013 {\\nu^{'}}(Q_{\\omega_c}(s, \\pi_{\\phi}(s)) \u2013 \\epsilon) \u2013 {\\alpha} log \\pi_{\\phi}(a|s)]$\n$= \\nabla_{\\phi} [Q_{\\omega_r}(s, \\pi_{\\phi}(s)) \u2212 (\\nu + {\\beta_{\\nu}}(Q_{\\omega_c}(s, \\pi_{\\phi}(s)) \u2013 \\epsilon)(Q_{\\omega_c}(s, \\pi_{\\phi}(s)) \u2013 \\epsilon) \u2013 {\\alpha} log \\pi_{\\phi}(a|s)]$\n$= \\nabla_{\\phi}Q_{\\omega_r}(s, \\pi_{\\phi}(s)) \u2013 \\nabla_{\\phi}Q_{\\omega_c}(s, \\pi_{\\phi}(s)) [{2{\\beta_{\\nu}}}Q_{\\omega_c}(s, \\pi_{\\phi}(s)) \u2013 {2{\\beta_{\\nu}}}\\epsilon + \\nu] \u2013 {\\alpha}\\nabla_{\\phi} log \\pi_{\\phi}(a|s)$\nThe actor parameters are then updated as:\n${\\phi}^{'} \\leftarrow \\phi + {\\beta_{\\phi}}\\nabla_{\\phi}J_{\\phi}$\n$= \\phi + {\\beta_{\\phi}} [\\nabla_{\\phi}Q_{\\omega_r}(s, \\pi_{\\phi}(s)) \u2013 \\nabla_{\\phi}Q_{\\omega_c}(s, \\pi_{\\phi}(s)) [{2{\\beta_{\\nu}}}Q_{\\omega_c}(s, \\pi_{\\phi}(s)) \u2013 {2{\\beta_{\\nu}}}\\epsilon + \\nu]\\\\\\$\\nThe meta-gradient"}]}