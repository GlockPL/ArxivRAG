{"title": "RAC: Efficient LLM Factuality Correction with Retrieval Augmentation", "authors": ["Changmao Li", "Jeffrey Flanigan"], "abstract": "Large Language Models (LLMs) exhibit impressive results across a wide range of natural language processing (NLP) tasks, yet they can often produce factually incorrect outputs. This paper introduces a simple but effective low-latency post-correction method, Retrieval Augmented Correction (RAC), aimed at enhancing the factual performance of LLMs without requiring additional fine-tuning. Our method is general and can be used with any instruction-tuned LLM, and has greatly reduced latency compared to prior approaches. RAC decomposes the LLM's output into atomic facts and applies a fine-grained verification and correction process with retrieved content to verify and correct the LLM-generated output. Our extensive experiments show that RAC yields up to 30% improvements over state-of-the-art baselines across two popular factuality evaluation datasets, validating its efficacy and robustness in both with and without the integration of Retrieval-Augmented Generation (RAG) across different LLMs.", "sections": [{"title": "1 Introduction", "content": "Recently Large Language Models (LLMs) have markedly changed the world of natural language processing (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023; OpenAI, 2022, 2023). Although LLMs can achieve superior performance on many NLP tasks, hallucination is a known issue for LLMs (Rawte et al., 2023; Ji et al., 2023; Zhang et al., 2023; Ye et al., 2023a; Huang et al., 2023). In particular, factually incorrect content generated by LLMs can be explicitly harmful to the application of LLMs (Li et al., 2024), including providing incorrect medical suggestions or wrong information for educational purposes. Misinformation can cause unpredictable harm to humans when LLMs are broadly used. Enhancing LLMs with better factuality can improve LLMs performance (Lee et al., 2022) and be less harmful to users.\nTo alleviate this factuality problem, previous research has investigated incorporating retrieved knowledge from a collection of documents into the LLM's context. This technique is called retrieval augmented generation (RAG) (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard et al., 2022). RAG first retrieves from a document set to acquire information related to the input task. Retrieval can be done with a search engine such as Google or a from a corpus such as Wikipedia. The retrieved information is then input to the LLM with the task instructions. This predisposes the LLM to generate content that is faithful to the retrieved content, achieving improved factual performance (Lewis et al., 2020; Asai et al., 2024b). However, RAG by itself does not guarantee factual content (Wu et al., 2024); even with entirely correct retrieved content in the context, the LLMs can still generate factually incorrect output. One of the reasons that LLMs may still generate incorrect output is due to constraints and uncertainty in their internal states (Neeman et al., 2022; Mallen et al., 2023).\nPrior work has attempted to improve RAG by improving the quality of retrieval (Asai et al., 2024a) or attempting to correct retrieved content (Yan et al., 2024). Surprisingly, we find that these steps are not necessary, and RAG with Google search produces results that are over ten points higher than previous baselines. Therefore, we focus on correcting generated output using the retrieved content. While we are not the first to use retrieved content to improve factuality (Gao et al., 2023), we greatly improve upon it (see Table 2).\nWe propose a method we call Retrieval Augmented Correction (RAC). RAC verifies and corrects LLM-generated content using retrieved knowledge to ensure factuality. Specifically, RAC"}, {"title": "2 Related Work", "content": "Hallucination has been a known issue for generation tasks, especially when using LLMs (Maynez et al., 2020; Rawte et al., 2023; Ji et al., 2023; Zhang et al., 2023; Ye et al., 2023a; Huang et al., 2023). Our work focuses on one of the hallucination types for LLMs, factual incorrectness. There are four lines of work regarding reducing factual incorrectness: 1) from the LLM decoding perspective, 2) from the factual enhancement perspective using retrieval augmentation or fine-tuning, 3) from a self-correction or self-alignment perspective, and 4) from a post-correction using retrieved content perspective.\nFrom the LLM decoding perspective, Li et al. (2023) proposes Inference-Time Intervention (ITI) to enhance the truthfulness of large language models (LLMs). ITI shifts model activations during inference, following a learned set of directions across a limited number of attention heads. Chuang et al. (2024) introduces a new decoding method that contrasts predictions made by different model layers to improve factuality performance. Das et al. (2024) proposed extrapolating critical token probabilities beyond the last layer for more accurate contrasting during LLMs decoding.\nFrom the factual enhancement perspective, there are two sub-types. One sub-type is factuality enhancement training or fine-tuning. Lee et al. (2022) proposed a modified top-k sampling strategy and a factuality-enhanced training method to improve the factuality of text generation. Yang et al. (2023) focus on honesty-based fine-tuning, empowering LLMs to admit limitations by acknowledging \u201cI don't know.\" Tian et al. (2024) constructed a direct preference optimization (DPO) dataset to fine-tune the LLM to improve factuality using reference-based and reference-free truthfulness annotation techniques.\nFor the factual enhancement perspective, the second sub-type is Retrieval Augmented Generation (RAG) (Chen et al., 2017; Lewis et al., 2020; Izacard et al., 2022). Self-RAG (Asai et al., 2023) is proposed to selectively retrieve knowledge and introduce a model to decide whether to retrieve it. Yoran et al. (2024) designed an NLI model to identify and remove irrelevant context in RAG and improve robustness. SAIL (Luo et al., 2023) is tuned on instructions to insert retrieved documents before instructions. Jiang et al. (2023b) actively anticipate future content and decide when and what to retrieve in long-form generation. Yan et al. (2024) designed a lightweight retrieval evaluator to assess the overall quality of retrieved documents for a query to filter out or correct incorrect or irrelevant retrieval content for RAG.\nFrom the self-correction or self-alignment perspective, Zhang et al. (2024) proposed a self-evaluation component. They prompt an LLM to validate the factuality of its own generated responses based on its internal knowledge and utilize Self-Knowledge Tuning to augment the LLM's self-\""}, {"title": "3 Retrieval Augmented Correction (RAC)", "content": "Our approach to improving factuality is to retrieve documents relevant to the input, and use these documents to revise the output so that it is factual."}, {"title": "3.1 Overview", "content": "Figures 1 and 2 show the approach overview with or without RAG. We first break down all original outputs from LLMs into atomic facts (Min et al., 2023), which allows our method to do a fine-grained correction of individual facts.\nFor LLMs without RAG, we propose to add correction and revision stages. The correction stage directly corrects the extracted incorrect statements and keeps the correct statements based on the retrieved document sets for the task input. The statements are then fed into the revision module to revise the original task outputs from LLMs.\nFor LLMs with RAG, correction by itself is not sufficient. We find that RAG by itself performs well enough that many of the statements do not need to be corrected; simply correcting all statements will introduce more hallucinations, which harm the performance rather than benefit. Considering this, we add a verification component to first to verify the statements and then only correct the false statements. This reduces the hallucinations during RAC since it ensures that truth statements are kept without passing into the correction stage.\nBoth our proposed correction and verification stages use a retriever. The retriever retrieves related factual documents from the input. We apply post-processing for the retrieved documents to make them related, faithful, and concise."}, {"title": "3.2 Retrieval", "content": "The retrieval step includes two parts: retrieval and retrieval post-processing. Retrieval directly retrieves the factual documents using task input from a trusted knowledge source (Guu et al., 2020; Lewis et al., 2020; Izacard et al., 2022). The retrieval post-processing conducts two things: 1. filtering out unrelated documents or reranking and picking up top-k documents; 2. truncating or compressing the"}, {"title": "3.3 Atomic Fact Extraction", "content": "Atomic fact extraction breaks down the original task outputs from LLMs into several independent factual statements. This strategy is inspired by Factscore (Min et al., 2023). This can lead to more detailed corrections to the statements, further enhancing the correction and providing a clear interpretation of which part of the original outputs are corrected.\nLet the LLM task outputs be Mout, and the extracted atomic facts S. Then:\nS = Extract(Mout) = {s1, s2, s3, ..., sn} (3)\nwhere n is the number of the statements, si is the ith atomic fact."}, {"title": "3.4 Correction (C)", "content": "We add a retrieval process into the factual correction stage, which improves upon the self-correction of prior work (Wang et al., 2024). When the retrieval document set is trustworthy, this step enhances factual correction. The correction stage corrects the statements based on the verification results using retrieved document sets; then, the revision stage can use them to revise the original LLM"}, {"title": "3.5 Revision (R)", "content": "While most works of self-revision use LLMs generated feedback, which can hallucinate all the time (Madaan et al., 2023; Ye et al., 2023b), we applied the previous corrected statements after correction during the revision stage, which enables the generated content to align with the ensured truths instead of the hallucinated feedback. The revision stage uses previously corrected statements to revise the original task outputs. To enable the revision to be still consistent with the task input, we include the task input X, and the revised outputs O is:\nO = Revise(X, Mout, C') (5)"}, {"title": "4 Combining RAC with RAG", "content": "To further improve results, we can combine our proposed method with retrieval augmented generation (RAG), which we discuss in this section."}, {"title": "4.1 RAG for LLMS", "content": "We first review retrieval augmented generation for LLMs.\nGiven an input X, RAG first retrieves documents related to X from a document set D = {d1, ....dm} (di represents the ith document) to obtain a relevant document set R = {d1,...,dn}. The generation probability Y is the standard next-token prediction probability conditioned on the input context X and retrieved relevant documents R.\nP(YX, R) (6)\nAssuming the retrieved documents are all related to the task input and contain the correct answers, RAG still suffers from potential hallucination issues because retrieved documents may contain other unrelated information that could cause hallucination (Shi et al., 2023), and the retrieved document may contradict what the model initially learns internally and the model sticks to their original training because of internal prior is very strong during training (Wu et al., 2024). To alleviate the above issues, we need to take extra steps to verify"}, {"title": "4.2 Verification (V)", "content": "Since many of the extracted statements after using RAG are correct, we find we do not need to correct all statements. Instead, we added a verification stage to enable the LLMs to correct only false statements, which reduces the hallucinations introduced by correcting already correct statements. Some previous self-verification works consider only LLM self-consistency (Manakul et al., 2023) or require additional models for verification (M\u00fcndler et al., 2024). Unlike these previous works, we add a retrieval process to the verification. The verification is done using LLMs without additional training. The verification stage verifies the extracted atomic facts using the retrieved documents. The verified results are then fed into the correction stage.\nLet V be the verification results, Then:\nV = Verify(R', S) = {b1, b2, b3, ..., bn} (7)\nwhere bi is the verification result for the ith atomic fact. The value of bi is True, or False, or Not Mentioned. True means a similar statement can be found in the retrieved documents and has the same meaning, which indicates the statement is consistent with the retrieved documents. False means a similar statement can be found in the retrieved documents but has a different meaning, which indicates the statement contradicts the retrieved documents. Not Mentioned means a similar statement cannot be found in the retrieved documents, which indicates the statement cannot be verified by the retrieved documents."}, {"title": "4.3 Correction (C)", "content": "Let St, Sf, and Snm be the set of atomic facts labeled by the verifier as True, False, or Not Mentioned, respectively. We use the following strategy to make the correction:\nC = S\u0141U{Correct(s, R', X)|s \u2208 Sf}USnm (8)\nwhere True statements are always kept, False statements always be corrected, and Not Mentioned statements are kept. We also experimented with a strategy that removes all not mentioned statements, but in preliminary experiments found it to give worse results:\nC = St \u222a {Correct(s, R', X)|s \u2208 Sf} (9)"}, {"title": "4.4 Revision (R)", "content": "The revision stage with the verification stage is the same as the revision stage without verification (see \u00a73.5). However, to avoid more newly introduced hallucinations for the initial model generations during revision, we also tried a Keep All True (KAT) strategy: only revise model generations with one or more incorrect statements during verification and keep those without any incorrect statements unchanged. Our ablation study in the appendix (\u00a7D) analyzes the performance of this strategy."}, {"title": "5 Experimental Settings", "content": ""}, {"title": "5.1 Datasets and Metrics", "content": "We use the two available datasets for factuality evaluation on open-ended generation (not classification): TruthfulQA (Lin et al., 2022) and biography generation (Min et al., 2023). For TruthfulQA we use the generation task, which is a short-form generation task. Following the TruthfulQA evaluation, we report the accuracy of BLEURT (Sellam et al., 2020), BLEU (Papineni et al., 2002), and ROUGE (Lin, 2004). Accuracy is computed by comparing the predictions with correct and incorrect answers collected. Biography is a long-form generation task where the evaluation metric is Factscore. Factscore uses OpenAI GPT-3 to judge the accuracy of factuality compared to the corresponding Wikipedia biography. Since GPT-3 is no longer available, all reported numbers for Factscore use GPT-3.5-Turbo-Instruct."}, {"title": "5.2 Models and Baselines", "content": "We use GPT-3.5-Turbo (OpenAI, 2024), Llama 2-7B-Chat (Touvron et al., 2023), Llama3-8B-Instruct (Meta, 2024), and Mistral-7B-Instruct (Jiang et al., 2023a) as baseline models to evaluate our method on closed and open LLMs. Please see Appendix B for hyperparameters.\nWe report numbers for previous state-of-the-art baselines. To compare our method to the previous method RARR (Gao et al., 2023), CRITIC (Gou et al., 2024) and EVER (Kang et al., 2024), we run them using the same model (GPT-3.5-Turbo) and search engine (Google search) or retrieved documents as ours. EVER is reproduced in a post-correction manner per sentence rather than correction of each sentence during generation to speed up experiments."}, {"title": "6 Results", "content": "Results are shown in Table 2. We report our findings for each dataset below.\nResults on TruthfulQA For the TruthfulQA dataset, our method improves upon all previous methods across all LLMs and metrics with and without RAG.\nWe note the instruction-tuned model Llama2-7B-Chat is better than previous methods using the Llama2-7B model (models listed under \"Llama2-7B With Additional Training\u201d in Table 2), in both RAG and non-RAG settings. In RAG settings, previous methods RARR (Gao et al., 2023), CRITIC (Gou et al., 2024) and EVER (Kang et al., 2024) have a lower performance than GPT-3.5-Turbo, indicating that these methods introduce new hallucinations when applied in the RAG setting. In contrast, our method improves upon GPT-3.5-Turbo even in the RAG setting. Across base LLM models, our method improves upon the baseline instruction tuned model by up to approximately 35% on BLEU accuracy, 18% on BLEURT accuracy, and 21% on ROUGE accuracy without RAG and up to 15% on BLEURT accuracy, 26% on BLEU accuracy and 20% on ROUGE accuracy with RAG. Surprisingly, our approach with Llama2-7B-Chat and LLama3-8B-Instruct without RAG is better with RAG, which indicates there are cases where using our approach without RAG is even better than with RAG.\nResults on Biography For the Biography dataset, our method improves upon all previous methods across all LLMs and metrics with and without RAG, with the exception of our re-implementation of EVER. However, we note that EVER is much slower than our method (see \u00a78).\nSimilar to TruthfulQA, we note the instruction-tuned model Llama2-7B-Chat is better than previous methods using Llama2-7B model, in both RAG and non-RAG settings. The previous approaches RARR and CRITIC improve performance slightly without RAG but have a degraded performance with RAG. In contrast, our method improves performance by up to 31% without RAG across three open-sourced models and up to 1.5% with RAG compared to strong instruction-tuned baselines. Although EVER is slightly better than our method with and without RAG setting, EVER's latency is much larger (see \u00a78). Considering the baseline RAG performance is already over 90% in this dataset, our method still shows robust improvement with and without RAG settings across the range of LLMs, especially for open-sourced models."}, {"title": "7 Ablation Experiments", "content": ""}, {"title": "7.1 Ablation of Verification", "content": "Table 3 shows ablation results with or without verification, and with or without RAG. For LLMs without RAG, in most cases, performance drops significantly after adding the verification, although the performance is still better than the baseline. The reason for this is that without RAG, the original generated content has more content that needs to be corrected, and the verification step removes some critical corrections. For LLMs with RAG, the situation is different and verification improves performance. The reason is that RAG's performance is already very high, so if we correct all the statements, the correction process may introduce hallucinations which lowers the performance.\nTo conclude, for models without RAG, correcting all statements is optimal, regardless of whether statements are true or false. In the RAG setting, adding a verification stage and correcting only false statements avoids introducing hallucinations during correction and revision."}, {"title": "7.2 Different LLMs Capabilities", "content": "Based on the analysis of the above results, we can infer the performance comparison of verification and correction with revision for selected LLMS in different RAG settings for each task. Table 6 in the appendix shows model ability ranking for each component inferred from the results. Generally, Llama2-7B-chat has the best performance among all settings, while LLama3-8B-Instruct has the worst performance. While Llama series performance is not stable across the dataset (either Llama2-7B-chat or LLama3-8B-Instruct has been ranked third in one or more settings and components), the performance of GPT-3.5-turbo has not been ranked third, indicating that the closed-source model is more robust than the open-sourced model. The model ability is also task-related, i.e., Mistral-7B-instruct performs decently in the Biography dataset but poorly in the TruthfulQA."}, {"title": "7.3 Effect of Retrieval Correctness", "content": "To analyze the effect of the retrieval correctness, we tested the performance using the gold data from the Biography dataset as the retrieved documents instead of our retrieving methods since this can ensure that the retrieval process is 100% accurate. We use GPT-3.5-turbo as the verification, correction, and revision model since it is the most robust model. The results are even promising compared to our sub-optimal retrieving accuracy. Table 4 shows the results of this case. Without RAG, using gold data as the retrieval data for correction only improves the performance little. RAG using gold data has improved RAG a lot, and our approach can further enhance the RAG with gold retrieved data, achieving a performance of nearly 98%. This demonstrates that high-quality retrieved data is important to the success of our approach."}, {"title": "8 Latency", "content": "Table 5 shows the experimentally measured latency on the Biography dataset for our method and previous approaches. Our method has reduced latency of 2x to 40x compared to previous approaches.\nWe describe the major sources of latency for each method. RARR generates a set of questions for each sentence in the output and then performs retrieval and reranking for each question, which introduces latency. CRITIC has several correction iterations, increasing the number of LLMs API calls and retrieval calls. EVER generates and corrects the output sentence by sentence, and for each sentence, retrieves using three different types of information; although the performance is slightly better than ours on the Biography dataset without RAG, the latency is the largest of all approaches and may be unacceptable for some applications. In contrast to prior methods, our method retrieves once and corrects once, which reduces latency while remaining highly effective."}, {"title": "9 Case Study", "content": "We analyze several examples manually to see the effect of our method. We find the baseline LLM often generates hallucinated content, which is factually incorrect. After applying our correction and revision on this setting without using RAG, all errors are corrected. However, there is still missing information. Using just RAG, the LLMs generate mostly factually correct answers, but there are still some factually incorrect texts. Only applying the"}, {"title": "10 Conclusion", "content": "We introduce a simple but effective post-processing approach for improving factual correctness for instruction-tuned LLMs. Our method has improved latency over prior methods, does not involve additional training, and can be applied to settings with and without RAG. Experiments demonstrate that the proposed Retrieval Augmented Correction (RAC) approach can enhance the performance on two popular factuality evaluation datasets by up to 30% for various LLMs and generation setups. For future work, we suggest focusing on reducing newly introduced hallucinations during those operations and improving performance for each operation."}, {"title": "Limitations", "content": "In rare cases, new hallucinations may be introduced during correction and revision, which should be further investigated in future work. Our verification,"}, {"title": "A Retrieval & Post-Processing", "content": "We apply Google search for the retrieval process to obtain high-quality retrieval data. We then applied different post-processing strategies for different tasks since different tasks have different features.\nFor the biography task, we use the keyword \"{Named Entity} Wikipedia\" to search Google since the biography dataset is mainly from Wikipedia. After retrieving the top 10 results, we have two stages for the postprocessing; one is the filtering, and the other is truncating to fit the input"}, {"title": "B Hyperparameters", "content": "For GPT-3.5-turbo, we use nucleus sampling with top_p = 0.3, meaning only the tokens comprising the top 30% probability mass are considered during generation. For Llama 2-Chat-7B or Llama 3-Chat-7B, we use their default setting. For different approaches, the hyperparameter settings for each LLM are the same."}, {"title": "C Performance Ranking for Each Model", "content": "Table 6 shows performance ranking for each LLM on different proposed operations based on evaluation results."}, {"title": "D Keep All True (KAT) Ablation", "content": "Table 7 shows results comparison of using and without using KAT."}]}