{"title": "A Score-Based Density Formula, with Applications in Diffusion Generative Models", "authors": ["Gen Li", "Yuling Yan"], "abstract": "Score-based generative models (SGMs) have revolutionized the field of generative modeling, achieving unprecedented success in generating realistic and diverse content. Despite empirical advances, the theoretical basis for why optimizing the evidence lower bound (ELBO) on the log-likelihood is effective for training diffusion generative models, such as DDPMs, remains largely unexplored. In this paper, we address this question by establishing a density formula for a continuous-time diffusion process, which can be viewed as the continuous-time limit of the forward process in an SGM. This formula reveals the connection between the target density and the score function associated with each step of the forward process. Building on this, we demonstrate that the minimizer of the optimization objective for training DDPMs nearly coincides with that of the true objective, providing a theoretical foundation for optimizing DDPMs using the ELBO. Furthermore, we offer new insights into the role of score-matching regularization in training GANs, the use of ELBO in diffusion classifiers, and the recently proposed diffusion loss.", "sections": [{"title": "1 Introduction", "content": "Score-based generative models (SGMs) represent a groundbreaking advancement in the realm of generative models, significantly impacting machine learning and artificial intelligence by their ability to synthesize high-fidelity data instances, including images, audio, and text. These models operate by progressively refining noisy data into samples that resemble the target distribution. Due to their innovative approach, SGMs have achieved unprecedented success, setting new standards in generative AI and demonstrating extraordinary proficiency in generating realistic and diverse content across various domains, from image synthesis and super-resolution to audio generation and molecular design.\nThe foundation of SGMs is rooted in the principles of stochastic processes, especially stochastic differential equations (SDEs). These models utilize a forward process, which involves the gradual corruption of an initial data sample with Gaussian noise over several time steps. This forward process can be described as:\n$$X_0 \\xrightarrow{\\text{add noise}} X_1 \\xrightarrow{\\text{add noise}} \\dots \\xrightarrow{\\text{add noise}} X_T,$$ (1.1)\nwhere $X_0 \\sim P_{data}$ is the original data sample, and $X_T$ is a sample close to pure Gaussian noise. The ingenuity of SGMs lies in constructing a reverse denoising process that iteratively removes the noise, thereby reconstructing the data distribution. This reverse process starts from a Gaussian sample $Y_T$ and moves backward as:\n$$Y_T \\xrightarrow{\\text{denoise}} Y_{T-1} \\xrightarrow{\\text{denoise}} \\dots \\xrightarrow{\\text{denoise}} Y_0$$ (1.2)\nensuring that $Y_0 \\approx X_0$ at each step $t$. The final output $Y_0$ is a new sample that closely mimics the distribution of the initial data $P_{data}$.\nInspired by the classical results on time-reversal of SDEs , SGMs construct the reverse process guided by score functions $\\nabla \\log p_{X_t}$, associated with each step of the forward process. Although these score functions are unknown, they are approximated by neural networks trained through score-matching techniques. This leads to two popular models: denoising diffusion probabilistic models (DDPMs) and denoising diffusion implicit models (DDIMs) . While the theoretical results in this paper do not depend on the specific construction of the reverse process, we will use the DDPM framework to discuss their implications for diffusion generative models.\nHowever, despite empirical advances, there remains a lack of theoretical understanding for diffusion generative models. For instance, the optimization target of DDPM is derived from a variational lower bound on the log-likelihood , which is also referred to as the evidence lower bound (ELBO) . It is not yet clear, from a theoretical standpoint, why optimizing a lower bound of the true objective is still a valid approach. More surprisingly, recent research suggests incorporating the ELBO of a pre-trained DDPM into other generative or learning frameworks to leverage the strengths of multiple architectures, effectively using it as a proxy for the negative log-likelihood of the data distribution. This approach has shown empirical success in areas such as GAN training, classification, and inverse problems. While it is conceivable that the ELBO is a reasonable optimization target for training DDPMs (as similar idea is utilized in e.g., the majorize-minimization algorithm), it is more mysterious why it serves as a good proxy for the negative log-likelihood in these applications.\nIn this paper, we take a step towards addressing the aforementioned question. On the theoretical side, we establish a density formula for a diffusion process $(X_t)_{0 \\le t<1}$ defined by the following SDE:\n$$dX_t = \\frac{1}{2(1-t)} X_t dt + \\frac{1}{\\sqrt{1-t}} dB_t \\quad (0 \\le t < 1), \\qquad X_0 \\sim P_{data},$$ (2.4)\nwhich can be viewed as a continuous-time limit of the forward process (1.1). Under some regularity conditions, this formula expresses the density of $X_0$ with the score function along this process, having the form\n$$\\log p_{X_0}(x) = \\frac{1 + \\log(2\\pi)}{2} d - \\int_0^1 \\left[ \\frac{1}{2(1-t)} E \\left[ \\left\\| \\frac{X_t - \\sqrt{1-t} X_0}{\\sqrt{t}} \\right\\|^2 \\left| X_0 = x \\right. \\right] + \\frac{t}{2} \\left\\| \\nabla \\log p_{X_t}(X_t) \\right\\|^2 \\right] dt,$$\nwhere $p_{X_t}(\\cdot)$ is the density of $X_t$. By time-discretization, this reveals the connection between the target density $P_{data}$ and the score function associated with each step of the forward process (1.1). These theoretical results will be presented in Section 3.\nFinally, using this density formula, we demonstrate that the minimizer of the optimization target for training DDPMs (derived from the ELBO) also nearly minimizes the true target\\textemdash the KL divergence between the target distribution and the generator distribution. This finding provides a theoretical foundation for optimizing DDPMs using the ELBO. Additionally, we use this formula to offer new insights into the role of score-matching regularization in training GANs , the use of ELBO in diffusion classifiers , and the recently proposed diffusion loss . These implications will be discussed in Section 4."}, {"title": "2 Problem set-up", "content": "In this section, we formally introduce the Denoising Diffusion Probabilistic Model (DDPM) and the stochastic differential equation (SDE) that describes the continuous-time limit of the forward process of DDPM."}, {"title": "2.1 Denoising diffusion probabilistic model", "content": "Consider the following forward Markov process in discrete time:\n$$X_t = \\sqrt{1 - \\beta_t} X_{t-1} + \\sqrt{\\beta_t} W_t \\quad (t = 1, ..., T), \\qquad X_0 \\sim P_{data},$$ (2.1)\nwhere $W_1, ..., W_T \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(0, I_d)$ and the learning rates $\\beta_t \\in (0,1)$. Since our main results do not depend on the specific choice of $\\beta_t$, we will specify them as needed in later discussions. For each $t \\in [T]$, let $q_t$ be the law or density function of $X_t$, and let $a_t := 1 - \\beta_t$ and $\\bar{a}_t := \\prod_{i=1}^t a_i$. A simple calculation shows that:\n$$X_t = \\sqrt{\\bar{a}_t} X_0 + \\sqrt{1 - \\bar{a}_t} W_t \\qquad \\text{where} \\quad W_t \\sim \\mathcal{N}(0, I_d).$$ (2.2)\nWe will choose the learning rates $\\beta_t$ to ensure that $\\bar{a}_T$ is sufficiently small, such that $q_T$ is close to the standard Gaussian distribution.\nThe key components for constructing the reverse process in the context of DDPM are the score functions $s_t: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ associated with each $q_t$, defined as the gradient of their log density:\n$$s_t(x) := \\nabla \\log q_t(x) \\quad (t = 1,...,T).$$ (2.3)\nWhile these score functions are not explicitly known, in practice, noise-prediction networks $\\epsilon_t(x)$ are trained to predict\n$$\\hat{\\epsilon}_t(x) := - \\sqrt{1 - \\bar{a}_t} s_t(x),$$"}, {"title": "2.2 A continuous-time SDE for the forward process", "content": "In this paper, we build our theoretical results on the continuous-time limit of the aforementioned forward process, described by the diffusion process:\n$$dX_t = \\frac{1}{2(1-t)} X_t dt + \\frac{1}{\\sqrt{1-t}} dB_t \\quad (0 \\le t < 1), \\qquad X_0 \\sim P_{data},$$ (2.4)\nwhere $(B_t)_{t\\ge 0}$ is a standard Brownian motion. The solution to this stochastic differential equation (SDE) has the closed-form expression:\n$$X_t = \\sqrt{1-t} X_0 + \\sqrt{t} Z_t \\qquad \\text{where} \\qquad Z_t = \\frac{1}{\\sqrt{t}} \\int_0^t \\frac{1}{\\sqrt{1-s}} dB_s \\sim \\mathcal{N}(0, I_d).$$ (2.5)\nIt is important to note that the process $X_t$ is not defined at $t = 1$, although it is straightforward to see from the above equation that $X_t$ converges to a Gaussian variable as $t \\rightarrow 1$.\nTo demonstrate the connection between this diffusion process and the forward process (2.1) of the diffusion model, we evaluate the diffusion process at times $t_i = \\sqrt{1 - \\bar{a}_i}$ for $1 \\le i \\le T$. It is straightforward to check that the marginal distribution of the resulting discrete-time process $\\{X_{t_i} : 1 \\le i \\le T\\}$ is identical to that of the forward process (2.1). Therefore the diffusion process (2.4) can be viewed as a continuous-time limit of the forward process. In the next section, we will establish theoretical results for the diffusion process (2.4). Through time discretization, our theory will provide insights for the DDPM.\nWe use the notation $X_t$ for both the discrete-time process $\\{X_t : t \\in [T]\\}$ in (2.1) and the continuous-time diffusion process $(X_t)_{0 \\le t<1}$ in (2.4) to maintain consistency with standard literature. The context will clarify which process is being referred to."}, {"title": "3 The score-based density formula", "content": "Our main results are based on the continuous-time diffusion process $(X_t)_{0 \\le t<1}$ defined in (2.4). While $X_0$ might not have a density, for any $t \\in (0, 1)$, the random variable $X_t$ has a smooth density, denoted by $p_t(\\cdot)$.\nOur main result characterizes the evolution of the conditional mean of $\\log p_t(X_t)$ given $X_0$, as stated below."}, {"title": "3.1 Main results", "content": "Theorem 1. Consider the diffusion process $(X_t)_{0 \\le t<1}$ defined in (2.4), and let $p_t$ be the density of $X_t$. For any $0 < t_1 < t_2 < 1$, we have\n$$E [\\log p_{t_2}(X_{t_2}) - \\log p_{t_1}(X_{t_1}) | X_0] = \\int_{t_1}^{t_2} \\left[ \\frac{1}{2(1-t)} E \\left[ \\left\\| \\frac{X_t - \\sqrt{1-t} X_0}{\\sqrt{t}} \\right\\|^2 \\left| X_0 \\right. \\right] + \\frac{t}{2} \\left\\| \\nabla \\log p_t(X_t) \\right\\|^2 \\right] dt.$$\nThe proof of this theorem is deferred to Appendix 5. A few remarks are as follows. First, it is worth mentioning that this formula does not describe the evolution of the (conditional) differential entropy of the process, because $p_t(\\cdot)$ represents the unconditional density of $X_t$, while the expectation is taken conditional on $X_0$. Second, without further assumptions, we cannot set $t_1 = 0$ or $t_2 = 1$ because $X_0$ might not have a density (hence $p_0$ is not well-defined), and $X_t$ is only defined for $t < 1$. By assuming that $X_0$ has a finite second moment, the following proposition characterizes the limit of $E[\\log p_t(X_t)|X_0]$ as $t \\rightarrow 1$.\nProposition 1. Suppose that $E[\\|X_0\\|^2] < \\infty$. Then for any $x_0 \\in \\mathbb{R}^d$, we have\n$$\\lim_{t \\rightarrow 1^-} E [\\log p_t (X_t) | X_0 = x_0] = - \\frac{1 + \\log (2\\pi)}{2} d.$$\nThe proof of this proposition is deferred to Appendix A. This result is not surprising, as it can be seen from (2.5) that $X_t$ converges to a standard Gaussian variable as $t \\rightarrow 1$ regardless of $x_0$, and we can check\n$$E[\\log \\phi(Z)] = - \\frac{1 + \\log (2\\pi)}{2} d,$$"}, {"title": "3.2 From continuous time to discrete time", "content": "In this section, to avoid ambiguity, we will use $(X_t^{sde})_{0 \\le t<1}$ to denote the continuous-time diffusion process (2.4) studied in the previous section, while keep using $\\{X_t : 1 \\le t \\le T\\}$ to denote the forward process (2.1). The density formula (3.1) is not readily implementable because of its continuous-time nature. Consider time discretization over the grid\n$$0 < t_1 < t_2 < \\dots < t_T < t_{T+1} = 1 \\quad \\text{where} \\quad t_i := 1 - \\bar{a}_i \\quad (1 \\le i \\le T).$$ Recall that the forward process $X_1, ..., X_T$ has the same marginal distribution as $X_{t_1}^{sde}, ..., X_{t_T}^{sde}$ snapshoted from the diffusion process (2.4). This gives the following approximation of the density formula (3.1a):\n$$\\log p_0(x_0) \\approx E [\\log p_{t_1}(X_{t_1}^{sde}) | X_0^{sde} = x_0] \\\\ (ii) \\approx \\frac{1 + \\log(2\\pi t_1)}{2} d - \\frac{1}{2} \\int_0^{t_1} \\left[ \\frac{1}{2(1-t)} E \\left[ \\left\\| \\frac{X_t^{sde} - \\sqrt{1-t} X_0^{sde}}{\\sqrt{t}} \\right\\|^2 \\left| X_0^{sde} = x_0 \\right. \\right] + \\frac{t}{2} \\left\\| \\nabla \\log p_t(X_t^{sde}) \\right\\|^2 \\right] dt; \\\\ (iii) \\approx \\frac{1 + \\log(2\\pi t_1)}{2} d - \\sum_{i=1}^{T} \\frac{t_{i+1} - t_i}{2} \\left[ \\frac{1}{2(1-t_i)} E \\left[ \\left\\| \\frac{X_{t_i}^{sde} - \\sqrt{1-t_i} X_0^{sde}}{\\sqrt{t_i}} \\right\\|^2 \\left| X_0^{sde} = x_0 \\right. \\right] + \\frac{t_i}{2} \\left\\| \\nabla \\log p_{t_i}(X_{t_i}^{sde}) \\right\\|^2 \\right]; \\\\ \\approx \\frac{1 + \\log(2\\pi t_1)}{2} d - \\sum_{t=1}^{T} \\frac{t_{i+1} - t_i}{2} E_{\\epsilon \\sim \\mathcal{N}(0, I_d)} \\left[ \\left\\| \\epsilon - \\hat{\\epsilon}_i(\\sqrt{\\bar{a}_t} x_0 + \\sqrt{1 - \\bar{a}_t} \\epsilon) \\right\\|^2 \\right].$$\nIn step (i) we approximate $\\log p_0(x_0)$ with a smoothed proxy; see the discussion around (3.1c) for details; step (ii) applies (3.1c), where we compute the integral $\\int_0^{\\delta} d/(2t) dt = -(d/2) \\log t_1$ in closed form and approximate the integral"}, {"title": "3.3 Comparison with other results", "content": "The density formulas (3.1) expresses the density of $X_0$ using the score function along the continuous-time limit of the forward process of the diffusion model. Other forms of score-based density formulas can be derived using normalizing flows. Notice that the probability flow ODE of the SDE (2.4) is\n$$\\dot{X}_t = v_t(x_t) \\qquad \\text{where} \\qquad v_t(x) = \\frac{x - \\nabla \\log p_t(x)}{2(1-t)},$$ (3.3)\nnamely, if we draw a particle $x_0 \\sim p_0$ and evolve it according to the ODE (3.3) to get the trajectory $t \\rightarrow X_t$ for $t \\in [0,1)$, then $x_t \\sim p_t$. See e.g., for the derivation of this result.\nUnder some smoothness condition, we can use the results developed in to show that for any given $x_0$\n$$\\log p_t (x_t) - \\log p_0 (x_0) = - \\int_0^t \\nabla \\cdot v_s(x_s) ds = - \\int_0^t \\frac{d}{2(1-s)} tr(\\nabla^2 \\log p_s(x_s)) ds.$$ (3.4)\nHere $t \\mapsto x_t$ is the solution to the ODE (3.3) with initial condition $x_0$. Since the ODE system (3.3) is based on the score functions (hence $x_t$ can be numerically solved), and the integral in (3.4) is based on the Jacobian of the score functions, we may take $t \\rightarrow 1$ and use the fact that $p_t(\\cdot) \\rightarrow \\phi(\\cdot)$ to obtain a score-based density formula\n$$\\log p_0(x) = - \\log \\phi(x_1) - \\frac{1}{2} \\int_0^1 \\frac{d}{2(1-s)} tr(\\nabla^2 \\log p_s(x_s)) ds.$$ (3.5)\nHowever, numerically, this formula is more difficult to compute than our formula (3.1) for the following reasons. First, (3.5) involves the Jacobian of the score functions, which are more challenging to estimate than the score functions themselves. In fact, existing convergence guarantees for DDPM do not depend on the accurate estimation of the Jacobian of the score functions. Second, using this density formula requires solving the ODE (3.3) accurately to obtain $x_1$, which might not be numerically stable, especially when the score function is not accurately estimated at early stages, due to error propagation. In contrast, computing (3.1) only requires evaluating a few Gaussian integrals (which can be efficiently approximated by the Monte Carlo method) and is more stable to score estimation error."}, {"title": "4 Implications", "content": "In the previous section, we established a density formula\n$$\\log q_0(x) \\approx \\frac{1 + \\log (2\\pi \\beta_1)}{2} d - \\sum_{t=1}^{T} \\frac{1 - \\bar{a}_{t+1}}{2 (1 - \\bar{a}_t)} E_{\\epsilon \\sim \\mathcal{N}(0, I_d)} \\left[ \\left\\| \\epsilon - \\hat{\\epsilon}_t(\\sqrt{\\bar{a}_t} x + \\sqrt{1 - \\bar{a}_t} \\epsilon) \\right\\|^2 \\right] \\qquad (4.1)$$\nup to discretization error (which vanishes as $T$ becomes large) and score estimation error. In this section, we will discuss the implications of this formula in various generative and learning frameworks."}, {"title": "4.1 Certifying the validity of optimizing ELBO in DDPM", "content": "The seminal work established the variational lower bound (VLB), also known as the evidence lower bound (ELBO), of the log-likelihood\n$$\\log p_0(x) \\ge \\sum_{t=2}^{T} E_{X_t \\sim P_{X_t | X_0}(\\cdot | x)} KL \\left( P_{X_{t-1} | X_t, X_0}(\\cdot | X_t, x) \\|\\| P_{Y_{t-1} | Y_t}(\\cdot | X_t) \\right) \\\\ =: L_{t-1}(x)\n - KL \\left( P_{Y_T} (\\cdot) \\|\\| P_{X_T | X_0}(\\cdot | x) \\right) + E_{X_1 \\sim P_{X_1 | X_0}(\\cdot|x)} \\left[ \\log P_{Y_0 | Y_1}(x | X_1) \\right], \\\\ =: L_T(x)\n =: C_0(x)$$\nwhere the reverse process $(Y_t)_{0<t<T}$ was defined in Section 2.1, and $p_0$ is the density of $Y_0$. Under the coefficient design recommended by  (other reasonable designs also lead to similar conclusions)\n$$\\eta_t = 1 - \\bar{a}_t \\qquad \\text{and} \\qquad \\sigma_t^2 = \\frac{(1 - \\bar{a}_t) (\\bar{a}_{t-1} - \\bar{a}_t)}{1 - \\bar{a}_t}$$\nit can be computed that for each $2 < t <T$:\n$$L_{t-1}(x) = \\frac{1 - \\bar{a}_{t+1}}{2 (\\bar{a}_t - \\bar{a}_t)} E_{\\epsilon \\sim \\mathcal{N}(0, I_d)} \\left[ \\left\\| \\epsilon - \\hat{\\epsilon}_t(\\sqrt{\\bar{a}_t} x + \\sqrt{1 - \\bar{a}_t} \\epsilon) \\right\\|^2 \\right].$$\nWe can verify that (i) for each $2 < t < T$, the coefficients in $L_{t-1}$ from (4.2) and $\\mathcal{L}_t$ from (4.1) are identical up to higher-order error; (ii) when $T$ is large, $L_T$ becomes vanishingly small; and (iii) the function\n$$C_0(x) = - \\frac{1 + \\log (2\\pi \\beta_1)}{2} d + O(\\beta_1) = C + O(\\beta_1)$$\nis nearly a constant. See Appendix D.1 for details. It is worth highlighting that as far as we know, existing literature haven't pointed out that $C_0(x)$ is nearly a constant. For instance, discretize this term to obtain discrete log-likelihood (see Section 3.3 therein), which is unnecessary in view of our observation. Additionally, some later works falsely claim that $C_0(x)$ is negligible, as we will discuss in the following sections.\nNow we discuss the validity of optimizing the variational bound for training DDPMs. Our discussion shows that\n$$KL(q_0 || p_0) = - E_{x \\sim q_0} [\\log p_0(x)] - H(q_0) \\le E_{x \\sim q_0} [L(x)] - C - H(q_0) + o(1), \\\\ =: L(\\epsilon_1,...,\\epsilon_T)\n =: L_{vb} (\\epsilon_1,...,\\epsilon_T)$$\nwhere $H(q_0) = - \\int \\log q_0(x) d q_0$ is the entropy of $q_0$, and $L(x)$ denotes the widely used (negative) ELBO\n$$L(x) := \\sum_{t=1}^{T} \\frac{1 - \\bar{a}_{t+1}}{2 (1 - \\bar{a}_t)} E_{\\epsilon \\sim \\mathcal{N}(0, I_d)} \\left[ \\left\\| \\epsilon - \\hat{\\epsilon}_t(\\sqrt{\\bar{a}_t} x + \\sqrt{1 - \\bar{a}_t} \\epsilon) \\right\\|^2 \\right].$$"}, {"title": "4.2 Understanding the role of regularization in GAN", "content": "Generative Adversarial Networks (GANs) are a powerful and flexible framework for learning the unknown probability distribution $\\mathcal{P}_{data}$ that generates a collection of training data. GANs operate on a game between a generator $G$ and a discriminator $D$, typically implemented using neural networks. The generator $G$ takes a random noise vector $z$ sampled from a simple distribution $\\mathcal{P}_{noise}$ (e.g., Gaussian) and maps it to a data sample resembling the training data, aiming for the distribution of $G(z)$ to be close to $\\mathcal{P}_{data}$. Meanwhile, the discriminator $D$ determines whether a sample $x$ is real (i.e., drawn from $\\mathcal{P}_{data}$) or fake (i.e., produced by the generator), outputting the probability $D(x)$ of the former. The two networks engage in a zero-sum game:\n$$\\min_G \\max_D V (G, D) := E_{x \\sim \\mathcal{P}_{data}} [\\log D(x)] + E_{z \\sim \\mathcal{P}_{noise}} [\\log(1 - D(G(z)))],$$\nwith the generator striving to produce realistic data while the discriminator tries to distinguish real data from fake. The generator and discriminator are trained iteratively\n$$D \\leftarrow \\underset{D}{\\text{argmin}} \\quad - E_{x \\sim \\mathcal{P}_{data}} [\\log D(x)] - E_{z \\sim \\mathcal{P}_{noise}} [\\log(1 - D(G(z)))],$$\n$$G \\leftarrow \\underset{G}{\\text{argmin}} \\quad - E_{z \\sim \\mathcal{P}_{noise}} [\\log D(G(z))]$$\nto approach the Nash equilibrium $(G^*, D^*)$, where the distribution of $G^*(z)$ with $z \\sim \\mathcal{P}_{noise}$ matches the target distribution $\\mathcal{P}_{data}$, and $D^*(x) = 1/2$ for all $x$.\nIt is believed that adding a regularization term to make the generated samples fit the VLB can improve the sampling quality of the generative model. For example, proposed adding the VLB $L(x)$ as a regularization term to the objective function, where $\\{\\hat{\\epsilon}_t(\\cdot) : 1 \\le t \\le T\\}$ are the learned epsilon predictors for $\\mathcal{P}_{data}$. The training procedure then becomes\n$$D \\leftarrow \\underset{D}{\\text{argmin}} \\quad - E_{x \\sim \\mathcal{P}_{data}} [\\log D(x)] - E_{z \\sim \\mathcal{P}_{noise}} [\\log(1 - D(G(z)))],$$\n$$G \\leftarrow \\underset{G}{\\text{argmin}} \\quad - E_{z \\sim \\mathcal{P}_{noise}} [\\log D(G(z))] + \\lambda E_{z \\sim \\mathcal{P}_{noise}} [L(G(z))],$$\nwhere $\\lambda > 0$ is some tuning parameter. However, it remains unclear what exactly is optimized through the above objective. According to our theory, $L(x) \\approx - \\log \\mathcal{P}_{data}(x) + C$. Assuming that this approximation is exact for intuitive understanding, the unique Nash equilibrium $(G_{\\lambda}, D_{\\lambda})$ satisfies\n$$\\mathcal{P}_{G_{\\lambda}} (x) = \\left( z \\mathcal{P}_{data}(x)^{\\lambda} - 1 \\right)_+ \\mathcal{P}_{data} (x)$$\nfor some normalizing factor $z > 0$, where $\\mathcal{P}_{G_{\\lambda}}$ is the density of $G_{\\lambda}(z)$ with $z \\sim \\mathcal{P}_{noise}$. See Appendix D.2 for details. This can be viewed as amplifying the density $\\mathcal{P}_{data}$ wherever it is not too small, while zeroing out the density where $\\mathcal{P}_{data}$ is vanishingly small (which is difficult to estimated accurately), thus improving the sampling quality."}, {"title": "4.3 Confirming the use of ELBO in diffusion classifier", "content": "Motivated by applications like image classification and text-to-image diffusion model, we consider a joint underlying distribution $p_0(x, c)$, where typically $x$ is the image data and the latent variable $c$ is the class index or text embedding, taking values in a finite set $\\mathcal{C}$. For each $c \\in \\mathcal{C}$, we train a diffusion model for the conditional data distribution $p_0(x | c)$, which provides a set of epsilon predictors $\\{\\hat{\\epsilon}_t(x; c) : 1 \\le t \\le T, c \\in \\mathcal{C}\\}$. Assuming a uniform prior over $\\mathcal{C}$, we can use Bayes' formula to obtain:\n$$p_0 (c | x) = \\frac{p_0 (c) p_0 (x | c)}{\\sum_{j \\in \\mathcal{C}} p_0 (c_j) p_0 (x | c_j)} = \\frac{p_0 (x | c)}{\\sum_{j \\in \\mathcal{C}} p_0 (x|c_j)}$$\nfor each $c \\in \\mathcal{C}$. Recent work proposed to use the ELBO\n$$-L(x; c) := - \\sum_{t=1}^{T} \\frac{1 - \\bar{a}_{t+1}}{2 (1 - \\bar{a}_t)} E_{\\epsilon \\sim \\mathcal{N}(0, I_d)} \\left[ \\left\\| \\epsilon - \\hat{\\epsilon}_t(\\sqrt{\\bar{a}_t} x + \\sqrt{1 - \\bar{a}_t} \\epsilon; c) \\right\\|^2 \\right]$$\nas an approximate class-conditional log-likelihood $\\log p_0(x | c)$ for each $c \\in \\mathcal{C}$, which allows them to obtain a posterior distribution\n$$p_0 (c | x) = \\frac{\\exp(-L(x; c))}{\\sum_{j \\in \\mathcal{C}} \\exp(-L(x; c_j))}.$$ (4.9)\nOur theory suggests that $-L(x; c) \\approx \\log p_0(x | c) - C$, where $C = - [1 + \\log(2\\pi \\beta_1)] d/2$ is a universal constant that does not depend on $p_0$ and $c$. This implies that\n$$p_0 (c | x) \\approx \\frac{\\exp (\\log p_0(x | c) - C)}{\\sum_{j \\in \\mathcal{C}} \\exp (\\log p_0(x | c_j) - C)} = \\frac{p_0 (x | c)}{\\sum_{j \\in \\mathcal{C}} p_0 (x | c_j)} = p_0 (c | x),$$\nproviding theoretical justification for using the computed posterior $p_0$ in classification tasks.\nIt is worth mentioning that, although this framework was proposed in the literature , it remains a heuristic method before our work. For example, in general, replacing the intractable log-likelihood with a lower bound does not guarantee good performance, as they might not be close. Additionally, recall"}, {"title": "4.4 Demystifying the diffusion loss in autoregressive models", "content": "Finally", "in": "in the context of autoregressive image generation. Let $x_k$ denote the next token to be predicted", "z)": 1, "follows": "for some weights $w_t \\ge 0$", "right": ".", "x^i,...,x_k^i)": 1, "risk": "n"}]}