{"title": "An Approach for Auto Generation of Labeling Functions for Software Engineering Chatbots", "authors": ["Ebube Alor", "Ahmad Abdellatif", "SayedHassan Khatoonabadi", "Emad Shihab"], "abstract": "Software engineering (SE) chatbots are increasingly gaining attention for their role in enhancing development processes. At the core of chatbots are the Natural Language Understanding platforms (NLUs), which enable them to comprehend and respond to user queries. Before deploying NLUs, there is a need to train them with labeled data. However, acquiring such labeled data for SE chatbots is challenging due to the scarcity of high-quality datasets. This challenge arises because training SE chatbots requires specialized vocabulary and phrases not found in typical language datasets. Consequently, chatbot developers often resort to manually annotating user queries to gather the data necessary for training effective chatbots, a process that is both time-consuming and resource-intensive. Previous studies propose approaches to support chatbot practitioners in annotating users' posed queries. However, these approaches require human intervention to generate rules, called labeling functions (LFs), that identify and categorize user queries based on specific patterns in the data. To address this issue, we propose an approach to automatically generate LFs by extracting patterns from labeled user queries. We evaluate the effectiveness of our approach by applying it to the queries of four diverse SE datasets (namely AskGit, MSA, Ask Ubuntu, and Stack Overflow) and measure the performance improvement gained from training the NLU on the queries labeled by the generated LFs. We find that the generated LFs effectively label data with AUC scores of up to 85.3%, and NLU's performance improvement of up to 27.2% across the studied datasets. Furthermore, our results show that the number of LFs used to generate LFs affects the labeling performance. We believe that our approach can save time and resources in labeling users' queries, allowing practitioners to focus on core chatbot functionalities rather than on manually labeling queries.", "sections": [{"title": "1 INTRODUCTION", "content": "In the field of software engineering (SE), chatbots are deployed as conversational tools to automate a wide range of tasks, from assisting in problem-solving to answering questions about repositories [1, 2, 3]. At the core of these chatbots are Natural Language Understanding platforms (NLUs). These NLUs serve as the backbone of the chatbot, responsible for interpreting human language into structured data that chatbots can act on [4]. They accomplish this by using machine learning and natural language processing techniques to dissect user queries, identifying the user's intent and extracting key entities like bug IDs or version numbers [5]. By doing so, NLUs make it possible for the chatbot to generate responses that are contextually relevant and specific to the user's queries.\nThe effectiveness of NLUs heavily relies on the availability of a large volume of high-quality training data [6, 7, 8]. However, prior work shows that obtaining such data is expensive, especially in specialized domains like SE [9, 10, 11], where the domain-specific language and terminology pose unique challenges for data collection and labeling [4]. For example, common words like 'push', 'fork', and 'commit' have distinct meanings in the context of software development, which differ from their everyday usage [4]. This makes it difficult to rely on general-purpose datasets for training SE chatbots. Furthermore, there is a notable scarcity of publicly available, high-quality datasets for training chatbots in the SE domain [12]. As a result, gathering sufficient and relevant training data for SE chatbots often requires significant effort and resources.\nTo obtain the necessary training data for SE chatbots, practitioners often resort to mining chatbot user queries as a data source [13]. However, chatbot developers need to label these queries before they can be used for training the NLU, introducing significant challenges [14]. First, manual labeling is labor-intensive and can incur substantial costs [15], particularly when domain expertise is essential for ensuring the accuracy of labels [16, 17]. This increases the financial burden and extends the time required to prepare data for training purposes [18, 15, 19]. Second, although semi-automated labeling methods exist, they still require human intervention, such as domain experts developing heuristics for labeling user queries [20, 21, 22]. These heuristics are also called Labeling Functions (referred hereafter simply as LFs). Specifically, LFs are programmatic rules or functions that assign labels to data points based on certain conditions or patterns. For example, an LF for an SE chatbot might label a query as 'bug-related' if it contains keywords like 'error', 'issue' or 'fix'.\nIn response to the challenges outlined above regarding data labeling, we introduce our approach that automates the process of generating LFs specifically for SE datasets. To accomplish this, our approach takes a small set of labeled data as an input and automatically analyzes and extracts patterns from it, and uses those patterns to generate the LFs capable of auto-labeling data. After the generation of LFs, we use them to label data and train the NLU of the chatbot on the trained data [23]. Our approach is organized into three main components. First, the Grouper is responsible for expanding the initial labeled data by identifying similar queries [24]. Second, the LF Generator takes on the role of extracting patterns from this expanded data to create LFs [25]. Finally, the Pruner filters out low-quality LFs out of the pool of generated LFs [26, 27]. We evaluate our approach based on four datasets (namely, AskGit, MSA, Ask Ubuntu, and Stack Overflow) used to develop chatbots for performing various SE tasks. Specifically, we aim to answer the following research questions in this paper:\nRQ1: How well do the generated LFs label data? We evaluate the quality of the generated LFs in terms of their effectiveness in labeling data, and subsequently, in training SE chatbots [28, 29]. Our analysis shows that the generated LFs effectively label data with AUC scores of above 75.5% for three out of the four studied datasets (except Stack Overflow). Additionally, we observe that for these three datasets, using the auto-labeled data for training can enhance the NLU's performance, with AUC score improvements of up to 27.2%.\nRQ2: What characteristics impact the performance of the generated LFs? We investigate the specific characteristics of the generated LFs (i.e., coverage, accuracy, and LF support) that contribute to the LFs performance. Our findings indicate that higher values in these LF characteristics generally correlate with improved labeling performance. For instance, high coverage LFs achieve AUC scores of up to 88.3%, compared to 50.5% for low coverage LFs. While the influence of characteristics on performance varies, focusing solely on one characteristic may negatively impact others, suggesting a balanced approach that considers all characteristics is essential for optimal performance.\nFinally, we discuss the impact of varying the number of LFs on the labeling performance. We find that a higher number of generated LFs tends to improve labeling performance. However, the rate of improvement varies across different datasets, highlighting the influence of dataset-specific characteristics on the effectiveness of the generated LFs.\nOur Contributions. In summary, our paper makes the following contributions:\n\u2022\tWe introduce an end-to-end approach to automatically generate LFs, facilitating the training of SE chatbots.\n\u2022\tWe show the effectiveness of our approach on multiple SE datasets and with the Rasa NLU platform.\n\u2022\tWe discuss the impact of varying the number of LFs on the labeling performance.\n\u2022\tWe make our dataset and code publicly available to facilitate future research in this area [30].\nPaper Organization. The remainder of the paper is organized as follows. Section 2 provides the background that forms the basis for our study. Section 3 presents the details of our approach and its key components. Section 4 describes the setup for our empirical study followed by Section 5 which presents the results of our RQs. Section 6 provides additional analysis on the impact of the number of LFs generated by our approach on performance and Section 7 discusses threats to the validity of our study. Finally, Section 8 reviews the related work and Section 9 concludes the paper."}, {"title": "2 BACKGROUND", "content": "Before proceeding to our approach, we explain in this section the chatbot related terminologies, data labeling process, and LFs. Also, we briefly discuss the role of NLU-based chatbots in the era of LLMs.\n2.1 Chatbot Training and Data Challenges\nSoftware chatbots serve as the conduit between users and services [31]. Users input their queries to the chatbot in natural language, which then performs the requested action (e.g., querying the database) and responds to the user's question. NLU platforms are the backbone for chatbots to understand the user's question. Specifically, NLU extracts two key aspects from the input query: the topic of the user's query or what is being asked about (known as the 'Entity') and what the user is asking it to do (known as the \u2018Intent').\nSimilar to any machine learning model, NLUs need to be trained to extract intents and entities. In particular, for each intent, the NLU needs to be trained on a set of queries that represent different ways a user could express that intent. Therefore, the training data should include a wide range of examples for each intent to capture the variety of ways in which people express their needs. Chatbot practitioners need to brainstorm the different ways that the user could ask the question for a specific intent [11] to train the NLU. Nevertheless, this is a resource-intensive and time-consuming task [13].\nAlternatively, chatbot practitioners leverage user-chatbot conversations to augment their dataset [13]. To demonstrate this, we present asks the chatbot software repository-related questions. For the first query, the user asks, 'I want to know who created map.json'. Here, the intent is to find out the developer who created the file. The chatbot's NLU analyzes this query, extracting the intent (FileCreator with a confidence score of 0.87) and the relevant entity (file_name is map.json). Based on this understanding, the chatbot then provides a response identifying the file creator.\nTo continuously improve the chatbot's performance, developers use these user-chatbot conversations to expand the NLU's training data. They review queries, especially those with low confidence scores, validate the extracted intent, make corrections if necessary, and then add these annotated queries to the training dataset. Although this process might require less time compared to brainstorming new queries for intent, the chatbot developer still requires annotating users' queries to improve the NLU's performance. To reduce the burden and save developers time, we propose an approach that automates the annotation process of the users' queries.\n2.2 Data Labeling\nThe process of data labeling involves assigning appropriate labels to a dataset [27, 20]. There are several data labeling techniques that are traditionally used [32, 33]. These include manual labeling, crowd-sourcing, and hiring domain experts [32, 15, 16, 34].\nManual labeling is the annotation of unlabeled data by humans [14]. The process typically begins with gathering the unlabeled data [35] and reviewing them. Next, appropriate labels for intents and entities are assigned to the unlabeled data, categorizing them into classes [35]. Once the data is suitably annotated, it is incorporated into the chatbot's training dataset to further enhance its accuracy [35]. Because of all these steps, the manual labeling process requires significant effort and is time-consuming [15]. It can also lead to inconsistencies due to human error, making it less practical for large-scale projects [8].\nCrowdsourcing involves distributing the task of labeling data to a large group of people, often through an online platform. This method can significantly accelerate the data labeling process by leveraging the crowd's collective effort. Although crowdsourcing can help label a large volume of data quickly, it may not always guarantee the quality necessary for specialized domains like SE. The lack of domain-specific knowledge among the crowd can lead to inaccuracies in labeling, affecting the overall quality of the training data [17, 36, 37].\nRecruiting domain experts refers to engaging individuals with specialized knowledge in a particular area, such as SE, to label the data. These experts bring a high level of accuracy and insight to the labeling process, ensuring the data is correctly annotated with the appropriate intents and entities. However, this method can be costly and may not scale well for large datasets, making it a challenging option for projects with limited budgets [16, 17]. Due to the limitations of these labeling techniques, an alternative approach is for domain experts to craft rules or heuristics that can then be applied to unlabeled data [38]. These rules are called LFs and will be discussed in the next subsection.\n2.3 Weak Supervision and LFs\nWeak supervision is a machine learning approach that addresses the challenge of obtaining large amounts of accurately labeled data [38, 39]. Instead of relying solely on expensive and time-consuming manual annotation, weak supervision leverages noisy, imprecise, or incomplete sources of information to generate training labels [39, 40]. Different forms of weak supervision exist [41], including:\n\u2022\tHeuristics: Rule-based labeling using domain knowledge [40] (e.g., labeling a query as 'bug-related' if it contains the word 'error').\n\u2022\tDistant Supervision: Using an external knowledge base or database to automatically generate labels [42] (e.g., labeling code comments based on the presence of specific API calls).\n\u2022\tThird-party Models: Using pre-trained models to generate labels for new data, even if the models were trained on different but related tasks [43].\nThe key advantage of weak supervision is its ability to significantly reduce the time and cost associated with data labeling while enabling the use of larger datasets [38]. This can lead to the development of more robust and accurate models. Moreover, weak supervision, especially through the use of heuristics, allows for the direct integration of valuable domain expertise into the labeling process [26].\nLFs are a common and practical way to implement weak supervision. They are essentially a set of heuristic rules or predefined conditions to assign labels to data [38, 44, 27]. Instead of manually labeling each piece of data, a domain expert formulates a set of rules or conditions to create an LF. The primary goal of LFs is to facilitate the labeling process by determining the appropriate labels for each piece of data [38, 44, 27]. LFs offer several key advantages over manual labeling, including:\n\u2022\tReusability: Once crafted, LFs can be applied multiple times across various batches of data [38, 44].\n\u2022\tEfficiency: LFs can significantly streamline the labeling process by automating label assignment based on well-defined criteria [38, 44, 27].\nDespite these advantages, developing effective LFs can be challenging, as it requires deep domain expertise and substantial time investment, especially as the complexity of the data and the rules increases [20]. To streamline the management and application of our LFs, we selected Snorkel as the underlying framework [27]. Snorkel is a framework specifically designed to enable and manage weak supervision through the use of LFs [38]. It is developed for programmatically building and managing training datasets without manual labeling [38]. Snorkel provides a structured approach for writing LFs in Python and uses a generative model to combine their outputs to create probabilistic labels for unlabeled data. Each LF in Snorkel is a standalone function that labels a subset of the data based on a certain rule or heuristic [38].\nConsider an example of a Snorkel LF implemented in Python,. It is designed to identify intents based on words a query contains. The ContainsWordLabeler class is initialized with a dictionary of labels corresponding to different intents. The apply method takes the input unlabeled data and checks for the presence of phrases indicative of the FileCreator or IssueClosingDate intents. If such phrases are detected, it returns the associated label; if not, it returns ABSTAIN, indicating that the LF cannot confidently assign a label.\n2.4 NLU-based Chatbots in the Era of LLMs\nLarge Language Models (LLMs) have transformed many fields, including SE [45, 46]. However, this raises the question of the relevance of NLU-based chatbots in the era of LLMs. LLMs and deep learning models require a substantial amount of labeled data for effective fine-tuning to be employed in intent classification tasks. Nevertheless, prior work has shown a scarcity of data in the SE domain [12], which hinders the utilization of deep learning models in labeling SE datasets. Even implementing the retrieval augmented generation using LLMs [47, 48, 49], may struggle to capture the nuances and specific terminology of SE tasks [50, 48]. While they can generate human-like responses, the accuracy and specificity of their outputs may vary, potentially lacking the precision required for SE context [50].\nIn contrast, NLU-based chatbots, which require training on smaller SE datasets compared to LLMs, excel in comprehending and addressing common development tasks, such as retrieving information about commits, branches, and issues [51, 4, 52]. By leveraging predefined intents and entities, these chatbots provide precise responses tailored to the SE context. Their deterministic nature enables developers to interpret and trace the reasoning behind each response, ensuring transparency and trust in the chatbot's answers [47, 53]. Since the NLU-based chatbot depends on predefined intents and entities to answer questions, it lacks generalizability. In other words, the NLU-based chatbot does not answer questions that it is not trained on.\nStill, we argue that both NLU-based and LLM-based chatbots have their own use cases. For instance, for a chatbot designed to answer specific questions about a software repository (e.g., identifying the fixing commit for a certain bug), an NLU-based chatbot would be a more suitable solution as it can be tailored to the specific project. Conversely, for a chatbot intended to address a broad spectrum of software development questions (e.g., learning best practices, fixing exceptions), an LLM-based chatbot would be a better choice since LLMs can be fine-tuned using general software development Q&A platforms (e.g., Stack Overflow)."}, {"title": "3 APPROACH", "content": "Figure 4 presents an overview of our approach, which automates the process of generating LFs. The approach takes queries along with their corresponding intents (labeled data) and queries that need to be labeled (unlabeled data) as inputs. The output of our approach is a set of generated LFs that can be used for labeling user queries. Our approach is composed of three main components: (1) Grouper, tasked with expanding the labeled data by detecting semantic similarities between queries in both the labeled and unlabeled data, (2) Generator, responsible for identifying and extracting patterns from the expanded labeled data to generate LFs, and (3) Pruner, which filters high-quality LFs based on their performance. We detail each component in this section.\n3.1 Grouper\nPrevious studies show that the size and diversity of the labeled dataset directly impact the quality of the generated LFs [26, 54, 55]. This is because a more diverse set of queries leads to the generation of LFs that cover different types of user queries. Similar to prior work, we leverage the unlabeled dataset to expand the labeled dataset [56, 57]. More specifically, the Grouper component groups similar queries in the unlabeled dataset and matches them to an intent, in the labeled dataset based on their semantic similarity. For this purpose, the Grouper component leverages the Sentence-t5-xxl [58] transformer to assess the similarity between queries in both labeled and unlabeled datasets. The Sentence-t5-xxl is an 11-billion parameter open-source sentence-based transformer trained in two stages: first on 2 billion question-answer pairs from Community Q&A sites, then fine-tuned on 275K sentence pairs with human-annotated labels from the Stanford Natural Language Inference dataset [58]. Sentence-t5-xxl has been used in prior work to identify the semantic similarity of text [59, 57]. Using Sentence-t5-xxl, The Grouper component identifies the semantic similarity between each query in the unlabelled dataset (unlabelled query) with each query in the labeled dataset (labeled query).\nNext, the Grouper augments the unlabelled query to the intent of the labeled query if the semantic similarity between the two queries is higher than a predefined threshold. Otherwise, the Grouper component abstains from adding the unlabelled query to the labeled dataset. Going forward, we refer to the augmented dataset from Grouper as the expanded dataset.\n3.2 Generator\nThe main goal of the Generator is to utilize the expanded dataset from the Grouper component to generate LFs. Prior work shows that NLU platforms better classify intents that contain queries with unique characteristics such as distinct entity types or exclusive words [4]. Therefore, the Generator scans all intents' queries in the labeled data to extract intents' characteristics that distinguish them from other intent classes. More specifically, the Generator inspects the following characteristics for each intent in the dataset:\nDistinct Entity Type: Some intents contain distinct entity types that are not present in other intents. For example, in Figure 1, the entity type 'file_name' appears only in the queries of the FileCreator intent. Some NLUs employ entity types as input for intent classification [60]. To identify the intents with distinct entity types, the Generator component scans all queries within the expanded labeled data, including those labeled by the Grouper component, for entities. Here, we utilize Rasa to detect the entities as it has been shown to perform well for SE tasks [4] and is also open-source. Next, the Generator component computes the entities that are distinct to the various intent classes. Finally, the Generator generates the LFs that can label queries that contain these distinct entity types. In the running example, the Generator generates an LF that labels queries with FileCreator intent if the query contains the 'file_name' entity type.\nExclusive words: In addition to distinct entity types, intents that contain exclusive words are easier to identify by NLUs [4]. For example, in Figure 1, the words 'created' and 'creator' appear only in queries related to the FileCreator intent. To identify exclusive words in the input dataset, the Generator components employ a two-step process. First, it extracts all words from the dataset. This is achieved using a CountVectorizer from the scikit-learn library, which converts the text data into a numerical representation and builds a vocabulary of unique words. The CountVectorizer is configured to extract individual words (unigrams), remove common English stop words, and strip accents for character normalization.\nThen, for each word, we compute the ratio of its occurrences within a specific intent class to the total occurrences of that word in the entire dataset. Specifically, the Generator component computes the following ratio:\n$Exclusivity(word, intent) = \\frac{Occurrences(word, intent)}{TotalOccurrences(word)}$\nwhere $Occurrences(word, intent)$ is the number of times the word appears in queries associated with a specific intent, and $TotalOccurrences(word)$ is the total number of times the word appears across all intents in the dataset. A ratio closer to 1 indicates that a word is highly exclusive to a particular intent. The ratio becomes our uniqueness threshold, a predefined threshold specified by the user. Once the word exclusivity ratios are computed, the Generator immediately proceeds to generate LFs based on words exceeding the predefined uniqueness threshold.\nDistinct entity and exclusive words: Intents within a dataset may not always be distinguishable merely by a single distinct entity type or by exclusive words. Instead, intents could be characterized through a specific amalgamation of an entity type and exclusive words. For instance, while the entity type 'file_name' and the word 'developer' may each appear in various intent classes, their combination could be distinctive for the FileCreator intent, as shown in Figure 1. The Generator examines the dataset for unique combinations where specific entities and words co-occur frequently enough to exceed a user-predefined threshold, indicating a particular intent. Upon identifying such an intent, the Generator creates LFs that classify the query as that intent if it contains both the entity type and exclusive words.\nMachine Learning (ML) Generated: Although we employ distinct query characteristics to generate the LFs, there are queries that lack clear distinctive features (e.g., distinct entity type). Thus, to make our approach more generalizable by labeling intents that have different characteristics than the ones in the labeled dataset, we employ an ML approach. Specifically, for each intent, we train five ML classifiers namely, Random Forest, Decision Tree, K-Nearest Neighbors, Logistic Regression, and Support-Vector Machine (SVM) on the expanded dataset. These classifiers have been commonly used within the SE literature [61, 62, 63, 64]. We used the labeled dataset as the training data for the classifiers. The input features for the ML-based LFs are extracted from the labeled queries using the CountVectorizer [65], which converts the text data into a matrix of token counts. Each trained classifier will then serve as an LF.\nThe output of the Generator is a list of LFs for all intents that have unique characteristics. The first LF (lines 2 \u2013 7) inspects whether the query contains the words 'created' and 'author'. If this condition is met, it assigns the label FileCreator to the query intent. Otherwise, it returns 'ABSTAIN'. The second and third LFs (lines 8 \u2013 20) follow a similar pattern, using the presence of specific entities and words to assign the appropriate intent labels. To enable the users of our approach maintain the generated LFs, we also include the class intent and the type of labeling strategy it employs.\n3.3 Pruner\nThe Generator produces a range of LFs that vary in quality. Some of the generated LFs might have low accuracy, meaning they frequently mislabel queries when gauged against the intents they intend to label [26]. Additionally, some LFs might overfit to a single data point in the training data. These types of LFs can negatively impact the performance of our approach. Therefore, we devise the Pruner to filter them out from the pool of generated LFs.\nTo achieve this, the Pruner leverages a portion, called evaluation data, of the expanded dataset (i.e., the output of the Grouper) to evaluate all LFs generated by the Generator. Since the expanded dataset contains labeled queries, the Pruner applies the LFs to the evaluation data and computes the accuracy of each LF. In particular, the Pruner discards LFs with lower accuracy because these LFs are likely to mislabel the queries, which could cause the NLU to be trained on incorrect data, thereby potentially degrading its performance. Another key factor is the coverage, which determines the range of applicability for an LF. Thus, the Pruner discards LFs with low coverage. This action also enhances performance speed by decreasing the number of LFs each query needs to be processed through. Another measure of LF quality is LF support, which refers to the number of queries (i.e., data rows) used to train each LF. LF Support captures the essence of training data diversity and volume used to generate each LF. We hypothesize that a large and diverse training dataset leads to the creation of LFs with higher accuracy and coverage. Coverage and Accuracy are established measures from the Snorkel framework [38]. Moreover. they are used by prior work [26, 55].\nUsing the calculated characteristics, the Pruner discards LFs that do not meet performance thresholds. It is important to note that the threshold of the accuracy, coverage, and support can be configured by the users of our approach, which provides the flexibility to adapt the approach to different datasets and thus extends its applicability. The final output is a list of high-quality LFs, ready for use in labeling tasks. In our running example, in Figure 4, LF 1 characteristics are higher than the specified threshold for coverage, accuracy, and support. Therefore, it is retained."}, {"title": "4 CASE STUDY SETUP", "content": "The main goal of the proposed approach is to automate the generation of LFs that label users' queries posed to SE chatbots. This section details the SE datasets used to evaluate the effectiveness of the generated LFs. Moreover, it describes the NLU platform used in the evaluation and the configurations employed in our approach for the assessment.\n4.1 Datasets\nTo evaluate the performance of our approach of generating LFs for SE chatbots, we selected four datasets previously used to develop SE chatbots [66, 9, 67, 4]. Table 1 provides an overview of the number of queries and intents for each of the selected datasets. The selected datasets represent various SE tasks, including seeking information related to software projects and software development tasks. Furthermore, the datasets vary in size, ranging from smaller collections with only a few queries per class to larger sets with hundreds of queries. The details for each dataset with their intents are available in the Appendix. This diversity enables us to assess the effectiveness of our approach across different scales. In particular, we use the following datasets:\nAskGit: AskGit [66] is a chatbot that answers software project-related questions (e.g., \"How many commits happened during March 2021?\") on Slack. It is published on GitHub Marketplace so that practitioners can install it on their software projects. AskGit developers brainstormed to create the initial training set for the intents supported by AskGit. Then, they piloted the chatbot with practitioners to gather additional training queries for each intent, expanding their final dataset. This dataset contains 749 queries grouped into 52 intents.\nMSA: MSA [9] is a chatbot that assists practitioners in creating microservice architectures by providing answers to questions about microservice environment settings (e.g., \u201cTell me the server's environment setting\u201d). The MSA dataset contains 83 queries across eight distinct intents.\nAsk Ubuntu: The Ask Ubuntu dataset [67] contains some of the most popular questions from the Ubuntu Q&A community on Stack Exchange. The intents of the collected questions were annotated through Amazon Mechanical Turk. This dataset includes 50 queries (e.g., \"What screenshot tools are available?\") divided into four intents (e.g., 'Make-Update').\nStack Overflow: Ye et al. [68] collected software practitioners' questions posted under the most popular tags on Stack Overflow. Abdellatif et al. [4] then annotated and categorized these questions (e.g., \"Use of session and cookies what is better?\") into different intents. This dataset contains 215 queries grouped into five intents (e.g., \u2018LookingForBestPractice\u2019).\n4.2 NLU Platform\nNLU platforms serve as the backbones for chatbots as they enable chatbots to understand the user's queries [5, 4, 69]. Typically, chatbot developers resort to off-the-shelf NLUs (e.g., Google Dialogflow) in their chatbot rather than developing an NLU from scratch because it requires both NLP and AI expertise [4]. Among the variety of NLUs, we select the Rasa NLU platform to evaluate the impact of the generated LFs on the NLU's performance. Our motivation for selecting Rasa is that it is an open-source platform, which makes its internal implementation consistent during our evaluation. Thus, it enables the replicability of our study by other researchers compared to the NLUs on the cloud, whose internal implementations could be changed without any prior notice. Rasa can be installed, configured, and run on local machines, which consumes fewer resources compared to the NLUs that operate on the Cloud. Furthermore, Rasa has been used by prior work to develop SE chatbots [9, 11, 70].\n4.3 Evaluation Settings\nHere, we explain the configuration settings used for the evaluation of our approach. For the management and application of LFs, we use Snorkel v0.9.8, which was the latest version at the time the project was initiated. Our approach also involves thresholds for the Generator and Pruner values that influence its performance.\n\u2022\tGrouper Threshold: This threshold determines the minimum semantic similarity score required for an unlabeled query to be added to an existing intent class in the labeled data.\n\u2022\tGenerator Threshold: This threshold determines the minimum exclusivity score (Equation 1) for a word to be considered exclusive to a particular intent and used in generating an LF.\n\u2022\tPruner Threshold: This threshold determines the minimum accuracy required for an LF to be retained by the Pruner.\nTo determine the optimal values for these thresholds, we conducted a systematic evaluation using the MSRBot dataset [11]. We first evaluated for the Grouper's threshold, varying from 0.1 to 1.0, while keeping other thresholds constant. We found that a threshold of 0.8 yielded the best overall performance. Next, we evaluated for the Generator's threshold, varying it from 0.1 to 1.0 in increments of 0.1, while keeping the other thresholds constant. We found that a threshold of 0.8 yielded the best overall performance in terms of labeling accuracy. Finally, we evaluated for the Pruner's threshold, also varying each from 0.1 to 1.0 in increments of 0.1, while keeping the other thresholds constant. We found that a threshold of 0.7 yielded the best overall performance in terms of labeling accuracy.\nIt is important to note that these threshold values, while optimal in the context of our study and the datasets we evaluated, are configurable parameters within our system. Users can adjust these thresholds based on the specific characteristics of their datasets and the desired level of granularity in pattern extraction. This flexibility allows the approach to be tailored to various domains and datasets.\nWe also set the Pruner's evaluation data size to be 40% of the expanded labeled data. This value was determined to be optimal through experimentation with different sizes on the MSRBot dataset, with the same increment as the thresholds. Similar to the thresholds, the evaluation data size is a configurable parameter that can be adjusted by the user.\nThe process of applying labels to data, particularly in situations where different LFs produce conflicting labels, is a critical step in our approach. To systematically manage these conflicts, we adopt Snorkel's Majority Label Voting (MLV) strategy [27]. This choice is informed by manual evaluations and is in alignment with methodologies employed in previous studies [38, 71, 44]. The outcome is labeled data ready for NLU training.\n4.4 Performance Evaluation\nTo evaluate the performance of the generated LFs and NLU's performance, we also compute the widely used metrics of precision, recall, and F1-score. Precision is the percentage of correctly labeled queries to the total number of labeled queries for that intent (i.e., $Precision = \\frac{TP}{TP+FP}$). Recall is the percentage of correctly labeled queries to the total number of queries for that intent in the oracle (i.e., $Recall = \\frac{TP}{TP+FN}$). To have an overall performance of the generated LFs, we use the weighted F1-measure that has been used by prior work [72, 73]. More specifically, we aggregate the precision and recall using F1-score (i.e., $F1-score = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}$) for each class and aggregate all classes F1-measure using a weighted average, with the class' support as weights.\nFurthermore, we compute the Area Under the ROC Curve (AUC). AUC assesses the model's ability to distinguish between classes by considering the trade-offs between true and false positive rates [28, 74]. An AUC value above 0.5 suggests that the model is capable of classifying instances more accurately than random guessing, making it a robust metric for assessing performance in diverse classification scenarios. The significance of AUC is emphasized by its application in prior research, particularly in studies dealing with class imbalance [74, 29, 75, 76], including SE studies that involved analyzing imbalanced datasets [77, 78, 79].\nGiven that our selected datasets involve multiple classes, it is essential to adopt a strategy tailored for multiclass classification. For this purpose, we use the 'One vs Rest' (OvR) strategy, which breaks down the multiclass classification problem into individual binary classification tasks, focusing on distinguishing each class against all others. This method allows us to evaluate the model's performance for each class separately and then average these results to obtain an overall performance metric."}, {"title": "5 RESULTS", "content": "In this section", "RQ1": "How well do the generated LFs label data?\nMotivation: Prior work shows that the creation of effective LFs is a tedious", "20": ".", "queries.\nApproach": "To evaluate the performance of our generated LFs", "sets": "labeled", "13": ".", "follows": "n\u2022\\"}]}