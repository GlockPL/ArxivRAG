{"title": "Assessing Social Alignment: Do Personality-Prompted Large Language Models Behave Like Humans?", "authors": ["Ivan Zakazov", "Mikolaj Boronski", "Lorenzo Drudi", "Robert West"], "abstract": "The ongoing revolution in language modelling has led to various novel applications, some of which rely on the emerging \"social abilities\" of large language models (LLMs). Already, many turn to the new \"cyber friends\" for advice during pivotal moments of their lives and trust them with their deepest secrets, implying that accurate shaping of LLMs' \"personalities\" is paramount. Leveraging the vast diversity of data on which LLMs are pretrained, state-of-the-art approaches prompt them to adopt a particular personality. We ask (i) if personality-prompted models behave (i.e., \"make\" decisions when presented with a social situation) in line with the ascribed personality, and (ii) if their behavior can be finely controlled. We use classic psychological experiments-the Milgram Experiment and the Ultimatum Game-as social interaction testbeds and apply personality prompting to GPT-3.5/4/4o-mini/4o. Our experiments reveal failure modes of the prompt-based modulation of the models' \"behavior\", thus challenging the feasibility of personality prompting with today's LLMs.", "sections": [{"title": "Introduction", "content": "With both start-ups (Character.ai,\u00b9 Replika2) and industry giants (Snapchat,3 Meta4) providing \u201cdigital friends\" for millions of users, an accurate shaping of the underlying models' personalities is no longer the subject of sci-fi novels. Just as in real human-to-human interaction, there is no \"one size fits all\" personality bound to \"match\" with everyone. Hence, agents should be tailored to the needs of each user, i.e., their behavior should be alterable in a controllable way. The requirements for personalized AI-powered assistants will grow more strict as Large Language Models reach increasingly wider audiences and domains.\nAlthough several studies examine the possibility of prompt-driven personality induction in LLMs and claim success (Jiang et al. 2023b; Serapio-Garc\u00eda et al. 2023; Jiang et al. 2023a), the methods used to evaluate personalized\nWe argue that any test designed to assess the model's personality should be put in perspective with the considered use cases, e.g. while a personality-prompted model might be shown to answer consistently to simple questions such as \"Are you helpful and unselfish with others\u201d or \u201cDo you like to cooperate with others\", there are no guarantees that it will be a tough negotiator unless explicitly tested. Moreover, just like we do not qualitatively assess LLM's math capabilities and instead compute the accuracy of the model-provided solutions, we advocate for a quantitative benchmark, allowing for personality-prompted model behavior assessment.\nFollowing Aher, Arriaga, and Kalai (2023), we employ Ultimatum Game (UG; targets tolerance to unfair offers), and Milgram Experiment (ME, reflects obedience to authority) as the social interaction benchmarks. We note that both benchmarks allow for (i) quantitative behavior assessment and (ii) comparison with human data, as we know how the personality of the human participant relates to the behavior in these experiments (Mehta 2007; B\u00e8gue et al. 2015). To this end, we conduct 4 case studies, varying agreeableness or openness in UG; agreeableness or consciousness in ME.\nWe employ quantitative benchmarks to compare personality-prompted LLMs' behavior with human data and pose the following research questions:\nRQ1. Does the induced LLM behavior match the behavior of humans with the same personality?\nRQ2. Can we reliably steer LLM behavior, i.e., does prompting a larger value of a Big Five trait (Raad 2000) monotonically lead to the more pronounced manifestation of it?\nSurprisingly, we find the answers to both of these research questions to be negative. In fact, in 2 of the 4 case studies we conducted, the model's behavior change with the trait variation was the opposite to the trend observed in humans, which highlights the insufficient reliability of personality prompting."}, {"title": "Related work", "content": "Drawing on the personality assessment methodology, several studies (Jiang et al. 2023a; Serapio-Garc\u00eda et al. 2023; Sorokovikova et al. 2024) probe LLMs with the questionnaires designed for BIG-5 traits assessment, and show that stable personality emerges in the most capable models, e.g. GPT-3.5 (Jiang et al. 2023a) and Flan-PaLM 540B (Serapio-Garc\u00eda et al. 2023).\nFollowing that observation, Mao et al. (2024) suggest editing the personality of the model, while (Jiang et al. 2023a,b; Serapio-Garc\u00eda et al. 2023) induce desired personality with a carefully crafted prompt. The latter approach is especially appealing, given the cutting-edge models' black-box nature and the ability to switch between various personalities with no fine-tuning incurred computational overhead.\nRegarding subsequent validation, various papers extend beyond questionnaires and propose more elaborate ways to test personality-prompted models. Serapio-Garc\u00eda et al. (2023) generate social media updates, which are then analyzed with the Apply Magic Sauce API, providing a BIG-5 score corresponding to each update. Jiang et al. (2023b) request a personal story and evaluate the response with (i) Linguistic Inquiry and Word Count (LIWC) analysis, (ii) human evaluation, (iii) LLM evaluation.\nIn our view, Jiang et al. (2023a) provides a better proxy for real-life use cases, since the model, tasked with writing an essay, is conditioned on a particular social setup. Each essay is then human labeled for positive, negative, or neutral induction of each of BIG-5 traits. Human evaluation is, however, intrinsically qualitative and can be influenced by the writing style, instead of being purely content-dependent; the latter holds for the linguistic-based assessment methods as well. Besides, only extremes of each trait are induced, leaving the fine-grained trait tuning out of the scope.\nNoh and Chang (2024) consider various negotiations between the agents prompted by the extremes of the BIG-5 traits. Their focus is very different from ours, though, with no attempt to tune the behavior or ground the results in the human data. While we seek to test the alignment of the demonstrated behavior with the expected one, they empirically study the way that \"LLMs encode definitions\" of the traits reflected in \"their subsequent behavior\", focusing on the optimal negotiation performance."}, {"title": "Behavioral Experiments for humans and LLMs", "content": "With no relation to personality prompting, Aher, Arriaga, and Kalai (2023) successfully replicated the results of various behavioral experiments, including the Milgram Experiment (ME) and the Ultimatum Game (UG), by presenting these experiments to a \"silicon population\" of LLM instances conditioned on different names (a name corresponds to a single \"silicon sample\").\nWe know from psychology research that (i) in UG, Agreeableness and Openness are positively and significantly (p < 0.05) correlated with accepting an unfair offer (Mehta 2007) (ii) in ME, the intensity of the shock delivered is positively and significantly (p < 0.05) correlated with both Consciousness and Agreeableness (B\u00e8gue et al. 2015)."}, {"title": "Shaping Personality", "content": "We ascribe personality characteristics according to the assigned score of the trait (varies from 1 to 9), following Serapio-Garc\u00eda et al. (2023), where the personality string is shaped as follows:\n1. extremely {low adjective 1}, ..., extremely { low adjective N}\n2. very low adjective 1}, ..., very low adjective N}\n3. {low adjective 1}, ..., {low adjective N}\n4. a bit {low adjective 1}, ..., a bit {low adjective N}\n5. neither {low adjective 1} nor high adjective 1}, ..., neither {low adjective N} nor {high adjective N}\n6. a bit {high adjective 1}, ..., a bit {high adjective N}\n7. {high adjective 1}, ..., {high adjective N}\n8. very high adjective 1}, ..., very high adjective N}\n9. extremely high adjective 1}, ..., extremely { high adjective N}."}, {"title": "Methodology", "content": "In Ultimatum Game (G\u00fcth, Schmittberger, and Schwarze 1982), the proposer is given $10, and has to decide on the amount to be shared with the responder, who, in turn, might agree or block the deal . We shape various responders, varying levels of Agreeableness and Openness from 1 to 9. For each character of the responder, we run the simulation 50 times and measure the probability that the offer is accepted, depending on its value.\nWe try to retrieve an answer in the valid form (\"reject\" or \"accept\") 3 times and filter out the cases with no valid answer elucidated - 373 out of 25300 cases for GPT-4, 1 case for GPT-4o, 0 cases for GPT-40, GPT-40-mini.\nPersonality shaping is performed with the prompt formatted as \"For the following task, imagine that you are a person and you are: <personality characteristic>\"."}, {"title": "Milgram Experiment (ME)", "content": "In Milgram Experiment (Milgram 1963), the prompted model plays the role of the teacher, who is supposed to punish the learner for every wrong answer with a gradually growing electric shock.\nThe setup consists of the story-like blocks. We modify the \"(5) Stop?\" and the \"(6) Obey?\" steps due to the unavailability of log probabilities \u2013 Aher, Arriaga, and Kalai (2023) measure probabilities of \"not stop\" and \"not obey\" responses, respectively. Instead, we rely on the model acting as a judge, kept in the story-like scenario.\nPersonality is added to the block (1) Add Intro Text in the form of \"Teacher's personality traits are: <personality characteristic string>\".\nWe measure the final level at which the model decides to withdraw from the experiment , as well as the cumulative number of disobediences in different runs . Due to budget constraints, we limit the set of considered personalities to the extremes of Agreeableness and Consciousness and perform 50 runs for each personality.\nUnlike Aher, Arriaga, and Kalai (2023), we do not condition the model on the participant's name, as we are solely interested in the effect of the personality prompt. In contrast, the use of names may introduce a confounder. We, therefore, use a naming scheme of the experimenter - \"The Experimenter\", the teacher - \"The Teacher\", and the learner \"The Learner\" for each experiment run.\nWe note that the third-person naming scheme allows us to discard data leakage concerns, i.e., even if ME-related data was encountered on the pretraining stage (which is most probably the case), we elucidate an LLM's internal model of how The Teacher of a given personality would behave, not the psychology papers grounded opinion on what the morally right behavior is. This reasoning is solidified by the observation that, according to the experiments described below, teachers of a certain personality do not withdraw.\nHowever, this setup still involves an inherent limitation of the LLM-based systems \u2013 randomness. There are two potential points of failure: narration-following in block (4) Add LM Text, and known imperfect judge behavior (Zheng et al. 2023) in blocks (5) Stop?, and (6) Obey?. To address these"}, {"title": "Models", "content": "We run experiments with the following models:\n\u2022 gpt-3.5-turbo-0613 (GPT-3.5)\n\u2022 gpt-4-turbo-2024-04-09 (GPT-4)\n\u2022 gpt-4o-mini-2024-07-18 (GPT-4o-mini)\n\u2022 gpt-40-2024-05-13 (GPT-40)\nIn the case of Milgram's Experiment, we decided to drop results for both GPT-3.5 and GPT-40-mini. All 50 runs of baseline GPT-40-mini experiments were filtered due to unexpected response when the model was asked to act as a judge in blocks (5) Stop?, and (6) Obey?. We also encountered this problem, on a smaller scale, with GPT-40. GPT-3.5 struggled to follow the story-like narration while generating completions in block (4) Add LM Text. Interestingly, GPT-4 did not struggle with any of the above. The detailed number of filtered runs is presented in Table 1."}, {"title": "Results and Discussion", "content": "To set the baseline for personality-induced behavior, we run UG and ME with no personality specified. In UG, GPT-3.5 is more likely to reject the deal compared to the average across the human population (except for the case of a 0 offer), while GPT-4 shows the opposite behavior. Although GPT-40 and GPT-40-mini are more closely aligned with human studies, the transition between the model predominantly accepting and rejecting an offer is more sharp with the acceptance rate 0 for Offer < 2 and the acceptance rate 1 for Offer \u2265 4 .\nIn ME, \"vanilla\" GPT-4 is more obedient than the human average and follows the protocol of the experiment, while GPT-40 tends to withdraw early.\nWe note that in both UG and ME, results of Aher, Arriaga, and Kalai (2023) are much better aligned with the results of"}, {"title": "Ultimatum Game", "content": "Mehta (2007) (study 4, page 98) shows that Openness and Agreeableness are significantly correlated with accepting unfair offers in UG. To this end, we present personality-prompted LLM \"behavior\" in Fig. 5. To reveal the general trend exposed by these results, we model acceptance $y \\in \\{0,1\\}$ as\n$y(trait, o) = \\sum_{i=1}^{9} \\Theta_i x_i + \\Theta_{0} o + c = \\Theta_{trait} + \\Theta_{o} o + c,$\nwhere $o \\in [0, 1]$ is the normalized offer, $trait \\in [1,9]$ is the value of the trait, c is the bias term, and $x$ is one-hot-encoding of the corresponding trait value:\n$x_i = \\begin{cases} 1 & \\text{if } i = trait \\\\ 0 & \\text{if } i \\neq trait. \\end{cases}$ \nThe general trend in the $\\Theta_i$ values characterizes the relationship between an induced trait and behavior (RQ1), while the consistency of this trend is related to RQ2, i.e. our ability to enhance a certain behavior via prompting the corresponding trait with greater intensity.\nSurprisingly, while we observe the upward trend in the case of Agreeableness, it is downward for Openness for all the models considered, suggesting that a more \"open\" model is more prone to reject an offer, which opposes human data (Mehta 2007). Moreover, $\\Theta_i$ progression is not monotonic for any combination of the trait and the model, except for GPT-4, which is now obsolete (e.g. GPT-4o-mini, agreeableness, 5 to 7 progression; GPT-40, openness, 1 to 2 progression). These observations suggest negative answers to both RQ1 and RQ2, i.e. neither the human-aligned behavioral trend nor this trend being monotonic is guaranteed.\nTo provide a more detailed analysis of the models' steerability for the particular offers, we compute Acceptance Rate AR(trait) regressions and present the corresponding $R^2$ coefficients in Fig. 7. In case of agreeableness, we observe $R^2 < 0.6$ for the lower offers: 0, 1, 2 (GPT-3.5); 0 (GPT-4); 1, 2 (GPT-40-mini); 0 (GPT-40), suggesting lower steerability in these cases - either AR(trait) dependency is not monotonic (GPT-3.5, agreeableness, 0 offer), or AR surges/collapses at a certain trait value (GPT-4, agreeableness, 0 offer)."}, {"title": "Milgram Experiment", "content": "From B\u00e8gue et al. (2015), we know that both Agreeableness and Consciousness are significantly associated with the willingness to administer higher-intensity shocks. While the real-life trend does hold for Consciousness, it is on the borderline of statistical significance for GPT-4 (Welch's t-test, used throughout this section, yields $p = 0.06$ for GPT-4 and $p = 0.01$ for GPT-40).\nIn the case of Agreeableness, the results of our simulation drastically oppose human data . While low-agreeable samples almost never withdraw from the experiment, high-agreeable samples withdraw much earlier than personality-neutral samples ($p < 0.001$), the trend being even more pronounced for GPT-40. provides further insight into the course of the simulation \u2013 high-agreeable samples disobey much more than the low-agreeable ones, even if not withdraw from the experiment altogether.\nIntuitively, Agreeableness acts as a proxy for how \"good\" or \"evil\" the Teacher is. Teacher, when modelled as highly agreeable, is interpreted by the model as highly good, showing less desire to obey, while hurting the Learner - thus showing higher levels of disobedience. On the other hand, when we model the Teacher as least agreeable, it obeys blindly, showing no mercy for the suffering Learner. This trend is visible not only in both final levels achieved in each experiment run , but also in the cumulative number of disobediences when the Teacher showed hesitation in continuing the experiment , and total number of disobediences .\nGPT-4, and GPT-40 fail to align with the injected personalities when put in the complicated social context of Milgram Experiment. This provides further evidence towards the negative answer to RQ1."}, {"title": "Conclusion", "content": "Recognizing the elegance of the personality prompting technique (Serapio-Garc\u00eda et al. 2023; Jiang et al. 2023a), we argue for the insufficiency of existing methods designed for the evaluation of induced personality. To this end, we employ 2 psychological experiments \u2013 Milgram Experiment (ME) and Ultimatum game (UG) \u2013 to quantitatively assess the personality-induced LLMs' behavior in a social setting."}, {"title": "Limitations", "content": "We acknowledge that the experiments considered are still a proxy for real-life social interactions, and the models might behave differently in other set-ups.\nMoreover, truly aligning the agent's behavior with that of the humans might be impossible under the current set-up of \"summoning\" agents for a brief conversation, as they should rather be allowed to persist in the world for a long time with long-term goals and the prospect of pain and death."}]}