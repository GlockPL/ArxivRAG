{"title": "CityEQA: A Hierarchical LLM Agent on Embodied Question Answering Benchmark in City Space", "authors": ["Yong Zhao", "Kai Xu", "Zhengqiu Zhu", "Yue Hu", "Zhiheng Zheng", "Yingfeng Chen", "Yatai Ji", "Chen Gao", "Yong Li", "Jincai Huang"], "abstract": "Embodied Question Answering (EQA) has primarily focused on indoor environments, leaving the complexities of urban settings-spanning environment, action, and perception-largely unexplored. To bridge this gap, we introduce CityEQA, a new task where an embodied agent answers open-vocabulary questions through active exploration in dynamic city spaces. To support this task, we present CityEQA-EC, the first benchmark dataset featuring 1,412 human-annotated tasks across six categories, grounded in a realistic 3D urban simulator. Moreover, we propose Planner-Manager-Actor (PMA), a novel agent tailored for CityEQA. PMA enables long-horizon planning and hierarchical task execution: the Planner breaks down the question answering into sub-tasks, the Manager maintains an object-centric cognitive map for spatial reasoning during the process control, and the specialized Actors handle navigation, exploration, and collection sub-tasks. Experiments demonstrate that PMA achieves 60.7% of human-level answering accuracy, significantly outperforming frontier-based baselines. While promising, the performance gap compared to humans highlights the need for enhanced visual reasoning in CityEQA. This work paves the way for future advancements in urban spatial intelligence.", "sections": [{"title": "1 Introduction", "content": "Embodied Question Answering (EQA) (Das et al., 2018) represents a challenging task at the intersection of natural language processing, computer vision, and robotics, where an embodied agent (e.g., a UAV) must actively explore its environment to answer questions posed in natural language. While most existing research has concentrated on indoor EQA tasks (Gao et al., 2023; Pe\u00f1a-Narvaez et al., 2023), such as exploring and answering questions within confined spaces like homes or offices (Liu et al., 2024a), relatively little attention has been dedicated to EQA tasks in open-ended city space. Nevertheless, extending EQA to city space is crucial for numerous real-world applications, including autonomous systems (Kalinowska et al., 2023), urban region profiling (Yan et al., 2024), and city planning (Gao et al., 2024).\nEQA tasks in city space (referred to as CityEQA) introduce a unique set of challenges that fundamentally differ from those encountered in indoor environments. Compared to indoor EQA, CityEQA faces three main challenges:\n1) Environmental complexity with ambiguous objects: Urban environments are inherently more complex, featuring a diverse range of objects and structures, many of which are visually similar and difficult to distinguish without detailed semantic information (e.g., buildings, roads, and vehicles). This complexity makes it challenging to construct task instructions and specify the desired information accurately, as shown in Figure 1.\n2) Action complexity in cross-scale space: The vast geographical scale of city space compels agents to adopt larger movement amplitudes to enhance exploration efficiency. However, it might risk overlooking detailed information within the scene. Therefore, agents require cross-scale action adjustment capabilities to effectively balance long-distance path planning with fine-grained movement and angular control.\n3) Perception complexity with observation dynamics: Observations can vary greatly depending on distance, orientation, and perspective. For example, an object may look completely different up close than it does from afar or from different angles. These differences pose challenges for consistency and can affect the accuracy of answer generation, as embodied agents must adapt to the dynamic and complex nature of urban environments.\nAs an initial step toward CityEQA, we develop-"}, {"title": "2 CityEQA-EC Dataset", "content": "In this section, we outline the formulation of the EQA task and describe the dataset collection process for CityEQA-EC. To address real-world demands, such as urban governance and public services, we draw upon previous research (Majumdar et al., 2024; Das et al., 2018) to define six distinct task types. Examples and statistics of the dataset are presented in Figure 2.\n2.1 Task Formulation\nAn instance of the EQA task is defined by the 4-tuple: $\\S = (e, q, y, p_0)$, where e is the simulated or real 3D scene that agent can interact with, q is the question, and y is the ground truth answer. The $p_0$ denotes the agent's initial pose, including 3D position and orientation. Given the instance $\\S$, the goal is for the embodied agent (e.g., drones) to complete the task by gathering the required information from e and generating the answer $\\hat{y}$ in response to q. Specifically, the agent starts at the initial pose $p_0$ and interacts with the scene e step by step. At each time stept, the agent can move to a specific pose $p_t$, and obtain an observation $o_t = (I_{rgb}, I_d)$ from the scene, where $I_{rgb} \\in \\mathbb{R}^{H \\times W \\times 3}$ is the RGB image and $I_d \\in \\mathbb{R}^{H \\times W}$ is the depth image. Based on these observations, the agent generates the answer $\\hat{y}$. The key challenge is to produce a high-quality answer while minimizing the time steps required.\n2.2 Dataset Collection and Validation\nTo obtain a high-quality dataset, we employed the EmbodiedCity (Gao et al., 2024), which is a highly realistic 3D simulation platform based on the buildings, roads, and other elements in a real city. It is implemented using Unreal Engine 4 (Sanders, 2016) and Microsoft AirSim plugins (Shah et al., 2018). The collection process is to determine the 4-tuple elements $\\S = (e, p_0, q, y)$ of each instance. Unlike indoor simulators with many different scenes, EmbodiedCity is a coherent and extensive scene. As a result, for all instances, their scene e corresponds to EmbodiedCity.\nThe dataset collection process involves two steps, completed by five human annotators. The first step is raw Q&A generation, where raw questions and answers are created. The second step is task supplementation, which includes determining the agent's initial pose and and refining the question descriptions accordingly. Once these steps are completed, the dataset undergoes validation and filtering. \nRaw Q&A Generation We instructed human annotators to explore the EmbodiedCity environment freely and generate question-answer pairs based on their observations of RGB images. The raw questions $q_r$ and answers y are presented as open-vocabulary text. In addition to documenting the question-answer pairs, annotators were also required to record the pose $p_{obs}$ from which the RGB images were captured, along with the pose $p_{tar}$ of the target object referenced in each question. These information can be leveraged for a comprehensive evaluation of the agent's performance. After basic revision process, we have finally collected a total of 443 such instances, with each raw task instance denoted as $\\dot{q}'' = (q'', y, p_{obs}, p_{tar})$."}, {"title": "Task Supplementation", "content": "Building upon the raw task instances, we further established the agent's initial pose and refined the questions accordingly. For each raw task, the initial pose $p_0$ of the agent was set within a 200-meter range of the target object's pose $p_{tar}$. Given the complexity of urban environments, and to ensure that each expected answer is unique, we enriched the questions with descriptions based on landmarks.  For each raw task, we generated at least four distinct initial poses and transformed each raw question into at least four different inquiries. Ultimately, this process yielded a total of 2,212 task instances.\nDataset Validation Each task instance created by human annotators was rigorously evaluated by two independent human reviewers. These reviewers were responsible for determining whether the questions posed were answerable and clear, as well as verifying the uniqueness and accuracy of the target objects and their corresponding answers. Any task instance identified with issues was excluded. The final dataset comprises 1,412 task instances, with detailed statistics presented in Figure 2."}, {"title": "3 PMA: A Hierarchical LLM Agent for CityEQA Task", "content": "3.1 Overview\nAn overview of the proposed PMA agent for CityEQA tasks is shown in Figure 3. The PMA agent comprises three major modules: Planner, Manager, and Actor, all powered by pre-trained large models. The Planner is responsible for parsing the question q and formulating an executable plan before any actions are taken. The Manager serves as the core module, receiving structured information from the Planner and processing observations at each time step to maintain an object-centric cognitive map using an MM-LLM. Additionally, through a process control module, the Manager issues task instructions to the Actor, which then utilizes various action generators to execute the required responses. Once the plan is completed, the Manager generates an answer based on its accumulated memory.\n3.2 Planner Module\nThe question descriptions in CityEQA tasks contain extensive information, including several objects, spatial relationships, and the information that needs to be collected. To address the open-ended question descriptions, we leveraged pre-trained LLMs and designed a few-shot prompt that employs a three-step Chain of Thought (CoT) reasoning (Wei et al., 2022) to parse the question and formulate a plan.\nAs illustrated in Figure 3, all objects and spatial relationships mentioned in the question are first extracted. Simultaneously, the information necessary to answer the question is identified as corresponding requirements. Based on these requirements, a plan is created consisting of three distinct types of sub-tasks: (1) Collection sub-tasks gather the requisite information, (2) Exploration sub-tasks identify landmarks or target objects, and (3) Navigation sub-tasks enable efficient access to specific areas, thereby narrowing the exploration scope. To ensure the plan is executable,\n3.3 Manager Module\nThe Manager possesses the capability to oversee and manage the gradual implementation of long-term plans. This is made possible by its Memory module and Map module, which facilitate the organized storage of observations and track execution progress as the plan unfolds.\nObject-Centric Cognitive Map The object-centric cognitive map takes the initial pose of the agent as the origin, uses 2D grids to discretize the surrounding environment, and records the distribution of landmark objects based on grid indices. The map at time step t-1 is represented as $M_{t-1}=\\{obj_1, obj_2, ...\\}$, where the $obj_1$ and $obj_2$ are the object IDs corresponding to specific objects in the environment. At each time step t, the agent leverages egocentric observations represented as $o_t = (I_{rgb}, I_d)$ to construct the added map $m_t$ to record the landmark objects appeared at current observation, denoting as $m_t = Construct(o_t, p_t)$. To implement the functionality of Construct(), we utilized the Ground-SAM model (Bousselham et al., 2024) for grounding and segmenting landmark objects from $I_{rgb}$. By integrating pose information with depth data from $I_d$, we can obtain a 3D point cloud representation of these objects, subsequently projected onto 2D grids. After denoising and filtering, we obtained the finalized added map, denoted by $m_t$. The added map $m_t$ will be fused with the $M_{t-1}$ by merging the same object observed at different time steps, so objects are guaranteed to be unique"}, {"title": "The processing procedure of the function", "content": "$\\{MoveForward, MoveBack, MoveLeft, MoveRight, MoveUp, MoveDown, TurnLeft, TurnRight, Keep Still\\}$.\nbined with the camera intrinsic parameters to obtain 3D point cloud data. Then, these two data are merged to obtain the object-centric 3D point cloud. Further, this data is projected onto a 2D grid, and the point cloud data outside the map range is filtered out to obtain the object-centric 2D grids. Finally, objects with repetitive grids are fused to obtain the object-centric added map.\nThe purpose of the function Merge() is to fuse the added objects in added map into the global map. This is to ensure that the same object observed from different views is uniquely recorded and retrieved on the map. Therefore, for each added object, we first determine whether the distribution of the object overlaps or is adjacent to any object in the global map. If so, the two objects are merged; if not, the object is directly added to the global map. This paper adopts a simple and effective strategy to determine whether objects are adjacent: when at least one pair of grids in which the two objects are distributed are adjacent, they are considered to have an adjacent relationship. Additionally, it should be noted that multiple object merges may occur in the same round, so the merged object needs to be judged against all other objects in the global map in another round."}, {"title": "4 Experiment", "content": "4.1 Experiment setup\nEvaluation Metrics In CityEQA, we adopted three widely used metrics for evaluating EQA tasks (Das et al., 2018): Question Answering Accuracy (QAA), Navigation Accuracy (NA), and Mean Time Step (MTS). QAA assesses the correctness of the answers by comparing them to the ground truth. The open-vocabulary nature of the CityEQA task poses challenges for evaluation. Inspired by OpenEQA (Majumdar et al., 2024), we employed an LLM as the judge to assign scores $\\theta \\in \\{1, 2, ..., 5\\}$ to the answers.\nImplementation Details We employed GPT-40 as the MM-LLM for visual analysis, which was utilized in both the Explorer and Collector modules. Meanwhile, GPT-4 was adopted as the text analysis model, responsible for question parsing, plan generation, answer generation, and automated scoring. For each task, the object-centric cognitive map is constructed centered around the agent's initial pose, with a side length of 400 meters and a resolution of 1 meter. We considered buildings as landmarks and accounted for four spatial relationships: north, south, east, and west. Additionally, we limited the total number of time steps for navigation and exploration to 50 steps and restricted the number of steps for collection to 10 steps.\nBaselines Our guiding principle is to investigate how to use foundation models to complete CityEQA tasks without any additional fine-tuning. Therefore, we employed four baselines that are widely employed in the studies of EQA tasks. \nBlind Agents generate answers based solely on the text of questions without obtaining any visual inputs. It serves as a reference for assessing the extent to which one can rely purely on prior world knowledge and/or random guessing (Majumdar et al., 2024).\nLLM-VQA bypasses the active exploration process and is directly provided with the RGB image obtained from the $p_{obs}$ to answer the questions. This approach aims to assess the visual perception and reasoning capabilities of MM-LLMs in urban environments, while eliminating the interference of embodied actions.\nFrontier-Based Exploration (FBE) Agent, commonly used indoorbaseline (Renet al., 2024), does not utilize landmarks or spatial relationships.\nHuman Agents are employed to establish human-level performance metrics on our benchmark. We categorized human agents into two types, H-VQA and H-EQA. H-VQA is directly provided with an RGB image to perform Visual VQA tasks, similar to the setup of LLM-VQA. H-EQA launches from the initial pose and actively explores the environment based on the question description to find the answer.\n4.2 Comparison with Baselines\n4.3 Study on Collector Module\nPrevious experimental results have confirmed the effectiveness of navigation and exploration strategies in PMA. Therefore, in this section, we aim to investigate the impact of fine-grained adjustments in observations on performance. To achieve this, we recorded the Collector's pose at each step (up to 10 steps) along with the generated responses and calculated relevant metrics."}, {"title": "5 Related Works", "content": "5.1 EQA Datasets\nEarly research on using language to guide perception from visual input is known as Visual Question Answering (VQA) (Ishmam et al., 2024; Guo et al., 2023). VQA tasks require agents to answer questions based solely on provided visual information (images or videos) (Chandrasegaran et al., 2024). In contrast, Embodied Question Answering (EQA) involves agents actively navigating within an environment to seek visual inputs and enhance answer reliability (Das et al., 2018). Due to cost and hardware limitations, several virtual indoor simulators have been developed for EQA tasks (Liu et al., 2024a), resulting in indoor-focused datasets such as EQA-v1 (Das et al., 2018) and MT-EQA (Yu et al., 2019). Recently, urban environment simulators like EmbodiedCity (Gao et al., 2024), CityNav (Lee et al., 2024), and AerialVLN (Liu et al., 2023) have emerged, though they mainly focus on navigation. EmbodiedCity provides an urban EQA dataset, but it functions more like VQA, as shown in Table 1. Moreover, due to the limited generalization capabilities of models at the time, only simple questions about basic attributes of objects were considered in these indoor datasets(Ren et al., 2024). However, with the continuous improvement in the understanding and reasoning capabilities of pre-trained MM-LLMs for visual inputs, several open-ended EQA datasets have recently been released, such as K-EQA (Tan et al., 2023) and OpenEQA (Majumdar et al., 2024).\nIn comparison, this paper is the first to study the EQA tasks in city space and introduces the benchmark CityEQA-EC \u2014- a high-quality dataset featuring diverse, open-vocabulary questions.\n5.2 LLMs-driven Embodied Agents\nThe indoor EQA tasks mainly involve exploration and answer generation sub-tasks (Ren et al., 2024). In early work(Duan et al., 2022; Das et al., 2018; Lu et al., 2019), the two sub-tasks are mainly addressed by building and fine-tuning various deep neural networks. Recently, researchers attempt to utilize pre-trained LLMs to solve EQA tasks without any additional fine-tuning(Mu et al., 2024; Xiang et al., 2024; Huang et al., 2024). OpenEQA employed a Frontier-Based Exploration (FBE) strategy for indoor environment exploration and tested the performance of various MM-LLMs on the answer generation (Majumdar et al., 2024). Besides, MM-LLMs was also used to determine which room to explore in indoor environment based their commonsense reasoning capabilities (Yin et al., 2025). These agents, however, cannot be directly used for CityEQA tasks. Unlike indoor spaces, which are confined and divided into rooms, city spaces are vast and open. Agents in cities must navigate using landmarks and spatial relationships for long-term exploration (Zeng et al., 2024; Liu et al., 2024b). The proposed PMA addresses this by breaking down and planning for long-horizon CityEQA tasks, using large models across multiple modules to effectively handle open-ended questions and unseen environments."}, {"title": "6 Conclusion", "content": "This paper pioneers the exploration of EQA tasks in outdoor urban environments. First, we introduced CityEQA-EC, the inaugural open-ended benchmark for CityEQA, comprising 1,412 tasks divided into six distinct categories. Second, we proposed a novel agent model (the PMA), designed to tackle long-horizon tasks through hierarchical planning, sensing, and execution. Experimental results validated the effectiveness of PMA, achieving 60.73% accuracy relative to human performance and outperforming traditional methods such as the FBE Agent. Nevertheless, challenges remain, including efficiency discrepancies (24.44 vs. 9.31 mean time steps taken by humans) and limitations in visual thinking capabilities. Future research could focus on enhancing PMA with self-reflection and error-correction mechanisms to mitigate error accumulation that can arise in long-horizon tasks."}, {"title": "7 Limitations", "content": "The work primarily focuses on object-centric question-answering tasks, such as identifying specific objects (e.g., buildings, vehicles) within city spaces. Further, while our approach is effective for tasks involving static physical entities, it overlooks the importance of social interactions and dynamic events, which are also critical in urban settings. For instance, questions related to dynamic events (e.g., \"Is there a traffic jam on Main Street?\"), or environmental conditions (e.g., \"Is the park crowded right now?\") are not considered up to now. These types of questions require some different sets of reasoning capabilities, such as temporal reasoning, event detection, and social context understanding, which are not currently supported by the Planner-Manager-Actor (PMA) agent. Future work should expand the scope of CityEQA to include these non-entity-based tasks, further extending PMA and enabling embodied agents to handle a broader range of urban spatial intelligence challenges."}, {"title": "8 Ethics Statement", "content": "In the data collection, we ensure there is no identifiable information about individuals (faces, license plates) or private properties. Thus, there is no ethical concern."}, {"title": "A Appendix", "content": "A.1 Dataset Collection and Validation\nThe collection and validation process of the CityEQA-EC dataset is shown in Figure 6, including Initialization (Step 1), Raw Q&A Generation (Step 2 to 4), Task Supplementation (Step 5 to 6), and Dataset Validation (Step 7).\nIn the initialization phase, human annotators were provided with comprehensive briefings and training, during which they were introduced to six distinct types of tasks. Subsequently, in the raw question-and-answer generation stage, annotators were randomly placed within the environment, allowing them to move freely and explore in order to generate questions and answers. Additionally, both the target pose $p_{tar}$ and observed pose $p_{obs}$ were recorded manually. Then, each question-answer pair was then reviewed by two additional annotators to identify specific issues: (1) Task Duplication, indicating that a similar instance had already been collected; (2) Task Invalidity, meaning that there was no match between the question and answer based on the image. Any tasks identified as problematic were discarded. Furthermore, to ensure the accuracy of pose annotations, we randomly selected 20% of raw task examples for two rounds of verification regarding their pose annotations.\nIn the task supplementation phase human annotators were asked to add the initial pose for the task and expand the question. Buildings are primarily used as landmark objects to expand the question. Then, in the validation stage, each task was independently evaluated by two human reviewers. The details of the review policy are as follows:\nSpelling and grammar check is conducted.\nThe target object must be uniquely identifiable based on descriptions of landmarks and spatial positions.\nThe distance between the initial pose and the target pose must be less than 200 meters.\nThe initial pose is located at a movable position rather than within an obstacle.\nAny tasks identified as problematic were removed.\nA.2 PMA Agent Details\nDetails of Planner We present the detailed CoT used by the Planner here."}]}