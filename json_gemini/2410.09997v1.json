{"title": "COLLU-BENCH: A BENCHMARK FOR PREDICTING\nLANGUAGE MODEL HALLUCINATIONS IN CODE", "authors": ["Nan Jiang", "Qi Li", "Lin Tan", "Tianyi Zhang"], "abstract": "Despite their success, large language models (LLMs) face the critical challenge of\nhallucinations, generating plausible but incorrect content. While much research\nhas focused on hallucinations in multiple modalities including images and natu-\nral language text, less attention has been given to hallucinations in source code,\nwhich leads to incorrect and vulnerable code that causes significant financial loss.\nTo pave the way for research in LLMs' hallucinations in code, we introduce Collu-\nBench, a benchmark for predicting code hallucinations of LLMs across code gen-\neration (CG) and automated program repair (APR) tasks. Collu-Bench includes\n13,234 code hallucination instances collected from five datasets and 11 diverse\nLLMs, ranging from open-source models to commercial ones. To better under-\nstand and predict code hallucinations, Collu-Bench provides detailed features such\nas the per-step log probabilities of LLMs' output, token types, and the execution\nfeedback of LLMs' generated code for in-depth analysis. In addition, we con-\nduct experiments to predict hallucination on Collu-Bench, using both traditional\nmachine learning techniques and neural networks, which achieves 22.03 \u2013 33.15%\naccuracy. Our experiments draw insightful findings of code hallucination patterns,\nreveal the challenge of accurately localizing LLMs' hallucinations, and highlight\nthe need for more sophisticated techniques.", "sections": [{"title": "1 INTRODUCTION", "content": "Despite the great potential and impressive success of LLMs (Touvron et al., 2023; Brown et al., 2020;\nLi et al., 2022a; OpenAI, 2024), a known issue of LLMs is hallucination, a phenomenon where the\nmodel generates fluent and plausible-sounding but unfaithful or fabricated content (Ji et al., 2023).\nThe hallucination issue poses a significant risk when deploying LLMs in real-world applications\nthat require precise information (Puchert et al., 2023). Due to this importance, researchers have\ndeveloped benchmarks such as TruthfulQA (Lin et al., 2022), FELM (chen et al., 2023), and HaluE-\nval (Li et al., 2023b) to understand and predict hallucinations of LLMs. Additionally, researchers\nare actively exploring methods to mitigate hallucinations (Liu et al., 2024b; Elaraby et al., 2023;\nDhuliawala et al., 2023; Yan et al., 2024).\nAnother domain where LLMs have been widely applied is source code. LLMs are used in many\ncode-related applications, such as code generation (Wang et al., 2023; Li et al., 2023a; Guo et al.,\n2024; Lozhkov et al., 2024; Rozi\u00e8re et al., 2024; Luo et al., 2024; Wei et al., 2023; Hui et al., 2024),\nautomated program repair (Hossain et al., 2024; Ruiz et al., 2024; Silva et al., 2023; Jimenez et al.,\n2024; Jiang et al., 2023; Xia et al., 2023), and software engineering agents (OpenAI, 2024; Yang\net al., 2024; Zhang et al., 2024). Unfortunately, in the code domain, LLMs also face the risk of\nhallucination, such as generating misused Application Programming Interfaces (APIs), insufficient\nerror handlers, or even vulnerable code. Such hallucinations can cause the breakage of code bases,\nthe shutdown of services, exploitation of vulnerabilities, and eventually lead to huge financial costs 1.\nAlthough important, hallucination in code is much less explored compared to that in natural lan-\nguage text and images. Some existing work explores the API misuse issue (Zhong & Wang, 2024),"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 TEXT AND IMAGES HALLUCINATION BENCHMARKS", "content": "Hallucination in natural language generation (NLG) refers to the phenomenon where models gener-\nate text that is fluent but factually incorrect or inconsistent with the input data. Several benchmarks"}, {"title": "2.2 CODE HALLUCINATION BENCHMARKS", "content": "HalluCode (Liu et al., 2024a) explores hallucinations in the context of code generation. It introduces\na comprehensive taxonomy of hallucinations specific to LLM-powered code generation, categoriz-\ning them into five primary types. The authors conducted a thematic analysis of LLM-generated\ncode to classify hallucinations based on deviations from user intent, internal inconsistencies, and\nmisalignment with factual knowledge. The benchmark evaluates LLMs' ability to recognize and\nmitigate hallucinations, revealing that current models face significant challenges.\nCodeHalu (Tian et al., 2024) focuses on investigating code hallucinations through execution-based\nverification. The authors categorize code hallucinations into four main types: mapping, naming,\nresource, and logic hallucinations, each of which highlights unique challenges in code generation.\nCodeHalu presents a dynamic detection algorithm to detect and quantify hallucinations and intro-\nduces the CodeHaluEval benchmark, which includes a large set of samples to evaluate LLM perfor-\nmance in code generation.\nCollu-Bench differs from both HalluCode and CodeHalu in two key aspects. First, Collu-Bench\nfocuses on identifying where the hallucination occurs by pinpointing the exact token at which the\nmodel first deviates from the expected output. Second, Collu-Bench provides additional signals,\nsuch as the types of generated tokens, helping researchers better understand the underlying patterns\nof code hallucinations."}, {"title": "3 BENCHMARK CONSTRUCTION", "content": "In this section, we describe the collection process of Collu-Bench. We first describe our automated\npipeline of handling program equivalency and identifier viability, which helps in collecting accurate\nhallucination token locations in Collu-Bench (Section 3.1). Then we introduce the selected datasets\nand LLMs in Section 3.2. Section 3.3 shows the process of using LLMs to generate outputs and\ncollect the hallucination token index automatically. Lastly, in Section 3.4, we explain the additional\nsignals Collu-Bench includes, that could help localize hallucination tokens in LLM-generated code."}, {"title": "3.1 HANDLING CODE EQUIVALENCE AND VARIATION", "content": "A standard approach for localizing the hallucinated token is to compare the generated solution with\nthe canonical solution. However, simply comparing the canonical solution and the generated code\ncan lead to many false positives, since the LLM may follow an alternative way to solve the task (Li\net al., 2022b; Austin et al., 2021; Chen et al., 2021). For instance, the task of sorting a list of integers\ncan be implemented with many different sorting algorithms. Even semantically equivalent solutions\nmay have a range of syntactic variations, e.g., naming variables differently, using a for loop instead\nof a while loop, etc.\nExisting hallucination benchmarks in natural language or vision domains although face similar chal-\nlenges of diversity in text, they can manually annotate the hallucinations in text or images. Compared"}, {"title": "i). Diverse Canonical Solution Collection:", "content": "For each problem in the dataset, besides the official\ncanonical solutions, we enhance the diversity of canonical solutions by using LLMs to sample more.\nFor the CG task, due to the simplicity of coding problems in HumanEval and MBPP, there could\nbe lots of different algorithms solving the problems correctly. To cover the equivalent canonical so-\nlutions as much as possible, we let each LLM (DeepSeek-Coder-1.3b/6.7b, StarCoder2-3b/7b/15b,\nCodeLlama-7b/13b, Llama3-8b, and GPT-40-mini) sample 100 programs per problem, using a tem-\nperature of 0.8. These sampled programs are run against EvalPlus for evaluation of correctness, and\nthose that pass all the test cases are considered equivalent canonical solutions.\nFor the APR task, we conduct the same sampling process (i.e., each LLM sample 100 outputs per\nrepair problem and run against test cases) for the HumanEval-Java dataset to collect canonical solu-\ntions, given its simplicity. For Defects4J and SWE-Bench, since (1) the program repair problems in\nthese two datasets are much more complex and thus are less likely to have many diverse equivalents,\nand (2) their execution of test cases are computationally expensive, we do not conduct sampling\nand only consider the developer fix provided in the datasets, as well as LLM-generated fixes using\ngreedy decoding that pass all the test cases, as the canonical solutions."}, {"title": "ii). Program Normalization:", "content": "Collecting diverse canonical solutions is effective in covering cor-\nrect programs implemented with different algorithms or logic. However, it cannot account for the\nlimitless variants of identifier names that can be used within the same program. For example, \"for\nx, y in zip(tup1, tup2)\" and \"for a, b in zip(tup1, tup2)\" are logically equivalent\nbut differ textually due to the use of different identifier names. Thus, we conduct program normal-\nization to replace all the user-defined identifiers with normalized names so that different choices of\nidentifier names will not be considered hallucinations.\nWe use tree-sitter (Brunsfeld et al., 2024), a static parser, to parse the generated code into AST,\nand walk through the AST to collect all the user-defined identifiers. Details can be found in Ap-\npendix A.1. After collecting a set of unique user-defined identifiers from a program generated by\nan LLM (e.g., collecting the identifiers {a, b} from the code snippet \"for a, b in zip(tup1,\ntup2)\", which is a \u201cfor statement\" in Python), we rename these identifiers sequentially as v1,\nv2, and so on, to normalize the program. For instance, a is replaced by v1 and b is replaced by\nv2, thus code snippet \u201cfor a, b in zip(tup1, tup2)\" is normalized into \u201cfor v1, v2 in\nzip(tup1, tup2)\". During this step, the logically equivalent programs with different identifier\nnames will be normalized into the same program.\""}, {"title": "3.2 DATASETS AND LLMS", "content": "We target two code-related tasks in Collu-Bench: code generation (CG) and automated program\nrepair (APR). In total, we select five datasets to build the benchmark.\nCode generation (CG): Code generation is the task of automatically producing code from natural\nlanguage descriptions. It plays a crucial role in software development by improving productivity\nand enabling non-programmers to create code through high-level specifications. It is widely used to\nevaluate the coding capability of LLMs. We use the following CG datasets to build Collu-Bench:\n\u2022 MBPP (Austin et al., 2021): MBPP is a code generation benchmark comprised of hand-written\nproblems solvable by entry-level Python programmers. We use the sanitized version from\nEvalPlus (Liu et al., 2023) which contains 343 problems.\n\u2022 HumanEval (Chen et al., 2021): The HumanEval benchmark contains 164 hand-written Python\nprogramming problems with function signatures, docstrings, and unit tests.\nAutomated Program Repair (APR): Automated program repair is the process of automatically\nfixing bugs in software programs, which can significantly reduce the time and effort required for\nmanual debugging and repair. We use the following APR datasets to build Collu-Bench:"}, {"title": "3.3 GENERATION AND AUTOMATED HALLUCINATION LOCALIZATION", "content": "Figure 1 illustrates the generation step that collects the LLMs' outputs for given coding or repairing\nproblems, and the hallucination token localization step which automatically calculates the index of\nthe first generated hallucination token.\nCode Generation: For each sample in the datasets (HumanEval, MBPP, etc.), we let each LLM gen-\nerate one solution code using few-shot prompting (Brown et al., 2020) and greedy decoding. Details\nand examples of the prompt we used to collect LLMs generated code are provided in Appendix A.2.\nLocalization of Hallucinated Tokens: This step collects the hallucination token indices from the\nincorrect generate code by normalizing it and comparing it with the large, diverse set of canonical\ncode (Section 3.1), as these will be the targets of Collu-Bench. Specifically, we compare the LLM-\ngenerated program with canonical solutions to decide the hallucination location. We normalize the\ngenerated code and compare it with each normalized solution one by one. Non-indentation white\nspace in Python programs and all white space in Java programs are ignored during the comparison as\nthey do not affect functionality. The first different character is mapped back to the original generated\ncode before normalization to locate the token where this mismatched character is from.\nFor instance, in the example shown in Figure 1, the normalized LLM-generated program \u201creturn\nall (v1 < v2 for v1, v2 in zip (tup1, tup2))", "return all(v1 > v2 for v1, v2 in zip(tup1, tup2))\" at character \u201c<": "n(highlighted in red). This character maps to the same \u201c<\u201d in the original LLM-generated code\n\"return all(x < y for x, y in zip(tup1, tup2))\", which is the fifth-generated token\nby LLM. As a result, the hallucination token index for this example is 5."}, {"title": "3.4 COLLECTION OF ADDITIONAL SIGNALS FOR HALLUCINATION LOCALIZATION", "content": "In addition to the raw generated output, we collect additional signals that could be relevant to hal-\nlucination, i.e., per-step log probabilities provided by the LLMs, types of generated tokens, and the\nerror messages of executing the incorrect program.\nPer-step Log Probabilities: Log probabilities can be obtained during the generation process\nthrough LLMs' inference API. The log probs. show the LLMs' confidence level at the corresponding\ndecoding step. We collect the log probs. of the top 100 tokens at each step.\nToken Types: In programming languages, each token can belong to different categories based on\nits role in the code, which is analogous to parts of speech in natural language. We categorize tokens\nof different types to provide code-specific information.\nTo determine the token types, we parse the code into an abstract syntax tree (AST), where each node\nhas its node type that we use to decide the token type. We classify code tokens, based on AST node\ntypes, into the following categories: Keyword, Delimiter, Operator, Constant, Identifier,\nand Type Identifier. Besides, we also add two additional types: Space for the white space\ntokens and <EOS> for the end-of-sequence token (a token that marks the end of generation). Figure 2\nshows examples of these token types in Java and Python programs."}, {"title": "4 BENCHMARK ANALYSIS", "content": "We present the statistics and analysis of Collu-Bench and show some key findings in this section.\nCollu-Bench contains 13,234 instances, each with an LLM-generated code, parsed token types, per-\nstep log probs., execution error messages, and the hallucination token index as target (code without\nhallucination is not included)."}, {"title": "4.1 ANALYSIS AND FINDINGS", "content": "LLMs are less confident when hallucinating. Figure 3 shows the probability distributions of cor-\nrect tokens and hallucinated tokens. (a) shows that for all the LLMs, the hallucinated tokens tend to\nhave a lower probability than the correct tokens. GPT-40-mini is much more confident than other\nLLMs when they are hallucinating. (b) shows that the code tokens generated for different datasets\nand tasks still hold the same pattern. Code tokens generated for the HumanEval-Java dataset overall\nhave a higher probability (for both correct and hallucinated ones) than those for other datasets. Hal-\nlucinated tokens generated for CG datasets overall have a lower probability than hallucinated\ntokens generated for APR datasets. (c) shows the probability distribution of correct and halluci-\nnated tokens with different types. Keyword is the only type that probability distributions of correct\nand hallucinated tokens overlap the most, suggesting LLMs are least confident when generating\nkeywords. And the hallucinated EOS tokens have the highest probability, suggesting LLMs tend to\nstop generation confidently, even at incorrect places."}, {"title": "4.2 ERROR RATE", "content": "Collu-Bench employs the proposed pipeline (Section 3.1) to automatically identify the first halluci-\nnation token as the target. This may not always align perfectly with human developer annotations.\nTo assess the accuracy, we randomly selected 100 samples from Collu-Bench and asked two de-\nvelopers to review the hallucination tokens in the LLM-generated code. The developers disagreed\nwith the identified hallucination tokens in 14 samples and concurred with that of the remaining 86\nsamples. We then further checked the 14 samples that the developers consider mislabeled and found\nthey were all due to missing a more extensive set of equivalent canonical solutions.\nGiven the difficulty of identifying code equivalency, it is impossible to exhaustively find and consider\nall the canonical solutions. Without the proposed solution in Section 3.1, there would only be\n57 samples matching the developers' annotation using a simple string match or token match (i.e.,\n43% error rate). We sample diverse canonical solutions and use program normalization to handle\nidentifier variability, which reduces the error rate of data labeling significantly."}, {"title": "5 PRELIMINARY RESULTS OF HALLUCINATION PREDICTION", "content": "Collu-Bench can be used to train and evaluate code hallucination localization methods. We formu-\nlate the task of code hallucination localization as follows: given a code generated by an LLM, which"}, {"title": "5.1 PER-TOKEN PREDICTION", "content": "We conduct experiments using traditional machine learning (ML) techniques including Support Vec-\ntor Classifier (SVC), Ada Boost Classifier (AB), Random Forest Classifier (RF), Gradient Boosting\nClassifier (GB), and Multi-layer Perceptron (MLP). For each token, the considered features include\nthe top 100 probability distribution, the token type (in a one-hot vector), and the token index in\nthe LLM-generated code. Table 2 shows the accuracy of hallucination token index prediction using\ndifferent models, under the first two data-split settings. We find in general, RF produces higher\naccuracy than SVC, AB, GB, and MLP. When training separate prediction models per dataset,\nthe model (train and test) on SWE-bench produces much higher accuracy than other datasets, and\nthe model on HumanEval produces the worst accuracy, which suggests that LLMs have different\npatterns in hallucination when generating code for different task or dataset."}, {"title": "5.2 PER-SAMPLE PREDICTION", "content": "For per-sample prediction, we conduct experiments using the same three settings. The predictors\ntake a list of tokens in the LLM-generated code, the feature of each token includes the top 100 prob-\nabilities and token type in a one-hot vector. The predictors encode the token list using CNN (Lecun\net al., 1998), RNN, LSTM (Hochreiter & Schmidhuber, 1997) or GRU (Cho et al., 2014)), or Trans-\nformer (Vaswani et al., 2017) layers to produce hidden states for each token. The hidden states of\nthe token list are fed to a pointer network (Vinyals et al., 2017; Hossain et al., 2024) to select the\nfirst hallucination token from the list."}, {"title": "6 LIMITATION", "content": "One limitation is the errors in the target hallucination token index provided in Collu-Bench, which is\ndetermined by an automated pipeline and thus is non-perfect. Compared with simple string match-\ning or token matching, we sample diverse canonical solutions and apply program normalization to\nhandle the equivalency and identifier variability of code to increase the accuracy of the hallucination\ntoken index in Collu-Bench significantly. It is non-trivial to find an automated solution to determine\nthe hallucination in code perfectly, which remains to be explored.\nAnother limitation is the range of select LLMs and datasets to build Collu-Bench. There exist lots of\ndifferent LLMs and code generation or program repair datasets, we select the set of state-of-the-art,\nwidely-used LLMs (including DeepSeekCoder series, CodeLlama series, StarCoder2 series, Llama3\nseries, and GPT-40-mini), and dataset. Overall, Collu-Bench's 13,234 data samples come from 11\nLLMs' output on five datasets. Studying the hallucination of more LLMs and datasets can be an\ninteresting future work."}, {"title": "7 CONCLUSION", "content": "This work presents Collu-Bench, a challenging benchmark for code hallucination localization.\nCollu-Bench includes 13,234 hallucination instances generated by 11 diverse LLMs on two im-\nportant code tasks, offering a comprehensive evaluation of hallucination localization across multiple\nmodels. Collu-Bench also provides additional information such as per-step log probs. produced\nby LLMs, types of generated tokens, and execution feedback as useful signals for predicting code\nhallucinations. Through extensive experiments using traditional machine learning techniques and\nneural network models as hallucination predictors, we provide an in-depth study of hallucination lo-\ncalization using Collu-Bench. The preliminary results reveal that traditional ML methods and neural\nnetworks can only achieve an accuracy of up to 33.15%, highlighting the complexity of this task,\nand underscoring the need for further research in improving the trustworthiness and reliability of\nLLMs in code-related applications."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 DETAILS OF PROGRAM NORMALIZATION", "content": "Table 6 lists the AST nodes in Python and Java languages that refer to code containing user-defined\nidentifiers. The underscored identifiers are those we collected in each example.\nOn average, after sampling diverse canonical solutions and normalizing program, we collected\n82.01, 50.01, 5.54, 1.31, and 1.53 unique normalized canonical solutions per problem in Hu-\nmanEval, MBPP, HumanEval-Java, Defects4J, SWE-Bench."}, {"title": "A.2 FEW-SHOT PROMPTING DESIGN", "content": "Figures 4 and 5 show the few-shot prompts we used during the collection of LLMs' outputs. For the\ncode generation task, we follow the prompt format in HumanEval that provides the task description\nand example inputs and outputs as a doc-string inside the function signature.\nFor the automated program repair task, we provide the task description which is important to under-\nstand the intention of the function. The original buggy code is enclosed by <bug> and </bug> to\nseparate from the surrounding context. The LLMs are only required to generate the corresponding\nfixed code to replace the buggy code."}]}