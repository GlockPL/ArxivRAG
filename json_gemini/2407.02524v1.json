{"title": "Meta Large Language Model Compiler: Foundation Models\nof Compiler Optimization", "authors": ["Chris Cummins", "Volker Seekert", "Dejan Grubisic", "Baptiste Rozi\u00e8re", "Jonas Gehring", "Gabriel Synnaeve", "Hugh Leather"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of\nsoftware engineering and coding tasks. However, their application in the domain of code and\ncompiler optimization remains underexplored. Training LLMs is resource-intensive, requiring\nsubstantial GPU hours and extensive data collection, which can be prohibitive. To address this\ngap, we introduce Meta Large Language Model Compiler (LLM COMPILER), a suite of robust,\nopenly available, pre-trained models specifically designed for code optimization tasks. Built\non the foundation of CODE LLAMA, LLM COMPILER enhances the understanding of compiler\nintermediate representations (IRs), assembly language, and optimization techniques. The\nmodel has been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code\nand has undergone instruction fine-tuning to interpret compiler behavior. LLM COMPILER\nis released under a bespoke commercial license to allow wide reuse and is available in two\nsizes: 7 billion and 13 billion parameters. We also present fine-tuned versions of the model,\ndemonstrating its enhanced capabilities in optimizing code size and disassembling from x86_64\nand ARM assembly back into LLVM-IR. These achieve 77% of the optimising potential of an\nautotuning search, and 45% disassembly round trip (14% exact match). This release aims to\nprovide a scalable, cost-effective foundation for further research and development in compiler\noptimization by both academic researchers and industry practitioners.", "sections": [{"title": "Introduction", "content": "There is increasing interest in large language models (LLMs) for software engineering tasks including\ncode generation, code translation, and code testing. Models such as StarCoder (Lozhkov et al., 2024),\nCODE LLAMA (Rozi\u00e8re et al., 2023), and GPT-4 (OpenAI, 2023) have a good statistical understanding of\ncode and can suggest likely completions for unfinished code, making them useful for editing and creating\nsoftware. However, there is little emphasis on training specifically to optimize code. Publicly available LLMs\ncan be prompted to make minor tweaks to a program such as tagging variables to be stored as registers, and\nwill even attempt more substantial optimizations like vectorization, though they easily become confused and\nmake mistakes, frequently resulting in incorrect code.\nPrior works on machine learning-guided code optimization have used a range of representations from hand-\nbuilt features (Wang & O'Boyle, 2018) to graph neural networks (GNNs) (Liang et al., 2023). However, in\nall cases, the way the input program is represented to the machine learning algorithm is incomplete, losing\nsome information along the way. For example, Trofin et al. (2021) use numeric features to provide hints for\nfunction inlining, but cannot faithfully reproduce the call graph or control flow. Cummins et al. (2021) form\ngraphs of the program to pass to a GNN, but exclude the values of constants and some type information\nwhich prevents reproducing instructions with fidelity.\nIn contrast, LLMs can accept source programs, as is, with a complete, lossless representation. Using text\nas the input and output representation for a machine learning optimizer has desirable properties: text is a"}, {"title": "Overview", "content": "Figure 1 shows an overview of our approach. LLM COMPILER models target compiler optimization. They\nare available in two model sizes: 7B and 13B parameters. The LLM COMPILER models are initialized\nwith CODE LLAMA model weights of the corresponding size and trained on an additional 546B tokens of\ndata comprising mostly compiler intermediate representations and assembly code. We then further train\nLLM COMPILER FTD models using an additional 164B tokens of data for two downstream compilation\ntasks: flag tuning and disassembly. At all stages of training a small amount of code and natural language\ndata from previous stages is used to help retain the capabilities of the base CODE LLAMA model."}, {"title": "LLM Compiler: Specializing Code Llama for compiler optimization", "content": ""}, {"title": "Pretraining on assembly code and compiler IRs", "content": "The data used to train coding LLMs are typically composed largely of high level source languages like Python.\nAssembly code contributes a negligible proportion of these datasets, and compiler IRs even less. To build an LLM\nwith a good understanding of these languages we initialize LLM COMPILER models with the weights of CODE LLAMA\nand then train for 401 billion tokens on a compiler-centric dataset composed mostly of assembly code and compiler\nIRs, shown in Table 1."}, {"title": "Instruction fine-tuning for compiler emulation", "content": "To understand the mechanism of code optimization we instruction fine-tune LLM COMPILER models to emulate\ncompiler optimizations, illustrated in Figure 2. The idea is to generate from a finite set of unoptimized seed programs\na large number of examples by applying randomly generated sequences of compiler optimizations to these programs.\nWe then train the model to predict the code generated by the optimizations. We also train the model to predict the\ncode size after the optimizations have been applied.\nGiven unoptimized LLVM-IR (as emitted by the clang frontend), a list of optimization passes,\nand a starting code size, generate the resulting code after those optimizations have been applied and the resulting\ncode size.\nThere are two flavors of this task: in the first the model is expected to output compiler IR, in the second the model is\nexpected to output assembly code. The input IR, optimization passes, and code size are the same for both flavors. The\nprompt dictates the required output format.\nWe use two metrics for code size: the number of IR instructions, and binary size. Binary size is\ncomputed by summing the size of the TEXT and .DATA sections of the IR or assembly after lowering to an object file;\nwe exclude. BSS section from our binary size metric since it does not affect on-disk size.\nIn this work we target LLVM 17.0.6 and use the New Pass Manager (PM, 2021) which\nclassifies passes for different levels such as module, function, loop, etc. as well as transformation and analysis passes.\nTransformation passes change given input IR while analysis passes generate information that influence subsequent\ntransformations.\nOf the 346 possible pass arguments for opt, we select 167 to use."}, {"title": "LLM Compiler FTD: Extending for downstream compiler tasks", "content": ""}, {"title": "Instruction fine-tuning for optimization flag tuning", "content": "Manipulating compiler flags is well known to have a considerable impact on both runtime performance and code\nsize (Fursin et al., 2005). We train LLM COMPILER FTD models on the downstream task of selecting flags for LLVM's\nIR optimization tool opt to produce the smallest code size. Machine learning approaches to flag tuning have shown\ngood results previously, but struggle with generalizing across different programs (Cummins et al., 2022). Previous\nworks usually need to compile new programs tens or hundreds of times to try out different configurations and find out\nthe best-performing option. We train and evaluate LLM COMPILER FTD models on the zero-shot version of this task\nby predicting flags to minimize code size of unseen programs. Our approach is agnostic to the chosen compiler and\noptimization metric, and we intend to target runtime performance in the future. For now, optimizing for code size\nsimplifies the collection of training data.\nWe present the LLM COMPILER FTD models with an unoptimized LLVM-IR (as emitted\nby the clang frontend) and ask it to produce a list of opt flags that should be applied to it, the binary size before\nand after these optimizations are applied, and the output code. If no improvement can be made over the input code,\na short output message is generated that contains only the unoptimized binary size.\nWe used the same constrained set of optimization passes as in the compiler emulation task, and compute binary size\nin the same manner.\nLLVM's optimizer is not free from bugs and running optimization passes in unexpected or untested\norders may expose subtle correctness errors that undermine the utility of the model. To mitigate this risk we developed\nPassListEval, a tool to help in automatically identifying pass lists that break program semantics or cause compiler\ncrashes. An overview of the tool is shown in Figure 4. PassListEval accepts as input a candidate pass list and evaluates\nit over a suite of 164 self-testing C++ programs, taken from HumanEval-X (Zheng et al., 2023). Each program\ncontains a reference solution for a programming challenge, e.g. \"Check if in given vector of numbers, are any two\nnumbers closer to each other than given threshold\", and a suite of unit tests that validate correctness. We apply the\ncandidate pass lists to the reference solution, and then link them against the test suites to produce a binary. When\nexecuted, the binary will crash if any of the tests fail. If any binary crashes, or if any of the compiler invocations fail,\nwe reject the candidate pass list."}, {"title": "Instruction fine-tuning for disassembly", "content": "The ability to lift code from assembly back into higher level structures enables running additional optimizations on\nlibrary code directly integrated with application code or porting of legacy code to new architectures. The field of\ndecompilation has seen advancements in applying machine learning techniques to generate readable and accurate code\nfrom binary executables. Several studies explore the use of machine learning for decompilation tasks, such as lifting\nbinaries into intermediate representations for evaluation against synthetic C programs (Cao et al., 2022), utilizing\nevolutionary approaches like genetic algorithms for program analysis (Schulte et al., 2018), and proposing methods\nlike XLIR for matching binary code across different programming languages (Gui et al., 2022). Armengol-Estap\u00e9 et al.\n(2024) have trained a language model to decompile x86 assembly into high level C code. In this study, we demonstrate\nhow LLM COMPILER FTD can learn the relationship between assembly code and compiler IR by fine-tuning it for\ndisassembly. The task is to learn the inverse translation of clang -xir -0 -S, shown in Figure 5.\nUsing an LLM for disassembly causes problems of correctness. The lifted code must be verified\nby an equivalence checker which is not always feasible or manually verified for correctness or subjected to sufficient\ntest cases to give confidence. However, a lower bound on correctness can be found by round-tripping. That is to say\nby compiling the lifted IR back into assembly, if the assembly is identical then the IR is correct. This gives an easy\nroute to using the results of the LLM and an easy way to measure the utility of a disassembly model.\nWe provide the model with assembly code and train it to emit the corresponding disassembled\nIR. Listing 7 shows the prompt format. The context length for this task is set to 8k tokens for the input assembly\ncode and 8k tokens for the output IR."}, {"title": "Training parameters", "content": "Data is tokenized via byte pair encoding (Gage, 1994), employing the same tokenizer as CODE LLAMA, Llama (Touvron\net al., 2023a), and Llama 2 (Touvron et al., 2023b).\nWe use the same training parameters for all four stages of training. Most of the training parameters we used are the\nsame as for the CODE LLAMA base model. We use the AdamW (Loshchilov & Hutter, 2017) optimizer with B\u2081 and B\u2082\nvalues of 0.9 and 0.95. We use a cosine schedule with 1000 warm-up steps, and set the final learning rate to be 1/30th\nof the peak learning rate. Compared to the CODE LLAMA base model, we increased the context length of individual\nsequences from 4,096 to 16,384, but kept the batch size constant at 4M tokens. To account for the longer context, we\nset our learning rate to 2e-5 and modified the parameters of the ROPE positional embeddings (Su et al., 2024) where"}, {"title": "Evaluation", "content": "In this section we evaluate the performance of LLM COMPILER models on the tasks of flag tuning and disassembly,\ncompiler emulation, next-token prediction, and finally software engineering tasks."}, {"title": "Flag tuning task", "content": "We evaluate LLM COMPILER FTD on the task of optimization flag tuning for unseen programs\nand compare to GPT-4 Turbo and CODE LLAMA - INSTRUCT. We run inference on each model and extract from the\nmodel output the optimization pass list. We then use this pass list to optimize the particular program and record the\nbinary size. The baseline is the binary size of the program when optimized using -Oz.\nFor GPT-4 Turbo and CODE LLAMA - INSTRUCT we append a suffix to the prompt with additional context to further\ndescribe the problem and expected output format. After some experimentation we found that the prompt suffix shown\nin Listing 6 provides the best performance.\nAll model-generated pass lists are validated using PassListEval, and -Oz is used as substitute if validation fails. To\nfurther validate correctness of model-generated pass lists we link the final program binaries and differential test their\noutputs against the outputs of the benchmark when optimized using a conservative -O2 optimization pipeline.\nWe evaluate on 2,398 test prompts extracted from the MiBench benchmark suite (Guthaus et al., 2001).\nTo generate these prompts we take all of the 713 translation units that make up the 24 MiBench benchmarks and\ngenerate unoptimized IRs from each. We then format them as prompts as per Listing 4. If the resulting prompt\nexceeds 15k tokens we split the LLVM module representing that translation unit into smaller modules, one for each\nfunction, using llum-extract. This results in 1,985 prompts which fit within the 15k token context window, leaving\n443 translation units which do not fit. We use -Oz when for the 443 excluded translation units when computing\nperformance scores.\nTable 3 shows zero-shot performance of all models on the flag tuning task. Only LLM COMPILER FTD\nmodels provide an improvement over -Oz, with the 13B parameter model marginally outperforming the smaller model,\ngenerating smaller object files than -Oz in 61% of cases.\nIn some cases the model-generated pass list causes a larger object file size than -Oz. For example, LLM COMPILER FTD\n13B regresses in 12% of cases. These regressions can be avoided by simply compiling the program twice: once using the\nmodel-generated pass list, once using -Oz, and selecting the pass list which produces the best result. By eliminating\nregressions wrt -Oz, these -Oz backup scores raise the overall improvement over -Oz to 5.26% for LLM COMPILER FTD\n13B, and enable modest improvements over -Oz for CODE LLAMA INSTRUCT and GPT-4 Turbo.\nWhile the model-generated binary size predictions have no effect on actual compilation,\nwe can evaluate the performance of the models at predicting binary sizes before and after optimization to give an\nindication of each model's understanding of optimization. Figure 7 shows the results. LLM COMPILER FTD binary\nsize predictions correlate well with ground truth, with the 7B parameter model achieving MAPE values of 0.083\nand 0.225 for unoptimized and optimized binary sizes respectively. The 13B parameter model improved has similar\nMAPE values of 0.082 and 0.225. CODE LLAMA - INSTRUCT and GPT-4 Turbo binary size predictions show little\ncorrelation with ground truth. We note that the LLM COMPILER FTD errors are slightly higher for optimized code\nthan unoptimized code. In particular, there is an occasional tendency for LLM COMPILER FTD to overestimate the\neffectiveness of optimization, resulting in a lower predicted binary size than actual.\nTable 4 ablates the performance of models on a small holdout validation set of 500 prompts\ntaken from the same distribution as our training data (though not used during training). We trained for flag tuning at\neach stage of the training pipeline from Figure 1 to compare performance. As shown, disassembly training causes a\nslight regression in performance from average 5.15% to 5.12% improvement over -Oz. We also show performance of\nthe autotuner used for generating the training data described in Section 2. LLM COMPILER FTD achieves 77% of the\nperformance of the autotuner."}, {"title": "Disassembly task", "content": "We evaluate the functional correctness of LLM-generated code when disassembling assembly code to\nLLVM-IR. As in Section 5.1 we evaluate LLM COMPILER FTD and compare to CODE LLAMA INSTRUCT and GPT-4\nTurbo, and find that an additional prompt suffix, shown in Listing 8, is required to extract the best performance from\nthese models. The suffix provides additional context about the task and the expected output format. To evaluate the\nperformance of models we round-trip the model-generated disassembled IR back down to assembly. This enables us to\nevaluate accuracy of the disassembly by comparing the BLEU score (Papineni et al., 2002) of the original assembly\nagainst the round-trip result. A lossless and perfect disassembly from assembly to IR will have a round-trip BLEU\nscore of 1.0 (exact match).\nWe evaluate on 2,015 test prompts extracted from the MiBench benchmark suite. We took the 2,398\ntranslation units used for the flag tuning evaluation above and generated disassembly prompts. We then filtered the\nprompts on a maximum 8k token length, allowing 8k tokens for the model output, leaving 2,015.\nTable 5 shows performance of the models on the disassembly task. LLM COMPILER FTD 7B has a\nslightly higher round-trip success rate than LLM COMPILER FTD 13B, but LLM COMPILER FTD 13B has the\nhighest accuracy of round-tripped assembly (round trip BLEU) and most frequently produces a perfect disassembly\n(round trip exact match). CODE LLAMA INSTRUCT and GPT-4 Turbo struggle with generating syntactically correct\nLLVM-IR.\nTable 6 ablates the performance of models on a small holdout validation set of 500 prompts\ntaken from the MiBench dataset used previously. We trained for disassembly at each stage of the training pipeline\nfrom Figure 1 to compare performance. Round trip rate is highest when going through the whole stack of training\ndata and drops consistently with every training stage, though round trip BLEU varies little with each stage."}, {"title": "Foundation model tasks", "content": "We ablate LLM COMPILER models on the two foundation model tasks of next-token prediction and\ncompiler emulation. We perform this evaluation at each stage of the training pipeline to see how training for each\nsuccessive task affects performance. For next-token prediction we compute perplexity on a small sample of LLVM-IR\nand assembly code from all optimization levels. We evaluate compiler emulation using two metrics: whether the\ngenerated IR or assembly code compiles, and whether the generated IR or assembly code is an exact match for what\nthe compiler would produce.\nFor next-token prediction we use a small holdout set of validation data that is drawn from the same\ndistribution as our training data but has not been used for training. We use a mixture of optimization levels including\nunoptimized code, code optimized with -Oz, and randomly generated pass lists. For compiler emulatino we evaluate\nusing 500 prompts generated from MiBench using randomly pass lists generated in the manner described in Section 2.2.\nTable 7 shows performance of LLM COMPILER FTD across all training stages on the two foundation\nmodel training tasks of next-token prediction and compiler emulation. Next-token prediction performance jumps\nsharply after CODE LLAMA, which has seen very little IR and assembly, and declines slightly with each subsequent\nstage of fine-tuning.\nFor compiler emulation, the CODE LLAMA base model and the pre-trained models perform poorly since they have not\nbeen trained on this task. The highest performance is achieved directly after compiler emulation training where 95.6%\nof IR and assembly generated by LLM COMPILER FTD 13B compiles, and 20% of it matches the compiler exactly.\nPerformance declines after fine-tuning for flag tuning and disassembly."}, {"title": "Software engineering tasks", "content": "While the purpose of LLM COMPILER FTD is to provide foundation models for code optimization,\nit builds upon base CODE LLAMA models which were trained for software engineering tasks. To evaluate how the\nadditional training of LLM COMPILER FTD has affected the performance of code generation we use the same\nbenchmark suites as in CODE LLAMA that evaluate the ability of LLMs to generate Python code from natural language\nprompts, such as \"Write a function to find the longest chain which can be formed from the given set of pairs.\""}, {"title": "Related work", "content": "There is increasing interest in LLMs for source code reasoning and generation (Jiang\net al., 2024; Hou et al., 2023). The main enablers of progress in this area are pretrained foundational models made\navailable for others to build upon, including CODE LLAMA (Rozi\u00e8re et al., 2023), StarCoder (Lozhkov et al., 2024),\nMagicoder (Wei et al., 2024), DeepSeek-Coder (Guo et al., 2024), GPT-4 (OpenAI, 2023) and others (Wang et al.,"}, {"title": "Discussion", "content": "In this paper, we introduced LLM COMPILER, a novel family of large language models specifically designed to address\nthe challenges of code and compiler optimization. By extending the capabilities of the foundational CODE LLAMA\nmodel, LLM COMPILER provides a robust, pre-trained platform that significantly enhances the understanding and\nmanipulation of compiler intermediate representations and assembly language.\nWe release LLM COMPILER under a bespoke commercial license to facilitate widespread access and collaboration,\nenabling both academic researchers and industry practitioners to explore, modify, and extend the model according to\ntheir specific needs."}, {"title": "Limitations", "content": "We have shown that LLM COMPILER performs well at compiler optimization tasks and has improved understanding\nof compiler representations and assembly code over prior works, but there are limitations. The main limitation is\nthe finite sequence length of inputs (context window). LLM COMPILER supports a 16k token context windows, but\nprogram codes may be far longer. For example, 67% of MiBench translation units exceeded this context window\nwhen formatted as flag tuning prompts, shown in Table 10. To mitigate this we split larger translation units into\nindividual functions, though this limits the scope of optimization that can be performed, and still 18% of the split\ntranslation units remain too large for the model to accept as input. Researchers are adopting ever-increasing context\nwindows (Ding et al., 2023), but finite context windows remain a common concern with LLMs.\nA second limitation, common to all LLMs, is the accuracy of model outputs. Users of LLM COMPILER are advised to\nassess their models using evaluation benchmarks specific to compilers. Given that compilers are not bug-free, any\nsuggested compiler optimizations must be rigorously tested. When a model decompiles assembly code, its accuracy\nshould be confirmed through round trip, manual inspection, or unit testing. For some applications LLM generations\ncan be constrained to regular expressions (Grubisic et al., 2024b), or combined with automatic verification to ensure\ncorrectness (Taneja et al., 2024)."}]}