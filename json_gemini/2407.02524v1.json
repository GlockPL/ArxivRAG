{"title": "Meta Large Language Model Compiler: Foundation Models of Compiler Optimization", "authors": ["Chris Cummins", "Volker Seekert", "Dejan Grubisic", "Baptiste Rozi\u00e8re", "Jonas Gehring", "Gabriel Synnaeve", "Hugh Leather"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, we introduce Meta Large Language Model Compiler (LLM COMPILER), a suite of robust, openly available, pre-trained models specifically designed for code optimization tasks. Built on the foundation of CODE LLAMA, LLM COMPILER enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The model has been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and has undergone instruction fine-tuning to interpret compiler behavior. LLM COMPILER is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. We also present fine-tuned versions of the model, demonstrating its enhanced capabilities in optimizing code size and disassembling from x86_64 and ARM assembly back into LLVM-IR. These achieve 77% of the optimising potential of an autotuning search, and 45% disassembly round trip (14% exact match). This release aims to provide a scalable, cost-effective foundation for further research and development in compiler optimization by both academic researchers and industry practitioners.", "sections": [{"title": "Introduction", "content": "There is increasing interest in large language models (LLMs) for software engineering tasks including code generation, code translation, and code testing. Models such as StarCoder (Lozhkov et al., 2024), CODE LLAMA (Rozi\u00e8re et al., 2023), and GPT-4 (OpenAI, 2023) have a good statistical understanding of code and can suggest likely completions for unfinished code, making them useful for editing and creating software. However, there is little emphasis on training specifically to optimize code. Publicly available LLMs can be prompted to make minor tweaks to a program such as tagging variables to be stored as registers, and will even attempt more substantial optimizations like vectorization, though they easily become confused and make mistakes, frequently resulting in incorrect code.\nPrior works on machine learning-guided code optimization have used a range of representations from hand-built features (Wang & O'Boyle, 2018) to graph neural networks (GNNs) (Liang et al., 2023). However, in all cases, the way the input program is represented to the machine learning algorithm is incomplete, losing some information along the way. For example, Trofin et al. (2021) use numeric features to provide hints for function inlining, but cannot faithfully reproduce the call graph or control flow. Cummins et al. (2021) form graphs of the program to pass to a GNN, but exclude the values of constants and some type information which prevents reproducing instructions with fidelity.\nIn contrast, LLMs can accept source programs, as is, with a complete, lossless representation. Using text as the input and output representation for a machine learning optimizer has desirable properties: text is a"}, {"title": "Overview", "content": "Figure 1 shows an overview of our approach. LLM COMPILER models target compiler optimization. They are available in two model sizes: 7B and 13B parameters. The LLM COMPILER models are initialized with CODE LLAMA model weights of the corresponding size and trained on an additional 546B tokens of data comprising mostly compiler intermediate representations and assembly code. We then further train LLM COMPILER FTD models using an additional 164B tokens of data for two downstream compilation tasks: flag tuning and disassembly. At all stages of training a small amount of code and natural language data from previous stages is used to help retain the capabilities of the base CODE LLAMA model."}, {"title": "LLM Compiler: Specializing Code Llama for compiler optimization", "content": ""}, {"title": "Pretraining on assembly code and compiler IRs", "content": "The data used to train coding LLMs are typically composed largely of high level source languages like Python. Assembly code contributes a negligible proportion of these datasets, and compiler IRs even less. To build an LLM with a good understanding of these languages we initialize LLM COMPILER models with the weights of CODE LLAMA and then train for 401 billion tokens on a compiler-centric dataset composed mostly of assembly code and compiler IRs, shown in Table 1.\nDataset LLM COMPILER is trained predominantly on compiler intermediate representations and assembly code generated by LLVM (Lattner & Adve, 2004) version 17.0.6. These are derived from the same dataset of publicly available code used to train CODE LLAMA. We summarize this dataset in Table 2. As in CODE LLAMA, we also source a small proportion of training batches from natural language datasets."}, {"title": "Instruction fine-tuning for compiler emulation", "content": "To understand the mechanism of code optimization we instruction fine-tune LLM COMPILER models to emulate compiler optimizations, illustrated in Figure 2. The idea is to generate from a finite set of unoptimized seed programs a large number of examples by applying randomly generated sequences of compiler optimizations to these programs. We then train the model to predict the code generated by the optimizations. We also train the model to predict the code size after the optimizations have been applied.\nTask specification. Given unoptimized LLVM-IR (as emitted by the clang frontend), a list of optimization passes, and a starting code size, generate the resulting code after those optimizations have been applied and the resulting code size.\nThere are two flavors of this task: in the first the model is expected to output compiler IR, in the second the model is expected to output assembly code. The input IR, optimization passes, and code size are the same for both flavors. The prompt dictates the required output format. Examples of each prompt are provided in Appendices Listings 2 and 3.\nCode size. We use two metrics for code size: the number of IR instructions, and binary size. Binary size is computed by summing the size of the TEXT and .DATA sections of the IR or assembly after lowering to an object file; we exclude .BSS section from our binary size metric since it does not affect on-disk size.\nOptimization passes. In this work we target LLVM 17.0.6 and use the New Pass Manager (PM, 2021) which classifies passes for different levels such as module, function, loop, etc. as well as transformation and analysis passes. Transformation passes change given input IR while analysis passes generate information that influence subsequent transformations.\nOf the 346 possible pass arguments for opt, we select 167 to use. This includes each of the default optimization pipelines (e.g. module (default<z>)), individual optimization transform passes (e.g. module (constmerge)), but"}, {"title": "LLM Compiler FTD: Extending for downstream compiler tasks", "content": ""}, {"title": "Instruction fine-tuning for optimization flag tuning", "content": "Manipulating compiler flags is well known to have a considerable impact on both runtime performance and code size (Fursin et al., 2005). We train LLM COMPILER FTD models on the downstream task of selecting flags for LLVM's IR optimization tool opt to produce the smallest code size. Machine learning approaches to flag tuning have shown good results previously, but struggle with generalizing across different programs (Cummins et al., 2022). Previous works usually need to compile new programs tens or hundreds of times to try out different configurations and find out the best-performing option. We train and evaluate LLM COMPILER FTD models on the zero-shot version of this task by predicting flags to minimize code size of unseen programs. Our approach is agnostic to the chosen compiler and optimization metric, and we intend to target runtime performance in the future. For now, optimizing for code size simplifies the collection of training data.\nTask specification. We present the LLM COMPILER FTD models with an unoptimized LLVM-IR (as emitted by the clang frontend) and ask it to produce a list of opt flags that should be applied to it, the binary size before and after these optimizations are applied, and the output code. If no improvement can be made over the input code, a short output message is generated that contains only the unoptimized binary size. Listings 4 and 5 provide the prompt and output templates for this task.\nWe used the same constrained set of optimization passes as in the compiler emulation task, and compute binary size in the same manner.\nFigure 3 illustrates the process used to generate training data (described below) and how the model is used for inference. Only the generated pass list is needed at evaluation time. We extract the pass list from the model output and run opt using the given arguments. We can then evaluate the accuracy of the model predicted binary sizes and optimized output code, but those are auxiliary learning tasks not required for use.\nCorrectness. LLVM's optimizer is not free from bugs and running optimization passes in unexpected or untested orders may expose subtle correctness errors that undermine the utility of the model. To mitigate this risk we developed PassListEval, a tool to help in automatically identifying pass lists that break program semantics or cause compiler crashes. An overview of the tool is shown in Figure 4. PassListEval accepts as input a candidate pass list and evaluates it over a suite of 164 self-testing C++ programs, taken from HumanEval-X (Zheng et al., 2023). Each program contains a reference solution for a programming challenge, e.g. \"Check if in given vector of numbers, are any two numbers closer to each other than given threshold\", and a suite of unit tests that validate correctness. We apply the candidate pass lists to the reference solution, and then link them against the test suites to produce a binary. When executed, the binary will crash if any of the tests fail. If any binary crashes, or if any of the compiler invocations fail, we reject the candidate pass list.\nDataset. We trained LLM COMPILER FTD models on a dataset of flag tuning examples derived from 4.5M of the unoptimized IRs used for pretraining. To generate the example optimal pass list for each program we ran an extensive iterative compilation process depicted in Figure 3 and outlined below:\n1. We used large-scale random search to generate an initial candidate best pass list for the programs. For each program we independently generated random lists of up to 50 passes by uniformly sampling from the set of 167 searchable passes described previously. Every time we evaluated a pass list on a program we recorded"}, {"title": "Instruction fine-tuning for disassembly", "content": "The ability to lift code from assembly back into higher level structures enables running additional optimizations on library code directly integrated with application code or porting of legacy code to new architectures. The field of decompilation has seen advancements in applying machine learning techniques to generate readable and accurate code from binary executables. Several studies explore the use of machine learning for decompilation tasks, such as lifting binaries into intermediate representations for evaluation against synthetic C programs (Cao et al., 2022), utilizing evolutionary approaches like genetic algorithms for program analysis (Schulte et al., 2018), and proposing methods like XLIR for matching binary code across different programming languages (Gui et al., 2022). Armengol-Estap\u00e9 et al. (2024) have trained a language model to decompile x86 assembly into high level C code. In this study, we demonstrate how LLM COMPILER FTD can learn the relationship between assembly code and compiler IR by fine-tuning it for disassembly. The task is to learn the inverse translation of clang -xir -O -S, shown in Figure 5.\nRound tripping. Using an LLM for disassembly causes problems of correctness. The lifted code must be verified by an equivalence checker which is not always feasible or manually verified for correctness or subjected to sufficient test cases to give confidence. However, a lower bound on correctness can be found by round-tripping. That is to say by compiling the lifted IR back into assembly, if the assembly is identical then the IR is correct. This gives an easy route to using the results of the LLM and an easy way to measure the utility of a disassembly model.\nTask specification. We provide the model with assembly code and train it to emit the corresponding disassembled IR. Listing 7 shows the prompt format. The context length for this task is set to 8k tokens for the input assembly code and 8k tokens for the output IR.\nDataset. We derive the assembly codes and IR pairs from the same dataset used in previous tasks. Our fine-tuning dataset consists in 4.7M samples. The input IR has been optimized with -Oz before being lowered to x86 assembly."}, {"title": "Training parameters", "content": "Data is tokenized via byte pair encoding (Gage, 1994), employing the same tokenizer as CODE LLAMA, Llama (Touvron et al., 2023a), and Llama 2 (Touvron et al., 2023b).\nWe use the same training parameters for all four stages of training. Most of the training parameters we used are the same as for the CODE LLAMA base model. We use the AdamW (Loshchilov & Hutter, 2017) optimizer with B\u2081 and B2 values of 0.9 and 0.95. We use a cosine schedule with 1000 warm-up steps, and set the final learning rate to be 1/30th of the peak learning rate. Compared to the CODE LLAMA base model, we increased the context length of individual sequences from 4,096 to 16,384, but kept the batch size constant at 4M tokens. To account for the longer context, we set our learning rate to 2e-5 and modified the parameters of the ROPE positional embeddings (Su et al., 2024) where"}, {"title": "Evaluation", "content": "In this section we evaluate the performance of LLM COMPILER models on the tasks of flag tuning and disassembly, compiler emulation, next-token prediction, and finally software engineering tasks."}, {"title": "Flag tuning task", "content": "Methodology. We evaluate LLM COMPILER FTD on the task of optimization flag tuning for unseen programs and compare to GPT-4 Turbo and CODE LLAMA - INSTRUCT. We run inference on each model and extract from the model output the optimization pass list. We then use this pass list to optimize the particular program and record the binary size. The baseline is the binary size of the program when optimized using -Oz.\nFor GPT-4 Turbo and CODE LLAMA INSTRUCT we append a suffix to the prompt with additional context to further describe the problem and expected output format. After some experimentation we found that the prompt suffix shown in Listing 6 provides the best performance.\nAll model-generated pass lists are validated using PassListEval, and -Oz is used as substitute if validation fails. To further validate correctness of model-generated pass lists we link the final program binaries and differential test their outputs against the outputs of the benchmark when optimized using a conservative -O2 optimization pipeline.\nDataset. We evaluate on 2,398 test prompts extracted from the MiBench benchmark suite (Guthaus et al., 2001). To generate these prompts we take all of the 713 translation units that make up the 24 MiBench benchmarks and generate unoptimized IRs from each. We then format them as prompts as per Listing 4. If the resulting prompt exceeds 15k tokens we split the LLVM module representing that translation unit into smaller modules, one for each function, using llum-extract. This results in 1,985 prompts which fit within the 15k token context window, leaving 443 translation units which do not fit. We use -Oz when for the 443 excluded translation units when computing performance scores. Table 10 summarizes the benchmarks.\nResults. Table 3 shows zero-shot performance of all models on the flag tuning task. Only LLM COMPILER FTD models provide an improvement over -Oz, with the 13B parameter model marginally outperforming the smaller model, generating smaller object files than -Oz in 61% of cases.\nIn some cases the model-generated pass list causes a larger object file size than -Oz. For example, LLM COMPILER FTD 13B regresses in 12% of cases. These regressions can be avoided by simply compiling the program twice: once using the model-generated pass list, once using -Oz, and selecting the pass list which produces the best result. By eliminating regressions wrt -Oz, these -Oz backup scores raise the overall improvement over -Oz to 5.26% for LLM COMPILER FTD 13B, and enable modest improvements over -Oz for CODE LLAMA INSTRUCT and GPT-4 Turbo. Figure 6 shows the performance of each model broken down by individual benchmark.\nBinary size accuracy. While the model-generated binary size predictions have no effect on actual compilation, we can evaluate the performance of the models at predicting binary sizes before and after optimization to give an indication of each model's understanding of optimization. Figure 7 shows the results. LLM COMPILER FTD binary size predictions correlate well with ground truth, with the 7B parameter model achieving MAPE values of 0.083 and 0.225 for unoptimized and optimized binary sizes respectively. The 13B parameter model improved has similar MAPE values of 0.082 and 0.225. CODE LLAMA - INSTRUCT and GPT-4 Turbo binary size predictions show little correlation with ground truth. We note that the LLM COMPILER FTD errors are slightly higher for optimized code than unoptimized code. In particular, there is an occasional tendency for LLM COMPILER FTD to overestimate the effectiveness of optimization, resulting in a lower predicted binary size than actual.\nAblation studies. Table 4 ablates the performance of models on a small holdout validation set of 500 prompts taken from the same distribution as our training data (though not used during training). We trained for flag tuning at each stage of the training pipeline from Figure 1 to compare performance. As shown, disassembly training causes a slight regression in performance from average 5.15% to 5.12% improvement over -Oz. We also show performance of the autotuner used for generating the training data described in Section 2. LLM COMPILER FTD achieves 77% of the performance of the autotuner."}, {"title": "Disassembly task", "content": "Methodology. We evaluate the functional correctness of LLM-generated code when disassembling assembly code to LLVM-IR. As in Section 5.1 we evaluate LLM COMPILER FTD and compare to CODE LLAMA INSTRUCT and GPT-4 Turbo, and find that an additional prompt suffix, shown in Listing 8, is required to extract the best performance from these models. The suffix provides additional context about the task and the expected output format. To evaluate the performance of models we round-trip the model-generated disassembled IR back down to assembly. This enables us to evaluate accuracy of the disassembly by comparing the BLEU score (Papineni et al., 2002) of the original assembly against the round-trip result. A lossless and perfect disassembly from assembly to IR will have a round-trip BLEU score of 1.0 (exact match).\nDataset. We evaluate on 2,015 test prompts extracted from the MiBench benchmark suite. We took the 2,398 translation units used for the flag tuning evaluation above and generated disassembly prompts. We then filtered the prompts on a maximum 8k token length, allowing 8k tokens for the model output, leaving 2,015. Table 11 summarizes the benchmarks.\nResults. Table 5 shows performance of the models on the disassembly task. LLM COMPILER FTD 7B has a slightly higher round-trip success rate than LLM COMPILER FTD 13B, but LLM COMPILER FTD 13B has the highest accuracy of round-tripped assembly (round trip BLEU) and most frequently produces a perfect disassembly (round trip exact match). CODE LLAMA INSTRUCT and GPT-4 Turbo struggle with generating syntactically correct LLVM-IR. Figure 8 shows the distribution of round-trip BLEU scores for all models.\nAblation studies. Table 6 ablates the performance of models on a small holdout validation set of 500 prompts taken from the MiBench dataset used previously. We trained for disassembly at each stage of the training pipeline from Figure 1 to compare performance. Round trip rate is highest when going through the whole stack of training data and drops consistently with every training stage, though round trip BLEU varies little with each stage."}, {"title": "Foundation model tasks", "content": "Methodology We ablate LLM COMPILER models on the two foundation model tasks of next-token prediction and compiler emulation. We perform this evaluation at each stage of the training pipeline to see how training for each successive task affects performance. For next-token prediction we compute perplexity on a small sample of LLVM-IR and assembly code from all optimization levels. We evaluate compiler emulation using two metrics: whether the generated IR or assembly code compiles, and whether the generated IR or assembly code is an exact match for what the compiler would produce.\nDataset. For next-token prediction we use a small holdout set of validation data that is drawn from the same distribution as our training data but has not been used for training. We use a mixture of optimization levels including unoptimized code, code optimized with -Oz, and randomly generated pass lists. For compiler emulatino we evaluate using 500 prompts generated from MiBench using randomly pass lists generated in the manner described in Section 2.2.\nResults Table 7 shows performance of LLM COMPILER FTD across all training stages on the two foundation model training tasks of next-token prediction and compiler emulation. Next-token prediction performance jumps sharply after CODE LLAMA, which has seen very little IR and assembly, and declines slightly with each subsequent stage of fine-tuning.\nFor compiler emulation, the CODE LLAMA base model and the pre-trained models perform poorly since they have not been trained on this task. The highest performance is achieved directly after compiler emulation training where 95.6% of IR and assembly generated by LLM COMPILER FTD 13B compiles, and 20% of it matches the compiler exactly. Performance declines after fine-tuning for flag tuning and disassembly."}, {"title": "Software engineering tasks", "content": "Methodology. While the purpose of LLM COMPILER FTD is to provide foundation models for code optimization, it builds upon base CODE LLAMA models which were trained for software engineering tasks. To evaluate how the additional training of LLM COMPILER FTD has affected the performance of code generation we use the same benchmark suites as in CODE LLAMA that evaluate the ability of LLMs to generate Python code from natural language prompts, such as \"Write a function to find the longest chain which can be formed from the given set of pairs.\"."}, {"title": "Related work", "content": "Language models over code. There is increasing interest in LLMs for source code reasoning and generation (Jiang et al., 2024; Hou et al., 2023). The main enablers of progress in this area are pretrained foundational models made available for others to build upon, including CODE LLAMA (Rozi\u00e8re et al., 2023), StarCoder (Lozhkov et al., 2024), Magicoder (Wei et al., 2024), DeepSeek-Coder (Guo et al., 2024), GPT-4 (OpenAI, 2023) and others (Wang et al.,"}, {"title": "Discussion", "content": "In this paper, we introduced LLM COMPILER, a novel family of large language models specifically designed to address the challenges of code and compiler optimization. By extending the capabilities of the foundational CODE LLAMA model, LLM COMPILER provides a robust, pre-trained platform that significantly enhances the understanding and manipulation of compiler intermediate representations and assembly language.\nWe release LLM COMPILER under a bespoke commercial license to facilitate widespread access and collaboration, enabling both academic researchers and industry practitioners to explore, modify, and extend the model according to their specific needs."}, {"title": "Limitations", "content": "We have shown that LLM COMPILER performs well at compiler optimization tasks and has improved understanding of compiler representations and assembly code over prior works, but there are limitations. The main limitation is the finite sequence length of inputs (context window). LLM COMPILER supports a 16k token context windows, but program codes may be far longer. For example, 67% of MiBench translation units exceeded this context window when formatted as flag tuning prompts, shown in Table 10. To mitigate this we split larger translation units into individual functions, though this limits the scope of optimization that can be performed, and still 18% of the split"}]}