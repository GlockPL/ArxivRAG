{"title": "Detecting Wildfires on UAVs with Real-time Segmentation Trained by Larger Teacher Models", "authors": ["Julius Pesonen", "Lauri Markelin", "Teemu Hakala", "V\u00e4in\u00f6 Karjalainen", "Niko Koivum\u00e4ki", "Anna-Maria Raita-Hakola", "Juha Suomalainen", "Ilkka P\u00f6l\u00f6nen", "Eija Honkavaara"], "abstract": "Early detection of wildfires is essential to prevent large-scale fires resulting in extensive environmental, structural, and societal damage. Uncrewed aerial vehicles (UAVs) can cover large remote areas effectively with quick deployment requiring minimal infrastructure and equipping them with small cameras and computers enables autonomous real-time detection. In remote areas, however, the UAVs are limited to on-board computing for detection due to the lack of high-bandwidth mobile networks. This limits the detection to methods which are light enough for the on-board computer alone. For accurate camera-based localisation, segmentation of the detected smoke is essential but training data for deep learning-based wildfire smoke segmentation is limited. This study shows how small specialised segmentation models can be trained using only bounding box labels, leveraging zero-shot foundation model supervision. The method offers the advantages of needing only fairly easily obtainable bounding box labels and requiring training solely for the smaller student network. The proposed method achieved 63.3% mIoU on a manually annotated and diverse wildfire dataset. The used model can perform in real-time at ~11 fps with a UAV-carried NVIDIA Jetson Orin NX computer while reliably recognising smoke, demonstrated at real-world forest burning events. Code is available at https://gitlab.com/fgi_nls/public/wildfire-real-time-segmentation", "sections": [{"title": "1. Introduction", "content": "The frequency and intensity of extreme wildfires are increasing on Earth, posing tremendous risks to the environment, ecosystems, and societies [11]. Early detection is an essential tool to prevent large-scale disasters [2]. Uncrewed aerial vehicle (UAV)-based detection is a promising technology with the potential to enable rapid deployment of surveys over large areas with minimal infrastructure. In remote areas, however, the UAVs are limited to on-board computing for detection due to the lack of high-bandwidth mobile networks.\nSpecialised models are required to enable real-time early detection and effective localisation with onboard resources. Methods for autonomous detection of wildfires have been studied and the use of UAV-based methods has been proposed as early as 2005 [4]. Modern deep learning-based computer vision methods, however, have opened new possibilities for obtaining more accurate information more reliably than what has been possible with methods limited to fewer computational resources.\nIn this work, we propose using light, specialised segmentation models to enable accurate detection and localisation of wildfires. For training, we leveraged bounding box-guided pseudo-label supervision obtained from large foundation models. We show that the training framework is efficient for targeted segmentation models in tasks with limited computational resources. A simplified training scheme is"}, {"title": "2. Related Work", "content": "The ideas of UAV-based wildfire monitoring, detection and localisation date back to the early 2000's [4, 5, 36]. However, at the dates of these studies, the available computational resources and non-learning-based computer vision methods posed serious limitations.\nMore recently, artificial neural network models, such as R-CNN [18], Faster R-CNN [43], YOLO [42], with its numerous variants, and FPN [29], have enabled efficient detection of practically anything that can be photographed and labelled with bounding boxes. Namely, the single-stage object detection networks which notably include the YOLO variants focus on the real-time performance of the task. These have also been applied to UAV-based wildfire detection to enable detection with on-board computation [47, 56, 64].\nThe bounding-box detections, however, can not fully capture the shape of the detected object which, in the case of UAV-based wildfire detection, could affect the localisation accuracy with even thousands of meters depending on the flight altitude and camera angle. Segmentation combats this issue by capturing the shape of the detected object with pixel-level accuracy. Recent segmentation models, such as ERFNet [44], BiSeNet [62], and ENet [38] have been proposed specifically for real-time use. Segmentation has also been applied to wildfire detection [45, 54, 63] but the studies have mainly focused on flame detection even though the smoke of the fire is visible far further and earlier than any flames in case of a new wildfire, especially when recorded with an airborne UAV during daytime. However, the segmentation of smoke is largely limited by the lack of public data, especially such, that is annotated at the pixel level.\nPublicly available wildfire datasets include the FLAME dataset [46] and its synthetic extension FLAME Diffuser [53] as well as the Corsican dataset [50]. All of these datasets focus on the flames of the fire. The only publicly available datasets with a focus on the resulting smoke that could be found at the date of this study were the Bounding Box Annotated Wildfire Smoke Dataset Versions 1.0 and 2.0 by AI For Mankind and HPWREN [17].\nWeakly supervised semantic segmentation refers to training semantic segmentation models using non-pixel-level labels or far fewer labels than the number of input pixels. This can be realised with image-level labels, scribbles, individual pixels, or bounding boxes, as in the case of this study. The earlier solutions for box-supervised training of semantic and instance segmentation have used iterative mask proposals [12] or a combination of classical (non-learning-based) and learning-based solutions [25]. More recently, the use of bounding-box supervision to train general-purpose instance segmentation models has also been studied. Instance segmentation differs from semantic segmentation by differentiating between object instances instead of classes. The combination of semantic and instance segmentation is called panoptic segmentation. The most recent proposals aimed at instance segmentation include Mask Auto-Labeler (MAL) [27], BoxTeacher [8], Semantic-aware Instance Mask Generation [28], and BoxSnake [60].\nKnowledge distillation presents a way to produce effective specialised networks by minimising the difference between the outputs of a larger model, which is assumed capable of better performance and those of a smaller, distilled, model [21]. Knowledge distillation has also been applied to semantic segmentation model training for different applications in various forms with positive results [32, 39, 58, 59].\nThe recent emergence of prompt-guided segmentation [35] and specific models for the task such as Segment Anything Model (SAM) [26] and High-Quality SAM (HQ-SAM) [24] provide new tools for generating pixel-level pseudo-labels from weak labels. The use of combined foundation model and weak label supervision has been shown successful with image level supervision [6, 7, 48, 61] alongside Class Activation Maps (CAMs) [65] or language understanding from language-image foundation models such as Grounded-DINO [31] or CLIP (Contrastive Language-Image Pre-training) [40].\nThe use of combined bounding box and foundation model supervision for training use case-specific models has been proposed for planetary geological mapping [22], cell nuclei segmentation [10], and remote sensing [52]. For cell segmentation of histopathological imagery, such a model has been compared to methods that also use SAM at test time [51]. While these studies show the utility of SAM for generating useful pseudo-labels based on human-generated bounding boxes, the use cases do not require using real-time performant models nor provide generalisable results or extensive comparison against other bounding box supervised"}, {"title": "3. Methods", "content": "In this study, we address the gaps in the conducted literature by analysing the performance of PIDNet [57] models trained for the segmentation of wildfire smoke using labels obtained from a larger teacher model. We evaluated different teacher models for training the inference models by comparing the resulting accuracy of the final PID-Nets. In the main proposed framework, the teacher model is a Segment Anything Model guided by the bounding box labels to generate pseudo-labels for supervising the inference model. We compared this method to an alternative, where the teacher model is a larger binary segmentation model trained using state-of-the-art bounding box supervision methods for segmentation.\nWe chose PIDNet as the final inference model due to its capability of real-time performance even with limited computational resources. The PIDNet model is a three-branch model which leverages an auxiliary task of boundary detection to improve the segmentation accuracy at the boundary regions of the objects. The model combines the output of three different branches to form a final segmentation output that is used at inference time. Each of the three branches focuses on a separate task: local semantic features, global semantic features, and boundary detection.\nThe full training method can be formulated as a knowledge distillation scheme with a larger supervising teacher model and a far smaller and more efficient student model, that is used for inference. The proposed SAM supervision benefits from the lack of teacher model training, offering far faster results with less required computation. The distillation scheme used for both methods is shown in Figure 3. The methods are presented in more detail starting with the used data, the pseudo-labels obtained from either SAM or the trained teacher model, and finally the training details of the inference model with the pseudo-label supervision."}, {"title": "3.1. Data", "content": "For training and validating the models, we used the Bounding Box Annotated Wildfire Smoke Dataset Versions 1.0 and 2.0 by AI For Mankind and HPWREN, which in this study are referred to as the AI For Mankind data, as well as UAV-collected and manually bounding box labelled data, introduced by Raita-Hakola et al. [41], which in this study is referred to as the UAV Data. Samples from the datasets are shown in Figure 2. The UAV dataset used for this study consists of 5035 bounding box annotated UAV captured aerial footage of boreal forest. 4749 or 94% of the images contain visible smoke originating from real burning forests. The rest of the images are captured from similar views without any visible wildfire smoke.\nThe UAV dataset could be described as the easier one in the sense that most smoke is easily visible in the samples and very close to the camera, while in many of the samples in the AI For Mankind data, the smoke is small and far away which is clearly harder to detect. However, for the nearby smoke in the UAV dataset, it is far harder to define sharp boundaries, as the smoke takes complex shapes and the boundaries are often times blurry. Both of the two datasets also represent different features of the expected real-world scenarios in which the final application would be deployed.\nThe AI For Mankind dataset, with smoke observed from a distance, represents the desired early detection scenario. However, the images are captured from environments very different from a Finnish boreal forest, which is the intended real-world deployment location for the application. In contrast, the UAV data samples depict a boreal forest environment but the smoke detection itself is far easier than in a realistic wildfire detection scenario because the data were collected at close distances. For these reasons, neither of the datasets was chosen to be weighed over the other in either the training or the evaluation of the methods.\nTo enable using the data for training and evaluation of the segmentation model, a three-part datasplit, to training, validation and test set, was performed. The UAV data con-"}, {"title": "3.2. Pseudo-labels", "content": "Since the original data is only bounding box labelled, we leverage larger models to obtain segmentation masks to be used as pseudo-labels for training the final inference model. The first proposed approach uses masks directly generated"}, {"title": "3.3. Inference model training", "content": "We used AdamW [34] with a starting learning rate of 1e-3 and weight decay coefficient of le-2 to optimise the inference models. The models were trained for 50 epochs and the validation loss and mean intersection over union (mIoU) was computed on each epoch, from which the best-performing model was chosen for evaluation on the test set and real-world inference based on the validation set mIoU. Minibatches of 16 images were used for the training and the models were pretrained on ImageNet [13].\nThe PIDNets were optimised with four separate loss functions. Two loss functions were used to optimise the two different segmentation outputs and one took into account both the boundary prediction and the final segmentation. In addition, the boundary prediction was optimised with one more separate loss function. In this study, we adapted these four loss functions for the binary smoke segmentation task to distil the knowledge from the larger teacher model. The inference output, which is the final segmentation output of the model, was used alone for evaluating the model. The full distillation scheme is shown in Figure 3.\nThe total distillation loss is computed as\n$Loss = \\lambda_0 l_0 + \\lambda_1 l_1 + \\lambda_2 l_2 + \\lambda_3 l_3, $   (2)\nwhere loss functions $l_0$ and $l_1$ are binary cross entropy losses from the two alternative segmentation outputs of the PIDNet. $l_2$ is a weighted boundary-awareness cross entropy loss [49] and $l_3$ is a BAS-Loss that is a binary cross entropy loss computed only at the pixels where the boundary output of the model exceeds the threshold $t$. For the weighting of the losses we use the parameters $\\lambda_0 = 0.4, \\lambda_1 = 20, \\lambda_2 = 1, \\lambda_3 = 1$ and $t = 0.8$ as set empirically in the study introducing the PIDNet model.\nThe training samples were augmented randomly, similar to TrivialAugment [37] where a single augmentation with a random strength was randomly chosen for a sample at training time from a predefined set of augmentations. We opted for the fairly heavy augmentation strategy to overcome the limitations of the small amount of data and possibly noisy pseudo-labels. The random augmentation space included cropping, vertical flips, rotation, perspective, erasing, grayscale, gaussian blur, inversion, sharpness, and colour jitter. All of them were applied with the same probability and, in addition, a horizontal flip was applied 50% of the time regardless of other augments."}, {"title": "3.4. UAV-based system and testing", "content": "A UAV-carried sensor and computation system were implemented to test the performance of the obtained model in a real-world scenario. The lightweight system consisted of a small global shutter RGB camera AR0234 [1], with a 1920\u00d71080 px resolution and 90\u00b0 horizontal and 75\u00b0 vertical field of view, and an NVIDIA Jetson Orin NX [9] computer. The total system weight was only ~200 grams meaning that small UAVs could easily carry it. The system was tested at real-world forest-burning events to demonstrate the capability of the real-time segmentation model.\nThe tests were conducted at prescribed burning events conducted by the Finnish Mets\u00e4hallitus (a governmental forest administration organisation) at conservation areas in Southern Finland. The tests were conducted by recording the model outputs and the corresponding inputs as videos on the UAV-carried system from varying distances between one and fifteen kilometres. The sensor system was carried by a DJI Matrice 300 RTK drone [14], flown at an altitude of 120 meters above ground level. The test flights were conducted during sunny weather in June. To ensure that video was captured where the smoke was in the view of the camera, the small AR0234 camera was mounted on a Zenmuse H20T [15], with a continuous video stream to the UAV pilot."}, {"title": "4. Results", "content": "Out of the PIDNet variants, PIDNet-M was found to be the best for the task regarding the test set metrics regardless of the teacher model. However, for practical inference, the PIDNet-S was chosen because of the far superior inference speed on the NVIDIA Jetson Orin NX, shown in Table 3. Surprisingly, the largest PIDNet-L performed consistently worse than the PIDNet-M and comparably with the smallest PIDNet-S variant. This was likely due to overfitting to noisy pseudo-labels, caused by the teacher models which did not perfectly capture the human perception of the smoke. Still, it is interesting how this behaviour was reflected in all teacher variants."}, {"title": "4.1. Qualitative results", "content": "As the PIDNet-S model trained using SAM supervision was chosen for the real-world application, qualitative results of SAM and the corresponding PIDNet-S are shown in Table 4 alongside the input image, the bounding box label, and the human-drawn ground truth mask. For the rest of the models, qualitative results are provided in Appendix Tabs. 8 to 12.\nBesides the individual sample in which the PIDNet model can not distinguish the smoke, any large conclusions were difficult to draw from the qualitative results besides the fact that both the teacher and the student model could segment smoke from the test images of both datasets. However, some variety could be observed in predictions between all trained models, such as more false positives in the PIDNet-L outputs."}, {"title": "4.2. Ablation", "content": "We performed an additional ablation on the four different loss terms used for the distillation optimisation to observe their effect on the model inference. The ablation was performed using the SAM-supervised PIDNet-S model by omitting individual loss terms.\nWhile the test errors improved notably when one of the loss terms $l_2$ or $l_3$, which depend on the boundary prediction, was omitted, the effect was assumed to be at least partially caused by the supervising model. As the boundaries of the SAM-generated masks were visibly misplaced in multiple situations, weighing the boundary losses too highly could understandably weaken the segmentation performance of the model. However, as the usefulness of these loss terms has been indicated clearly in the studies they've been presented in, omitting their use completely in"}, {"title": "4.3. Real-world performance", "content": "The model was tested in practice at real forest-burning events described in Section 3.4. The model ran on the NVIDIA Jetson Orin NX computer carried by the UAV at approximately 6 fps due to the data recording and some CPU preprocessing which could be optimised further. The model itself was capable of inference at ~11 fps, which can be expected for a practical application in which video data is not saved, but the model output is only checked for positives, and an alert is sent in case fire is detected.\nThe model successfully detected the smoke at distances between 1.4 and 9.7 km, even though the presented smoke was small, especially compared to the UAV data used for training. Both inputs and outputs of the model were recorded as videos, some samples of which are provided as supplementary material. The resulting videos showed that, while some false positives appeared in regions without smoke, they were temporally far less consistent than the true positives caused by smoke, meaning they could be filtered with simple temporal constraints. Table 7 shows still frames from the test flight recordings.\nThe real-world tests provided clear evidence that the proposed methodology is suitable for practical applications. Importantly, it also shows that the trained model generalises well to new scenarios as the test situation was very different from what was seen in any of the training data. In addition, the quality of the input images was far worse in terms of lighting and contrast compared to the UAV training and test data."}, {"title": "5. Conclusions", "content": "The study shows that smoke segmentation for real-time early detection of wildfires is feasible with computational resources limited to UAV-carried edge computing resources. These models can be trained using bounding box-guided foundation models for pseudo-label supervision. The capability was tested on a diverse offline dataset against manual annotations where the proposed model achieved an accuracy of 63.3% mIoU. The system was also demonstrated in real-world tests where qualitative video results proved the system's capability in detection. The inference could run at ~11 fps on the lightweight NVIDIA Jetson Orin NX computer while sufficient results for reliable real-world onboard sensing were achieved with the system running at ~6 fps with the workflow including both the inference and video recording. The model successfully captured smoke from up to 9.7 kilometres in a scenario with little visible smoke.\nWhile the larger PIDNet-M model showed better results when trained with pseudo-labels obtained from the models trained using box supervision, we propose the SAM-supervised PIDNet-S as the best method for such tasks. The method benefits greatly from the zero-shot capability of SAM, requiring no task-specific training of the teacher model, thus enabling faster and more efficient adoption of the models for practical tasks. In addition, the box-supervised methods only showed larger benefits in the UAV dataset, containing easier-to-detect scenarios and only when using the larger inference models, which can not achieve sufficient inference speed for practical real-time use with the proposed hardware.\nWhile the study shows that the trained models are capable of distinguishing the wildfire smoke in practical scenarios, the study is still limited by the data collection environment and to extend the method to other locations, more varied data would have to be collected. Thus future work includes the possibility of using different types of weakly supervised data such as image-level labels or diffusion model-generated data [23, 55] which are even easier to obtain than bounding boxes. Additionally, the study opens avenues to further develop the practical early detection of wildfires. This could include incorporating 3D understanding and smoke pose estimation to effectively locate the fire. Also, the study was uniquely targeted at wildfire early detection but the use of the model and training framework should be further investigated in general with other real-time segmentation tasks with computational resource limitations.\nThe study showed that leveraging zero-shot prompt-guided foundation models for pseudo-labelling yields better results on the downstream task of real-time single-class segmentation than direct bounding box supervision methods and comparable results to pseudo-labels generated by fine-tuned but non-prompt-guided models. The results are limited to a single task with fairly restrictive data distribution but the results in the domain are definitive. However, the study does not imply that the statement holds for multi-class semantic, instance or panoptic segmentation and for those the proposed methods should be studied further."}]}