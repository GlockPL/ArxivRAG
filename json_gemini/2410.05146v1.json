{"title": "CTC-GMM: CTC GUIDED MODALITY MATCHING FOR FAST AND ACCURATE\nSTREAMING SPEECH TRANSLATION", "authors": ["Rui Zhao", "Jinyu Li", "Ruchao Fan", "Matt Post"], "abstract": "Models for streaming speech translation (ST) can achieve\nhigh accuracy and low latency if they're developed with vast\namounts of paired audio in the source language and written\ntext in the target language. Yet, these text labels for the tar-\nget language are often pseudo labels due to the prohibitive\ncost of manual ST data labeling. In this paper, we introduce\na methodology named Connectionist Temporal Classifica-\ntion guided modality matching (CTC-GMM) that enhances\nthe streaming ST model by leveraging extensive machine\ntranslation (MT) text data. This technique employs CTC to\ncompress the speech sequence into a compact embedding\nsequence that matches the corresponding text sequence, al-\nlowing us to utilize matched source-target language text pairs\nfrom the MT corpora to refine the streaming ST model fur-\nther. Our evaluations with FLEURS and CoVoST2 show\nthat the CTC-GMM approach can increase translation accu-\nracy relatively by 13.9% and 6.4% respectively, while also\nboosting decoding speed by 59.7% on GPU.", "sections": [{"title": "1. INTRODUCTION", "content": "Speech translation (ST) targets on the translation of spoken\naudio in one language into text in another language. Most re-\ncent ST systems use end-to-end (E2E) models [1], which are\ndifferent from the traditional cascaded systems that translate\nautomatic speech recognition (ASR) models' output with the\nmachine translation (MT) models. The most common E2E\nST models [2, 3, 4, 5] are attention-based encoder-decoder\n(AED) models [6, 7].\nStreaming simultaneous speech translation (ST) [8],\nwhich translates speech as it is spoken, is a crucial topic\nin the ST area. To stream AED models, chunkwise attention\nis needed, such as MoChA [9], MILk [10] and monotonic\nmulti-head attention [11, 8]. The most recent representation\nof streaming AED ST model is SeamlessM4T v2 [12].\nStreaming is also the most requested feature in ASR [13,\n14]. The streaming AED methods have been shown to be\nless effective than the recurrent neural network Transducer\n(RNN-T) [15], which is now the most common streaming\nE2E ASR model [16, 17]. Building on the success of stream-\ning E2E ASR models, RNN-T based E2E models were devel-\noped for low-latency and high-quality ST [18, 19]. Although\nRNN-T models are monotonic, they can deal with the word-\nreordering challenge in ST by using the flexible RNN-T path\nduring decoding as described in [18]. Those models are re-\nported to be used in commercial products [20], even without\nthe need of cloud connection [21].\nTo develop high-quality E2E ST models, we usually need\na large amount of paired speech and text data. However,\nit is much more expensive to use humans to label ST data\nthan ASR data. Therefore, a common practice is to use a\nMT model to generate pseudo labels of target language from\nthe reference texts in source-language ASR corpus [22, 23].\nThese pseudo labeled data are then used to train E2E ST mod-\nels. However, pseudo labels have errors that may affect the\nperformance of E2E ST models. To further improve ST model\nquality, we should also use the paired source/target language\ntext corpus that are used for training MT models.\nIn this paper, we propose CTC guided modality match-\ning (CTC-GMM) to improve the RNN-T based streaming ST\nmodel by using MT text training data. This is achieved by\nusing Connectionist Temporal Classification (CTC) [24] to\ntransform the speech sequence into a shorter embedding se-\nquence that resembles the text sequence. Then the speech\nembedding sequence or the text sequence is fed into a shared\nencoder. In this way, the matched source-target text pairs in\nMT corpus can be used along with matched speech-text data\nto build the streaming ST model. Due to the fact that the\nspeech embedding sequence is much shorter than the origi-\nnal speech sequence, the cost of the shared encoder inference\nand the decoding steps can be reduced, making the proposed\nmodel faster and more accurate. We evaluate the proposed\nmethod for translating German audio into English text with\nFLEURS [25] and CoVoST2 [26] test sets. Our proposed\nCTC-GMM model achieves a relative improvement of 13.9%\nand 6.4% respectively in BLEU scores compared to the base-\nline ST model.\nThe structure of this paper is as follows: Section 2 out-\nlines the related work. In Section 3, we detail our CTC-GMM\nmethod. Section 4 describes the experimental setup, while\nSection 5 presents the findings. We conclude the paper in\nSection 6."}, {"title": "2. RELATED WORKS", "content": "The authors of [27] introduced CTC compression to reduce\nthe length of the speech sequence based on its phonetic fea-\ntures. Our CTC-GMM work instead applies CTC to match the\nspeech embedding and text sequence so that MT training data\ncan be used. Moreover, to overcome the discrepancy between\nthe actual labels in training and inferred labels in testing, we\npropose a sampling strategy inspired by [28], which is shown\nto be essential for our algorithm.\nMany works have been done on joint speech-text model-\ning. SpeechT5 [29] and speechLM [30] align the speech and\ntext modalities with mix-up or swapping strategy. In [12],\nlength adaptor is applied to minimize the length discrepancy\nbetween speech and text, similar to M-adaptor [31] which\nuses multi-head pooled self attention. Prior works also heav-\nily rely on pre-training for speech and text modality align-\nment. In contrast, our proposed CTC-GMM does not need\nany pre-training and directly learn the alignment through CTC\ntargets on top of the speech encoder.\nAll the related works mentioned above are based on either\nAED or CTC models. Maestro [32] learns shared representa-\ntion for speech and text in RNN-T framework to benefit ASR.\nHowever, it requires duration modeling to upsample the text\nsequence to match the speech sequence length. Neither the\nshared encoder inference nor the decoding steps are reduced\nin terms of the computational costs.\nOne approach to enhance ST models with MT data in-\nvolves using text-to-speech (TTS) systems to generate audio\nfrom the source text in paired MT data [22]. The result-\ning source audio and target text pair is then used for train-\ning ST models. However, this method is expensive as it re-\nquires a separate TTS system and careful handling, like freez-\ning the speech encoder, to prevent bias towards TTS speak-\ners [33]. Although the recent advancement of zero-shot TTS\n[34, 35, 36, 37] can enrich the speaker diversity of generated\naudio, it needs even larger cost. In contrast, our proposed\nmethod doesn't have those constraints, directly consuming\nthe text data within the model.\nOne benefit of our proposed method is to significantly re-\nduce the decoding time. There are methods using uniform or\nadaptive downsampling to reduce the frame rate for fast com-\nputation [38, 39, 40]. However, these methods cannot be used\nto leverage text training data for model quality improvement."}, {"title": "3. STREAMING SPEECH TRANSLATION WITH\nCTC GUIDED MODALITY MATCHING", "content": "In this section, we will first introduce streaming speech trans-\nlation with RNN-T models, and then describe the proposed\nCTC guided modality matching method in detail."}, {"title": "3.1. Streaming speech translation with RNN-T", "content": "Compared to ASR, it is much more expensive to obtain\nhuman-labeled data for ST. A common practice is to use a\nMT model to generate pseudo labels from the reference texts\nin ASR corpus [22, 23] and those pseudo labeled data are\nthen used to train E2E ST models. The source speech and\ntranscription are represented as XS and YS, respectively. Af-\nter feeding YS into a MT system, the pseudo label in target\nlanguage is represented as \u0176T. Here, S and T denote the\nsource and target languages, respectively.\nThe baseline streaming speech translation model with\nRNN-T structure is shown in Figure 1. The speech sequence\nXS in the source language is mapped to the text sequence\nYT in the target language. The speech encoder produces\nhigh-level feature from XS, and the prediction network pro-\nduces predictor feature from previous non-blank output token.\nThen, a joint network combines the outputs of the speech\nencoder and prediction network, and generates the output\ntranslated text.\nRNN-T inference is a frame-synchronized decoding with\nbeam search. Therefore, the decoding cost is proportional\nto the sequence length of the speech encoder output. The\nstandard RNN-T beam search decoding algorithm is shown\nin Algorithm 1. Here Pr(y) is the approximate proba-\nbility of emitting output sequence y found by the search\nso far. Pr(kly,t) is the probability of extending y by\nk\u2208 Y at time t, where y is the extended output space\nset including all output label set Y and blank \u00d8. pref(y)\nis the set of proper prefixes of y, and for \u0177 \u2208 pref(y), let\nPr(y|\u0177,t) = \u03a0u=l+1 Pr(yu|Y[0:u\u22121],t). W is the beam\nsize. The probability Pr(k|y, t) can be written as:\nPr(ky,t) = softmax(zt,y)\nzt,y = Joint(henc, hdec) (1)\nAs shown in Algorithm 1, for each time t, given the en-\ncoder output ht henc, we need to generate new W candidates set"}, {"title": "3.2. CTC guided modality matching", "content": "In Figure 2, we show the proposed CTC guided modality\nmatching (CTC-GMM) based on the RNN-T structure, which\nenables the usage of MT data to further improve the ST qual-\nity. The text of source and target in the MT corpus are denoted\nas WS and WT, respectively. The source audio, source tran-\nscription, and target pseudo transcription in the speech corpus\nare XS, YS, and \u0176T respectively. Note that the source text\nin the MT and source transcription in speech corpus are dif-\nferent, denoted as WS and YS, respectively. However, the\nwords of them are converted into the same set of BPE tokens\nusing the same dictionary to ensure better alignment the text\nand speech modalities. The model takes either the speech in-\nput XS or the text input WS.\nThe speech encoder consumes XS as the input and pre-\ndicts the ASR label YS using the CTC loss. The speech\nencoder output sequence is denoted as {h1, h2, ......, hL},\nwhere L is the sequence length. The text encoder is a text\nembedding layer with the same output dimension as the\nspeech encoder output. After linear transform and softmax\noperation, the corresponding output CTC probability vector\nis {01, 02, ......, OL }.\nThen the CTC compression module squeezes the encoder\noutput based on the CTC probability in two steps. In the\nfirst step, the predicted token index for each time l is ob-\ntained by either selecting the index with the highest CTC\nprobability or by sampling method similar as in [28]. Specifi-\ncally for the sampling-based method, for each CTC output \u03bf\u03b9\n(l\u2208 [1, ......, L]), the indexes of the top N largest output val-\nues are picked, denoted as {k1,k2, kN} where kn \u2208 V,\nand then we generate a rand number r between 1 and N ac-\ncording to the below probabilities\nprob(r = n) = ol,kn / (\u03a3n=1 Ol,kn),  (n = 1, 2, ......, \u039d). (2)\nkr is finally selected as the predicted token index of time l. N\nis set to be 5 in the experimental settings.\nThe second step is to merge the consecutive speech en-\ncoder output hi with the same predicted tokens into one\nframe. Suppose the predicted token of time l is vr, the frames\nl = {i, ...., j} have the same predicted tokens v. The merged\nencoder output h could be calculated with several options:\n\u2022 Average: h = \u03a3\u00ed\u2212\u00bfhi/(j \u2212 i + 1)\n\u2022 Attention: h = Att(qu, WKh\u00b3, WVh),\n\u2022 Discrete: h = Embedding(v)\nwhere WK and WV represents the linear layers to obtain key\nand value in the attention layer, respectively. qu is the sinu-\nsoidal positional embedding with position u (the uth merged\noutput). Both the Average and Attention options generate\ncontinuous embedding which can be directly hooked up with\nshared encoder. For the attention option, the consecutive\nframes with blank as the predicted token are merged with"}, {"title": "4. EXPERIMENTAL SETTINGS", "content": "The experiments are to build ST models that translate Ger-\nman audio into English text. The speech training data is from\nan in-house German ASR corpus with 30 thousand (K) hours\nspeech data with the mixed bandwidth data [41]. All the train-\ning data is anonymized with personally identifiable informa-\ntion removed. We use a text-based MT service to convert the\nGerman ASR transcriptions into English texts as the pseudo\nlabels for ST training. The MT corpus has totally 280 million\n(M) paired German to English text sentences.\nWe use the German-to-English test sets in FLEURS and\nCoVoST2 to evaluate the performance of the proposed CTC-\nGMM model. The training speech and MT corpora do not\ncontain the FLEURS and CoVoSR2 data.\nThe acoustic feature is 80-dimension log Mel filter bank\nfor every 10 ms speech. The baseline E2E ST model is built\non the conformer transducer structure[42] with a streaming\nsetup [43]. The Convolution sub-sampling layer contains 2\nCNN layers with stride 2 for time dimension for each layer,\nwhich results in time reduction rate 4 (TR4). The base frame\nrate is 10ms so the frame rate after sub-sampling layer is\n40ms. The baseline model has 18 Conformer blocks in the\nspeech encoder, each contains 512 hidden nodes, 8 attention\nheads, and 1024 feed-forward nodes. The prediction network\nhas 2 LSTM layers with 320 embedding dimension and 1024\nhidden nodes. The joint network is a single feed-forward\nlayer with 512 nodes and the English word-piece size is 4K.\nThe total number of parameters is 100M. The baseline model\ncan produce high quality translation results. An additional\nbaseline with time reduction rate 8 (TR8) by stacking 3 CNN\nlayers with stride 2 for each layer is also built to further re-\nduce decoding time.\nFor CTC-GMM models, we set the CTC prediction\nbranch at the 12th layer of the speech encoder, and the shared\nencoder has 6 layers of Conformer blocks. The Conformer\nblock setup is the same as the baseline conformer transducer.\nTherefore, the total number of parameters is still 100M. The\nCTC output reference is in German word pieces and the total\nvocabulary size is 2K."}, {"title": "5. RESULTS", "content": "The evaluation results of the baseline and CTC-GMM models\non FLEURS and CoVOST2 are shown in Table 2. The trans-\nlation quality is evaluated with both BLEU [44] and COMET\n[45] metrics. The real time factor (RTF) is measured on a\nsingle H100 GPU. The baseline conformer transducer ST\nmodel achieves 28.0 and 34.5 BLEU scores on FLEURS and\nL = 0.1LcTC(Y|XS)+LRNN-T(\u0176T|XS)+LRNN-T(WT\\WS). \nIn every minibatch during training, the ratio of data from\nspeech corpus and MT corpus is 1: 1."}, {"title": "5.1. CTC-GMM with average operation", "content": "Without using the MT corpus, the CTC-guided compres-\nsion with the average operation degrades the BLEU score on"}, {"title": "5.2. CTC-GMM with attention operation", "content": "The CTC-GMM model using CTC compression and sampling\nstrategy obtains almost the same BLEU scores as the base-\nline ST model, with 28.0 and 34.7 BLEU scores on FLEURS\nand CoVoST2, respectively. With the help of MT data, the\nBLEU scores are improved to 29.1 and 35.7 on FLEURS and\nCoVoST2, respectively. Compared to its counter part with\naverage operation, the CTC-GMM with attention operation\ndoes not show significant improvements on translation qual-\nity although it has an additional attention layer. However,\nit achieves a better RTF as 0.024 than the average operation\n(0.27) without the use of the big shared encoder. The reason\nis that the attention operation will merge blank frames into\nthe non-blank frames as described in Section 3.2, and thus\nfurther reduces the sequence length compared to the average\noperation.\nFinally, by increasing the shared encoder with additional\n12 layers, the BLEU scores on FLEURS and CoVoST2 are\nboosted to 31.0 and 37.0, respectively. This only slightly in-\ncreases the decoding RTF to 0.025."}, {"title": "5.3. CTC-GMM with discrete operation", "content": "We have built two CTC-GMM recipes for the discrete option;\none preserves the blank token, while the other excludes it. In\nboth approaches, there is a notable decline in BLEU scores\nwithout supplemental MT text data; for instance, scores\ndropped from 28.0 to 23.6 on the FLEURS dataset. Intro-\nducing MT text data contributes to a significant increase in\nscores, yet they remain below the baseline. The scores are\nrose to 26.3 for the \"keeping blank\u201d approach and to 26.0\nfor \"removing blank\" approach on the FLEURS set, with\nlittle difference in ST quality. If we increase the shared en-\ncoder size to 18 layers, the \u201ckeeping blank\u201d recipe gets better\naccuracy than the \u201cremoving blank\" recipe, exemplified by\nBLEU scores of 28.9 versus 28.3 on FLEURS. Overall, the\noptimal outcomes for discrete operations achieved 28.9 on\nFLEURS and 34.4 on CoVoST2, comparable to or surpass-\ning the baseline. In terms of decoding speed, the \"removing\nblank\" method outperforms the \"keeping blank\" due to its\nfurther reduction in acoustic sequence length. However, the\n\"keeping blank\u201d incurs a higher RTF than the average strat-\negy, attributed to additional embedding processes."}, {"title": "5.4. Frame rate evaluation", "content": "As the RTF evaluation is machine dependent, we also list\nframe span of different models in Table 4 for reference. With\nthe time reduction 4 and 8, the baseline models with TR 4 and\nTR 8 are with 40 ms and 80 ms frame rates, respectively. C\u0422\u0421\ncompressor can significantly reduce the frame rate. On aver-\nage, each frame after the CTC compressor spans 147 ms for\nboth CTC average and discrete operations. CTC compressor\nwith attention operation or removing blank in discrete opera-\ntion can almost half the frame rate, with each frame spanning\n280 ms on average. The longer frame span results in the re-\nduced RTF as shown in Table 2."}, {"title": "5.5. Entity translation evaluation", "content": "In Table 3, we observed several examples that the CTC-GMM\nmodel can better translate the entity. Therefore in Table 5, we\nconducted a further evaluation by using an in-house German-\nto-English speech translation test set which has rich human-\ntagged entities. This test set contains 1942 utterances with to-\ntally 49454 words in source language transcription and 1700\nentities.\nThe baseline and the CTC-GMM (with the options of av-\nerage, sampling, MT text, and big shared encoder) models\nare the ones in Table 2. In addition to BLEU and COMET\nscore improvement for this new in-house test set, there was a"}, {"title": "6. CONCLUSIONS", "content": "In this paper, we presented CTC-GMM, the proposed method\nthat uses CTC to compress the speech sequence to better align\nthe text sequence. As a consequence, the MT training data can\nbe employed to improve the quality of streaming ST models.\nIn the proposed CTC-GMM, the sampling-based method was\nexplored to predict the token for each frame instead of se-\nlecting the one with highest probabilities. Besides, several\nCTC compression methods: average, attention and discrete,\nwere investigated, and the one with average operation gave\nthe overall best ST quality improvements. We conduct the\nexperiments by training ST models that translate German au-\ndio into English text. The BLEU scores were increased from\n28.0 and 34.5 to 31.9 and 36.7 on FLEURS and CoVoST2,\nrespectively. Since the compressed embedding sequence is\nmuch shorter, the decoding speed is significantly reduced ac-\ncordingly. The RTF was decreased from 0.072 to 0.029 when\nevaluating with a single H100 GPU machine.\nIn conclusion, CTC-GMM could help to get fast and ac-\ncurate speech translation model by using extra text transla-\ntion data. Although we only reported the ST results using the\nGerman to English direction, We started to expand to a large\namount of language pairs and are already seeing initial bene-\nfits which will be reported in the future. This approach is also\napplicable to other speech-related tasks like speech recogni-\ntion, as well as different model structures such as AED."}]}