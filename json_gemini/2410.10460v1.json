{"title": "Compositional Shielding and Reinforcement Learning for Multi-Agent Systems", "authors": ["Asger Horn Brorholt", "Kim Guldstrand Larsen", "Christian Schilling"], "abstract": "Deep reinforcement learning has emerged as a powerful tool for obtaining high-performance policies. However, the safety of these policies has been a long-standing issue. One promising paradigm to guarantee safety is a shield, which \u201cshields\u201d a policy from making unsafe actions. However, computing a shield scales exponentially in the number of state variables. This is a particular concern in multi-agent systems with many agents. In this work, we propose a novel approach for multi-agent shielding. We address scalability by computing individual shields for each agent. The challenge is that typical safety specifications are global properties, but the shields of individual agents only ensure local properties. Our key to overcome this challenge is to apply assume-guarantee reasoning. Specifically, we present a sound proof rule that decomposes a (global, complex) safety specification into (local, simple) obligations for the shields of the individual agents. Moreover, we show that applying the shields during reinforcement learning significantly improves the quality of the policies obtained for a given training budget. We demonstrate the effectiveness and scalability of our multi-agent shielding framework in two case studies, reducing the computation time from hours to seconds and achieving fast learning convergence.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement learning (RL) [27, 32], and in particular deep RL, has demonstrated success in automatically learning high-performance policies for complex systems [4, 23]. However, learned policies lack guarantees, which prevents applications in safety-critical domains. An attractive algorithmic paradigm to provably safe RL is shield-ing [3]. In this paradigm, one constructs a shield, which is a non-deterministic policy that only allows safe actions. The shield acts as a guardrail for the RL agent to enforce safety both during learn-ing (of a concrete policy) and operation. This way, one obtains a safe-by-design shielded policy with high performance.\nShield synthesis automatically computes a shield from a safety specification and a model of the system, but scales exponentially in the number of state variables. This is a particular concern in multi-agent (MA) systems, which typically consist of many variables. Shielding of MA systems will be our focus in this work.\nExisting approaches to MA shielding address scalability by com-puting individual shields for each agent. Yet, these shields are either not truly safe or not truly independent; rather, they require online communication among all agents, which is often unrealistic.\nIn this paper, we present the first MA shielding approach that is truly compositional, does not require online communication, and provides absolute safety guarantees. Concretely, we assume that agents observe a subset of all system variables (i.e., operate in a projection of the global state space). We show how to tractably synthesize individual shields in low-dimensional projections. The challenge we need to overcome is that a straightforward general-ization of the classical shield synthesis to the MA setting for truly independent shields often fails. The reason is that the projection removes the potential to coordinate between the agents, but often some form of coordination is required.\nTo address the need for coordination, we get inspiration from compositional reasoning, which is a powerful approach, allowing to scale up the analysis of distributed systems. The underlying principle is to construct a correctness proof of multi-component systems by smaller, \"local\" proofs for each individual component. In particular, assume-guarantee reasoning for concurrent programs was popularized in seminal works [5, 19, 24, 26, 31]. By writing <A>C(G) for \"assuming A, component C will guarantee G,\" the standard (acyclic) assume-guarantee rule for finite state machines with handshake synchronization looks as follows [14]:\n$\\langle \\texttt{T}\\rangle C_1 \\langle G_1 \\rangle, \\langle G_1\\rangle C_2 \\langle G_2 \\rangle, ..., \\langle G_{n-2}\\rangle C_{n-1} \\langle G_{n-1}\\rangle, \\langle G_{n-1}\\rangle C_n \\langle \\texttt{G} \\rangle$\n$\\langle \\texttt{T} \\rangle C_1 ||C_2|| ... ||C_n \\langle \\texttt{G} \\rangle$\nBy this chain of assume-guarantee pairs, it is clear that, together, the components ensure safety property G.\nIn this work, we adapt the above rule to multi-agent shielding. Instead of one shield for the whole system, we synthesize an indi-vidual shield for each agent, which together we call a distributed shield. Thus, we arrive at n shield synthesis problems (correspond-ing to the rule's premise), but each of which is efficient. In our case studies, this reduces the synthesis time from hours to sec-onds. The guarantees G\u012f allow the individual shields to coordinate on responsibilities at synthesis time. Yet, distributed shields do not require communication when deployed. Altogether, this allows us to synthesize safe shields in a compositional and scalable way.\nThe crucial challenge is that, in the classical setting, the com-ponents Ci are fixed. In our synthesis setting, the components Ci are our agents, which are not fixed at the time of the shield syn-thesis. In this work, we assume that the guarantees G\u012f are given, which allows us to derive corresponding individual agent shields via standard shield synthesis.\nMotivating Example. A multi-agent car platoon with adaptive cruise controls consists of n cars, numbered from back to front [20]"}, {"title": "1.1 Related Work", "content": "Shielding. As mentioned, shielding is a technique that computes a shield, which prevents an agent from taking unsafe actions. Thus, any policy under a shield is safe, which makes it attractive for safety both during learning and after deployment. Shields are typically based on game-theoretic results, where they are called winning strategies [8]. Early applications of shields in learning were pro-posed for timed systems [12] and discrete systems [3]. The idea has since been extended to probabilistic systems [16, 34], partial observability [10], and continuous-time dynamics [9]. For more background we refer to surveys [17, 18]. In this work, we focus on discrete but multi-agent systems, which we now review in detail.\nMulti-agent shielding. An early work on multi-agent enforce-ment considered a very restricted setting with deterministic envi-ronments where the specification is already given in terms of valid actions and not in terms of states [7]. Thus, the shield does not reason about the dynamics and simply overrides forbidden actions. Model-predictive shielding assumes a backup policy together with a set of recoverable states from which this policy can guaran-tee safety. Such a backup policy may for instance be implemented by a shield, and is combined with another (typically learned) policy. First, a step with the second policy is simulated and, when the target state is recoverable, this step is executed; otherwise, the fallback policy is executed. Crucially, this assumes that the environment is deterministic. Zhang et al. proposed a multi-agent version [37], where the key insight is that only some agents need to use the backup policy. For scalability, the authors propose a greedy algo-rithm to identify a sufficiently small subset of agents. However, the \"shield\" is centralized, which makes this approach not scalable. Another work computes a safe policy online [29], which may be slow. Agents in close proximity create a communication group, and they communicate their planned trajectories for the next k steps. Each agent has an agreed-on priority in which they have to resolve safety violations, but if that is not possible, agents may disturb higher-priority agents. The approach requires strong assumptions like deterministic system dynamics and immediate communication.\nOne work suggests to directly reinforcement-learn policies by simply encouraging safety [28]. Here, the loss function encodes a safety proof called barrier certificate. But, as with any reward engineering, this approach does not guarantee safety in any way. Another way to scale up shielding for multi-agent systems is a so-called factored shield, which safeguards only a subset of the state space, independent of the number of agents [13]. When an agent moves, it joins or leaves a shield at border states. However, this approach relies on very few agents ever interacting with each other, as otherwise, there is no significant scalability gain.\nFactored shields were extended to dynamic shields [33]. The idea is that, in order to reduce the communication overhead, an agent's shield should \"merge\u201d dynamically with the shields of other agents in the proximity. Since the shields are computed with a k-step lookahead only, safety is not guaranteed invariantly.\nMulti-agent verification. Rational verification proposes to study specifications only from initial states in Nash equilibria, i.e., as-suming that all agents act completely rationally [1]. While that assumption may be useful for rational/optimal agents, we typically have learned agents in mind, which do not always act optimally.\nThe tool Verse lets users specify multi-agent scenarios in a Python dialect and provides black-box (simulations) and white-box (formal proofs; our setting) analysis for time-bounded specifications [21]."}, {"title": "2 PRELIMINARIES", "content": "Given an n-vector v = (01, ..., vn), v[i] denotes the i-th element vi.\n2.1 Transition Systems (MDPs & LTSs)\nWe start with some basic definitions of transition systems.\nDefinition 1 (Labeled transition system). A labeled transition sys-tem (LTS) is a triple T = (St, Act, T) where St is the finite state space, Act is the action space, and TC St\u00d7 Act \u00d7 St is the transition relation with no dead ends, i.e., for all se St there exists some a \u2208 Act and s' \u2208 St such that (s, a, s') \u2208 T.\nDefinition 2 (Markov decision process). A Markov decision pro-cess (MDP) is a triple M = (St, Act, P) where St is the finite state space, Act is the action space, and P: St \u00d7 Act \u00d7 St\u2192 [0, 1] is the probabilistic transition relation satisfying \\(\\Sigma_{s' \\in St} P(s, a, s') \\in \\{0, 1\\}\\) for all s e St and a \u2208 Act, and for at least one action, the sum is 1.\nWe will view an LTS as an abstraction of an MDP where proba-bilities are replaced by possibilities.\nDefinition 3 (Induced LTS). Given an MDP M = (St, Act, P), the induced LTS is TM = (St, Act, T) with (s, a, s') \u2208 T iff P(s, a, s') > 0.\nDefinition 4 (Run). Assume an LTS T = (St, Act, T) and a finite alternating sequence of states and actions p = s0a0s1a1 . . . ; then, p is a run of T if (si, ai, Si+1) \u2208 T for all i \u2265 0. Similarly, for an MDP M = (St, Act, P), p is a run of M if P(si, ai, Si+1) > 0 for all i \u2265 0.\nWe distinguish between strategies and policies in this work. A strategy prescribes a nondeterministic choice of actions in each LTS state. Similarly, a policy prescribes a probabilistic choice of actions in each MDP state. Before defining them formally, we need a notion of restricting the actions to sensible choices.\nDefinition 5 (Enabled actions). Given an LTS, &(s) = {a \u2208 Act | \u2203s': (s, a, s') \u2208 T} denotes the enabled actions in state s. Similarly, given an MDP, &(s) = {a \u2208 Act | \u2203s': P(s, a, s') > 0}.\nDefinition 6 (Strategy; policy). Given an LTS, a (nondeterminis-tic) strategy is a function \u03c3: St \u2192 2Act such that 0 \u2260 \u03c3(s) \u2286 &(s) for all se St. Given an MDP, a (probabilistic) policy is a func-tion \u03c0: St \u00d7 Act \u2192 [0,1] such that \\(\\Sigma_{a\\in &(s)} \\pi(s, a) = 1\\) and \\(\\Lambda_{a' \\in Act \\&\\&(s)} \\pi(s, a') = 0\\) for all s \u2208 St.\nNote that our strategies and policies are memoryless. This is justified as we will only consider safety properties in this work, for which memory is not required [8]. Strategies and policies restrict the possible runs, and we call these runs the outcomes.\nDefinition 7 (Outcome). A run p = s0a0s1a1 .. of an LTS is an outcome of a strategy o if a\u00a1 \u2208 \u03c3(si) for all i \u2265 0. Similarly, a run p = soa0s1a1... of an MDP is an outcome of a policy n if \u03c0(si, ai) > 0 for all i \u2265 0."}, {"title": "2.2 Safety and Shielding", "content": "In this work, we are interested in safety properties, which are characterized by a set of safe (resp. unsafe) states. The goal is to stay in the safe (resp. avoid the unsafe) states. In this section, we introduce corresponding notions, in particular (classical) shields and how they can be applied.\nDefinition 8 (Safety property). A safety property is a set of states St.\nDefinition 9 (Safe run). Given a safety property St, a run S0A0S1 A1 is safe if si e o for all i \u2265 0.\nGiven an LTS, a safety property & St partitions the states into two sets: the winning states, from which a strategy exists whose outcomes are all safe, and the complement. The latter can be com-puted as the attractor set of the complement St \\\\ $ [8]. Since it is hopeless to ensure safe behavior from the complement states, in the following we will only be interested in outcomes starting in winning states, which we abstain from mentioning explicitly.\nA shield is a (typically nondeterministic) strategy that ensures safety. In game-theory terms, a shield is called a winning strategy.\nDefinition 10 (Shield). Given an LTS (St, Act, T) and a safety property \u2286 St, a shield [] is a strategy whose outcomes starting in any winning state are all safe wrt. \u0444.\nWe often omit & and just write. Among all shields, it is known that there is a \"best\" one that allows the most actions.\nDefinition 11 (Most permissive shield). Given an LTS and a safety property 6, the most permissive shield \u25bd* [6] is the shield that allows the largest set of actions for each state s \u2208 St.\nLemma 1 ([8]). * is unique and obtained as the union of all shields for : \u2207* (s) = {a \u2208 Act | \u2203: a \u2208 \u25bd(s)}.\nThe standard usage of a shield is to restrict the actions of a policy for guaranteeing safety. In this work, we also compose it with another strategy. For that, we introduce the notion of composition of strategies (recall that a shield is also a strategy). We can, however, only compose strategies that are compatible in the sense that they allow at least one common action in each state (otherwise the result is not a strategy according to our definition).\nDefinition 12 (Composition). Two strategies \u03c3\u2081 and 62 over an LTS (St, Act, T) are compatible if \u03c3\u2081 (s) \u2229 \u03c3\u2082(s) \u2260 0 for all s \u2208 St. Given compatible strategies \u03c3 and \u03c3', their composition \u03c3\u03a0\u03c3' is the strategy (\u03c3\u03c0\u03c3') (s) = \u03c3(s) \u2229 \u03c3' (s).\nWe write \u03a0i<j \u03c3\u00a1 to denote \u03c3\u2081 \u03a0... \u03a0 \u03c3j\u22121, and \u03a0\u1f76 \u03c3\u00a1 to denote \u03c3\u03b9 \u03a0... \u03a0\u03c3\u03b7 when n is clear from the context.\nGiven a strategy & and a compatible shield, we also use the alternative notation of the shielded strategy \u25bd(\u03c3) = \u03c3\u03a0\u03a0.\nGiven a set of states 6, we are interested whether an LTS ensures that we will stay in that set 6, independent of the strategy.\nDefinition 13. Assume an LTS T and a set of states . We write Tif for all strategies \u03c3, all corresponding outcomes s0a0s1a1 satisfy si e o for all i \u2265 0.\nWe now use a different view on a shield and apply it to an LTS in order to \"filter out\" those actions that are forbidden by the shield."}, {"title": "Definition 14 (Shielded LTS). Given an LTS", "content": "Definition 14 (Shielded LTS). Given an LTS T = (St, Act, T), a safety property 6, and a shield [], the shielded LTS T = (St, Act, T) with T = {(s, a, s') \u2208 T | a \u2208 \u2207(s)} is restricted to transitions whose actions are allowed by the shield.\nThe next proposition asserts that a shielded LTS is safe.\nProposition 1. Given an LTST, a safety property $, and a corre-sponding shield\u2207[6], all outcomes of any strategy for To are safe.\nIn other words, T = $. We analogously define shielded MDPs.\nDefinition 15 (Shielded MDP). Given an MDP M = (St, Act, P), a safety property 6, and a shield for TM, the shielded MDP M = (St, Act, P) is restricted to transitions with actions allowed by: P(s, a, s') = P(s, a, s') if a \u2208 \u25bd(s), and P(s, a, s') = 0 otherwise.\nProposition 2. Assume an MDP M, a safety property $, and a corresponding shield \u2207[6] for TM. Then all outcomes of any policy for M are safe.\nThe last proposition explains how standard shielding is applied to learn safe policies. Given an MDP M, we first compute a shield over the induced LTS TM. Then we apply the shield to the MDP M to obtain M and filter unsafe actions. The shield guarantees that the agent is safe both during and after learning.\nFrom now on we mainly focus on computing shields from an LTS, as the generalization to MDPs is straightforward."}, {"title": "2.3 Compositional Systems", "content": "Now we turn to compositional systems (LTSs and MDPs) with mul-tiple agents. We restrict ourselves to k-dimensional state spaces St, i.e., products of variables St = X; Sti. We allow for sharing some of these variables among the agents by projecting to observation subspaces. The following is the standard definition of projecting out certain variables while retaining others.\nDefinition 16 (Projection). A projection is a mapping prj: St\u2192 0 that maps k-dimensional vectors s \u2208 St to j-dimensional vectors o \u2208 O, where j \u2264 k. Formally, prj is associated with a sequence of j indices 1 \u2264 i\u2081 < \u2026\u2026\u2026 < ij \u2264 k such that prj(s) = (s[i], ..., s[ij]). Additionally, we define prj($) = Us\u2208${prj(s)}.\nDefinition 17 (Extension). Given projection prj: St \u2192 O, the set of states projected to o is the extension \u2191(o) = {s \u2208 St | prj(s) = 0}.\nLater we will also use an alternative projection, which we call restricted. The motivation is that the standard projection above sometimes retains too many states. The restricted projection instead only keeps those states such that the extension of the projection (\u2191()) is contained in the original set. For instance, for the state space St = {0, 1}2, the set of states $ = {(0, 0), (0, 1), (1, 0)}, and the one-dimensional projection prj(s) = s[1], we have that prj($) = {0, 1}. The restricted projection removes 1 as (1, 1) \u2209 \u03c6.\nDefinition 18 (Restricted projection). A restricted projection is a mapping prj: 2st \u2192 20 that maps sets of k-dimensional vectors se St to sets of j-dimensional vectors o \u2208 O, where j \u2264 k. Formally, prj is associated with a sequence of j indices 1 \u2264 i\u2081 < ... < ij \u2264 k. Let prj be the corresponding (standard) projection and \u2286 St. Then prj($) = {0 \u2208 O | {s \u2208 St | prj(s) = o} \u2286 $}. Again, we define prj($) = Use${prj(s)}."}, {"title": "We will apply prj only to safety properties \u0444. The following", "content": "We will apply prj only to safety properties \u0444. The following alternative characterization may help with the intuition: prj($) = prj($) = 0 \\ prj(St \\ \u03c6), where \u222e denotes the complement St \\ (resp. O \\ $) of a set of states \u2286 St (resp. observations & \u2286 O). Crucially, prj and prj coincide if \u2191(prj($)) = \u00a2, i.e., if the projec-tion of $ preserves correlations. We will later turn our attention to agent safety properties, where this is commonly the case.\nNow we can define a multi-agent LTS and MDP.\nDefinition 19 (n-agent LTS/MDP). An n-agent LTS (St, Act, T) or an n-agent MDP (St, Act, P) have an n-dimensional action space Act = Act\u2081\u00d7\u2026\u00d7 Act, and a family of n projections prj\u012f, i = 1, . . ., n. Each agent i is associated with the projection prj\u2081: St \u2192 O\u00a1 from St to its observation space Oi.\nWe note that the observation space introduces partial observabil-ity. Obtaining optimal strategies/policies for partial observability is difficult and generally requires infinite memory [11]. Since this is impractical, we restrict ourselves to memoryless strategies/policies. We can apply the projection function prj to obtain a \"local\u201d LTS, modeling partial observability.\nDefinition 20 (Projected LTS). For an n-agent LTS T = (St, Act, T) and an agent i with projection function prj;: St\u2192 Oi, the projected LTS to agent i is Ti = (Oi, Acti, Ti) where Act\u2081 = {a[i] | a \u2208 Act} and T\u2081 = {(prj;(s), a[i], prj\u012f (s)') | (s, a, s') \u2208 T}."}, {"title": "3 DISTRIBUTED SHIELD SYNTHESIS", "content": "We now turn to shielding in a multi-agent setting. The straight-forward approach is to consider the full-dimensional system and compute a global shield. This has, however, two issues. First, a global shield assumes communication among the agents, which we generally do not want to assume. Second, and more importantly, shield computation scales exponentially in the number of variables.\nTo address these issues, we instead compute local shields, one for each agent. A local shield still keeps its agent safe. But since we only consider the agent's observation space, the shield does not require communication, and the computation is much cheaper.\n3.1 Projection-Based Shield Synthesis\nRather than enforcing the global safety property, local shields will enforce agent-specific properties, which we characterize next.\nDefinition 21 (n-agent safety property). Given an n-agent LTS or MDP with state space St, a safety property & \u2286 St is an n-agent safety property if \u00a2 = \u2229i=1 di consists of agent safety properties di for each agent i.\nNote that we can let $i = $ for all i, so this is not a restriction. But typically we are interested in properties that can be accurately assessed in the agents' observation space (i.e., prj\u2081($i) = prj;(i)). Next, we define a local shield of an agent, which, like the agent, operates in the observation space.\nDefinition 22 (Local shield). Given an n-agent LTS T = (St, Act, T) with observation spaces O\u00a1 and an n-agent safety property $ = N=1 i St, let\u00a1 : O\u00a1 \u2192 2Acti be a shield for Ti wrt. prj;($i), for some agent i \u2208 {1, ..., n}, i.e., Ti Fo, prj;($i). We call \u012f a local shield of agent i."}, {"title": "Definition 23 (Extended shield). Assume an n-agent LTS", "content": "Definition 23 (Extended shield). Assume an n-agent LTS T = (St"}, {"title": "PROOF. By definition, each local shield", "content": "PROOF. By definition, each local shield; ensures that the (re-stricted projected) agent safety property i holds in Ti. Since Ti is a projection of T, any distributed shield with i-th componenti also preserves di in T (by Lemma 2). Hence, = \u3107\u2081 \u2191 (\u2207\u2081) ensures all agent safety properties \u00fei and thus $ = =1 i. \u03a0\nWe call = \u2191(\u2207\u2081) a distributed shield.\nUnfortunately, the theorem is often not useful in practice because the local shields may not exist. The projection generally removes the possibility to coordinate with other agents. By coordination we do not mean (online) communication but simply (offline) agreement on \"who does what.\" Often, this coordination is necessary to achieve agent safety. We address this lack of coordination in the next section."}, {"title": "3.2 Assume-Guarantee Shield Synthesis", "content": "3.2 Assume-Guarantee Shield Synthesis\nShielding an LTS removes some transitions. Thus, by repeatedly applying multiple shields to the same LTS, we obtain a sequence of more and more restricted LTSs.\nDefinition 25 (Restricted LTS). Assume two LTSs T = (St, Act, T), T' = (St, Act, T'). We write T < T' if T \u2286 \u03a4'.\nLemma 3. Let T < T' be two LTSs. Then T' \u3151 \u03c6 \u21d2 T\u3151 \u03c6.\nPROOF. AS T' contains all transitions of T, it has at least the same outcomes. If no outcome of T' leaves d, the same holds for T. \u03a0\nWe now turn to the main contribution of this section. For a safety property f', we assume an n-agent safety property $ = 1 i is given such that \u00a2 \u2286 \u03c6' (i.e., $ is more restrictive). We use these agent safety properties i to filter out behavior during shield synthesis. They may contain additional guarantees, which are used to coordinate responsibilities between agents.\nCrucially, in our work, the guarantees are given in a certain order. We assume wlog that the agent indices are ordered from 1 to n such that agent i can only rely on the safety properties of all agents j < i. Thus, agent i guarantees \u00fei by assuming \u2229j<i \u00d8j. This is important to avoid problems with (generally unsound) circular reasoning. In particular, agent 1 cannot rely on anything, and on is not relied on. The theorem then states that if each agent guarantees its safety property i, and only relies on guarantees dj such that j < i. The result is a (safe) distributed shield. The described condition is i formally expressed as (To\u00b0[Nj<i$j]) Fo\u2081 Pri\u2081($i), where we use the most permissive shield * for unicity.\nTheorem 2 (Assume-guarantee shield synthesis). Assume an n-agent LTST = (St, Act, T) with projections prj; and an n-agent safety property $ = Ni \u00f3i. Moreover, assume (local) shields \u2207\u00a1 for all i such that (To\u00b0[Nj<1$j]) Fo\u2081 Prj\u2081($i). Then, if \u2207 = n\u2081 \u2191(i) exists, it is a shield for T wrt. \u00a2 (i.e., T = $)."}, {"title": "PROOF. Assume T, 4, and local shields\u00a1 as in the assumptions.", "content": "PROOF. Assume T, 4, and local shields\u00a1 as in the assumptions. Observe that for i = 1, \u2229j<i $i = St, and that T* [St] = T. Then:\n$\\langle T^* \\rangle \\langle \\cap_{j < i} \\Phi_j \\rangle \\langle \\nabla_i \\rangle prj_i \\langle \\Phi_i \\rangle$\n$ \\Updownarrow Lem. 2$\n$\\langle T^{\\nabla^* [\\cap_{j < i} \\Phi_j]} \\rangle \\langle \\uparrow (\\nabla_i) \\rangle \\Phi_i$\n$ \\Updownarrow Def. 24$\n$\\langle T\\rangle \\langle \\cap_{j \\le i} \\uparrow (\\nabla_j) \\rangle \\langle \\Phi_i \\rangle$\n$\\langle T \\rangle \\langle \\cap_{j < i} \\uparrow (\\nabla_j) \\rangle \\Phi_i$\nStep (*) holds because the composition \u3107j\u2264i\u2191(\u2207j) of the local shields up to index i satisfy di under the previous guarantees $j, j < i. Thus, Trnj<i\u2191 (\u2207;) \u2264 To*[Nj<i \u00f8j], and the conclusion follows by applying Lemma 3. \u03a0\nFinding the local safety properties i is an art, and we leave algorithmic synthesis of these properties to future work. But we will show in our case studies that natural choices often exist, sometimes directly obtained from the (global) safety property."}, {"title": "4 CASCADING LEARNING", "content": "In the previous section, we have seen how to efficiently compute a distributed shield based on assume-guarantee reasoning. In this section, we turn to the question how and under which condition we can efficiently learn multi-agent policies in a similar manner. We start by defining the multi-agent learning objective.\nDefinition 26 (n-agent cost function). Given an n-agent MDP M = (St, Act, P) with projections prj\u2081: St \u2192 O\u00a1, an n-agent cost function c = (c1,..., cn) consists of (local) cost functions ci: O\u00a1 \u00d7 Acti\u2192 R. The total immediate cost c: St \u00d7 Act \u2192 R is c(s, a) = \\(\\Sigma_{i=1}^n c_i (prj_i (s), a[i])\\) for s \u2208 St and a \u2208 Act.\nAn agent policy is obtained by projection, analogous to a local shield. Next, we define the notion of instantiating an n-agent MDP with a policy, yielding an (n - 1)-agent MDP.\nDefinition 27 (Instantiating an agent). Given an n-agent MDP M = (St, Act, P) and agent policy \u03c0: \u039f\u00a1 \u00d7 Act\u00a1 \u2192 [0, 1], the instan-tiated MDP is \u039c\u03c0 = (St, Act', P'), where Act' = Act\u2081\u00d7\u2026\u00d7Acti-1\u00d7 Acti+1\u00d7\u2026\u00d7 Actn and, for all s, s' \u2208 St and a' \u2208 Act', P' (s, a', s') = \u03a3\u03b1;\u03c0(prji (s), ai).P(s, (a\u2032 [1], . . ., a' [i-1], ai, a\u2032 [i], . . ., a' [n-1]), s').\nWe will need the concept of a projected, local run of an agent.\nDefinition 28 (Local run). Given a run p = s0a0s1a1 ... over an n-agent MDP (St, Act, P), the projection to agent i is the local run prj;(p) = prji (so) ao [i] prj\u012f (s1) a\u2081 [i] ...\nGiven a policy \u03c0: St \u00d7 Act \u2192 [0, 1], the probability of a finite lo-cal run prj; (p) being an outcome of t is the sum of the probabilities of outcomes of n whose projection to i is prj\u012f (p).\nThe probability of a run p of length I being an outcome of pol-icy is Pr(p | \u03c0) = \u03a0i=0 \u03c0(Si, ai) P(Si, ai, Si+1). We say that agent i depends on agent j if agent j's action choice influences the probability for agent i to observe a (local) run.\n\u03a3\u03c1':\nPr(p' | \u03c0) \u2260 \u03a3\u03c1': prji (p')=prj;(p)\nDefinition 29 (Dependency). Given an n-agent MDP (St, Act, P), agent i depends on agent j if there exists a local run prji (p) of length l and n-agent policies \u03c0, \u03c0' that differ only in the j-th agent policy, i.e., \u03c0 = (\u03c01, ..., \u03c0\u03b7) and \u03c0' = (\u03c01, ..., \u03c0j\u22121, \u03c0';, \u03c0j+1, ..., \u03c0\u03b7), such that the probability of observing prj\u012f (p) under \u03c0 and \u03c0' differ:\n$\\Sigma_{p': prj_i (p')=prj_j (p)} Pr(\\rho' | \\pi) \\ne \\Sigma_{p': prj_i (p')=prj_j (p)} Pr(\\rho' | \\pi')$"}, {"title": "where we sum over all runs p' of length l with the same projection.", "content": "where we sum over all runs p' of length l with the same projection.\nIn practice, we can typically perform an equivalent syntactic check. Next, we show how to arrange dependencies in a graph.\nDefinition 30 (Dependency graph). The dependency graph of an n-agent MDP is a directed"}]}