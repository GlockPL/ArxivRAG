{"title": "PunchBench: Benchmarking MLLMs in Multimodal Punchline Comprehension", "authors": ["Kun Ouyang", "Yuanxin Liu", "Shicheng Li", "Yi Liu", "Hao Zhou", "Fandong Meng", "Jie Zhou", "Xu Sun"], "abstract": "Multimodal punchlines, which involve humor or sarcasm conveyed in image-caption pairs, are a popular way of communication on online multimedia platforms. With the rapid development of multimodal large language models (MLLMs), it is essential to assess their ability to effectively comprehend these punchlines. However, existing benchmarks on punchline comprehension suffer from three major limitations: 1) language shortcuts that allow models to solely rely on text, 2) lack of question diversity, and 3) narrow focus on a specific domain of multimodal content (e.g., cartoon). To address these limitations, we introduce a multimodal Punchline comprehension Benchmark, named PunchBench, which is tailored for accurate and comprehensive evaluation of punchline comprehension. To enhance the evaluation accuracy, we generate synonymous and antonymous captions by modifying original captions, which mitigates the impact of shortcuts in the captions. To provide a comprehensive evaluation, PunchBench incorporates diverse question formats and image-captions from various domains. On this basis, we conduct extensive evaluations and reveal a significant gap between state-of-the-art MLLMs and humans in punchline comprehension. To improve punchline comprehension, we propose Simple-to-Complex Chain-of-Question (SC-CoQ) strategy, enabling the models to incrementally address complicated questions by first mastering simple ones. SC-CoQ effectively enhances the performance of various MLLMs on PunchBench, surpassing in-context learning and chain-of-thought.", "sections": [{"title": "1 Introduction", "content": "Recent research on Multimodal Large Language Models (MLLMs) (Wang et al., 2024; OpenAI, 2024) has made rapid progress in vision-language tasks such as visual question answering (Antol et al., 2015), dense image captioning (Johnson et al., 2016) and optical character recognition (Islam et al., 2017). Despite the advanced capabilities of modern MLLMs in comprehending factual information from visual content, whether they can effectively grasp punchlines within the multimodal context remains an open question.\nAs illustrated in Figure 1, multimodal punchlines are typically presented as image-caption pairs (Cai et al., 2019), where humor or sarcasm is elicited through a striking contrast or alignment between visual and textual elements. Understanding these punchlines is important yet challenging for the development of MLLMs. On the one hand, multimodal punchlines are an essential way of communication on online multimedia platforms. Improving comprehension of punchlines is crucial for many real-world applications, including Human-AI interaction (Hempelmann and Petrenko, 2015) and sentiment analysis (Mahdaouy et al., 2021). On the other hand, unlike conventional visual question answering and captioning tasks, multimodal punchline understanding necessitates a nuanced perception of visual content, a strong grasp of language prior knowledge, as well as a deep understanding of the interplay between visual and textual information (Jing et al., 2023).\nThere are some prior studies on multimodal punchline comprehension, attempting to evaluate sarcasm explanation (Desai et al., 2022) and humor comprehension (Hessel et al., 2023), respectively. However, despite the valuable benchmarks presented by these studies, they suffer from three major limitations that hinder an accurate and comprehensive assessment of multimodal punchline comprehension. First, existing benchmarks overlook the potential shortcuts in the captions. As shown in the Yes/No QA task from Figure 1, CogVLM2 (Hong et al., 2024) can correctly identify that the original caption conveys a punchline regarding the image but fails when some words in the original caption are replaced with antonymous or synonymous ones. Additionally, the model can correctly answer Yes/No QA solely based on an inconsistent caption without visual input. This suggests that the model may exploit biased words (e.g., \"enjoy,\" \"plenty of\") or text-only inconsistencies (e.g., \"enjoy flying\" versus \"not enough legroom\") to arrive at the correct answer rather than genuinely understanding the multimodal punchline. Second, most previous benchmarks are constrained to a single question format (Cai et al., 2019; Desai et al., 2022), limiting their ability to assess the robustness of MLLMs across various user question formats. As depicted in Figure 1, the model can answer the Yes/No QA correctly but struggle with the Matching QA, highlighting performance variations across question formats. Third, prior works (Qiao et al., 2023; Hessel et al., 2023) solely focus on humor or sarcasm within a narrow domain (e.g., cartoon). This limits their applicability to broader real-world scenarios that convey punchlines, and hence causes insufficient evaluations.\nIn light of the above limitations, we introduce a novel multimodal Punchline comprehension Benchmark, PunchBench for short, designed to provide an accurate and comprehensive evaluation of this task. To enhance evaluation accuracy, we modify captions to mitigate the impact of potential shortcuts. Specifically, we apply context consistency adaptation to eliminate inconsistent captions, and then use word substitution and inversion to generate synonymous and antonymous captions with the help of ChatGPT (OpenAI, 2022). Regarding evaluation comprehensiveness, PunchBench features diversity across multiple dimensions. For punchline types, it includes both humor and sarcasm. For task types, it involves two levels of punchline understanding: shallow-level punchline perception and deep-level punchline reasoning. Each task employs diverse question formats:"}, {"title": "2 Related Works", "content": "2.1 Multimodal Large Language Models\nLarge Language Models (LLMs) for pure text like ChatGPT (OpenAI, 2022), GPT-4 (OpenAI et al., 2024), and LLaMA (Touvron et al., 2023) have proved impressive comprehension capabilities of text. Following this success and to expand it on multimodal tasks, many efforts (Li et al., 2023; Liu et al., 2023a) have been made to integrate visual comprehension capability into LLMs, and lead to a blowout of Multimodal Large Language Models (MLLMs), both closed-source models (e.g., GPT-4V (OpenAI, 2023a) and GPT-4o (OpenAI, 2024)) and open-source models (e.g., LLaVA series (Liu et al., 2023a, 2024a,b), CogVLM series (Wang et al., 2023; Hong et al., 2024), Qwen-VL family (Bai et al., 2023; Wang et al., 2024) and GLM-4V (GLM et al., 2024)). They demonstrate unprecedented and surprising multimodal understanding capabilities in vision-language tasks such as visual question answering (Antol et al., 2015), dense image captioning (Johnson et al., 2016) and optical character recognition (Islam et al., 2017).\n2.2 Punchline Comprehension\nDespite significant progress of MLLMs in understanding factual information from visual content (Long et al., 2023; Jian et al., 2024), the punchline comprehension capabilities (Cai et al., 2019; Ouyang et al., 2024) of MLLMs still lack sufficient evaluations. Prior works (Desai et al., 2022; Kumar et al., 2022; Hessel et al., 2023) related to multimodal punchline comprehension have concentrated on sarcasm or humor. For example, Desai et al. curated the MORE dataset for multimodal sarcasm explanation, which aims to explain the ironic semantics of multimodal post. Furthermore, previous benchmarks overlooked potential shortcuts in captions that MLLMs may exploit to answer questions, undermining true comprehension of punchlines. Noticing these concerns, our benchmark is introduced to provide an accurate and comprehensive evaluation of multimodal punchline comprehension."}, {"title": "3 PunchBench", "content": "As illustrated in Figure 2, our PunchBench is constructed in four steps: Source Data Collection & Annotation (\u00a7 3.1), Synonymous & Antonymous Caption Generation (\u00a7 3.2), Instruction Construction (\u00a7 3.3), Quality Checking (\u00a7 3.4). In this section, we elaborate on the construction process as well as the data statistics (\u00a7 3.5).\n3.1 Source Data Collection & Annotation\nThe image-caption pairs in our dataset are obtained from two sources. 1) Prior datasets. Recognizing the wealth of resources in prior datasets that contribute to punchline comprehension, we select three relevant datasets, i.e., MTSD (Castro et al., 2019), MORE (Kumar et al., 2022) and HUB (Hessel et al., 2023). Then, we meticulously filter the high-quality image-caption pairs using a hybrid approach that combines both manual and automatic filtering, as detailed in Appendix A.1. 2) Multimedia platforms. To ensure up-to-date of our dataset, we gather image-caption pairs from the social media platforms, such as X, Instagram, and YouTube. Additionally, we include image-caption pairs from the cartoon websites like Cartoon Movement and Cartoon Stock. The information about these multimedia platforms is provided in Appendix F.\nAfter obtaining the raw set of image-caption pairs, we implement a crowd voting process, which is outlined in Appendix A.1, to identify a label indicating whether the image-caption pair contains punchline. Ultimately, we compile a collection of 6,000 image-caption pairs spanning diverse scenarios (e.g., cartoon, post, comment, and meme), half of which are identified as containing punchline. To explain why the particular pair contains punchline, we employ three human annotators to handcraft reasoning sentence for it, which is detailed in Appendix A.1. Finally, we acquire 6,000 image-caption pairs along with their corresponding labels and reasoning sentences. To emphasize the superiority of PunchBench, we provide a comparison between our PunchBench and prior datasets in Table 4.\n3.2 Synonymous & Antonymous Caption Generation\nAs aforementioned, MLLMs may exploit shortcuts in the captions, such as word bias and context inconsistency, to answer the question without truly understanding the image-caption pair. To prevent these shortcuts, we generate synonymous caption and antonymous caption for each image-caption pair through following methods. 1) Word substitution and inversion. Assisted by gpt-3.5-turbo-0125, we substitute the sentiment, action, object and other words with synonymous words to generate synonymous caption,"}, {"title": "3.3 Instruction Construction", "content": "Based on the collected image-caption pairs and corresponding annotations, we now construct instructions for two types of tasks: Punchline Perception, which assesses whether an MLLM can identify the existence of punchline in image-caption pairs, and Punchline Reasoning, which requires the model to understand the reason why a particular image-caption pair contains punchline. Before delving into the details, we first clarify some notations.\nNotations. Each image-caption pair $P_i = <I_i, C_i>$ consists of an image $I_i$ and a caption $C_i^x$ where $x \\in \\{o, s, a\\}$ denotes the original ($C^o$), synonymous ($C^s$) and antonymous ($C^a$) caption. And each pair is assigned a label $L \\in \\{0, 1\\}$, where 1 indicates that the pair contains punchline while 0 is opposite. Notably, $P^s$ shares the same label as $P^o$, while $P^a$ serves as the contrast. We detail instruction construction process as follows, temporally omitting the subscript $i$ that indexes the samples for simplicity.\n3.3.1 Punchline Perception\nYes/No QA. The model is required to answer whether the given image-caption pair $P^x$ contains punchline. The instruction is derived based on various instruction templates, with the answer \u201cYes\u201d or \"No\u201d being determined by the label $L$. To attain a balance, the number of negative answers is equal to that of positive answers.\nMatching QA. The model is asked to select between two captions, recognizing which one effectively conveys punchline with the given image. For pair $P^x$ containing punchline, we utilize gpt-4o-2024-05-13\u00b9 to generate a distractor caption $C^d$ for the image $I$. The distractor caption $C^d$ just describes the content of image $I$ without conveying the punchline. Finally, the image-caption"}, {"title": "3.4 Quality Checking", "content": "To ensure the quality of PunchBench, we randomly sample 100 instructions for each question format, excluding Generation QA, for quality checking process. Three human annotators are employed to answer the questions guided by the sampled instructions. Human annotators have an extra option \"CBA\" that means \"Cannot Be Answered\" for each question. Among 500 instructions, only 1 is labeled by \"CBA\u201d, which verifies the high quality of the instructions. Moreover, they answer the questions with high accuracy as results reported in Table 1, which further demonstrates the superior quality of our dataset."}, {"title": "3.5 Dataset Statistics", "content": "We illustrate Figure 3 to exhibit the dataset statistics of our PunchBench. PunchBench consists of 6,000 image-caption pairs, spanning cartoon, post, comment and meme. Each image has three types of captions: original, synonymous, and antonymous captions. Our question formats include Yes/No QA, Matching QA, and Multi-option QA for punchline perception, and Yes/No QA, Matching QA, and Generation QA for punchline reasoning. Above all, our PunchBench covers a diverse question formats and domains, which can provide a comprehensive evaluation. We also compare our PunchBench with previous Benchmarks in Appendix A.4."}, {"title": "4 Simple-to-Complex Chain-of-Question", "content": "In our initial evaluation (the \u201cZero-shot\" results in Table 1), we observe that different question formats present varying levels of difficulty for the MLLMs. The general trend for punchline perception is Yes/No QA < Matching QA < Multi-option QA, and for punchline reasoning, it is Yes/No QA < Matching QA < Generation QA, where < indicates easier than. Inspired by these observations, we propose a Simple-to-Complex Chain-of-Question (SC-CoQ) strategy, which prompts MLLMs to answer the simpler questions before solving the most complex questions. Specifically, we introduce two variations of SC-CoQ, Intra-task and Inter-task:\nIntra-task SC-CoQ integrates the various formats of questions within the same task to improve performance on the most challenging question (i.e., Multi-option QA and Generation QA). We sequence the questions in a specific order mirroring simple to complex, i.e., <Yes/No QA, Matching QA, Multi-option QA or Generation QA>.\nInter-task SC-CoQ incorporates similar question formats (i.e., Yes/No QA and Matching QA) across different tasks to enhance punchline comprehension. For Yes/No QA, we sequentially link the questions from the two tasks, i.e., <Yes/No $QA_m$, Yes/No $QA_n$> or <Yes/No $QA_n$, Yes/No $QA_m$>, where m refers to punchline perception task and n denotes punchline reasoning task. For Matching QA, this chain utilizes both Yes/No QA and Matching QA to reinforce punchline comprehension across tasks, i.e., <Yes/No $QA_m$, Yes/No $QA_n$, Matching $QA_m$, Matching $QA_n$> or <Yes/No $QA_n$, Yes/No $QA_m$, Matching $QA_n$, Matching $QA_m$>. More details of SC-CoQ and specific prompting examples can be found in Appendix B."}, {"title": "5 Experiments", "content": "5.1 Baselines\nWe include both MLLMs and human baseline for evaluation as follows.\nEvaluated MLLMs. We evaluate four open-source MLLMs (i.e., LLaVA (Liu et al., 2024b), GLM-4V (GLM et al., 2024), Qwen2-VL (Wang et al., 2024), and CogVLM2 (Hong et al., 2024)) and two closed-source MLLMs (i.e., GPT-4V (OpenAI, 2023a) and GPT-4o (OpenAI, 2024)). And we adopt zero-shot, 3-shot (in-context learning) and Chain-of-Thought (CoT) as the baselines for prompting MLLMs. A detailed description of these models, their parameter settings, and introduction for in-context learning (Brown et al., 2020) and CoT (Wei et al., 2022) are provided in Appendix C.\nHuman Baseline. To make a comparison with human performance on punchline comprehension, we introduce a human baseline. Specifically, 1) for punchline perception, we first randomly select 100 instructions for each question format except Generation QA, and we then recruit human annotators (three undergraduates outside of the work) to answer the questions guided by the instructions. Notably, the manually annotated reasoning sentences serve as the performance of human baseline for the Generation QA.\n5.2 Evaluation Metric\nFor Yes/No QA, Matching QA and Multi-option QA, we utilize accuracy as the metric. A response is deemed correct when the candidate option (e.g., Yes/No, Option A/Option B, or A/B/C/D) mentioned in the response matches the ground truth option. The accuracy is then calculated as the ratio of correct responses to the total number of questions. For Generation QA, where the responses from MLLMs are free-form, we resort to gpt-3.5-turbo-0125\u00b2 to assess whether the response matches the semantics of the annotated reasoning sentence with a binary judgment \u201cYes\u201d or \u201cNo\u201d. Responses marked by \"Yes\" are considered correct and their ratio serves as the accuracy metric. To ensure the reliability of automatic evaluation, we analyze the correlation between automatic and human assessments. The details provided in the Appendix D.3 demonstrate that the automatic metrics align well with human judgments."}, {"title": "5.3 Main Results", "content": "The evaluation results of punchline perception and reasoning are presented in Table 1, and we conclude the following findings from five aspects.\nOverall Performance. The evaluated MLLMs exhibit limited capability of punchline comprehension, with the accuracy across different question formats for both punchline perception and reasoning falling below 80% in zero-shot setting. As can be seen, the closed-source models consistently surpass the open-source models, where GPT-4o achieves the leading performance among the evaluated MLLMs. Regrettably, GPT-4o still lags substantially behind human-level performance, revealing a substantial gap in punchline comprehension between MLLMs and humans.\nCross-task Performance. Comparing performance of MLLMs cross the two tasks, we can see that punchline reasoning poses greater challenges than punchline perception, since MLLMs perform worse in punchline reasoning. This disparity is expected, as punchline reasoning demands a deeper understanding to explain why a particular pair contains punchline, rather than simply identifying its presence. Consequently, punchline reasoning proves to be a more complex task for MLLMs compared to punchline perception.\nCross-question Performance. Comparing the results cross question formats within each task, we can observe that there exists a significant variation in performance. The reasons can be two folds. On the one hand, the complexity of the question formats varies inherently. From simplest to most complex, the question formats can be ranked as follows: Yes/No QA, Matching QA, Multi-option QA/Generation QA. MLLMs show a noticeable decline in performance as the complexity of the questions increases. On the other hand, individual models have varying innate strengths and weaknesses across different question formats. For instance, LLaVA exceeds GLM-4V in Yes/No QA but falls behind GLM-4V in Matching QA for punchline perception task.\nEffectiveness of SC-CoQ. Compared to the zero-shot setting, both 3-shot and SC-CoQ methods consistently improve performance across all question formats. While CoT method slightly degrades performance in Yes/No QA for punchline perception, it enhances performance in other question formats. Notably, SC-CoQ outperforms both 3-shot and CoT approaches corss various question formats, highlighting its superiority. The effectiveness of SC-CoQ is further validated in Section 5.4, where its performance improvements in synonymous and antonymous caption settings are analyzed."}, {"title": "5.4 Effect of Synonymous and Antonymous Captions", "content": "To explore the effect of synonymous and antonymous captions, we compare the performance of CogVLM2 cross the original, synonymous and antonymous captions, as illustrated in Figure 4. And the performance comparison for other models are provided in Appendix D.3. We analyze"}, {"title": "5.5 Qualitative Analysis", "content": "To provide an intuitive display, we illustrate some testing samples in Figure 5 for qualitative analysis. Part (a) showcases the responses from two representative models CogVLM2 and GPT-4o in the Yes/No QA. Both of them answer correctly when given the original caption, but fail when the original caption is replaced by the synonymous or antonymous caption. This indicates the biases existing in the captions and hence the models may not truly understand the inherent semantics of the image-caption pair to attain the answer. And it underscores the significance of introducing synonymous and antonymous captions in assessing punchline comprehension. Part (b) exhibits the responses of CogVLM2 with zero-shot, 3-shot, CoT and SC-CoQ for Multi-option QA. Notably, with the guidance of SC-CoQ, CogVLM2 successfully answers the question, whereas it fails under the other settings (i.e., zero-shot, 3-shot, and CoT). It highlights the effectiveness of SC-CoQ in enhancing punchline comprehension. More qualitative results for other question formats can be found in Appendix D.4."}, {"title": "6 Conclusions", "content": "We introduce PunchBench, a benchmark designed to evaluate the ability of MLLMs to comprehend multimodal punchlines. PunchBench distinguish itself from existing benchmarks in two key ways: First, it incorporates synonymous and antonymous captions to mitigate the risk of models relying on shortcuts in the original captions, achieving a more accurate assessment of their capabilities. Second, PunchBench includes a diverse range of punchline types, evaluation tasks, question formats, and multimodal content domains, ensuring a comprehensive evaluation. Our evaluation results highlight a significant gap between the performance of state-of-the-art MLLMs and human capabilities in understanding multimodal punchlines. To address this, we design the Simple-to-Complex Chain-of-Question (SC-CoQ), which effectively enhances the punchline comprehension ability of MLLMS and outperforms widely-used inference-time techniques such as in-context learning and chain-of-thought."}, {"title": "Limitations", "content": "In this work, we focus on multimodal punchline comprehension for the image-caption pairs, which only consist of static content. According to the evaluation results, MLLMs struggle with the punchline comprehension and fall behind humans. Extending this challenge to videos, where punchlines are often embedded in dynamic flows of information, poses even greater complexity. Unlike static images, videos require models to process temporal dynamics and integrate contextual cues across frames, demanding more advanced comprehension capabilities. Given the added challenges of punchline comprehension in video content, such as comedy, this area presents a meaningful avenue for further exploration. In future work, we aim to evaluate MLLMs' ability to understand punchlines within videos, advancing their capability to process and interpret dynamic multimodal content."}, {"title": "A More Details for PunchBench", "content": "Here we provide more details for the dataset construction for both punchline perception and reasoning.\nA.1 Source Data Collection & Annotation\nWe detail the data collection process.\nData Collection. 1) Data filtering. To reduce time-consuming and labor-cost, we introduce MLLM-based filtering method to answer the above questions to help filter the image-caption pairs. To prevent biases from MLLM, we randomly select a model from the set of evaluated MLLMs as the judge. It then is required to assess the quality of image-caption pairs by responding the following questions. Q1: \u201cWhether it contains possible ethics conflict?\u201d If No, go to the next question. Q2: \"Whether the content of image is clearly visible?\" If Yes, go to the next question. Q3: \u201cWhether the caption is well-written from the aspects of fluency, length and readability?\u201d If Yes, this image-caption pair passes the filtering process. To make sure the filtering quality, we randomly sample 500 image-caption pairs and then employ three undergraduates outside of this work to answer the above questions. Only 1 pairs of 500 fail to pass the manual filtering process, which verifies the reliability of automatic filtering process. 2) Crowd voting. To determine whether a collected image-caption pair contains a punchline, we conducted a crowd voting process using a questionnaire. Participants were asked, \"Does the given image-caption pair make you laugh?\" and could choose between \u201cYes\u201d and \u201cNo.\u201d Each questionnaire was considered valid if it received more than 10 votes. If one option garnered over 80% of the votes, it was assigned as the label for the corresponding pair. Notably, for the pairs collected from the prior datasets, we adopted the original labels. Specifically, if the pair is identified as humorous or sarcastic in previous datasets, we regarded it as containing punchline.\nData Annotation. To acquire reasoning sentences for particular pairs containing punchline, we employ three human annotators to write reasoning sentence based on the content of image and caption. Specifically, we provide the annotated sarcasm or humor explanations for the pairs existing in the previous datasets, which can be referred to write reasoning sentence. Reasoning sentence must cover the key components in image and caption that convey punchline, and the annotators should state how the interplay between visual content and textual information conveys punchline."}, {"title": "A.2 Synonymous & Antonymous Caption", "content": "We illustrate Figure 16 to present the prompts to guide gpt-3.5-turbo-0125 to generate synonymous and antonymous captions. And we provide more implementation details for context consistency adaption as follows. After identifying and isolating the two conflicting parts of inconsistent caption, we adopt word substitution and inversion to derive synonymous and antonymous captions. Specifically, we conduct word substitution for the former part and utilize word inversion for the latter part, if the generated caption maintain the punchline, we regard it as the synonymous caption. And we then conduct word substitution for the latter part and utilize word inversion for the former part, if the generated caption loses the punchline, we regard it as the antonymous caption."}, {"title": "A.3 Instruction Construction", "content": "Instruction Template. We provide various instruction templates for each question format, as follows. For punchline perception, the templates for Yes/NO QA are shown in Figure 21. The prompts for distractor captions generation and instruction templates for Matching QA are exhibited in Figure 22. The prompts for distractor options generation and instruction templates for Multi-option QA are exhibited in Figure 23. For punchline reasoning, the prompts for distractor reasoning sentence generation and instruction templates for Yes/No QA are exhibited in Figure 24. The instruction templates for Matching QA are exhibited in Figure 25. The instruction templates for Generation QA are exhibited in Figure 26."}, {"title": "A.4 Benchmark Comparison", "content": "We compare our PunchBench with the prior benchmarks related to multimodal punchline comprehension in Table 4. PunchBench shows superiority in domain, task, question format, punchline type."}, {"title": "B More Details for SC-CoQ", "content": "For the simplest question format Yes/No QA, we construct Inter-task SC-CoQ, i.e., <Yes/No $QA_m$, Yes/No $QA_n$>, <Yes/No $QA_n$, Yes/No $QA_m$>. m denotes punchline perception and n means punchline reasoning. Specifically, For a specific Yes/No $QA_m$ in punchline perception task, Yes/No $QA_n$>$ is filled by a randomly sampled Yes/No QA from punchline"}, {"title": "C More Details for Evaluation", "content": "Introduction for the MLLMs.\n\u2022 LLaVA (Liu et al., 2024b). We use llava-v1.6-mistral-7b in our experiment. It reuses the pre-trained connector of LLaVA-1.5 (Liu et al., 2023b) and adopts Mistral (Team, 2023) as the base LLM.\n\u2022 GLM-4V (GLM et al., 2024). It consists of GLMTransformer with 40 GLM Blocks and an EVA2CLIP Model with 63 Transformer Layers, along with a GLU mechanism.\n\u2022 Qwen2-VL (Wang et al., 2024). Qwen2-VL employs a 675M parameter ViT across various-sized LLMs, ensuring that the computational load of the ViT remains constant regardless of the scale of the LLM. In terms of language processing, we have opted for the more powerful Qwen2 (Yang et al., 2024).\n\u2022 CogVLM2 (Hong et al., 2024). It is a stronger version of CogVLM, which is an extension of Vicuna, incorporating ViT (Dosovitskiy et al., 2021) as the vision encoder, a two-layer MLP (Shazeer, 2020) as adapter, and introducing Visual expert module.\n\u2022 GPT-4V (OpenAI, 2023a) and GPT-4o (OpenAI, 2024). They are the leading MLLMs proposed by OpenAI.\nInference settings of the MLLMs. We present the inference settings, including decoding strategy and parameters of MLLMs in Table 2."}, {"title": "Introduction for in-conext learning and chain-of-thought.", "content": "1) In-context learning (ICL) (Brown et al., 2020). ICL enables models to perform tasks without explicit parameter updates by conditioning on a sequence of input-output examples, often referred to as a prompt. The model implicitly learns the task by observing these examples within the context, leveraging its pre-trained knowledge to generate predictions for new inputs. In this work, we adopt 3-shot prompt as one of the baselines. 2) Chain-of-Thought (CoT) (Wei et al., 2022). CoT prompting encourages models to generate intermediate reasoning steps in natural language, leading to more accurate and interpretable outputs for complex problems. By including step-by-step explanations in the prompt, CoT facilitates the decomposition of multi-step tasks, such as arithmetic, logical reasoning, or commonsense inference, into manageable sub-tasks. This approach significantly improves performance on reasoning-heavy benchmarks and highlights the potential of leveraging language models for tasks requiring structured thought processes."}, {"title": "D Evaluation and Analysis", "content": "D.1 Performance Variations\nWe compare the results cross the original, synonymous, and antonymous captions for all the evaluated MLLMs. The results for LLaVA, GLM-4V, Qwen2-VL, GPT-4V and GPT-4o cross different captions are exhibited in Figure 8, Figure 9, Figure 10, Figure 11, and Figure 12. As can be seen, synonymous and antonymous captions effectively eliminate shortcuts in the original captions, challenging models to fully comprehend the image-caption pairs. This leads to a more comprehensive evaluation of punchline comprehension capabilities. When using 3-shot and CoT methods, model performance with synonymous and antonymous captions lags behind that with original captions. However, when applying SC-CoQ, models show significant improvement across all caption types. This demonstrates that SC-CoQ enhances the models' ability to grasp the semantics of image-caption pairs, leading to better punchline comprehension.\nD.2 Human Evaluation\nTo validate the reliability of automatic evaluation for Generation QA, we conduct human evaluation through pairwise test. Specifically, we first randomly sample 100 pairs of reasoning sentences from two candidate models. And we then involve three independent annotators (undergraduate students uninvolved in this work) to compare reasoning sentences generated by two models (A and B) for the same image-caption pair. The annotators are supposed to choose one of three options: i.e., \u201cA Wins\u201d, \u201cA Draws B\u201d and \u201cB Wins\u201d. Finally, the winner is determined by the \u201cWin\" votes. If both models receive an equal number of \u201cWin\u201d votes, the final result is recorded as \"A Draws B\". In addition, we calculate Gwet's \u03b3 (Gwet, 2014) to represent inter-annotator agreement. The results for human evaluation of the generated reasoning sentences from evaluated models are shown in Table 3.\nD.3 Correlation between Automatic and Human Evaluation\nHuman evaluation results, which are presented in Appendix D.2, show substantial agreement among annotators since Gwet's \u03b3 (Gwet, 2014) is consistently larger than 70%. And we exhibit the correlation between Automatic and Human evaluation in Figure 7 to emphasize the reliability of automatic evaluation for Generation QA. As observed, the models or methods that rank higher in human evaluation also show better accuracy in automatic evaluation. And our SC-CoQ achieves the best performance in both automatic and human evaluation. It not only verifies the credibility of the automatic evaluation results, but also further demonstrates the advantages of our SC-CoQ."}, {"title": "D.4 More Qualitative Results", "content": "We provide result examples for Matching QA of punchline perception in Figure 13. As we can see, when using SC-CoQ, the model correctly answers the question, while failing when utilizing other prompting methods. For punchline reasoning task, we supply result examples for Yes/No QA and Matching QA in Figure 14. In addition, we present result examples for Generation QA in Figure 15."}, {"title": "E Documentation, Licensing, Potential risk and Intended Use of PunchBench", "content": "PunchBench encompasses 6,000 image-caption pairs and 54,000 question-answer pairs for multimodal punchline comprehension. We evaluate punchline comprehension in two levels: shallow-level punchline perception and deep-level punchline reasoning. We introduce three question formats for each task. We will release the dataset under CC BY-NC 4.0\u00b3. And for the preview use, the data source can be referred in the anonymous website Notably, there may be some offensive information in the images, despite we have made efforts to exclude the potential offensive information in the collection and filtering process. Furthermore, PunchBench should only be used for research purpose only."}, {"title": "F Annotators Recruitment and Multimedia Platforms", "content": "For human baseline, we employed three undergraduates outside of the work as the annotators. For human evaluation, we asked another three undergraduates"}]}