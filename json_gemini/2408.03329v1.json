{"title": "Coverage-aware and Reinforcement Learning Using Multi-agent Approach for HD Map QoS in a Realistic Environment", "authors": ["Jeffrey Redondo", "Zhenhui Yuan", "Nauman Aslam", "Juan Zhang"], "abstract": "One effective way to optimize the offloading process is by minimizing the transmission time. This is particularly true in a Vehicular Adhoc Network (VANET) where vehicles frequently download and upload High-definition (HD) map data which requires constant updates. This implies that latency and throughput requirements must be guaranteed by the wireless system. To achieve this, adjustable contention windows (CW) allocation strategies in the standard IEEE802.11p have been explored by numerous researchers. Nevertheless, their implementations demand alterations to the existing standard which is not always desirable. To address this issue, we proposed a Q-Learning algorithm that operates at the application layer. Moreover, it could be deployed in any wireless network thereby mitigating the compatibility issues. The solution has demonstrated a better network performance with relatively fewer optimization requirements as compared to the Deep Q Network (DQN) and Actor-Critic algorithms. The same is observed while evaluating the model in a multi-agent setup showing higher performance compared to the single-agent setup.", "sections": [{"title": "I. INTRODUCTION", "content": "By 2025, it is expected to have autonomous vehicles (AVs) on the road [1] in many developed countries around the world. However, to accomplish this, AVs must achieve the highest level of automation, Level-5 [2]. To this end, the new HD map application has been developed with the potential to provide the AV with a significant amount of road information with accuracy in centimeters to guarantee a safe driving experience. The construction of the HD map is possible due to the post-processing of the raw data generated by the AV's sensors, such as cameras or LiDARs. Furthermore, the HD map application has emerged to enable the next level of automation [3]. Nevertheless, Generating an HD map demands a high-powered computing capacity that can overwhelm the CPU [4], and it may be inefficient for each AV to create its map. Therefore, it is suitable that the raw data must be offloaded to the edge/cloud servers. For instance in [5], results demonstrated an improvement of 66% (from 23s to 7.8s for a 20m HD Map) for the generation of the HD Map. Nonetheless, for a Vehicular Ad-hoc Network (VANET) that utilizes the standard IEEE802.11p, there is a problem with the fixed CW size which increases the packet collision as studied in [6]. Consequently, in a high-density and high-mobility network such as VANET, there might be an increase in latency which translates into a higher processing time for the generation of HD maps. There have been efforts to improve the latency, the throughput, and the packet delivery ratio, through developing a new access category [7], utilizing machine learning algorithm such as Q-learning [8], or game theory solution [9].\nNonetheless, those solutions require adapting or modifying the current standard which is not suitable for a worldwide network with a defined standard. Furthermore, some of these solutions are limited to specific types of services or only HD maps, which may not align with real-world scenarios where AVs may transmit various types of data, including Best-effort (BE), video (VI), and Voice (VO). To address this, we have developed a multi-service and multi-agent system in a realistic scenario in a city in the United Kingdom. The solution operates in the application layer to mitigate the compatibility issue between wireless technologies. Additionally, we have incorporated the sojourn time which represents the time a vehicle spends in the coverage area."}, {"title": "A. Contributions", "content": "\u2022 A comprehensive model evaluation was conducted with a more realistic traffic flow scenario derived from a real-world environment. The multi-agent solution demonstrated the robustness of the model, which is re-trained to showcase the adaptability of the RL algorithm to new scenarios. This provides valuable insights into the practical significance of real-world scenarios.\nThe structure of the paper is as follows: Section II includes a thorough literature review on the CW, and Enhanced Dis-tributed Channel Access (EDCA). Section III describes the problem statement, while Section IV contains the machine learning algorithm. Section V provides the assumption con-sidered for the simulation and tools used. Section VI refers to"}, {"title": "II. RELATED WORK", "content": "In this section, it is described the related work performed with the aim of improving network performance in terms of latency and throughput in IEEE802.11 wireless networks."}, {"title": "A. Contention Windows & EDCA", "content": "Research efforts focused on improving IEEE802.11 by finding the optimal CW or modifying the Enhanced Distributed Channel Access (EDCA) access categories (ACs). One of the approaches is related to the management of the AC queues. For instance, Shinzaki et al. [10] has improved the latency by mapping packets between low and high-priority ACs queues which results in an enhancement of 13.8%.\nMoving forward, researchers have opted for modifying the current EDCA which provides new priority for specific services. For instance, a new AC for low-latency applications has been developed in [7], and others implemented an AC for each different type of video resolution [11]. Nonetheless, this is not sufficient. As it is known each AC has a fixed CW parameter which produces higher packet collision in a dense environment as studied in [6]. More specifically, in a Vehicular network as stated in [12] authors highlighted the negative impact the fixed CW has in the network in regards to the number of AVs and their velocity. In order to mitigate this problem, researchers have proposed new strategies for finding the optimal value of CW using machine learning paradigms. Those efforts include the use of Q-learning [8], policy gradient [10], and deep RL [13] algorithm. In [8], authors developed a Q-learning solution that led to a 12% fairness improvement whilst considering the delay in determining and assigning the optimal CW.\nNevertheless, these studies have shown improvements in latency, throughput, and fairness. their implementation can be quite complex as it necessitates specific changes to the current standard. Besides, many of those studies do not consider the use of HD Map data o multi-service scenarios. Furthermore, some of the required modifications may not be feasible for a multi-agent system. To address these challenges, we propose a multi-agent system where each Roadside Unit (RSU) functions as an agent, in addition to the inclusion of multi-service scenarios."}, {"title": "III. PROBLEM STATEMENT", "content": "The vehicles are defined as the set V = {1, ..., N}, after the vehicle enters the environment it is assigned a category within the set C = {1, ..., M}, which denotes the type of service the vehicle will transmit.\nIn environments like VANET, characterized by high mobility and density, it's crucial to distribute wireless resources effec-tively to meet the latency and throughput needs of each service. To enhance Quality of Service (QoS), we have implemented a utility function considering both latency and throughput, as detailed in [9]. This function provides a comprehensive evaluation of the network and ensures a balance for real-time applications that demand low latency while maintaining throughput. Moreover, it considers penalties and bonuses, as discussed in [14], which are applied when the latency (4) and throughput (5) constraints for each category are exceeded. Our ultimate goal is to maximize the utility function defined in eq. (1). The coefficients a1, and a2 are weights to provide a trade-off between R and L.\n$U(c) = \\alpha_1 \\frac{R(c)}{R_{max}(c)} + \\alpha_2 \\frac{L(c)}{L_{max}(c)} + P + B$ (1)\nHenceforth, the maximization problem for the RSU is for-mulated as follows:\n$max \\sum_{v \\in V} \\chi_vU_v(c), \\forall c \\in C, v \\in V$ (2)\nsubject to:\n$\\chi_v \\in \\{0,1\\}$ (3)\n$\\frac{1}{M} \\sum_{v=1}^M L_v(c) \\leq L_{max}(c), L \\in R$ (4)\n$\\sum_{v=1}^M R_v(c) \\geq R_{min}(c), R \\in R$ (5)\n$w_v(c) \\leq W_{max} (c) \u201aw\\in R, w\\neq 0$ (6)\nThe constraint (4) is the maximum latency, (5) is the minimum data rate, and (6) is the maximum waiting time allowed. Finally, $\\chi_v$ is the binary index that could be either 0 or 1 indicating that a vehicle is allowed to transmit.\nThe maximization problem at hand becomes more complex due to the involvement of multiple variables and relationships. The variables L and R exhibit direct and inverse proportional relationships with CW and the number of vehicles, respec-tively. Consequently, it is advantageous to employ reinforce-ment learning (RL) to solve the problem as it is well-suited for uncovering hidden patterns that may not easily perceptible using analytical strategies."}, {"title": "IV. DESIGN", "content": "The paper [14] suggests a QoS solution operating at the application layer without MAC layer modifications. In this paper, we introduced a decentralized multi-agent setup where RSUs act as agents, reducing latency by allocating waiting time for vehicles. Agents employ individual learning processes and act based on environmental feedback. The Q-learning model is compared to other updated RL algorithms such as DQN and Actor-Critic."}, {"title": "A. Reinforcement Learning", "content": "The RL algorithm is described in [14], it is a Q-learning Temporal Difference (TD), a model-free method. Thus, the agent receives the states of the environment and performs an action given by the policy $\\pi(\\alpha,s)$ using e-greedy epsilon. After the agent receives a reward r, the Q value table is updated using eq. (7), that contains the value of the next state with a discount factor (\u03b3), and a learning rate (a).\n$Q(s, a) = Q(s, a) + a [r + y * max_{a'}Q(s', a') \u2013 Q(s, a)]$ (7)\nIn [14], the utility function eq. (1) is selected as the reward function. Thus, substituting the utility function (1) in (7), and for simplicity denoting y = y * maxa' Q(s', a') \u2013 Q(s, a), the proceeding eq. (7) becomes,\n$Q(s, a) = Q(s, a) + a[U + y]$ (8)\n1) DQN: The DQN algorithm comprises several essential stages. Firstly, it requires calculating the target value using eq. (9). Subsequently, the loss function is computed using eq. (10). Finally, the main network parameter @ is updated according to eq. (11).\n$y = U + y * max_{a'} Q_{\\theta'}(s', a')$ (9)\n$L(\\theta) = \\frac{1}{K} \\sum_{i=1}^K(Yi - Q_{\\theta}(S_i, a_i)^2$ (10)\n$\\theta = \\theta \u2013 \u03b1\u2207L(\u03b8)$ (11)\n2) Actor-Critic: The Actor-Critic algorithm involves several key steps. Initially, the policy gradients are computed accord-ing to eq. (12). Subsequently, the actor network parameters \u03b8 are updated based on eq. (13). Following this, the loss function of the critic is computed using eq. (14). Finally, the critic network parameters \u00a2 are updated according to eq. (15).\n$\\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{s_t\u223cp^\u03c0} [\u2207_{\\theta} log \u03c0_{\\theta}(a_t|s_t)(r_t + \u03b3V_\u03c6(s_t) \u2212 V_\u03c6(s_t))]$\n$\\theta = \\theta + \u03b1\u2207J(\u03b8)$ (13)\n$J(\u03b8) = (r + yV_\u03c6(s_t) \u2013 V_\u03c6(s_t))$ (14)\n$\u03c6 = \u00a2 \u2212 \u03b1\u2207_\u00a2L($) (15)"}, {"title": "B. State, Action, Reward", "content": "For the state, action, and reward, we followed the same approach as developed by us previously in [14]. Moving on with the Q-learning design, the state, action, and reward are described in this subsection."}, {"title": "1) State", "content": "The state set S is defined as follows:\n$S = \\{S_j, T_v, C, T_{cv}\\}$ (16)\n$S_j$ being the sojourn time with discrete values from zero to four [14]. It describes the time a vehicle will spend in a certain base stations. The calculation is described in [14]. $T_v$ is the total number of vehicles active in the RSU. C is the set of categories, and $T_{cv}$ the total number of actives vehicles per category c.\na) Categories: The vehicles are designated one of the four categories randomly during the simulation, denoted by C = {VO, VI, BE, HD Map}."}, {"title": "2) Actions", "content": "In this design, we have opted for eight number of actions as stated in [14], each corresponding to a specific waiting time before the next transmission. To map the action to each category, we have used the equation (17) and Table I. The variable $a \\in N$ denotes the action number, while $W_{max}$ represents the maximum waiting time specified in Table I. Notably, the equation transforms the discrete action number into a continuous value.\n$w(c) = \u03b1 \\cdot (\\frac{w_{max}(c)}{\\Delta})$ (17)"}, {"title": "3) Reward", "content": "The utility function (1) will be utilized to calculate the reward, while taking into account the constraints for each of the categories and their respective penalties and bonuses. The constraints are linked with to the threshold values registered on Tables I. The utility, the values of 0.3 and 0.7 for $\\alpha_1$, and $\\alpha_2$ respectively, have been selected after exhaustive simulation. The allocation of these values revealed that the data rate is maintained while obtaining lower latency."}, {"title": "V. SIMULATION", "content": "To evaluate the solution, OMNet++ version 6 [19] is utilized as the network simulator, plus the library Inet frameworks [20]. The open source simulator and library provide the Physical, and MAC layer stack to simulate a wireless network using IEEE802.1p. Regarding the traffic flow, we use veins that connect OMNet++ with SUMO a continuous traffic simulation. Finally, for the machine learning, we opted to utilize and accommodate the solution developed by M. Schettler et al [21] to connect our RL algorithm to the OMNet++ simulation en-vironment. Please refer to Table II our simulation parameters.\nThe traffic flow generation is calculated by the number of vehicles on a heavy traffic road and rush hour. This information is extracted from the Traffic and Accident Data Unit / North East Regional Road Safety Resource [22]. Thus by dividing the total number of vehicles per hour, the entry interval is 0.66s.\nDuring the simulation, RSU functions as an autonomous agent, exercising independent decision-making capabilities. Information exchange occurs solely through the observation of the environment state, facilitating a decentralized decision-making process.\nThe analysis of the simulation is performed in terms of latency, and throughput Key Performance Indicators (KPIs). Latency is computed by subtracting the difference between the generated packet time, and the arrival time at the destination. Fairness is introduced only in the comparison between RL approaches. The formula to express fairness is Jain's fairness index [23]."}, {"title": "A. Validation of the model", "content": "We conducted various tests to evaluate the model's robust-ness across diverse scenarios. Initially, we configured each ser-vice with distinct packet sizes. Then, we investigated advanced RL algorithms such as deep Q-Network (DQN) and Actor-Critic. Finally, we subjected the model to tests in a multi-RSU setting, with traffic generation extracted from the dataset [22] to measure its performance.\n1) Adjusting different Packet Sizes for each Category: When considering different packet sizes, we ensured a consis-tent sending interval for each category: 10ms for voice, 1ms for video, 1.2ms for HD map, and 250us for best-effort data. However, the packet size has been changed from 900Bytes to 125Bytes for voice, 600Bytes for video, 600Bytes for HD map, and 900Bytes for Best-effort data. These adjustments were made while preserving the data rate requirements."}, {"title": "2) Comparison RL Complexity Algorithm", "content": "In addition, we conducted a comparison of various RL methods, including DQN and Actor-Critic, we kept all states, hyper-parameters, and actions the same as the Q-learning design. The results presented in Fig. 3 reveal that both DQN and Actor-Critic require extra fine-tuning work to achieve comparable or better results than the Q-learning algorithm proposed in [14]. For instance, DQN results exhibited similar behavior to Q-learning in terms of the level of priorities; However, it experienced a delay increase of 133% in voice, 50% in video and HD map, and 71% in best-effort. Despite this, the DQN approach demonstrated a throughput performance closer to the threshold of 1.25Mbps."}, {"title": "3) Multi RSU in Real Environment", "content": "Finally, we opted to evaluate the model in a more realistic environment, selecting the city of Newcastle within the UK as our city of choice. Our traffic data was supplied by Traffic and Accident Data Unit/North East Regional Road Safety Resource [22]. As shown in Fig. 4, we had a set of 7 RSUs configured with the technology IEEE802.11p. The AVs select the RSU based on the shortest distance. Firstly, the simulation was performed using the standard with and without EDCA. Secondly, the new AC for HD map solution was also evaluated. Afterward, the previously trained model was evaluated. Finally, the model was trained for ten episodes in two scenarios. The first scenario used a single agent, while the second used a multi-agent system with each RSU acting as an agent."}, {"title": "VI. RESULT AND DISCUSSION", "content": "The results of the suggested Q-Learning TD RL are con-trasted with the existing standard IEEE802.11p in various scenarios, including standard without QoS, and with QoS, new AC HD Map, and the single agent model without the incorporation of penalties and bonuses."}, {"title": "VII. CONCLUSION", "content": "Our research has resulted in the development of a multi-agent system for a multi-RSU scenario. It has shown that the single agent solution in [14] is transferable and could be re-trained to improve the network performance in terms of latency and throughput. Additionally, it reveals that the standard IEEE802.11p is insufficient to support a real-time HD map system in a real-world environment without adapting the packet size of each service. Nevertheless, it demonstrated that the reward function selected provides enough feedback in a multi-agent scenario to learn an optimal policy without communication between the agents directly. This is favorable as we avoid the misuse of the wireless channel. Also, the Q-learning solution demonstrated more efficiency. The latency and throughput were improved compared with a more complex and computationally-intensity RL algorithm such as DQN, and Actor-critic. It is important to recall, that there was no opti-mization performed on DQN and Actor-Critic algorithm. This outcome emphasizes the solution's potential for achieving de-sirable results with comparatively less tuning and optimization effort. It also indicates its viability for real-world deployment. Nevertheless, future endeavors may require adopting a multi-agent approach with more sophisticated reinforcement learning algorithms."}]}