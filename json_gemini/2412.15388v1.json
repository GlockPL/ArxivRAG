{"title": "Investigating Relational State Abstraction in Collaborative MARL", "authors": ["Sharlin Utke", "Jeremie Houssineau", "Giovanni Montana"], "abstract": "This paper explores the impact of relational state abstraction on sample efficiency and performance in collaborative Multi-Agent Reinforcement Learning. The proposed abstraction is based on spatial relationships in environments where direct communication between agents is not allowed, leveraging the ubiquity of spatial reasoning in real-world multi-agent scenarios. We introduce MARC (Multi-Agent Relational Critic), a simple yet effective critic architecture incorporating spatial relational inductive biases by transforming the state into a spatial graph and processing it through a relational graph neural network. The performance of MARC is evaluated across six collaborative tasks, including a novel environment with heterogeneous agents. We conduct a comprehensive empirical analysis, comparing MARC against state-of-the-art MARL baselines, demonstrating improvements in both sample efficiency and asymptotic performance, as well as its potential for generalization. Our findings suggest that a minimal integration of spatial relational inductive biases as abstraction can yield substantial benefits without requiring complex designs or task-specific engineering. This work provides insights into the potential of relational state abstraction to address sample efficiency, a key challenge in MARL, offering a promising direction for developing more efficient algorithms in spatially complex environments.", "sections": [{"title": "1 Introduction", "content": "Multi-Agent Reinforcement Learning (MARL) has emerged as an extension of single-agent RL, where multiple agents simultaneously interact with the environment to derive their optimal behavior by trial and error. Despite the complexity and dynamics of the learning environment, MARL holds significant promise for modeling real-world systems involving nuanced interactions between multiple entities, such as autonomous vehicle coordination (Shalev-Shwartz, Shammah, and Shashua 2016), traffic flow optimization (Agogino and Tumer 2012), and team robotics (Matignon, Jeanpierre, and Mouaddib 2012). These applications often involve collaborative or competitive dynamics that single-agent RL struggles to capture adequately. However, the increase in dimensionality over state and action spaces, the additional agent interactions, and the information influx this entails make sample efficiency a key challenge.\n\nA critical aspect to sample efficiency is how agents represent the information of what they observe. The observation received by an agent can hold a lot of information, but not all of it is necessary to make an optimal decision. The ability to find abstract features allows for reasoning on a higher, conceptual level and adds robustness to small, task-irrelevant changes; a common principle for good representations (Bengio, Courville, and Vincent 2012). Abstraction leverages the underlying structure of the problem to focus on relevant information while reducing its complexity. That way, agents can learn optimal policies with fewer interactions, leading to improved sample efficiency and knowledge transfer to new situations (Mohan, Zhang, and Lindauer 2024).\n\nA natural structure to present the environment is the decomposition into objects and their relations. Recent work in both deep single-agent RL and MARL has demonstrated the benefits of leveraging this relational structure as a graph-based representation in the learning architecture (Bapst et al. 2019; Jiang et al. 2021; Nayak et al. 2023; Agarwal et al. 2020), improving sample efficiency and generalization capabilities. This seems especially important in MARL, where the complexity often scales with the number of agents. The ability of Graph Convolutional Networks (GCNs) (Scarselli et al. 2009; Welling and Kipf 2016; Gilmer et al. 2017) to model systems where the relationships between entities are critical has brought them to the forefront in many fields of multi-agent systems, ranging from modeling behavior and trajectories in multi-agent systems (e.g. Kipf et al. 2018; Tacchetti et al. 2019; Li et al. 2020; Kipf, van der Pol, and Welling 2020) to enhancing communication (e.g Niu, Paleja, and Gombolay 2021; Jiang et al. 2020; Zhang et al. 2021b).\n\nIn this study, we investigate the integration of a simple yet effective relational abstraction to MARL using the architectural flexibility of GCNs. We focus on collaborative tasks with high spatial complexity where direct communication between agents is not permitted due to cost or security constraints, e.g., in underwater robotics (Song, Stojanovic, and Chitre 2019). We specifically emphasize spatial relationships, as these are ubiquitous and readily available in real-world multi-agent scenarios. Spatial relations provide fundamental information about the relative positions"}, {"title": "2 Related work", "content": "State Abstraction in RL Abstraction has been widely studied, with early work showing theoretical properties in single-agent RL (Li, Walsh, and Littman 2006; Abel 2022). Inspired by Zucker (2003) and focusing on state abstraction, we define abstraction as a mapping of the ground-truth representation of a state to a simpler, more compact representation by preserving desirable properties and removing less critical information. In other words, abstraction simplifies the information representation by dropping the information that is not essential to the task. Many methods have shown the success of embedding an abstract representation. For example, Kipf, van der Pol, and Welling (2020) factorize state inputs into objects and apply a relational, object-centric state abstraction to model a multi-object system. Zhang et al. (2021a) aim to learn an abstract state representation from high-dimensional observations based on the behavioral similarity between states to encode only information relevant to the task. Abdel-Aziz et al. (2024) reduce the computational complexity between communicating agents by learning a state abstraction based on quadtree decomposition (Samet 1984). Zhang et al. (2021b) use a state abstraction component in the MARL setting to reduce the high-dimensional observations into a more compact latent presentation using dense neural networks. While the abstraction method and assumption we take are different, we leverage these methods' underlying idea to discard any information irrelevant to the task to create a more compact and efficient representation.\n\nRelational Representation in RL Before the integration of deep learning, traditional RL methods often falter in environments with relational structures or when generalization beyond initial training conditions is necessary. Relational RL addresses these challenges by learning the optimal policies over the objects and relations using a relational representation such as first-order logic. Whilst this approach has shown improved generalization and scalability, both in single-agent RL (D\u017eeroski, De Raedt, and Driessens 2001; Sanner and Boutilier 2009; Driessens and D\u017eeroski 2001) and in MARL (Croonenborghs et al. 2006; Ponsen et al. 2010; Li et al. 2022), the use of first-order logic comes with constraints, such as the need to hand-engineer features (Garnelo, Arulkumaran, and Shanahan 2016). Contemporary methods tackle this issue by learning the relations between objects using deep learning methods (Garnelo, Arulkumaran, and Shanahan 2016; Jiang et al. 2021; Zambaldi et al. 2019). These methods assume that observations comprise entities and the relationships between them while using deep learning methods as an inductive bias to learn over these structures. For example, Zambaldi et al. (2019) learns the importance of non-spatial relations between entities using attention mechanisms (Vaswani et al. 2017). They show superior performance and generalization capabilities compared to purely local relations in single-agent RL. Jiang et al. (2021) connect entities of a grid with a broader set of spatial relations, including remote relations, into a heterogeneous graph and passes them through a Relational Graph Convolutional Network (R-GCN) (Schlichtkrull et al. 2018). Their findings indicate that the imposition of structure by inducing a spatial bias can lead to improved asymptotic performance and generalization capabilities in single-agent tasks. However, they treat every cell in the grid as an entity, whether or not it contains an environment object. We extend this idea to MARL by employing a relational representation between agents and objects in the environment where the importance of the induced relations is implicitly learned using R-GCNs. Additionally, we only consider the environment objects, i.e. agents and other objects, as entities and use fewer relations, proposing a lean and abstract representation that better aligns with the computational complexity of MARL and works on continuous domains as well.\n\nRelational Inductive Bias in MARL Relational inductive bias, loosely defined as the imposition of structural constraints on the learning process based on the relationship between objects (Battaglia et al. 2018), is a principle commonly embedded in MARL architectures.\n\nA natural way to leverage structure in MARL is on the agent level. In fact, many of the commonly known MARL algorithms own an architecture that represents a form of relational inductive bias based on the structure between agents. For example, value decomposition methods such as QMIX (Rashid et al. 2018) assume conditional independence between the agents to decompose their value function. MAAC (Iqbal and Sha 2019) assumes that the influence of one agent's information on another can vary. Each critic can dynamically select which agents to focus on by assessing the relevance of the encoded information from other agents via a multi-head attention layer that is shared between critics. The actor-critic methods introduced by Liu et al. (2020) extend the constraint imposed in MAAC by using an additional hard-attention layer to strengthen the assumption that not all other agents' information is relevant to succeed, further re-"}, {"title": "3 Methodology", "content": "3.1 Preliminaries\nWe work under the framework of partially observable Markov Games, with S being the state space in which each of the N agents has their own action space $A_i$, with i = 1,..., N, forming a joint action space $A = A_1 \\times A_2 X ... \\times A_N$. After taking an action, each agent i receives an observation $o_i \\in O \\subset S$. Moreover, we assume individual reward functions, $R_i : S \\times A \\rightarrow R$, which gives a reward signal after every step. At each time step, the agents simultaneously choose actions according to their respective policies, $\\pi_i: O_i \\rightarrow P(A_i)$, which depend on the observation they receive. Consequently, the environment changes in line with the transition dynamics $T : S \\times A \\times S \\rightarrow [0,1]$ to a new state. The goal is that every agent finds the optimal policy that maximizes their expected cumulative return $J_i(\\pi_i) = \\Sigma_{t=0} \\gamma^t r_t$, where $\\gamma \\in (0, 1]$ is the discount factor incorporating uncertainty about future returns, and where r is the individual reward received at time step t.\n\n3.2 Abstract Observation Representation\nOur objective is to design a sample-efficient multi-agent actor-critic architecture that decomposes the observations based on spatial inductive biases. We achieve this by employing a form of state abstraction: we simplify the observation representation by dropping information that is not essential to the task. This can also be described as domain reduction where we collapse observations into equivalent clusters, causing some observations to be indistinguishable and ultimately reducing observation complexity (Zucker 2003).\n\nThe abstraction assumption we make is that the relative positioning of entities is relevant, not their absolute positions. We are inducing an equivalence between observations, where we group observations with a similar spatial structure. This induces a translation invariance that applies to both remote and local relations in the observation space. For this to hold, we assume that the relative spatial relations can be extracted from the observation. This type of spatial information is inherent in many common environments and real-world scenarios and offers an intuitive example of using existing structures in the observations.\n\nWe hypothesize that this abstraction is particularly fruitful in discrete domains. Discrete states can be clearly separated from each other, which makes it easy to exactly define boundaries for any spatial relations. In contrast, continuous state spaces have a higher state complexity, as they are infinite expressions of the state and small changes can have a significant impact on the optimal action. Hence, clustering continuous state spaces can lead to a stronger loss of information in the representation that could impact performance (Li, Walsh, and Littman 2006). Whilst these challenges may affect the effectiveness of our abstraction, we test the robustness of our approach on continuous domains as well.\n\nEffectively leveraging graph structures to impose such inductive bias in MARL poses the key challenges of (a) determining how entities are represented; (b) finding an informative yet efficient use of relations; (c) aggregating information across the graph to propagate relevant signals; (d) finding a computationally efficient way of incorporating the structural information into the MARL architecture. In the following, we address these challenges through specific design choices to create a structured, more compact observation representation that leverages the permutation invariance to the order of entities and the translation invariance to the absolute position between entities. An overview of the steps and our overall architecture can be seen in Figure 1.\n\nEntity Representation In many of the discussed MARL methods, the focus lies on the interaction between agents, where the information they share is usually an encoding of their individual observation and action (e.g. Iqbal and Sha 2019; Liu et al. 2020; Jiang et al. 2020; Zhang et al. 2021b). We want to emphasize the structure already present within the observation itself. Hence, we aim to find a structured representation of the observation that does not only consider the agents but also all the other objects in the environment.\n\nTypically, observations are given as fixed-sized vectors that contain the positions and attributes of agents and objects. This enforces an artificial ordering between the entities that is not desirable. The structure of such an observation is commonly a design choice and can be varied without great loss of generality. Consequently, we assume that the positions and attributes of the agents and environment objects can be extracted. In detail, we first construct an entity set V from all agents and objects. We then take the non-spatial information from all agents and objects, such as their level,"}, {"title": "4 Experiments", "content": "In this section, we detail the experimental setup designed to evaluate the performance and capabilities of our proposed algorithm. We first describe the environments chosen, which are tailored to challenge and showcase the algorithm's spatial reasoning and collaborative capabilities in relationally complex tasks. We then outline the baseline algorithms against which we compare our approach to understand the added value of the relational inductive bias, followed by a comparative analysis of our results.\n\n4.1 Environments\nWe hypothesize that the introduced abstraction learns effectively in spatially complex coordination tasks with sparse rewards. On this basis, we designed a new highly collaborative environment that requires coordination between different types of agents and several object types. Furthermore, we select other collaborative grid environments as they naturally challenge the algorithm's capabilities in spatial reasoning and cooperation under sparse rewards.\n\nWhilst the employed state abstraction is particularly well suited to the discrete nature of these environments, we also evaluate our method on a continuous domain. Following is an overview of the chosen environment and for further details, we refer to the appendix.\nCollaborative Pick and Place (CPP) is a new, collaborative environment with two types of agents that need to pick up and drop off a box at a designated goal, entailing heterogeneous, collaborative agents. Only the picker agents can collect a box whereas the delivery agents can only receive a box and drop it at at a goal location. Once a box is dropped at the goal location, no other box can be placed there. At the beginning of each episode, the boxes, agents and goals are randomly spawned on the grid. Depending on their role, agents receive a reward for successful pick-ups, passes, and drop-offs, as well as for prompt completion of the task. In our experiments, we test the challenging setting of a 10 \u00d7 10 grid, 2 picker agents, 2 delivery agents and 3 objects\u00b9.\nLevel-based Foraging (LBF) (Christianos, Sch\u00e4fer, and Albrecht 2020) situates agents in a grid world where they are rewarded for collecting fruits. As opposed to the original LBF environment, we assume that fruits are on trees that remain on the grid after the fruit has been collected, with a value of -1. This alteration demands a higher relational reasoning capability from agents, as they must now navigate around the trees, recognizing them as noncollectable obstacles. For testing high cooperation, our experiments run on a 10 \u00d7 10 grid with 4 agents and 4 foods, enforcing cooperation (denoted as 10x10-4a-4f-coop). To assess scalability, we extend the environment to a 15 \u00d7 15 grid with 8 agents and 1 fruit (denoted as 15x15-8a-1f-coop).\nIn Wolfpack (Rahman et al. 2023), 3 agents are placed in a 10 x 10 grid to capture 2 prey. In a departure from the original setup, we have introduced sparse rewards by removing additional rewards based on the proximity to prey, significantly weakening the learning signal.\n\nThe Target task, based on the multi-agent particle environment (Lowe et al. 2017) and modified by Nayak et al. (2023), is a continuous domain environment where a number of agents try to reach their target landmarks while avoiding collision with obstacles and other agents.\n\n4.2 Baselines\nIn our study, we choose the baselines based on the following criteria: performance, reproducibility, ability to handle discrete action spaces and similarity to our approach. Following is an overview of the selected baselines; implementation details and hyper-parameter selection can be found in the appendix.\nMAAC (Iqbal and Sha 2019) also uses SAC as the base RL algorithm. The use of attention between agents represents a different form of relational inductive bias on agent interaction rather than our object-centric representation.\nGA-AC is the AC algorithm that makes use of the G2ANet mechanism (Liu et al. 2020). It builds on MAAC with an ad-"}, {"title": "4.3 Asymptotic Performance and Sample\nEfficiency in Discrete Domains", "content": "In this section, we present a comparative analysis of the asymptotic performance and sample efficiency as illustrated in Figure 2. Asymptotically, MARC is competitive and outperforms all baselines across the implemented tasks. Additionally, MARC demonstrates superior sample efficiency, learning all the tasks the fastest. In the LBF-15x15-8a-1f-coop task, MARC reaches an average performance of 99% after 5.9e5 environment steps, whereas the second-best algorithm, MAAC, takes 7.3 times the number of steps to reach the same performance.\n\nThe most significant margins in asymptotic performance are achieved in CPP and LBF-10x10-4a-4f-coop, where MARC achieves a performance gain of 69.9% and 35.2% respectively, as displayed in Figure 2a and Figure 2b. They require a high level of coordination and spatial understanding between entities to succeed in the task. In the LBF-10x10-4a-4f-coop setting, MARC reaches 26% of the maximum returns, on average, in 1e6 steps, while the second-best algorithm, MAPPO, reaches the same performance in 5.6 times the number of steps. MAAC performs relatively well in tasks that highly depend on coordination between agents, such as LBF-15x15-8a-1f-coop and Wolfpack, as visualized in Figure 2c and Figure 2d, respectively. However, MAAC's performance deteriorates in CPP and LBF-10x10-4a-4f-coop, where information about objects is essential to gain a good understanding of the environment. GA-AC and MAAC do not have a significant performance difference, indicating that the additional hard-attention layer on the agent interactions"}, {"title": "4.4 Generalisation Performance", "content": "To assess the ability of our method to generalize to out-of-distribution settings, we evaluate our model trained on the most difficult scenario of LBF, 10x10-4a-4f-coop, where MARC achieves 81% of the maximum performance, on a varying number of agents and fruits. We then compare our algorithm by training the best-performing algorithm on this task, MAPPO, with the same varied number of fruits and agents. When reducing the number of agents available to collect fruits to 3, MARC still achieves 38% of the performance, whilst MAPPO's performance fully deteriorates to 0%. Increasing the number of agents by 1 makes the task easier and yields an improved performance of 93% vs. 88% for MAPPO. This indicates that MARC learns an invariance to the number of agents. The performance decreases to 59% with an increase in fruits (from 4 to 6 fruits), but given that the number of environment steps remains fixed it generally becomes more difficult to fulfill in time and can still be considered robust. In comparison, MAPPO's performance decreases by 40% down to 19%. An overview of all generalization results can be found in the appendix."}, {"title": "4.5 Extension to Continuous Domain", "content": "As seen in Table 1, MARC performs stronger than the SOTA graph-based algorithm InforMARL, underlining the strength of our graph design also in continuous domains. It is also competitive with the best-performing baselines, MAAC, GA-AC and MAA2C. Deeper analysis shows that the performance margin comes from MARC taking, on average, 1-2 steps longer to reach the target. There is a trade-off"}, {"title": "5 Ablation Studies", "content": "Given the immense flexibility of graph architectures, we aim to shed light on how different design choices affect performance by systematically varying the following aspects:\n\nChoice of Relations To understand how the choice of relations impacts performance, we evaluate our experiments with 3 different groups: our default relations, local relations representing a convolutional kernel, and all relations as the union of the two, detailed in the appendix. We found that purely local relations are not sufficient to learn the task, achieving only 10.6% of the performance achieved by the chosen architecture. This seems intuitive, as the agent gains a deeper spatial understanding if they can infer information from all entities, even if they are further away. Additionally, adding local relations to our default set does not elevate the performance, indicating that our default relations offer a sufficient and strong enough spatial bias.\n\nNumber of Entities We compared our approach of considering only agents and objects in the graph to using all grid elements as entities. Learning over the full grid compared to our choice of compact representation is, despite being more informative, not as sample efficient, reaching only 46.9% of the performance achieved by the chosen architecture in 8e6 environment steps, along with a higher computational cost.\n\nChoice of Graph Architecture We explore alternative choices of aggregating information from connecting entities in the graph. Whilst the choice is vast, we focus on previous work in the single-agent literature, where relational inductive bias between the entities is introduced via multi-head attention (Zambaldi et al. 2019). For this, we construct a binary graph and pass it through a Graph Attention Network (GAT) Veli\u010dkovi\u0107 et al. (2018). Furthermore, we combine the approaches of spatial relations and varying importance between entities as in GATs by using an R-GAT layer (Busbridge et al. 2019) on the graph constructed in Section 3. For a detailed display of these alternative implementations, we refer to the appendix. Our R-GAT and R-GCN implementation yield indistinguishable performance, indicating that implicitly specifying different importance between entities does not yield a more expressive representation and its computation is therefore not required. In contrast, the use of a GAT layer yields suboptimal performance, asymptotically reaching only 23.4% of the chosen architecture's performance. The non-spatial, weighted interactions among entities might not serve as a robust inductive bias to effectively reason about the inherent structure of the task."}, {"title": "6 Conclusion and Future Work", "content": "In this work, we presented a relational state abstraction approach for MARL and demonstrated its effectiveness in environments requiring spatial reasoning and coordination among agents. By incorporating spatial inductive biases into our abstraction, we achieved significant improvements in sample efficiency and asymptotic performance compared to SOTA MARL algorithms. Our findings provide strong evidence for the potential of leveraging relational inductive biases to address the challenges of sample efficiency and generalization in MARL.\n\nTo further enhance our method, future research could explore the incorporation of inductive biases beyond spatial reasoning, an even stronger incorporation of structured representations, for example into the policy network as well, and the fine-tuning to more complex, high-dimensional environments. Investigating the interpretability and transparency of the structured representation could also facilitate the deployment into real-world scenarios."}, {"title": "Environments", "content": "To explore the effectiveness of learning a relational state representation in MARL, we have selected a diverse set of environments that offer a suitable testbed for examining the learning of spatial relationships between pairs of entities. The chosen environments all involve multiple agents interacting with each other and their surroundings in ways that require them to reason about the relative positions, distances, and spatial configurations of entities in the environment.\n\nCollaborative Pick and Place The Collaborative Pick and Place (CPP) environment introduces a novel multi-agent challenge that involves heterogeneous agent roles working together to complete a task. In this environment, picker agents and delivery agents must cooperate to pick up boxes and drop them off at designated goal locations. The picker agents are responsible for collecting the boxes, while the delivery agents take the boxes from the picker agents and deliver them to the goal locations. Once a box is placed at a goal, it secures the spot, preventing any other boxes from being placed there. The agents operate within a grid world and, at the beginning of each episode, the environment is initialized with agents, boxes, and goals randomly distributed across the grid. An example of this environment involving two agents and two goals is illustrated in Figure 3.\n\nOur experiments are conducted in a more complex 10\u00d710 grid setup that includes 2 picker agents, 2 delivery agents, and 3 boxes. The agents have a set of six actions at their disposal: move up, down, left, right, pass a box, or wait. Rewards are assigned based on successful interactions between the agents and the environment. Picker agents receive rewards for picking up boxes, while both types of agents earn rewards for successful passes and drop-offs. To encourage cooperation and discourage redundancy, the reward structure is designed as follows: the first pass between a picker agent and a delivery agent grants each agent a reward of 0.5, while repeated passes of the same box result in a penalty of -1. To promote efficient task completion, agents receive a step penalty of -0.1 at each time step, incentivizing them to finish the task quickly. Additionally, if the agents complete"}, {"title": "Level-based Foraging", "content": "The Level-based Foraging (LBF) environment, originally introduced by Christianos, Sch\u00e4fer, and Albrecht (2020), places agents in a grid world where they are tasked with collecting fruits to receive rewards. The environment incorporates a level-based system that determines an agent's ability to collect a fruit. An agent can only collect a fruit if their level is equal to or higher than the fruit's level. This mechanic introduces a collaborative aspect to the task, as fruit levels can be set higher than the level of individual agents, requiring them to work together. This is a sparse environment, as the agents only receive a reward when they successfully collect a fruit, where the reward is proportional to the agent's level.\n\nWe have made a notable alteration to the original LBF environment as seen in Figure 4 by introducing the concept of trees. In our version, fruits are assumed to be on trees that remain on the grid even after the fruit has been collected. These trees have a value of -1 and serve as obstacles that the agents must navigate around. This modification adds a layer of complexity to the task, requiring agents to possess higher relational reasoning capabilities to recognize and avoid the noncollectable tree obstacles while searching for fruits 2.\nTo evaluate the agents' ability to cooperate under challenging conditions, we conduct experiments on a 10 \u00d7 10 grid with 4 agents and 4 fruits, enforcing cooperation (denoted as 10x10-4a-4f-coop). This setup presents a scenario with sparse rewards, demanding effective coordination among the agents to succeed. Furthermore, to assess the"}, {"title": "Wolfpack", "content": "Wolfpack Wolfpack is a MARL environment inspired by the implementation of (Rahman et al. 2023). In this environment, a team of predator agents is tasked with capturing prey within a 2D grid world. The predators must learn to coordinate their actions and form packs to successfully surround and capture the prey. The objective of the predator agents is to capture the prey as efficiently as possible. To capture prey, at least two predator agents must surround it by occupying adjacent grid cells. When a prey is successfully captured, the predator agents involved in the capture are rewarded based on the size of the pack. The captured prey is then removed from the grid and respawned at a random location.\n\nIn our specific implementation, we place 3 predator agents and 2 prey in a 10\u00d710 grid. The predator agents have full observability, meaning they can see the positions of all objects within the grid. However, the prey agents, which are trained using Deep Q-Networks (DQN) (Mnih et al. 2015), operate under partial observability and can only perceive a 3x3 grid centered on their position. The predator agents are allowed to move in any direction (up, down, left, right) or choose to remain stationary at each time step. The prey agents, on the other hand, follow their own learned policy based on DQN.\nIn a departure from the original setup, we have modified the reward structure to introduce sparse rewards. We have removed the additional rewards based on the proximity of predator agents to the prey, which were present in the original implementation. This change significantly weakens the learning signal, making the task more challenging for the predator agents to learn optimal coordination strategies 3.\n\nThe observations entail information about the position and agent type of all entities."}, {"title": "Target", "content": "Target In the target task, agents try to minimize the distance to specific target landmarks while navigating through moving obstacles and other agents. The environment rewards efficient pathfinding and penalizes collisions, forcing agents to balance speed and caution. This setup requires agents to handle various movements and interactions, similar to real-world scenarios. In our experiment, we test this setting with 3 and 7 agents, that need to reach their assigned markets (3 and 7 targets respectively) whilst avoiding collision with 3 obstacles and the other agents. The observations entail information about the position, velocity and entity type of all entities."}, {"title": "Technical Appendix", "content": "Constructing Edges for the Observation Graph\nTo model interactions and proximities in the observation graph, we define relationships between entities based on their spatial arrangements. These relationships are categorized into three distinct groups: Remote Relations, Contiguous Relations, and Local Relations. Each group serves a specific purpose and represents different levels of proximity and interaction potential between entities.\n\nRemote Relations Remote relations identify long-range interactions where entities do not need to be immediately adjacent:\n$left(a, b) \\leftarrow x_a < x_b$,\n$right(a, b) \\leftarrow x_a > X_b$,\n$down(a, b) \\leftarrow y_a < Y_b$,\n$top(a, b) \\leftarrow y_a > Y_b$,\nContiguous Relations Contiguous relations define direct adjacency or alignment, suitable for modeling interactions within immediate reach:\n$aligned(a, b) \\leftarrow (x_a = x_b) \\wedge (y_a = y_b)$,\n$adjacent(a, b) \\leftarrow (|x_a - x_b| \\leq 1) \\vee (|y_a - y_b| \\leq 1)$,"}, {"title": "Local Relations", "content": "Local relations are more granular, detailing the specific neighboring positions around an entity, and are crucial for detailed spatial reasoning:\n$rightAdj(a, b) \\leftarrow (x_a = x_b + 1) \\wedge (y_a = y_b)$,\n$leftAdj(a, b) \\leftarrow (x_a = x_b - 1) \\wedge (y_a = y_b)$,\n$topAdj(a, b) \\leftarrow (x_a = x_b) \\wedge (y_a = y_b + 1)$,\n$bottomAdj(a, b) \\leftarrow (x_a = x_b) \\wedge (y_a = y_b - 1)$,\n$bottomLeftAdj (a, b) \\leftarrow (x_a = x_b - 1) \\wedge (y_a = y_b - 1)$,\n$bottomRightAdj(a, b) \\leftarrow (x_a = x_b + 1) \\wedge (y_a = y_b - 1)$,\n$topLeftAdj(a, b) \\leftarrow (x_a = x_b - 1) \\wedge (y_a = y_b + 1)$,\n$topRightAdj(a, b) \\leftarrow (x_a = x_b + 1) \\wedge (y_a = y_b + 1)$,\nAn illustrative example of a few of these relations can be seen in Figure 9. For our ablation studies, we categorize the relations into specific groups based on their use:\n\nDefault set of relations: These include the most commonly used spatial relationships which cover basic proximity and directional interactions. The set comprises:\n{adjacent, aligned, left, right, top, bottom}\nLocal set of relations: This set includes more detailed and localized spatial relations, providing finer control and specificity for modeling interactions:\n{leftAdj, rightAdj, topAdj, topLeftAdj, topRightAdj, bottomAdj, bottomLeftAdj, bottomRightAdj}\nSet of all relations: Combines both default and local relations for comprehensive coverage:\n{default relations \u222a local relations}\nDefault relations are applied in the discrete task unless specified otherwise, offering a balance between computational efficiency and the resolution of spatial relationships."}, {"title": "Invariances of the State Abstraction", "content": "By using graph neural network computations", "as": "n$r(u(x"}, {"as": "n$r(T(u"}]}