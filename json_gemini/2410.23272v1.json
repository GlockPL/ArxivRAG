{"title": "A Monte Carlo Framework for Calibrated Uncertainty Estimation in Sequence Prediction", "authors": ["Qidong Yang", "Weicheng Zhu", "Joseph Keslin", "Laure Zanna", "Tim G. J. Rudner", "Carlos Fernandez-Granda"], "abstract": "Probabilistic prediction of sequences from images and other high-dimensional data remains a key challenge, particularly in safety-critical domains. In these settings, it is often desirable to quantify the uncertainty associated with the prediction (instead of just determining the most likely sequence, as in language modeling). In this paper, we propose a Monte Carlo framework to estimate probabilities and confidence intervals associated with sequences. Our framework uses a Monte Carlo simulator, implemented as an autoregressively trained neural network, to sample sequences conditioned on an image input. We then use these samples to estimate probabilities and confidence intervals. Experiments on synthetic and real data show that the framework produces accurate discriminative predictions, but can suffer from miscalibration. To address this shortcoming, we propose a time-dependent regularization method, which produces calibrated predictions.", "sections": [{"title": "1 INTRODUCTION", "content": "We consider the problem of predicting a sequence of multi-class labels from high-dimensional input data, such as images. Potential applications include patient prognostics from medical imaging data, weather forecasting, and modeling of human behavior. Our focus is on probabilistic prediction, which requires uncertainty quantification in the form of probabilities or confidence intervals."}, {"title": "2 RELATED WORK", "content": "Sequence generation. Discrete sequence generation plays a fundamental role in many natural language processing applications, such as language modeling, image captioning, language translation, and text summarization. In these examples, sequence generation is typically performed in an autoregressive manner, where each token is generated based on the tokens previously generated. Alternatively, sequences can also be generated non-autoregressively, where tokens are produced simultaneously or with fewer dependencies on earlier tokens. In both paradigms, the primary objective is to generate the most likely sequence. However, these approaches typically do not focus on quantifying the uncertainty associated with generated sequences, which is the goal of this paper.\nImitation and reinforcement learning. Our focus is on predicting sequences from high-dimensional data, which is fundamentally different from imitation learning, which seeks to replicate human behavior, and reinforcement learning, which seeks to determine an optimal policy by allowing the agent to interact with the environment, guided by a reward function.\nCalibration. Miscalibration is a well-known challenge in classification models based on deep learning, particularly when the goal is to provide accurate uncertainty quantification. While significant progress has been made in calibrating unitask classification models, calibration in sequence prediction tasks remains underexplored and even lacks a clear definition. While Marx et al. explores calibration in sequences, their focus is on step-wise calibration, where each sequence step has an input-output pair, which makes the problem step-wise calibration within sequences, whereas our work focuses on calibration for the entire sequence. Kuleshov and Liang offers a framework for measuring calibration in structured high-dimensional random vectors via event pooling, which inspires our approach to uncertainty estimation in sequence prediction.\nVarious methods have been developed to enhance calibration in classification, including post-processing the logits, ensembling methods, soft labeling and training with regularization. In this work, we propose a time-dependent regularization method to improve calibration specifically in sequence prediction tasks."}, {"title": "3 A MONTE CARLO FRAMEWORK", "content": "We consider the problem of predicting a sequence of multi-class labels from high-dimensional data. More formally, our goal is to estimate the conditional distribution of a sequence Y consisting of $l$ discrete random variables each with $c$ possible states given an observed input $x$, interpreted as a sample of a random vector X. In our health-monitoring example, the $c:= 3$ states are healthy, ill and dead, and X represents an input image.\nEven for short sequence lengths, directly estimating the joint conditional probability mass function of Y given X = x is intractable due to the combinatorial explosion of possible sequences (e.g., for $c := 3$ and $l := 100$ there are $3^{100} > 10^{47}$ possible sequences!), which is an instance of the notorious curse of dimensionality. Instead, we propose to estimate the following probabilities and confidence intervals characterizing the sequence, which are illustrated in Figure 1:\n1.  The marginal probability $P(Y_i = a | X = x)$ that the $i$th entry $Y$ of the sequence is equal to $a \\in \\{1, ..., c\\}$. We refer to this as a marginal probability, but-strictly speaking it is a conditional probability, as it is conditioned on $X = x$. In health monitoring, this is the probability that a subject is healthy, ill, or dead at time $i$.\n2.  The conditional probability $P(Y_i = a | Y_j = b, X = x)$ that the $i$th entry $Y$ of the sequence is equal to $a \\in \\{1, ..., c\\}$ given that the $j$th entry is equal to $b\\in \\{1, ..., c\\}$. In health monitoring, this could be the conditional probability that a subject is dead at time $i$ given that they are ill at time $j$.\n3.  The $a$ confidence interval $I_a$ for the time $T$ until a certain event associated with the sequence occurs (e.g. $T := \\min\\{i : Y_i = a\\}$ for some $a \\in \\{1, ..., c\\}$), which should satisfy $P(T \\in I_a) = a$, where $a$ is typically set to 0.9 or 0.95. For example, in health monitoring, $T$ can represent the time of death or recovery."}, {"title": "3.1 Monte Carlo Estimation", "content": "As noted in the previous section, a key challenge in estimating arbitrary probabilities and confidence intervals associated with a sequence of random variables Y is that it is intractable to explicitly estimate the sequence joint distribution given the input X (unless we make highly simplifying modeling assumptions, such that the sequence forms a Markov chain). Our proposed framework for Monte Carlo uncertainty quantification of sequences (foCus) addresses this challenge by instead implicitly sampling from the conditional distribution using a neural network simulator, described in Section 3.2.\nGiven an input x, we apply the simulator to generate M sequences $\\{(\\hat{y}_1^{(m)}, ..., \\hat{y}_\\ell^{(m)})\\}_{m=1}^M$. As shown in panel (c) of Figure 2, the sequences are then used to estimate any desired probability or confidence interval. The marginal probability of state a at time i is estimated by the fraction of sequences in state a at time i:\n$P(Y_i = a | X = x) = \\frac{1}{M} \\sum_{m=1}^M 1\\{\\hat{y}_i^{(m)} = a\\}$ (1)\nThe conditional probability of state a at time i given state b at time j is estimated by the fraction of sequences in state a at time i out of sequences in state b at time j:\n$P(Y_i = a | Y_j = b, X = x) = \\frac{\\sum_{m=1}^M 1\\{\\hat{y}_i^{(m)} = a, \\hat{y}_j^{(m)} = b\\}}{\\sum_{m=1}^M 1\\{\\hat{y}_j^{(m)} = b\\}}$ (2)\nTo estimate the $a$ confidence interval $I_a$ of the time-to-event T, we first compute the value of T associated with each simulated sequence. These yields M times $\\{T^{(1)}, ..., T^{(M)}\\}$, which can be sorted to calculate the $(1-a)/2$ and $(1+a)/2$ percentiles $q_{(1-a)/2}$ and $q_{(1+a)/2}$. The confidence interval $I_a$ is set to $[q_{(1-a)/2}, q_{(1+a)/2}]$."}, {"title": "3.2 Autoregressive Simulation", "content": "In order to produce the simulated sequences required by our Monte Carlo framework, we employ a neural network simulator. The simulator consists of a convolutional neural network (CNN) that encodes the input image $x$, and a recurrent neural network (RNN) that iteratively estimates the conditional distribution of the $i$th entry $Y_i$ of the sequence given $X = x$ and the values of all previous entries, i.e. $P (Y_i = y_i | X = x, Y_1 = y_1, ..., Y_{i-1} = y_{i-1})$.\nFigure 2(b) illustrates how the simulator is used to obtain a sample sequence $(\\hat{y}_1,..., \\hat{y}_\\ell)$. The input image $x$ is fed into the CNN, producing a hidden vector $h_0$ that is fed into the RNN to then generate the simulated sequence iteratively. At each iteration $i \\in \\{1, ..., \\ell\\}$, the input of the RNN is the value $\\hat{y}_{i-1}$ of the previous entry (except for $i = 1$) and the hidden vector $h_{i-1}$. The outputs are an estimate of the conditional distribution of $Y_i$ given the previous entries and $X = x$, and an updated hidden vector $h_i$. The $i$th entry $\\hat{y}_i$ of the sample sequence is sampled from this conditional distribution.\nThe simulator is trained using a dataset of image-sequence pairs, as illustrated in Figure 2(a). During training, the model uses the ground truth value $Y_{i-1}$ of the previous entry along with the hidden vector $h_{i-1}$ to predict the subsequent $i$th entry $Y_i$. As explained in Sections 5 and 6 the design of the training loss plays a crucial role in avoiding miscalibration in the downstream probabilities and confidence intervals computed using the simulator."}, {"title": "4 EVALUATING UNCERTAINTY", "content": ""}, {"title": "4.1 Evaluation metrics", "content": "We assess marginal and conditional probability estimates with a set of complementary metrics. Macro Area Under the ROC Curve (AUC) quantifies discriminative ability. Expected Calibration Error (ECE) quantifies calibration. Brier Score (BS) and Cross Entropy (CE) provide a more holistic evaluation. These metrics are computed for each entry and then aggregated via averaging to obtain sequence-level metrics.\nTo assess confidence intervals, we evaluate discriminability via the Mean Absolute Error (MAE) (normalized by the average sequence length). Calibration is evaluated by computing the coverage probability of the intervals,"}, {"title": "5 MISCALIBRATION", "content": "In this section, we report the results of applying foCus when we train the simulator described in Section 3.2 using a standard unregularized cross-entropy loss\n$CE(\\theta) = \\mathbb{E}_{ (x, y) \\sim \\mathcal{D} } [ - \\sum_{i=1}^\\ell \\log p_\\theta(Y_i | X, Y_1, ..., Y_{i-1}) ],$ (3)"}, {"title": "6 TIME-DEPENDENT LOGIT REGULARIZATION", "content": "To address simulator-induced miscalibration in foCus, we incorporate a regularization term that penalizes the $l_2$-norm of the logits within the simulator. This regularizer is motivated by recent work on function-space regularization, which interprets regularized objectives as performing maximum a posteriori (MAP) estimation under data-driven-priors over neural network parameters. Intuitively, the $l_2$-norm penalty prevents the logits from becoming very large during training. This mitigates overconfidence, since large logits result in more extreme probability estimates. We can think of the $l_2$-norm penalty as promoting neural network parameters that induce predictive functions with higher predictive entropy. As in Section 5, let $p_\\theta (Y_i | X, Y_1, ..., Y_{i-1})$ denote the estimate of the conditional probability $P (Y_i = Y_i | X = x, Y_1 = y_1, ..., Y_{i-1} = y_{i-1})$ produced by the simulator for $i \\in \\{1, ..., \\ell\\}$, which is obtained by feeding a logit vector $z(x, y_1, ..., y_{i-1}) \\in \\mathbb{R}^c$ into a softmax function. The training loss is\n$\\mathcal{L}(\\theta) = \\mathbb{E}_{ (x, y) \\sim \\mathcal{D} } [ \\sum_{i=1}^\\ell - \\log p_\\theta (Y_i|x, Y_1, ..., Y_{i-1}) ] + \\lambda_i ||z(x, Y_1, ..., Y_{i-1}) ||_2$ (4)\nwhere $\\lambda_i$ is a regularization coefficient that governs the regularization strength when predicting the $i$th entry of the sequence. The regularization $\\lambda_i$ is designed to be time dependent, motivated by our observation that the baseline version of foCus suffers from different degrees of miscalibration at different entries.\nA crucial challenge is how to select the value of this hyperparameter, given the large dimensionality of the hyperparameter space. We propose a selection procedure, based on the observation that miscalibration in the initial entries is propagated by the autoregressive structure of the simulator (see Section 3.2). Consequently, optimizing the regularization parameter at the beginning of the sequence has more impact on the overall calibration performance of foCus (see Appendix E for additional analysis). The procedure is as follows:"}, {"title": "6.1 Results", "content": "Marginal and conditional probabilities. Table 1 compares the sequence-level evaluation metrics for marginal probability estimation of foCus with (1) no regularization as described in Section 5, (2) our proposed time-dependent regularization described in Section 6, and (3) constant regularization where the regularization parameter in Equation (4) is set to a single constant $ \\lambda_i = \\lambda_{\\text{const}}$ (determined based on validation ECE). All methods achieve similar AUCs in each dataset, indicating a similar discriminative ability. In contrast, the ECE is significantly lower for time-dependent regularization for all datasets, indicating better calibration performance. This results in better probability estimates, as evinced by the lower cross entropy (CE) and Brier scores (BS). For FaceMed this is confirmed by comparing the estimated probabilities to the ground-truth marginal probabilities. The unregularized baseline method and constant regularization yield RMSEs of 0.1847\u00b10.0014 and 0.1754\u00b10.0028, respectively, while time-dependent regularization reduces the RMSE to 0.1720\u00b10.0009. This result further demonstrates the improved performance achieved by time-dependent regularization.\nThe same holds for conditional probability predictions, as reported in Table 4: Time-dependent regularization again significantly improves calibration, and as a result the overall probability estimates. Appendix F provides a detailed description of the conditional probability estimates for one of the video games.\nFigure 3 further demonstrates the improvement in calibration provided by time-dependent regularization. Interestingly, we observe a calibration propagation phenomenon, where regularizing a small number of early entries produces improved calibration across the whole sequence. For example, for H.E.R.O. regularization is applied to the first 6 entries (0.2 seconds), yet the ECE improvement is evident up until entry 150 (5 seconds).\nTime-to-event confidence intervals. Table 5 compares the evaluation metrics for the confidence intervals"}, {"title": "7 DISCUSSION AND LIMITATIONS", "content": "In this paper, we studied an important, yet under-explored topic: how to achieve reliable uncertainty quantification when predicting sequences from high-dimensional data. We proposed a Monte Carlo framework based on learned autoregressive simulators that enables flexible estimation of probabilities and confidence intervals. Our experiments on sequential decision-making tasks revealed that simulator models learned via maximum likelihood estimation can lead to severely miscalibrated uncertainty estimates. We showed that this shortcoming can be addressed by training the autoregressive simulator model using a time-dependent regularizer, which we find consistently leads to well-calibrated uncertainty estimates.\nOur proposed regularization is conceptually and mathematically simple but requires choosing a set of regularization coefficients $ \\{\\lambda_i\\}_{i=1}^\\ell$ from a combinatorially large space, making an exhaustive search infeasible in practice. This is not unique to our approach: Real-world sequences often display non-stationary statistical properties that are difficult to model in a data-driven fashion. Nevertheless, we find that our simple coefficient selection protocol leads to significant improvement in calibration, although more sophisticated strategies could well result in further gains. Other potentially fruitful directions for future research are to perform uncertainty quantification of continuous-valued and spatiotemporal sequences in weather and climate applications areas in which neural-network simulators are rapidly gaining popularity."}, {"title": "A Evaluation metrics", "content": ""}, {"title": "A.1 Marginal and conditional probability", "content": "For each class a \u2208 {1,\u2026\u2026, c} and each entry i of the sequence, we evaluate the estimated probabilities P(Yi = a | X = x) or P(Yi = a | Yj = b, X = x) using the relevant data (for marginal probabilities, these are all sequences; for conditional probabilities, all sequences such that the jth entry equals b). The entry-level metrics we propose are defined below. The sequence-level metrics are computed by averaging the entry-level metrics across all entries.\nMacro AUC. The Area Under the Curve (AUC) per class is computed separately for each class a at each entry i using a one-vs-all approach. We aggregate all the class AUCs via averaging to obtain the overall macro AUC.\nBrier Score The Brier Score (BS) evaluates both calibration and discriminative ability. The Brier Score per class is the mean-squared error between the predicted probabilities and binarized label per class. The entry-level BS is the mean of BS per class, averaged over all the classes.\nCross Entropy The cross-entropy (CE) loss is computed following (3).\nExpected Calibration Error. We use confidence expected calibration error (ECE) to assess calibration. The confidence is defined as the predicted probability of the class a with the highest estimated probability. These confidences are grouped into B bins, based on B-quantiles. ECE is the mean absolute difference between the accuracy (empirical probability of correct predictions) and the average confidence within each bin. A lower ECE indicates better calibration.\nTo provide further insight, we also plot reliability diagrams. These diagrams compare the empirical probability (accuracy) with the estimated probability (confidence) in each bin. A well-calibrated model will produce a reliability diagram that is close to the diagonal."}, {"title": "A.2 Confidence intervals", "content": "Let the ground truth time-to-event for the kth data point be denoted as T[k], and the estimated confidence interval as $I_a[k] = [q_{(1-a)/2}[k], q_{(1+a)/2}[k]]$. Coverage Probability The coverage probability measures the proportion of samples where the true time-to-event T[k] lies within the estimated confidence interval Ia[k]. This metric reflects how well calibrated the estimated confidence intervals are. Ideally, for a confidence level a, the coverage probability should equal a.\nRelative Confidence Interval Width The width of the confidence interval quantifies the uncertainty in the model estimates. A wider confidence interval indicates higher uncertainty. To account for different sequence lengths across datasets, we normalize the confidence interval width by the mean of the true time-to-event averaged over each dataset. The relative confidence interval width is defined as:\n$\\frac{\\frac{1}{N} \\sum_{k=1}^N (q_{(1+\\alpha)/2}[k] - q_{(1-a)/2}[k])}{\\frac{1}{N} \\sum T[k]}$, (5)\nwhere N is the number of data.\nRelative Mean Absolute Error (MAE) The relative MAE is the mean of the absolute difference between the estimated and ground truth time-to-event, normalized by the mean of the ground truth times-to-event. We estimate the time-to-event by averaging over the time-to-event values $\\widehat{T}^{(m)}, ..., \\widehat{T}^{(m)}$ corresponding to the $m$ Monte Carlo simulations:\n$\\text{Relative MAE} = \\frac{\\sum_{k=1}^N |T[k] - \\frac{1}{M} \\sum_{m=1}^M \\widehat{T}^{(m)}[k]|}{\\sum T[k]}$ (6)"}, {"title": "B Datasets", "content": "FaceMed FaceMed is a synthetic dataset based on the UTKFace dataset, which contains face images along with corresponding ages. We simulate health status transitions between three distinct states: 1 for healthy, 2 for ill, and 3 for dead. The simulated health states per each year form a sequence for each patient. The underlying transition probabilities among these health states are determined by the individual's age. The goal of the sequence prediction task is to forecast a patient's health status trajectory using their facial images.\nTo simulate the dynamics of health status, we use an age-dependent Markov process, where the health status at the $i$th entry, $Y_i$, only depends on the previous state $Y_{i-1}$, for any $i > 1$. The conditional probability between states is given by:\n$P(Y_i = a | Y_{i-1} = b) = P_{b,a}$. (7)\nThe transitions among health statuses are illustrated in Figure 5. Every individual is healthy as the initial state. The transition probabilities for the simulation are defined as follows: For individuals younger than 40, the health status never changes (Figure 5 (a)). For those aged 40 to 80, the health status can transition between healthy and ill, with transition probabilities $p_{0,*} = (0.6, 0.4, 0)$. For those aged 40 to 80, the health status can transition between healthy and ill, with transition probabilities $P_{1,1} = P_{2,2} = 0.9, P_{1,2} = P_{2,1} = 0.1$ (Figure 5 (b)). For individuals older than 80, the likelihood of becoming ill increases, with transition probabilities $p_{0,*} = (0.6, 0.4, 0)$; for ill individuals in this age group, there is a chance of death, reflected in the transition probabilities $p_{1,*} = (0.1, 0.7, 0.2)$. For those aged 40 to 80, the health status can transition between healthy and ill, with transition probabilities p\u2080,\u2217 = (0.6, 0.4, 0); for ill individuals in this age group, there is a chance of death, reflected in the transition probabilities p\u2081,\u2217 = (0.1, 0.7, 0.2) (Figure 5 (c)). These probabilities vary as the individual becomes \"older\" in the simulation, reflecting the increasing risks associated with aging. The average survival time of the simulated sequences is 45.45 years. In the experiments, For individuals who die before 100 years from the beginning, their sequences are padded to cover 100 years. The dataset is split into training, validation, and test sets with 16641, 4738, and 2329 samples, respectively.\nAtari games Our real-data experiments are based on the Atari-HEAD dataset, a large-scale, high-quality imitation learning dataset that captures human actions alongside eye movements and game frames while playing Atari video games. The dataset employs a unique semi-frame-by-frame gameplay format, where the game pauses at each frame until the player performs a keyboard action. This ensures that each frame in the video of game and the corresponding human action are aligned.\nIn this work, the sequence prediction task aims to predict a player's action trajectory based on a given game frame. Each frame serves as a high-dimensional input X. The subsequent actions until the next scoring event are treated as a sequence Y. Since actions are recorded at a high frame-by-frame frequency, they often repeat several times before transitioning to a new action, yielding sequences with redundant information. To reduce this redundancy, we sample actions at a constant frequency determined by the number of frames per sequence entry"}, {"title": "C Technical Details", "content": ""}, {"title": "C.1 Model Architecture", "content": "As illustrated in Figure 2 (a), we employ a neural-network simulator. The simulator consists of a convolutional neural network (CNN) that encodes the input image $x$, and a recurrent neural network (RNN). The CNN encoder is a Resnet-18, which produces an image embedding of dimension 256. The RNN decoder is implemented as a single-layer LSTM with a hidden vector size 256. The image embedding from the CNN encoder is fed to the RNN decoder as the initial hidden vector $h_0$. The RNN iteratively updates its hidden state $h_i$ based on the previous hidden state $h_{i-1}$ and the preceding input $y_{i-1}$ from the $(i - 1)$th entry of the sequence. For each step $i > 1$, the RNN outputs a logit vector $z(x, Y_1,..., Y_{i-1}) \\in \\mathbb{R}^c$ through a linear layer with input $h_i$. The logit vector is normalized with a softmax function and used to estimate the class probabilities of a multinoulli distribution:\n$\\text{P}\\_\\theta (a | x, Y\\_1, ..., Y\\_{i-1}) = \\frac{\\exp (z(x, Y\\_1, ..., Y\\_{i-1})[a])}{\\sum\\_{k=1}^c \\exp(z (x, y\\_1, ..., Y\\_{i-1})[k])}$, (8)\nwhere $a \\in \\{1,...,c\\}$ and $z(x, Y_1, ..., Y_{i-1})[k]$ is the $k$th entry of the logit vector.\nFor the health-status prediction task, the RNN decoder outputs a 3-dimensional logit vector corresponding to the three possible health states (c = 3). In the case of Atari games, where each action can belong to one of 19 possible classes, the RNN generates a 19-dimensional logit vector (c = 19)."}, {"title": "C.2 Training", "content": "As explained in Section 6, the neural network simulator is trained by minimizing the cross-entropy loss between the predicted distribution and the one-hot encoded ground truth for each variable, with an additional regularization term that penalizes the $l_2$-norm of each entry in the logit vector.\nWe train each model for 200 epochs for each scenario, with a batch size of 256, using the Adam optimizer without weight decay. The learning rates are kept constant for each scenario: 1 \u00d7 10\u22125 for Seaquest, River Raid, Bank Heist, and Road Runner, and 5 \u00d7 10\u22125 for H.E.R.O.\nModel selection during training is challenging due to the numerous metrics involved in probability estimation tasks. Figures 11, 12, and 13 show the evolution of different metrics during training. We observe that the models are most discriminative (lower relative MAE and higher AUC) toward the end of training in most scenarios."}, {"title": "C.3 Inference", "content": "Figure 2(b) illustrates how the neural-network simulator is used to obtain a sample sequence (\u01771,..., \u0177\u2113). The input image x is fed into the CNN encoder, producing a hidden vector h0 that is fed into the RNN decoder to then generate the simulated sequence iteratively. At each iteration i \u2208 {1, ..., l}, the input of the RNN decoder is the value \u0177i-1 of the previous entry (except for i = 1) and the hidden vector hi-1. The outputs are an estimate of the conditional distribution of Y given the previous entries and X = x, and an updated hidden vector hi. The ith entry \u0177 of the sample sequence is sampled from this conditional distribution. Since each entry is drawn randomly from the predicted distribution, the simulator is capable of generating multiple different sequences from the same input image, acting as a simulator. When performing Monte Carlo estimation, we generate m = 100 sampled sequences for each input image."}, {"title": "C.4 Hyperparameter search", "content": "The hyperparameters for time-dependent regularization were determined via the following procedure:\n1.  For 1 < i < k1 (where k1 is a hyperparameter) we use the sequence-level ECE of marginal probabilities (see Section 4.1) computed over validation set to iteratively select \u03bbi, setting \u03bbj = 0 for all j > i.\n2.  For k1 < i < k2 (where k2 is a hyperparameter) we constrain all the parameters to equal the same constant, \u03bbi = \u03bball, selected also based on the validation ECE.\n3.  For i > k2 we set \u03bbi = 0.\nWe set k1 = 3. To determine each \u03bbi and \u03bball we performed a search on the fixed grid {0.001, 0.005, 0.01, 0.05} based on the validation ECE for the marginal probability estimation task. For k2 we used the grid {1, 11, 21, 51, 101}.\nFor constant regularization, we constrain all the parameters to be the same, \u03bbi = \u03bbconst. Then \u03bbconst we performed a search on the grid {0.001,0.005, 0.01, 0.05}, also based on validation ECE."}, {"title": "D Supplementary Experimental Results", "content": ""}, {"title": "D.1 Marginal Probability", "content": "Figure 6 shows plots of the entry-level metrics (ECE, AUC, BS, and CE) for marginal probability estimation, complementing Figure 3. Time-dependent regularization leads to a substantial improvement in calibration, as demonstrated by the significantly lower ECE of the time-regularized model, which has a comparable AUC to the model without regularization. As a result, the cross-entropy and Brier Score metrics are also improved. Constant regularization also improves the probability estimates, but not as much as time-dependent regularization. Figure 7 shows additional reliability diagrams like the ones in Figure 3 and includes a comparison with constant regularization, confirming that time-dependent regularization consistently improves calibration for individual entries.\nIn Section 6.1, we also compare the estimated marginal probability with underlying data generating distribution on the synthetic FaceMed data. Since the ground truth sequences in FaceMed are generated using an assumed transitional probability model, as detailed in Appendix B, we can analytically compute the ground truth marginal probability of Yi based on the transitional probabilities and the marginal probability of the previous entry, Yi-1:\n$P(Y_i = a | X = x) = \\sum_{b=1}^C P(Y_{i-1} = b | X = x) P(Y_i = a | Y_{i-1} = b)$ (9)\nStarting from an initial state of \"healthy,\" we compute the ground truth marginal probabilities for each entry in the sequence and compare them against the estimates obtained via foCus. We calculate the root mean square error (RMSE) between the ground truth and the estimated marginal probabilities for each entry, then aggregate these values to derive a sequence-level RMSE by averaging. The unregularized baseline method and constant regularization yield RMSE of 0.1847 \u00b1 0.0014 and 0.1754 \u00b1 0.0028, respectively, while time-dependent regularization reduces the RMSE to 0.1720 \u00b1 0.0009. This result further demonstrates the improved performance achieved by time-dependent regularization."}, {"title": "D.2 Conditional Proabability", "content": "We evaluate the conditional probability estimation, given a fixed event in each scenario. For conditional probability estimation, FaceMed is conditioned on the status of the first year being Healthy. Video games are conditioned on the first entry being equal to the most frequent first action in the training set. Specifically, Seaquest is conditioned on first action as NOOP (no operation), River Raid as NOOP, Bank Heist as Right, H.E.R.O. as NOOP, and Road Runner as Left. Table 4 reports the sequence-level metrics for conditional probability estimation. Figure 8 shows entry-level metrics (ECE, AUC, BS, and CE) for conditional probability estimation. Similar to the case of marginal probability estimation Appendix D.1, time-dependent regularization improves calibration and the overall quality of probability estimation."}, {"title": "D.3 Confidence Interval for Time-to-event", "content": "Table 5 reports the results for time-to-event confident interval estimation. In this case, we observe a certain trade-off between discriminative performance, quantified by the relative MAE, and calibration, quantified by coverage probabilities. The MAE of no regularization and constant regularization are consistently lower than those of time-dependent regularization, but the coverage probabilities of time-dependent regularization are a lot closer to 90% (between 69% and 92%, compared to at most 70% for the other two methods).\nFigure 9, complementing Figure 4, presents heatmaps of the confidence interval widths for different ground-truth time-to-event (upper panel of each subplot) for River Raid, H.E.R.O., Road Runner, and Bank Heist, as well as a histogram showing the fraction of intervals containing the ground-truth (lower panel of each subplot). Unregularized foCus produces very narrow confidence intervals with very poor coverage, whereas time-dependent regularization yields intervals that tend to be larger when the true time-to-event becomes greater (and hence generally more uncertain), achieving much better coverage. These finding further supports that time-dependent regularization performs better in confidence interval estimation."}, {"title": "E Regularization Sensitivity Analysis", "content": "In this section, we explore how regularization at a single entry affects model performance. Using the Seaquest game, we train four versions of foCus, each applying regularization to a different entry in the sequence, corresponding to 0, 3, 6, and 9 seconds. As shown in Tables 6 and 7, the model with regularization applied at the beginning (0 seconds) exhibits the most significant improvement in calibration, both for marginal probability estimation and time-to-event prediction confidence intervals. This suggests that applying regularization early in the sequence is crucial for maintaining proper calibration throughout the whole sequence. These results also validate our heuristic procedure to implement time-dependent regularization."}, {"title": "F Conditional Probability Estimation", "content": "In this section, we illustrate the conditional probability estimation task by showcasing how early actions substantially influence probability predictions at later stages, as shown in Figure 10.\nWe analyze the probabilities estimated from a frame of the game Seaquest (right panel of Figure 10) by foCus using time-dependent regularization. We estimate the conditional probabilities given two different first-step actions: Up Fire and Up Left Fire. The left panel of Figure 10 shows the resulting conditional probability estimation. When the first action is Up Left Fire instead of Up Fire, the frequency of NOOP (no operation) decreases significantly, accelerating the player's progress toward scoring. This makes intuitive sense, as Up Left Fire moves the green submarine closer to the enemy blue submarine, allowing the torpedo to reach its target more quickly. This analysis illustrates that the model effectively learns meaningful conditional probability estimates."}, {"title": "G Supplementary Figures", "content": "This section displays the learning curves for various sequence-level metrics on the test set at different epochs during training. All models were trained for 200 epochs, and the metrics were calculated separately for time-to-event"}]}