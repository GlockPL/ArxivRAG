{"title": "Provable Benefit of Cutout and CutMix for Feature Learning", "authors": ["Junsoo Oh", "Chulhee Yun"], "abstract": "Patch-level data augmentation techniques such as Cutout and CutMix have demonstrated significant efficacy in enhancing the performance of vision tasks. However, a comprehensive theoretical understanding of these methods remains elusive. In this paper, we study two-layer neural networks trained using three distinct methods: vanilla training without augmentation, Cutout training, and CutMix training. Our analysis focuses on a feature-noise data model, which consists of several label-dependent features of varying rarity and label-independent noises of differing strengths. Our theorems demonstrate that Cutout training can learn low-frequency features that vanilla training cannot, while CutMix training can learn even rarer features that Cutout cannot capture. From this, we establish that CutMix yields the highest test accuracy among the three. Our novel analysis reveals that CutMix training makes the network learn all features and noise vectors \u201cevenly\u201d regardless of the rarity and strength, which provides an interesting insight into understanding patch-level augmentation.", "sections": [{"title": "Introduction", "content": "Data augmentation is a crucial technique in deep learning, particularly in the image domain. It involves creating additional training examples by applying various transformations to the original data, thereby enhancing the generalization performance and robustness of deep learning models. Traditional data augmentation techniques typically focus on geometric transformations such as random rotations, horizontal and vertical flips, and cropping (Krizhevsky et al., 2012), or color-based adjustments such as color jittering (Simonyan and Zisserman, 2014).\nIn recent years, several new data augmentation techniques have appeared. Among them, patch-level data augmentation techniques like Cutout (DeVries and Taylor, 2017) and CutMix (Yun et al., 2019) have received considerable attention for their effectiveness in improving generalization. Cutout is a straightforward method where random rectangular regions of an image are removed during training. In comparison, CutMix adopts a more complex strategy by cutting and pasting sections from different images and using mixed labels, encouraging the model to learn from blended contexts. The success of Cutout and CutMix has triggered the development of numerous variants including Random Erasing (Zhong et al., 2020), GridMask (Chen et al., 2020a), CutBlur (Yoo et al., 2020), Puzzle Mix (Kim et al., 2020), and Co-Mixup (Kim et al., 2021). However, despite the empirical success of these patch-level data augmentation techniques in various image-related tasks, a lack of comprehensive theoretical understanding persists: why and how do they work?\nIn this paper, we aim to address this gap by offering a theoretical analysis of two important patch-level data augmentation techniques: Cutout and CutMix. Our theoretical framework draws inspiration from a study by Shen et al. (2022), which explores a data model comprising multiple label-dependent feature vectors and label-independent noises of varying frequencies and intensities. The key idea of this work is that learning features with low frequency can be challenging due to strong noises (i.e., low signal-to-noise ratio). We focus on how Cutout and CutMix can aid in learning such rare features."}, {"title": "1.1 Our Contributions", "content": "In this paper, we consider a patch-wise data model consisting of features and noises, and use two-layer convolutional neural networks as learner networks. We focus on three different training methods: vanilla training without any augmentation, Cutout training, and CutMix training. We refer to these training methods in our problem setting as ERM, Cutout, and CutMix. We investigate how these methods affect the network's ability to learn features. We summarize our contributions below:\n\u2022 We analyze ERM, Cutout, and CutMix, revealing that Cutout outperforms ERM since it enables the learning of rarer features compared to ERM (Theorem 3.1 and Theorem 3.2). Furthermore, CutMix demonstrates almost perfect performance (Theorem 3.3) by learning all features.\n\u2022 Our main intuition behind the negative result for ERM is that ERM learns to classify training samples by memorizing noise vectors instead of learning meaningful features if the features do not appear frequently enough. Hence, ERM suffers low test accuracy because it cannot learn rare features. However, Cutout alleviates this challenge by removing some of the strong noise patches, allowing it to learn rare features to some extent.\n\u2022 We prove the near-perfect performance of CutMix based on a novel technique that views the non-convex loss as a composition of a convex function and reparameterization. This enables us to characterize the global minimum of the loss and show that CutMix forces the model to activate almost uniformly across every patch of inputs, allowing it to learn all features."}, {"title": "1.2 Related Works", "content": "Feature Learning Theory. Our work aligns with a recent line of studies investigating how training methods and neural network architectures influence feature learning. These studies focus on a specific data distribution composed of two components: label-dependent features and label-independent noise. The key contribution of this body of work is the exploration of which training methods or neural networks are most effective at learning meaningful features and achieving good generalization performance. Allen-Zhu and Li (2020) demonstrate that an ensemble model can achieve near-perfect performance by learning diverse features, while a single model tends to learn only certain parts of the feature space, leading to lower test accuracy. In other works, Cao et al. (2022); Kou et al. (2023a) explore the phenomenon of benign overfitting when training a two-layer convolutional neural network. The authors identify the specific conditions under which benign overfitting occurs, providing valuable insights into how these networks behave during training. Several other studies seek to understand various aspects of deep learning through the lens of feature learning (Zou et al., 2021; Jelassi and Li, 2022; Chen et al., 2022, 2023; Li and Li, 2023; Huang et al., 2023a,b).\nTheoretical Analysis of Data Augmentation. Several works aim to analyze traditional data augmentation from different perspectives, including kernel theory (Dao et al., 2019), margin-based approach (Rajput et al., 2019), regularization effects (Wu et al., 2020), group invariance (Chen et al., 2020b), and impact on optimization (Hanin and Sun, 2021). Moreover, many papers have explored various aspects of a recent technique called Mixup (Zhang et al., 2017). For example, studies have explored its regularization effects (Carratino et al., 2020; Zhang et al., 2020), its role in improving calibration (Zhang et al., 2022), its ability to find optimal decision boundaries (Oh and Yun, 2023) and its potential negative effects (Chidambaram et al., 2021; Chidambaram and Ge, 2024). Some works investigate the broader framework of Mixup, including CutMix, which aligns with the scope of our work. Park et al. (2022) study the regularization effect of mixed-sample data augmentation within a unified framework that contains both Mixup and CutMix. In Oh and Yun (2023), the authors analyze masking-based Mixup, which is a class of Mixup variants that also includes CutMix. In their context, they show that masking-based Mixup can deviate from the Bayes optimal classifier but require less training sample complexity. However, neither work provides a rigorous explanation for why CutMix has been successful. The studies most closely related to our work include Shen et al. (2022); Chidambaram et al. (2023); Zou et al. (2023). Shen et al. (2022) regard traditional data augmentation as a form of feature manipulation and investigate its advantages from a feature learning perspective. Both Chidambaram et al. (2023) and Zou et al. (2023) analyze Mixup within a feature learning framework. However, patch-level data augmentation such as Cutout and CutMix, which are the focus of our work, have not yet been explored within this context."}, {"title": "2 Problem Setting", "content": "In this section, we introduce the data distribution and neural network architecture, and formally describe the three training methods considered in this paper."}, {"title": "2.1 Data Distribution", "content": "We consider a binary classification problem on structured data, consisting of patches of label- dependent vectors (referred to as features) and label-independent vectors (referred to as noise).\nDefinition 2.1 (Feature Noise Patch Data). We define a data distribution \\( \\mathcal{D} \\) on \\( \\mathbb{R}^{d \\times P} \\times \\{-1,1\\} \\) such that \\( (X, y) \\sim \\mathcal{D} \\) where \\( X = (x^{(1)},...,x^{(P)}) \\in \\mathbb{R}^{d\\times P} \\) and \\( y \\in \\{ \\pm 1\\} \\) is constructed as follows.\n1. Choose the label \\( y \\in \\{\\pm 1\\} \\) uniformly at random.\n2. Let \\( \\{v_{s,k}\\}_{s\\in\\{\\pm 1\\},k\\in[K]} \\subset \\mathbb{R}^d \\) be a set of orthonormal feature vectors. Choose the feature vector \\( v \\in \\mathbb{R}^d \\) for data point \\( X \\) as \\( v = v_{y,k} \\) with probability \\( p_k \\) from \\( \\{v_{y,k}\\}_{k\\in[K]} \\subset \\mathbb{R}^d \\), where \\( \\sum_{k=1}^K p_k = 1 \\) and \\( p_1 \\ge \\dots \\ge p_K \\). In our setting, there are three types of features with significantly different frequencies: common features, rare features, and extremely rare features, ordered from most to least frequent. The indices of these features partition \\( [K] \\) into \\( (K_C, K_R, K_E) \\).\n3. We construct \\( P \\) patches of \\( X \\) as follows.\n\u2022 Feature Patch: Choose \\( p^* \\) uniformly from \\( [P] \\) and we set \\( x^{(p^*)} = v \\).\n\u2022 Dominant Noise Patch: Choose \\( \\tilde{p} \\) uniformly from \\( [P] \\setminus \\{p^*\\} \\). We construct \\( x^{(\\tilde{p})} = a u + \\xi^{(\\tilde{p})} \\) where \\( au \\) is feature noise drawn uniformly from \\( \\{a u_{1,1}, a u_{-1,1}\\} \\) with \\( 0 < a < 1 \\) and \\( \\xi^{(\\tilde{p})} \\) is Gaussian dominant noise drawn from \\( \\mathcal{N}(0, \\sigma_d \\Lambda) \\).\n\u2022 Background Noise Patch: The remaining patches \\( p \\in [P] \\setminus \\{p^*, \\tilde{p}\\} \\) consist of Gaussian background noise, i.e., we set \\( x^{(p)} = \\xi^{(p)} \\) where \\( \\xi^{(p)} \\sim \\mathcal{N}(0, \\sigma_e \\Lambda) \\).\nHere, the noise covariance matrix is defined as \\( \\Lambda := I - \\sum_{s,k} v_{s,k}v_{s,k}^\\top \\), which ensures that Gaussian noises are orthogonal to all features. We assume that the dominant noise is stronger than the background noise, i.e., \\( \\sigma_e < \\sigma_d \\).\nOur data distribution captures characteristics of image data, where the input consists of several patches. Some patches contain information relevant to the image labels, such as cat faces for the label \"cat,\" while other patches contain information irrelevant to the labels, such as the background. Intuitively, there are two ways to fit the given data: learning features or memorizing noise. If a model fits the data by learning features, it can correctly classify test data having the same features. However, if a model fits the data by memorizing noise, it cannot generalize to unseen data because noise patches are not relevant to labels. Thus, learning more features is crucial to achieving better generalization.\nIn real-world scenarios, different features may appear with varying frequencies. For instance, the occurrences of cat's faces and cat's tails in a dataset might differ significantly, although both are relevant to the \"cat\" label. Our data distribution reflects these characteristics by considering features with varying frequencies. To emphasize the distinctions between the three training methods we analyze, we categorize features into three groups: common, rare, and extremely rare. We refer to data points containing these features as common data, rare data, and extremely rare data, respectively. We emphasize that these terminologies are chosen merely to distinguish the three different levels of rarity, and even \u201cextremely rare\u201d features appear in a nontrivial fraction of the training data with high probability (see our assumptions in Section 2.4).\nComparison to Previous Work. Our data distribution is similar to those considered in Shen et al. (2022) and Zou et al. (2023), which investigate the benefits of standard data augmentation methods and Mixup by comparing them to vanilla training without any augmentation. These results consider two types of features\u2014common and rare\u2014with different levels of rarity, along with two types of noise: feature noise and Gaussian noise. However, we consider three types of features: common, rare, and extremely rare, and three types of noise: feature noise, dominant noise, and background noise. This distinction allows us to compare three distinct methods and demonstrate the differences between them, whereas Shen et al. (2022) and Zou et al. (2023) compared only two methods."}, {"title": "2.2 Neural Network Architecture", "content": "For the prediction model, we focus on the following two-layer convolutional neural network where the weights in the second layer are fixed at 1 and -1, with only the first layer being trainable. Several works including Shen et al. (2022) and Zou et al. (2023) also focus on similar two-layer convolutional neural networks.\nDefinition 2.2 (2-Layer CNN). We define 2-layer CNN \\( f_W : \\mathbb{R}^{d \\times P} \\rightarrow \\mathbb{R} \\) parameterized by \\( W = \\{W_1, W_{-1}\\}\\in \\mathbb{R}^{d \\times 2} \\). For each input \\( X = (x^{(1)},...,x^{(P)}) \\in \\mathbb{R}^{d\\times P} \\), we define\n\\[ f_W(X) := \\sum_{p \\in [P]} \\Phi(\\langle w_1, x^{(p)} \\rangle) - \\sum_{p \\in [P]} \\Phi(\\langle w_{-1}, x^{(p)} \\rangle), \\]\nwhere \\( \\Phi(\\cdot) \\) is a smoothed version of leaky ReLU activation, defined as follows.\n\\[\\Phi(z) := \\begin{cases} z & z \\ge r \\\\\n\\frac{1}{2r}(1-\\beta)z^2 + \\beta z & 0 \\le z \\le r,\\\\\n\\beta z & z \\le 0\\end{cases}\\]\nwhere \\( 0 < \\beta < 1 \\) and \\( r > 0 \\).\nPrevious works on the theory of feature learning often consider neural networks with (smoothed) ReLU or polynomial activation functions. However, we adopt a smoothed leaky ReLU activation, which always has a positive slope, to exclude the possibility of neurons \u201cdying\u201d during the complex optimization trajectory. Using smoothed leaky ReLU to analyze the learning dynamics of neural networks is not entirely new; there is a body of work that studies phenomena such as benign overfitting (Frei et al., 2022a) and implicit bias (Frei et al., 2022b; Kou et al., 2023b) by analyzing neural networks with (smoothed) leaky ReLU activation.\nA key difference between ReLU and leaky ReLU lies in the possibility of ReLU neurons \"dying\" in the negative region, where some negatively initialized neurons remain unchanged throughout training. As a result, using ReLU activation requires multiple neurons to ensure the survival of neurons at initialization, which becomes increasingly probable as the number of neurons increases. In contrast, the derivative of leaky ReLU is always positive, ensuring that a single neuron is often sufficient. Therefore, for mathematical simplicity, we consider the case where the network has a single neuron for each positive and negative output. We believe that our analysis can be extended to the multi-neuron case as we validate numerically in Appendix A.2."}, {"title": "2.3 Training Methods", "content": "Using a training set sampled from the distribution \\( \\mathcal{D} \\), we would like to train our network \\( f_W \\) to learn to correctly classify unseen data points from \\( \\mathcal{D} \\). We consider three learning methods: vanilla training without any augmentation, Cutout, and CutMix. We first introduce necessary notation for our data and parameters, and then formalize training methods within our framework.\nTraining Data. We consider a training set \\( \\mathcal{Z} = \\{(X_i, Y_i)\\}_{i\\in[n]} \\) comprising \\( n \\) data points, each independently drawn from \\( \\mathcal{D} \\). For each \\( i \\in [n] \\), we denote \\( X_i = (x_i^{(1)},...,x_i^{(P)}) \\).\nInitialization. We initialize the model parameters in our neural network using random initializa- tion. Specifically, we initialize the model parameter \\( W^{(0)} = \\{w_1^{(0)}, w_{-1}^{(0)}\\} \\), where \\( w_1^{(0)}, w_{-1}^{(0)} \\sim \\mathcal{N}(0, \\sigma_0 I_d) \\). Let us denote updated model parameters at iteration \\( t \\) as \\( W^{(t)} = \\{w_1^{(t)}, w_{-1}^{(t)}\\} \\)."}, {"title": "2.3.1 Vanilla Training", "content": "The vanilla approach to training a model \\( f_W \\) is solving the empirical risk minimization problem using gradient descent. We refer to this method as ERM. Then, ERM updates parameters \\( W^{(t)} \\) of a model using the following rule.\n\\[ W^{(t+1)} = W^{(t)} - \\eta \\nabla_W L_{\\text{ERM}}(W^{(t)}), \\]"}, {"title": "2.3.2 Cutout Training.", "content": "Cutout (DeVries and Taylor, 2017) is a data augmentation technique that randomly cuts out rectangular regions of image inputs. In our patch-wise data, we regard Cutout training as using inputs with masked patches from the original data. For each subset \\( \\mathcal{C} \\) of \\( [P] \\) and \\( i \\in [n] \\), we define augmented data \\( X_{i, \\mathcal{C}} \\in \\mathbb{R}^{d \\times P} \\) as a data point generated by cutting the patches with indices in \\( \\mathcal{C} \\) out of \\( X_i \\). We can represent \\( X_{i, \\mathcal{C}} \\) as\n\\[ X_{i,\\mathcal{C}}=\\begin{pmatrix} x_{i,\\mathcal{C}}^{(1)}, ..., x_{i,\\mathcal{C}}^{(P)} \\end{pmatrix}, \\text{ where } x_{i,\\mathcal{C}}^{(p)} = \\begin{cases} x_i^{(p)} & \\text{if } p \\notin \\mathcal{C},\\\\\n\\textbf{0} & \\text{otherwise}. \\end{cases}\\]\nNote that the output of the model \\( f_W(\\cdot) \\) on this augmented data point \\( X_{i,\\mathcal{C}} \\) is\n\\[ f_W(X_{i,\\mathcal{C}}) = \\sum_{p \\notin \\mathcal{C}} \\Phi(\\langle w_1, x_i^{(p)} \\rangle) - \\sum_{p \\notin \\mathcal{C}} \\Phi(\\langle w_{-1}, x_i^{(p)} \\rangle). \\]\nThen, the objective function for Cutout training can be defined as\n\\[ L_{\\text{Cutout}}(W) := \\frac{1}{n} \\sum_{i \\in [n]} \\mathbb{E}_{\\mathcal{C} \\sim D_{\\mathcal{C}}}[l(Y_i f_W (X_{i,\\mathcal{C}}))], \\]\nwhere \\( D_{\\mathcal{C}} \\) is a uniform distribution on the collection of subsets of \\( [P] \\) with cardinality \\( C \\), where \\( C \\) is a hyperparameter satisfying \\( 1 < C < \\frac{P}{2} \\). We refer to the process of training our model using gradient descent on Cutout loss \\( L_{\\text{Cutout}}(W) \\) as Cutout, and its update rule is\n\\[ W^{(t+1)} = W^{(t)} - \\eta \\nabla_W L_{\\text{Cutout}}(W^{(t)}), \\]\nwhere \\( \\eta \\) is a learning rate."}, {"title": "2.3.3 CutMix Training.", "content": "CutMix (Yun et al., 2019) involves not only cutting parts of images, but also pasting them into different images as well as assigning them mixed labels. For each subset \\( S \\) of \\( [P] \\) and \\( i, j \\in [n] \\), we define the augmented data point \\( X_{i,j,S} \\in \\mathbb{R}^{d \\times P} \\) as the data obtained by cutting patches with indices in \\( S \\) from data \\( X_i \\) and pasting them into \\( X_j \\) at the same indices \\( S \\). We can write \\( X_{i,j,S} \\) as\n\\[ X_{i,j,S} = \\begin{pmatrix} x_{i,j,S}^{(1)}, ..., x_{i,j,S}^{(P)} \\end{pmatrix}, \\text{ where } x_{i,j,S}^{(p)} = \\begin{cases} x_i^{(p)} & \\text{if } p \\in S,\\\\\n x_j^{(p)} & \\text{otherwise}. \\end{cases}\\]\nThe one-hot encoding of the labels \\( y_i \\) and \\( y_j \\) are also mixed with proportions \\( \\frac{|S|}{P} \\) and \\( 1 - \\frac{|S|}{P} \\) respectively. This mixed label results in the loss of the form\n\\[ \\frac{|S|}{P}l(y_i f_W(X_{i,j,S})) + \\left(1-\\frac{|S|}{P}\\right)l(y_j f_W(X_{i,j,S})). \\]\nFrom this, the CutMix training loss \\( L_{\\text{CutMix}}(W) \\) can be defined as\n\\[ L_{\\text{CutMix}}(W) := \\frac{1}{n^2} \\sum_{i,j\\in[n]} \\mathbb{E}_{S \\sim D_S} \\left[ \\left(1-\\frac{|S|}{P}\\right)l(y_i f_W(X_{i,j,S})) + \\frac{|S|}{P} l(y_j f_W(X_{i,j,S})) \\right], \\]\nwhere \\( D_S \\) is a probability distribution on the set of subsets of \\( [P] \\) which samples \\( S \\sim D_S \\) as follows."}, {"title": "2.4 Assumptions on the Choice of Problem Parameters", "content": "To control the quantities that appear in the analysis of training dynamics, we make assumptions on several quantities in our problem setting. For simplicity, we use choices of problem parameters as a function of the dimension of patches d and consider sufficiently large d.\nWe use the standard asymptotic notation \\( O(\\cdot), \\Omega(\\cdot), \\Theta(\\cdot), o(\\cdot), \\omega(\\cdot) \\) to express the dependency on d. We also use \\( \\tilde{O}(\\cdot), \\tilde{\\Omega}(\\cdot), \\tilde{\\Theta}(\\cdot) \\) to hide logarithmic factors of d. Additionally, poly(d) (or polylog(d)) represents quantities that increase faster than \\( d^{c_1} \\) (or \\( (\\log d)^{c_1} \\)) and slower than \\( d^{c_2} \\) (or \\( (\\log d)^{c_2} \\)) for some constant \\( 0 < C_1 < C_2 \\). Similarly, \\( o(1/\\text{poly}(d)) \\) (or \\( o(1/\\text{polylog}(d)) \\)) denotes some quantities that decrease faster than \\( 1/d^c \\) (or \\( 1/(\\log d)^c \\)) for any constant c. Finally, we use \\( f(d) = o(g(d)/\\text{polylog}(d)) \\) when \\( f(d)/g(d) = o(1/\\text{polylog}(d)) \\) for some function f and g of d.\nAssumptions. We assume that \\( P = \\Theta(1) \\) and \\( P > 8 \\) for simplicity. Additionally, we consider a high-dimensional regime where the number of data points is much smaller than the dimension d, which is expressed as \\( n = o(\\alpha^2 \\beta \\sigma_d^{-1} \\sigma_e d/\\text{polylog}(d)) \\). We also assume that \\( p_k n = \\omega(\\eta \\log^2 d) \\) for all \\( k \\in [K] \\), which ensures the sufficiency of data points with each feature.\nIn addition, as we will describe in Section 4, the relative scales between the frequencies of features and the strengths of noises play crucial roles in our analysis, as they serve as a proxy for the \u201clearning speed\" in the initial phase. For common features \\( k \\in K_C \\), we assume \\( p_k = \\Theta(1) \\) and the learning speed of common features is much faster than that of dominant noise, which translates into the assumption \\( \\sigma_d^2 d = \\omega(\\beta \\eta) \\). For rare features \\( k \\in K_R \\), we assume \\( p_k = \\Theta(p_R) \\) for some \\( p_R \\), and we consider the case where the learning speed of rare features is much slower than that of dominant noise but faster than background noise, which is expressed as \\( p_R \\eta = o(\\alpha^2 \\sigma_d \\sigma_e d/\\text{polylog}(d)) \\) and \\( \\sigma_d \\sigma_e = o(\\beta p_R \\eta) \\). Finally, for extremely rare features \\( k \\in K_E \\), we say \\( p_k = \\Theta(p_E) \\) for some \\( p_E \\) and their learning is even slower than that of background noises, which can be expressed as \\( p_E \\eta = o(\\alpha^2 \\sigma_e d/\\text{polylog}(d)) \\).\nLastly, we assume the strength of feature noise satisfies \\( a = o(\\eta^{-1} \\beta \\sigma_e d/\\text{polylog}(d)) \\), and \\( r, \\sigma_0, \\eta > 0 \\) are sufficiently small so that \\( \\sigma_0, r = o(a/\\text{polylog}(d)) \\), \\( \\eta = o(r \\sigma_e^2 d^{-1}/\\text{polylog}(d)) \\).\nWe list our assumptions in Assumption B.1 and there are many choices of parameters satisfying the set of assumptions, including:\n\\( P = 8, C = \\frac{P}{2} = 2, \\eta = \\Theta(d^{-0.4}), a = \\Theta(d^{-0.02}), \\beta = \\frac{1}{\\text{polylog}(d)}, \\sigma_0 = \\Theta(d^{-0.2}), r = \\Theta(d^{-0.2}), \\sigma_d = \\Theta(d^{-0.305}), \\sigma_e = \\Theta(d^{-0.375}), p_R = \\Theta(d^{-0.1}), p_E = \\Theta(d^{-0.195}), \\eta = \\Theta(d^{-1}) \\)."}, {"title": "3 Main Results", "content": "In this section, we provide a characterization of the high probability guarantees for the behavior of models trained using three distinct methods we have introduced. We denote by \\( T^* \\) the maximum admissible training iterates and we assume \\( T^* = \\text{poly}(d) \\) with a sufficiently large polynomial in d. In all of our theorem statements, the randomness is over the sampling of training data and the initialization of models and all results hold under the condition that d is sufficiently large.\nThe following theorem characterizes training accuracy and test accuracy achieved by ERM."}, {"title": "Theorem 3.1.", "content": "Let \\( W^{(t)} \\) be iterates of ERM. Then with probability at least \\( 1 - o(\\frac{1}{\\text{poly}(d)}) \\), there exists \\( T_{\\text{ERM}} \\) such that any \\( T \\in [T_{\\text{ERM}}, T^*] \\) satisfies the following:\n\u2022 (Perfectly fits training set): For all \\( i \\in [n] \\), \\( Y_i f_W^{(T)}(X_i) > 0 \\).\n\u2022 (Random on (extremely) rare data)): \\( \\mathbb{P}_{(X,y)\\sim \\mathcal{D}} [y f_W^{(T)}(X) > 0] = 1 - \\frac{1}{2} \\sum_{k \\in K_E \\cup K_R \\cup K_E} p_k \\pm o(\\frac{1}{\\text{poly}(d)}) \\)."}, {"title": "Theorem 3.2.", "content": "Let \\( W^{(t)} \\) be iterates of Cutout training. Then with probability at least \\( 1 - o(\\frac{1}{\\text{poly}(d)}) \\), there exists \\( T_{\\text{Cutout}} \\) such that any \\( T \\in [T_{\\text{Cutout}}, T^*] \\) satisfies the following:\n\u2022 (Perfectly fits augmented data): For all \\( i \\in [n] \\) and \\( \\mathcal{C} \\subset [P] \\) with \\( |\\mathcal{C}| = C \\), \\( Y_i f_W^{(T)}(X_{i,\\mathcal{C}}) > 0 \\).\n\u2022 (Perfectly fits original training data): For all \\( i \\in [n] \\), \\( Y_i f_W^{(T)}(X_i) > 0 \\).\n\u2022 (Random on extremely rare data): \\( \\mathbb{P}_{(X,y)\\sim \\mathcal{D}} [y f_W^{(T)}(X) > 0] = \\frac{1}{2} - \\frac{1}{2} \\sum_{k \\in K_E \\cup K_E} p_k \\pm o(\\frac{1}{\\text{poly}(d)}) \\)."}, {"title": "Theorem 3.3.", "content": "Let \\( W^{(t)} \\) be iterates of CutMix training. Then with probability at least \\( 1 - o(\\frac{1}{\\text{poly}(d)}) \\), there exists some \\( T_{\\text{CutMix}} \\in [0, T^*] \\) that satisfies the following:\n\u2022 (Finds a near stationary point): \\( ||\\nabla_W L_{\\text{CutMix}} (W^{(T_{\\text{CutMix}})} )|| = o(\\frac{1}{\\text{poly}(d)}) \\).\n\u2022 (Perfectly fits original training data): For all \\( i \\in [n] \\), \\( Y_i f_W^{(T_{\\text{CutMix}})}(X_i) > 0 \\).\n\u2022 (Almost perfectly classifies test data): \\( \\mathbb{P}_{(X,y)\\sim \\mathcal{D}} [y f_W^{(T_{\\text{CutMix}})}(x) > 0] = 1-o(\\frac{1}{\\text{poly}(d)}) \\)."}, {"title": "4 Overview of Analysis", "content": "In this section, we discuss key proof ideas and the main challenges in our analysis. For ease of presentation, we consider the case a = 0. Although our assumptions do not allow the choice a = 0, the choice of nonzero a is to show guarantees on the test accuracy and does not significantly affect the feature learning aspect."}, {"title": "4.1 Vanilla Training and Cutout Training", "content": "We now explain why ERM fails to learn (extremely) rare features, while Cutout can learn rare features but not extremely rare features. Let us consider ERM. From (1), for \\( s, s' \\in \\{\\pm 1\\}, k \\in [K", "n": "and \\( p \\in [P", "s direction is updated as\n\\[ (w_s^{(t+1)}, v_{s": "k}) = (w_s^{(t)}, v_{s',k}) - s'\\eta \\frac{1}{n} \\sum_{j \\in V_{s',k}} l'(y_j f_W^{(t)}(X_j)) \\Phi'(\\langle w_s^{(t)}, v_{s',k} \\rangle), \\", "update": "f inner product of \\( w_s \\) with a noise patch \\( \\xi_i^{(p)} \\) can be written as\n\\[ (w_s^{(t+1)}, \\xi_i^{(p)}) = (w_s^{(t)}, \\xi_i^{(p)}) - s_i\\eta \\frac{1}{n} l'(y_i f_W^{(t)}(X_i)) \\Phi'(\\langle w_s^{(t)}, \\xi_i^{(p)} \\rangle) ||\\xi_i^{(p)}||^2, \\", "s and \\( (w_s^{(t+1)}, \\xi_i^{(p)}) \\)": "are almost monotonically increasing or decreasing. We address the approximation errors using a variant of the technique introduced by Cao et al. (2022), as detailed in Appendix B.3.\nFrom (4) and (5), we can observe that in the early phase of training satisfying \\( -l'(y_i f_W^{(t)}(X_i)) ="}]}