{"title": "Robustness of LLMs to Perturbations in Text", "authors": ["Ayush Singh", "Navpreet Singh", "Shubham Vatsal"], "abstract": "Having a clean dataset has been the foundational assumption of most natural language processing (NLP) systems. However, properly written text is rarely found in real-world scenarios and hence, oftentimes invalidates this foundational assumption. Recently, large language models (LLMs) have achieved remarkable performance in a wide array of NLP tasks. Nevertheless, the degree to which these LLMs are robust to semantic preservation of morphological variations in text has been sparsely studied. In a world becoming increasingly dependent on LLMs for most of the NLP tasks, it becomes crucial to know their robustness to numerous forms of noise found in real-world text. In this work, we systematically evaluate LLMs' resilience to corrupt variations of the original text. We do so by artificially introducing different levels of noise into the discussed datasets. We show that contrary to popular beliefs, generative LLMs are quiet robust to commonly found perturbations in text. Additionally, we test LLMs' performance on multiple benchmarks achieving a new state of the art on the task of grammar error correction. To empower future research, we are also releasing a dataset annotated by humans stating their preference for LLM vs. human-corrected outputs along with the code to reproduce our results.", "sections": [{"title": "1 Introduction", "content": "Modern day language processing (NLP) pipelines have been highly dependent on the input data to be as clean as possible. While this assumption works well in well-curated settings, it often gets invalidated in real-world setting leading to brittle systems that work well in laboratories, albeit, breaking down on the noisy naturally occurring data (Wu et al., 2021). Consequently, there emerges a need to test any new innovation in the field of NLP on it's robustness against several different forms of noise that are pervasive in real-world scenarios (Galliers and Sp\u00e4rck Jones, 1993).\nNoise in real-world datasets can originate from a plethora of resources. Some of the errors can originate from human factors like spelling or grammatical errors while others can be machine induced like errors from optical character recognition (OCR), or automated speech recognition (ASR) systems. These various forms of noise have been a significant impediment deploying systems built on cleaner datasets in a real word setting. For example, even notes taken by native speakers oftentimes include spelling and grammatical mistakes, while text written by non-native speakers miss determiners and exhibit numerous forms of valid but orthogonal variations. Their impact on downstream performance can vary in degree ranging from a slight change in prediction probability to completely flipping the polarity or semantic meaning of a text (See Table 1 for examples). The field of NLP that deals with detecting when a meaning has changed or shifted is known as Lexical Semantic Change (LSC) detection (Gulordava and Baroni, 2011). Although LSC approaches help detect this shift, they do not offer the types of change that can be made to shift from one semantic to another such as translating an incorrect text into a lexical and grammatically correct one without changing the meaning. Traditionally, machine learning (ML) systems have handled noise in text by using data cleaning pipelines comprising multiple phases, the most important one being known as grammar error correction (GEC) phase. Bryant et al. (2023) pointed out that GEC is a misnomer and has lately been more generally referred to as language error correction (LEC) that involves not only grammatical mistakes such as improper subject-verb agreement, but also spelling or type errors and other forms of errors as well. However, LEC is not an easy task, which is why ML has been employed for it. Although ML has progressed the state of LEC, it has not solved it entirely. In the last decade, emergence of advanced methods like subword embeddings has increased"}, {"title": "2 Related Work", "content": "Noise in natural language text has been a well-studied area of research for some time now. Research in this domain started with crude categorization of different types and progressed to more recent fine-grained specifications (Lopresti, 2008; Dey and Haque, 2009; Passonneau et al., 2009; Xing et al., 2013; Al Sharou et al., 2021). Lopresti (2008) studied the negative effects of OCR systems errors on NLP systems while Dey and Haque (2009) studied the negative effects of noise on text mining applications. With the advent of LLMs, Srivastava et al. (2020); Wang et al. (2023); N\u00e1plava et al. (2021) studied the impact of noisy text on LLMs and showed that it has negative results. However, they did not evaluate the more recent modern-day generative LLMs.\nLexical Semantic Change. LSC detection techniques are split into following three categories (i) semantic vector spaces, (ii) topic distributions,"}, {"title": "3 Methods", "content": "In this section, we elaborate on the LSC detection methods used to assess the degree of the effect of"}, {"title": "3.1 Problem statement", "content": "We hypothesize that any change in how an LLM encodes a sequence can be measured by calculating the difference in its dense representations. Therefore, we measure the difference between an originally corrupt or noisy text \\( \\mathcal{T} \\) with its corrected counterpart \\( \\tilde{\\mathcal{T}} \\).\nOur null hypothesis \\( \\mathcal{H}_0 \\), therefore is that the internal encoding of LLM should be different for a text and it's semantically similar but incorrect version. On the other hand, our alternative hypothesis \\( \\mathcal{H}_a \\) states that there should be no difference in how LLM encodes a text and it's grammatically incorrect version as the LLM has learned to extract semantic meaning despite of problems in how a text is written. To prove our hypothesis, we measure how distant is the embedding of the corrupted text from it's correct counterpart. If this distance is not significant enough then our null hypothesis is rejected."}, {"title": "3.2 Lexical Semantic Change (LSC) Detection", "content": "In order to measure the similarity of dense representations of \\( \\mathcal{T} \\) and \\( \\tilde{\\mathcal{T}} \\), or when a LSC is detected, we use a standard metric called cosine similarity. The cosine similarity measures the angular distance of two vectors by first performing their dot product, divided by the product of their lengths as depicted by the following equation:\n\\[ Sim_{cosine}(\\vec{x}, \\vec{y}) = \\frac{\\vec{x} \\cdot \\vec{y}}{\\|\\vec{x}\\| \\cdot \\|\\vec{y}\\|} = \\frac{\\sum_{i=1}^{N} x_i y_i}{\\sqrt{\\sum_{i=1}^{N} x_i^2} \\cdot \\sqrt{\\sum_{i=1}^{N} y_i^2}} \\] (1)"}, {"title": "3.3 Perturbations", "content": "Even though benchmark LEC datasets have a incorrect version and it's corresponding human corrected version, the benchmarks only capture a certain aspect of error occurring in text. Fortunately, prior research has done the immense job of categorizing the type of errors pervasive in real-world data because of which we can simulate the errors and measure robustness of LLM against it. To that end, among the numerous form of errors, we used domain knowledge and heuristics to rank the errors and selected for most relevant errors. These errors are elaborated as follows:"}, {"title": "3.4 Prompting", "content": "We evaluate LLMs for LEC task by prompting them to correct the errors in the dataset. Even though there are numerous advanced forms of prompting, however, they have been found to be task and dataset dependent. Therefore, in this work, we kept prompting variations to minimal. We ask the model to correct the language of the text with as little external knowledge as possible in the following prompt format:"}, {"title": "4 Experiments", "content": "We split our experiments into increasingly complex phases starting with single perturbations and ramping the combinations up to five perturbations. For perturbations, we sequentially corrupt where each method's probability of an augmentation is 30% with a maximum of 10 words that can be operated with either of substitution, insertion or deletion. Additionally, we discard any samples whose unigram Jaccard similarity coefficient less than 0.7 after all the corruptions have taken place. The models we chose for comparing dense representations are latest (fourth) version of GPT (OpenAI, 2023) with 8k context window (text-embedding-ada-002), the 7 billion version of an open source model LLaMa 3 (Touvron et al., 2023) with 4k context window (decoder head embedding) and a non-generative model BERT (Devlin et al., 2018) with 512 context window (CLS token embedding). We kept all the hyper-parameters to default as the temperature, frequency penalty, and presence penalty to 0 and max tokens to 1000. All of the aforementioned LLMs are based on similar transformer based architectures (Vaswani et al., 2017). Additionally, we evaluate LLMs on two recent LEC benchmark datasets that earlier works have not evaluated against."}, {"title": "4.1 Datasets", "content": "For LEC task, we use two benchmark datasets named JFLEG and BEA-19. The JHU FLuency-Extended GUG corpus (JFLEG) extends the GUG (Grammatical/Ungrammatical) corpus by Heilman et al. (2014) with a layer of annotation via four human annotators (Napoles et al., 2017). The key differentiator with JFLEG is that the corrections were made with inclination for fluency rather than minimal edits. This is done on the GUG corpus which is a cross-section of ungrammatical data, containing sentences written by English language learners with different L1s and proficiency levels. BEA-19 (Bryant et al., 2019) was introduced as a shared task in the workshop of Building Educational Applications 2019. BEA dataset contains essays on approximately 50 topics written by more than 300 authors from around the world (including native English as well as British and American undergraduates speakers).\nApart from including both the aforementioned datasets in our LSC task, we also generate synthetic datasets for LSC, we use the IMDB movie review dataset (Maas et al., 2011) and sub-sample 1000 reviews from it. The IMDB dataset was crowd-sourced reviews of movies with varying ratings. Even though parallel rating is available for each review, we did not need it for our usecase. For perturbations, we used the work by (Ma, 2019) to generate different semantic preserving variations of a given text."}, {"title": "4.2 Annotation", "content": "(Bryant et al., 2023) posited that most of the LEC evaluation metrics have been calibrated against human preferences as it is a challenging task to evaluate the quality of a correction computation even when ground truth is present. Taking this into account, we also setup an annotation task as a true measure of LLM's ability to perform GEC. We hypothesized that LLMs might behaviorally perform LEC that might be different from the way humans approach LEC.\nWe sub-sample records from both the JFLEG and BEA dataset to be corrected by both GPT and humans (for humans, we already have the ground truth). We focus more on JFLEG over BEA as the former has four expert human annotations per data point compared to that of one from BEA in the dev set. Additionally, BEA dataset itself was"}, {"title": "5 Results", "content": "As shown in Table 3, even after severe degradation of the original text, the embeddings of clean and corrupted version of text remain fairly same throughout. This leads us to reject our null hypothesis \\( \\mathcal{H}_0 \\) and accept alternative hypothesis \\( \\mathcal{H}_a \\) that LLMs are robust to semantic-preserving variations of text. To our surprise, LLaMa did not fair well to different forms of perturbations and the null hypothesis cannot be rejected for LLaMa. Additionally, as can be seen in figure 1, both BERT (Devlin et al., 2018) and GPT (OpenAI, 2023) remain fairly robust to different forms of perturbations whereas"}, {"title": "6 Discussion", "content": "The authors of BEA-19 use a span-based evaluation metric F0.5. In span-based correction, a system is only rewarded if a system edit exactly matches a reference edit in terms of both its token offsets and correction string. This is a harsh metric especially for a task like LEC where there could be so many possible variations of the correct answer to any given input. Additionally, because all of the metrics for GEC are dependent on comparing the correction with human corrected ground truth. This comparison makes an inherent assumption that human correction is the utmost form of correction, whereas from our findings, this turned out not to be true. On the contrary, we argue that because LLMs have been trained on far larger datasets than humans can ever peruse, therefore, LLM will have better estimate of the correction. In some of the cases, we even found out that GPT's LEC was far better than the ground truth corrections themselves.\nTo measure the aspect or novelty of GPT that pre-existing metrics were not able to surface, we setup a preference learning task among 3 annotators where each document was annotated thrice. We sampled 100 records from both the JFLEG and BEA-19 dataset which were corrected by both GPT and then compared to corrections of humans. To assess the reliability of the annotations, we use Fleiss Kappa score which came to 0.62 and the inter-annotator agreement (IAA) which was found to be 76%. Furthermore, the annotators found the correction of GPT and human to be the same on average 11% of the time. There were also occurrences when both the corrections seemed correct and therefore annotators were unable to decide, this happened 9% of the time. As shown in table 6, analysing the annotations revealed that annotators preferred the correction by GPT 73% and 68% respectively for JFLEG and BEA-19 datasets more than those done by human themselves. This shows the superior capabilities of GPT on tasks like LEC.\nFurthermore, unlike pre-existing LEC systems, GPT goes beyond by offering the ability to further tune how one would like the correction to be made with respect to focus on fluency, formalism, or strict grammar and more by means of prompts. Raheja et al. (2023) shows the extent of different types of edits that can be formulated by LLMs. Examples of this can be seen in Table 5.\nOn the one hand, GPT performed well on LEC and LSC, on the other hand, LLaMa, did not perform as well, even though being an LLM. We hypothesized that this might be happening due to 1) LLaMa being a decoder only architecture where GPT's embedding API might only be using encoder model 2) LLaMa not being used to operating on sentences that are short or even single sentences. We validated the latter hypothesis by setting up an experiment where we combined upto ten sentences before passing them to LLaMa for LEC. As expected, the LSC detection performance increased by 54%.\nThe methodology behind how to measure performance of LEC systems goes as far back as the LEC tasks itself, one that has always been called out as not on par with judging true performance of the system (Bryant et al., 2023). This is why most works ultimately aim to measure performance with human preferences. Similarly, in our study, we found GLEU and ERRANT do not paint the complete picture of GPT. Our annotators reported that the ground truth themselves were oftentimes incorrect, and as evident by preferences, annotators preferred GPT's correction over that of the humans. Another important aspect to note here, apart from the ground truth being unclean, is that our study compares supervised with unsupervised methods. Supervised models use 70%-80% of the available data as their training set which allows learning the nuisances of the dataset unlike GPT, where no matter the incorrect ground truths, all experiments were conducted in a zero-shot setting. Therefore, even though GPT's performance on BEA dataset lags behind state-of-the-art system, we posit that it is not due to lack of ability rather the state of art systems being trained to mimic human corrections, which in of itself are sub-optimal.\nUnlike BEA-19, JFLEG uses GLEU as its evaluation metric which is not only less stringent than F0.5, but as our experiments show also aligns with human preferences better than F0.5. To reiterate, in a LEC task, there could be many possible variations of the correct answer for any given input. The authors of JFLEG take this factor into consideration and provide four different variations of ground"}, {"title": "7 Conclusion & Future Work", "content": "Discerning whether or not a system is robust to noise and more importantly, understands semantically what a corrupted text means is the foundation of NLP. By building systems that are, for the most part, immune to noises occurring in real-world data, we make sure our NLP systems are not fragile and exhibit unintended behavior when deployed in the wild. In this work, we set out to show that modern-day LLMs do not care about corruptions as long as they are semantically the same. We do this by combining two tangential fields of NLP, Lexical Semantic Change detection and Language error correction. On the one hand, we used LSC techniques to show that the internal encoding of LLMs remains unchanged in response to corruptions in text; on the other hand, we show that unsupervised LLMs can perform zero-shot on par and even better in the downstream task of LEC. We also share preference dataset with the community. Our work paves the way for advanced LLM based LEC systems as we depart from the predominant inclusion of the LEC module as part of standard NLP systems.\nAs a part of future work, we have several fronts where we strive to extend our work. First, we aim to expand our study to build and study LEC on longer passages and documents rather than just sentence-level corrections. Second, we also aim to include machine translation as part of standard LEC practices, and motivate the community at large to consider this an important step forward in the field of text normalization and future of LEC systems. Third, we aim to refine the perturbation methods as they could change the meaning of sentences at times. Finally, as discussed in Section 6, we would like to improve the state of open source models like LLaMa to make further progress in unsupervised LEC as it has been novel in the corrections as compared to humans, as evident from our study."}, {"title": "8 Limitations", "content": "One drawback of using LLMs like GPT is the side-effects incurred i.e. unintended transformations being performed on the text. This gets even worse with smaller LLMs like LLaMa. As an example from BEA-19 dataset, a sentence \u201cAround the city, you can find many places where people throw frigo, kitchen, \"amianto\", old things or furniture.\" contains a French and an Italian phrases frigo and amianto respectively. Even though we did not explicitly asked in the prompt shown in 3.4, when GPT error corrected this, it automatically translated frigo and amianto to their corresponding English translations of fridge and asbestos respectively. This can be seen both as an advantage and disadvantage, for instance, GPT did an even superior task of LEC and implicit multi-lingual machine translation as a part of LEC task. We leave further investigation into this phenomena for future work."}]}