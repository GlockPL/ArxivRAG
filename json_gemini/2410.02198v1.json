{"title": "G2T-LLM: GRAPH-TO-TREE TEXT ENCODING FOR\nMOLECULE GENERATION WITH FINE-TUNED LARGE\nLANGUAGE MODELS", "authors": ["Zhaoning Yu", "Xiangyang Xu", "Hongyang Gao"], "abstract": "We introduce G2T-LLM, a novel approach for molecule generation that uses\ngraph-to-tree text encoding to transform graph-based molecular structures into a\nhierarchical text format optimized for large language models (LLMs). This en-\ncoding converts complex molecular graphs into tree-structured formats, such as\nJSON and XML, which LLMs are particularly adept at processing due to their ex-\ntensive pre-training on these types of data. By leveraging the flexibility of LLMs,\nour approach allows for intuitive interaction using natural language prompts, pro-\nviding a more accessible interface for molecular design. Through supervised fine-\ntuning, G2T-LLM generates valid and coherent chemical structures, addressing\ncommon challenges like invalid outputs seen in traditional graph-based methods.\nWhile LLMs are computationally intensive, they offer superior generalization and\nadaptability, enabling the generation of diverse molecular structures with minimal\ntask-specific customization. The proposed approach achieved comparable perfor-\nmances with state-of-the-art methods on various benchmark molecular generation\ndatasets, demonstrating its potential as a flexible and innovative tool for AI-driven\nmolecular design.", "sections": [{"title": "1 INTRODUCTION", "content": "Molecular generation is a critical task in fields such as drug discovery, material science, and chem-\nistry (Schneider & Fechner, 2005; Simonovsky & Komodakis, 2018; Elton et al., 2019). The ability\nto design and create novel molecules with specific properties can accelerate the development of\nnew therapies, advanced materials, and innovative chemicals. Traditional approaches to molecular\ngeneration, such as rule-based systems (Schneider & Fechner, 2005; Sastry et al., 2011) and graph-\nbased (You et al., 2018; Madhawa et al., 2019; Shi et al., 2020) models, have provided foundational\ntools. However, these methods often face limitations in generating diverse, valid, and chemically\ncoherent molecular structures, restricting their ability to explore the vast chemical space effectively\n(Vignac et al., 2022; Jo et al., 2022). Recent advancements in deep learning, especially the rise of\nlarge language models (LLMs), offer new opportunities for molecular generation (Brahmavar et al.,\n2024; Wang et al., 2024; Yao et al., 2024). Unlike traditional methods, LLMs are not constrained\nby domain-specific rules and can generalize from vast amounts of data. This flexibility allows them\nto generate creative and diverse content, potentially uncovering novel chemical compounds. Prior\nnon-LLM approaches, such as graph-based generative models (You et al., 2018; Madhawa et al.,\n2019; Shi et al., 2020; Luo et al., 2021; Vignac et al., 2022; Jo et al., 2022), often struggle with lim-\nited generalization, rule-based rigidity, or difficulty scaling to more complex chemical structures. In\ncontrast, LLMs can adapt to a wide range of prompts and provide greater flexibility, making them\nan attractive choice for AI-driven molecular generation.\nDespite the promise of LLMs, applying them to molecular generation presents a unique challenge.\nMolecular structures are typically represented as graphs, with atoms as nodes and bonds as edges.\nLLMs, however, are trained to understand sequences of tokens (Vaswani, 2017), particularly in\nstructured text formats such as XML and JSON (Brown, 2020), and are not inherently designed to\nprocess graph-based data. This mismatch creates a barrier when attempting to use LLMs for tasks\nthat require understanding the relational and non-linear properties of molecular structures. LLMs"}, {"title": "2 RELATED WORK", "content": "Graph Generation. The graph generation task aims to learn the distribution of graphs. The tradi-\ntional approaches (Zang & Wang, 2020; Shi et al., 2020; Luo et al., 2021; You et al., 2018; Madhawa\net al., 2019; Dai et al., 2018) such as auto-regression, Generative Adversarial Network (GAN), and\nVariational Autoencoder (VAE) have been explored for this purpose. However, they have faced\nchallenges in modeling the permutation-invariant nature of graph distribution and learning the rela-\ntionship between edges and nodes, often due to limitations in their model capacity. Recent advance-\nments in diffusion methods (Niu et al., 2020; Jo et al., 2022; Vignac et al., 2022; Jo et al., 2023)\nhave significantly improved graph generation. GDSS (Jo et al., 2022) generates both node features\nand adjacency matrices simultaneously, resulting in better alignment with graph datasets. DiGress\n(Vignac et al., 2022) addresses the challenge of generating graphs with categorical node and edge\nattributes, which is a difficult task due to the unordered nature and sparsity of graphs. GruM (Jo\net al., 2023) directly learns graph topology, improving connectivity and structure recovery.\nGraph to Text for LLM. The emergence of large language models (LLMs) has driven significant\nadvancements in the natural sciences (Taylor et al., 2022; Liu et al., 2024). These models are trained\non vast amounts of text data, the most abundant type of data, contributing to their success across\nmany tasks. Multi-modal methods (Luo et al., 2023; Le et al., 2024) have been proposed to incorpo-\nrate both graph and text information. They typically rely on graph neural networks or transformers\nto encode graphs. However, these methods often use text, such as SMILES, to represent molecular\nfeatures. SMILES may not tokenize the molecular structure effectively, limiting the ability to rep-\nresent the molecule structure accurately. As a result, the graph embeddings may be too weak for\nintricate molecular structures, limiting performance in molecular generation tasks."}, {"title": "3 G2T-LLM", "content": "This section introduces G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with\nFine-Tuned Large Language Models."}, {"title": "3.1 CHALLENGES AND MOTIVATIONS", "content": "Molecular graphs pose a challenge for LLMs due to their inherently complex, non-linear struc-\ntures, where atoms (nodes) and bonds (edges) form intricate connectivity patterns, including rings,\nbranches, and cycles. Traditional LLMs excel at processing sequential data, such as natural lan-\nguage, where information flows in a linear manner. However, molecular graphs do not naturally\nconform to this format, as their connections often lack a clear, ordered sequence. This mismatch\ncomplicates the application of LLMs to molecule-related tasks.\nDespite these challenges, LLMs have shown a capacity to handle structured, hierarchical data for-\nmats, such as JSON and XML. These formats share some of the complexity of graphs but are still\nexpressed as trees, with clear parent-child relationships between elements. LLMs trained on such\ndata can handle hierarchical structures by processing them as sequences while maintaining the re-\nlationships and nested dependencies inherent to these structures. This training has made LLMs\nparticularly adept at handling data that can be decomposed into nested layers, making them better\nsuited for tree-like representations than arbitrary graphs.\nTo leverage this strength, we propose encoding molecular graphs into a tree structure. This approach\nis inspired by SMILEs, which are essentially tree representations of molecular graphs, proving that\nmolecular graphs can be effectively serialized as trees while preserving their chemical properties.\nThis encoding acts as a bridge between the graph-based molecular structures and the LLM's ability\nto process and generate hierarchical data. The LLM can be trained on these tree-encoded molecules,\nand it can also output molecules in the same structured format, facilitating the generation of coherent"}, {"title": "3.2 GRAPH-TO-TREE TEXT ENCODING", "content": "To make molecular graphs accessible to LLMs, we introduce a tree-based encoding inspired by\nthe SMILES format. SMILES encodes molecules by performing a depth-first traversal over the\nmolecular graph and representing it as a linear string. In our approach, we extend this traversal to\nbuild a hierarchical tree structure, where atoms are represented as nodes and their bonds as edges\nconnecting them. The hierarchical nature of the tree is well-suited for the LLM's training with\ntree-like structures.\nHowever, molecular graphs often contain rings and cycles-features that trees cannot naturally rep-\nresent. To address this, we assign each atom in the molecule a unique identifier (ID). When the\ntraversal encounters a ring closure or cycle, the tree refers back to the atom's unique ID rather\nthan creating a new node, thereby preserving both the hierarchical structure and chemical validity.\nThis encoding technique ensures that we accurately capture the full molecular graph in a way the\nLLM can process, while maintaining the integrity of complex molecular features such as rings and\nbranches. Algorithm 1 and Algorithm 2 describe the processes for converting a molecular graph to\na tree-structured text representation and for reconstructing the graph from this format, respectively.\nFigure 3 illustrates the graph-to-tree text encoding."}, {"title": "3.3 \u03a4\u039f\u039a\u0395N CONSTRAINING FOR VALID TREE-STRUCTURE GENERATION", "content": "Despite the advancements in LLMs, there remains a significant challenge in ensuring that the out-\nputs adhere to valid tree-structured formats. LLMs, while capable of generating coherent text, may\nproduce sequences that do not respect the hierarchical relationships required for molecular repre-"}, {"title": "3.4 SUPERVISED FINE-TUNING LLMS FOR MOLECULAR GENERATION", "content": "A key challenge in leveraging large language models for molecular generation is that, without spe-\ncialized training, they may struggle to produce valid molecular structures, particularly when dealing\nwith complex features such as rings, cycles, and the inherent chemical constraints that govern molec-\nular formation. Supervised fine-tuning addresses this issue by teaching the LLM domain-specific\nrules and patterns, enabling it to generate valid molecular structures that adhere to chemical princi-\nples.\nWe structure the fine-tuning process as a molecular completion task. The LLM is trained by prompt-\ning it with a partial molecular structure, encoded using the graph-to-tree text encoding and tasking\nit with predicting the remaining atoms and bonds necessary to complete the molecule. For each\ntraining example, we provide the LLM with an incomplete molecular graph, and the model is then\nexpected to generate the missing parts based on the information provided. The model's output is\nevaluated against the full molecular structure's text encoding, and the loss is computed based on the"}, {"title": "3.5 INFERENCE PROCESS OF G2T-LLM", "content": "The molecular generation process begins with selecting a random molecular component, which\ncould be an atom, a bond, or even a larger motif. This component serves as the initial prompt\nfor the fine-tuned LLM. The component is encoded into the graph-to-tree text format, creating a\ntree-structured representation that the LLM can process.\nOnce the LLM receives this initial prompt, it is tasked with generating the subsequent components\nof the molecular structure. At each step, the LLM's output is constrained by the Token Constraining\nmechanism, ensuring that only chemical and schema-valid tokens such as specific atom types and\nbond types are generated. These constraints help guide the LLM in maintaining the coherence of\nthe structure, preventing invalid or nonsensical outputs, and ensuring that the generated molecule\nadheres to the expected chemical rules. As the LLM iteratively predicts new components, these\noutputs are progressively combined into an expanding tree-structured text. This generated text rep-\nresents the molecular graph, with nodes corresponding to atoms and edges corresponding to bonds.\nOnce the generation process is complete, the final tree-structured text is decoded back into a full\nmolecular graph. This graph is then translated into a standard molecular format, fully reconstructing\nthe molecule from the text generated by the LLM. Figure 3.4 illustrates the inference process of\nG2T-LLM."}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct comprehensive experiments on two real-world datasets to evaluate the\neffectiveness of our proposed methods."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Datasets and Metrics. We evaluate the quality of molecule generation using two real-world\ndatasets: QM9 (Ramakrishnan et al., 2014) and ZINC250k (Irwin et al., 2012). Following the evalu-\nation setting used in (Jo et al., 2023), we measure model performance across four metrics. Validity is\nthe proportion of generated molecules that are valid without any valency corrections. Novelty is the\nproportion of valid molecules that are not present in the training dataset. Frechet ChemNet Distance\n(FCD) (Preuer et al., 2018) measures the similarity between two molecule sets by comparing the\nactivations of the penultimate layer of the ChemNet model. Scaffold similarity (Scaf.) evaluates the\nmodel's ability to generate similar substructures.\nBaselines. We compare our model with following molecular graph generation methods.\nMoFlow (Zang & Wang, 2020) is a one-shot flow-based model that generates entire molecular\ngraphs in a single step. GraphAF (Shi et al., 2020) and GraphDF(Luo et al., 2021) are autore-\ngressive flow-based models, generating molecules sequentially. Additionally, we evaluate against\nthe diffusion models. EDP-GNN (Niu et al., 2020) is a score-based model designed for generating\nadjacency matrices. GDSS (Jo et al., 2022) uses a continuous diffusion process for molecule gen-\neration, DiGress (Vignac et al., 2022) employs a discrete diffusion approach, and Grum (Jo et al.,\n2023) designed a mixture of endpoint-conditioned diffusion processes.\nAlthough several studies have explored using LLMs for molecular generation, direct comparisons\nwith our approach are not feasible. For instance, LMLF (Brahmavar et al., 2024), Grammar Prompt-\ning (Wang et al., 2024), and LLM4GraphGen (Yao et al., 2024) all employ rule-based prompt-\nengineering techniques that fundamentally differ from our SFT LLM approach. These models rely\non predefined rules and heuristics to guide the generation process, which restricts their ability to\nlearn from the underlying data distributions. In contrast, our method leverages a more flexible and\nadaptive encoding, allowing the LLM to capture the complexities of molecular structures more ef-\nfectively.\nMoreover, the baseline models utilize significantly larger architectures, such as GPT-4, whereas our\nexperiments are conducted with LLaMA3.1-8B. This disparity in model size and complexity further\ncomplicates direct comparisons, as the performance capabilities and learned representations of these\nmodels can vary widely. Therefore, assessing our results against those achieved by larger, rule-based\nmodels may not provide a meaningful evaluation of performance, given the substantial differences\nin methodologies and model architectures."}, {"title": "4.2 EXPERIMENTAL RESULTS", "content": "Table 1 presents performance comparisons on both the QM9 and ZINC250k datasets against baseline\nmodels. Our approach consistently achieves top-two validity scores across both datasets, demon-\nstrating its effectiveness in enabling the LLM to capture the underlying chemical rules essential\nfor accurate molecule generation. For novelty, our method attains a perfect score of 100% on\nthe ZINC250k dataset and 88% on QM9, highlighting its ability to consistently generate novel\nmolecular structures. In terms of FCD and Scaf metrics-critical indicators of a model's ability\nto explore and replicate chemical space\u2014our method delivers competitive performance compared\nto other baselines. While DiGress and Grum show strong FCD and Scaf scores on the QM9 dataset,\ntheir novelty scores fall significantly short (below 40%), suggesting potential overfitting to the train-\ning data rather than true generalization of molecular distributions. In contrast, our method not only\nmaintains high novelty rates but also achieves strong performance on FCD and Scaf metrics. On\nthe ZINC250k dataset, our approach attains the highest Scaf score and the second-best FCD score,\nfurther demonstrating its superior ability to generalize and innovate within chemical spaces. This\nrobust performance underscores our model's advanced understanding and application of molecular\ndistributions, making it a powerful tool for innovative molecular design in computational chemistry."}, {"title": "4.3 VISUALIZATION RESULTS OF GENERATED MOLECULES", "content": "In Fig. 4, we follow the experimental setup outlined in (Jo et al., 2022), using Tanimoto simi-\nlarity based on Morgan fingerprints to evaluate the generated molecular graphs. For consistency\nand comparability, we select the same molecules as (Jo et al., 2022). Additionally, we perform\nexperiments on molecular graphs generated by Grum (Jo et al., 2023). Across most cases, our\nmethod demonstrates superior performance compared to previous state-of-the-art diffusion-based\napproaches, showcasing its effectiveness and robustness in molecular graph generation."}, {"title": "4.4 ABLATION STUDY: IMPACT OF TREE-STRUCTURED TEXT ENCODING", "content": "To evaluate how our proposed graph-\nto-tree text encoding improves the\nLLM's ability to learn graph struc-\ntures compared to the previous graph-\nto-text methods such as Talk Like a\nGraph (Fatemi et al., 2023), we con-\nducted experiments on the challeng-\ning Zinc250K dataset (Irwin et al., 2012), which contains larger molecules. Talk Like a Graph\nencodes graph structures by converting them into natural language, where each node's connections\nand attributes are described in sentence form. For the fine-tuning process, we randomly selected\n5,000 molecules from the training set and generated 1,000 molecules for performance comparison.\nAs shown in Table 2, our method significantly outperforms the previous approach across all metrics,\ndemonstrating that encoding molecular structures in JSON format enables LLMs to more effectively\nlearn and replicate complex molecular structures."}, {"title": "4.5 ABLATION STUDY: IMPACT OF SUPERVISED FINE-TUNING LLM", "content": "In this study, we aim to evaluate the impact of\nsupervised fine-tuning on LLM performance.\nSpecifically, we generate 1,000 molecules us-\ning the same prompt to compare the perfor-\nmance of the LLM before and after fine-tuning.\nThis direct comparison allows us to assess how\nfine-tuning enhances the model's ability to ac-\ncurately generate molecular structures. We\nconduct this experiment using the ZINC250k\ndataset, and the results are presented in Table 3. The results reveal that without fine-tuning, the LLM\nproduces molecules with only 70.8% validity and 61.12% uniqueness, indicating that the model, in\nits initial state, struggles to fully comprehend and accurately replicate the text representation of"}, {"title": "4.6 ABLATION STUDY: IMPACT OF SIZE OF THE FINE-TUNING DATASET", "content": "In this section, we investigate the impact\nof dataset size on the performance of a\nLLM during fine-tuning. Our experiments\nuse the QM9 dataset with three distinct\ndataset sizes for fine-tuning: 1,000, 5,000,\nand 10,000 molecules. Each model is\ntrained over 10 epochs. This setup enables\na systematic evaluation of how variations\nin fine-tuning data size affect the model's learning efficacy and its ability to generalize. Table 4\npresents the results of these experiments. The results indicate an improvement in the FCD and Scaf\nscores as the dataset size increases. This improvement likely stems from the LLM's exposure to a\nlarger array of data points, which enhances its understanding of the chemical distribution within the\ndataset. Conversely, we observe a decrease in novelty scores with larger datasets. This reduction\nmay be attributed to the relatively small and structurally simple nature of the QM9 dataset, which\ncomprises only four types of atoms and molecules not exceeding nine atoms. As the model en-\ncounters more data, it increasingly reproduces similar outputs, reflecting the limited diversity in the\ndataset."}, {"title": "4.7 ABLATION STUDY: IMPACT OF TOKEN CONSTRAINING", "content": "In this section, we examine the impact of token\nconstraining on molecular generation, as intro-\nduced in Section 3.3. Token constraining is im-\nplemented to guide the LLM toward generating\nvalid molecular structures by restricting its out-\nput to adhere to chemical rules. To evaluate the\neffectiveness of this approach, we perform an\nexperimental comparison using the ZINC250k\ndataset. Specifically, we generate 1,000 molecules to compare the validity of the output with and\nwithout token constraining. The results, presented in Table 5, clearly demonstrate the efficacy of\ntoken constraining in improving the validity of generated molecules. Without token constraining,\nthe validity of the generated molecules is only 41.6%. However, when token constraining is applied,\nvalidity dramatically increases to 98.6%. This significant improvement underscores the critical role\nof token constraining in guiding the LLM to produce valid molecular structures, ensuring closer ad-\nherence to the fundamental rules of chemical structure and leading to a higher rate of valid outputs."}, {"title": "5 CONCLUSION", "content": "In this work, we introduced G2T-LLM, a novel approach for molecular generation that leverages\nLLMs to generate valid molecular structures through a novel graph-to-tree text encoding. By con-\nverting molecular graphs into hierarchical representations inspired by SMILES but adapted for\nLLMs, we bridge the gap between non-linear molecular structures and sequential data processing.\nThis encoding allows the LLM to understand the molecular structure better and produce coherent\nchemical outputs. Our method addresses the challenges of generating valid molecular structures\nby introducing token constraints during the generation process, ensuring that the outputs respect\nsome chemical and structural rules. Through supervised fine-tuning, we further align the LLM with\nmolecular generation tasks, improving its ability to produce chemically valid molecules based on the\nlearned data patterns from benchmark datasets like Zinc250K and QM9. Our results demonstrate\nthe effectiveness of G2T-LLM, achieving state-of-the-art performance on benchmark datasets. This\nwork highlights the potential of utilizing LLMs in molecular design, opening up new avenues for\nAI-driven discoveries in chemistry. The combination of hierarchical encoding, token constraining,"}]}