{"title": "Creating Hierarchical Dispositions of Needs in an Agent", "authors": ["Tofara Moyo"], "abstract": "We present a novel method for learning hierarchical abstractions that prioritize competing objectives, leading to improved global expected rewards. Our approach employs a secondary rewarding agent with multiple scalar outputs, each associated with a distinct level of abstraction. The traditional agent then learns to maximize these outputs in a hierarchical manner, conditioning each level on the maximization of the preceding level. We derive an equation that orders these scalar values and the global reward by priority, inducing a hierarchy of needs that informs goal formation. Experimental results on the Pendulum v1 environment demonstrate superior performance compared to a baseline implementation.We achieved state of the art results.", "sections": [{"title": "I. INTRODUCTION", "content": "The complexity of a policy learned by reinforcement learning (RL) algorithms is inherently bounded by the complexity of the reward function. Consequently, significant efforts have been devoted to crafting intricate reward functions that can guide RL agents towards sophisticated behaviors.In contrast, humans and other animals appear to develop complex behaviors through a hierarchical process, wherein an initially simple reward function focused on fundamental drives such as pain avoidance and pleasure seeking serves as the foundation for a layered structure of dispositions.\nEach level in this hierarchy is oriented towards satisfying the preceding levels, ultimately referencing the base reward function.\nThe mechanisms underlying this process remain unclear. However, if we could induce artificial agents to learn hierarchical reward functions, it would enable the specification of simple base reward functions, allowing the algorithm to autonomously develop complex goals. A hierarchical reward function would confer upon the agent the capacity to pursue intricate objectives. The hierarchical structure of human needs has been extensively studied, yielding frameworks such as Maslow's Hierarchy of Needs. This hierarchy progresses from fundamental, essential needs to more abstract and complex requirements, including self-actualization.\nThis paper presents a novel approach to inducing hierarchical reward structures in artificial agents. Our method involves introducing a secondary rewarding agent that parallels the traditional agent, receiving identical state inputs. The rewarding agent features a continuous action output layer, wherein the outputs serve as signals rather than control inputs.\nWe propose an equation that integrates these signals, yielding a reward signal that is used to reinforce the traditional agent. This framework is designed to elicit a hierarchical organization of needs within the traditional agent, promoting more effective and efficient learning.\nOur approach offers two primary benefits: enhanced stability throughout the training process and improved accuracy in the learned policy."}, {"title": "II. BACKGROUND", "content": "We formulate our model of continuous control reinforcement within the framework of a finite Markov Decision Process (MDP). An MDP is defined by the tuple: M = (S, A, s_0, r) where S denotes the state space A denotes the action space $s_0 \\in S$ denotes the initial state r(s, a) : S\u00d7A \u2192 R denotes the reward function, which assigns a scalar value to each state-action pair.At each time step t, the agent selects an action a_{t+1} according to a policy : S \u2192 A, which can be either stochastic or deterministic.A stochastic policy is defined as a probability distribution over actions given a state \\pi(a | s) : S \u2192 P(A) where P(A) denotes the set of probability distributions over A.The objective of the agent is to maximize its future expected reward: max_\\pi E[\\Sigma_{t=0}^\\infty \\gamma^t r(s_t, a_t)].\nPolicy gradient methods are a type of reinforcement learning algorithm that learns to optimize the policy directly, rather than learning the value function. The policy gradient theorem provides the foundation for policy gradient methods. It states that the gradient of the expected cumulative reward with respect to the policy parameters can be computed as:"}, {"title": "A. Markov decision process", "content": null}, {"title": "B. Policy Gradient Methods", "content": null}, {"title": "III. HIERARCHICAL REWARD FUNCTIONS", "content": "When creating a reward function that fosters a hierarchical structure of dispositions, it is crucial to establish a sensitive relationship between variables. This can be achieved through a sequential approach. The reward function's equation provides insight into this relationship:"}, {"title": "IV. EXPERIMENTS", "content": "This section presents the results of our experimental evaluation of the proposed hierarchical reward function on a continuous control problem from the OpenAI Gym suite: the Pendulum-v1 environment with a low-dimensional state space.\nThe architecture of our experimental setup for the Pendulum-v1 environment consisted of a neural network with a final layer outputting a 1-dimensional real-valued vector. Our implementation of the Proximal Policy Optimization (PPO) algorithm was based on a publicly available GitHub repository.\nFor each environment, we trained five models using different random seeds for a fixed total number of time steps. Following completion of training, each model was evaluated over 100 consecutive episodes to assess its performance.\nThe performance of each model was evaluated using the cumulative reward obtained over the 100 evaluation episodes. This metric provides a comprehensive assessment of the model's ability to maximize the reward function while adapting to the environment's dynamics."}, {"title": "A. Pendulum-v1", "content": "The Pendulum-v0 environment is a well-established continuous control task from the OpenAI Gym suite. The primary objective of this task is to stabilize a pendulum by applying a torque, effectively balancing the pole in an upright position.\nThe Pendulum-v0 environment is characterized by: An unbounded, 3-dimensional observation space A 1-dimensional action space, where actions represent the torque applied to the pendulum Bounded actions within the interval [-2, 2].\nThe agent follows an actor-critic framework. The actor \\pi_{\\theta}(a|s) consists of a neural network made of 3 fully-connected layers of 64 units each, with tanh activation functions. The output layer has 1 linear neuron. The critic V_{\\theta}(s) does not share layers with the actor, but has an equivalent architecture of 3 hidden layers, and one output neuron representing the value function."}, {"title": "B. Reward agent", "content": "The reward agent follows an actor-critic framework. The actor \\pi_{\\theta}(a|s) consists of a neural network made of 5 fully-connected layers of 64 units each, with tanh activation functions. The output layer has 3 linear neuron. This is because we wanted to set up a 3 level heirarchy.The equation we used was of the form of the equation we presented earlier but evolved to include a hierarchy of 3 steps. It took the following form.\nAs you can see the equation is a rewrite of the earlier equation, with a replacement of terms."}, {"title": "V. RESULTS AND DISCUSSION", "content": null}, {"title": "A. Pendulum-v1", "content": "For the Pendulum-v1 environment, we observe that our method learnt faster, with greater stability and higher rewards than the PPO method without our adjustments."}, {"title": "VI. FUTURE WORK", "content": "In our initial approach, we assumed a linear reward function, where scalar values are prioritized and each level is multiplied by the level above, with an additional term. However, this simplistic model may not accurately capture the complexities of real-world systems.\nA more comprehensive approach would involve using a graph to model the reward dynamics of the system. In this framework nodes at the same depth would be summed, rather than multiplied, to capture the cumulative effects of different factors and nodes above would be multiplied to represent the hierarchical relationships between different components.This graph-based approach would enable a more nuanced and accurate representation of the reward function.\nTo implement this graph-based reward modeling, we propose a reward critic architecture that takes the state of the agent as input and outputs a graph representing the reward dynamics. We would then trace each leaf node up to the root, collecting values in an array to form a line-based partial reward function for the traditional agent and sum the rewards over all leaves to obtain the final reward.\nTo further enhance the reward critic, we can modify it to take into account the actions of the traditional agent, in addition to the state. This would enable the reward critic to evaluate both states and actions, providing a more comprehensive assessment of the agent's behavior.By exploring these graph-based reward modeling and reward critic architectures, we can develop more sophisticated and accurate reward functions that capture the complexities of real-world systems."}, {"title": "VII. CONCLUSIONS", "content": "In this study, we conducted a comprehensive evaluation of the effectiveness of hierarchical reward functions in reinforcement learning. Our results demonstrate that agents trained with hierarchical reward functions exhibit faster convergence, improved stability, and higher final rewards compared to agents implementing standard Proximal Policy Optimization (PPO) algorithms.\nA comparative analysis of the performance of agents trained with hierarchical reward functions and standard PPO algorithms reveals significant advantages of the former approach. Specifically: Agents trained with hierarchical reward functions exhibit faster convergence rates, achieving optimal performance in fewer iterations .The stability of agents trained with hierarchical reward functions is improved, with reduced variance in performance across different trials and the final rewards obtained by agents trained with hierarchical reward functions are consistently higher than those achieved by agents implementing standard PPO algorithms.\nOur results suggest that the proposed method of implementing hierarchical reward functions is effective for simple cases. To further establish the scalability and generalization of this approach, we plan to extend our experiments to more complex environments with intricate dynamics.\nWe propose to develop more complex graph-based hierarchical reward functions to capture nuanced relationships between different components. This will enable the creation of more sophisticated reward functions that can effectively guide the learning process in complex environments.\nA key advantage of the proposed hierarchical reward function approach is the potential for component reuse and transfer learning. By fostering the reuse of components learned early on in the development process, we can accelerate the learning process and improve the overall performance of the agent.\nThe proposed hierarchical reward function approach has significant implications for complex goal formation and navigation in real-world environments. By emulating the hierarchy of needs exhibited by humans, we can create agents that are capable of navigating complex environments and achieving sophisticated goals.\nFuture work will focus on extending the proposed hierarchical reward function approach to more complex environments and developing more sophisticated graph-based reward functions."}]}