{"title": "Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models", "authors": ["Artyom Kharinaev", "Viktor Moskvoretskii", "Egor Shvetsov", "Kseniia Studenikina", "Bykov Mikhail", "Evgeny Burnaev"], "abstract": "Large Language Models (LLMs) have emerged as powerful tools for addressing modern challenges and enabling practical applications. However, their computational expense remains a significant barrier to widespread adoption. Quantization has emerged as a promising technique to democratize access and enable low resource device deployment. Despite these advancements, the safety and trustworthiness of quantized models remain underexplored, as prior studies often overlook contemporary architectures and rely on overly simplistic benchmarks and evaluations. To address this gap, we introduce OpenSafetyMini, a novel open-ended safety dataset designed to better distinguish between models. We evaluate 4 state-of-the-art quantization techniques across LLaMA and Mistral models using 4 benchmarks, including human evaluations. Our findings reveal that the optimal quantization method varies for 4-bit precision, while vector quantization techniques deliver the best safety and trustworthiness performance at 2-bit precision, providing foundation for future research.", "sections": [{"title": "Introduction", "content": "The modern state of artificial intelligence (AI) is built on the scaling paradigm, initially focusing on increasing model size (Hoffmann et al., 2022) and later shifting toward test-time compute scaling (Snell et al., 2024; Geiping et al., 2025). These paradigms demand substantial computational resources, particularly for inference involving longer meta-reasoning (Gao et al., 2024). To democratize access to these computationally expensive systems and enable deployment on local devices while reducing costs and improving efficiency, quantization has emerged as a powerful technique, significantly reducing model size (Lin et al., 2024; Ashkboos et al., 2023).\nQuantization maps weight matrices to lower precision, improving computational efficiency by leveraging faster integer arithmetic and reducing memory consumption. However, the performance of quantized models is typically assessed using closed-book benchmarks, with limited evaluation of their safety and trustworthiness. This oversight hinders real-world deployment, as AI systems inherently present safety and reliability challenges, potentially leading to harmful outcomes (Zhang et al., 2023; Ren et al., 2024).\nPrevious studies on the safety evaluation of quantized models primarily focused on older architectures (Li et al., 2024a), quantization techniques (Xu et al., 2024), and bit ranges (Belkhiter et al., 2024), as well as outdated datasets that are insufficiently challenging for modern models (Liu et al., 2024; Yang et al., 2024). Furthermore, existing evaluations rely on either multiple-choice assessments or the LLM-as-a-Judge paradigm (Xu et al., 2024), which may not align well with human judgment (Bavaresco et al., 2024).\nTo address this gap, we introduce a novel challenging dataset OpenSafetyMini, curated with human assessments to enhance specificity in evaluating quantized model performance in open-ended generation. We further demonstrate that the LLM-as-a-Judge approach exhibits high alignment with human judgment.\nFinally, we conduct 24 rigorous evaluations by applying 4 state-of-the-art quantization techniques to 2 modern models across 3 precision ranges. These evaluations span 4 diverse benchmarks, covering both open-ended and multiple-choice assessments of safety and trustworthiness, with additional human assessments to ensure reliability and alignment with real-world judgment. Our findings reveal that quantized models exhibit unsafe behavior when rigorously tested, with 4-bit precision and 2-bit vector quantization delivering the most safe"}, {"title": "Related Work", "content": "Quantization has been widely studied for efficiency gains, but its impact on safety remains an evolving research area. Our work expands on prior studies by introducing new datasets and evaluation methodologies, reflected in Table 1.\nQuantization and Model Robustness. Liu et al. (2024) found that quantizing weights to 3-4 bits generally preserves performance across tasks, but sensitivity varies by dataset, requiring task-specific optimization. Meanwhile, Li et al. (2024b) found no clear link between adversarial robustness and quantization, whereas Belkhiter et al. (2024) observed that quantized models showed increased resistance to complex jailbreaking attempts. Jin et al. (2024) showed that social biases largely remain post-quantization, but truthfulness drops significantly at 2-bit precision using GPTQ. Similarly, Xu et al. (2024) found that extreme quantization introduces unpredictable representational harm, disproportionately affecting protected groups.\nPost-Training Quantization and Safety Most recent efforts focus on post-training quantization (PTQ) due to the computational infeasibility of quantization-aware training (QAT) for large models. Linear uniform quantization remains common but struggles with precision loss. Alternative methods, such as companding and vector quantization, attempt to mitigate these issues by modifying weight distributions or leveraging lookup-based recovery mechanisms (Gray, 1984; Gray and Neuhoff, 1998). Our work evaluates PTQ techniques across these categories, specifically targeting 4-bit and 2-bit weight-only quantization (Li et al., 2024b; Liu et al., 2024; Jin et al., 2024).\nAlignment and Safety Considerations Model alignment strategies like reinforcement learning from human feedback (RLHF) (Ouyang et al., 2024) and direct preference optimization (DPO) (Rafailov et al., 2023) seek to reduce harmful outputs, but quantization may affect alignment properties. Ren et al. (2024) suggest that performance degradation due to quantization correlates with increased safety risks. We investigate this hy-"}, {"title": "OpenSafetyMini: Challenging Safety Dataset", "content": "In this section we describe the OpenSafetyMini, our proposed dataset, which challenges modern models, encompassing harder responses of higher quality\nOne of the previous open-questions benchmarks XSAFETY (Wang et al., 2023a) was consisted of two existing benchmarks and translated them into multiple languages. Our preliminary analysis revealed ambiguities in English-language prompts that introduce noise during model evaluation. For instance, certain questions contained vague phrasing or cultural references that permitted multiple valid interpretations.\nTo address these quality issues, we implemented a two-stage filtration procedure. First, we aimed to estimate question\u2019s deflection score - the likelihood (0-100 scale) that the model would refuse to reply based on ethical considerations.\nThis approach is reasonable since modern LLM undergoed reinforcement learning alignment are capable of verbalized uncertainty estimation - an ability of modern models to estimate their uncertainty towards respodingin verbally, a property known to well reflect ambiguity (Kadavath et al., 2022; Ni et al., 2024b). Therefore, we observe that the model is likely capable of reflecting verbal deflection score.\nWe employed GPT-40 (Hurst et al., 2024) with 10 independent evaluations of each prompt to ensure consistency in resulting score (Wang et al., 2023b). We further removed all questions with average deflection score below 50. The prompt template is reflected in Appendix C.3.\nFinally, we manually reviewed the remaining ones, filtering the questions of low quality. This process yielded OpenSafetyMini - a refined dataset of 1,067 English prompts."}, {"title": "Experimental Procedure", "content": "In this section, we describe the experimental procedure employed to assess the safety of quantized models and the robustness of LLM-as-a-Judge for safety evaluation.\nModels\nTo compare the impact of quantization methods on LLMs, we selected LLaMA 3.1 8B Instruct (Dubey et al., 2024), which has undergone safety alignment using reinforcement learning, and the unaligned"}, {"title": "Quantization Procedures", "content": "We employed four state-of-the-art quantization methods, each representing a distinct quantization technique family and offering optimal quality:\nAWQ: (Lin et al., 2024) This method employs linear quantization with weight scaling, allowing for both 8-bit and 4-bit representations.\nQUIK: (Ashkboos et al., 2023) Similar to AWQ, QUIK uses linear quantization but incorporates non-quantizable salient vectors, also supporting 8-bit and 4-bit formats.\nAQLM: (Egiazarian et al., 2024) This approach utilizes additive vector quantization, focusing on 4-bit and lower (including 2-bit) precision.\nQUIP#: (Chee et al., 2024) This method applies vector quantization combined with a Hadamard Transform to smooth the weights, allowing for 4-bit and lower (including 2-bit) precision.\nDetails and description of each quantization method can be found in Appendix B."}, {"title": "Becnhmarks and Evaluation", "content": "We used four benchmarks combining automated evaluation and human assessment to comprehensively assess the safety and reliability of LLMs and their quantized versions.\nFor a comprehensive overview of these benchmarks, please refer to Appendix A.\nOpen-Ended Safety\nHuman evaluation was conducted by five linguistic experts across four quantization methods. All assessors held bachelor\u2019s degrees in linguistics and had prior experience in the safety evaluation of LLM responses. Each expert was compensated at a rate four times the minimum wage. The inter-rater agreement, measured using Krippendorff\u2019s \u03b1, yielded a score of 0.623. Each expert evaluated 1,067 samples per model, including both quantized and non-quantized variants, see Table 3, resulting in a total of over 20,000 human evaluations, or 4,268 unique labeled question-answer pairs. The presentation order of the models was randomized for each expert. Each model\u2019s response was classified into one of four categories: Unsafe, Safe, Error, or Ambiguous. The latter two categories were introduced because responses that contain errors or are ambiguous cannot be definitively evaluated as safe or unsafe. The descriptions of these categories are provided below:\nError: This category refers to instances where a model produces gibberish or repeats text. In such cases, the response cannot be evaluated as either safe or unsafe and should be disregarded entirely.\nAmbiguous: This indicates that human experts were unable to determine whether the response was safe or unsafe.\nInstructions given to experts provided in Appendix E.\nLLM-as-a-Judge To ensure a comprehensive evaluation of model response safety, we employed the \u201cLLM-as-a-Judge\u201d approach using Gemma 2 27B (Team et al., 2024). Our results demonstrate a high 92% agreement with human judgments, validating the reliability of our evaluation procedure. More details provided in Appendix C.4.\nMultiple-Choice Safety\nWe use SafetyBench (Zhang et al., 2023), a multiple-choice benchmark, to assess a model\u2019s understanding of safety concepts rather than its refusal behavior. Accuracy on a private test set serves as the evaluation metric.\nModels were evaluated on the English version of SafetyBench in a 5-shot setting, following the authors\u2019 provided examples and prompt template. To ensure reliable parsing, we selected the answer with the highest model output logit for each question. For further details, see Appendix C.5.\nTrustworthiness\nWe utilize the factual Question Answering multi-hop dataset HotPotQA (Yang et al., 2018) to evaluate LLM trustworthiness and reliability in mitigating hallucinations. Following the original paper, we assess model performance in a Retrieval-Augmented Generation (RAG) setting, where the model receives three contexts: two distracting and"}, {"title": "Results", "content": "Open-Ended Safety\nIn this section, we discuss the safety of open-ended models using XSafety and OpenSafetyMini dataset, incorporating human evaluations and LLM-as-a-Judge. We show that our dataset is more challenging and better distinguishes quantized models.\nHuman Evaluation\nThe results in Table 3 present human evaluations of safety for LLaMA models. As expected, the Abliterated LLaMA model is the least safe. Notably, QUIK int4 demonstrates strong robustness, with less than a 0.5% drop from the FP16 model, while also producing fewer ambiguous responses and errors. At the same time we observe a lower performance with 2 bit precision for QUIP#, accompanied by a significant increase in errors. This indicates that not only did the number of unsafe responses double, but the overall response quality also deteriorated significantly.\nAutomatic Evaluation\nResults are presented in Table 2 for LLaMA and Mistral models for both XSafety and OpenSafetyMini.\nInt4. At 4-bit precision, QUIP# consistently ranks the lowest, producing the least safe responses across both datasets. While QUIK and AWQ perform similarly on XSafety, their behavior diverges on OpenSafetyMini, with AWQ experiencing a larger drop in safety while QUIK maintains nearly the same quality.\nInterestingly, this trend is completely reversed for Mistral, where AWQ achieves the safest performance, while QUIK ranks as the most unsafe, showing a much greater drop in safety on Open-SafetyMini.\nInt2. For LLaMA at 2-bit precision, we observe the overall stability of vector quantization with AQLM, maintaining safety across both XSafety and OpenSafetyMini, even outperforming some Int4 settings. Meanwhile, QUIP# continues to show a decline in performance relative to Int4.\nFor Mistral, the trend remains generally consistent, though, as with Int4, the performance drop is more pronounced on OpenSafetyMini.\nMistral vs. LLaMA. Our results indicate that LLaMA consistently demonstrates higher safety levels across quantized settings and on OpenSafetyMini. Notably, OpenSafetyMini reveals Mistral\u2019s lower safety even at FP16, highlighting its inherent vulnerabilities.\nWhile Mistral exhibits a more significant safety drop at 2-bit quantization, the trend is less consistent at 4-bit, where AWQ and QUIP# experience a larger decline for LLaMA. We hypothesize that this may be related to QUIK\u2019s approach of preserving salient weights from quantization, which could retain some of the LLaMA safety-related capabilities.\nOpenSafety Mini Advantages For the LLaMA model, the Abliterated variant consistently yields the lowest safety scores across both XSafety and OpenSafetyMini, with a significantly larger drop on OpenSafetyMini. This trend extends across all settings, where models experience a more pro-"}, {"title": "Multiple-Choice Safety", "content": "The results are presented in Table 4, showcasing the performance of various quantized models.\nInt4. Interestingly, Int4 models outperform FP16 models on average for both LLaMA and Mistral. Consistent with open-ended safety evaluations, QUIK demonstrates the strongest performance for LLaMA, while AWQ leads for Mistral, with QUIK being the weakest for Mistral. However, despite its lower overall performance, QUIK for Mistral exhibits notable strengths in Unfairness and Bias, while QUIP# shows strong capabilities in handling Illegal Activities.\nInt2. We observe a significant drop in performance for the LLaMA model under lower precision conditions, while the Mistral model exhibits only a mild decline. Notably, the overall trend remains consistent with open-ended safety benchmarks, where vector quantization methods such as AQLM consistently outperform QUIP#.\nBenchmark Specificity. Overall, we observe differences between quantized and full-precision models on the multi-choice benchmark, which align with the trends identified in open-ended benchmarks. However, we note that this benchmark, while consistent in trends, may lack specificity. Specifically, it fails to distinguish Abliterated LLaMA model from regular FP16. We also observe a significant performance improvement for Abliterated, such as in tasks related to Illegal Activities. This suggests that marginally reducing a model\u2019s safety constraints can enhance its rea-"}, {"title": "Trustworthiness", "content": "We further investigate model trustworthiness, with results detailed in Table 5.\nInt4. For Int4 models, distinctions are less pronounced compared to safety benchmarks. LLaMA shows different trends, with QUIK underperforming in both automated and rule-based metrics, while QUIP# delivers acceptable results. For Mistral, QUIK low performance aligns with safety benchmarks, whereas AWQ dominates, showing minimal decline in In-accuracy and even higher AlignScore results.\nInt2. For Int2 models, LLaMA maintains AQLM stability and higher factuality, while Mistral exhibits a slight decline, with AQLM outperforming QUIP# only in AlignScore, however, the difference in In-accuracy is negligible.\nNotably, the Abliterated version of LLaMA experiences a significant loss in factuality, which is unexpected since uncensuring typically targets safety rather than factual accuracy. This suggests a po-"}, {"title": "Discussion", "content": "In this section we discuss obtained critical insights into the effects of quantization bit-range, methods, safety benchmarks, and model architectures on the safety and trustworthiness of language models. Below, we summarize the key implications:\nQuantization Bit-Range\nInt4 Precision: At 4-bit precision, models generally maintain strong performance, with minimal degradation in safety and trustworthiness. However, the effectiveness of quantization methods varies significantly across models. For instance, QUIK excels for LLaMA but underperforms for Mistral, while AWQ shows robustness for Mistral.\nInt2 Precision: Reducing precision to 2 bits introduces more pronounced performance declines, particularly for LLaMA. Vector quantization methods like AQLM demonstrate greater stability compared to scalar methods like QUIP#, which struggles with maintaining safety and factual accuracy.\nQuantization Methods\nQUIK: Performs well for LLaMA at 4-bit precision, showing minimal safety degradation and strong robustness. However, its performance drops significantly for Mistral, particularly in safety benchmarks.\nAWQ: Consistently delivers strong results for Mistral, outperforming other methods in both safety and trustworthiness. For LLaMA, AWQ shows vulnerabilities at lower precision.\nQUIP#: Generally underperforms compared to QUIK and AWQ, especially at 2-bit precision, where safety and factual accuracy decline significantly.\nAQLM: Demonstrates stability and higher factuality at 2-bit precision, particularly for LLaMA, making it a promising method for low-bit quantization.\nSafety Benchmarks\nOpenSafetyMini vs. XSafety: The Open-SafetyMini dataset proves more challenging and effective in distinguishing between quantization methods and models. It reveals nuanced safety vulnerabilities that XSafety fails to capture, such as the significant safety drop for Mistral at FP16 and the performance gaps between LLaMA and Mistral.\nMulti-Choice Benchmarks: While consistent with open-ended benchmarks, multi-choice benchmarks lack specificity in distinguishing between quantized, full-precision and abliterated models.\nModels\nLLaMA vs. Mistral: LLaMA consistently demonstrates higher safety and trustworthiness across quantization settings, particularly on OpenSafetyMini. Mistral, while competitive at FP16, exhibits more significant safety drops at lower precision.\nAbliterated Models: The Abliterated LLaMA model shows a significant loss in factuality, suggesting a potential link between safety mechanisms and trustworthiness. This unexpected result highlights the need for further investigation into the interplay between safety alignment and factual accuracy."}, {"title": "Conclusion", "content": "In this paper, we focused on evaluating the safety and trustworthiness of quantized models. First, we introduced a challenging open-ended safety dataset, OpenSafetyMini, consisting of 1,067 questions curated with human assessments. Additionally, we collected 21,328 human evaluations of open-ended quantized model safety, demonstrating a high agreement between human evaluators and the LLM-as-a-Judge approach. Finally, we conducted an extensive evaluation across 24 settings on 4 distinct benchmarks, encompassing 2 modern LLMs, 4 state-of-the-art quantization techniques, and 3 bit ranges. Our findings reveal that quantized models exhibit unsafe behavior under rigorous testing, with performance trends differing at 4-bit precision. Notably, vector quantization delivers the safest and most trustworthy results for 2-bit precision."}, {"title": "Limitations", "content": "Our dataset filtering relies on the GPT-estimated deflection score, followed by human validation to remove incorrectly marked"}]}