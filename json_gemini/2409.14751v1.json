{"title": "UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object\nDetection", "authors": ["Haocheng Zhao", "Runwei Guan", "Taoyu Wu", "Ka Lok Man", "Limin Yu", "Yutao Yue"], "abstract": "Abstract-4D millimeter-wave (MMW) radar, which pro-\nvides both height information and dense point cloud data\nover 3D MMW radar, has become increasingly popular in 3D\nobject detection. In recent years, radar-vision fusion models\nhave demonstrated performance close to that of LiDAR-based\nmodels, offering advantages in terms of lower hardware costs\nand better resilience in extreme conditions. However, many\nradar-vision fusion models treat radar as a sparse LiDAR,\nunderutilizing radar-specific information. Additionally, these\nmulti-modal networks are often sensitive to the failure of a\nsingle modality, particularly vision. To address these challenges,\nwe propose the Radar Depth Lift-Splat-Shoot (RDL) module,\nwhich integrates radar-specific data into the depth prediction\nprocess, enhancing the quality of visual Bird's-Eye View (BEV)\nfeatures. We further introduce a Unified Feature Fusion (UFF)\napproach that extracts BEV features across different modal-\nities using shared module. To assess the robustness of multi-\nmodal models, we develop a novel Failure Test (FT) ablation\nexperiment, which simulates vision modality failure by injecting\nGaussian noise. We conduct extensive experiments on the View-\nof-Delft (VoD) and TJ4D datasets. The results demonstrate\nthat our proposed Unified BEVFusion (UniBEVFusion) network\nsignificantly outperforms state-of-the-art models on the TJ4D\ndataset, with improvements of 1.44 in 3D and 1.72 in BEV\nobject detection accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "Millimeter-wave (MMW) radar is widely used in roadside\nand vehicle-mounted transportation applications due to its\nreliable distance and velocity detection capabilities, even\nunder extreme weather conditions [1], [2], [3]. However,\nthe sparse nature of radar point cloud data and the lack\nof height information have posed challenges for accurate\n3D object detection [4]. With recent advancements in 4D\nMMW radar technology, there is growing interest in utilizing\nthis radar for 3D object detection, either as a standalone\nradar modality or fused with cameras [5], [6]. Radar-vision\nfusion has been shown to reduce hardware costs, enhance\nperformance in extreme conditions, and maintain reasonable\n3D object detection accuracy [7].\nIn vision-based 3D object detection, a widely adopted\napproach is to project 2D image features into a Bird's-Eye\nView (BEV) using intrinsic and extrinsic camera parameters\nalong with accurate depth prediction [8], [1], [2]. BEVFusion\n[9], a well-known LiDAR-Vision fusion model, provides an\nefficient architecture for fusing multi-modal data, improving\nupon methods like Lift-Splat-Shoot (LSS) [8] and pool-\ning through optimizations and parallelization. Additionally,\nBEVFusion uses point cloud coordinates to assist with depth\nprediction, which is crucial for maintaining stability and\naccuracy in the model. Our reproduction shows competi-\ntive results in the radar-vision datasets, and our proposed\nUniBEVFusion network further improves the design.\nHowever, in recent researches, radar has often been treated\nas a sparse LiDAR [10], and its specific characteristics are\nunderutilized. A recent reproduction [7] of BEVFusion in\nradar-vision performs even similar to the results of pure radar\ndetection. We argue that radar data should be fully leveraged\nin fusion models, and radar-specific information should be\nintegrated into the depth prediction process to improve over-\nall model performance. To address this, we propose Radar\nDepth LSS (RDL), which incorporates additional radar data,\nsuch as Radar Cross-Section (RCS), into the depth prediction\nprocess to enhance detection accuracy.\nMoreover, multi-modal networks are particularly vulnera-\nble to the failure of a single modality [11], [4], especially\nvisual data. These networks often rely heavily on existence\nof both radar and image inputs, and their performance can\ndegrade significantly when one modality is damaged or in\nadverse environment [12], [13]. To evaluate the robustness\nof multi-modal models in such cases, we propose a novel\nablation experiment called the Failure Test (FT), in which\nsubstantial noise is added to the visual input to simulate\nvisual failure. As shown in our experiments, applying FT to\nBEVFusion results in a dramatic drop in performance, even\nbelow that of single-modal networks. To address this issue,\nwe developed a novel multi-modal fusion module, Unified\nFeature Fusion (UFF), which unifies feature extraction and\nenhances features across different modalities to mitigate the\nimpact of failure.\nThe contribution points of this paper are summarized as:\nWe propose the Radar Depth LSS (RDL) module, which\nintegrates radar-specific information into the depth pre-\ndiction process to improve the vision BEV feature\ntransformation.\nWe propose the novel fusion module Unified Feature\nFusion (UFF) to extract features from different modal-\nities and fuse them together.\nWe propose the novel Failure Test (FT) ablation exper-\niment for multi-modal fusion in the case of near-failure"}, {"title": "II. RELATED WORKS", "content": "A. LiDAR Point Cloud 3D Object Detection\nPoint cloud-based 3D object detection has evolved signif-\nicantly with point-based, projection-based, and voxel-based\nmethods [3]. PointNet [14] and PointNet++ [14] capture\nglobal spatial information from raw point clouds but are\ncomputationally intensive due to their two-stage structure.\nProjection-based methods reduce computation cost by pro-\njecting point clouds into three 2D feature map [15], [16].\nVoxel-based methods convert irregular point clouds into a\nregular voxel grid, reducing computational costs without\nsacrificing spatial feature resolution [17]. Building on voxel\ngrids, PointPillars further optimize the computation by using\npillars-based instead of voxels [18].\nPoint clouds provide accurate depth information, while im-\nages offer rich semantic information [1], [2], [19]. Aligning\nthese modalities is fundamental to fusion networks. Camera\ndata can be projected into the 3D coordinate system using\nintrinsic and extrinsic parameters, facilitating fusion with\nLiDAR point clouds [1], [2], [19]. To balance speed and\nperformance, a common approach is to project both image\nand point cloud features into the bird's-eye view (BEV)\ncoordinate system. BEVFusion [9] optimized the Lift-Splat-\nShoot (LSS) pipeline and added point cloud projections to\nthe camera coordinate system to aid in depth prediction\n[8]. Our proposed UniBEVFusion builds upon BEVFusion,\noptimizing radar feature integration for radar-vision fusion.\nB. Radar Point Cloud 3D Object Detection\nWith the development of 4D millimeter-wave radar and the\navailability of open datasets, more researchers have explored\nradar-based object detection. Early work treated radar point\nclouds as a sparse LiDAR-like data [10], applying LiDAR\nobject detection model such as PP-Radar [20], reproduced\nBEVFusion [7]. Although promising, a significant gap re-\nmains between radar and LiDAR performance. Utilizing\nradar velocity [6], radar coordination [21], novel network\nmodules [22], [23], [10], [24], [25], LiDAR distillation [26],\nadding gate [27], [26] and semantic alignment [28] can\nimprove the performance of radar-based object detection.\nIn this paper, we focus on radar-vision feature fusion,\nwhich has shown promising results in recent studies. RADI-\nANT [29] proposed a multi-stage fusion, including feature\nand detection head. FUTR3D [30] propose a modality-\nagnostic feature sampler to fuse radar, lidar, and cam-\nera. RCBEVDet [31] proposed an multi-head query-based\nmethod and a RCS-aware encoder that aligns BEV features\nusing radar-specific information. RCFusion [5] generates\npseudo-images from radar data and improves model per-\nformance with orthogonal feature transformations. LXL [7]\nenhances depth feature fusion by integrating radar and visual\nvoxel features, achieving State-of-The-Art (SOTA) results on\nmultiple radar-vision datasets. Our proposed UniBEVFusion\nwill comparison the performance with these SOTA networks\non the VoD [20] and TJ4D [32] datasets."}, {"title": "III. METHODOLOGY", "content": "A. Overview\nFig.1 shows the overall architecture of our proposed\nUniBEVFusion network, which contains four main parts:\nImage, Radar, Fusion, and BEV. Image and radar stream are\nresponsible for extracting BEV features from the image and\nradar, respectively. The fusion stage handle the fusion of the\nBEV feature from the image and radar stream. The BEV\nstage is responsible for the final BEV feature extraction and\n3D object detection head.\nBesides, the image encoder in image stream is a pre-\ntrained swinTransformer [33], which is used to extract fea-\ntures from the image. The radar encoder in radar stream\nand BEV stream is basically similar to PointPillar from the\nbaseline of View-of-Delft (VoD) [20], which use PillarFea-\ntureNet, SECOND, and SECONDFPN [34]. The 3D object\ndetection head is a common 3D object detection head, which\nis used to predict the 3D bounding box and classification\nresults.\nB. Radar Depth Lift-Splat-Shoot (RDL)\nLSS is an important milestone in visual-based 3D object\ndetection [8], but relies on correct depth prediction and\ncomputation is inefficient. BEVFusion [9] provides a better\noptimzied LSS module, which gives projected point cloud\nas initial value of depth. Therefore, we inherit the View\nTransform module of BEVFusion and our RDL is based on\nthis design.\nAs shown in the Fig.2, we first extract the coordination\nand RCS information, and then concat them to the depth\nprediction module. In fact, at this stage, we performes a\nearly fusion of radar data and visual features. The extra\ninformation of point cloud data on VoD [20], TJ4D [32],\nand common LiDAR are shown in Table I.\nRCS is a key feature of radar data, which is related\nto the size, shape, and material of the object [4]. RDL\nreflects the physical characteristics of the objects in the depth\nprediction and retains this information in the later BEV\nfeatures. The transform module is used to transform the radar\ndepth features input channel number from N+1 to 64, where\nN and 1 are the number of extra information channels (e.g.,\nRCS, velocity) and depth information, respectively.\nC. Unified Feature Fusion (UFF)\nThe UFF module, shown in Fig.3, is specifically de-\nsigned to improve the reliability of multi-modal fusion by\naddressing the inherent differences between different sen-\nsor modalities. It consists of several key components: the\nChannel Unifier, the Shared Feature Encoder, the Softmax\nConcatenation Fusion, and the Fused Feature Encoder. The\nChannel Unifier aligns the feature dimensions of different\nmodalities using 1x1 convolutions, ensuring a consistent\nchannel representation across modalities. This not only sim-\nplifies the fusion process, but also enables more effective\nextraction of cross-modal features.\nThe Shared Feature Encoder plays a critical role in\nthe normalization of feature representations from different\nmodalities, mitigating discrepancies that may be due to\nmodality-specific characteristics. Thus, it helps reduce per-\nformance degradation when a modality fails or provides\nsuboptimal data. The softmax concatenation fusion integrates\nthese processed features, while the use of softmax weighting\nallows the network to emphasize the most salient information\nacross modalities, improving the overall quality of the feature\nfusion.\nBoth the Shared Feature Encoder and the Fused Feature\nEncoder are implemented as residual blocks, which facilitates\ndeeper feature learning and promotes gradient flow during\ntraining. In addition to increasing the robustness of the fusion\nprocess, this architecture ensures that the fused features\npreserve essential information from each modality.\nD. Failure Test (FT)\nIn order to rigorously evaluate the robustness of the model\nunder conditions where the vision modality fails, we propose\na vision failure test. In contrast to the multi-view approach\nused in the CRN [21], where robustness is evaluated for\nmultiple views, we introduce Gaussian noise directly into\nthe single-view image data sets to simulate the degradation\nof the visual input. This allows us to observe how detection\nperformance changes with increasing noise level. The noisy\nimage $I'$ in FT, is defined as\n$I' = I + \\rho \\cdot N(0, \\sigma^2),$ (1)\nwhere $\\rho$ is the noise level, $I$ is the original clean image,\n$I'$ is the noise corrupted image, and $N(0,\\sigma^2)$ is Gaussian\nnoise with mean 0 and variance $\\sigma^2$. By systematically\nvarying $\\rho$, we evaluate the performance of the model under\ndifferent noise intensities. As presented in section IV-D,\nboth BEVFusion and our proposed UniBEVFusion show\nsensitivity to noise in the visual modality, highlighting the\nimpact of modality-specific degradation on overall model\nperformance. This analysis underscores the importance of"}, {"title": "IV. EXPERIMENTS", "content": "We first give the brief introduction to the datasets used\nin the experiments in Section IV-A. Then we compare the\nperformance of different models on VoD [20] and TJ4D [32]\nin Section IV-B and Section IV-C, respectively. The results of\nFT and the ablation study is shown in Section IV-D. Lastly,\nwe test the performance of different image resolutions in\nSection IV-E.\nA. Datasets\nThe datasets used in this paper, VoD [20] and TJ4D [32],\nboth provide 4D MMW radar data. Radar point clouds in\nVoD includes [x, y, z, RCS, $V_r$, $V_t$, t], while TJ4D includes\n[x, y, z, R, RCS, $\\alpha$, $\\beta$], where x, y, z represent coordinates,\nRCS is radar signal strength, $V_r$ and $V_t$ are relative and\nabsolute velocities, R is distance, and $\\alpha$, $\\beta$ are angles.\nThe VoD dataset includes categories for car, pedestrian,\nand cyclist, while TJ4D adds trucks. We followed the offi-\ncial method, segmenting VoD's 6435 frames into 5139 for\ntraining and 1296 for validation, and TJ4D's 7757 frames\ninto 5717 for training and 2040 for validation. In this paper,\nour experimental camera resolutions are resized to [608, 968]\nfor VoD and [480, 640] for TJ4D.\nFor evaluation, we used Mean Average Precision (mAP)\nwith IoU thresholds of 0.5 for cars/trucks and 0.25 for\npedestrians/bicycles. VoD's official evaluation includes RoI\n3D detection within [-4 < x \u2264 4m, z < 25m], while TJ4D\nevaluates 3D and BEV detection across all ranges.\nB. Results on VoD\nTable II shows the performance of our model on the vali-\ndation set of VoD [20], where the mAP is slightly lower than\nthat of the LXL fusion network in the Entire Annotation Area\n(EAA). LXL achieves State-of-the-Art (SOTA) performance\nacross the multi-modal radar datasets. However, in the more\ncritical Driving Corridor Area (DCA), which is constrained\nby distance, UniBEVFusion outperforms LXL. While our\nmodel performs slightly worse than LXL in the detection of\ncars and pedestrians, it significantly outperforms LXL in the\ndetection of cyclists. Overall, UniBEVFusion shows superior\nperformance in the DCA, which is crucial for autonomous\ndriving tasks, and maintains competitive results in the EAA,\nwhere it outperforms the other algorithms.\nFurthermore, it is noteworthy that our reproduced BEVFu-\nsion outperforms previously reported results [7]. By modify-\ning the detection head and radar PillarFeatureNet to align\nwith UniBEVFusion, we have achieved a higher level of\nperformance. This improved BEVFusion serves as a robust\nbaseline for evaluating the effectiveness of our proposed\nUniBEVFusion network.\nResults in Fig.4 validate the performance of UniBEVFu-\nsion compared to Ground Truth (GT) and BEVFusion [9].\nThe right section of the figure shows the fused BEV features,\nwhere UniBEVFusion covers a larger area than BEVFusion,\nthough with lower overall feature magnitudes due to the\nSoftmax layer in the UFF module. Despite this, the features\nin key regions remain strong, and the UFF module effectively\nextracts features from different modalities, providing broader\ncontext and more stable fused features for object detection.\nUniBEVFusion demonstrates superior performance in han-\ndling occlusions (Fig.4 A, C, E, F), where its larger feature\nfield allows it to detect occluded objects more reliably,\nreducing the likelihood of dismissing them as noise. In shad-\nowed and partially occluded scenarios (Fig.4 B, C), where\nvision alone struggles, UniBEVFusion accurately identifies\nthe target using radar-specific information from the RDL\nmodule. Additionally, in close-range detection (Fig.4 D),\nUniBEVFusion succeeds where BEVFusion fails, likely due\nto the latter's lack of sufficient contextual information in the\nfused BEV feature. Overall, UniBEVFusion performs better\nin occlusion, shadow, and both short- and long-range detec-\ntion, with the UFF and RDL modules enhancing performance\nin various scenarios.\nC. Results on TJ4D\nCompared to the VoD dataset's point cloud range [[0,\n51.2], [-25.6, 25.6], [-3, 2]] [20], the TJ4D dataset covers\na significantly larger range [[0, 69.12], [-39.68, 39.68],\n[-4, 2]] [32], which introduces additional complexity for\n3D object detection. Despite this increased difficulty, the\nperformance of UniBEVFusion on TJ4D, as shown in Table\nIII, is consistent with its results on VoD, and it even surpasses\nthe validation outcomes of the LXL algorithm [7].\nUniBEVFusion achieves improvements of 1.44 and 1.72\nover LXL in 3D object detection and BEV accuracy, re-\nspectively. Notably, in the Car detection task, it outperforms\nRCFusion [5] by 5.54 in 3D detection and by 9.37 in BEV\ndetection. These results highlight the effectiveness of the\nRDL and UFF modules, which significantly enhance the"}, {"title": "D. Failure Test", "content": "Based on the previous introduction, we evaluate BEVFu-\nsion [9] and our proposed UniBEVFusion model using $\\rho$ = [0.5,0.7,0.9]. Since the design of the noise is related to the\nrandom numbers, the average of 10 operations was taken for\nall the test results. Results in Table IV shows the performance\nof baseline FT0, and evaluations FT0.5, FT0.7, and FT0.9. As\nthe baseline FTo is the normal evaluation results of these\nmodel, thus, we will also discuss the effectiveness of the\nRDL and UFF in this section.\nFor BEVFusion model in TJ4D FT evaluation, adding\nRDL improves much in baseline performance, but the FT\nresults are close. RDL is designed for accurate depth pre-\ndiction in image stream, and it can not guarantee the robust\nresults when image is failure. Adding UFF improves both the\nbaseline and FT performance, which indicates that the UFF\nis effective in improving the robustness of the model. As for\nthe UniBEVFusion model, the conclusion is similar to the\nBEVFusion model, and the overall FT results are better than\nBEVFusion.\nIn VoD FT evaluation, conclusion are different in two\ndifferent evaluation range. For entire annotation area, the\nresults are close, and we can not tell the effectiveness of the\nRDL on this dataset. The UFF bring much leading than the\nBEVFusion model, which indicates that the UFF is effective\nin improving the robustness of the model. However, for the\ndriving corridor area, the basic conclusion are similar to\nthe TJ4D. Moreover, with the noise level $\\rho$ increasing, the\nperformance gap between the two models is getting smaller\nfor our UniBEVFusion in all evaluation.\nOn top of the results, we can conclude that the UFF and\nRDL are effective in improving the performance of multi-\nmodal model. Besides, UFF provides a better robustness in\nthe case of vision failure."}, {"title": "E. Image Resolution", "content": "Evaluating scaling from 0.25 to 0.75 shows a consistent\ntrend with RCFusion, where smaller scales result in missing\ninformation and reduced performance. Interestingly, full-size\nimages performed worse than 0.5 and 0.75 due to the model\nbeing tuned for 0.5 and overfitting on detailed images. The\n0.25 scale yielded the worst results due to excessive detail\nloss and sparse features after BEV transformation. Despite\nthe slightly better performance at 0.75, we opted for 0.50\nscaling for operational speed.\nMoreover, comparing the effectiveness of our proposed\nRDL, the results of 0.50 1 outperform the original BEVFu-\nsion [9] by at least 5.84% and 3.88% in 3D and BEV, respec-\ntively. However, the performance at 0.25 is reduced by 14.6%\nand 2.1%, respectively. In the absence of image information,\nimage features and coordinate information are misaligned.\nRCS information representing the shape, material, and size\nof the object is also incorrectly added to features, resulting\nin learning wrong features and worse performance."}, {"title": "V. CONCLUSION", "content": "In this paper, we demonstrated that the UniBEVFusion\nnetwork achieves state-of-the-art performance on the TJ4D\n32 and driving corridor of the View-of-Delft (VoD) datasets\n[20]. The results indicate that UniBEVFusion significantly\nimproves detection performance, particularly in challenging\nconditions such as shadows, occlusions, short-range, and\nlong-range scenarios. Our proposed Radar Depth Lift-Splat-\nShoot (RDL) module and Unified Feature Fusion (UFF)\nframework are effective in enhancing the model's perfor-\nmance. Specifically, RDL integrates radar depth and RCS\ninformation into the depth prediction process, boosting the\naccuracy of vision-based 3D object detection. UFF mitigates\nthe model's reliance on the simultaneous availability of\nmultiple modalities, improving its robustness against single-\nmodality failures. Although Gaussian noise was the only\nsimulation solution used in the Failure Test (FT), it still\nprovided valuable insights into the model's robustness. In\nfuture work, we plan to further optimize UFF and RDL to\nimprove the performance of multi-modal models in scenarios\nwhere one modality fails. In addition, we will incorporate\nmore diverse failure modes into the FT and develop more\nprecise evaluation metrics to better assess robustness."}]}