{"title": "Evaluating Sample Utility for Data Selection by Mimicking Model Weights", "authors": ["Tzu-Heng Huang", "Manjot Bilkhu", "Frederic Sala", "Javier Movellan"], "abstract": "Foundation models rely on large-scale web-crawled datasets, which frequently contain noisy data, biases, and irrelevant content. Existing data selection techniques typically use human heuristics, downstream evaluation datasets, or specialized scoring models, and can overlook samples' utility in the training process. Instead, we propose a new approach, Mimic Score, a data quality metric that uses a pretrained reference model as a guide to assess the usefulness of data samples for training a new model. It relies on the alignment between the gradient of the new model parameters and the vector pointing toward the reference model in weight space. Samples that misalign with this direction are considered low-value and can be filtered out. Motivated by the Mimic score, we develop Grad-Mimic, a data selection framework that identifies and prioritizes useful samples, automating the selection process to create effective filters. Empirically, using Mimic scores to guide model training results in consistent performance gains across six image datasets and enhances the performance of CLIP models. Moreover, Mimic scores and their associated filters improve upon existing filtering methods and offer accurate estimation of dataset quality.", "sections": [{"title": "1 Introduction", "content": "Large-scale web-crawled datasets are fundamental to the success of foundation models, e.g., MM1.5 [1], OpenAI CLIP [2], LLaMa-3.1 [3], and GPT-4 [4]. These datasets provide vast quantities of information but also carry noise, biases, and irrelevant content from their web sources. To mitigate these, data selection\u2014ruling out undesirable samples\u2014has emerged as a critical step in the model development pipeline [5, 6]. For example, the FineWeb dataset [7], containing 15 trillion tokens used for training large language models, is created through eight carefully designed filtering steps meant to refine raw web content, ensuring high model performance.\nWhile effective, such filtering recipes face several limitations. The choices of filtering steps and rules often rely on handcrafted heuristics and require domain expertise and expensive experimentation. Such filters often fail to provide fine-grained insights into individual samples and overlook samples' utility in training. These lead to coarse selection and suboptimal model performance.\nExisting techniques often suggest selecting samples based on semantic similarity to downstream evaluation datasets [8, 9], using specialized filtering networks [10, 11, 12, 13], or training influence models to score samples [14, 15]. Yet, these approaches require access to additional datasets or involve specialized model training, adding dependencies and complexity.\nWe introduce the Mimic Score, a new data quality metric that assesses sample contribution in the weight updates using a pretrained reference model. We show that by leveraging a model located in a more optimal part of the weight space, we bypass the need for downstream datasets or specialized training, and can use it as a selection guide. Our approach is based on the alignment between each sample's gradient and the direction towards reference model in weight space."}, {"title": "2 Related Work", "content": "Data Selection. Data selection techniques can generally be categorized into group- and sample-level approaches. Group-level approaches focus on optimizing the mixture of data domains to curate high-quality datasets [18, 19, 20, 21]. Sample-level methods, which are the focus of Grad-Mimic, aim to filter out individual noisy and unhelpful samples. Prior research has explored various strategies, including identifying impactful samples through gradient magnitudes [22], analyzing gradient similarities across batches [23], and selecting key samples that can capture full training update [24, 25]. These methods often rely on batch-level gradients, which can struggle with large amounts of noise in a mini-batch. Grad-Mimic addresses this challenge with the help of a reference model, acting as a reliable guide to select samples.\nData Curation for Multimodal Models. Multimodal models are typically trained on large-scale web datasets [26, 27, 16, 28], which are often noisy and require careful curation. Previous approaches have included selecting samples based on their semantic similarity to the downstream evaluation datasets [8, 9], using semantic deduplication [29], developing specialized filtering networks [10, 11, 12, 13], or training influence models [14, 15]. While effective, they require access to target datasets or introduce additional training complexities. Grad-Mimic overcomes these limitations and offers a more efficient alternative: using pretrained weights to identify useful samples.\nWeak Supervision. Weak supervision is an emerging paradigm for constructing labeled datasets by combining multiple noisy label estimates [30, 31, 32, 33, 34]. These estimates typically come from sources such as heuristic labeling rules, domain knowledge, or pretrained models [35, 36], often encoded as labeling functions. The outputs of these labeling functions are modeled to assess their reliability and then aggregated to produce final labels [30]. Weak supervision has demonstrated success in various domains, including segmentation tasks [37], relation extraction [38], MRI sequence analysis [39], sensor data [40], chemical reaction extraction [41], and enhancing predictions from pretrained models [42, 43]. Unlike most existing works focusing on label aggregation for data annotation, Grad-Mimic uses weak supervision for filtering purposes. We mitigate assessment noise across training steps, aggregating filter outputs into an effective ensemble filter."}, {"title": "3 Evaluating Sample Utility", "content": "We quantify samples by their contributions to the learning process. Our principle is that samples that potentially pull the model in undesirable directions, thereby misdirecting weight updates, should be considered low-value.\nWe start with setup and notation, then explain how our scoring metric is derived and used in our data selection framework, Grad-Mimic.\nNotation. Let $D = \\{$s_i\\}_{i=1}^n$ denote a dataset of n samples drawn from a distribution D supported on the space S. At training step t, model parameters $\\theta_t$, are iteratively optimized using the dataset D. While our framework supports various training settings, we focus on supervised learning for clarity. We assume $S = X \\times Y$, where X is the input space and Y is the label space. Each sample $s_i$ can be expressed as $(x_i, Y_i)$, where noise may be"}, {"title": "3.1 Mimic Score Calculation", "content": "To evaluate whether a sample steers the model in an undesirable direction, we use a pretrained model as a reference. This reference model, denoted by $\\theta_{ref}$, resides in a more optima part of the weight space, such that $l(\\theta_{ref}) < l(\\theta_t)$.\nWe use the vector from the current model's weight space $\\theta_t$ to $\\theta_{ref}$ to measure each sample's utility in approximating a better weight configuration. These reference weights can be layer-specific, e.g., model weights in the last layer, which usually store more informative features [44, 45].\nThe reference model can be obtained in two ways: either by training it on the dataset to achieve a target performance, if resources permit, or more efficiently, by using publicly available pretrained models, which eliminate the need for data access and expensive training.\nThe vector pointing toward the reference model, at training step t, is represented by $v_t := \\theta_{ref} - \\theta_t$. We examine how each sample's negative gradient $-g_{i,t}$, intended for updating model weights, aligns with vector $v_t$. We measure the alignment degree by considering both the direction and magnitude of the negative gradient. Specifically, we compute the projection length of $-g_{i,t}$ onto $v_t$, yielding an alignment score $m_{i,t}$, computed as follows\n$\\displaystyle mimic\\_score(s_{i,t}) := m_{i,t} = \\frac{(-g_{i,t}, v_t)}{|| v_t ||^2}$    (1)\nThis alignment score, named the mimic score, reflects how much a sample can drive the model closer to the reference model. A sample having a lower mimic score suggests it has limited utility in guiding model updates, making it a potential candidate for exclusion from future training."}, {"title": "4 Grad-Mimic: Data Selection Framework", "content": "The mimic score represents each sample's usefulness in guiding the model in a better direction. Building on this metric, we propose Grad-Mimic, a two-stage data selection framework that first prioritizes samples to learn and then identifies useful samples to design an effective filter. A complete Grad-Mimic workflow is illustrated in Fig. 2."}, {"title": "4.1 Stage 1: Reweighted Gradient Updates", "content": "We first use mimic scores to aid model training by re-weighting sample gradients. Unlike standard gradient descent, which assigns equal weight to all the samples in the mini-batch, Grad-Mimic uses mimic scores to amplify helpful samples and down-weight unhelpful ones.\nTo achieve this, each sample's mimic score is first normalized using the softmax function with a temperature parameter $\\tau$. The normalized score for a sample $s_i$ in a batch of size b is computed as\n$\\displaystyle m_{i,t} = \\frac{e^{-m_{i,t}/\\tau}}{\\Sigma_{j=1}^b e^{-m_{j,t}/\\tau}}$     (2)\nThen, the weight update step in Grad-Mimic is modified as\n$\\displaystyle \\theta_{t+1} := \\theta_t - \\eta \\Sigma_{i=1}^b m_{i,t} g_{i,t}$.   (3)\nThe temperature $\\tau$ controls the sensitivity of sample reweighting, allowing us to adjust how sharply the model prioritizes samples. A lower temperature results in a more aggressive focus on learning the most aligned samples, while a higher temperature encourages Grad-Mimic converges to standard gradient descent."}, {"title": "4.2 Stage 2: Automated Filter Design", "content": "The second stage of Grad-Mimic uses computed mimic scores to identify valuable samples, automate data selection based on these assessments, and estimate dataset quality."}, {"title": "4.2.1 Sample Identification", "content": "Grad-Mimic first gathers normalized mimic scores for each sample at every training step. These scores indicate the sample contributions over training, which allows us to decide whether to retain or discard a sample. Grad-Mimic supports several sample identification methods:\n\u2022 Threshold-based Selection: A sample is chosen if its mimic score exceeds a defined threshold. This threshold could be set as 1/b (indicating greater than uniform weight).\n\u2022 1D Clustering: Samples are categorized into two groups using clustering techniques such as k-means clustering [46] or Gaussian Mixture Models (GMMs), allowing more automated selection based on their assigned cluster.\n\u2022 Top-k Percent Selection: Samples are ranked by mimic score, and the top-k percent are chosen. The value of k can be adjusted based on the available training budget.\nAfter identification, we binarize mimic scores and categorize samples into \"retain\" or \"discard\" at each training step. We treat these assigned groups as filter outputs and then combine them for a final filter."}, {"title": "4.2.2 Filter Output Aggregation", "content": "While filtering at an individual training step can refine the dataset, relying on isolated filters risks overlooking training dynamics. Additionally, considering sample utility across different stages may yield complementary signals.\nTo address this, Grad-Mimic leverages weak supervision techniques [30, 31, 32, 33, 34], which are usually used for constructing datasets from noisy signals, to combine filter outputs across training steps. We begin with learning a model to evaluate the reliability of each step assessment [32]. Once established, this model aggregates filtering decisions into a high-quality ensemble filter. Through aggregation, Grad-Mimic mitigates assessment noise and captures training dynamics. Grad-Mimic employs the Snorkel framework [31], a widely used method in the weak supervision community.\nUltimately, the ensemble filter selects high-value samples for a refined dataset. We can estimate dataset quality by analyzing the proportion of retained samples or using samples' mimic scores as an overall metric. We summarize steps in Grad-Mimic in Algorithm 1 placed in Appendix B."}, {"title": "5 Experiments", "content": "We assess the effectiveness of mimic score within Grad-Mimic using two experimental setups across datasets of varying scales and domains. We first test in a controllable setting, adding noise into sample labels (Sec. 5.1), followed by evaluations on large-scale web-crawled datasets (Sec. 5.2). Our goals are to validate the following claims in both setups:\n\u2022 Enhanced Model Performance: The reference model acts as a reliable guide. Training a new model with samples prioritized by mimic scores enhances performance.\n\u2022 Accurate Sample Identification: Grad-Mimic effectively detects noisy samples, automates data selection, and complements existing filtering methods.\n\u2022 Dataset Quality Assessment: Mimic scores and their associated filters offer reliable estimates of dataset quality, highly correlated with noise levels and performance gains."}, {"title": "5.1 Simulating Mislabeled Samples", "content": "Setups. We begin with a controlled experiment by adding various levels of label noise to six image classification datasets. They are DTD [47], Flowers102 [48], STL10 [49], OxfordIIIT Pet [50], CIFAR10 [51], and CIFAR100 [51]. We fine-tune ViT-B/16 models [52] on these noisy datasets under two configurations: linear probing, where only the final layer is tuned, and full fine-tuning, where gradients of all model parameters are reweighted based on mimic scores to maximize the impact of useful samples. We normalize mimic scores with a temperature of 0.5 and use a batch size of 32. We simulate pretrained reference models by training ViT-B/16 models on the noise-free version of each dataset and use the last layer weights as the reference to navigate training on the noisy datasets. We detail more training configurations in Appendix C.1.\nAfter training, we identify samples in two ways: setting one over batch size (1/b) as our threshold and clustering methods using k-means and GMM, then aggregate filter outputs across training steps using Snorkel framework [31]."}, {"title": "5.2 Data Selection in Large-scale Web Datasets", "content": "Setups. We evaluate Grad-Mimic in a more challenging setting using million-scale web-crawled datasets. We use the small- and medium-scale DataComp datasets [16], which contain approximately 10 million and 100 million image-caption pairs, respectively, for CLIP model pretraining [2]. We follow the training setup from DataComp [53, 16] and use publicly available pretrained CLIP weights as our reference. This pretrained model,"}, {"title": "6 Conclusion", "content": "We introduce a new data quality metric called Mimic Score, designed to assess samples' utility in the training process. We quantify samples by measuring the alignment of per-sample gradient and the vector toward a pretrained reference model. Building on Mimic Score, we develop Grad-Mimic to automate data selection process. Empirically, we demonstrate that Grad-Mimic effectively prioritizes identified samples to learn, resulting in enhanced model performance. Moreover, Mimic Score can serve as a reliable metric for creating effective filters and estimating dataset quality. Developed filters outperform human-designed ones and exhibit a strong correlation with performance gains."}, {"title": "A.1 Broader Impacts and Limitations.", "content": "The importance of data selection has grown in building foundation models. We demonstrate that Grad-Mimic can effectively identify and exclude noisy or unhelpful samples. This leads to reduced computational and resource costs.\nBesides, Grad-Mimic provides the second benefit by enabling models to focus their training on high-value samples, which contributes to performance improvements.\nWe do not foresee explicit negative impacts arising from Grad-Mimic. However, if the reference model used for sample selection is unreliable, there remains a risk of refining suboptimal datasets. To address this, incorporating human expert-designed filtering rules may be necessary to ensure the quality of curated datasets."}, {"title": "B Grad-Mimic Algorithm", "content": "We summarize the algorithm steps in Grad-Mimic for computing mimic scores and updating model weights. Our algorithm supports various training settings, such as supervised learning and self-supervised learning. We have validated both training scenarios through simulation experiments (Sec. 5.1) and by training on web-crawled datasets with CLIP models (Sec.5.2). Moreover, these experiments evaluate Grad-Mimic performance across three different training configurations: full-parameter fine-tuning, linear probing, and training models from scratch."}, {"title": "C Experimental Details", "content": "We provide more details about our training setups, and computational resources."}, {"title": "C.1 Simulation Experiments", "content": "In the experiments of simulating mislabeled samples (Sec. 5.1), we fine-tune ViT-B/16 models pretrained on the ImageNet-21k dataset [55]. Each dataset is trained with a batch size of 32, a learning rate of 1e-4, the AdamW optimizer, and for 5 epochs. We evaluate Grad-Mimic using various temperature values $\\tau$ (1.0, 0.9, 0.8, 0.7, 0.6, and 0.5) and report the performance at $\\tau$ = 0.5 in the main paper. Results for other temperature values are presented in Table 6. To simulate the reference model, we train ViT-B/16 models on noise-free versions of the datasets using identical configurations but with different random seeds for weight initialization. These experiments were performed on a Nvidia Tesla A100."}, {"title": "C.2 DataComp Experiments", "content": "We adopt the training setup from DataComp [53, 16]. CLIP models are trained from scratch using a contrastive objective over image-caption pairs. Each model is trained for 5 epochs with a batch size of 4096. The total number of seen samples is 12.8M for the small-scale dataset and 128M for the medium-scale dataset. These experiments ran on 8 Nvidia Tesla A100s.\nWe test Grad-Mimic using different parts of weights in the reference model, specifically the final MLP layer in the image and text encoders. The layers used are \"visual.transformer.resblocks.11.mlp.c_fc.weight\" and \"text.transformer.resblocks.11.mlp.c_fc.weight\". Grad-Mimic's performance is evaluated using 38 diverse downstream tasks [16] with various temperature values $\\tau$ (0.03, 0.05, 0.07, 0.3, and 0.5). Results are presented in Table 3."}, {"title": "D Ablation Studies", "content": "The Choice of Temperature. We present Grad-Mimic results with different temperature values and compare them to baseline methods (SGD and Grad-Match [24]). The results in the simulation experiments are presented in Table 6. We set the noise level to 0.3 and fine-tune ViT model under linear probing configuration. Grad-Mimic outperforms the baseline methods across all temperature settings. Lower temperatures yield better testing accuracy, as they normalize mimic scores in the way that encourages the model to focus more on high-value samples during training."}]}