{"title": "IKUN: Initialization to Keep snn training and generalization great with sUrrogate-stable variaNce", "authors": ["Da Chang", "Deliang Wang", "Xiao Yang"], "abstract": "Weight initialization significantly impacts the con- vergence and performance of neural networks. While traditional methods like Xavier and Kaim- ing initialization are widely used, they often fall short for spiking neural networks (SNNs), which have distinct requirements compared to artificial neural networks (ANNs). To address this, we introduce IKUN, a variance- stabilizing initialization method integrated with surrogate gradient functions, specifically designed for SNNs. IKUN stabilizes signal propagation, accelerates convergence, and enhances generaliza- tion. Experiments show IKUN improves training efficiency by up to 50%, achieving 95% training accuracy and 91% generalization accuracy. Hessian analysis reveals that IKUN-trained mod- els converge to flatter minima, characterized by Hessian eigenvalues near zero on the posi- tive side, promoting better generalization. The method is open-sourced for further exploration: https://github.com/MaeChd/SurrogateVarStabe.", "sections": [{"title": "1. Introduction", "content": "Spiking Neural Networks (SNNs) have garnered signifi- cant attention in both academia and industry in recent years (Ghosh-Dastidar & Adeli, 2009)(Bohte et al., 2000)(Ton- nelier et al., 2007)(Gerum & Schilling, 2021). As a biologically-inspired computational model, SNNs emu- late the spiking behavior of biological neurons, exhibiting unique advantages such as sparsity and event-driven com- putation. These characteristics endow SNNs with great potential in low-power and efficient computing, particularly in scenarios demanding high energy efficiency and real-time processing, such as neuromorphic hardware and edge com- puting. However, compared to traditional Artificial Neural Networks (ANNs), the discontinuity and complex temporal dynamics of SNNs pose significant challenges for training and optimization.\nIn neural network research, initialization methods are widely regarded as a critical factor influencing network perfor- mance and are key to successful training. Effective initial- ization contributes to network performance in the following three aspects:\n\u2022 Maintaining stable signal propagation across layers to prevent signal magnitude imbalance (LeCun et al., 2002)(Glorot & Bengio, 2010).\n\u2022 Facilitating gradient flow to mitigate issues such as vanishing or exploding gradients during training (He et al., 2015)(He et al., 2016).\n\u2022 Accelerating convergence and improving training effi- ciency (Glorot & Bengio, 2010).\nHowever, the spiking nature of SNNs and the surrogate gra- dient (SG) mechanism adopted in some training methods introduce new challenges for initialization. The SG mech- anism defines a smooth gradient approximation to address the non-differentiable nature of SNNs. However, recent studies indicate that this approach introduces bias: SG pro- vides a biased gradient approximation even in deterministic networks with well-defined gradients. Moreover, in SNNs, the gradients derived from SG often do not align with the actual gradients of the surrogate loss, potentially affecting the effectiveness of training (Gygax & Zenke, 2024).\nA more prominent issue lies in the inconsistency of SNN activation functions between forward and backward propa- gation. This raises a critical question:\nCan existing neural network initialization methods be directly applied to SNNs?\nTraditional ANN initialization methods typically assume continuous activation functions, overlooking the nonlinear, discrete spiking characteristics of SNNs. This assumption"}, {"title": "2. Related Work", "content": "may not hold for SNNs. The dynamic behavior of spik- ing neurons fundamentally differs from the properties of continuous activation functions, making it difficult to main- tain stable signal magnitudes in SNNs (Wu et al., 2018). Additionally, the impact of the SG mechanism on signal propagation in backpropagation is often neglected, exacer- bating the problem. Existing ANN initialization methods are not optimized for SNN training based on SG, potentially limiting the generalization ability of SNNs, especially in deeper networks.\nTo address these issues, this paper proposes a novel ini- tialization method specifically designed for SNNs, named IKUN (Initialization to Keep SNN training and gener- alization great with sUrrogate-stable variaNce). This method integrates the spiking characteristics of SNNs with the SG mechanism, optimizing both signal propagation and gradient dynamics. By controlling signal and gradient vari- ances, IKUN achieves stability in both forward and back- ward propagation. Simultaneously, it leverages the proper- ties of surrogate gradients to improve training efficiency and generalization performance.\nThe main contributions of this paper are as follows:\n\u2022 Proposing the IKUN initialization method: Through theoretical derivations and experimental validations, we design a dynamic initialization method that adapts to surrogate gradient characteristics, significantly en- hancing the training and generalization performance of SNNs.\n\u2022 Plug-and-play compatibility: The IKUN method is compatible with various surrogate gradient functions (e.g., sigmoid, tanh, and linear surrogate gradients), demonstrating strong versatility.\n\u2022 Experimental validation: Experimental results on benchmark datasets show that compared to traditional initialization methods, IKUN achieves notable im- provements in training stability, convergence speed, and generalization performance."}, {"title": "2.1. Weight Initialization", "content": "Weight initialization is a critical component of deep neural network training. Its primary goal is to assign appropriate initial weights to the network at the start of training, en- suring that the variance of signals remains stable as they propagate through the network. This stability helps to effec- tively prevent issues such as gradient vanishing or gradient explosion. To address these challenges, researchers have proposed several classic weight initialization methods.\nLeCun et al.(LeCun et al., 2002) introduced one of the ear-"}, {"title": "2.2. SNN Training", "content": "liest initialization methods focusing on variance stability. Their method aimed to stabilize signal propagation by op- timizing the weight distribution, specifically designed for Sigmoid and Tanh activation functions. The initialization formula is as follows:\n$\\displaystyle {\\sigma_{w}=\\frac{1}{\\sqrt{\\text{fan}\\_\\text{in}}}}$\nSubsequently, Xavier initialization (Glorot & Bengio, 2010) advanced this idea by ensuring consistency in signal vari- ance during both forward and backward propagation when using symmetric activation functions (e.g., Tanh, Sigmoid). This improvement enhanced network training performance. The weight initialization formula is:\n$\\displaystyle {\\sigma_{w}=\\sqrt{\\frac{2}{\\text{fan}\\_\\text{in}+\\text{fan}\\_\\text{out}}}}$\nXavier initialization stands out for simultaneously consider- ing the number of input and output nodes (fan-in and fan- out), making it suitable for bidirectional nonlinear activation functions and stabilizing signal variance during propagation.\nKaiming initialization (He et al., 2015) extended this ap- proach to scenarios involving asymmetric activation func- tions, particularly ReLU (Glorot et al., 2011) and its variants. This method, tailored to the unidirectional characteristics of ReLU activation functions, only considers the influence of fan-in, similar to LeCun initialization. The formula is:\n$\\displaystyle {\\sigma_{w}=\\sqrt{\\frac{2}{\\text{fan}\\_\\text{in}}}}$\nThis strategy adapts well to unidirectional nonlinear activa- tion functions like ReLU but requires additional adjustments to account for their asymmetry, ensuring stable signal prop- agation.\nThe above weight initialization methods have been widely applied in ANNs, providing reliable stability for signal vari- ance during propagation. However, these methods were primarily designed for ANN models optimized using con- ventional gradient-based approaches. In SNNs, the introduc- tion of surrogate gradient mechanisms results in significant differences in signal propagation characteristics compared to ANNs. This can lead to signal attenuation or obstructed gradient flow in SNNs.\nTo address these issues, our method builds upon the core principles of the aforementioned initialization techniques, while incorporating the unique characteristics of surrogate activation functions. We designed a weight initialization strategy better suited for SNNs to mitigate signal degrada- tion and gradient blockage, thereby enhancing the training efficiency and performance of the network."}, {"title": "2.2.1. OVERVIEW OF SNNS", "content": "Spiking Neural Networks (SNNs) are computational models inspired by biological nervous systems (Ghosh-Dastidar & Adeli, 2009). Unlike traditional Artificial Neural Networks (ANNs), neurons in SNNs communicate through discrete spikes at specific time points. This means that the communi- cation between neurons is based on the timing and frequency of spikes, exhibiting complex nonlinear dynamic behaviors (Bohte et al., 2000). Two commonly used neuron models in SNNs are the IF (Integrate-and-Fire) model (Tonnelier et al., 2007) and the LIF (Leaky Integrate-and-Fire) model (Gerum & Schilling, 2021).\nThe IF model is the simplest spiking neuron model, as- suming that the membrane potential V(t) of a neuron ac- cumulates without any leakage. The change in membrane potential follows the equation:\n$\\displaystyle {V(t)=V(t-1)+I(t)*\\Delta t/C}$\nwhere V(t) is the membrane potential at time t, I(t) is the input signal to the neuron at time t, At is the time step, and C is the membrane capacitance. When the membrane potential reaches or exceeds the threshold Vth, the neuron emits a spike and resets the membrane potential to a resting value Vreset.\nThe LIF model builds on the IF model by adding a leakage property. If the neuron does not receive sufficient stimuli to reach the threshold within a certain period, its membrane potential gradually decays back to the resting level Vreset. This model abstracts the neuron as an RC circuit, as shown in Figure 1, and follows the equation:\n$\\displaystyle {\\tau \\frac{dV}{dt}=-V(t)+RI(t)}$\nwhere T = RC is the time constant, R is the membrane resistance, C is the membrane capacitance, I(t) is the input signal at time t, and V (t) represents the membrane potential at time t. To integrate this neuron model into a deep learning framework, the equation can be discretized, yielding:\n$\\displaystyle {V(t)=\\left(1-\\frac{1}{\\tau}\\right)*V(t-1)+\\frac{1}{\\tau}*I(t)}$\nFrom this equation, it is evident that the membrane po- tential at the current time depends on the potential at the previous time step, implying that the membrane potential accumulates over time until it reaches the threshold. Once the threshold is reached, the neuron emits a spike, and the membrane potential resets to the resting level or below.\nWith the basic neuron module, an SNN can be constructed by connecting neurons with weighted synapses, forming"}, {"title": "2.2.2. SURROGATE GRADIENT MECHANISM IN SNNS", "content": "a hierarchical structure as shown in Figure 2. Since the input signals for SNNs must be discrete, the data fed into the network needs to be encoded. Two commonly used encoding methods are rate encoding and temporal encoding. Temporal encoding represents information through the time intervals of spikes, capturing important information about the temporal structure of the original data (Comsa et al., 2020). Rate encoding encodes information using the spiking frequency of neurons, where stronger stimuli are typically represented by higher spiking frequencies (Jin et al., 2018).\nInspired by ANN models, SNNs have incorporated oper- ations such as convolution and pooling, supporting multi-layer fully connected structures (Xu et al., 2018). Com- bining the feature extraction capabilities of convolutional neural networks with the biological plausibility of spiking neural networks, SNNs have evolved into deeper and more complex structures, offering outstanding processing speed and low energy consumption.\nCurrently, the training methods for SNNs can be catego- rized into three main approaches: training based on biologi- cal synaptic plasticity (Bi & Poo, 1998), backpropagation algorithms using surrogate gradients (Neftci et al., 2019) (Ledinauskas et al., 2020), and training based on artificial neural network (ANN) conversion (Rueckauer et al., 2017). This paper primarily focuses on the weight initialization problem in SNNs trained using surrogate gradient-based backpropagation algorithms.\nDue to the discrete and non-differentiable nature of SNN activation functions, traditional backpropagation algorithms"}, {"title": "3. Method", "content": "cannot directly optimize their weights. To address this issue, surrogate gradient methods were introduced. These meth- ods approximate the gradient of spiking activation functions with continuous and differentiable functions, enabling the application of backpropagation for optimization. Specifi- cally, the core of surrogate gradient methods is to replace the non-differentiable spiking activation function's gradient with a continuous surrogate function (e.g., the approxima- tion function for the IFNode (Fang et al., 2023)) for gradient computation.\nIt is worth noting that the challenge of non-differentiable activation functions is not unique to SNNs. In ANNs, a com- mon method for gradient estimation is the Straight-Through Estimator (STE) (Bengio et al., 2013). For example, Yin et al. (Yin et al., 2019) analyzed the training stability of models using STE and studied the impact of different types of STE on weight updates. Building on such studies, we hypothesize that the characteristics of surrogate gradients may significantly influence gradient flow in SNNs. It is well known that gradient explosion or vanishing in ANNs often results from inappropriate weight initialization or activa- tion function selection. A similar mechanism might affect training stability in SNNs, albeit in a different form.\nTo better understand this issue, consider an extreme case: if the true gradient of the spiking activation function (e.g., the Dirac & function) is directly used in backpropagation, its discontinuity and potentially large magnitude would lead to highly unstable training. Under such circumstances, no ini-"}, {"title": "3.1. Core Idea of the IKUN Initialization Method", "content": "SNNs fall between these two extremes. On one hand, their activation functions can be adjusted using surrogate gradi- ents. The design of surrogate gradients allows for a trade-off between approximating the true spiking activation function and maintaining training stability. For instance, as shown in Figure 3, for $\\sigma(x, \\alpha) = \\frac{1}{1+\\exp(-ax)}$, increasing the hy- perparameter a makes the function increasingly resemble a step function. As the surrogate gradient becomes closer to the true spiking activation function, the optimization pro- cess tends to become less stable. On the other hand, current weight initialization schemes for SNNs typically reuse ANN methods or rely on default configurations, lacking optimiza- tions tailored to SNN-specific characteristics. This raises questions about their applicability and effectiveness.\nTherefore, the tunability of surrogate gradients provides a space for balancing approximation accuracy and training stability. At the same time, it highlights the need to con- sider the potential impact of SNN initialization schemes on gradient flow and model training. Exploring initialization strategies better suited for SNNs may be a key direction for improving their training performance.\nVariance-Stable Initialization Variance-stable initializa- tion aims to ensure that the variance of signals remains constant as they propagate between network layers, thereby avoiding issues like gradient vanishing or explosion, espe- cially in deep networks. This is crucial for accelerating convergence and improving model performance (LeCun et al., 2002)(Glorot & Bengio, 2010)(He et al., 2015). On one hand, the spiking mechanism of neurons in the forward propagation of SNNs differs from traditional activation func-"}, {"title": "3.2. Theoretical Guarantee of the IKUN Initialization Method", "content": "tions (such as ReLU), altering the statistical properties of signal propagation. On the other hand, in backpropaga- tion, smooth surrogate functions (e.g., ATan(x)) are used to compute gradients to avoid instability caused by directly handling derivatives of impulse functions. This inconsis- tency between forward and backward processes requires a comprehensive consideration of the variance effects of both mechanisms to achieve balanced propagation of signals and gradients. Therefore, a targeted initialization strategy needs to be designed.\nCore Idea In forward propagation, maintain the variance of each layer's output consistent with its input, and in back-propagation, preserve stable gradient variance to address the differences arising from the unique discrete spiking activa- tion mechanisms of SNNs and surrogate gradient methods. The key to variance-stable initialization is to adapt to these characteristics, ensuring efficient signal propagation in deep networks and enhancing training stability and model perfor- mance.\nTHEOREM 1: SURROGATE-STABLE VARIANCE INITIALIZATION (IKUN)\nIn SNNs, suppose the membrane potential update formula is\n$\\displaystyle {H[t] = T V[t - 1] + \\sum_{i=1}^{m} W_i X_i[t],}$\nThe weight initialization strategy satisfies $w_i \\sim N(0, \\sigma_v)$, and the threshold is set as $V_{\\text{threshold}} = \\mu_H$. By the following weight variance condition:\n$\\displaystyle {\\sigma_v \\propto \\frac{\\alpha}{\\sqrt{\\text{fan}_{\text{in}} \\cdot E[f'(H[t])^2]}}},$\nwe can ensure variance stability of signals during forward propagation while avoiding gradient vanishing or explosion during backpropagation. Here, $\\alpha$ is a given hyperparameter.\nWe provide a detailed derivation in the proof section of the appendix.\nRemarks Usually, we assume the input follows a normal distribution with mean 0 and variance 1. Therefore, the simplified form of Equation (8) resembles Kaiming initial-ization, which we refer to as IKUN v1. Furthermore, we propose an improved initialization scheme:\n$\\displaystyle {\\sigma_{w}^{2}=\\frac{\\alpha}{(\\text{fan}_{\text{in}} + \\text{fan}_{\text{out}}) \\cdot \\sigma \\cdot E[f'(H[t])^2]}},$"}, {"title": "4. Experiments", "content": "which we call IKUN v2 initialization, to more comprehen- sively balance the influence of input and output neurons."}, {"title": "4.1. Experimental Setup", "content": "Dataset\nThis experiment evaluates the proposed method on the Fash- ionMNIST dataset (Xiao et al., 2017). FashionMNIST is a classical image classification dataset consisting of 28\u00d728 grayscale images across 10 categories. The training set contains 60,000 images, and the test set contains 10,000 images.\nModel Architecture\nThe experiment uses a typical SNN model architecture-a two-layer convolutional SNN, which consists of two con- volutional layers. Each layer includes convolution, pooling, and spiking activation functions. The specific structure is shown in Figure 4.\nComparison Methods\nTo validate the effectiveness of the proposed method, the following initialization methods are selected for comparison: Xavier Initialization (Glorot & Bengio, 2010), Kaiming Initialization (He et al., 2015), LeCun Initialization (Bi & Poo, 1998), and the default random initialization.\nOptimizers\nTwo optimizers are used for training: SGD (Robbins & Monro, 1951) and Adam (Kingma, 2014).\nExperimental Environment\nAll experiments are conducted on a server equipped with an NVIDIA 1650Ti GPU with 4 GB of memory. The software environment is as follows: Operating System: Windows 10; Python version: 3.8; Deep Learning Framework: PyTorch 1.12.0; SNN Library: SpikingJelly 0.0.0.14; CUDA version: 11.3. These settings ensure the reliability, fairness, and reproducibility of the experiments."}, {"title": "4.2. Evaluation Metrics", "content": "Loss Function\nThe training loss is measured using Mean Squared Error (MSE), defined as:\n$\\displaystyle {L = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y_i}|^2,}$\nwhere N is the number of samples, and yi and \u0177r represent the ground truth and predicted values, respectively. The convergence efficiency and training stability are assessed by analyzing the rate of decline and fluctuation of the loss curve.\nClassification Accuracy\nClassification accuracy evaluates the generalization ability of the model on the test set and is defined as:\n$\\displaystyle {\\text{Accuracy} = \\frac{1}{M} \\sum_{i=1}^{M} \\mathbb{I}(Y_i = \\hat{Y_i}),}$\nwhere M is the number of test samples, and $\\mathbb{I}(\\cdot)$ is the indicator function, which equals 1 if the prediction is correct and 0 otherwise.\nHessian Analysis\nTo investigate training dynamics and optimization perfor- mance, we analyze the Hessian matrix characteristics of the model after training:\n\u2022 Hessian Spectrum Distribution: By analyzing the eigenvalues {\u03bb1, \u03bb2, . . . , \u03bbp} of the Hessian matrix, we assess the flatness of the loss landscape. A smaller maximum eigenvalue or narrower spectrum distribu- tion is often associated with better generalization per- formance (Wang et al., 2018).\n\u2022 Hessian Matrix Trace: The trace is defined as the sum of diagonal elements:\n$\\displaystyle {\\text{Tr}(H) = \\sum_{i=1}^{P} H_{ii}}$\nLower Tr(H) values indicate that the model resides in a flatter region at the end of optimization, typically cor- responding to better generalization ability (Yao et al., 2020).\nBy combining these analyses, we provide a comprehensive evaluation of training dynamics and local curvature, offer- ing a solid basis for interpreting model performance and comparing initialization methods."}, {"title": "4.3. Experimental Results", "content": "We present the training and testing accuracy, as well as the training and testing loss curves, under both Adam and SGD optimizers. From Figure 5 and Figure 6, it is evident that"}, {"title": "4.4. Performance Validation", "content": "the IKUN initialization method demonstrates faster loss re- duction in the early stages of training compared to other initialization methods, while maintaining better stability in later stages. However, the results also show that IKUN ini- tialization does not always exhibit significant advantages. For instance, under the Adam optimizer, IKUN initializa- tion performs worse than Kaiming initialization; under the SGD optimizer, its performance is slightly inferior to LeCun initialization. It is worth noting that traditional initialization methods fail to achieve optimal performance simultaneously on both optimizers, highlighting a potential advantage of our method.\nAdditionally, the differences in testing errors during the later stages of training are relatively minor, making it difficult to fully demonstrate the effectiveness of the method based solely on these results. To further analyze and compare the key metrics of the model during training, we adopt the following two schemes:\nScheme 1: Identify the training epochs when both training accuracy and testing accuracy reach predefined thresholds (e.g., 90% or 95%). This scheme evaluates the impact of different initialization methods on training speed and gen- eralization ability, focusing on the model's performance during the transition from training to generalization.\nScheme 2: Identify the training epochs when testing accu- racy reaches its peak and record the corresponding Hessian Trace (reflecting the curvature characteristics of the parame- ter space). This scheme emphasizes evaluating the stability and robustness of the model at its best performance point, providing a comprehensive reference for assessing initial- ization methods.\nThe specific experimental results are shown in Table 1 and Table 2:\nTable 1 shows that under different training thresholds (we selected training accuracy greater than 95% and testing ac- curacy greater than 91%), the distribution of training epochs varies significantly. By introducing our IKUN initialization method, training efficiency improves substantially: train- ing epochs are reduced by up to 59.38% under the SGD optimizer and 42.31% under the Adam optimizer. This demonstrates that IKUN initialization effectively shortens training time while maintaining high-quality training perfor- mance.\nTable 2 reveals the relationship between the best testing accuracy, the corresponding testing loss, and the Hessian Trace. Lower testing loss and smaller absolute Hessian Trace values are typically associated with better generaliza- tion performance, indicating that the model parameters are optimized in a smoother and more stable space. The results show that IKUN initialization achieves superior Hessian Trace values under both SGD and Adam optimizers (5.27 and 1.17, respectively). Traditional methods often achieve optimal Hessian Trace values under either SGD or Adam alone, whereas IKUN initialization performs well under both optimizers.\nIn summary, these results indicate that the proposed ini- tialization method not only significantly improves training efficiency but also achieves better training behavior and generalization ability across different optimizer settings, offering new perspectives and methodological support for SNN model optimization."}, {"title": "4.4.1. \u0421\u043eMPARISON OF HESSIAN EIGENVALUE DISTRIBUTIONS", "content": "In the experimental results section, we validated the ef- fectiveness of the IKUN initialization method in terms of training efficiency and testing performance. To further ex- plore its inherent advantages, we used Hessian eigenvalue distribution analysis as a tool to deeply investigate the opti- mization characteristics of the method. The distribution of Hessian eigenvalues reflects the curvature properties of the loss function's local region, serving as an important indica- tor for evaluating the generalization ability and optimization stability of the model (Wang et al., 2018).\nWe used the PyHessian package (Yao et al., 2020) to com- pute and analyze the Hessian eigenvalues of the neural net- work, focusing on the performance comparison between the IKUN initialization method and other mainstream initializa- tion methods.\nIKUN vs. Kaiming Initialization: Eigenvalues Close to 0 and Positive\nAs shown in Figure 7, Figure 8, and Figure 9, the Hessian eigenvalues of IKUN and Kaiming initialization are almost entirely positive and close to 0, indicating that the optimiza- tion ends in flat minima regions. These curvature charac- teristics are typically associated with better generalization ability and enhanced robustness to parameter perturbations. Although Kaiming initialization, as the default method for CSNNs, performs well, it is not optimal; in contrast, IKUN initialization further improves generalization performance."}, {"title": "4.4.2. COMPREHENSIVE ANALYSIS", "content": "Other Methods: Wider Eigenvalue Distribution with Positive and Negative Values\nTraditional methods (e.g., LeCun, Xavier, and Normal ini- tialization) exhibit a broader Hessian eigenvalue distribution, including both positive and negative values. This indicates convexity in certain directions and concavity in others, sug- gesting that these methods may remain stuck in saddle-point regions, reducing optimization efficiency and parameter ro- bustness. Furthermore, while these methods show strong exploratory behavior during training, their performance on the test set is often slightly inferior to that of the IKUN method.\nBased on the experimental results and the Hessian analysis in this section, the main advantages of the IKUN initializa- tion method can be summarized as follows:\n1. Improved Generalization Ability: The model tends to converge to flat minima regions with low curvature, resulting in greater robustness and reduced sensitivity to unseen data, thereby enhancing generalization performance.\n2. Stable Optimization Process: Positive eigenvalues pre- vent saddle-point issues, leading to higher optimization effi- ciency."}, {"title": "5. Conclusion", "content": "We proposed the IKUN initialization method tailored for Spiking Neural Networks (SNNs). By leveraging the char- acteristics of surrogate gradients, this method significantly improves training stability and model generalization per- formance. Both theoretical analysis and experimental vali- dation demonstrate the superior performance of the IKUN method: the Hessian eigenvalue distribution is almost en- tirely centered around zero, indicating that model param- eters reside in flat minima regions\u2014a property closely as- sociated with better generalization ability. Compared to traditional methods (which exhibit both positive and nega- tive Hessian eigenvalues), the IKUN method greatly reduces the impact of negative eigenvalues, avoiding stagnation at saddle points or regions with abrupt curvature changes. Furthermore, the potential of combining this method with other optimization strategies (e.g., dynamic learning rates, en- hanced regularization) warrants further investigation.\nDue to time and computational resource constraints, our experiments were conducted on a two-layer convolutional SNN using a fixed subset of the Fashion-MNIST dataset. De- spite these limitations, the results indicate that our method demonstrates significant advantages in Hessian eigenvalue distribution, generalization performance, and test set out- comes.\nFuture research will focus on validating the effectiveness of the proposed method on larger datasets (such as CIFAR-10 and ImageNet) and more complex model architectures. The preliminary findings of this study provide a new perspective on SNN training optimization and generalization perfor- mance research. Future work will aim to further validate its potential value through broader experimentation."}]}