{"title": "VISIONTS: VISUAL MASKED AUTOENCODERS ARE FREE-LUNCH ZERO-SHOT TIME SERIES FORECASTERS", "authors": ["Mouxiang Chen", "Lefei Shen", "Zhuo Li", "Xiaoyun Joy Wang", "Jianling Sun", "Chenghao Liu"], "abstract": "Foundation models have emerged as a promising approach in time series forecasting (TSF). Existing approaches either fine-tune large language models (LLMs) or build large-scale time-series datasets to develop TSF foundation models. However, these methods face challenges due to the severe cross-domain gap or in-domain heterogeneity. In this paper, we explore a new road to building a TSF foundation model from rich and high-quality natural images, based on the intrinsic similarities between images and time series. To bridge the gap between the two domains, we reformulate the TSF task as an image reconstruction task, which is further processed by a visual masked autoencoder (MAE) self-supervised pre-trained on the ImageNet dataset. Surprisingly, without further adaptation in the time-series domain, the proposed VISIONTS could achieve superior zero-shot forecasting performance compared to existing TSF foundation models. With minimal fine-tuning, VISIONTS could further improve the forecasting and achieve state-of-the-art performance in most cases. These findings suggest that visual models could be a free lunch for TSF and highlight the potential for future cross-domain research between computer vision and TSF. Our code is publicly available at https://github.com/Keytoyze/VisionTS.", "sections": [{"title": "1 INTRODUCTION", "content": "Foundation models (Bommasani et al., 2021) have revolutionized natural language processing (NLP) and computer vision (CV) in recent years (Brown et al., 2020; He et al., 2022). By pretraining on large-scale data, they have shown remarkable few-shot and even zero-shot performance across various downstream tasks. This has motivated an emergent paradigm shift in time series forecasting (TSF), moving from a traditional one-model-per-dataset framework to universal forecasting with a single pre-trained model (Woo et al., 2024; Goswami et al., 2024). A TSF foundation model can greatly reduce the need for downstream data and demonstrate strong forecasting performance on diverse domains, such as energy consumption planning, weather forecasting, and traffic flow.\nWe have recently witnessed two roads to building a TSF foundation model. The first tries to fine-tune the weights of large language models (LLMs) which have been pre-trained on text data for TSF tasks (i.e., text-based) (Zhou et al., 2023; Jin et al., 2024), based on the observation that LLMs and TSF models share a similar left-to-right forecasting paradigm. However, due to the significant gap between these two modalities, the effectiveness of such transferability between language and time series has recently been questioned by Tan et al. (2024).\nThe second road focuses on constructing large-scale time-series datasets collected from diverse domains to train a TSF foundation model from scratch (i.e., TS-based) (Woo et al., 2024; Das et al., 2024). Nevertheless, unlike images or language with unified formats, time series data is highly heterogeneous in length, frequency, dimensionality, domains, and semantics, limiting the transferability between pre-training and downstream domains. Until recently, constructing a high-quality dataset remains challenging and is still in the early exploration stage.\nIn this paper, we investigate a third road that is less explored yet promising: building TSF foundation models with pre-trained visual models. Our starting point is the intrinsic similarities between natural images and TS: \u25cf Similar modalities: Unlike discrete texts, both images and TS are continuous. \u25cf Similar origin: Both TS and images are observations of real-world physical systems, whereas languages are products of human cognitive processes. 3 Similar information density: Languages are human-generated signals with high semantic density, while images and TS data are natural signals with heavy redundancy (He et al., 2022). 4 Similar features: As shown in Fig. 2, images often display many features of real-world time series, which are rarely found in language data. These findings suggest that visual models could be easier to adapt to TSF tasks than language models. Based on these insights, we are motivated to answer the question: \u201cCan a visual model pre-trained on images be a free-lunch foundation model for zero-shot time series forecasting?\u201d\nWe focus on visual masked autoencoder (MAE)\u00b9, a popular CV foundation model (He et al., 2022) by self-supervised pre-training on ImageNet (Deng et al., 2009). Inspired by the well-known prompt technique in NLP (Schick & Sch\u00fctze, 2021), we propose a simple method to reformulate TSF as a"}, {"title": "2 PRELIMINARIES", "content": "Time Series Forecasting (TSF) For a multivariate time series with M variables, let $x_t \\in \\mathbb{R}^M$ represent the value at t-th time step. Given a historical sequence (i.e., look-back window) $X_{t-L:t} = [x_{t-L},\u2026\u2026, x_{t-1}] \\in \\mathbb{R}^{L \\times M}$ with context length L, the TSF task is to predict future values (i.e., forecast horizon) with prediction length H: $X_{t:t+H} = [x_t,\u00b7\u00b7\u00b7, x_{t+H-1}] \\in \\mathbb{R}^{H \\times M}$.\nPatch-Level Image Reconstruction To obtain high-quality visual representation for downstream CV tasks, He et al. (2022) proposed masked autoencoder (MAE) to pre-train a Vision Transformer (ViT) (Dosovitskiy et al., 2021) using a patch-level image reconstruction task on ImageNet. Specifically, for an image of size W \u00d7 W (where W represents both the width and height, as ImageNet images are square), the image is evenly divided into N \u00d7 N patches, each with a width and height of S = W/N. During pre-training, some random patches are masked, while the remaining visible patches are fed into the ViT with their position encodings. MAE are trained to reconstruct the masked pixel values from these visible patches."}, {"title": "3 METHODOLOGY", "content": "As noted in the Introduction, TS and images share intrinsic similarities, suggesting the transfer poten-tial of visual pre-trained models (particularly MAE in this paper) for TSF tasks. This section explains how to reformulate TSF tasks into MAE's pre-training task, i.e., patch-level image reconstruction.\nOur idea is straightforward: map the look-back/forecasting windows to visible/masked patches, respectively. This idea is supported by the recent success of prompt tuning (Schick & Sch\u00fctze, 2021) in NLP, where the predictions for [mask] token in pre-trained language models (e.g., BERT (Devlin et al., 2019)) are directly used for downstream tasks. By unifying the forms of the two tasks, we bridge the gap between the two domains, enabling a MAE for zero-shot TSF directly without adapting the pre-trained parameters.\nNotably, this idea is limited to univariate forecasting since multivariates are intractable to be encoded in a single image. Fortunately, recent work shows that channel independence \u2013 predicting each variable separately for multivariate forecasting can still be effective (Nie et al., 2022; Han et al., 2024). Therefore, we leave the exploration of multivariate interactions for future work."}, {"title": "4 EXPERIMENTS", "content": "We use MAE (Base) with 112M parameters as our backbone. We conduct experiments on popular TSF benchmarks, including long-term TSF datasets (Zhou et al., 2021; Wu et al., 2021) and Monash benchmark (Godahewa et al., 2021). We select representative baselines for comparison, including TS-based foundation models: MOIRAI (Woo et al., 2024) and TimesFM (Das et al., 2024); Text-based foundation models: TimeLLM (Jin et al., 2024), GPT4TS (Zhou et al., 2023), and LLMTime (Gruver et al., 2023); and other popular TSF baselines covering both Transformer-based, MLP-based and CNN-based architectures. Details are elaborated in Appendix A."}, {"title": "4.1 ZERO-SHOT TIME SERIES FORECASTING", "content": "Setups We first evaluate VISIONTS's zero-shot TSF performance without any fine-tuning on time-series modalities. To prevent data leakage and assess the out-of-distribution capabilities, we selected six widely-used datasets from the long-term TSF benchmark that are not included in MOIRAI'S pre-training set for evaluation. Since most baselines cannot perform zero-shot forecasting, we report their few-shot results by fine-tuning on the 10% of the individual target datasets. We also evaluate the Monash benchmark including 29 test datasets, which is more challenging for VISIONTS since they were used in MOIRAI's pre-training but not for VISIONTS. We set the hyperparameters to r = c = 0.4. Following common practice (Nie et al., 2022; Zhou et al., 2023; Woo et al., 2024), we conduct hyperparameter tuning on validation sets to determine the optimal context length L, detailed in Appendix B.1.\nResults on Long-Term TSF Benchmark Table 1 shows that VISIONTS surprisingly achieves the best forecasting performance in most cases (7 out of 14). Specifically, VISIONTS demonstrates a relative average MSE reduction of approximately 6% compared to MOIRAISmall and MOIRAILarge, and performs comparably to MOIRAIBase. When compared to the various few-shot baselines, VISIONTS shows a relative average MSE reduction ranging from 8% to 84%. Given that all baselines except for VISIONTS are trained on the time-series domain, this result is particularly encouraging. It suggests that the transferability from images to time-series is stronger than from text to time-series, and even comparable to the in-domain transferability between time-series. We also include a comparison with traditional algorithms (ETS, ARIMA, and Seasonal Na\u00efve) in Appendix B.4, where VISIONTS still outperforms all of these traditional methods."}, {"title": "6 CONCLUSION", "content": "In this paper, we explore a novel approach to building a time series forecasting (TSF) foundation model using natural images, offering a new perspective distinct from the traditional text-based and TS-based methods. By leveraging the intrinsic similarities between images and time series, we introduced VISIONTS, an MAE-based TSF foundation model that reformulates the TSF task as an image reconstruction problem. Our extensive evaluations demonstrate that VISIONTS achieves outstanding forecasting performance in zero-shot and full-shot settings, being a free lunch for a TSF foundation model. We hope our findings could open new avenues for further cross-domain research.\nLimitations and Future Directions. As a preliminary study, we employed a basic MAE model and reformulated TSF as a patch-based image reconstruction task. Reformulating TSF as a more refined image inpainting task and utilizing more advanced models like diffusion models (Rombach et al., 2022; Peebles & Xie, 2023) presents a promising research direction."}]}