{"title": "VISIONTS: VISUAL MASKED AUTOENCODERS ARE FREE-LUNCH ZERO-SHOT TIME SERIES FORECASTERS", "authors": ["Mouxiang Chen", "Lefei Shen", "Zhuo Li", "Xiaoyun Joy Wang", "Jianling Sun", "Chenghao Liu"], "abstract": "Foundation models have emerged as a promising approach in time series forecasting (TSF). Existing approaches either fine-tune large language models (LLMs) or build large-scale time-series datasets to develop TSF foundation models. However, these methods face challenges due to the severe cross-domain gap or in-domain heterogeneity. In this paper, we explore a new road to building a TSF foundation model from rich and high-quality natural images, based on the intrinsic similarities between images and time series. To bridge the gap between the two domains, we reformulate the TSF task as an image reconstruction task, which is further processed by a visual masked autoencoder (MAE) self-supervised pre-trained on the ImageNet dataset. Surprisingly, without further adaptation in the time-series domain, the proposed VISIONTS could achieve superior zero-shot forecasting performance compared to existing TSF foundation models. With minimal fine-tuning, VISIONTS could further improve the forecasting and achieve state-of-the-art performance in most cases. These findings suggest that visual models could be a free lunch for TSF and highlight the potential for future cross-domain research between computer vision and TSF. Our code is publicly available at https://github.com/Keytoyze/VisionTS.", "sections": [{"title": "1 INTRODUCTION", "content": "Foundation models (Bommasani et al., 2021) have revolutionized natural language processing (NLP) and computer vision (CV) in recent years (Brown et al., 2020; He et al., 2022). By pretraining on large-scale data, they have shown remarkable few-shot and even zero-shot performance across various downstream tasks. This has motivated an emergent paradigm shift in time series forecasting (TSF), moving from a traditional one-model-per-dataset framework to universal forecasting with a single pre-trained model (Woo et al., 2024; Goswami et al., 2024). A TSF foundation model can greatly reduce the need for downstream data and demonstrate strong forecasting performance on diverse domains, such as energy consumption planning, weather forecasting, and traffic flow.\nWe have recently witnessed two roads to building a TSF foundation model. The first tries to fine-tune the weights of large language models (LLMs) which have been pre-trained on text data for TSF tasks (i.e., text-based) (Zhou et al., 2023; Jin et al., 2024), based on the observation that LLMs and TSF models share a similar left-to-right forecasting paradigm. However, due to the significant gap between these two modalities, the effectiveness of such transferability between language and time series has recently been questioned by Tan et al. (2024).\nThe second road focuses on constructing large-scale time-series datasets collected from diverse domains to train a TSF foundation model from scratch (i.e., TS-based) (Woo et al., 2024; Das et al., 2024). Nevertheless, unlike images or language with unified formats, time series data is highly heterogeneous in length, frequency, dimensionality, domains, and semantics, limiting the transferability between pre-training and downstream domains. Until recently, constructing a high-quality dataset remains challenging and is still in the early exploration stage.\nIn this paper, we investigate a third road that is less explored yet promising: building TSF foundation models with pre-trained visual models. Our starting point is the intrinsic similarities between natural images and TS: \u25cf Similar modalities: Unlike discrete texts, both images and TS are continuous. \u25cf Similar origin: Both TS and images are observations of real-world physical systems, whereas languages are products of human cognitive processes. 3 Similar information density: Languages are human-generated signals with high semantic density, while images and TS data are natural signals with heavy redundancy (He et al., 2022). 4 Similar features: As shown in Fig. 2, images often display many features of real-world time series, which are rarely found in language data. These findings suggest that visual models could be easier to adapt to TSF tasks than language models. Based on these insights, we are motivated to answer the question: \u201cCan a visual model pre-trained on images be a free-lunch foundation model for zero-shot time series forecasting?\"\nWe focus on visual masked autoencoder (MAE), a popular CV foundation model (He et al., 2022) by self-supervised pre-training on ImageNet (Deng et al., 2009). Inspired by the well-known prompt technique in NLP (Schick & Sch\u00fctze, 2021), we propose a simple method to reformulate TSF as a"}, {"title": "2 PRELIMINARIES", "content": "Time Series Forecasting (TSF) For a multivariate time series with M variables, let $x_t \\in \\mathbb{R}^M$ represent the value at t-th time step. Given a historical sequence (i.e., look-back window) $X_{t-L:t} = [x_{t-L},\u2026\u2026, x_{t-1}] \\in \\mathbb{R}^{L \\times M}$ with context length L, the TSF task is to predict future values (i.e., forecast horizon) with prediction length H: $X_{t:t+H} = [x_t,\\cdotp,\\cdotp,\\cdotp, x_{t+H-1}] \\in \\mathbb{R}^{H \\times M}$.\nPatch-Level Image Reconstruction To obtain high-quality visual representation for downstream CV tasks, He et al. (2022) proposed masked autoencoder (MAE) to pre-train a Vision Transformer (ViT) (Dosovitskiy et al., 2021) using a patch-level image reconstruction task on ImageNet. Specifically, for an image of size W \u00d7 W (where W represents both the width and height, as ImageNet images are square), the image is evenly divided into N \u00d7 N patches, each with a width and height of S = W/N. During pre-training, some random patches are masked, while the remaining visible patches are fed into the ViT with their position encodings. MAE are trained to reconstruct the masked pixel values from these visible patches."}, {"title": "3 METHODOLOGY", "content": "As noted in the Introduction, TS and images share intrinsic similarities, suggesting the transfer potential of visual pre-trained models (particularly MAE in this paper) for TSF tasks. This section explains how to reformulate TSF tasks into MAE's pre-training task, i.e., patch-level image reconstruction.\nOur idea is straightforward: map the look-back/forecasting windows to visible/masked patches, respectively. This idea is supported by the recent success of prompt tuning (Schick & Sch\u00fctze, 2021) in NLP, where the predictions for [mask] token in pre-trained language models (e.g., BERT (Devlin et al., 2019)) are directly used for downstream tasks. By unifying the forms of the two tasks, we bridge the gap between the two domains, enabling a MAE for zero-shot TSF directly without adapting the pre-trained parameters.\nNotably, this idea is limited to univariate forecasting since multivariates are intractable to be encoded in a single image. Fortunately, recent work shows that channel independence \u2013 predicting each variable separately for multivariate forecasting can still be effective (Nie et al., 2022; Han et al., 2024). Therefore, we leave the exploration of multivariate interactions for future work."}, {"title": "4 EXPERIMENTS", "content": "We use MAE (Base) with 112M parameters as our backbone. We conduct experiments on popular TSF benchmarks, including long-term TSF datasets (Zhou et al., 2021; Wu et al., 2021) and Monash benchmark (Godahewa et al., 2021). We select representative baselines for comparison, including TS-based foundation models: MOIRAI (Woo et al., 2024) and TimesFM (Das et al., 2024); Text-based foundation models: TimeLLM (Jin et al., 2024), GPT4TS (Zhou et al., 2023), and LLMTime (Gruver et al., 2023); and other popular TSF baselines covering both Transformer-based, MLP-based and CNN-based architectures. Details are elaborated in Appendix A.\n4.1 ZERO-SHOT TIME SERIES FORECASTING\nSetups We first evaluate VISIONTS's zero-shot TSF performance without any fine-tuning on time-series modalities. To prevent data leakage and assess the out-of-distribution capabilities, we selected six widely-used datasets from the long-term TSF benchmark that are not included in MOIRAI's pre-training set for evaluation. Since most baselines cannot perform zero-shot forecasting, we report their few-shot results by fine-tuning on the 10% of the individual target datasets. We also evaluate the Monash benchmark including 29 test datasets, which is more challenging for VISIONTS since they were used in MOIRAI's pre-training but not for VISIONTS. We set the hyperparameters to r = c = 0.4. Following common practice (Nie et al., 2022; Zhou et al., 2023; Woo et al., 2024), we conduct hyperparameter tuning on validation sets to determine the optimal context length L, detailed in Appendix B.1.\nResults on Long-Term TSF Benchmark Table 1 shows that VISIONTS surprisingly achieves the best forecasting performance in most cases (7 out of 14). Specifically, VISIONTS demonstrates a relative average MSE reduction of approximately 6% compared to MOIRAISmall and MOIRAILarge, and performs comparably to MOIRAIBase. When compared to the various few-shot baselines, VISIONTS shows a relative average MSE reduction ranging from 8% to 84%. Given that all baselines except for VISIONTS are trained on the time-series domain, this result is particularly encouraging. It suggests that the transferability from images to time-series is stronger than from text to time-series, and even comparable to the in-domain transferability between time-series. We also include a comparison with traditional algorithms (ETS, ARIMA, and Seasonal Na\u00efve) in Appendix B.4, where VISIONTS still outperforms all of these traditional methods."}, {"title": "5 RELATED WORK", "content": "Depending on the pre-training data, TSF foundation models can be categorized into Text-based and TS-based. We firstly review these related work, then introduce recent research for image-based time series analysis.\nText-based TSF Foundation Models Large Language Models (LLMs) pre-trained on large amounts of text data are being applied to TSF tasks. For example, Zhou et al. (2023) fine-tuned a pre-trained GPT (Radford et al., 2019) on each time-series downstream task, such as forecasting, classification, imputation, and anomaly detection. Based on Llama (Touvron et al., 2023), Jin et al. (2024) froze the pre-trained LLM and reprogrammed the time series to align with the language modality. Bian et al. (2024) adopted a two-stage approach by continually pre-training GPT (Radford et al., 2019) on the time-series domain. Nevertheless, the TSF performance of LLMs has recently been questioned by Tan et al. (2024), which designed several ablation studies to show that textual knowledge is unnecessary for forecasting. In this paper, we attribute it to the large modality gap. Some recent approaches focus on directly transforming the time series into natural texts for LLMs, allowing for zero-shot forecasting. For example, PromptCast (Xue & Salim, 2023) uses pre-defined templates to describe numerical time series data, while LLMTime (Gruver et al., 2023) directly separates time steps using commas and separates digits using spaces to construct the text input. However, due to the efficiency issue caused by the autoregressive decoding strategy, their practical use is limited.\nTime Series-Based TSF Foundation Models Self-supervised pre-training a TSF model on the same dataset used for downstream TSF tasks is a well-explored topic (Ma et al., 2023; Zhang et al., 2024), such as denoising autoencoders (Zerveas et al., 2021) or contrastive learning (Woo et al., 2022a; Yue et al., 2022). They follow a similar paradigm to the masked autoencoder (MAE) in computer vision. However, these methods rarely examine the cross-dataset generalization capabilities. Recently, research has shifted towards training universal foundation models, by collecting large-scale time series datasets from diverse domains (Goswami et al., 2024; Liu et al., 2024; Das et al., 2024; Dong et al., 2024; Feng et al., 2024) or generating numerous synthetic time series data (Yang et al., 2024). As a representative method, Woo et al. (2024) collected 27 billion observations across nine domains and trained TSF foundation models of various scales, achieving strong zero-shot performance. However,"}, {"title": "6 CONCLUSION", "content": "In this paper, we explore a novel approach to building a time series forecasting (TSF) foundation model using natural images, offering a new perspective distinct from the traditional text-based and TS-based methods. By leveraging the intrinsic similarities between images and time series, we introduced VISIONTS, an MAE-based TSF foundation model that reformulates the TSF task as an image reconstruction problem. Our extensive evaluations demonstrate that VISIONTS achieves outstanding forecasting performance in zero-shot and full-shot settings, being a free lunch for a TSF foundation model. We hope our findings could open new avenues for further cross-domain research.\nLimitations and Future Directions. As a preliminary study, we employed a basic MAE model and reformulated TSF as a patch-based image reconstruction task. Reformulating TSF as a more refined image inpainting task and utilizing more advanced models like diffusion models (Rombach et al., 2022; Peebles & Xie, 2023) presents a promising research direction."}, {"title": "A DETAILS OF EXPERIMENTS", "content": "Long-term TSF Benchmark We evaluate our model on 8 widely used long-term TSF datasets (Zhou et al., 2021; Wu et al., 2021), summarized in Table 6. We also report the periodicity of each dataset for our segmentation step.\n1. ETTh1/ETTh2/ETTm1/ETTm2 record 7 indicators from electricity transformers between July 2016 and July 2018. Data is collected hourly for ETTh1/ETTh2 and every 15 minutes for ETTm1/ETTm2.\n2. Electricity contains hourly electricity consumption (in KWh) data from 321 customers between 2012 and 2014.\n3. Traffic provides hourly road occupancy rates measured by various sensors on San Francisco Bay area freeways over 2 years, sourced from the California Department of Transportation.\n4. Illness tracks 7 weekly indicators related to influenza-like illness from the CDC in the United States, spanning 2002 to 2021.\n5. Weather includes 21 meteorological indicators, such as temperature and humidity, recorded every 10 minutes throughout 2020.\nMonash Benchmark Following Woo et al. (2024), we tested 29 Monash datasets (Godahewa et al., 2021) using GluonTS (Alexandrov et al., 2020), including M1 Monthly, M3 Monthly, M3 Other, M4 Monthly, M4 Weekly, M4 Daily, M4 Hourly, Tourism Quarterly, Tourism Monthly, CIF 2016, Australian Electricity Demand, Bitcoin, Pedestrian Counts, Vehicle Trips, KDD Cup, Weather, NN5 Daily, NN5 Weekly, Carparts, FRED-MD, Traffic Hourly, Traffic Weekly, Rideshare, Hospital, COVID Deaths, Temperature Rain, Sunspot, Saugeen River Flow, and US Births.\nBaselines The baseline models selected for comparison are briefly described below:\n1. MOIRAI (Woo et al., 2024) is a TSF foundation model trained on the Large-scale Open Time Series Archive (LOTSA), with over 27B observations across nine domains. It has three variants: small, base, and large.\n2. TimesFM (Das et al., 2024) is a decoder-style TSF foundation model, using a large time-series corpus comprising both real-world and synthetic datasets.\n3. Time-LLM (Jin et al., 2024) is a text-based TSF foundation model built on Llama, which reprograms time series data to align with the language modality, keeping the LLM frozen.\n4. GPT4TS (Zhou et al., 2023) (OneFitsAll) is another text-based model based on GPT, fine-tuned for forecasting tasks.\n5. LLMTime (Gruver et al., 2023) encodes time series data to a text sequence, supporting zero-shot forecasting.\n6. DLinear (Zeng et al., 2023) proposes a linear forecasting model, enhanced by seasonal-trend decomposition or normalization.\n7. PatchTST (Nie et al., 2022) uses Transformer encoders with patching and channel independence techniques for improved predictions.\n8. TimesNet (Wu et al., 2023) applies convolution kernels along the time dimension, using temporal decomposition and periodical segmentation to capture temporal patterns."}, {"title": "B ZERO-SHOT FORECASTING", "content": "B.1 HYPERPARAMETERS\nWe conduct hyperparameter tuning on validation sets to determine the optimal context length L. Final used hyperparameters are summarized in Table 7.\nB.2 FULL FORECASTING RESULTS OF THE LONG-TERM TSF BENCHMARK\nTable 8 shows the full results of zero-shot/few-shot long-term forecasting performance. VISIONTS achieves the best results in most cases (32 out of 62), outperforming MOIRAIBase (10 out of 62) and MOIRAILarge (8 out of 62).\nB.3 COMPARISON OF TIMESFM AND LLMTIME\nDue to the step-by-step output of the decoder architecture, the efficiency of TimesFM (Das et al., 2024) and LLMTime (Gruver et al., 2023) are relatively slower. Thus, Das et al. (2024) only reported results for the last test window of the original split. We compared VISIONTS with their results under the same setting, as shown in Table 9. VISIONTS outperforms TimesFM and LLMTime in terms of MAE, indicating that image-based TSF models are on par with or even better than TS-based and text-based models.\nB.4 COMPARISON OF TRADITIONAL METHODS\nIn addition to deep learning models, we also compare traditional methods, including ARIMA, ETS, and two methods that require periodicity as our VISIONTS: Seasonal Na\u00efve (repeating the last period) and Seasonal Avg (similar to Seasonal Na\u00efve but repeating the average of all periods in the look-back window). Due to the high computational cost of ARIMA and ETS, we only compare them on the small-scale benchmarks, i.e., four ETT datasets. Table 10 shows that VISIONTS also achieves the best performance."}, {"title": "C FULL-SHOT FORECASTING", "content": "C.1 TRAINING DETAILS\nBased on the principle of channel independence (Nie et al., 2022; Han et al., 2024), we treat the variables of each time series as individual data samples. We use an Adam optimizer with a learning rate 0.0001 and a batch size 256 to fine-tune MAE. All experiments are repeated three times. The training epoch is one for all the datasets except Illness, for which we train MAE for 100 epochs with an early stop due to the limited training dataset scale. We conduct tuning on validation sets for the three hyperparameters, r, c, and L. The final hyperparameters used are summarized in Table 16.\nC.2 STANDARD DEVIATIONS"}, {"title": "C.3 ABLATION STUDY AND FINE-TUNING STRATEGY COMPARISON", "content": "We compare the following ablation variants to verify the role of the visual model (VM), similar to Tan et al. (2024).\n\u2022 w/o VM removes all the transformer blocks in encoders and decoders.\n\u2022 VM2Attn replaces both the encoder and decoder with a self-attention layer, matching MAE structure but with random initialization.\n\u2022 VM2Trsf is similar to VM2Attn but replaces them with a Transformer block (i.e., a self-attention layer plus an MLP layer).\n\u2022 Rand-VM keeps the same architecture as the vanilla MAE, but all the weights are randomly initialized.\nWe also compare fine-tuning different components in MAE as follows:\n\u2022 All fine-tunes all the trainable weights in MAE.\n\u2022 LN fine-tunes only the layer normalization, which is the default setting used in our experiments.\n\u2022 Bias fine-tunes only the bias term of all the linear layers, proposed by Zaken et al. (2022).\n\u2022 MLP and Attn fine-tune only the feed-forward layer and the self-attention layer, respectively.\n\u2022 Freeze does not fine-tune any weight. Note that it differs from the previous zero-shot experiment, where a longer context length was used (see Table 7 and Table 16).\nThe results are shown in Table 18, suggesting that visual knowledge is crucial for VISIONTS and fine-tuning the layer normalization is the best.\nD VISUALIZATION\nWe visualized the predictions of VISIONTS in the zero-shot setting, including its input and recon-structed images. We also visualized the predictions of MOIRAILarge and Seasonal Na\u00efve, wich their MAE metrics for comparison. Figs. 8 to 10 show examples where VISIONTS performed well, with Fig. 8 depicting a more regular pattern, while Figs. 9 and 10 display less obvious patterns. Fig. 11 illustrates a case where VISIONTS underperformed, as it aggressively predicted the trend despite the lack of clear patterns in the input sequence, whereas MOIRAILarge made more conservative predictions."}, {"title": "Normalization", "content": "MAE standardizes each image based on the mean and standard deviation computed\non ImageNet. Therefore, we apply instance normalization to $I_{raw}$, which is also a standard practice in\ncurrent TSF (Kim et al., 2022). Notably, we observed that normalizing $I_{raw}$ to a standard deviation\nof r, where r is a hyperparameter less than 1, yields superior performance. One explanation is that\nthe magnitude of inputs/outputs during MAE pretraining is constrained by the limited range of color\nvalues. Therefore, reducing the magnitude of $I_{raw}$ prevents exceeding these limits. However, an\nexcessively low r can result in values that are difficult to distinguish. We found that a moderate value\n(0.4) of r performs well across most scenarios (Appendix B.9). Let $I_{norm}$ denote the normalized\nmatrix, which is computed as follows:\n$I_{norm} = r\\frac{I_{raw} - Mean(I_{raw})}{Standard-Deviation(I_{raw})}$."}, {"title": "Alignment", "content": "Our goal is to predict the columns on the right of $I_{grey}$ to forecast the future sequence.\nA straightforward approach is to treat $I_{grey}$ as the visible left portion and the predicted columns as the\nmasked right portion. However, since the image size during pre-training may not match the size of\n$I_{grey}$, we propose to resize $I_{grey}$ to align with the pre-training data. Formally, let the total number of\n2D patches used in pre-training be N \u00d7 N and the size of each patch be S \u00d7 S. We set the number\nof visible patches to N \u00d7 n and the masked patches to N \u00d7 (N \u2212 n), where $n = [N\\cdot L/(L+H)]$ is\ndetermined by the ratio of context length L to prediction length H. We resample the image $I_{grey}$ to\nadjust the size from the original dimensions (P, [L/P]) to (N\u00b7 S, n \u00b7 S), making it more compatible\nwith MAE. We select bilinear interpolation for the resampling process.\nMoreover, we found that reducing the width of the visible portion can further improve performance,\nlikely because the number of input periods ([L/P]) is generally smaller than the period length P.\nTherefore, we propose multiplying n by a hyperparameter $c \\in [0, 1]$ to reduce the width, i.e.,\n$n = \\frac{C\\cdot N \\cdot L}{L+H}$"}, {"title": "Segmentation", "content": "Given a univariate input $X \\in \\mathbb{R}^L$, the first goal is to transform it into a 2D matrix.\nWe propose to segment it into $[L/P]$ subsequences of length P, where P is the periodicity\u00b2. These\nsubsequences are then stacked into a 2D matrix, denoted by $I_{raw} \\in \\mathbb{R}^{P \\times [L/P]}$. This encoding\nstrategy is proved to be efficient by recent work (Wu et al., 2023; Lin et al., 2024), as it allows for\nthe simultaneous capture of both variations within the same period (i.e., intra-period) and across\nperiods with the same phase (i.e., inter-period). Moreover, it ensures that each element in $I_{raw}$ and its\nneighbors align with the spatial locality property of images (Krizhevsky et al., 2012), where nearby\npixels tend to be similar due to the inherent cohesiveness of objects in the real world. Therefore, this\nperiodicity segmentation technique further narrows the gap between TS and images."}, {"title": "Rendering", "content": "It is well-known that each image has three channels. We simply render $I_{norm}$ as a\ngreyscale image $I_{grey} \\in \\mathbb{R}^{P \\times [L/P] \\times 3}$, where all three channels are identical to $I_{norm}$. This choice\nis purely result-driven: in our early experiments, we added a convolutional layer with three output"}, {"title": "Reconstruction and Forecasting", "content": "After obtaining the MAE-reconstructed image, we simply reverse\nthe previous steps for forecasting. Specifically, we resize the entire image back to the original time\nseries segmentations through the same bilinear interpolation, and average the three channels to obtain\na single-channel image. After de-normalizing and flattening, the forecasting window can be extracted."}]}