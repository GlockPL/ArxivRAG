{"title": "Identifying Task Groupings for Multi-Task Learning Using Pointwise V-Usable Information", "authors": ["Yingya Li", "Timothy Miller", "Steven Bethard", "Guergana Savova"], "abstract": "The success of multi-task learning can depend heavily on which tasks are grouped together. Naively grouping all tasks or a random set of tasks can result in negative transfer, with the multi-task models performing worse than single-task models. Though many efforts have been made to identify task groupings and to measure the relatedness among different tasks, it remains a challenging research topic to define a metric to identify the best task grouping out of a pool of many potential task combinations. We propose a metric of task relatedness based on task difficulty measured by pointwise V-usable information (PVI). PVI is a recently proposed metric to estimate how much usable information a dataset contains given a model. We hypothesize that tasks with not statistically different PVI estimates are similar enough to benefit from the joint learning process. We conduct comprehensive experiments to evaluate the feasibility of this metric for task grouping on 15 NLP datasets in the general, biomedical, and clinical domains. We compare the results of the joint learners against single learners, existing baseline methods, and recent large language models, including Llama 2 and GPT-4. The results show that by grouping tasks with similar PVI estimates, the joint learners yielded competitive results with fewer total parameters, with consistent performance across domains.", "sections": [{"title": "1 Introduction", "content": "Multi-task learning (MTL) learns shared representations across tasks and jointly optimizes the losses of all included tasks, which reduces the risk of over-fitting (Caruana, 1997). Compared to single-task learning (STL), MTL has been shown to improve performance and generalization capabilities in many natural language processing (NLP) tasks (e.g. Liu et al., 2019a; Peng et al., 2020; Khandelwal and Britto, 2020; Zhang et al., 2020a). However, empirical results also suggest that MTL is not always effective and naively grouping tasks brings negative transfer (Wu et al., 2019; Alonso and Plank, 2017; Wu et al., 2019; Guo et al., 2020; Fifty et al., 2021; Aghajanyan et al., 2021; Li et al., 2023a). The space of possible task combinations can be massive, and naively searching that space to find the best joint learning models is not efficient (Fifty et al., 2021; Song et al., 2022).\nTo find the best task combinations, some recent studies have developed new optimization methods that focus on measuring the relatedness among tasks (Fifty et al., 2021; Song et al., 2022; Ni et al., 2023; Li et al., 2023a; Ni et al., 2023; Li et al., 2023b). For example, Vu et al. (2020) applied task embedding to predict the transferability of source tasks to a target task. Fifty et al. (2021) compared the inter-task affinity by examining how one task's gradient updates on the shared parameters would influence the objective of another task. Song et al. (2022) leveraged a meta-learning framework on task combinations. Li et al. (2023a) applied surrogate models to identify negative transfers among different groupings during MTL and to identify the best combinations for joint learning. However, finding the optimal task grouping usually involves combining many, if not all, tasks for training and optimization, which becomes computationally intensive as the number of tasks increases. For a deeper understanding of the task-relatedness of MTL in neural networks, researchers also provide initial clues to formalize the definition through measurable variables. Specifically, some work suggests that auxiliary tasks with compact and more uniform label distributions are preferable for semantic sequence prediction problems (Alonso and Plank, 2017). Others found that gains are more likely to occur for main tasks that plateau quickly with non-plateauing auxiliary tasks (Bingel and S\u00f8gaard, 2017). In certain domains like financial NLP tasks, study results show that MTL works well when tasks are related and with diverse skills (Ni et al., 2023)."}, {"title": "2 Related Work", "content": "Nevertheless, we still lack a shared definition of task-relatedness or a metric to measure the amount of cross-task usable information for a given model under the joint learning context.\nThis work studies the use of pointwise V-usable information (PVI) (Ethayarajh et al., 2022) to measure the usable information of different datasets and to jointly train tasks with similar information gains given a model. PVI, recently introduced by Ethayarajh et al. (2022), estimates the difficulty of data instances for a given model in supervised learning. It builds on the predictive V-information framework (Xu et al., 2020) which incorporates mutual information and the coefficient of determination to quantify data instance difficulty. The metric applies instance-level predictions to quantify how much information a given model can extract from a dataset. The higher the PVI estimate, the easier it is for the model to represent a given data point. Under this context, we cast PVI as an estimate of task-relatedness to guide MTL. By grouping tasks according to similar PVI distributions, or in other words, tasks of comparable difficulty, we hypothesize that this approach promotes model generalization across the targeted tasks in MTL.\nTo investigate the feasibility of identifying the best task groupings for MTL using PVI, we conducted experiments with 15 NLP datasets in the general, biomedical and clinical domains. We compared the MTL results with task groupings selected by the PVI estimate distributions against the best-performing fine-tuned single-learner models. The performances were also compared against recent large language models (LLMs), including Llama 2 (Touvron et al., 2023) and GPT-4 (Achiam et al., 2023) which have demonstrated their ability as general-purpose NLP task solvers across a wide range of NLP tasks, either with or without downstream data adaptation (Liu et al., 2023; Brown et al., 2020; Chowdhery et al., 2023; Qin et al., 2023). We also provide comparison to two baseline task grouping methods: task embedding (Vu et al., 2020) and surrogate models (Li et al., 2023a) considered state of the art. Our contributions are summarized as follows:\n\u2022 We introduce a new method to identify task groupings using PVI estimates (Ethayarajh et al., 2022) for MTL.\n\u2022 We provide a thorough empirical analysis across NLP tasks in different domains, demonstrating that our method could effectively find high-performing task groupings that achieve or surpass STL performance.\n\u2022 We offer a new perspective on using PVI estimates for task groupings in MTL highlighting its effectiveness for tasks that fall in roughly the same domain."}, {"title": "2.1 MTL with similar tasks", "content": "Jointly training similar tasks is the main premise of MTL especially in the era of neural models and transfer learning. The more similar shared tasks are, the more hidden units would be shared in a given model, which would potentially benefit the joint training process through these shared representations (Caruana, 1997). In the context of MTL, measuring task similarity as well as automatically and reliably determining the optimal task grouping from numerous possible configurations remains challenging. Zhang et al. (2023) surveyed the efforts in the NLP field for task relatedness and training methods. Overall, the empirical selection of similar tasks remains the most commonly used method, and in most cases, the problem of deciding which tasks to combine for MTL is often left to human experts (Zhang and Yang, 2021). In recent years, a few methods have been developed to automatically select similar tasks (Vu et al., 2020; Fifty et al., 2021; Aribandi et al., 2021; Song et al., 2022; Ni et al., 2023; Li et al., 2023a; Ni et al., 2023; Li et al., 2023b), though a shared definition of task similarity for joint training is still lacking (Zhang et al., 2023). As a result, negative transfers among tasks in joint training have been observed by researchers (Wu et al., 2019; Alonso and Plank, 2017; Wu et al., 2019; Guo et al., 2020; Fifty et al., 2021; Aghajanyan et al., 2021; Li et al., 2023a). It would also be computationally costly to search for the best task grouping by iteratively combining all tasks in pairs or n-task groupings. Therefore, a straightforward and efficient method is needed to identify the best task groupings for MTL."}, {"title": "2.2 PVI and dataset difficulty", "content": "Understanding the difficulty of a task helps guiding the machine learning process, e.g. what architectures and classification methods are feasible (Torralba and Efros, 2011; Zhao et al., 2022; Cui et al., 2023). The significance of dataset difficulty has been widely discussed in the NLP field (Hahn et al., 2021; Perez et al., 2021; Zhao et al., 2022; Gadre et al., 2023). As an extension of the predictive V-information framework (Xu et al., 2020), PVI measures dataset or instance difficulty by the lack of usable information given a model. Algorithm 1 shows how PVI score (Ethayarajh et al., 2022) is calculated (see Section 3 for a detailed description). A high PVI estimate indicates a good representation of the input in the model, and thus the instance is regarded to be easier for the given model. Contrarily, a low PVI estimate indicates the input has less usable information to the model, and the instance is thus considered to be harder. Under the supervised learning context, PVI offers a practical metric that is able to compare the dataset difficulties among different NLP tasks.\nPVI has proven effective across downstream tasks, such as quality estimate for universal dependencies (Kulmizev and Nivre, 2023), informativeness evaluation in reasoning chain (Prasad et al., 2023) and data augmentation for intent detection (Lin et al., 2023). Lu et al. (2023) further adapted PVI to in-context learning for LLMs, illustrating its consistency and effectiveness in this new paradigm. In the current study, we propose to explore this metric as a proxy for task similarity to identify the best task groupings for MTL."}, {"title": "3 Method", "content": "In this section, we describe our proposed method of utilizing PVI for task groupings in MTL. The method consists of two stages: 1) calculating the PVI estimates for each task; 2) grouping tasks based on PVI score distributions for MTL."}, {"title": "4 Experimental Setup", "content": "We conducted experiments on 15 datasets, including 7 general, 2 biomedical, and 6 clinical NLP benchmarks. The details of the datasets, including the data statistics of train, dev, and test splits are shown in Tables 9 and 10 in Appendix A.1."}, {"title": "4.1 Datasets and tasks", "content": "General-domain tasks The general domain tasks include BoolQ (Clark et al., 2019), CB (De Marneffe et al., 2019), RTE (Wang et al., 2019), COPA (Roemmele et al., 2011), WiC (Pilehvar and Camacho-Collados, 2019), CoLA (Warstadt et al., 2019), SST2 (Socher et al., 2013). They are tasks from GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019), commonly used as benchmarks for natural language understanding covering question answering, natural language inference, word sense disambiguation, linguistic acceptability, and sentiment analysis.\nBiomedical tasks We used the Health Advice (Li et al., 2021) and Causal Language (Yu et al., 2019) datasets representing two fundamental tasks in the biomedical domain: classification and reasoning. Health Advice categorizes biomedical literature sentences into \"no advice\", \"weak advice\", and \"strong advice\u201d based on recommendation strength. Causal Language identifies correlational and causal statements in biomedical research findings, labeling each as \"correlation\u201d, \u201cconditional causation\u201d, \"direct causation\u201d or \u201cno relationship\".\nClinical tasks The clinical datasets cover 6 tasks from the THYME corpus (Temporal Histories of Your Medical Events) (Styler IV et al., 2014), SHARP Seed (Seed), and SHARP Stratified (Strat). The THYME corpus contains 594 clinical and pathology notes on colon cancer patients, annotated with different syntactic and semantic information. We used the THYME+ version of the corpus (Wright-Bettner et al., 2020) and worked with two tasks: negation (THYMENeg) and contextual modality (THYMEMod). Negation indicates if an entity or an event is \"negated\u201d or \u201cnot negated\". Contextual modality flags an entity or an event as \"actual\" (an event has happened or is scheduled to happen), \"hedged/uncertain\u201d (an event is mentioned with some degree of hedging/uncertainty), \"hypothetical\" (an event is conditional on some other event, usually introduced by \u201cif\u201d) or \"generic\" (an event is mentioned in a general sense, not related to a specific person). The Seed corpus contains notes from patients with pulmonary arterial disease from the Mayo Clinic and patients with breast cancer from Seattle Group Health Cooperative (now part of Keiser Permanante). The Strat corpus contains notes from patients from the Mayo Clinic, with a variety of specialties and note types representing the entire Electronic Medical Records (EMR). We selected the tasks of Negation (SeedNeg, StratNeg) and Uncertainty (SeedUncert, StratUncert) from the two datasets. All of these clinical corpora are de-identified."}, {"title": "4.2 Models", "content": "For the general and biomedical datasets, the base settings for STL and MTL were based on the setting in Liu et al. (2019b) using roberta-large as the pre-trained model. Though prior work suggests a modest benefit from domain-specific pre-training of the two biomedical tasks (Yu et al., 2019; Li et al., 2021), we used one pre-trained model across the general and biomedical tasks, to allow task combinations across these domains.\nGiven the unique characteristics of clinical language which is the EMR clinical narrative, we used Bio+Clinical BERT (Alsentzer et al., 2019) as the pre-trained model for STL and MTL for the 6 selected clinical tasks. The model was trained on all clinical notes from the MIMIC III dataset (Johnson et al., 2016) atop of BioBERT (Lee et al., 2020) and has been shown to work better for clinical NLP tasks in comparison to other encoder-only BERT-based models (Alsentzer et al., 2019).\nWe compared our approach with two baseline task grouping methods: task embedding (Vu et al., 2020) and surrogate models (Li et al., 2023a). The task embedding method effectively identifies transferability across 33 NLP tasks (Vu et al., 2020). The surrogate model method (Li et al., 2023a) outperforms other existing task grouping strategies in both NLP and computer vision tasks, such as HOA (Standley et al., 2020) and TAG (Fifty et al., 2021).\nTo further evaluate the MTL performance using PVI estimates, we compared the results with recent LLMs including LlaMA2-7B-chat, LlaMA2-70B-chat (Touvron et al., 2023) and GPT-4 (Achiam et al., 2023), using few-shot prompting with the number of shots equal to the number of classes in each task. This comparison aims to determine if LLMs' performance could match or exceed our best MTL models, as LLMs have been claimed to handle various NLP tasks with impressive performance even without downstream adaptation, and thus could be considered a type of multi-task learners. Tables 17 and 18 in Appendix A.2 show examples of prompts used in the study. We limited the evaluation of GPT-4 to experiments on the 7 general and 2 biomedical datasets, due to the HIPAA (Health Insurance Portability and Accountability Act) regulation of the clinical datasets."}, {"title": "4.3 Setting and evaluation", "content": "To obtain the PVI estimates for all datasets, we fine-tuned the base models for 10 epochs with a batch size of 32, and a learning rate of 2e-5 through HuggingFace's Transformer API (Wolf et al., 2020)."}, {"title": "5 Results", "content": "We conducted paired t-tests to compare the PVI estimates of pairs of tasks (group size = 2) using the same model. This analysis yielded 36 task groupings for the general and biomedical domains and 15 groupings for the clinical domain. Table 1 shows the results of task groupings where there is no statistically significant difference in PVI estimates between the grouped tasks (p-value > 0.01) and therefore the two tasks have a similar level of difficulty for the given model.\nFor task groupings with more than 2 tasks (group size > 2), we used one-way ANOVA to examine the difference of PVI estimates across different tasks. Table 2 shows the results of task groupings with similar PVI estimates distribution (p-value > 0.01). We also observed that in group sizes larger than 3, no tasks have similar difficulty levels, as measured by the one-way ANOVA of PVI estimates. This means that among all tasks we experimented with, no more than three tasks are equally challenging for the pre-trained model, i.e. no more than three tasks would fit our criteria for task grouping."}, {"title": "5.1 MTL results of grouped tasks with similar PVI estimates", "content": "To test the feasibility of utilizing task difficulty for task grouping, we jointly trained on the datasets with similar PVI estimates. Tables 3 and 4 show the accuracy and macro F1 results of the single-task learners and joint learning models when combining tasks with similar estimates. For the tasks that have the PVI estimate similar to more than one task, such as CB as shown in Table 1, we selected the grouping with the larger T-statistic values for the joint training.\nFor all general and biomedical tasks the joint learning (MTL) results when grouping two tasks with statistically similar PVI estimates are either similar or better than the STL (margin of error factored in). Based on the accuracy and F1 scores, for the 9 general and biomedical tasks, MTL for groupings (group size = 2) of statistically similar PVI estimates produced results that were either comparable to or better than those of the single learners for all datasets. For WiC, Health Advice, and CoLA when paired with COPA, Causal Language, CB and COPA the MTL results are within the margin of error when compared to the STL results. The CB task has the largest F1-score improvement (\u2206 = 0.257). Similar patterns are also observed with grouping sizes larger than 2 when 3 tasks with similar PVI estimates are combined for joint training.\nResults on the clinical tasks are in Table 4. The observed consistent improvement of PVI-based MTL grouping as compared to single learners may be due to the more constrained and formulaic nature of the language in this field, which better supports task semantics than the broader variability seen in the general domain.\nThe same trends are exhibited when we experimented with other language models (bert-base-uncased, BioBERT) shown in Tables 15-16 in Appendix A.1.\nCompared with the best MTL learning results of the two baseline approaches, shown in Tables 11-14 in Appendix A.1, the method of utilizing similar PVI estimates also shows either the same (e.g., Health advice and Causal Language) or a large improvement among the tasks in different domains. Results for THYMENeg combined with THYMEMod are similar within the margin of error. Our method trends toward better performance when compared to the recent surrogate model method (Li et al., 2023a) reported to outperform other existing task grouping strategies on both NLP and computer vision tasks, such as HOA (Standley et al., 2020) and TAG (Fifty et al., 2021)."}, {"title": "5.2 MTL results of grouped tasks with different PVI estimates", "content": "To further examine how task difficulty may affect the performance of joint learning when grouping tasks by their PVI estimates, we combined tasks with the most statistically different PVI estimates (p-value < 0.01) for joint learning. Tables 5 and 6 show the results on the general, biomedical, and clinical tasks individually. Compared to STL and MTL with task grouping of similar PVI estimates, the performance is much lower. Even though the CB dataset showed a better performance when combined with the Health Advice datasets, it came at a cost for the other task - the performance of Health Advice was lower compared to its single learner result. On the other hand, the results of the Health Advice dataset are similar to those from MTL with similar PVI grouping however it comes at a cost for the the other task (as shown in Table 5).\nWe observed a similar pattern for the tasks in the clinical domain, except for the slight improvement over STL when THYMEMod is jointly trained with THYMENeg, and a better performance of StratUncert when combined with THYMEMod (at cost for THYMEMod). This result concurs with previous findings that naively grouping tasks for joint training brings negative transfer among the tasks, thus leading to the incorrect conclusion that MTL does not work."}, {"title": "5.3 Performance of LLMs on the tasks", "content": "LLMs are intended to provide solutions for many tasks, thus may be considered multi-task learners. Therefore, we compare our approach to the performance of LLMs as a baseline. Table 7 shows the performance of Llama2-7B, Llama2-70B, and GPT-4 on the general and biomedical tasks using few-shot learning. Overall, our best STL and MTL models performed better than the Llama 2 models for all tasks. On the other hand, GPT-4 shows mixed results in terms of performance compared to the best STL and MTL. It outperformed the best MTL results on RTE, CB, COPA, COLA and SST2 but underperformed on BoolQ and WiC. It is unclear to what extent these tasks are included in the training corpus of GPT-4. As for the two biomedical tasks, our best MTL model outperformed GPT-4 by a wide margin (0.234-0.331 F1 points), with much higher performance with MTL with task groupings with a similar PVI-based difficulty given the roberta-large model.\nTable 8 shows the results of Llama2-7B and Llama2-70B models on the clinical tasks. Similar to the biomedical datasets, the STL and MTL models of the clinical tasks outperformed the Llama 2 models, which indicates that for domain-specific NLP tasks, fine-tuning tasks-specific transformer-based models still works better than prompting the LLMs. Moreover, EMR text is highly unlikely to have been included in the training data of any model given the HIPAA provisions, thus it is ideal for independent evaluations of LLMs."}, {"title": "6 Discussion and Conclusion", "content": "In this study, we propose a new method of identifying best-performing task groupings for MTL based on PVI estimates. We conducted experiments on 15 NLP datasets in the general, biomedical, and clinical domains. We showed that when grouping tasks with similar (i.e. not significantly different) PVI estimates, MTL yielded competitive or better results on the majority of the general domain tasks and all biomedical and clinical tasks included in our study. The performance was much better compared to the task grouping with PVI estimates of datasets that are significantly different, suggesting the importance of considering task difficulty as task relatedness and the feasibility of utilizing PVI as a metric for selecting task combinations.\nThough MTL may lead to only minor improvements for some tasks as compared to STL, it is more efficient than STL, given the reduced need for training and deployment of multiple models within the computational environment of the average institution, e.g. medical academic center or hospital. For instance, applying two separate roberta-large models for the two biomedical tasks in our experiment would require tuning around 710 million parameters (355 million for each model). In contrast, MTL with hard parameter sharing involves tuning the shared 355 million parameters once, plus a small percentage of task-specific parameters, with the total number of parameters significantly less than 710 million. While LLMs yielded varying results on the general domain datasets, both STL and MTL models consistently outperformed Llama 2 and GPT-4 models in the biomedical domain. This indicates that for domain-specific tasks, fine-tuned models may remain a preferable option. Due to the computational capacity needed for fine-tuning LLMs and the data privacy regulation when using the LLMs, the PVI-based method of grouping tasks for MTL could be particularly beneficial for domain-specific tasks.\nThis study focuses on task grouping for MTL using PVI. This metric could also be applied for data instance selection to optimize training sets for joint learning. Moreover, investigating parameter sharing and monitoring the variation in weights for both single learners and joint learning models, by ranking parameters based on their change after grouping tasks, represents another avenue for improved MTL's effectiveness and efficiency. We will explore these directions in future work.\nWe believe our proposed approach could be incorporated into frameworks for stacking LMs to algorithmically optimize LM prompting, fine-tuning, augmentation, and reasoning, e.g. DSPy framework4 (Khattab et al., 2022, 2023)."}, {"title": "Limitations", "content": "As noted, one limitation of the current study is that we were not able to test GPT-4 performance on the clinical tasks because we do not have access to a HIPAA-compliant GPT-4 model and are not allowed to transmit data to the GPT4 public API.\nWe chose to experiment with two of the most competitive LLMs - the Llama 2 family of models which are locally downloadable and GPT-4. Experiments with other LLMs can be pursued, however we believe the LLMs results we report in this study are likely representative of the general trends.\nThe results of the single learners are with smaller models (less than 7B parameters) as fine-tuning and PVI estimates are computationally feasible given our computational resources. We encourage those with more abundant computational resources to experiment with our methodology using LLMs as the base models which we believe will further improve our reported results."}, {"title": "Ethics Statement", "content": "The clinical datasets we worked with represent a Limited Data Set where all confidential data are removed except for dates. We did not transmit any part of these clinical datasets to any public API and processed the dataset locally on a HIPAA-compliant server. We have an approved IRB for the study described in this paper."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Experiment details", "content": "We used an NVIDIA Titan RTX GPU cluster of 7 nodes for the single- and multi-task training experiments. We experimented with training epochs of {5, 6, 7, 8, 9, 10}, random seeds of {42, 52, 62, 72, 82}, a max sequence length of {126, 512}, a learning rate of {1e-5, 2e-5}, with a fixed batch size of 32 and gradient accumulation steps of 2. All experiments with LLMs were run using an NVIDIA A100 or through OpenAI API. Table 9 provides details of the datasets we used. Table 10 shows the label distribution of each dataset."}, {"title": "A.2 Example of prompts", "content": "Tables 17 and 18 are examples of the prompts used in the current study for Llama 2 and GPT-4.\nSENTENCE1: Then the silence in the Zoo became complete. Woil stared around him and then suddenly with a push of his wings raised himself into the air, turned, and landed ten feet away on the back of a green bench. Creggan could see that he was afraid and that his fear was making him terribly uncertain.\nSENTENCE2: Woil was afraid\nQUESTION: Is this (0) entailment, (1) contradiction, or (2) neutral?\nANSWER: 0\nSENTENCE1: He's weird enough to have undressed me without thinking, according to some mad notion of the \"proper\" thing to do. Perhaps he thought I couldn't lie in bed with my clothes on.\nSENTENCE2: she couldn't lie in bed with her clothes on\nQUESTION: Is this (0) entailment, (1) contradiction, or (2) neutral?\nANSWER: 1\nSENTENCE1: I hope you are settling down and the cat is well. This was a lie. She did not hope the cat was well.\nSENTENCE2: the cat was well\nQUESTION: Is this (0) entailment, (1) contradiction, or (2) neutral?\nANSWER: 2\nCONTEXT: Dietary interventions in older people were effective in maintaining fruit and fish intake, but this did not lead to a significant reduction in cognitive decline.\nQUESTION: Is this (0) no relationship, (1) correlation, (2) conditional causation, or (3) direct causation?\nANSWER: 3\nCONTEXT: Further research is needed to evaluate clinical applications of the PA, such as a more accurate identification of malnourished cardiac surgery patients.\nQUESTION: Is this (0) no relationship, (1) correlation, (2) conditional causation, or (3) direct causation?\nANSWER: 0\nCONTEXT: The etiology of anemia appears to be iron-related and precipitated by the female sex.\nQUESTION: Is this (0) no relationship, (1) correlation, (2) conditional causation, or (3) direct causation?\nANSWER: 1\nCONTEXT: Diet may influence the pharmacokinetics of ASA, but effects may be through modulation of glycine conjugation rather than glucuronidation.\nQUESTION: Is this (0) no relationship, (1) correlation, (2) conditional causation, or (3) direct causation?\nANSWER: 2\nCONTEXT: Dietary interventions in older people were effective in maintaining fruit and fish intake, but this did not lead to a significant reduction in cognitive decline.\nQUESTION: Is this (0) no relationship, (1) correlation, (2) conditional causation, or (3) direct causation?\nANSWER: 3"}]}