{"title": "Mantis Shrimp: Exploring Photometric Band Utilization in Computer Vision Networks for Photometric Redshift Estimation", "authors": ["ANDREW W. ENGEL", "NELL BYLER", "ADAM TSOU", "Gautham NarayAN", "EMMANUEL BONILLA", "IAN SMITH"], "abstract": "We present Mantis Shrimp, a multi-survey deep learning model for photometric redshift estimation that fuses ultra-violet (GALEX), optical (PanSTARRS), and infrared (UnWISE) imagery. Machine learning is now an established approach for photometric redshift estimation, with generally acknowledged higher performance in areas with a high density of spectroscopically identified galaxies over template-based methods. Multiple works have shown that image-based convolutional neural networks can outperform tabular-based color/magnitude models. In comparison to tabular models, image models have additional design complexities: it is largely unknown how to fuse inputs from different instruments which have different resolutions or noise properties. The Mantis Shrimp model estimates the conditional density estimate of redshift using cutout images. The density estimates are well calibrated and the point estimates perform well in the distribution of available spectroscopically confirmed galaxies with (bias = 1e-2), scatter (NMAD = 2.44e-2) and catastrophic outlier rate (n=17.53%). We find that early fusion approaches (e.g., resampling and stacking images from different instruments) match the performance of late fusion approaches (e.g., concatenating latent space representations), so that the design choice ultimately is left to the user. Finally, we study how the models learn to use information across bands, finding evidence that our models successfully incorporates information from all surveys. The applicability of our model to the analysis of large populations of galaxies is limited by the speed of downloading cutouts from external servers; however, our model could be useful in smaller studies such as generating priors over redshift for stellar population synthesis.", "sections": [{"title": "1. INTRODUCTION", "content": "Measuring redshifts to distant galaxies is fundamental to extragalactic astronomy and cosmology research. The gold standard for redshift measurements is spectroscopic observations, but spectroscopy is expensive and time-consuming compared to photometric observations. Algorithms that use photometry to estimate redshift have emerged as an alternative to match the scale of on-going and future photometric surveys. Obtaining more accurate photometric redshifts is in fact necessary to complete these survey's science objectives as errors associated with them are still the leading term of uncertainty in multiple cosmological analyses, including weak lensing . See Newman & Gruen (2022) for a recent review.\nThere are two historical strategies to estimate redshift from photometry: template fitting approaches and empirical approaches. In template fitting approaches, a library of galaxy spectra is convolved with observatory photometric bandpass filters to measure the expected observed flux as a function of redshift. These expected fluxes are compared with the observed flux to constrain the correct galaxy spectral model and its redshift. A recent development in this direction has been the use of stellar population synthesis models to replace a static library of galaxy templates for this task, allowing both galaxy properties and redshift to be simultaneously fit. In contrast, empirical approaches use machine learning algorithms to learn the mapping between observed photometry and redshift from a labeled data set collected from spectroscopic surveys. It is generally acknowledged that in areas of high label density (i.e. within the targeting criteria of the major spectroscopic surveys and consequently nearby redshift), these empirical approaches tend to outperform template fitting models . In this work, we will focus our attention onto empirical approaches.\nPrevious empirical photometric redshift works have investigated many kinds of machine learning algorithms, but recent works have utilized computer vision models (e.g., Convolutional Neural Networks or CNNs) to estimate the redshift directly from cutouts, rather than from derived features available in catalogs. In these works, computer vision networks show replicable improved performance across the range of community accepted point-estimate metrics (summarized in section 6.1) in the SDSS main galaxy sample over tabular models. In particular, Pasquet et al. (2019), hereafter P19, is an influential work that first showed major improvements over the official SDSS photometric redshift algorithm . Computer vision models are believed to exceed tabular performance because they bypass manual feature extraction, allowing artificial intelligence to learn directly from image pixels and the full two-dimensional variation in galaxy surface brightness, size, and morphology . However, it has not been shown whether computer vision models still outperform tabular models in a higher redshift regime, which is what we will explore in this work.\nPrior works have also demonstrated that deep learning models can enhance photometric redshift estimation by combining observations across multiple surveys, but have been limited to exploring tabular models. Notably, Beck et al. (2022) (hereafter B22) showed that integrating WISE with the PanSTARRS catalog improves upon their previous work using PanSTARRS alone . This study extends B22's work by incorporating GALEX, PanSTARRS, and WISE photometry and utilizing image cutouts instead of tabular data.\nOur choice to fuse cutouts from various surveys forms a unique challenge that has not been addressed in prior works: what strategies can be used from the computer vision literature to combine images of varying pixel resolutions, depths, from different photo-reduction pipelines? Furthermore, how do these different strategies compare against each other in both model behavior and performance? Specifically, we investigate two such strategies: we can resample images of different pixel resolution to a common pixel scale (early fusion) or keep the native resolution and combine a latent feature vector specific to each survey together (late fusion). It can be difficult to ascertain whether a multi-modal model is leveraging all the available information across each photometric band . One aim of this work is to evaluate the trade-offs and behavioral differences between these fusion approaches in the context of a model for photometric redshift estimation. For a recent review of modality fusion in computer vision, see Summaira et al. (2021).\nOne unique advantage that computer vision models hold over tabular models are that computer vision methods do not rely upon a detection of an object in each survey to make a valid join between catalogs. For example, if a source is detected in PanSTARRS, but is not detected in WISE, a catalog join between the two surveys would possibly match to the nearest but incorrect source in WISE, or simply have missing flux values. Computer vision models bypass photo-reduction pipelines to be run at arbitrary user-given coordinates. This is an additional freedom that we leave exposed to users, something that we call a \"forced photo-z\". This freedom comes with additional responsibilities that we leave to the user to evaluate the context of our model's predictions in terms of blending, low signal-to-noise, etc, that we do not consider in this work.\nGiven the increasing prominence of computer vision in the physical sciences plus the rise of large language models whose ability to be partners in the scientific process are currently being evaluated , an emerging topic of research is in defining the role domain scientists can play in the evaluation of AI models. In this work, we qualitatively explore the utility of each photometric band to the importance we measure each band plays for our AI model's photometric redshift estimate. We employ Shapley Values to measure feature importance , which have become a popular tool in AI interpretability. Previous works in the field have attempted to explain the performance of photometric redshift algorithms through feature importance , but did not compare to the expected behavior of the model given domain knowledge. Additionally, prior works investigated the use of Shapley value interpretability measures for cosmology, but not specifically for photometric redshift estimation. This specific framing of comparing the expected behavior given domain knowledge to the behavior we infer from Shapley values is unexplored in prior works.\nTo summarize, we present Mantis Shrimp, a multi-modal CNN for the estimation of photometric redshifts and the associated conditional density estimates (CDEs). We will use a convolutional neural network to combine images from the GALEX, PanSTARRS, and WISE surveys. We match image cutouts from each survey centered on galaxy coordinates with matching spectroscopic redshift labels, compiled from a list of major spectroscopic surveys. Our model predicts a completely non-parametric CDE of redshift given the cutouts, which we can use to make point estimates and perform uncertainty quantification. In contrast to previous studies, our model learns to extract features directly from images from multiple surveys. We make our unique multi-modal galaxy cutout dataset available to download via PNNL's DataHub. We make our model training and data collection pipeline available as an open-source GitHub repository. We host a web-app where users may evaluate our model by simply uploading a coordinate. We hope that the ease-of-use of our web-app and data already formatted for deep learning applications is an additional value add for the community."}, {"title": "2. BACKGROUND", "content": ""}, {"title": "2.1. The Photometric Redshift Conditional Density Estimation Task", "content": "Our goal is to model a density estimate of the spectroscopic redshift conditioned on the observed cutout data (and implicitly our training). The advantage of modeling the density rather than just providing point-estimates is that we can compute confidence regions and follow-on analyses can leverage the full complexity of our non-parametric density estimate (e.g., using our density estimates as a prior for redshift in stellar population synthesis modeling ). We formally introduce the photometric redshift density estimation task in the following paragraph. See Dalmasso et al. (2020) for another formal introduction to conditional density estimation for this task.\nWhile the Mantis Shrimp dataset is composed of paired cutout images and scalar spectroscopic redshifts, the photometric redshift conditional density estimation task is more easily framed as a supervised classification task, and so in what follows we will describe a dataset formed from cutouts and class labels. We construct classes by binning redshifts into C = 400 classes. Given a choice of maximum redshift $Z_{max} = 1.6$, the bin-width is $dz = \\frac{Z_{max}}{C}$. In what follows let $\\mathcal{E}_c = \\{e_1, e_2, ..., e_c\\}$, where $e_i$ is the i-th standard normal basis vector in $\\mathbb{R}^C$, or equivalently in regular machine learning parlance is a one-hot encoded label of class i.\nLet $D$ be a labeled dataset of length N composed of individual data-label pairs drawn from the population of high confidence spectroscopically identified galaxies, $(x_i, C_i) \\in (\\mathcal{X},\\mathcal{E}_c)$ with data in the space of hyperspectral images, $\\mathcal{X} \\in \\mathbb{R}^{B\\times H \\times W}$, and where B is the number of photometric bands (B=9), such that $D \\\\= \\\\{(x_1, C_1), (x_2, C_2),..., (x_N, C_N)\\}$. The class label $c_i$ is determined by the true spectroscopic redshift $z_i$ as $c_i = e_j | j = floor[\\frac{z_i}{dz}]$. Our neural network function F is a parameterized and learnable mapping from galaxy hyperspectral image cutouts to a redshift latent vector, $F: \\mathcal{X} \\rightarrow \\mathcal{Y}$ with $\\mathcal{y}\\subseteq \\Delta^C$, where $\\Delta^C$ is the C-dimensional unit simplex. Let $\\theta$ be the vector of learnable parameters. The output of the network is a vector where each element $y_j$ is approximately the probability that the sample has true redshift in a small redshift bin, $y_j \\approx P(dz \\cdot (j - 1) < z_i \\leq dz \\cdot j)$. We will evaluate whether the values of $y$ are well-calibrated to reflect the true probability empirically. The weights $\\theta$ are learned via gradient descent to minimize the cross entropy loss evaluated between $y_i$ and $c_i$. Using this interpretation of $y_i$, we define our point estimate of redshift, $\\hat{z}_i$, as the expected value $\\hat{z}_i = E(y_i)$.\nAs a final note, $y$ as defined above is formally a probability mass function; however it is common practice to interpolate values on $y_i$ to define a probability density function that can be evaluated at arbitrary redshift along the interval (0, Zmax) . This requires a re-normalization to satisfy $\\int y_i dz = 1$. We will continue to refer to $y_i$ as a conditional density estimate and in-fact report values from our web-app with this re-normalization applied by default."}, {"title": "2.2. Galactic Radiation and Photometric Redshifts", "content": "As part of our analysis into the behavior of our neural network models, we will investigate how the models use information from each photometric band and whether that use agrees with our expectations given astronomical domain knowledge. To make this assessment, we will briefly introduce some background on galactic radiation. To understand photometry, we introduce a model of observed flux in a band as the convolution of a galaxy's observed spectral energy density (SED) and the transmission curve of that band.\nGalaxy spectra are the synthesis of all radiative and absorptive processes in a galaxy, dominated by the stellar population present in the galaxy. The result of this synthesis is a roughly bell-shaped SED, with upper tail at a rest frame wavelength of 912\u00c5, (the Lyman limit, caused by the ionization energy of the neutral hydrogen atom), or 4000\u00c5 for galaxies particularly bereft of young hot UV emitting stars . The lower tail of the spectra occurs from 1\u00b5m - 5\u00b5m, and unlike the upper tail, this IR feature is common to many galaxies independent of Hubble type or star formation history .\nWe can model the total flux density observed, $\\Phi$ [Jy], from an object in any particular photometric band with transmission curve R(\u03bb) [unitless] and galaxy's SED $F_\\lambda(\\lambda)$ [Jy m-1 s-1] as:\n$\\Phi = \\frac{\\int F_\\lambda(\\lambda) R(\\lambda)d\\lambda}{\\int R(\\lambda)d\\lambda}$ (1)\nThe observed SED redshifts as $\\lambda_{obs} = \\lambda_{emit}(1 + z)$, but the photometric band transmission curves stay fixed in our observation frame.\nWe visualize an example Galaxy SED, labeled with the relevant major features and photometric bands used in this work, in Figure 1. As an SED becomes redshifted, energy density leaves the UV/Optical bands and moves into the IR bands. This overall shift can explain why the $r - W_1$ color is known to be a good predictor of redshift, and in fact is used in DESI spectroscopic targeting. Individual features such as the 4000\u00c5 break move through filters successively. At low redshifts, the 4000\u00c5 break should be important as the break transitions through the g band and into the r band, making the color g-r predictive of redshift, until about z=0.4, when the 4000\u00c5 break transitions out of the g band ."}, {"title": "2.3. Shapley Values For Band Importance", "content": "To analyze how the model uses different photometric bands over the range of redshifts we will perform a Shapley value analysis. Shapley values have seen a resurgence in popularity as a tool for neural network explainability . As intuition, Shapley values were originally introduced to measure how \"valuable\" a player is to a team in an N-person game ; in analogy, we will use Shapley values to measure how important the flux from the targeted galaxy in each photometric band is to the point estimation of redshift. Shapley values are computed by ablating features then observing the effect of that ablation on the point estimate of redshift. This comparison is performed across all possible permutations of feature-ablation to understand the synergy between features. In our implementation we will define the flux of the target galaxy in each band as the feature that we will ablate. We describe the specifics of our implementation in Section 5.1, but for now we formally introduce Shapley values.\nAssume we have a hyper-spectral image dataset, with samples $x_i \\in \\mathcal{X}$. Recall that B is the total number of bands, and let the j-th band of $x_i$ be $x_i^j$. As mentioned above, Shapley values ablate features, and so for clarity let there be a baseline image $b_i \\in \\mathcal{B} \\subseteq \\mathbb{R}^{B \\times H \\times W}$ which contains the \"feature ablated\" images found by a feature ablation function, $A : \\mathcal{X}^i \\rightarrow \\mathcal{B}$, which maps the original observed band image to the baseline, $A(x_i) = b_i$.\nNext, we define S to be a subset of the channel indices, $S\\subseteq \\{1,...,B\\}$, where the set S contains the indices of bands to be left unmodified in a forward pass of the model. Let $\\mathcal{P}(S)$ be the power-set of S, such that $\\mathcal{P}(S) = \\{\\{\\emptyset\\},\\{1\\},\\{1,2\\}, . . ., \\{1, . . ., B\\}, . . ., \\{2\\}, \\{2, 3\\}, ...,\\{B\\}\\}$. Furthermore, we will slightly abuse notation and allow $\\mathcal{P}(S)/\\{j\\}$ to mean the powerset of S but with subsets that included the integer j having been removed. We can then define a feature replacement function, $H : (\\mathcal{X},\\mathcal{P}(S)) \\rightarrow \\mathcal{X}\\cup\\mathcal{B}$, which replaces the bands whose indices are not in S with the baseline image bands, $H(x_i, S) = x_i | x_i^j = b_i \\forall j \\notin S$. We define the value of including bands, $V(S)$, to be equal to the model's point prediction of redshift on the modified i input. For clarity, $V(S) = E[F(H(x_i, S), \\theta)]$."}, {"title": "3. DATA", "content": "We provide a brief overview of the Mantis Shrimp dataset in this introductory section then go into greater detail in the following subsections. The data consists of N = 4.4 \u00d7 106 9-band image cutouts of galaxies paired with ground-truth redshifts compiled from a diverse set of spectroscopic surveys. Although spectroscopic redshifts are the most secure method of identifying redshift, we employ cuts based on provided spectroscopic survey reported quality to ensure a high-confidence sample of labels for our model, following a similar methodology as in B22. In Figure2 we visualize the distribution of targets and the relative distribution of targets.\nTo construct our image inputs, we query public APIs of image cutout servers for each of the three instruments, centered on the coordinates provided by the spectroscopic surveys. We include 2 UV bands from GALEX , 5 optical bands from PanSTARRS , and 2 IR bands from UnWISE . We describe in greater detail the collection of photometry data in appendix D, while image pre-processing and our augmentation pipeline are detailed below. To summarize: we ensure our images are on a quasi-logarithmic flux scale , then map all pixels with flux less than zero to zero. It has been shown that quasi-logarithmic scales for astronomical images work well in convolutional neural networks for segmentation tasks . Where our pipeline encounters NaNs (typically caused by bright-source masks in the PanSTARRS cutouts), we replace with zero. We show the on sky distribution of labeled galaxies in Figure3."}, {"title": "3.1. Spectroscopic Survey Compilation", "content": "Spectroscopic samples were compiled from available surveys and combined together, including the Sloan Digital Sky Survey (SDSS) at data release 17, including that of the Main Galaxy Survey (MGS) , the Baryonic Oscillation Spectroscopic Survey (BOSS) , and the extended Baryonic Oscillation Spectroscopic Survey (eBOSS) . We include the Dark Energy Spectroscopic Instrument survey DESI where at time of writing, only the early data release is publicly available. SDSS and DESI make up the vast majority of our sample. We also include measurements from DEEP2 , GAMA , VVDS , VIPERS , 6dF , and the WiggleZ surveys. Information on the quality cuts used for each survey are provided in Table 1, including the final total amount of labels taken from each survey. Our quality cuts and compilation of survey data closely match that of B22 (described in (Beck et al. 2021)); our main advantage is the inclusion of the sizable DESI EDA spectroscopic sample, adding some 1 million additional targets for training. The targeting guidelines of each of the spectroscopic surveys essentially create a biased and incomplete sample of the input feature space with respect to the entire population of photometrically observable galaxies . We do not consider here how to correct for or flag examples that fall outside the support of the training dataset. We do provide a brief introduction to each survey used in this study in appendix C, including general information about survey goals and what populations they specifically targeted. Spectroscopic catalogs report the target coordinates alongside redshift, so we simply query cutout services at the reported coordinates to build the mantis shrimp dataset."}, {"title": "3.2. Image Processing", "content": "We combine photometric cutout data collected from GALEX , PanSTARRS , and WISE observations (the latter re-processed by UnWISE for filters W1, W2) for a total 9 different bands of photometry. The Photometric bands are summarized in Table 2. We chose the PanSTARRS survey for its depth, large footprint, and to ensure our model would be compatible with ongoing surveys using the PanSTARRS telescope and related host-galaxy identification software , then chose GALEX and UnWISE surveys for their nearly all sky completion at complimentary wavelengths. Future work would investigate switching PanSTARRS for DECALS optical imaging, when available.\nWe are notably missing any near-IR wavelengths, (e.g., that could be provided by 2MASS , UKIDSS , or VISTA ); specifically, 2MASS data is shallow compared to the depth of our targets, while UKIDSS and VISTA are not an all sky survey so a relatively small number of our spectroscopic targets would have available data. In this work we are interested in explaining the behavior of a model that could reasonably be deployed on a wide footprint of the night sky. Ongoing or up-coming surveys in the NIR (Euclid and SPHEREX ) would make excellent additional photometric inputs to our work, but were unavailable at time of writing."}, {"title": "3.3. Image Pipeline", "content": "Our dataset is randomly split into 16 nearly-equal sized chunks, and further split into static train-validation-test arrays comprising 70%-10%-20% of each chunk. We then use the FFCV library to compile each chunk and speed up the rate of data loading . During training we use data-augmentation to increase the effective number of samples . We use random flips and rotations of the images then crop around the centers so that each spans a constant 30 arcsec width and height. We had queried each cutout API so that this crop removes any undefined pixels from the rotation. We also randomly sample from our training data with replacement, re-weighting samples from each class to ensure that each class is sampled with the same probability."}, {"title": "3.4. Data Availability", "content": "The entire dataset, accompanying documentation and tutorials are made available online via PNNL DataHub, with URL found at the beginning of this article. The dataset includes our image vectors as numpy binary FP32 arrays, spectroscopy tables and indices as comma separated value (.csv) files, and other necessary index and mask metadata to make use of the dataset. We also include our compiled FFCV datasets which automatically perform all pre-formatting for the user, and can be used in combination with the PyTorch dataset software created for this project."}, {"title": "4. NEURAL NETWORKS", "content": ""}, {"title": "4.1. Model Architecture", "content": "This study evaluates the effectiveness of early and late fusion models; in this section we will describe both architectures and any specific modifications to the image pipeline. Recall that in early fusion, images are up-sampled to a common pixel grid and then stacked on top of each other, while in late-fusion the cutout from each survey is kept separate and fed through a unique embedding model. The resulting embedding vector is then concatenated together and fed to a dense head. See Figure 4 for a visual depiction comparing the two architectures.\nOur early fusion model is a ConvNext-Large architecture with the input layer modified to accommodate our hyper-spectral images. We initialize our weights using the ZooBotV2 embedding model weights . To adapt the weights to our architecture, we take an average of the original first convolutional layer across the channel dimension, divide this average by 9, and then replace each of the channel-weights with this reduced average. This choice is reasonable given the convolutional operation performs a sum across channels, so we should approximately retain the same embedding statistics; however, we do not particularly expect the choice to be of great importance as we perform end-to-end finetuning.\nFor our late fusion model, we use a ConvNext-Small encoder for the GALEX and UnWISE images combined with a ConvNext-Large encoder for the PanSTARRS images. Because there is less weight sharing compared to the early-fusion model, the late-fusion models has a higher trainable parameter count. Our ConvNext-Large encoder is initialized using the same grey-scale weights as the early-fusion model, with the same scheme for modifying the first layer to accept 5-band hyperspectral images."}, {"title": "4.2. Accounting for Foreground Dust", "content": "Interstellar dust along the line of sight between Earth and the target galaxy preferentially scatters blue-light, which has the effect of reddening the observed color of the galaxy . This effect is called dust extinction and can be confused with the reddening due to redshift. Following P19, the dust extinction is incorporated into the neural network following the image-pipeline through feature vector concatenation. Following B22, we concatenate both the extinction from the corrected SFD map and the extinction measured from the Planck 2016 map , both provided by the dustmaps library ."}, {"title": "4.3. Hyperparameter Tuning", "content": "Training neural networks is a process by which model parameters are successively updated to minimize an objective function. ML practitioners typically delineate these learnable model parameters from \"hyperparameters\" that are set as part of the training algorithm and not updated through the optimization process, e.g., the optimizer's step size . The value of these hyperparameters have a large influence on the outcome of training, so careful tuning of these hyperparameters must be performed to search for optimal values .\nWe perform hyperparameter tuning by searching over learning rate, hidden dimension size of the classification head, the output dimension (which sets the bin-width of the vector that spans redshift space), and the weight decay parameter, using a simple grid search. We separately run hyperparameter tuning for both our late fusion and early fusion pipeline. Models are evaluated using the performance on the validation set as measured by the point prediction metrics introduced in section 6.1. We then train models using the best hyperparameters from both the early and late fusion runs of the tuner. Table 3 show the values we search over. All hyperparameter tuning runs are allowed a maximum of 60 training epochs."}, {"title": "4.4. Training", "content": "Using the best hyperparameters selected from both our early and late fusion hyperparameter tuning runs (shown in Table 3), we train a fiduciary model combining each of our photometric surveys. We use the NAdam optimizer to minimize cross entropy loss. We train our models for 150 epochs, stopping after an estimated 2 days of total training time on 16 NVIDIA A100 GPUs (40 GB VRAM) distributed across 2 compute nodes. We anneal the learning rate throughout training using a learning rate scheduler tracking the validation cross entropy loss, which reduces the effective learning rate as LRnew = LR0*0.5 if the validation loss is not seen to improve over a period of six epochs. We use a batch size of 16. We sample from our training dataset using a weighted random sampler with weights set to provide a uniform sampling over the class occurrences. Because we sample from the training data with replacement, its possible to see the same sample more than once per epoch. We set a static 1000 total batches per epoch. We track both the loss and our evaluation metrics at regular intervals on the validation set.\nAfter training fiducial models that combine all the photometric bands together, we train three more sets of models where we ablate the IR, the UV, or both the IR and UV in order to better understand if/how our model utilizes the information from each band. This ablation is performed by simply removing the bands from the data input in the case of the early fusion model. For the late fusion model both the data and the encoders used to learn latent feature vectors are removed in this case, we note that the total number of model parameters does change with this ablation, which creates a confounding variable. This is unfortunately unavoidable."}, {"title": "5. INTERPRETABILITY", "content": "In this section, we share our specific implementation of Shapley values. Much of the discussion centers on the choice of baseline image $b'$, or how different choices of $x'$ can be used to help probe different model behavior."}, {"title": "5.1. Shapley Values:", "content": "In this work, we apply Shapley values to understand the contribution from individual photometric bands. For each datapoint, we can modify each band individually to suppress information about the target galaxy from that band, then measure the resultant change in the predicted redshift from our ablation. The exact choice of how to modify the input band is critical and affects the interpretation of the result . Luckily, by applying our domain knowledge, we will introduce a well-motivated choice for the creation of ablated baseline images, $b'$, that can be easily constructed using familiar astronomical python libraries.\nWe modify the input band by using the sep python library , a python-binding of Source Extractor , to identify the sources in an image with a 2$\\sigma$ detection limit. Because we know that the cutouts were centered on the astrometry of the target galaxy, we take the object whose position is nearest to the center of the cutout as the target galaxy. If no galaxy is detected within a 5 pixel length to the center, we instead use a static aperture which we describe in the next paragraph. The sep library returns best-fit ellipses to the objects: using the ellipse of the target galaxy, we mask out the target galaxy by scaling the ellipse to 4x the Kron radius measured in each band . We then fill in this mask with Gaussian noise centered at the median and with standard deviation equal to the normalized median absolute deviation of the image, with these summary statistics computed only on pixels not containing a source. We allow the resulting image to represent the ablation of features of the target galaxy in that band. When computing Shapley values, we modify our image by replacing the original band images with the feature ablated band image. This choice keeps the pixel values of other sources and the background noise static. Because Shapley values are coupled to the difference between the NN output on original and feature-ablated images (see (2)), this ensures only pixels related to our target galaxy influence the Shapley value.\nAs mentioned in the preceding paragraph, we use a threshold on the maximum allowed pixel distance of a source from the center of the cutout to be considered a detection of the target galaxy. If no sources are detected satisfying this constraint, we instead use a small circular aperture of pixel radius 2,4,2 for GALEX, PanSTARRS, and WISE, respectfully. This small aperture choice is made to eliminate any possible localized central luminosity that exists below our 2$\\sigma$ detection threshold. We then proceed to mask out this central region and fill with Gaussian noise sampled from the image."}, {"title": "6. METRICS", "content": ""}, {"title": "6.1. Evaluation Metrics:", "content": "Recall that our model produces a probability density of redshift for each input, and that we define a point-estimate from these probability densities as the expectation value. Following standards in the field we evaluate our model using both point-like metrics and evaluate the probability densities produced by our model via cumulative density calibration metrics. For the point based metrics, we define the following using the set of scaled residuals $r_i = \\frac{z_i-\\hat{z}_i}{(1+z_i)}$, and allow $r_i$ to be the i-th element of that set. In the following let med(.) represent the median over a set. We define a robust measure of spread, the scaled median absolute deviation (NMAD) = 1.4826 \u00d7 med(|ri \u2013 med(ri)|), the bias of the residuals (BIAS) = \\frac{1}{N} \u00d7 \u03a3ri, and the percentage of catastrophic outliers (\u03b7) defined to be the percentage of scaled residuals with values greater than 0.05.\nWe report uncertainties in these point estimate statistics by resampling the set of residuals using the bootstrap strategy with 100,000 resamples. Other sources of uncertainty are not factored into the values we report: one important factor often overlooked would be the variation induced by randomness in the gradient descent training process and model initializations. Understanding the effects of early and late fusion rigorously would require drawing samples from the distribution of models that could be trained from either choice, i.e., training multiple models from scratch with different seeds . However, sampling from the distribution of models this way is very expensive on the scale of this work and beyond the scope of this pathfinder study.\nConditional density estimation and proper calibration is an active area of study in photometric redshift estimation. Generally, neural networks do not accurately quantify uncertainty . Prior works in the photometric redshift field  have evaluated calibration using the probability integral transform (PIT) , and a \"CDE loss\" ; we will introduce both in turn in the following paragraphs.\nThe PIT is a histogram of the occurrences of true redshift in the set of CDFs measured from our conditional density estimates. While in truth our model estimates discrete probability mass functions, in the following we will allow these to approximate the probability density function F(xi, \u03b8) \u2261 pi(z), so that we can define the cumulative density as $CDF(z_i, p_i) = \\int^z_0 p_i(z') dz'$. If pi(z) equals the true PDF, pi(z), then the histogram visualizing the density of the set of CDFs evaluated at the true redshift, $\\{CDF(z_0, p_0),...,CDF(z_N, p_N)\\}$ will be flat. It is important to remember that PIT is evaluated on the entire ensemble of our test dataset and does not guarantee the probabilistic calibration of any individual estimation.\nAs a single scalar metric of probabilistic calibration we will also calculate the conditional density estimate loss (CDE loss) test statistic . The CDE loss is an approximation to the squared error between the true conditional density of redshift and the estimate our model produces, up to a constant that depends on the true conditional density. Both the PIT visual metric and the CDE loss are included in the LSST RAIL standard suite of benchmarks .\nFinally, we also re-calibrate our final model using the CalPIT library . This re-calibration technique trains an auxiliary network to learn a local (in feature space) calibration of the initial CDE estimate to improve overall performance on our total CDE calibration. So as not to confound the effects of our ablation study, we use this re-calibration only on our final model that we expose to users in the web-app. Users of our web-app may optionally apply the calibration from CalPIT. We provide additional detail on CalPIT and re-evaluation in appendix \u0412."}, {"title": "7. RESULTS", "content": ""}, {"title": "7.1. Early vs Late Fusion:", "content": "To compare the effectiveness of early vs late fusion architectures, we perform two independent hyperparameter searches then train both an early fusion and late fusion model at the best set of values found during the hyperparameter search. We then ablate the different surveys and re-train, which allows us to directly evaluate wheter adding additional surveys together will increase the overall performance of the model. We compare models in Table 4 using the suite of metrics defined in Section 6.1, which includes the scaled median absolute deviation (NMAD), the bias of the residuals, and the percentage of catastrophic outliers, \u03b7, and the conditional density estimate (CDE) loss.\nThe first key takeaway from our results is that the early and late fusion architectures show very similar performance improvements in each group of ablation experiment. In fact we will show in the following sections that overall the early and late fusion models perform and behave very similarly. The inclusion of IR data makes a more significant improvement over the optical only model than the inclusion of the UV data; however, we also show that including both UV + Optical + IR gives marginally better results than only IR + Optical. Most importantly, these performance improvements indicate that each fusion strategy is successfully integrating data from each survey. The fact that the UV data is not as useful as the IR data might be expected given the depth of GALEX and relative low"}]}