{"title": "VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation", "authors": ["Mingzhe Zheng", "Yongqi Xu", "Haojian Huang", "Xuran Ma", "Yexin Liu", "Wenjie Shu", "Yatian Pang", "Feilong Tang", "Qifeng Chen", "Harry Yang", "Ser-Nam Lim"], "abstract": "Current video generation models excel at generating short clips but still struggle with creating multi-shot, movie-like videos. Existing models trained on large-scale data on the back of rich computational resources are unsurprisingly inadequate for maintaining a logical storyline and visual consistency across multiple shots of a cohesive script since they are often trained with a single-shot objective. To this end, we propose VideoGen-of-Thought (VGoT), a collaborative and training-free architecture designed specifically for multi-shot video generation. VGoT is designed with three goals in mind as follows. Multi-Shot Video Generation: We divide the video generation process into a structured, modular sequence, including (1) Script Generation, which translates a curt story into detailed prompts for each shot; (2) Keyframe Generation, responsible for creating visually consistent keyframes faithful to character portrayals; and (3) Shot-Level Video Generation, which transforms information from scripts and keyframes into shots; (4) Smoothing Mechanism that ensures a consistent multi-shot output. Reasonable Narrative Design: Inspired by cinematic scriptwriting, our prompt generation approach spans five key domains, ensuring logical consistency, character development, and narrative flow across the entire video. Cross-Shot Consistency: We ensure temporal and identity consistency by leveraging identity-preserving (IP) embeddings across shots, which are automatically created from the narrative. Additionally, we incorporate a cross-shot smoothing mechanism, which integrates a reset boundary that effectively combines latent features from adjacent shots, resulting in smooth transitions and maintaining visual coherence throughout the video. Our experiments demonstrate that VGoT surpasses existing video generation methods in producing high-quality, coherent, multi-shot videos.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in video generation techniques have yielded impressive results, particularly in creating short, visually appealing clips [6, 8, 9, 28, 69]. These advancements have been powered by increasingly sophisticated generative models, ranging from diffusion models [6, 29, 49, 56] to auto-regressive models [20, 39, 62, 65], supported by large-scale datasets [31, 50, 51]. These methods have enabled the generation of high-quality, realistic short videos. However, generating multi-shot videos from a brief user input script remains a substantial challenge. Despite existing works successfully achieving long-term video generation, the primary focus has been on extending the duration of single-shot videos [19, 35, 40, 61]. In contrast, multi-shot video generation involves additional complexities in ensuring coherence across different shots. Moreover, while simply scaling up models or using larger datasets, as done in single-shot long video generation methods [45, 63], can enhance the visual quality of individual clips, it requires vast computational resources and extensive training times. More importantly, they fail to solve the challenges inherent in multi-shot video generation, which is a more complex task that requires maintaining reasonable logic in the narrative, and visual consistency across different scenes-all of which are essential for producing multi-shot, movie-like content.\nThe limitations of existing approaches underscore a fundamental gap in the field: increasing model size and dataset volume alone is insufficient for generating reasonable and consistent videos with multiple shots. The primary challenge is not solely to achieve high visual fidelity but also to ensure that each shot seamlessly connects to the next, thereby contributing to a coherent and compelling narrative structure. When applied to multi-shot video generation, we observed that current methods often fall short in maintaining a consistent storyline and visual dynamics across multiple shots [8, 9, 69]. The absence of a structured approach for organizing and managing the generation process often results in a breakdown of narrative flow, with logical transitions between scenes being lost. Therefore, there is a pressing need for an approach that effectively balances visual quality with narrative consistency, managing the generation process without relying purely on computational scaling.\nInspired by the \u201cthought process\" behind the creation of real movies, we propose VideoGen-of-Thought (VGoT)\u2014a collaborative framework designed to tackle these challenges and create multi-shot videos in a step-by-step manner. An existing approach with a similar manner called \"Chain of Thought\" [64] has been proven efficient in Large Language Models (LLMs) [1, 15] and some visual perception tasks [18]. In this paper, we extend this approach to video generation by breaking down the complex process into four distinct yet collaborative modules as shown in Fig. 1: Script Generation Module converts an simple and high-level story description into detailed prompts for each shot to provide a solid narrative foundation; Keyframe Generation Module generates visually consistent keyframes based on the script, ensuring consistency in the appearance of characters, environments, and objects; Shot-Level Video Module produces video latents from corresponding keyframes and prompts at each shot that represents a shot-level video clip; finally, the Smooth Module integrates a cross-shot smoothing mechanism that processes information embeddings for the upcoming shot when the current one is nearing completion, ensuring a seamless connection facilitated by a reset boundary. This mechanism provides a smooth transition between shots, resulting in an end-to-end shot-by-shot video generation process that maintains visual and narrative continuity across the entire video.\nAs a text-to-video [53, 57] (T2V) architecture, our method VGoT employs a structured prompt design approach inspired by cinematic scriptwriting to ensure the generated videos are reasonable. Given a one-sentence script as user input, our method first prepares it into short descriptions for each shot and then extends them into five domains including character, background, relations, camera pose, and High Dynamic Range (HDR) descriptions-which together form a comprehensive movie-like script, as shown in the left part of Fig 2. Reasonability here refers to the logical flow and narrative coherence of the generated video, ensuring that each shot contributes meaningfully to the storyline. Furthermore, our method also tackles the issue of consistency across shots by introducing mechanisms for both temporal and visual coherence. Specifically, we use identity-preserving (IP) embeddings from avatars automatically derived from the narrative to maintain consistent character portrayal throughout the video. Additionally, our cross-shot smoothing mechanism, which leverages latent features from adjacent shots, ensures seamless transitions, resulting in a cohesive and consistent multi-shot video.\nOur experimental results demonstrate the efficacy of VGoT in generating high-quality, logically consistent, multi-shot videos. We evaluated VGoT using ten different stories, and a fair comparison with existing baselines highlights the superior performance of our approach in terms of narrative coherence, visual quality, and cross-shot consistency. The main contributions of this paper are as follows:\n1. We propose a collaborative, training-free architecture VideoGen-of-Thought for multi-shot video generation, addressing the key challenges of reasonability and consistency across multiple shots.\n2. We introduce a structured prompt design that emulates cinematic scriptwriting, ensuring logical coherence and narrative depth in generated videos.\n3. We develop a cross-shot consistency mechanism using identity-preserving embeddings and a cross-shot smoothing technique to maintain visual continuity and temporal coherence across different shots."}, {"title": "2. Related Work", "content": "Video generation has progressed significantly following the great success of diffusion models, leading to two important research categories: long video synthesis and multi-shot story generation. These areas focus on generating high-quality, consistent videos either as extended single shots or as coherent sequences across multiple scenes.\nLong Video Synthesis. Long video synthesis has advanced through diffusion-based methods and autoregressive approaches. Diffusion models based on the Latent Diffusion Model [49] utilize iterative refinement to generate visually consistent frames and have been effective for short sequences and other domains [6-9, 16, 22, 25, 26, 32, 38, 41-44, 46, 54, 58, 66, 68, 71, 76, 78-80, 82]. However, they struggle with maintaining coherence over extended video lengths. Autoregressive methods [20, 36, 73] predict frames sequentially but often face error accumulation, making long-term consistency challenging and computationally expensive. Additionally, training-free methods like FIFO-Diffusion [33] generate long sequences without training but lack mechanisms to manage transitions across shots, limiting their effectiveness in narrative-driven content. Overall, while these approaches achieve visual fidelity, they fail to ensure logical coherence across extended sequences. In contrast, VideoGen-of-Thought (VGOT) leverages a modular approach that includes cross-shot smoothing mechanisms to ensure both visual consistency and narrative coherence, offering a more holistic solution for generating long-form videos.\nMultiple Shot Story Generation. Multiple Shot Story Generation focuses on maintaining narrative coherency across distinct scenes. Animate-a-Story [27] uses retrieval-augmented generation to ensure visual consistency but struggles with maintaining logical narrative transitions. DreamFactory [67] employs a multi-agent system for collaboration across shots, but lacks robust mechanisms for logical continuity, leading to inconsistencies. MovieDreamer [81] uses a hierarchical framework to manage coherence but relies heavily on predefined structures, limiting adaptability to dynamic stories. Flexifilm [40] attempts to address scene adaptability by incorporating flexible conditions, but it still falls short in ensuring logical cohesion between shots and often requires manual adjustments for consistency. Collectively, these methods often require significant manual intervention to achieve a consistent narrative, limiting scalability for fully automated storytelling. In contrast, our work provides a detailed and integrated framework that not only ensures cross-shot coherence through a novel smoothing mechanism but also employs rigorous evaluation methods to achieve both logical consistency and scalability, setting a new paradigm for multi-shot video generation."}, {"title": "3. Preliminaries and Problem Formulation", "content": "3.1. Preliminaries\nDiffusion Models [29, 55, 56] are generative models that trained to approximate data distribution p(x) from random noise $\\epsilon \\sim \\mathcal{N}(0, I)$. In the forward process, noise scheduled by the variance $\\beta_t$ is gradually added as $q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_t\\mathbf{I})$ over T time steps, resulting in noisy latent variables $x_T$. Then a learnable model $\\mu_\\theta$ parameterized by $\\theta$ aims to denoise $x_t$ back to the original data sample $x_0$, with a parameterized distribution given by $p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$ where $\\mu_\\theta$ and $\\Sigma_\\theta$ are predicted mean and variance. The objective is to minimize the difference between predicted and true noise through the loss function:\n$\\mathcal{L}_{\\text{uncond diff}} = \\mathbb{E}_{x_0,\\epsilon\\sim\\mathcal{N}(0,1),t} [\\lVert\\epsilon - \\epsilon_\\theta(x_t, t)\\rVert^2]$  (1)\nIn the reverse process, an overall sample queue is maintained as $[x_T; x_{T-1}; ... ; x_0]$ at the pixel level. Latent diffusion models [49] successfully map this process into a lower-dimensional latent space using a variational autoencoder (VAE) [10, 34, 37, 70]. This transformation significantly enhances flexibility, allowing additional conditions to be incorporated through cross-attention mechanisms [60], which can be formulated as:\n$\\text{Attention}(Q, K, V) = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$ (2)\nwhere $Q$ is derived from the noisy image embeddings $e_i$, and $K$ and $V$ are derived from the text embeddings $e_t$ in text-to-image tasks. This mechanism has been further extended to the temporal dimension in conditional diffusion models for video generation [6, 8, 26, 28, 74, 75, 77]. For a video $V \\in \\mathbb{R}^{F \\times H \\times W \\times 3}$ with F frames and spatial resolution $H \\times W$, a series of latent codes $\\{z_f\\}_{f=0}^{F-1}$, where $z_f \\in \\mathbb{R}^{h \\times w \\times d}$ with d channels will be denoise in the same level. These video diffusion models, denoted as $\\epsilon_\\theta (z_t, t, c)$, are capable of predicting $\\epsilon$, which can be decoded into video frames based on given conditions c.\nWhile most existing methods aim to achieve long video generation through enlarging the shape of latent code z and the size of model parameters $\\theta$, FIFO-Diffusion [33] provides a new approach through denoising a queue of latents"}, {"title": "3.2. Problem Definition", "content": "Given an one-sentence user input S containing N number of shots, e.g., A set of one-sentence prompts, 30 shots, describe a story of a classic American woman Mary's life, from birth to death, our method aims to generate a multi-shot, reasonable, and consistent video V with minimal manual intervention. The key challenges are defined as:\n\u2022 Multi-Shot: Generating minute-level video containing plenty of shots.\n\u2022 Reasonability: Ensuring the generated video maintains a logical narrative and storyline.\n\u2022 Consistency: Maintaining temporal and identity consistency across different shots.\nOur proposed solution, VideoGen-of-Thought (VGoT), starts from preparing the script $\\{p_i\\}_{i=0}^{N-1}$ for each shot, drafting keyframes $\\{I_i\\}_{i=0}^{N-1}$ with identity-preserving, then generating video latents $\\{z_i\\}_{i=0}^{N-1}$ with a cross-shot smooth mechanism within N shots. VGoT provides a novel approach to overcoming the limitations of existing multiple shots video generation methods."}, {"title": "4. Method: VideoGen-of-Thought", "content": "In this section, we introduce each module in our proposed framework, VideoGen-of-Thought (VGoT), including Script Generation Module, Keyframe Generation Module, Shot-Level Video Generation Module, and Cross-Shot Smooth Module. By breaking down the complex task of long video generation into a series of smaller, well-defined modules, VGoT systematically addresses the key challenges of video length, logic, and cross-shot consistency. This modular decomposition allows each component to focus on specific aspects of the problem, resulting in long, reasonable, and consistent video results."}, {"title": "4.1. Module 1: Script Generation", "content": "The script generation task is modeled as a sequence generation problem using a pretrained large language model (LLM) [1, 4, 15]. The process starts with a short one-sentence user prompt S with the number of shots N defined, e.g., a set of one-sentence prompts, 30 shots, describe a story about three avatars (two friends Mike and Jane, one bad guy Tom) and their Fantasy Adventure in an ancient pyramid. We leverage an LLM to prepare short shot descriptions $S' = \\{s_i\\}_{i=0}^{N-1}$ for each shot, which builds the foundation logic for the whole story. The script $p_i$ for one shot can be fulfilled from $s_i$ as:\n$p_i(p_{\\text{cha}}, p_b, p_r, p_{\\text{cam}}, p_h) = M_{\\text{LLM}}(s_i, i)$ (4)\nwhere $p_{\\text{cha}}$ represents the character description, $p_b$ is the background description, $p_r$ describes the relation between characters and elements in the scene, $p_{\\text{cam}}$ describes the camera pose, and $p_h$ defines the HDR lighting details. An example is shown in the left top of Figure 3. The LLM is conditioned on both the current $s_i$ and the previously generated prompts $p_{i-1}$ to maintain narrative coherence and avoid abrupt transitions between scenes. The algorithm for script generation is iterative, producing a prompt for each video shot as shown in Algorithm 1."}, {"title": "4.2. Module 2: Keyframe Generation", "content": "The keyframes generation task is based on the scripts prepared in the Script Module. For each shot, we use a pretrained CLIP [47] encoder $E_{\\text{text}}$ to obtain text embedding $e_i = E_{\\text{text}}(p_i)$. We leverage the knowledge of LLM used in script module to create some avatar descriptions for this story based on short shot descriptions S'. For instance, given the S' of Mary's life provided in Sec 3.2, LLM provides the avatar prompts $P_a = [P_{a1}, P_{a2}, ...]$ of Child Mary, Teenager Mary, Mid-aged Mary, Elder-aged Mary, and Old Mary from five domains defined in Eq 4, and specify each shot prompt with a corresponding avatar.\nAs introduced in section 3, we use pretrained text-to-image diffusion models $M_1$ [49, 59] to generate $I_a = [M_1(P_{a1}), M_1(P_{a2}), ...]$ representing the avatar appearance from $P_a$ through cross-attention [60]. Then we employ IP-Adapter [72] to decouple the key K and value V in the cross-attention in $M_1$ by introducing an additional block that involves the identity-preserving embedding from $I_a$. This mechanism ensures that each generated keyframe not only matches the given prompt but also maintains visual consistency with the given avatar through copied K' and V'. The generation of each keyframe can be formalized as:\n$I_i = M_1(e_i^t, e_i^i)$ (5)\nwhere $e_i^t$ is the text embedding from $p_i$ as mentioned above, $e_i^i$ is the image embedding of the jth IP avatar selected for ith shot which is obtained by a CLIP vision model [47]. This approach ensures keyframes $\\{I_i\\}_{i=0}^{N-1}$ maintain visual consistency in the visual appearance of characters and backgrounds style for the same avatar in different shots."}, {"title": "4.3. Module 3: Shot-level Video Generation", "content": "Through Script Module and KeyFrame Module, we have already got the scripts and keyframes required by each shot-level video. We employ DynamiCrafter [68] as a conditional video model $M_v$ that is capable of taking both the keyframes and the corresponding textual prompts as input to generate a series of latents $Z_i = [Z_1, Z_2, ..., Z_k]$ representing the ith shot video clip $V_i$ which includes k frames, and this process can be formulated as:\n$Z_i = M_v(e_i^t, e_i^i, \\epsilon),$ (6)\nwhere the $e_i^t$ and $e_i^i$ are the text embedding and image embedding of shot i, $Z_i \\in \\mathbb{R}^{f \\times h \\times w \\times d}$ is a series of latent code that can be decoded into the video sequence $V_i$ containing f frames generated for shot i, and $\\epsilon_i$ is a noise map with same shape to initialize $Z_i$. Note that we use short shot description $s_i$ but not the detailed script $p_i$ to obtain the $e_i^t$, because we found in our experiments that shot-level videos $V_i$ will be less dynamic when given prompts that are too complex even though we obtain much better keyframe through $p_i$. This phenomenon may have been caused by our video model $M_v$ receiving simpler textual descriptions during training. As future work, this may be improved if $M_v$ is trained with detailed scripts like Eq 4.\nBy incorporating both the keyframe and the textual description to obtain latents z in this video generation process, we secure the generation of high-quality, smooth video clips in the subsequent module."}, {"title": "4.4. Module 4: Smooth Mechanism", "content": "The smoothing mechanism ensures a seamless transition from the current shot i to the next shot i + 1, ultimately achieving overall coherence throughout the final video. Inspired by FIFO [33] that uses latent partitioning to divide $Q_k = \\{z_f\\}_{f=0}^{F-1}$ into several parts to support different noise levels across frames, we implement a simple but efficient strategy\n$Q_k = \\{z_f\\}_{f=k}^{k+n}$, updating Q with DDIM[55] sampler $\\Phi$ as:\n$Q_k \\leftarrow \\Phi(Q_k, \\tau_k, C; \\epsilon_\\theta)$ (3)\nfor each timestep $\\tau_k \\in [0,T - n]$ from different level, in effect implementing long-term frame-by-frame prediction. Despite FIFO excelling in frame-by-frame prediction, it fails to handle the quick transition for different shots in multi-shot video generation. Moreover, as a text guided inference method, FIFO can't receive conditions like image. In our work, we use the generative capability of diffusion models and process cross-shot smoothness with a FIFO-like mechanism at the shot level."}, {"title": "5. Experiments", "content": "5.1. Experiment Settings\nDatasets Since current datasets can't satisfy our requirements for cross-shot story, as defined in Sec 3.2, we use VGoT to create 10 stories, each containing 30 shots, with a user input S and 30 short shot scripts S', and a detailed 30 shot scripts P from five domains as Eq 4, making a total of 300 video shots for comparison.\nMetrics In this work, we mainly utilize three metrics for quantitative evaluation: CLIP score [47], PSNR [17], IS [5], Face Consistency (FC) score and Style Consistency (SC) score. FC score is calculated by similarities between face features captured by InsightFace [2, 3, 11-14, 21, 23, 24, 48] in generated videos, and we obtain SC score by style feature similarities from VGG19 [52]. Specifically, we calculate FC score and SC score from both within one shot and cross shots to evaluate the face identity and visual style consistency."}, {"title": "6. Conclusion", "content": "In this paper, we introduce VideoGen-of-Thought (VGoT), a modular framework that tackles the challenges of generating multi-shot videos with narrative coherence, visual consistency, and stylistic fidelity. VGoT breaks down video generation into four collaborative modules, ensuring high semantic alignment, keyframe consistency, smooth transitions, and scene-specific guidance. Our evaluations demonstrate that VGOT outperforms existing baselines in maintaining flow consistency, face coherence, and stylistic alignment, offering a promising approach for scalable and coherent multi-shot video generation."}]}