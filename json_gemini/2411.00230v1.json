{"title": "From Easy to Hard: Tackling Quantum Problems with Learned Gadgets For Real Hardware", "authors": ["Akash Kundu", "Leopoldo Sarra"], "abstract": "Building quantum circuits that perform a given task is a notoriously difficult problem. Reinforcement learning has proven to be a powerful approach, but many limitations remain due to the exponential scaling of the space of possible operations on qubits. In this paper, we develop an algorithm that automatically learns composite gates (\"gadgets\") and adds them as additional actions to the reinforcement learning agent to facilitate the search, namely the Gadget Reinforcement Learning (GRL) algorithm. We apply our algorithm to finding parameterized quantum circuits (PQCs) that implement the ground state of a given quantum Hamiltonian, a well-known NP-hard challenge. In particular, we focus on the transverse field Ising model (TFIM), since understanding its ground state is crucial for studying quantum phase transitions and critical behavior, and serves as a benchmark for validating quantum algorithms and simulation techniques. We show that with GRL we can find very compact PQCs that improve the error in estimating the ground state of TFIM by up to 107 fold and make it suitable for implementation on real hardware compared to a pure reinforcement learning approach. Moreover, GRL scales better with increasing difficulty and to larger systems. The generality of the algorithm shows the potential for applications to other settings, including optimization tailored to specific real-world quantum platforms.", "sections": [{"title": "1. Introduction", "content": "Quantum computing has seen remarkable advancements in recent years, yet substantial barriers remain before it can fully address practical, real-world applications. While current quantum algorithms, such as Shor's algorithm (Shor, 1999) for factorization and Grover's algorithm (Grover, 1996) for search, highlight the transformative potential of quantum technology, implementing these methods on a large scale remains challenging (Monz et al., 2016; Mandviwalla et al., 2018) because the currently available quantum hardware is of small scale and very noise-prone. Hence, a hybrid quantum-classical approach called variational quantum algorithms (VQAs) was introduced, where the design is divided into three subroutines: 1) Preparing the quantum state on quantum hardware by constructing a parameterized quantum circuit (PQC), say U(\u03b8), that contains single-qubit parameterized rotations (of angle \u03b8) and non-parameterized 2-qubit entangling gates; 2) Measuring the output of the PQC and evaluating the cost function of the form\n$C(\\theta) = \\langle 0|U^{\\dagger}(\\theta) H U(\\theta)|0\\rangle,$\nwhere H is the Hamiltonian that encodes the problem; 3) Optimizing the C(\u03b8) on a classical computer using a classical optimization method. Hence, the search for a quantum algorithm that solves a problem H boils down to the search for a PQC such that the cost function in Eq. 1 is minimized.\nDue to the availability of various interfaces of quantum hardware with different qubit connectivity topologies and noise levels, it is very difficult to construct hardware-specific PQC. To tackle this difficulty, recent research focuses on the construction of VQAs using different variations of adaptive methods (Grimsley et al., 2019; Tang et al., 2021; Feniou et al., 2023; Anastasiou et al., 2024), and sophisticated optimization methods (Zhou et al., 2020; Zhu et al., 2022; Cheng et al., 2024; Kundu et al., 2024a) where the VQA transforms a simple problem into a target complex problem. Moreover, recent research has also explored the possibility of machine learning (Krenn et al., 2023; Bang et al., 2014; Sarra et al., 2024; Patel et al., 2024b; He et al., 2023a; 2024; Kuo et al., 2021; He et al., 2023b; Kundu et al., 2024b; Sun et al., 2024; Zhang et al., 2021; Ma et al., 2024; Sadhu et al., 2024; Ding & Spector, 2022; Kundu, 2024; Ostaszewski et al., 2021; Trenkwalder et al., 2023) to automate quantum algorithm design, presenting a promising avenue for maximizing the capabilities of quantum hardware.\nAmong machine learning methods for PQC design, reinforcement learning (RL) stands out as a powerful approach"}, {"title": "2. Related work", "content": "Reinforcement learning (RL) has become one of the most prominent methods for effectively searching for optimal parameterized quantum circuits (PQCs) in variational quantum algorithms. Typically, RL approaches employ a carefully designed reward function to train the agent to choose suitable gates. In (Ostaszewski et al., 2021), the authors employed double-deep Q-Network (DDQN) and e-greedy policy to estimate the ground state of chemical Hamiltonian. Meanwhile, in (Ye & Chen, 2021) a DQN with actor-critic policy and proximal policy optimization is used to construct multi-qubit maximally entangled states. Whereas, (F\u00f6sel et al., 2021) presented a novel approach to quantum circuit optimization using deep reinforcement learning, demonstrating significant improvements in circuit efficiency and paving the way for hardware-aware optimization strategies. Following this line of approach, in (Patel et al., 2024b), the authors tackle PQCs problems under realistic quantum hardware. This is achieved by introducing a curriculum reinforcement learning approach and other sophisticated pruning techniques to the agent and environment. Moreover, (Tang et al., 2024) introduces an RL-based router that integrates Monte Carlo tree search to reduce routing overhead in quantum circuits. In (Foder\u00e0 et al., 2024), the authors propose an RL-based method for autonomously generating quantum circuits as ansatzes in VQAs, demonstrating its effectiveness in solving diverse problems and discovering a novel family of ansatzes for the Maximum Cut problem. The authors in (Moflic & Paler, 2023) propose a cost explosion strategy for reinforcement learning-based quantum circuit optimization, demonstrating its potential to improve RL training efficiency and help reach optimum circuits. In (Kundu, 2024), by utilizing a novel encoding method for the PQCs, a dense reward function, and a e-greedy policy, the authors tackle the quantum state diagonalization problem. Additionally, in (Patel et al., 2024a), the authors show that by utilizing RL, it is possible to solve the hard instances of combinatorial optimization problems where state-of-the-art algorithms perform sub-optimally. In (Sadhu et al., 2024), the authors leverage insights from quantum information theory, which helps the RL-agent to prioritize certain architectural features that are likely to provide better performance in PQC search and optimization.\nIt should be noted that in all these approaches the action space of the RL agent is kept fixed, making the performance degrade when the number of qubits or the difficulty of the problem is increased. Also, as the action space primarily consists of non-native gates that can not be directly implemented on real quantum hardware, the PQCs need to be transpiled\u00b9 to fit the hardware. Additional techniques are also needed for making them more noise-resilient. The problem of extending the action space with high-level actions is often called option discovery or skill learning in the reinforcement learning literature (Bacon et al., 2017; Nachum et al., 2018; Machado et al., 2024). Multiple approaches have been explored, including maximizing a single reward function while learning multiple policies and a meta-controller (Krishnan et al., 2017; Frans et al., 2018), the connection with information theory (Gregor et al., 2016; Florensa et al., 2017) and diversity maximization (Eysenbach et al., 2019). In this work, we employ a different approach, inspired by recent work on program synthesis (El-\nTranspilation is a critical step in quantum computing that bridges the gap between high-level quantum algorithms and the physical constraints of the available quantum hardware. It involves complex transformations and optimizations to ensure efficient execution while maintaining the circuit's intended functionality."}, {"title": "3. Methods", "content": "We propose a general technique to build circuits that solve quantum optimization problems. Our approach combines a reinforcement learning agent to search for the parameterized quantum circuit (PQC) space with a program synthesis algorithm that analyzes the best circuit to extract gadgets (i.e., new composite components) that extend the agent's action space. This method, which we call gadget reinforcement learning (GRL), is particularly useful when solving a parametrized class of problems. Especially when the problem has different degrees of difficulty according to its parameters, we can learn a set of operations from simpler problems and use them subsequently to help solve the harder ones."}, {"title": "3.1. Gadget reinforcement learning", "content": "We provide an overview of the GRL algorithm for constructing PQCs in a VQA task. Consecutively, we provide details on the state and action representations as well as the reward function employed in this study.\nThe GRL algorithm initiates with an empty quantum circuit. The RL agent, based on a double deep Q-network and e-greedy policy (for further details see Appendix D.2), sequentially appends the gates to the circuit until the maximum number of actions has been reached. The actions are chosen from an action space of available elementary gates. In particular, in our application, it contains RZ, SX, X as single qubit gates, where RZ is the only parameterized gate in the action space. Furthermore, to entangle the qubits we use Controlled-Z (CZ) gate. The main motive to choose such an action space is that all these gates are native gateset of the newly introduced IBM Heron processor. Therefore, we do not need to further transpile the circuits,"}, {"title": "3.2. Library building", "content": "To update the action space in GRL, a library building algorithm that leverages a program synthesis framework inspired by (Sarra et al., 2024; Ellis et al., 2020) is employed. The algorithm analyzes the top-k PQCs to identify and extract common, useful gate sequences and structures. The PQCs are expressed as programs in a typed-\u03bb-calculus formalism (Pierce, 2002), where the gates act as functions that take a quantum circuit and the target qubits as inputs and"}, {"title": "4. Results", "content": "As an example application for our algorithm, we consider the transverse field Ising model (TFIM). The goal is to design a circuit which finds the ground state of the system, i.e. the system with the lowest energy. This problem is well-known to be NP-Hard (O'Connor et al., 2022). The system is defined by\n$H = -J\\sum_{(i,j)} \\sigma_i^z \\sigma_j^z - h \\sum_i \\sigma_i^x,$\nwhere N is the number of qubits, J is the coupling constant between neighboring spins, h is the strength of the transverse field, \u03c3z and \u03c3x are the Pauli matrices acting on the i-th spin in the z- and x-direction, respectively, and (i, j) denotes summation over nearest neighbors. This model presents a ferromagnetic phase transition at J \u226b h and has been studied thoroughly in literature, for example with hybrid quantum-classical approaches (Sumeet et al., 2023)."}, {"title": "4.1. Improved performance", "content": "We recall that GRL runs iteratively, with the agent and environment specifications as provided in Appendix D.3. It first considers a small N = 2 qubit system and finds the ground state in the simple regime, e.g. weak transverse field (h = 10-3). The agent can find the ground state with the target accuracy within our compute budget, given by a fixed number of episodes. Subsequently, we try to solve an intermediate regime with a larger transverse field (h = 5\u00d710-2). We can find a reasonable approximation to the ground state, but the size of the parameterized quantum circuits (PQCs) is large and the error compared is relatively high, especially given the circuit size. At this point, to improve the efficiency of the reinforcement learning algorithm, we analyze the top k parameterized quantum circuits (PQCs) that we obtained in the previous cases and extract the most useful components as new primitive composite gates i.e. gadgets. In particular, we start by adding the gadget with the largest log-likelihood to the RL agent's action space. By running the reinforcement learning agent again with the extended action space, we find a much better approximation of the ground state. Results are summarized in Fig. 2(a), where we compared our agent with a state-of-the-art curriculum-based RL approach, as presented in (Patel et al., 2024b). The figure shows the effectiveness of the gadget extraction features introduced in this work, especially for the hardest regimes. It should be noted that the gadget extraction is conducted on an easier task. Based on the outcome on the easy task, the action space is modified and the agent is then used to solve more difficult problems (e.g. h = 1).\nFinally, we can also generalize to N = 3 qubits. As Fig. 2(b) shows, by also adding a second extracted gate, we can get a very good approximation not only in the easy regime but also in the hardest one also for a 3-qubit system. In contrast, the RL-only agent was not able to catch any learning signal in that case, because of the necessary depth of the search. Especially in the hard regime, h = 1, we see that the pure reinforcement learning agent finds a state with a very large error, probably obtained by randomly assembling gates. In contrast, as we add more gadgets to the action space, we see that RL can find much better solutions, effectively solving the problem. Please refer to the Appendix E for the numerical results of our ablation study."}, {"title": "4.2. Found circuits are suitable for real hardware", "content": "In this section, we compare the PQCs that arise using gadget reinforcement learning (GRL) with state-of-the-art RL methods for finding the ground state of TFIM. To benchmark the results, we consider curriculum reinforcement learning (CRL) with an action space consisting of the universal gateset RX, RY, RZ, CX, as in (Patel et al., 2024b; Kundu, 2024), and compare the PQC with GRL containing an action space extended with gadgets. As in the previous section, this action space contains the native gateset of IBM Heron processor in addition to the composite gates obtained through the analysis of the k best-performing PQCs in solving 2-qubit TFIM of h = 10\u00ae\u00b3 and h = 5 \u00d7 10-2. Using this gateset, we estimate the ground state of 2-qubit and 3-qubit TFIM at the phase change point at h = 1. We show that circuits obtained by GRL achieve a similar error to CRL, but are generally more compact when transpiled to real quantum hardware. The results are summarized in Table 1, where we transpile the PQCs for the IBMQ Torino that operates on IBM Heron processor. In Appendix G, we provide the topology of the hardware which defines the connectivity among the qubits, and show that to obtain a similar error (in the order of 10-4) for 3-qubit TFIM GRL takes about 3\u00d7 fewer CZ, RZ and SX gates. By looking at it, there seems to be an advantage in solving the problem directly on the target hardware components rather than first finding the solution in a universal gateset and then decomposing it for the target hardware. We emphasize that no constraint on the circuit depth has been enforced in the GRL agent, even though this can be considered in future applications to encourage shorter circuits, or avoid using expensive gates."}, {"title": "5. Outlook", "content": "In this paper, we have shown how to learn reusable components from different regimes for efficiently building quantum circuits that solve some given problems. Instead of considering a single specific problem, we start from a trivial regime and gradually tackle the harder one. By finding the ground state in the low transverse field regime, we discover sequences of gates that are recurrent, and we can extract them as gadgets and use them to extend the action space of subsequent iterations. This proves to be very effective because it largely reduces the required depth of the circuit at the cost of a slightly increased breadth of the search. In other words, the extracted gates serve as a data-driven inductive bias for solving the given class of problems.\nIn terms of shortcomings of our approach, the main overhead to consider is the necessity of performing multiple iterations. In particular, it is important that the target class of problems has a structure with different degrees of difficulty: if the problem is too difficult, the reinforcement learning agent does not receive any signal, it will only learn to produce random circuits and the extracted gates will not be necessarily useful. On the other hand, if one regime is trivial and the other one is too hard, there is a low chance of generalization. Also, to extend the actions of the reinforcement learning agent multiple approaches are possible. In our example, we reinitialized the agent after extending the action space. However, smarter approaches, for example by just adding extra output neurons at the last layer of the policy, associating them to the added gadgets, may allow starting from the previous policy, while adding a small bias to encourage the exploration of the new action.\nOur technique is very general and can be directly generalized to other quantum problems in future studies. Furthermore, it may be suitable for real hardware optimizations. Indeed, it allows to explicitly define the elementary gates to use for the decomposition, as opposed to finding the solution in a high-level gate set first (e.g. rotation gates Rx, Ry,Rz) and transpiling them later. This can arguably produce more efficient circuits. Also, penalties for the length of the circuit or for the use of specific gates could be enforced, encouraging gates that are more reliable or cheap to implement on real hardware. In addition, the elementary components could also be modified to include some model of the noise on the real hardware, thus possibly finding a solution for some quantum problem that already includes some noise mitigation effects."}, {"title": "A. The Transverse Field Ising Model: different regimes", "content": "As shown in Fig. 3, by looking at the ground state energy gap, we can identify three different regimes in the Transverse Field Ising Model:\n1. in the low external field, the first excited state is almost degenerate with the ground state, therefore it is easy to find a low-energy state;\n2. the regime where h ~ 0.1, where the energy gap increases and the ground state starts to have a visibly different energy from the first excited state;\n3. h > 0.1 where the energy gap is larger and the ground state energy is much smaller than that of the first excited state."}, {"title": "B. Feedback-driven curriculum reinforcement learning", "content": "During learning, the agent maintains a pre-defined threshold \u03b6 representing the lowest energy observed so far, updating it based on defined rules. Initially, \u03b6 is set to a hyperparameter $1. When a lower threshold is found, \u03b6 is updated to this new value. A fake minimum energy hyperparameter, \u03bc, serves as a target energy, approximated by the following:\nfake minimum energy = (N-1) \u00d7 (-J) + N \u00d7 (-h),\nwhere N is the number of interacting spins, J is the coupling strength between the spins and h is the strength of the magnetic field.\nWithout amortization, the threshold updates to |\u03bc \u2013 \u03b6| when \u03b6 changes; with amortization, it becomes |\u03bc \u2013 \u03b6| + \u03b4, where \u03b4 is an amortization hyperparameter. The agent then explores subsequent actions and records successes.\nTwo threshold adjustment rules apply: a greedy shift to |\u03bc \u2013 \u03b6| after G episodes (where G is a hyperparameter) and a gradual decrease by \u03b4/\u03ba with each successful episode, where \u03ba is a shift radius hyperparameter. If repeated failures occur after setting the threshold to |\u03bc \u2013 \u03b6|, it reverts to |\u03bc \u2013 \u03b6| + \u03b4, allowing the agent to backtrack if stuck in a local minimum."}, {"title": "C. Detailed description of the Library Building algorithm", "content": "The library building algorithm analyzes a set of circuits D in relation to a given set of elementary gates g, called grammar. In this framework, each grammar g consists of elementary gates (also called \u201cprimitives\u201d) with assigned probabilities based"}, {"title": "D. Implementation details", "content": ""}, {"title": "D.1. Quantum circuit encoding", "content": "We employ a refined version of the tensor-based binary encoding introduced in (Kundu et al., 2024b), which is inspired by the encoding presented in (Patel et al., 2024b), to capture the architecture of a parametric quantum circuit (PQC), specifically by encoding the sequence and arrangement of quantum gates. Unlike the encoding presented in (Patel et al., 2024b), which is only the function of the number of qubits N, the refined encoding is a function of N and the number of 1-qubit gates N1q. This makes it suitable for the encoding of a broad range of action spaces and enables the agent to access a complete description of the circuit. To ensure a consistent input size across varying circuit depths, we construct the tensor for the maximum anticipated circuit depth.\nTo build this tensor, we define the hyperparameter Tmax, which restricts the number of allowable gates (actions) across all episodes. A moment in a PQC refers to all simultaneously executable gates, corresponding to the circuit's depth. We represent PQCs as three-dimensional tensors where, at the start of each episode, an empty circuit of depth Tmax is initialized. This tensor is dimensioned as [Tmax \u00d7 ((N + N1q) \u00d7 N)], where N denotes the number of qubits and N1q the number of 1-qubit gates. Each matrix slice within the tensor contains N rows that specify control and target qubit locations in CNOT"}, {"title": "D.2. Double Deep Q-Network (DDQN)", "content": "Deep Reinforcement Learning (RL) methods employ Neural Networks (NNs) to refine the agent's policy in order to maximize the cumulative return:\n$G_t = \\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+1},$\nwhere \u03b3\u2208 [0, 1) denotes the discount factor. An action value function is assigned to each state-action pair (s, a), capturing the expected return when action a is taken in state s at time t under policy \u03c0:\n$q_{\\pi}(s, a) = E_{\\pi}[G_t | S_t = s, a_t = a].$\nThe objective is to find an optimal policy that maximizes the expected return. This can be achieved through the optimal action-value function q*, which satisfies the Bellman optimality equation:\n$q_*(s, a) = E[r_{t+1} + \\gamma \\max_{a'} q_*(s_{t+1}, a') | S_t = s, a_t = a].$\nRather than solving the Bellman equation directly, value-based RL focuses on approximating the optimal action-value function through sampled data. Q-learning, a widely used value-based RL algorithm, initializes with arbitrary Q-values for each (s, a) pair and iteratively updates them to approach q*. The update rule for Q-learning is:\n$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha (r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)),$\nwhere \u03b1 is the learning rate, rt+1 is the reward received at step t + 1, and st+1 is the resulting state after taking action at in state st. Convergence to the optimal Q-values is guaranteed under the tabular setup if all state-action pairs are visited infinitely often (Melo, 2001). To promote exploration in Q-learning, an \u03b5-greedy policy is adopted, defined as:\n$\\pi(a|s) = \\begin{cases} 1 - \\epsilon & \\text{if } a = \\arg\\max_{a'} Q(s, a'), \\\\ \\frac{\\epsilon}{\\text{# of actions}} & \\text{otherwise.} \\end{cases}$"}, {"title": "D.3. Reinforcement learning agent hyperparameters", "content": "The hyperparameters of the double deep-Q network algorithm were selected through coarse grain search and the employed network architecture depicts a feed-forward neural network whose hyperparameters as provided in Tab. 2.\nThis \u03b5-greedy policy adds randomness during learning, while the policy becomes deterministic after training.\nTo handle large state and action spaces, NN-based function approximations are used to extend Q-learning. Since NN training relies on independently and identically distributed samples, this requirement is met through experience replay. With experience replay, transitions are stored and randomly sampled in mini-batches, reducing the correlation between samples. For stable training, two NNs are employed: a policy network that is frequently updated, and a target network, which is a delayed copy of the policy network. The target value Y used in updates is given by:\n$Y_{DQN} = r_{t+1} + \\gamma \\max_{a'} Q_{target}(s_{t+1}, a').$\nIn the double DQN (DDQN) approach, the action used for estimating the target is derived from the policy network, minimizing the overestimation bias observed in standard DQN. The target is thus defined as:\n$Y_{DDQN} = r_{t+1} + \\gamma Q_{target} (s_{t+1}, \\arg\\max_{a'} Q_{policy} (s_{t+1}, a')).$\nThis target value is then approximated through a loss function, which in our work is chosen to be the smooth L1-norm given by\n$SmoothL1(x) = \\begin{cases} 0.5x^2 & \\text{if } |x| < 1, \\\\ |x| - 0.5 & \\text{otherwise.} \\end{cases}$"}, {"title": "E. Numerical results", "content": "Table 3 gives a more detailed overview of the results of our experiments."}, {"title": "F. Comparison of transpiled circuits", "content": "In this Appendix, we compare the length of the circuits obtained to solve the 3-qubit TFIM ground state in the regime h = 1 using the RL agent with a universal basis and then transpile the obtained circuit, and our GRL agent with extended action space.\nFirst, we consider the same curriculum reinforcement learning agent as in the main text, but we apply it using a universal gate set of {RX, RY, RZ, CX}. One of the best obtained circuits is shown in Figure 5.\nOn the other hand, in Fig. 6, we show the best circuit obtained for solving the same problem using our gadget reinforcement learning agent with the hardware-specific gate set.\nBefore implementation on real hardware, we would need to transpile the circuits to only use the instructions available on the"}, {"title": "G. IBM Heron processor: IBMQ Torino", "content": "Figure 8 shows the topology of the IBMQ Torino platform."}]}