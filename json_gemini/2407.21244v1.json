{"title": "VITAL: Visual Teleoperation to Enhance Robot Learning through Human-in-the-Loop Corrections", "authors": ["Hamidreza Kasaei", "Mohammadreza Kasaei"], "abstract": "Imitation Learning (IL) has emerged as a powerful approach in robotics, allowing robots to acquire new skills by mimicking human actions. Despite its potential, the data collection process for IL remains a significant challenge due to the logistical difficulties and high costs associated with obtaining high-quality demonstrations. To address these issues, we propose a low-cost visual teleoperation system for bimanual manipulation tasks, called VITAL. Our approach leverages affordable hardware and visual processing techniques to collect demonstrations, which are then augmented to create extensive training datasets for imitation learning. We enhance the generalizability and robustness of the learned policies by utilizing both real and simulated environments and human-in-the-loop corrections. We evaluated our method through several rounds of experiments in simulated and real-robot settings, focusing on tasks of varying complexity, including bottle collecting, stacking objects, and hammering. Our experimental results validate the effectiveness of our approach in learning robust robot policies from simulated data, significantly improved by human-in-the-loop corrections and real-world data integration. Additionally, we demonstrate the framework's capability to generalize to new tasks, such as setting a drink tray, showcasing its adaptability and potential for handling a wide range of real-world bimanual manipulation tasks.", "sections": [{"title": "1 Introduction", "content": "Imitation Learning [1] has emerged as a fundamental approach in robotics, enabling robots to acquire new skills by observing and mimicking human actions [2, 3]. This method holds significant promise for developing complex robotic behaviors without the need for explicitly programming the robot. However, the primary challenge with imitation learning lies in the data collection process [4, 5, 6, 7]. On the one hand, acquiring high-quality demonstrations is costly and logistically challenging. On the other hand, the need for extensive and diverse demonstrations to train models magnifies these difficulties, as each new task or environment often requires a fresh set of demonstrations [8, 9]. \u03a4\u03bf address these challenges, teleoperation has been proposed as a viable solution. In recent years, there has been a growing interest in the development of bimanual teleoperation interfaces that facilitate the learning and execution of various household and industrial tasks [10, 11, 12, 13].\nOne example of such a system is the ALOHA platform [10, 12], which has garnered considerable attention from the robotics research community. Such teleoperation platforms enable the execution of various tasks such as pushing chairs, using cabinets, wiping spills, and even cooking shrimp. Despite the advantages offered by such interfaces, there remain significant challenges that hinder their widespread adoption. One of the primary issues is that such teleoperation interfaces are hardware-specific and costly. The cost of such interfaces is approximately $30K, which makes"}, {"title": "2 Related Work", "content": "Robot Learning with Teleoperation: IL is a powerful technique for robot learning, enabling robots to acquire new skills by mimicking human demonstrations [15, 16, 17]. However, collecting in-domain demonstrations is often difficult and not scalable due to the need for physical interaction with the robot hardware. This process is also expensive as it requires constant access to the robot. Some works use scripted \"expert\" in simulation to generate demonstrations, but scaling these approaches to more complex tasks is challenging [18, 3]. Other IL methods leveraged visual deep neural networks to train reactive models from images captured during demonstrations [19, 20, 21]. While these models provide greater flexibility, they necessitate collecting a large number of demonstrations to learn simple pick-and-place tasks, making them user and computationally expensive [14]. Other approaches utilize human activity videos to guide robot learning but often fall short in training 6-DoF manipulation policies [22, 23]. These methods typically require in-domain teleoperation data to bridge the gap between video demonstrations and robotic execution [11]. A common problem with these IL methods is the lack of corrective feedback, leading to the accumulation of errors over the length of the demonstration, known as the compounding errors problem [24]. Similar to our approach, some methods have focused on dividing demonstrations into several waypoints to address this issue [24, 25, 26]. Unlike these approaches, we aim to automatically extract the waypoints of a demonstration without adding any burden on the human user. Other IL methods used bimanual teleoperation interfaces, such as ALOHA [10], for mapping joints' value from master to slave robots. However, these approaches are quite expensive and not scalable as they require four robots for bimanual manipulation: two masters and two slaves. In contrast, we propose a low-cost teleoperation interface that allows controlling robot end-effectors to perform complex bimanual tasks and collect demonstrations in both real and simulation environments. Teleoperation within a digital twin of the robot enables us to scale up the training data easily without constant access to the real robot. Additionally, we introduce a human-in-the-loop correction mechanism to repair faulty demonstrations, enhancing the robot's performance in complex tasks with minimal effort.\nSim2Real Transfer: Some approaches, such as MimicGen [27] and CyberDemo [28], attempt to collect demonstrations in simulation to reduce the cost of collecting demonstrations on robots."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Collecting Demonstrations through Teleoperation", "content": "The architecture of our low-cost visual teleoperation system is illustrated in Figure 1. We use an external RGB-D camera to capture human motions and two Bluetooth-enabled selfie sticks for commanding the left and right grippers. The captured human motion data and gripper commands are translated into real-time motor commands for the robot arm and the gripper. In particular, we utilized the Mediapipe library for skeleton tracking, specifically focusing on 24 upper body keypoints. These keypoints are crucial for capturing the operator's movements accurately. To establish a stable huamn-world reference frame, we considered the left shoulder, right shoulder, left hip, and right hip keypoints. We define the origin of this reference frame between left and right hip keypoints. This reference frame serves as the basis for calculating the pose of each hand relative to the world frame. Given the differences in body morphology and arm length between humans and the robot, two scaling factors, $\u03b1$ for the right arm and $\u03b2$ for the left arm, are applied to bridge the gap, thereby making teleoperation more intuitive and precise. For controlling the grippers of the robot's arms, we utilized two Bluetooth selfie sticks. These sticks were adapted to send commands for opening and closing the grippers. To track the sticks' orientation, we attached AruCo markers [35, 36] to the sticks as shown in Fig. 2.\nTo efficiently manage the complexity of long-horizon tasks and reduce the compounding errors, we employ task decomposition. Each task, $T$, is decomposed into a sequence of subtasks, $S_n$, $T = (S_1, S_2, ..., S_n)$. The start and end poses of the trajectory for each subtask are defined based on the current pose of the manipulator and the pose of the object that should be grasped or placed. The detection of these subtasks is facilitated by the commands given by the user for grasping or placing objects. When a user issues a command, the system identifies the relevant subtask, captures the necessary pose information, and creates a trajectory segment for that subtask. By structuring tasks this way, we ensure that the demonstration data is organized and can be effectively used for training purposes.\nTo replicate the physical environment in the simulation, we build a digital twin of our robot in Gazebo. This includes accurately modeling the robot and objects to ensure consistency between the simulated and real-world environments. During the demonstration phase, we performed a task in Gazebo while recording all relevant ROS topics, including the initial state of the objects, joint"}, {"title": "3.2 Augmenting Demonstration in Simulation", "content": "To enhance the collected demonstration and improve the robustness of the learned policies, we perform data augmentation on the trajectory level (i.e. subtasks). Our augmentation method uses several techniques to generate a diverse set of trajectories from the initial demonstrations. First, we extract waypoints from the recorded bag file by reading the positions and orientations of the robot's end effectors. These waypoints are then used to fit a polynomial trajectory. We exclude the last few points from the polynomial fitting to ensure the end of the trajectory which represents the affordance pattern is preserved. We then apply uniform sampling to the trajectory by calculating the cumulative arc length of the waypoints to ensure uniform spacing between the sampled points. This method helps in creating a smooth and evenly distributed trajectory, which is then segmented into smaller parts for detailed analysis and augmentation.\nWe first augment the trajectory by adding Gaussian noise to the waypoints. This involves introducing random variations to the trajectory points to simulate real-world imperfections and variations. By adding noise, we can create multiple versions of the trajectory, enhancing the model's ability to generalize across different scenarios. The noise can be applied to all waypoints or selectively to preserve certain critical points, such as the trajectory's endpoint. We also perform translations and flips on the trajectory. Translating the waypoints involves shifting the entire trajectory in the x, y, and z directions, which helps in creating varied instances of the task. Flipping the trajectory along the x-axis generates a mirrored version, providing additional diversity in the dataset and making the initial trajectory of one arm useful for the other arm. Furthermore, we sample new initial points within specified bounds and connect these points to the nearest waypoints in the existing trajectory. This process involves finding the nearest index in the trajectory and creating new paths from these initial points to the rest of the trajectory. By varying the starting points, we can simulate different initial conditions for the task. Each trajectory is annotated with an arm identifier, distinguishing between left and right arm movements. This comprehensive augmentation process significantly expands the dataset, making it thousands of times larger than the original set of demonstrations. However, augmented trajectories can sometimes be imperfect and not executable due to the robot's arm kinematics, potentially leading to task failures. To ensure the reliability of the augmented data, we validate each augmented trajectory in the digital twin environment. Only successfully executed trajectories are used for training a policy. This way we ensure that the learned policies are robust and capable of handling a wide range of real-world conditions."}, {"title": "3.3 Hierarchical Policy Learning", "content": "To effectively learn and execute long-horizon tasks, we formulate the learning policy as a hierarchical policy learning problem through Behavioral Cloning (BC). This approach involves training a high-level policy to select a sequence of subtasks and low-level policies to execute each subtask. For a given task, we assume that we can detect the sequence of subtasks based on the demonstration by detecting commands for grasp or release (indicated by pressing the bottom of the selfie stick). These commands allow us to identify the start and end poses for each subtask, facilitating the decomposition of long-horizon tasks into a series of subtasks.\nThe high-level policy, $\u03c0_\u0397$, is responsible for selecting the appropriate subtask, $S_i$, based on the current state of the environment and the task progression. This high-level policy can be implemented as a state machine or a learned model. In this work, we use a state machine that receives the current state of the robot, and the pose of the objects to infer the current subtask, $\u03c0_\u0397(o, r) \u2192 S_i$, where $o$ shows the current state of objects and $r$ is the state of the robot. Alternatively, a learned model could receive the task ID and the state of the objects and robot, and predict the subtask."}, {"title": "3.4 Residual Learning with Human-in-the-Loop", "content": "Despite the robustness of the trained policy, there can be scenarios where the robot fails to complete the task due to a sim-to-real gap, pose estimation errors, or small errors in executing a trajectory. To address these issues, we incorporate a residual learning approach with human-in-the-loop corrections during the policy roll-outs. When the robot encounters a failure during task execution, a human operator steps in to correct the errors. The user plays the recorded rosbag file of the experiment and provides the correction feedback whenever it is needed. The correction involves measuring the 3D delta position changes of the wrist of the left and right hands of the demonstrator relative to the initial poses of the hands. These delta changes are then scaled by empirically determined factors $\u03b1$ (for the right hand) and $\u03b2$ (for the left hand), both < 0.1 to avoid fast movements, and added to the robot's wrist movements. This minimal adjustment helps the robot to accomplish the task with improved accuracy. If the user observes a large error in the current prediction of the policy, they can switch the system to full teleoperation mode. In this mode, the robot will directly follow the human wrist movements to accomplish the task. The data from these human-in-the-loop corrections is recorded into a separate dataset, D'. This dataset includes both residual corrections and instances where full teleoperation was required. To fine-tune the policy, training data is sampled equally from both the original dataset $D$ and the new dataset D'. This balanced sampling ensures that the model learns from the augmented experiences and corrections provided by the human operator, thereby improving its performance and adaptability. This iterative process of learning ensures that the policy continuously improves and adapts to real-world conditions."}, {"title": "4 Experimental Setup", "content": "We performed multiple rounds of experiments on both simulation and real-robot settings to validate our method. Specifically, we study the following questions: (Q1) Is it feasible to train robot policies using exclusively simulation data without relying on on-robot data? (Q2) Which model architectures and data composition ratios are most effective for policy training? Specifically, what is the optimal balance between simulated and real-world data? (Q3) To what extent does human-in-the-loop correction enhance policy performance when teleoperation data is limited? (Q4) Can our framework successfully execute another complex bimanual task, such as setting a drink tray, which is somewhat similar to one of the learned tasks?"}, {"title": "4.1 Setup", "content": "Our experimental setups, both in simulation and on real robots, are illustrated in Fig. 4 and Fig. 5, respectively. We developed a simulation environment in Gazebo, leveraging the ODE physics engine to closely mimic the behavior of our dual-arm robot. The hardware setup in both simulation and real robot includes an Asus Xtion camera, two Universal Robots (UR5e) equipped with Robotiq"}, {"title": "4.2 Results", "content": "Q1 - Feasibility of Training Policies with Simulation Data: We conducted extensive sets of experiments across all tasks by training the policy solely on augmented simulation trajectories. For each task, we collected five successful in-domain demonstrations and augmented them to 80K demonstrations. The BC-LSTM policy trained on this augmented data was then tested on both simulated and real robots. Results are summarized in Fig. 6. In simulation, the robot successfully performed most tasks. However, in real-world experiments, certain discrepancies such as minor inaccuracies in object grasping and placement were observed. Specifically, in the bottle collecting task, the robot achieved an average task success rate of 92% in simulation and 80% in real-robot scenarios. In the stacking objects task, the robot successfully completed 84% of the experiments in simulation but only 60% in real-world tests. Similarly, for the hammering task, the success rate was 78% in simulation and 50% in real-world experiments. The main failure in bottle collecting was due to inaccurate trajectory prediction, which led to"}, {"title": "5 Conclusion and Limitations", "content": "In this work, we presented a low-cost visual teleoperation system for bimanual manipulation tasks. Our approach leverages an RGB-D camera and visual processing techniques to collect demonstration data, which is then augmented and used to train robot policies. We employed a combination of simulated and real-world data, augmented with human-in-the-loop corrections, to improve the accuracy and reliability of the learned policies. Our system was evaluated on three tasks with differing complexity levels, demonstrating its effectiveness in both simulated and real-robot environments. The experimental results validate the effectiveness of our approach in learning robust policies from simulated data, enhanced by human-in-the-loop corrections, and real-world data integration. The framework successfully generalized to a new, complex task -setting a drink tray-demonstrating the adaptability of the learned policies to similar tasks with minimal effort.\nThe main limitation of this work is that for tasks requiring a high degree of precision at certain steps (e.g., staking, hammering, etc), waypoint trajectory learning may not be sufficient, and (closed-loop) vision feedback should be considered. To address this limitation, we plan to explore the integration of visual feedback mechanisms to further enhance the accuracy and reliability of task execution. By incorporating real-time visual feedback, the system can adjust and refine its actions based on the visual context, leading to more precise and robust manipulation capabilities."}, {"title": "Appendix", "content": ""}, {"title": "1 Impact of Model Architecture and Data Composition (Q2)", "content": ""}, {"title": "1.1 Training data composition", "content": "We trained several policies using different ratios of simulated to real-robot demonstrations, including 70%-30%, 50%-50%, 30%-70%, and 0-100% mixes. For each task, we recorded five real-world demonstrations and five simulation demonstrations, which were then augmented to create a larger dataset. The final training data was sampled from these augmented demonstrations according to the specified ratios. This approach allowed us to determine the optimal balance between simulation and real-world data for training robust and effective robot policies."}, {"title": "2 Visualization of ground-truth and predicted trajectories", "content": "We learned a policy using a single-shot demonstration and tested its performance on the validation set of the first subtask of the bottle-collecting task. Specifically, we plot 2D/3D visualization of predicted and actual trajectories for two randomly selected samples from the validation set. The network's performance is depicted through both 2D and 3D plots in Figure 11. Furthermore, we tested the performance of the policy on the digital twin of the robot by randomly placing a bottle object in front of the robot, retrieving the current pose of the end effector and the pose of the object, and forwarding them as input to the learned policy. We then executed the predicted trajectory and visualized the robot's performance.\nFigure 12 shows the performance of the robot in three experiments, highlighting its ability to follow the predicted trajectory and accurately grasp the object based on the learned policy. The visualizations include both images of the robotic arm and plots of the predicted versus actual trajectories for the X, Y, and Z axes."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Success and failure criteria for the proposed tasks", "content": "For the collecting objects task, success is determined by placing all objects into the basket and failure is considered if one of the objects is not placed into the basket and remains outside. Figure 13 shows examples of successful (left) and failed (right) attempts for the stacking and hammering tasks. In the stacking task, success is achieved when objects are stacked stably. In the hammering task, success criteria are based on hitting the target object accurately. These criteria help in evaluating the performance and reliability of the robotic system in executing these tasks. In the case of setting up a drink tray task, success is considered when the robot places all items (such as a mug and a bottle) accurately on their designated spots on the tray, and failure is defined when items are not placed correctly on the tray, or the robot fails to place all items."}, {"title": "3.2 Performance of the robot in simulation: known scenarios - known objects", "content": "We show the performance of the robot in the simulated environment where the robot interacts with known objects across known scenarios in Figure 14. The tasks include collecting bottles (top-row), stacking objects (second-row), and hammering (lower-row), all using the same objects from the collecting demonstration. The robot's ability to follow a predicted trajectory and complete these tasks accurately showcases the robustness of the learned policy."}, {"title": "3.3 Performance of the robot: known scenario - new objects", "content": "In this section, we test the robot's ability to adapt to new objects in a familiar scenario. The experiment involves using bottles that differ in weight, size, and shape from the ones used in the initial demonstration. This assessment helps determine the robot's generalization capability when faced with varying object properties while performing the same task. Despite the differences in the object's physical characteristics, the robot successfully follows the predicted trajectory and completes the collection task. The sequence of snapshots in Figure 15 demonstrates the robot's generalization ability to complete the collecting objects task with new objects."}, {"title": "3.4 Performance of the robot in simulation: new scenario - new objects", "content": "We also evaluate the robot's performance in a completely new scenario using new objects. The experiment involves setting up a drink tray with various items, testing the robot's ability to adapt to a new task and handle unfamiliar objects. For this task, we utilized the stacking object policy and adjusted the subtask trajectory end poses to accomplish the task successfully. Figure 16 shows the sequence of snapshots detailing the robot's performance in setting up a drink tray. This image highlights the robot's capacity to generalize from its training and successfully perform tasks in new scenarios with new objects."}, {"title": "3.5 Perception of our real robot experiments", "content": "To discuss our real robot experiments, we provide an example of our experiment setup for the collecting bottles task (Fig. 18), for the stacking objects task (Fig. 19), and for the hammering"}, {"title": "3.6 Failure cases during real robot experiments", "content": "During the real robot experiments, several failure cases were observed that can be attributed to the differences between simulation and real-world conditions (See Fig. 21). These failures provide valuable insights into the challenges of transferring learned policies from simulation to real robots. The main factors contributing to these failures include:\nGap between simulation and real robot: The objects used during the experiments in the real world differ in shape, size, weight, and material properties compared to those used in the simulation. These discrepancies can affect the robot's ability to accurately predict and execute the required trajectories, leading to unsuccessful task completion."}]}