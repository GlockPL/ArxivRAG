{"title": "VITAL: Visual Teleoperation to Enhance Robot Learning through Human-in-the-Loop Corrections", "authors": ["Hamidreza Kasaei", "Mohammadreza Kasaei"], "abstract": "Imitation Learning (IL) has emerged as a powerful approach in robotics, allowing robots to acquire new skills by mimicking human actions. Despite its potential, the data collection process for IL remains a significant challenge due to the logistical difficulties and high costs associated with obtaining high-quality demonstrations. To address these issues, we propose a low-cost visual teleoperation system for bimanual manipulation tasks, called VITAL. Our approach leverages affordable hardware and visual processing techniques to collect demonstrations, which are then augmented to create extensive training datasets for imitation learning. We enhance the generalizability and robustness of the learned policies by utilizing both real and simulated environments and human-in-the-loop corrections. We evaluated our method through several rounds of experiments in simulated and real-robot settings, focusing on tasks of varying complexity, including bottle collecting, stacking objects, and hammering. Our experimental results validate the effectiveness of our approach in learning robust robot policies from simulated data, significantly improved by human-in-the-loop corrections and real-world data integration. Additionally, we demonstrate the framework's capability to generalize to new tasks, such as setting a drink tray, showcasing its adaptability and potential for handling a wide range of real-world bimanual manipulation tasks.", "sections": [{"title": "1 Introduction", "content": "Imitation Learning [1] has emerged as a fundamental approach in robotics, enabling robots to acquire new skills by observing and mimicking human actions [2, 3]. This method holds significant promise for developing complex robotic behaviors without the need for explicitly programming the robot. However, the primary challenge with imitation learning lies in the data collection process [4, 5, 6, 7]. On the one hand, acquiring high-quality demonstrations is costly and logistically challenging. On the other hand, the need for extensive and diverse demonstrations to train models magnifies these difficulties, as each new task or environment often requires a fresh set of demonstrations [8, 9]. \u03a4\u03bf address these challenges, teleoperation has been proposed as a viable solution. In recent years, there has been a growing interest in the development of bimanual teleoperation interfaces that facilitate the learning and execution of various household and industrial tasks [10, 11, 12, 13].\nOne example of such a system is the ALOHA platform [10, 12], which has garnered considerable attention from the robotics research community. Such teleoperation platforms enable the execution of various tasks such as pushing chairs, using cabinets, wiping spills, and even cooking shrimp. Despite the advantages offered by such interfaces, there remain significant challenges that hinder their widespread adoption. One of the primary issues is that such teleoperation interfaces are hardware-specific and costly. The cost of such interfaces is approximately $30K, which makes them non-scalable and expensive for many research laboratories and practical applications. To overcome these challenges, we propose a low-cost visual teleoperation interface to collect high-quality demonstrations for long-horizon bimanual manipulation tasks.\nIn robot manipulation tasks, IL is often trained via in-domain demonstrations, which correspond to the data obtained directly from the deployment environment [14]. Although it is widely accepted that gathering direct examples from the actual robot is the most efficient way to learn long-horizon manipulation tasks, we show that collecting human demonstrations in real and digital twin environments can produce better outcomes for real-world tasks. Our approach allows the creation of a large dataset of augmented demonstrations based on a few initial demonstrations. By leveraging the flexibility and functionalities of our digital twin environment, we can enhance the number of demonstrations and the generalizability of the learned policy. These augmented demonstrations are then used to train a policy, which is fine-tuned using real-world demonstrations. Experimental results show that our approach significantly improves the performance of the robot in real-world scenarios. In summary, the main contributions of this work include:\n\u2022 We introduce a low-cost visual teleoperation approach that enables high-quality data collection for bimanual manipulation tasks.\n\u2022 We leverage a digital twin of the robot to create a vast dataset of augmented demonstrations from a limited number of initial demonstrations. This enhances the scalability and generalizability of the learned policies without requiring extensive real-world demonstration.\n\u2022 We incorporate human-in-the-loop corrections to refine and improve the robot's performance in complex tasks. This method allows for interactive learning and adaptation, ensuring higher success rates in real-world applications."}, {"title": "2 Related Work", "content": "Robot Learning with Teleoperation: IL is a powerful technique for robot learning, enabling robots to acquire new skills by mimicking human demonstrations [15, 16, 17]. However, collecting in-domain demonstrations is often difficult and not scalable due to the need for physical interaction with the robot hardware. This process is also expensive as it requires constant access to the robot. Some works use scripted \"expert\" in simulation to generate demonstrations, but scaling these approaches to more complex tasks is challenging [18, 3]. Other IL methods leveraged visual deep neural networks to train reactive models from images captured during demonstrations [19, 20, 21]. While these models provide greater flexibility, they necessitate collecting a large number of demonstrations to learn simple pick-and-place tasks, making them user and computationally expensive [14]. Other approaches utilize human activity videos to guide robot learning but often fall short in training 6-DoF manipulation policies [22, 23]. These methods typically require in-domain teleoperation data to bridge the gap between video demonstrations and robotic execution [11]. A common problem with these IL methods is the lack of corrective feedback, leading to the accumulation of errors over the length of the demonstration, known as the compounding errors problem [24]. Similar to our approach, some methods have focused on dividing demonstrations into several waypoints to address this issue [24, 25, 26]. Unlike these approaches, we aim to automatically extract the waypoints of a demonstration without adding any burden on the human user. Other IL methods used bimanual teleoperation interfaces, such as ALOHA [10], for mapping joints' value from master to slave robots. However, these approaches are quite expensive and not scalable as they require four robots for bimanual manipulation: two masters and two slaves. In contrast, we propose a low-cost teleoperation interface that allows controlling robot end-effectors to perform complex bimanual tasks and collect demonstrations in both real and simulation environments. Teleoperation within a digital twin of the robot enables us to scale up the training data easily without constant access to the real robot. Additionally, we introduce a human-in-the-loop correction mechanism to repair faulty demonstrations, enhancing the robot's performance in complex tasks with minimal effort.\nSim2Real Transfer: Some approaches, such as MimicGen [27] and CyberDemo [28], attempt to collect demonstrations in simulation to reduce the cost of collecting demonstrations on robots."}, {"title": "3 Method", "content": "The architecture of our low-cost visual teleoperation system is illustrated in Figure 1. We use an external RGB-D camera to capture human motions and two Bluetooth-enabled selfie sticks for commanding the left and right grippers. The captured human motion data and gripper commands are translated into real-time motor commands for the robot arm and the gripper. In particular, we utilized the Mediapipe library for skeleton tracking, specifically focusing on 24 upper body keypoints. These keypoints are crucial for capturing the operator's movements accurately. To establish a stable huamn-world reference frame, we considered the left shoulder, right shoulder, left hip, and right hip keypoints. We define the origin of this reference frame between left and right hip keypoints. This reference frame serves as the basis for calculating the pose of each hand relative to the world frame. Given the differences in body morphology and arm length between humans and the robot, two scaling factors, \u03b1 for the right arm and \u03b2 for the left arm, are applied to bridge the gap, thereby making teleoperation more intuitive and precise. For controlling the grippers of the robot's arms, we utilized two Bluetooth selfie sticks. These sticks were adapted to send commands for opening and closing the grippers. To track the sticks' orientation, we attached AruCo markers [35, 36] to the sticks as shown in Fig. 2.\nTo efficiently manage the complexity of long-horizon tasks and reduce the compounding errors, we employ task decomposition. Each task, T, is decomposed into a sequence of subtasks, \\(S_n\\), \\(T = (S_1, S_2, . . . , S_n)\\). The start and end poses of the trajectory for each subtask are defined based on the current pose of the manipulator and the pose of the object that should be grasped or placed. The detection of these subtasks is facilitated by the commands given by the user for grasping or placing objects. When a user issues a command, the system identifies the relevant subtask, captures the necessary pose information, and creates a trajectory segment for that subtask. By structuring tasks this way, we ensure that the demonstration data is organized and can be effectively used for training purposes.\nTo replicate the physical environment in the simulation, we build a digital twin of our robot in Gazebo. This includes accurately modeling the robot and objects to ensure consistency between the simulated and real-world environments. During the demonstration phase, we performed a task in Gazebo while recording all relevant ROS topics, including the initial state of the objects, joint values, end-effector poses, outputs from RGB-D cameras (head-mounted and on the left and right arms), commands from the instructor, activity videos with corresponding 3D skeleton data, object poses, and success or failure indicators, at their respective publication rates into a ROS bag file. The comprehensive nature of this recorded data ensures that all necessary information is captured for the next stage of the methodology, where data augmentation will be applied to enhance the dataset."}, {"title": "3.2 Augmenting Demonstration in Simulation", "content": "To enhance the collected demonstration and improve the robustness of the learned policies, we perform data augmentation on the trajectory level (i.e. subtasks). Our augmentation method uses several techniques to generate a diverse set of trajectories from the initial demonstrations. First, we extract waypoints from the recorded bag file by reading the positions and orientations of the robot's end effectors. These waypoints are then used to fit a polynomial trajectory. We exclude the last few points from the polynomial fitting to ensure the end of the trajectory which represents the affordance pattern is preserved. We then apply uniform sampling to the trajectory by calculating the cumulative arc length of the waypoints to ensure uniform spacing between the sampled points. This method helps in creating a smooth and evenly distributed trajectory, which is then segmented into smaller parts for detailed analysis and augmentation.\nWe first augment the trajectory by adding Gaussian noise to the waypoints. This involves introducing random variations to the trajectory points to simulate real-world imperfections and variations. By adding noise, we can create multiple versions of the trajectory, enhancing the model's ability to generalize across different scenarios. The noise can be applied to all waypoints or selectively to preserve certain critical points, such as the trajectory's endpoint. We also perform translations and flips on the trajectory. Translating the waypoints involves shifting the entire trajectory in the x, y, and z directions, which helps in creating varied instances of the task. Flipping the trajectory along the x-axis generates a mirrored version, providing additional diversity in the dataset and making the initial trajectory of one arm useful for the other arm. Furthermore, we sample new initial points within specified bounds and connect these points to the nearest waypoints in the existing trajectory. This process involves finding the nearest index in the trajectory and creating new paths from these initial points to the rest of the trajectory. By varying the starting points, we can simulate different initial conditions for the task. Each trajectory is annotated with an arm identifier, distinguishing between left and right arm movements. This comprehensive augmentation process significantly expands the dataset, making it thousands of times larger than the original set of demonstrations. However, augmented trajectories can sometimes be imperfect and not executable due to the robot's arm kinematics, potentially leading to task failures. To ensure the reliability of the augmented data, we validate each augmented trajectory in the digital twin environment. Only successfully executed trajectories are used for training a policy. This way we ensure that the learned policies are robust and capable of handling a wide range of real-world conditions."}, {"title": "3.3 Hierarchical Policy Learning", "content": "To effectively learn and execute long-horizon tasks, we formulate the learning policy as a hierarchical policy learning problem through Behavioral Cloning (BC). This approach involves training a high-level policy to select a sequence of subtasks and low-level policies to execute each subtask. For a given task, we assume that we can detect the sequence of subtasks based on the demonstration by detecting commands for grasp or release (indicated by pressing the bottom of the selfie stick). These commands allow us to identify the start and end poses for each subtask, facilitating the decomposition of long-horizon tasks into a series of subtasks.\nThe high-level policy, \\(\\pi_H\\), is responsible for selecting the appropriate subtask, \\(S_i\\), based on the current state of the environment and the task progression. This high-level policy can be implemented as a state machine or a learned model. In this work, we use a state machine that receives the current state of the robot, and the pose of the objects to infer the current subtask, \\(\\pi_H(o, r) \\rightarrow S_i\\), where o shows the current state of objects and r is the state of the robot. Alternatively, a learned model could receive the task ID and the state of the objects and robot, and predict the subtask.\nFor the low-level policy, we propose a unified multi-subtask policy learning, \\(\\pi_{ms}\\), that can handle all subtasks. This policy receives the predicted subtask ID, \\(S_i\\), along with the start, \\(p_s\\), and end poses, \\(p_e\\), and generates the appropriate trajectory conditioned on the input subtask, \\(\\pi_{ms}(S_i, p_s, p_e) \\rightarrow trajectory\\). The training process begins by loading the augmented trajectory data of a task. The trajectories are padded to ensure uniform length, which is critical for batch processing in neural networks. Our model is designed to predict the intermediate points of a trajectory given the start and end points. The high-level policy coordinates these subtasks, ensuring smooth transitions and overall task completion."}, {"title": "3.4 Residual Learning with Human-in-the-Loop", "content": "Despite the robustness of the trained policy, there can be scenarios where the robot fails to complete the task due to a sim-to-real gap, pose estimation errors, or small errors in executing a trajectory. To address these issues, we incorporate a residual learning approach with human-in-the-loop corrections during the policy roll-outs. When the robot encounters a failure during task execution, a human operator steps in to correct the errors. The user plays the recorded rosbag file of the experiment and provides the correction feedback whenever it is needed. The correction involves measuring the 3D delta position changes of the wrist of the left and right hands of the demonstrator relative to the initial poses of the hands. These delta changes are then scaled by empirically determined factors \u03b1 (for the right hand) and \u03b2 (for the left hand), both < 0.1 to avoid fast movements, and added to the robot's wrist movements. This minimal adjustment helps the robot to accomplish the task with improved accuracy. If the user observes a large error in the current prediction of the policy, they can switch the system to full teleoperation mode. In this mode, the robot will directly follow the human wrist movements to accomplish the task. The data from these human-in-the-loop corrections is recorded into a separate dataset, D'. This dataset includes both residual corrections and instances where full teleoperation was required. To fine-tune the policy, training data is sampled equally from both the original dataset D and the new dataset D'. This balanced sampling ensures that the model learns from the augmented experiences and corrections provided by the human operator, thereby improving its performance and adaptability. This iterative process of learning ensures that the policy continuously improves and adapts to real-world conditions."}, {"title": "4 Experimental Setup", "content": "We performed multiple rounds of experiments on both simulation and real-robot settings to validate our method. Specifically, we study the following questions: (Q1) Is it feasible to train robot policies using exclusively simulation data without relying on on-robot data? (Q2) Which model architectures and data composition ratios are most effective for policy training? Specifically, what is the optimal balance between simulated and real-world data? (Q3) To what extent does human-in-the-loop correction enhance policy performance when teleoperation data is limited? (Q4) Can our framework successfully execute another complex bimanual task, such as setting a drink tray, which is somewhat similar to one of the learned tasks?"}, {"title": "4.1 Setup", "content": "Our experimental setups, both in simulation and on real robots, are illustrated in Fig. 4 and Fig. 5, respectively. We developed a simulation environment in Gazebo, leveraging the ODE physics engine to closely mimic the behavior of our dual-arm robot. The hardware setup in both simulation and real robot includes an Asus Xtion camera, two Universal Robots (UR5e) equipped with Robotiq 2F-140 grippers, and an interface for managing the initiation and conclusion of experiments. For the Teleoperation interface, we use a RealSense D435 RGB-D camera to capture the motion of the user. All evaluations were performed with a PC running Ubuntu 18.04 with a 3.20 GHz Intel Xeon(R) i7, and a Quadro P5000 NVIDIA.\nTasks: We designed three tasks of varying difficulty levels to test the performance of the teleoperation and the robustness of the learned policies: (i) Bottle Collecting: In this task, the robot should collect multiple bottles with different shapes from different locations. (ii) Stacking Pringles: In this task, the robot must stack Pringles cans on top of each other in various locations. This requires precise control and coordination to ensure the cans are aligned and stable. (iii) Hammering: The most challenging task involves the robot using a hammer to strike a cylinder. This task tests the robot's ability to handle a tool effectively. To evaluate the performance of the robot on each task, we performed 100 simulation and 10 real-robot experiments. In the case of real robot experiments, we used a 3D perception system to get the pose and label of the objects [37, 38].\nBaselines and Metrics: To determine the best model architecture for bimanual manipulation tasks, we evaluated multiple Behavioral Cloning Recurrent Neural Network (BC-RNN) baselines. The baselines include: LSTM (Long Short-Term Memory), GRU (Gated Recurrent Unit), and Transformer-based network. PyTorch is used as a deep learning framework. We use Mean Squared Error (MSE) as the loss function for trajectory prediction. An Adam optimizer is employed to adjust the model weights during training. Early stopping is implemented to halt training if the validation loss does not improve for a specified number of epochs. Each network has been trained for 200 epochs. We split the data into training (70%), validation (15%), and test sets (15%) to evaluate the model's performance. Details of the networks are discussed as a part of supplementary materials. The Task Success Rate is used as the primary metric for evaluation. This metric is defined as the percentage of successfully completed tasks out of the total number of attempts."}, {"title": "4.2 Results", "content": "Q1 - Feasibility of Training Policies with Simulation Data: We conducted extensive sets of experiments across all tasks by training the policy solely on augmented simulation trajectories. For each task, we collected five successful in-domain demonstrations and augmented them to 80K demonstrations. The BC-LSTM policy trained on this augmented data was then tested on both simulated and real robots. Results are summarized in Fig. 6. In simulation, the robot successfully performed most tasks. However, in real-world experiments, certain discrepancies such as minor inaccuracies in object grasping and placement were observed. Specifically, in the bottle collecting task, the robot achieved an average task success rate of 92% in simulation and 80% in real-robot scenarios. In the stacking objects task, the robot successfully completed 84% of the experiments in simulation but only 60% in real-world tests. Similarly, for the hammering task, the success rate was 78% in simulation and 50% in real-world experiments. The main failure in bottle collecting was due to inaccurate trajectory prediction, which led to inefficiencies in approaching, grasping, or placing the bottles. The primary reasons for failures in the stacking and hammering tasks were inaccuracies in object grasping. Specifically, when the robot approached to grasp the object, the tip of the gripper touched the object, causing it to move slightly. This offset often led to failures as objects falling during the last step of stacking or missing the target when hammering. To address these challenges, we will check the impact of the model architecture and the effect of incorporating real demonstrations in addition to digital-twin demonstrations for training the policy to address sim-to-real gap in the following subsection (Q2). Furthermore, we believe incorporating feedback mechanisms through vision sensing could provide information on the exact part of the object being grasped. This feedback would enable the system to update the trajectory accordingly by learning suitable residual corrections, thereby improving the accuracy and reliability of the task execution.\nQ2 Impact of Model Architecture and Data Composition: To understand the influence of different model architectures and the ratio of simulated to real data, we conducted experiments with various configurations. We first trained LSTM, GRU, and Transformer-based models solely on augmented simulation demonstrations and compared their performance across the proposed tasks in the simulation environment. After identifying the best-performing architecture, we trained policies using different ratios of simulated to real-robot demonstrations, including 70% - 30%, 50% - 50%, 30% - 70%, and 0 100% mixes. For each task, we recorded five real-world demonstrations and five simulation demonstrations, which were then augmented to create a larger dataset. The final training data was sampled from these augmented demonstrations according to the specified ratios. This approach allowed us to determine the optimal balance between simulation and real-world data for training robust and effective robot policies. The LSTM model provided a good balance of performance and training efficiency, whereas GRU models exhibited slightly lower performance but benefited from reduced computational overhead. Transformer-based models demonstrated considerable potential for handling more complex tasks; however, they demanded significantly more computational resources (see Fig. 7). Upon comparing the results, we identified that the optimal ratio of simulated to real data was 70% simulated and 30% real. This ratio offered the best generalization and performance in real-world tasks, enhancing the task success rate across all evaluated tasks. Specifically, for bottle collecting, the task success rate increased from 80% to 90%; for stacking Pringles, the success rate rose from 60% to 80% (see Fig. 9); and for hammering, the success rate improved from 50% to 70%. This balanced approach effectively mitigated the sim-to-real gap, thereby enhancing the robustness and reliability of the learned policies.\nQ3: Effectiveness of Human-in-the-Loop Corrections: In this round of experiments, we investigated the effectiveness of human-in-the-loop correction in scenarios when the policy failed during task execution. We recorded 5 failure experiments for each task in the simulation. We then replayed the bag file and allowed the human user to correct the robot's movements using residual learning or full teleoperation mode. The updated demonstrations were tested to ensure they could successfully accomplish the task. These corrected demonstrations were then augmented and used for policy fine-tuning. By comparing the results, we observed that human-in-the-loop corrections significantly improved task success rates, particularly for more complex tasks such as stacking and hammering. The inclusion of human corrections from dataset D', resulted in an average improvement of approximately 5% in task success rates, demonstrating the value of interactive learning. Specifically, the success rate for the hammering task increased from 78% to 86%, for the stacking task from 84% to 89%, and for the bottle collecting task from 92% to 95%."}, {"title": "Q4: Capability to Execute A New Complex Bimanual Task: Setting a Drink Tray", "content": "To evaluate the generalization capabilities of our framework, we tested its ability to execute a new bimanual task that shares similarities with the learned tasks: setting a drink tray (see Fig. 10). This task involves placing a cup and a bottle on specific markers on a tray. The tray has two circular markers: one red for placing the cup and one blue for placing the bottle. The robot must pick up a cup and a bottle from a starting position and place them accurately on the designated red and blue markers on the tray. The policy was initially trained on the previously learned object stacking task. The trained policy was then evaluated only in simulation, as we could obtain precise positions of the markers and objects. The task was considered a success if the robot accurately placed the cup and bottle on the respective markers and a failure if the distance between the object and the marker exceeded 3 cm. We repeated this experiment 100 times, with the objects placed in random positions each time. The robot achieved a task success rate of 87%. The primary source of error in the failure cases was slight misalignment during grasping, which led to minor placement inaccuracies, despite accurate trajectory predictions. Our approach successfully generalized to the new task, demonstrating that the learned policies could be adapted to similar but distinct tasks with minimal effort. This showcases the robustness and adaptability of our teleoperation and learning framework in handling a variety of real-world scenarios. We believe that incorporating feedback mechanisms through vision, such as detecting the exact positions of the markers and adjusting the trajectory accordingly, could further enhance the accuracy and reliability of task execution."}, {"title": "5 Conclusion and Limitations", "content": "In this work, we presented a low-cost visual teleoperation system for bimanual manipulation tasks. Our approach leverages an RGB-D camera and visual processing techniques to collect demonstration data, which is then augmented and used to train robot policies. We employed a combination of simulated and real-world data, augmented with human-in-the-loop corrections, to improve the accuracy and reliability of the learned policies. Our system was evaluated on three tasks with differing complexity levels, demonstrating its effectiveness in both simulated and real-robot environments. The experimental results validate the effectiveness of our approach in learning robust policies from simulated data, enhanced by human-in-the-loop corrections, and real-world data integration. The framework successfully generalized to a new, complex task -setting a drink tray-demonstrating the adaptability of the learned policies to similar tasks with minimal effort.\nThe main limitation of this work is that for tasks requiring a high degree of precision at certain steps (e.g., staking, hammering, etc), waypoint trajectory learning may not be sufficient, and (closed-loop) vision feedback should be considered. To address this limitation, we plan to explore the integration of visual feedback mechanisms to further enhance the accuracy and reliability of task execution. By incorporating real-time visual feedback, the system can adjust and refine its actions based on the visual context, leading to more precise and robust manipulation capabilities."}]}