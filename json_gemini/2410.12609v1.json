{"title": "Towards Graph Foundation Models: The Perspective of Zero-shot Reasoning on Knowledge Graphs", "authors": ["Kai Wang", "Siqiang Luo"], "abstract": "Inspired by the success of artificial general intelligence, there is a trend towards developing Graph Foundation Models that excel in generalization across various graph tasks and domains. However, current models often require extensive training or fine-tuning to capture structural and semantic insights on new graphs, which limits their versatility. In this work, we explore graph foundation models from the perspective of zero-shot reasoning on Knowledge Graphs (KGs). Our focus is on utilizing KGs as a unified topological structure to tackle diverse tasks, while addressing semantic isolation challenges in KG reasoning to effectively integrate diverse semantic and structural features. This brings us new methodological insights into KG reasoning, as well as high generalizability towards foundation models in practice. Methodologically, we introduce SCORE, a unified graph reasoning framework that effectively generalizes diverse graph tasks using zero-shot learning. At the core of SCORE is semantic conditional message passing, a technique designed to capture both structural and semantic invariances in graphs, with theoretical backing for its expressive power. Practically, we evaluate the zero-shot reasoning capability of SCORE using 38 diverse graph datasets, covering node-level, link-level, and graph-level tasks across multiple domains. Our experiments reveal a substantial performance improvement over prior foundation models and supervised baselines, highlighting the efficacy and adaptability of our approach.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs (KGs) have become indispensable in various Web applications, providing a unified, multi-relational data structure that represents complex relations between entities [25]. KG reasoning, also referred to as KG completion, involves deducing new facts from existing knowledge, significantly enhancing the quality and utility of knowledge graphs in areas such as information retrieval, e-commerce recommendation systems, and financial forecasting [11, 55, 56]. Given the dynamic nature of real-world KGs, recent efforts have been directed towards advancing Zero-shot KG Reasoning (or Inductive Reasoning) which can generalize to new KGs that contain entities and relations not encountered during model training [16, 50, 58, 60, 72, 77]. This development leads us to a research question: how can zero-shot KG reasoning be effectively generalized across diverse semantic domains and graph tasks?\nThis motivation aligns with the concurrent trend of artificial general intelligence (AGI) on graphs, i.e. designing graph foundation models to learn generalizable representations from graph data and apply them across a wide spectrum of tasks and domains [30, 32]. Except task-specific foundation model studies [3, 16, 74], several recent studies have made efforts in two promising directions: few-shot graph prompt learning [14, 31, 45, 45] and zero-shot homogeneous foundation models [64, 65]. The former augments graph structure or node features with few-shot task samples to fit a frozen pre-trained model [78], requiring an additional learning phase for task-specific parameters. The latter standardizes various graphs and features into a unified form for training a model to generalize to zero-shot graphs [65], but primarily targets homogeneous graphs and encounters difficulties with graph-level classification tasks.\nInspired by the exciting advancements in current research, this paper explores graph foundation models from a novel perspective, Knowledge Graph Reasoning. We argue that zero-shot KG reasoning, as a unified graph task, offers three key advantages:\n(i) Multi-relational Structure: The inherent flexibility of heterogeneous graph structures allows the model to effectively adapt to various graph data. For instance, in a graph classification task, multiple graph samples can be consolidated into a single large KG, incorporating diverse entity types (such as nodes, graphs, and labels) and multi-relational edges that interconnect these entities.\n(ii) Unsupervised Link Prediction: The zero-shot KG reasoning task is classified under link prediction, a prevalent task type in graph unsupervised pre-training. Unlike graph prompt learning methods, which often treat various tasks as subgraph classification, a unified link prediction task can reduce the discrepancy between model pre-training and inference, streamlining the learning process."}, {"title": "2 Background", "content": "2.1 Zero-shot KG Inductive Reasoning\nLet a knowledge graph (KG) be represented as G = {E,R,T}, where E is the set of entities and R is the set of relations. The factual triples in the KG are denoted by T = {(e_h, r, e_t) | e_h, e_t \\in E, r \\in R}, where each triple consists of a head entity e_h, a relation r, and a tail entity e_t. Given a query (e_q, r_q), where e_q \\in E is the query entity and r_q \\in R is the query relation, the goal of KG Reasoning is to identify the correct entity e_a \\in E, such that either (e_q, r_q, e_a) or (e_a, r_q, e_q) forms a valid triple in G. In addition, we define a feature matrix X \\in R^{|E|\\times d_o}, where each row represents a feature vector of dimension d_o for the corresponding entity in the set E. Now, consider a model trained on a knowledge graph G_{tr} = {E_{tr}, R_{tr}, T_{tr}}. The task of zero-shot inductive reasoning on knowledge graphs is to test the model on a new inference graph G_{inf} = {E_{inf}, R_{inf}, T_{inf}}, where both entities and relations are completely unseen during training. In other words, the sets of entities and relations in the inference graph are disjoint from those in the training graph, i.e., E_{inf} \\cap E_{tr} = \\emptyset and R_{inf} \\cap R_{tr} = \\emptyset. The notations used in this paper and their descriptions are summarized in Table 4 in the Appendix.\n2.2 CMP-based Backbone Model\nRecent studies utilize graph neural networks based on Conditional Message Passing (CMP) to represent KG triples for inductive KG reasoning [58, 72, 73, 76, 77]. Traditional message passing neural networks, such as GCN [26], GAT [53], and GraphSAGE [22], compute unary node representations and lack the ability to model interactions in a node set (such as edges) [71]. Differently, for a KG G, a CMP-based model M = CMP(q, G, \u0398) calculates triple representations h^{L}_{u|q} iteratively, for each target entity e_v conditioned on a query q = (e_q, r_q), as follows:\n\\begin{align}\nh^{(0)}_{v|q} &= \\text{INIT}(e_q, r_q, e_v), \\\\ \\text{H}^{(l+1)}_q &= \\text{AGG}(\\{\\text{MSG}_r(h^{(l)}_{w|q})| w \\in \\mathcal{N}_r(e_v), r \\in \\mathcal{R}\\}\\}), \\\\h^{(l+1)}_{v|q} &= \\text{UPD}(h^{(l)}_{v|q},  \\text{H}^{(l+1)}_q).\n\\end{align}\nHere, INIT() is denoted as the initialization function that initializes the query entity e_q with the learnable query vector r_q while other entities are initialized with zeros. Msg() is a differentiable relation-specific message function relevant to the relations r and r_q. The representation of a node is iteratively updated over L layers using the aggregated features of its neighbors in N_r(e_v) through the functions AGG() and UPD(). The final representations of nodes h^{(L)}_{v|q} are conditioned on the query q and used as triple representations of (e_q, r_q, e_v) for predicting triple plausibility in KG reasoning.\nThese conditional representations have been demonstrated to be theoretically expressive [24] and practically effective [16]. Due to the specific initialization function INIT(), CMP-based models eliminate the necessity for learning unique parameters for each entity, relying solely on structural information from the graph, which allows for inductive reasoning on new KGs. Additionally, CMP enables parallel learning of triple representations h^{L}_{v|q} for all v \\in V, effectively amortizing the computational overhead. Detailed"}, {"title": "2.3 Preliminaries: Semantic Isolation Issues", "content": "Despite advancements, CMP-based models still face a significant challenge in effectively leveraging diverse node semantic features. Based on preliminary results, we discuss two failed attempts to address the semantic isolation issue in CMP-based models:\nAttempt 1: Semantic Features as Initialization. In this attempt, entity features are used as the initial node representation h^{(0)}, similar to traditional GNNs. However, as shown in Figure 1(a), the model performance degrades after injecting features (\"+Bert\") and even after fine-tuning with such initialization (\"+BertTune\"). This decline occurs because the core target node distinguishability assumption of CMP is violated [24]. This requires, for all r_q \\in R and e_v \\neq e_q \\in E, the condition INIT(e_q, e_q, r_q) \\neq INIT(e_q, e_v, r_q) must hold [71]. Additionally, performance degradation is observed when embeddings from other language models were used as input (\"+OtherTune\"). This suggests that the fine-tuned model had difficulty generalizing across different semantic spaces, highlighting the need to effectively address semantic isolation.\nAttempt 2: Semantic Features as Graph Edges. Another approach introduces feature similarity into the graph structure. It involves constructing a k-nearest neighbor (KNN) graph based on node similarities in the feature space and combining it with the existing knowledge graph. As illustrated in Figure 1(b), increasing the value of k reduces the reasoning performance of ULTRA. This is due to the added edges diluting local information and causing distant nodes to lose their distinctiveness. The resulting dense topology either amplifies or compresses embeddings unevenly, leading to over-smoothing and a decline in link prediction performance.\nIn summary, our preliminary findings highlight the challenges posed by semantic isolation in knowledge graph reasoning. Incorporating semantic features while ensuring generalization across diverse domain graphs remains a complex and non-trivial task."}, {"title": "3 Methodology", "content": "Our preliminary study highlights the limitations of alternative methods and underscores the challenges of semantic isolation in knowledge graph reasoning. We thus propose the Semantic Conditional Reasoning Engine, SCORE, focusing on enhancing the model's foundational ability to exploit semantic features while preserving topological and semantic generalizability across graph domains and tasks. The overall framework of SCORE is illustrated in Figure 2.\n3.1 Architecture of SCORE\nTo perform inductive reasoning on any KGs, SCORE utilizes an encoder-decoder architecture consisting of a CMP-based encoder M_E and a multilayer perceptron (MLP)-based decoder M_D. Given a knowledge graph G = {E,R,T} and the feature matrix X \\in R^{|E|\\times d_o}, which represents the feature vectors of the entities, the encoder generates triple representations conditioned on an input query q = (e_q, r_q). The decoder then computes a plausibility score for the triple, where a higher score indicates a higher likelihood that the triple exists in G. The prediction process can be formalized as follows:\np(q, e_v) = M_D(h_{v|q}, \\Theta_D), H_q = M_E(q, G, X, \\Theta_E), (4)\nwhere h_{v|q} \\in H_q denotes the final node representation of the entity e_v after conditional message passing. Unlike previous CMP-based models, the encoder M_E considers both the graph structure in G and the node features in X. Note that, we focus on node semantics in this work, edge semantics can be supported with trivial modifications to our framework.\nWe then introduce the basic framework of the CMP-based encoder for arbitrary KGs. In general, we first preprocess the feature matrix X and KG data G to construct the unified features X and a relational graph G_r. After that, a two-stage CMP procedure is conducted where query-conditional relation representations are generated in G_r via one CMP-based module and then utilized for entity-level conditional message passing in G. The KG encoding process proceeds as follows:\nG_r = \\text{RelGraph}(G, X), \\quad X = \\text{SemUnifier}(X, d), (5)\nH_q = \\text{SCMP}(q, G, X, R_q, \\Theta_e), \\quad R_q = \\text{CMP}(r_q, G_r, \\Theta_r), (6)\nHere, the SemUnifier module transforms input features into a unified, fixed-dimensional feature space (R^{|E|\\times d}), addressing the feature heterogeneity across domains. Details will be presented in Sec. 3.2. Following previous work [16], the relation graph G_r is constructed to handle unseen relation types, while in SCORE the unified semantic features are involved in this construction process (in Sec. 3.3). After that, we propose a novel Semantic Conditional Message Passing (SCMP) module in Sec. 3.4, which leverages the semantic features sufficiently while addressing potential issues that cause performance degradation in preliminaries. Finally, we describe the specialized training strategy for SCORE in Sec. 3.5."}, {"title": "3.2 Unified Semantic Feature Space", "content": "To enable more effective reasoning across diverse KGs and other graph data represented in KG form, it is essential to address the variability in node semantic features. A unified semantic feature space standardizes these diverse representations, allowing models to generalize more effectively. We begin by introducing two common types of node semantic features in knowledge graphs.\n\\begin{equation}\nX = \\text{LayerNorm}(U_x \\Lambda_x V_x^T), \\quad (U_x, \\Lambda_x, V_x) = \\text{SVD}(X, d), (7)\n\\end{equation}\nwhere LayerNorm() represents the layer normalization function, ensuring numerical stability. If min (d_o, |E|) is smaller than d, SVD will use a reduced rank to decompose X, with the remaining dimensions zero-padded to reach d. This ensures that the unified features X \\in R^{|E|\\times d} maintain a consistent dimensionality d across different graph data. Besides, the relative spatial distances between nodes are preserved in the unified features due to the nature of SVD."}, {"title": "3.3 Semantic-Augmented Relation Graph", "content": "CMP-based models eliminate the necessity for learning unique embeddings for each entity. Instead, they depend on trainable relational representations to facilitate relation-specific message functions. To accommodate varied relational vocabularies in new KGs, recent research [17, 19, 75] emphasizes the importance of identifying the \"invariance\" present in the KG relational structure, thereby enabling any new relation type to be represented using a predefined set of parameters.\nDrawing from insights in prior research [16], we construct a relation graph G_r = {R, R_{fund}, T_r}, where the nodes represent the relations in G, and the edges R_{fund} capture four types of interactions between relations: \"head-to-head\", \"tail-to-tail\", \"head-to-tail\", and \"tail-to-head\". For instance, if two triples (e_1, r_1, e_2) and (e_2, r_2, e_3) are linked tail-to-head, an edge (r_1, \"t-h\u201d, r_2) would be added to T_r. Since these four interaction types are inherently derived from the triple structure in knowledge graphs, the pre-trained embeddings of these interaction types can be universally shared across KGs, allowing for the parameterization of any unseen relations.\nIn our SCORE framework, we refine the relation graph by supplementing the original triple data T with additional edges obtained through semantic augmentation. Specifically, we derive semantic interactions among entities from the unified features X. For each entity e_v, we identify the top k spatially nearest entities in the unified feature space via pairwise similarities, while excluding its direct topological neighbors. The set of semantic neighbors S_{e_v} is defined as follows:\nS_{e_v} = \\{e_i \\in E | e_i \\in \\text{TopKSim}(X, e_v, k, \\delta) \\land e_i \\notin N_e \\}, (8)\nHere, N_e = \\{e_i \\in E | (e_v, r, e_i) \\in T, r \\in R\\} refers to the topological neighbor set of e_v. The hyperparameters k and \\delta refer to the number of neighbors and the similarity threshold, respectively. The semantic interaction between e_v and each element in S_{e_v} is regarded as an additional relation type r_s. Finally, the construction"}, {"title": "3.4 Semantic Conditional Message Passing", "content": "In preliminary results, we observe that incorporating semantic features into CMP is a non-trivial task. Initializing entity representations with node features hinders target node distinguishability in conditional message passing, while converting semantic features into KNN edges exacerbates the GNN oversmoothing issue. To effectively leverage semantic features in the CMP process while avoiding these challenges, we propose a novel message passing framework called Semantic Conditional Message Passing (SCMP), including two core techniques:\nSemantic-injected Entity Initialization: Given a query q = (e_q, r_q), we first recall the initialization function in CMP as follows:\n\\text{INIT}^1 (e_q, r_q, e_v) = 1_{e_q=e_v} * r_q, (12)\nwhere * denotes element-wise multiplication, the function 1_{e_q=e_v}() serves as the indicator function. It produces the all-ones vector 1 when e_q = e_v; otherwise, it outputs the all-zeros vector 0. Instead of using the original semantic features, we inject the semantic neighbor labels into the entity initialization. The improved initialization function is defined as follows:\n\\text{INIT}^2 (e_q, r_q, e_v) = 1_{e_q=e_v} * r_q + 1_{e_v\\in S_{e_q}} * v_a. (13)\nHere, v_a denotes a trainable vector sharing for all semantic neighbors. The neighbor set S_{e_q}, as mentioned in Sec. 3.3, is calculated from the unified feature matrix X. In this schema, the initial representations of these neighbor entities are not all-zeros vector, enabling them propagating effective high-order messages at the beginning of the CMP process. Note that, according to the theoretical study of CMP [24], if we assume r_q \\neq v_a and neither of them contains zero entries, the initialization function INIT^2 satisfies the target node distinguishability assumption. Specifically, for all r_q \\in R and for any e_v \\neq e_q \\in E, it holds that INIT^2 (e_q, e_q, r_q) \\neq INIT^2 (e_q, e_v, r_q).\nGlobal-local Semantic Encoding: Although the improved relation graph and initialization function incorporate high-level semantic associations among entities, the original semantic features remain isolated from the CMP calculations. To address this, the SCMP module employs two CMP channels to encode global and local representations. The local representations are derived from the existing query-specific CMP process, while the global representations are encoded independently of the query, using all-ones vectors for relation-level initialization and semantic features for entity-level initialization. The complete two-channel calculations"}, {"title": "3.5 Domain-Adaptive Model Training", "content": "For zero-shot reasoning on knowledge graphs (KGs), CMP-based models excel in their ability to generalize to unseen KG structural data. Building on this, SCORE is trained to overcome semantic isolation by tackling both variations in feature spaces and differences in domain-specific semantics. With a differentiable training objective L and an evaluation metric C to assess reasoning accuracy, the task of constructing a foundation reasoning engine f_{\\Theta}, parameterized by \\Theta, is formalized as follows:\narg \\max \\sum_{f, L \\\\{G_{inf}\\\\} } C (f_{\\Theta} (G_{inf})), \\Theta = arg \\min \\sum_{\\{\\tilde{G}_{tr}\\\\} } L (f_{\\Theta} (\\tilde{G}_{tr})).\nCMP-based models are typically trained by minimizing the binary cross-entropy loss over positive and negative triples. To handle semantic feature heterogeneity across domains, we pre-train the SCORE model with multiple types of semantic features. Specifically, we use three feature options: textual features, ontology features, and no features. At predefined mini-batch intervals, the feature type is reselected to help the model adapt to diverse input features and improve its generalization ability. The total pretraining"}, {"title": "F Complexity Analysis", "content": "Given a knowledge graph G = (E, R, T), we have that |E|, |R|, |T| represents the size of entities, relation types, and triples, respectively. Gr = (R, Rfund, Tr) denotes the relation graph of G. X \u2208 R|E|\u00d7do is the input feature matrix, d is the hidden dimension of the model, and L is the number of layers in the model. In the preprocessing stage, unifying semantic features requires a time complexity of O(|E|ddo) for the SVD low-rank approximation. Constructing relation graphs involves extracting the top K most similar neighbors for each entity, with a time complexity of O(|E|2(d + log K)), which simplifies to O(|E|2d) as d \u00bb log K. Therefore, the overall complexity is O((d o + |E|)|E|d).\nIn terms of the CMP module, as shown in Zhu et al. [77], the time complexity for a forward pass on G is O(L(|T|d + |E|d 2)) to compute one query reasoning. The runtimes of CMP and SCMP are comparable because SCMP's global encoding is shared across all queries, resulting in only a linear overhead. Combining the CMP calculations on Gr, the total complexity is O((|T| + |Tr|)Ld + (|E| + |R|)Ld2). Because one CMP-based reasoning calculates |E| candidate triples at the same time, resulting an amortized complexity is better than traditional relational message-passing models, such as RGCN [41], CompGCN [52], and GraIL [50]."}]}