{"title": "StreamAdapter: Efficient Test Time Adaptation from Contextual Streams", "authors": ["Dilxat Muhtar", "Yelong Shen", "Yaming Yang", "Xiaodong Liu", "Yadong Lu", "Jianfeng Liu", "Yuefeng Zhan", "Hao Sun", "Weiwei Deng", "Feng Sun", "Xueliang Zhang", "Jianfeng Gao", "Weizhu Chen", "Qi Zhang"], "abstract": "In-context learning (ICL) allows large language models (LLMs) to adapt to new tasks directly from the given demonstrations without requiring gradient updates. While recent advances have expanded context windows to accommodate more demonstrations, this approach increases inference costs without necessarily im- proving performance. To mitigate these issues, We propose StreamAdapter, a novel approach that directly updates model parameters from context at test time, elim- inating the need for explicit in-context demonstrations. StreamAdapter employs context mapping and weight absorption mechanisms to dynamically transform ICL demonstrations into parameter updates with minimal additional parameters. By reducing reliance on numerous in-context examples, StreamAdapter significantly reduce inference costs and allows for efficient inference with constant time com- plexity, regardless of demonstration count. Extensive experiments across diverse tasks and model architectures demonstrate that StreamAdapter achieves compa- rable or superior adaptation capability to ICL while requiring significantly fewer demonstrations. The superior task adaptation and context encoding capabilities of StreamAdapter on both language understanding and generation tasks provides a new perspective for adapting LLMs at test time using context, allowing for more efficient adaptation across scenarios and more cost-effective inference.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have emerged as a powerful tool in natural language process- ing, demonstrating exceptional performance across a diverse range of tasks, including text genera- tion [Yuan et al., 2022], question answering [Kumar et al., 2023], open-ended conversations [Zhang et al., 2023a], and mathematical problem-solving [Shao et al., 2024]. A key factor behind the success of LLMs is their ability to perform in-context learning (ICL) [Brown et al., 2020], where the model adapts to new tasks by conditioning on a small number of input-output demonstrations provided in the context. Without any gradient updates, ICL enables LLMs to acquire new knowledge and capabilities at test time, while also enabling LLMs to solve complex tasks through step-by-step guidance [Wei et al., 2023].\nDespite its remarkable capabilities, ICL faces several limitations that hinder its full potential. Firstly, the effectiveness of ICL heavily depends on the quality and relevance of the provided demonstra- tions, making the selection of appropriate examples a challenging task that often requires domain"}, {"title": "Related Work", "content": ""}, {"title": "In-Context Learning", "content": "ICL enables LLMs to acquire new knowledge or adapt to new tasks using in-context examples at test time without any gradient updates [Brown et al., 2020]. Recent studies show that with proper instruction and more demonstrations, ICL can surpass model fine-tuning and mitigate inherent biases in pre-trained LLMs [Agarwal et al., 2024, Li et al., 2024b]. This exceptional capability has inspired research into ICL's working mechanisms, leading to various hypotheses such as induction heads [Olsson et al., 2022], task vectors [Hendel et al., 2023, Zheng et al., 2024], and structured task hypothesis [Li et al., 2024a]. A popular assumption posits that ICL performs meta-gradient descent during inference. von Oswald et al. [2022] demonstrate how a linear attention-only transformer model can implicitly perform a gradient descent-like procedure, while Dai et al. [2023] compare standard gradient descent-based fine-tuning and ICL, revealing that transformer attention in ICL exhibits a dual form of gradient descent-based optimization. Inspired by these findings, our work seeks to develop a learning algorithm that directly performs parameter updates from the context without backpropagation at test time, aiming to achieve performance similar to ICL while requiring limited or no demonstrations in the context."}, {"title": "Test-Time Adaptation", "content": "Test-time adaptation (TTA) enhances model capabilities at inference by learning directly from test data [Niu et al., 2024]. In-context learning (ICL) represents a form of TTA where models adapt to new tasks using demonstrations within the context at test time. Recent TTA research primarily follows two directions: a) Condition Augmentation: This approach focuses on modifying the context conditioning to improve performance, either through heuristic rules for adjusting conditional prediction distributions [Li et al., 2024c, Zhang et al., 2023c] or through sampling strategies like best-of-N and reward-model based sampling [Cobbe et al., 2021, Chen et al., 2024, Yao et al., 2024]. b) Parameter Updates: This direction explores modifying model parameters at inference time. Early approaches build on fast weight programming [Hinton and Plaut, 1987], exemplified by fast weight programmers [Schlag et al., 2021a] and Hopfield networks [Ramsauer et al., 2020], which update pre-trained weights using input-based products. Meta-learning approaches [Finn et al., 2017, Beck et al., 2023] employ hypernetworks to generate auxiliary parameters for test-time adaptation. TempLORA [Wang et al., 2024] extends this concept by training chunk-specific low-rank adapters [Hu et al., 2021] for next-chunk prediction. Recent work [Sun et al., 2024] formalizes test-time parameter updates through self-supervised learning with TTT-Linear and TTT-MLP, treating model parameters as latent RNN states.\nOur approach, StreamAdapter, aligns with parameter update methods but uniquely maps context directly into parameter updates at test time without backpropagation."}, {"title": "Low-Rank Adaptation", "content": "Inspired by the observation that pre-trained models have low intrinsic dimension during fine-tuning [Aghajanyan et al., 2020], low-rank adaptation (LoRA) [Hu et al., 2021] employs two trainable low-rank matrices to estimate the accumulated gradient updates, thereby adapting pre-trained models with minimal additional parameters. Given its lower inference latency and superior adaptation performance, LoRA has been widely adopted, with subsequent research enhancing its efficiency and stability through dynamic rank allocation across layers [Zhang et al., 2023b] and further matrix decomposition [Liu et al., 2024]. Our work also employs low-rank adaptation to adapt LLMs with minimal parameters. However, instead of training the adapter for specific tasks or datasets, StreamAdapter learns directly from previous context at test time, enabling more customized and flexible adaptation."}, {"title": "Method", "content": "We propose StreamAdapter to directly map contextual information into parameter updates, serving as a temporary weight-level associative memory that encodes new knowledge and adapts to new tasks without relying on full explicit context. The overall structure of StreamAdapteris presented"}, {"title": "Duality between In-Context Learning and Weight Updates", "content": "Recent studies have highlighted the inherent similarities between ICL and parameter updates through gradient descent Dai et al. [2023], von Oswald et al. [2022]. Specifically, let $x_i$ be the current input token, $X'$ be the previous context, and $W_q, W_k, W_v$ be the projection matrices of the self-attention (SA) layer. By approximating standard SA with linear attention, the output of single-head SA is formulated as:\n$F_{ICL}(x_i) \\approx W_v[X', x_i](W_k[X', x_i])^TW_q x_i$\n$\\qquad = W_v x_i (W_k x_i)^T W_q x_i + W_v X' (W_k X')^T W_q x_i$\n$\\qquad = (W_0 + \\Delta W_{ICL})W_q x_i,$\\nwhere $W_0 = W_v x_i (W_k x_i)^T$ are the initial result without any context, and $\\Delta W_{ICL} = W_v X' (W_k X')^T$ represents the \"parameter updates\" obtained from the given context. Moreover, denoting $\\Delta W_k$ and $\\Delta W$ as the accumulated gradient updates from fine-tuning, the result of linear attention can be expressed as:\n$F_{FT}(x_i) = (W_v + \\Delta W_v) x_i (W_k + \\Delta W_k) W_q x_i$\n$\\qquad = (W_0 + \\Delta W_{FT})W_q x_i.$\nFrom the similarity between $F_{ICL}(x_i)$ and $F_{FT}(x_i)$, it can be hypothesized that ICL actually functions as a meta-optimizer, updating the underlying parameters through context-level associations [Dai et al., 2023].\nIn this study, we delve deeper into the potential of leveraging context to directly update model parameters, thereby integrating context information into the model's weights. The objective of StreamAdapter is to learn a mapping function $F$ that, given context $X'$, maps the key-value (KV) caches $W_k X'$ and $W_v X'$ of $X'$ to parameter update $\\Delta W$:\n$F(W_k X', W_v X') \\rightarrow \\Delta W.$\nWe anticipate that updating the model parameters with $\\Delta W$ will achieve results comparable to full ICL without the need for complete demonstrations filling the context window."}, {"title": "Context Mapping", "content": "The KV cache scales linearly with the context, whereas the model's parameters remain constant in size. Consequently, a context mapping strategy that condenses the cache information into a fixed-size state is essential for absorbing context information into the model's weights. The most straightforward approach to achieving this is to compress the KV cache into a latent hidden state, similar to recurrent models [Hochreiter and Schmidhuber, 1997, Gu and Dao, 2023]. However, token-by-token recurrence requires substantial memory, as it necessitates materializing all time step states. To mitigate this issue, we propose splitting the KV cache into fixed-size chunks and leveraging a number of learnable queries to summarize each chunk of caches. We then perform inter-chunk recurrence across each chunk of summarized results to convert the cache into a constant-size context state. More specifically, let the KV cache be denoted as $K, V \\in \\mathbb{R}^{H\\times L\\times d}$, where $H$ is the number of heads, $L$ is the length of cache, and $d$ is the hidden dimension for each head. Let $C$ be the predefined chunk size, and define $K_{[i]} := K_{iC+1:(i+1)C+1} \\in \\mathbb{R}^{H\\times C\\times d}$ as the key cache corresponding to the i-th chunk (with similar notation for $V_{[i]}$). Suppose the learnable query is denoted as $Q \\in \\mathbb{R}^{H\\times r\\times d}$, where $r$ is a hyperparameter determining how many queries are used to summarize the KV cache in the current chunk. For each chunk, StreamAdapter performs multi-head cross-attention between $Q$ and $K_{[i]}$, $V_{[i]}$ to obtain the summarized result $S_i$ for chunk i:\n$S_i = \\text{Softmax}(\\frac{QK_{[i]}}{\\sqrt{d}})V_{[i]} \\in \\mathbb{R}^{r\\times d_{ku}},$"}, {"title": "Weight Absorption", "content": "We expect the context states h to serve as newly learned knowledge from the context, which can be absorbed into the model's weights. Drawing inspiration from the low-rank adaptation method Hu et al. [2021], StreamAdapter assigns learnable queries to each linear layer in the pre-trained model and maps the KV cache corresponding to the block where the current linear layer resides to the context state using these queries. The parameters of the linear layer are then updated by integrating the context state with two low-rank matrices in a sandwich-like structure (Figure 1).\nSpecifically, a typical transformer-based LLMs is built by stacking a series of identical blocks, each containing a multi-head self-attention (MHA) layer and a FFN layer. Each block stores the KV cache computed by its MHA layer. Therefore, for each parameter $W \\in \\mathbb{R}^{d_i\\times d_o}$ (where $d_i$ and $d_o$ denote the input and output dimensions, respectively) of the linear layer in the l-th block, and the stored KV cache $K^l, V^l$ of that block, StreamAdapter assigns a unique learnable query $Q$ to each W and summarize $K^l$ and $V^l$ into the context state h, following Equations 4 and 5. This strategy of summarizing context with a unique query for each parameter allows the compression process"}, {"title": "Training Strategy", "content": "StreamAdapter's reliance on the KV cache for parameter updates necessitates a departure from conventional next-token prediction training methods. To address this, we have developed two distinct training strategies tailored to the specific requirements of language generation and language understanding tasks: sliding window training and in-context training (Figure 2)."}, {"title": "Sliding Window Training", "content": "For general language generation tasks, we employ a sliding window strategy to train Strea- mAdapter for mapping context into parameter updates. This process begins with general language corpora, which we divide into sequences X of length L. For each sequence, we utilize a window size C' and a stride size \u2206. It's important to note that C' here is larger than the context length C used in Section 3.2. We start by initializing the window with the first C' tokens of X. Then, we begin an"}, {"title": "In-Context Training", "content": "To adapt StreamAdapter for language understanding tasks, we employ an in-context training strategy using a selected set of tasks. For each sample in each task's training set, we first randomly sample k examples to form a few-shot context and store their KV caches (1st forward pass without gradient computation). We then update the base model parameters using this cache and compute the loss for the current sample to optimize the parameters introduced by StreamAdapter (2nd forward pass with backpropagation)."}, {"title": "Inference Strategy", "content": "For model inference, inspired by context-locality [Li et al., 2024c, Zhang et al., 2023c], we adopt a hybrid approach tailored to different task types: For language understanding tasks, we convert most demonstrations into weight updates, retaining only a small portion of recent context. For long context generation tasks, we use a sliding window strategy with stride size \u2206 smaller than window size C'. We keep the most recent context intact while transforming the evicted context into temporary model updates via StreamAdapter. This adaptive strategy balances immediate context and adapted knowledge from earlier inputs, optimizing efficiency and performance across different scenarios."}, {"title": "Experiments and Results", "content": "We evaluate StreamAdapter across various model scales and architectures, focusing on both lan- guage understanding tasks and language generation tasks. We also explore the scaling ability of StreamAdapter with different numbers of in-context demonstrations across various tasks and lengths. Additionally, We evaluate StreamAdapter's efficiency and robustness through comprehensive ablation studies and in-depth analyses."}, {"title": "Experimental Setting", "content": "Base Model we select TinyLlama-1.1B [Zhang et al., 2024], LLaMA-3-8B, and Phi-3-Medium [Ab- din et al., 2024] as our base model. In all experiments, we froze the original model weights and only trained the parameters introduced by StreamAdapter.\nBase Setting Without explicit specialization, we apply StreamAdapter to every linear layer of the pre-trained model. The default chunk size C in Section 3.2 is set to 128, and the down-projected value dimension d' in Equation 8 is set to 32 for all base models. When performing chunk-wise cross-attention, in cases where the input KV cache is not divisible by C', we performe an additional cross-attention operation on the remaining KV cache after division and concatenate the result with the chunk-wise result. The number of learnable queries in StreamAdapter is set to 16 for TinyLlama-1.1B and LLaMA-3-8B, and 48 for Phi-3-Medium."}, {"title": "Language Understanding Task", "content": "Training Details For adapting StreamAdapter to language understanding tasks, we employ the in-context training approach introduced in Section 3.4.2. We carefully select several tasks for training. The tasks included BoolQ [Christopher et al., 2019], CoPA [Melissa et al., 2011], SST2 [Richard et al., 2013], CB [De Marneffe et al., 2019], and RTE [Bentivogli et al., 2009]. For each sample in training set across all tasks, we randomly select context examples from the training set for computing the KV cache in the first forward pass. The number of demonstrations tailored to each model's"}, {"title": "Language Generation Task", "content": "Training Details For training StreamAdapter on language generation tasks, we utilize the training set of the PG19 dataset [Rae et al., 2019], employing the sliding window strategy introduced in Section 3.4.1. The sequence length L is set to 8192 for TinyLlama-1.1B, and 16384 for LLaMA-3-8B and Phi-3-Medium. The SW size C' for all models is fixed at 1024, with a stride \u2206 of 512. For additional training hyperparameters, please refer to Appendix A.2.\nEvaluation and Baselines We evaluate StreamAdapter on the PG19 test set using various maximum truncation lengths. For each sample, we employ the sliding window evaluation strategy with a window size C' of 1024 and a stride A of 512. Perplexity is computed in the incoming stride window, and we report the average perplexity across the entire test set. For comparison, we use two baselines: naive"}, {"title": "Analysis", "content": "Efficiency We compare the end-to-end latency and peak memory consumption of model generation with TinyLlama-1.1B across various prefill context lengths. Our evaluation process begins by generating the KV cache for a given prefill context length, followed by measuring the latency of generating 128 tokens using three methods: full context, TempLoRA, and our StreamAdapter.\nThe hyperparameter settings for TempLoRA and StreamAdapter are consistent with those described in Section 4.2.1. All results are averaged across five runs with a single NVIDIA A100-80G GPU.\nThe results, presented in Figure 5, clearly demonstrate that StreamAdapter maintains constant generation latency across different prefill context lengths (i.e., different KV cache sizes). In contrast, the latency of full context generation and TTA with TempLoRA increases almost linearly with the context size. Moreover, TempLoRA's need for gradient backpropagation during adaptation leads to substantial GPU memory consumption as the prefill context increases. While this can be mitigated using sequential chunk-wise adaptation (with a chunk size of 2048 in our setting), this approach increases the generation latency. Conversely, StreamAdapter's recurrent design allows simultaneous mapping of all context without requiring sequential chunk-wise processing. Although StreamAdapter's peak memory consumption also increases with larger prefill contexts, we attribute this to the current implementation materializing all intermediate states. As only the final state is needed, we believe further optimizations, similar to those in [Gu and Dao, 2023], could reduce StreamAdapter's memory demands."}, {"title": "Adaptation Ratio", "content": "In the language understanding tasks described in Section 4.2, we adapt a fixed ratio of context into model weights and evaluate the model with the remaining context in context. To explore the relationship between adaptation ratio and final accuracy on both seen and unseen tasks, we evaluate TinyLlama-1.1B with fixed 10-shot samples across different adaptation ratios.\nThe results of our adaptation ratio analysis are presented in Figure 6. For a more detailed breakdown of acuracy on each individual task, please refer to Appendix C.2. StreamAdapter generally performs better on seen tasks when adapting more demonstrations. For unseen tasks, StreamAdapter outper- forms 10-shot ICL when adapting 10%-80% of demonstrations but shows a decline with extreme adaptation ratios (90% or 100%). Although teh adaptation accuracy remains better than zero-shot prompting, we hypothesize that retaining a small portion of demonstrations is necessary to guide adaptation direction on unseen tasks. This is likely because StreamAdapter learns mapping relations from a limited set of tasks and may adapt the base model in a direction different from the target unseen task. We posit that training StreamAdapter with a more diverse task set could address this issue, which we leave for future work."}, {"title": "Robustness", "content": "We evaluate the influence of using different templates for in-context examples and target evaluated samples to analyze the robustness of different TTA methods as patterns change. For this analysis, we use the TinyLlama-1.1B model trained from Section 4.2. We select three seen tasks (BoolQ, SST2, RTE) and three unseen tasks (ARC-Challenge [Clark et al., 2018], ARC-Easy [Clark et al., 2018], PIQA [Bisk et al., 2020]) to verify the robustness of full in-context learning (ICL) and StreamAdapter. We fix the number of in-context examples at 10, with other details for StreamAdapter remaining the same as in Section 4.2.\nThe results, presented in Table 3, show that although both full ICL and StreamAdapter exhibit degraded accuracy compared to Table 1, StreamAdapter still outperforms ICL on both seen and unseen tasks. Moreover, as illustrated in Figure 7, ICL's average accuracy decreases as the number of in-context examples increases, suggesting that ICL primarily memorizes patterns and fails to adapt when these patterns change. Conversely, StreamAdapter consistently achieves higher accuracy with additional demonstrations, indicating that TTA with StreamAdapter leverages contextual information to enhance model capability rather than simply memorizing task-specific patterns."}, {"title": "Ablation", "content": "We examine the impact of different components and settings of StreamAdapter, focusing our analysis on the TinyLlama-1.1B model and evaluating its adaptation capability on language understanding tasks. Except for the specific parameter settings under investigation, all other training and evaluation settings remain consistent with those described in Section 4.2.\nWe begin by examining the effectiveness of the chunk-wise design and the influence of chunk size on StreamAdapter's performance. For this analysis, we fix the number of queries used to compress each chunk at 16. In the absence of a chunk-wise design, we would directly summarize the entire KV cache using queries with cross-attention to generate new model weights, eliminating the need for inter-chunk recurrence. Table 4 shows that the chunk-wise approach outperforms the non-chunked version, with a chunk size of 128 achieving the best results on both seen (86.99%) and unseen tasks (51.92%). Smaller (64) and larger (256) chunk sizes show suboptimal results, indicating that 128 strikes the right balance in capturing contextual information with 16 queries.\nNext, we examine the effect of different numbers of queries, with results presented in Table 5. As the number of queries per chunk increases, accuracy improves on seen tasks but declines on unseen tasks. We hypothesize that increasing the number of learnable parameters through additional queries causes"}, {"title": "Conclusion", "content": "We introduce StreamAdapter, a novel approach for adapting pretrained LLMs at test time directly from given context. StreamAdapter employs context mapping and weight absorption mechanisms to efficiently transform context tokens into parameter updates, achieving similar or superior results to full-context generation while reducing both memory consumption and inference time. Evaluations across diverse language understanding and generation tasks with various model scales demonstrate StreamAdapter's effectiveness in adapting to new tasks, outperforming fine-tuning and zero-shot prompting, while also surpassing full ICL. Analysis reveals StreamAdapter's superior scalability and robustness across varying context lengths and adaptation ratios, while maintaining constant inference time and memory consumption. These promising results open new avenues for efficient TTA of LLMs, paving the way for more flexible and customized language model deployments. Future work could explore StreamAdapter's application to more diverse tasks and larger model scales, potentially extending its principles to other modalities."}, {"title": "A Training Details", "content": ""}, {"title": "Language Understanding Task", "content": "We use the training sets of BoolQ [Christopher et al., 2019], COPA [Melissa et al., 2011], SST2 [Richard et al., 2013], CB [De Marneffe et al., 2019], and RTE [Bentivogli et al., 2009] for training on language understanding tasks. We construct each sample with pre-defined template, the template for each task is presented in Table 7.\nFor training StreamAdapter, we employ the WarmupCosine learning rate scheduler and the AdamW optimizer with (\u03b2\u2081, \u03b2\u2082) = (0.9, 0.95) and weight decay 0.01 for 3 epochs. The hyperparameters vary across models: for TinyLlama-1.1B, we use a batch size of 16, learning rate of 5 \u00d7 10\u207b\u2075, and 100 warmup steps; for LLaMA-3-8B, a batch size of 4, learning rate of 2 \u00d7 10\u207b\u2075, and 500 warmup steps; and for Phi-3-Medium, a batch size of 2, learning rate of 1 \u00d7 10\u207b\u2075, and 800 warmup steps."}, {"title": "Language Generation Task", "content": "For training on language generation tasks, we utilize the training set of the PG19 dataset. We employ the WarmupCosine learning rate scheduler with 500 warmup steps and the AdamW optimizer with (\u03b2\u2081, \u03b2\u2082) = (0.9, 0.95) and weight decay 0.01 for 1 epoch. The hyperparameters are adjusted for each model: TinyLlama-1.1B uses a batch size of 8 and a learning rate of 5 \u00d7 10\u207b\u2075; LLaMA-3-8B uses a batch size of 4 and a learning rate of 2 \u00d7 10\u207b\u2075; and Phi-3-Medium uses a batch size of 2 and a learning rate of 1 \u00d7 10\u207b\u2075."}, {"title": "B Evaluation Details", "content": ""}, {"title": "Language Understanding Task", "content": "Unless otherwise specified, we use the task templates introduced in lm-evaluation-harness [Gao et al., 2024] for all our evaluations on language understanding tasks. We report the accuracy for task BoolQ, COPA, SST2, CB, RTE, OpenbookQA, ARC-Challenge, Winogrande, PIQA, and ARC-Easy, while report the normalized accuracy for Hellaswag.\nFor a fair comparison when using multi-shot demonstration contexts, we generate the required number of demonstrations from the training set of each task. These same demonstrations are then used as context for evaluating all methods. This approach eliminates potential variability due to demonstration selection, allowing for a more direct comparison of different methods. The results we report are averaged from three independent runs.\nTempLoRA: We apply LORA to every linear layer of the base model and directly train it on the given in-context examples. For optimization, we use the AdamW optimizer with a OneCycleLR learning rate scheduler. The rank and \u03b1 of LoRA are both set to 64 across all models. We use a fixed learning rate of 1 \u00d7 10\u207b\u2075 and train for 5 epochs.\nH2O: We retain 20% of the context, with both the heavy ratio and recent ratio set to 0.1.\nSnapKV: For SnapKV, we allocate 10% of the context for the observation window and retain an additional 10% for inference, leading to a total context retention of 20%.\nStreamAdapter: Unless otherwise specified, we convert 80% of the context into a parameter update, leaving the remaining 20% of the context unchanged."}, {"title": "Language Generation Task", "content": "For TempLoRA, we apply the LoRA adapter to every linear layer, with the rank and \u03b1 both set to 64."}, {"title": "Robustness Analysis", "content": "For evaluting the robustness of ICL and StreamAdapter adaption capability from different prompt template, we use different prompt template for in-contetx examples and targer sample. For in-context"}, {"title": "C Additional Results", "content": ""}, {"title": "Scaling Analysis on Language Understanding Task", "content": "We further present the results on language understanding tasks with varying numbers of demon- strations for TinyLlama-1.1B and Phi-3-Medium in Figure 8 and Figure 9, respectively. These results further demonstrate that StreamAdapter clearly outperforms full ICL and other TTA meth- ods. Moreover, StreamAdapter exhibits better scaling capability as the number of demonstrations increases."}, {"title": "Evaluation with Different Adaptation Ratio", "content": "Table 9 presents the detailed accuracy of StreamAdapteracross different adaptation ratios, as discussed in Section 4.3."}]}