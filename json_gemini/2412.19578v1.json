{"title": "Graph-attention-based Casual Discovery with Trust Region-navigated Clipping Policy Optimization", "authors": ["Shixuan Liu", "Yanghe Feng", "Keyu Wu", "Guangquan Cheng", "Jincai Huang", "Zhong Liu"], "abstract": "In many domains of empirical sciences, discovering the causal structure within variables remains an indispensable task. Recently, to tackle with unoriented edges or latent assumptions violation suffered by conventional methods, researchers formulated a reinforcement learning (RL) procedure for causal discovery, and equipped REINFORCE algorithm to search for the best-rewarded directed acyclic graph. The two keys to the overall performance of the procedure are the robustness of RL methods and the efficient encoding of variables. However, on the one hand, REINFORCE is prone to local convergence and unstable performance during training. Neither trust region policy optimization, being computationally-expensive, nor proximal policy optimization (PPO), suffering from aggregate constraint deviation, is decent alternative for combinatory optimization problems with considerable individual subactions. We propose a trust region-navigated clipping policy optimization method for causal discovery that guarantees both better search efficiency and steadiness in policy optimization, in comparison with REINFORCE, PPO and our prioritized sampling-guided REINFORCE implementation. On the other hand, to boost the efficient encoding of variables, we propose a refined graph attention encoder called SDGAT that can grasp more feature information without priori neighbourhood information. With these improvements, the proposed method outperforms former RL method in both synthetic and benchmark datasets in terms of output results and optimization robustness.", "sections": [{"title": "I. INTRODUCTION", "content": "HE causal structure of the data generation process remains pivotal for many research issues. Ascertaining the causal mechanisms behind natural phenomenon are important in many scientific domains, e.g. we can hope to develop new drugs or prevent epidemic outbreak given the knowledge of virus mechanism [1]. Recent research has also shown that the integration of causal discovery is beneficial to semi-supervised learning and transfer learning tasks, e.g. in func-tion estimation cases with inferred causality [2]. Although controlled randomized experiment is the common approach for causality discovery, yet in some scientific domains, it is almost impossible to simulate such experiment [3]. In this sense, recent researches in causal discovery methods focus on inferring causality from passively observable data [4].\nScore-based methods formulate such structure discovery problem as adjusting the graph adjacency matrix to minimize predefined score function with acyclicity constraints. However, given the gigantic search space, whose complexity would rocket exponentially as the number of nodes grows, the optimization problem remains hideously tough to solve [5]. Fascinated by the search efficiency and ability to incorporate multiple indifferentiable score functions and constraints, a reinforcement learning (RL) procedure [6] was formulated recently to search for the best-scored directed acyclic graph (DAG) with Bayesian information criterion (BIC) [7] as the score function using an encoder-decoder network architecture. The insight is, after graph generation using the observed data, a specific reinforcement learning agent optimize its policy w.r.t a stream of reward signals that comprised of the score function and acyclicity constraints calculated for the generated graph. The overall performance of such RL procedure relies heavily on the efficiency of RL methods and the compact encoding of variables that best capture the intrinsic relations.\nFirstly, from a RL perspective, this paradigm has not reached its ultimate potential as the adopted REINFORCE [8], [9] algorithm is susceptible to local convergence and poor search efficiency. Although trust region policy optimization (TRPO) [10] can guarantee a reliable and steady performance in resolving these issues, it is computationally expensive even with some approximation methods like conjugate gradient. On the other hand, even though proximal policy optimization (PPO) [11] entails some of the benefits of TRPO and is simpler to implement, yet we find that the deviation from trust region constraints, caused by trivially-bounded clipping, shall bring exponentially-large aggregate deviation after product calculation during training when the action is comprised of considerable subactions. This results in aberrant exploratory behaviours as the agent cannot optimize some sub-action or make them fall in local optima, particularly when they are not favored by the old policy.\nSecondly, despite Graph Attention Network (GAT) [12] outperforms other state-of-the-art graph neural networks in both transductive and inductive graph benchmark test, it still fails to provide a robust causal understanding in our task as the simple additive mechanism cannot capture the interrelations among variables without prior adjacency information."}, {"title": "II. RELATED WORK", "content": "Traditional causal discovery methods are comprised of score-based, constraint-based and hybrid methods. The prevailing score-based methods rely on pre-defined score functions to model the causal problem as searching over the DAGs space for the best-scored DAG. However, this problem remains NP-hard [5] to solve, therefore for practical problems with sizable node set, approximate search with extra structure assumption is often adopted. [13] There are also hybrid methods that reduce the score-based search space given some assumed constraints [14], but these methods lack formalization on score functions and heuristics strategies.\nZheng et al. [15] formulate an equivalent acyclicity constraint with a continuous function of the adjacency matrix, which change the combinatorial nature of the problem to a continuous optimization problem. Despite this optimization problem only has stationary-point solutions rather than global optimum given the nonlinear nature, such local solutions are empirically highly comparable to the global solutions with expensive combinatorial searches. However, many ef-fective score functions, e.g. generalized score function [16] and independence-based score functions [17], cannot be in-corporate with this approach as they either can be far too complicated or cannot be represented in closed forms.\nSpurred by the applications of machine learning [18] and deep learning [19] in industrial systems [20] and traffic man-agement [21], recent advancement in neural networks also gives rise to the surface of neural-network-based causal dis-covery approaches. To learn the generative model of the joint distribution, Causal Generative Neural Network [22], which is trained by minimizing the maximum mean discrepancy, is proposed. Kalainathan et al. [23] present a GAN-style method called Structural Agnostic Modelling, to recover full causal models from continuous observational data in an adversarial way. Still, Yu et al. [24] propose DAG-GNN to generate DAG with an innovative graph neural network (GNN) architecture.\nTo extend neural networks to deal with arbitrarily-structured graphs, graph neural networks were introduced in Gori et al. [25] and Scarselli et al. [26], to directly deal with a general class of graphs. Nevertheless, as an attempt to generalize convolutions to the graph domain, both spectral [27] and non-spectral [28] approaches are proposed. Nevertheless, as attention mechanisms have de facto become a norm in many sequence-to-sequence problems, Veli\u010dkovi\u0107 et al. [12] incorpo-rate self-attention into the propagation step, achieving decent performance on both transductive and inductive tasks. Graph embedding\nReinforcement Learning (RL) methods also spur many applications in recent decades [29]. Apart from its mostly renowned application in gaming, RL also shows decent appli-cability, robustness and generalization in robotics [30], [31], object localization [32] and industrial processes [33]. As an important insight for this paper, RL is also adopted to carry out neural architectural search, and achieved human-level results with robustness [34]."}, {"title": "III. PRELIMINARIES", "content": "Causal graphs are all DAGs, and the objective of this paper is to search for the best-scored DAG G that best describes the data generation process:\n$\\min_{G \\in DAGS} S(G)$  (1)\nBefore we detail the graph generation specifications and learning methods, we specify the causal model adopted in this paper and formalize RL w.r.t the decision process as they are the foundations for further studies."}, {"title": "A. Causal Model Definition", "content": "Consider a finite observed random variables $X = (X_i)_{i=1, ..., n}$, each scalar variable $x_i \\in X_i$ is associated with a node i in graph $G = (V,E)$ that consists of n nodes V and edges $E \\subset V^2$. A node i can be regarded as a parent of j if (i, j) $\\in$ E. For any v $\\in$ V, (v, v) $\\notin$ E. In this paper, we consider the data generation procedure as a DAG-based additive noise model, in which the observed value of $x_i$ is calculated by a function $f_i$ with variables on its parents node set $PA_i$ in G as inputs, along with an independent additive noise $N_i$,\n$x_i = f_i(PA_i) + N_i, i = 1, ..., n$ (2)\nIt is assumed that all noise variables $N_i$ have a strictly positive density and are jointly independent and thereby causal minimality reduces to that each function $f_i$ is not constant in any of its arguments [17]. The above model can only be up to Markov equivalence class (DAGs set that encode the identical"}, {"title": "B. Reinforcement Learning Formalization", "content": "The RL model for causal discovery is shown in Fig. 1, which follows the actor-critic pattern. With this model, the objective of causal discovery is to discover the hidden causal DAG by continuously using the sampled variables $S_t$ from the observed data X. Based on an encoder-decoder module, the actor generates the adjacency matrix for graph G according to $S_t$. Given the actor output $A_t$, the reward module then calculates the reward $R_t$ for the critic to estimate the value function $V(S_t)$, with which $R_t$ forms the advantage signal that optimize the actor. To give a basic insight of the learning process, a training illustration using REINFORCE is shown in Fig. 2.\n1) Samples, Actions and Policy:\n$\\bullet$ $S \\in R^{n \\times m}$ denotes the sample where m is the feature depth for the sample. During training, $S_t$ is randomly sampled from the whole observed dataset X at time t. Since the dataset X does not change over time, the probability of $S_t$ being sampled remains the same.\n$\\bullet$ $A_t \\in {0,1}^{n \\times n}$ denotes the action and can be understood as the generated binary adjacency matrix. At time t, $A_t = (a^t_{ij})_{i,j=1...n}$, and its subaction $a^t_{ij} = 1$ implies that there is an edge from node i to node j at time t. Each subaction $a^t_{ij}$ is generated according to a Bernouli distribution with the value of a given subpolicy."}, {"title": "2) Rewards:", "content": "$R \\in R$ is the reward signal which incorporates both score function and acyclicity constraints. Traditional score-based methods adopt a parametric model for causality, which introduces a set of parameters $\\mu$.\nBayesian information criterion (BIC) is an asymptotic met-ric that measures the fitness of estimated models on a fixed dataset with identical numerical values for dependent variables and model with the lowest value of BIC is preferred. We leverage BIC as the score function for its consistency and decomposability. The BIC score for adjacency matrix $A_t$ given the whole dataset X is,\n$BIC(A_t) = -2 ln p (X; \\hat{\\mu}, A_t) + d_{\\mu} ln M$ (4)\nwhere $\\hat{\\mu}$ denotes the maximum likelihood estimator parametrized by $\\mu$ and $d_{\\mu}$ is the dimensionality of $\\mu$. If each causal relation is modelled using linear models, the BIC score can be given as,\n$BIC_1(A_t) = \\sum_{i=1}^{n} \\left(M *ln(\\frac{\\sum_{j=1}^{M} (X_{ij} - \\hat{X_{ij}})^2}{M})\\right) + n_e ln M$ (5)\nwhere $\\hat{X}_{ij}$ denotes the estimate for $X_{ij}$, the i-th entry in the k-th sample, using entries from parent node sets as indicated by $A_t$. $n_e$ is the number of edges, and the second term is therefore used to penalise edges redundancy. If we"}, {"title": "IV. NEURAL NETWORK ARCHITECTURE FOR GRAPH GENERATION", "content": "To discover the causality relations that best describes the gen-eration process described in Equation (2) from the observed data, we need to design a proper neural network architecture for graph generation. Encoder-decoder architecture has been the top priority for many sequence-to-sequence tasks like machine translation [38] and text summarization [39], and it is also adopted in this paper. As shown in Fig. 1, encoder and decoder together form the actore module parameterized by $\\theta$ that outputs the graph adjacency matrix $A_t$ given $S_t$. The encoder shall comprehend the intrinsic relation among variables and output an encoding $enc_i$ that best describes causality whilst the decoder shall use the encodings to interpret the interrelations among variables."}, {"title": "A. Scaled Dot-Product Graph Attention Encoder", "content": "In order to provide sufficient interactions among variables, the design of GAT is often adopted by recent researches. However, in our experiment, its performance turns out to be miserable in terms of the generated graph and convergence speed, which will be analyzed in the experiment section. As there is no priori knowledge of the graph structure, the original model, ignoring all neighbourhood status, computes the mutual information among all node pairs. The two most commonly adopted attention functions are additive attention and multiplicative attention, and we note that the additive attention adopted in GAT cannot efficiently apprehend the interrelations among variables without adjacency information.\nWe replace the original additive attention in GAT with scaled dot-product attention and they together form the SDGAT. Similar to GAT, SDGAT is also stacked by $n_s$ single attentional layers. For a set of n-node variables with m features, $h = {h_1, h_2, ..., h_n}, h_i \\in R^m$, a single layer shall output a new set of n-node variables of cardinality m'.\nAs shown in Fig. 3, a two-hierarchical multi-head designs is employed in SDGAT. The first hierarchy resembles the counterpart in GAT whilst the second follows the pattern in Transformers. In the first hierarchy, h is duplicated as $n_{gat}$ copies, each represented by $h^{(p)} \\in R^{n \\times m}$ and indexed by p. Each $h^{(p)}$ is then plugged in parallel into the second hierarchy that has $n_{sd}$ heads, each of which follow the basic structure of scaled dot-product attention. In the (p,q)-th sublayer, the query $Q^{pq}$, key $K^{pq}$ and value $V^{pq}$ are obtained after plugging $h^{(p)}$ into three sets of independent linear projections,\n$Q^{pq} = h^{(p)}W^{pq}_Q, K^{pq} = h^{(p)}W^{pq}_K, V^{pq} = h^{(p)}W^{pq}_V$ (10)\nwhere $W^{pq}_Q, W^{pq}_K \\in R^{m \\times d_k}, W^{pq}_V \\in R^{m \\times \\frac{m'}{n_{gat}n_{sd}}}$, and $d_k$ is the hidden dimension size for $Q^{pq}$ and $K^{pq}$. With corresponding queries and keys, all the attention matrices $a^{pq} \\in R^{n \\times n}$ for the (p, q)-th sublayer are computed simulta-neously as,\n$a^{pq} = softmax(\\frac{Q^{pq}{K^{pq}}^T}{\\sqrt{d_k}}) \\\\ = softmax(\\frac{h^{(p)}W^{pq}_Q({h^{(p)}W^{pq}_K})^T}{\\sqrt{d_k}}) = softmax(\\frac{h^{(p)}W^{pq}_Q({W^{pq}_K})^Th^{(p)^T}}{\\sqrt{d_k}})$ (11)\nwhere $a^{pq}_{ij} \\in a^{pq}$ indicates the importance level of node j's features to node i and $\\sqrt{d_k}$ is the corresponding scaling factor. In the current setting, the model allows every node to get connected to other nodes as there is no initial structural information before graph generation. However, given existed graph structure, we could inject structure information into the"}, {"title": "V. REINFORCEMENT LEARNING ALGORITHMS FOR CAUSAL DISCOVERY", "content": "As we leverage RL agent to search over the DAG space, its search efficiency shall be optimized to its best. In this section, we specify the traditional and proposed RL methods that boost its search ability."}, {"title": "A. REINFORCE with Moving Average Baseline", "content": "Algorithm 1 REINFORCE with Moving Average Baseline\nRequire: Batch size N; Moving average update rate am;\nLearning rates for actor and critic $\\alpha_{\\theta}$, $\\alpha_{\\omega}$; Entropy term weight $\\lambda_{\\epsilon}$;\n1: for t = 1, 2, ... do\n2: Generate online experience batch {$S_t, A_t, R_t$}$_N$\n3: $R_m \\leftarrow (1-a_m){R_t}_N+a_mR_m$\n4: {$\\hat{A_t}$}$_N \\leftarrow {R_t}_N - R_m - V_{\\omega}({S_t}_N)$\n5: $L_e \\leftarrow  {{\\hat{A_t} * [\\sum\\ln(\\pi_{\\theta}(A^t_{ij} | S_t))]}N +\\lambda_{\\epsilon}{H(\\pi_{\\theta}(\u00b7 | S_t))}_N$\n6: $L_w \\leftarrow ({R_t}_N - R_m - V_{\\omega}({S_t}_N))^2$\n7: Minimize surrogate $L_e, L_w$ w.r.t $\\theta$ and $\\omega$ with learning rate $\\alpha_{\\theta}, \\alpha_{\\omega}$\n8: end for\nAccording to REINFORCE [8], [9] and baseline techniques, Equation (9) can be optimized in the direction of,\n$\\nabla J(\\theta | S) = {\\hat{A_t} * \\nabla ln P_{\\theta}(A_t | S_t)}\n= ({R_t - R_m - V_{\\omega}(S_t)) * \\nabla ln (\\pi_{\\theta}(A^t_{ij} | S_t))$ (14)\n$\\hat{A_t}$ denotes the advantage function estimated with $R_t-R_m-V_{\\omega} (S_t)$, where $R_m$ indicates the moving average, and $V_{\\omega} (S_t)$ is the value function estimated w.r.t. the encoding {enc} by the critic module parameterized by $\\omega$. $R_m$, updated by a batch of $R_t$ with rate $a_m \\in (0,1)$, is used to stabilize the training process by reducing the variance of the parametric baseline, which will be further discussed in Appendix. The critic is a 2-layer feed-forward network with ReLU layers in our experiment and trained to minimize the mean squared error $L_w$, between its value function estimate and the rewards with the moving average.\nIt is shown in [40] that an extra entropy regularization term can encourage the exploratory behaviour of the agent, and therefore we add it to the surrogate loss $L_e$ as,\nH (${\\pi_{\\theta}}(\u00b7 | S_t))$ = -${{\\pi_{\\theta}}(\u00b7 | S_t)}) ln ({\\pi_{\\theta}}(\u00b7 | S_t))$ (15)\nThe REINFORCE with moving average baseline is given in Algorithm 1, where {\u00b7}$_N$ means a batch of items and {\u00b7}$_N$ denotes an averaging operation for the batch. Through minimizing two surrogate losses $L_e$ and $L_w$, the actor and critic modules are trained using Adam optimizer with learning rates $\\alpha_{\\theta}, \\alpha_{\\omega}$ respectively."}, {"title": "B. Prioritized Sampling-Guided REINFORCE (PSR)", "content": "Algorithm 2 Prioritized Sampling-Guided REINFORCE\nRequire: Replayer R with size $S_R$; Batch size N; Moving average update rate am; Learning rates for actor and critic $\\alpha_{\\theta}$, $\\alpha_{\\omega}$; Entropy term weight $\\lambda_{\\epsilon}$;\n1: for t = 1, 2, ... do\n2: {$A'_t$}$_N \\leftarrow {R_t}_N + R_m - V_{\\omega}({S_t}_N)$\n3: Store {$S_t, A_t, R_t, \\hat{A_t}$}$_N$ into R\n4: Sample {$S'_t, A'_t, R'_t$}$'_N$ from R w.r.t Equation (16)\n5: $R_m \\leftarrow (1 - a_m){R'_t}_N + a_mR_m$\n6: {$\\hat{A'_t}$}$_N \\leftarrow {R'_t}_N + R_m - V_{\\omega}({S'_t}_N)$\n7: $L_e \\leftarrow  {{{\\hat{A'_t} * [\\sum\\ln(\\pi_{\\theta} (A'_{ij} | S'_t))]}N +\\lambda_{\\epsilon}{H(\\pi_{\\theta}(\u00b7 | S'_t))}_N$\n8: $L_{\\epsilon} \\leftarrow ({R'_t}_N + R_m - V_{\\omega}({S'_t}_N))^2$\n9: Minimize surrogate $L_e, L_w$ w.r.t $\\theta$ and $\\omega$ with learning rate $\\alpha_{\\theta}, \\alpha_{\\omega}$\n10: end for\nWe start to refine the original training method by incorporat-ing prioritized sampling. The insight to adopt a replay buffer in this paper is to remember rare but latently useful experience for update and to help ensure the i.i.d assumption of estimates during update. On this basis, prioritized sampling offers a better strategy to select and reuse some useful experience"}, {"title": "C. Trust Region-navigated Clipping Policy Optimization (TRC)", "content": "TRPO addresses the aforementioned issue by imposing a trust region constraint on the objective function to limit the KL divergence between the old and new policies. Specifically, in our paper, the update direction and its constraint are given as,\n$\\nabla J(\\theta|S) = {\\hat{A_t}* \\nabla \\frac{P_{\\theta}(A_t | S_t)}{P_b(A_t | S_t)}}$,\ns.t. $D(b, \\pi_{\\theta}|A_t, S_t) < \\sigma$\n${\\hat{A_t} * \\Pi {ratio_{i,j}}}$ (17)\nwhere b($S_t$) represents the old policy. As noted in III-B1, each entry in $\\pi_{\\theta}(A_t | S_t)$ represents the probability for an independent subaction whose action space is {0,1}, thus the Kullback-Leibler (KL) divergence $D_{KL}(b, \\pi_{\\theta}|A_t, S_t)$, between old and new policies given $A_t$ and $S_t$, is calculated as,\n$D_{KL}(b, \\pi_{\\theta}|A_t | S_t) = b^t_{ij}(A_t | S_t) * ln \\frac{b^t_{ij}(A_t | S_t)}{{{\\pi_{\\theta}}^t_{ij} (A_t | S_t)}}\n + (1-b^t_{ij} (A_t | S_t)) * ln \\frac{1 - b^t_{ij} (A_t | S_t)}{{1 - {\\pi_{\\theta}}^t_{ij} (A_t | S_t)}}$ (18)"}]}