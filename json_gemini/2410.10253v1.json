{"title": "FEEDBACK FAVORS THE GENERALIZATION OF NEURAL ODES", "authors": ["Jindou Jia", "Zihan Yang", "Meng Wang", "Kexin Guo", "Jianfei Yang", "Xiang Yu", "Lei Guo"], "abstract": "The well-known generalization problem hinders the application of artificial neural networks in continuous-time prediction tasks with varying latent dynamics. In sharp contrast, biological systems can neatly adapt to evolving environments benefiting from real-time feedback mechanisms. Inspired by the feedback philosophy, we present feedback neural networks, showing that a feedback loop can flexibly correct the learned latent dynamics of neural ordinary differential equations (neural ODEs), leading to a prominent generalization improvement. The feedback neural network is a novel two-DOF neural network, which possesses robust performance in unseen scenarios with no loss of accuracy performance on previous tasks. A linear feedback form is presented to correct the learned latent dynamics firstly, with a convergence guarantee. Then, domain randomization is utilized to learn a nonlinear neural feedback form. Finally, extensive tests including trajectory prediction of a real irregular object and model predictive control of a quadrotor with various uncertainties, are implemented, indicating significant improvements over state-of-the-art model-based and learning-based methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Stemming from residual neural networks (He et al., 2016), neural ordinary differential equation (neural ODE) (Chen et al., 2018) emerges as a novel learning strategy aiming at learning the latent dynamic model of an unknown system. Recently, neural ODEs have been successfully applied to various scenarios, especially continuous-time missions (Liu & Stacey, 2024; Verma et al., 2024; Greydanus et al., 2019; Cranmer et al., 2020). However, like traditional neural networks, the generalization problem limits the application of neural ODEs in real-world applications.\nTraditional strategies like model simplification, fit coarsening, data augmentation, and transfer learning have considerably improved the generalization performance of neural networks on unseen tasks (Rohlfs, 2022). However, these strategies usually reduce the accuracy performance on previous tasks, and large-scale training data and network structures are often required to approximate previous accuracy. The objective of this work is to develop a novel network architecture, acquiring the generalization improvement while preserving the accuracy performance.\nLiving beings can neatly adapt to unseen environments, even with limited neurons and computing power. One reason can be attributed to the existence of internal feedback (Aoki et al., 2019). Internal feedback has been shown to exist in biological control, perception, and communication systems, handling external disturbances, internal uncertainties, and noises (Sarma et al., 2022; Markov et al., 2021). In neural circuits, feedback inhibition is able to regulate the duration and magnitude of excitatory signals (Luo, 2021). In engineering systems, internal feedback indicates impressive effects across filtering and control tasks, such as Kalman filter (Kalman, 1960), Luenberger observer (Luenberger, 1966), extended state observer (Guo et al., 2020), and proportional-integral-derivative control (Ang et al., 2005). The effectiveness of feedback lies in its ability to harness real-time deviations between internal predictions/estimations and external measurements to infer dynamical uncertainties. The cognitive corrections are then performed timely. However, existing neural networks rarely incorporate such a real-time feedback mechanism.\nIn this work, we attempt to enhance the generalization of neural ODEs by incorporating the feedback scheme. The key idea is to correct the learned latent dynamical model of a Neural ODE according to the deviation between measured and predicted states, as illustrated in Figure 1. We introduce two types of feedback: linear form and nonlinear neural form. Unlike previous training methods that compromise accuracy for generalization, the developed feedback neural network is a two-DOF framework that exhibits generalization performance on unseen tasks while maintaining accuracy on previous tasks. The effectiveness of the presented feedback neural network is demonstrated through several intuitional and practical examples, including trajectory prediction of a spiral curve, trajectory prediction of an irregular object and model predictive control (MPC) of a quadrotor."}, {"title": "2 NEURAL ODES AND LEARNING RESIDUES", "content": "A significant application of artificial neural networks centers around the prediction task., x(t) \u2192 x(t + At). Note that t indicates the input \u00e6 evolves with time. Chen et al. (2018) utilize neural networks to directly learn latent ODEs of target systems, named Neural ODEs. Neural ODEs greatly improve the modeling ability of neural networks, especially for continuous-time dynamic systems (Massaroli et al., 2020), while maintaining a constant memory cost. The ODE describes the instantaneous change of a state x(t)\n$\\frac{dx(t)}{dt} = f(x(t), I(t), t)$                                                                                                          (1)\nwhere f(\u00b7) represents a latent nonlinear mapping, and I(t) denotes external input. Note that compared with Chen et al. (2018), we further consider I(t) that can extend the ODE to non-autonomous cases. The adjoint sensitive method is employed in Chen et al. (2018) to train neural ODEs without considering I(t). In Appendix A.1, we provide an alternative training strategy in the presence of I(t), from the view of optimal control.\nGiven the ODE (1) and an initial state x(t), future state can be predicted as an initial value problem\nx(t + \u2206t) = x(t) + $\\int_{t}^{t+\u2206t} f(x(\u03c4), I(\u03c4), \u03c4) d\u03c4$.                                                                        (2)\nThe workflow of neural ODEs is depicted in Figure 1. However, like traditional learning methods, generalization is a major bottleneck for neural ODEs (Marion, 2024). Learning residuals will appear if the network has not been trained properly (e.g., underfitting and overfitting) or the applied scenario has a slightly different latent dynamic model. Take a spiral function as an example (Appendix A.3.1). When a network trained from a given training set (Figure 5 (a)) is transferred to a new case (Figure 5 (b)), the learning performance will dramatically degrade (Figure 5 (d)). Without loss of generality, the learning residual error is formalized as\nf(x(t), I(t), t) = $f_{neural}(x(t), I(t), t, \u03b8) + \u2206f(t)$                                                                    (3)\nwhere $f_{neural}$ (\u00b7) represents the learned ODE model parameterized by \u03b8, and \u2206f(t) denotes the unknown learning residual error. In the presence of \u2206f(t), the prediction error of (2) will accumulate over time. The objective of this work is to improve neural ODEs with as few modifications as possible to suppress the effects of \u2206 f(t)."}, {"title": "3 NEURAL ODES WITH A LINEAR FEEDBACK", "content": "Even though learned experiences are encoded by neurons in the brain, living organisms can still adeptly handle unexpected internal and external disturbances with the assistance of feedback mechanisms (Aoki et al., 2019; Sarma et al., 2022). The feedback scheme has also proven effective in traditional control systems, facilitating high-performance estimation and control objectives. Examples include Kalman filter (Kalman, 1960), Luenberger observer (Luenberger, 1966), extended state observer (Guo et al., 2020), and proportional-integral-derivative control (Ang et al., 2005).\nWe attempt to introduce the feedback scheme into neural ODEs, named feedback neural networks, as shown in Figure 1. Neural ODEs have exploited latent dynamical models $f_{neural}(t)$ of target systems in training set. The key idea of feedback neural networks is to further correct $f_{neural}(t)$ according to state feedback. Denote $t_i$ as the historical evaluation moment satisfying $t_i < t$. At current moment t, we collect k historical state measurements {$x(t_0), x(t_1),\u2026, x(t_k)$}, in which $t_k$ = t. As portrayed in Figure 2, $f_{neural}(t)$ is modified by historical evaluation errors to approach the truth dynamics f(t), i.e.,\n$f_{neural}(t) = f_{neural}(t) + \\sum_{i=0}^{k} L (x(t_i) - \\hat{x}(t_i))$                                                                    (4)\nwhere L represents the positive definite gain and $\\hat{x}(t_i)$ represents the predicted state from the last evaluation moment, e.g., an Euler integration\n$\\hat{x} (t_i) = x (t_{i\u22121}) + T_s f_{neural}(t_{i\u22121})$                                                                               (5)\nwith the prediction step $T_s$.\nTo avoid storing more and more historical measurements over time, define an auxiliary variable\n$\\tilde{x}(t) = x(t) - \\sum_{i=0}^{k-1} (x (t_i) - \\hat{x} (t_i))$                                                                                   (6)\nwhere $\\tilde{x}$ (t) can be regarded as an estimation of x (t). Combining (4) and (6), can lead to\n$f_{neural}(t) = f_{neural}(t) + L(x(t) \u2013 \\tilde{x}(t))$.                                                                                    (7)\nFrom (5) and (6), it can be further rendered that\n$\\tilde{x} (t_k) = \\tilde{x} (t_{k\u22121}) + T_s f_{neural}(t_{k\u22121})$.                                                                                 (8)\nBy continuating the above Euler integration, it can be seen that $\\tilde{x}(t)$ is the continuous state of the modified dynamics, i.e., $\\tilde{x}(t) = f_{neural}(t)$. Finally, $f_{neural}(t)$ can be persistently obtained through (7) and (8) recursively, instead of (4) and (5) accumulatively."}, {"title": "3.2 CONVERGENCE ANALYSIS", "content": "In this part, the convergence property of the feedback neural network is analyzed. The state observation error of the feedback neural network is defined as $\\bar{x}(t) = x(t) \u2212 \\tilde{x}(t)$, and its derivative $\\dot{\\bar{x}}(t)$, i.e., the approximated error of latent dynamics is defied as $\\bar{f}(t) = f(t) \u2013 f_{neural}(t)$. Substitute (1) and (3) into (7), one can obtain the error dynamics\n$\\dot{\\bar{x}}(t) = \u2212L\\bar{x}(t) + \u2206f(t)$.                                                                                                     (9)\nBefore proceeding, a reasonable bounded assumption on the learning residual error \u2206f(t) is made."}, {"title": "3.3 MULTI-STEP PREDICTION", "content": "With the modified dynamics $\\hat{f}(t)$ and current x(t), the next step is to predict x(t + \u2206t) as in (2). By defining z(t) = $[x(t)^T, \\tilde{x}(t)^T]$, from (8), we have\n$\\dot{z}(t) = [\\hat{f}^T (t), \\tilde{f}^T (t)]$                                                                                                              (1)\nOne intuitional means to obtain $\\dot{z}(t + \u2206t)$ is to solve the ODE problem with modern solvers. However, as shown in Theorem 1, the convergence of $\\hat{f}(t)$ can only be guaranteed as current t. In other words, the one-step prediction result by solving the above ODE is accurate, while the error will accumulate in the long-term prediction. In this part, an alternative multi-step prediction strategy is developed to circumvent this problem.\nThe proposed multi-step prediction strategy is portrayed in Figure 3, which can be regarded as a cascaded form of one-step prediction. The output of each feedback neural network is regarded as the input of the next layer. Take the first two layers as an example. The first-step prediction x(t + Ts) is obtained by x(t + Ts) = x(t) + f(x(t), \\tilde{x}(t), \u03b8)Ts. The second layer with the input of x(t + Ts) will output x(t + 2Ts). In such a framework, the convergence of later layers will not affect the convergence of previous layers. Thus, the prediction error will converge from top to bottom in order.\nNote that the cascaded prediction strategy can amplify the data noise in case of large L. A gain decay strategy is designed to alleviate this issue. Denote the feedback gain of i-th later as Li, which decays as i increases\n$L_i = L e^{-\u03b2i}$                                                                                                                               (11)"}, {"title": "3.4 GAIN ADJUSTMENT", "content": "The adjustment of linear feedback gain L can be separated from the training of neural ODEs, which can increase the flexibility of the structure. In other words, the feedback loop can be easily embedded into existing trained neural ODEs, without retraining.\nThe gain adjustment strategy is intuitional. Theorem 1 indicates that the prediction error will converge to a bounded set as the minimum eigenvalue of feedback gain is greater than 1/2. And the converged set can shrink with the increase of the minimum eigenvalue. In reality, the amplitude of $A_m(L)$ is limited since the feedback \u00e6 is usually noised. The manual adjustment of $A_m(L)$ needs the trade-off between prediction accuracy and noise amplification."}, {"title": "4 NEURAL ODES WITH A NEURAL FEEDBACK", "content": "Section 3 has shown a linear feedback form can promptly improve the adaptability of neural ODEs in unseen scenarios. However, two improvements could be further made. At first, it will be more practical if the gain tuning procedure could be avoided. Moreover, the linear feedback form can be extended to a nonlinear one $h(x(t) \u2013 \\tilde{x}(t))$ to adopt more intricate scenes, as experienced in the control field (Han, 2009).\nAn effectual solution is to model the feedback part using another neural network, i.e., $h_{neural}(x(t) \u2013 \\tilde{x}(t), \u00a7)$ parameterized by \u00a7. Here we design a separate learning strategy to learn \u00a7. At first, the neural ODE is trained on the nominal task without considering the feedback part. Then the feedback part is trained through domain randomization by freezing the neural ODE. In this way, the obtained feedback neural network is skillfully considered as a two-DOF network. On the one hand, the original neural ODE preserves the accuracy on the previous nominal task. On the other hand, with the aid of feedback, the generalization performance is available in the presence of unknown uncertainties."}, {"title": "4.1 DOMAIN RANDOMIZATION", "content": "The key idea of domain randomization (Tobin et al., 2017; Peng et al., 2018) is to randomize the system parameters, noises, and perturbations as collecting training data so that the real applied case can be covered as much as possible. Taking the spiral example as an example (Figure 5 (a)), training with domain randomization requires datasets collected under various periods, decay rates, and bias parameters, so that the learned networks are robust to the real case with a certain of uncertainty.\nTwo shortcomings exist when employing domain randomization. On the one hand, the existing trained network needs to be retrained and the computation burden of training is dramatically increased. On the other hand, the training objective is forced to focus on the average performance among different parameters, such that the prediction ability on the previous nominal task will degraded, as shown in Figure 6 (a). To maintain the previous accuracy performance, larger-scale network designs are often required. In other words, the domain randomization trades precision for robustness. In the proposed learning strategy, the generalization ability is endowed to the feedback loop independently, so that the above shortcomings can be circumvented."}, {"title": "4.2 LEARNING A NEURAL FEEDBACK", "content": "In this work, we specialize the virtue from domain randomization to the feedback part $h_{neural}(t)$ rather than the previous neural network $f_{neural}(t)$. The training framework is formalized as follows\n$\\xi^{*} = arg min_{\\xi} \\sum_{i=1}^{N_{case}} \\sum_{j \\in D_{tra}} ||x_{i,j} - \\hat{x}_{i,j} ||$\ns.t.\n$\\hat{x}_{i,j} = x_{i,j-1} + T_s (f_{neural}(x_{i,j-1}) + h_{neural} (x_{i,j-1} - \\hat{x}_{i,j-1}, \\xi))$\nwhere $n_{case}$ denotes the number of randomized cases, $D_{tra} = {x_{i,j-1}, \\hat{x}_{i,j-1}, x_{xj}|j = 1,..., m}$ denotes the training set of the i-th case with m samples, \u00e6 denotes the labeled sate, and $\\hat{x}_{i,j}$ denotes one-step prediction of state, which is approximated by Euler integration method here.\nThe learning procedure of the feedback part $h_{neural}(t)$ is summarized as Algorithm 1. After training the neural ODE $f_{neural}(t)$ on the nominal task, the parameters of simulation model are randomized"}, {"title": "5 EMPIRICAL STUDY", "content": "Precise trajectory prediction of a free-flying irregular object is a challenging task due to the complicated aerodynamic effects. Previous methods can be mainly classified into model-based scheme (Frese et al., 2001; M\u00fcller et al., 2011; Bouffard et al., 2012) and learning-based scheme (Kim et al., 2014; Yu et al., 2021). With historical data, model-based methods aim at accurately fitting the drag coefficient of an analytical drag model, while learning-based ones try to directly learn an acceleration model using specific basis functions. However, the above methods lack of online adaptive ability as employing. Benefiting from the feedback mechanism, our feedback neural network can correct the learned model in real time, leading to a more generalized performance in cases out of training datasets.\nWe test the effectiveness of the proposed method on an open-source dataset (Jia et al., 2024), in comparison with the model-based method (Frese et al., 2001; M\u00fcller et al., 2011; Bouffard et al., 2012) and the learning-based method (Chen et al., 2018). The objective of this mission is to accurately predict the object's position after 0.5 s, as it is thrown by hand. 21 trajectories are used for training, while 9 trajectories are used for testing. The prediction result is presented in Figure 7. It can be seen"}, {"title": "5.2 MODEL PREDICTIVE CONTROL OF A QUADROTOR", "content": "MPC works in the form of receding-horizon trajectory optimizations with a dynamic model, and then determines the current optimal control input. Approving optimization results highly rely on accurate dynamical models. Befitting from the powerful representation capability of neural networks for complex real-world physics, noticeable works (Torrente et al., 2021; Salzmann et al., 2023; Sukhija et al., 2023) have demonstrated that models incorporating first principles with learning-based components can enhance control performance. However, as the above models are offline-learned within fixed environments, the control performance would degrade under uncertainties in unseen environments.\nIn this part, the proposed feedback neural network is employed on the quadrotor trajectory tracking scenario concerning model uncertainties and external disturbances, to demonstrate its online adaptive capability. In offline training, a neural ODE is augmented with the nominal dynamics firstly to account for aerodynamic residuals. The augmented model is then integrated with an MPC controller. Note that parameter uncertainties of mass, inertia, and aerodynamic coefficients, and external disturbances are all applied in tests, despite the neural ODE only capture aerodynamic residuals in training. For the feedback neural network, the proposed multi-step prediction strategy is embedded into the model prediction process in MPC. Therefore, the formed feedback-enhanced hybrid model can effectively improve prediction results, further leading to a precise tracking performance. More implementation details refer to Appendix A.5.3."}, {"title": "5.2.1 LEARNING AERODYNAMIC EFFECTS", "content": "While learning the dynamics, the augmented model requires the participation of external control inputs, i.e., motor thrusts. Earning a quadrotor model augmented with a neural ODE could be tricky with end-to-end learning patterns since the open-loop model are intensively unstable, leading to the diverge of numerical integration. To address this problem, a baseline controller is applied to form a stable closed-loop system. As shown in Figure 8, the training trajectories are generated by randomly sampling positional waypoints in a limited space, followed by optimizing polynomials that connect these waypoints (Mellinger & Kumar, 2011). The adjoint sensitive method is employed in Chen et al. (2018) to train neural ODEs without considering external control inputs. We provide an alternative training strategy concerning external inputs in Appendix A.1, from the view of optimal control. 6 trials of training are carried out, each with distinct initial values for network parameters. The trajectory validations are carried out using 3 randomly generated trajectories (Figures S4-S7). More learning details refer to Appendix A.5.2."}, {"title": "5.2.2 FLIGHT TESTS", "content": "In tests, MPC is implemented with four different models: the nominal model, the neural ODE augmented model, the feedback enhanced nominal model, and the feedback neural network augmented model, abbreviated as Nomi-MPC, Neural-MPC, FB-MPC, and FNN-MPC, for the sake of simplification. Moreover, 37.6% mass uncertainty, [40%, 40%, 0] inertia uncertainties, [14.3%, 14.3%, 25.0%] drag coefficient uncertainties, and [0.3, 0.3, 0.3]N translational disturbances are applied. The flight results on a Lissajous trajectory are presented in Figure 9. It can be seen the Neural-MPC outperforms the Nomi-MPC since intricate aerodynamic effects are captured by the neural ODE. However, due to the fact that unseen parameter uncertainties and external disturbances are not involved in training set, Neural-MPC still has considerable tracking errors. In contrast, FNN-MPC achieves the best tracking performance. The reason can be attributed to the multi-step prediction of the feedback neural network improves the prediction accuracy subject to multiple uncertainties, as shown in Figure S8."}, {"title": "6 RELATED WORK", "content": "Most dynamical systems can be described by ODEs. The establishments of ODEs rely on analytical physics laws and expert experiences previously. To avoid such laborious procedures, Chen et al. (2018) propose to approximate ODEs by directly using neural networks, named neural ODEs. The prevalent residual neural networks (He et al., 2016) can be regarded as an Euler discretization of neural ODEs Marion et al. (2024). The universal approximation property of neural ODEs has been studied theoretically (Zhang et al., 2020; Teshima et al., 2020; Li et al., 2022), which show the sup-universality for C\u00b2 diffeomorphisms maps (Teshima et al., 2020) and L\u00ba-universality for general continuous maps (Li et al., 2022). Marion (2024) further provides the generalization bound (i.e., upper bound on the difference between the theoretical and empirical risks) for a wide range of parameterized ODEs.\nRecently, plenty of neural ODE variants have been developed concerning different purposes. Considering the discrete and instantaneous changes in dynamics systems (e.g., a bouncing ball), neural ODEs can be extended by employing auxiliary neural event functions to model the switching moments (Chen et al., 2020). From the point of view of conservation, Hamiltonian neural networks are designed to learn the Hamiltonian of a dynamics system in an unsupervised way (Greydanus et al., 2019). However, canonical coordinates are needed for Hamiltonian neural networks. Lagrangian neural networks are subsequently developed to circumvent the requirement of canonical coordinates (Cranmer et al., 2020). Concerning dynamical systems modeled by integro-differential equations (e.g., brain activity), the neural networks employed in Zappala et al. (2023) can capture both Markovian and non-Markovian behaviors. Other variants include neural controlled differential equations (Kidger et al., 2020), neural stochastic differential equations (Oh et al., 2024), and characteristic neural ODEs Xu et al. (2023). Different from our feedback scheme, all of the above methods belong to the forward propagation mechanism."}, {"title": "6.2 GENERALIZATION OF NEURAL NETWORKS", "content": "In classification tasks, neural network models face the generalization problem across samples, distributions, domains, tasks, modalities, and scopes (Rohlfs, 2022). Plenty of empirical strategies have been developed to improve the generalization of neural networks, such as model simplification, fit coarsening, and data augmentation for sample generalization, identification of causal relationships for distribution generalization, and transfer learning for domain generalization. More details of these approaches on classification tasks can refer to Rohlfs (2022). Here, we mainly review state-of-the-art research related to continuous-time prediction missions.\nDomain randomization (Tobin et al., 2017; Peng et al., 2018) has shown promising effects to improve the generalization for sim-to-real transfer applications, such as drone racing (Kaufmann et al., 2023), quadrupedal locomotion (Choi et al., 2023), and humanoid locomotion (Radosavovic et al., 2024). The key idea is to randomize the system parameters, noises, and perturbations in simulation so that the real-world case can be covered as much as possible. Although the system's robustness can be improved through domain randomization, there are two costs to pay. One is that the computation burden in the training process is dramatically increased. The other is that the training result has a certain of conservativeness since the training performance is an average of different scenarios, instead of a specific case.\nRecently, domain randomization has proven inadequate to cope with unexpected disturbances (Shi et al., 2024). An adversarial learning framework is formalized in Shi et al. (2024) to exploit sequential adversarial attacks for quadrupedal robots, and further utilize them to finetune previous reinforcement learning-based controllers. Brain-inspired neural networks have shown striking generalization performance in new environments with drastic changes (Chahine et al., 2023; Lechner et al., 2020), benefiting from its attention concentration feature. By incorporating symbolic knowledge, Wang et al. (2024) show the generalization of neural networks can be enhanced across different robot tasks.\nAll of the above strategies try to learn a powerful model for coping with diverse scenarios, which may be laborious and computationally intensive. In this work, it is shown that only a closed-loop feedback adjustment is sufficient to improve the generalization, without changing the original feedforward network structure or training algorithm. The proposed strategy is simple but efficient."}, {"title": "7 CONCLUSION", "content": "Inspired by the feedback philosophy in biological and engineering systems, we proposed to incorporate a feedback loop into the neural network structure for the first time, as far as we known. In such a way, the learned latent dynamics can be corrected flexibly according to real-time feedback, leading to better generalization performance in continuous-time missions. The convergence property under a linear feedback form was analyzed. Subsequently, domain randomization was employed to learn a nonlinear neural feedback, resulting in a two-DOF neural network. Finally, applications on trajectory prediction of irregular objects and MPC of robots were shown. Future work will pursue to test the proposed feedback neural network on more open-source datasets of continuous prediction missions.\nLimitations. As analyzed in Theorem 1, accurate prediction necessitates a convergence time. Specifically, the convergence time for multi-step prediction scales linearly with the prediction horizon. While experimental results on selected examples indicate that a satisfactory prediction performance can be attained after a short convergence period, future demonstrations across a broader range of cases, particularly those demanding transient performance, are necessary."}, {"title": "A APPENDIX", "content": "Firstly, we formulate the learning problem as an optimization problem:\nmin J = $\\sum_{i=1}^{d} l_i (x_i, x_i^*, \\xi) + l_N (x_N, x_N^*)$                                                                      (13)\n$\\theta$\ns.t. $x_{i+1} = f_{neural} (x_i, I_i, t_i, \\theta)$                                                                                              (14)\nwhere $x_i$ and $I_i$ denotes the model rollout state and the real sample at time $t_i$ respectively, $x_{i+1} = f_{neural}(x_i, I_i, t_i, \u03b8)$ refers to the discretized integration of $f_{neural} (x(t), I(t), t, \u03b8)$ with fixed discrete step since real-world state trajectories $x_i^*$ are sequentially recorded with fixed timestep based on the onboard working frequency. $l_i(\u00b7)$, $l_N(\u00b7)$ are defined to quantify the state differences between model rollout $x_i$ and real-world state $x_i^*$. In this article, we select the functions in a weighted quadratic form, i.e., $(x_i^* \u2013 x_i)^T L_i(x_i^* \u2013 x_i)$.\nBy utilizing the optimal control theory and variational method, the first-order optimality conditions of the learning problem could be derived as\nH = J+ $\\sum_{i=1}^{N-1} \u03bb_i f_{neural} (x_i, I_i, t_i, \u03b8)$                                                                                 (15)\n$X_{i+1} = \\nabla_{\u03bb}H = f_{neural}(x_i, I_i, t_i, \u03b8), x_0 = x(0)$                                                                   (16)\n$\\frac{\u2202H}{\u2202x} = \\nabla_{x}l_i + (\\frac{\u2202f_{neural}}{\u2202x})^T \u03bb_{i+1}$, $\u03bb_N = \\frac{\u2202l_N}{\u2202x_N}$                                                                                                  (17)\n$\\frac{\u2202H}{\u2202\u03b8} = \\sum_{i=1}^{N-1} \\nabla_{\u03b8}l_i + \u03bb_i^T \\nabla_{\u03b8}f_{neural} = 0$                                                                                    (18)\nwhere H stands for the Hamiltonian of this problem. Solving (18) could be done by applying gradient descent on \u03b8. The gradient is analytic and available (summarized in Algorithm 2) by sequentially doing forward rollout (16) of \u00e6 and backward rollout (17) of A, where the latter one is also known as the term adjoint solve or reverse-mode differentiation."}, {"title": "A.3 IMPLEMENTATION DETAILS OF SPIRAL CASE", "content": "The adopted spiral model is formalized as\n$\\dot{x}(t) = \\begin{bmatrix} -\u03b7 & \u03c9 \\\\ -\u03c9 & -\u03b7 \\end{bmatrix} x(t) + \\begin{bmatrix} \u03b5 \\\\ \u03b5 \\end{bmatrix}$                                                                                             (25)\nwith period w, decay rate n, and bias \u025b.\nIn tests, the initial value is set as \u00e6(0) = [9, 0]T. For the nominal task, \u03c9, \u03b7, and \u025b are set as 2, 0.1, and 0, respectively."}, {"title": "A.5 IMPLEMENTATION DETAILS OF MODEL PREDICTIVE CONTROL OF A QUADROTOR", "content": "A quadrotor dynamics can be defined as a state-space model with a 12-dimensional state vector x = [p, \u03c5, \u0398, \u03c9]T and a 4-dimensional input vector u = [T1, T2, T3, T4]\u0f0b of motor thrusts. Two coordinate systems are defined, the earth-fixed frame E = [XE, YE, ZE] and the body-fixed frame B = [XB, YB, ZB]. The position p and the velocity v are defined in & while the body rate w is defined in B. The relationship between E and B is decided by the Euler angle \u0398. The translational and rotational dynamics can be formalized as\np = v, v = a = $\\frac{1}{m} Z_B T+gZ_E$\n$\\dot{\u0398} = W(\u0398)\u03c9$, $J\\dot{\u03c9} = -\u03c9 \u00d7 (J\u03c9) + \u03c4$\n[T, T] = C[T1, T2, T3, T4]                                                                                                             (26)\nwhere g stands for the magnitude of gravitational acceleration, W(\u00b7) refers to the rotational mapping matrix of Euler angle dynamics and C is the control allocation matrix. We note the nominal dynamics of quadrotor as $\\dot{x}$ = f(x, u).\nNext, differential flatness-based controller (DFBC) (Mellinger & Kumar, 2011) for the quadrotor is introduced, which is adopted here to form a closed-loop system for end-to-end learning that remains stable and differentiable numerical integration. By receiving the flat outputs \u03a8 = [p, v, a, j], the positional signal and its higher-order derivatives, as the command signal, DFBC computes the desired motor thrusts for the actuators under the 12-dimensional state feedback. By virtue of the differential flatness property of the quadrotor, one can covert the flat outputs into nominal states \u00e6 and inputs u using related differential flatness mappings if the yaw motion remains zero. We note this controller as [\u017c, u] = \u03c0(z, x, \u5323), where z is auxiliary state of controller for the expression integrators and approximated derivatives in the rotational controller."}, {"title": "A.5.2 IMPLEMENTION OF LEARNING AERODYNAMICS EFFECTS", "content": "In training, the aerodynamic drag can be modeled as RDR\u00afv (Faessler et al., 2017), where R refers to the current rotational matrix that map the frame B to the frame E, and D = diag{[0.6, 0.6, 0.1]} is a coefficient matrix.\nA neural ODE $f_{neural}$ (with parameters (0) is augmented with the nominal dynamics to capture the aerodynamic effect, i.e., $\\dot{v}$ = a = \u2212$\\frac{1}{m}$+ZBT + gZE + $f_{neural}(v, \u0398, \u03b8)$. A MLP with 2 hidden layers with 36 hidden units is adopted.\nEnd-to-end learning of $f_{neural}$ could be done using the algorithm 3, but a stable numerical integration is necessary. A closed-loop system of the augmented dynamics using DFBC is employed, noted as [\u017c, \u017c] = P([x, z]\u0f0b, \u012a). In the proposed algorithm, [\u017c, \u017c]\u0f0b turns out to be the new state and \u012a becomes the auxiliary input instead of the input of the augmented dynamics u.\nWe generate 40 \u5323 trajectories with the discrete nodes of 200 each for learning by randomly sampling the positional waypoints in a limited space, followed by optimizing polynomials that connect these waypoints, as shown in Figure 8. For validations of the learned neural ODE, we generate another 3 random trajectories 2.5\u00d7 longer than that used in training, the result illustrated in Figures S4-S7 indicates a good prediction on all 12 states."}, {"title": "A.5.3 IMPLEMENTATION OF MPC WITH FEEDBACK NEURAL NETWORKS", "content": "MPC works in the form of trajectory optimization"}]}