{"title": "Boosting Self-Efficacy and Performance of Large Language Models via Verbal Efficacy Stimulations", "authors": ["Rui Chen", "Tailai Peng", "Xinran Xie", "Dekun Lin", "Zhe Cui", "Zheng Chen"], "abstract": "Significant improvements have been observed in the zero-shot capabilities of the Large Language Models (LLMs). Due to their high sensitivity to input, research has increasingly focused on enhancing LLMs' performance via direct and simple prompt engineering rather than intricate domain adaptation. Studies suggest that LLMs exhibit emotional intelligence, and both positive and negative emotions can potentially enhance task performances. However, prior interaction prompts have predominantly concentrated on a single stimulus type, neglecting to compare different stimulus effects, examine the influence of varying task difficulties, or explore underlying mechanisms. This paper, inspired by the positive correlation between self-efficacy and task performance within the social cognitive theory, introduces Verbal Efficacy Stimulations (VES). Our VES comprises three types of verbal prompts: encouraging, provocative, and critical, addressing six aspects such as helpfulness and competence. And we further categorize task difficulty, aiming to extensively investigate how distinct VES influence the self-efficacy and task achievements of language models at varied levels of difficulty. The experimental results show that the three types of VES improve the performance of LLMs on most tasks, and the most effective VES varies for different models. In extensive experiments, we have obtained some findings consistent with psychological theories, providing novel insights for future research.", "sections": [{"title": "Introduction", "content": "Researchers specializing in Natural Language Processing (NLP) have long focused on studying the fundamental principles of more general artificial intelligence for many years [1]. Capitalizing on the extensive scale of model sizes"}, {"title": "Background", "content": ""}, {"title": "Self-efficacy in Psychology", "content": "Self-efficacy, a concept central to Albert Bandura's social cognitive theory [18, 19], refers to an individual's belief in their capability to execute behaviors necessary to produce specific performance attainments. Research consistently shows that higher induced self-efficacy enhances performance achievements and lowers emotional arousal [22, 23]. According to Bandura, self-efficacy is shaped by four primary sources: mastery experiences (past achievements), vicarious experiences (observing others' successes), verbal persuasion (feedback or encouragement from coaches or leaders), and physiological feedback (emotional states such as anxiety and arousal). Verbal persuasion operates through social interactions, influencing individuals with others' assessments of their abilities, which may also impact their physiological or cognitive states [20]. Some researchers [24,25] have attempted to regulate the behavior and abilities of subjects through verbal persuasion. Studies indicate that positive reinforcement, such as compliments and encouragement, boosts motivation, self-esteem, and self-efficacy [26], while criticism may either diminish or bolster self-efficacy [27], depending on how it is perceived. The application of these theories is well-established in education [28], sports [29], and health care [30]."}, {"title": "LLMs and Prompt Engineering", "content": "Large language models (LLMs) have shown significant advancements in unsupervised reasoning without no task-specific training [31]. Various prompt techniques can enhance the performance of LLMs in unsupervised or weakly supervised settings, including in-context learning [32], chain-of-thought [33], and tree of thoughts [34]. Beyond prompt engineering for model inference, research has also started focusing on the interaction aspect. For instance, Salewski et al. [35] find that LLMs achieve higher accuracy in answering domain-specific questions when prompted to impersonate a domain expert. Li et al. [12] incorporate positive emotional stimuli into prompts, resulting in improved performance compared to the original prompts. Wang et al. [14] show that negative emotions similarly impact LLMs, boosting their performance. Research has established that LLMs exhibit emotional intelligence [15,36], revealing their propensity to offer human-like responses during interactions after undergoing extensive training on human datasets. However, previous interaction prompts have focused on a single type of stimulus, neglecting the performance disparities of models under different stimuli and their underlying causes. Therefore, we aim to further explore the impact of encouraging, provocative, and critical persuasion on the efficacy and performance of models and to investigate whether these effects are analogous to those observed in humans."}, {"title": "Verbal Efficacy Stimulations", "content": ""}, {"title": "Design Purpose of VES", "content": "In the context of examining the modulation of LLMs' self-efficacy and performance through verbal persuasion, we introduce the Verbal Efficacy Stimulations (VES). Our goal is to integrate prompting engineering with psychology and explore whether LLMs can exhibit human-like responses in social science inter-disciplines. To be more precisely, the primary objective of VES is to investigate how different forms of verbal persuasion encouragement, provocation, and critique-affect the self-efficacy score of LLMs and accuracy on various tasks. All three forms of VES are designed to target six unique aspects, with a one-to-one correspondence, allowing for the stimulation of students' different affective states within the same aspect. As shown in Fig. 2, this design not only aids in assessing the relative merits of the three forms, but also enables an analysis of the impacts of various aspects."}, {"title": "Theoretical Foundations for VES Design", "content": "Incorporating the insights from Bandura's Social Cognitive Theory [18], which highlights the role of verbal persuasion in shaping self-efficacy, this study extends the theoretical model to examine the physiological and affective responses elicited by different forms of verbal interactions. Research has shown that feedback from significant authorities not only alters cognitive perceptions of capability but also"}, {"title": "Detailed Contents of VES", "content": "As depicted in Fig. 2, our three verbal efficacy stimulations encouraging, provocative, and critical VES each have unique tones and psychological impacts, and we have developed a total of 18 verbal persuasion prompts from six aspects: (i) Answer Accuracy, (ii) Assistant Assessment, (iii) Helpfulness, (iv) Competence, (v) Self-belief, and (vi) Progress.\nFor encouraging VES, encapsulated in multiple stimulations such as \u201cCome on!\" and affirmations of capability and progress, these phrases are specifically designed to motivate and sustain ongoing effort and engagement. Provocative VES create a scenario where the assistant is prompted to confirm its capabilities or to validate its progress actively, rather than passively receiving affirmation. Challenging and competitive prompts such as \"Show me\u201d and \u201cProve it\" can stimulate a defensive or more aggressively competent response from LLMs, aimed at proving the assertions wrong or meeting the challenge head-on. Critical VES are distinctly negative, emphasizing failures or shortcomings and framed as warnings or expressions of disappointment. Cautious application is\""}, {"title": "Task Zoning for Discussing VES", "content": "To evaluate the effectiveness of Verbal Efficacy Stimulations on tasks of varying difficulty, we utilized insights from cognitive and social psychology to classify tasks into Comfort, Stretch, and Panic Zones, based on the initial performance of LLMs under original task descriptions. As illustrated in Fig. 3, comfort zone tasks are where large language models (LLMs) exhibit high efficacy, achieving over 85% accuracy. The stretch zone encompasses tasks that are challenging yet manageable, with LLMs maintaining 60% to 85% accuracy. Tasks in the panic zone, however, are marked by low performance (below 60% accuracy) and self-efficacy levels, reflecting significant difficulties. Since the stretch zone are most conducive to human learning and advancement, we will investigate whether similar patterns exist in LLMs."}, {"title": "Experimental Settings", "content": ""}, {"title": "Models and Baselines", "content": "In our comprehensive examining of VES, we evaluate a range of well-known LLMs, including ChatGPT [2], LLaMA2 [3], Vicuna [4], and all three models are"}, {"title": "Datasets and Tasks", "content": "We evaluate VES in zero-shot learning setting on 9 BIG-Bench Hard: Disambiguation QA (Disam), Hyperbaton (Hyper), Movie recommendation (Movie), Multistep Arithmetic-Two (Mulari), Penguins in a table (Peng), Ruin names (Ruin), Salient translation error detection (Salient), Temporal sequences (Temp), Tracking shuffled objects seven objects (Tracking7), and 14 tasks from Instruction Induction tasks: Active to Passive (AP), Cause and Effect (Cau), Diff (Diff), First word Letter (FL), Larger Animal (Lar), Letters List (LL), Second Word Letter (SL), Sentence Similarity (SS), Sentiment (SE), Singular to Plural (SI), orthography Starts With (ST), Sum (Sum), Taxonomy Animal (TA), Word in Context (WC). These tasks encompass various aspects of language comprehension, ranging from basic phrase structure analysis to the identification of similarity and causality."}, {"title": "Metrics and Answer Triggers", "content": "Metrics: We adopt Accuracy Metric for selection and classification tasks, while Exactly Match is used for other types of questions. In addition, we used t-test to verify the significant difference between prompts in pairs. Specifically, for any two prompts $p_i$ and $p_j$, their accuracy scores on all 23 tasks are denoted as {${S}_{i,1},..., {S}_{i,23}$} and {${S}_{j,1},..., {S}_{j,23}$}. Then the score differences for each task can be calculated as $D$ = {$d_1,...,d_{23}$}, where $d_k$ = ${S}_{i,k}$ - ${S}_{j,k}$. Subsequently, we performed the t-test on $D$ to obtain the corresponding t-value and p-value for each prompt, which were used to generate a significance heat map.\nAnswer Triggers: To improve the formatting of the LLM's output, we designed task-specific answer triggers. For example, the trigger for math problems is \"Therefore, the answer of the equation (arabic numeral) is:\"; And for multiple-choice questions, the trigger is \"Therefore, the answer (the corresponding lettered option) is:\"."}, {"title": "Experiments", "content": "As introduced in Section 1, our experiment investigates the effects of three types of Verbal Efficacy Stimulations (VES) \u2014 encouraging, provocative, and critical on various models performing tasks of varying difficulty. The statistical significance of our results is established through t-tests. Additionally, we examined fluctuations in the self-efficacy scores of ChatGPT following the input of VES under different task complexities. Finally, we present some intriguing responses from LLMs, discussing the effects of VES on model outputs."}, {"title": "Main Comparison Results", "content": "The main comparison results on 9 tasks of BIG-Bench Hard and 14 tasks of Instruction Induction are respectively shown in Table 1 and Table 2. The term \"Original\" denotes the initial performance attained with the original task descriptions. \"+ VES-E\", \"+ VES-P\", and \"+ VES-C\" designate the performances associated with encouraging, provocative, and critical VES, respectively. The terms \"avg\" and \"max\" indicate the average and peak performances for the respective VES types within the task.\nGPT-3.5: Encouraging VES (VES-E) lead to the greatest improvements in average accuracy of GPT-3.5 on various BIG-Bench Hard and Instruction Induction tasks, with enhancements reaching up to 13% compared to original performance. The max performance results show that the improvements by VES-E on BBH-Tracking7 range from 1.6% to an impressive 30.8%. This demonstrates that VES-E can significantly enhance the model's performance without requiring complex designs or extensive prompt engineering. Interestingly, provocative VES (VES-P) improves the average accuracy of the model on about half of the tasks, with improvements ranging from 0.8% to 34%. In some cases, VES-P even outperforms VES-E, suggesting that GPT-3.5 can be motivated by challenge or competition. Results from the VES (VES-C) show a slight decrease in average performance across three tasks within both the BIG-Bench Hard and Instruction Induction, compared to the original prompts. Notwithstanding, improvements are observed in the performance on other tasks."}, {"title": "Comparison Between Task Zones", "content": "We categorized all 23 tasks into three groups based on the initial accuracy of each language model using the original prompts, corresponding to the model's Comfort Zone (accuracy greater than 85%), Stretch Zone (accuracy between 60%"}, {"title": "Statistical Significance Analysis", "content": "We performed pairwise comparisons of VES prompts and utilized t-tests to analyze the significance of the differences between them. Figure 4 shows the results of the pairwise comparisons among the 18 prompts, where a green cell xij indicates pi outperforms pj, a red cell indicates the opposite, and darker colors represent more significant differences.\nGPT-3.5: In the horizontal view, the rows corresponding to P1, C1, and C2 are predominantly red, indicating that these prompts perform worse than others. Conversely, the rows for E2, E3, and E4 are mostly green, showing that these prompts outperform the rest. Additionally, the intersection of E3 and C2 is the darkest, signifying the greatest disparity between C2 and E3. This suggests that the impact of critique on GPT-3.5 can be detrimental, leading to poor results. Therefore, encouragement might be the optimal approach for GPT-3.5.\nFurthermore, compared to Figures b and c, Figure a has an overall lighter color, suggesting that more advanced models exhibit greater stability under self-efficacy stimulation."}, {"title": "Fluctuations in Self-Efficacy Scores of LLMS", "content": "In Figure 5, all three discounts show the same trend, which means that the effect of emotional stimulation may be relatively independent of task difficulty. It can be clearly seen that P1, P5 and C2 all make LLMs enter a state of low self-efficacy under different difficulty tasks. Second, the model's self-efficacy scores were relatively stable in the face of encouragement, but showed large fluctuations in the face of provocation and critique. This suggests that encouragement can steadily improve the model's self-efficacy score, while the effect of provocation"}, {"title": "LLMs Response Variations to Different VES", "content": "In Fig. 6, we present the responses of GPT-3.5 to tasks of varying difficulties, modulated by different Verbal Efficacy Stimulations (VES). When performing simple tasks in the Comfort Zone, the model demonstrates elevated self-efficacy and responded positively. However, under challenging statements about being the best assistant, it often overlooks the input words of the questions, focusing solely on asserting its superiority. In the Stretch Zone, Encouraging VES-1 provides more detailed and accurate analyses, whereas Provocative VES-2 not only claims to be the best assistant but also accurately addresses the questions, a trend that continued into the Panic Zone. As task difficulty escalated within the Stretch and Panic Zones, the model responded to Critical VES with avoidance behaviors analogous to those observed in humans. Specifically, it either analyzed the problem without delivering a final answer due to fear of errors, or it issued an unjustified response."}, {"title": "Conclusion", "content": "In this paper, we combined the principles of engineering and psychology to investigate whether Large Language Models (LLMs) can present human-like responses in the domain of social science disciplines. We devise a total of 18 Verbal Efficacy Stimulations (VES), employing three distinctive forms: encouragement, provocation, and critique. For each strategy, we explored the impact of VES on the self-efficacy and performance of LLMs from six aspects such as answer accuracy and assistant assessment. The experimental outcomes demonstrate that the three forms of VES enhance the performance of LLMs across a majority of tasks. Our analysis further reveals that the LLMs under the stimulation of VES is consistent with established psychological theories, and may offer novel insights into the practical applications of LLMs in real-world scenarios."}]}