{"title": "CarMem: Enhancing Long-Term Memory in LLM Voice Assistants through Category-Bounding", "authors": ["Johannes Kirmayr", "Lukas Stappen", "Phillip Schneider", "Florian Matthes", "Elisabeth Andr\u00e9"], "abstract": "In today's assistant landscape, personalisation enhances interactions, fosters long-term relationships, and deepens engagement. However, many systems struggle with retaining user preferences, leading to repetitive user requests and disengagement. Furthermore, the unregulated and opaque extraction of user preferences in industry applications raises significant concerns about privacy and trust, especially in regions with stringent regulations like Europe. In response to these challenges, we propose a long-term memory system for voice assistants, structured around predefined categories. This approach leverages Large Language Models to efficiently extract, store, and retrieve preferences within these categories, ensuring both personalisation and transparency. We also introduce a synthetic multi-turn, multi-session conversation dataset (CARMEM), grounded in real industry data, tailored to an in-car voice assistant setting. Benchmarked on the dataset, our system achieves an F1-score of .78 to .95 in preference extraction, depending on category granularity. Our maintenance strategy reduces redundant preferences by 95% and contradictory ones by 92%, while the accuracy of optimal retrieval is at .87. Collectively, the results demonstrate the system's suitability for industrial applications.", "sections": [{"title": "1 Introduction", "content": "Memory retention is essential in human interaction for building long-term relationships (Alea and Bluck, 2003; Brewer et al., 2017). Similarly, virtual dialogue systems aim to leverage conversation memories for a more personalised user experience. Large Language Models (LLMs) have become a prominent technology in powering such virtual dialogue systems. Given that LLMs are inherently stateless, all relevant memories need to be presented during each interaction. Presenting all past messages to an LLM degrades performance (Liu et al., 2024) and increases costs. Therefore, an external preference memory system is needed that selectively presents a relevant subset of previously extracted memories for the current conversation turn. However, when engaging with virtual non-human assistants like an in-car personal voice assistant, limitations and concerns arise:\n(1) Privacy Concerns: End-users may have concerns about the extraction and storage of private information from their interactions. In Europe, the GDPR (Commision, 2016) enforces data minimization, requiring that data be \"adequate, relevant, and limited to what is necessary\" for the purposes it is processed under Article 5(1)(c). Additionally, the EU AI Act (Parliament and Council, 2024) mandates a high degree of transparency, reinforcing the need for clear communication about how user data is handled. (2) Technological Constraints: In-car voice assistants are limited in the information they can actually use due to the restricted action space of the vehicle's systems. For example, the preferred radio station can be set as a parameter in the entertainment system, while the favourite movie genre is not applicable. Unbounded information extraction would lead to irrelevant and resource-inefficient storage of memories.\nOur work addresses these industry-relevant challenges by proposing a category-bound preference memory system. This system restricts information extraction, with a focus on user preferences, to hierarchically predefined categories. Thereby, companies pre-define categories to prevent capturing non-actionable information, and users have the control to further refine this by opting out of specific categories. An overview of the category-bound preference memory flow is shown in Figure 1. The memory system consists of three main components. (1) Extraction, which captures in-category preferences after conversations while ignoring out-of-category ones. (2) Maintenance based on Bae"}, {"title": "2 Related Work", "content": "Cognitive neuroscience distinguishes between semantic memory (general knowledge) and episodic memory (personal events) (Tulving, 1972). While LLMs effectively cover semantic memory, episodic memory must be handled manually. Personalized dialogue systems aim to leverage episodic memory to enhance user experience by tailoring interactions based on individual preferences. Early approaches used static user profiles (Zhang et al., 2018), while more dynamic methods include memory-augmented networks (Meng and Huang, 2018), memory-augmented LLMs (Wang et al., 2023b), and external memories that continuously update user memories (Xu et al., 2022a,b, 2023). Due to scalability issues with memory-augmented LLMs, we focus on external memory systems that retrieve relevant information as needed. Several works have explored external memories. Park et al. (2023) used an event-based memory in LLM-powered characters for personalized interaction with other characters. MemGPT (Packer et al., 2023) introduces an operating-system-inspired dual-memory structure. Meanwhile, Zhong et al. (2024) enhances its memory mechanism by introducing a human-like forgetting curve.\nThese advancements, however, have brought new challenges: they deploy unstructured extraction methods, which result in unordered memory pieces in text format, making structured and transparent information an underexplored area. Additionally, with the growing focus on transparency in AI (Adadi and Berrada, 2018), regulations like GDPR (Commision, 2016), and the EU AI Act (Parliament and Council, 2024), there is increasing demand for systems that offer users more control. OpenAI introduced a memory feature in their ChatGPT interface (OpenAI, 2024c), where user control is limited to deleting memories after extraction. Our approach differs by allowing users to control what gets extracted initially through the ability to opt-out from specific category topics.\nFor maintaining relevant memory, Xu et al."}, {"title": "3 Structured and Category-Bound User-Preference-Memory", "content": "Our system manages user preferences through three stages: hierarchical preference extraction, ongoing maintenance, and retrieval for future interactions."}, {"title": "3.1 Preference Extraction", "content": "Preferences are extracted from conversations and constrained to predefined hierarchical categories. Relevant categories aligned to the in-car assistant are shown in Figure 2. With this, a user could have a preference for Italian food within the category Points of Interest (Main), Restaurant (Sub), Favourite Cuisine (Detail). Category-bound extraction (1) increases the transparency by showing which preferences are stored and where; (2) allows users to opt out of categories, for instance, due to privacy concerns; and (3) aligns with the limited action space of downstream car functions, avoiding irrelevant preferences. Hierarchical, category-based extraction is achieved via LLM function calling.\nLLM Function Calling: Function calling enhances control and reliability in extracting structured information compared to simple prompt-based methods. The LLM is trained to match a predefined parameter schema, ensuring a specific output format (JSON) and extracting only relevant information from the input text for the designated function parameters.\nA function definition consists of the name of the function, a description of the purpose, and a parameter schema. We define a function to extract preferences and use the function parameter schema to represent our categories and their hierarchy as parameters. The parameter schema is defined with pydantic (Colvin et al., 2024) and presented in Appendix E. In the schema, we define every parameter, representing one category (favourite cuisine, preferred radio station, etc.), as Optional so that the LLM is not forced to extract a preference within that category. By using the extraction function on a conversation, the LLM fills in the values of the nested schema, effectively extracting preferences according to the predefined categories and their hierarchy. Out-of-category preferences are either ignored by the LLM as there is no fitting function parameter or extracted in our designated no_or_other_preference parameter within the sub- and detail categories which are later discarded."}, {"title": "3.2 Preference Maintenance", "content": "Once extracted, it is essential to maintain the preferences by checking for redundancy or contradictions before storage. Following Bae et al. (2022), we have implemented three maintenance functions to account for this: Pass: The incoming preference already exists in the storage and is not inserted"}, {"title": "3.3 Preference Retrieval", "content": "After maintaining an up-to-date database, the next step is to ensure that relevant preferences are retrieved during future interactions. To achieve this, we generate an embedding representation from a concatenated string of the detail category, preference attribute, and the sentence revealing the preference. Embeddings capture semantic relationships between preferences and context, enabling robust, low-latency retrieval, even with varied user phrasing. We retrieve the most relevant preferences by embedding similarity with the user utterance."}, {"title": "4 Data", "content": "This section outlines the construction of our synthetically generated dataset CARMEM. To evaluate the reliability of the category-bound extraction, the dataset features realistic multi-turn in-car Extraction Conversations where the user reveals exactly one given preference. Additionally, the dataset includes, in a second session, Retrieval Utterances for recalling preferences, and Maintenance Utterances for benchmarking maintenance scenarios.\nTo generate the dataset, we use the LLM GPT-4-1106-preview (OpenAI, 2024d) with temperature 0.7, balancing creativity and coherence (cf. Appendix B). To ensure realistic conversations, we prompt the LLM with an elaborate input framework. For this, we have created 100 user profiles with varying characteristics in age, technological proficiency, user location, and conversation style. The latter, derived from real-world in-car conversations, ranges from commanding, keyword-only,"}, {"title": "5 Experiments", "content": "The results are benchmarked on our dataset CARMEM. We applied a 50-50 split on validation and testing, resulting in 500 test entries. The experiments including an LLM, i.e. extraction and maintenance, were performed using function-calling with the LLM GPT-4o (2024-08-06) (OpenAI, 2024d) at a temperature of 0 to maximize deterministic output (cf. Appendix B)."}, {"title": "5.1 Preference Extraction", "content": "We conducted two experiments to evaluate preference extraction from the Extraction Conversations:\n1. In-Schema: Evaluates if the ground-truth preference can be extracted within the correct categories in the schema. An extraction is considered correct if the main-, sub-, and detail categories match those of the ground-truth preference.\n2. Out-of-Schema: Evaluates if the ground-truth preference is not extracted when the corresponding subcategory is excluded from the schema, simulating a user opt-out. For the example \"I want kosher food\" the sub-category 'Restaurant' and corresponding detail categories would be excluded from the schema. A data point is considered correct if the ground-truth preference is not extracted.\nExperiment Setting Both experiments were conducted on 500 Extraction Conversations, each containing exactly one ground-truth user preference.\nThe general extraction statistics in Table 2 show a low risk (6%) of non-extraction when a preference is present and represented in the schema. However, when excluding the subcategory from the schema, the non-extraction is desired and achieved 75% of the time, demonstrating strong boundness to the predefined categories. In general, we see an incorrect over-extraction with rates of 12% and 25%. The high number of valid structured outputs indicates the reliable adherence to the complex extraction schema, as misformatted JSON outputs and incorrect parameter (=category) names and hierarchies are labelled as invalid."}, {"title": "5.2 Preference Maintenance", "content": "Table 4 shows that each of the three Maintenance Utterance types is assigned a specific function call as its ground truth label. This mapping is based on the incoming preference from the Maintenance Utterance, the existing preference from the Extraction Conversation, and the detail category type. A data point is considered correct if the ground truth maintenance function is called.\nExperiment Setting To ensure an independent evaluation, we perform the maintenance evaluation only on the dataset entries with perfect extraction"}, {"title": "5.3 Preference Retrieval", "content": "In the CARMEM dataset, each Retrieval Utterance is designed to focus on the topic of the ground-truth subcategory, targeting the retrieval of the corresponding ground-truth preference. While the k for semantic retrieval is fixed in practice, we adapt it dynamically to provide more insightful results. Consequently, retrieval is considered optimal if the ground-truth preference is among the top-ni,j retrieved preferences, where ni,j represents the number of preferences stored for user i within subcategory j. On average, the parameter n is 1.57 and"}, {"title": "6 Conclusion", "content": "We presented a structured, category-bound preference memory system capable of extracting, maintaining, and retrieving user preferences, while enhancing transparency and user control in privacy-critical contexts. Our approach utilizes a synthetic dataset grounded in real in-car conversations to ensure realism. Benchmarking the core components of the preference memory on this dataset demonstrated both the system's utility and strong performance. Future work could build upon the dataset, refine our baseline methods, and explore generalizing to other industry domains such as smart homes, further validating the approach's adaptability."}, {"title": "7 Limitations", "content": "The dataset contains exactly one preference per conversation, which is beneficial for evaluation but does not account for conversations containing no or multiple preferences. While we carefully simulated realistic in-car user-assistant interactions, we did not incorporate additional speech recognition errors or repeated user requests, both of which are common in real-world scenarios. Although LLMs often provide automatic corrections for such issues in practice, structural testing could yield further insights into robustness.\nMoreover, the dataset represents interactions across only two timeframes, limiting our evaluation to the basic functionalities without testing the long-term ability to adapt to changing user preferences. Incorporating techniques such as temporal decay of memorized preferences (Zhong et al., 2024) or assigning importance ratings(Park et al., 2023) could improve our maintenance methods.\nAlthough the preference extraction experiment adhered well to the category schema, incorrect over-extraction occurred at rates of 12% to 15%. To mitigate this, we propose to leverage in-context learning capabilities of the LLM and provide explicit few-shot examples where no preference should be extracted. Furthermore, we used OpenAI's JSON mode for data extraction. However, the just-released structured output mode by OpenAI (2024b) reportedly adheres 100% to the provided schema, which could further improve our preference extraction results."}, {"title": "8 Ethical considerations", "content": "Our dataset was synthetically generated and does not contain any personally identifiable information. The attributes for the categories such as 'favourite artist' or 'preferred radio station' were also generated, ensuring no real persons or brand names were included. For the user profiles used in dataset generation, we only incorporated neutral information such as age or conversation style, avoiding sensitive attributes like gender or ethnic background. However, since LLMs are trained on vast amounts of mostly online data, they inherit harmful social biases (Gallegos et al., 2024), which could be reflected in our dataset. By prompting the LLM with bias-neutral few-shot examples, we aimed to guide the model toward fairer extractions.\nOur proposed preference memory system is designed to be transparent and explainable in its approach for extracting and managing user preferences. This aligns with emerging AI regulations such as the EU AI Act (Parliament and Council, 2024) which mandates transparency, and the General Data Protection Regulation (GDPR) (Commision, 2016), which emphasizes data protection and user consent. A key aspect of our system is category-bound extraction, which follows the principles of data minimization and user control. By aiming to extract and store only actionable information and allowing users to opt out of specific categories, we preserve user privacy while maintaining system intelligence.\nHowever, despite our system's safeguards, it does not achieve perfect accuracy, and LLMs may hallucinate. This introduces potential risks, such as the extraction of false or irrelevant preferences. To mitigate this, integrating extracted data in the UX flow and transparently displaying them on the user interface, provides users with the ability to manually delete memories. Additionally, offering an interaction tool via voice allows users to review, edit, or delete preferences, maintaining system accuracy and trust. Future work may explore confidence thresholds that trigger user confirmation for uncertain extractions."}, {"title": "A Prompts", "content": "The prompts for dataset generation, preference extraction, and maintenance function calling are available in our released code on https://github.com/johanneskirmayr/CarMem."}, {"title": "B LLM Temperature Settings", "content": "The temperature parameter controls the randomness and creativity of the generated text. We used different settings of temperature depending on the task:\n\u2022 Dataset Generation: According to GPT-4 technical report, a temperature of 0.6 is recommended for free-text generation (OpenAI, 2024a). Considering the need for creativity and diversity in dataset generation task, and referencing related work by Wang et al. (2023a), which employs a temperature of 0.75, we decided on a temperature setting of 0.7.\n\u2022 Extraction and Maintenance Function Calling: For the tasks of extraction and maintenance function calling, we set the temperature to 0. These tasks require precise and consistent outputs without creativity, maximizing deterministic and reproducible results."}, {"title": "C CARMEM Dataset", "content": ""}, {"title": "C.1 Human Evaluation", "content": "In this section, we present the results of the human evaluation conducted to assess the quality and relevance of the dataset. A subset of 40 data points, systematically selected from 40 users in the CARMEM dataset, was evaluated by three human judges. The preferences, which are ordered correspondent to the category list, were chosen in a repeating pattern from the first to the tenth preference. This approach ensured a representative coverage of all preference categories and user profiles. To ensure high intercoder reliability, the judges were provided with detailed instructions. The instructions included the goals for each dataset component, an explanation of the dynamic inputs (user profile, conversation criteria), the evaluation criteria, and guidelines for the different evaluation values. Furthermore, one independent data point was evaluated collaboratively to establish a consistent evaluation standard.\nThe evaluation criteria for the Extraction Conversation part of the CARMEM dataset are as follows:\n1. Realism of User Behavior: Does the simulated user behave and communicate in a manner that reflects how real users would act in a similar in-car situation?\n2. Realism of Assistant Responses: Are the assistant's responses contextually appropriate, relevant, and reflective of a natural understanding of human speech patterns?\n3. Organicness of User Preference Revelation: Is the user preference revealed naturally within the flow of the conversation without being forced or out of place?\n4. Clarity of User Preference: Is the user preference communicated clearly, making it distinct from a temporary wish or a one-off statement?\n5. Environment Understanding: Does the model demonstrate an understanding of the context in which the conversation is taking place?\nEach criterion was assessed on a Likert scale from 1 (worst) to 3 (best). Additionally, each Extraction Conversation, Retrieval Utterance, and Maintenance Utterance is assessed for appropriateness within the dataset and scored for subjective quality on an overall Likert scale rating (1-3). A data point should be scored inappropriate if, for example, the user preference is unclear, the conversation contains multiple preferences, the retrieval utterance already included the ground-truth preference or the maintenance utterances do not fulfil the intended purpose. The majority vote was taken in discordant situations."}, {"title": "C.2 Increased Diversity through User Profiles and Conversation Criteria", "content": "As detailed in Section 4, dynamic prompt inputs are sampled for generating each conversation. We hypothesize that this variation in user profiles and conversation criteria will result in increased diversity in the generated text.\nExperiment Setting To test our hypothesis, we randomly sampled four different user preferences. For each preference, we \"regenerate\" the conversations 10 times with 2 methods: (1) regenerate with varying dynamic inputs, and (2) regenerate with non-varying fixed inputs. To compare the diversity of the generated conversations, we increasingly concatenate (from 1-10) the regenerated conversations for both methods and calculate the Distinct-1, Distinct-2, and Distinct-3 scores. Calculating the three Distinct-N scores allows for a comprehensive assessment of text diversity across varying levels of lexical and syntactic granularity. The same prompt was used for both methods. The dynamic inputs to the prompt are: User Profile Data (Age, Technological Proficiency, Conversation Style, Location), Conversation Criteria (Position User Preference, Preference Strength Modulation, Level of Proactivity Assistant), and the Few Shot Example. Note: To mitigate the issue of unequal evaluation due to varying text lengths, the conversation length was fixed to six messages for both methods. For the fixed input method, the dynamic inputs were sampled once at the beginning and kept constant across the 10 conversations. For the dynamic input method, inputs were resampled for each conversation. Since the conversation style was found to have a significant influence on the generated text, we exceptionally manually set the conversation style to the four possible values for the four different user preferences in the fixed input method to ensure more representative results. The temperature of each LLM is set to 0.7. The averaged Distinct-N scores across the four preference generations can be seen in Figure 4."}, {"title": "D Predefined Categories", "content": ""}, {"title": "D.1 Full List of Preference Categories with Attributes", "content": "In the following, the full list of preference categories with attributes is shown. From this list, every user profile gets sampled 10 preferences.\n1. Points of Interest\n(a) Restaurant\ni. MP: Favorite Cuisine\n\u2022 Attributes: Italian, Chinese, Mexican, Indian, American\nii. MP: Preferred Restaurant Type\n\u2022 Attributes: Fast food, Casual dining, Fine dining, Buffet\niii. MP: Fast Food Preference\n\u2022 Attributes: BiteBox Burgers, GrillGusto, SnackSprint, ZippyZest, WrapRapid\niv. SP: Desired Price Range\n\u2022 Attributes: cheap, normal, expensive\nv. MP: Dietary Preferences\n\u2022 Attributes: Vegetarian, Vegan, Gluten-Free, Dairy-Free, Halal, Kosher, Nut Allergies, Seafood Allergies\nvi. SP: Preferred Payment Method\n\u2022 Attributes: Cash, Card\n(b) Gas Station\ni. MP: Preferred Gas Station\n\u2022 Attributes: PetroLux, FuelNexa, GasGlo, ZephyrFuel, AeroPump"}, {"title": "E Methodology: Preference Extraction", "content": "We define the LLM function for extracting user preferences as follows:\n\"type\": \"function\",\n\"function\": {\n\"name\": \"extract_user_preference\",\n\"description\": \"A function that extracts personal preferences of the user .\",\n\"parameters\": \"\", \"}\n\u2190\nThe parameter schema, defined using Pydantic, includes categories and their hierarchy. Below is a representative subset for the main category Points of Interest, sub-category Restaurant, and detail-category Favourite Cuisine:\nclass PreferencesFunctionOutput (BaseModel):\n points_of_interest:\n Optional [PointsOfInterest] =\n Field(default=None,\n description=\"The user's preferences in\n the category 'Points of\n Interest'.\",)\n navigation_and_routing:\n Optional [NavAndRouting] = Field(...)\nclass PointsOfInterest (BaseModel):\n no_or_other_preference:\n restaurant: Optional [Restaurant] =\n Field(defualt=None, description=\"...\")\nclass Restaurant (BaseModel):\n no_or_other_preference:\n favourite_cuisine:\n Optional [List [OutputFormat]] =\n Field(default=[], description=\"...\",\n examples=[\"Italian\", \"Chinese\", ...])\nclass OutputFormat (BaseModel):\n user_sentence_preference_revealed:\n Optional [str] = Field(default=None,\n description=\"user sentence where the\n user revealed the preference.\")\n user_preference: Optional [str] =\n Field(default=None, description=\"The\n preference of the user.\")\nEach category is represented as a parameter with a type, default value, description, and optional examples. The nested schema represents the relationship of the categories. As every parameter is Optional, the LLM is not forced to extract a preference for every parameter within that category. We found that including the parameter no_or_other_preference within the sub- and detail categories reduces over-extraction, as the LLM must actively decide not to place a preference there if it intends to extract one. Through the Output"}, {"title": "F Additional Experiment Results", "content": ""}, {"title": "F.1 Confusion Matrices of Preference Extraction", "content": "Figure 5 shows the multi-label confusion matrix on the detail category level for the In-Schema experiment (refer to Section 5.1).\nThe strong diagonal in the confusion matrix indicates that the extraction process reliably adheres to the category schema. Most incorrect extractions occur in semantically related categories. After manual analysis, we found that the increased misclassifications in the detail category 'avoidance of specific roadtypes', 'shortest time or distance', and 'tolerance for traffic' are mostly due to the dataset. During dataset generation, an extra preference is occasionally included in the user utterances within these categories, as in-car conversations often evolve toward these topics naturally.\nFigure 6 shows the multi-label confusion matrix on the subcategory level for the Out-of-Schema experiment (refer to Section 5.1). As the category of the ground-truth preference is excluded in the schema for this experiment, we expect the system to perform no extraction. We see that we have few incorrect extractions when excluding semantically distinct categories such as 'Climate Control' (0 incorrect extraction), but significantly more if there is still a closely related category like in 'Music' and 'Radio and Podcast'. This indicates that the definition of clear and semantically distinct categories is key to a reliable category-bound extraction."}]}