{"title": "Learning-Based TSP-Solvers Tend to Be Overly Greedy", "authors": ["Xiayang Li", "Shihua Zhang"], "abstract": "Deep learning has shown significant potential in solving combinatorial optimization problems such as the Euclidean traveling salesman problem (TSP). However, most training and test instances for existing TSP algorithms are generated randomly from specific distributions like uniform distribution. This has led to a lack of analysis and understanding of the performance of deep learning algorithms in out-of-distribution (OOD) generalization scenarios, which has a close relationship with the worst-case performance in the combinatorial optimization field. For data-driven algorithms, the statistical properties of randomly generated datasets are critical. This study constructs a statistical measure called nearest-neighbor density to verify the asymptotic properties of randomly generated datasets and reveal the greedy behavior of learning-based solvers, i.e., always choosing the nearest neighbor nodes to construct the solution path. Based on this statistical measure, we develop interpretable data augmentation methods that rely on distribution shifts or instance perturbations and validate that the performance of the learning-based solvers degenerates much on such augmented data. Moreover, fine-tuning learning-based solvers with augmented data further enhances their generalization abilities. In short, we decipher the limitations of learning-based TSP solvers tending to be overly greedy, which may have profound implications for AI-empowered combinatorial optimization solvers.", "sections": [{"title": "Introduction", "content": "Traveling Salesman Problem (TSP) is a critical and widely applied combinatorial optimization problem. It involves finding the shortest possible route that visits a given set of cities exactly once and returns to the origin city. Due to its NP-hard nature, TSP is a central problem in optimization theory with numerous practical applications in logistics [5], routing[24], manufacturing[4], and even circuit design [23].\nPrevious studies have developed various exact and heuristic methods to tackle TSP, including dynamic programming [13], branch-and-bound[30], and genetic algorithms [20]. However, due to the combinatorial explosion of possible solutions as the number of cities increases, finding the optimal solution becomes computationally infeasible for large instances. This has led to significant interest in approximation algorithms and learning-based approaches that can offer near-optimal solutions in a reasonable amount of time. Deep learning techniques [21] provide a novel 'data-driven' approach to problem-solving, enabling researchers to address challenges from computational complexity more effectively. The learning-based algorithms can be divided into two main categories [28]: i) Learning-based end-to-end algorithms, especially the end-to-end construction algorithm; ii) Hybrid algorithms combining deep learning with heuristic algorithms. This study investigates the potential biases in"}, {"title": "Overview", "content": "In this study, we construct a statistical measure called the nearest-neighbor density and validate the greedy behavior of the learning-based solvers introduced by data sampling from uniform or normal distributions and real-world city instances (Fig. 1a,b). We provide the asymptotic lower bound of the nearest-neighbor density under uniform distribution and formulate a mathematical conjecture. This conjecture could be a significant open problem in probability theory, graph theory, and combinatorial optimization. We leverage the nearest-neighbor density to develop augmented instances for TSP deep-learning solvers. These instances are constructed based on distributional shifts and node perturbations. We verify that these more diverse instances effectively interfere with learning-based solvers. By using these instances as augmented TSP instances, we fine-tune some base solvers [14,16,19,34,36] and confirm a substantial improvement in the generalization performance. Although our data augmentation method based on nearest-neighbor density can improve the learning-based solvers' performance, it is not complete. Unless NP = co-NP, there does not exist an effective and complete data augmentation method. Also, we introduce the concept of efficient algorithm covering and demonstrate that unless NP = P, there is no solution to obtain a general-purpose solver by integrating polynomially many (biased) algorithms."}, {"title": "The nearest-neighbor density", "content": ""}, {"title": "From the nearest-neighbor algorithm to the nearest-neighbor density", "content": "Given a set of cities {C1, C2,\u2026\u2026, CN } and a distances d(ci, cj) for each pair of distinct cities {Ci, Cj },\nthe goal of TSP is to find an ordering \u3160 of the cities that minimizes the quantity:\n$\\sum_{i=1}^{N-1}d(C_{\\pi(i)}, C_{\\pi(i+1)}) + d(C_{\\pi(N)}, C_{\\pi(1)})$                                                                            (1)\nThis quantity is referred to as the tour length that a salesman would make when visiting the cities in the order specified by the permutation, returning to the initial one at the end. We concentrate on the symmetric TSP on the two-dimensional Euclidean plane, satisfying the triangle inequality.\nThe most natural heuristic or greedy strategy for TSP is the famous nearest-neighbor algorithm (NN). It always selects the nearest as-yet-unvisited location as the next node. Due to its poor performance, it is generally not used for TSP. There is only a theoretical guarantee that $OPT(I) \\leq NN(I) \\leq 0.5([log_2 N] + 1)$, and no substantially better guarantee is possible as there are instances for which ratio grows as $\\Theta(log N)$ [26]. In this paper, we consider the question from another view: For general instances, what proportion do the nearest-neighbor nodes or edges occupy in the optimal solution? We generate instances uniformly distributed in a two-dimensional Euclidean plane with node sizes of 50, 100, and 200 and plot the images of the nearest-neighbor edges and the optimal solution (Fig. 2). We found that the proportion of nearest-neighbor edges in the optimal solution is very high. For example, in the instance with 50 nodes, only five nearest-neighbor edges do not appear in the optimal solution.\nHere, we propose a statistical measure that characterizes the proportion of nearest-neighbor edges in the optimal solution. We introduce a node-based nearest-neighbor density considering that the algorithm progressively selects the next node to visit during its execution.\nDefinition 1 (The Nearest-Neighbor Density). For a TSP instance G = (V, E, d), where V is the set of vertices with the size of n, E is the set of edges with the size of m and d is the weight of the edges, i.e., the Euclidean distance between two points in the two-dimensional Euclidean plane. Given the optimal tour \u0442* found, we denote N(ci) and N'(ci) as the set of the nearest-neighbors of ci on G"}, {"title": "The Nearest-Neighbor Density", "content": "and on T*, respectively. The nearest-neighbor density $\\rho_n$ is calculated as follows:\n$\\rho_n = \\frac{\\sum_{C_i \\in V}{|N(c_i) \\cap N'(c_i)|}}{|N(c_i)|}$          (2)\nRemark 1. For a continuous probability distribution sampling dataset, the set of points where a given node c has more than one nearest-neighbor, i.e., two or more nodes are equidistant from c constitute a null set. In this case, the calculation of pn simplifies to a specific form:\n$\\rho_n^* = \\frac{1}{n}\\sum_{C_i \\in V}{I(c_i)},$\nwhere $I (c_i) = \\begin{cases}1, c_i \\text{ is adjacent to } c \\in N(c_i) \\text{ on } \\tau^*,\\\\0, \\text{others}.\\end{cases}$                                                                                   However, in some real-world or synthetic datasets, a node may have multiple nearest neighbors. Therefore, it is necessary to use Equation 2 for calculation. For specific examples, refer to those provided in TSPLIB."}, {"title": "Calculation of nearest-neighbor density under different distributions", "content": ""}, {"title": "Random uniform Euclidean (RUE) dataset", "content": "First, we evaluate the statistical results of the nearest-neighbor density on instances generated from the uniform distribution with the number of nodes varying from 20 to 50 in step 5. The nearest-neighbor density maintains a high level regardless of the node scale, indicating that each node is adjacent to its nearest neighbor on the optimal Hamiltonian circuit with a high probability. We observe an upward trend as the number of nodes increases on the RUE data (Table 1).\nIntuitively, a larger pn of an instance indicates a lower difficulty. We test the performance of Transformer-TSP solver[34] on the RUE test set with 10,000 samples of 50 nodes. We classify some samples as 'defective' based on the optimal gap and then calculate the defect rate for different instances (Fig. 2c). Specifically, we consider the optimal gap (defined as $\\frac{NN(G)}{OPT(G)} - 1$) of the solution output NN(G) by the Transformer-TSP solver on a specific instance G under the greedy decoding type. When the gap exceeds a threshold (i.e., 0.1% in this study), the solution is classified as 'defective'. The defect rate refers to the proportion of defective solutions for the test instances. Generally, The model performs better (worse) on instances with high (low) nearest-neighbor density.\nBased on the above experiments, we analyze the asymptotic bounds of the nearest-neighbor density Pn when n\u2192 8.\nTheorem 1. Given the TSP instance G = (V, E,b) on the two-dimensional Euclidean plane and Ci = (xi, Yi) \u2208 V(G), where (ci) ~ Uniform([0, 1] \u00d7 [0, 1]), the nearest-neighbor density pn has an asymptotic lower bound $p_n \\geq \\frac{27-32 \\beta}{8}$, as n \u2192 \u221e, a.s..\nRemark 2. B is a known constant called the Euclidean TSP Constant. When placing n cities on a square of area [0,1] \u00d7 [0,1]. uniformly at random, the optimal tour length lopt approaches the limit[8]:\n$\\lim_{n\\to\\infty} \\frac{l_{opt}}{\\sqrt{n}} = \\beta . \\alpha .\\varsigma$.                                                (3)\nThe best estimate for \u1e9e is 0.7124[15]. Thus our lower bound for pn takes 0.6005 approximately. However, the theoretical upper bound estimation of the TSP constant for TSP remains an open problem. Currently, the best theoretical upper bound obtained is \u1e9e < 0.90304.\nAccording to our observation in Table 1, we propose the following conjecture.\nConjecture 2. Given the TSP instance G = (V, E, b) on the two-dimensional Euclidean plane and Ci = (xi, Yi) \u2208 V(G), where (ci) ~ Uniform([0, 1] \u00d7 [0, 1]), the following two statements hold\n1. E(pn) increases with n.\n2. \u03c1\u03b7 \u2192p \u0395(\u03c1\u03b7), as n \u2192 \u221e, where E(pn) > 0.87, n \u2192 \u221e."}, {"title": "Random normal Euclidean (RNE) dataset", "content": "We also estimate pn of normal Euclidean instances with the number of nodes varying between 20 and 50. According to Table 1, the nearest-neighbor density remains in a relatively high level (\u2265 0.85)."}, {"title": "TSPLib95 dataset", "content": "TSPLib95 is a well-known benchmark library[25] of sample instances for TSP and related combi- natorial optimization problems. In this part, we selected 38 instances from TSPLib95 (with node sizes less than 400 for computational convenience and the problem type is EUC_2D) to test their nearest-neighbor density (Table 2). The results show that some datasets have a nearest-neighbor density smaller than 0.85, indicating that the topological characteristics of the city datasets may not be approximated well by simple distributions. Notably, the pn values for the two instances, a280 and pr107, are less than 0.8. We will discuss the potential patterns in such instances later."}, {"title": "Data augmentation", "content": ""}, {"title": "TSP instances induced by scale-free network", "content": "Scale-free networks [7] are a type of complex network characterized by a small number of highly connected nodes and many nodes with fewer connections. This structure follows a power-law degree distribution, i.e., P(d) x d\u00af\u00ba. Scale-free networks are commonly found in various real- world systems, such as social networks, the internet, and biological networks [6]. For the TSP, the topological structure of the nodes is trivial, and the metric information between nodes is critical. The nearest-neighbor distance is closely related to the nearest-neighbor density we proposed, and the nearest-neighbor distance reflects the local clustering feature of nodes. For example, in reality, the clustering feature in developed and underdeveloped areas is different, such as in the distribution map of town nodes in the United States (Fig. 3a). However, for the uniform euclidean distribution with node size n, the probability density function of the nearest-neighbor distance $P(r_1) \\propto r(1-\\pi r^2)^{n-2}$,"}, {"title": "Drilling problem instances", "content": "The drilling problem [23] is an industrial optimization task to model the optimal path for drilling predetermined locations in mechanical manufacturing or electronic device production (e.g., circuit boards or mechanical parts). The objective is to minimize the distance or time for the drill head to move between drilling locations. These points often follow specific distribution patterns, reflecting practical production requirements and potentially displaying some regularity. Compared to general TSP instances sampled by the uniform distribution, where node layouts might be random or lack a specific arrangement, the points in the drilling problem are often associated with specific processes or product structures (e.g., a280. tsp in the TSPLIB dataset) (Fig. 4a,b).\nIt is easy to see that the nodes in the drilling problem instance exhibit a locally parallel grid-like layout, which often results in parallel line segments in the tour path (Fig. 4a). Similar examples are also frequently found in real-world city-based datasets. From the arrangement of shops along parallel streets to city layouts shaped by specific terrain or historical reasons, there can often be a relatively more regular grid-like structure.\nInstances with grid-like patterns may seem relatively easy to address. If the spacing between two parallel grid segments is smaller than the spacing between points within each segment, the nearest- neighbor density will significantly decrease. We construct synthetic instances to demonstrate it. Specifically, we sample equally spaced points on each of the parallel lines y = x and y = x+0.05, where n is the size of the TSP instance. It is straightforward to calculate that the nearest-neighbor density on this constructed counterexample is $p_n = \\frac{2}{n}$ when n is sufficiently large. We used several deep learning-based solvers on this type of data and found that the results were unsatisfactory (Fig. 4c)."}, {"title": "Fine-tune the base solver with augmented instances", "content": "Utilizing the examples from the previous subsection as the basic construction instances, we create a perturbation-based instance augmentation algorithm and generate an instance set of 10000 sample instances with a node size of 50, where p50 spans values between 0.06 and 0.90 (Fig. 5b). The core idea is similar to stochastic offset data generation, where Gaussian perturbations of different scales are applied to the basic construction instances. The process generates instances along the transformation path from basic construction instances to RNE instances, which in turn results in samples with more diverse nearest neighbor density distributions (Algorithm 2).\nWe performed full fine-tuning on several classical neural solvers using augmented data (Table 3). It is worth noting that, without fine-tuning, these neural solvers showed varying degrees of performance decline on the constructed instances, with CycleFormer[36] performing the worst. Compared with the base solvers, most solvers show performance improvements on the constructed instances such as the drilling model and scale-free model instances. Additionally, we observed that except for POMO, other neural solvers also achieved improvements on standard benchmarks (e.g., the RUE instances) after fine-tuning (The experimental details could refer to Appendix A.6).\nTo further explore the generality of our data augmentation method, we also tested the performance of solvers before and after fine-tuning on the synthetic instances presented in the previous work [32], which adopt the convolution distribution of both uniform and normal distributions, resulting in"}, {"title": "Limitations of data-driven methods: Is augmentation all you need?", "content": "We have two problems to solve in this section.\n1.  (About Data) As mentioned earlier, we constructed two types of instances with low nearest- neighbor density, enhancing the robustness of the network as training data. However, the examples we constructed exhibit distinct characteristics. Are there other instances that are not covered by our construction?\n2.  (About Model) On uniform distribution, we seem to be able to obtain effective algorithms using neural network methods. However, this efficient algorithm for uniform distribution cannot cover all instances. Can we utilize different algorithms, i.e., ensemble methods, to cover all instances?\nIn the following, we present some (negative) results regarding the following two questions separately."}, {"title": "No efficient complete generator based on p", "content": "First, we need to clarify whether there are more general augmentation methods based on the nearest- neighbor density pn, such as those using generative deep learning. Unfortunately, we will prove that any polynomial-time generative method is not complete. As a preparation, we first introduce several concepts of computational complexity [27,35].\nDefinition 2 (Language). A language L is a set of strings. Every language L induces a binary classification task: the positive class contains all the strings in L, and the negative class contains all the strings in L's complement LC.\nDefinition 3 (Generator). A generator for a language L is a nondeterministic Turing machine. A generator is complete if it can generate every example: for every sufficiently large n, for every w \u2208 L of length n it holds that w \u2208 SL(n). A generator S\u2081 is called efficient if it runs in polynomial time.\nRemark 3. The definition of generator implies generating both a sample w and its correct label (w \u2208 L). One widely used efficient generator satisfies this definition which starts with a seed set of deterministically-labeled samples and applies class-preserving rewrites.\nThe following theorem indicates that for a generator for TSP instances based on pn, only one of efficiency and completeness can be achieved.\nTheorem 3. There is no efficient complete generator to generate the instances with the nearest- neighbor density pn \u2264 \u03b4, where \u03b4 \u2208 (0, 1), unless NP = CoNP.\nAs mentioned before, we can cover the range of nearest-neighbor density values by using Algorithm 2. However, according to Theorem 3, since Algorithm 2 operates in polynomial time, it is still not complete. In other words, for a specific nearest-neighbor density value, there exist certain"}, {"title": "No efficient algorithmic coverage", "content": "We have found that existing neural network algorithms perform excellently on the RUE TSP instances, at least when the problem scale is not particularly large. In this paper, we discover and confirm that learning-based solvers are biased. Nevertheless, is it possible to train solvers with different preferences for different problem scenarios and use ensemble methods to obtain a universal solver? We will explain that achieving this would at least require an exponential number of solvers in the ensemble unless P = NP.\nDefinition 4 (Efficient Algorithmic Coverage). For a combinatorial optimization problem P \u2208 NPC, let n be the size of the instances, C be the set of all problem instances of size n, C1, C2,\u2026\u2026\u2026, CN are subsets of C,and N = O(poly(n)) . For every Ci, there exists a corresponding efficient algorithm Ai. If {Ci, Ai}=1 satisfies the following properties:\n\u2022 (Completeness) $\\bigcup_{i=1}^{N} C_i = C$ ensures that all instances are represented.\n\u2022 (Effectiveness) $\\forall x_i \\in C_i, \\frac{|A_i(x_i) - A^*(x_i)|}{A^*(x_i)} < \\epsilon$.\n\u2022 (Independence) For all Ci, there exists xi \u2208 Ci, such that\n$\\forall j \\neq i, \\frac{|A_j(x_i) - A^*(x_i)|}{A^*(x_i)} > \\epsilon$,\nwhere A* is the exact algorithm and \u025b is the approximation ratio.\nthen we denote {(Ci, Ai)}=1 as the efficient algorithmic coverage for P with instance size n.\nTheorem 4. There does not exist an exact efficient algorithmic coverage for P \u2208 NPC unless NP = P.\nTheorem 5. If the original problem P \u2208 NPC does not have a polynomial-time approximation algorithm with the approximation ratio of \u025b, there does not exist an efficient algorithmic coverage with the approximation ratio of \u025b for P.\nThe above results are consistent with our previous discussion. From Figure 2 and Conjecture 2, we know that the concentration of nearest-neighbor density in the uniform distribution instances increases as the number of nodes grows. This means that, from the perspective of nearest-neighbor density, the range of scenarios that the uniform distribution can cover becomes increasingly limited. Therefore, as the number of nodes increases, the number of specific solvers required for different instances will also grow. The results indicate that this growth is at least exponential."}, {"title": "Related works", "content": "The generalization ability of neural solvers. Existing learning-based solvers often struggle with generalization when faced with changes in problem distributions. Some studies have focused on creating new distributions[31,37]. Zhang et al. [37] defined the concept of hardness of TSP instance. However, the definition of hardness is solver-specific. We know this type of instance is difficult for some solvers, but do not understand why. Min et al. evaluated the solver-agnostic hardness of various distributions based on the parameter T = lopt/\u221anA defined in the reference[11], where A denotes the area covered by the TSP instance, lopt represents the length of the optimal solution and n is the number of cities. However, this complexity is only applicable to decision TSP, whose goal is to determine whether there exists a path whose total distance does not exceed a given threshold. Lischka et al. [22] used different data distributions to train and test their solvers, but their data augmentation is based on the traditional mutation operators[9]. They did not show why the learning solvers training on the uniform distribution dataset may fail. In this paper, we address this challenge by defining a statistic called the nearest-neighbor density that not only reflects the inherent learning difficulty of the samples but is also closely related to the solving process of prediction-based learning algorithms."}, {"title": "Conclusion", "content": "We introduced the nearest-neighbor density to describe the nearest-neighbor bias introduced by the training dataset (uniform distribution) in neural solvers for TSP. Extensive experiments thoroughly validated this. In particular, conjecture 2 may serve as an intriguing open problem in the intersection of combinatorial optimization and probability theory.\nWe developed a data augmentation method based on nearest-neighbor density and real-world scenarios to alleviate the nearest-neighbor preference. Fine-tuning the base solver with these augmented instances significantly improved its generalization performance. However, this does not imply that we have achieved a universal solver. Data augmentation as a solution is not fundamental to address generalization challenges in combinatorial optimization problems. We analyze the limitations of data-driven methods, highlighting that achieving a universal neural solver through data augmentation (either manual or adaptive) or ensemble methods is infeasible, even for small-scale instances.\nThe challenge faced by deep learning models lies in handling corner cases, which is precisely a characteristic of combinatorial optimization problems. Therefore, pursuing a universal neural solver seems unrealistic. Focusing on real-world optimization tasks might be more meaningful than improving performance on unrepresentative benchmarks. Developing fair and appropriate benchmarks to evaluate neural solvers is one of our future research directions.\nOn the other hand, due to their lack of interpretability, neural networks are prone to shortcut learning-solving complex problems by capturing superficial features in the data. How to better design data representations or model architectures to reduce the network's reliance on superficial features is another critical question for the fields of AI for combinatorial optimization or even AI for Science."}, {"title": "Appendix", "content": ""}, {"title": "Proof of Theorem 1", "content": "Proof. According to the Remark 1, we know for the uniform distribution, the calculation of pn simplifies to a specific form\n$\\rho_n^* = \\frac{1}{n}\\sum_{c_i \\in V}{I(c_i)}$\nLet rk denote the distance from a given reference point to its k-th nearest-neighbor. For an instance with n nodes, a proportion pn of the nodes are connected to their nearest-neighbors on the optimal hamiltonian tour. Therefore, the lower bound for the edges associated with these pnn nodes is given by:\n$\\frac{1}{n}p_n n (r_1 + r_2) + \\theta_1$,                                                                                          (4)\nwhile the lower bound for the edges associated with the other nodes is given by:\n$\\frac{1}{n}(1-p_n) n (r_2 + r_3) + \\theta_2$,                                                                            (5)\nwhere \u03b81 and \u03b82 represent the deviation.\nTherefore, we need to figure out the statistical distribution of the k-nearest-neighbor distances.\nFor a given reference point the absolute probability of finding its k -th neighbour (k < n) at a distance between rk and rk + drk from it is given by the probability that out of the n - 1 random points (other than the reference point) distributed uniformly within the hypersphere of unit volume, exactly k - 1 points lie within a concentric hypersphere of radius rk and at least one of the remaining n - k points lie within the shell of internal radius rk and thickness drk. Therefore,\n$P(r_k) dr_k = \\binom{n-1}{k-1} \\left( \\frac{V_k}{V_R} \\right)^{k-1} \\binom{n-k}{q} \\left( 1 - \\frac{V_k}{V_R} \\right)^{n-k-q} \\left( d \\frac{V_k}{V_R} \\right)^q$,                                (6)\nand E(rk) = $\\int_0^R r_k P(r_k)$, where R is the radius of the D-dimensional hypersphere of unit volum. Thus, we have\n$E(r_k) = \\frac{\\Gamma(\\frac{D}{2} + 1)}{\\sqrt{\\pi}} \\frac{\\binom{n-1}{k-1}}{\\Gamma(n)} \\int_0^1 V_k^{\\frac{D}{2}+k-1}(1 - V_k)^{n-k-1} dV_k$\n$\\frac{\\Gamma(\\frac{D}{2} + 1)}{\\sqrt{\\pi}} \\frac{\\binom{n-1}{k-1}}{\\Gamma(n)} (n-k) B(k + \\frac{D}{2}, n-k)$\n$\\frac{\\Gamma(\\frac{D}{2} + 1)}{\\sqrt{\\pi}} \\frac{\\Gamma(k + \\frac{D}{2}) \\Gamma(n)}{\\Gamma(k) \\Gamma(n + \\frac{D}{2})}$,                                                  (7)\nand\n$E(r_k^2) = \\frac{\\Gamma(\\frac{D}{2} + 1)}{\\sqrt{\\pi}} \\frac{\\binom{n-1}{k-1}}{\\Gamma(n)} (n-k) B(k + \\frac{D}{2}, n-k)$\n$\\frac{\\Gamma(\\frac{D}{2} + 1)}{\\sqrt{\\pi}} \\frac{\\Gamma(k + \\frac{D}{2}) \\Gamma(n)}{\\Gamma(k) \\Gamma(n + \\frac{D}{2})}$.                                                (8)\nWhen taking D = 2 and using Stirling's approximation, we get E(rk) = $\\frac{\\Gamma(k)}{\\sqrt{n \\pi}}$ and E(r) = $\\frac{\\Gamma(k)}{\\sqrt{n \\pi}}$. Let rk,1,..., rk,n be n identically distributed copies of the random variable rk, thus\n$E[|\\frac{1}{n} \\sum_{i=1}^{n} r_{k,i} - \\sqrt{n} E(r_k)|] = O(\\frac{1}{n})$."}, {"title": "Proof of Theorem 3", "content": "Proof. Define the language L as follows: Given an instance I of the two-dimensional Euclidean TSP and a value \u03b4 \u2208 (0, 1), if p(I) \u2264 \u03b4, then I \u2208 L. Otherwise I \u2208 LC. L is NP-hard because it depends on solving the TSP itself. And the classification task T(L) introduced by L is NP-hard, i.e., given an instance I, decide if I \u2208 L.\nAssume that there exists an efficient complete sampler SL for L. Since L is complete, the classification task T(SL) induced by SL, i.e., given an instance I generated by SL, determine if I \u2208 L, is equal to the task T(L) introduced by L. Given a I generated by SL, and I \u2208 L, let sq be the sequence of random bits used by SL to generate I. Since S\u2081 runs in polynomial time, it must use at most polynomial number of random bits. Then \u2200I \u2208 SL, there exists a string sq with length polynomial in |I|. Therefore a deterministic Turing machine that given I and sq can verify in polynomial time if I \u2208 L. So T(SL) \u2208 NP. Similarly, we can show that T(SL) \u2208 coNP. Thus T(SL) \u2208 N P\u2229 coN P. Then, we have T(L) in NP \u2229 coN P. However, T(L) \u2208 NP \u2013 hard, and if T(L) \u2208 N P \u2229 coNP, then NP = coNP. This leads to a contradiction."}, {"title": "Proof of Theorem 4", "content": "Proof. Using proof by contradiction, assume that the efficient algorithm covering exists. We can simply obtain a polynomial-time exact solving algorithm by sequentially running A\u2081. Then P\u2208 P. Since P \u2208 NPC, this would imply P = NP, which is a contradiction."}, {"title": "Proof of Theorem 5", "content": "Proof. The proof idea follows the proof of Theorem 4."}, {"title": "Introduction to Barab\u00e1si-Albert Model", "content": "The Barab\u00e1si-Albert model is a fundamental framework for generating scale-free networks charac- terized by growth and preferential attachment. This model, also referred to as the scale-free model, operates as follows:\nInitially, the network consists of mo nodes, connected arbitrarily, ensuring each node has at least one link. The network evolves through two primary mechanisms:\n\u2022 Growth. At each time step, a new node is introduced to the network with m (where m\u2264 mo) links. These links connect the new node to m existing nodes in the network.\n\u2022 Preferential Attachment. The probability P(ki) that a new node will connect to an existing node i is proportional to the degree ki of node i. This probability is given by:\n$P(k_i) = \\frac{k_i}{\\sum_j k_j}$\nPreferential attachment is a probabilistic process, allowing new nodes to connect to any existing node, regardless of whether it is a highly connected hub or a node with few links.\nThis model effectively captures the dynamics of real-world networks, where new nodes tend to connect to already well-connected nodes, leading to a scale-free topology."}, {"title": "Experimental Details", "content": "We use the model by Bresson et al. [34] as the base solver. It is a Transformer-based architecture. The transformer is learned by reinforcement learning. Hence, no TSP solutions/approximations are required. The solver has quadratic complexity O(n\u00b2L), where L is the depth of the models. Details of the network architecture and training process are provided in Table S1.\nThe base solvers involved in the comparison all have open-source model parameters. The decoding type of beam search is with the beam width uniformly set to 1000. During fine-tuning, we replaced one-fourth of the original training set (uniformly distributed) with sample instances constructed using the drilling model examples. The number of fine-tuning steps for different solvers is provided in Table S1. It should be noted that for POMO[19] and Pointerformer [14], the authors adopted an optional unit square transformations scheme during the inference phase. We omitted this step during comparisons to ensure fairness with other models.\nThe RUE test set contains 1280 instances with the node size of 50, derived from the test set used in prior works [29,36]. However, we have revised the exact solutions for this test set. For the other two types of instances with a node size of 50, we have used our own generated datasets, each consisting of 1000 instances."}, {"title": "Convolutional distribution Instances", "content": "Wang et al. proposed a new method called ASP[32]: Adaptive Staircase Policy Space Responce Oracle (PSRO), which aims to address the generalization issues faced by previous neural solvers and provide a \"universal neural solver\" that can be applied to a wide range of problem distributions and scales. They generate data by randomly sampling x \u2208 R\u00b2 from the unit square, and sampling y \u2208 R\u00b2 from N(0, \u03a3) where \u03a3\u2208 R2\u00d72 is a diagonal matrix whose elements are sampled from [0, \u5165] and x ~ U(0, 1). Next, a two-dimensional coordinate is generated by z = x + y. This describes a convolutional distribution which is the convolution of the uniform distribution and the Gaussian distribution.\nWe analyzed the nearest-neighbor density of these instances and found that they did not exhibit significantly low-density characteristics (Table S2). Interestingly, however, compared to a uniform distribution, there was indeed a noticeable decrease. This further demonstrates the effectiveness of our metric in evaluating the difficulty of instances for neural solvers. Additionally, we visualized some of the instances (Fig. S1), and interestingly, their distributions revealed regions of both sparsity and density. This closely resembles the examples we constructed based on scale-free complex networks."}]}