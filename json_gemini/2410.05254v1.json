{"title": "GLEE: A UNIFIED FRAMEWORK AND BENCHMARK\nFOR LANGUAGE-BASED ECONOMIC ENVIRONMENTS", "authors": ["Eilam Shapira", "Omer Madmon", "Itamar Reinman", "Samuel Joseph Amouyal", "Roi Reichart", "Moshe Tennenholtz"], "abstract": "Large Language Models (LLMs) show significant potential in economic and\nstrategic interactions, where communication via natural language is often preva-\nlent. This raises key questions: Do LLMs behave rationally? Can they mimic\nhuman behavior? Do they tend to reach an efficient and fair outcome? What is\nthe role of natural language in the strategic interaction? How do characteristics\nof the economic environment influence these dynamics? These questions become\ncrucial concerning the economic and societal implications of integrating LLM-\nbased agents into real-world data-driven systems, such as online retail platforms\nand recommender systems. While the ML community has been exploring the po-\ntential of LLMs in such multi-agent setups, varying assumptions, design choices\nand evaluation criteria across studies make it difficult to draw robust and mean-\ningful conclusions. To address this, we introduce a benchmark for standardizing\nresearch on two-player, sequential, language-based games. Inspired by the eco-\nnomic literature, we define three base families of games with consistent param-\neterization, degrees of freedom and economic measures to evaluate agents' per-\nformance (self-gain), as well as the game outcome (efficiency and fairness). We\ndevelop an open-source framework for interaction simulation and analysis, and\nutilize it to collect a dataset of LLM vs. LLM interactions across numerous game\nconfigurations and an additional dataset of human vs. LLM interactions. Through\nextensive experimentation, we demonstrate how our framework and dataset can\nbe used to: (i) compare the behavior of LLM-based agents to human players in\nvarious economic contexts; (ii) evaluate agents in both individual and collective\nperformance measures; and (iii) quantify the effect of the economic characteris-\ntics of the environments on the behavior of agents. We believe that our framework\ncan contribute to the growing intersection of LLMs, ML, and economics, and we\nencourage researchers to explore it further and build on its foundation.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent research has increasingly focused on the capabilities of Large Language Models (LLMs) in\ndecision-making tasks, revealing their potential to operate as autonomous agents in various eco-\nnomic environments, that typically require involving complex strategic thinking (Horton 2023;\nWang et al. 2023; Zhu et al. 2024; Li et al. 2024). Applications of LLM-based agents in such\nenvironments include Chess-playing (Feng et al. 2024), task-oriented dialogue handling (Ulmer\net al. 2024), financial advisement (Lakkaraju et al. 2023) and beyond. The promising capabilities\nof LLMs in strategic decision-making call for the study of these agents' behavior through the lens\nof game theory, e.g. whether and when LLM-based agents naturally converge to Nash-equilibrium\n(Guo et al. 2024), and how well they learn to cooperate in repeated interactions (Akata et al. 2023).\nThe rise of LLM agents also calls for a clear benchmark for assessing the extent to which such agent\nbehaves rationally, which is addressed by a recent work of Raman et al. (2024).\nAn important property of many real-world economic environments is that communication between\nagents typically occurs through natural language. While economic modeling usually abstracts away\nthese nuances for the sake of keeping the model simple and tractable, in practical scenarios the fact\nthat natural language is involved may significantly affect the interaction outcome. For instance, in"}, {"title": "2 GAME FAMILIES AND PARAMETRIZATION", "content": "In this section, we formally define the three game families discussed in the introduction: bargaining,\nnegotiation and persuasion.\nIn our modeling, we focus on three main degrees of freedom that are critical for understanding\nthe dynamics of LLM-based agents in economic interactions: game horizon, information structure,\nand communication form. The game horizon refers to the number of time periods during which\nthe game is played and whether the length of the horizon is known or unknown to the agents.\nThis factor influences the strategies agents adopt, particularly in terms of long-term planning and\nanticipation of future moves. The information structure determines whether agents are aware of each\nother's preferences, impacting their ability to predict and respond to the actions of others. Lastly, the\ncommunication form specifies whether communication between agents occurs through free language\nor structured, concise messages, which affects the richness and clarity of the exchanges."}, {"title": "2.1 BARGAINING GAMES", "content": "The first family of games is inspired by the celebrated bargaining model of Rubinstein (1982).\nThe model encompasses a class of bargaining games where two players, Alice and Bob, alternate\noffers over a time horizon $T$ (usually, $T = \\infty$) to divide a fixed sum of money $M$ between them.\nImportantly, in these games delays are costly, a concept captured by discount factors $\\delta_A$, $\\delta_B$ assigned\nto each player, reflecting the decreasing value of future payoffs as time progresses. Formally:\nAt each odd stage $t$, Alice offers a division $(p, 1 - p)$ for some $p \\in [0, 1]$. Bob decides whether to\naccept or reject. If Bob accepts, the (Alice, Bob) utility vector is given by $M(\\delta_A^{t-1}p, \\delta_B^{t-1}(1 - p))$.\nAt each even stage $t$, Bob offers a division $(q, 1 - q)$ for some $q \\in [0, 1]$. Alice decides whether to\naccept or reject. If Alice accepts, the game terminates, and the (Alice, Bob) utility vector is given\nby $M(\\delta_A^{t-1}q, \\delta_B^{t-1}(1 - q))$. If no agreement is reached at any stage, the utilities are defined to be\n$(0, 0)$. In the standard version of the game, the time horizon is infinite and the discount factors are\ncommon knowledge (i.e., Alice and Bob know both $\\delta_A$ and $\\delta_B$).\nIn our experiments, we simulate a wide range of such bargaining games, differing in the following\ndegrees of freedom: (i) whether or not the players know their opponent's discount factor (complete\nvs. incomplete information); (ii) whether or not the players know when the game terminates (finite\nvs. infinite); (iii) whether or not players communication involve natural language (structured vs.\nlinguistic); (iv) the values of $\\delta_A$, $\\delta_B \\in (0, 1)$ and $M$; (v) the value of $T$ in the finite horizon case.\nAn outcome of the game is a pair $(t_{ev}, p_{ev})$, where $t_{ev}$ is the stage index at which the game ter-\nminated, and $p_{ev}$ is the share of $M$ that Alice obtained (without considering the discount in the\nutilities). When the game terminates without reaching an agreement, we define $t_{ev} = \\infty$, and\nthe gain for both players is zero. The evaluation metrics used to assess the economic outcome are\nefficiency and fairness. Efficiency is now measured by the normalized sum of Alice's and Bob's"}, {"title": "2.2 NEGOTIATION GAMES", "content": "In the second family of games, referred to as negotiation games, a seller (Alice) and a buyer (Bob)\nare negotiating over the price of a product. At the outset, Alice owns a product she subjectively\nvalues at $V_A$. The subjective valuation of the potential buyer, Bob, is $V_B$. To capture the notion\nof valuation scale in negotiation games, we parameterize $V_i = M \\cdot F_i$ for $i \\in \\{A, B\\}$, where\n$F_i \\in (0, 1)$ is a factor parameter $M$ is a scale parameter.\nAs in the case of bargaining games, Alice and Bob alternate offers: at each odd stage, Alice posts a\nprice and Bob decides whether to buy the product or move on to the next stage. At each even stage,\nBob is the one to post a price and Alice decides whether to sell at this price or reject and move on\nto the next stage. The game is played for $T$ stages, which again can be either finite or \"infinite\"\n(i.e., large and unknown to both players). Unlike the bargaining game, here we assume no discount\nfactors on the utilities, hence whenever a price $p$ is accepted, the utilities for Alice and Bob are\n$p - V_A$ and $V_B - p$ respectively. If no trade is made, then the utilities are defined to be $(0, 0)$.\nIn this class of games, we consider the following degrees of freedom: (i) whether or not the players\nknow their opponent's product valuation (complete vs. incomplete information); (ii) whether or\nnot the players know when the game terminates (finite vs. infinite); (iii) whether or not players\ncommunication involve natural language (structured vs. linguistic); (iv) the values of $F_A, F_B \\in\n(0, 1)$ and $M$; (v) the value of $T$ in the finite horizon case.\nAn outcome of a negotiation game is captured by $p_{ev}$, which is the price at which the product is\nsold when there is a trade, and defined to be $p_{ev} = \\emptyset$ whenever no trade is made. We consider the\nfollowing evaluation measures of the game outcome fairness and efficiency. When there is trade,\nfairness is measured by $1 - 4 \\cdot \\frac{(p_{ev} - p_f)^2}{M^2}$, where $p_f = \\frac{V_A + V_B}{2}$ is the \"fairest price\". When trade\nis not made, we define the fairness to be 1 (i.e., maximal fairness) to reflect that no-trade does not\nchange the default allocation of the product. Efficiency is defined to be 1 in the following cases (and\nzero otherwise): (a) Alice values the product more than Bob, and does not sell it ($V_A \\ge V_B$ and\n$p = 0$); (b) Alice values the product less Bob, and sells the product at a price that is beneficial for\nboth players ($V_A \\le p < V_B$); When averaged over a large number of simulated games for a certain\ngame configuration, this measure estimated the probability of the event \"an efficient trade occurs\nwhen it should occur\"."}, {"title": "2.3 PERSUASION GAMES", "content": "In a persuasion game, a seller (Alice) tried to persuade a buyer (Bob) to buy a product at a fixed\nprice $\\pi$. Alice privately knows the true product quality, which can be either high or low. Bob only\nknows that the prior probability of the product being of high quality is $p$. Alice gets a utility of 1 if\nBob buys (regardless of the true product quality), and 0 otherwise.\nBob values a high-quality product at $v > \\pi$ and a low-quality product at $u < \\pi$. Therefore, the utility\nof Bob from buying a high-quality product is $v - \\pi > 0$, and from buying a low-quality product is\n$u - \\pi < 0$. For simplicity, we normalize the price to $\\pi = 1$ and the value of a low-quality product to\n$u = 0$. In addition, we consider a currency scale parameter $M$ that serves as a multiplicative term of\nBob's utility function. Overall Bob gets a utility of $M(v - 1)$ from buying a high-quality product,\n$-M$ from buying a low-quality product, and 0 from not buying the product.\nThe timing of a single round is as follows. First, Alice observes the product quality (which is\nrealized to be high-quality w.p. $p$, independently of other rounds). Then, Alice sends a message\nto Bob. Lastly, Bob decides whether to buy or not to buy the product, and utilities are realized"}, {"title": "3 DATA COLLECTION", "content": "In this section, we describe the data collection process. We developed a game management system to\nfacilitate data collection from the games described in \u00a72. The system is written in Python and is easy\nto use and customize, allowing future researchers to collect data seamlessly. New language models\ncan be added easily, enabling them to play in any configurable setup. With a single command line,\nthe system allows running either a single configuration or a set of configurations. Prompt samples\nare described in Appendix B."}, {"title": "3.1 LLM DATA COLLECTION", "content": ""}, {"title": "3.2 HUMAN DATA COLLECTION", "content": "One of the main objectives of collecting data from language games is to compare the behavior of\nLLMs with human behavior in economic and strategic situations. To facilitate this comparison, we\ndeveloped an interface that allows human players to play all the language games that can be defined\nusing GLEE. The interface transforms the various prompts presented to LLMs into user-friendly\nscreens for human participants, displaying the prompt and requesting them to send messages and\nmake decisions. Through this interface, we enable human players to take on the role of one of the\nplayers while the other player is a pre-selected LLM. The interface is one of the contributions of this\npaper. Screenshots of the interface can be found in Appendix C.1.\nThe interface, developed using oTree (Chen et al. 2016), enables integration with Amazon's crowd-\nsourcing platform, mTurk, through which we recruited 3,405 players who participated in various\nconfigurations against Gemini-1.5-flash. We chose Gemini-1.5-flash since it demonstrated strong\nperformance compared to other LLMs and allowed comprehensive data collection due to its low\nusage cost. Since we aimed to collect multiple games from each configuration, we had to select\na limited set of configurations for human participants to play. The process of selecting these sets"}, {"title": "4 EXPLORATORY DATA ANALYSIS", "content": "In this section, we first present a method to ensure an adequate comparison between models that\nplayed different configurations. Then, we address two key research questions: Q1: How did the\nlanguage models perform in each game family in terms of self-gain, efficiency, and fairness? Q2:\nHow did the humans perform compared to the language models? Q3: How do the parameters\nd Defining the games influence the various metrics?\nAdequate Comparison Between Models For the four language models introduced in \u00a73, we\ncollected data covering the entire set of the configurations defined in Table 1 for every possible\npair of language models. This enables seamless comparison of their performance across all defined\nmetrics. Henceforth, this dataset will be referred to as the baseline set, and the models that generated\nit will be referred to as the baseline models.\nWhen evaluating data generated by a new model or by humans, it is often not feasible (due to cost\nconsiderations) to have them play all games against all existing language models. To evaluate\nmodel performance on a large variety of language games, we can collect data from a subset of\nconfigurations, calculate the metrics for those, and use the results (along with the metrics from the\nbaseline set) to train a predictive model that estimates the metrics for configurations for which data\nwas not collected. This method allows for an adequate comparison between models that played\ndifferent configuration sets.\nFor the calculation of the metrics, we train a simple regression model for each metric. The model\nreceives as input the values defining the configuration as binary features (e.g., \"Is M = 100?\") and\nbinary features describing the interaction between the language models and other parameter values\n(e.g., \"Did Alice played by Gemini-1.5-Flash and M = 100?\" and \"Did Alice played by Gemini\n1.5-Flash, Bob played by Llama-3.1-8B, and M = 100?\"). To prevent the model from learning\nrelationships between parameter values directly from configuration settings, we used binary features\neven for parameters that contain numerical values. We trained the models on the data from all games\nand predicted the values for all configurations: both those for which data was collected and those for\nwhich it was not. Therefore, to maintain consistency, all the metrics are calculated by averaging the\npredicted values over all game configurations. This approach allows us to reasonably estimate the\nmetric values expected for configurations where no human data was collected. The exact formulas\nd Defining each regression model are presented in Appendix D.\nTo assess the quality of the models, we examined the adjusted R-squared values of the regression\nmodels. In 10 out of 12 trained models, we obtained adjusted R-squared values higher than 0.38.\nTable 3 shows the adjusted R-squared values of the models. The table shows that, in general, persua-\nsion games are the easiest to predict, and we hypothesize that this is because their outcomes consist\nof a collection of multiple decisions. Negotiation games are the most difficult to predict, possibly\nbecause, unlike the other games, players could exit the game at any given moment. It seems that fair-"}, {"title": "5 CONCLUSION", "content": "We present GLEE, a framework for evaluating the behavior of Large Language Models (LLMs) in\nlanguage-based economic games. The goal of GLEE is to provide a comparative tool for assessing\nthe performance of LLMs in various economic scenarios and enable their comparison to human\nplayers. We defined the game space within three main families of games: bargaining, negotiation,"}, {"title": "ETHICS STATEMENT", "content": "This paper aims to provide a platform for experimenting with agents in language-based economic\nenvironments. Naturally, this line of research may have various societal and ethical implications, as\nwe now discuss.\nFirst, studying the economic aspects of LLM-based agents has the potential to enhance the ability of\nagent designers to control and optimize the behavior of these agents. This capability can be utilized\nfor a variety of purposes, ranging from encouraging self-interested behavior at the expense of other\nparticipants in the environment (e.g., for maximizing revenue in competitive settings) to promoting\nefficient trade and fair behavior, or any combination of these sometimes non-aligned objectives. As\nthis research increases the power of LLM-based agents in economic environments, it is essential to\nemphasize that with great power comes great responsibility. We call for the responsible and ethical\nuse of these emerging capabilities to ensure they are leveraged for socially beneficial purposes rather\nthan exploitative ones.\nFurthermore, our framework demonstrates the capability of collecting data from human players to\nunderstand the differences and similarities between LLMs and humans in economic environments.\nWhile this line of research has the potential for a strong scientific contribution, particularly in the\nfield of behavioral economics, it also raises several ethical considerations. The collection of human\ndata must be conducted with careful regulation and adherence to clear ethical guidelines, such as\nthose established in venues like ICLR. The authors declare that there are no violations of the ICLR\nethics code in our study. The entire process of data collection from human participants is elaborated\nin \u00a73.2.\nIn addition to the challenges associated with the collection of human data, enhancing our under-\nstanding of LLMs from the lens of human behavior carries inherent risks. For instance, this research\ncould enhance our ability to design LLM agents that are difficult to distinguish from real humans.\nSuch capabilities could be misused for malicious purposes, including deception or manipulation.\nWhile the answer to whether these capabilities could be used for harmful causes is likely yes, we\nbelieve that the benefits of pursuing this line of research outweigh the risks when balanced with\nproper regulations. We advocate for pushing research forward while ensuring that any new tech-\nnologies are accompanied by safeguards to prevent harmful usage, particularly when human-like\nLLMs are involved.\nOur proposed framework can make these research directions more accessible to researchers from the\nML community and beyond, thereby encouraging a broader understanding of LLM behavior in eco-\nnomic contexts. However, as such accessibility increases, it is crucial to maintain ethical oversight\nand foster an open dialogue on potential misuse. We encourage researchers to use our framework\nwith full transparency and careful attention to potential misuse and negative consequences."}, {"title": "A GAME FAMILIES THROUGH THE LENS OF ECONOMIC LITERATURE", "content": "In this section, we discuss the economic literature related to the three game families considered in\nthis paper, and review some known theoretical results.\nBargining As mentioned in \u00a72, the standard bargaining model of Rubinstein (1982) consists of\ntwo players, Alice and Bob, engaging in alternating offers for a finite horizon ($T = \\infty$) with com-\nmonly known discount factors $\\delta_A, \\delta_B$. Our parameterization considers several additional degrees\nof freedom, such as finite vs. infinite time horizon, complete vs. incomplete information, and free\nlanguage messages vs. structured and concise messages. In the standard model, Rubinstein (1982)\nshowed that in the unique subgame-perfect equilibrium an agreement is reached in the first stage\n(i.e., utilities are not discounted), and the share of Alice is given by $Mp^*$, where $p^* = \\frac{1-\\delta_B}{1-\\delta_A\\delta_B}$.\nThe case of finite horizon can be solved using backward induction, and typically results in a different\noutcome compared to the infinite case. As $T$ grows, the equilibrium outcome approaches the one of\nthe infinite case. Extensions to incomplete information regarding the opponent's discount factor are\ntypically more challenging, and some of them are studied in the literature, e.g. Rubinstein (1985).\nNegotiation In a negotiation game, Alice and Bob negotiate over the price of an indivisible good.\nThe negotiation game differs from the bargaining games in several key aspects:\n1. Negotiation involves an indivisible product (e.g., Alice's product), while bargaining focuses\non dividing a divisible resource, such as money.\n2. In negotiation, Alice and Bob may have different subjective valuations of the product,\nwhereas in bargaining, both parties value the divisible resource similarly.\n3. Negotiation has no discounting, so the utility remains constant over time. In bargaining,\ndelays reduce the total value, encouraging faster agreement.\nIf the seller is uncertain regarding the buyer's valuation but has a prior belief distribution, a classical\nresult by Harris & Raviv (1981) and Riley & Zeckhauser (1983) states that it is always optimal to\nsell the product at a fixed price. In contrast, if the seller does not have a prior belief over the buyer's\nvaluation, and instead aims to minimize regret, then an optional pricing policy will be to randomly\nchoose a price from a carefully chosen distribution (Bergemann & Schlag, 2011).\nPersuasion Our persuasion game follows the structure of a cheap talk game (Crawford & Sobel,\n1982; Farrell & Rabin, 1996), where the sender (Alice) cannot commit to a signaling policy in\nadvance, unlike in Bayesian persuasion models (Kamenica & Gentzkow, 2011). Under the particular\npayoff structure considered in our persuasion game, it is well-known that the cheap-talk game only\nadmits a babbling equilibrium, i.e., an equilibrium in which all information is kept hidden (this is\ndue to the strong misalignment of interests between the two players). In contrast, if the seller can\ncommit to a signaling policy at the outset, as in Bayesian persuasion, then there exists a subgame-\nperfect equilibrium in which the seller commits to the following policy: When the product is of\nhigh quality, the seller recommends buying the product with probability 1. When the product is of\nlow quality, then the seller recommends with probability $q = min\\{\\frac{\\pi}{(v - \\pi)}, 1\\}$. This policy\nis also incentive-compatible, in the sense that the buyer always buys the product when the seller\nrecommends buying. While the long-living buyer case is well-studied in the economic literature,\nsuch games often admit multiple equilibria, which makes the games difficult to analyze and predict\n(Kim, 1996; Aumann & Hart, 2003). As for the case of myopic buyers, Best & Quigley (2024) draws\na connection between the repeated cheap talk game and the case of one-shot Bayesian persuasion,\nleading to an elegant analytical solution of the repeated game. Intuitively, the repetitive nature of the\ngame induces a reputation effect, which plays a similar role to the commitment power in standard\none-shot Bayesian persuasion."}, {"title": "B DATA COLLECTION SYSTEM", "content": "In this appendix, we present an example of the conversation structures with the language models\nused for data collection in GLEE. The example is from a bargaining game, but the system's operating\nprinciple is the same in negotiation and persuasion games as well.\nThe prompts are divided into two types: (1) instructions and guidelines sent to the language model\nas a system message, and (2) game management messages sent to the language model as a user mes-\nsage. The language model performed its action, which was documented in the prompt as an assistant\nmessage. The system message begins with precise game instructions, which are constructed at the\nstart of the game using the parameters that define the configuration. This is followed by a guideline\nspecifying the format in which the LLM is required to respond. Each time the language model is\nrequired to make a decision (of any type), the language model receives a message containing all\nthe information that has accumulated since its previous turn. This information appears as a system\nmessage and sometimes includes text written by the language model that played the second player.\nThroughout the appendix, the system prompt (or parts of it) that Alice received are highlighted in\nred. Game management messages Alice received are shown in purple, and her response is shown in\nmagenta. The system prompt that Bob received is highlighted in blue. Game management messages\nBob received are shown in teal, and his response is shown in cyan.\nInstructions to Alice:\nLet's play a game. In this game, there are two players. You are playing as Alice. You and Bob need\nto divide 1,000$ between you. In each round of the game, one of the players will make a proposal\nfor division and can describe their proposal in a few words. The other player will decide whether to\naccept the offer and finish the game, or to proceed to the next round, in which they will be the one\nmaking the proposal. We will relay messages between you and Bob. Beware of inflation! With each\npassing round, the money is worth 0% less for you. For Bob, the money is worth 10% less. You\nhave 10 rounds to divide the money, or both of you will get nothing!\nInstructions to Bob:\nLet's play a game. In this game, there are two players. You are playing as Bob. You and Alice need\nto divide 1,000$ between you. In each round of the game, one of the players will make a proposal\nfor division and can describe their proposal in a few words. The other player will decide whether to\naccept the offer and finish the game, or to proceed to the next round, in which they will be the one\nmaking the proposal. We will relay messages between you and Alice. Beware of inflation! With\neach passing round, the money is worth 10% less for you. For Alice, the money is worth 0% less.\nYou have 10 rounds to divide the money, or both of you will get nothing!\nAlice turn.\nAlice got this message from the system:\n'Round 1\nSend your offer to divide $1,000 and a message to Bob.'\nAlice guideline:\nSend your offer to divide 1,000$ and the message you attached in the JSON format:\n{\"alice_gain\": The part that you will receive in your offer,\n\"bob_gain\": The part that Bob will receive in your offer,\n\"message\": The message you pass to Bob}\nAlice's response:\n\"\"json {\"alice_gain\": 900, \"bob_gain\": 100,\n\"message\": \"Let's start fair. I'll take the bigger share, but you get something too.\"}"}, {"title": "C DATA COLLECTION FROM HUMAN PLAYERS", "content": "This appendix provides additional information on the human data collection interface described in\n\u00a73.2, which facilitates data collection from GLEE games played between humans and LLMs.\nC.1 SCREENSHOTS OF THE DATA COLLECTION INTERFACE\nGeneral application structure The structure of the application and the games consists of fixed\nparts and parts that vary between different game families. Each game starts with a screen where\nthe player enters their name, followed by an instruction screen (Figure 2 in bargaining, Figure 6 in\nnegotiation, and Figure 10 in persuasion). The instructions themselves can change depending on the\ntype of game and the parameters of the game. On the instruction screen, there is a hidden prompt\nto enter a code word in the text box below, which is designed to filter out unfocused participants. If\nthe player fails this test, they are taken to a screen that informs them of their failure. Otherwise, the\ngame itself begins. In each round, both the human player and the LLM player perform an action,\nwith the order depending on the game and configuration. An action could involve sending an offer\nto the other player (figure 3 in bargaining games, Figure 7 in negotiation games and Figure 11 in\nPersuasion games) or responding to the other player's offer (for instance, Figure 4 in bargaining\ngames, Figure 8 in negotiation games and Figure 12 in persuasion games). After both players have\ncompleted their actions, the human player is taken to a response screen (Figure 5), where he sees\nthe decision of the LLM player. Afterward, if the game is still not over, the human player continues\nfor another round. Once the game is finished, the player is taken to a quiz screen where they must\nanswer a question related to the technical details of the game (Figure 9). If the human answer\ncorrectly, they are directed to the final screen (Figure 13), where they receive a code to enter on the\nmTurk website. If the player fails the final quiz, they do not receive a completion code and are taken\nto a screen that informs them of their failure. In this case, we erase the game from our\ndatabase."}, {"title": "C.1.1 BARGAINING GAMES SCREENSHOTS", "content": ""}, {"title": "C.1.2 NEGOTIATION GAMES SCREENSHOTS", "content": ""}, {"title": "C.1.3 PERSUASION GAMES SCREENSHOTS", "content": ""}, {"title": "C.2 SELECTION OF CONFIGURATIONS FOR HUMAN DATA COLLECTION", "content": "In this appendix, we describe the method used to select which configurations human players would\nplay and how many times each configuration would be played. Each configuration is defined by both\nthe game parameters and the role of the human player (Alice or Bob).\nFor each family of games, we arbitrarily defined one configuration, which we referred to as the main\nconfiguration. This configuration contains the parameters that we deemed most interesting. For\nevery main configuration, we collected data for both possible roles of the human player (Alice or\nBob). In persuasion games, we defined two main configurations: one for recurring buyers and one\nfor manipulated buyers, due to the significant theoretical differences arising from this parameter.\nWe collected the largest amount of data from the main configurations to allow for more in-depth\nfollow-up research.\nConfigurations that were identical to one of the main configurations except for one parameter were\ncalled variants of the main configuration. We collected data for all of these configurations as well.\nAdditionally, we randomly sampled 5% of the other configurations and collected data from them as\nwell. These configurations were referred to as random configurations.\nDue to the desire to allocate the data collection budget to complex games, we did not collect any\ndata from games in which the human player was required to play at most one round (single-round\nbargaining games and persuasion games in which the human player is a manipulated buyer).\nFor each category, we determined the number of games we wanted to collect from each configuration\nbelonging to it. This decision was made based on budgetary considerations. In persuasion games, we\nwere able to collect fewer games from each configuration due to the fact that these games take longer\nto complete (and therefore, the payment players received for participating in them was higher).\nTable 5 describes the number of configurations that belonged to each category for each game and\nthe minimum number of games we collected from each category."}, {"title": "C.3 ATTENTION CHECKS FOR HUMAN PLAYERS", "content": "In this appendix, we describe the two attention tests that human players were required to complete.\nThe purpose of these tests was to ensure that the human players stayed focused on the game and\nmade conscious decisions, rather than random choices to finish the game as quickly as possible. The\nplayers were aware that their attention would be tested during the game, and they knew that they\nwould not be paid for the task if they failed these tests. Out of the 4,652 players who started the\ngame, 1,247 players (representing 26.8% of those who began the game) failed one of the tests and\nwere not included in the final dataset.\nThe first test appeared on the instruction screen. Toward the end of the instructions, a line requested\nplayers to write the code word \"sdkot\" in a text box that appeared at the end of the instructions phase.\nPlayers who did not write this word were immediately disqualified and did not start the game, as\nthey did not carefully read the instructions. A total of 412 players, representing 8.9% of those who\nbegan the game, failed this test.\nThe second test appeared at the end of the game. The human players were asked a basic question that\ndepended on the family of the game they played. They were required to select the correct answer\nfrom four possible options. Players who participated in a bargaining game were asked about the\ninflation rate in their game (499 players, representing 22.7% of respondents, failed this question\nand were excluded from the dataset). Players who participated in a negotiation game were asked\nabout the value of the product for them (68 players, representing 12.3% of respondents, failed this\nquestion and were excluded from the dataset). Players who participated in a persuasion game were\nasked about the price of products in the game (268 players, representing 18% of respondents, failed\nthis question and were excluded from the dataset). In total, 835 players, representing 19.7% of\nrespondents, failed the final question and were excluded from the dataset."}, {"title": "D FORMULAS USED TO TRAIN REGRESSION MODELS"}]}