{"title": "Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success", "authors": ["Moo Jin Kim", "Chelsea Finn", "Percy Liang"], "abstract": "Recent vision-language-action models (VLAs) build upon pretrained vision-language models and leverage diverse robot datasets to demonstrate strong task execution, language following ability, and semantic generalization. Despite these successes, VLAs struggle with novel robot setups and require fine-tuning to achieve good performance, yet how to most effectively fine-tune them is unclear given many possible strategies. In this work, we study key VLA adaptation design choices such as different action decoding schemes, action representations, and learning objectives for fine-tuning, using OpenVLA as our representative base model. Our empirical analysis informs an Optimized Fine-Tuning (OFT) recipe that integrates parallel decoding, action chunking, a continuous action representation, and a simple L1 regression-based learning objective to altogether improve inference efficiency, policy performance, and flexibility in the model's input-output specifications. We propose OpenVLA-OFT, an instantiation of this recipe, which sets a new state of the art on the LIBERO simulation benchmark, significantly boosting OpenVLA's average success rate across four task suites from 76.5% to 97.1% while increasing action generation throughput by 26x. In real-world evaluations, our fine-tuning recipe enables OpenVLA to successfully execute dexterous, high-frequency control tasks on a bimanual ALOHA robot and outperform other VLAs (\u03c0\u03bf and RDT-1B) fine-tuned using their default recipes, as well as strong imitation learning policies trained from scratch (Diffusion Policy and ACT) by up to 15% (absolute) in average success rate. We release code for OFT and pretrained model checkpoints at https://openvla-oft.github.io.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent vision-language-action models (VLAs)-robot poli- cies built by fine-tuning pretrained vision-language models on large-scale robot datasets for low-level robotic control-have demonstrated strong task performance, semantic generaliza- tion, and language following abilities across diverse robots and tasks [4, 33, 23, 22, 7, 15, 8, 50, 55, 3]. Despite their strengths, fine-tuning is crucial for satisfactory deployment of VLAs on novel robots and tasks, yet it is unclear what the most effective approach for adaptation is given the large design space. A robotics practitioner who wishes to fine-tune a VLA to a new robot setup and task may default to using the same training recipe used for pretraining (or a parameter-efficient variant), but it is not obvious whether this would yield the best policy, and there is limited empirical analysis of alternative fine-tuning approaches in the literature.\nPrior work has begun exploring VLA adaptation strategies, with Kim et al. [22] proposing parameter-efficient fine-tuning via LoRA. However, their autoregressive action generation remains too slow (3-5 Hz) for high-frequency control (25- 50+ Hz), and both LoRA and full fine-tuning of autoregressive VLAs often yield unsatisfactory performance in bimanual ma- nipulation tasks [51, 26, 3]. While recent approaches improve efficiency through better action tokenization schemes [2, 36], achieving 2 to 13\u00d7 speedups, significant latency between action chunks (e.g., 750 ms for the recent FAST approach [36]) still limits real-time deployment on high-frequency bimanual robots. Exploring alternative VLA adaptation approaches that achieve both satisfactory speed and quality remains an under- explored area of research.\nIn this work, we study key design decisions for adapt- ing VLAs to novel robots and tasks using OpenVLA, a representative autoregressive VLA, as our base model. We examine three key design choices: action decoding scheme (autoregressive vs. parallel generation), action representation (discrete vs. continuous), and learning objective (next-token prediction vs. L1 regression vs. diffusion). Our study reveals several key insights that build on each other: (1) parallel decoding with action chunking not only boosts inference efficiency but also improves success rates on downstream tasks while enabling greater flexibility in the model's input-output specifications; (2) continuous action representations further improve model quality compared to discrete representations; and (3) fine-tuning the VLA with an L1 regression objective yields comparable performance to diffusion-based fine-tuning while offering faster training convergence and inference speed. Building on these insights, we introduce OpenVLA-OFT: an instantiation of an Optimized Fine-Tuning (OFT) recipe that integrates parallel decoding and action chunking, continuous action representations, and an L1 regression objective to en- hance inference efficiency, task performance, and model input- output flexibility while maintaining algorithmic simplicity. We conduct experiments on both the standardized LIBERO simulation benchmark and dexterous tasks on a real bimanual ALOHA robot. In LIBERO, OpenVLA-OFT establishes a new state of the art by achieving 97.1% average success rate across four task suites, outperforming both fine-tuned OpenVLA policies [22] (76.5%) and \u03c0\u03bf policies [3] (94.2%) while achieving a 26\u00d7 speedup in action generation with 8-step action chunks. For real-world ALOHA tasks [53], we augment our recipe with FiLM [35] for enhanced language grounding,"}, {"title": "II. RELATED WORK", "content": "Prior works have leveraged language and vision founda- tion models to enhance robotic capabilities, utilizing them as pretrained visual representations that accelerate robotic policy learning [29, 32, 30, 19, 31], for object localization in robotics tasks [9, 45], and for high-level planning and reasoning [1, 17, 42, 16, 43, 18, 6]. More recently, researchers have explored fine-tuning vision-language models (VLMs) to directly predict low-level robotic control actions, producing \u201cvision-language-action\u201d models (VLAs) [4, 33, 23, 22, 7, 15, 8, 50, 55, 51, 3, 2], which have demonstrated effective generalization to out-of-distribution test conditions and unseen semantic concepts. These works focus primarily on model development, while we focus on developing a recipe for fine- tuning such models, justifying individual design decisions with insights that we gain from our empirical analysis.\nDespite the importance of fine-tuning for real-world VLA deployment, empirical analysis of effective fine-tuning recipes remains limited. While Kim et al. [22] study various parameter update strategies and from their findings show that LoRA fine- tuning enables effective adaptation to single-arm robots oper- ating at low control frequencies (< 10 Hz), their analysis does not extend to bimanual robots with high control frequencies (25-50+ Hz), a more complex control scenario. We address this gap by exploring VLA adaptation design decisions for fast inference and reliable task execution on a real-world bimanual manipulator with a 25 Hz controller.\nRecent works by Belkhale and Sadigh [2] and Pertsch et al. [36] improve VLA efficiency through new action to- kenization schemes, using vector quantization or discrete co- sine transform-based compression to represent action chunks (sequences of actions) with fewer tokens than simple per- dimension binning (as used in RT-2 [4] and OpenVLA [22]). While these approaches achieve 2 to 13\u00d7 speedups for autoregressive VLAs, we explore design decisions beyond autoregressive modeling, which remains inherently limited by iterative generation. Our parallel decoding approach, when paired with action chunking, achieves significantly greater speedups: 26\u00d7 to 43\u00d7 throughput with much lower latency (0.07 ms for single-arm tasks with one input image and 0.321 ms for bimanual tasks with three input images).\nAnother line of research [51, 26, 3] demonstrates effective VLA fine-tuning for high-frequency, bimanual manipulation using generative approaches like diffusion or flow match-"}, {"title": "III. PRELIMINARIES", "content": "Original OpenVLA formulation. We use OpenVLA [22] as our representative base VLA, a 7B-parameter manipulation policy created by fine-tuning the Prismatic VLM [20] on 1M episodes from the Open X-Embodiment dataset [33]. See Appendix A for architecture details. OpenVLA's original train- ing formulation uses autoregressive prediction of 7 discrete robot action tokens per timestep: 3 for position control, 3 for orientation control, and 1 for gripper control. It employs next-token prediction with cross-entropy loss as its learning objective, similar to language models. We explore alternative formulations including parallel decoding, continuous action representations, and learning objectives like L1 regression and diffusion modeling in the next few sections.\nAction chunking. Prior works have shown that action chunking-i.e., predicting and executing a sequence of future actions without intermediate replanning-improves policy suc- cess rates across many manipulation tasks [53, 5, 27]. How- ever, OpenVLA's autoregressive generation scheme makes action chunking impractical, as generating even a single- timestep action takes 0.33 seconds on an NVIDIA A100 GPU. For a chunk size of K timesteps and action dimensionality D, OpenVLA requires KD sequential decoder forward passes versus just D passes without chunking. This K-fold increase in latency makes action chunking impractical for high-frequency robots under the original formulation. In the next section, we present a parallel generation scheme that enables efficient action chunking."}, {"title": "IV. STUDYING KEY VLA FINE-TUNING DESIGN DECISIONS", "content": "In this section, we first outline key design decisions for adapting VLAs to novel robot setups and tasks and provide details on their implementation.\nA. VLA Fine-Tuning Design Decisions\nExisting approaches that fine-tune VLAs using the base model's autoregressive training recipe face two key limitations: slow inference speed (3-5 Hz) unsuitable for high-frequency control, and unreliable task execution on bimanual manipula- tors [51, 26, 3].\nTo address these challenges, we investigate three key design components for VLA fine-tuning:\n(a) Action generation strategy (Figure 2, left): We com- pare autoregressive generation, which requires sequential token-by-token processing, with parallel decoding, which generates all actions simultaneously and enables efficient action chunking.\n(b) Action representation (Figure 2, right): We examine discrete actions (256-bin discretization of normalized ac- tions) processed through softmax-based token prediction, versus continuous actions directly generated by an MLP action head. For discrete actions, the final hidden states of the language model decoder are linearly projected into logits, which are processed by a softmax operation to form the probability distribution over action tokens. For continuous actions, the final hidden states are instead mapped directly to normalized continuous actions by a separate action head MLP.\n(c) Learning objective (Figure 2, right): We compare policies fine-tuned with next-token prediction for discrete actions, L1 regression for continuous actions [53], and conditional denoising diffusion [5] for continous actions (similar to Chi et al. [5]).\nWe conduct our study using OpenVLA [22] as the base model, adapting it via LoRA fine-tuning [14] due to our relatively small training datasets (500 demonstrations versus 1M demonstrations for pretraining).\nB. Implementing Alternative Design Components\nThe base OpenVLA model originally employs autoregres- sive generation of discrete action tokens optimized via next- token prediction. We implement alternative design decisions"}, {"title": "V. EXPERIMENTS: EVALUATING VLA FINE-TUNING DESIGN DECISIONS", "content": "In this section, we evaluate the effects of our proposed VLA adaptation design decisions through controlled experiments aimed at answering three key questions:\n1) How does each design decision affect the fine-tuned policy's success rate on downstream tasks?\n2) How does each design decision affect model inference efficiency (action generation throughput and latency)?\n3) How do the alternative fine-tuning formulations affect flexibility in model input-output specifications?\nA. LIBERO Experimental Setup\nWe evaluate on the LIBERO simulation benchmark [25], which features a Franka Emika Panda arm in simulation with demonstrations containing camera images, robot state, task annotations, and delta end-effector pose actions. We use four task suites\u2014LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-Long-each providing 500 expert demon- strations across 10 tasks to assess policy generalization to different spatial layouts, objects, goals, and long-horizon tasks. Following Kim et al. [22], we filter unsuccessful demon- strations and fine-tune OpenVLA via LoRA [14] on each task suite independently. We train for 50-150K gradient steps for non-diffusion methods and 100-250K steps for diffusion methods (which converge slower), using a batch size of 64-128 across 8 A100/H100 GPUs. We test checkpoints every 50K steps and report the best performance for each run. Unless specified otherwise, policies receive one third-person image and language instruction as input. For methods using action chunking, we set chunk size to K 8 to match the Diffusion Policy baseline [5], and execute full chunks before replanning, which we find improves both speed and performance. See Appendix D for hyperparameter details.\nOur primary baseline in this study is the base OpenVLA model fine-tuned using the original fine-tuning recipe. How- ever, for broader comparison, we also include LIBERO results"}, {"title": "VI. EXPERIMENTS: ADAPTING OPENVLA TO A REAL-WORLD ALOHA ROBOT", "content": "While our experimental results in the prior section demon- strate OpenVLA-OFT's effectiveness in simulation, successful deployment in the real world, on robot platforms that differ substantially from those seen during pretraining, is crucial for showing broad applicability. We thus assess the efficacy of our optimized fine-tuning recipe on the ALOHA robot setup [53], a real bimanual manipulation platform operating at a high control frequency. We evaluate on novel dexterous manipulation tasks that have never been encountered before during OpenVLA's pretraining (which only involves single- arm robot data).\nPrior works [51, 26, 3] have shown that vanilla LoRA fine- tuning with autoregressive VLAs [22] is impractical for such tasks, as its throughput (3-5 Hz for single-arm robots and even lower for bimanual tasks) falls well below the 25-50 Hz required for real-time deployment. We therefore exclude this baseline from our experiments and compare more effective methods that we discuss shortly.\nIn this section, we use an augmented version of our VLA fine-tuning recipe (OFT+) that additionally includes feature- wise linear modulation (FiLM) for enhanced language ground- ing, as described in Section IV-C. We denote the OpenVLA policy instantiated through this augmented fine-tuning recipe as OpenVLA-OFT+.\nA. ALOHA Experimental Setup\nThe ALOHA platform comprises two ViperX 300 S arms, three camera viewpoints (one top-down, two wrist-mounted), and robot state inputs (14-dimensional joint angles). It operates at 25 Hz (reduced from the original 50 Hz to enable faster training while still maintaining smooth robotic control), with actions representing target absolute joint angles. This setup dif- fers significantly from OpenVLA's pretraining, which includes single-arm robot data only, a single camera viewpoint from a third-person camera, no robot state inputs, low-frequency control (3-10 Hz), and relative end-effector pose actions. The distribution shift poses a challenge to the adaptation of this model.\nWe design four representative tasks testing deformable object manipulation, long-horizon skills, tool usage, and language-driven control:\n1) \u201cfold shorts\u201d: Fold white shorts on a table with two consecutive bimanual folds. Training: 20 demonstrations. Evaluation: 10 trials.\n2) \u201cfold shirt\u201d: Fold white T-shirt through multiple synchro- nized bimanual folds, testing contact-rich, long-horizon manipulation. Training: 30 demonstrations. Evaluation: 10 trials.\n3) \u201cscoop X into bowl\u201d: Move bowl to center of table with left arm, scoop specified ingredient (\u201craisins,", "almonds and green M&Ms,\u201d or \u201cpretzels": "with right arm using metal spoon. Training: 45 demonstrations (15 per ingre- dient). Evaluation: 12 trials (4 per ingredient).\n4) \u201cput X into pot\u201d: Open pot with left arm, place specified item (", "pepper,": "red pepper,", "yellow corn": "with right arm, close pot. Training: 300 demonstrations (100 per object). Evaluation: 24 trials (12 in-distribution, 12 out-of-distribution).\nWe fine-tune OpenVLA using OFT+ on each task indepen- dently for 50-150K gradient steps (total batch size 32 with 8 A100/H100-80GB GPUs) with action chunk size K = 25. At inference time, we execute the full action chunk before requerying the model for the next chunk.\nB. Methods in Comparison\nThe ALOHA tasks present a significant adaptation chal- lenge for OpenVLA as the base model, given the substantial differences from its pretraining platforms in terms of con- trol frequency, action space, and input modalities. For this reason, we compare OpenVLA-OFT+ against more recent VLAs-RDT-1B [26] and \u03c0\u03bf [3]-that were pretrained on bimanual manipulation data and might reasonably be expected to perform better on these downstream tasks. We evaluate these models after fine-tuning them using their authors' rec- ommended recipes, and these methods serve as important points of comparison. Additionally, to provide comparisons with computationally efficient alternatives, we evaluate two popular imitation learning baselines: ACT [53] and Diffusion Policy [5], trained from scratch on each task.\nTo enable language following in these baseline methods, we use language-conditioned implementations. For ACT, we modify EfficientNet-B0 [46] to process CLIP [37] language embeddings via FiLM [35, 41].* For Diffusion Policy, we use the DROID dataset [21] implementation that conditions action denoising on DistilBERT [40] language embeddings, modified to support bimanual control and multiple image inputs."}, {"title": "VII. DISCUSSION", "content": "Our study on VLA fine-tuning design decisions reveals how different components impact inference efficiency, task performance, model input-output flexibility, and language fol- lowing ability. These insights lead to our Optimized Fine- Tuning (OFT) recipe, which enables effective VLA adaptation to novel robots and tasks through parallel decoding, action chunking, continuous actions, L1 regression, and (option- ally) FiLM language conditioning. The success of OFT is particularly noteworthy with OpenVLA: despite having no exposure to bimanual robots or multi-view image inputs during pretraining, OpenVLA fine-tuned with OFT can adapt to such configurations and match or even outperform more recent diffusion-based VLAs (\u03c0\u03bf and RDT-1B) which have encoun- tered bimanual manipulators and multiple input images during pretraining. This demonstrates that a well-designed fine-tuning recipe can have a significant impact on final performance, and existing VLAs can be successfully adapted to new robotic systems without extensive retraining from scratch. Moreover,"}, {"title": "VIII. LIMITATIONS", "content": "While our Optimized Fine-Tuning (OFT) recipe shows promise for adapting VLAs to novel robots and tasks, several important questions remain.\nHandling multimodal demonstrations. Our experiments use focused demonstration datasets with a consistent strategy per task. While L1 regression may help smoothen out noise in training demonstrations by encouraging the policy to learn the median mode in demonstrated actions, it may struggle to accurately model truly multimodal action distributions where multiple valid actions exist for the same input, which may not be ideal in cases where the ability to generate alternative action sequences would be beneficial for task completion. Conversely, diffusion-based approaches may better capture such multi- modality but risk overfitting to suboptimal modes in training data (see our website for discussions and video illustrations of these nuances). Understanding OFT's effectiveness with multimodal demonstrations remains an important direction for future work.\nPretraining versus fine-tuning. Our study focuses specif- ically on fine-tuning VLAs for downstream tasks. Whether OFT's benefits extend effectively to pretraining, or whether more expressive algorithms like diffusion are necessary for large-scale training, requires further investigation.\nInconsistent language grounding. Our ALOHA exper- iments reveal that OpenVLA without FiLM exhibits poor language grounding, despite showing no such issues in LIBERO simulation benchmark experiments. The source of this discrepancy-whether from the lack of bimanual data in pretraining or other factors-remains unclear and warrants further study."}, {"title": "APPENDIX", "content": "A. Model Architecture Details\nBase OpenVLA Architecture. OpenVLA combines a fused vision backbone (with both SigLIP [52] and DINOv2 [34] vision transformers), a Llama-2 7B language model [49], and a 3-layer MLP projector with GELU activation [11] for projecting visual features into the language embedding space. The original model processes a single third-person image and a language instruction (e.g., \"put eggplant into pot\"). The fused vision encoder extracts 256 patch embeddings from each vision transformer, concatenates them along the hidden dimension, and projects them into the language embedding space. These projected features are concatenated with language embeddings along the sequence dimension before being pro- cessed by the Llama-2 decoder to output a 7-dimensional robot action representing delta end-effector pose, represented by a string of discrete action tokens.\nOpenVLA-OFT architecture modifications. OpenVLA- OFT introduces six key changes:\n1) processes multiple input images (e.g., third-person image plus wrist camera images) through the shared SigLIP- DINOv2 backbone\n2) projects robot proprioceptive state to language embedding space via a 2-layer MLP with GELU activation\n3) replaces causal attention with bidirectional attention for parallel decoding\n4) substitutes the language model decoder output layer with a 4-layer MLP (ReLU activation) for generation of con- tinuous actions (instead of discrete actions)\n5) outputs chunks of K actions instead of single-timestep actions\n6) (for OpenVLA-OFT+) adds FiLM [35] modules that use the average task language embedding to modulate visual features in both SigLIP and DINOv2 vision transformers (see Appendix C for details)\nThe complete OpenVLA-OFT+ architecture is illustrated in Figure 1.\nB. Implementation Details\n1) Parallel Decoding Implementation: In the original OpenVLA autoregressive training scheme, the model receives ground-truth action tokens shifted right by one position as input (a setup known as teacher forcing). A causal attention mask ensures the model only attends to current and previous tokens. At test time, each predicted token is fed back as input for the next prediction.\nFor parallel decoding, we replace this input with empty action embeddings that differ only in their positional encoding values (similar to [53]). We also use a bidirectional attention mask (instead of causal), enabling the model to leverage all intermediate features non-causally when predicting each element in the action chunk.\n2) Continuous Action Representations: For discrete actions, increasing the number of bins used for discretization improves precision but reduces the frequency of individual tokens in the training data, potentially hurting generalization. On the other hand, with a continuous action representation, the VLA can directly model the action distribution without lossy discretiza- tion.\nOur continuous representation implementations use the fol- lowing specifications:\nL1 regression: The MLP action head consists of 4 layers with ReLU activation, mapping final Llama-2 decoder layer hidden states directly to continuous actions.\nDiffusion: We use:\n\u2022 DDIM [44] sampler with 50 diffusion timesteps\n\u2022 Squared cosine beta schedule following [5, 54]\n\u2022 4-layer noise predictor with same MLP architecture as the L1 regression head\n3) Input Processing Details: Passing each input image through the OpenVLA fused vision encoder produces 256 patch embeddings, which are projected to the langauge model embedding space via a 3-layer MLP with GELU activation [11]. Low-dimensional robot states are also projected to the language embedding space through a 2-layer MLP with GELU activation.\nC. Feature-wise Linear Modulation (FiLM) Implementation Details\nFiLM schematic. Section IV-C describes how we imple- ment feature-wise linear modulation (FiLM) [35] for Open- VLA. Figure 8 illustrates our implementation. OpenVLA has a fused vision encoder with both SigLIP [52] and DINOv2 [34] vision transformers, and we apply FiLM to both transformers.\nDesign considerations. In our implementation, following Perez et al. [35], we multiply F by (1+\u03b3) instead of y since y and \u1e9e are near zero at initialization. This helps preserve the visual encoder's original activations at the start of fine-tuning, minimizing perturbation in the pretrained representation.\nImplementation specifics. The functions f(x) and h(x) that project language embeddings to obtain y and \u1e9e are imple- mented as simple affine transformations. Separate projectors are learned for each transformer block to allow for block- specific modulation patterns. This design enables the model to learn different modulation patterns at different levels of visual feature processing.\nOne might initially consider modulating each patch embed- ding independently, as opposed to each hidden dimension of each embedding as discussed in Section IV-C. However, our spatially-agnostic modulation approach more closely mirrors FiLM's operation in convolutional networks, where modula- tion applies globally across spatial dimensions since entire feature maps are scaled and shifted by individual elements of \u03b3 and \u03b2. This design choice better maintains the benefits of FiLM and improves the policy's language grounding substan- tially. We find that an alternative formulation that modulates each patch embedding independently leads to weaker language grounding.\nD. OpenVLA-OFT Hyperparameters and Training Details\nOpenVLA-OFT training details for LIBERO. Hyper- parameters for OpenVLA-OFT fine-tuning on LIBERO are"}]}