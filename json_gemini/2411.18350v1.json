{"title": "TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using Diffusion Models", "authors": ["Riza Velioglu", "Petra Bevandic", "Robin Chan", "Barbara Hammer"], "abstract": "This paper introduces Virtual Try-Off (VTOFF), a novel\ntask focused on generating standardized garment images\nfrom single photos of clothed individuals. Unlike traditional\nVirtual Try-On (VTON), which digitally dresses models,\nVTOFF aims to extract a canonical garment image, posing\nunique challenges in capturing garment shape, texture, and\nintricate patterns. This well-defined target makes VTOFF\nparticularly effective for evaluating reconstruction fidelity\nin generative models. We present TryOffDiff, a model that\nadapts Stable Diffusion with SigLIP-based visual condition-\ning to ensure high fidelity and detail retention. Experiments\non a modified VITON-HD dataset show that our approach\noutperforms baseline methods based on pose transfer and\nvirtual try-on with fewer pre- and post-processing steps.\nOur analysis reveals that traditional image generation met-\nrics inadequately assess reconstruction quality, prompting\nus to rely on DISTS for more accurate evaluation. Our re-\nsults highlight the potential of VTOFF to enhance product\nimagery in e-commerce applications, advance generative\nmodel evaluation, and inspire future work on high-fidelity\nreconstruction. Demo, code, and models are available at:\nhttps://rizavelioglu.github.io/tryoffdiff/", "sections": [{"title": "1. Introduction", "content": "Image-based virtual try-on (VTON) [23] is a key computer\nvision task aimed at generating images of a person wear-\ning a specified garment. Typically, two input images are\nrequired: one showing the garment in a standardized form\n(often from an e-commerce catalog) and another of the per-\nson that needs to be 'dressed'. Recent methods focus on a\nmodified formulation where the catalog image is replaced\nwith a photo of another person wearing the target garment.\nThis introduces additional processing complexity [55] as\nthe model does not have access to full garment information.\nFrom an application perspective, VTON offers an inter-\nactive shopping experience that helps users make better-\ninformed purchasing decisions. On the research side, it\nraises intriguing research questions, particularly around hu-\nman pose detection as well as clothing shape, pattern, and\ntexture analysis [17]. Best-performing models are usu-\nally guided generative models focused on creating specific,\nphysically accurate outputs. Unlike general generative tasks\nthat produce diverse outputs, reconstruction requires mod-\nels to generate images that align with the correct appearance\nof the garment on a person.\nHowever, one drawback of VTON is the lack of a clearly\ndefined target output, often resulting in stylistic variations"}, {"title": "2. Related Work", "content": "Virtual Try-Off seeks to reconstruct a canonical image of\nclothing, typically resembling garments worn by a person\nin a neutral pose. While virtual try-on and pose-transfer\nmethods could be adapted to produce these standardized\noutputs, our experiments indicate that such adaptations un-\nderperform. Instead, we base our solution on conditional\ndiffusion models, which have demonstrated robust perfor-\nmance across diverse generative tasks.\nImage-based Virtual Try-On. The objective of image-\nbased virtual try-on is to produce composite images that re-\nalistically depict a specific garment on a target person, pre-\nserving the person's identity, pose, and body shape, while\ncapturing fine garment details. CAGAN [23] introduced\nthis task with a cycle-GAN approach, while VITON [17]\nformalized it as a two-step, supervised framework: warp-\ning the garment through non-parametric geometric transfor-"}, {"title": "3. Methodology", "content": "This section provides the formal definition of the virtual try-\noff task. We propose a suitable evaluation setup and perfor-\nmance metrics. We further provide details of our TryOffDiff\nmodel which relies on StableDiffusion and SigLIP features\nfor image-based conditioning."}, {"title": "3.1. Virtual Try-Off", "content": "Problem Formulation. Let $I \\in \\mathbb{R}^{H \\times W \\times 3}$ be an RGB\nimage with height $H \\in \\mathbb{N}$ and width $W \\in \\mathbb{N}$, respectively.\nIn the task of virtual try-off, $I$ represents a reference im-\nage displaying a clothed person. Given the reference im-\nage, VTOFF aims to generate a standardized product image\n$G\\in \\{0,...,255\\}^{H \\times W \\times 3}$, displaying the garment accord-\ning to commercial catalog standards.\nFormally, the goal is to train a generative model that\nlearns the conditional distribution $P(G|C)$, where $G$ and\n$C$ represent the variables corresponding to garment im-\nages and reference images (serving as condition), respec-\ntively. Suppose the model approximates this target distri-\nbution with $Q(G|C)$. Then, given a specific reference im-\nage $I$ as conditioning input, the objective is for a sample\n$\\hat{G} \\sim Q(G|C = I)$ to resemble a true sample of a garment\nimage $G \\sim P(G|C = I)$ as closely as possible.\nPerformance Measures. To evaluate VTOFF perfor-\nmance effectively, evaluation metrics must capture both re-\nconstruction and perceptual quality. Reconstruction quality\nquantifies how accurately the model's prediction $\\hat{G}$ matches\nthe ground truth $G$, focusing on pixel-level fidelity. In con-\ntrast, perceptual quality assesses how natural and visually\nappealing the generated image appears to human observers,\naligning with common visual standards.\nTo estimate reconstruction, we may use full-reference\nmetrics such as Structural Similarity Index Measure\n(SSIM) [54]. However, neither SSIM, nor its multi-\nscale (MS-SSIM) and complex-wavelet (CW-SSIM) vari-\nants align well with human perception, as noted in prior\nstudies [11, 45]. We observe similar behavior in our exper-\niments as well, and illustrate our findings in Figure 3.\nPerceptual quality may be captured with no-reference\nmetrics like Fr\u00e9chet Inception Distance (FID) [20] and Ker-\nnel Inception Distance (KID) [5]. These metrics usually\ncompare distributions of image feature representations be-\ntween generated and real images. They are however unsuit-\nable for single image pair comparison since they are sen-\nsitive to sample size and potential outliers. Additionally,\nboth FID and KID rely on features from the classical In-\nception [44] model, which does not necessarily align with\nhuman judgment in assessing perceptual quality, especially\nin the context of modern generative models such as diffu-\nsion models [42]."}, {"title": "3.2. TryOffDiff", "content": "We base our TryOffDiff model on Stable Diffusion [35]\n(v1.4), a latent diffusion model originally designed for text-\nconditioned image generation using CLIP's [34] text en-\ncoder. We replace text prompts for direct image-guided im-\nage generation.\nImage Conditioning. A core challenge in image-guided\ngeneration is effectively incorporating visual features into\nthe conditioning mechanism of the generative model.\nCLIP's ViT [34] has become a popular choice for image fea-\nture extraction due to its general-purpose capabilities. Re-\ncently, SigLIP [59] introduced modifications that improve\nperformance, particularly for tasks requiring more detailed\nand domain-specific visual representations. Therefore, we"}, {"title": "4. Experiments", "content": "We establish several baseline approaches for the virtual try-\noff task, adapting virtual try-on and pose transfer models as\ndiscussed in Section 2, and compare them against our pro-\nposed TryOffDiff method described in Section 3. To ensure\nreproducibility, we detail our experimental setup. We use\nDISTS as the primary evaluation metric, while also report-\ning other standard generative metrics for comparison. Addi-\ntionally, we provide extensive qualitative results to illustrate\nhow our model manages various challenging inputs."}, {"title": "4.1. Experimental Setup", "content": "Dataset. Our experiments are conducted on the pub-\nlicly available VITON-HD [27] dataset, which consists of\n13, 679 high-resolution (1024 \u00d7 768) image pairs of frontal\nhalf-body models and corresponding upper-body garments.\nWhile the VITON-HD dataset was originally curated for the\nVTON task, it is also well-suited to our purposes as it pro-\nvides the required (I, G) image pairs, where I represents\nthe reference image of a clothed person and G the corre-\nsponding garment image.\nUpon closer inspection of VITON-HD, we identified 95\nduplicate image pairs (0.8%) in the training set and 6 du-\nplicate pairs (0.3%) in the test set. Additionally, we found\n36 pairs (1.8%) in the training set that had been included in\nthe original test split. To ensure the integrity of our exper-\niments, we cleaned the dataset by removing all duplicates\nin both subsets as well as all leaked examples from the test\nset. The resulting cleaned dataset, contains 11,552 unique"}, {"title": "4.2. Baseline Approaches", "content": "To establish the baselines, we adapted state-of-the-art pose\ntransfer and virtual try-on methods, modifying each to ap-\nproximate garment reconstruction functionality as closely\nas possible. We illustrate these approaches in Figure 5.\nGAN-Pose [36] is a GAN-based pose transfer method\nthat expects three inputs: a reference image, and pose\nheatmaps of the reference and target subject. Garment im-\nages from VITON-HD are used to estimate the heatmap for\na fixed, neutral pose. This setup enables the transfer of hu-\nman poses from diverse reference images to a standardized\npose, aligning the output to the typical view of product im-\nages.\nViscoNet [7] requires a text prompt, a pose, a mask, and\nmultiple masked conditioning images as inputs. For the\ntext prompt, we use a description such as \"a photo of an\ne-commerce clothing product\". We choose a garment im-\nage from VITON-HD to estimate a neutral pose as well as\na generic target mask. Since ViscoNet is originally trained\nwith masked conditioning images, we apply an off-the-shelf\nfashion parser [50] to mask the upper-body garment, which\nis then provided as input.\nOOTDiffusion [57] takes a garment image and a refer-\nence image to generate a VTON output. To adapt this model\nfor VTOFF, we again apply the fashion parser [50] to mask\nthe upper-body garment to create the garment image. We\nselect a reference image with a mannequin in a neutral pose\nas further input. An intermediate step involves masking the\nupper-body within the reference image, for which we use a\nhand-crafted masked version of the reference image."}, {"title": "4.3. Quantitative Results", "content": "The numerical results of our experiments on the VITON-\nHD dataset are reported in Table 1. Our tailored TryOffD-\niff approach outperforms all baseline methods across all\ngenerative performance metrics. However, baseline rank-\nings vary significantly depending on the chosen metric. For\nexample, GAN-Pose has the second best results when us-\ning full-reference metrics like SSIM, MS-SSIM, and CW-\nSSIM. In contrast, for no-reference metrics such as FID,\nCLIP-FID, and KID, CatVTON emerges as the strongest\nbaseline, while GAN-Pose has the lowest performance.\nThe DISTS metric is our main metric as it balances struc-\ntural and textural information, offering a more nuanced as-\nsessment of generated image quality. When examining the\nranking of the baseline methods, CatVTON slightly out-\nperforms GAN-Pose, which in turn shows marginally bet-\nter performance than ViscoNet and OOTDiff. This rank-\ning aligns well with our own subjective visual perception,\nwhich will be further discussed in the following Section 4.4.\nWe emphasize that TryOffDiff shows a significant improve-\nment of 5.2 percentage points over the next best performing\nbaseline method."}, {"title": "4.4. Qualitative Analysis", "content": "The qualitative results are shown in Figure 6. We find that\nthey align with the quantitative results and illustrate how\neach metric emphasizes different aspects of garment recon-\nstruction leading to inconsistent rankings, as discussed in"}, {"title": "5. Conclusion", "content": "In this paper, we introduced VTOFF, a novel task focused\non reconstructing a standardized garment image based on\none reference image of a person wearing it. While VTOFF\nshares similarities to VTON, we demonstrate it is better\nsuited for evaluating the garment reconstruction accuracy of\ngenerative models since it targets a clearly defined output.\nWe further propose TryOffDiff, a first tailored VTOFF\nmodel which adapts Stable Diffusion. We substitute Stable\nDiffusion text conditioning with adapted SigLIP features to\nguide the generative process. In our experiments, we repur-\npose the existing VITON-HD dataset, enabling direct com-\nparisons of our method against several baselines based on\nexisting VTON approaches. TryOffDiff significantly out-\nperforms these baselines, with fewer requirements for pre-"}]}