{"title": "CoAst: Validation-Free Contribution Assessment for Federated Learning based on Cross-Round Valuation", "authors": ["Hao Wu", "Likun Zhang", "Shucheng Li", "Fengyuan Xu*", "Sheng Zhong"], "abstract": "In the federated learning (FL) process, since the data held by each participant is different, it is necessary to figure out which participant has a higher contribution to the model performance. Effective contribution assessment can help motivate data owners to participate in the FL training. Research works in this field can be divided into two directions based on whether a validation dataset is required. Validation-based methods need to use representative validation data to measure the model accuracy, which is difficult to obtain in practical FL scenarios. Existing validation-free methods assess the contribution based on the parameters and gradients of local models and the global model in a single training round, which is easily compromised by the stochasticity of model training. In this work, we propose CoAst, a practical method to assess the FL participants' contribution without access to any validation data. The core idea of CoAst involves two aspects: one is to only count the most important part of model parameters through a weights quantization, and the other is a cross-round valuation based on the similarity between the current local parameters and the global parameter updates in several subsequent communication rounds. Extensive experiments show that CoAst has comparable assessment reliability to existing validation-based methods and outperforms existing validation-free methods.", "sections": [{"title": "1 Introduction", "content": "With the development of deep learning (DL), the concept that \"Data is the new oil\" has gained more and more consensus among people [7]. The emerging remarkable capabilities demonstrated by large language models [20] further bring attention to the enormous value of collaborating on a large amount of data. To train DL models on data owned by different parties, collaborative learning techniques, represented by federated learning (FL) [9, 25], have been extensively studied. However, due to disparities in the data held by different participants, including variations in data quality and quantity, each participant's contribution to the performance of the FL model differs a lot. How to accurately evaluate the contribution of each participant is crucial for the fair distribution of rewards. This process benefits the promotion of data quality and creates incentives for data sharing [19].\nThere has been a line of research in the community on the contribution assessment of participants in FL, which can be mainly divided into two categories, i.e., validation-based methods [4, 8, 21] and validation-free methods [12, 16, 23, 24]. The effectiveness of validation-based methods heavily relies on a representative validation dataset, which is used to evaluate the model performance. However, in real-world FL scenarios, obtaining the representative validation dataset that covers the distribution of all clients' data can be challenging. To overcome the limitations imposed by the validation dataset, validation-free methods are proposed to assess the contribution based on the statistical characteristics of model parameters. They estimate the parameters' correlation, information gain, and mutual information among local models (produced by clients) and the global model (aggregated by the server) to answer whose contribution is higher."}, {"title": "2 Related Works", "content": "Validation-based methods. The validation-based methods use a validation dataset to assess a client's contribution by evaluating its impact on the performance of the aggregated model. The leave-one-out is the most natural way to assess the value. It assesses the data value of one contributor by calculating the model performance change when the contributor is removed from the set of contributors. However, the leave-one-out is unfair to multiple similar and mutually substitutable contributors. Ruoxi et al. [8] use the Shapley value to assess data value in the FL scenario. They compute the marginal increase of the average accuracy of the model due to the addition of one data contributor. Guan et al. [17] extend the application scenarios of Shapley value-based solutions to FL scenarios. Zhenan et al. [4] apply the Shapley value-based solutions to vertical federated learning and improve the efficiency through approximation. Zhaoxuan et al. [21] allow for efficient data valuation without long-term model training. They assess the contribution through a domain-aware generalization bound, which is derived from the neural tangent kernel (NTK) theory. There are also research efforts to improve the system performance [13, 15, 18] and to analyze the fairness [27]. When the server uses this line of work to assess the clients' contribution, it requires a representative dataset which covers the distribution among all clients' data. However, in real-world FL scenarios, obtaining such a representative validation dataset is infeasible.\nValidation-free methods. The validation-free methods use statistics of training data or the correlation among local and global parameters to value the clients. These works usually have some specific assumptions on the distribution of gradients, model parameters, or local training data. Therefore, they may face performance degradation in the real world when their assumptions are not satisfied. Xinyi et al. [24] propose a volume measure on the replication robustness, which assesses the contribution based on the diversity of training data. However, work [21] shows that this idea not only suffers from exploding volumes in high-dimensional inputs but also entirely ignores the useful information in the validation dataset. Rachael et al. [16] measure the data value based on the information gains of the model parameters. They hold that the contributors with the highest value can reduce the uncertainty of the model parameters. However, when the distribution of data is complex, the accuracy of the information gains is biased. Xinyi et al. [23] use the gradient similarity to measure the data value of the contributors' combination by comparing the data of one combination of the contributors with the gradient similarity of the global FL model trained"}, {"title": "3 Problem Overview", "content": "3.1 Targeted Scenario\nWe assume all participants, including clients and the server, are honest and follow the agreed-on training protocol of FL. Due to differences in training data quality and quantity among clients, each client's contribution to overall model performance varies. The server can access the model parameters uploaded by each client, but it lacks a representative validation dataset. Each client delegates the server to evaluate the contribution of each client without offering validation data. Without loss of generality, we assume that each client is involved in all training rounds. The training data of each client is prepared before the FL training, and they will not add new data during the training process.\n3.2 Problem Formalization\nConsider an FL training procedure with one server and N clients. The training procedure consists of M training rounds. Recall that, in one training round, a client uploads its local parameters to the server once and receives the corresponding aggregated parameters (a.k.a. global parameters). After M training rounds, the contribution of all clients is determined. We denote the ground-truth contribution of all clients as P = {P1, P2, ..., PN}, where pi is the contribution of client i. The ranking of all clients' contribution is denoted as R = {r1, r2, ..., rN }, and\n$r_i = |\\{j|p_j \\geq P_i\\}|,$ (1)\nwhere $|\\cdot |$ returns the number of elements in a set.\nOur goal is to design a function L, which can measure how much each client improves the performance of the global model. That is,\nP = L(\u0398, {\u03b8_i^t}_{i \\in [1,N],t\\in [1,M]}), (2)\nwhere denotes the parameters of the global model in the last round, and denotes the parameters of the local model trained by client i in round t. Then the predicted R can be calculated based on the P through Equation 1. The objective of function L is to minimize the distance of the predicted R and R, i.e.,\nmin d(R, \\hat{R}), (3)\nwhere d(,) is the distance measurement function.\nDue to the multi-round property of FL algorithm, the contribution score of each client in round t (denoted as pti ) can be naturally represented as:\nP^t = L(\u0398^{t-1}, \\{\u03b8_i^t\\}). (4)"}, {"title": "4 CoAst Design", "content": "4.1 Design Overview\nWe propose two key techniques to address the challenges introduced in the introduction section. First, we design a cross-round valuation mechanism. Due to stochastic gradient descent, gradient pruning, and parameter regularization, the parameter updates in a certain round (e.g., round t) may not accurately reflect the true value of the client, and even high-value clients may be assigned negative contributions. Fortunately, the FL training process minimizes the optimization objective and improves the model's accuracy, which means that the global parameter updates have a positive contribution in most training rounds. It allows us to value the client i in round t with global parameters of subsequent several rounds.\nSecond, we borrow ideas from efforts on model compression and quantization to filter out those unimportant parameters. Binary weight quantization [6] is proposed for the efficiency of computation and storage and demonstrates that retaining only the sign of parameters during model updates can still reach acceptable accuracy. Then, the ternary weight quantization [11] sets unimportant parameter updates to zero on top of binary weight quantization and reaches comparable accuracy to full precision training. It shows that by setting a threshold, parameter updates that contribute minimally to the model performance can be effectively filtered out. Thus, we apply the idea of ternary weight quantization to the global model aggregation and then value the client's contribution with the remaining important parameters.\nWe demonstrate the workflow of CoAst in Figure 2. The contribution assessment is transparent to clients, and there is nothing to change during the local training procedure. On the server, when receiving the local parameters trained in round t, CoAst first prunes the parameters by their importance through the ternary weight quantization, then performs the aggregation on the pruned models. After aggregation, we use cross-round valuation to measure the contributions of local models. In the next part, we will detail these two key designs."}, {"title": "4.2 Parameter Pruning", "content": "Without loss of generality, we use the training process of round t as an example to detail the algorithm design. N clients first locally train the local models of round t based on the global parameters of round t - 1, denoted as \u0398t\u22121. Then, all clients upload local parameters of round t to the server, which are denoted as {\u03b8it}i\u2208{1,...,N}. In the following procedure, clients do nothing but wait for the aggregated global parameters of round t, denoted as \u0398t, to continue their local training in the next round.\nOnce all local parameters {\u03b8it}i\u2208{1,...,N} are received, CoAst calculates the local updates of round t, denoted as {\u2206it}i\u2208{1,...,N},\nwhere\n\u2206it = \u03b8it \u2212 \u0398t\u22121. (6)\nThen CoAst performs parameter pruning according to their importance, similar to the idea of model quantization such as binarization and ternarization. Considering the structural and functional differences between layers of DL models, CoAst quantifies the parameter importance in a layer-wise manner. Here we denote the parameter updates in each layer as \u2206i := [\u03b41, \u03b42, ..., \u03b4l] where l refers to the number of layers. Take the j-th layer as an example. CoAst first calculates the r-th percentile of |\u03b4j|, denoted as \u03b4j, where r is a hyperparameter controlling the pruning rate. Then, CoAst prunes the parameters by clipping the parameter updates. For each element of \u03b4j, denoted as u, it is clipped as:\n\u0169 = -1 if u > \u03b4,\n0 if u < -\u03b4,\n1 otherwise (7)\nIt means we regard the elements of \u03b4j whose absolute values are greater than \u03b4 as important and only keep their signs, while the remaining elements are pruned to 0. Given the clipped parameter updates \u2206it, the pruned local parameters can be calculated as\n\u03b8it = \u0398t\u22121 + \u03b1 \u00b7 \u2206it, (8)\nwhere \u03b1 is a hyperparameter for normalization. We report the whole parameter pruning procedure in Algorithm 1."}, {"title": "4.3 Cross-round Valuation", "content": "In each round, the contribution of the model is valued by the global model aggregated of the next k rounds, and it also values the local models of the last k rounds. Recall that the parameters of the global model are the average of the pruned local parameters (Equation 8), which are calculated by adding the sign of the parameter update. Therefore, the global parameters is calculated by\n\u0398t = 1/N \u2211i=1N \u03b8it = \u0398t\u22121 + \u03b1/N \u2211i=1N \u2206it (9)\nTherefore, in round t, the parameter update of the global model denoted as Ut := \u0398t \u2013 \u0398t\u22121, is proportional to the sum of the pruned local updates, i.e.,\nUt \u221d \u03b1 \u2211i=1N \u2206it (10)\nRecall that the value of element \u0169 \u2208 \u2206it belongs to {\u22121, 0, 1}. That is, the Ut (Equation 10) is the normalized voting result, which indicates, for each element b \u2208 \u0398t\u22121, how many clients believe that its value should be increased by, and how many clients believe that it should be decreased by.\nAfter k rounds following the t-th round, we obtain the parameters \u0398t+k, which can be calculated by\n\u0398t+k = \u0398t + \u2211e=t+1^{t+k} Ue (11)\nIn the following part, we denote \u2211e=t+1^{t+k} Ue as U(t,k) for simplification. Since the global parameters are obtained by averaging all local parameters, we can regard the parameters of any client as a correction to the sum of parameters of the other N \u2013 1 clients. Without loss of generality, for any client i, we reformulate Equation 11 to\n\u0398t+k = 1/(N \u2212 1)\u2211j \u2208 {1...,N}\\{i} \u03b8t+k + 1/N \u0398it (12)\nWe assume that the model's performance is improved after the k rounds. That is, among all clients, if a client's local model of round t, i.e., \u03b8it, during the parameter aggregation process is more similar to the subsequent global parameter updates, i.e., U(t,k), then the contribution of this client in this training round is greater.\nTherefore, the contribution of client i can be measured by the similarity between \u03b8it and U(t,k). Here, we use Signed Cosine similarity to measure the similarity because the \u03b8it and U(t,k) are local parameters and global updates, respectively. Note that Signed Cosine similarity is sensitive to the sign information of vectors and can better reflect the directional relationship between vectors. Due to the parameter pruning, the model updates can be considered as the sign of each parameter's update (Equation 10). That is, in our design, the sign of these updates is more important than their magnitude.\nNote that \u03b8it and U(t,k) share the same shape, and we assume that they can be indexed through h. CoAst calculates the client's contribution in round t by\n\u03c1it = \u2211h=1^{|\u0398i|} sgn(\u03b8it[h]) sgn(U(t,k)[h]), (13)\nwhere sgn() is the function to indicate the sign of the value."}, {"title": "5 Implementation", "content": "5.1 Dataset Settings\nWe evaluate the CoAst's performance on three datasets, i.e., CIFAR-10 [10], CIFAR-100 [10], and STL-10 [3]. We randomly partition the training dataset among each participant. We assume that by randomly and evenly partitioning these three datasets according to the number of clients, several datasets with the same data valuation can be obtained. Therefore, if a group of clients uses these partitioned datasets for training, the contribution of these clients is the same. We have set up four scenarios to mimic the contribution differences caused by data quality and quantity differences. N in the following settings denotes the number of clients.\n5.1.1 Setting 1: Different quantity. Assuming that randomly partitioned datasets share a similar distribution, the more samples in the training dataset, the higher the contribution to the model accuracy. In this scenario, we prepare datasets with different contributions by randomly assigning different numbers of samples to each client. Let the number of clients be N and the size of the training dataset be Xtrain. Then, the size of the dataset for the i-th client is Di = 1 \u2212 0.5/(N) Xtrain.\n5.1.2 Setting 2: Adding noise. In this scenario, we prepare the training datasets of different qualities for clients by adding Gaussian noise of different intensities. We first randomly and evenly partition the dataset according to the number of clients. Then we perform the Gaussian noise to the dataset of client i with a mean of \u00b5i and standard deviation of \u03c3i, where i \u2208 {1, ..., N}. We set the mean and variance of Gaussian noise decrease linearly, i.e., \u00b5i = 0.01\u00b7 i, \u03c3i = 0.625\u00b7 i. \n5.1.3 Setting 3: Adjusting resolution. In this scenario, we mimic the data of different quality by adjusting the resolution of the training data through Gaussian blur. Different degrees of Gaussian blur can be achieved by setting kernels of different sizes and variances. We first randomly and evenly partition the dataset according to the number of clients. Then, we use different degrees of Gaussian blur to preprocess the training data. Let the sequence of kernel sizes and standard deviation be si and \u03c3i, where i \u2208 {1, ..., N}. We select a linearly decreasing sequence of kernel sizes and standard deviation, i.e., si = 2 \u00b7i + 1, \u03c3i = 0.4 \u00b7 i + 1. \n5.1.4 Setting 4: Masking. In this scenario, we prepare the dataset with different quality by adding a mask on training data. The content covered by the mask is set to 0. We first randomly and evenly partition the dataset according to the number of clients. Then, for each client, we randomly mask a part of the image. The area of the mask covers ri% of the image for client i, and its position is randomly generated. The ri is a random number between li and ui, where li = 0.5 \u00b7 i/N, ui = 0.75 \u00b7 i/N."}, {"title": "5.2 Implementation Details", "content": "We perform experiments with Pytorch on a server with two A100 (80G) GPU cards. We use three model architectures, i.e., TinyResNet, ConvNet [26], and ResNet-4. The TinyResNet consists of a convolution layer, whose weight shape is 3 \u00d7 7 \u00d7 7 \u00d7 64, and a ResBlock [5], whose kernel size is 3 \u00d7 3 and output channel is 64. The ResNet-4 consists of 4 ResBlocks, whose kernel size is 3 \u00d7 3 and output channel is 64, 128, 128, and 128. In our setting, we use 1 central server and 5 clients, (i.e., N = 5). We initialize the learning rate to 0.01 and gradually decrease it as the training progresses. We use the FedAvg method to aggregate the local models."}, {"title": "5.3 Baseline Methods", "content": "We use three baseline methods. The validation-based method [17], denoted as baseline 1, measures the accuracy of the aggregated models w/ or w/o one local model with the validation dataset to calculate the contribution. Although baseline 1 is computationally time-consuming, it has the highest accuracy among Shapley value-based solutions by exhaustively considering all possible cases. We implement two SOTA validation-free methods, i.e., Fed-PCA [12] and CGSV [23], as the baseline 2 and baseline 3."}, {"title": "5.4 Metrics", "content": "To measure the accuracy of the contribution assessment, we use the Spearman correlation coefficient [14] as the distance measurement function d in Equation 3. The Spearman correlation coefficient is good at measuring the degree to which the ranks of the two variables are associated with each other. And it also is used in related works [19, 23]. Formally, R and \u0154 are two sequences of length n, which denote the ground-truth order and predicted order of the clients' contribution, i.e., R = [o1, o2, ..., on], \u0154 = [\u00f41, \u00f42, ..., \u00f4n]. We calculate the Spearman correlation coefficient (\u03c1) through the:\n\u03c1 = 1- 6/(n(n\u00b2 \u2212 1)) \u2211i=1^{n} ||oi - \u00f4i||^2. (14)\nThe \u03c1 ranges from -1 to 1, where -1 indicates a perfectly negative correlation, i.e., sequences R and \u0154 are in reverse order, 0 indicates no correlation, i.e., random guessing, and 1 indicates a perfectly positive correlation."}, {"title": "6 Evaluation", "content": "We measure the performance of our CoAst in four configurations with three datasets and three model architectures. Configuration 1 is CIFAR-10 and TinyResNet; Configuration 2 is CIFAR-100 and TinyResNet; Configuration 3 is STL-10 and ResNet-4; Configuration 4 is CIFAR-100 and ConvNet."}, {"title": "6.1 Overall Performance", "content": "In the CoAst's experiments, for hyperparameters in Algorithm 1, we set r to 10, \u03b1 to 0.02, and N to 5. We set the k in Equation 13 to 2. We report the overall performance in Table 1. SV, PCA, and CGSV represent baseline 1, baseline 2, and baseline 3, respectively. The average accuracy of baseline 1 and CoAst are 0.855 and 0.9, which means that our CoAst is comparable to that of baseline 1, which is a validation-based method. CoAst even outperforms baseline 1 by 0.22, 0.02, and 0.12 in setting 1, setting 2, and setting 4. Our CoAst's performance outperforms the SOTA validation-free methods, i.e., Fed-PCA (baseline 2) and CGSV (baseline 3), in almost all cases. On average, our CoAst outperforms Fed-PCA by 0.94 and CGSV by 0.52 in all cases. The poor performance of Fed-PCA is because the model architecture used in our experiment is too complex to perform precise probability analysis. Experimental results demonstrate the CoAst's effectiveness and robustness in different cases."}, {"title": "6.2 Ablation Study", "content": "6.2.1 k in cross-round valuation. We explore how k affects the performance of CoAst. We perform the experiment with different k values, which means that different numbers of global updates are used to assess the local models' contribution. We report the results in Table 2. In different experimental settings, CoAst with k = 5 reaches the best performance, and the performance of CoAst with k = 2 is comparable to that of CoAst with k = 5. The small and large k values, i.e., k = 1 and k = 10, have relatively poor performance. This is because a small k value may still let the contribution assessment be affected by stochasticity, while a large value of k may make CoAst lack attention to the stochastic gradient descent process. We recommend setting k to 2 or 5 when using our CoAst in practical use.\n6.2.2 Parameter pruning. Recall that the parameter pruning consists of a parameter update clipping procedure (Line 9 in Algorithm 1) and a top r% update selection procedure (Line 7 in Algorithm 1). To measure the effect of parameter pruning, we perform the following contrast experiments with different settings, which are as follows.\n(1) Ours. r = 10, w/ update clipping.\n(2) Exp1. r = 20, w/ update clipping.\n(3) Exp2. r = 10, w/o update clipping.\n(4) Exp3. r = 100, w/o update clipping. (No parameter pruning.)\nWe report the results of these contrast experiments in Table 3. By comparing the results of ours and Exp1, we can conclude that increasing the proportion of parameter selection leads to a slight decrease in performance, which is likely due to the noise introduced by the selected parameters. Our CoAst's average performance is 0.40625 and 0.25625 higher than Exp2 and Exp3, respectively, indicating that the design of parameter pruning effectively ensures the accuracy of contribution assessment.\n6.2.3 Update clipping strategy. In the CoAst, we use a hyperparameter \u03b1 (Equation 8) to normalize the local parameter update in each round. Here we explore how the hyperparameter \u03b1 value affects the contribution assessment effectiveness. We perform three experiments with CIFAR-10 and TinyResNet. We set M to 100, N to 5, and r to 10. In the first two experiments, we set the value of \u03b1 to 0.01 and 0.02, respectively. In the third experiment, we use an adaptive clipping strategy, where we set \u03b1 as the average value of the selected r% parameters (i.e., 10) of each layer. We report the results of these three experiments in Table 4. By comparing the experimental results of the first two experiments, it can be seen that the choice of hyperparameters has little impact on the assessment performance. However, in the third experiment, the assessment performance decreases. This is because clipping the parameter update to different values interferes with the assessment process. In our design, we aim to quantize all parameter updates and assess the contribution based on the direction of the local parameters and global parameter updates.\n6.2.4 Client number. Here we explore how the number of clients, i.e., N, affects the stability of CoAst. We experiment with CIFAR-100"}, {"title": "7 Discussion and Limitation", "content": "Performance of model pruning. Recall that in the procedure of parameter aggregation, CoAst prunes local parameters according to their importance. Although weight quantization methods are often used in practical FL scenarios to reduce network bandwidth, they tend to incur performance degradation of the model as well as slower convergence speed. Thus the convergence time of our CoAst is slightly longer than that of the typical method, i.e., without applying any quantization.\nWe report the model performance trained through the typical method and ours in Table 6. The model trained through the typical method converges after 100 rounds (i.e., M = 100), while the model trained in our method requires 200 rounds (i.e., M = 200) to converge. As can be seen, the performance of the model trained through CoAst outperforms that of the model trained through the typical method at the 100th round only in one case. When the model converges, the performance of the model trained through CoAst exceeds the model trained in the typical scenario in two cases. However, experiments in Section 6.2.2 show that weight quantization is important to the accuracy of the assessment since quantization mitigates the negative impact of redundant model parameters on the assessment. In future work, we will draw on research ideas in model quantization to improve the model performance in accuracy and convergence.\nApplication value of contribution assessment. Concerns regarding data quality assessment and contribution evaluation have become ubiquitous among nations, communities, and individuals alike. Some countries have timely developed advanced standards and sophisticated sophisticated indicators to guide norms for contribution and data quality assessment [1, 2]. The technologies presented in this work could provide pivotal support for the application of these standards within industrial contexts."}, {"title": "8 Conclusion", "content": "This work proposes a validation-free contribution assessment, CoAst, for FL scenarios. Compared with existing efforts, it greatly improves the contribution assessment performance under different dataset settings by introducing two key designs: parameter pruning and cross-round valuation. Comprehensive evaluations showed that our CoAst outperforms existing methods on different dataset settings and models. We believe that CoAst will inspire the community to improve the FL paradigm with an inherent contribution assessment."}]}