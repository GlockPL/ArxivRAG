{"title": "Distributive Fairness in Large Language Models: Evaluating Alignment with Human Values", "authors": ["Hadi Hosseini", "Samarth Khanna"], "abstract": "The growing interest in employing large language models (LLMs) for decision-making in social and economic contexts has raised questions about their potential to function as agents in these domains. A significant number of societal problems involve the distribution of resources, where fairness, along with economic efficiency, play a critical role in the desirability of outcomes. In this paper, we examine whether LLM responses adhere to fundamental fairness concepts such as equitability, envy-freeness, and Rawlsian maximin, and investigate their alignment with human preferences. We evaluate the performance of several LLMs, providing a comparative benchmark of their ability to reflect these measures. Our results demonstrate a lack of alignment between current LLM responses and human distributional preferences. Moreover, LLMs are unable to utilize money as a transferable resource to mitigate inequality. Nonetheless, we demonstrate a stark contrast when (some) LLMs are tasked with selecting from a predefined menu of options rather than generating one. In addition, we analyze the robustness of LLM responses to variations in semantic factors (e.g. intentions or personas) or non-semantic prompting changes (e.g. templates or orderings). Finally, we highlight potential strategies aimed at enhancing the alignment of LLM behavior with well-established fairness concepts.", "sections": [{"title": "1 Introduction", "content": "The growing interest in deploying Artificial Intelligence (AI) systems in social or economic contexts has sparked a wave of critical inquiry into their role as agents that interact with or simulate humans. This exploration has largely focused on studying pre-trained Large Language Models (LLMs) in representing collective human behavior [Bommasani et al., 2023, Zhi-Xuan et al., 2024], performing complex decision-making [Yang et al., 2023, Liu et al., 2024b], modeling human values [Horton, 2023], acting as research assistants [Korinek, 2023], and representing human subjects in social science [Argyle et al., 2023] or market research [Brand et al., 2023], among other applications. The reliance on LLM-powered systems highlights the critical need to understand the ethical values (e.g. fairness) these systems represent, as misaligned representations of humans or their societal values\u2014either due to mismatched beliefs or failure to adhere to instructions [Milli and Dragan, 2020, Liu et al., 2024a]\u2014may result in detrimental outcomes with an adverse effect on downstream applications.\nFairness is among the most essential societal principles for advancing ethical approaches in algorithmic decision-making. In particular, it serves as the fundamental driving force for achieving the socially acceptable allocation of resources, goods, or responsibilities within a society. The study of fairness has long been a focal point across diverse disciplines, inspiring systematic efforts"}, {"title": "1.1 Main Results", "content": "We conduct a series of studies for the allocation of indivisible resources with and without money.\nWe contrast the responses generated by the state-of-the-art large language models (GPT-40, Claude-3.5S,"}, {"title": "2 Related Work", "content": "Theories of Human Preferences and Distributive Fairness. Different allocation principles, such as inequality aversion (aka equitability) [Fehr and Schmidt, 1999, Bolton and Ockenfels, 2000], Rawl-"}, {"title": "3 Resource Allocation Problems", "content": "An instance of a resource allocation task is composed of a set of $n$ individuals, $N$, a set of $m$ indivisible goods, $M$, and possibly a fixed amount of a divisible resource, aka money, denoted by $P$. Each individual $i$ has a non-negative valuation function $v_i : 2^M \\rightarrow \\mathbb{R}_{\\geq 0}$. The function $v_i$ specifies a value $v_i(S)$ for a bundle of goods $S \\subseteq M$ and is assumed to be additive, that is, $v_i(S) = \\sum_{g \\in S} v_i(\\lbrace g \\rbrace )$, and $v_i(\\emptyset) = 0$. Thus, an instance can be presented with a valuation profile $v = (v_{i,g})_{i \\in N, g \\in M}$. An allocation $A = (A_1, ..., A_n)$ is a partition of indivisible goods $M$ into $n$ bundles, where $A_i$ denotes the bundle of goods allocated to individual $i$. Note that an allocation may not be complete, that is, $\\bigcup_{i \\in N} A_i \\subseteq M$. The division of money is represented through a vector $p \\in \\mathbb{R}^n$ such that $\\sum_{i=1}^n p_i \\leq P$, where $p_i$ is the money given to individual $i$. An outcome $(A, p)$ is a pair consisting of an allocation of goods and a division of money, where $(A_i, p_i)$ denotes individual $i$'s bundle-payment pair. When an instance does not include any money, we simply use $A$ or say an \u2018allocation' to denote an outcome.\nThe quasi-linear utility of individual $i$ for a bundle-payment pair $(A_i, p_i)$ is $u_i(A_i, p_i) = v_i(A_i) + p_i$. For simplicity, we sometimes abuse the notation and write $(u_1,..., u_n)$ to refer to the payoff vector of an outcome. We note that the exact valuation functions of individuals or their utility models are often unknown. A large body of work has focused on designing utility functions based on experimental findings (see, for example, [Fehr and Schmidt, 1999, Bolton and Ockenfels, 2000]), but there has been no consensus on the proposed utility models. The presented model (along with its assumptions) is used solely to evaluate the outcomes proposed by human subjects and LLMs.\nNext, we describe the relevant fairness and efficiency notions (see Appendix A for formal definitions).\nFairness Notions. An outcome is equitable if the subjective \u2018happiness level', or utility, of every individual, is the same [Dubins and Spanier, 1961]. Given an outcome $(A,p)$, $\\Delta(A, p)$ is the difference between the utilities of the best-off individual and the worst-off individual under $(A, p)$. An outcome $(A^*, p^*)$ is called equitable (EQ) if it minimizes the inequality disparity. Equitability is sometimes referred to as a 'perfectly equal' outcome when $\\Delta(A,p) = 0$ (denoted by EQ*). An outcome $(A, p)$ is envy-free if no individual prefers the bundle-payment pair of another. Formally, an outcome $(A,p)$ is envy-free if for every pair of individuals $i, j \\in N$, $u_i(A_i, p_i) \\geq u_i(A_j, p_j)$. Lastly, a Rawlsian maximin (RMM) solution aims at maximizing the utility of the worst-off individual [Rawls, 1971]. Herreiner and Puppe showed that minimizing inequality (aka \u2018inequality aversion') plays a fundamental role in humans' perception of fairness [Herreiner and Puppe, 2007, 2010]. Equitability is also a desirable property in practical applications such as divorce settlement [Brams and Taylor, 1996]. Several studies involving humans demonstrate that equitability is a significant predictor of perceived fairness, often more so than envy-freeness [Herreiner and Puppe, 2007, 2010].\nEconomic Efficiency. An outcome (A,p) is maximizing the utilitarian social welfare (USW) if it maximizes $\\sum_{i \\in N} u_i(A_i, p_i)$. An outcome is Pareto optimal (PO) if no individual's utility can be improved without making at least one other individual worse off. Clearly, every welfare-maximizing allocation is PO, but the converse may not hold. The following example illustrates the above desiderata on a simple instance with three goods and two individuals."}, {"title": "3.1 Dataset, Models, and Prompting", "content": "Dataset. We adopt instances from the dataset that was developed by Herreiner and Puppe [2007]. To maintain consistency with the original study, instances are denoted as I1 to I10, involving few individuals with preferences over several (\\{3, . . ., 6\\}) goods. The dataset contains distinct instances that were carefully designed to capture the trade-offs between various fairness or efficiency measures. For example, some instances test the trade-off between efficiency and fairness (I1 and I4) by discarding goods; some measure the trade-off between equitability and envy-freeness (I2, I6, and I9) or involve larger number of goods (I3 and I5); some involve the allocation of money alongside of goods (17, 18, and I10); and some examine the self-serving bias of the decision maker (19 and I10). The details of the instances (along with additional carefully designed instances), and the human responses are provided in Appendix H.1.\nModels. We consider several state-of-the-art LLMs, namely GPT-4 (Omni) [OpenAI, 2023], Claude-3.5 (Sonnet) [Anthropic, 2024], Llama3 (70b) [Touvron et al., 2023], and Gemini-1.5 (Pro) [Reid et al., 2024]. For each model, we choose versions that balance cost and running time with reasoning capabilities. Each model is used with the default temperature of 1.0 to enable a wider range of responses. See Appendix G for comparisons with other models. We adopt in-depth reporting strategies proposed by Burnell et al. [2023] and provide granular instance-by-instance evaluations along with aggregate metrics.\nGenerating Prompts. We adapt the instructions provided to human respondents as part of the study conducted by Herreiner and Puppe [2007]. Each prompt includes a description of the concerned instance followed by an instruction to \u2018determine' the fairest allocation. We implement an approach we call two-stage prompting strategy to eliminate sensitivity to templates. We refer the reader to Appendix I and Appendix F for details on prompt design and prompt sensitivity analysis. To generate a representative set of responses, each model was queried 100 times on each instance."}, {"title": "4 Distributional Preferences", "content": "Figure 2 illustrates the distribution of responses returned by LLMs and humans on various instances of the allocation problems consisting of indivisible goods without money (see Section 4.2 for instances involving money). Each plot illustrates the responses according to the top-5 notions selected by humans. The specific allocations along with additional details are provided in Appendix H.1.\nThere is a significant difference between human distributional preferences and those returned by all LLM models. Nonetheless, GPT-40 is more aligned with solutions proposed by humans in most instances, while Gemini-1.5P, Llama3-70b, and Claude-3.5S have rather inconsistent behavior. For instance, in I3 and I5 (instances involving a larger number of goods), they often return allocations that do not satisfy any clear fairness or efficiency properties, or those that humans rarely propose.\nEquitability. The primary distinction between humans' distributional preferences and LLM responses is their attitude toward equitability. Unlike humans, who tend to prefer allocations that minimize inequality [Herreiner and Puppe, 2007, Fehr and Schmidt, 1999, Bolton and Ockenfels, 2000, Herreiner and Puppe, 2010], LLMs rarely return an EQ allocation unless such an allocation also satisfies other properties (see Figure 2). For instance, all LLMs only return an EQ allocation when such an allocation coincides with an EF solution. Moreover, in instances (e.g. I2 and 16) where no allocation simultaneously satisfies both EF and EQ, LLMs frequently return EF allocations but rarely (if at all) return EQ allocations. In Section 4.1, we discuss a stronger notion of perfectly equitable solutions (i.e. inequality disparity of zero) and LLMs' tolerance to inequality.\nEnvy-freeness. Interestingly, similar to humans, all LLMs choose to discard a single good that is valued less by every individual to preserve envy-freeness, instead of allocating it to maximize welfare, as illustrated in instances I\u2081 and I4. A closer look shows that when LLMs find an EF allocation, it is often the case that EF is accompanied by another notion (EQ, RMM, PO). While GPT-40 consistently returns an EF allocation (among possibly many), Claude-3.5S chooses EF allocations in a majority of responses (51.1%) across all instances and it is the only model to return EF allocations more frequently than humans (43.8%). This behavior is due to the fact that Claude-3.5S tries to allocate to each individual a single item with the highest utility while, and if needed, discarding the rest of the goods (as in I\u2081 and I4).\nRawlsian Maximin. It is postulated that humans sometimes prioritize RMM solutions due to their egalitarian appeal, i.e., maximizing the worst-off individuals [Frohlich et al., 1987, Charness"}, {"title": "4.1 Are LLMs Tolerant to Inequality?", "content": "Table 2 shows that across all instances humans prefer allocations that satisfy (only) EQ*, whereas LLMs neglect EQ* allocations, and prioritize economic efficiency (See Appendix B.1 for an instance-by-instance analysis).\nA noticeable departure from human distributional preferences is LLMs\u2019 behavior towards inequality, especially when a perfectly equitable allocation (EQ*) does not coincide with other notions. This is best illustrated in instances where there is exactly one allocation satisfying EQ* (e.g. I6): EQ* is returned most frequently by humans (32.6% responses), while it is returned only once (out of 100 responses) by GPT-4o and never by other models.\nThis observation raises the question of how tolerant LLMs are to inequality disparity, i.e. the difference between the highest and the lowest payoff. Given that the inequality disparity (when it exists) is rather small in the original instances, we create new instances by modifying two of the original instances (namely I2 and I4) such that the inequality disparity is magnified.\nAll models continue to ignore the EQ* allocation even though the inequality disparity is significantly higher in all other allocations (see Appendix H.3 for details about the new instances created and LLMs\u2019 responses). In Section 5.1, we discuss the behavior of LLMs regarding inequality disparity when they are asked to select from a menu of options (in contrast to generating solutions)."}, {"title": "4.2 Utilizing Money to Mitigate Inequality", "content": "In settings that include money, as a transferrable resource, human respondents often tend to utilize it to offset inequality. In particular, money is often used by human respondents to address the \u2018inequality shortcomings\u2019 of envy-free or efficient (Pareto optimal) allocations in instances that involve the allocation of goods and money (I7, I8, and I10) [Herreiner and Puppe, 2007]. To illustrate this point, let us consider a simple instance (I7) that has unique solutions satisfying notions such as EQ* or EF (Table 3)."}, {"title": "4.3 How Do LLMs Utilize Money?", "content": "To better understand LLMs' behavior in utilizing money, we create a set of benchmark instances with goods and money (see Appendix H.4 for details). In each instance, there is a unique way to allocate goods and money such that EQ*, EF, and USW are all satisfied. Similarly, each instance (except I1.1) admits multiple additional ways in which money can be divided among the players to ensure EQ* and EF (and not USW).\nFigure 3 illustrates LLMs' behavior in utilizing money. All LLMs (except Gemini-1.5P) most frequently utilize money to maximize utilitarian social welfare (USW). Moreover, the EF allocations are chosen at the second option. This behavior could be attributed to the fact that there are simply more possibilities to achieve any EF or any USW solution (see Appendix H.4). Gemini more frequently achieves EF primarily by discarding some of the money, which results in economic inefficiency (and thus, not achieving USW).\nA large fraction of GPT-4o's responses correspond to the unique EQ*+EF+USW allocation, while this allocation is chosen rarely by other models. A similar observation holds about EQ*+EF allocations. When individuals have identical valuations (e.g. in I1.4), all LLMs (except GPT-40) split the money equally among them, which violates EF and EQ*."}, {"title": "4.4 Fairness and Economic Efficiency", "content": "Given the above observations, a high-level question arises about whether in general, and across all instances, LLMs prioritize efficiency over fairness. And whether their behavior is aligned with human responses.\nFigure 4 illustrates the distributional preferences of humans and LLMs across all instances. First, it shows that, unlike human respondents, LLMs primarily return efficient allocations (PO), even when payoffs are significantly unequal.\nSecond, LLMs frequently return EF allocations and only rarely return an EQ solution. Note that in these instances a large fraction of responses simultaneously satisfy EF and PO. On the other hand, EQ is incompatible with PO in every instance (except I7) and is often satisfied only by a unique allocation. This observation suggests that choosing EQ requires a more deliberate process with the primary objective of decreasing the inequality gap among the individuals (see Appendix B.3 for more details). In Section 6, we investigate the impact of assigning specific fairness objectives or personas on LLM responses."}, {"title": "5 Alignment with Human Preferences", "content": "Thus far, we have illustrated that the solutions \u2018generated' by various state-of-the-art language models are inconsistent with respect to the given fairness notions and are often misaligned with human preferences. In this section, we further investigate the sources of misalignment between LLMs and human values, and propose a few strategies that can help better align LLM responses with human preferences."}, {"title": "5.1 Selection from a Menu of Options", "content": "In Section 4, we observed that the solutions \u2018generated' by the language models are not consistent with any of the fairness notions, and are often not aligned with human preferences. But how do LLMs perform when they are tasked with selecting a solution from a menu of predefined options?\nTo answer this question, we consider five different instances with specific characteristics with respect to the number of individuals/goods as well as the potential allocations, how various fairness notions overlap with one another, and the efficiency requirements.\nMenu Based on Human Responses. In every instance, the model is given five (or four in smaller instances) allocation options and is asked to select one. These options are derived from the top"}, {"title": "Menu with High Inequality Disparity", "content": "Given that GPT-40 and Claude-3.5S overwhelmingly select EQ* allocations, one may wonder whether this behavior is intentional. As discussed in Section 4.1, LLMs seem to be primarily tolerant to inequality. Yet, the five options derived from human preferences seem to all have small inequality dispersion. This raises the question of whether these models remain tolerant of inequality even under large inequalities. To put this question to test, we prompt the models with a new menu consisting of carefully designed allocations with amplified inequalities (see Appendix C.2 for the exact options given).\nTable 9 (in Appendix C) shows the distribution of responses returned by each of the LLMs when the task is to select from a menu of allocations with different levels of inequality disparity. Here, GPT-40 and Claude-3.5S choose options that minimize the inequality in most responses, while Gemini-1.5P and Llama3-70b frequently select allocations with a larger inequality among the individuals."}, {"title": "Augmenting Prompts with Context", "content": "In the previous experiments, the models were not given any information about whether the options are derived from human preferences or are randomly generated. We tested the impact of including additional information to the model about i) the share of human responses corresponding to a given allocation, and ii) explanations about fairness notions being satisfied. Note that the explanations are provided in a manner resembling Chain-of-Thought (CoT) reasoning [Wei et al., 2022].\nOur experiments show that informing LLMs about human responses significantly changes the top solution (most frequent) selected by each model. However, providing additional step-by-step explanations about the fairness of human preferences seems to inconsistently impact the outcome (see Appendix C for a detailed discussion)."}, {"title": "5.2 Chain-of-Thought Prompting", "content": "Chain-of-Thought prompting (CoT) [Wei et al., 2022] is widely used to enhance the mathematical reasoning capabilities of LLMs [Chu et al., 2024, Qiao et al., 2023]. Given that there is no correct answer, or set of steps, in the task of resource allocation, we develop a variation of the CoT method to evaluate whether it improves the alignment of LLMs' choices with those of humans. We provide LLMs with a CoT prompt where we list the possible fair or efficient allocations in an example instance (I\u0151 for instances with money and Io for those without), and then ask them to choose the allocation they think is fairest in instances such as I2, I6, and I7 (see Appendix I.6 for a sample prompt). The effect of CoT prompting on LLMs' responses is summarized in Table 10 (Appendix D).\nThe main observation is that GPT-40 and Claude-3.5S more frequently return allocations that satisfy EQ* and RMM with CoT prompting as compared to the default method. However, this behavior is not always consistent: CoT prompting i) improve GPT-40 and Claude-3.5S's responses in some instances (in particular, I2 for both and 17 only for GPT-40), ii) when an EQ* allocation does not coincide with RMM (as is the case in I6) there is no significant improvement (or changes) in the returned responses."}, {"title": "6 Intentions, Personas, and Cognitive Bias", "content": "In Section 4.4, we observed that LLMs prioritize efficiency over fairness, when asked to provide fair solutions. In fact, in Appendix E.1 we show that LLMs are stubborn welfare-maximizing agents under various given intentions. These observations raise the question of whether assigning personas will influence LLMs' behavior toward fairness."}, {"title": "6.1 Personas", "content": "In the context of language models, personas are used to guide LLMs to pursue certain goals or take certain positions. There is evidence in the literature of language models suggesting that endowing the AI with various social preferences affects play [Horton, 2023]. For instance, instructing the LLM to care about equity causes it to choose equitable outcomes. Moreover, predefined 'personas' tend to skew LLM responses towards pre-determined behaviors, such as altruism or selfishness [Fontana et al., 2024].\nWe select a series of instances (from the original dataset) and augment the prompts with personas reflecting that LLM \u2018cares' about a specific fairness notion. The main result is that assigning"}, {"title": "6.2 Cognitive Bias", "content": "In scenarios involving multiple stakeholders, the decision-maker may hold some cognitive bias during the decision-making process. In particular, if the decision-maker has any stake in the solution, her decision may be impacted by an unintentional cognitive bias called self-serving bias [Miller and Ross, 1975]. In resource allocation scenarios, the fairness of the outcome may be affected by this bias when the decision-maker has \u2018skin in the game' [Hosseini, 2024]. The original experiment of Herreiner and Puppe [2007] shows that there is no significant difference when the human respondents are one of the participant players (see Figure 7).\nGiven that LLMs often possess human-like biases\u2014reflecting existing ethical and moral norms of society [Schramowski et al., 2022]\u2014a question arises about whether LLM responses remain unaffected when the model acts as a participating individual or whether LLMs are affected by self-serving bias. Figure 7 shows the responses of humans and LLMs in two instances one where the decision maker is not one of the beneficiaries (16) and another wherein the model is one of the participants (19)."}, {"title": "7 Limitations and Discussion", "content": "In this section, we discuss some limitations of our findings and outline potential directions for future research.\nThe lack of alignment seems to stem from a variety of shortcomings in generating responses. The explanations provided by LLMs reveal that they use greedy procedures that involve distributing goods one by one to individuals who value them highly. These greedy procedures often result in solutions that are envy-free (in some instances) or maximize utilitarian welfare, but rarely in solutions that satisfy equitability.\nThe responses provided by LLMs often contain logical errors or mismatches between the intended objective and their explanations of the objective. For example, LLMs often incorrectly compare values or misuse the definition of different fairness notions (see Appendix J for an example of an erroneous explanation). In addition, LLMs' responses are sensitive to non-semantic prompting factors, for example, changing the prompt templates or the order in which goods or individuals are presented (see Appendix F for details).\nOur experiments (see Section 5.2) showed that one-shot CoT prompting does not uniformly improve LLMs' alignment with humans. One possible direction is to investigate more advanced prompt engineering techniques that enable reasoning such as self-consistency [Wang et al., 2022], few-shot in-context learning [Dong et al., 2022], Tree-of-Thought prompting [Yao et al., 2024], or using mechanistic interpretability methods (e.g., studying attention weights for relevant tokens) [Belrose et al., 2023, Halawi et al., 2024]. Another intriguing direction is exploring methods for fine-tuning LLMs through human feedback, pre-training models with domain-specific synthetic samples, or augmenting a fairness inference module. These approaches pose significant challenges in economic domains due to difficulties in devising appropriate loss functions [D\u00fctting et al., 2024, Ravindranath et al., 2021] and out of distribution error in learning from a limited sample size or synthetic data [Setlur et al., 2024, Zhang et al., 2024a].\nStudies on human preferences are limited to controlled laboratory environments, resulting in limited sample size, potential overfitting, and challenges due to high dimensionality with few samples. Moreover, experimental studies are often affected by i) context-dependent human perception; for instance, fundamental differences between goods (positive utility) or chores (negative utility), or strategic vs. non-strategic settings. ii) cognitive biases; e.g. self-serving bias (as discussed in Section 6.2), priming bias, and many more, iii) diverse backgrounds across individuals and societies; for instance, education, gender, or wealth [Casari et al., 2007, Murphy-Berman et al., 1984]. These limitations call for the collection and analysis of meta-data and validation of human preferences through real-world experimentation [Levitt and List, 2007]."}]}