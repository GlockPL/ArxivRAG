{"title": "Enhanced Contrastive Learning with Multi-view Longitudinal Data for Chest X-ray Report Generation", "authors": ["Kang Liu", "Zhuoqi Ma", "Xiaolu Kang", "Yunan Li", "Kun Xie", "Zhicheng Jiao", "Qiguang Miao"], "abstract": "Automated radiology report generation offers an effective solution to alleviate radiologists' workload. However, most existing methods focus primarily on single or fixed-view images to model current disease conditions, which limits diagnostic accuracy and overlooks disease progression. Although some approaches utilize longitudinal data to track disease progression, they still rely on single images to analyze current visits. To address these issues, we propose enhanced contrastive learning with Multi-view Longitudinal data to facilitate chest X-ray Report Generation, named MLRG. Specifically, we introduce a multi-view longitudinal contrastive learning method that integrates spatial information from current multi-view images and temporal information from longitudinal data. This method also utilizes the inherent spatiotemporal information of radiology reports to supervise the pre-training of visual and textual representations. Subsequently, we present a tokenized absence encoding technique to flexibly handle missing patient-specific prior knowledge, allowing the model to produce more accurate radiology reports based on available prior knowledge. Extensive experiments on MIMIC-CXR, MIMIC-ABN, and Two-view CXR datasets demonstrate that our MLRG outperforms recent state-of-the-art methods, achieving a 2.3% BLEU-4 improvement on MIMIC-CXR, a 5.5% F1 score improvement on MIMIC-ABN, and a 2.7% F1 RadGraph improvement on Two-view CXR.", "sections": [{"title": "1. Introduction", "content": "Chest X-ray (CXR) is a widely employed diagnostic tool in clinical practice, primarily for evaluating the lungs, heart, pleura, and skeletal structures. It is critical for diagnosing conditions, such as pneumonia, fracture, pneumothorax, pleural effusion, and cardiomegaly [19]. To ensure effective communication across departments and between physicians and patients, radiologists manually document detailed reports based on their interpretation of CXR images. However, this process is both expertise-dependent and time-consuming [29, 51]. As the demand for imaging studies continues to grow, the workload associated with manual report generation may intensify, potentially impacting medical efficiency and compromising patient care quality [3, 50]. To mitigate these challenges, radiology report generation (RRG) [14, 44] has emerged as a promising solution. By automatically analyzing imaging data from X-ray [32], CT [16], or pathology [15], RRG generates clinical findings using factual terminology [30, 57] and descriptive language. This automation aids radiologists by providing high-quality draft reports [29], improving diagnostic efficiency.\nIn clinical practice, radiologists typically conduct comprehensive evaluations using multi-view images from the current visit, incorporate patient medical histories (i.e., longitudinal data) to track disease progression, and integrate patient-specific prior knowledge to assist in diagnosis and report generation. However, most existing RRG methods [9, 34, 36] focus solely on single images when generating reports and struggle to effectively differentiate between views, such as posteroanterior (PA), anteroposterior (AP), lateral, or left anterior oblique. These views exhibit inherent differences; for example, although both PA and AP views are frontal images, geometric variations can cause cardiac enlargement in the AP view, potentially impacting diagnostic accuracy. To address this issue, some studies [10, 60] have introduced dual-view report generation, as illustrated in Figure 1. Empirical results [9, 10] reveal that incorporating dual views enhances the quality of generated reports. Nevertheless, these methods merely distinguish between frontal and lateral views, neglecting more subtle differences across multiple views. Moreover, both single-image and dual-view methods focus solely on the images from the current visit, disregarding the descriptions of disease progression found in radiology reports. This limitation may lead to model hallucinations. To combat this problem, some studies [23, 50, 66] have sought to leverage longitudinal data to model disease progression, as shown in Figure 1. However, these methods still rely on a single image to characterize the current visit, limiting diagnostic accuracy. Additionally, some patients may lack \u201cINDICATION\u201d, \u201cprevious report", "previous image\u201d due to their first visit or improper data storage. This variability challenges the flexible use of available data to generate accurate reports.\nTo mimic radiologists' diagnostic pipeline and address these challenges, we propose a two-stage MLRG for chest X-ray report generation. In Stage 1, the key part is our proposed multi-view longitudinal contrastive learning approach, which utilizes the inherent spatiotemporal information in radiology reports to supervise the pre-training of visual and textual representations. Specifically, we incorporate learnable position embeddings for each view to identify differences across varying numbers of views. We then employ a multi-view longitudinal fusion network that flexibly integrates spatial information from current multi-view images and temporal information from longitudinal data. Subsequently, we learn visual and textual representations by leveraging agreements between multi-view longitudinal data (see Figure 1) and their corresponding radiology reports. In Stage 2, we introduce a tokenized absence encoding technique to handle missing patient-specific prior knowledge (i.e., \u201cINDICATION\u201d and \u201cprevious report": ".", "follows": "n\u2022 We propose a novel multi-view longitudinal contrastive learning method that flexibly integrates multi-view longitudinal data and leverages the inherent spatiotemporal information from reports to supervise the pre-training of visual and textual representations.\n\u2022 We introduce a tokenized absence encoding technique to handle missing patient-specific prior knowledge. This technique enables the model to adapt flexibly to scenarios with or without such data, ensuring the text generator can utilize available prior knowledge effectively.\n\u2022 Our MLRG shows competitive results compared to various state-of-the-art methods across three public datasets: MIMIC-CXR, MIMIC-ABN, and Two-view CXR."}, {"title": "2. Related Work", "content": "Radiology report generation (RRG). RRG is akin to image captioning [25, 62], but requires generating detailed content with specialized medical terminology. Existing RRG methods consist of a vision encoder (like ResNet101 [10, 18, 60], CvT [36, 37], or ViT [33, 54]) and a text generator (such as Memory-driven Transformer [9, 30], MiniGPT-4 [28], DistilGPT2 [32, 36], or LLaMA [24, 33, 53]). To improve clinical accuracy in RRG, researchers have incorporated various techniques or prior knowledge, including knowledge graphs [59, 65], cross-modal alignment [7, 30], region-guided frameworks [46, 63], warm starting [36], patient-specific \"INDICATION\u201d [29, 31], disease labels [60], and disease progression [17, 50]. However, these methods rely on single-image or fixed-view data, missing the comprehensive insights supplied by multi-view longitudinal data. To address this issue, we propose the MLRG method, which flexibly captures spatiotemporal features from multi-view longitudinal data and generates radiology reports based on patient-specific prior knowledge.\nMedical vision-language models. These models aim to learn generalized medical visual representations by maximizing agreements between image-report pairs. MGCA [49] presents multi-granularity cross-modal alignment, harnessing agreements at the instance, pathological region, and disease levels. KAD [64] enhances visual representation using established knowledge graphs. MedCLIP [55] expands training sets by decoupling images and reports, reducing false negatives through semantic matching loss. BioViL-T [2] captures disease progression by analyzing longitudinal data. Despite notable improvements in tasks like medical image classification and image-text retrieval, the utilization of multi-view longitudinal data remains limited, constraining diagnostic accuracy. Therefore, we present a multi-view longitudinal contrastive learning approach that utilizes the"}, {"title": "3. Method", "content": "Figure 2 presents an overview of our proposed MLRG. In Stage 1, we introduce a multi-view longitudinal contrastive learning approach that leverages inherent spatiotemporal information from radiology reports to supervise the pre-training of visual and textual representations.\nEnhancing medical image analysis via multi-view data. Multi-view learning [56] empowers models to derive shared and complementary insights from multiple views of the same subject. FMVP [34] treats single-view images and auxiliary inputs (i.e., disease labels and medical concepts) as multi-view data to assist the text generator in producing radiology reports. However, its reliance on additional annotated disease labels limits broader applicability. CXRMate [37] synthesizes previous reports based on previous images and integrates them with current multi-view images to produce final reports. However, this method overlooks subtle differences across views and can introduce additional noise from previous reports. In response, we propose the MLRG approach, which captures inter-view differences and leverages spatiotemporal information in reports to guide the pre-training, all without relying on additional manual labels.\n3.1. Problem Formation\nLet  $\\mathcal{D}_{tr} = \\{(x_{pri}, y_{pri}, z_i, X_{cur}, y_{cur})\\}_{i=1}^{n}$ be the training set, where n denotes the total number of visits. Each visit consists of a frontal previous image $x_{pri}$ (which may be absent), a previous report $y_{pri}$ (which may be absent), an \"INDICATION\u201d $z_i$ (which may be absent), $m_i$ current images (views) $X_{cur}$, and a reference report $y_{cur}$. Notably, The number of current multi-view images $m_i$ may vary across visits. Our goal is to learn the function $F_{\\Theta}(\\cdot)$ that maps $(x_{pri}, y_{pri}, z_i, X_{cur})$ to $y_{cur}$ on the training set  $\\mathcal{D}_{tr}$, such that $F_{\\Theta}(x_{pri}, y_{pri}, z_i, X_{cur}) \\rightarrow y_{cur}$. We then utilize the learned function $F_{\\Theta}(\\cdot)$ to generate a radiology report based on current multi-view images, the previous image, and patient-specific prior knowledge (i.e., $y_{port}$ and $z_i$).\n3.2. Multi-view Longitudinal Contrastive Learning\nVisual features extraction. We employ RAD-DINO [40], a vision transformer model [13] trained solely on chest X-"}, {"title": "rays using DINOv2 [38], as the vision encoder. The feature maps from the last hidden state are treated as visual features $V \\in \\mathbb{R}^{B \\times M \\times p \\times d_1}$, where $M = \\sum_{i=1}^{n} m_i$ denotes the total number of images in the mini-batch. Here, B, p, and $d_1$ represent the batch size, the number of patches, and the feature dimension, respectively.\nTextual features extraction. Inspired by FSE [30], we first adopt the structural entities approach [30] to extract factual serialization, which consists exclusively of clinical descriptions from radiology reports, as shown in Figure 2. This method enables the model to concentrate on alignment between images and factual serialization. We then consider CXR-BERT [4], a language model tailored for chest X-rays, as the text encoder. This is followed by a simple projection head that generates textual features $R \\in \\mathbb{R}^{B \\times t \\times d}$, where t denote the number of tokens, and d is the hidden size.\nMulti-positive contrastive learning between current multi-view images to improve the consistency of visual features. In clinical practice, radiologists often select some representative images as primary references, with other images as auxiliary support. Therefore, we treat each image from current multi-view images  $X_{cur}$ as an anchor scan $x_{i, a}^{cur}$ while considering the remaining images as auxiliary references $x_{cur} = \\{x_{j}^{cur} | j \\ne a, x_{j}^{cur} \\in X_{cur} \\}$, where $a \\in [1, m_i]$. To identify differences among views, we incorporate learnable view positional embeddings $E_v \\in \\mathbb{R}^{M \\times 1 \\times d_1}$ into visual features. This is followed by a simple projection head $P_v(\\cdot)$ that maps the features to a specific dimension d. These processes are formulated as follows:", "content": "$V = P_v(V + E_v) \\in \\mathbb{R}^{M \\times p \\times d}$.\nTo enhance the consistency of visual features, we employ multi-positive contrastive learning [47] to maximize the similarity between images from the same visit while minimizing the similarity to images from different visits. Specifically, we first exclude visits with only one image, as they do not provide positive pairs (Notably, these visits are still used for subsequent cross-modal alignment). Following this, we calculate the predicted categorical distribution $q \\in \\mathbb{R}^{K \\times (K-1)}$ to estimate the similarity between images:\n$q_i = \\frac{exp\\left(v_i \\cdot v_{\\backslash i} / \\tau_1 \\right)}{\\sum_{j=1,j\\ne i}^{M} exp\\left(v_i \\cdot v_{j} / \\tau_1 \\right)} ,s.t. m_i \\ne 1,$"}, {"content": "where $K = \\sum_{i=1, m_i\\ne 1}^{M} m_i$ represents the total number of multi-view images in the mini-batch, and $\\tau_1 \\in \\mathbb{R}^+$ is a temperature parameter. $v_i \\in \\mathbb{R}^{1\\times d}$ refers to the global visual feature of the ith image, while $v_{\\backslash i} \\in \\mathbb{R}^{(K-1)\\times d}$ denotes global visual features of all multi-view images except the ith image. Next, we compute the ground-truth categorical distribution $p \\in \\mathbb{R}^{K \\times (K-1)}$ by assigning the same labels to"}, {"content": "images from the same visit, formulated as:\n$p_i = \\frac{I_{match}(V_i, V_{\\backslash i})}{\\sum_{j=1,m_i\\ne 1}^{M} I_{match}(v_i, v_{j})}$,\nwhere $I_{match}(\\cdot, \\cdot)$ is an indicator function that determines whether two visual features originate from the same visit. Although the number of current views $m_i$ may vary across visits, the different number of non-zero elements in $p_i$ account for this variability. Finally, the multi-positive contrastive (MPC) loss is calculated using the cross-entropy between q and p, represented as:\n$L_{MPC} = - \\frac{1}{K} \\sum_{i=1,m_i\\ne 1}^{M} p_i \\log q_i$\nMulti-view longitudinal fusion network. Due to the varying number of current multi-view images and the absence of previous images for some patients, integrating this information flexibly presents certain challenges. To address this issue, we design the multi-view longitudinal fusion (MLF) network, as illustrated in Figure 3(A). We select the most recent previous visit to model temporal information, as it typically holds the highest reference value. To distinguish different time points, we integrate temporal positional embeddings into visual features, as depicted in Figure 2. Subsequently, the spatiotemporal features $V_{st} = \\{v_{1}^{st}, v_{2}^{st}, ..., v_{n}^{st}\\}$ are extracted using the MLF network:\n$v_{i}^{st} = MLF([v_{a_v}^{cur}, v_{a_v}^{cur}, v_{pri}]) \\in \\mathbb{R}^{p \\times d}$,\nwhere the anchor scan $v_{a_v}^{cur}$ functions as the query, and the concatenation of auxiliary references and previous image,"}, {"title": "$\\begin{bmatrix}v_{j}^{cur}, v_{pri}\\end{bmatrix}$, serves as the key and value. Although the number of images for the current visit varies and some patients may lack previous images, we process one sample at a time, allowing for flexible adaptation to these changes.\nInstance-wise cross-modal alignment. Radiology reports not only describe the current visit's condition but may also include comparisons with the patient's medical history. Therefore, relying solely on the current multi-view images and corresponding reports for cross-modal alignment could lead to model hallucinations. To address this, We first extract spatiotemporal features from multi-view longitudinal data using the multi-view longitudinal fusion network (see Figure 3(A)). Subsequently, we utilize the inherent spatiotemporal information from radiology reports to supervise the pre-training of visual and textual representations. Inspired by CLIP [41] and MGCA [49], we employ instance-wise cross-modal alignment to learn uni-modal representations. Specifically, we compute the image-to-text predicted categorical distribution $q_{v2r} \\in \\mathbb{R}^{B \\times B}$, defined as:", "content": "$q_{v2r}^{ij} = \\frac{exp\\left(V_i R_j / \\tau_2\\right)}{\\sum_{i=1}^{B} exp\\left(V_i R_j / \\tau_2\\right)},$"}, {"content": "where $V$ and $R$ denote global features of spatiotemporal features $V_{st}$ and textual features $R$, respectively. Similarly, we can also obtain the symmetric text-to-image predicted categorical distribution $q_{r2v}$. Given that radiology report content may be consistent across visits, we consider both spatiotemporal and textual features from the same visit as positive pairs, as well as those from different visits with identical reports. Consequently, the image-to-text ground-truth categorical distribution $p_{v2r} \\in \\mathbb{R}^{B \\times B}$ is defined as:\n$p_{ij}^{v2r} = \\frac{I_{identical}\\left(y_{i}^{cur}, y_{j}^{cur}\\right)}{\\sum_{k=1}^{B} I_{identical}\\left(y_{i}^{cur}, y_{k}^{cur}\\right)},$"}, {"content": "where $I_{identical}(\\cdot, \\cdot)$ denotes an indicator function that determines whether two radiology reports are identical. Finally, the cross-modal alignment loss is defined as:\n$\\mathcal{L}_G = -\\frac{1}{B} \\sum_{i=1}^{B} \\Big[p_{i}^{v2r} \\log q_{i}^{v2r} + p_{i}^{r2v} \\log q_{i}^{r2v}\\Big].$"}, {"content": "Overall objective in Stage 1. We train our MLRG by jointly optimizing $L_{MPC}$ and $L_{G}$, formulated as:\n$\\mathcal{L}_{pretrain} = L_{MPC} + L_{G}.$\n3.3. Chest X-ray Report Generation\nIntegrating patient-specific prior knowledge into the text generator. Radiologists commonly refer to patient-specific prior knowledge, such as the \u201cINDICATION\u201d (which outlines the visit reasons or symptoms) and the \u201cprevious report", "INDICATION": "nd \u201cprevious report", "[NHI]": "nd \u201c[NHPR]\u201d, to simulate their presence. For existing \u201cINDICATION\u201d, we apply a preprocessing strategy from SEI [29] to remove de-identification noise (e.g., -year-old, and \\). For available", "report": "we employ the structural entities approach [30] to extract factual serialization, enabling the model to focus on clinically relevant details. We then combine the cleaned"}, {"INDICATION": "ith factual serialization extracted from the \u201cprevious report"}, {"title": "4. Experiments", "content": "4.1. Experimental Settings\nDatasets. 1) MIMIC-CXR [21] is a large-scale, publicly available dataset comprising paired chest X-rays and radiology reports. Each pair contains a varying number of images compared to others, and all pairs for a patient are organized chronologically, facilitating the construction of multi-view longitudinal data. 2) MIMIC-ABN [35] is a subset of MIMIC-CXR, focusing solely on radiology reports that describe abnormal clinical findings. 3) Two-view CXR [31] aggregates visits with two current images from both MIMIC-CXR and IU X-ray [12]. Notably, as the IU X-ray does not include previous visits, both previous images and reports are unavailable. We adhere to official splits for these datasets and summarize the sample counts for the training, validation, and test set in Table 1. In line with [6, 9, 26, 29, 46], we treat the \u201cFINDINGS\" section in radiology reports as the reference reports.\nEvaluation metrics. Following prior works [5, 9, 50, 52, 54], we evaluate the effectiveness of our MLRG using both natural language generation (NLG) and clinical efficacy (CE) metrics. NLG metrics, which assess the linguistic similarities between generated and reference reports, include BLEU-n (B-n), METEOR (MTR), and ROUGE-L (R-L). For CE metrics, we utilize CheXpert [19] to label the generated reports with 14 observations (see Appendix Table A5) and compute the micro-average Precision (P), Recall (R), and F1 score (F1) based on ground truths. CE metrics also include F1 RadGraph (RG) [20], which evaluates the overlap of clinical entities and their relations, aligning more closely with radiologists than B-3 and F1 metrics [61]. All metrics are computed using official libraries [8, 20, 45], with higher values indicating better performance.\nImplementation details. Both $T_1$ and $T_2$ are set to 0.5. The number of blocks $L_1$ and $L_2$ in Figure 3 are set to 3 and 1, respectively. Each dataset is configured to generate a maximum of 100 tokens. We identify the best model as the\""}, {"title": "4.2. Main Results", "content": "We compare our MLRG with 14 state-of-the-art methods: R2Gen [9], CMN [10], SA [57], MET [54], KiUT [18], COFE [27], MAN [43], B-LLM [28], DCG [32], Med-LLM [33], SEI [29], FVMP [34], HERGen [50], and CXRMate [37]. Results are presented in Table 2, where \u201cSI\u201d, \u201cInd\u201d, \u201cMVD\u201d, \u201cLong\u201d, \u201cMVL\u201d, and \u201cDV\u201d represent different input types: single images, \u201cINDICATION\u201d, multi-view data, longitudinal data, multi-view longitudinal data, and dual views, respectively. We observe that our MLRG achieves SOTA performance across most metrics, with particular strength in B-4, RG, and F1. This suggests that MLRG excels in generating both coherent and accurate radiology reports. Although MLRG shows slightly lower Recall than B-LLM [28], its F1 and other metrics are significantly better. In Appendix Table A7, we also show our MLRG's ability to generate \"FINDINGS\u201d and \u201cIMPRESSION\" sections.\""}, {"title": "4.3. Ablation Study", "content": "Table 3 presents an ablation study on the MIMIC-CXR [21] dataset, analyzing the effect of different components on model performance.\nEffect of multi-view longitudinal contrastive learning (Stage 1). In Table 3, (a) represents a report generation scheme based solely on patient-specific prior knowledge, excluding Stage 1. Results reveal that our MLRG significantly exceeds (a), highlighting the critical role of multi-view longitudinal contrastive learning in enhancing the accuracy and coherence of generated reports. Additionally, we observe that both  $\\mathcal{L}_G$ ((c) vs. (a)) and  $\\mathcal{L}_{MPC}$ ((d) vs. (c)) have a positive impact on model performance.\nEffect of patient-specific prior knowledge (Stage 2). As shown in Table 3, MLRG significantly outperforms (b), which lacks patient-specific prior knowledge, emphasizing the importance of incorporating such knowledge to improve the coherence and clinical accuracy of generated reports. Moreover, the independent integration of \u201cINDICATION\u201d ((e) vs. (b)) and \"previous report\u201d ((f) vs. (b)) contributes positively to model performance.\nEffect of current multi-view images. Table 3 demonstrates that generating reports using current multi-view images outperforms those derived from single images (MLRG vs. (h)), highlighting the effectiveness of multi-view images in modeling the current disease conditions.\nEffect of previous images. As shown in Table 3, MLRG shows a clear advantage over (d), indicating that MLF network (see Figure 3) effectively integrates previous images. This capability allows the model to track disease progression, thereby generating more clinically accurate reports."}, {"title": "4.4. Case Study", "content": "Model benefits from current multi-view images, multi-view longitudinal data, and \"INDICATION\u201d. Table"}, {"title": "5. Conclusion", "content": "In this paper, we introduced the MLRG method for chest X-ray report generation. We first proposed a multi-view longitudinal contrastive learning approach that leveraged the inherent spatiotemporal information from radiology reports to guide the pre-training of visual and textual representations. This approach not only captured differences among views but also flexibly extracted spatial features from current multi-view images and temporal features from longitudinal data, effectively leveraging spatiotemporal information in reports for pre-training. Subsequently, we presented a tokenized absence encoding technique to handle missing"}]}