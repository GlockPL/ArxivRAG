{"title": "Enhanced Contrastive Learning with Multi-view Longitudinal Data for Chest X-ray Report Generation", "authors": ["Kang Liu", "Zhuoqi Ma", "Xiaolu Kang", "Yunan Li", "Kun Xie", "Zhicheng Jiao", "Qiguang Miao"], "abstract": "Automated radiology report generation offers an effective solution to alleviate radiologists' workload. However, most existing methods focus primarily on single or fixed-view images to model current disease conditions, which limits diagnostic accuracy and overlooks disease progression. Although some approaches utilize longitudinal data to track disease progression, they still rely on single images to analyze current visits. To address these issues, we propose enhanced contrastive learning with Multi-view Longitudinal data to facilitate chest X-ray Report Generation, named MLRG. Specifically, we introduce a multi-view longitudinal contrastive learning method that integrates spatial information from current multi-view images and temporal information from longitudinal data. This method also utilizes the inherent spatiotemporal information of radiology reports to supervise the pre-training of visual and textual representations. Subsequently, we present a tokenized absence encoding technique to flexibly handle missing patient-specific prior knowledge, allowing the model to produce more accurate radiology reports based on available prior knowledge. Extensive experiments on MIMIC-CXR, MIMIC-ABN, and Two-view CXR datasets demonstrate that our MLRG outperforms recent state-of-the-art methods, achieving a 2.3% BLEU-4 improvement on MIMIC-CXR, a 5.5% F1 score improvement on MIMIC-ABN, and a 2.7% F1 RadGraph improvement on Two-view CXR.", "sections": [{"title": "1. Introduction", "content": "Chest X-ray (CXR) is a widely employed diagnostic tool in clinical practice, primarily for evaluating the lungs, heart, pleura, and skeletal structures. It is critical for diagnosing conditions, such as pneumonia, fracture, pneumothorax, pleural effusion, and cardiomegaly [19]. To ensure effective communication across departments and between physicians and patients, radiologists manually document detailed reports based on their interpretation of CXR images. However, this process is both expertise-dependent and time-consuming [29, 51]. As the demand for imaging studies continues to grow, the workload associated with manual report generation may intensify, potentially impacting medical efficiency and compromising patient care quality [3, 50]. To mitigate these challenges, radiology report generation (RRG) [14, 44] has emerged as a promising solution. By automatically analyzing imaging data from X-ray [32], CT [16], or pathology [15], RRG generates clinical findings using factual terminology [30, 57] and descriptive language. This automation aids radiologists by providing high-quality draft reports [29], improving diagnostic efficiency.\nIn clinical practice, radiologists typically conduct comprehensive evaluations using multi-view images from the current visit, incorporate patient medical histories (i.e., longitudinal data) to track disease progression, and integrate patient-specific prior knowledge to assist in diagnosis and report generation. However, most existing RRG methods [9, 34, 36] focus solely on single images when generating reports and struggle to effectively differentiate between views, such as posteroanterior (PA), anteroposterior (AP), lateral, or left anterior oblique. These views exhibit inherent differences; for example, although both PA and AP views are frontal images, geometric variations can cause cardiac enlargement in the AP view, potentially impacting diagnostic accuracy. To address this issue, some studies [10, 60] have introduced dual-view report generation, as illustrated in Figure 1. Empirical results [9, 10] reveal that incorporating dual views enhances the quality of generated reports. Nevertheless, these methods merely distinguish between frontal and lateral views, neglecting more subtle differences across multiple views. Moreover, both single-image and dual-view methods focus solely on the images from the current visit, disregarding the descriptions of disease progression found in radiology reports. This limitation may lead to model hallucinations. To combat this problem, some studies [23, 50, 66] have sought to leverage longitudinal data to model disease progression, as shown in Figure 1. However, these methods still rely on a single image to characterize the current visit, limiting diagnostic accuracy. Additionally, some patients may lack \u201cINDICATION\u201d, \u201cprevious report\u201d, or \"previous image\" due to their first visit or improper data storage. This variability challenges the flexible use of available data to generate accurate reports.\nTo mimic radiologists' diagnostic pipeline and address these challenges, we propose a two-stage MLRG for chest X-ray report generation. In Stage 1, the key part is our proposed multi-view longitudinal contrastive learning approach, which utilizes the inherent spatiotemporal information in radiology reports to supervise the pre-training of visual and textual representations. Specifically, we incorporate learnable position embeddings for each view to identify differences across varying numbers of views. We then employ a multi-view longitudinal fusion network that flexibly integrates spatial information from current multi-view images and temporal information from longitudinal data. Subsequently, we learn visual and textual representations by leveraging agreements between multi-view longitudinal data (see Figure 1) and their corresponding radiology reports. In Stage 2, we introduce a tokenized absence encoding technique to handle missing patient-specific prior knowledge (i.e., \u201cINDICATION\u201d and \u201cprevious report\"). This allows the multi-modal fusion network to adapt flexibly to the presence or absence of such data, improving the accuracy of the generated reports. Extensive experiments on MIMIC-CXR [21], MIMIC-ABN [35], and Two-view CXR [31] datasets demonstrate the superiority of MLRG in producing clinically accurate reports. Our contributions are stated as follows:\n\u2022 We propose a novel multi-view longitudinal contrastive learning method that flexibly integrates multi-view longitudinal data and leverages the inherent spatiotemporal information from reports to supervise the pre-training of visual and textual representations.\n\u2022 We introduce a tokenized absence encoding technique to handle missing patient-specific prior knowledge. This technique enables the model to adapt flexibly to scenarios with or without such data, ensuring the text generator can utilize available prior knowledge effectively.\n\u2022 Our MLRG shows competitive results compared to various state-of-the-art methods across three public datasets: MIMIC-CXR, MIMIC-ABN, and Two-view CXR."}, {"title": "2. Related Work", "content": "Radiology report generation (RRG). RRG is akin to image captioning [25, 62], but requires generating detailed content with specialized medical terminology. Existing RRG methods consist of a vision encoder (like ResNet101 [10, 18, 60], CvT [36, 37], or ViT [33, 54]) and a text generator (such as Memory-driven Transformer [9, 30], MiniGPT-4 [28], DistilGPT2 [32, 36], or LLaMA [24, 33, 53]). To improve clinical accuracy in RRG, researchers have incorporated various techniques or prior knowledge, including knowledge graphs [59, 65], cross-modal alignment [7, 30], region-guided frameworks [46, 63], warm starting [36], patient-specific \"INDICATION\" [29, 31], disease labels [60], and disease progression [17, 50]. However, these methods rely on single-image or fixed-view data, missing the comprehensive insights supplied by multi-view longitudinal data. To address this issue, we propose the MLRG method, which flexibly captures spatiotemporal features from multi-view longitudinal data and generates radiology reports based on patient-specific prior knowledge.\nMedical vision-language models. These models aim to learn generalized medical visual representations by maximizing agreements between image-report pairs. MGCA [49] presents multi-granularity cross-modal alignment, harnessing agreements at the instance, pathological region, and disease levels. KAD [64] enhances visual representation using established knowledge graphs. MedCLIP [55] expands training sets by decoupling images and reports, reducing false negatives through semantic matching loss. BioViL-T [2] captures disease progression by analyzing longitudinal data. Despite notable improvements in tasks like medical image classification and image-text retrieval, the utilization of multi-view longitudinal data remains limited, constraining diagnostic accuracy. Therefore, we present a multi-view longitudinal contrastive learning approach that utilizes the"}, {"title": "3. Method", "content": "Figure 2 presents an overview of our proposed MLRG. In Stage 1, we introduce a multi-view longitudinal contrastive learning approach that leverages inherent spatiotemporal information from radiology reports to supervise the pre-training of visual and textual representations. In Stage 2, we propose a tokenized absence encoding technique to handle missing patient-specific prior knowledge, ensuring the generation of more coherent and accurate radiology reports based on available prior knowledge.\nLet $D^{tr} = \\{(x^{pri}, y^{pri}, z_i, X^{cur}, y^{cur})\\}_{i=1}^n$ be the training set, where n denotes the total number of visits. Each visit consists of a frontal previous image $x^{pri}$ (which may be absent), a previous report $y^{pri}$ (which may be absent), an \"INDICATION\" $z_i$ (which may be absent), $m_i$ current images (views) $X^{cur}$, and a reference report $y^{cur}$. Notably, The number of current multi-view images $m_i$ may vary across visits. Our goal is to learn the function $F_{\\theta}(\\cdot)$ that maps $(x^{pri}, y^{pri}, z_i, X^{cur})$ to $y^{cur}$ on the training set $D^{tr}$, such that $F_{\\theta}(x^{pri}, y^{pri}, z_i, X^{cur}) \\rightarrow y^{cur}$. We then utilize the learned function $F_{\\theta}(\\cdot)$ to generate a radiology report based on current multi-view images, the previous image, and patient-specific prior knowledge (i.e., $y^{pri}$ and $z_i$).\nVisual features extraction. We employ RAD-DINO [40], a vision transformer model [13] trained solely on chest X-rays using DINOv2 [38], as the vision encoder. The feature maps from the last hidden state are treated as visual features $V \\in \\mathbb{R}^{M \\times p \\times d_1}$, where $M = \\sum_{i=1}^{B} m_i$ denotes the total number of images in the mini-batch. Here, $B$, $p$, and $d_1$ represent the batch size, the number of patches, and the feature dimension, respectively.\nTextual features extraction. Inspired by FSE [30], we first adopt the structural entities approach [30] to extract factual serialization, which consists exclusively of clinical descriptions from radiology reports, as shown in Figure 2. This method enables the model to concentrate on alignment between images and factual serialization. We then consider CXR-BERT [4], a language model tailored for chest X-rays, as the text encoder. This is followed by a simple projection head that generates textual features $R \\in \\mathbb{R}^{B \\times t \\times d}$, where t denote the number of tokens, and $d$ is the hidden size.\nMulti-positive contrastive learning between current multi-view images to improve the consistency of visual features. In clinical practice, radiologists often select some representative images as primary references, with other images as auxiliary support. Therefore, we treat each image from current multi-view images $X^{cur}$ as an anchor scan $x_i^{cur}$ while considering the remaining images as auxiliary references $X_\\backslash i^{cur} = \\{x_j^{cur}|j \\neq i, x_j^{cur} \\in X^{cur} \\}$, where $a \\in [1, m_i]$. To identify differences among views, we incorporate learnable view positional embeddings $E_v \\in \\mathbb{R}^{M \\times 1 \\times d_1}$ into visual features. This is followed by a simple projection head $P_v(\\cdot)$ that maps the features to a specific dimension $d$. These processes are formulated as follows:\n$V = P_v(V + E_v) \\in \\mathbb{R}^{M \\times p \\times d}.$\nTo enhance the consistency of visual features, we employ multi-positive contrastive learning [47] to maximize the similarity between images from the same visit while minimizing the similarity to images from different visits. Specifically, we first exclude visits with only one image, as they do not provide positive pairs (Notably, these visits are still used for subsequent cross-modal alignment). Following this, we calculate the predicted categorical distribution $q \\in \\mathbb{R}^{K \\times (K-1)}$ to estimate the similarity between images:\n$q_i = \\frac{\\exp (v_i\\cdot v_\\backslash i / \\tau_1)}{\\sum_{j=1,j\\neq i}^{M} \\exp (v_i\\cdot v_j / \\tau_1)} ,s.t.m_i \\neq 1,$ \nwhere $K = \\sum_{i=1,m_i \\neq 1}^{M} m_i$ represents the total number of multi-view images in the mini-batch, and $\\tau_1 \\in \\mathbb{R}^+$ is a temperature parameter. $v_i \\in \\mathbb{R}^{1 \\times d}$ refers to the global visual feature of the ith image, while $v_{\\backslash i} \\in \\mathbb{R}^{(K-1) \\times d}$ denotes global visual features of all multi-view images except the ith image. Next, we compute the ground-truth categorical distribution $p \\in \\mathbb{R}^{K \\times (K-1)}$ by assigning the same labels to images from the same visit, formulated as:\n$p_i = \\frac{\\mathbb{I}_{match} (v_i, V_{\\backslash i})}{\\sum_{j=1,m_j \\neq 1}^{M} \\mathbb{I}_{match} (v_i, v_j)}$\nwhere $\\mathbb{I}_{match} (\\cdot,\\cdot)$ is an indicator function that determines whether two visual features originate from the same visit. Although the number of current views $m_i$ may vary across visits, the different number of non-zero elements in $p_i$ account for this variability. Finally, the multi-positive contrastive (MPC) loss is calculated using the cross-entropy between q and p, represented as:\n$\\mathcal{L}_{MPC} = -\\frac{1}{K} \\sum_{i=1,m_i \\neq 1}^{M} p_i\\log q_i$\nMulti-view longitudinal fusion network. Due to the varying number of current multi-view images and the absence of previous images for some patients, integrating this information flexibly presents certain challenges. To address this issue, we design the multi-view longitudinal fusion (MLF) network, as illustrated in Figure 3(A). We select the most recent previous visit to model temporal information, as it typically holds the highest reference value. To distinguish different time points, we integrate temporal positional embeddings into visual features, as depicted in Figure 2. Subsequently, the spatiotemporal features $V^{st} = \\{v_1^{st}, v_2^{st}, ..., v_i^{st} \\}$ are extracted using the MLF network:\n$v_i^{st} = MLF([v_i^{cur}, V_{\\backslash i}^{cur}, v^{pri}]) \\in \\mathbb{R}^{p \\times d},$\nwhere the anchor scan $v_i^{cur}$ functions as the query, and the concatenation of auxiliary references and previous image"}, {"title": "4. Experiments", "content": "4.1. Experimental Settings\nDatasets. 1) MIMIC-CXR [21] is a large-scale, publicly available dataset comprising paired chest X-rays and radiology reports. Each pair contains a varying number of images compared to others, and all pairs for a patient are organized chronologically, facilitating the construction of multi-view longitudinal data. 2) MIMIC-ABN [35] is a subset of MIMIC-CXR, focusing solely on radiology reports that describe abnormal clinical findings. 3) Two-view CXR [31] aggregates visits with two current images from both MIMIC-CXR and IU X-ray [12]. Notably, as the IU X-ray does not include previous visits, both previous images and reports are unavailable. We adhere to official splits for these datasets and summarize the sample counts for the training, validation, and test set in Table 1. In line with [6, 9, 26, 29, 46], we treat the \u201cFINDINGS\" section in radiology reports as the reference reports.\nEvaluation metrics. Following prior works [5, 9, 50, 52, 54], we evaluate the effectiveness of our MLRG using both natural language generation (NLG) and clinical efficacy (CE) metrics. NLG metrics, which assess the linguistic similarities between generated and reference reports, include BLEU-n (B-n), METEOR (MTR), and ROUGE-L (R-L). For CE metrics, we utilize CheXpert [19] to label the generated reports with 14 observations (see Appendix Table A5) and compute the micro-average Precision (P), Recall (R), and F1 score (F1) based on ground truths. CE metrics also include F1 RadGraph (RG) [20], which evaluates the overlap of clinical entities and their relations, aligning more closely with radiologists than B-3 and F1 metrics [61]. All metrics are computed using official libraries [8, 20, 45], with higher values indicating better performance.\nImplementation details. Both $\\tau_1$ and $\\tau_2$ are set to 0.5. The number of blocks $L_1$ and $L_2$ in Figure 3 are set to 3 and 1, respectively. Each dataset is configured to generate a maximum of 100 tokens. We identify the best model as the"}, {"title": "4.2. Main Results", "content": "We compare our MLRG with 14 state-of-the-art methods: R2Gen [9], CMN [10], SA [57], MET [54], KiUT [18], COFE [27], MAN [43], B-LLM [28], DCG [32], Med-LLM [33], SEI [29], FVMP [34], HERGen [50], and CXRMate [37]. Results are presented in Table 2, where \u201cSI\u201d, \u201cInd\u201d, \u201cMVD\u201d, \u201cLong\u201d, \u201cMVL\u201d, and \u201cDV\u201d represent different input types: single images, \u201cINDICATION\u201d, multi-view data, longitudinal data, multi-view longitudinal data, and dual views, respectively. We observe that our MLRG achieves SOTA performance across most metrics, with particular strength in B-4, RG, and F1. This suggests that MLRG excels in generating both coherent and accurate radiology reports. Although MLRG shows slightly lower Recall than B-LLM [28], its F1 and other metrics are significantly better. In Appendix Table A7, we also show our MLRG\u2019s ability to generate \"FINDINGS\u201d and \u201cIMPRESSION\" sections."}, {"title": "4.3. Ablation Study", "content": "Table 3 presents an ablation study on the MIMIC-CXR [21] dataset, analyzing the effect of different components on model performance.\nEffect of multi-view longitudinal contrastive learning (Stage 1). In Table 3, (a) represents a report generation scheme based solely on patient-specific prior knowledge, excluding Stage 1. Results reveal that our MLRG significantly exceeds (a), highlighting the critical role of multi-view longitudinal contrastive learning in enhancing the accuracy and coherence of generated reports. Additionally, we observe that both $\\mathcal{L}_{G}$ ((c) vs. (a)) and $\\mathcal{L}_{MPC}$ ((d) vs. (c)) have a positive impact on model performance.\nEffect of patient-specific prior knowledge (Stage 2). As shown in Table 3, MLRG significantly outperforms (b), which lacks patient-specific prior knowledge, emphasizing the importance of incorporating such knowledge to improve the coherence and clinical accuracy of generated reports. Moreover, the independent integration of \u201cINDICATION\u201d ((e) vs. (b)) and \"previous report\" ((f) vs. (b)) contributes positively to model performance.\nEffect of current multi-view images. Table 3 demonstrates that generating reports using current multi-view images outperforms those derived from single images (MLRG vs. (h)), highlighting the effectiveness of multi-view images in modeling the current disease conditions.\nEffect of previous images. As shown in Table 3, MLRG shows a clear advantage over (d), indicating that MLF network (see Figure 3) effectively integrates previous images. This capability allows the model to track disease progression, thereby generating more clinically accurate reports."}, {"title": "4.4. Case Study", "content": "Model benefits from current multi-view images, multi-view longitudinal data, and \u201cINDICATION\u201d. Table"}, {"title": "5. Conclusion", "content": "In this paper, we introduced the MLRG method for chest X-ray report generation. We first proposed a multi-view longitudinal contrastive learning approach that leveraged the inherent spatiotemporal information from radiology reports to guide the pre-training of visual and textual representations. This approach not only captured differences among views but also flexibly extracted spatial features from current multi-view images and temporal features from longitudinal data, effectively leveraging spatiotemporal information in reports for pre-training. Subsequently, we presented a tokenized absence encoding technique to handle missing patient-specific prior knowledge. This technique allowed the multi-modal fusion network to adapt flexibly to scenarios with or without such data, ensuring the text generator can utilize available prior knowledge effectively. Extensive experiments on MIMIC-CXR, MIMIC-ABN, and Two-view datasets demonstrated that our MLRG outperforms existing SOTA methods in generating coherent and clinically accurate radiology reports, making it a strong contender for chest X-ray report generation. Future work will focus on using saliency maps [58] to learn region-based features and predict uncertainty [22] to improve model reliability."}, {"title": "A. Experiments", "content": "A.1. Implementation Details\n1) MIMIC-CXR [21]: In stage 1, the model is trained for 50 epochs with a learning rate of 5e-5 and a batch size of 32. In Stage 2, we train MLRG for another 50 epochs, using a batch size of 14. The learning rate is set to 5e-6 for parameters from Stage 1 and 5e-5 for the remaining parameters. 2) MIMIC-ABN [35] and Two-view CXR [31]: Since most images are derived from the MIMIC-CXR dataset, we directly fine-tune the model from Stage 2 on MIMIC-CXR, using a learning rate of 5e-6 and a batch size of 12. 3) Common settings: Early stopping with a patience of 15 is employed to prevent overfitting. The ReduceLROnPlateau scheduler and the AdamW optimizer are applied. The natural language generation (NLG) metrics are calculated with the pycocoevalcap1. For clinical efficacy (CE) metrics, Precision, Recall, and F1 score metrics are computed using the f1chexbert library, and the F1 RadGraph metric is calculated with the radgraph3 library.\nA.2. Clinical Accuracy of 14 Observations\nTables A5 and A6 show the clinical accuracy of 14 observations annotated by CheXpert [19] on the MIMIC-CXR, MIMIC-ABN, and Two-view CXR datasets. Results show that our MLRG outperforms SEI [29] on most observations. Even though MLRG is not specifically tailored for imbalanced observations, it still slightly surpasses the baseline on challenging observations like Pneumothorax and Pleural Other.\nA.3. Performance of Generating \u201cFINDINGS\" and \"IMPRESSION\" Sections\nRadiology reports typically consist of three key sections: \u201cINDICATION\u201d, which outlines the visit reasons or symptoms; \"FINDINGS\", which details observations from current multi-view images and comparisons with the patient\u2019s medical history; and \"IMPRESSION\u201d, which summarizes the key conclusions or diagnostic interpretations based on the \u201cFINDINGS\". Table A7 presents the performance of generating \u201cFINDINGS\u201d and \u201cIMPRESSION\u201d sections, with specific examples in Figure A7. Since most existing methods focus primarily on generating the \u201cFINDINGS\u201d section, peer methods are not included in Table A7. The re-sults indicate that our MLRG is capable of generating both sections with minor modifications. Specifically, we utilize special tokens, \u201c[FINDINGS]\u201d and \u201c[IMPRESSION]", "FINDINGS\" generation.\nA.4. Qualitative Analysis\nFigure A6 provides additional examples of the \"FIND-INGS\" section generated by SEI [29] and our MLRG, while Figure A7 presents examples of both the \"FINDINGS\" and \"IMPRESSION\" sections from MLRG. These results suggest that 1) Our MLRG is highly competitive in generating both \"FINDINGS\" and \"IMPRESSION\" sections, as well as the \"FINDINGS\" section alone, for chest X-ray reports. 2) MLRG still has room for improvement in describing lesion attributes. For example, in Figure A6, MLRG incorrectly describes the \u201cproximal parts of the stomach\u201d as the \u201cmiddle parts\u201d. This occurs because MLRG has not fully learned region-level features. To improve this, we are exploring the use of saliency maps [58] to enhance regional feature learning and the accuracy of lesion descriptions.\nA.5. Evaluation Using Large Language Models\nInspired by [61], the GREEN model [39] identifies six categories of clinical errors": "a) False report of a finding in the candidate; (b) Missing a finding present in the reference; (c) Misidentification of a finding\u2019s anatomic location/position; (d) Misassessment of the severity of a finding; (e) Mentioning a comparison that isn't in the reference; (f) Omitting a comparison detailing a change from a prior study. The GREEN score for the ith sample is defined as:\n$GREEN_i = \\frac{\\#MatchedFindings_i}{\\#MatchedFindings_i + \\sum_{j=(a)}^{(f)} \\#Error_{i,j}}$\nwhere $\\sum_{j=(a)}^{(f)} \\#Error_{i,j}$ represents the total clinically sig-nificant errors for the ith sample across categories (a) to (f). \u201c#Matched Findings\u201d denotes the number of matched findings between generated and reference reports. Figure A8 illustrates the GREEN model\u2019s output on the MIMIC-CXR test set. Furthermore, we compare our MLRG with R2Gen [9], CMN [10], CGPT2 [36], and SEI [29] in terms of"}]}