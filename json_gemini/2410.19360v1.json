{"title": "LArctan-SKAN: Simple and Efficient Single-Parameterized Kolmogorov-Arnold Networks using Learnable Trigonometric Function", "authors": ["Zhijie Chen", "Xinglin Zhang"], "abstract": "This paper proposes a novel approach for designing Single-Parameterized Kolmogorov-Arnold Networks (SKAN) by utilizing a Single-Parameterized Function (SFunc) constructed from trigonometric functions. Three new SKAN variants are developed: LSin-SKAN, LCos-SKAN, and LArctan-SKAN. Experimental validation on the MNIST dataset demonstrates that LArctan-SKAN excels in both accuracy and computational efficiency. Specifically, LArctan-SKAN significantly improves test set accuracy over existing models, outperforming all pure KAN variants compared, including FourierKAN, LSS-SKAN, and Spl-KAN. It also surpasses mixed MLP-based models such as MLP+rKAN and MLP+fKAN in accuracy. Furthermore, LArctan-SKAN exhibits remarkable computational efficiency, with a training speed increase of 535.01% and 49.55% compared to MLP+rKAN and MLP+fKAN, respectively. These results confirm the effectiveness and potential of SKANS constructed with trigonometric functions. The experiment code is available at https://github.com/chikkkit/LArctan-SKAN.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent developments in Kolmogorov-Arnold Networks (KAN) have attracted widespread attention in research [1]. Similar to literature [2], this paper uses \"KAN\" to refer broadly to the family of KAN type networks, while specifically using \"Spl-KAN\" for KAN variants constructed with cubic spline functions. For benchmarking, the Spl-KAN implementation from the efficient-kan library on GitHub is employed.\nOne of the hot topics of current research focuses on constructing general network architectures for KAN. Initially, KANs built with cubic spline functions outperformed multilayer perceptrons (MLP) in modeling commonly used formulas in scientific computation, sparking extensive research into KAN [1]. Several KAN variants have since emerged. Wav-KAN, developed by Bozorgas and Chen, leverages wavelet functions as basis functions, surpassing Spl-KAN in both accuracy and speed [3]. Fast-KAN, introduced by Li, utilizes Gaussian radial basis functions, improving forward propagation speeds to 3.33 times that of Spl-KAN [4]. Additionally, Gist Noesis proposed FourierKAN, which constructs KAN using 1D Fourier coefficients instead of spline coefficients [5]. By employing periodic trigonometric functions, FourierKAN circumvents the issue of grids running out of bounds, a common problem in Spl-KAN, while also offering faster computation. Further notable innovations include fKAN and rKAN, developed by Aghaei et al., which use trainable adaptive fractional orthogonal Jacobi functions and Pad\u00e9 approximation with rational Jacobi functions, respectively, as basis functions to enhance network performance [6] [7].\nChen and Zhang's Efficient KAN Expansion (EKE) Principle and the subsequent development of Single-Parameterized KAN (SKAN) provided a new framework for improving KAN scalability [2]. The EKE Principle emphasizes parameter allocation as a more effective means of improving KAN performance over the use of more complex basis functions. Guided by this principle, LSS-SKAN (Learnable Shifted Softplus based SKAN) was developed, which surpassed models such as Spl-KAN, WavKAN, FastKAN, and FourierKAN in terms of accuracy and computational efficiency.\nThis study is also inspired by Fourier transform theory, which indicates that periodic functions can be expressed through summations of sine and cosine waves, highlighting the potential of trigonometric functions in function approximation. Based on this theory, the current work explores constructing SKAN base functions using trigonometric functions and systematically tests these on the MNIST dataset. The results show that"}, {"title": "II. CONSTRUCTING SKAN USING TRIGONOMETRIC FUNCTIONS", "content": "The systematic analysis of common trigonometric functions provides the foundation for constructing SKAN base functions. Criteria for \u201cfeasibility\u201d require that the domain encompasses the real numbers to ensure stable operation in multi-layer networks. Two core principles were adhered to in constructing learnable functions:\nPrinciple 1: Prioritize placing parameters to extend the range of the reference function to the entire real number set.\nPrinciple 2: When the reference function already spans the real number set or cannot be extended through a single trainable parameter, apply function extensions according to a specific sequence, preferring multiplicative terms over additive terms, with parameters positioned closer to the independent variable holding higher priority.\nFollowing these principles, multiple learnable variants were developed. For sine and cosine functions, whose ranges are limited to [-1, 1], trainable parameters were added externally, forming the Learnable Sine (LSin) and Learnable Cosine (LCos) functions. This design expands their range, enhancing network expressiveness. The formulas for LSin and LCos are as follows:\nLSin (k, x) = k sin (x)"}, {"title": "III. EXPERIMENT", "content": "To evaluate the performance of the proposed method, this study replicates the experimental design from [2] and compares LSin, LCos, and LArctan with various existing KAN variants, including FourierKAN, LSS-SKAN, MLP+rKAN, MLP+fKAN, Spl-KAN, FastKAN, and WavKAN. The MNIST dataset is used for testing, with a learning rate determined via linear search, selecting 9 points equally spaced in the range [0.0001, 0.0009] and 10 points in [0.001, 0.01]. All experiments were conducted independently on an NVIDIA RTX 4060Ti gpu five times, with the optimal result selected for each network at each learning rate.\nResults indicate a substantial breakthrough in performance with LArctan-SKAN. From comparisons of loss functions and accuracy (see Fig. 1), LArctan-SKAN not"}, {"title": "IV. CONCLUSION", "content": "This study serves as a significant extension of [2], proposing Single-Parameterized functions (SFunc) based on trigonometric functions for the design of SKAN and further developing three SKAN variants: LSin-SKAN, LCos-SKAN, and LArctan-SKAN. Experimental results indicate that LArctan-SKAN demonstrates outstanding accuracy, not only surpassing the performance of all pure KAN networks but also outperforming hybrid models based on MLP, specifically MLP+rKAN and MLP+fKAN. The test accuracy improvements achieved by LArctan-SKAN are as follows: an increase of 0.93% over FourierKAN, 0.53% over LSS-SKAN, 0.14% over MLP+rKAN, 0.19% over MLP+fKAN, 1.94% over Spl-KAN, 2.99% over FastKAN, 0.50% over WavKAN, 0.28% over LSin-SKAN, and 2.01% over LCos-SKAN.\nIn terms of computational efficiency, LArctan-SKAN shows remarkable improvements in training speed, outperforming all comparison networks: a 14.89% increase over FourierKAN, 9.41% over LSS-SKAN, 535.01% over MLP+rKAN, 49.55% over MLP+fKAN, 48.38% over Spl-KAN, 14.58% over FastKAN, and 21.93% over WavKAN. Among the three variants, LSin-SKAN and LCos-SKAN exhibit faster"}], "equations": ["LCos (k, x) = k cos (x)", "LArctan (k, x) = arctan (kx)"], "index_terms": ["Kolmogorov-Arnold Networks", "KAN", "Single-Parameterized Function", "Neural Networks", "SKAN", "LArctan-SKAN"]}