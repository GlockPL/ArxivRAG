{"title": "DETECTING AND APPROXIMATING REDUNDANT COMPUTATIONAL BLOCKS IN NEURAL NETWORKS", "authors": ["Irene Cannistraci", "Emanuele Rodol\u00e0", "Bastian Rieck"], "abstract": "Deep neural networks often learn similar internal representations, both across different models and within their own layers. While inter-network similarities have enabled techniques such as model stitching and merging, intra-network similarities present new opportunities for designing more efficient architectures. In this paper, we investigate the emergence of these internal similarities across different layers in diverse neural architectures, showing that similarity patterns emerge independently of the datataset used. We introduce a simple metric, Block Redundancy, to detect redundant blocks, providing a foundation for future architectural optimization methods. Building on this, we propose Redundant Blocks Approximation (RBA), a general framework that identifies and approximates one or more redundant computational blocks using simpler transformations. We show that the transformation T between two representations can be efficiently computed in closed-form, and it is enough to replace the redundant blocks from the network. RBA reduces model parameters and time complexity while maintaining good performance. We validate our method on classification tasks in the vision domain, using a variety of pretrained foundational models and datasets.", "sections": [{"title": "INTRODUCTION", "content": "As Neural Networks (NNs) grow in size and complexity, their demand for computational resources has become a significant bottleneck. Despite the impressive performance of large models, they often come with substantial trade-offs, such as slower inference times and increased memory and power consumption. This has led to a growing interest in methods that can reduce model complexity without sacrificing performance. However, most approaches to mitigating these challenges either require additional training or complex fine-tuning, or they result in a non-trivial loss in performance. However, recent research showed that there exists internal representation similarities within and between NNs. Thus, many layers or components within these networks may perform similar functions or yield highly correlated outputs, suggesting the potential for simplifying these networks. Understanding and leveraging these internal similarities can open up new opportunities for reducing model size, enhancing inference speed, and improving computational efficiency.\nIn this paper, we address two key research questions: (i) how to identify redundant blocks, and (ii) how to effectively approximate these blocks while preserving the final representations and the network's"}, {"title": "RELATED WORK", "content": "Measuring Similarities. A range of metrics have been introduced to assess the similarity between latent spaces generated by different NNs Klabunde et al. (2023); Ballester et al. (2023). One established approach is Canonical Correlation Analysis (CCA) (Hotelling, 1992), known for its invariance to linear transformations. Variants of CCA, such Singular Value Decomposition (SVD) and Singular Value CCA (SVCCA) (Raghu et al., 2017), aim to enhance robustness, while techniques like Projection Weighted CCA (PWCCA) (Morcos et al., 2018) mitigate sensitivity to small perturbations. Another widely used metric, Centered Kernel Alignment (CKA) (Kornblith et al., 2019), captures the similarity between latent spaces while ignoring orthogonal transformations. However, recent work (Davari et al., 2022) highlights that this metric can be sensitive to shifts in the latent space. Additionally, Barannikov et al. (2021) proposes a method to compare two data representations by measuring the multi-scale topological dissimilarity, while Fumero et al. (2024) leverages the principles of spectral geometry to model and analyze the relationships between distinct latent spaces.\nLeveraging Similarities. Analyzing the similarities between internal representations, both within and across NNs, has received significant attention in recent research. For instance, Valeriani et al. (2024) examines the intrinsic dimensions and neighbor compositions of representations in various transformer models. Similarly, Kvinge et al. (2022) explores how models process different variations of data points across layers, while Nguyen et al. (2020) investigates how changes in network depth and width impact hidden representations, revealing characteristic block structures in larger-capacity models. Finally Crisostomi et al. (2023) investigates under what assumptions two latent spaces be merged into one. All these insights have been applied across various contexts. For instance, Moschella et al. (2023) constructs a unified space shared by different NNs, enabling zero-shot stitching of independently trained models across different modalities Norelli et al. (2023), even without explicit assumptions on the transformation class that connects the latent manifold embeddings Cannistraci et al. (2024) or with partial correspondence within the latent spaces Cannistraci et al. (2023). While Ricciardi et al. (2023) proves the feasibility of zero-shot stitching between encoders and policies trained on different environmental variations. Other works L\u00e4hner & Moeller (2024); Maiorca et al. (2024) demonstrate that representations learned by dinstinct NNs can be aligned using simple transformations. Finally,"}, {"title": "REDUNDANT BLOCKS APPROXIMATION", "content": "The core principle of our approach, RBA, is to detect similar representations within NNs, identifying redundant blocks, and approximate them with simpler transformations instead of executing the entire DNN. A visual overview is provided in Figure 1.\nIn this section, we first show how to identify redundant blocks, and how to effectively approximate their representations while preserving the network's overall functionality.\nIdentifying Redundant Representations. We hypothesize that certain foundation model archi- tectures, such as Vision Transformers (ViTs), may contain redundant blocks that produce similar representations. This redundancy may stem from overparameterization or task-specific characteristics. In this context, a \"block\" refers to a self-contained unit in the model that typically contains several layers, such as self-attention, normalization, or feed-forward layers, but functions as a cohesive unit.\nTo quantify redundancy, we introduce a simple metric called Block Redundancy (BR), which measures the degree of change in internal representations between blocks. This helps to identify essential blocks versus those that contribute minimally to the overall model.\nLet $B$ represent the total number of blocks in the model, and let $h^{(b)}$ denote the internal representation (i.e., the output) of block $b$, where $b \\in {1, 2, ..., B}$. For a given subset of the training data $D_{sub}$, we compute the representations $h^{(b)}(x)$ for each input $x \\in D_{sub}$. The BR for block $b$ is defined as the negative Mean Squared Error (MSE) between the output representations of blocks $b$ and $b - 1$:\n$\\qquad BR(b) = -\\frac{1}{|D_{sub}|} \\sum_{x \\in D_{sub}} ||h^{(b)}(x) - h^{(b-1)}(x)||^2 \\qquad (1)$\nA higher $BR(b)$ indicates a minimal change between the outputs of block $b$ and the preceding block $b - 1$, suggesting a potential redundancy in block $b$. Conversely, a lower $BR(b)$ implies that block $b$ plays a significant role in transforming the internal representations.\nBy systematically evaluating the BR for each block, we can identify redundant components that can be simplified, enabling a reduction in the NN's complexity without compromising the original final representation or its performance.\nApproximating Redundant Blocks. After identifying redundant representations using BR, the next step is to approximate their outputs through more computationally efficient transformations, rather than directly removing the blocks. While this approach applies to consecutive blocks such as $b_i$ and $b_{i+1}$, it generalizes naturally to non-consecutive blocks as well. Specifically, for any block $b_i$ and block $b_{i+n}$ (where $n \\geq 1$), our method enables the approximation of the output of block $b_{i+n}$ from the output of block $b_i$, provided they exhibit low BR scores. This allows us to skip the computation of blocks $b_{i+1}, b_{i+2}, ..., b_{i+n}$, effectively reducing the overall computation."}, {"title": "EXPERIMENTS", "content": "In this section, we analyze the representation of foundation pre-trained models and we show quantita- tive experiments to evaluate the effectiveness of our proposed framework. We begin by empirically motivating our study in Section 4.1, where we analyze the similarity between different blocks of pretrained foundation models for image classification. Then in Section 4.2, we assess the impact of approximating blocks on latent representations and explore the correlation between layer approx- imations and high BR. Finally in Section 4.3, we conduct quantitative experiments on the image classification task to further evaluate the performance of our framework across various models and datasets, demonstrating its general applicability and effectiveness."}, {"title": "BLOCK SIMILARITIES", "content": "Experimental Setting. In this section, we analyze the latent spaces generated by pretrained founda- tional models in the vision domain. Our analysis focuses on five distinct transformer-based models: ViT-S, ViT-B, DINO-S, DINO-B, and DEiT-S. We evaluate their similarities using four well- known datasets: CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), MNIST (Deng, 2012), and F-MNIST (Xiao et al., 2017). Since these models classify input based on the representation of the [CLS] token, the analysis is conducted using the [CLS] token from each block, rather than the full representation. This ensures that the analysis remains aligned with the key components of the model's final predictions. This flexibility enables the method to adapt to different model architectures and tasks, where tokens other than the [CLS] may hold more relevant information. Model and dataset details can be found in Table 3 and Table 4, respectively.\nResults and Analysis. Figure 2 presents the cosine similarity matrices between blocks of the ViT-B and DiNO-S models on MNIST and CIFAR-100. These matrices illustrate the internal block-by- block similarities within each architecture. Our results reveal that while the patterns of similarity vary across architectures, they remain consistent across different datasets. This suggests that the similarity"}, {"title": "REDUDANT BLOCK APPROXIMATION", "content": "Experimental Setting. In Section 4.1, we empirically demonstrate that different blocks in pretrained models exhibit similarities. To further investigate this, introduce the Block Redundancy metric. As illustrated in Equation (1), this metric measures the level of redundancy of a block: a high score indicates a minimal change between two blocks' output, suggesting that the second block may be redundant. Conversely, a low score implies that the second block contributes significantly to the final prediction. After identifying redundant blocks, we restructure the models accordingly to reduce their complexity and parameter count. These redundant blocks are approximated using a shared linear transformation applied across all tokens based on a subset of 3,000 training samples. We compute BR scores for each block across different datasets and pretrained encoders: ViT-S, DINO-S, DEIT-S, utilizing MNIST,F-MNIST,CIFAR-10, and CIFAR-100. Additionally, we compute the MSE between the representations of the last layer in the original model and the RBA model when skipping the $i^{th}$ block. We also visualize the Principal Component Analysis (PCA) projections of these representations when specific blocks are approximated to assess the impact on representation fidelity."}, {"title": "DOWNSTREAM TASK: CLASSIFICATION", "content": "Experimental Setting. We finally conduct image classification using the same datasets and pretrained models described in previous sections, with all models remaining pretrained and frozen. After identifying redundant blocks, the models are restructured accordingly. Approximations between blocks are computed using a shared linear transformation across all tokens, based on a subset of 3,000 training samples. Subsequently, a single linear layer is trained for classification using the Adam optimizer with a learning rate of 0.001 over 5 epochs, three seeds, and a batch size of 256.\nResults and Analysis. As illustrated in Table 1, employing RBA allows for reducing model size while maintaining, and in some cases even improving, performance. Notably, as discussed in Section 4.2 and illustrated in Figure 3 and Figure 5, using DEiT-S to approximate the last blocks yields better results, even when approximating multiple blocks such as 9\u219211 or 8\u219210. In contrast, with ViT-S, the same approximations result in a slight decrease in performance. However, overall, performance remains similar or improved, demonstrating that a simple linear transformation is sufficient to approximate different blocks of a NN, significantly reducing the number of parameters and model complexity. It's important to note that this transformation is shared across all tokens, further optimizing the process. Additional results on classification performance can be found in Table 5.\nAblation Analysis. Additionally, we evaluated the model's performance when completely skipping blocks instead of approximating them. The results in Table 2 show the accuracy scores for ViT-S on CIFAR-10 and CIFAR-100F, where the \"Skip\" column represents the operation of skipping a block entirely rather than applying an approximation. The findings consistently demonstrate that approximating blocks significantly outperforms skipping them in all cases. This underscores the effectiveness of RBA in preserving model performance while reducing complexity. Please refer to Table 7 for results on other datasets."}, {"title": "CONCLUSION", "content": "In this paper, we introduced a novel framework for approximating redundant representations in transformer-based foundation models and proposed a simple yet effective metric to identify such redundancies. By leveraging a simple linear transformation, shared across all tokens, between consecutive and non-consecutive blocks output, we demonstrated that it is possible to significantly reduce model parameters and complexity without sacrificing performance, and in some cases even improving it. Our approach provides an efficient way to optimize model architecture, maintaining essential representation fidelity while streamlining the network for downstream tasks.\nLimitations and Future Works. While our framework shows promising results, it has been primarily tested on transformer-based architectures. We leave to future work to explore the application of our framework across different modalities (e.g., text), architectures (e.g., ResNets and AutoEncoders), and downstream tasks (e.g., reconstruction). Additionally, we plan to enhance the representation analysis by incorporating topological metrics, which could provide a different perspective on structural similarities between representations. This alternative viewpoint may uncover new insights into redundancy patterns and further refine our approach. By expanding the framework's scope, we aim to validate its versatility and continue optimizing model efficiency across a broader set of architectures and tasks, advancing its practical applicability in diverse settings."}, {"title": "APPENDIX", "content": "A.1 REPRODUCIBILITY STATEMENT\nIn Section Section 3, we provide a detailed description of the proposed framework and the experi- mental settings for the various scenarios. In the following sections, we present all implementation details that are not described in the main manuscript. Additionally, we will release a modular PyTorch implementation.\nA.2 IMPLEMENTATION DETAILS\nThis section details the experiments conducted in Section 4. Table 3 contains the full list of the pretrained models, while Table 4 contains dataset information.\nA.2.1 TOOLS & TECHNOLOGIES\nAll the experiments presented in this work employ the following tools:\n\u2022 PyTorch Lightning, to ensure reproducible results while also getting a clean and modular codebase;\n\u2022 NN-Template GrokAI (2021), to easily bootstrap the project and enforce best practices;\n\u2022 Transformers by HuggingFace, to get ready-to-use transformers for both text and images;\n\u2022 Datasets by HuggingFace, to access most of the datasets;\n\u2022 DVC (Kuprieiev et al., 2022), for data versioning;\nA.3 ADDITIONAL EXPERIMENTS"}]}