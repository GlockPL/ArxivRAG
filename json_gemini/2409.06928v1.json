{"title": "Intrapartum Ultrasound Image Segmentation of Pubic Symphysis and Fetal Head Using Dual Student-Teacher Framework with CNN-ViT Collaborative Learning", "authors": ["Jianmei Jiang", "Huijin Wang", "Jieyun Bai", "Shun Long", "Shuangping Chen", "Victor M. Campello", "Karim Lekadir"], "abstract": "The segmentation of the pubic symphysis and fetal head (PSFH) constitutes a pivotal step in monitoring labor progression and identifying potential delivery complications. Despite the advances in deep learning, the lack of annotated medical images hinders the training of segmentation. Traditional semi-supervised learning approaches primarily utilize a unified network model based on Convolutional Neural Networks (CNNs) and apply consistency regularization to mitigate the reliance on extensive annotated data. However, these methods often fall short in capturing the discriminative features of unlabeled data and in delineating the long-range dependencies inherent in the ambiguous boundaries of PSFH within ultrasound images. To address these limitations, we introduce a novel framework, the Dual-Student and Teacher Combining CNN and Transformer (DSTCT), which synergistically integrates the capabilities of CNNs and Transformers. Our framework comprises a Vision Transformer (ViT) as the 'teacher' and two 'student' models\\u2014one ViT and one CNN. This dual-student setup enables mutual supervision through the generation of both hard and soft pseudo-labels, with the consistency in their predictions being refined by minimizing the classifier determinacy discrepancy. The teacher model further reinforces learning within this architecture through the imposition of consistency regularization constraints. To augment the generalization abilities of our approach, we employ a blend of data and model perturbation techniques. Comprehensive evaluations on the benchmark dataset of the PSFH Segmentation Grand Challenge at MICCAI 2023 demonstrate our DSTCT framework", "sections": [{"title": "1 Introduction", "content": "The segmentation of the pubic symphysis and fetal head (PSFH) from intra-partum ultrasound (US) images is a critical step in developing an automated di-agnostic system. This process is essential for generating quantitative descriptors, such as shape and size of PSFH. These descriptors are crucial in assessing labor progression and identifying potential delivery complications [7,1,12,2,15,4,6,23]. Recent advancements in deep learning, particularly in Convolutional Neural Net-works (CNNs) and Transformers, have significantly improved medical image seg-mentation. However, these models face challenges in clinical applications due to the scarcity of large-scale annotated training datasets. Deep learning approaches generally require extensive, labeled datasets to ensure model generalization. The collection of densely annotated US images is a complex task. It demands con-siderable time, medical expertise, and clinical experience for accurate pixel-wise labeling [3,24]. In clinical practice, there is often a larger amount of unlabeled data than labeled data, emphasizing the importance of semi-supervised learning techniques. These techniques aim to improve the segmentation performance of US images by leveraging unlabeled data, a practice that is garnering increasing research interest.\nDespite the progress in semi-supervised learning for US image segmentation, challenges persist due to common US imaging issues like shadow artifacts and unclear boundary lines. Most semi-supervised approaches rely on CNN archi-tectures, which may lead to under-segmentation or over-segmentation due to their localized processing nature. In contrast, Transformer-based models offer a promising alternative. They excel in capturing wide-ranging, non-local interac-tions, potentially resolving CNNs' inherent limitations. These models are adept at identifying and assimilating features from distant but visually similar re-gions, thus improving network feature discrimination. However, their effective-ness largely hinges on having access to extensive annotated datasets\\u2014a condition seldom met in medical imaging due to the limited availability of labeled data. Thus, developing methods to efficiently train Transformer-based models with a limited set of annotated data remains a formidable challenge in the area of US image segmentation.\nThis paper proposes a novel framework, the Dual-Student and Teacher Com-bining CNN and Transformer (DSTCT). Our approach uses a dual-student con-figuration, employing cross-supervision with hard pseudo labels to expand the training dataset. Additionally, our Consistency Learning with Soft Pseudo La-bels (CLS) strategy aims to mitigate label noise and foster entropy minimization. Given the stark differences between CNN and Transformer models, we introduce"}, {"title": "2 Method", "content": "For general semi-supervised learning, the training set is composed of a labeled dataset Dy with N labeled images and an unlabeled dataset D_M with M (M >> N) raw images, and the full training dataset is denoted as D_{N+M} = D_N \\cup D_M. For an image X \\in D_N, its ground truth Y is available. But the ground truth is not provided for X \\in D_M. P_{s1} and P_{s2} are the probability outputs derived from student1 and student2, respectively. P_1^* is student1's soft pseudo labels.\nAn overview of the proposed DSTCT architecture is shown in Fig.1. It com-prises a dual-student model and a single-teacher model. The dual-student model consists of a CNN model UNet (namely student1) and a Transformer model Swin-UNet (SUNet, namely student2), while the single-teacher model uses the same Transformer model SUNet. In particular, our DSTCT tackles the semi-supervised image segmentation from five various aspects: supervised learning (L_{sup}), cross-supervision with hard pseudo labels (L_h), consistency learning with soft pseudo labels (L_s), minimization of classifier determinacy discrepancy (L_{cdd}) and consistency regularization constraints from the teacher model (L_{cr}). Therefore, the overall training loss function for student1 or student2 can be defined as:\n$L_{total} = L_{sup} + \\alpha L_h + \\beta L_s + \\gamma L_{cdd} + \\mu L_{cr}$\t\t\t(1)\nwhere \\alpha, \\beta, \\gamma and \\mu are trade-off weights, which were set \\alpha = 0.5, \\beta = 1.0, \\gamma = 3.0, and \\mu = 0.1 [18], respectively, in the proposed cooperative training process. Note: L_{cdd} is a common part of both students.\nSupervised learning (L_{sup}). We use the labeled data to train the student models. Cross-entropy loss L_{ce} and Dice loss L_{dice} are used as follows:\n$L_{sup} = \\frac{1}{2} \\sum_{X \\in D_N} (L_{ce}(P_{s1}, Y) + L_{dice}(P_{s1}, Y))$\t\t\t(2)\nCross-supervision with hard pseudo labels (L_h). The predictions between the CNN and ViT have different properties, essentially in the output level. Based"}, {"title": "Consistency learning with soft pseudo labels (L5)", "content": "However, the hard pseudo labels generated by the maximum confidence are inevitably noisy, which may cause confusion bias during the segmentation training. To further reduce the noise of the hard pseudo labels and focus on unlabeled challenging regions, a sharpening function [9,20] is utilized to generate soft pseudo labels, which can decrease the prediction uncertainty of the models. Soft pseudo labels can be obtained as follows:\n$P_{s1}^* = \\frac{(P_{s2})^{1/\\tau}}{(P_{s2})^{1/\\tau} + (1 - P_{s2})^{1/\\tau}}$\t\t\t(4)\nwhere \\tau is a hyper-parameter to control the temperature of sharpening, and is set to 0.1 in our experiment [19]. Consistency learning is performed between the probability output of one model and the soft pseudo label of the other. The final loss function of consistency learning with soft pseudo labels is:\n$L_s = \\sum_{X \\in D_{N+M}} E[P_{s1}, P_{s1}^*]$\t\t\t(5)"}, {"title": "Classifier Determinacy Disparity (Lcdd)", "content": "The diversity caused by structural differences between CNN and ViT may compromise the model's performance. Directly aligning the discrepancy between the predictions is somewhat not con-fident. Therefore, we investigate classifier discrepancy by Bi-classifier Prediction Relevance Matrix: A = P_{s1}P_{s2}^T [8]. The sum of the diagonal elements in matrix A represents the consistency of the classifier's predictions, while the off-diagonal elements reflect the uncertainty of the predictions. We aim to maximize the for-mer and minimize the latter. The minimizing classifier determinacy discrepancy loss is defined as follows:\n$L_{cdd} = \\sum_{X \\in D_{N+M}} \\sum_{m,n=1}^{C} [\\sum_{n=1}^{C} A^{m,n} - \\sum_{m=1}^{C} A^{m,m}]$\t\t\t(6)\nwhere A^{m,n} denotes the element in the m-th row and n-th column, and C is the number of categories."}, {"title": "Consistency Regularization (Lcr)", "content": "The teacher architecture aims to mini-mize discrepancies between the predictions of the dual-student networks and the teacher network under both data and network perturbations. The consistency loss between the output probabilities of them is defined as follows:\n$L_{cr} = \\sum_{X \\in D_M} E[f_t(X; \\theta; \\sigma'), f_{s1}(X; \\varphi; \\sigma)]$\t\t\t(7)\nwhere \\sigma and \\sigma' represent different data perturbations and random dropout op-erations at the network layer, and \\varphi and \\theta represent the network parameter of student1 and teacher. \\theta is updated via exponential moving average (EMA) from student2's network. E is the MSE loss."}, {"title": "3 Experiments and Results", "content": null}, {"title": "3.1 Dataset and Implementations", "content": "Dataset. This study leverages a dataset sourced from the MICCAI 2023 Grand Challenge [10], focused on the segmentation of the pubic symphysis (PS) and fetal head (FH). The comprehensive dataset includes 5,101 images, which have been methodically partitioned into training (70%), validation (10%), and testing (20%) subsets. Each image is paired with a precise segmentation mask delineat-ing the FH and the PS, enabling the effective development and evaluation of pertinent segmentation models.\nImplementation details. In this study, a compact version of ViT, pre-initialized with ImageNet weights is used for Swin-UNet (SUNet) architecture. The im-plementation was implemented on an Ubuntu 20.04 operating system, utiliz-ing Python 3.8, PyTorch 1.10, and CUDA 11.3. A Tesla T4 GPU facilitated the computations. The network training regimen encompassed 30,000 iterations, employing the Stochastic Gradient Descent (SGD) optimizer, configured with"}, {"title": "3.2 Comparison with Other Methods", "content": "Table 1 presents the quantitative outcomes on the PSFH dataset when trained with 20% of the total labeled training data. Notably, our proposed method sig-nificantly outperforms existing state-of-the-art approaches across most evalua-tion metrics. Remarkably, with only 20% labeled data for training, our DSTCT achieves 89.3% DSC performance, only 6% inferior to the upper bound perfor-mance."}, {"title": "3.3 Ablation Study", "content": "Training strategy analysis. Each component of our DSTCT contributes dif-ferently to the enhancement of semi-supervised learning. From Table 2, it can be seen that each of the components and the combination of them can improve the PSFH segmentation performance, thus demonstrating the effectiveness of our method. Specifically, CDD and CR improve the DSC performance by 0.8% and 0.6%, respectively. By combining CLS, CDD and CR, the DSC performance is improved by 2.1%, the ASD is decreased by 0.626 mm, and the HD95 is de-creased by 3.767 mm.\nDifferent combination of Transformer and CNN. As illustrated in Table 2, The first row represents the combination of models proposed by the DSTCT framework, and its result is the best. While collaboratively training the SUNet-SUNet-SUNet or UNet-UNet-UNet models, the performance is relatively inferior to the UNet-SUNet-SUNet models, indicating the effectiveness of the comple-mentary of these two models."}, {"title": "4 Conclusion", "content": "In this study, we introduce a novel framework termed Dual-Student and Teacher Combining CNN and Transformer (DSTCT), designed to leverage the distinct inherent characteristics of CNN and ViT models through a synergistic train-ing approach. The DSTCT framework intricately combines three key compo-nents: Consistency Learning with Soft Pseudo Labels (CLS), minimizing Clas-sifier Determinacy Discrepancy (CDD), and Consistency Regularization (CR) through mean teacher architecture. This comprehensive integration aims to bol-ster the generalization capabilities of CNNs. Empirical evaluations conducted on a widely recognized benchmark indicate that the DSTCT framework signif-icantly enhances the performance of CNN architecture, outstripping competing state-of-the-art methods by a substantial margin. Furthermore, this study cat-alyzes advancing the application of Transformer models within the realm of semi-supervised image segmentation tasks, encouraging further research and development in this promising area. The PSFHS Challenge of MICCAI 2023 and the IUGC Challenge of MICCAI 2024 are available at https://ps-fh-aop-2023.grand-challenge.org/ and https://codalab.lisn.upsaclay.fr/competitions/18413, respec-tively."}]}