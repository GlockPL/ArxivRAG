{"title": "Evaluating Explanations Through LLMs: Beyond Traditional User Studies", "authors": ["Francesco Bombassei De Bona", "Gabriele Dominici", "Tim Miller", "Marc Langheinrich", "Martin Gjoreski"], "abstract": "As AI becomes fundamental in sectors like healthcare, explainable AI (XAI) tools are essential for trust and transparency. However, traditional user studies used to evaluate these tools are often costly, time consuming, and difficult to scale. In this paper, we explore the use of Large Language Models (LLMs) to replicate human participants to help streamline XAI evaluation. We reproduce a user study comparing counterfactual and causal explanations, replicating human participants with seven LLMs under various settings. Our results show that (i) LLMs can replicate most conclusions from the original study, (ii) different LLMs yield varying levels of alignment in the results, and (iii) experimental factors such as LLM memory and output variability affect alignment with human responses. These initial findings suggest that LLMs could provide a scalable and cost-effective way to simplify qualitative XAI evaluation.", "sections": [{"title": "1 Introduction", "content": "As artificial intelligence (AI) becomes integrated into critical sectors such as healthcare [1-3], the adoption of explainable AI (XAI) becomes inevitable [4-6]. For example, AI models can help diagnose diseases [7], predict patient outcomes [8], and recommend treatments [9]. The decisions of these models are often opaque, making it difficult for practitioners to fully trust or understand them. Therefore, XAI tools can have a huge impact in the integration of AI in healthcare. This necessity is also highlighted by regulatory efforts such as the EU AI Act [10], which enforces transparency and accountability in AI systems, particularly in critical sectors, where understanding AI-driven decisions can mean the difference between life and death.\nThis need for effective XAI tools has led to a significant number of studies aimed at advancing the field [11]. Many of these efforts have focused mainly on developing new techniques and algorithms [12-17] to explain models and evaluate them through quantitative metrics. However, this approach holds significant challenges, as there are no clear and unique metrics (e.g., surrugate model fidelity [12], counterfactual validity [17], and proximity score [18]) to evaluate these tools. The choice of metrics is often highly dependent on the specific XAI technique and the domain of application, and these metrics frequently fail to capture the actual benefits from the end-user's perspective. As a result, many tools are optimized to maximize performance in these quantitative metrics, ignoring the ultimate goal of providing explanations that help users understand the model's decisions [19]. In contrast, fewer studies involve qualitative evaluations in which users assess key properties such as the effectiveness, helpfulness, and trustworthiness of the explanations [20-24]. Furthermore, there is no standardized process for structuring these evaluations, leading to inconsistencies in the"}, {"title": "2 Traditional user studies to evaluate XAI tools", "content": "In the evaluation of XAI tools, particularly within healthcare, user studies are considered the gold standard for assessing how well these tools perform in real-world scenarios [29]. Typically, these studies involve a structured process in which healthcare professionals or end users interact with XAI tools under controlled conditions. In previous user studies that evaluated XAI tools [20-23], participants were assigned with activities such as predicting the output of the AI model based on AI-generated explanations and/or completing questionnaires about the perceived usefulness of the explanations and the degree of trust in the model."}, {"title": "3 Can LLMs evaluate XAI tools?", "content": "LLMs are advanced AI systems designed and trained to process text data and generate human-like text based on vast amounts of data, covering a wide range of topics. This training enables them to estimate context, generate coherent answers, and mimic human-like reasoning in their responses. LLMs excel in tasks that require the comprehension and generation of natural language, making them particularly effective in simulating human interactions [31], such as those involved in user studies. LLMs have also demonstrated significant performance in tasks beyond their original training without the need for additional fine-tuning. For example, LLMs can classify various data types, such as tabular data [32] and time series [33], or generate synthetic data [34]. Although they may not yet represent the state-of-the-art models for these tasks (with specialized models often being more capable), they offer valuable versatility. This is especially relevant when dealing with niche domains like healthcare, where specialized models are typically preferred for specific tasks due to their superior and tailored capabilities. However, LLMs can offer additional capabilities in conjunction with specialized models and tools, leveraging their human-like conversational abilities and contextual understanding.\nTo exploit these abilities in conjunction with specialized models and XAI tools, we propose using LLMs to qualitatively evaluate XAI tools, simulating human-based studies. Querying LLMs instead of humans offers several significant advantages. Firstly, LLMs provide a cost-effective alternative, eliminating the need for expensive and time-consuming recruitment and compensation of human participants. This allows for the rapid and inexpensive execution of studies that can be conducted repeatedly with unlimited queries, leading to large-scale data collection and analysis. In general,"}, {"title": "4 Experiments", "content": "We designed our experiments to explore whether it is feasible to estimate human preferences in XAI user studies using LLMs. More specifically, we tested for general alignment (i.e., can we arrive to the same findings/conclusions on a population/study level) and absolute alignment (i.e., can we come to similar responses on a case-by-case level) between our LLM-based study and an existing use-based study. Additionally, we aim to explore the impact of different factors that might influence the outcomes of such studies when using LLMs. To achieve this, we address the following research questions:\n\u2022 Alignment: Is it possible to replicate the results of an XAI user study using LLMs? Are LLMs answers aligned with human preferences in the context of evaluating explanations in absolute terms?\n\u2022 LLM Variability: Do different LLMs lead to different general alignment?\n\u2022 Framework Variability: Does the use of memory in LLMs influence the results of the user study? How does variability in LLM responses impact the general alignment?"}, {"title": "4.1 Evaluation Setting", "content": ""}, {"title": "4.1.1 User study", "content": "As a foundation for the experimental setting, we use the first set of experiments of the user study by Celar and Byrne [22]. Their study was designed to better understand the relationship between causal and counterfactual explanations in terms of their helpfulness and effectiveness in transmitting insights from AI systems to end users. Participants interacted with the predictive AI system's input and output, along with explanations from an XAI tool providing insights into the system's decision-making process. The study featured four experimental conditions: counterfactual versus causal explanations in high versus low familiarity scenarios. Counterfactual explanations provide alternative scenarios by"}, {"title": "4.1.2 Estimating human preferences with LLMs", "content": "To replicate the above-mentioned user study [22], we transpose the experimental setting designed for human evaluators into a compatible setting for LLMs. In this context, each LLM is treated as a participant, tasked with generating responses across the same experimental conditions (high/low familiarity and causal/counterfactual explanations).\nLLM models are used to generate responses in place of human participants. A run corresponds to the execution of one experiment formed by the helpfulness, prediction, and confidence tasks. A state refers to the specific conditions under which the model generates responses, such as the combination of familiarity and explanation type for each task. We use two approaches to aggregate and compare the results across different models and runs. In the first approach, we conduct multiple inference runs for each model and calculate the mean response at each question. This method, similar to the self-consistency technique, helps to mitigate the variability in the generated outputs and produces a more stable, reliable average response for each experimental condition. By averaging across multiple runs, we reduce the impact of any outlier responses and ensure consistency in the model's performance. In the second approach, we treat each inference run as if it were generated by a different participant. This method simulates the diversity typically seen in a group of human participants. By treating each run independently, we capture the variability that may arise in model-generated responses, mirroring how individual differences exist in human participants.\nIn addition, the use of instruction-following models enables us to explore the influence of conversation history during task execution. We assume that while performing the tasks, users undergo a learning process. Thus, to replicate the same process, we enable the LLM to use previously generated answers and instructions as context for every new inference. We test this scenario against a baseline where the models do not have access to any previously generated answers and inputs, and every inference is treated as a separate task. We refer to these two scenarios as \"with memory\" and \"in isolation\", respectively. The isolation setting poses two challenges. The first issue arises in the third task, where the LLM is asked to provide a confidence level about the prediction made in the previous task. However, if each inference is treated without memory, the LLM has no information about the previous task or the answer given. As a result, it cannot express a confidence level about its performance. To address this issue, we switch to a hybrid approach regarding memory by allowing the second and third tasks to be completed in pairs and enabling the conversation history between the two tasks. The second issue involves the assumption that the users, while performing the first task learn how to make the predictions in the second task. This assumption is violated due to the lack of presence of the conversation memory. To address this issue we apply a few-shot prompting technique. This method uses synthetic data of input-output examples to instruct the LLM on what answer is expected and"}, {"title": "4.1.3 Metrics and statistical tests", "content": "Our primary objective is to compare the results obtained from LLMs with those from the original user study. To ensure consistency, we use the same statistical metrics and tests as those proposed in the original user study [22].\nThe original study compares four experimental conditions: low familiarity with causal explanations, low familiarity with counterfactual explanations, high familiarity with causal explanations, and high familiarity with counterfactual explanations. The analysis focuses on the mean responses provided by participants, aiming to show that high familiarity scenarios lead to better outcomes compared to low familiarity ones and that counterfactual explanations are generally more helpful and insightful than causal explanations. We replicate this approach by calculating the mean values for the LLM-generated responses in each condition and comparing the outcomes across the four experimental scenarios. This allows us to get information about the absolute alignment between LLMs and humans.\nHowever, the main conclusion of the paper are drawn on statistical tests regarding the effect of the two primary variables - familiarity (high vs. low) and explanation type (causal vs. counterfactual) and their interaction. Therefore, we apply a two-way ANOVA statistical test [39], just as in the original study, to assess whether these factors significantly influence the responses of LLMs, as they do for human participants (general alignment). This method allows us to investigate whether LLMs exhibit similar patterns of reasoning and judgment, even if they do not present an absolute alignment. Since we aim to assess the alignment between LLMs and the user study, we assume that the answer distributions match in shape and, therefore, we test for normality of the LLMs' answers. The ANOVA test assumptions are only partially satisfied for certain models, specifically Mistral-Nemo-Instruct-2407, Mistral-7B-Instruct-v0.3, and GPT-40-Mini. As a result, we exercise caution and refrain from drawing strong conclusions based on the statistical outcomes for these models.\nBy aligning our evaluation techniques with those used in the original work, we aim to determine how well LLMs replicate human reasoning processes and whether they demonstrate the same preferences and patterns when presented with familiar or unfamiliar scenarios, as well as causal or counterfactual explanations."}, {"title": "4.2 Results", "content": "Evaluating XAI tools with LLMs partially mirrors user study conclusions. (Figure 2) Using the results of the Qwen 2 72B model (sampling the same number of \u201cLLM-users\u201d as participants in the original user study), we successfully replicated 6 of the 9 statistical outcomes from the first part of the user study. Figure 2 illustrates the concordance between the LLM results and those of the original human-based study for each statistical test. This shows that, under specific conditions, partial alignment between human and LLM conclusions can be achieved when replacing human participants with LLMs. Specifically, we observed perfect general alignment in tasks that require the prediction of the model output given an explanation. However, alignment was more challenging in tasks that involved confidence-related questions, where the LLM struggled to match human responses."}, {"title": "LLMs exhibit slight differences in absolute alignment with human preferences. (Figure 3)", "content": "Although LLMs can replicate overall trends in user studies, their responses still show some deviations from human participants. Figure 3 presents the MSE of the Qwen 2 72B model across different categories (helpfulness, accuracy, confidence) in both familiar and unfamiliar conditions under causal and counterfactual settings. While the MSE for helpfulness is relatively low, particularly in familiar contexts, the model struggles more with accuracy, especially in unfamiliar settings. Confidence shows the largest errors, mainly in unfamiliar conditions. These results suggest that, although the model's absolute predictions differ from human results, its comparative judgments remain consistent, showing potential as an alternative in user evaluations across different scenarios."}, {"title": "Different LLMs exhibit varying levels of general alignment with human responses. (Figure 2)", "content": "The degree of general alignment between LLMs and human participants varies between models, reflecting differences in size, capabilities, and behaviors. Figure 2 shows that some LLMs align more closely with human judgments in specific tasks, while others diverge. Among the tested models, Qwen 2 72B and GPT-40 Mini achieved the highest general alignment, matching human conclusions in 6 out of 9 cases. Interestingly, the smaller LLaMA 8B demonstrated better general alignment than its larger counterpart, LLaMA 70B. In contrast, the largest Qwen 2 model (72B) significantly outperformed the smaller Qwen 2 7B, indicating that a larger model does not necessarily guarantee better alignment with human responses across all architectures. Additionally, all models showed their best performance on the prediction tasks, while they performed worst in the confidence-related"}, {"title": "Usage of memory impacts LLMs' general alignment. (Figure 4)", "content": "Figure 4 illustrates the concordance between the results of the LLMs and those of the original human-based study in different settings, specifically comparing models with and without memory use. LLMs that utilize memory behave differently from those that do not. Generally, LLMs without memory exhibit more uniform performance across all tests, likely due to the absence of influence from the prior context. In contrast, the use of memory introduces variability, as the model's responses are affected by the way it interprets and incorporates information from previous interactions. Despite this variability, models that employed memory tended to perform better overall, showing higher concordance with human judgments. This suggests that memory, when used effectively, can enhance an LLM's ability to align with human responses. However, this benefit depends on how well the memory mechanism is utilized."}, {"title": "Simulating different users through aggregation leads to opposite results. (Figure 4)", "content": "Evaluating the impact of aggregation allows us to better assess the impact of LLM generation variability. Figure 4 compares the outcomes of different models using both aggregated and non-aggregated methods. The results reveal that prior to aggregation, different LLMs exhibit varying degrees of initial variability,"}, {"title": "5 Limitations", "content": "One significant limitation of our research is that it is based on a single publicly available user study, which focused on evaluating explanations generated by two similar XAI methods. The use of only two explanation techniques in a single user study limits the breadth of the conclusions we can draw. XAI encompasses a wide variety of techniques and applications across different domains, and our findings may not generalize to all types of explanations or contexts. Moreover, our approach cannot replicate certain types of human studies, such as qualitative interviews or assessments that measure real trust, particularly in expert-driven fields like healthcare, where the trust of clinicians is crucial. Similarly, this approach would struggle to handle entirely new domains that fall outside the LLM's training set, as the models rely on prior knowledge to generate responses. However, this study highlights an interesting path for further exploration. We plan to extend the experimental settings in future work to include a broader range of XAI techniques and user studies. Additionally, we aim to incorporate queries to Vision Language Models (VLMs) to evaluate visually oriented XAI techniques, such as saliency maps, which are important in the healthcare field.\nAnother limitation is the possibility that the tasks or responses from the original Celar and Byrne [22] study may be part of the LLM training set. This could introduce bias and compromise the validity of the results. Ensuring that the LLM responses are genuinely independent of the study's prior knowledge will be critical in addressing this limitation.\nLastly, a limitation lies in the specific LLMs used in this study. Although we used up-to-date LLMs at the time of evaluation, the rapid pace of advancements in AI technology, especially in LLMs, means that future models may exhibit different behavior, reasoning abilities, or alignment capabilities. Similarly, improvements in alignment techniques could lead to better alignment with human preferences, potentially altering the conclusions drawn in this study. As a result, future research will need to inspect LLM performance again as new models and alignment methods emerge. Nonetheless, this paper serves as an illustration of this idea, providing promising results and laying the groundwork for further investigation."}, {"title": "6 Conclusions", "content": "In conclusion, our investigation into the use of Large Language Models (LLMs) to complement and integrate user studies for evaluating Explainable AI (XAI) tools offers promising initial results. By replicating a user study on counterfactual and causal explanations, we found: (i) LLMs can replicate most of the conclusions derived from traditional user studies, indicating their potential as scalable and cost-effective alternatives, (ii) different LLM architectures and capabilities can produce varying outcomes, emphasizing the importance of selecting appropriate models for specific evaluation tasks, and (iii) experimental factors, such as the use of memory and the impact of variability in generating responses, significantly affect the alignment between LLM and human preferences. Our findings suggest LLM-based evaluations could greatly improve the scalability and reproducibility of XAI assessments. Future work should aim to enhance the alignment of LLMs with human judgment in the evaluation of XAI tools and explore the broader applicability of this approach across various XAI techniques and domains."}, {"title": "A Implementation details", "content": "Running local and remote models We executed Hugging Face models (Mistral, Llama 3.1, and Qwen2 families) on a local server equipped with the following hardware:\n\u2022 CPU: 2 x AMD EPYC 7513 32-Core Processor\n\u2022 RAM: 512 GB\n\u2022 GPU: 4 x RTX A6000 (48 GB VRAM each)\nCUDA acceleration was utilized to parallelize and distribute computation across the GPUs, significantly speeding up the processing.\nThe total inference time for running the full experiment was approximately 76 hours.\nFor GPT 40 Mini, inference was run using the OpenAI API. Inference time depends on the usage Tier available on the API.\nCode and licenses Our code implementation is built using Python 3.12 and leverages the open-source library LangChain [40] (MIT License) to develop the inference infrastructure for both local and remote executions. All plots were generated using the Matplotlib [41] (BSD License) library. The dataset used in the experiment is freely available, following the guidelines provided in the original paper [22]."}]}