{"title": "Nearly Optimal Sample Complexity of Offline KL-Regularized Contextual Bandits under Single-Policy Concentrability", "authors": ["Qingyue Zhao", "Kaixuan Ji", "Heyang Zhao", "Tong Zhang", "Quanquan Gu"], "abstract": "KL-regularized policy optimization has become a workhorse in learning-based decision making, while its theoretical understanding is still very limited. Although recent progress has been made towards settling the sample complexity of KL-regularized contextual bandits, existing sample complexity bounds are either \\( \\tilde{O}(\\epsilon^{-2}) \\) under single-policy concentrability or \\( O(\\epsilon^{-1}) \\) under all-policy concentrability. In this paper, we propose the first algorithm with \\( O(\\epsilon^{-1}) \\) sample complexity under single-policy concentrability for offline contextual bandits. Our algorithm is designed for general function approximation and based on the principle of pessimism in the face of uncertainty. The core of our proof leverages the strong convexity of the KL regularization, and the conditional non-negativity of the gap between the true reward and its pessimistic estimator to refine a mean-value-type risk upper bound to its extreme. This in turn leads to a novel covariance-based analysis, effectively bypassing the need for uniform control over the discrepancy between any two functions in the function class. The near-optimality of our algorithm is demonstrated by an \\( \\Omega(\\epsilon^{-1}) \\) lower bound. Furthermore, we extend our algorithm to contextual dueling bandits and achieve a similar nearly optimal sample complexity.", "sections": [{"title": "Introduction", "content": "Due to the data-hungry and instable nature of reinforcement learning (RL), divergences that are straightforward to estimate via Monte Carlo methods or amenable to constrained optimization stand out from numerous candidates (R\u00e9nyi, 1961; M\u00fcller, 1997; Basseville, 2013) as regularizers; the former family of which is typically f-divergence because any of them is an expectation under one policy (thus easy for on-policy estimation) (Levine, 2018; Levine et al., 2020), and the latter are\n\\[\nJ(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi} [r_{\\tau}] - \\frac{1}{\\eta} KL(\\pi || \\pi^{\\text{ref}}),\n\\]\nwhere r is a reward function, \\( \\pi \\) is the policy to be optimized, \\( \\pi^{\\text{ref}} \\) is a reference policy, \\( KL(\\pi || \\pi^{\\text{ref}}) \\) is the reverse KL divergence, and \\( \\eta > 0 \\) is the inverse temperature. When \\( \\pi^{\\text{ref}} \\) is chosen to be the uniform distribution, the objective function reduces to the (Shannon) entropy-regularized RL objective that encourages diverse actions and enhances robustness (Williams, 1992; Ziebart et al., 2008; Ziebart, 2010; Levine and Koltun, 2013; Fox et al., 2015; Levine et al., 2016; Haarnoja et al., 2018; Richemond et al., 2024; Liu et al., 2024). The KL-regularized objective has also been widely used in the RL fine-tuning of large language models (LLMs) (Ouyang et al., 2022; Rafailov et al., 2023, 2024), where \\( \\pi^{\\text{ref}} \\) is a pretrained LLM.\nThere has also been a surge of interest in understanding the principle behind KL-regularized RL. Ahmed et al. (2019); Liu et al. (2019) studied by ablation the effect of entropy regularization on the stability and policy improvement in policy optimization, the regret of which has been rigorously settled under the classic online mirror descent framework (Cai et al., 2020; He et al., 2022; Ji et al., 2023). Neu et al. (2017) unified popular KL-regularized policy optimization algorithms under a convex optimization framework, but the interplay of which with data were left untouched. A series of work (Geist et al., 2019; Vieillard et al., 2020; Kozuno et al., 2022) then analyzed the sample complexity of algorithms using KL w.r.t. the previous iteration or/and entropy regularizers with improved dependence on the effective horizon in discounted Markov decision processes (MDPs). However, the metric of suboptimality in all these studies is still with respect to the unregularized reward maximization objective, and its optimal sample complexity is \\( O(\\epsilon^{-2}) \\) for finding the \\( \\epsilon \\)-optimal policy.\nSeveral recent studies (Xiong et al., 2024; Xie et al., 2024; Zhao et al., 2024) switched the focus to suboptimality guarantee with respect to the regularized objective in (1.1). In particular, Xiong et al. (2024) proposed an Offline GSHF algorithm via the principle of pessimism in the face of uncertainty, and proved \\( O(\\epsilon^{-2}) \\) sample complexity under single-policy concentrability (See Section 2.1 for detailed definitions of concentrability.). On the other hand, the sharp analysis in Zhao et al. (2024) yields the optimal sample complexity \\( O(\\epsilon^{-1}) \\), but requires all-policy concentrability (Zhao et al., 2024, Definition 2.6), i.e., the data-generating policy \\( \\pi^{\\text{ref}} \\) is required to cover the entire function class for all possible policies. Xie et al. (2024) studied token-level Markov decision processes (MDPs) and proposed a KL-regularized RL algorithm named XPO, which achieves \\( O(\\epsilon^{-2}) \\) sample complexity under all-policy concentrability. In fact, even for offline contextual bandits (Foster and Rakhlin, 2023, Chapter 3) with i.i.d. data, which is among the simplest setting for offline decision making, existing sample complexity under KL-regularization are still primitive. Thus, a natural question arises:\nIs the \\( \\tilde{O}(\\epsilon^{-1}) \\) sample complexity w.r.t. (1.1) achievable for offline KL-regularized contextual bandits under single-policy concentrability?"}, {"title": "1.1 Contributions", "content": "\u2022 We propose a new algorithm dubbed KL-PCB and develop a novel proof technique that simultaneously utilizes the curvature of KL-regularized objectives and integrates pessimism with a novel covariance-based argument, for deriving its \\( O(\\epsilon^{-1}) \\) sample complexity under single-policy concentrability.\n\u2022 To demonstrate the versatility of our techniques beyond absolute reward feedback, we also extend our results to the offline contextual dueling bandit (CDB) setting, and provide a tight sample complexity analysis.\n\u2022 The matching lower bounds (up to polylog factors) in both settings suggest that our sample complexity results are nearly optimal, which renders a complete picture of the sample complexity for KL-regularized offline contextual under single policy concentrability.\nFor the ease of comparison, we summarize the relevant sample complexity results along with the corresponding coverage assumptions in Table 1."}, {"title": "1.2 Additional Related Work", "content": "We review two additional lines of theoretical progress that are closely related to our algorithm design and analysis."}, {"title": "2 KL-Regularized Contextual Bandits", "content": "In this section, we propose a pessimism-based algorithm, dubbed PCB-KL, for offline KL-regularized contextual bandits. In the following subsections, we also showcase our key technical novelty which couples the pessimism of our reward estimator to the non-trivial curvature property of KL regularization."}, {"title": "2.1 Problem Setup", "content": "We consider contextual bandit, which is denoted by a tuple \\( (\\mathcal{S}, \\mathcal{A}, r, \\pi^{\\text{ref}}) \\). Specifically, \\( \\mathcal{S} \\) is the context space, \\( \\mathcal{A} \\) is the action space and \\( r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1] \\) is the reward function. In the offline setting, the agent only has access to an i.i.d. dataset \\( \\mathcal{D} = \\{(s_i, a_i, r_i)\\}_{i=1}^n \\). Here \\( s_i \\in \\mathcal{S} \\) is the state sampled from a distribution \\( \\rho \\), \\( a_i \\in \\mathcal{A} \\) is the action taken from a behavior policy, and \\( r_i \\) is the observed reward given by \\( r_i = r(s_i, a_i) + \\varepsilon_i \\), where \\( \\varepsilon_t \\) is 1-sub-Gaussian (Lattimore and Szepesv\u00e1ri, 2020, Definition 5.2). In this work, we consider the KL-regularized objective\n\\[\nJ(\\pi) := \\mathbb{E}_{(\\mathbf{s}, \\mathbf{a}) \\sim \\rho \\times \\pi} \\left[r(\\mathbf{s}, \\mathbf{a}) - \\frac{1}{\\eta} \\log \\frac{\\pi(\\mathbf{a} | \\mathbf{s})}{\\pi^{\\text{ref}}(\\mathbf{a} | \\mathbf{s})} \\right],\n\\]\nwhere \\( \\pi^{\\text{ref}} \\) is a known reference policy and the \u201cinverse temperature\u201d \\( \\eta \\) controls the intensity of regularization. For simplicity, we assume that \\( \\pi^{\\text{ref}} \\) is also the behavior policy that generates the dataset \\( \\mathcal{D} \\). This type of \u201cbehavior regularization\" has been also been studied in Zhan et al. (2022). The unique optimal policy \\( \\pi^* := \\arg\\max_{\\pi \\in \\Delta(\\mathcal{A} \\vert \\mathcal{S})} J(\\pi) \\) is in the following closed-form (See, e.g., Zhang 2023, Proposition 7.16)\n\\[\n\\pi^* (\\cdot | s) \\propto \\exp (\\eta \\cdot r(s, \\cdot)), \\forall s \\in \\mathcal{S}.\n\\]\nA policy \\( \\pi \\) is said to be \\( \\epsilon \\)-optimal if \\( J(\\pi^*) - J(\\pi) \\le \\epsilon \\) and the goal of the agent is to find one such policy based on \\( \\mathcal{D} \\). To ensure \\( \\epsilon \\)-optimality is achievable, we assume that \\( r \\) lies in a known function class \\( \\mathcal{F} \\subset (\\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]) \\), from which the agent obtains her estimator \\( f \\).\nAssumption 2.1 (General Function Approximation). There exists a known function class \\( \\mathcal{F} : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1] \\) such that \\( \\exists f^* \\in \\mathcal{F} \\) such that \\( f^* = r \\).\nWe also need a standard condition to control the complexity of \\( \\mathcal{F} \\) through the notion of covering number (Wainwright, 2019, Definition 5.1).\nDefinition 2.2 (\\( \\epsilon \\)-net and covering number). Given a function class \\( \\mathcal{G} \\subset (\\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}) \\), a finite set \\( \\mathcal{G}(\\epsilon) \\subset \\mathcal{G} \\) is an \\( \\epsilon \\)-net of \\( \\mathcal{G} \\) w.r.t. \\( || \\cdot ||_\\infty \\), if for any \\( g \\in \\mathcal{G} \\), there exists \\( g' \\in \\mathcal{G}(\\epsilon) \\) such that \\( ||g - g'||_\\infty \\le \\epsilon \\). The \\( \\epsilon \\)-covering number is the smallest cardinality \\( N_{\\mathcal{G}}(\\epsilon) \\) of such \\( \\mathcal{G}(\\epsilon) \\).\nAssumption 2.3. For any \\( \\epsilon_c > 0 \\), the \\( \\epsilon_c \\)-covering number \\( N_{\\mathcal{F}}(\\epsilon_c) \\) of \\( \\mathcal{F} \\) is \\( \\text{poly}(\\epsilon_c^{-1}) \\).\nRemark 2.4. When \\( \\mathcal{F} \\) is the class of linear functions of dimension \\( d \\) and radius \\( R \\), the covering number of \\( \\mathcal{F} \\) is given by \\( N_{\\mathcal{F}} = O((1 + R\\epsilon^{-1})^d) \\) (Jin et al., 2020, Lemma D.6), indicating that the Assumption 2.3 on \\( N_{\\mathcal{F}} \\) is mild."}, {"title": "Concentrability", "content": "The data quality of \\( \\mathcal{D} \\) collected by \\( \\pi^{\\text{ref}} \\) is typically characterized by concentrability in offline RL (Farahmand et al., 2010; Chen and Jiang, 2019; Jiang and Xie, 2024). In the traditional definition based on density ratio (See, e.g., Zhan et al. 2022, Table 1), a strong concentrability condition intuitively corresponds to a large \\( \\text{supp}(\\pi^{\\text{ref}}) \\), which reduces the agent's difficulty of learning since (with high probability) can hardly underfit on many state-action pairs. The two extreme cases of concentrability coefficients, all- and single-policy concentrability, intuitively correspond to (1) \\( \\text{supp}(\\pi^{\\text{ref}}) \\) covers all possible inputs; and (2) \\( \\text{supp}(\\pi^{\\text{ref}}) \\) only subsumes \\( \\text{supp}(\\pi^*) \\). Our definition of concentrability, however, is different from the density ratio-based one, because we take the function class \\( \\mathcal{F} \\) into account. In detail, we first introduce \\( D^2 \\)-divergence as follows.\nDefinition 2.5. Given a function class \\( \\mathcal{F} \\subset (\\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}) \\) and a fixed policy \\( \\pi \\), define the \\( D^2 \\)-divergence \\( D_{\\mathcal{F}}^2((\\mathbf{s}, \\mathbf{a}); \\pi) \\) as\n\\[\nD_{\\mathcal{F}}^2((\\mathbf{s}, \\mathbf{a}); \\pi) := \\sup_{f,g \\in \\mathcal{F}} \\frac{(f(s, a) - g(s, a))^2}{\\mathbb{E}_{(\\mathbf{s}', \\mathbf{a}') \\sim \\rho \\times \\pi}[(f(s', a') - g(s', a'))^2]}.\n\\]\nThe \"Eluder dimension\"-type Definition 2.5 is directly inspired by Di et al. (2023); Zhao et al. (2024), the intuition behind which is that given \\( (s, a) \\in \\mathcal{S} \\times \\mathcal{A} \\), a small \\( D^2 \\)-divergence indicates that for two functions \\( f \\) and \\( g \\), if they are close under the behavior policy \\( \\pi \\), then they will also be close on such pair \\( (s, a) \\). Therefore, the \\( D^2 \\)-divergence quantifies how well the estimation on dataset collected by the behavior policy can be generalized to a specific state-action pair. We are now ready to define the two notions of concentrability conditions.\nAssumption 2.6 (All-Policy Concentrability). Given a reference policy \\( \\pi^{\\text{ref}} \\), there exists \\( D > 0 \\) such that \\( D^2 = \\sup_{(\\mathbf{s}, \\mathbf{a}) \\in \\mathcal{S} \\times \\mathcal{A}} D_{\\mathcal{F}}^2((\\mathbf{s}, \\mathbf{a}); \\pi^{\\text{ref}}) \\).\nAssumption 2.6 indicates that the errors on any state-action pairs can be bounded by the error on the samples from \\( \\rho \\times \\pi \\) up to a factor \\( D \\).\nAssumption 2.7 (Single-Policy Concentrability). Given a reference policy \\( \\pi^{\\text{ref}} \\), there exists \\( D_{\\pi^*} > 0 \\) such that \\( D_{\\pi^*}^2 = \\mathbb{E}_{(\\mathbf{s}, \\mathbf{a}) \\sim \\rho \\times \\pi^*}[D_{\\mathcal{F}}^2((\\mathbf{s}, \\mathbf{a}); \\pi^{\\text{ref}})] \\).\nSingle-policy concentrability indicates that the errors on the distributions of state-action pairs \\( \\rho \\times \\pi \\) can be bounded by the error on the samples from \\( \\rho \\times \\pi^{\\text{ref}} \\) up to some constant. The single-policy concentrability assumption is strictly weaker than the all-policy concentrability in Assumption 2.6."}, {"title": "2.2 Review of Existing Results", "content": "In this subsection, we discuss the direct adaptation of previous results to our setting and demonstrate that their results cannot imply an \\( O(\\epsilon^{-1}) \\) sample complexity without all-policy concentrability. Specifically, Xiong et al. (2024) obtained a performance gap upper bound under linear function approximation\n\\[\nJ(\\pi^*) - J(\\pi) \\le ||\\mathbb{E}_{\\rho \\times \\pi^*} [\\Phi(s, a)] - v||_{\\Sigma_{\\text{off}}^{-1}} =: \\text{RHS},\n\\]\nwhere \\( v \\) is the reference vector, \\( \\Phi(s, a) \\in \\mathbb{R}^d \\) is the feature map, and \\( \\Sigma_{\\text{off}} = \\sum_{i=1}^n \\Phi(s_i, a_i)\\Phi(s_i, a_i)^{\\top} \\) is the sample covariance matrix. However, we can show that RHS can be bounded from below by\n\\[\n||\\mathbb{E}_{(\\mathbf{s}, \\mathbf{a}) \\sim \\rho \\times \\pi^*} [\\Phi(s, a)] - v||_{\\lambda_{\\min}(\\Sigma_{\\text{off}})} \\le ||\\mathbb{E}_{(\\mathbf{s}, \\mathbf{a}) \\sim \\rho \\times \\pi^*} [\\Phi(s, a)] - v||_{\\lambda_{\\max}(\\Sigma_{\\text{off}})}^{-1/2}\n\\]"}, {"title": "2.3 Algorithm", "content": "In this subsection, we present an offline bandit algorithm, KL-PCB, for KL-regularized contextual bandits in Algorithm 1. KL-PCB first leverages least-square estimator to find a function \\( \\hat{f} \\in \\mathcal{F} \\) that minimizes its risk on the offline dataset. In Zhao et al. (2024), such \\( \\hat{f} \\) is directly applied to construct the estimated policy. In contrast, we construct a pessimistic estimator of \\( f^* \\) following the well-known pessimism principle in offline RL (Jin et al., 2021).\nSpecifically, we consider the bonus term defined as\n\\[\n\\Gamma_{\\eta}(s, a) = \\beta D_{\\mathcal{F}}((s, a), \\pi^{\\text{ref}}), \\forall (s, a) \\in \\mathcal{S} \\times \\mathcal{A};\n\\]\nwhere\n\\[\n\\beta = \\sqrt{\\frac{128 \\log (2 N_{\\mathcal{F}}(\\epsilon_c) / \\delta)}{3 n} + 18 \\epsilon_c}.\n\\]\nWe then obtain our pessimistic estimation \\( \\tilde{f} \\) by setting \\( \\tilde{f} = \\hat{f} - \\Gamma_{\\eta} \\), which is less than \\( f^* \\) with high probability. After obtaining the pessimistic estimation, KL-PCB output the policy \\( \\tilde{\\pi} \\), which maximizes the estimated objective\n\\[\n\\tilde{J}(\\pi) = \\mathbb{E}_{(\\mathbf{s}, \\mathbf{a}) \\sim \\rho \\times \\pi} \\left[\\tilde{f}(\\mathbf{s}, \\mathbf{a}) - \\frac{1}{\\eta} \\log \\frac{\\pi(\\mathbf{a} | \\mathbf{s})}{\\pi^{\\text{ref}}(\\mathbf{a} | \\mathbf{s})} \\right],\n\\]\nthe maximizer of which is the counterpart of (2.2), i.e.,\n\\[\n\\tilde{\\pi}(\\cdot | s) \\propto \\pi^{\\text{ref}}(\\cdot | s) \\exp (\\eta \\cdot \\tilde{f}(s, \\cdot)).\n\\]"}, {"title": "2.4 Theoretical Results", "content": "The sample complexity for KL-regularized contextual bandits is settled in this subsection. We first give the upper bound of KL-PCB.\nTheorem 2.8. Under Assumption 2.7, for sufficiently small \\( \\epsilon \\in (0, 1) \\), if we set \\( \\Gamma_{\\eta} \\) as in (2.3), then \\( n = O(nD_{\\pi^*} \\epsilon^{-1}) \\) suffices to guarantee the output policy \\( \\tilde{\\pi} \\) of Algorithm 1 to be \\( \\epsilon \\)-optimal with probability at least \\( 1 - \\delta \\)."}, {"title": "2.5 Proof of Theorem 2.8", "content": "In this section, we finish the proof of Theorem 2.8. At a high level, if we consider the regularized objective (1.1) multi-arm bandits, then \\( P \\rightarrow KL(P||Q) \\) is 1-strongly convex w.r.t. TV (\\( \\cdot \\Vert \\cdot \\)) (Polyanskiy and Wu, 2024, Exercise 1.37), and thus \\( J(\\pi) \\) is strongly concave. Therefore, \\( J(\\pi^*) - J(\\tilde{\\pi}) \\) is possible to be of the order \\( [TV(\\pi^* \\Vert \\tilde{\\pi})]^2 \\approx \\tilde{O}(n^{-1}) \\), pretending that \\( \\pi^* \\) is the unconstrained maximizer. This intuition guides our analysis for contextual bandits.\nWe begin the proof with the definition of the event \\( \\mathcal{E}(\\delta) \\) given \\( \\delta > 0 \\) as\n\\[\n\\mathcal{E}(\\delta) := \\left\\{ \\sup_{(\\mathbf{s}, \\mathbf{a}) \\in \\mathcal{S} \\times \\mathcal{A}} |\\hat{f} - \\tilde{f} - \\Gamma_{\\eta}|(\\mathbf{s}, \\mathbf{a}) \\le 0 \\right\\},\n\\]\nwhere \\( \\Gamma_{\\eta} \\) is defined in (2.3). Event \\( \\mathcal{E}(\\delta) \\) holds indicates that the least square estimation \\( \\hat{f} \\) obtained in Line 1 of Algorithm 1 does not deviate too much from the true function \\( f^* \\). More specifically, we have the following lemma, whose proof are deferred to Please refer to Appendix A.1."}, {"title": "Lemma 2.13.", "content": "For all \\( \\delta > 0 \\), \\( \\mathcal{E}(\\delta) \\) holds with probability at least \\( 1 - \\delta \\).\nThe following covariance-type observation is the first pivot."}, {"title": "Lemma 2.14.", "content": "If a bounded random variable \\( X \\le 0 \\) almost surely, then \\( \\mathbb{E}[X^3] - \\mathbb{E}[X^2]\\mathbb{E}[X] \\le 0 \\).\nRemark 2.15. While this lemma is elementary, to the best of our knowledge, we are the first to isolate this structure from our non-standard analysis of offline RL, from which the sharp upper bound is derived."}, {"title": "2 KL-Regularized Contextual Dueling Bandits", "content": "In this section, we extend our algorithm to KL-regularized dueling bandit setting, where the learner receives preference feedback instead of absolute signals. We formally introduce setting and extended algorithm. Except the objective, the core setting follows Zhu et al. (2023); Zhan et al. (2023). Our objective follows Xiong et al. (2024); Zhao et al. (2024)."}, {"title": "3.1 Problem Setup", "content": "We still\u2074 consider contextual bandits \\( (\\mathcal{S}, \\mathcal{A}, r, \\pi^{\\text{ref}}) \\) where \\( \\mathcal{S} \\) is the state space, \\( \\mathcal{A} \\) is the action space and \\( r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1] \\) is the reward function. But only relative preference feedback is available, viz., we have an i.i.d. offline dataset \\( \\mathcal{D} = \\{(s_i, a_i^1, a_i^2, y_i)\\}_{i=1}^n \\), where \\( s_i \\in \\mathcal{S} \\) is generated from distribution \\( \\rho \\) and \\( a_i^1, a_i^2 \\sim \\pi^{\\text{ref}} \\). The binary preference label \\( y_i = 1 \\) indicates \\( a_i^1 \\) is preferred over \\( a_i^2 \\) and 0 for the opposite. Given a state \\( s \\) and two corresponding action \\( a^1 \\) and \\( a^2 \\), let \\( y \\) be the preference label such that \\( y = 1 \\) indicates \\( a^1 \\succ a^2 \\) and \\( y = 0 \\) for \\( a^2 \\succ a^1 \\). In this work we consider the"}, {"title": "Concentrability", "content": "Analogous to Section 2, we need our estimation from offline dataset generalizable to the state-action pairs visited by our obtained policy, and therefore requires similar \\( D^2 \\)-divergence and corresponding policy coverage conditions. However, in dueling bandit, we are unable to observe and estimate the absolute value of rewards. Therefore, intuitively, the best estimation \\( f \\) we can achieve is that for any state \\( s \\) and actions \\( a^1, a^2 \\), our estimated \\( f(s, a^1) - f(s, a^2) \\approx r(s, a^1) - r(s, a^2) \\). This implies that there exists some mapping \\( b : \\mathcal{S} \\rightarrow [-1, 1] \\) such that \\( f(s, a) - b(s) \\approx r(s, a) \\) on the offline data. Consequently, it is natural to define the the following modified \\( D^2 \\)-divergence for characterizing how the error \\( f(s, a) - b(s) - r(s, a) \\) generalize to unseen samples.\nDefinition 3.1. Given a class of functions \\( \\mathcal{F} \\subset (\\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}) \\) and some policy \\( \\pi \\), let \\( \\mathcal{B} = (\\mathcal{S} \\rightarrow [-1, 1]) \\) be the function class, define the \\( D^2 \\)-divergence \\( D_{\\mathcal{F}, \\mathcal{B}}^2((\\mathbf{s}, \\mathbf{a}); \\pi) \\) as\n\\[\nD_{\\mathcal{F}, \\mathcal{B}}^2((\\mathbf{s}, \\mathbf{a}); \\pi) := \\sup_{f,g \\in \\mathcal{F}} \\inf_{b \\in \\mathcal{B}} \\frac{(f(s, a) - g(s, a) - b(s))^2}{\\mathbb{E}_{s' \\sim \\rho} \\text{Var}_{a' \\sim \\pi(\\cdot | s')}[f (s', a') - g(s', a')]}.\n\\]\nA similar definition has been introduced in Zhao et al. (2024, Definition 2.6), which underpins the following two assumptions characterizing the coverage ability of \\( \\pi^{\\text{ref}} \\).\nEquipped with the overloaded \\( D^2 \\)-divergence definition, we are now able to define all-policy concentrability and single concentrability similar as in Section 2."}, {"title": "3.2 Algorithm", "content": "We elucidate KL-PCDB for offline KL-regularized contextual dueling bandits, whose pseudocode is summarized in Algorithm 2. KL-PCDB first estimate the ground truth function \\( f^* \\) on offline dataset"}, {"title": "3.3 Main Results", "content": "In this subsection, we provide our main results for KL-regularized contextual dueling bandits. The sample complexity upper bound of KL-PCDB is presented in the following theorem.\nTheorem 3.5. Under Assumption 3.3, if we set \\( \\Gamma_{\\eta} \\) according to Lemma 3.1, then for sufficiently small \\( \\epsilon \\in (0, 1) \\), with probability at least \\( 1 - \\delta \\), \\( \\eta = \\tilde{O}(nD_{\\pi^*} \\epsilon^{-1}) \\) is sufficient to guarantee the output policy \\( \\tilde{\\pi} \\) of Algorithm 2 to be \\( \\epsilon \\)-optimal."}, {"title": "3.4 Proof of Theorem 3.5", "content": "The proof follows the proof in Section 2. At the beginning, we first define the event \\( \\mathcal{E}(\\delta) \\) given \\( \\delta > 0 \\) as\n\\[\n\\mathcal{E}(\\delta) := \\{\\exists b : \\mathcal{S} \\rightarrow [-1, 1], \\forall (s, a) \\in \\mathcal{S} \\times \\mathcal{A}, |f(s, a) - b(s) - f^*(s, a)| \\le \\Gamma_{\\eta}(s, a) \\}.\n\\]\nHere, \\( \\Gamma_{\\eta} \\) is defined in (3.1). We abuse the notation and define \\( b(\\cdot) \\) as\n\\[\nb = \\underset{B}{\\text{argmin}} \\sup_{(\\mathbf{s}, \\mathbf{a}) \\in \\mathcal{S} \\times \\mathcal{A}} \\Phi_b(s, a) - \\Gamma_{\\eta}(s, a),\n\\]\nwhere \\( \\Phi_b(s, a) = |\\hat{f}(s, a) - b(s) - f^*(s, a)| \\) and when \\( \\mathcal{E} \\) holds, for all \\( (s, a) \\in \\mathcal{S} \\times \\mathcal{A} \\), we have \\( \\Phi_b(s, a) \\le \\Gamma_{\\eta}(s, a) \\) This indicates that the least square estimation \\( \\hat{f} \\) obtained in Line 1 of Algorithm 2, after adjusted by some bias function \\( b \\), is close to the true function \\( f^* \\). The following lemma shows that this event holds with high probability. Please refer to Appendix B.1 for the proof."}, {"title": "3.7.", "content": "For any \\( \\epsilon \\in (0, 1) \\), \\( \\eta > 0 \\), and offline RL algorithm Alg, there is a KL-regularized contextual dueling bandit instance with O(1) single-policy concentrability such that Alg requires at least \\( \\Omega(\\min\\{\\eta \\log N_{\\mathcal{F}}(\\epsilon) / \\epsilon, \\log N_{\\mathcal{F}}(\\epsilon) / \\epsilon^2\\}) \\) samples to return an \\( \\epsilon \\)-optimal policy."}, {"title": "3.4 Proof of Theorem 3.5", "content": "The proof follows the proof in Section 2. At the beginning", "as\n\\[\n\\mathcal{E}(\\delta)": {"b": "mathcal{S} \\rightarrow [-1, 1", "n\\": "nHere, \\( \\Gamma_{\\eta} \\) is defined in (3.1). We abuse the notation and define \\( b(\\cdot) \\) as\n\\[\nb = \\underset{B}{\\text{argmin}} \\sup_{(\\mathbf{s}, \\mathbf{a}) \\in \\mathcal{S} \\times \\mathcal{A}} \\Phi_b(s, a) - \\Gamma_{\\eta}(s, a),\n\\"}}]}