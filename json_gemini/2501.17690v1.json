{"title": "Segmentation-Aware Generative Reinforcement Network (GRN) for Tissue Layer Segmentation in 3-D Ultrasound Images for Chronic Low-back Pain (cLBP) Assessment", "authors": ["Zixue Zeng", "Xiaoyan Zhao", "Matthew Cartier", "Tong Yu", "Jing Wang", "Xin Meng", "Zhiyu Sheng", "Maryam Satarpour", "John M Cormack", "Allison Bean", "Ryan Nussbaum", "Maya Maurer", "Emily Landis-Walkenhorst", "Dinesh Kumbhare", "Kang Kim", "Ajay Wasan", "Jiantao Pu"], "abstract": "We introduce a novel segmentation-aware joint training framework called generative reinforcement network (GRN) that integrates segmentation loss feedback to optimize both image generation and segmentation performance in a single stage. An image enhancement technique called segmentation-guided enhancement (SGE) is also developed, where the generator produces images tailored specifically for the segmentation model. Two variants of GRN were also developed, including GRN for sample-efficient learning (GRN-SEL) and GRN for semi-supervised learning (GRN-SSL). GRN's performance was evaluated using a dataset of 69 fully annotated 3D ultrasound scans from 29 subjects. The annotations included six anatomical structures: dermis, superficial fat, superficial fascial membrane (SFM), deep fat, deep fascial membrane (DFM), and muscle. Our results show that GRN-SEL with SGE reduces labeling efforts by up to 70% while achieving a 1.98% improvement in the Dice Similarity Coefficient (DSC) compared to models trained on fully labeled datasets. GRN-SEL alone reduces labeling efforts by 60%, GRN-SSL with SGE decreases labeling requirements by 70%, and GRN-SSL alone by 60%, all while maintaining performance comparable to fully supervised models. These findings suggest the effectiveness of the GRN framework in optimizing segmentation performance with significantly less labeled data, offering a scalable and efficient solution for ultrasound image analysis and reducing the burdens associated with data annotation.", "sections": [{"title": "I. INTRODUCTION", "content": "CHRONIC lower back pain (cLBP) is a complex and multifaceted condition [1]. It is highly prevalent, with 70-85% of individuals experiencing lower back pain (LBP) at some point in their lives [2]. Additionally, 19.6% of people aged between 20 and 59 report LBP [3, 4], significantly affecting their quality of life [5]. Despite extensive research, etiology-based diagnosis and successful management of cLBP remain elusive, with no consensus on the most effective approaches [6].\nVarious clinical imaging techniques, including ultrasound, computed tomography (CT), and magnetic resonance imaging (MRI), have been employed to explore the underlying classification and mechanisms of cLBP. CT provides high-resolution cross-sectional images, offering detailed visualization of various anatomical structures, including the vertebrae and muscles [7-9]. However, CT imaging exposes patients to ionizing radiation, which carries potential long-term health risks. Also, it has limited ability to differentiate between soft tissues, making it less effective for detailed assessment of muscular structures [10]. MRI is a non-invasive diagnostic tool that uses powerful magnetic fields and radio waves to produce high-resolution images, particularly suited for visualizing soft tissues, organs, and nervous systems [11, 12]. However, MRI imaging is generally more expensive, less accessible, and time-consuming compared to other imaging methods, limiting its use. By comparison, ultrasound imaging offers a cost- and time-effective alternative without the risks associated with ionizing radiation, making it a valuable and widely used tool in clinical practice for assessing cLBP.\nCurrent ultrasound research on cLBP employs predominantly two-dimensional (2-D) ultrasound imaging to assess muscle thickness, contraction, and tissue echogenicity, which are believed to be associated with LBP[13]. This 2-D imaging approach overlooks volumetric and intricate anatomical structures and thus limits the comprehensiveness of the analysis [14]. In contrast, three-dimensional (3-D) ultrasound imaging offers improved spatial visualization, enabling a more detailed and accurate layer-by-layer analysis from the dermis to the"}, {"title": "II. MATERIALS AND METHODS", "content": "The dataset is acquired from an ongoing project funded by the National Institute of Health (NIH), which enrolls participants with cLBP and healthy controls (Institutional Review Board: STUDY22090014). All participants were positioned face-down on an examination table. The ultrasound array transducer was placed laterally to the midline at the lumbar 3-4 (L3-L4) vertebral interspace (Fig. 1, Part 1), specifically targeting the multifidus (MF) and erector spinae (ES) muscles. A row-column array (RCA) transducer (RC6gV, Vermon), operating at a center frequency of 6 MHz with an active aperture of 25.6 mm \u00d7 25.6 mm, was connected to the Vantage 256 ultrasound system (Verasonics Inc., WA, USA) to acquire 3-D volumetric ultrasound data. A synthetic aperture technique was used during acquisition to improve image quality. To acquire a single frame, each element of the transducer individually transmits one pulse in sequence until all the elements have performed transmissions, and after each transmission, all the elements simultaneously receive the echo signals. This enabled a synthetic aperture (SA) approach to achieve dynamic focusing during beamforming for image reconstruction.\nTemporal compounding was applied by averaging three consecutive frames to improve the image signal-to-noise ratio. Both the right and left sides of the back were scanned. On each side, two specific locations over the MF and ES muscles were targeted by experienced ultrasound examiners (with 9 (ACB) and 25 (ADW) years of clinical experience). Each designated location was scanned three times, yielding a total of 12 B-mode scans per participant. A total of 29 participants were enrolled and used for this study. We randomly sampled 69 scans ( N_{image} = 17,664 ) taken from various locations across 29 patients for algorithm development (Table I). An experienced ultrasound operator (Ms. Zhao) meticulously annotated six distinct anatomical layers depicted on the 3-D ultrasound scans, including the dermis, superficial fat, superficial fascial membrane (SFM), deep fat, deep fascial membrane (DFM), and MF muscle (Figure 1, Parts 2 & 3). For the purpose of deep learning model training, the annotated B-mode images were divided into training ( N_{image} = 10,722 , N_{scan} = 44 , N_{patient} = 16 ), internal validation ( N_{image} = 512 , N_{scan} = 2 , N_{patient} = 2 ), and independent test ( N_{image} = 6,430 , N_{scan} = 23 , N_{patient} = 11 ) sets at the patient level. To ensure a more reliable estimate of model performance, we set aside a"}, {"title": "B. Generative Reinforcement Network (GRN)", "content": "GRN is designed to generate both challenging interpolated samples and denoised samples to improve segmentation model performance (Fig. 2.). Within this framework, we introduce two variants: GRN-SEL for sample-efficient learning and GRN-SSL for semi-supervised learning. In GRN-SEL, the generator G reconstructs the original image, which is then passed through the segmentor. Similar to smart augmentation, we utilize a feedback mechanism where the segmentor provides feedback to the generator via the segmentor loss during each training iteration [21]. This feedback mechanism allows the generator to create images tailored to denoise the original input and generate reconstructed images. This approach, termed segmentation-aware denoising augmentation, optimizes the following objective function:\n\\min_{G} (\\min_{S} L_{seg} (S(G(x)), y)), where x \\sim P_{data}(x) \t{      (1)\nwhere x is sampled from the prior distribution Pdata(x), and y represents the ground truth. G and S denote the generator and segmentor, respectively. Lseg is the segmentor's loss. To further enhance model performance, GRN-SSL incorporates interpolation consistency training, which generates challenging interpolated samples that lie outside the original training data distribution. Training the model on these interpolated samples exposes it to a broader variety of data, significantly improving robustness and enabling better generalization to unseen data."}, {"title": "1) Sample-efficient and Semi-supervised Generative Reinforcement Network", "content": "GRN-SEL combines a GAN model with the segmentor feedback mechanism (Fig. 3). In each training iteration, the generator Ge processes each labeled image to produce a reconstructed version. The segmentor Se\" then generated predicted masks from those reconstructed images. A Dice loss is calculated based on the difference between these predicted masks and the ground truth masks, and the loss is backpropagated to the generator. This feedback encourages Ge to refine its reconstruction by removing noise and irrelevant details, enabling the segmentor to focus on important features and thereby improving robustness.\nLike a traditional GAN model, the generator Ge also receives feedback from the discriminator De,, which classifies each reconstructed image as real or fake. Additionally, the segmentor processes both original, unaltered images and the reconstructed images and calculates a combined Dice loss based on mask prediction from both. This dual-processing approach promotes the segmentation model's robustness and stability by ensuring consistent performance on both generator-processed and original images. The comprehensive loss functions are defined as follows:\nL_{G} = \\lambda_{adv} L_{MSE} (D_{G} (\\hat{I}_{L}), 1) + \\lambda_{seg} L_{Dice} (M_{L}, M) + 2L_{1} (\\hat{I}_{L}, I_{L}) \t{      (2)\nL_{S} = \\frac{L_{Dice} (M_{L}, M) + L_{Dice} (\\hat{M}_{L}, M)}{2} \t{      (3)\nL_{D} = \\frac{L_{MSE} (D_{G} (I_{L}), 1) + L_{MSE} (D_{G} (\\hat{I}_{L}), 0)}{2} \\ // 1: Real, 0: Fake \t{      (4)\nwhere LMSE represents the Mean Squared Error (MSE) loss. Lpice represents the Dice loss and L1 represents the L1 loss. adv, weights Aseg and A\u2081\u2081 control the contributions to each loss component. I\u2081 and are the labeled image and its reconstructed version. M is the ground truth mask. M\u2081 and M\u2081 are the mask prediction for the labeled image and reconstructed image, respectively. The generator loss function combines both generator and discriminator feedback loss terms, ensuring image generation supports not only realism but also effective learning by the segmentor."}, {"title": "2) Dynamic Generator Augmentation (DGA) and Segmentation Guided Enhancement (SGE)", "content": "Traditional GAN-based augmentation methods involve pre-training the generator to create a fixed reconstructed dataset, which is then used for segmentation model training. In contrast, the proposed GRN integrates both the generator and the segmentor within a single training framework, allowing the generator Ge to update its weight dynamically in each training iteration. An example formula for updating the generator weights at training iteration t is shown below:\n\\theta_{\\tau+1} - \\theta_{\\tau} = -\\eta \\cdot \\nabla_{\\theta} L_{G} \t{      (10)\nwhere \u03b7 is the learning rate, and V\u03b8 LG is the gradient calculated at training iteration t. Even when the generator Ge receives the same image I, the Get+1(I) \u2260 Get(I) due to continuous updates of the hyperparameter @ for each epoch. This method enables the generator to produce progressively varied images from the original input images. By dynamically generating images, the segmentation model is trained on an almost limitless number of image variations. As a result, we refer to this approach as Dynamic generator augmentation (DGA) in this study.\nIn the GRN framework, the objective function is defined as mingminsLseg(S(G(x)),y), where the generator G and the segmentor S share the same goal to minimize the segmentor loss. Consequently, during the inference phase, the generator and the segmentor can be integrated to provide a joint prediction y = S(G(x)). This two-step process, termed Segmentation Guided Enhancement (SGE), ensures that image enhancements are designed to improve segmentation outcomes."}, {"title": "C. Interpolation Consistency Loss", "content": "Similar to a previous study [32], we adopt interpolation consistency loss as our customized loss for GRN-SSL. For each image in the unlabeled batch BUL, we randomly sample a second image from the same batch. Both images are processed by the generator G and segmentor S to produce their respective segmentation masks. We then randomly generate an interpolation coefficient \u03b1 from a predefined distribution to create a mixed image through an interpolation of the original and sampled images. This mixed image is subsequently processed by the generator and segmentor to obtain a corresponding segmentation mask. The interpolation consistency loss quantifies the alignment between the segmentation mask of the mixed image and the interpolated masks of the original and sampled images.\nL_{ICT} = MSE(M_{mixed}, Mix_{\\alpha}(M_{UL}, M_{s})) \t{      (11)\nwhere Mmixed denotes the predicted mask derived from the mixed image. MUL and Ms are the mask predictions based on the unlabeled image and randomly sampled image, respectively. Mixa() performs the interpolation of two images based on Mix_{\\alpha} (Tensor_{a}, Tensor_{b}) = \\lambda Tensor_{a} + (1 - \\lambda) Tensor_{b}."}, {"title": "D. Backbone Network Architecture", "content": "The backbone segmentation network utilizes a 2D Unet model configured with encoder channels of (16, 32, 64, 128, 256). The model is designed to accept single-channel input and produce segmentation masks for 7 classes, making it well-suited for our ultrasound imaging segmentation task. The generator encoder, illustrated in the upper section of Fig 5, incorporates residual blocks to improve gradient flow efficiency and facilitate the training of deeper network layers. The encoding process begins with an initial convolution layer that reduces spatial dimensions while increasing feature depth. This is followed by a series of residual blocks that downsample the input data. In contrast, the decoder utilizes 2D transposed convolutions to upscale high-dimensional feature representations, effectively reconstructing the images. Similar to the pix2pix model [33], we utilize a PatchGAN discriminator to assess the realism of generated images at the patch level [33]. The discriminator architecture comprises several convolution layers that progressively extract features from the input images. The discriminator outputs a patch-wise label map that evaluates the authenticity of each patch on the image."}, {"title": "E. Model Training and Performance Evaluation", "content": "In line with standard practices in semi-supervised learning and sample-efficient learning, we designated 5%, 10%, and 20% of"}, {"title": "III. RESULTS", "content": "Table II presents the segmentation performance of our proposed GRN methods compared to existing techniques on the independent test set with varying proportions of labeled data (5%, 10%, 20%) involved in the training. Across all labeled data proportions, the GRN-SSL variant and GRN-SEL with"}, {"title": "IV. DISCUSSION", "content": "We developed and validated a novel method called GRN, which synergistically integrates the joint training of a generator and a segmentor to accurately segment tissue layers depicted on B-model ultrasound imaging while significantly reducing the need for extensive image annotations. Unlike traditional unconstrained GAN-based data augmentation methods, our GRN framework features a feedback mechanism in which the segmentation model provides feedback loss to the generator, thereby guiding the image generation process to produce denoised segmentation-optimized images. Our results demonstrate that GRN-SSL with SGE can reduce data labeling efforts by up to 70% (Table IV). Meanwhile, GRN-SSL alone achieves a 60% reduction, GRN-SEL with SGE also reduces data labeling efforts by 70%, and GRN-SEL alone reduces by 60%, all while maintaining performance comparable to fully supervised models. Notably, GRN-SSL with SGE outperforms models trained on all labeled images, achieving a 2.59% higher DSC with only 40% of the labeled images in the training set. These findings suggest the effectiveness of the GRN framework in optimizing segmentation performance with significantly less labeled data, offering a scalable and efficient solution for medical image analysis while alleviating the burdens associated with data annotation.\nThe GRN framework offers several advantages. First, the generator continuously updates its weights, providing the segmentor with dynamically augmented images across different epochs. This process substantially increases the diversity of input data and finally improves the robustness of the model. Second, the generator serves as an optional image enhancement tool, specifically optimized to generate images that minimize the segmentor's loss, thereby further enhancing segmentation performance. Third, the GRN framework exhibits significant versatility. Based on the GRN-SEL, the Dice loss used to measure the performance of the segmentor could be replaced by other loss functions, such as interpolation consistency loss for GRN-SSL or rectified pyramid consistent loss in the URPC model [32, 34]. Lastly, unlike other semi-supervised learning approaches that typically require a large set of unlabeled images, GRN-SEL only relies on a small set of annotated data and supports semi-supervised learning without special requirements for the quantity of unlabeled images.\nContrary to the prevailing belief that augmentation should generate challenging and diverse datasets to enhance model performance [20], our ablation study results demonstrate that using a less diverse, denoised augmented dataset can improve model performance (Table VI). This is because training on a denoised dataset allows the model to focus on informative features. Consider a sample with two feature components (Xinformative, Xnoise), and corresponding weight matrices (Winformative, Wnoise), the extracted feature can be expressed as: F = Winformative \u00d7 Xinformative + Wnoise \u00d7 Xnoise. If the noise features are set to 0, the gradient can be expressed as:\n\\frac{\\partial L}{\\partial W_{noise}} = \\frac{\\partial L}{\\partial F} \\frac{\\partial F}{\\partial W_{noise}} + \\frac{\\partial L}{\\partial X_{noise}} \\times X_{noise} = 0\n\\ when X_{noise} = 0 \t{      (12)\nIn this scenario, the weights associated with noise features Wnoise are not updated and remain near their initial values, close to zero. This enables the model to ignore noise features and concentrate on the informative parts of the data, thereby improving overall performance.\nWe did not freeze segmentor weights during the backpropagation of the generator loss LG. Although GRN shares a feedback mechanism similar to GANs by allowing the generator to receive guidance, the role of the discriminator and segmentor diverse in GRN differ from those in conventional GAN architectures. In GRN, the objective function is ming (minsLseg (S(G(x)),y)), where the generator G and segmentor S work together to minimize the segmentation loss. Additionally, during the inference stage, the generator can serve as an image enhancement module, preprocessing input images before they enter the segmentor. Therefore, we opted not to freeze the segmentor weights. This design allows both the generator G and segmentor S to adapt dynamically with each training iteration.\nThe component analysis showed that the DGA significantly improves model performance when the proportion of labeled data is low; however, its impact diminishes as the amount of labeled data in the training set increases (Table V). One explanation is when dataset diversity is limited, even a small amount of augmentation can substantially increase diversity and thereby improve model performance. As the labeled data becomes more abundant, each additional augmentation contributes less to diversity, leading to diminishing returns and a plateau in performance gains. In contrast, SGE consistently enhances model performance across all levels of labeled data, including scenarios with a fully labeled dataset. This finding suggests that beyond optimizing the encoder and decoder architectures for improved performance, incorporating a"}, {"title": "V. CONCLUSION", "content": "We introduce a segmentation-aware joint training framework called GRN designed to segment tissue layers in 3-D B-mode ultrasound images. This innovative approach incorporates a segmentation loss feedback mechanism that enables the generator to produce reconstructed images specifically optimized to enhance the performance of the segmentation model. Our model achieves a higher performance compared to models trained on 100% labeled datasets, all while reducing the data labeling efforts by 60%. As a semi-supervised learning method, GRN-SEL relies on a small set of annotated data and can effectively utilize available unlabeled images, even when their quantity is limited."}]}