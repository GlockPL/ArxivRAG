{"title": "Semantic Codebook Learning for Dynamic Recommendation Models", "authors": ["Zheqi Lv", "Shaoxuan He", "Tianyu Zhan", "Shengyu Zhang", "Wenqiao Zhang", "Jingyuan Chen", "Zhou Zhao", "Fei Wu"], "abstract": "Dynamic sequential recommendation (DSR) can generate model parameters based on user behavior to improve the personalization of sequential recommendation under various user preferences. However, it faces the challenges of large parameter search space and sparse and noisy user-item interactions, which reduces the applicability of the generated model parameters. The Semantic Codebook Learning for Dynamic Recommendation Models (SOLID) framework presents a significant advancement in DSR by effectively tackling these challenges. By transforming item sequences into semantic sequences and employing a dual parameter model, SOLID compresses the parameter generation search space and leverages homogeneity within the recommendation system. The introduction of the semantic metacode and semantic codebook, which stores disentangled item representations, ensures robust and accurate parameter generation. Extensive experiments demonstrates that SOLID consistently outperforms existing DSR, delivering more accurate, stable, and robust recommendations.", "sections": [{"title": "1 INTRODUCTION", "content": "Nowadays, as an important branch of recommendation systems, sequential recommendation has emerged, including DIN [60], GRU4Rec [12], SASRec [17], BERT4Rec [34] and other models that are crucial in the field of recommendation systems. However, the behavior logic of most users is not universally applicable, and as interests can change, it necessitates that sequence recommendation models be able to adjust their parameters in real-time according to the user's current interest preferences. Consequently, dynamic sequential recommendation models (DSR) like DUET [29] and APG [48] have been developed.\nThe DSR paradigm consists of two parts: (1) The primary model. This model has a structure similar to conventional sequential recommendation models like SASRec, but it is divided into a static layer and a dynamic layer. The parameters of the static layer remain unchanged after pre-training, whereas the parameters of the dynamic layer change with the user's behavior. (2) The parameter generation model. This is mainly used to sparse user behavior and generate the parameters for the dynamic layer of the primary model based on this behavior. The DSR paradigm enables traditional static sequential recommendation models to quickly adjust their parameters according to the potential shift of interests and intentions reflected in user behaviors, thus dynamically obtaining more interest-aligned models in real time.\nDespite the promising potential of Dynamic Sequential Recommendation (DSR) systems, they face significant challenges, primarily stemming from the item-to-parameter modeling scheme: (1) A large number of items result in a vast search space for the parameter generation model. Slight variations in user behavior sequences, such as \"shirt, tie, suit\" versus \"tie, shirt, suit,\" which suggest similar preferences, can unpredictably alter the item-to-parameter modeling, introducing complexity and potential instability. (2) The interaction between users and items is generally sparse and potentially noisy (e.g., the notorious implicit feedback issue), leading to heterogeneous behavior sequences that complicate the learning of accurate item representations. This results in inaccurate item representation learning, weakening the precision of model parameter customization based on item sequence features, and further exacerbating the inaccuracy of generated parameters.\nTo address these issues, we propose the Semantic Codebook Learning for Dynamic Recommendation Models (SOLID). The core objective of SOLID is to compress the search space of the parameter generation model, promoting homogeneity signals utilization within the recommendation system. We construct a semantic codebook that better utilizes these homogeneity signals. In the codebook, item representations are disentangled into semantics that are learned to be absorbed in the codebook elements, such that the homogeneity between items in the disentangled latent space can be established. The user-item interactions are transformed into density-enriched user-semantic interactions in the latent space. The enriched density reduces the heterogeneity and complexity of user behavior space modeling in the parameter generator. Moreover, SOLID shifts from a traditional item sequence-based parameter generation mode to a dual (item sequence + semantic sequence) \u2192 model parameter generation mode, effectively merging both uniform and diverse information in a structured manner. Uniform information derived from the semantic-to-parameter part is utilized to develop parameters that generalize across certain user behaviors, while diverse information allows for the crafting of specific parameters tailored to individual behavioral nuances. Crucially, by aligning the dimensions of the codebook with those of the semantic encoder, we transform the semantic encoder into a meta-code that serves as an initial state for the codebook, further easing the modeling of parameter generation.\nSpecifically, to reduce the search space of the parameter generation model through the semantic codebook, SOLID involves three main modules. Initially, SOLID employs a pretrained model to extract semantic components from item, image, and text features. This disentanglement transitions the focus from item sequences to semantic sequences, shifting the modeling approach from item-based to semantics-based parameter generation. This design results in trunk parameters that generalize behaviors from the entire user base to specific groups, and branch parameters that cater to individual user behaviors, both derived from semantic and item sequences respectively. Parameters derived from items are tightly controlled (e.g., \u00b10.01) before their integration into the dynamic layer of the primary model, ensuring a responsive and adaptive system based on real-time user activity. Despite this, branch parameters still adhere to an item-centric approach, necessitating the use of a Semantic Codebook (SC) to maintain personalization and stability in representation. This codebook stores semantic vectors of behavior,"}, {"title": "2 RELATED WORK", "content": "Recommendation system predicts user preferences based on user behavior history [7, 19, 20, 22-25, 32, 33, 47, 51, 52, 56, 57]. Sequential recommendation, as an important branch of the recommendation system, arranges users' recent historical behaviors in chronological order to more accurately capture users' recent preferences. Recent advancements [4, 12, 17, 26, 27, 29, 34, 45, 48, 60] have shifted towards deep learning-based sequential recommendation systems. For instance, GRU4Rec [12] employs Gated Recurrent Units to effectively model sequential behavior, demonstrating impressive results. Additionally, DIN [60] and SASRec [17] incorporate attention mechanisms and transformers, respectively. BERT4Rec [34] further applies BERT for superior outcomes in recommendation task. The models have significantly impacted academic research and industry practices. However, these SR Models struggle to achieve optimal performance across every data distribution when dealing with users' real-time changing behaviors and interest preferences."}, {"title": "2.2 Disentangled Representation Learning", "content": "The goal of disentangled representation learning is to parse the data into distinct, interpretable components by identifying different underlying latent factors [2, 3]. Variational autoencoders (VAE) [5] and \u03b2-VAE [13] provide more possibilities for disentangled learning by adjusting the balance between the model's disentanglement ability and its ability to represent information. By incorporating multi-interest methods [18, 30] along with disentangled representation learning, several studies [41-44, 58] have demonstrated significant advancements in recommendation tasks. We draw on the idea of disentangling and apply it to dynamic model parameter generation to reduce the parameter search space and leverage the homogeneous information of user behavior."}, {"title": "2.3 Dynamic Neural Network", "content": "Research in dynamic neural networks focuses on HyperNetworks [11] and Dynamic Filter Networks [16], which have better ability to adapt to distribution deviations than traditional static model learning or other efficient fine-tuning strategies [6, 9, 14, 15, 21, 36, 38, 39, 55, 59, 63, 65]. Similar situations also exist in the study of large models [53, 61, 62, 64]. HyperNetworks, introduced by Ha et al. [11], use one neural network to dynamically generate parameters for another, reducing the number of parameters needed and achieving model compression. This concept has led to extensive exploration and enhancements in various applications [1, 8, 10, 31, 35, 37, 46, 50, 53, 54]. Some recent research includes: HyperInverter [8], HyperStyle [1], Detective [54] introduces dynamic neural networks into multiple computer vision tasks to improve the model's personalization capabilities under various data distributions. IntellectReq [28] detects when such dynamic networks need to modify parameters to adapt to samples, thereby achieving better performance with fewer parameter modifier calls. APG [48] and DUET [29] are the latest and state-of-the-art examples of using dynamic neural networks for sequence recommendation. However, existing DSR models are affected by the heterogeneity of user behavior, the sparsity of user-item interactions, etc., leading to drawbacks such as an overly large parameter search space and inaccurate parameter generation. Our method effectively addresses these shortcomings."}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 Notations and Problem Formulation", "content": "First, we introduce the notation in sequential recommendations."}, {"title": "3.1.1 Data.", "content": "We use $X_{ori} = \\{u, v, s_v\\}$ to represent a piece of data, $X_{dec} = \\{u, c, s_c\\}$ to represent a piece of disentangled data, $X_{mm} = \\{i, t\\}$ to represent multimodal information, and $Y = \\{y\\}$ to represent the label indicating whether the user will interact with the item. In brief, $X = X_{ori} \\cup X_{dec} \\cup X_{mm} = \\{u, v, s_v, c, s_c, i, t\\}$, where $u, v, c, s_v, s_c, i, t$ represent user ID, item ID, category ID, user's click sequence consists of item ID, user's click sequence consists of category ID, the image of the item, and the title of the item respectively. We represent the dataset as $D$, where $D = \\{X, Y\\}$. More specifically, we use $D_{Train}$ to represent the training set and $D_{Test}$ to represent the test set. Roughly speaking, let $L$ be the loss obtained from training on dataset $D_{Train}$. For simplicity, we simplify the symbol $D_{Train}$ to $D$. Then, the model parameters $W$ can be obtained through the optimization function arg min $L$. The sequence length inputted into the model is set to $L_s$, so the lengths of both $s_u$ and $s_c$ in a sample are $L_s$."}, {"title": "3.1.2 Model.", "content": "The recommendation model is represented by $M$ and the parameters of the $M$ is $\\Theta$, where $\\Theta = \\Theta_s, \\Theta_d$. The model $M$ is utilized to generate the $\\Theta_d$ according to the item id sequence $s_u$, $M_c$ is utilized to generate the $\\Theta_d$ according to the category id sequence $s_c$, $M(\\cdot)$ and $M_c(\\cdot)$ represent the forward propagation processes of two models, where denotes the input."}, {"title": "3.1.3 Feature.", "content": "We use $E_v$ and $E_c$ to represent the item feature set and semantic feature set extracted from $s_v$ and $s_c$ respectively. Specifically, $E_v = \\{e_1^v, e_2^v, ..., e_{L_s}^v \\}$, $E_c = \\{e_1^c, e_2^c, ..., e_{L_s}^c \\}$. $e_v$ and $e_c$ are the sequence features obtained through sequence feature extraction models such as Transformer or GRU, via $E_v$ and $E_c$, respectively. The length of an item representation or a semantic representation is set to $L_r$."}, {"title": "3.1.4 Formula.", "content": "Sequential Recommendation Models (SR), Dynamic Sequential Recommendation Models (DSR), and Disentangled Multimodal Dynamic Sequential Recommendation Models (SOLID) can be formalized as follows:\n$SR: M(X_{ori}; \\Theta) \\rightarrow (Y, \\hat{Y})$  (1)\n$DSR: M(X_{ori}; \\Theta_s, \\Theta_d = M_v(X_{ori})) \\rightarrow (Y, \\hat{Y})$ (2)\n$\\text{SOLID} : \\begin{cases}\nX_{ori}, X_{mm} \\rightarrow c = f(v, i, t) \\rightarrow X_{dec},\n\\\\ \\Theta_d = M_v(X_{ori}) + M_c(X_{dec}),\n\\\\ M(X_{ori}; \\Theta_s, \\Theta_d) \\rightarrow (Y, \\hat{Y})\n\\end{cases}$ (3)\nIn the aforementioned formula, $a \\rightarrow b$ indicates indicates information transfer from a to b, with the text next to it representing the content of the transfer. $a \\rightarrow b$ signifies that b is derived from a."}, {"title": "3.2 Preliminary", "content": "Here we first retrospect the paradigm of sequential recommendation."}, {"title": "3.2.1 Sequential Recommendation Models.", "content": "In the training stage, the loss can be calculated to optimize the sequential recommendation models as follows,\nmin $L = \\sum_{u,v,s_v, Y \\in D} L_{CE} (y, \\hat{y} = M(u, v, s_v; \\Theta))$. (4)\nThe loss function can set to CE (Cross Entropy) loss and MSE (Mean Squared Error) loss, etc. However, since sequential recommendation often focuses more on CTR (Click-Through Rate) prediction tasks, and this paper is also focused on CTR prediction, the recommendation loss in this paper is CE loss and represented by $L_{CE}$."}, {"title": "3.2.2 Dynamic Sequential Recommendation Models.", "content": "DSR generate model parameters based on users' real-time user behaviors. Then the updated model is used for current recommendations. In this paper, the network layer that can adjust model parameters as the data distribution changes is called an adaptive layer.\nDSR treat the parameters of one of the adaptive layers as a matrix $K \\in \\mathbb{R}^{N_{in} \\times N_{out}}$, where $N_{in}$ and $N_{out}$ represent the number of input neurons and output neurons of a fully connected layer (FCL), respectively. DSR utilize a encoder $E_u$ to extract the sequence feature $e$ from the user's behavior sequence $s_u$ to generate the parameters of the model's adaptive layers.\n$\\Theta_d = M_v(E_v(s_u))$, (5)\nAfter parameter generation, the parameters of the model will be reshaped into the shape of $K$."}, {"title": "3.3 SOLID Framework", "content": "The architecture of our proposed SOLID is shown in the Figure 2."}, {"title": "3.3.1 Semantic Parameter Generation.", "content": "Transforming the Item-based Dynamic Recommendation Model into a Semantic-based Dynamic Recommendation Model is an important step in disentangling personalized model parameters. First, items need to be transformed into semantics. For data without category labels, clustering can be directly applied to obtain semantics, i.e.,\n$Cluster(\\{e_i\\}_{i=1}^N) \\rightarrow \\{c_i\\}_{i=1}^k, c_i \\in \\{1, 2, ..., k\\}$. (7)\nFor data with category labels, since the same item often belongs to multiple categories, we select a primary category as semantic it. First, we define the centroid $m_c$ of each category $c$, which is the average of embeddings $e$ for all items belonging to category $c$. Assuming $n_c$ is the number of items belonging to category $c$, the centroid $m_c$ for category $c$ can be represented as:\n$m_c = \\frac{1}{n_c} \\sum (e_v \\text{ or } e_i \\text{ or } e_t)$, (8)\nwhere $e_v, e_i, e_t$ are the representation of item ID $v$, item image $i$, item title $t$, respectively. Next, we compute its distance to each category center $m_c$. Assuming we use the Euclidean distance, it can be represented as,\n$d(v, c) = ||(e_v \\text{ or } e_i \\text{ or } e_t) - m_c||$, (9)\nwhere $|| \\cdot ||$ denotes the norm of the vector, typically the Euclidean norm. Finally, we select the closest category as the semantic for item $v$. That is, the semantic $c_p$ for item $v$ can be represented as:\n$c_p = \\underset{c}{\\text{arg min }} d((v \\text{ or } i \\text{ or } t), c)$. (10)\nAfter converting items into semantics, a semantic-to-parameter model can be trained. The training process is similar to that of the item-to-parameter model. The only differences are that the input for the item-to-parameter model is an item sequence, whereas for the semantic-to-parameter model, it is a semantic sequence; similarly, the outputs are the target item and target semantic, respectively.\n$\\underset{\\Theta_s, \\Theta_c}{\\text{min }} L = \\sum_{u,v,s_c, Y \\in D} L_{CE} (y, \\hat{y}),$\n$\\hat{y} = M(u, v, s_c; \\Theta_s, \\Theta_d),$\n$\\Theta_d = M_c(E_c(s_c))$. (11)"}, {"title": "3.3.2 Semantic Metacode Learning.", "content": "To balance the use of personalized user behavior information and homogeneous information from similar user behaviors, we combine the item-to-parameter and semantic-to-parameter models for the parameter generation process. The former's advantage lies in providing personalized information, but its disadvantage is the inaccuracy in parameter generation due to strong data heterogeneity and sparse user-item interactions. The latter's advantage is providing homogeneous information from similar user behaviors, and dense user-item interactions make the parameter generation process more robust. However, its disadvantage is that the semantic sequence is less personalized compared to the item sequence.\nTherefore, our approach primarily uses the semantic-to-parameter method to generate the main part of the model parameters. Since similar semantic sequences are easier to obtain than similar item sequences, the parameters derived from the semantic sequence can be viewed as a user group model. Then, the item-to-parameter method is used as a branch, with parameters generated from item sequences being constrained within a smaller threshold and merged with the parameters obtained from the semantic sequence. This merging process is seen as a transition from a user group model to an individual user model, thus balancing homogeneous information and personalized information. Therefore, the training process can be formulated as the following optimization problem,\n$\\underset{\\Theta_s, \\Theta_c, \\Theta_v}{\\text{min }} L = \\sum_{u,v,s_c, Y \\in D} L_{CE} (Y, \\hat{y}),$\n$\\hat{y} = M(u, v, s_v; \\Theta_s, \\Theta_d),$\n$\\Theta_d = M_c(E_c(s_c)) + \\text{Clip}(M_v(E_v(s_v))); \\Gamma)$, (12)\nwhere $\\Gamma$ is a hyperparameter used to control the threshold for parameter deviation, thereby also controlling the impact of personalized information on the model parameters. Semantic Encoder can be transformed into a Semantic Metacode(SM), which can be used to further enhance the initialization of the Semantic Codebook for the item-to-parameter process. The Semantic Metacode can be effectively learned through the above process."}, {"title": "3.3.3 Semantic Codebook Learning.", "content": "Even if the model parameter generation process is disentangled, the item-to-parameter mode is still needed because it is the source of personalized information. Therefore, to further improve the accuracy of the item-to-parameter mapping, we design a Semantic Codebook (SC). Upon obtaining the semantic metacode, we initialize the semantic codebook with it. Subsequently, we continue using the trunk and branch method of parameter generation, specifically semantic-to-parameter and item-to-parameter, to derive the parameters for the adaptive layer of the model. In the branch branch, the item representations are replaced with semantic codes from the codebook, which are then used to further predict model parameters. The generated model parameters are used for click prediction on item sequences, just as before, ultimately allowing for the training of the semantic codebook. The specific method for computing the loss is described below. SC is denoted as $D$, and $D \\in \\mathbb{R}^{N_c \\times L_r}$. Specifically, we first use the weights of the semantic encoder in the semantic-to-parameter to initialize the item representation, as their dimensions are the same."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 Experimental Setup", "content": ""}, {"title": "4.1.1 Datasets and Preprocessing.", "content": "We evaluate SOLID and baselines on eight datasets. Amazon Arts (Arts), Amazon Instruments (Instruments), Amazon Office (Office), Amazon Scientific (Scientific), which are four benchmarks that was recently released but has been widely used in the multimodal recommendation tasks [40]. Amazon CDs (CDs), Amazon Electronic (Electronic), Douban Book (Book), and Douban Music (Music), which are four widely used public benchmarks in the recommendation tasks. We choose the leave-one-out approach to process the dataset, taking the last action of each user for testing and all previous actions for training and validation. Our task is CTR (Click-through Rate) prediction, so we process these datasets into CTR prediction datasets. These datasets consist of user rating datasets with complete reviews. We treat all user-item interactions in the dataset as positive samples because having a rating implies that the user clicked on the item. Further, to ensure the training process goes smoothly with both positive and negative samples, we sample 4 negative samples for each positive sample in the training set and 99 negative samples for each positive sample in the test set."}, {"title": "4.1.2 Baselines.", "content": "The baselines we select are as follows:\n\u2022 Static Recommendation Models. DIN [60], GRU4Rec [12], SASRec [17], and BERT4Rec [34] are all highly prevalent sequential recommendation methods in both academic research and the industry. They each incorporate different techniques, such as Attention, GRU (Gated Recurrent Unit), and Self-Attention, to enhance the recommendation process.\n\u2022 Dynamic Recommendation Models. DUET [29] and APG [48] consists of two parts: a parameter generation model and a primary model. The primary model refers to the aforementioned models like DIN, GRU4Rec, SASRec, BERT4Rec, etc. After pre-training, the parameter generation model can generate model parameters for the primary model during inference based on the samples."}, {"title": "4.1.3 Evaluation Metrics.", "content": "We use the widely adopted AUC, UAUC, NDCG, and Recall as the metrics to evaluate model performance."}, {"title": "4.2 Overall Results", "content": "As shown in Table 1, we evaluate the overall performance across four multimodal datasets: Arts, Instruments, Office, and Scientific. For each dataset, we test the performance of four SR Models: DIN, GRU4Rec, SASRec, and BERT4Rec. We evaluate performance via AUC, UAUC, NDCG@10, Recall@10, NDCG@20, and Recall@20. For each SR Model, there are five options for DSR Models: None (\"-\"), APG, Ours (APG), DUET, and Ours (DUET), where \"-\" indicates no DSR Model usage, i.e., the inherent performance of the SR Model itself. Since the \"-\" option consistently performs worse than using a DSR Model, our comparison primarily focuses on the performance of APG vs. Ours (APG) and DUET vs. Ours (DUET) for each SR Model. Across all datasets, all SR Models, and all metrics, our proposed methods significantly outperform both APG and DUET. We conducted experiments on four other commonly used recommendation datasets and compared the UAUC metric in Figures 3 and 4. Our method ({SR=SASRec, DSR=DUET}) significantly outperforms other SR and DSR Models across all the datasets."}, {"title": "4.3 Ablation Study", "content": "We conduct ablation studies on each dataset, each SR and each DSR to further analyze the impact of modules and modalities. The ablation results on each dataset, DR, and DSR combinations are similar, so we only show the results under the condition {Dataset=Arts, SR=SASRec, DSR=DUET}. Each row's \u2713 and X respectively indicate with and without the module/modality."}, {"title": "4.3.1 Ablation Study on Modules.", "content": "As shown in Table 2, we conduct an ablation study on each module proposed in our method, SPG stands for Semantic Parameter Generation, SML stands for Semancic Metacode Learning, and SCL stands for Semancic Codebook Learning. Since SPG is a prerequisite for SML, SML cannot exist independently of SPG; therefore, there is no separate performance data for SML alone in the table. The first line represents the traditional DSR model where parameters are generated using an item sequence. The second line represents generating parameters using a semantic sequence. The third line represents the joint generation of parameters using both item sequence and semantic sequence, with joint training. The fourth line represents using semantic codebook learning without using semantic information. The fifth line represents our complete method. The experiments show that the model performs best when all three modules are used. In terms of individual modules, SCL has the greatest impact on performance."}, {"title": "4.3.2 Ablation Study on Modalities.", "content": "As shown in Table 3, we conduct ablation study on each modality. The experimental results show that the fusion of three modalities-ID, Image, and Text-is not necessarily the best option. In terms of the impact on performance for individual modalities, Text > Image > ID. For the fusion of two modalities, in terms of impact on performance, ID + Text > Image + Text > ID + Image."}, {"title": "4.4 Depth Analysis", "content": "We further conduct depth analysis to demonstrate the effectiveness. Unless otherwise specified, the dataset, SR, and DSR default to Arts, SASRec, and DUET, respectively. Note that we get similar results for all settings, but only a subset of them are shown here."}, {"title": "4.4.1 Stability and Robustness.", "content": "We tested the variance of the UAUC for SOLID and DUET on each user in the Arts dataset when faced with similar user behaviors. Specifically, we added one user behavior at a time for each user behavior and calculated the performance variance. We then aggregated the variances for all users to obtain the median, mean, minimum, and maximum of these variances. Table 4 shows that SOLID has stronger stability and robustness compared to DUET."}, {"title": "4.4.2 Cost Comparison.", "content": "In Table 5, we do analysis based on the BERT4Rec (the biggest SR in our paper), the increased memory and time are not important because the increase is slight and does not affect real-time performance [29, 49]."}, {"title": "4.4.3 Hyperparameter Analysis.", "content": "To analyze the impace of the main hyperparameters $\\lambda$ and $\\Gamma$, we conduct grid search experiment. As shown in Figure 5, the horizontal axis represents $\\lambda$, and the vertical axis represents $\\Gamma$. The depth of the color and the radius of the circle represent the magnitude of the value; the larger the value, the deeper the color and the larger the circle (i.e., the larger the radius). Blue, green, and orange represent the metrics UAUC, NDCG@10, and Recall@10, respectively. The results show that the best performance is achieved when $\\lambda$ = 0.1 and $\\Gamma$ = 0.01."}, {"title": "5 CONCLUSION", "content": "In this paper, we have presented the Semantic Codebook Learning for Dynamic Recommendation Models (SOLID) as a solution to the limitations faced by existing dynamic sequence recommendation systems (DSR). Our framework integrates multimodal information, including images and text, with user-item interactions to enhance recommendation accuracy and adaptability. By disentangling model parameters into trunk parameters capturing generalized user behavior trends and branch parameters tailored to individual user actions, SOLID offers a more efficient and effective recommendation system. Through extensive experimentation across multiple datasets, we have demonstrated that SOLID significantly outperforms previous DSR models, with an significant improvement on extensive datasets and models. These results underscore the potential of leveraging multimodal information to advance the capabilities of dynamic recommendation systems, paving the way for more personalized and responsive user experiences in the era of digital personalization."}, {"title": "A APPENDIX", "content": "This is the Appendix for \"Semantic Codebook Learning for Dynamic Recommendation Models\"."}, {"title": "A.1 Supplementary Experiments", "content": ""}, {"title": "A.1.1 Datasets.", "content": "The statistics of the datasets used in the experiments is shown in Table 6."}, {"title": "A.1.2 Hyperparameters and Training Schedules.", "content": "We summarize the hyperparameters and training schedules of the datasets used in the experiments in Table 7."}]}