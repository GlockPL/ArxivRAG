{"title": "DLF: Disentangled-Language-Focused Multimodal Sentiment Analysis", "authors": ["Pan Wang", "Qiang Zhou", "Yawen Wu", "Tianlong Chen", "Jingtong Hu"], "abstract": "Multimodal Sentiment Analysis (MSA) leverages heterogeneous modalities, such as language, vision, and audio, to enhance the understanding of human sentiment. While existing models often focus on extracting shared information across modalities or directly fusing heterogeneous modalities, such approaches can introduce redundancy and conflicts due to equal treatment of all modalities and the mutual transfer of information between modality pairs. To address these issues, we propose a Disentangled-Language-Focused (DLF) multimodal representation learning framework, which incorporates a feature disentanglement module to separate modality-shared and modality-specific information. To further reduce redundancy and enhance language-targeted features, four geometric measures are introduced to refine the disentanglement process. A Language-Focused Attractor (LFA) is further developed to strengthen language representation by leveraging complementary modality-specific information through a language-guided cross-attention mechanism. The framework also employs hierarchical predictions to improve overall accuracy. Extensive experiments on two popular MSA datasets, CMU-MOSI and CMU-MOSEI, demonstrate the significant performance gains achieved by the proposed DLF framework. Comprehensive ablation studies further validate the effectiveness of the feature disentanglement module, language-focused attractor, and hierarchical predictions.", "sections": [{"title": "Introduction", "content": "With the rapid development of social media, multimodal interaction has become increasingly popular, which attracts many researchers to transfer uni-modal learning to multimodal learning tasks (Awal et al. 2024; Guan et al. 2024; Xu, Zhu, and Clifton 2023). One of the most significant subfields is multimodal sentiment analysis (MSA) (Geetha et al. 2024; Yang et al. 2022a). MSA aims to perceive human sentiment through multiple heterogeneous modalities, such as language, vision, and audio, playing a crucial role in many applications including cognitive psychology, scenario understanding, and mental health (Ali and Hughes 2023; Ezzameli and Mahersia 2023; Yang et al. 2023a). Compared with unimodal solutions, MSA often presents a more robust performance by leveraging complementary information from different modalities. How to effectively learn essential representations without redundant and conflicting information from multiple heterogeneous modalities, however, remains an open question in the academic field, especially in multimodal learning communities.\nIn recent years, researchers have shown an increased interest in MSA. Many multimodal models have been proposed to facilitate MSA, and they can be categorized into two groups: representation learning-oriented methods (Guo et al. 2022; Hazarika, Zimmermann, and Poria 2020; Sun et al. 2023; Yang et al. 2022c) and multimodal fusion-oriented methods (Zhang et al. 2023; Huang et al. 2020; Yang et al. 2022b; Tsai et al. 2019; Lv et al. 2021; Rahman et al. 2020). The former primarily aims to acquire an advanced semantic understanding of various modalities enriched with diverse clues of human sentiments, resulting in more powerful human sentiment encoders. Conversely, the latter emphasizes designing sophisticated fusion strategies at various levels, including feature-level, decision-level, and model-level fusion, to derive unified representations from multimodal data. It is worth noting that the fundamental aspect of MSA lies in learning and integrating multimodal representations, where the goal is to accurately process and integrate various modal inputs to discern sentiments from the underlying data. Although current leading methods in MSA"}, {"title": null, "content": "(Hazarika, Zimmermann, and Poria 2020; Tsai et al. 2019; Zadeh et al. 2017; Yu et al. 2021) have shown considerable progress, the inherent disparities across diverse modalities continue to present challenges, complicating the development of stable and effective multimodal representation. For MSA, as shown in Figure 1, existing works and our ablation study (see Table 2) have shown that language, vision, and audio sources contribute differently to the overall prediction performance (Pham et al. 2019; Tsai et al. 2019; Kim and Park 2023; Li et al. 2024a; Lei et al. 2023), which indicates the big distribution gap among different modalities hinders the final performance.\nTo mitigate the distribution gap among heterogeneous modalities, as shown in Figure 1, knowledge distillation-based methods, such as cross-modal and graph distillation, are introduced to transfer reliable information between different modalities (Gupta, Hoffman, and Malik 2016; Guo et al. 2020; Aslam et al. 2023; Kim and Kang 2022; Hazarika, Zimmermann, and Poria 2020; Li, Wang, and Cui 2023; Tsai et al. 2019). Cross-modal distillation typically leverages the stronger Language modality to teach weaker modalities (Vision and Audio), while graph distillation completely performs bidirectional information transfer between all modality pairs. However, it is important to note that conventional distillation is inherently asymmetric-it is effective when transferring information from one modality to another, but the benefits of the reciprocal process are unclear. This asymmetry can lead to redundant or even conflicting information in cross-modal and graph distillation, ultimately limiting overall performance.\nTo this end, we critically reconsider the characteristics illustrated in Figure 1: Why focus solely on bridging the gap between different modalities, rather than strategically enhancing the strengths of the dominant one? However, directly enhancing the dominant modality while treating all modalities equally and employing bidirectional information transfer across all modality pairs often introduces redundancy and conflicts (Hazarika, Zimmermann, and Poria 2020), thereby reducing overall performance. In contrast, our work strategically leverages a pivotal characteristic of MSA: language has been empirically recognized as the dominant modality (Tsai et al. 2019). Building on this insight, we intend to develop a novel Language-Focused Attractor (LFA), a targeted enhancement scheme designed to transfer complementary information exclusively to the dominant language modality, which consolidates information through pathways such as Video \u2192 Language, Audio \u2192 Language, and Language \u2192 Language, resulting in effectively minimizing redundancy and conflicting information and improving overall MSA accuracy.\nTo achieve this, we propose a Disentangled-Language-Focused (DLF) multimodal representation learning framework to fully exploit the potential of language-dominant MSA. The framework follows a structured pipeline: feature extraction, disentanglement, enhancement, fusion, and prediction. To specifically address the issues of redundancy and conflicting information to facilitate language-targeted feature enhancement, DLF introduces four geometric measures as regularization terms in the total loss function, ef-"}, {"title": null, "content": "fectively refining shared and specific spaces both separately and jointly. Within the modality-specific space, we further develop the LFA to enhance language representation by attracting complementary information from other modalities. This process is guided by a Language-Query-based multimodal cross-attention mechanism, ensuring precise and targeted feature enhancement between heterogeneous modality pairs (X\u2192Language, where X refers to Language, Video, or Audio). Finally, the enhanced shared and specific features are fused, followed by hierarchical predictions to further improve overall prediction accuracy.\nOur main contributions can be summarized as follows:\n\u2022 Proposed Framework: In this study, we propose a Disentangled-Language-Focused (DLF) multimodal representation learning framework to promote MSA tasks. The DLF framework presents a structured pipeline: feature extraction, disentanglement, enhancement, fusion, and prediction.\n\u2022 Language-Focused Attractor (LFA): We develop the LFA to fully harness the potential of the dominant language modality within the modality-specific space. The LFA exploits the language-guided multimodal cross-attention mechanisms to achieve a targeted feature enhancement (X\u2192Language).\n\u2022 Hierarchical Predictions: We devise hierarchical predictions to leverage the pre-fused and post-fused features, improving the total MSA accuracy. Comprehensive ablation studies further validate the effectiveness of each component in the DLF framework."}, {"title": "Related Work", "content": "Multimodal Sentiment Analysis\nMultimodal Sentiment Analysis (MSA) integrates information from diverse modalities, such as language, video, and audio (Ali and Hughes 2023; Ezzameli and Mahersia 2023). Mainstream methods can be categorized into representation learning-oriented (Guo et al. 2022; Sun et al. 2023; Yang et al. 2022c) and fusion-oriented approaches (Zhang et al. 2023; Huang et al. 2020; Tsai et al. 2019). Representation methods like (Guo et al. 2022), enhance cross-modal interactions, while fusion techniques, including Transformer-based model (Huang et al. 2020), focus on combining features effectively. Despite progress, performance disparities among modalities hinder the overall prediction accuracy.\nTo this end, distillation-based strategies were introduced to bridge the gap. For instance, Kim and Kang (2022) proposed cross-modal distillation between textual and auditory modalities to enhance emotion classification granularity. To dynamically adapt to distillation, ternary-symmetric architectures (MulT) (Tsai et al. 2019) or graph distillation units (Zadeh et al. 2018b; Li, Wang, and Cui 2023) were introduced, representing modalities as vertices and their interactions as edges. Recent approaches also leverage large multimodal language models for flexible interactions (Wu et al. 2023) and integrate contextual knowledge to boost predictions (Wang et al. 2024). However, previous paradigms treat different modalities equally, easily causing redundant and"}, {"title": "Proposed Approach", "content": "Preliminaries. As shown in Figure 2, the task of MSA aims to predict the sentiment intensity or label of given multimodal inputs. In DLF, three modalities are concurrently considered, such as Language (L), Vision (V), and Audio (A), represented as 2D tensors $\\hat{X}_m \\in \\mathbb{R}^{N_m\\times d_m}$, where $N_m$ is the sequence length, $d_m$ is the embedding dimension, and $m \\in \\{L, V, A\\}$ means different modalities.\nOverview\nThe framework of the proposed DLF is illustrated in Figure 2. It adopts a structured pipeline comprising feature extraction, disentanglement, enhancement, fusion, and prediction. The framework integrates three core components: the feature disentanglement module, the Language-Focused Attractor (LFA), and hierarchical predictions (shared, specific, and final predictions). DLF decomposes multimodal features into modality-shared and modality-specific spaces to minimize redundancy and conflicts among heterogeneous modalities. To reinforce this decoupling, four geometric measures are incorporated into the total loss as regularization terms. Additionally, the LFA is designed to leverage the dominant language modality by integrating complementary information from other modalities, thereby enhancing language representation. Finally, hierarchical predictions are performed to boost overall MSA performance. The details are as follows:"}, {"title": "Feature Disentanglement Module", "content": "To reduce redundant and conflicting information, the proposed DLF framework utilizes a shared encoder and three modality-specific encoders to decompose multimodal information into modality-shared and modality-specific feature spaces, denoted as $S_h^m$ and $S_p^m$, respectively, where"}, {"title": null, "content": "$m \\in \\{V, L, A\\}$. Formally, the shared and specific encoders are defined as:\n$\\begin{aligned}\nS_h^m &= E_{sh}(X_m),  \\\\\nS_p^m &= E_{sp}(X_m),\n\\end{aligned}$\nwhere $E_{sh}$ and $E_{sp}$ represent the shared and specific encoders, respectively. In this work, both encoders are implemented as cascaded Transformer layers.\nFor effective disentanglement, DLF incorporates the regularization effect of carefully designed regularization terms. While classical approaches often employ distribution similarity measures such as KL-Divergence along the hidden dimensions (Kim and Mnih 2018), we adopt four geometric measures based on Euclidean distances and cosine similarity due to their intuitive nature and computational efficiency.\nAfter initial disentanglement, DLF concatenates $S_h^m$ and $S_p^m$ for each modality and reconstruct the multimodal input $\\hat{X}_m$, resulting $X'_m$ by decoding the fused features $[S_h^m \\oplus S_p^m]$. This process can be formulated as follows:\n$X'_m = D_m([S_h^m \\oplus S_p^m]),$\nwhere $D_m$ is the 1D convolution decoder, and $\\oplus$ is the concatenation operation. The discrepancy between the low-level features $\\hat{X}_m$ and the generated features $X'_m$, called the reconstruction loss $L_r$, can serve as a regularization term contributing to the feature disentanglement module:\n$L_r = ||\\hat{X}_m - X'_m||^2.$\nFurthermore, the modality-specific reconstruction process can be formulated as:\n$S_p^{m'} = E_{sp}(X'_m),$\nwhere $S_p^{m'}$ is estimated modality-specific features from the modality-specific reconstruction process. Naturally, the discrepancy between original modality-specific features $S_p^m$ and the estimated ones $S_p^{m'}$ can be regarded as the specific loss $L_s$:\n$L_s = ||S_p^m - S_p^{m'}||^2.$\nAlthough the reconstruction loss $L_r$ and specific loss $L_s$ contribute to the decoupling process, their effectiveness in achieving robust disentanglement remains limited. This limitation arises from the potential sub-optimal performance of the shared encoder during the initial training phase, especially when compared to the modality-specific encoder. Such a disparity, if left unaddressed, is exacerbated by these loss functions, causing an increasing divergence between the two encoders as training progresses. Therefore, we incorporate a modified triplet loss (Schroff, Kalenichenko, and Philbin 2015) to enhance the performance of the modality-shared encoder. The triplet loss is defined as:\n$\\begin{aligned}\nL_m = \\frac{1}{T} max (0, d (S, P) - d (S, N) + \\mu), \n\\end{aligned}$\nwhere $S$ represents a sampled modality in the modality-shared space, $P$ denotes the positive sample corresponding to the representation of the same sentiment across different"}, {"title": null, "content": "modalities, and $N$ refers to the negative sample representing distinct sentiments within the same modality. $T$ is the total number of positive and negative samples, $d(\\cdot, \\cdot)$ computes the cosine similarity between two feature vectors, and $\\mu$ represents a distance margin.\nThe aforementioned losses, $L_r$, $L_s$, and $L_m$, regulate the shared and specific features to ensure they focus on their respective objectives. To further refine the decoupling between these two spaces, a soft orthogonality loss, $L_o$, is introduced to minimize redundancy and conflicts between shared and specific multimodal features. It is defined as:\n$L_o = O(S_h^m, S_p^m),$\nwhere $O(\\cdot,\\cdot)$ represents a non-negative counterpart of cosine similarity, promoting orthogonality between the two feature spaces.\nEventually, the four geometric-measure-based regularization terms, addressing both inter- and intra-decoupled spaces, are combined to form the decoupling loss:\n$L_d = \\sum_{k \\in \\{r,s,m,o\\}} \\lambda_k L_k,$\nwhere $\\lambda_k$ are weighting coefficients for the individual regularization terms, providing a flexible mechanism to balance their contributions and calibrate the model effectively."}, {"title": "Language-Focused Attractor (LFA)", "content": "Unlike conventional feature enhancement methods that aim to bridge modality gaps through cross-modal and graph distillation (Gupta, Hoffman, and Malik 2016; Guo et al. 2020; Aslam et al. 2023; Li, Wang, and Cui 2023), we propose the LFA in the modality-specific space after feature decoupling. The detailed structure of LFA is depicted in Figure 3.\nIn the LFA, decoupled modality-specific features $S_p^m$ (where $m \\in \\{L, V, A\\}$, representing language, vision, and audio) are first processed through positional embedding and dropout, then fed into Multimodal Transformer layers. The core operation within these layers is the Multi-modal Cross-Attention (MCA) mechanism. LFA performs three branches of MCA, including one self-attention and two cross-attention mechanisms, all centered on the language modality as the Query ($Q_L$). This setup allows the language modality to attract complementary information from other modality-specific features $S_p^m$, where $m \\in \\{L, V, A\\}$. The corresponding Key-Value pairs are defined as $(K_m, V_m)$. The MCA operation is mathematically expressed as:\n$\\begin{aligned}\n\\operatorname{MCA}(Q_L, K_m, V_m)=\n\\operatorname{softmax}\\left(\\frac{Q_L K_m^T}{\\sqrt{d}}\\right) (S_p^m)W_{V_m}\n=\\operatorname{softmax}\\left(\\frac{(S_p^L)W_{Q_L}((S_p^m)W_{K_m})^T}{\\sqrt{d}}\\right) (S_p^m)W_{V_m}\n\\end{aligned}$\nwhere $m \\in \\{L, V, A\\}$, softmax represents normalized attention score between $Q_L$ and $K_m$, $W_{Q_L}$ and $W_{K_m}$ are learnable parameters, $d$ indicates the dimension of $Q_L$ and"}, {"title": null, "content": "$K_m$. Consequently, the language-focused feature enhancement in the Multimodal Transformer is defined as:\n$\\begin{aligned}\nh^{m+1} &= \\operatorname{LayerNorm}(h^m + \\operatorname{Drop}(\\operatorname{MCA}(h^m)))\\\\\nh^m &= \\operatorname{LayerNorm}(h^{m+1} + \\operatorname{FFN}(h^{m+1}))\n\\end{aligned}$\nwhere $m \\in \\{L, V, A\\}$, $\\operatorname{Drop}(\\cdot)$ denotes the Dropout operation, and $\\operatorname{FFN}(\\cdot)$ represents a feed-forward module. Here, $h^m$ and $h^{m+1}$ are the input and intermediate features, respectively, while $h_o^m$ is the output of a Multimodal Transformer layer. As illustrated in Figure 3, cascaded Multimodal Transformers are employed to enhance modality-specific features.\nThe LFA effectively leverages modality-specific features from three modalities, aligning them with the language modality to strengthen multimodal representation. As shown in Figure 2, the enhanced features are projected into higher-level specific features, denoted as $HS_p^m$ ($m \\in \\{L, V, A\\}$), and then integrated with enhanced shared features. Additionally, these features are processed by modality-specific predictors for the specific prediction.\nMultimodal Fusion. As illustrated in Figure 2, modality-shared features are first processed through a unified Transformer layer, followed by two fully connected layers, and then projected into higher-level shared features, denoted as $HS_h$. Finally, the multimodal fusion layer combines $HS_h$ with $HS_p^m$ to form the final multimodal features:\n$F(HS_h, HS_p^m)$\n$= Concat(HS_p^L, HS_p^V, HS_p^A, HS_h),$\nwhere $Concat$ denotes the concatenation operation.\nHierarchical Predictions\nAs shown in Figure 2, a classifier can predict the MSA output $\\hat{y}$ after multimodal fusion. The final MSA output loss $L_f$"}, {"title": null, "content": "is defined as:\n$L_f = \\frac{1}{N_d} \\sum_{n=0}^{N_d} |\\hat{y}_n - y_n|,$ (13)\nwhere $y_n$ is the MSA label, $N_d$ is the number of samples. Unlike traditional MSA learning, which only involves a single output loss $L_f$, the proposed DLF explores hierarchical predictions considering modality-shared loss $L_{sh}$, modality-specific loss $L_{sp^m}$, and the output loss $L_f$ concurrently. The total MSA learning loss is thus expressed as:\n$L_{MSA} = \\sum_{l \\in \\{f,Sh, Sp^m\\}} \\beta_l L_l,$ (14)\nwhere $m \\in \\{L, V, A\\}$, $\\beta_l$ are weighting coefficients that control the relative importance of different losses.\nOverall Learning Objective. The proposed DLF framework integrates the decoupling loss $L_d$ and the total MSA learning loss $L_{MSA}$ to form the overall learning objective:\n$L_{DLF} = L_d + L_{MSA}.$"}, {"title": "Experiments", "content": "Datasets and Evaluation Metrics\nWe evaluate DLF on two widely used datasets: CMU Multimodal Sentiment Intensity (MOSI) (Zadeh et al. 2016) and CMU Multimodal Opinion Sentiment and Emotion Intensity (MOSEI) (Zadeh et al. 2018b).\nMOSI. The MOSI dataset comprises 2,199 monologue video clips, with audio and visual features extracted at 12.5 Hz and 15 Hz, respectively. The dataset is divided into 1,284 training, 229 validation, and 686 test samples.\nMOSEI. The MOSEI dataset, significantly larger, consists of 22,856 movie review video clips sourced from YouTube. Features are extracted at 20 Hz for audio and 15 Hz for visual modalities. The dataset is split into 16,326 training samples, 1,871 validation samples, and 4,659 test samples. For both datasets, each video clip is annotated with a sentiment score ranging from -3 to 3, representing a spectrum from highly negative to highly positive sentiment.\nEvaluation Metrics. Consistent with established practices in previous studies (Liang et al. 2021; Lv et al. 2021; Mao et al. 2022), the performance of MSA is evaluated using multiple metrics: 7-class accuracy (Acc-7), 5-class accuracy (Acc-5), binary accuracy (Acc-2), F1 score, correlation between model predictions and human annotations (Corr), and mean absolute error (MAE). These metrics collectively offer a comprehensive assessment of DLF's effectiveness across various sentiment analysis tasks.\nImplementation Details\nIn this study, we align our methodology with previous works (Hazarika, Zimmermann, and Poria 2020; Mao et al. 2022) by utilizing the BERT-base-uncased model (Devlin et al. 2018) to extract unimodal linguistic features. This process generates word representations with a 768-dimensional hidden state. For visual data, DLF employs the Facet framework (Baltru\u0161aitis, Robinson, and Morency 2016) to encode each"}, {"title": "Ablation Study", "content": "We conduct extensive ablation studies to thoroughly examine the impact of various modality combinations, different regularization strategies, and critical components including the Feature Disentanglement Module (FDM), Language-Focused Attractor (LFA), and Hierarchical Predictions (HP).\nVarious Modality Combinations. As shown in Table 2, all analysis are conducted on the MOSI dataset. We first present the performance of each unimodality, it is easy to notice that language modality (L) serves as the dominant one. In the bi-modalities case, we consider both (L,A) and (L,V) pairs in our DLF framework, the results not only demonstrate the performance improvement, especially the fine-grained classification, through two modalities but also showcase that language modality attracts useful information from vision (V) or audio (A) modality by LFA to enhance the multimodal representation capability. Furthermore, the tri-modalities DLF consistently outperforms the bi-modalities DLF on all metrics, which indicates that each modality provides a unique contribution and multimodal learning can effectively improve the MSA performance by reasonably exploiting the information from different modalities.\nDifferent Regularization. We remove each loss to verify the importance of different regularization terms. When removing the soft orthogonality loss $L_o$, the DLF learns decoupled features under the constraints that focus on separate subspaces. The worst performance suggests the importance of the soft orthogonality loss considering the shared and specific subspaces jointly in the feature disentanglement module. Meanwhile, we also notice that the modified triplet loss $L_m$ in the shared subspace improves the overall performance, which indicates the importance of $L_m$ in learning shared features in the shared subspace. Besides, we observe that both reconstruction loss $L_r$ and specific loss $L_s$ contribute to the model's performance. This is because these two losses ensure feature consistency during disentanglement.\nCritical Components. To verify the effectiveness of dif-"}, {"title": "Further Analysis", "content": "To further study the impact of sentiment granularity on MSA, we present the confusion matrix and corresponding accuracy of each sentiment for the MOSI benchmark. As shown in Figure 4, we observe that most sentiment classes have similar accuracy above 40%. However, \u201cHN\u201d and \"HP\", especially \u201cHN\u201d, have the worst performance, limiting the overall MSA performance. Furthermore, when we dive into the confusion matrix, it can be noticed that the samples for \"HN\u201d and \u201cHP\" are relatively less than other sentiments, which strongly indicates that the long-tailed distribution of the data limits the overall MSA performance, which can be studied in the future.\nTo better understand the effectiveness of our method, we visualize the distribution of fused multimodal representations. As depicted in Figure 5, compared to DMD, which is a decoupled-multimodal-distillation strategy, our proposed DLF shows superior performance to separate different sentiments. This is mainly due to that the LFA mitigates redundant and conflicting information during multimodal interaction compared to the interaction between random pairs."}, {"title": "Conclusion", "content": "In this paper, we propose the DLF framework to improve the MSA performance. DLF yields powerful multimodal representations by following the pipeline of feature extraction, disentanglement, enhancement, fusion, and prediction, mainly benefiting from the feature disentanglement module, language-focused attractor, and hierarchical predictions. Extensive results verify the superiority of DLF by comparisons with eleven baselines and comprehensive ablation studies.\nBroad impacts. (i) This study demonstrates its potential to exploit the imbalanced capabilities of various modalities in multimodal learning, thereby setting a new benchmark in this field. (ii) The proposed LFA facilitates the generalization of our method to other multimodal scenarios by changing the dominant modality. Limitation and future work. Our method only considers the scenarios of complete modalities. When facing missing modalities, the feature disentanglement and enhancement modules are potentially limited."}]}