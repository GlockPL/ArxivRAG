{"title": "MULTI-CONVFORMER: Extending Conformer with Multiple Convolution Kernels", "authors": ["Darshan Prabhu", "Yifan Peng", "Preethi Jyothi", "Shinji Watanabe"], "abstract": "Convolutions have become essential in state-of-the-art end-to-end Automatic Speech Recognition (ASR) systems due to their efficient modelling of local context. Notably, its use in Conformers has led to superior performance compared to vanilla Transformer-based ASR systems. While components other than the convolution module in the Conformer have been reexamined, altering the convolution module itself has been far less explored. Towards this, we introduce MULTI-CONVFORMER that uses multiple convolution kernels within the convolution module of the Conformer in conjunction with gating. This helps in improved modeling of local dependencies at varying granularities. Our model rivals existing Conformer variants such as CgMLP and E-Branchformer in performance, while being more parameter efficient. We empirically compare our approach with Conformer and its variants across four different datasets and three different modelling paradigms and show up to 8% relative word error rate (WER) improvements.", "sections": [{"title": "1. Introduction", "content": "In recent years, end-to-end automatic speech recognition (ASR) systems have emerged as the preferred model of choice to obtain state-of-the-art ASR performance. An integral component of such systems is the speech encoder [1, 2] that uses Attention via Transformers [3] to map the input speech into high-level acoustic representations. Transformer-based ASR systems [4, 5] often struggle to model local relationships effectively. To address this limitation, Gulati et al. [6] proposed the Conformer [7] architecture, that combines multi-headed attention with convolutions [8-10]. With the advent of Conformer models, the idea of using convolutions alongside attention to independently model both local and global relationships has been widely explored [11-14].\nDespite these notable advancements, prior work [11] has shown that the use of fixed-kernel convolutions within these models creates a bottleneck, forcing the model to re-purpose some of its attention heads to function as local information extractors. This negatively impacts the performance of attention, whose primary purpose is to model global information. To address this limitation, in our work, we propose a multiple convolution-based enhancement to the Conformer architecture (that we call MULTI-CONVFORMER). Additionally, we incorporate gating, a technique that has proven to be effective in encoder architectures [11, 12, 15]. By combining these approaches, we achieve significant improvements (up to 8% relative WER improvement) over the original Conformer architecture and perform at par or better than its variants such as CgMLP [15] and E-Branchformer [12]. The use of multiple convolutions in order to generate better local context has been widely adopted in image-related tasks [16-20]. Such enhancements have also been used in speech emotion recognition [21] and robust ASR [22, 23]; the latter works use multiple convolutions within fully convolutional or TDNN-style architectures (unlike our work).\nIn summary, our main contributions are as follows:\n\u2022 We propose MULTI-CONVFORMER, a variant of Conformer that uses multiple convolution kernels instead of a fixed kernel convolution to capture local context more effectively.\n\u2022 We show the effectiveness of our approach by comparing with Conformer and its variants on ASR and Spoken Language Understanding (SLU). We experiment with multiple datasets (Librispeech [24], Tedlium2 [25], AISHELL [26] and SLURP [27]) and various modelling paradigms and obtain up to 8% relative WER improvement over Conformer. We also conduct several analyses and ablations to showcase the effectiveness and interpretability of our approach."}, {"title": "2. Methodology", "content": "In this work, we experiment with the three most popular ASR architectures: the attention-based encoder-decoder model (AED) [28], the encoder-only model (pure CTC) [29] and the RNN-Transducer model (RNN-T) [30]. An essential component common to all these architectures is the encoder (ENC) module that maps an input sequence of speech features \\(X = \\{X_1, X_2,..., X_L | X_i \u2208 R^d\\}\\) to a (typically smaller) sequence of contextualized representations \\(H = ENC(X) = \\{h_1, h_2,...h_t | h_i \u2208 R^d\\}\\). Thereafter, the manner in which H is trained to predict the final M-length ground-truth token sequence \\(Y = \\{Y_1, Y_2,..., Y_M | Y_i \u2208 N^+\\}\\) depends on the underlying architecture. AED uses an attention-based decoder and a CTC module to jointly learn a mapping from H to Y. However, pure CTC and RNN-T are encoder-only models that employ CTC [31] and RNN-T [30] losses, respectively, to generate Y. Since our modifications are constrained to the encoder, in subsequent sections, we restrict our discussion only to the composition of the ENC module."}, {"title": "2.1. MULTI-CONVFORMER Encoder", "content": "Figure 1 illustrates the overall architecture of a single MULTICONVFORMER encoder layer and Figure 2 gives an overview of how the outputs from multiple convolutions are merged together. The MULTI-CONVFORMER encoder layer consists of four blocks that are stacked together and interspersed with layer normalization [32] and residual connections. The two positionwise feed-forward layers aid in refining the point-wise information, while the multi-head attention and convolution are responsible for incorporating contextual information. This stacked architecture has been widely used in prior work [6, 11, 12, 33]. We adopt this same stack, but replace the fixed single-kernel convolution block with a more expressive multi-kernel convolution module that will henceforth be referred to as MULTICONV.\nAs illustrated in Figure 1, in a single encoder layer, the output from the multi-head attention block \\(A = \\{a_1, a_2,..., a_j | a_j \u2208 R^d\\}\\) is first normalized with a layer normalization. Subsequently, it undergoes channel projection to increase its dimensionality from d to \\(d_{inter}\\). To introduce non-linearity to the representation, we apply GELU [37] activation. The resulting output, \\(\\hat{A} = \\{\\hat{a}_1, \\hat{a}_2,\u2026\u2026\u2026, \\hat{a}_{j} | \\hat{a}_{j} \u2208 R^{d_{inter}}\\}\\), is then passed through our Multi-kernel Convolutional Spatial Gating Unit (M-CSGU). M-CSGU is a more powerful alternative [11, 12] to the standard convolution block due to its usage of gates [38] along with convolutions. Finally, we employ another channel projection layer that projects the output from \\(R^{d_{inter}}\\) back to \\(R^d\\), followed by dropout for regularization.\nMulti-kernel Convolutional Spatial Gating Unit (M-CSGU): M-CSGU module takes \\(\\hat{A}\\) as its input. We first bifurcate each representation in \\(\\hat{A}\\) into two parts, each having dimension \\(d' = d_{inter}/2\\). Only one part undergoes layer normalization and passes through multiple convolutions; the other part stays intact. These parts are multiplied element-wise, thus creating a gate. Next, we use P depthwise convolutions with kernel sizes \\(K = \\{k_1, k_2,..., k_P\\}\\). Formally, these operations are as follows:\n\\([Z_l, Z_r] = [\\hat{A}[:, : d'], LayerNorm(\\hat{A}[:, d' :])]\\)\n\\([V_1, V_2, ..., V_P] = [Conv_{k_1}(Z_r),..., Conv_{k_P}(Z_r)]\\)\n\\(\\tilde{Z_r} = FUSION([V_1, V_2, ..., V_P])\\)\n\\(\\hat{C} = Z_l \\odot \\tilde{Z_r}\\)\nwhere \\(\\hat{A}, \\hat{C} \u2208 R^{T \\times d_{inter}}\\), \\(Z_l, Z_r, V_j, \\tilde{Z_r} \u2208 R^{T\\times d'}\\), \\(\\odot\\) represents element-wise products, \\(Conv_{k_i}()\\) refers to a depthwise convolution with kernel size of \\(k_i\\) and \\(\\hat{C}\\) is the final output from this block which is further passed to a position-wise feed-forward layer. Since FUSION must preserve the dimensionality of the input, the number of input channels to each \\(Conv_{k_i}\\) becomes \\(d'\\), however the size of the output channels depends on the nature of the \\(FUSION()\\) operation. We explore four different fusion mechanisms, shown in Figure 2, that we can further group into two categories: Sum-based and Concat-based FUSION.\nSum-based FUSION: In this mechanism, both the input and output channels are of the same size. The outputs obtained from each convolution are combined using an element-wise addition operation as shown in Figure 2(a). That is, \\(\\tilde{Z_r}\\) in Equation 1 is computed as: \\(\\tilde{Z_r} = V_1 + V_2 ... + V_P\\). In our experiments, we refer to this fusion approach as MULTICONVsum. We further enhance this fusion by learning weights that decide the importance of each convolution for every frame of the input, as shown in Figure 2(b) and defined formally below:\n\\(\\alpha = \\{\\alpha_1,..., \\alpha_P\\} = Softmax(FFN_{d'\\rightarrow P}(Z))\\)\n\\(\\tilde{Z_r} = \\alpha_1 V_1 + \\alpha_2 V_2 + ... + \\alpha_P. V_P\\)\n\\(\\tilde{Z_r} = [\\tilde{Z}, Z_2,..., \\tilde{Z_T}]\\) "}, {"title": "3. Experimental Setup", "content": "We conduct experiments on five datasets namely Librispeech-100h (LS-100) [24], Librispeech-960h (LS-960) [24], Tedlium-2 [25], AISHELL-1 [26] and SLURP [27]. We use ESPnet toolkit [39] to run all our experiments on a combination of NVIDIA RTX A6000 and NVIDIA Tesla V100 GPUs. All our models take 80-dimensional log-Mel features as input that are extracted with a 25ms window size and 10ms stride. We also use 3-way speed perturbation with ratios {0.9, 1.0, 1.1} and SpecAugment [40]. In all our experiments, we use the experimental settings recommended in ESPnet recipes. For all datasets except LS-960 and SLURP, the encoder-decoder architecture consists of 12 encoder and 6 decoder layers with an attention dimension of d = 256 and 4 attention heads. However, for LS-960 and SLURP, we use an 18 layer encoder with 8 attention heads and an attention dimension of d = 512."}, {"title": "4. Experimental Results and Analysis", "content": "Table 1 compares all four variants of our proposed MULTICONVFORMER (elaborated in Section 2.1) to Transformer [35] and Conformer [6] models. Our method significantly outperforms both these approaches across all three datasets and multiple modelling paradigms; results that are statistically significant are shown with \u2021. Additionally, we find that among the four FUSION methods (Figure 2), MULTICONVsum and MULTICONVdepth perform better. Henceforth, we will only show results with using the MULTICONVdepth fusion strategy."}, {"title": "4.1. ASR and SLU Experiments", "content": "Comparison with Conformer variants. Table 2 shows the WER comparison between our system and two Conformer variants: CgConv (Conformer with convolution replaced by Convolutional Spatial Gating Unit [15]) and E-Branchformer (Conformer with disentangled attention and convolution branches) [12, 42]. We note here that MULTICONVsum can be considered as an improved version of CgConv, as it reduces to CgConv when a single fixed convolution kernel is employed instead of multiple kernels. Further, we find that the use of a gate in conjunction with convolution allows the model to selectively utilize convolution, thereby introducing a natural branching capability. As a result, our proposed architecture can be viewed as an enhancement to CgConv and a parameter-efficient variant of Branchformer that achieves comparable or better performance on speech-related tasks.\nIn Figure 3, we compare the diagonal properties of self-attention blocks among Transformer, Conformer, and MULTI-CONVFORMER via the diagonality metric [11, 41]. This metric is an indication of the degree to which attention heads focus on capturing local information rather than global information. We find that MULTI-CONVFORMER allows for more global self-attention blocks, resulting in a decreased diagonality value when compared to both Transformer and Conformer, yielding 17% and 3% relative reductions in average diagonality values across all layers, respectively.\nPerformance on Librispeech-960h. Table 3 compares WERs of our proposed system with other architectures on the full Librispeech [24] dataset. For baselines, we reuse the numbers reported by Peng et al. [12]. We evaluate with and without the use of an external language model (LM) during inference. In both settings, our model is on par with state-of-the-art architectures achieving comparable or slightly better performance when trained with large amounts of data.\nSLU experiments. To evaluate the effectiveness of our proposed approach on tasks other than ASR, in Table 4 we evaluate our method on SLU with the SLURP [27] dataset. MULTI-CONVFORMER outperforms both Conformer and E-Branchformer achieving the best accuracy and F1-score for both intent classification and entity recognition tasks, while having the least number of parameters.\nSummary of results. In small-scale training data scenarios for ASR (Librispeech-100h, Tedlium2 and AISHELL shown in Table 1) and SLU (SLURP), MULTI-CONVFORMER performs consistently better than Conformer. Moreover, our proposed system exhibits comparable or improved performance when compared to other state-of-the-art Conformer variants such as CgMLP and E-Branchformer. With large scale datasets like Librispeech-960h, we find MULTI-CONVFORMER to be on par with all these architectures."}, {"title": "4.2. Analysis of Convolution Kernels", "content": "To better understand how the kernels are being utilized, in Figure 4, we visualize the weights learned by the gate in MULTICONVweighted (shown in Figure 2(b)) on the test-other split of Librispeech-100h, Tedlium2, and AISHELL datasets. We observe that not every layer benefits from having multiple convolution kernels. While the initial few encoder layers rely mostly on the smaller kernels, the intermediate and final encoder layers reap the most benefits of having multiple convolution kernels.\nIn Table 5, we evaluate the performance of our system by varying the number and size of the convolution kernels. We observe that when convolutions with large kernel sizes (i.e. greater than 32) are used, performance is negatively impacted. Performance also diminishes when there is a large gap between the chosen kernel sizes. Finally, using four kernels instead of two further improves performance. Thus in all our experiments, we use kernel sizes K = {7, 15, 23, 31}."}, {"title": "5. Conclusion", "content": "In this work, we propose MULTI-CONVFORMER that utilizes multiple convolution kernels instead of a single fixed convolution as in Conformers. We demonstrate the effectiveness of MULTI-CONVFORMER by comparing with Conformer and its variants on multiple datasets (Librispeech-960, Tedlium2, AISHELL and Librispeech-100), diverse modelling paradigms (AED, CTC, RNN-T) and different speech tasks (ASR, SLU). We also conduct ablations and analysis for a more comprehensive understanding of our architecture."}]}