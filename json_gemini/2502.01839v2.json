{"title": "Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification", "authors": ["Eric Zhao", "Pranjal Awasthi", "Sreenivas Gollapudi"], "abstract": "Sampling-based search, a simple paradigm for utilizing test-time compute, involves generating multiple candidate responses and selecting the best one-typically by having models self-verify each response for correctness. In this paper, we study the scaling trends governing sampling-based search. Among our findings is that simply scaling up a minimalist implementation of sampling-based search, using only random sampling and direct self-verification, provides a practical inference method that, for example, elevates the reasoning capabilities of Gemini v1.5 Pro above that of ol-Preview on popular benchmarks. We partially attribute the scalability of sampling-based search to a phenomenon of implicit scaling, where sampling a larger pool of responses in turn improves self-verification accuracy. We further identify two useful principles for improving self-verification capabilities with test-time compute: (1) comparing across responses provides helpful signals about the locations of errors and hallucinations, and (2) different model output styles are useful for different contexts\u2014chains of thought are useful for reasoning but harder to verify. We also find that, though accurate verification can be elicited, frontier models demonstrate remarkably weak out-of-box verification capabilities and introduce a benchmark to measure progress on these deficiencies.", "sections": [{"title": "1 Introduction", "content": "Recent advances in language models highlight the importance of test-time compute scaling wherein one uses more compute during inference to enhance reasoning capabilities [OpenAI, 2024, Team, 2025, Agarwal et al., 2024, Wei et al., 2022, Yao et al., 2023, Aky\u00fcrek et al., 2024]. There are many methods for increasing test-time compute usage, including implicitly encouraging longer responses via reinforcement learning [OpenAI, 2024, Team, 2025] or explicitly via prompting [Wei et al., 2022, Yao et al., 2023]. However, sampling-based search\u2014an instance of the generate-and-test approach where a model generates many responses in parallel, e.g. via random sampling or delegation, and selects what the model guesses to be the best one\u2014remains one of the most natural and fundamental paradigms. In addition to being complementary with other test-time compute scaling strategies, it also has the unique advantage of being embarrassingly parallel and allowing for arbitrarily scaling: simply sample more responses [Cobbe et al., 2021, Wang et al., 2023]. As a result, sampling-based search plays an increasingly crucial role as language"}, {"title": "2 Scaling Trends of Sampling-Based Search", "content": "This section examines how reasoning capability scales with two fundamental test-time compute axes:"}, {"title": "2.1 Scaling Trends", "content": "Figure 2.1 provides a heatmap of Verification@k on each benchmark in Table 1 as we scale search and verification. In addition to clear burn-in costs along both axes of scale, we can observe that the largest performance gains are realized when search and verification are both scaled. These trends also indicate"}, {"title": "2.2 Implicit Scaling", "content": "Scaling sampling-based search along the search axis by sampling more solutions, i.e. increasing k, should have two effects on performance that partially cancel out: (1) the verifier must discriminate between more solutions, increasing the likelihood of error and (2) the generator is more likely to produce at least one solution that reaches a correct final answer, i.e. Pass@k increases.\nTo isolate the first effect, we study the model's Verification@k accuracy on \u201cambiguous\u201d questions: questions where at least one of the model's k candidate solutions reaches the correct final answer (note that Pass@k equals the number of ambiguous questions). Figure 2.2 and Figure 2.3 do exactly this, plotting Verification@k accuracy measured only on ambiguous questions from each benchmark. To reduce noise in these figures, we deterministically omit benchmark questions that Consistency@200 answers correctly or where, with high probability, 50 random responses result in either all correct or all incorrect final answers.\nAfter controlling for the growth of Pass@k, we should expect a trend of decreasing accuracy if we increase k but keep the number of verification attempts constant. However, Figure 2.2 shows the reverse trend: accuracy increases with k. This demonstrates an implicit scaling of verification accuracy, where increasing the number of generated responses increases not only the chance that at least one response is correct (Pass@k) but also the chance that at least one of the correct responses is of higher quality. Here, quality can be understood as the rigour or flawlessness of a response; a lower quality solution may be generally correct but fail to justify a non-trivial step or err in a non-critical step of its reasoning.\nImplicit scaling suggests that verification should become more accurate, and sampling-based search should become more effective, with the use of more capable base models that produce more sound reasoning and compelling proofs of correctness. Because the number of ambiguous questions strictly increases with more candidate solutions, the implicit scaling effect also explains the overall accuracy scaling gains in Figure 2.1: larger k increases both the number of ambiguous questions (Pass@k) and accuracy on the set of ambiguous questions."}, {"title": "2.3 The Long Tail of Response Distributions", "content": "We can directly observe Verification@k scaling beyond the saturation point of Consistency@k in Figure 2.4, where we plot their performance after fixing the number of verification attempts at 50. On AIME, the most technically challenging benchmark, Verification@k demonstrates power law scaling even as Consistency@k begins to plateau. The rapid saturation of Consistency@k can be attributed to the fact that, while it is effective at small scales in averaging out noisy mistakes, it necessarily plateaus as it converges on the most probable response; for example, Consistency@50 has the same accuracy as Consistency@10,000 on AIME. Consider cheaply sampling a vast set of solutions from a weak but ergodic model: Consistency@k is unlikely to return a correct solution, but an effective verifier should still be expected to detect rare but correct solutions in the long-tail of the response distribution. We find an example of this on the AIME 2024 exam, where the Gemini v1.5 model struggles to identify the correct answer to Problem 11 on Exam II. Table 2 shows the final answers from 200 randomly sampled Gemini v1.5 solutions, of which only one is correct (\"601,\" in green). Consistency returns the incorrect answer of \"1\" (in red), which appears in over half the responses. In contrast, Verification successfully identifies the solution reaching the correct answer from the response distribution's long-tail, assigning a <36% score to each solution reaching a final answer of \"1\" but a 98% score to the single solution reaching \"601\". Scaling verification capability is key to driving improved search, allowing for discerning between answers that appear correct with 98% vs. 76% confidence. The fact that verification can be used to so effectively leverage the long-tail of model response distributions also suggests that Pass@k, not Pass@1, should be the key performance metric for search applications. Existing post-training techniques (e.g., reinforcement learning from human feedback (RLHF) [Ouyang et al., 2022]) which explicitly optimize for Pass@1 may potentially be doing so at the expense of Pass@k and inhibiting search capability."}, {"title": "3 Effective Self-Verification in Natural Language", "content": "In the process of scaling sampling-based search, we identified two general principles for eliciting more accurate language model self-verification, that may be of independent interest.\n1. Compare responses to localize errors. Disagreements between candidate solutions strongly signal the potential locations of their errors. This can be leveraged to combat the fact that language models"}, {"title": "3.1 Sampling-Based Search Implementation", "content": "We now detail our minimalist implementation of sampling-based search (summarized in Algorithm 1) that uses only parallelizable blackbox queries to a language model. It generates candidate responses by randomly sampling from models and select responses by asking models to self-verify; prompts are identical across all benchmarks and provided in the source code.\nStep 1: Generate Candidate Responses. A language model generates $k_{inf}$ candidate responses (candidate solutions) in parallel to each question, using temperature $\\tau_{inf}$."}, {"title": "Step 2: Verify Candidate Responses.", "content": "A language model generates $k_{verif}$ binary \u201cverification scores\" for each candidate in parallel, indicating whether its final answer is correct. Each scoring attempt is a single conversation thread that rewrites the response as a theorem, supporting lemmas, and proofs (examples in Appendix E) and systematically scans for errors. The highest scoring response is selected.\nTie-Break: Compare Candidate Responses. When the three highest scoring candidates score within 5% of one another and disagree on the final answer, a language model directly compares the responses in pairwise matchups. Each matchup is a single conversation thread that identifies where responses diverge and, at each such point, determines which side is correct. Each matchup is repeated $k_{tie} = 100$ times. The response with the most wins in the round-robin tournament is selected."}, {"title": "3.2 Ablation Studies", "content": "We can individually ablate the practices of comparing and rewriting candidate responses to confirm their role in eliciting greater verification capability.\nAblating comparisons. The step of asking models to directly compare candidate solutions with similar verification scores significantly increases sampling-based search performance. This is demonstrated in Table 3, where we depict the accuracy rates from Table 1 alongside the accuracy rates after ablating the tie-breaking step. These comparisons have the greatest impact when models struggle from low recall and excessively assign high verification scores. On the MATH benchmark, which sees the greatest lift from comparisons, the average verification score of the top 3 candidate responses is nearly 90%. Recall that, as a result, the figures reported in Section 2 that omit tiebreaking significantly underestimate sampling-based search performances (Verification@k).\nAblating rewritings. We explored a limited number of prompts for self-verification, including prompts which omit instructing the model to rewrite responses. We did not perform further prompt optimization and expect refinements would boost accuracy. Table 4 shows each prompt's probability of mislabeling correct solutions (false positive) and incorrect solutions (false negative), with the former generally having a more severe impact on downstream performance. We evaluated these prompts on 1,080 candidate responses to 54 level-5 questions from the MATH training split, and 120 candidate responses to 6 questions from AIME 2023. A response is marked as incorrect if, of 20 verification attempts, the number finding an error in the solution exceeds the equal error rate threshold."}, {"title": "4 Additional Experiments", "content": null}, {"title": "4.1 Smaller Models", "content": "We also observe sampling-based search to be a powerful tool for enhancing smaller, lower-cost models. Here, we apply sampling-based search to Gemini v1.5 Flash model, which has a nearly 20x lower inference cost than Gemini v1.5 Pro. Table 5 lists the performance of using the Flash model to evaluate candidate responses generated by the Pro model (Pro+Flash), and the performance of using the Flash model end-to-end for sampling-based search (Flash). Sampling-based search still provides a significant improvement in performance for both Flash and Pro+Flash. Moreover, Verification@200 still provides significant improvements over Consistency@200, albeit lesser in magnitude than for end-to-end use of Gemini Pro. In addition, Flash Verification@200 using Gemini Flash is competitive with Pro Consistency@200, while Pro+Flash Verification@200 exceeds Pro Consistency@200. We highlight that Pro+Flash Verification@200 has roughly the compute cost of Consistency@500-as our sampling-based search implementation is minimally optimized for efficiency, we expect costs to further decrease."}, {"title": "4.2 Performance by Subtask", "content": "The LiveBench benchmarks each consist of multiple subtasks. In Table 6, we break down the numbers reported in Table 1 for each of these subtasks. We also provide in Table 6 the Pass@200 scores of the Gemini Pro model, which measure the probability that of 200 attempted responses to a question at least one is correct. Pass@200 upper bounds what one can hope to achieve through Verification or Consistency. Verification provides the greatest gains on AIME 2024, Web-of-Lies, Competition, and Zebra Puzzle. In contrast, Verification does not improve on Consistency on the Olympiad task of the LiveBench Math"}, {"title": "5 Technical Details", "content": "All experiments are run on Google Cloud with Gemini v1.5-Pro-002 and Gemini v1.5-Flash-002 models dated to September 2024. Unless otherwise specified, the default parameters for our implementation of sampling-based search (Section 3) are $k_{inf} = 200$, $\\tau_{inf} = 1.5$, $k_{verif} = 50$, $\\tau_{verif} = 1$, and a maximum of 8,192 output tokens per query. For all benchmarks, the scoring of candidate responses is performed using a language model rather than literal string comparison; details are in Appendix A.2.\nPreliminary scoring. When generating $k_{verif} = 50$ verification scores per candidate solution is too expensive, we first generate $k_{verif} = 10$ preliminary verification scores and discard candidate solutions with an average score below 0.2. If a final answer is represented by more than 15 candidate responses, only the top 15-as measured by average preliminary score, tie-breaking randomly-are kept. This results in a smaller pool of candidate solutions for which we compute all $k_{verif} = 50$ verification scores. Preliminary scoring is used on all datasets except AIME, which consists of 15 questions.\nCompute. On AIME, the verification process involves 32,000 characters (roughly 13,000 tokens) of model output. Extrapolating from these figures, running the full sampling-based search pipeline on a question for $k_{inf} = 200$ and $k_{verif} = 50$ requires $200 \\cdot 50 \\cdot 13,000 \\approx 130M$ output tokens. At around $5/1M output tokens (public pricing of Gemini v1.5 Pro), this evaluates to approximately $650 in cost. Preliminary scoring reduces usage of output tokens by roughly 70%, resulting in a per-question cost of $200. The use of Gemini Flash for verification further decreases cost to $12 per question.\nDatasets. Our MATH benchmark consists of 500 questions from the PRM800K [Lightman et al., 2024] test split of Berkeley MATH [Hendrycks et al., 2021]. Our LiveBench Math benchmark consists of 200 randomly subsampled questions from the 368 available as of October 21st 2024, including AMC12 2023, AIME 2024, SMC 2023, USAMO 2023, IMO 2023, and synthetic math questions [White et al., 2024]. Our LiveBench Reasoning benchmark consists of 140 questions from the 150 available as of October 21st 2024, including Zebra puzzles, Web-Of-Lies, and Spatial reasoning questions [White et al., 2024]. Our AIME benchmark consists of the 15 questions in Exam II of AIME 2024 [MAA, 2024]."}, {"title": "6 A Verification Benchmark", "content": "Frontier language models demonstrate a remarkable mismatch between their problem-solving capabilities and poor out-of-box verification capabilities. These limitations have largely been attributed to the inability of current language models to self-diagnose hallucinations or enforce rigour [Zhang et al., 2023, Orgad et al., 2024, Snyder et al., 2024, Kamoi et al., 2024a, Tyen et al., 2024, Huang et al., 2024]. However, our findings that models can be directed to accurately perform verifications at scale suggest that these out-of-box limitations can be addressed with standard methods like instruction tuning. We compiled a set of challenging reasoning problems and candidate solutions to provide a benchmark for these deficits.\nEach entry in this benchmark consists of a question, a correct candidate response, and an incorrect candidate response, and is manually curated from the residuals of our sampling-based search experiments (Section 3). An example entry from this benchmark can be found below (see Appendix D for more)."}, {"title": "7 Related Work", "content": "Test-time compute. Many of the recent advances in language model reasoning capabilities can be traced to increasing use of test-time compute. Inference strategies like chain-of-thought reasoning [Wei et al., 2022], tree-of-thoughts [Yao et al., 2023] and self-critique [Valmeekam et al., 2023] result in improved reasoning performance at the cost of forming longer responses. Reinforcement learning has emerged as a particularly successful strategy for effectively leveraging more test-time compute, wherein models learn from exploration to form lengthy chain-of-thought outputs that incorporate backtracking and search, despite not being explicitly taught to do so [OpenAI, 2024, Team, 2025]. Inference-time model adaptation, whether through many-shot learning [Agarwal et al., 2024, Anil et al., 2024] or finetuning [Aky\u00fcrek et al., 2024], provides another avenue when training data is available. We study sampling-based search: obtain a set of candidate responses from a model and apply an aggregation method to select a response, such as self-consistency/plurality voting [Wang et al., 2023] or selecting a response with a reward/verifier model [Cobbe et al., 2021]. These various methods for scaling test-time compute are complementary; for example, sampling-based search can also be used on models trained to produce longer outputs. We note that it is possible for models trained to produce long chains of thought to perform something resembling sampling-based search internally, in which case we still expect our observed scaling trends to hold. However, we also expect explicit sampling-based search will remain indispensable, due to its greater parallelism and robustness than internally implemented search.\nScaling sampling-based search. The paradigm of sampling-based search provides three main knobs for scaling: generation, sampling, and selection. While the cost of generating each individual response"}, {"title": "8 Conclusion", "content": "This paper studied the scaling trends governing sampling-based search, finding that (1) it scales remarkably well even with simple implementations, (2) implicit scaling plays a big role in this scalability, and (3) self-verification capability can be scaled with test-time compute using two key principles: comparisons localize errors, and responses should be rewritten for output style suitability. To this end, we scaled a minimalist, embarrassingly parallel implementation of sampling-based search that, with sufficient test-time compute, is sufficient to attain state-of-art performance on a range of reasoning benchmarks.\nOur results underscore the importance of the sampling-based search paradigm. Given that it complements other test-time compute scaling strategies, is parallelizable and allows for arbitrarily scaling, and admits simple implementations that are demonstrably effective, we expect sampling-based search to play a crucial role as language models are tasked with solving increasingly complex problems with increasingly large compute budgets. We also see the performance of sampling-based search as providing both a strong baseline scaling trend that any non-trivial inference strategy should exceed, and a meaningful measure of a model's search capability when Pass@k is uninformative (e.g. on multiple choice exams). We anticipate model self-verification capabilities to rapidly improve in the short term, as models learn to leverage the principles of implicit scaling and output style suitability, and drive improved scaling rates for sampling-based search. Finally, our results also highlight the importance of being able to effectively sample massive and diverse sets of solutions for search. This calls for more systematic inference alternatives to random sampling, such as agentic approaches that delegate search, and inference-aware optimization methods that maximize, e.g., Pass@k performance rather than Pass@1."}, {"title": "A Additional Technical Details", "content": null}, {"title": "A.1 Inference Prompts", "content": "For questions from the MATH and AIME benchmarks, we use the following prompt."}, {"title": "A.2 LM-Based Scoring", "content": "Given a tuple consisting of a question, ground-truth solution, and candidate response, we grade the correctness of the candidate response by querying a Gemini-v1.5-Pro-002 model to compare the candidate and ground-truth solutions. This involves repeating the following process five times: (1) send a prompt to the model that provides the question, the correct ground-truth solution, and the candidate response, and asks the model to deliberate on the correctness of the candidate response; and (2) send a followup prompt to the model to obtain a correctness ruling in a structured format. If a strict majority of (valid) responses to the second prompt evaluate to a JSON object with the key-value pair \"student final answer is correct\" = True rather than \"student final answer is correct\" = False the candidate response is labeled correct. Otherwise, the candidate response is labeled incorrect. These queries are all processed with temperature zero. The prompts, which can be found at the end of this subsection, ask the language model to (1) identify the final answer of the given response, (2) identify the final answer of the reference (ground truth) response, and (3) determine whether the final answer of the given response satisfactorily matches that of the reference response, ignoring any non-substantive formatting disagreements. In line with convention, we instruct our scoring system to ignore the correctness of the logic used to reach the final answer and rather only judge the correctness of the final answer. The model is asked to label all non-sensical and incomplete responses as being incorrect.\nAs a form of quality assurance, every scoring output for the Consistency@200 and Verification@200 figures depicted in Table 1 was manually compared against human scoring. No discrepancies between automated and human scoring were found on the MATH and AIME datasets for both Consistency@200 and Verification@200. No discrepancies were found on LiveBench Reasoning for Consistency@200. For Verification@200, one false positive (answer labeled by automated system as being incorrect but labeled by human as being correct) and one false negative (answer labeled by automated system as being correct but labeled by human as being incorrect) were identified on LiveBench Reasoning; three false positives and four false negatives were identified on LiveBench Math. For Consistency@200, two false negatives were identified on LiveBench Math. This means that LM scoring matched human scoring 99% of the time, and the choice of human versus automated scoring matters little to our results."}, {"title": "A.3 Implementation of Consistency@k", "content": "Consistency@k measures the performance of a model by evaluating the correctness of the most common answer reached by the model after being run k times. An important consideration with implementing consistency@k is that there are many choices for the equivalence relation one can use to define \"the most common answer\". We define two candidate responses as reaching the same answer if their final answer is the same. We determine a candidate response's final answer by prompting a language model to identify the final answer from the candidate response; we then strip the extracted final answer of leading and trailing whitespace. We determine equivalence with a literal string match. After determining the most common final answer to a question, we use the string \u201cThe final answer is {final answer}\" as the consistency@k response. Note that we could have instead randomly chosen a candidate response corresponding to the most common final answer, and used that selected response as the consistency@k response we have found that, because our LM-based scoring system evaluates correctness using only the final answer, this\""}, {"title": "A.4 Benchmark Evaluation Prompts", "content": "The benchmark performances reported in Table 7 are obtained with the following prompts. The following prompt is used for the comparison task."}, {"title": "B Additional Experiments", "content": null}, {"title": "B.1 Temperature Tuning", "content": "After attempting temperature tuning on the training split of the MATH dataset, we found that the choices of temperatures $\\tau_{inf}$ and $\\tau_{verif}$ did not significantly affect performance. In Table 8 and Table 9, we compare the post-verification accuracy of our pipeline for various temperature choices. The Verification@20 figures are obtained without running tie-breaking and with only 20 verification attempts."}, {"title": "B.2 Olympiad LiveBench Math Subtask", "content": "The one task for which we saw no lift from verification is the Olympiad questions from LiveBench MATH. These questions are not formatted as open-ended problems. Rather, they take a very specific form of asking one to fill in a pre-written proof from a menu of expression options, and to output a specific sequence of indices corresponding to these options. This is incompatible with our verification pipeline, which asks the verification model to rewrite candidate responses in a theorem-lemma format where the theorem states the final answer. For example, the final answer to the Olympiad question at the bottom of this section is the following sequence:\n19,32,20,2,14,1,27,21,31,36,3,30,5,16,29,34,7,4,6,18,15,22,9,25,28,35,26,8,13,24,23,17,33,11,10,12"}, {"title": "C Self-Verification Prompts", "content": "In this section we present the prompts used to query models to self-verify candidate responses. In Table 4, this corresponds to the \u201cMain\" prompt style. We also provide the prompts used for tie-breaking comparisons. These prompts are broken into multiple parts to encourage longer responses."}, {"title": "C.1 Verification Scoring Prompts", "content": null}, {"title": "C.2 Comparison Prompts", "content": null}, {"title": "D Examples of Benchmark Entries", "content": null}, {"title": "D.1 Example Entry from LiveBench Reasoning", "content": null}, {"title": "D.2 Example Entry from LiveBench Math", "content": null}, {"title": "D.3 Example Entry from MATH", "content": null}, {"title": "D.4 Example of Response with Invalid Reasoning but Correct Final Answer", "content": null}, {"title": "E Examples of Rewritten Responses", "content": null}, {"title": "E.1 AIME Example", "content": null}, {"title": "E.2 LiveBench Reasoning Example", "content": null}]}