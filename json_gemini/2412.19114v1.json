{"title": "Discrete vs. Continuous Trade-offs for\nGenerative Models", "authors": ["Jathin Korrapati", "Tanish Baranwal", "Rahul Shah"], "abstract": "This work explores the theoretical and practical foundations of de-\nnoising diffusion probabilistic models (DDPMs) and score-based gen-\nerative models, which leverage stochastic processes and Brownian mo-\ntion to model complex data distributions. These models employ for-\nward and reverse diffusion processes defined through stochastic differ-\nential equations (SDEs) to iteratively add and remove noise, enabling\nhigh-quality data generation. By analyzing the performance bounds of\nthese models, we demonstrate how score estimation errors propagate\nthrough the reverse process and bound the total variation distance\nusing discrete Girsanov transformations, Pinsker's inequality, and the\ndata processing inequality (DPI) for an information theoretic lens.", "sections": [{"title": "Introduction and Background", "content": ""}, {"title": "Background", "content": "Before we start the introduction of our paper, we would like to provide some\nfamiliarity/introduction with core concepts relevant to our work, such as\nBrownian motion and score generative models. Readers familiar with these\nconcepts may proceed directly to Section 1.2."}, {"title": "Background on denoising diffusion probabilistic modeling", "content": "A denoising diffusion probabilistic model (DDPM) is a type of machine learn-\ning model that aims to estimate the underlying distribution of data to gen-"}, {"title": "", "content": "erate new similar data that could be found as generated from the same dis-\ntribution [1]. They are defined by a forward process which iteratively noises\nthe data distribution, and a reverse process which recovers the original data\ndistribution. The forward process transforms samples from some data dis-\ntribution, i.e. q, into pure noise. The forward process is typically defined\nas:\ndX = - \\frac{Xt}{2}dt + \\sqrt{2}dBt, Xo ~ q\n\nThe reverse process involves sampling the outputs of the forward process in\norder to transform the noise samples into the distribution sampled from q.\nThe equation is:\ndX_{T-t} = {X_{T-t} + 2\\nabla lnq_{T-t}(X_{T-t})}dt + \\sqrt{2}dBt\n\nWhere \\nabla ln qt is the score function for qt [2].\nSince q and thus qt are not explicitly known, the score function is typically\nestimated using a neural network based on samples. For this report, we will\nassume that there is a method to estimate the score with an error bound\nE[|| St - \\nabla ln qt ||2] < Escore, where st is the estimated score. Similarly, since\nqr is not explicitly known, we can take advantage of the fact that qT \u2248 \u03b3\u03b1\nand initialize the algorithm at X\u00f3 ~ yd, or from pure noise. The DDPM\nalgorithm and subsequent diffusion models implement a discrete version of\nthis score-matching algorithm. More precisely,\nX_{-(k+1)h} = e^{h} X_{-kh} + 2\\nabla_{T-kh}(X_{-kh})(e^{h} - 1) + g,g ~ N(0, e^{2h} \u2013 1)"}, {"title": "Score Generative Models", "content": "In particular, a score generative model does not try to model the data dis-\ntribution, q, but instead, the score function, which is defined as the gradient\nof the log probability density using SDEs (stochastic differential equations):\n\\nabla log(q(x))\n\nIn its de-noising process, it tries to estimate the score matching function\ninstead of minimizing the KL divergence between the predicted distribution\nand q [3]. Diffusion models, as shown in [4], surpass GANs in generating\nhigh-quality samples, particularly in image synthesis tasks."}, {"title": "TVD and W2 Distances", "content": "When comparing probability distributions in the context of generative mod-\nels, two key metrics are often employed: the Total Variation Distance (TVD)\nand the Wasserstein-2 (W2) distance."}, {"title": "Total Variation Distance (TVD)", "content": "For two probability measures P and\nQ, the total variation distance is defined by\nTV(P,Q) = sup | P(A) - Q(A)| = \\frac{1}{2}|| P - Q||1\n\nThis captures the maximum difference in the probabilities assigned by P and\nQ to any measurable event A; equivalently, the factor ensures TV(\u00b7,\u00b7) stays\nin [0, 1]."}, {"title": "Wasserstein-2 Distance (W2)", "content": "Also called the 2nd-order Wasserstein or\noptimal transport distance, W2 is defined for distributions P and Q over Rd\nas\nW\u2082(P,Q) = ( inf E_{(x,y)~\u03c0} [|| x - y ||^{2}] )^{{\\frac{1}{2}}}\n \u03c0\u0395\u0393(P,Q)\n\nwhere \u0393(P,Q) is the set of joint measures on Rd \u00d7 Rd whose marginals are\nP and Q, respectively. Unlike TVD, W2 incorporates the geometry of the\nunderlying space and quantifies how much\u201cwork\u201d or \u201ceffort\u201d is required to\ntransport one distribution into the other."}, {"title": "Brownian Motion", "content": "From the equations above, (Bt)t>o is the standard motion for Brownian mo-\ntion in Rd. Brownian motion is defined as a stochastic process that models\nthe random movement of particles in a fluid. It is often characterized by\ncontinuous random trajectories with independent and normally distributed\nincrements over time. Score-generative models generate data by modeling\na noising process and then learning to de-noise it (as described above) with\nthe forward and reverse processes. These equations are often formalized as\nstochastic differential equations (SDEs) that are inspired by Brownian motion\n[5]. Brownian motion describes a natural framework for modeling the contin-\nuous addition of noise to data, which, in turn, helps to generate and model"}, {"title": "Introduction", "content": "Practically, score generative models have had considerable success, starting\nwith the DDPM [1] paper in 2020. However, until [2], there wasn't a definitive\nbound on how well score generative models capture the true score matching\nprocess described earlier.\nFor Pr being the law of the SGM reverse process initialized at yd, the\nstandard isotropic gaussian and using the estimated score, and Qr being the\nlaw of the reverse OU process initialized at the fully noised point qr and with\nscore \u2207 log qt, we want to bound the difference between these two probability\ndistributions. This can be done using total variation distance.\nTo bound Total Variation Distance, we start with the data processing\ninequality (DPI)[6]:\nTV(Pr, Q\u0442) \u2264 TV (PT, PIT) + TV (PT, QT)\n< TV (qr, yd) + TV(PT, QT)\n\nPinsker's inequality[7] says:\nTV(PT, Q+)\u00b2 \u2264 KC(PT||QF).\n\nWe have that PORT = P(X) and Q = Q(X), where:\nP := law of SGM algo initialized at fully noised point qr, law of Pet Pot, with Skho.\n:= law of the reverse process with \u2207 log qkh (true score)\nBy Discrete Girsanov and the KL formula proved in 1.3:\nKL(P(X)||Q(X)) = EP(x) [\\frac{1}{N-1} \\sum_{k=0} h||Skh (0) - \u2207 log qkh ||2]\n\n= 0 (Te score)\n\nas || Skh (0) - \u2207 log qkh ||2 \u2264 Escore  || Skh (0) - \u2207 log qkh ||2 \u2264 Ecore"}, {"title": "Analysis Approach", "content": "We directly analyze the discrete definition of score matching models by first\nderiving a discrete analog to Girsanov's theorem [10], and then bounding the\nerror in the Denoising Diffusion Probabilistic Models framework (DDPM)\n[3] using discrete Girsanov to bound the error due to the score estimation\nerror, and bounding the initialization error due to initializing at an isotropic\ngaussian."}, {"title": "Statement of Girsanov's Theorem", "content": "Here we introduce the concept of Girsanov's Theorem, which is core to our\npaper. Girsanov's theorem is a result in stochastic calculus that describes\nhow the measure associated with a stochastic process changes when the drift\nterm of a Brownian motion is altered. At its core, Girsanov's Theorem\nprovides a framework for relating two probability measures under a change\nof drift, which is particularly useful in the context of stochastic differential\nequations (SDEs). The relationship between stochastic interpolants and Gir-\nsanov's theorem is particularly useful for understanding changes in drift [11].\nGirsanov's Theorem states that there exists a new measure Q, absolutely\ncontinuous with respect to P, under which the process Xt becomes a stan-\ndard Brownian motion. The relationship between the two measures P and Q\nis given by the Radon-Nikodym derivative:\nd\\frac{Q}{P}_{F_t} = exp(-\\int_{0}^{t} \u03bc_s dW_s - \\frac{1}{2} \\int_{0}^{t} ||\u03bc_s||^{2} ds)\n\nwhere Ft is the natural filtration up to time t, \u00b5s is the drift term, and Wt\nis the original brownian motion under measure P."}, {"title": "Proof of Girsanov's Discrete theorem", "content": "(1) X_{(k+1)h} = X_{kh} + hb_{kh} (X_{kh}) + \\sqrt{2h}g_{kh}\n(2) X_{(k+1)h} = X_{kh} + hb'_{kh} (x_{kh}) + \\sqrt{2h}g_{kh},\nb_{kh} (X_{kh}), b'_{kh} (X_{kh}) are any two functions \u011dkh, 9kh ~ N (0, Id)\n\nSay we have trajectory X = (x0, Xh, X2h, \u00b7\u00b7\u00b7 XNh)\nUnder process (1), the likelihood of X is:\nP(x) = \\prod_{k=0}^{N-1} exp( -\\frac{||X_{(k+1)h} - (X_{kh} + hb_{kh}(X_{kh}))||^{2}}{4h})\n\nUnder process (2), the likelihood of X is:\nQ(x) = \\prod_{i=1}^{N-1} exp( -\\frac{||X_{(k+1)h} - (x_{kh} + hb'_{kh}(x_{kh}))||^{2}}{4h})"}, {"title": "", "content": "We aim to quantify the difference between the distributions P(X) and\nQ(X). The most natural way for us to do so is via using the KL Divergence:\nKL(P(X)||Q(X)) = E_{P(X)} [log(\\frac{P(x)}{Q(x)})]\n\nDefine bkh = bkh (Xkh) and bkh = bkh (Xkh).\nDefine Ak = X(k+1)h - Xkh\nDefine Ab = bkh - bkh\n\\frac{P(X)}{Q(X)} = \\prod_{k=0}^{N-1} exp( -\\frac{1}{4h}|| Ak - hb'_{kh}||^{2} - || Ak - hb_{kh}||^{2})\n\nd\u2081  ||\u2206k - hbkh||\u00b2 = ||\u2206k||\u00b2 \u2013 2h(\u2206k,bkh) + h\u00b2||bkh||2\nd2  ||\u2206k - hbkh||\u00b2 = ||\u2206k||\u00b2 \u2013 2h\u27e8\u2206k, b'kh) + h\u00b2||bkh ||2\nd\u2081 \u2013 d2 = h\u00b2||bkh||2 \u2013 h2||b|kh||2 \u2013 2h\u3008\u2206k, bkh) \u2013 (\u0394k, b\u0384\u03ba\u03b7)\n\nThus:\n\\frac{P(X)}{Q(X)} = \\prod_{k=0}^{N-1} exp(\\frac{-1}{4h} [h^{2}||b_{kh}||^{2} \u2013 h^{2}||b'_{kh}||^{2} \u2013 2h\u27e8\u2206k, \u2206b\u27e9])\n\nLet Ak = hbkh + \\sqrt{2h}gkh = X_{(k+1)h} - X_{kh} under process (2):\nh2||bkh||2 - h2||bkh ||2 \u2013 2hhbkh, bkh - b'kh)\nh\u00b2 (bkh + bkh - 26kh, bkh - bkh)\n= h2||bkh - bkh ||\n= h\u00b2 || Ab||\n\n\\frac{P(X)}{Q(X)} = \\prod_{k=0}^{N-1} exp(-\\frac{1}{4h}[h^{2}||\u2206b||^{2} - 2h\\sqrt{2}(\u221ahghh, \u2206b)])\n\n\n= exp(-\\frac{1}{4} [\\sum_{k=0}^{N-1}h||Ab||^{2} + \\frac{1}{\u221a2} \\sum_{k=0}^{N-1} (\u221ah\\tilde{g}_{kh}, \u2206b)])"}, {"title": "", "content": "setting processes (1), (2) equal, Xkh+hbkh+\\sqrt{2h}gkh = Xkh+hbkh+\\sqrt{2h}gkh\n\u221ahghh = \\frac{h}{\u221a2}(Ab) + \u221ahgkh\n\\frac{P(X)}{Q(X)} = exp(\\sum_{k=0}^{N-1} [\\frac{1}{4}h||Ab||^{2} + \\frac{1}{\u221a2}(\u221ahgen: Ab) + \\frac{1}{\u221a2}\u27e9(\u221ahghh, Ab)])\n\n\n= exp(\\sum_{k=0}^{N-1} [\\frac{1}{4}h||Ab||^{2} + \\frac{1}{\u221a2} \\sum_{k=0}^{N-1} \u221a(\u221ahghh, Ab)])\n\nNote that Discrete Girsanov is usually written as the reciprocal (X).\nWe reciprocate later between (39) and (38).\nKL(P(X)||Q(X)) = E log(\\frac{P(X)}{Q(X)}) = E [-\\frac{P(X)}{Q(X)}]\n\n= E [\\frac{1}{4} \\sum_{k=0}^{N-1}h||Ab||^{2} - \\frac{1}{\u221a2} \\sum_{k=0}^{N-1} \u03a3(\u221ahghh, b_{kh} \u2013 b'_{kh})]\n\nKL(P(X)||Q(X)) = EP(X) [\\sum_{k=0}^{N-1} \\frac{1}{4}h||b_{kh} \u2013 b'_{kh} || 2 ]"}, {"title": "", "content": "This discrete version of Girsanov's theorem offers a pivotal link between\ncontinuous stochastic calculus and discrete-time simulations - it allows us\nto rigorously bound how deviations in the drift at each discrete timestep\naccumulate into global errors, ensuring stability and theoretical guarantees\nwhich is essential for algorithms like DDPMs.\nFrom an information-theoretic viewpoint, the discrete Girsanov bound\nindicates how many \u201cbits\u201d are required to distinguish or measure a drift\nmismatch at each timestep, thereby ensuring that the global KL divergence\nacross the entire process remains bounded. By linking drift differences to\nthis bit cost, we gain a clearer picture of how small local changes \u2013 in drift\nestimates can accumulate into large divergences if not carefully controlled."}, {"title": "", "content": "Typically, Girsanov's says that P-Brownian motion B\u2081 can be modified\nto Q-Brownian motion by suitably changing the drift. Specifically:\nBt = Bt+ \\int_{0}^{t} \u03b3_s ds\n\nfor drift Ys.\nConsider a process xt evolving under two measures P and Q. Under P,\nwe have the equivalent SDE:\ndxt = b+ dt + \\sqrt{2}dBt,\n\nwhere Bt is a Brownian motion under P and be is the drift under P.\nUnder Q, the same process evolves as:\ndxt = bdt + \\sqrt{2}dBt,\n\nwhere Bt is a Brownian motion under Q and by is the drift under Q.\nDefine\n\u03b3t = \\frac{b_t-b'_t}{\u221a2}"}, {"title": "", "content": "Then we can write\ndBt = dBt + Yt dt.\n\nGirsanov's theorem says that the Radon-Nikodym derivative[12] gives the\nchange of measure from P to Q:\nd\\frac{Q}{P} = exp(-\\int_{0}^{T} Ys dB_s - \\frac{1}{2} \\int_{0}^{T} Y_s^{2} ds)\n\nInverting this relationship and sub'ing in eq. (36) for ys, we get:\nd\\frac{P}{Q} = exp(\\int_{0}^{T} \\frac{(b_s - b'_s)}{\u221a2} dB_s + \\frac{1}{4} \\int_{0}^{T} (b_s - b'_s)^{2} ds)\n\n\n= exp(\\sum_{k=0}^{N-1} \\frac{1}{\u221a2}(\u221angth: b_s - b'_s) + \\frac{1}{4} \\sum_{k=0}^{N-1} ||b_s - b'_s||^{2} \u00b7 h).\n\nThis is precisely the relationship we aimed to prove."}, {"title": "Implementation & Results", "content": "For our experiment, we simulate an OU process with the forward and reverse\nprocesses with what we expect and what we simulate, which we use to show\nthe significance of the discrete Girsanov Theorem here.\n1. Define a true forward OU process that evolves from some initial data.\nWe generate data from the distribution using the function y = x.\n2. From the noising equations from above, we create a noised version of\nthe data by simulating the forward process to time T.\n3. Define two reverse processes:\n\u2022 The correct reverse process (what we expect from ground truth).\n\u2022 The estimated reverse process (with an estimated score that has\nsome error).\nWe will then:\n\u2022 Simulate sample paths from both the correct and approximate reverse\nprocesses starting from the final noised distribution.\n\u2022 Compare the denoised outputs y to the original data y.\n\u2022 Plot the original data, the noised version, and the denoised versions.\n\u2022 Plot the errors and estimate the KL divergence between the two dis-\ntributions using the discrete Girsanov formula derived in the paper."}, {"title": "", "content": "These results show the significance of the data distribution we generated\nfrom the process above \u2013 specifically they highlight the accuracy of the dis-\ncrete Girsanov estimates in recovering the original data distribution - where\nwe first plot the original and reverse trajectories (as seen in Figure 3). We"}, {"title": "", "content": "see using the discrete Girsanov estimates that we are able to estimate the\noriginal q distribution well, as seen in Figure 4. Figures 5 and 6 display the\nerrors for the KL divergence and Brownian motion, which both represent\nlow errors/KL, meaning our discrete Girsanov representation estimates the\ndistribution well."}, {"title": "Benefits & Drawbacks between Discrete and Continuous Methods", "content": ""}, {"title": "", "content": "In many cases, discrete approximations can serve as a bridge to their con-\ntinuous counterparts, allowing insights and results from discrete-time settings\nto be refined and extended into continuous-time frameworks. Discrete ver-\nsions are often easier to implement computationally, providing practitioners\nwith finer control over numerical stability, step-size selection, and algorithmic\ndesign. This level of control can facilitate rapid experimentation, debugging,\nand the integration of techniques like adaptive step sizing or variance reduc-\ntion strategies that might be less straightforward in purely continuous for-\nmulations. Conversely, continuous-time models, while theoretically elegant\nand rich in analytical tools, may be more challenging to simulate directly,\noften requiring specialized solvers or sophisticated approximations. Thus,\nthe discrete analogs not only approximate their continuous counterparts but\ncan also offer practical advantages and a more flexible platform for testing,\nrefinement, and exploration before passing to the continuous case in the\nlimit.\nWhile Brownian motion is inherently continuous, score matching is a\ndiscrete process (due to DDPM being a discrete algorithm) and as such,\ndiscretization works very well giving not only a more tractable solution but\nalso a more efficient one. Thus we are able to tackle a series of problems that\nwe otherwise could not."}, {"title": "Conclusion", "content": "In this paper, we explored several of the computational trade-offs between\ndiscrete and continuous methods, such as Denoising Diffusion Probabilis-\ntic Models, highlighting that while continuous methods are theoretically\ngrounded, their computational complexity and challenges in modeling of-\nten limit their practical application. By leveraging Girsanov's Theorem to\nbound our results, we were able to establish rigorous mathematical founda-\ntions that enable more efficient simulation generation. This approach not\nonly provides valuable insights into the relationship between discrete and\ncontinuous methods but also lays the groundwork for future research by of-\nfering a robust framework for bounding and refining model predictions in\ncomplex systems."}]}