{"title": "Enhancing LLM Agents for Code Generation with Possibility and Pass-rate Prioritized Experience Replay", "authors": ["Yuyang Chen", "Kaiyan Zhao", "Yiming Wang", "Ming Yang", "Jian Zhang", "Xiaoguang Niu"], "abstract": "Nowadays transformer-based Large Language Models (LLM) for code generation tasks usually apply sampling and filtering pipelines. Due to the sparse reward problem in code generation tasks caused by one-token incorrectness, transformer-based models will sample redundant programs till they find a correct one, leading to low efficiency. To overcome the challenge, we incorporate Experience Replay (ER) in the fine-tuning phase, where codes and programs produced are stored and will be replayed to give the LLM agent a chance to learn from past experiences. Based on the spirit of ER, we introduce a novel approach called BTP pipeline which consists of three phases: beam search sampling, testing phase, and prioritized experience replay phase. The approach makes use of failed programs collected by code models and replays programs with high Possibility and Pass-rate Prioritized value (P2Value) from the replay buffer to improve efficiency. P2Value comprehensively considers the possibility of transformers' output and pass rate and can make use of the redundant resources caused by the problem that most programs collected by LLMs fail to pass any tests. We empirically apply our approach in several LLMs, demonstrating that it enhances their performance in code generation tasks and surpasses existing baselines.", "sections": [{"title": "Introduction", "content": "In recent years, there has been significant progress in the development of Large Language Models (LLMs) like Transformer (Vaswani et al. 2017) and Llama (Touvron et al. 2023) across various domains. A particular trend has emerged in leveraging LLMs for automatic code generation tasks. Models such as WizardCode (Luo et al. 2023) and StarCode (Li et al. 2023) have been developed to address these tasks. To evaluate the effectiveness and performance of LLMs in code generation, various benchmarks have been established. For instance, APPS (Hendrycks et al. 2021) is widely used in evaluations of code models, and Code-Contests (Li et al. 2022) has been established as a standard for competition-level coding tasks. Among all models, transformers have demonstrated significant success in benchmarking tasks such as code translation, code completion, and challenging problem-solving (Svyatkovskiy et al. 2020). Some transformer-based pipelines have even achieved remarkable results on difficult tasks (Zhang et al. 2023).\nHowever, traditional transformer-based pipelines, which consist of sampling and filtering phases, have obvious shortcomings due to their structure. A salient problem in code generation tasks is the significant waste of redundant resources caused by low efficiency. Specifically, when tasks are provided as input, the code agent samples a large number of programs from pre-trained transformer-based LLMs and passes them through public test sets, where they are tested and filtered based on their pass rates. Low efficiency arises in cases where most programs fail to pass the tests due to even a single incorrect token (Zhang et al. 2023). As a result, models must sample many incorrect programs to find a precisely accurate one, leading to the wastage of redundant resources, including unsuccessful programs that are not reused.\nHowever, programs that fail some tests do not necessarily lack value. On the contrary, most pre-trained LLMs are well-trained on large corpora, which means that the programs they generate are almost accurate but may fail due to minor errors. Thus, it would save time and improve efficiency if we could reduce the waste of these valuable resources.\nTo leverage the value hidden in these redundant resources and increase efficiency, we introduce Experience Replay (ER), a buffer that stores programs sampled by LLMs along with each program's P2Value (possibility and pass rate value), which we consider as its value. P2Value comprehensively considers both the likelihood of a transformer's output and the pass rate. On one hand, a program with a higher pass rate in public test sets demonstrates good performance in a particular task; on the other hand, a program with a higher likelihood of output is considered to have higher value according to the results calculated by pre-trained transformers. Based on ER, we introduce a novel approach called the BTP pipeline, which consists of three phases: beam search sampling, testing, and prioritized experience replay. The core algorithm, PPER (P2Value-Prioritized Experience Replay), utilizes beam search to sample and store programs in ER, and then replays programs in ER based on a probability dependent on their P2Value.\nWe empirically demonstrate that our pipeline improves the performance of LLMs in code generation tasks, outperforming the original models regardless of whether the training data is self-generated or generated by models of higher quality. More specifically, our contributions are as follows:\n\u2022 First, we propose a novel algorithm, BTP pipeline, con-"}, {"title": "Related Work", "content": "Considering the association of ER and LLMs in our pipeline, here we will introduce them respectively.\nLLMs for code generation: Our work is closely related to LLMs for code generation. In recent years, high-performing LLMs such as GPT-4 (OpenAI 2023), Llama (Touvron et al. 2023), PaLM (Chowdhery et al. 2022), and Chinchilla (Hoffmann et al. 2022) have emerged in different areas. Particularly, our work is based on transformers (Vaswani et al. 2017) for code generation tasks (Roziere et al. 2020). Code models such as BERT (Devlin et al. 2019; Feng et al. 2020; Guo et al. 2020), T5 (Raffel et al. 2020), GPT-2 (Radford et al. 2019), Codex (Chen et al. 2021a), CodeT5 (Wang et al. 2021), StarCode (Li et al. 2023), and WizardCoder (Luo et al. 2023) have become backbones for code understanding and generation. Meanwhile, to evaluate the performance of code models, different benchmarks have been created, such as APPS (Hendrycks et al. 2021), CODE-CONTESTS (Li et al. 2022), OpenOrca (Lian et al. 2023), and HumanEval (Chen et al. 2021b). Additionally, (Roziere et al. 2022) constructs training datasets for unsupervised code translation tasks, and (Ellis et al. 2019) constructs test cases from different specific areas to train an RL agent. Moreover, many new methods have been proposed in code generation. For example, (Chen et al. 2021b) fine-tunes powerful pre-trained LLMs to index knowledge and refine performance in code completion. (Austin et al. 2022) summarizes that LLMs can be applied to code generation tasks, and (Wei et al. 2022) introduces Chain-of-Thought (CoT) prompting to encourage LLMs to think step by step and reduce error rates.\nRL for Code Generation: (Bunel et al. 2018) claims that code generation tasks can be broken down into a series of decision-making problems, which are similar to problem definitions in RL. This implies that RL algorithms can be applied to transformers, effectively leveraging their sequential decision-making capabilities. For instance, (Zhang et al. 2023) combine Monte Carlo Tree Search (MCTS) with transformers in code generation tasks. In this approach, MCTS is used to explore potential sequences of code by simulating different code paths, evaluating them based on a reward function that measures code correctness and efficiency. This enables the model to select the most promising code sequences during inference(Yang et al. 2024a). Similarly, (Le et al. 2022) optimize the correctness of generated programs by framing it as a reward maximization problem, a common objective in RL(Yang et al. 2024b). They employ policy gradient methods, where the transformer model's policy is iteratively improved by sampling code sequences, estimating the reward (e.g., program correctness)(Wang et al. 2023), and updating the model to increase the likelihood of generating correct code sequences in future iterations. These approaches demonstrate how RL's exploration-exploitation(Yang et al. 2023) mechanisms can be integrated with transformers to enhance the performance of code generation models by not just predicting the next token but optimizing over entire sequences based on cumulative rewards.\nExperience Replay: (Lin 1992) first introduced the concept of experience replay, suggesting that an agent can store its experiences in a buffer and later sample from this buffer to break the temporal correlation of consecutive observations, thus stabilizing the learning process. By replaying these experiences multiple times, the agent can improve sample efficiency, as it can learn from past experiences that may have been missed during the initial training phase.\nHindsight Experience Replay (HER) (Andrychowicz et al. 2017) expanded on this by addressing the sparse reward problem in goal-conditioned reinforcement learning (RL). In HER, after a failed episode, the transitions leading up to the failure are stored in the replay buffer. During training, these transitions are re-labeled with different goals than originally intended, particularly goals that were actually achieved in the failed episode. By assigning new rewards corresponding to these new goals, HER enables the agent to learn from episodes where the original goal was not achieved, effectively turning failures into learning opportunities.\nMeanwhile, Prioritized Experience Replay (PER) (Schaul et al. 2016) introduced the idea that not all transitions are equally valuable for learning. PER modifies the experience replay mechanism by sampling transitions with a probability proportional to their temporal-difference (TD) error, which indicates how surprising or unexpected the transition is. High TD-error transitions, where the agent's prediction was significantly different from the actual outcome, are more informative and thus are replayed more frequently. This approach ensures that the agent focuses on learning from the most informative experiences, speeding up the convergence of the learning process by reducing the time spent on less useful transitions."}, {"title": "Methodology", "content": "In this section, we first briefly present the framework of the BTP pipeline, which is composed of beam search sampling, testing, and PPER. Then, we illustrate the details of the proposed framework. Figure 1 provides a complete process of the BTP pipeline."}, {"title": "BTP pipeline", "content": "As shown in Figure 2, most previous works adopt a traditional framework where the transformer model utilizes a sampling and filtering pipeline. In the sampling phase, LLM simply finds best code sequences in every time step according to\n$P(Y|X) = P(y_1|X)P(y_2|X, y_1)...P(y_T|X, y_1, y_2,..., y_{T-1})$\n(1)"}, {"title": "Beam search sampling phase", "content": "As shown in Figure 4, at every time step, the code model finds the top-k most probable candidate sequences and keeps them in a container called \"beams.\" At each step i, all candidate sequences explore all possible tokens from the vocabulary, generating different new sequences. Then, the LLM selects the top-k sequences based on the combined probability and adds them to the new beam for the next time step.\n$P(y_1y_2... y_i) = P(y_1y_2... y_{i-1})P(y_i)$ (2)\nRepeat the process till the model finds a complete program $y_1y_2... y_T$ where T is the end time step. Store the top-k programs $t_1t_2... t_k$ in the experience replay buffer along with their possibilities $P(t_1)P(t_2) ... P(t_k)$, the code task X and its corresponding test sets S in the following tuple form:\nT = (X, S, $t_i$, P($t_i$))\n(3)"}, {"title": "Testing phase", "content": "In the testing phase, the model will sequentially take out every tuple from T and test $t_i$ in every test case of test set S, and compute the pass rate $pass\\_rate_i$:\n$pass\\_rate_i = \\sum_{S_k \\in S} 1(if \\ t_i \\ pass \\ S_k)$ (4)"}, {"title": "PPER phase", "content": "In the Possibility and Pass-rate Prioritized Experience Replay (PPER) phase, the code model will be fine-tuned using the method we call PPER. Specifically, the programs stored in the replay buffer will be sampled with probabilities that are associated with their possibility and pass rate. The sampled programs will then be used to construct a minibatch, which will be used to fine-tune the code model.\nP2Value In our PPER method, the most important factor is to establish a standard for defining the priorities of every program in the ER. While it is challenging to determine an accurate measurement standard for priority, a reasonable alternative is to consider the P2Value, which combines the output probability of the transformer and the pass rate on test sets. Particularly for any tuple $ER_i$ = (X, S, $t_i$, P($t_i$), pass_rate_i) sampled from ER, P2value is calculated as followed:\nP2Value $\\triangleq \\alpha \\cdot P(t_i) + (1-\\alpha) \\cdot pass\\_rate_i$ (6)\nWhere $\\alpha$ is a parameter that determines the weights of possibility and pass rate.The closer it gets to 1, the more important the possibility becomes. Correspondingly, sampled programs that the original code model prefers will have more influence in the fine-tuning process. Conversely, programs that pass the most test sets but are not as highly preferred by the original code model will carry more weight in the fine-tuning process.\nThe reason we consider such a formula is that programs with higher pass rates are more suitable and valuable for particular code tasks. However, due to the possibility of low pass rates, which can even approach zero, we consider applying possibility to value a program. It is evident that a program preferred by the pre-trained LLM holds higher value in the LLM's corpus. And how to balance their weights is the reason why we set parameter $\\alpha$\nRandom Proprotization sampling It is straightforward to uniformly sample programs from the ER or to fine-tune the LLM using the entire ER. However, it is more efficient to give programs with higher value a greater chance of being selected. Therefore, we introduce a random sampling method to ensure that every program stored in the ER is sampled in a strictly monotonic manner with respect to its priority. This method increases the likelihood of sampling programs with higher priority, while still maintaining a fixed non-zero probability of sampling the program with the lowest priority, ensuring that every trajectory in the ER is utilized.\nSpecifically, we define the sampling probability of a transition i in Equation 5.\n$P(i) = \\frac{p_i}{\\sum_{k} p_k}$ (7)\nwhere pi is the priority of program $t_i$. The index $\\alpha$ determines the level of prioritization, with $\\alpha$ = 0 We consider two ways to define pi. In the first case, we directly define $p_i$ as P2Value. It intuitively depicts the relationship between sampling possibility and priority.\nHowever, this method is sensitive to points that deviate significantly from the average value. For instance, trajectories with much higher P2Value will be sampled too frequently. To solve this problem, we introduce the second definition\n$p_i = \\frac{1}{rank(i)}$\n(8)\nwhere rank(i) represents the rank of the program's priority among all trajectories. This method has several advantages compared with the previous approach. Firstly, it follows a power law distribution, meaning that most data are concentrated around the centroid, while a small proportion is distributed around the very large and very small values. Moreover, it is more robust and less sensitive to points that deviate significantly from the average value. For instance, P(i) of a trajectory with the lowest rank will not vary significantly even if its P2Value decreases substantially.\nIt is noteworthy that the possibility of the code model's output is non-zero, which means that P2Value is also non-zero, so there is no need for a constant to prevent zero probability."}, {"title": "BTP Pipeline", "content": "Algorithm 1: BTP Pipeline\nRequire: T: Code model; beam: a buffer that stores programs in the beam search sampling phase; k: size of beam; Xset: task sets with test sets; ER: experience replay buffer; batch: a minibatch that stockpiles programs used to fine-tune T; n: size of minibatch"}, {"title": "Experiments", "content": "In this section, we empirically measure the effectiveness of our BTP pipeline. We conduct experiments sequentially to verify the following conjectures.\n1: Our BTP pipeline helps code models generate better programs in the scenario where programs sampled from a better model are used to fine-tune a standard model.\n2: Our BTP pipeline helps code models generate better programs in the scenario where programs sampled from the code model itself are used to fine-tune the model.\n3: The best code model fine-tuned by our BTP pipeline is competitive compared to baseline methods.\n4: Is there a better way to maximize the effectiveness of our BTP pipeline? (e.g., mixing sampled programs in the ER)"}, {"title": "Experiment Settings", "content": "Datasets In recent years, a variety of open-source programming datasets have emerged, providing a robust foundation for evaluating code models. To ensure the robustness and generalizability of our proposed BTP pipeline, we applied it to fine-tune several state-of-the-art code models and evaluated them on a diverse set of popular benchmark datasets, including CodeContests from AlphaCode (Li et al. 2022), APPS (Hendrycks et al. 2021), and HumanEval (Chen et al. 2021b). For HumanEval, which comprises 164 programming problems complete with function signatures, docstrings, bodies, and unit tests, we utilized all unit tests for a given problem as the test set, while the remaining descriptions were used as code generation tasks. For CodeContests, we similarly treated the problem descriptions as code generation tasks and combined all public and private test cases into a unified test\nset. In the case of APPS, where public and private test cases are not differentiated, we aggregated all test cases to form a comprehensive test set for each code generation task.\nModels We categorized the models used in our experiments into two distinct groups. The first group comprises models that undergo fine-tuning, including GPT-2 and GPT-Neo. The second group consists of models employed for generating code samples. Within this latter category, we explored two scenarios: in the first, we utilized advanced code models such as GPT-4-turbo (OpenAI 2023), GPT-3.5-turbo, CodeLlama-34B (Roziere et al. 2022), and WizardCoder-34B (Luo et al. 2023); in the second, we utilized code models that were identical to those used for fine-tuning.\nHyperparameter Optimization We conducted a series of experiments to determine the most effective hyperparameters for our models. Initially, we investigated whether beam search sampling outperforms simple sampling in terms of effectiveness. The results confirmed the superiority of beam search sampling, prompting further experiments to determine the optimal value for the beam search parameter k. Balancing effectiveness and resource consumption, we selected k = 3 for our primary experiments, sampling the top-3 programs based on their probabilities. Detailed results and analysis are provided in Appendix A.\nAdditionally, we explored the impact of the hyperparameter $\\alpha$ during the PPER phase. However, our findings revealed that the optimal $\\alpha$ value varies across different models and datasets. To address this, we conducted targeted experiments across various datasets to identify the best-performing $\\alpha$ for each scenario. The outcomes of these experiments, along with a comprehensive analysis, are presented in Appendix B and Appendix C."}, {"title": "Fine-tuning Code Models with the B\u0422\u0420 Pipeline", "content": "In this section, we systematically address the four key questions posed earlier by dividing our experiments into four distinct parts.\nLeveraging Advanced Models to Enhance Baseline Models To investigate our first hypothesis, denoted as C1, we conducted an experiment to test whether the performance of baseline models can be significantly improved by leveraging advanced models within our proposed BTP (Better Transformer Programming) pipeline. Specifically, we employed the APPS dataset, which is structured into three levels of difficulty: introductory, intermediate, and competition-level tasks. These tasks were designed to assess the models' capabilities across a range of programming challenges.\nAs described in Table 1, we consolidated all three sections of the APPS dataset into a single, comprehensive dataset referred to as APPS mixed. This combined dataset was used to train and evaluate the models, ensuring that they were exposed to a diverse array of task difficulties, thereby providing a robust assessment of their generalization capabilities.\nFor this experiment, we selected four state-of-the-art transformer-based models: GPT-4-turbo, GPT-3.5-turbo, CodeLlama-34B, and WizardCoder-34B. These models were tasked with generating sample programs, which were subsequently used to fine-tune two baseline models: GPT-2 and GPT-Neo. The fine-tuning process involved using the sample programs generated by each advanced model to create eight fine-tuned variants of the baseline models, named as follows:\nWe utilized four advanced transformer models to generate sample programs, which were then used to fine-tune two baseline models. Specifically, the GPT-4-turbo, GPT-3.5-turbo, CodeLlama-34B, and WizardCoder-34B models were employed to generate samples that were subsequently used to fine-tune GPT-2 and GPT-Neo. This process resulted in eight fine-tuned models: GPT-2 fine-tuned with samples from GPT-4-turbo, GPT-3.5-turbo, CodeLlama-34B, and WizardCoder-34B, respectively named GPT-2-GPT4, GPT-2-GPT3.5, GPT-2-Llama, and GPT-2-Wizard; similarly, GPT-Neo was fine-tuned with samples from these four models, resulting in GPT-Neo-GPT4, GPT-Neo-GPT3.5, GPT-Neo-Llama, and GPT-Neo-Wizard.\nAfter the fine-tuning process, we evaluated each of these models on the three distinct sections of the APPS dataset as well as on the combined APPS mixed dataset. The objective was to assess the extent to which the fine-tuned models could improve their performance on code generation tasks of varying complexity.\nThe results, presented in Table 2, reveal a substantial improvement in the performance of the fine-tuned models compared to their original, unmodified versions. This improvement is observed consistently across all sections of the APPS dataset, which underscores the effectiveness of our BTP pipeline. By incorporating advanced models for program sampling, we significantly enhance the capabilities of"}, {"title": "Conclusion", "content": "In code generation tasks, large language models (LLMs) often need to sample a large number of programs to find a completely correct one, as even a single incorrect token can lead to failure in testing. Consequently, many sampled programs are wasted.\nTo utilize these resources and improve efficiency, in this work, we propose a novel algorithm called the BTP pipeline, which combines beam search sampling with prioritized experience replay to fine-tune LLMs. We empirically applied our algorithm to fine-tune several LLMs and found that they showed improvement compared to previous models. We also demonstrate that our algorithm is effective not only in scenarios where programs sampled by a better code model are used to enhance a standard code model, but also in scenarios where a code model enhances itself using programs it has sampled.\nBeyond improving LLM performance in code generation tasks, we believe our BTP pipeline can be beneficial for enhancing general LLMs, particularly in cases where results sampled from LLMs are difficult to pass tests. A key limitation of this work is its reliance on code tasks and corresponding test cases. Tasks with few test cases typically result in pass rates close to zero, which can hinder the effectiveness of our algorithm. In future work, we plan to explore similar test sets and expand the available test sets to address this limitation."}]}