{"title": "Automated Strategy Invention for Confluence of Term Rewrite Systems", "authors": ["Liao Zhang", "Fabian Mitterwallner", "Jan Jakub\u016fv", "Cezary Kaliszyk"], "abstract": "Term rewriting plays a crucial role in software verification and compiler optimization. With dozens of highly parameterizable techniques developed to prove various system properties, automatic term rewriting tools work in an extensive parameter space. This complexity exceeds human capacity for parameter selection, motivating an investigation into automated strategy invention. In this paper, we focus on confluence, an important property of term rewriting, and apply machine learning to develop the first learning-guided automatic confluence prover. Moreover, we randomly generate a large dataset to analyze confluence for term rewrite systems. Our results focus on improving the state-of-the-art automatic confluence prover CSI: When equipped with our invented strategies, it surpasses its human-designed strategies both on the augmented dataset and on the original human-created benchmark dataset Cops, proving/disproving the confluence of several term rewrite systems for which no automated proofs were known before.", "sections": [{"title": "Introduction", "content": "Term rewriting studies substituting subterms of a formula with other terms (Baader and Nipkow 1998), playing an important role in automated reasoning (Bachmair and Ganzinger 1994), software verification (Meseguer 2003), and compiler optimization (Willsey et al. 2021). Mathematicians have developed various techniques to analyze the properties of term rewrite systems (TRSs). However, many properties are undecidable (Baader and Nipkow 1998), implying that no technique can consistently prove a particular property. To navigate this undecidability, modern term rewriting provers typically employ complicated strategies, incorporating wide arrays of rewriting analysis techniques, with the hope that one will be effective. Each technique often accompanies several flags to control its behavior. The diversity of techniques and their controlling flags result in a vast parameter space for modern automation term rewriting provers.\nManually optimizing strategies for undecidable problems is beyond human capacity given the extensive parameter space, inspiring us to apply machine learning to search for appropriate strategies automatically. In this paper, we focus on confluence, an important property of term rewriting, and discuss automated strategy invention for the state-of-the-art confluence prover CSI (Nagele, Felgenhauer, and Middeldorp 2017). We modify Grackle (H\u016fla and Jakub\u016fv 2022), an automatic tool to generate a strategy portfolio for a solver, encoding strategies that require transformations and complex schedules such as parallelism.\nWe also conduct data augmentation on the human-built confluence problems database (Cops)\u00b9, a representative benchmark for the annual confluence competition (CoCo)\u00b2. As Cops has been created manually, it includes only 577 TRSs. They are of high quality, but the relatively small number is still inadequate for data-driven machine learning techniques that require large amounts of training data. To handle this problem, we generate a large number of TRSs randomly, but ensure that they are interesting enough to analyze. For this, we develop a procedure to confirm a relative balance in the number of problems most efficiently solved by different confluent analysis techniques within the dataset.\nWe evaluate our strategy invention approach in Cops and the augmented dataset. On both of the datasets, the invented strategies surpass CSI's default strategy. In particular, we prove (non-)confluence for several TRSs that have not been proved by any automatic confluence provers in the history of the CoCo competition.\nAs an example, our invented strategy is able to prove the non-confluence for the Cops problem 991.trs, never proved by any participant in CoCo. The key to solving the problem is the application of the redundant rule technique (Nagele, Felgenhauer, and Middeldorp 2015) with non-standard arguments. CSI's default strategy performs redundant -narrowfwd -narrowbwd-size 7 prior to performing non-confluence analysis. The flags narrowfwd and narrowbwd determine the categories of redundant rules to generate. Our tool automatically discovered that by changing the original redundant rule transformation to\nredundant -development 3 -size 7, we are able to prove this problem. A larger value for the flag development causes a larger number of development redundant rules to be added. We notice that the value three is crucial as values below three are ineffective for 991.trs. This is only one of the several problems which our new strategies can solve as discussed in the later sections.\nContributions"}, {"title": "Background", "content": "We informally define some theoretical properties of term rewriting in this section, hoping to ease the understanding of the behavior underlining automatic confluence provers. A formal description can be found in the appendix.\nWe assume a disjoint set of variable symbols $V$ and a finite signature $F$. The set of terms $T(F, V)$ is built up from $V$ and $F$. The set of variables occurring in a term $t$ is denoted by $Var(t)$. A term rewrite system (TRS) consists of a set of rewrite rules $l \\rightarrow r$ where $l,r \\in T(F,V)$, $l \\notin V$, and $Var(r) \\subseteq Var(l)$. We write $t_1 \\rightarrow^* t_n$ to denote $t_1 \\rightarrow t_2 \\rightarrow \\dots \\rightarrow t_n$ where n could be one. A TRS is confluent if and only if $\\forall s,t,u \\in T(F,V)(s \\rightarrow^* t \\land s \\rightarrow^* u \\Rightarrow \\exists v \\in T(F,V)(t \\rightarrow^* v \\land u \\rightarrow^* v))$. Consider the TRS of {f(g(x), h(x)) \u2192 a,g(b) \u2192 d,h(c) \u2192 d} (Gramlich 1996). It is not confluent since f(d, h(b)) \u2190 f(g(b), h(b)) \u2192 a, and no rules are applicable to f(d, h(b)) and a. A term is called linear if no variable multiply occurs in it. A rewrite rule $l \\rightarrow r$ is called left-linear if $l$ is linear. A TRS is called left-linear if all its rules are left-linear. Left-linearity is crucial for confluence analysis since most existing confluence techniques only apply to such systems. In this paper, a term is called compositional if it is neither a variable nor a constant."}, {"title": "CSI", "content": "CSI is one of the state-of-the-art automatic confluence provers that participates in the annual confluence competition. It ranks first in five categories of competitions in CoCo 2024. To show (non-)confluence of TRSs, CSI automatically executes a range of techniques, scheduled by a complicated configuration document written by experts in confluence analysis. Subsequently, CSI either outputs YES, NO, or MAYBE indicating confluence, non-confluence, or indetermination, respectively.\nCSI implements 93 term rewriting analysis techniques (many of them parametrized or transforming the system into one that can be analyzed by other techniques) and utilizes a complicated strategy language to control them. In CSI, these techniques are called processors. They are designed to prove the properties of TRSs, perform various transformations, and check the satisfiability of certain conditions."}, {"title": "Grackle", "content": "Grackle (H\u016fla and Jakub\u016fv 2022) is a strategy optimization system designed to automate the generation of various effective strategies for a given solver based on benchmark problems. Such solvers receive a problem and decide the satisfiability of a particular property of the problem. It was originally designed for automated reasoning tools and has been applied to various solvers such as Prover9 (McCune 2005-2010), E (Schulz 2013) and Lash (Brown and Kaliszyk 2022) for proving boolean satisfiability problems (De Moura and Bj\u00f8rner 2011) and Vampire (Kov\u00e1cs and Voronkov 2013) for proving satisfiability modulo theories problems (De Moura and Bj\u00f8rner 2011). We choose Grackle for our research, as it is highly adaptable and we are not aware of any strategy invention program that would allow the kinds of strategies needed for automatic rewriting tools. Additionally, Grackle has achieved good results with the solvers it was previously applied to.\nGiven a set of problems as input, Grackle automatically invents a large number of strategies and selects those most complementary in problem solving. Here, complementation first means that each selected strategy must be most efficient for solving a number of unique problems. Moreover, it indicates that within a limit of the number of selected strategies, the combination of all selected strategies can maximize the number of provable problems.\nGrackle repeatedly executes a dual-phase process to generate complementary strategies. The first phase is strategy evaluation, followed by strategy invention. Grackle users need to provide a number of initial strategies before the execution. In the evaluation phase, Grackle evaluates all strategies in its portfolio on a comprehensive benchmark with a selected time limit. Initially, the portfolio only comprises the initial strategies. New strategies are provided by the invention phase, and the strategies in the portfolio are updated based on the evaluation results. This evaluation clusters the set of benchmark problems into subsets $P_s$ for each strategy $S$, where $P_s$ contains the problems where the strategy S performs best. Only the strategies mastering at least a given number of unique problems remain in the portfolio. In the invention phase, Grackle invents new strategies using the best-performing strategy S and its corresponding benchmark problems $P_s$. New strategies are invented and tested on $P_s$ via external parameter tuning programs"}, {"title": "Strategy Invention and Combination", "content": "To generate a better strategy for CSI, we first invent a large set of complementary strategies, and then appropriately combine a subset of the invented strategies into a single strategy."}, {"title": "Strategy Invention", "content": "In order to find new strategies for CSI, we need first to represent the parameter space in a meaningful way. Directly using a tool like Grackle to randomly combine parameters may produce unsound results. This is a unique challenge compared to previous applications of strategy invention. The solvers to which Grackle was previously applied cannot produce unsound results, while CSI's users need to carefully specify their strategies to ensure soundness.\nThere are three reasons why CSI may produce unsound results given an entirely random strategy. First, some processors are not intended for confluence analysis. They may intend to prove other properties of TRSs, such as termination (Baader and Nipkow 1998). Second, even for the same processor, it may be designed to prove different properties of TRSs with different flags. Third, some transformation processors may transform the confluent problem into an unexpected problem. For example, they can transform it to a relative termination problem (Zantema 2004).\nBy default, CSI utilizes a complicated configuration document with 255 lines written by term rewriting experts to perform confluence analysis. A comprehensive explanation of a previous default strategy can be found in (Nagele, Felgenhauer, and Middeldorp 2017), which slightly differs from the contemporary default strategy.\nWe separate the default strategy of CSI into 23 sub-strategies, which, along with CSI's default strategy, also serve as the initial strategies for Grackle. Among the 23 sub-strategies, eight are designed to show confluence, 14 are utilized to show non-confluence, and one is capable of showing both.\nWe maintain the structure used in CSI's default strategy during the strategy invention. We make such a decision because CSI relies on proved theorems to (dis)prove confluence; however, not all theorems are implemented as a single processor. Such a theorem states that under particular conditions, if some properties of a TRS can be proved, then it is confluent. To utilize such theorems, we need to combine the strategy language and processors to perform transformations on the original TRS and prove the necessary properties of the transformed problem. If we generate strategies randomly, it will be difficult to generate such useful structures and may produce unsound strategies due to inappropriate transformations.\nWe search for three categories of parameters.\nFirst, we search for processor flags which do not violate the soundness guarantee. To ensure soundness, we only search for flags for processors existing in CSI's default strategy. Second, we include iteration parameters, such as time limits or repeated numbers of execution, to regulate the running of a certain sub-strategy. These parameters are defined in CSI's strategy language. To reduce the overall parameter space and ensure soundness, our parameter space only incorporates the iteration parameters used for sub-strategies within CSI's default strategy. These parameters may offer particular advantages since they are applied by term rewriting experts. Moreover, we add a boolean execution-controlling parameter for every parallel or sequential executed sub-strategies, indicating whether to run the particular sub-strategies in confluence analysis. Assume a strategy $A \\|\\| B$, where $\\|\\|$ denotes a parallel execution. The boolean parameters for A and B can represent whether to run one, both, or neither of them. We do not assign boolean execution-controlling parameters to those sequentially executed sub-strategies if we are uncertain whether their exclusion will cause unsoundness.\nWe need to construct a strategy for CSI using the parameters searched by Grackle. To achieve this, we start with CSI's default strategy, replacing the processor flags and iteration parameters with relevant invented parameters. Then, we disable sub-strategies according to the boolean execution-controlling parameters."}, {"title": "Strategy Combination", "content": "After inventing a number of complementary strategies, we want to properly combine them into a single strategy and compare it with the default strategy of CSI. The combination is performed by choosing several strategies from Grackle's final portfolio and appropriately assigning a time limit to each of them.\nIn order to effectively divide the time, we split the whole one minute into several time splits. Next, we greedily allocate a strategy to each time split in the sequence by order. Each newly chosen strategy aims at proving the largest number of remaining benchmark problems that have not been proved by the previously chosen strategies. We shuffle the sequence 100 times and greedily select strategies for each shuffled sequence, resulting in strategy schedules comprising sequences of pairs of strategies and time splits. To utilize a strategy schedule, CSI executes each strategy in it by order for a duration of the relevant time split. We split the one-minute duration into many sequences and perform the greedy strategy selection for each. We finally choose the strategy schedule that maximizes the number of provable problems."}, {"title": "Dataset Augmentation", "content": "Although Cops is meticulously built by term rewriting experts, it is unsuitable for machine learning for three reasons. First, it is relatively small which is insufficient for contemporary machine learning techniques. Second, there may"}, {"title": "TRS Generation Procedure", "content": "We develop a program to randomly generate a large dataset of TRSs, receiving multiple parameters to control the overall generation procedure.\nFirst, the maximum number of available function symbols F, constants C, variables V, and rules R establish the upper bound of the respective quantities of symbols and rules. For each of F, C, and V, a value is randomly selected between zero and the specified maximum, determining the actual number of available symbols. The actual number of rules is randomly chosen between one and R. Second, we define a parameter M, used during the initialization of function symbols. For each function symbol, an arity is randomly chosen between one and M\nAnother important parameter is the probability of generating a left-linear TRS L, which is associated with the likelihood of producing provably confluent TRSs. The majority of contemporary techniques for proving confluence are merely effective for left-linear TRSs. Without regulating the ratios of left-linearity, randomly generated TRSs rarely exhibit left-linearity, making it theoretically difficult to show confluence for them. We also notice that, in practice, CSI can merely prove confluence of very few generated TRSs if the ratios of left-linearity are not controlled. By default, we force 60% of generated TRSs to be left-linear.\nMoreover, for a rule $l \\rightarrow r$, there is a parameter called CT regulating the probability of generating $l$ and $r$ that are compositional terms. We necessitate it because we prefer complex terms, while constants and variables are quite simple. The value of CT can be larger than one, as it indicates the maximum probability. For each TRS, a value is randomly chosen between zero and CT, determining the likelihood of generating compositional terms. If the randomly chosen value is larger than one, we can only generate compositional terms.\nAlgorithm 1 presents the generation procedure of a single term. While choosing the root symbol, we first randomly sample a value between zero and one and compare it with comp to determine whether to only use funs as candidates for the root symbol. Here, comp is a value randomly chosen between zero and CT during the initialization stage of the generation of a TRS. Meanwhile, according to the definition of rewrite rules in Section 2.1, the left term $l$ in $l \\rightarrow r$ cannot be a variable. After choosing a root symbol for the term t, we continuously choose new symbols for undefined function arguments until all of them are defined. After selecting a new variable, we need to remove it from the set of available variables if we are generating a left-linear TRS. The size of the terms generated by us is at most 15, where the size of a term is defined as the number of symbols in it. We choose 15 as the maximum value because the sizes of most terms in Cops are smaller than 15."}, {"title": "Dataset Generation", "content": "We utilize the program explained in this section to construct a large dataset, facilitating the application of machine learning to confluence analysis. First, we randomly generate 100,000 TRSs with the parameters of the maximum number of available function symbols $F = 12$, constants $C = 5$, variables $V = 8$, and rules $R = 15$. Other parameters include the maximum arity of function symbols $M = 8$, the probability of generating left-linear TRSs $L = 0.6$, and the maximum probability of generating compositional terms $CT = 1.6$.\nHowever, the randomly generated dataset can be imbalanced for two reasons. First, there may be significant differences in the number of confluent, non-confluent, and indeterminate problems. Meanwhile, the number of problems mastered by different confluence analysis techniques may vary considerably.\nWe develop a multi-step procedure to build a relatively balanced dataset. First, we execute CSI's default strategy on all generated TRSs for one minute using a single CPU. CSI"}, {"title": "Experimental Results", "content": "Performance on Cops Table 1 depicts the statistics of Grackle's training procedure. The value total shows the number of solved problems after the training, while init is the number solved by the initial strategies. When using four CPUs, Grackle's final portfolio contains more strategies than using one CPU. The most likely reason is that executing with four CPUs can discover some strategies that are only effective with enough computation resources.\nThe invented strategies additionally (dis)prove several problems that have never been proved by different versions of CSI or any participant in CoCo, as depicted in Table 2. If we compare the invented strategies with CSI's results in CoCo 2023, 10 more problems are proved. However, three of them can be proved by previous versions of CSI that employ a simpler much faster strategy. Furthermore, the repeated execution of some sub-strategies contributes to some of the newly solved problems. In total, we show (non-)confluence for seven TRSs that could not be solved by any versions of CSI in CoCo with four of these proofs formally certified. Three of the seven newly discovered proofs have never been proven by any participant in CoCo, and two of them can be certified. For those that cannot, we manually analyzed the proofs to confirm their soundness. The details of the newly proved problems and our manual verification are presented in the appendix.\nTable 3 compares the invented strategies with CSI's default strategy. With a single CPU per each strategy evaluation, Grackle's final portfolio proves 14 more problems than CSI's default strategy. With four CPUs, total proves eight more problems than default.\nWe combine the invented strategies as a single strategy to compare it with CSI's default strategy. The number of time splits and the exact time assigned for each invented strategy are presented in the appendix. With single and four CPUs, combine proves seven and three more problems than the default strategy, respectively.\nWhen using one CPU, we gain more improvements over CSI's default strategy compared to using four CPUs. We assume the reason is that our strategy invention approach is particularly good at generating efficient strategies. With four CPUs, CSI can run several processors in parallel, effectively reducing the runtime.\nWe execute CeTA to verify the proofs produced by our strategies. For each strategy in Grackle's final portfolio, we run CSI on its mastered problems and apply CeTA to verify the proofs. Due to the limitation of CeTA and CSI as explained in Section 2.2, only 238 and 245 proofs can be verified when one and four CPUs are employed for strategy invention, respectively. We manually check the unverifiable proofs and discover no errors. We attach our manual check of CeTA's output and other manual verification procedures in the appendix."}, {"title": "Examples", "content": "Besides the example in Section 1, we present two more examples of the invented strategies that (dis)prove problems unprovable by any participant in CoCo.\nThe core structure of the first example is (REDUNDANT_DEL?; KB). It proves confluence for 939.trs in Cops. The sub-strategy KB, denoting Knuth-Bendix certrion (Knuth and Bendix 1983), is"}, {"title": "Related Work", "content": "There have been several attempts to apply machine learning to rewriting; however, none have been applied to automatic confluence provers. Winkler investigates (Winkler and Moser 2019) feature characterization of term rewrite systems but they do not build any learning models based on the features. There are works analyzing the termination of programs using neural networks to learn from the execution traces of the program (Giacobbe, Kroening, and Parsert 2022; Abate, Giacobbe, and Roy 2021). Nevertheless, they do not transform programs to term rewrite systems and apply machine learning to guide automatic term rewriting tools in termination analysis. MCTS-GEB (He, Singh, and Yoneki 2023) applies reinforcement learning to build equivalence graphs for E-graph rewriting, but it focuses on optimization problems, not on confluence."}, {"title": "Conclusion and Future Work", "content": "We have proposed an approach to automatically invent strategies for the state-of-the-art confluence analysis prover CSI. We have performed data augmentation by randomly generating a large number of term rewrite systems and mixing these with the human-built dataset Cops. We have evaluated the invented combined strategy both on the original Cops dataset and the augmented dataset. The invented strategies discover significantly more proofs than CSI's default strategy on both datasets. Notably, three of the human-written problems have never been proved by any automatic confluence provers in the annual confluence competitions.\nFuture work includes applying machine learning also to individual term-rewriting techniques, for example those that perform search in a large space. Prioritizing the more promising parts of the search space could improve the individual techniques. Our strategy invention approach could also be extended to other automatic term rewriting provers. It would also be possible to apply neural networks to directly predict appropriate strategies for automatic term rewriting tools, however, soundness of proofs generated using such an approach remains a major challenge."}, {"title": "Term Rewriting", "content": "We assume a disjoint set of variable symbols V and a finite signature F. The set of terms T(F, V) is built up from V and F. The set of variables occurring in a term t is denoted by Var(t). A substitution is a mapping \ud835\udf0e from variables to terms, and t\ud835\udf0e denotes the application of the substitution \ud835\udf0e to the term t. A hole is denoted by a special symbol \u22c4 \u2209 F, and a context C is a term that contains exactly one hole. The notation C[t] denotes substituting the hole in C with the term t. A term rewrite system (TRS) consists of a set of rewrite rules l \u2192 r where l,r \u2208 T(F, V), l \u2209 V, and Var(r) \u2286 Var(l). Consider the TRS R, we write t \u2192R u for terms t, u if there exists a rewrite rule l \u2192 r\u2208 R, a context C, and a substitution \ud835\udf0e such that t = C[l\ud835\udf0e] and u = C[r\ud835\udf0e]. We write \u2192* to denote the transitive-reflexive closure of \u2192R. We drop the subscript R for the relations on terms in the subsequent appendix if it is contextually inferrable. A TRS is confluent if and only if \ud835\udefbs, t,u \u2208 T(F,V)(s \u2192* t \u2227 s \u2192* u \u21d2 \u2203v \u2208 T(F,V)(t \u2192* v \u2227 u \u2192* v)). A term is called linear if no variable multiply occurs in it. A rewrite rule l \u2192 r is called left-linear if l is linear. A TRS is called left-linear if all its rules are left-linear. Left-linearity is crucial for confluence analysis since most existing confluence analysis techniques depend on it to determine the confluence of TRSs. In this paper, a term is called compositional if it is neither a variable nor a constant."}, {"title": "Individual Invented Strategies", "content": "Listing 1 presents a strategy invented by us. Assume we are in a directory containing a problem 160.trs and invented.conf. To invoke the invented strategy for the problem 160.trs in Cops, we need the command csi -C CR-S AUTO -c invented.conf 160.trs. Here, the flag -C CR denotes we are performing confluence analysis. The flag -c specifies the configuration document to choose, and -s specifies the strategy to choose in the document. We choose the strategy AUTO in every invented configuration document. Table 6 depicts the unified strategy learned on Cops when one CPU is allocated to each strategy. To utilize the unified strategy, we need to run each strategy in the sequence with the given time limit t using the notion AUTO [t]. Here, [t] specifies the time limit for running AUTO. Below, we present the time-split sequences for other learned unified strategies. We ignore the names of the invented strategies since they contain hashes that are difficult to understand. When four CPUs are allocated to each strategy, and Grackle trains on Cops, the following time-split sequence is learned 0.5, 0.5, 0.5, 0.5, 1, 2, 3, 3, 3, 5, 6, 6, 6, 7, 8, 8. When one CPU is allocated to each strategy, and Grackle trains on the augmentation dataset, the following time-split sequence is learned 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1, 6, 7, 8, 9, 10, 12. When four CPUs are allocated to each strategy, and Grackle trains on the augmentation dataset, the following time-split sequence is learned 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1, 7, 8, 8, 9, 10, 12."}, {"title": "Newly Proved Problems", "content": "When we use one CPU for each strategy, we newly proved the following problems could not be proved by CSI in CoCo. If the proof cannot be certified by CeTA, we learn from the proof and use defined strategies in CSI's configuration document to construct the proof. Since the discovered proofs can be simulated by sub-strategies in CSI's default configuration document, we conclude the newly discovered proofs sound. The defined strategies are capitalized.\n\u2022 991.trs is non-confluent, and the proof can be certified.\n\u2022 999.trs is non-confluent, and the proof can be certified.\n\u2022 997.trs is non-confluent, and the proof can be certified.\n\u2022 1652.trs is confluent and the proof can be certified.\n\u2022 1024.trs is confluent, but the proof cannot be certified. It can be proven by (at -bound 16; SN)!. The flag bound is sound according to CSI's manual.\nWhen we use four CPUs per strategy, we newly proved the following problems cannot be proved by CSI before.\n\u2022 991.trs is non-confluent, and the proof can be certified.\n\u2022 999.trs is non-confluent, and the proof cannot be certified. The strategy invented for 999.trs by allocating one CPU per strategy can be verified because Grackle invents different strategies.\n\u2022 1652.trs is confluent, and the proof can be certified.\n\u2022 1024.trs is confluent, but the proof cannot be certified. It can be proven by (at -bound 16; SN)!. \u2022 170.trsis confluent, but the proof cannot be certified. It can be proven by ((REDUNDANT_DEL?;AC)3*! || CPCS) *! \u2022 939.trs is confluent, but the proof cannot be certified. It can be proven by (REDUNDANT_DEL?; KB)!\nThe problems 991.trs, 997.trs, and 939.trs have never been proved by any participant in CoCo."}]}