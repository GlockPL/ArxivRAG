{"title": "Knowledge Management in the Companion Cognitive Architecture", "authors": ["Constantine Nakos", "Kenneth D. Forbus"], "abstract": "One of the fundamental aspects of cognitive architectures is their ability to encode and manipulate knowledge. Without a consistent, well-designed, and scalable knowledge management scheme, an architecture will be unable to move past toy problems and tackle the broader problems of cognition. In this paper, we document some of the challenges we have faced in developing the knowledge stack for the Companion cognitive architecture and discuss the tools, representations, and practices we have developed to overcome them. We also lay out a series of potential next steps that will allow Companion agents to play a greater role in managing their own knowledge. It is our hope that these observations will prove useful to other cognitive architecture developers facing similar challenges.", "sections": [{"title": "1. Introduction", "content": "Knowledge is the fuel for cognitive architectures. Whether the task is question answering (Crouse, 2021), moral reasoning (Dehghani, 2009; Olson & Forbus, 2023), visual understanding (Chen, 2023), or game playing (Hancock, Forbus, & Hinrichs, 2020; Hancock & Forbus, 2021; Hinrichs & Forbus, 2011), cognitive architectures need a way to encode and manipulate knowledge about the world around them. The specific format will depend on the design of the architecture, its core claims, and the way it is deployed. Each architecture will also develop its own best practices for authoring, storing, and accessing the knowledge it uses, and these practices will evolve over time as the architecture scales up, changes, and develops new capabilities.\nThe Companion cognitive architecture (Forbus & Hinrichs, 2017) is undergoing just such a transition. Recent work on knowledge extraction (Ribeiro & Forbus, 2021) has laid the groundwork for automatically acquiring world knowledge that can be reused across domains, while the ongoing SocialBot project, an online expansion of the information kiosk described by Wilson et al. (2019), has required new knowledge management capabilities to support Companions in their role as social agents. For the Companion architecture to reach the next stage of its development, it will need a new set of tools and practices for knowledge management, enabling the agent itself to play a larger role in its own learning.\nWith new developments on the horizon, we take this opportunity to document the Companion knowledge stack as it currently stands, centered around the challenges we have faced and how we have addressed them. We discuss three major challenges: 1) knowledge representation, the design problem of encoding knowledge in a format amenable to reasoning, 2) knowledge access, the practical problem of efficiently making the knowledge available to a running Companion, and"}, {"title": "2. Companions", "content": "The main purpose of the Companion cognitive architecture (Forbus & Hinrichs, 2017) is to learn how to build software social organisms\u2014agents that can learn, reason, and interact the way people do-as a step towards achieving human-level AI (Forbus, 2016). In support of this, Companions are equipped with a wide range of capabilities, from natural input modalities such as sketching and language to Hierarchical Task Network (HTN) planning to a robust analogy stack for performing mapping (Forbus et al., 2016), retrieval (Forbus, Gentner, & Law, 1995), and generalization (Kandaswamy & Forbus, 2012).\nEvery Companion consists of several agents operating in parallel, each one responsible for handling different tasks. For example, the Interaction Manager can parse a question asked by the user, then pass the query off to the Session Reasoner to determine the answer. Other agents handle capabilities like sketch understanding (Forbus et al., 2011) or strategic reasoning (Hancock & Forbus, 2021). This architecture provides coarse-grained parallelism and allows the Companion's capabilities to be expanded as needed by creating new types of agents.\nEach agent in a Companion has its own instance of the FIRE reasoning engine (Forbus et al., 2010). FIRE gives the agent access to the analogy stack, logical inference, abduction, procedural attachments, and a working memory for storing reasoning results. But its most important duty is interfacing with the KB, allowing the agent to retrieve knowledge seamlessly as part of reasoning.\nCompanions depend heavily on conceptual knowledge. Rather than starting with an empty KB and trying to learn from scratch, Companions use NextKB (Section 3.3 ) as an approximation of the kind of conceptual knowledge a human has access to. While the coverage of NextKB is necessarily imperfect, it has been sufficient to support a variety of reasoning and learning tasks, including question answering (Crouse, 2021), legal reasoning (Blass, 2023), and learning by reading (Ribeiro & Forbus, 2021).\nTwo particular applications illustrate how Companions' knowledge management needs have grown in recent years: learning by reading and the SocialBot. We discuss each in turn before"}, {"title": "2.1 Learning by Reading", "content": "Ribeiro & Forbus (2021) show how a Companion can learn commonsense knowledge from text. They use analogical techniques to extract the knowledge and a BERT-based classifier to filter out erroneous facts. For example, the system can extract the fact (properPhysicalPartTypes (MaleFn Deer) Antler) from the sentence \"Male deer have antlers.\" The system can learn to extract new relations from just a few examples. When tested on Simple English Wikipedia, it was able to extract 976 commonsense facts across 58 relation types from 2,679 articles with a precision of 71.4% (Ribeiro, 2023).\nThe long-term goal for this research is to enable a Companion to learn on its own and apply what it has learned to other reasoning tasks. However, this requires a level of autonomy that Companions do not yet possess. One of the main bottlenecks is the Companion's ability to manage the knowledge it learns: organizing it for reasoning with FIRE, integrating it into NextKB so other Companions can benefit from what one Companion learns, and tracking the provenance of learned knowledge so the quality of different sources can be monitored and errors can be corrected. These concerns have helped motivate the additions to the Companion knowledge stack discussed in Section 5.."}, {"title": "2.2 SocialBot", "content": "The SocialBot is a Companion-powered bot for the Microsoft Teams\u00b9 platform, an expanded, online version of the multimodal information kiosk reported in Wilson et al. (2019). The SocialBot automatically scrapes the Northwestern University Computer Science Department website for information about courses, faculty, and events, then uses that information to answer user questions on Teams. Users can also share their food and drink preferences and their interest in CS topics so the bot can potentially provide personalized event recommendations. Ongoing research will expand the range of feedback users can provide, improving the SocialBot's recommendations and filling in gaps in its knowledge.\nThe SocialBot has stretched the Companion knowledge stack in two ways. First, its nightly scrapes of the CS Department website require tracking different versions of the same knowledge (e.g., updated event listings) to prevent stale data from appearing in its KB and thus its answers to user questions. This was one of the motivations behind the creation of the provenance cache, discussed in Sections 5.1 and 5.2.\nSecond, the SocialBot must manage the knowledge it learns from users: organizing it, storing it persistently, and ensuring that it is available when a user asks a question. These requirements motivated the creation of the Archivist, discussed in Section 5.3, a Companion agent dedicated to saving learned knowledge and serving it to other agents that need it."}, {"title": "3. Challenge #1: Knowledge Representation", "content": ""}, {"title": "3.1 Knowledge Representation Basics", "content": "The foundation of any reasoning system is knowledge representation. The Companion cognitive architecture follows in the footsteps of Cyc (Lenat, 1995), a multi-decade effort to encode common-sense knowledge to support machine reasoning. Specifically, Companions use CycL, a language based on higher-order predicate calculus that represents assertions as Lisp-style s-expressions. Properly formed assertions consist of a predicate and its arguments, each of which may be an atomic constant or non-atomic term denoting an entity or a nested assertion. For example, the English statement \u201cJoe likes apples.\" might be represented as the CycL assertion\n(likesType Joe (FruitFn AppleTree)), where likesType is the predicate, Joe is the constant for Joe, and (FruitFn AppleTree) is the non-atomic term that represents the fruit (the function FruitFn) of an apple tree (AppleTree).\nIn this example, (FruitFn AppleTree) denotes the collection of all apples, whereas Joe denotes an individual entity. Individuals and collections form the backbone of the ontology. The two are linked through the isa predicate: (isa Nero-TheCat Cat) states that the individual Nero-TheCat is a member of the collection Cat. Collections are related to each other through the genls\u00b2 predicate: (genls Cat Mammal) states that all members of the collection Cat are members of the collection Mammal. genls is transitive, so if (genls Mammal Animal), then all Cats are Animals.\nHigher-order collections support the categorization of collections themselves, such as (isa Cat BiologicalSpecies).\nKnowledge is stored in KRF files. Each file consists of a series of assertions to load into a running KB, along with processing directives that affect how they are loaded. (The directives are in-microtheory, discussed in Section 3.2, and with-provenance, discussed in Section 5.2.) KRF files also support a handful of macros that do not conform to CycL syntax. These are useful for encoding groups of related facts in a compact way. For example, HTN methods, preconditions, and preference rules are bundled together with the defPlan macro. Macros are expanded into valid CycL assertions before being loaded into the KB."}, {"title": "3.2 Microtheories", "content": "Not all knowledge belongs together. Conflicting knowledge about the world can lead to logical contradictions. Reasoning problems in one domain may be derailed by knowledge intruding from another domain, such as the inclusion of fictional characters in a real-world planning task. More broadly, knowledge must be maintained by humans, and thus must be organized in a way that facilitates common ontologizing tasks, such as lookup, knowledge entry, and revision.\nTo that end, a Companion's knowledge is organized into microtheories (MTs; Guha, 1992; Lenat, 1998), bundles of assertions that are logically consistent and belong in the same reasoning context. All KB queries and reasoning operations are contextualized with a microtheory that determines which assertions are visible to that operation. For example, two physics models can be stored in different microtheories and queried independently, even if they are incompatible with each other."}, {"title": "3.3 NextKB", "content": "With the knowledge representation format settled, the next question becomes what knowledge to represent. Companions uses NextKB4, a knowledge base derived from OpenCyc and extended with linguistic knowledge, qualitative representations, Companions rules and plans, and more.\nNextKB serves as a seed for a Companion's learning and reasoning, providing it with broad common-sense knowledge about the world. NextKB consists of over 700,000 facts spread across 1,300 microtheories and 1,000 KRF files. The ontology contains 83,000 collections, 26,000 predicates, and 5,000 functions, with over 3,000 rules to support reasoning.\nSo far, the knowledge in NextKB comes from three sources: the original OpenCyc ontology, manual extensions written by Companion users, and extensions generated semi-automatically from existing resources. All of these require some degree of hand curation. In Sections 5 and 6, we discuss the shift towards automatic extension of NextKB and how our representation and storage schemes are changing to accommodate it."}, {"title": "4. Challenge #2: Knowledge Access", "content": ""}, {"title": "4.1 PlanB", "content": ""}, {"title": "4.2 AllegroCache", "content": "PlanB uses AllegroCache, a fast and scalable database that natively supports Lisp data types. In addition to typical database functionality such as persistent storage and rollback, AllegroCache offers two features of interest: a persistent-class Lisp metaclass that allows Lisp objects to be stored directly in the database, and the map-range class, which supports key-value storage.\npersistent-class is the backbone of PlanB. Facts are stored as persistent objects with slots for their Lisp form, an integer ID, and the microtheories in which they are believed. KB concepts are stored as conceptual entities, persistent objects that can be specialized according to the type of the concept. For example, the subclass for predicates has extra slots for storing the predicate's arity, whether it is commutative, and so on. Like facts, conceptual entities can be looked up by their Lisp form (s-expression) or their integer ID.\nmap-ranges also play an important role in PlanB. They function as persistent hash tables whose keys can be any basic Lisp data type. Internally, the keys are sorted in a way that makes retrieval of sequential keys efficient using cursors. map-ranges' flexibility makes them useful for any data that does not fit neatly into the persistent-class paradigm, such as the mentions map or special facts (both discussed below).\nTesting has revealed that map-ranges are more efficient than persistent-classes. As the usage patterns for PlanB stabilize over time, we are planning to shift more PlanB functionality to map-ranges, trading the convenience of persistent-classes for improved speed. The shift will have the added benefit of making PlanB more portable, as AllegroCache's map-range is closer to the kind of functionality supported by other database systems."}, {"title": "4.3 Retrieval", "content": "The primary function of a knowledge base is to retrieve stored facts on demand. PlanB supports unification-based retrieval. Given a query pattern with zero or more variables, PlanB will retrieve all facts which unify with the pattern. Queries can be contextualized with a microtheory, in"}, {"title": "4.4 Special Facts & Indexed Facts", "content": "PlanB supports the definition of special facts, facts which are stored and retrieved using custom handlers. Special facts are used when efficiency is paramount, or when the nature of the fact lends itself to a specific type of storage. For example, genlMt statements are stored in a persistent TMS that allows efficient computation of microtheory inheritance (Forbus et al., 2010). Other special facts include fact probability and provenance (discussed below), bookkeeping for analogical retrieval, and templates for converting KB concepts to natural language.\nSpecial facts are retrieved much faster than regular facts, as they can take advantage of custom lookup strategies, but the need for specialized code for each type of fact makes them"}, {"title": "5. Challenge #3: Knowledge Management", "content": ""}, {"title": "5.1 The Epistemic Layer", "content": "The knowledge organization scheme we have discussed so far can be broken down into two layers: the physical layer and the logical layer. The physical layer consists of the KRF files where knowledge is stored before being loaded into the KB.\u201d The basic unit of organization is the file, but directory structure also matters, both for the developers tasked with expanding a Companion's knowledge and for the index files that determine which KRF files should be incorporated into a KB build.\nThe primary way changes to the physical layer are propagated is through version control. Updated KRF files are checked in to a repository, where other developers can pull and reload them, or else wait for a fresh build of the KB that will contain all of the latest updates. This setup works well for the way NextKB has been developed so far: hand-curated knowledge maintained by a small set of developers and only one KB configuration. In Section 6, we discuss how these circumstances are changing.\nIn contrast, the logical layer refers to knowledge as it exists in the KB. The basic unit of organization is the microtheory, which bundles knowledge into units that are useful for reasoning. The logical layer is independent of the physical layer. One file can have multiple microtheories, and the contents of one microtheory can be spread out across multiple files.\nThis independence is very useful. Files can be merged or split without changing the logical structure of the KB, and the KB need only concern itself with facts and microtheories, not the files themselves. However, the independence of the logical and physical layers also leads to a problem. Once read into the KB, there is no way to tell what file a fact came from, so there is no easy way to correct a fact after it has been stored. Iterative correction of KRF files is a core part of Companion development, making this a pain point. Common workarounds include forgetting an entire microtheory so that its contents can be loaded fresh (cumbersome for large MTs split across multiple files) or saving a copy of the last-loaded version of a KRF file so its facts can be forgotten before the corrected version is reloaded (an easy step to forget)."}, {"title": "5.2 The Provenance Cache", "content": "Provenance information is stored by PlanB in the provenance cache, a set of map-ranges that provide a lightweight mapping from facts to provenance events and vice versa. The provenance predicate interfaces with the provenance cache via special fact handlers, while"}, {"title": "5.3 The Archivist", "content": "There has been one other noteworthy development in the way Companions handle knowledge. The Archivist is a Companion agent tasked with tracking knowledge and serving it up to other agents during a session. It receives updates to KRF files from other agents, serves the updated files to agents upon request, and maintains a queue of changes to commit to a version control repository. The net result is that a Companion agent can gather new information during a session, propagate that information to other agents, and save that information persistently.\nThe Archivist was designed to keep knowledge up to date in the SocialBot. To handle conversations with multiple users in parallel, the SocialBot maintains a pool of Companion agents running on a cluster, each with its own FIRE reasoner and instance of the KB. At the end of a conversation, the agent sends any knowledge it learned to the Archivist for persistent storage. At the beginning of a new conversation, the agent queries the Archivist for any relevant updates that have been posted by other agents."}, {"title": "6. Next Steps", "content": "While developments like the provenance cache and the Archivist have expanded the knowledge management capabilities of Companions, we have only scratched the surface of what is possible. The ultimate goal is a self-directed agent capable of acquiring and organizing knowledge on its own. In the remainder of this section, we discuss potential next steps towards this goal."}, {"title": "6.1 Self-Directed Learning", "content": "While prior work has explored how a Companion can direct its own learning (e.g. Hinrichs & Forbus, 2019), this capability has not been tested at scale. Ideally, a Companion should have a set of broad learning goals that drive its behavior. When not otherwise occupied, it should seek to satisfy its goals by reading text, asking a user questions, ruminating on previously acquired knowledge, or running games or simulations.\nSupporting this behavior will require extending the Companion knowledge stack. Acquired knowledge must be vetted and stored persistently so that other Companions can benefit from what"}, {"title": "6.2 Knowledge Integration & Trust", "content": "One of the more interesting problems that arise as a Companion takes responsibility for more of its learning is figuring out how to integrate knowledge from multiple sources. Knowledge from an external source may not necessarily be accurate, and it may not be consistent with what the Companion has learned from other sources. To robustly learn from webpages, books, interactions with humans, and/or other outside sources of information, a Companion must be equipped with ways to compare, assess, and deploy the knowledge it has learned.\nOne important aspect of this problem is trust. Not all knowledge sources are trustworthy. Users may speak in ignorance or with intent to deceive, webpages vary wildly in terms of their veracity, and, in the extreme case, large language models (LLMs) generate plausible-sounding text with no specific grounding in reality. As such, a Companion needs to build up a model of how trustworthy its sources are so it can reconcile contradictions, proactively vet knowledge from suspect sources, and prioritize learning from more reliable ones.\nThe provenance cache provides a basis for this, tagging the source of each fact in the KB and making it available for introspection. When the Companion detects a suspect fact in the course of its reasoning, it can note the source that was to blame and update its trust model accordingly.\nDempster-Shafer Theory (Shafer, 1976) is one option for tracking reliability. The theory lays out how to combine claims from different sources, producing belief and plausibility estimates that provide lower and upper bounds on how probable the argument from evidence to conclusion is. Olson & Forbus (2021) show how Dempster-Shafer Theory can be used to resolve competing testimony about norms. Extending this to general learned knowledge is an avenue of future work."}, {"title": "6.3 Extending the Provenance Cache", "content": "While the current iteration of the provenance cache has proven useful, there are several additions that will help it support Companions' learning. First, the provenance cache should support events with hierarchical structure, such as scraping a single webpage as part of a larger scraping pass or running one trial within a larger experiment. The more information a Companion has about the structure of its experiences, the better equipped it will be to optimize its learning.\nSecond, the provenance ontology should be expanded to track different types of provenance events and sources. The current ontology consists of a handful of concepts in NextKB, which has been sufficient so far but lacks the systematicity of an ontology like PROV (Moreau et al., 2015)."}, {"title": "7. Related Work", "content": ""}, {"title": "7.1 Knowledge in Cyc", "content": "Cyc shares the Companion cognitive architecture's focus on conceptual knowledge, and due to the massive size of its KB, its need for knowledge management is even greater. As such, Cycorp has developed a robust knowledge entry pipeline for the Cyc KB (A. Sharma, personal communication, March 10, 2024; Siegel et al., 2004). Ontological engineers work locally through a UI, and transcripts of their changes are sent to a central server for incorporation into the nightly KB build.\nNotably, the Cyc build process is incremental, beginning with the most recent version of the KB and applying accumulated changes until the KB is up to date. In contrast, NextKB is rebuilt from scratch using standalone KRF files. Extending the provenance cache to track deletions will allow Companions to adopt Cyc-style updates as necessary by writing out the KB changes made during a session and using them to patch another KB instance.\nThe Cyc KB tracks the provenance of its assertions, including the ontologist who added them, the date and time they were added, and the original source of the information. Records of deletions are not present in the KB but can be derived from transcript files. This is in line with the aims of the Companion provenance cache, and as we plan our next steps, we take inspiration from Cyc's ability to show the user the original sources of facts used during inference."}, {"title": "7.2 Knowledge in Soar", "content": "The Soar cognitive architecture (Laird, 2012) provides an interesting contrast to the Companion knowledge stack. Where the Companion cognitive architecture prioritizes conceptual knowledge and reasoning, Soar prioritizes procedural knowledge and skill learning. This focus allows Soar to make concrete psychological claims about human performance and procedural learning while maintaining an architecture both general and performant enough for real-world applications.\nThe difference in emphasis leads to a variety of differences in knowledge representation and storage. Soar represents declarative knowledge with graph structures. Working memory consists of a single connected graph, while long-term declarative memory stores multiple graphs that can be retrieved into working memory based on cues. Soar's declarative memory is partitioned into semantic memory, which contains general knowledge about the world, and episodic memory, which records the agent's experiences, saved as snapshots of previous working memory states.\nIn contrast, Companions treat CycL assertions as the basic unit of declarative knowledge, have a single long-term memory organized into microtheories, and lack a single, system-wide, automatic mechanism for episodic memory (cf. Forbus & Kuehne, 2007; Hancock, Forbus, &"}, {"title": "7.3 Provenance", "content": "There is a large body of existing work on representing the provenance of information, especially for the Semantic Web (see Herschel, Diestelk\u00e4mper, & Ben Lahmar (2017), P\u00e9rez, Rubio, & S\u00e1enz-Ad\u00e1n (2018), and Sikos & Philp (2020) for reviews). Languages such as PML (Pinheiro da Silva, McGuinness, & McCool, 2003) and PML 2 (McGuinness et al., 2007) can encode explanations for the conclusions produced by reasoning systems, including the provenance of the information used, while the PROV family of documents (Moreau et al., 2015) lays out a formal specification for encoding different types of provenance.\nThese endeavors are more ambitious than the Companion provenance cache, and they serve a different purpose. Unlike applications that use the Semantic Web, Companions perform most of their reasoning locally using knowledge aggregated at a central repository. Provenance matters for managing knowledge within the KB, tracking the sources of learned knowledge, and assessing their reliability, but it plays much less of a role than in the Semantic Web, where reasoning is frequently distributed and knowledge is drawn ad hoc from a variety of sources. Consequently we have opted for a lightweight provenance ontology rather than replicating the full complexity of, say, PROV. However, as Companions' provenance needs grow, we will expand the ontology as needed, drawing on existing work for inspiration. In particular, PROV's tripartite distinction between entities, agents, and activities might be useful for our needs."}, {"title": "8. Conclusion", "content": "One of the most important aspects of a cognitive architecture is how it deals with knowledge. In this paper, we have outlined the Companion knowledge stack, including the ways Companions represent, access, and manage their knowledge. We have also discussed potential next steps for the architecture that will enable Companions to play a greater role in their own learning. It is our hope that these observations will prove useful to cognitive architecture developers facing similar challenges."}]}