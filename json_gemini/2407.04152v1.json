{"title": "VoxAct-B: Voxel-Based Acting and Stabilizing Policy\nfor Bimanual Manipulation", "authors": ["I-Chun Arthur Liu", "Daniel Seita", "Sicheng He", "Gaurav S. Sukhatme"], "abstract": "Bimanual manipulation is critical to many robotics applications. In\ncontrast to single-arm manipulation, bimanual manipulation tasks are challeng-\ning due to higher-dimensional action spaces. Prior works leverage large amounts\nof data and primitive actions to address this problem, but may suffer from sam-\nple inefficiency and limited generalization across various tasks. To this end, we\npropose VoxAct-B, a language-conditioned, voxel-based method that leverages\nVision Language Models (VLMs) to prioritize key regions within the scene and\nreconstruct a voxel grid. We provide this voxel grid to our bimanual manipulation\npolicy to learn acting and stabilizing actions. This approach enables more efficient\npolicy learning from voxels and is generalizable to different tasks. In simulation,\nwe show that VoxAct-B outperforms strong baselines on fine-grained bimanual\nmanipulation tasks. Furthermore, we demonstrate VoxAct-B on real-world Open\nDrawer and Open Jar tasks using two UR5s. Code, data, and videos will be\navailable at https://voxact-b.github.io.", "sections": [{"title": "1 Introduction", "content": "Bimanual manipulation is essential for robotics tasks, such as when objects are too large to be\ncontrolled by one gripper or when one arm stabilizes an object of interest to make it simpler for the\nother arm to manipulate [1]. In this work, we focus on asymmetric bimanual manipulation. Here,\n\"asymmetry\" refers to the functions of the two arms, where one is a stabilizing arm, while the other is\nthe acting arm. Asymmetric tasks are common in household and industrial settings, such as cutting\nfood, opening bottles, and packaging boxes. They typically require two-hand coordination and\nhigh-precision, fine-grained manipulation, which are challenging for current robotic manipulation\nsystems. To tackle bimanual manipulation, some methods [2, 3] train policies on large datasets, and\nsome exploit primitive actions [4, 5, 6, 7, 8, 9, 10]. However, they are generally sample inefficient,\nand using primitives can hinder generalization to different tasks.\nTo this end, we propose VoxAct-B, a novel voxel-based, language-conditioned method for bimanual\nmanipulation. Voxel representations, when coupled with discretized action spaces, can increase sam-\nple efficiency and generalization by introducing spatial equivariance into a learned system, where\ntransformations of the input lead to corresponding transformations of the output [11]. However, pro-\ncessing voxels is computationally demanding [12, 13]. To address this, we propose utilizing VLMs\nto focus on the most pertinent regions within the scene by cropping out less relevant regions. This\nsubstantially reduces the overall physical dimensions of the areas used to construct a voxel grid,\nenabling an increase in voxel resolution without incurring computational costs. To our knowledge,\nthis is the first study to apply voxel representations in bimanual manipulation."}, {"title": "2 Related Work", "content": "Bimanual Manipulation. There has been much prior work in bimanual manipulation for folding\ncloth [5, 8, 17, 18, 19, 20, 21], cable untangling [6], scooping [9], bagging [22, 23, 24], throw-\ning [25], catching [26], and untwisting lids [27]. Other works study bimanual manipulation with\ndexterous manipulators [28, 29, 30, 31] or mobile robots [32]. In contrast to these works, our focus\nis on a general approach to bimanual manipulation with parallel-jaw grippers on fixed-base ma-\nnipulators. Works that study general approaches for bimanual manipulation include [2, 4, 7, 33],\nwhich use primitive actions or skills to reduce the search space across actions. Other general ap-\nproaches focus on orthogonal tools such as interaction primitives [34] or screw motions [35]. Re-\ncently, Zhou et al. [3] introduced another general approach, based on \"action chunking\" to learn\nhigh-frequency controls with closed-loop feedback and applied their method on multiple asymmetric\nbimanual manipulation tasks using low-cost hardware. Other works extended this by either incorpo-"}, {"title": "3 Problem Statement", "content": "Given access to a pre-trained VLM and expert demonstrations, the objective is to produce a bimanual\npolicy \u03c0 for a variety of language-conditioned manipulation tasks. We assume a flat workspace with\ntwo fixed-base robot manipulators, each with a parallel-jaw gripper. A policy \u03c0 controls both arms\nby producing actions $a_t = (a_t^s, a_t^a)$ at each time step t, where $a_t^s$ and $a_t^a$ refer to\nthe stabilizing and acting arm actions, respectively. For simplicity, we suppress the time t when the\ndistinction is unnecessary. We use the low-level action representation from PerAct [11] with $a^a =$\n$(a_{pose}, a_{open}, a_{collide})$ and $a^s =$\n$(a_{pose}, a_{open}, a_{collide})$. These specify each arm's 6-DOF gripper\npose, its gripper open state, and whether a motion planner for the arms used collision avoidance to\nreach an intermediate pose. We assume task-specific demonstrations $D_e = {\\xi_1, \\xi_2, ..., \\xi_n}$ and two"}, {"title": "4 Method", "content": "4.1 Extending PerAct for Bimanual Manipulation\nPerAct [11] was originally designed and tested for single-arm manipulation. We extend it to support\nbimanual manipulation. A natural way to do this would be to train separate policies for the two\narms. However, we exploit the discretized action space that predicts the next best voxel with spatial\nequivariance properties and formulate a system that uses acting and stabilizing policies. In contrast\nto a policy that operates in joint-space control, acting and stabilizing policies perform the same\nfunctions irrespective of whether it is a left arm or a right arm, assuming the next best voxel is\nkinematically feasible for both arms. Hence, either arm can execute an acting policy or a stabilizing\npolicy, which improves policy learning. In the low-level action space, the arms execute one low-level\naction $a^a$ and $a^s$ (see Section 3) at each time t. In the following, we use similar notation as [11] but\nindex components as belonging to an arm using the superscript: $arm \\in {acting, stabilizing}$.\nAt each time step, the input to each arm is a voxel observation v, proprioception data of both robot\narms p, a language goal $l \\in {l_{as}, l_{sa}}$, and an arm ID $\\xi \\in {0, 1}$, and the task is to predict an\naction. During training, the language goal is given in the data, but during evaluation, we use VLMs\nto determine which language goal, $l_{as}$ or $l_{sa}$, to use. If the language goal is $l_{as}$, we assign the left\narm ($\\xi = 0$) to the acting policy and the right arm ($\\xi = 1$) to the stabilizing policy, and conversely\nfor $l_{sa}$. During training, this allows our method to learn to map the appropriate acting or stabilizing\nactions to a given arm, and during evaluation, this informs each arm's actions. Note that the predicted\narm ID is discarded. PerAct uses value maps to represent different components of the action space,\nwhere predictions for each arm are Q-functions with state-action values. Formally, we have the\nfollowing five value maps per arm, as the output of the arm's learned deep neural network, where:\n$V_{trans}^{arm} = softmax (Q_{trans}^{arm} ((x, y, z)|v, \\rho, l, \\xi))$\n$V_{rot}^{arm} = softmax (Q_{rot}^{arm} ((\\psi, \\theta, \\phi)|v, \\rho, l, \\xi))$\n$V_{open}^{arm} = softmax (Q_{open}^{arm} (w|v, \\rho, l, \\xi))$\n$V_{collide}^{arm} = softmax (Q_{collide}^{arm} (\\kappa|v, \\rho, l, \\xi))$\n$V_{id}^{arm} = softmax (Q_{id}^{arm} (v|v, \\rho, l, \\xi))$"}, {"title": "4.2 VoxAct-B: Voxel Representations and PerAct for Bimanual Manipulation", "content": "When using voxel representations for fine-grained manipulation, a high voxel resolution is essential.\nWhile one can increase the number of voxels, this would consume more memory, slow down train-\ning, and adversely affect learning as the policy is optimizing over a larger state space. Therefore,\ngiven a voxel grid observational input v of size (L \u00d7 W \u00d7 H) that spans $x^3$ meters of the workspace,\nwe keep the number of voxels the same but reduce the relevant workspace. We use VLMs to de-\ntect the object of interest in the scene and \u201ccrop\" the grid around this object, resulting in a voxel\ngrid that spans $\\alpha x^3$ meters of the workspace, where \u03b1 is a fraction that determines the size of the\ncrop. This allows zooming into the more important region of interest. The voxel resolution becomes\n$\\frac{(L, W, H)}{\\alpha x}$ voxels/meters from the original resolution of $\\frac{(L, W, H)}{x}$ voxels/meters.\nTo detect the object of interest reliably, we use a two-stage approach similar to [16]. We input a text\nquery to an open-vocabulary object detector to detect the object. Then, we use a foundational image\nsegmentation model to obtain the segmentation mask of the object and use the mask's centroid along\nwith point cloud data to retrieve the object's pose with respect to the front camera. We use the pose\nof the object to determine the task-specific roles of each arm and the language goal. This cropped\nvoxel grid and language goal are the input to our bimanual manipulation policy. We call our method\nVoxAct-B: Voxel-Based Acting and Stabilizing Policy. See Figures 2 and 3 for an overview."}, {"title": "4.3 Additional Implementation Details", "content": "The bimanual manipulation policy uses a voxel grid size of $50^3$ that spans $2^3$ meters. The pro-\nprioception data includes: the gripper opening state of both arms, the positions of the left arm\nleft finger, left arm right finger, right arm left finger, right arm right finger, and timestep. Fol-\nlowing PerAct [11], we apply data augmentations to the training data using SE(3) transformations:\n[\u00b10.125 m, \u00b10.125 m, \u00b10.125m] in translations and \u00b145\u00b0 in the yaw axis. We use 2048 latents\nof dimension 512 in the Perceiver Transformer [63] and optimize the entire network using the\nLAMB [64] optimizer. We use \u03b1 = 0.3 for Open Jar and \u03b1 = 0.4 for the drawer tasks. We\nselect these \u03b1 values by using a starting state of the environment with the largest scaling size factor\nfor the object of interest and checking whether the object remains entirely contained in the voxel\ngrid after cropping. We train the policy with a batch size of 1 on a single Nvidia 3000 series GPU\nfor two days. For VLMs, we use OWL-ViT [65] as our open-vocabulary object detection algorithm\nand Segment Anything [66] as our foundational image segmentation model."}, {"title": "5 Experiments", "content": "5.1 Tasks\nIn simulation, we build on top of RLBench [14], a popular robot manipulation benchmark widely\nused in prior work, including VoxPoser and PerAct. We extend it to support bimanual manipulation\n(see Appendix A for details). We do not perform simulation-to-real transfer in this paper; simulation\nis for algorithm development and benchmarking. We design the following three bimanual tasks:\n\u2022 Open Jar: a jar with a screw-on lid is randomly spawned and scaled from 90% to 100% of the\noriginal size within the robot's 0.43 \u00d7 0.48 meters of workspace. The jar color is uniformly\nsampled from a set of 20 colors. The robot must first grasp the jar with one hand and use the other\nto unscrew the lid in an anti-clockwise direction until it is completely removed.\n\u2022 Open Drawer: a drawer is randomly spawned inside a workspace of 0.65 \u00d7 0.91 meters. It is\nrandomly scaled from 90% to 100% of its original size, and its rotation is randomized between\n$- \\pi$ and $\\pi$ radians. The robot needs to stabilize the top of the drawer with one hand and then open\nthe bottom drawer with the other.\n\u2022 Put Item in Drawer: a drawer (the same type from Open Drawer) is randomly spawned in a\nworkspace of 0.65 \u00d7 0.91 meters, and is randomly scaled and rotated using the same sampling\nranges from Open Drawer. The robot needs to open the top drawer with one hand, grasp the item\nplaced on top of the drawer with the other hand, and place it in the top drawer.\nSee Figure 1 for an illustration. In the real world, we test Open Jar and Open Drawer using a\ncoffee jar with dimensions 3.35 \u00d7 2.85 \u00d7 4.8 inches and a drawer of dimensions 12 \u00d7 12 \u00d7 12 inches."}, {"title": "5.2 Baselines and Ablations", "content": "In simulation, we compare against several strong baseline methods: Action Chunking with Trans-\nformers (ACT) [3], Diffusion Policy [15], and VoxPoser [16]. ACT is a state-of-the-art method\nfor bimanual manipulation. Diffusion Policy represents the policy as a conditional denoising diffu-\nsion process and excels at learning multimodal distributions. ACT and Diffusion Policy use joint\npositions for their action space instead of predicting end-effector poses as our method. We adapt\nthe Mobile ALOHA repository for ACT and a CNN-based Diffusion Policy, and we tune their pa-\nrameters (e.g., chunk size and action horizon) to improve performance. For VoxPoser, we write and\ntune their LLM prompts to work on our bimanual manipulation tasks using the VoxPoser repository.\nAdditionally, we include a Bimanual PerActs baseline, which trains separate PerAct policies for\nthe left and right arms, to show how a straightforward bimanual adaptation of a single-arm, state-\nof-the-art voxel-based method performs. It uses the same number of voxels, $100^3$, as the original\nPerAct. See the Appendix for further details. We also test the following ablations of VoxAct-B:"}, {"title": "5.3 Experiment Protocol and Evaluation", "content": "To generate demonstrations in simulation, we follow the convention from RLBench and define a\nsequence of waypoints to complete the task, and use motion planning to control the robot arms to\nreach waypoints. We generate 10 and 100 demonstrations of training data. Half of this data con-\nsists of left-acting and right-stabilizing demonstrations, and the other half contains right-acting and\nleft-stabilizing demonstrations. We generate 25 episodes of validation and test data using different\nrandom seeds. We train and evaluate all methods using three random seeds and report the average\nof the results. We evaluate all methods on the same set of test demonstrations for a fair comparison.\nEach method saves a checkpoint every 10,000 training steps. For all methods, we use the best-\nperforming checkpoint, evaluated on the validation data, to obtain the test success rate and report this\nresult. Deciding the best checkpoints for VoxAct-B and ablations is nontrivial since iterating over\nall possible combinations is computationally expensive. For example, with 400,000 training steps,\nusing the same 10,000 checkpoint interval means there are 40 \u00d7 40 = 1600 possible combinations.\nTherefore, with the validation data, we use the latest stabilizing checkpoint to evaluate all acting\ncheckpoints; we use the best acting checkpoint to evaluate all stabilizing checkpoints. Then, we use\nthe best-performing acting and stabilizing checkpoints to obtain the test success rate.\nIn the real world, we use a dual-arm CB2 UR5 robot setup. Each arm has 6-DOFs and has a\nRobotiq 2F-85 parallel-jaw gripper. We collect ten demonstrations for each task with the GELLO\nteleoperation interface [67]. We use a flat workspace with dimension 0.97 m by 0.79 m and mount\nan Intel RealSense D415 RGBD camera at a height of 0.42 m at a pose which reduces occlusions of\nthe object. For evaluation, we perform 10 consecutive rollouts to record the results for each task. In\nOpen Drawer, the arms have fixed roles of right acting and left stabilizing, and the acting arm opens\nthe top drawer. The drawer has variations of 10 cm in translations and 20\u00b0 of rotations. In Open\nJar, the roles of the arms are reversed and fixed, and the jar has variations of 12 cm in translations.\nSee Appendix B for more details."}, {"title": "6 Results", "content": "6.1 Simulation Results\nComparisons with baselines.\nMethod\nDiffusion Policy\nACT w/Transformers\nVoxPoser\nBimanual PerActs\nVoxAct-B (ours)\nOpen\nJar\n10\n5.3\n1.3\n8.0\n9.3\n38.6\nOpen\nDrawer\n10\n4.0\n10.7\n32.0\nPut Item\nin Drawer\n10\n6.7\n2.7\n4.0\n6.7\n40.0\n8\n4.7\n10\n1.9\n503\n3.8\nWhen\n0.185 m3\n0.3\n0.657.8\n24.0\n6.7\n2.33\n73.3\n7.06\n(100), 2020. URL https://api.semanticscholar.org/CorpusID:\nare identical.\n(ICML), 2022.\nlanguage in robotic affordances\nTable 1 reports the test success rates\nof baselines and VoxAct-B. When\nwe train all methods using ten\ndemonstrations, VoxAct-B outper-\nforms all baselines by a large mar-\ngin. In a low-data regime, the dis-\ncretized action space with spatial\nequivariance properties (as used in\nVoxAct-B and Bimanual PerActs)\nmay be more sample-efficient and\neasier for learning-based methods\ncompared to methods that use joint\nspace (ACT and Diffusion Policy).\nWhen we train all methods using\nmore demonstrations (100), VoxAct-B still outperforms baselines on most tasks, except ACT for"}, {"title": "6.2 Physical Results", "content": "Figure 4 shows real-world examples of VoxAct-B. In Open Drawer, success is when the stabilizing\narm holds the drawer from the top while the acting arm pulls the top part. VoxAct-B succeeds in\n6 out of 10 trials; the failures include robot joints hitting their limits, imprecision in grasping the\nhandle, and collisions with the drawer. In Open Jar, a success is when the stabilizing arm grasps\nthe jar while the acting arm unscrews the lid. VoxAct-B succeeds in 5 out of 10 trials. While the\nstabilizing arm performs well in grasping the jar (9 out of 10 successes), the acting arm struggles\nwith unscrewing the lid, succeeding only 5 out of 10 times due to imprecise grasping of the lid."}, {"title": "6.3 Limitations and Failure Cases", "content": "VoxAct-B implicitly assumes the object of interest does not encompass most of the workspace. If it\ndoes, it will be difficult to crop the voxel grid without losing relevant information. Another limitation\nis that VoxAct-B depends on the quality of VLMs. We have observed that some failures come from\npoor detection and segmentation from VLMs, which causes VoxAct-B to output undesirable ac-\ntions. In addition to common errors described in Section 6.1, for Put Item in Drawer, VoxAct-B\ntends to struggle more with executing acting actions (e.g., drawer-opening and cube-picking/placing\nactions) in contrast to stabilizing actions."}, {"title": "7 Conclusion", "content": "In this paper, we present VoxAct-B, a voxel-based, language-conditioned method for bimanual ma-\nnipulation. We use VLMs to focus on the most important regions in the scene and reconstruct a voxel\ngrid around them. This approach enables the policy to process the same number of voxels within\na reduced physical space, resulting in a higher voxel resolution necessary for accurate, fine-grained\nbimanual manipulation. VoxAct-B outperforms strong baselines, such as ACT, Diffusion Policy,\nand VoxPoser, by a large margin on difficult bimanual manipulation tasks. We also demonstrate\nVoxAct-B on real-world Open Drawer and Open Jar tasks using a dual-arm UR5 robot. We hope\nthat this inspires future work in asymmetric bimanual manipulation tasks."}, {"title": "A Simulation Benchmark for Bimanual Manipulation", "content": "We chose RLBench [14] as our choice of simulator since it is well-maintained by the research com-\nmunity and has been used in a number of prior works [13, 12, 68, 69, 70, 16], including PerAct [11],\nwhich is a core component of VoxAct-B."}, {"title": "A.1 Additional Simulation and Task Details", "content": "We extend RLBench to support bimanual manipulation by incorporating an additional Franka Panda\narm into the RLBench's training and evaluation pipelines. Importantly, we do not modify the under-\nlying APIs of CoppeliaSim (the backend of RLBench) to control the additional arm; consequently,\nthe robot arms cannot operate simultaneously, resulting in a delay in their control. However, this\nlimitation is acceptable as our tasks do not require real-time, dual-arm collaboration.\nMoreover, we modify Open Jar, Open Drawer, and Put Item in Drawer to support bimanual\nmanipulation: (1) adding an additional Franka Panda arm with a wrist camera; (2) adding new\nwaypoints for the additional arm; (3) adjusting the front camera's position to capture the entire\nworkspace; (4) removing the left shoulder, right shoulder, and overhead cameras. The new tasks\nuse a three-camera setup: front, left wrist, and right wrist. We also modify the data generation\npipeline to use motion planning with the new waypoints, process RGB-D images and the new arm's\nproprioception data (joint position, joint velocities, gripper open state, gripper pose), and include\nthe [x, y, z] position (world coordinates) of the object of interest.\nThe success conditions of these tasks have also been modified: for Open Jar, we define a proximity\nsensor in the jar bottle to detect whether an arm has a firm grasp of the jar (the gripper's opening\namount is between 0.5 and 0.93); for Open Drawer, we define a proximity sensor on the top of the\ndrawer to detect whether an arm is stabilizing the drawer. While a robot arm could still \"open\" the\ndrawer without the other arm's stabilization, we would not classify it a success in Open Drawer."}, {"title": "B Real-World Experimental Details", "content": "Hardware Setup. An overview of the hardware setup is described in Section 5.3. Our perception\nsystem utilizes the D415 camera to capture RGB and depth images at a resolution of 1280 \u00d7 720\npixels, where the depth images contain values in meters. We apply zero-padding to these images,\nresulting in a resolution of 1280 \u00d7 1280 pixels. Hand-eye calibration is performed to determine the\ntransformation matrices between the camera frame and the left robot base frame, as well as between\nthe camera frame and the right robot base frame, using the MoveIt Calibration package. We use the\npython-urx library to control the robot arms. Additionally, I/O programming is employed to control\nthe Robotiq grippers, as CB2 UR5 robots do not support URCaps.\nData Collection. We utilize the GELLO teleoperation framework to collect real-world demon-\nstrations. Due to the lack of Real-Time Data Exchange (RTDE) protocol support in CB2 UR5s,\na noticeable lag is present when operating the GELLO arms. For Open Jar, a dedicated function\ncontrols the gripper's counterclockwise rotations for unscrewing the lid and lifting it into the air, mit-\nigating the instability caused by latency. This function is triggered when the operator activates the\nGELLO arm's trigger. Additionally, we found that fixing the stabilizing arm while the acting arm is\nin motion is crucial for effective policy learning, as it eliminates noise introduced by unintentional,\nslight movements of the stabilizing arm. Observations are recorded at a frequency of 2 Hz.\nTraining and Execution. For training, we use a higher value for stopped_buffer_timesteps,\na hyper-parameter that determines how frequently keyframes are extracted from the continuous ac-\ntions based on how long the joint velocities have been near 0 and the gripper state has not been\nchanged, in PerAct's keyframe extraction function to account for the slower movements of the robot\narms due to latency compared to simulation. We apply the inverse of the transformation matri-\nces obtained from hand-eye calibration to project each arm's gripper position to the camera frame.\nUsing the camera's intrinsics and an identity extrinsic matrix, we construct the point cloud in the"}, {"title": "C Additional Implementation Details", "content": "C.1 Baselines\nWe carefully tune the baselines and include the hyperparameters used in Table 3. We report the\nresults of the best-tuned baselines in Table 1. For our three tasks, we found that for ACT, a chunk size\nof 100 worked well, consistent with the findings reported in [3]. The temporal aggregation technique\ndid not improve performance in our tasks, so we disabled this feature. For Diffusion Policy, lower\nvalues (e.g., 16) of the action prediction horizon were inadequate, leading to agents getting stuck\nat certain poses and failing to complete the tasks, so we used an action prediction horizon of 100.\nWe found the Time-series Diffusion Transformer to outperform the CNN-based Diffusion Policy on\nOpen Drawer and Open Jar, while both of them achieved comparable success rates on Put Item\nin Drawer. We use a batch size of 32 for both methods, and the observation resolution is 128 \u00d7 128\n(same as VoxAct-B). For Diffusion Policy, we use the same image augmentation techniques as in [3].\nAs shown in Table 4, the performance of ACT and Diffusion Policy progressively improves as more\nenvironment variations are removed. For VoxPoser, we modified the LLM prompts to work with\nour bimanual manipulation tasks. See our VoxPoser prompts for details. For Bimanual PerActs, we\ndeliberately chose to use 1003 voxels instead of the 503 voxels used in VoxAct-B. The increased\nnumber of voxels provides higher voxel resolution, which is essential for fine-grained bimanual\nmanipulation. This is demonstrated in the VoxAct-B w/o VLM ablation, which only utilizes 503\nvoxels and shows a huge drop in performance compared to VoxAct-B."}]}