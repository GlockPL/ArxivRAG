[{"title": "MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents", "authors": ["Ruochen Li", "Teerth Patel", "Qingyun Wang", "Xinya Du"], "abstract": "Machine learning research, crucial for technological advancements and innovation, often faces significant challenges due to its inherent complexity, slow pace of experimentation, and the necessity for specialized expertise.\nMotivated by this, we present a new systematic framework, autonomous Machine Learning Research with large language models (MLR-Copilot), designed to enhance machine learning research productivity through the automatic generation and implementation of research ideas using Large Language Model (LLM) agents. The framework consists of three phases: research idea generation, experiment implementation, and implementation execution. First, existing research papers are used to generate hypotheses and experimental plans via IdeaAgent powered by LLMs. Next, the implementation generation phase translates these plans into executables with ExperimentAgent. This phase leverages retrieved prototype code and optionally retrieves candidate models and data. Finally, the execution phase, also managed by ExperimentAgent, involves running experiments with mechanisms for human feedback and iterative debugging to enhance the likelihood of achieving executable research outcomes. We evaluate our framework on five machine learning research tasks and the experimental results show the framework's potential to facilitate the research progress and innovations.", "sections": [{"title": "1 Introduction", "content": "The increasing complexity of scientific research and the rapid expansion of scientific knowledge necessitates innovative approaches to facilitate and accelerate the research process (Choudhury, 2021). Traditional research methodologies often involve labor-intensive tasks such as literature review, hypothesis formulation, experimental design, implementation, and execution to obtain the results (Powell, 2015). These tasks can be time-consuming and prone to human error, potentially hindering scientific progress (Bornmann et al., 2010). These highlight the advantages of incorporating AI technologies to boost the efficiency and productivity of scientific research.\nLarge Language Models (LLMs) have shown impressive capabilities in generating text and code (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023), outperforming human experts across scientific and engineering domains, including computer science (Wang et al., 2024; Baek et al., 2024), biomedical (AI4Science and Quantum, 2023), social science (Yang et al., 2023), etc. Moreover, autonomous agents based on LLMs have shown potential in solving complex tasks such as web interactions (Zhou et al., 2023) and simulating interactions between humans (Park et al., 2023). Based on this progress, LLMs have huge potential to advance and accelerate the scientific discovery process including autonomous research in the machine learning discipline. They would act as a \"copilot\" (Dakhel et al., 2023; GitHub, Inc.) for researchers (Figure 1), specifically, given the research paper, LLM-agent analyzes and extracts research problems and propose novel research ideas consisting of hypothesis (e.g., new models) and experimental plan, then implement experiments and execute the implementations to obtain results. In this work, we focus on all three phases of this research task, namely, research idea generation, experiment implementation, and implementation execution. Our goal is to build an LLM-based framework, which takes as input the paper, outputs research ideas, and conducts experiments that verify/validate the hypothesis.\nRecently, there have been few works in the domain of LLM for scientific discovery, they focus on various scenarios/parts and largely differ from ours. Yang et al. (2023); Wang et al. (2024); Qi et al. (2023a); Baek et al. (2024) only investigate generating natural language research hypothesis based on general scientific literature, which is similar to stage 1 in our work. Furthermore, they are not specifically tailored for the Machine Learning Research domain (MLR); for example, they work in the open-ended setting without explicit identification of the research problem/task definition, which arguably loses focus and is too broad for a certain machine learning topic. Similarly, they do not explicitly take into account the limitations of current/prior work of the methods for the specific problem.\nOn the other hand, Huang et al. (2023); Zhang et al. (2023) target automatically conducting experiments for machine learning tasks, which can potentially accelerate the hypothesis validation processes (Stage 2 and 3). However, their settings are much more constricted \u2013 they start with a predefined task and mature code template, instead of research literature. Moreover, they typically apply small coding editing, such as trying hyperparameters, without trying novel approaches such as models and data. Furthermore, there is no guarantee that their experimentation process will converge/stop since the framework when faced with issues, the framework has no feedback on whether it's because of the idea or the bugs in the implementation.\nDifferent from all the above, we aim at tackling the entire process of machine learning research across different stages. In response to prior works limitations and these challenges, we present MLR-Copilot (Figure 2), a systematic framework designed to enhance machine learning research productivity through the automatic generation and implementation/verification of research ideas using LLM agents. MLR-Copilot operates in three integrated phases: research idea generation, experiment implementation, and implementation execution. In this first stage, we construct an input prompt consisting of relevant research papers and extracted research problems (including task definition); then IdeaAgent (Baek et al., 2024), an LLM-powered agent, takes in the prompt and generates research hypothesis and experimental plans. This ensures that the proposed research directions are well-grounded in existing literature and address current gaps (Zhang and Teng, 2023; Cohan and Goharian, 2018; Baek et al., 2024). In the second stage, the framework translates these experimental plans into executable experiments. It is facilitated by ExperimentAgent (Smith et al., 2023), which incorporates the utility of model and data retrieval, and leverages retrieved prototype code (from relevant papers) to generate the necessary implementations (Hocky and White, 2022; Viswanathan et al., 2023). Later, ExperimentAgent leverages feedback from the execution results from Stage 3. Finally, the implementation execution phase, also managed by ExperimentAgent, involves running the experiments and generating execution/debugging feedback, as well as optional human feedback. The feedback allows for the refinement of the experiment implementations (Stage 2). The implementation and execution process is iterative, and the human-in-the-loop feature ensures that the final research outcomes are robust, reproducible, and scientifically sound (Viswanathan et al., 2023).\nThis paper details the architecture and functionalities of our automated research framework. We conduct manual and automatic evaluations on generated hypotheses and experimental executions/results. We also present case studies demonstrating the practical applications of our system on five machine learning research papers/problems. Through evaluations and examples, we illustrate that our framework can generate novel and feasible hypotheses for research, enabling researchers to focus on high-level scientific inquiry and innovation. We also show that MLR-Copilot is able to help finish the full research process and obtain significant results/improvements and conclusions."}, {"title": "2 MLR-Copilot Framework", "content": "MLR-Copilot automates the generation and implementation of research ideas using LLM agents, organized into three integrated phases: research idea generation, experiment implementation, and implementation execution."}, {"title": "2.1 Research Idea Generation", "content": "In the first stage, IdeaAgent, an LLM-powered agent, generates research hypotheses and experimental plans. For each task, the process begins with an individual research paper $c = {c_1, c_2, ..., c_n}$, where $c_i$ represents the selected contents of the paper with Semantic Scholar API\u00b2, including the title, abstract, introduction, and related work.\nThe input processing involves analyzing the literature to extract essential information. Specifically, the initial input prompt is used to extract research tasks $t$, research gaps $g$, and keywords $k = {k_1,k_2,...,k_m}$ with LLM. Then $P = {c,t,g,k}$ are provided to retrieve a set of recent works in the literature, denoted as $R = {r_1,r_2,...,r_l}$.\nIdeaAgent extracts and synthesizes relevant information from the literature (Baek et al., 2024). Using updated information, the LLM generates new hypotheses with prompt detailed as $P_1 = {P,R} \\rightarrow h$ based on identified trends and gaps in the existing research, ensuring both relevance and grounding in current studies."}, {"title": "2.2 Experiment Implementation", "content": "The second phase involves translating experimental plans into executable experiments. This phase is facilitated by ExperimentAgent, an LLM-based agent. Given research idea RI that contains experiment plan $e$, ExperimentAgent performs several critical actions:\nFirst, it retrieves prototype implementation $I$ from the original paper. Leveraging existing $I$, ExperimentAgent adapts and integrates this code, and optionally retrieves suitable models $M_\\nabla$ from a model repository $M = {M_1, M_2, ..., M_p}$ to fit the specific needs of the experimental plan. The selection process is guided by the requirements of the experimental plan $e_j$, ensuring that the chosen models are appropriate for the specified tasks. If needed, relevant datasets $D \\in {D_1, D_2, . . ., D_q}$ are identified and retrieved. We ensure that these datasets align with the experimental requirements by post-checkup, facilitating accurate and comprehensive testing of the hypotheses (Hocky and White, 2022). The ExperimentAgent modifies the code to ensure compatibility with the selected models and datasets (Viswanathan et al., 2023). Finally, the retrieved models, datasets, and prototype code are integrated into a cohesive experimental setup with experimental implementation $(I, M_\\nabla,D) \\rightarrow S$, ExperimentAgent ensures seamless interaction between these components, preparing the experimental setup for execution."}, {"title": "2.3 Implemetation Execution", "content": "In the final phase, ExperimentAgent manages the execution of the experiments. The execution phase encompasses running the experiments, incorporating mechanisms for human feedback, and supporting iterative debugging.\nThe experimental setups $(I, M_\\nabla, D) \\rightarrow S$ are executed under the management of ExperimentAgent. The agent oversees the allocation of computational resources, monitoring the progress and performance of the experiments. Additionally, ExperimentAgent integrates mechanisms for human feedback, allowing researchers to provide input and adjustments during the execution phase. This feedback loop ensures that the experimental design and implementation can be refined in real-time.\nFrom the global point of view, ExperimentAgent provides feedback and enables researchers (or stage 1) to refine their hypotheses and experimental designs based on intermediate and final execution results (e.g. feasibility). This iterative approach ensures that the final research outcomes are robust, reproducible, and scientifically sound."}, {"title": "3 Experiments", "content": "To evaluate the effectiveness of MLR-Copilot, we conduct experiments across five machine learning research task papers. These tasks of the papers were chosen to cover a range of domains and complexities, demonstrating the versatility and robustness of our framework."}, {"title": "3.1 Experimental Setup and Datasets", "content": "SemRel (Ousidhoum et al., 2024) from SemEval 2024 Task 1 focuses on semantic textual relatedness across 13 languages and is popular for its diversity and real-world relevance. We use the supervised track for our experiments and adopt Pearson correlation as the metrics.\nMLAgentBenchmark (Huang et al., 2023) includes several datasets for evaluating LLMs in automated research idea generation and implementation. We use the following datasets: feedback (ELLIPSE) (Franklin et al., 2022; Doe and Smith, 2023) used for machine learning-based feedback prediction, suitable for regression tasks like MCRMSE. IMDB (Maas et al., 2011) consists of movie reviews labeled by sentiment, commonly used for sentiment analysis and NLP tasks. Spaceship-Titanic dataset predicts passenger survival based on features like passenger class, age, and ticket fare. Identify-Contrails involves identifying contrails in satellite images, suitable for image classification tasks. Classification accuracy is used as the metric for these tasks."}, {"title": "3.2 Evaluation and Results", "content": "We evaluate different stages of our framework, i.e. the hypothesis generation stage (Section 3.2.1), the experiment implementation and implementation execution stages (Section 3.2.2) separately."}, {"title": "3.2.1 Evaluating Research Idea Generation", "content": "Following the setting of (Baek et al., 2024), we conduct both manual evaluations and automated evaluations. For baselines, we compare to an LLM in (Baek et al., 2024) which prompts with only a core paper to generate research ideas.\nFor manual evaluation, we invite three domain expert reviewers to assess the generated hypotheses based on criteria adapted from the (Baek et al., 2024): clarity, validity, rigor, innovativeness, and generalizability. Additionally, the experimental designs are evaluated for clarity, validity, robustness, feasibility, and reproducibility. Each criterion is scored on a 5-point Likert scale(refer to (Baek et al., 2024) for detailed definitions), with human researchers who have published at least three papers providing the annotations.\nFor automated evaluation, we employ an LLM reviewing agent to assess the clarity and validity of the hypotheses and the robustness and feasibility of the experimental designs, scoring each criterion on a 5-point Likert scale. Similarity analysis is performed to compare the new hypotheses with the original hypotheses from existing papers on a scale from 0 to 1."}, {"title": "3.2.2 Evaluating Experiment Implementation and Implementation Execution", "content": "We assess experiment implementation and execution by measuring average task performance improvement and success rate over 8 trials with human instructions comparing to the prototype code."}, {"title": "4 Analysis: Case Study for Sentiment Analysis Research", "content": "To demonstrate MLR-Copilot's practical application, we conducted a case study where researchers used the system to generate hypotheses and conduct sentiment analysis experiments on the ELLIPSE dataset. As shown in Figure 3, the process involves interaction between the ExperimentAgent, Action Executor, and various Utility Modules.\nThe action sequences illustrate how the MLR-Copilot system helps researchers systematically generate hypotheses and conduct experiments. The system inspects scripts, executes models, retrieves models, and analyzes results. Details are provided in Appendix A (IdeaAgent) and B (ExperimentAgent).\nThis comprehensive action log highlights the MLR-Copilot's systematic approach, allowing researchers to understand, modify, and execute scripts for sentiment analysis. Each action, driven by reasoning, objectives, observations, and feedback, refines the model and experimental design, leading to successful evaluation."}, {"title": "5 Related Work", "content": ""}, {"title": "5.1 LLM as Scientific Agents.", "content": "The automation of idea generation in scientific research received great interest, particularly with the advent of LLMs. Previous studies have explored the potential of LLMs to assist in generating hypotheses and research questions based on literature-based discovery (Swanson, 1986). For instance, LLMs have been leveraged to provide initial drafts of research questions and even entire research proposals (Brown et al., 2020; Zhong et al., 2023; Qi et al., 2023b; Yang et al., 2023; Wang et al., 2024). However, these efforts primarily focus on the hypotheses generation phase but not on implementing and validating them. On the contrary, our work focuses on more realistic settings, investigating building LLM agents that tackle the entire process and how each stage can benefit and provide feedback for other stages."}, {"title": "5.2 Model and Data Retrieval Systems.", "content": "Efficient models and data retrieval are critical components of modern AI systems. Hugging Face's Datasets and Model Hub provide researchers with vast repositories of datasets and pre-trained models (Lhoest et al., 2021; Wolf et al., 2020). These systems enable users to find relevant data and models quickly through natural language prompts, facilitating seamless integration into the research workflow. Our framework incorporates the model and data retrieval utilities, which play a crucial role in the experiment implementation process based on natural language prompts (Viswanathan et al., 2023). This allows for translating research questions and problem statements into specific model requirements, facilitating the automated retrieval of the most relevant models for hypothesis testing and validation."}, {"title": "6 Conclusion", "content": "We propose MLR-Copilot, a framework for automating machine learning research using LLM agents. It helps generate novel research ideas, implements & executes the experiments, and refines the implementations based on both automatic and human feedback. Evaluations from domain experts highlight it as a powerful tool for research idea generation and the experimentation process."}, {"title": "A IdeaAgent Example: Sentiment Analysis Paper", "content": ""}, {"title": "A.1 Hypothesis Generation Prompt:", "content": "You are an AI assistant whose primary goal is to propose innovative,\nrigorous, and valid methodologies to solve newly identified\nscientific problems derived from existing scientific literature,\nin order to empower researchers to pioneer groundbreaking\nsolutions that catalyze breakthroughs in their fields.\nYou are going to propose a scientific method to address a specific\nresearch problem. Your method should be clear, innovative,\nrigorous, valid, and generalizable. This will be based on a deep\nunderstanding of the research problem, its rationale, existing\nstudies, and various entities. Understanding of the research\nproblem, existing studies, and entities is essential:\n- The research problem has been formulated based on an in-depth\nreview of existing studies\nand a potential exploration of relevant entities, which should be the\ncornerstone of your method\ndevelopment.\n- The existing studies refer to the target paper that has been\npivotal in identifying the problem, as\nwell as the related papers that have been additionally referenced in\nthe problem discovery phase,\nall serving as foundational material for developing the method.\n- The entities can include topics, keywords, individuals, events, or\nany subjects with possible\ndirect or indirect connections to the existing studies, serving as\nauxiliary sources of inspiration or\ninformation that may be instrumental in method development.\nYour approach should be systematic:\n- Start by thoroughly reading the research problem and its rationale,\nto understand your primary\nfocus.\n- Next, proceed to review the titles and abstracts of existing\nstudies, to gain a broader perspective\nand insights relevant to the primary research topic.\n- Finally, explore the entities to further broaden your perspective,\ndrawing upon a diverse pool of\ninspiration and information, while keeping in mind that not all may\nbe relevant.\nI am going to provide the research problem, existing studies (target\npaper & related papers), and\nentities, as follows:\nTitle\nDataset and Baseline for Automatic Student Feedback Analysis\nAbstract\nThis paper presents a student feedback corpus containing 3000\ninstances of feedback written by university students. The dataset\nhas been annotated for aspect terms, opinion terms, polarities of\nthe opinion terms towards targeted aspects, document-level opinion\npolarities, and sentence separations. A hierarchical taxonomy for\naspect categorization covering all areas of the teaching-learning\nprocess was developed. Both implicit and explicit aspects were\nannotated using this taxonomy. The paper discusses the annotation\nmethodology, difficulties faced during the annotation, and details\nabout aspect term categorization. The annotated corpus can be\nused for Aspect Extraction, Aspect Level Sentiment Analysis, and\nDocument Level Sentiment Analysis. Baseline results for all three\ntasks are provided.\nIntroduction\nThe paper introduces the need for a comprehensive dataset for\nautomatic analysis of student feedback to improve the teaching-\nlearning process. Previous datasets were limited in scope and\nlacked comprehensive annotations necessary for detailed analysis.\nThe authors aimed to fill this gap by creating a dataset that\nincludes detailed annotations for various aspects of student\nfeedback. The introduction outlines the importance of aspect-level\nsentiment analysis and the potential applications of the dataset\nin educational research.\nRelated Work\nThe related work section reviews existing datasets and methodologies\nfor sentiment analysis and feedback categorization in educational\ncontexts. It highlights the limitations of previous works, such as\nthe lack of detailed aspect-level annotations and the focus on\ndocument-level sentiment analysis. The authors compare their work\nwith existing datasets and emphasize the novelty of their approach\nin providing a more granular level of annotation.\nResearch Tasks (t)\nThe primary research tasks undertaken in this study include the\ncreation of a comprehensive student feedback corpus, consisting of\n3000 instances of feedback written by university students. The\nfeedback data is meticulously annotated for aspect terms, opinion\nterms, and the polarities of these opinion terms towards targeted\naspects. Additionally, the study involves the development of a\nhierarchical taxonomy for aspect categorization, covering all\nareas of the teaching-learning process. Baseline results for\nAspect Extraction, Aspect Level Sentiment Analysis, and Document\nLevel Sentiment Analysis are also provided, using the annotated\ncorpus as a benchmark.\nResearch Gaps (g)\nThis research addresses several critical gaps in the field. One\nsignificant gap is the lack of detailed aspect-level annotations\nin existing datasets, which limits the granularity required for in\n-depth sentiment analysis. Previous works have primarily focused\non document-level sentiment analysis, neglecting the need for a\nmore nuanced understanding of specific feedback aspects. Another\ngap is the absence of a comprehensive feedback dataset that\nincludes both implicit and explicit aspects, annotated with a\ndetailed methodology. The study also tackles the challenges faced\nduring the annotation process, ensuring accuracy and consistency\nin the data.\nKeywords (k)\nStudent Feedback Corpus, Aspect Terms, Opinion Terms,Polarity\nHierarchical Taxonomy, Aspect Extraction, Aspect Level Sentiment\nAnalysis, Document Level Sentiment Analysis\nRecent works(R)\nTitle: \"Students feedback analysis model using deep learning-based\nmethod and linguistic knowledge for intelligent educational\nsystems\"\n* Abstract: This study explores a new deep learning-based method for\ndesigning an automated system to analyze student feedback more\naccurately, termed DTLP (Deep Learning and Teaching Process). DTLP\nintegrates convolutional neural networks (CNNs), bidirectional\nLSTM (BiLSTM), and attention mechanisms to address various\nchallenges in sentiment analysis, such as semantic context, word\nsense variations, and contextual polarity. The method combines\nstatistical, linguistic, and sentiment knowledge features to\nenhance the accuracy of sentiment classification and provide\ncomprehensive feedback analysis.\nTitle: \"An Automated Approach for Analysing Students Feedback Using\nSentiment Analysis Techniques\"\n* Abstract: This paper discusses a machine learning approach to\nclassify the sentiment of student feedback on MOOCs. It uses a\ncombination of machine learning models and sentiment analysis\ntechniques to evaluate the feedback's polarity and provide\ninsights into students' learning experiences. The approach aims to\nsupport educational institutions in improving teaching quality by\nanalyzing feedback data collected over several semesters.\nWith the provided research problem, existing studies, and entities,\nyour objective now is to formulate a method that not only\nleverages these resources but also strives to be clear, innovative\nrigorous, valid, and generalizable. Before crafting the method,\nrevisit the research problem, to ensure it remains the focal point\nof your method development process.\nResearch problem: {researchProblem}\nRationale: {researchProblemRationale}\nThen, following your review of the above content, please proceed to\npropose your method with its rationale, in the format of\nMethod:\nRationale:"}, {"title": "A.2 Experiment Generation Prompt:", "content": "You are an AI assistant whose primary goal is to design robust,\nfeasible, and impactful experiments based on identified scientific\nproblems and proposed methodologies from existing scientific\nliterature, in order to enable researchers to systematically test\nhypotheses and validate groundbreaking discoveries that can\ntransform their respective fields.\nUser Message\nYou are going to design an experiment, aimed at validating a proposed\nmethod to address a\nspecific research problem. Your experiment design should be clear,\nrobust, reproducible, valid,\nand feasible. This will be based on a deep understanding of the\nresearch problem, scientific\nmethod, existing studies, and various entities.\nUnderstanding of the research problem, scientific method, existing\nstudies, and entities is essential:\n- The research problem has been formulated based on an in-depth\nreview of existing studies and a\npotential exploration of relevant entities.\n- The scientific method has been proposed to tackle the research\nproblem, which has been informed by insights gained from existing\nstudies and relevant entities.\n- The existing studies refer to the target paper that has been\npivotal in identifying the problem and method, as well as the\nrelated papers that have been additionally referenced in the\ndiscovery phase of the problem and method, all serving as\nfoundational material for designing the experiment.\n- The entities can include topics, keywords, individuals, events, or\nany subjects with possible direct or indirect connections to the\nexisting studies, serving as auxiliary sources of inspiration or\ninformation that may be instrumental in your experiment design.\nYour approach should be systematic:\n- Start by thoroughly reading the research problem and its rationale\nfollowed by the proposed method and its rationale, to pinpoint\nyour primary focus.\n- Next, proceed to review the titles and abstracts of existing\nstudies, to gain a broader perspective and insights relevant to\nthe primary research topic.\n- Finally, explore the entities to further broaden your perspective,\ndrawing upon a diverse pool of inspiration and information, while\nkeeping in mind that not all may be relevant. With the provided\nresearch problem, scientific method, existing studies, and\nentities, your objective now is to design an experiment that not\nonly leverages these resources but also strives to be clear,\nrobust, reproducible, valid, and feasible. Before crafting the\nexperiment design, revisit the research problem and proposed\nmethod, to ensure they remain at the center of your experiment\ndesign process."}, {"title": "A.3 Generated research idea", "content": "Research Problem:\n\"\"\"\nTitle:\nDataset and Baseline for Automatic Student Feedback Analysis\nAbstract:\nThis paper presents a student feedback corpus containing 3000\ninstances of feedback written by university students. The dataset\nhas been annotated for aspect terms, opinion terms, polarities of\nthe opinion terms towards targeted aspects, document-level opinion\npolarities, and sentence separations. A hierarchical taxonomy for\naspect categorization covering all areas of the teaching-learning\nprocess was developed. Both implicit and explicit aspects were\nannotated using this taxonomy. The paper discusses the annotation\nmethodology, difficulties faced during the annotation, and details\nabout aspect term categorization. The annotated corpus can be\nused for Aspect Extraction, Aspect Level Sentiment Analysis, and\nDocument Level Sentiment Analysis. Baseline results for all three\ntasks are provided.\nIntroduction:\nThe paper introduces the need for a comprehensive dataset for\nautomatic analysis of student feedback to improve the teaching-\nlearning process. Previous datasets were limited in scope and\nlacked comprehensive annotations necessary for detailed analysis.\nThe authors aimed to fill this gap by creating a dataset that\nincludes detailed annotations for various aspects of student\nfeedback. The introduction outlines the importance of aspect-level\nsentiment analysis and the potential applications of the dataset\nin educational research.\nRelated Work:\nThe related work section reviews existing datasets and methodologies\nfor sentiment analysis and feedback categorization in educational\ncontexts. It highlights the limitations of previous works, such as\nthe lack of detailed aspect-level annotations and the focus on\ndocument-level sentiment analysis. The authors compare their work\nwith existing datasets and emphasize the novelty of their approach\nin providing a more granular level of annotation.\nResearch Tasks (t):\nThe primary research tasks undertaken in this study include the\ncreation of a comprehensive student feedback corpus, consisting of\n3000 instances of feedback written by university students. The\nfeedback data is meticulously annotated for aspect terms, opinion\nterms, and the polarities of these opinion terms towards targeted\naspects. Additionally, the study involves the development of a\nhierarchical taxonomy for aspect categorization, covering all\nareas of the teaching-learning process. Baseline results for\nAspect Extraction, Aspect Level Sentiment Analysis, and Document\nLevel Sentiment Analysis are also provided, using the annotated\ncorpus as a benchmark.\nResearch Gaps (g):\nThis research addresses several critical gaps in the field. One\nsignificant gap is the lack of detailed aspect-level annotations\nin existing datasets, which limits the granularity required for in\n-depth sentiment analysis. Previous works have primarily focused\non document-level sentiment analysis, neglecting the need for a\nmore nuanced understanding of specific feedback aspects. Another\ngap is the absence of a comprehensive feedback dataset that\nincludes both implicit and explicit aspects, annotated with a\ndetailed methodology. The study also tackles the challenges faced\nduring the annotation process, ensuring accuracy and consistency\nin the data.\nKeywords (k):\nStudent Feedback Corpus, Aspect Terms, Opinion Terms, Polarity\nHierarchical Taxonomy, Aspect Extraction, Aspect Level Sentiment\nAnalysis, Document Level Sentiment Analysis\nRecent works(R):\nTitle: \"Students feedback analysis model using deep learning-based\nmethod and linguistic knowledge for intelligent educational\nsystems\"\nAbstract: This study explores a new deep learning-based method for\ndesigning an automated system to analyze student feedback more\naccurately, termed DTLP (Deep Learning and Teaching Process). DTLP\nintegrates convolutional neural networks (CNNs), bidirectional\nLSTM (BiLSTM), and attention mechanisms to address various\nchallenges in sentiment analysis, such as semantic context, word\nsense variations, and contextual polarity. The method combines\nstatistical, linguistic, and sentiment knowledge features to\nenhance the accuracy of sentiment classification and provide\ncomprehensive feedback analysis.\nTitle: \"An Automated Approach for Analysing Students Feedback Using\nSentiment Analysis Techniques\"\nAbstract: This paper discusses a machine learning approach to"}, {"title": "B ExperimentAgent Example: Sentiment Analysis Research Paper", "content": "You are a helpful research assistant. You have access to the\nfollowing tools:\n- List Files:\nUse this to navigate the file system.\nUsage:\nAction: List Files\nAction Input: {\n\"dir_path\": [a valid relative path to a directory", ".\" or \"folder1/folder2\"": "n"}, "nObservation: [The observation will be a list of files and\nfolders in dir_path or current directory is dir_path is\nempty, or an error message if dir_path is invalid."], "File": "nUse this to copy a file to a new location with a new name.\nUsage:\nAction: Copy File\nAction Input: {\n\"source\": [a valid file name with relative path to\ncurrent directory if needed]", "n\"destination\"": ["a valid file name with relative path to\ncurrent directory if needed"], "n}\nObservation": ["A success message if the file is copied\nsuccessfully, or an error message if the file cannot be\ncopied."], "Script": "nUse this to undo the last edit of the python script.\nUsage:\nAction: Undo Edit Script\nAction Input: {\n\"script_name\": [a valid python script name with relative\npath to current directory if needed]"}, ["The observation will be the content of the\nscript before the last edit. If the script does not exist,\nthe observation will be an error message."], {"n\"script_name\"": ["a valid python script name with relative\npath to current directory if needed"], "n}\nObservation": ["The observation will be output of the script or\nerrors."], "Help": "nUse this to request help from human. Use this only when the\nprovided tools and files are not enough for accomplishing\nnecessary steps", "first.\nUsage": "nAction: Request Help\nAction Input: {\n\"request\": [a detailed description on what to do]"}, ["The observation will be the response from human\n."], {"n\"final_answer\"": ["a detailed description on the final\nanswer"], "n}\nObservation": "The observation will be the content of the\nscript between start_line_number and end_line_number", "File": "nUse this to read the whole file and understand certain\naspects. You should provide detailed description on what\nto look for and what should be returned. To get a better\nunderstanding of the file", "file.\nUsage": "nAction: Understand File\nAction Input: {\n\"file_name\": [a valid file name with relative path to\ncurrent directory if needed]", "n\"things_to_look_for\"": ["a detailed description on what to\nlook for and what should returned"], "Lines": "nUse this to inspect specific part of a python script\nprecisely", "debugging.\nUsage": "nAction: Inspect Script Lines\nAction Input: {\n\"script_name\": [a valid python script name with relative\npath to current directory if needed]", "n\"start_line_number\"": ["a valid line number"], "n\"end_line_number\"": ["a valid line number"]}]