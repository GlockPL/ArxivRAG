{"title": "MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents", "authors": ["Ruochen Li", "Teerth Patel", "Qingyun Wang", "Xinya Du"], "abstract": "Machine learning research, crucial for technological advancements and innovation, often faces significant challenges due to its inherent complexity, slow pace of experimentation, and the necessity for specialized expertise.\nMotivated by this, we present a new systematic framework, autonomous Machine Learning Research with large language models (MLR-Copilot), designed to enhance machine learning research productivity through the automatic generation and implementation of research ideas using Large Language Model (LLM) agents. The framework consists of three phases: research idea generation, experiment implementation, and implementation execution. First, existing research papers are used to generate hypotheses and experimental plans via IdeaAgent powered by LLMs. Next, the implementation generation phase translates these plans into executables with ExperimentAgent. This phase leverages retrieved prototype code and optionally retrieves candidate models and data. Finally, the execution phase, also managed by ExperimentAgent, involves running experiments with mechanisms for human feedback and iterative debugging to enhance the likelihood of achieving executable research outcomes. We evaluate our framework on five machine learning research tasks and the experimental results show the framework's potential to facilitate the research progress and innovations.", "sections": [{"title": "1 Introduction", "content": "The increasing complexity of scientific research and the rapid expansion of scientific knowledge necessitates innovative approaches to facilitate and accelerate the research process (Choudhury, 2021). Traditional research methodologies often involve labor-intensive tasks such as literature review, hypothesis formulation, experimental design, implementation, and execution to obtain the results (Powell, 2015). These tasks can be time-consuming and prone to human error, potentially hindering scientific progress (Bornmann et al., 2010). These highlight the advantages of incorporating AI technologies to boost the efficiency and productivity of scientific research.\nLarge Language Models (LLMs) have shown impressive capabilities in generating text and code (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023), outperforming human experts across scientific and engineering domains, including computer science (Wang et al., 2024; Baek et al., 2024), biomedical (AI4Science and Quantum, 2023), social science (Yang et al., 2023), etc. Moreover, autonomous agents based on LLMs have shown potential in solving complex tasks such as web interactions (Zhou et al., 2023) and simulating interactions between humans (Park et al., 2023). Based on this progress, LLMs have huge potential to advance and accelerate the scientific discovery process including autonomous research in the machine learning discipline. They would act as a \"copilot\" (Dakhel et al., 2023; GitHub, Inc.) for researchers (Figure 1), specifically, given the research paper, LLM-agent analyzes and extracts research problems and propose novel research ideas consisting of hypothesis (e.g., new models) and experimental plan, then implement experiments and execute the implementations to obtain results. In this work, we focus on all three phases of this research task, namely, research idea generation, experiment implementation, and implementation execution. Our goal is to build an LLM-based framework, which takes as input the paper, outputs research ideas, and conducts experiments that verify/validate the hypothesis.\nRecently, there have been few works in the domain of LLM for scientific discovery, they focus on various scenarios/parts and largely differ from ours. Yang et al. (2023); Wang et al. (2024); Qi et al. (2023a); Baek et al. (2024) only investigate generating natural language research hypothesis based on general scientific literature, which is similar to stage 1 in our work. Furthermore, they are not specifically tailored for the Machine Learning Research domain (MLR); for example, they work in the open-ended setting without explicit identification of the research problem/task definition, which arguably loses focus and is too broad for a certain machine learning topic. Similarly, they do not explicitly take into account the limitations of current/prior work of the methods for the specific problem.\nOn the other hand, Huang et al. (2023); Zhang et al. (2023) target automatically conducting experiments for machine learning tasks, which can potentially accelerate the hypothesis validation processes (Stage 2 and 3). However, their settings are much more constricted \u2013 they start with a predefined task and mature code template, instead of research literature. Moreover, they typically apply small coding editing, such as trying hyperparameters, without trying novel approaches such as models and data. Furthermore, there is no guarantee that their experimentation process will converge/stop since the framework when faced with issues, the framework has no feedback on whether it's because of the idea or the bugs in the implementation.\nDifferent from all the above, we aim at tackling the entire process of machine learning research across different stages. In response to prior works limitations and these challenges, we present MLR-Copilot (Figure 2), a systematic framework designed to enhance machine learning research productivity through the automatic generation and implementation/verification of research ideas using LLM agents. MLR-Copilot operates in three integrated phases: research idea generation, experiment implementation, and implementation execution. In this first stage, we construct an input prompt consisting of relevant research papers and extracted research problems (including task definition); then IdeaAgent (Baek et al., 2024), an LLM-powered agent, takes in the prompt and generates research hypothesis and experimental plans. This ensures that the proposed research directions are well-grounded in existing literature and address current gaps (Zhang and Teng, 2023; Cohan and Goharian, 2018; Baek et al., 2024). In the second stage, the framework translates these experimental plans into executable experiments. It is facilitated by ExperimentAgent (Smith et al., 2023), which incorporates the utility of model and data retrieval, and leverages retrieved prototype code (from relevant papers) to generate the necessary implementations (Hocky and White, 2022; Viswanathan et al., 2023). Later, ExperimentAgent leverages feedback from the execution results from Stage 3. Finally, the implementation execution phase, also managed by ExperimentAgent, involves running the experiments and generating execution/debugging feedback, as well as optional human feedback. The feedback allows for the refinement of the experiment implementations (Stage 2). The implementation and execution process is iterative, and the human-in-the-loop feature ensures that the final research outcomes are robust, reproducible, and scientifically sound (Viswanathan et al., 2023).\nThis paper details the architecture and functionalities of our automated research framework. We conduct manual and automatic evaluations on generated hypotheses and experimental executions/results. We also present case studies demonstrating the practical applications of our system on five machine learning research papers/problems. Through evaluations and examples, we illustrate that our framework can generate novel and feasible hypotheses for research, enabling researchers to focus on high-level scientific inquiry and innovation. We also show that MLR-Copilot is able to help finish the full research process and obtain significant results/improvements and conclusions."}, {"title": "2 MLR-Copilot Framework", "content": "MLR-Copilot automates the generation and implementation of research ideas using LLM agents, organized into three integrated phases: research idea generation, experiment implementation, and implementation execution."}, {"title": "2.1 Research Idea Generation", "content": "In the first stage, IdeaAgent, an LLM-powered agent, generates research hypotheses and experimental plans. For each task, the process begins with an individual research paper c = {C\u2081, C\u2082, ..., C\u2099}, where c\u1d62 represents the selected contents of the paper with Semantic Scholar API\u00b2, including the title, abstract, introduction, and related work.\nThe input processing involves analyzing the literature to extract essential information. Specifically, the initial input prompt is used to extract research tasks t, research gaps g, and keywords k = {k\u2081, k\u2082, ..., k\u2098} with LLM. Then P = {c, t, g, k} are provided to retrieve a set of recent works in the literature, denoted as R = {r\u2081, r\u2082, ..., r\u2097}.\nIdeaAgent extracts and synthesizes relevant information from the literature (Baek et al., 2024). Using updated information, the LLM generates new hypotheses with prompt detailed as P\u2081 = {P, R} \u2192 h based on identified trends and gaps in the existing research, ensuring both relevance and grounding in current studies.\nThis initial hypothesis set P\u2081 is then appended to create a detailed experimental plan P\u2082 = {P\u2081, h} \u2192 e. The experiment plan outlines the methodology, expected outcomes, and potential challenges associated with testing the hypothesis. Finally, we represent a research idea as:\nRI = {P, R, h, e}\nwhere: P denotes the information from original paper, R denotes the recent research findings, h represents the generated hypothesis, e outlines the experiment plan."}, {"title": "2.2 Experiment Implementation", "content": "The second phase involves translating experimental plans into executable experiments. This phase is facilitated by ExperimentAgent, an LLM-based agent. Given research idea RI that contains experiment plan e, ExperimentAgent performs several critical actions:\nFirst, it retrieves prototype implementation I from the original paper. Leveraging existing I, ExperimentAgent adapts and integrates this code, and optionally retrieves suitable models M\u2207 from a model repository M = {M\u2081, M\u2082, ..., M\u209a} to fit the specific needs of the experimental plan. The selection process is guided by the requirements of the experimental plan e\u2c7c, ensuring that the chosen models are appropriate for the specified tasks. If needed, relevant datasets D \u2208 {D\u2081, D\u2082, . . ., Dq} are identified and retrieved. We ensure that these datasets align with the experimental requirements by post-checkup, facilitating accurate and comprehensive testing of the hypotheses (Hocky and White, 2022). The ExperimentAgent modifies the code to ensure compatibility with the selected models and"}, {"title": "2.3 Implemetation Execution", "content": "In the final phase, ExperimentAgent manages the execution of the experiments. The execution phase encompasses running the experiments, incorporating mechanisms for human feedback, and supporting iterative debugging.\nThe experimental setups (I, M\u2207, D) \u2192 S are executed under the management of ExperimentAgent. The agent oversees the allocation of computational resources, monitoring the progress and performance of the experiments. Additionally, ExperimentAgent integrates mechanisms for human feedback, allowing researchers to provide input and adjustments during the execution phase. This feedback loop ensures that the experimental design and implementation can be refined in real-time.\nFrom the global point of view, ExperimentAgent provides feedback and enables researchers (or stage 1) to refine their hypotheses and experimental designs based on intermediate and final execution results (e.g. feasibility). This iterative approach ensures that the final research outcomes are robust, reproducible, and scientifically sound."}, {"title": "3 Experiments", "content": "To evaluate the effectiveness of MLR-Copilot, we conduct experiments across five machine learning research task papers. These tasks of the papers were chosen to cover a range of domains and complexities, demonstrating the versatility and robustness of our framework.\nSemRel (Ousidhoum et al., 2024) from SemEval 2024 Task 1 focuses on semantic textual relatedness across 13 languages and is popular for its diversity and real-world relevance. We use the supervised track for our experiments and adopt Pearson correlation as the metrics.\nMLAgentBenchmark (Huang et al., 2023) includes several datasets for evaluating LLMs in automated research idea generation and implementation. We use the following datasets: feed-"}, {"title": "3.2 Evaluation and Results", "content": "We evaluate different stages of our framework, i.e. the hypothesis generation stage (Section 3.2.1), the experiment implementation and implementation execution stages (Section 3.2.2) separately."}, {"title": "3.2.1 Evaluating Research Idea Generation", "content": "Following the setting of (Baek et al., 2024), we conduct both manual evaluations and automated evaluations. For baselines, we compare to an LLM in (Baek et al., 2024) which prompts with only a core paper to generate research ideas.\nFor manual evaluation, we invite three domain expert reviewers to assess the generated hypotheses based on criteria adapted from the (Baek et al., 2024): clarity, validity, rigor, innovativeness, and generalizability. Additionally, the experimental designs are evaluated for clarity, validity, robustness, feasibility, and reproducibility. Each criterion is scored on a 5-point Likert scale(refer to (Baek et al., 2024) for detailed definitions), with human researchers who have published at least three papers providing the annotations.\nFor automated evaluation, we employ an LLM reviewing agent to assess the clarity and validity of the hypotheses and the robustness and feasibility of the experimental designs, scoring each criterion on a 5-point Likert scale. Similarity analysis is"}, {"title": "3.2.2 Evaluating Experiment Implementation and Implementation Execution", "content": "We assess experiment implementation and execution by measuring average task performance improvement and success rate over 8 trials with human instructions comparing to the prototype code."}, {"title": "4 Analysis: Case Study for Sentiment Analysis Research", "content": "To demonstrate MLR-Copilot's practical application, we conducted a case study where researchers used the system to generate hypotheses and conduct sentiment analysis experiments on the ELLIPSE dataset. As shown in Figure 3, the process involves interaction between the ExperimentAgent, Action Executor, and various Utility Modules.\nThe action sequences illustrate how the MLR-Copilot system helps researchers systematically generate hypotheses and conduct experiments. The system inspects scripts, executes models, retrieves models, and analyzes results. Details are provided in Appendix A (IdeaAgent) and B (ExperimentAgent).\nThis comprehensive action log highlights the MLR-Copilot's systematic approach, allowing researchers to understand, modify, and execute scripts for sentiment analysis. Each action, driven by reasoning, objectives, observations, and feedback, refines the model and experimental design, leading to successful evaluation."}, {"title": "5 Related Work", "content": "The automation of idea generation in scientific research received great interest, particularly with the advent of LLMs. Previous studies have explored the potential of LLMs to assist in generating hypotheses and research questions based on literature-based discovery (Swanson, 1986). For instance, LLMs have been leveraged to provide initial drafts of research questions and even entire research pro-"}, {"title": "5.1 LLM as Scientific Agents.", "content": "The automation of idea generation in scientific research received great interest, particularly with the advent of LLMs. Previous studies have explored the potential of LLMs to assist in generating hypotheses and research questions based on literature-based discovery (Swanson, 1986). For instance, LLMs have been leveraged to provide initial drafts of research questions and even entire research proposals (Brown et al., 2020; Zhong et al., 2023; Qi et al., 2023b; Yang et al., 2023; Wang et al., 2024). However, these efforts primarily focus on the hypotheses generation phase but not on implementing and validating them. On the contrary, our work focuses on more realistic settings, investigating building LLM agents that tackle the entire process and how each stage can benefit and provide feedback for other stages.\nAlso related to our work are concurrent papers that explore using LLM for AutoML type of tasks (ScienceDirect, 2023; Zhang et al., 2023). For instance, Huang et al. (2023) benchmarks language models in the machine learning domain, with MLA-gent handling diverse tasks across datasets and models, and MLAgentBench allowing performance comparisons among MLAgents on standardized tasks. In contrast to our work on automatic machine learning hypothesis generation and research with broad utilities (action space), these models operate under more restricted conditions, focusing on predefined tasks with existing code and limited interaction capabilities based on parametric knowledge."}, {"title": "5.2 Model and Data Retrieval Systems.", "content": "Efficient models and data retrieval are critical components of modern AI systems. Hugging Face's Datasets and Model Hub provide researchers with vast repositories of datasets and pre-trained models (Lhoest et al., 2021; Wolf et al., 2020). These systems enable users to find relevant data and models quickly through natural language prompts, facilitating seamless integration into the research workflow. Our framework incorporates the model and data retrieval utilities, which play a crucial role in the experiment implementation process based on natural language prompts (Viswanathan et al., 2023). This allows for translating research questions and problem statements into specific model requirements, facilitating the automated retrieval of the most relevant models for hypothesis testing and validation."}, {"title": "6 Conclusion", "content": "We propose MLR-Copilot, a framework for automating machine learning research using LLM agents. It helps generate novel research ideas, implements & executes the experiments, and refines the implementations based on both automatic and human feedback. Evaluations from domain experts highlight it as a powerful tool for research idea generation and the experimentation process."}]}