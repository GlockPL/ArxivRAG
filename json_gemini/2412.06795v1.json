{"title": "SpikeFI: A Fault Injection Framework for Spiking Neural Networks", "authors": ["Theofilos Spyrou", "Said Hamdioui", "Haralampos-G. Stratigopoulos"], "abstract": "Neuromorphic computing and spiking neural networks (SNNs) are gaining traction across various artificial intelligence (AI) tasks thanks to their potential for efficient energy usage and faster computation speed. This comparative advantage comes from mimicking the structure, function, and efficiency of the biological brain, which arguably is the most brilliant and green computing machine. As SNNs are eventually deployed on a hardware processor, the reliability of the application in light of hardware-level faults becomes a concern, especially for safety- and mission-critical applications. In this work, we propose SpikeFI, a fault injection framework for SNNs that can be used for automating the reliability analysis and test generation. SpikeFI is built upon the SLAYER PyTorch framework with fault injection experiments accelerated on a single or multiple GPUs. It has a comprehensive integrated neuron and synapse fault model library, in accordance to the literature in the domain, which is extendable by the user if needed. It supports: single and multiple faults; permanent and transient faults; specified, random layer-wise, and random network-wise fault locations; and pre-, during, and post-training fault injection. It also offers several optimization speedups and built-in functions for results visualization. SpikeFI is open-source and available for download via GitHub at https://github.com/SpikeFI.", "sections": [{"title": "I. INTRODUCTION", "content": "Neuromorphic computing is an emerging computing paradigm that has its roots in mimicking the spike-based operation of neurons in the biological brain. A neuromorphic processor essentially maps a Spiking Neural Network (SNN). SNNs can offer orders of magnitude more energy efficiency and inference speed compared to the more conventional Ar- tificial Neural Networks (ANNs) [1], [2]. For this reason, SNNs open exciting new possibilities for realizing the next- generation Artificial Intelligence (AI) systems and for pow- ering intelligent and autonomous edge devices with local AI processing. A major leap forward in the recent years is the development of several large-scale neuromorphic processors, e.g., SpiNNaker [3], TrueNorth [4], Loihi [5], BrainScaleS [6], and Neurogrid [7], supported also with software frameworks. In this work, we address the dependability aspects of neuromorphic processors in view of the rare yet inevitable hardware-level faults. Hardware-level faults include bit-flips caused by cosmic ray particle strikes (a.k.a. soft errors) and defects and process parameter variations that are induced during manufacturing or occur in the field due to silicon aging mechanisms. SNNs show a large degree of inherent fault tolerance thanks to the analogy to the biological brain that has remarkable fault tolerance capabilities. Thus, most faults end up being benign: they are masked, i.e., their effect is not propagated to the output, or they can be tolerated, i.e., the output changes but the cognitive decision is still correct. However, there exist critical faults that will cause a wrong output disrupting the application.\nMore specifically, we propose a generic Fault Injection (FI) tool, named SpikeFI, for automating fault analysis of SNNs. Starting with an SNN model, the user is able to inject faults on different locations in the SNN architecture and assess their impact on the success of training and the accuracy of inference. The tool supports any SNN model, i.e., fully-connected. convolutional, or recurrent. It embeds a comprehensive fault model library that can be customized by the user. It supports single or multiple faults, transient or permanent faults, as well as statistical fault injection layer-wise and network-wise. Fault injection can be performed before, during or after training. SpikeFI also offers several simulation speedup options, such as early stop and late start, and has built-in various types of results visualisation functions.\nSpikeFI has several use cases:\n1) Understand the vulnerability of the SNN application to faults.\n2) Assess how architectural choices (i.e., depth, layer size, feature map size, weight quantization, network compres- sion, etc.) and the different per-layer hyper-parameters (i.e., neuron threshold, leakage, and refractory period) affect the resilience to faults so as to make early design decisions with reliability in mind.\n3) Guide test generation algorithms aiming at generating test inputs for sensitizing and detecting critical faults [8]\u2013[11]. Compact test sets can be used for post-manufacturing testing or can be replayed in idle times or periodically for in-field on-line testing.\n4) Evaluate training algorithms in terms of their fault tolerance capabilities and develop fault-aware training algorithms [12]. For example, faults can be inserted during training, transiently across the epochs, to increase the robustness of the network to faults once deployed. Once the training is over the faults are ejected.\n5) Assess the criticality of faults towards developing cost- effective hardware-level fault tolerance techniques [13]\u2013 [20].\nSpikeFI performs fault analysis at the application level in software. Fault injection can be performed instead at the hardware description level, i.e., RTL, gate-level or transistor- level, but this requires the availability of the hardware im-"}, {"title": "II. BACKGROUND INFORMATION ON SNNS", "content": "Neural network models are classified into three generations. The first generation was based on McCulloch-Pitts neurons, also referred to as perceptrons, which give a digital output. The second generation of models applied an activation function to the output of the neurons, such as a rectified linear unit (ReLU) or sigmoid, and, in this way, they supported analog computa- tion and learning algorithms based on gradient-descend, such as backpropagation. SNNs are the third generation [45], dis- tinguishing themselves from their predecessors by their ability to mimic more realistically the biological brain. However, they constitute simplified models of their complex biological counterparts maintaining some of their aspects, since they are primarily used for computational purposes, rather than simulating the human brain.\nInspired from biological neural systems, SNNs encode the information in the timing of single action potentials, or spikes, and incorporate the time between successive spikes as a source of computation and communication among their spiking neurons. Spikes correspond to events generated when- ever a change occurs providing a continuous time processing with very detailed time resolution reaching the micro- or nanosecond scale.\nFrom a hardware perspective, SNNs form the basis of neuro- morphic computing. Spiking neurons operate asynchronously to each other, as they are only utilized when an incoming spike stimulates them via their pre-synaptic connections. This makes SNNs event-based computation systems, which offers low latency and energy consumption compared to level-based ANNs. However, there are still challenges facing SNNs, such as the complexity of training.\nAnother characteristic of SNNs is the input type, which needs to be in a spiking form as well, i.e., the network is fed with a continuous-time event flow instead of static frames. A natural way to achieve this is with a neuromorphic camera, also known as Dynamic Vision Sensor (DVS). A DVS resembles the retina of the human eye and is composed of pixel-neurons that react to changes in brightness. When a sufficient change has occurred to the brightness of a pixel- neuron, it generates a positive or negative event, depending on the polarity of the change."}, {"title": "B. Spiking neuron models", "content": "There exist several spiking neuron models, ranging from biologically detailed ones, such as the Hodgkin-Huxley, to simplified ones that are more computational efficient for large networks and hardware implementation while still incorpo- rating the main neuronal dynamics, such as the Integrate & Fire (I&F) [46]. As SpikeFI is built on top of SLAYER [25], and given that SLAYER employs the Spike Response Model (SRM) which is a generalization of the I&F model, herein we discuss in detail the SRM.\nIn the SRM, the state of the neuron at any given time is described by its membrane potential u. At its resting state, the membrane potential is set to a low value $U_{rest}$. The neuron integrates the incoming spikes from the synapses at its input and the membrane potential is increased or decreased according to the spike polarity. Once the potential reaches a certain threshold $\\theta$, the neuron fires a spike, which is propagated to the next layer of neurons via the synapses connected to its output, and the neuron is reset to its resting state again. At the same time, the neuron is regulated to not fire again for a while. The minimum time in-between successive spikes is called refractory period.\nTo mathematically express the above functionality, the SRM considers that the action of a neuron at any given time is a response to both the incoming activity and the neuron's own output. For this purpose, two response functions are used, namely, the synaptic kernel $\\epsilon$ and the refractory kernel $\\eta$. The"}, {"title": "C. Training schemes", "content": "Training large SNN models remains a challenge [1], [2], [47]. The classic backpropagation algorithm used in ANNs cannot be applied directly to work with spiking events due to their non-differentiable nature. Several learning schemes have been developed to overcome this challenge, including the biologically inspired Spike-Timing-Dependent Plasticity (STDP), training an ANN and converting it to a SNN, evo- lutional algorithms, and spike-based backpropagation. Spike- based backpropagation is currently one of the most practical and accurate techniques for training SNNs. SLAYER [25] trains a SNN with a variation of backpropagation using the probability of a spiking neuron to change state, i.e., fire a spike or move back to its resting state. The fact that SNNs can have the same topologies as ANNs, e.g., fully-connected, convolutional, recurrent, etc., allows for embedding SLAYER in already mature Machine Learning (ML) frameworks with some adjustments to add support for the spiking functionality. To this end, there is a PyTorch version of SLAYER that can be used for both the training and the inference of any SNN"}, {"title": "III. FAULT MODELING", "content": "SpikeFI is a fault injector at the application level. Hardware- level faults are translated to behavioral-level faults which, thereafter, are reproduced mathematically into the SRM de- scribed in Section II-B. We consider that the processing elements of the SNN, i.e., neurons and synapses, are discrete entities that can fail independently. A bottom-up approach can be followed to extract the spiking neuron and synapse faulty behaviors starting from transistor-level simulations [48]. SpikeFI conforms to the established practice and adopts all widespread and conventional fault models in the literature that are derived from various digital, analog, and mixed analog- digital SNN hardware implementations. These fault models are built-in within SpikeFI and are delivered as a library. The library is fully modifiable and extendable, i.e., developers are free to create their own fault models depending on the hardware implementation and fault occurrence probabilities.\nModeling faults at behavioral-level allows evaluating their impact without the need of knowing the details of their source or mechanism, avoiding in this way costly low-level simula- tions, i.e., at the transistor, gate, microarchitectural level, etc., performed at the network scale. Additionally, behavioral-level fault modeling provides the flexibility to model any possible hardware-level fault, as long as a mathematical formula de- scribing it can be derived. Another advantage is that the results are not tied to a specific hardware accelerator design, thus the drawn conclusions tend to be more generic.\nNext, we describe the built-in fault models, which are summarized in Table I, by separating them into neuron and synapse fault models. Fault models can be further divided into hard and parametric fault models, depending on whether the processing element presents an outright failure or deviation."}, {"title": "A. Neuron faults", "content": "a) Dead neuron: A fault that halts the neuron's spiking activity and makes it unresponsive to any input. To model a dead neuron, its output spike train is set to zero, i.e., $\\hat{S}_o(t) = 0$.\nb) Saturated neuron: A fault that causes a neuron to be firing non-stop, even in the absence of input activity. A saturated neuron can be considered as the complementary extreme case of a dead neuron, where the output spike train is never zero, i.e., $S_o(t) = \\sum_{n=0}^{\\infty} \\delta(t - n)$.\nc) Stuck-at-x neuron: A fault that causes the neuron's output to be stuck-at a value x, i.e., $\\hat{S}_o(t) = x \\sum_{n=0}^{\\infty} \\delta(t - n)$, where $x \\in \\mathbb{R}$. A stuck-at neuron can be viewed as the generic hard neuron fault, with the extreme dead and saturated neuron faults being derived by setting x = 0 and x = 1, respectively."}, {"title": "B. Synapse faults", "content": "a) Dead synapse: A fault that holds the synaptic weight to zero, i.e., $w_i = 0$, disabling the synapse and \"cutting\" the connection between the pre- and post-synaptic neurons.\nb) Saturated synapse: A fault that saturates the synaptic weight to an extreme positive $\\hat{w}_i >> w_i$ or extreme negative $\\hat{w}_i << w_i$ value. Some representative positive and negative saturation values could be respectively the highest and lowest values of the weight distribution resulting from training."}, {"title": "C. Permanent and transient fault effect", "content": "Depending on the nature of a fault and the factors that led to its occurrence, its effect may be either permanent or transient. SNNs have a global internal clock that defines the discrete times when neurons can be firing. If a fault is transient, its effect is active for a limited period or number of clock cycles. For the rest of the FI experiment time, the components affected by the transient fault are restored to their nominal state and, therefore, the original behavior is expected. For example, denoting the transient fault duration by $[t_1, t_2]$, the behavioral description of a transient neuron saturation fault is:\n$\\hat{S}_o(t) = \\begin{cases} \\delta(0) &: t \\in [t_1, t_2] \\\\ S_o(t) &: \\text{otherwise} \\end{cases}$\nand the behavioral description of a perturbed synapse fault is given by:\n$\\hat{w}_i(t) = \\begin{cases} \\rho w_i &: t \\in [t_1, t_2] \\\\ w_i &: \\text{otherwise} \\end{cases}$"}, {"title": "IV. THE SpikeFI FRAMEWORK", "content": "SpikeFI is a native PyTorch framework [49] built upon the SLAYER framework [25], extending the capabilities of SLAYER to add support for setting and automatically exe- cuting FI and reliability analysis experiments. SLAYER is a notable training framework for SNNs that is contributing to the growing interest in SNNs. It is developed to enable efficient spike-based backpropagation learning for deep SNNs. It has been incorporated into Lava, Intel's open-source software framework for developing SNNs. SpikeFI employs the same principles and programming concepts and paradigms as its un- derlying frameworks PyTorch and SLAYER preserving com- patibility. Any arbitrary SNN model implemented in SLAYER can be the subject of FI and reliability analysis, without needing to make any modifications to the model. Researchers and developers already familiar with SLAYER can therefore jump directly into using SpikeFI. SpikeFI is offered as open- source software and is available for download and to contribute via the GitHub platform: github.com/SpikeFI/.\nSpikeFI supports the following features and scenarios:\n1) Flexible fault modeling scheme: SpikeFI has an integrated comprehensive library of predefined fault models to select from, as described in Section III. Faults can be injected into any processing element, i.e., neuron or synapse, and at different levels, i.e., isolated processing element, layer-wise, and network-wise. SpikeFI being open-source allows the user to design and integrate custom fault models.\n2) Pre-/during/post-training FI injection: SpikeFI allows in- jecting faults before, during, or after the training phase prior to inference. The motivation is different across these scenarios. In pre-training fault injection, the goals can be to perform fault-aware training, re-train the network after the occurrence of a critical fault, and, in general, to study the network's capability to learn around faults. During training fault injection aims at studying the effect of faults occurring while training is progressing. This is useful when training deep SNNs as it can take significant time during which the hardware can suffer a fault. Post-training fault injection aims at studying the effect of faults on the inference accuracy. The analysis here aims at studying the inherent reliability and deriving critical fault types and locations. This information can subsequently be used for developing cost-effective test and fault tolerance strategies.\n3) Multi-round FI campaign: A FI campaign may be com- posed of multiple experiments that are conducted sequen- tially independent of each other.\n4) Single/multiple/cumulative FI injection: Each of the fault rounds may contain a single fault, multiple faults or accu- mulated faults. In the latter scenario, in each FI experiment the set of faults is increased to observe the accumulative effect of the new faults added.\n5) Permanent/transient fault analysis: A fault can be designed to be permanent or transient of varying duration, as de- scribed in Section III-C.\n6) Optimization options: SpikeFI, being built on top of Py- Torch, utilizes GPU acceleration. It also offers various op- timization options to speedup fault injection experiments, such as proper for-loop ordering, late start, early stop, and batch-wise inference, which will be described in detail in"}, {"title": "B. Structuring of a FI experiment", "content": "Fig. 2 summarizes the hierarchical organization of program- ming elements that shape a FI campaign in SpikeFI.\n1) Fault Round: A group of faults to be injected altogether into the network. A fault round can contain a single or multiple faults. The collective effect of all faults belonging to the same round is evaluated simultaneously in a single inference. SpikeFI also offers the possibility to perform cumulative fault analysis, i.e., define multiple fault rounds where each fault round contains the faults of the previous fault round plus some additional faults. Once a fault round has been evaluated, the faults are withdrawn from the network before continuing to the next fault round.\n2) Fault: The actual fault to be injected into a network, composed of a fault model, one or more fault sites, and a fault duration.\n3) Fault model: The fault model is in turn composed of a fault target and a fault function.\na) Fault target: The fault target can be: (i) the neuron output; (ii) the SRM parameters, i.e., neuron's membrane time constant, threshold, and refractory time constant; and (iii) the weight of a synapse, as described in Sections III-A and III-B.\nb) Fault function: The fault function is the mathematical formula that describes how the fault is to affect the operation of the targeted processing element, as described in Sections III-A and III-B.\n4) Fault Site: The fault site is the location of the fault within the network. The fault can be isolated affecting a specific processing element or it can be applied to multiple processing elements simultaneously. The user has also the option to create random fault sites per layer or across the network following some statistical fault distribution. The site of a fault is composed of the layer and coordinates of the processing element within the layer. In the case of a neuron, the site is the quadruplet (l, c, x, y), where l is the layer name or number, c is the feature map number within the layer, and (x, y) are the coordinates of the neuron within the feature map. In the case of a synapse, the site is defined by the quadruplets of the two neurons connected by the synapse.\n5) Fault Duration: The fault duration provides information on when the fault is activated and for how long it remains active, as described in Section III-C. By default, a permanent fault means that is active for the whole duration of the input sample, whereas the duration of a transient fault is only a portion of the duration of the input sample."}, {"title": "C. FI implementation into SLAYER", "content": "To inject a fault, SpikeFI modifies the PyTorch computation flow in SLAYER. For every fault model, there is a suitable modification to be applied. To apply the modification, SpikeFI makes use of PyTorch pre-hook and hook functions, i.e., a function called right before and right after the evaluation of a module, respectively.\nLet us first consider neuron hard faults in layer l. The fault function of the fault model returns the output of the faulty neurons and updates the output of layer l for these neurons while the rest of the neurons retain their fault-free output. Then, the output of layer l is propagated to the input of the subsequent layer l + 1 of the network. This is implemented with a neuron pre-hook function attached to the input of layer l+1.\nRegarding neuron parametric faults, in SLAYER the SRM parameters are set globally for a layer and are shared among its neurons, thus it is not possible to modify the parameters only for a subset of neurons. For this reason, SpikeFI uses a hook and a pre-hook function. The first one, called neuron parametric hook, receives the same input as the faulty layer l, creates a \"dummy\" copy of the layer with altered SRM parameter for all neurons according to the fault model, repeats the parts of the calculation on this \"dummy\" copy that concern the affected SRM parameters, and feeds the result to the next neuron pre-hook function attached to the input of layer l + 1. The neuron pre-hook function selects only the faulty neuron(s) and replaces their output spike train with the spike train of the corresponding neuron(s) in the dummy layer.\nRegarding the synaptic weights, SpikeFI uses a synapse pre- hook function to alter the synaptic weight value according to the fault model prior to the forward pass through the faulty layer. At the end of the faulty layer evaluation, there is a synapse restore hook function that restores the original synaptic weight, so that there is no interference between successive fault rounds."}, {"title": "D. Optimizations", "content": "1) Ordering of nested for loops: All fault rounds are evalu- ated by performing inference for the same set of input samples, which could correspond to the complete testing dataset or part of it. Essentially, a FI campaign is a nested loop iterating over all fault rounds and over the set of input samples. The ordering of the two for loops has in fact an effect on the FI campaign runtime. SpikeFI places the input samples in the outer for loop and the fault rounds in the inner for loop since this results in faster runtime, as it will be demonstrated quantitatively in Section V-B. The underlying reason is that the alternative for-loops ordering would require transferring the dataset to the GPU for the computation multiple times, equal to the number of fault rounds, which would add a significant time overhead. Instead, with the selected for-loop ordering, a batch transferred to the GPU is reused for all fault rounds, thus circumventing this time overhead.\n2) Late Start: As typically there are multiple fault rounds in a FI campaign, there is significant repetition of forward passes through initial fault-free layers of the network. For example, consider two fault rounds with a single fault each, located at layers i and j > i, respectively. The forward pass is repeated twice up to layer i - 1 which is redundant. To save simulation time, for every fault round, SpikeFI uses the late start option that skips layer computations up to the leftmost faulty layer, denoted by $l_{left}$.\nFor this purpose, in a preparatory phase prior to the FI experiment, SpikeFI performs a nominal inference for the complete testing dataset and records the deterministic golden output of all layers. More formally, for each layer l, SpikeFI computes the matrix $A^l$ of its output spike trains with dimen- sions $N^l \\times d$, where $N^l$ is the number of flattened neurons in layer l and d is the number of timestamps within the inference window. The SNN has a global clock with period T. The timestamps are denoted by $t_j = j * T, j = 1, \\dots, d$. $A^l(i,j) = 1$ if neuron i in layer i fires at timestamp $t_j$, otherwise $A^l(i, j) = 0$. The layer matrices $A^l$ are combined to generate the network matrix $A = [A^1, \\dots, A^L]$, where L is the number of layers.\nDuring the FI experiment, for every fault round, SpikeFI first orders the faults based on the layer wherein they occur in ascending order. If the first faults appear at layer $l_{left}$, then SpikeFI uses the golden output of layer $l_{left} - 1$ and continues the simulation from this point onward.\nNote that if the fault round contains only hard neuron faults, then the simulation can continue from layer $l_{left} + 1$. This is because of the implementation in PyTorch which injects the fault with a neuron pre-hook function attached to the input of the layer following the faulty layer. This means that late start can offer speeds-up even when the leftmost faulty layer is the first layer.\n3) Early Stop: As mentioned in the introduction, many faults end up being benign. If in a fault round the output of the rightmost faulty layer, denoted by $l_{right}$, is unaffected matching the golden response, then it is pointless to continue the simulation as it will be like simulating a nominal fault- free network. SpikeFI uses the early stop option to skip this redundant computation. More specifically, early stop uses the golden layer output matrices $A^l$ as in late start. Assuming a fault round with the last faults occurring in layer l, the early stop option computes the same matrix denoted by $A^l_f$, where the subscript f indicates a fault round, subtracts it from the golden matrix $A^l$ to produce the matrix $B^l = A^l - A^l_f$, and computes the summation of all elements of $B^l$ denoted with the elementwise 1-norm $\\|B^l\\|_1$. If $\\|B^l\\|_1 = 0$ it means that all faults in this fault round are benign and the simulation stops at"}, {"title": "E. Complete FI experiment flow", "content": "A FI experiment is divided into three stages, namely the preparation, execution, and results extraction and visualization stages.\n1) Preparation stage: First, the validity of all faults in all fault rounds is checked. If a fault is invalid, for example the fault site is nonexistent, then the fault is dropped from the experiment. Then, random faults are assigned a random site. After these two steps, the faults within each fault round are ordered according to the layer they belong to in ascending order so as to identify the leftmost $l_{left}$ and rightmost $l_{right}$ layers in the fault round. The verified fault rounds together with their sorted list of faults is communicated back to the user to acknowledge the FI experiment setup. Lastly, for each batch and before the FI starts, SpikeFI performs an inference on the nominal network so as to record the golden outputs of all layers and form the matrices $A^l$ used in the late start and early stop optimizations.\n2) Execution stage: Fig. 4 shows the SpikeFI complete FI experiment flow including all optimizations. The dataset is transferred to the GPU in batches and for each batch the same FI experiment is performed in parallel for every input sample in the batch. The FI experiment is composed of a number of fault rounds simulated sequentially. For a given fault round, the simulation starts from the leftmost faulty layer $l_{left}$ using the golden output of the previous layer, or from layer $l_{left} + 1$ if $l_{left}$ comprises only hard neuron faults. Inference continues up to the rightmost faulty layer $l_{right}$. In the case of a fault round with a single fault, $l_{left}$ and $l_{right}$ coincide. At this point, if the early stop criterion is met then the simulation stops and we proceed to the next fault round. Otherwise, the simulation continues up to the last layer. Before the next fault round starts, the results are saved and the network is initialized back to its original fault-free state.\n3) Results extraction and visualization stage: Based on the spike encoding method employed by the SNN, i.e., rate coding, temporal coding, etc., for each fault round, SpikeFI uses the output spike information in the matrices $A^l$ and $A_f^l$ to report the accuracy of the faulty network. For example, let us assume rate encoding, which is the most widely used spike encoding method, and a classification cognitive task. In this case, the output layer comprises one neuron per class and the winning class, i.e., the top-1 prediction, is that whose neuron fires the largest number of spikes within the inference time window. These spike counts are computed using the matrices $A^l$ and"}, {"title": "F. Open source code and example", "content": "A detailed documentation is provided in the GitHub plat- form for the easy integration and usage of the SpikeFI frame- work. The structure of the SpikeFI framework is organized in the form of a Python package to be imported and use its functions immediately. There are four main modules within the Python package, namely, the core, fault, models, and visual modules, each containing relative classes and functions to implement the framework's functionalities. An additional demo module is included with the implementations of the two SNN architectures of Section V-A and example scripts to showcase the usage of SpikeFI in various scenarios.\nAlgorithm 1 presents a pseudo-code example resembling Python syntax of two FI campaigns. First, the campaign object cmpn is created and initialized with the network model net, a vector with the dimensions of the input data samples shape_in, and the spiking-related information object slayer, which is provided by SLAYER and is initialized by the user. More specifically, slayer contains information about the SRM parameters of the spiking neurons, the duration of the input data samples, the global clock period, the target number of spikes for the winning class neuron, etc. Next, the faults fx,"}, {"title": "V. RESULTS", "content": "SpikeFI is demonstrated on two convolutional SNNs trained to classify the N-MNIST [50] and IBM's DVS128 Gesture [51] datasets. These SNNs are modelled and trained in SLAYER, and are included in the demo package of the SpikeFI frame- work in GitHub. They use rate coding, i.e., the winning class is selected after the neuron at the output layer which is triggered the most producing the highest number of spikes.\nThe N-MNIST dataset is a neuromorphic, i.e., spiking, version of the MNIST dataset, which comprises images of handwritten arithmetic digits in gray-scale format [50]. It consists of 70000 sample images that are generated from the saccadic motion of a DVS in front of the original images in the MNIST dataset. The samples in the N-MNIST dataset have a duration of 300 ms. The dataset is split into a training set of 60000 samples and a testing set of 10000 samples. The SNN architecture, shown in Fig. 5, is a spiking version of the LeNet-5 architecture [52]. It consists of 3 convolutional layers with 2 2x2 sum-pooling layers in between them and 2 fully-connected layers at the end for the final decision of the network. The classification accuracy on the testing set is 97.8%.\nThe IBM's DVS128 Gesture dataset consists of 29 individ- uals performing 11 hand and arm gestures in front of a DVS,"}, {"title": "B. Optimization speedups", "content": "Herein, we use the N-MNIST SNN as a benchmark for quantifying the speedup improvements when using the differ- ent optimizations.\nFor a fair comparison, all the FI experiments below were executed one at a time on the same system configuration composed of an Intel Xeon\u00ae W-2133 CPU and a NVIDIA Quadro\u00ae RTX 4000 GPU, with the system being reserved for the experiment.\n1) Ordering of nested for loops: First, we show that placing the fault rounds in the inner for loop as opposed to the outer for loop speeds up the analysis. For this purpose, we perform successive FI campaigns with a batch size of 1 and"}, {"title": "C. Demonstrations", "content": "1) Neuron hard faults: In the first experiment, we inject single hard neuron faults considering all neurons. We ran two separate FI campaigns for dead and saturated neuron faults. Fig. 11 shows a possible visualization of the results using comparative bar plots. The x-axis shows the different layers of the network and for each layer there are two bars, one for the dead and one for the saturated neuron faults. Note that pooling layers were excluded from the analysis since their functionality is to aggregate regions of spikes of their previous layers and do not contain any spiking neurons. A bar is separated into chunks of different colors, each corresponding to a specific classification accuracy according to the color shading shown at the bottom of Fig. 11. The height of the chunk projected on the y-axis shows the percentage of neurons in this layer which when exhibit this type of fault the classification accuracy drops to the value indicated by the color of the chunk.\nA first observation from Fig. 11 is that saturated faults have a far stronger impact on the classification accuracy compared to dead faults. At the output layer, a saturated neuron always wins the race, thus samples from all classes except the one corresponding to the winning neuron are always misclassified. In the case of a dead neuron, an input with class label corresponding to this neuron is always mislassified, while samples from other classes are not affected. Taking the N- MINST SNN as an example, a saturated neuron at the output layer causes the accuracy to plummet to a value of 10% on average, while a dead neuron reduces the accuracy by 10% on average. The fact that saturated faults are more lethal than dead faults is also evident in the SF1 layer of the two networks. The IBM DVS128 Gesture SNN is impacted also in the SC1 layer, while the N-MNIST SNN is insensitive to faults in the first two layers and in the third layer only a 2% of neurons are critical.\n2) Neuron parametric faults: The effect of neuron paramet- ric faults on the classification accuracy is shown in Fig. 12. For a given layer, we vary the Ts, Tref, and 0 parameters in the SRM for one neuron at a time. The main curve represents in the y-axis the average classification accuracy observed across all neurons of the layer as a function of the parameter deviation in the x-axis expressed in % of the nominal value, i.e., 100% corresponds to zero deviation. The colored region surrounding the curve demonstrates the minimum and maximum classifi- cation accuracy. Neuron parametric faults were found to have a noticeable impact only for the output layer, thus in Fig. 12 we show only the results for the output and last hidden layer.\nThe N-MNIST SNN shows a high resilience even at the output layer. The classification accuracy starts degrading when Ts, 0, and Tref are reduced at 40%, 40%, and 20%, respec- tively, while positive deviations have practically no effect as accuracy degradation starts being noticeable only for 6 when it increases beyond 200%. In contrast, the SNN DVS128 Gesture SNN shows vulnerability even for small Ts and @ fluctuations, while it shows a high degree of resiliency for Tref.\nAs mentioned in Section III-A2, increasing Ts, decreasing 0, or decreasing Tref makes the neuron spike more easily. At the extreme, this direction of deviation may make the neuron saturate, which, as we observed in Section V-C1, is far more fatal than a dead fault. This behavior can be observed in Fig. 12. Extreme positive deviation of Ts for the IBM DVS128 Gesture SNN has an effect equivalent to neuron saturation as the accuracy drops to 9.09%, i.e., only 1 out of 11 classes is predicted correctly. Similarly, extreme negative deviation of is equivalent to neuron saturation in both networks."}, {"title": "5) Training in the presence of faults", "content": "Herein, SpikeFI is used to assess the ability of a SNN to learn in the"}]}