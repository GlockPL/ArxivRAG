{"title": "A3: Android Agent Arena for Mobile GUI Agents", "authors": ["Yuxiang Chai", "Hanhao Li", "Jiayu Zhang", "Liang Liu", "Guozhi Wang", "Shuai Ren", "Siyuan Huang", "Hongsheng Li"], "abstract": "AI agents have become increasingly prevalent in recent years, driven by significant advancements in the field of large language models (LLMs). Mobile GUI agents, a subset of AI agents, are designed to autonomously perform tasks on mobile devices. While numerous studies have introduced agents, datasets, and benchmarks to advance mobile GUI agent research, many existing datasets focus on static frame evaluations and fail to provide a comprehensive platform for assessing performance on real-world, in-the-wild tasks. To address this gap, we present Android Agent Arena (A3), a novel evaluation platform. Unlike existing in-the-wild systems, A3 offers: (1) meaningful and practical tasks, such as real-time online information retrieval and operational instructions; (2) a larger, more flexible action space, enabling compatibility with agents trained on any dataset; and (3) automated business-level LLM-based evaluation process. A3 includes 21 widely used general third-party apps and 201 tasks representative of common user scenarios, providing a robust foundation for evaluating mobile GUI agents in real-world situations and a new autonomous evaluation process for less human labor and coding expertise. The project is available at https://yuxiangchai.github.io/Android-Agent-Arena/.", "sections": [{"title": "1 Introduction", "content": "The significant advancement in the Large Language Model (LLM) area has boosted the development of AI agents, which are able to autonomously complete tasks following the instructions from users. Existing mobile AI assistants such as Siri, Xiao AI, and Bixby have demonstrated the potential of mobile agents to facilitate interactions between human users and mobile devices. However, those assistants are only effective in managing the routine tasks such as reporting weather condition and performing web searches due to the nature that they use APIs to perform task automation. To broaden the capability of AI agents, researchers have proposed GUI agents, which leverage the extended world knowledge and robust capabilities of multimodal large language models (MLLM) to effectively complete tasks on third-party general apps without the reliance on the APIs.\nDespite the promising advancements in GUI agents, the majority of existing GUI-control datasets (Rawles et al., 2024b; Chai et al., 2024; Li et al., 2024) and primarily focus on static frame evaluations, which have significant limitations. These datasets typically provide a collection of screenshots or UI states, requiring agents to predict the next action based on a single, frozen frame. Such an approach fails to capture the dynamic and interactive nature of real-world mobile tasks, where agents must navigate through sequences of actions, adapt to changing app states, and handle unexpected outcomes. Furthermore, static frame evaluations often lack contextual information about task flows, making it difficult to assess an agent's ability to perform multi-step or goal-oriented tasks. This disconnect between static frame evaluations and real-world usage scenarios results in a gap between the capabilities of current GUI agents and the demands of practical applications, underscoring the need for a more comprehensive and interactive evaluation platform.\nSeveral recent works (Rawles et al., 2024a; Xing et al., 2024; Xu et al., 2024; Lee et al., 2024; Zhang et al., 2023b) have introduced dynamic evaluation platforms for Android GUI agents. While these efforts represent significant progress, they suffer from critical limitations that hinder their effectiveness as comprehensive evaluation benchmarks. For instance, many platforms restrict app selection to Google apps, F-Droid apps (non-mainstream open-source apps), or static offline apps, which do not reflect the diversity or complexity of real-world usage. Additionally, these platforms often provide only a limited diversity of tasks, or fail to include information query tasks, which are essential for evaluating practical agent performance. To address these shortcomings, we propose Android Agent Arena (A3), a novel evaluation system that offers: (i) integration with 21 widely used third-party apps and 201 tasks designed around real-world app functionalities, (ii) a diverse set of tasks categorized into three distinct types, and (iii) a larger action space, enabling compatibility with agents trained on any dataset. Furthermore, we introduce a new evaluation method that leverages the capabilities of business-level large language models (LLMs) to automate task evaluation, significantly reducing the need for human intervention and manual coding."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 GUI Agent", "content": "(Wang et al., 2023) applies large language models (LLMs) to GUI tasks, but their focus remains limited to single-page interactions, resembling question-answering tasks rather than fully end-to-end instructional tasks. Recent advancements have begun leveraging the extensive world knowledge and robust reasoning capabilities of LLMs (Gu and Dao, 2024; Gur et al., 2023; Touvron et al., 2023) for task planning and execution. A notable approach involves using general-purpose business-level models, such as GPT-4v, as GUI-control agents. Studies like (Zhang et al., 2023a; Zheng et al., 2024) employ extensive prompt engineering to guide these models in performing complex tasks. However, the effectiveness of such methods depends heavily on meticulous prompt design to achieve satisfactory results. An alternative research direction focuses on fine-tuning smaller LLMs using GUI-specific datasets to imbue them with domain-specific knowledge, thereby improving their efficiency and task performance. For instance, CogAgent (Hong et al., 2024) enhances GUI task performance by incorporating a high-resolution cross-module that fuses image features at multiple levels. Similarly, MobileAgent (Ding, 2024) optimizes input data structuring and sample handling to make it more consistent and compatible with LLMs. SphAgent (Chai et al., 2024) utilizes the element functionalities to further enhance the screen and element understanding. CoCo-Agent (Ma et al., 2024) and ANDROIDCONTROL (Li et al., 2024) take a different approach by integrating element layouts from accessibility trees or view hierarchies as additional inputs, rather than relying solely on screenshots. While this approach enhances performance, it faces limitations as many apps either lack accessibility information or provide only minimal accessibility data, which restricts its applicability"}, {"title": "2.2 GUI-related Dataset", "content": "The introduction of the Rico dataset series (Deka et al., 2017; Sunkara et al., 2022) marked a significant milestone in GUI-related research by providing foundational datasets for GUI element classification and detection. Subsequent works (Burns et al., 2021; Gubbi Venkatesh et al., 2024) introduced small-scale, instruction-based GUI control datasets. Among these, UGIF (Gubbi Venkatesh et al., 2024) stands out as a multilingual dataset supporting eight languages. AITW(Rawles et al., 2024b) expanded the field with a large-scale dataset, but it suffered from significant redundancy in instructions and frequent mis-annotations. To address this, AITZ(Zhang et al., 2024) refined AITW by applying Chain-of-Action-Thought re-annotation, resulting in a cleaner but much smaller dataset. ANDROIDCONTROL(Li et al., 2024) further introduced a large-scale dataset, albeit with simpler tasks and a distinct action space compared to AITW and AITZ. Meanwhile, AMEX(Chai et al., 2024) redefined GUI element annotation by incorporating element functionality, enabling agents to better interpret mobile GUI designs, and pushed the boundaries with more complex tasks than prior datasets. However, despite their contributions, these datasets are limited to static frame evaluations, where agents predict actions based on a single screenshot, an instruction, and a ground-truth history of actions. This approach fails to capture the dynamic and interactive nature of real-world scenarios, where historical actions are unavailable, and a single error can cascade and severely impact subsequent performance. This highlights the need for evaluation systems that better reflect the complexities of real-world task execution."}, {"title": "2.3 Dynamic Evaluation Benchmark", "content": "To overcome the limitations of static frame evaluations, researchers have developed several dynamic evaluation systems (Lee et al., 2024; Xu et al., 2024; Xing et al., 2024; Rawles et al., 2024a; Zhang et al., 2023b) that aim to better simulate real-world environments (See Table ??). For instance, Mobile-Env (Zhang et al., 2023b) incorporates a broader range of general apps but is restricted to only 74 tasks. AndroidArena (Xing et al., 2024) introduces a larger number of tasks, including cross-app tasks, but is limited to Google apps and built-in system apps (e.g., settings and clock), which are already manageable by API-based assistants. B-Moca (Lee et al., 2024) supports a Korean language setting but offers tasks that are overly simplistic and lack diversity. AndroidWorld (Rawles et al., 2024a) uses open-source apps from F-Droid\u00b9, but these apps differ significantly from mainstream app designs, making them unrepresentative of real-world scenarios. Notably, all of these systems focus only on operational instructions and their corresponding evaluations. AndroidLab (Xu et al., 2024) is the first system to incorporate information query instructions and evaluations, addressing a key gap. However, its app selection is limited to offline and static apps, failing to include categories such as news, shopping, email, and music, which are critical for real-world usability. To specify, the evaluation methods in existing systems predominantly rely on element matching (Lee et al., 2024) or predefined answers (Xu et al., 2024)."}, {"title": "3 Android Agent Arena (A3)", "content": ""}, {"title": "3.1 Overview", "content": "A3 is a lightweight system built on Appium2, an open-source framework for controlling Android and iOS devices. As shown in Figure 2, A3 acts as a bridge between a GUI agent and an Android device. It integrates tasks with their corresponding evaluation functions. The process begins with the controller retrieving the device's current state, which includes a screenshot and an Extensible Markup Language (XML) file. This state and task instruction, along with additional information such as previous screenshots, XML files, and actions, is then sent to the agent. The agent analyzes the input and predicts the next action to take based on the current state. The predicted action is passed to the translator, which converts it into device control commands to interact with the device. This loop continues until the agent signals task completion or the predefined maximum number of steps is reached. At the end of the process, the evaluator determines whether the task was successfully completed using the evaluation function. The system is designed with flexibility and extensibility, allowing users to easily add new apps and tasks, and it also provides a universal translator system for any agent."}, {"title": "3.2 Action Space", "content": "AITW, AITZ, and AMEX share the same action space: CLICK, SCROLL, TYPE, ENTER, BACK, HOME, COMPLETE, IMPOSSIBLE. In contrast, ANDROIDCONTROL introduces a different action space that includes two additional actions: Open, Long Press and WAIT. The Open action is specifically defined to directly launch an app and the WAIT action means the current state is still loading and needs to wait. However, no existing evaluation system currently supports these additional actions, making it impossible to test agents trained on ANDROIDCONTROL. To address this limitation, we extend A3 to accommodate a larger action space, which contains all the action types of all datasets, ensuring compatibility with agents trained on any dataset."}, {"title": "3.3 Task", "content": "Unlike existing approaches, A3 incorporates over 200 tasks derived from 21 widely used third-party applications, thereby significantly broadening the scope and variety of real-world scenarios. Each task is deliberately chosen to represent the most common functionalities and use cases of a given application. Moreover, every task is distinct, minimizing the repetition of actions and intentions. To better characterize the types of tasks included, we classify them into three categories: (i) operation tasks, (ii) single-frame query tasks, and (iii) multi-frame query tasks."}, {"title": "3.4 Evaluation", "content": "In A3, we present two evaluation methods: (i) a task-specific evaluation function and (ii) a business-level LLM evaluation system. These methods operate independently and can be chosen by users, with the first focusing on predefined tasks and the second offering a scalable solution for adding tasks across various apps. And to mimic the real-world scenario, all tasks are evaluated by real-time states, which means all the contents are real-time, not predefined offline contents."}, {"title": "3.4.1 Evaluation Function", "content": "For over 200 tasks, we pair each task with a corresponding evaluation function. This function is used to assess whether the agent successfully completes the given task through various methods. Since each task involves different actions and goals, the evaluation criteria vary accordingly. The evaluation methods can be broadly categorized into two types:\n\u2022 Element matching is the most commonly used evaluation method. It involves identifying key elements in an XML tree and comparing their attributes with ground-truth values. For instance, consider the task: \"Open Downloads in Coursera and tell me how much storage is used.\" In this case, the final state XML should contain an element that displays the total storage used by the app. The ground-truth value can be extracted by parsing the XML tree and then compared to the agent's response. In more complex scenarios, multiple elements may need to be retrieved, and several conditions must be met to determine if the agent's answer is correct. Additionally, when the XML data is insufficient,"}, {"title": "3.4.2 LLM Evaluation System", "content": "One of the challenges faced by previous online evaluation systems is the difficulty of scaling tasks alongside their corresponding evaluation functions. During the development of A3, we also encountered this issue. Each task requires a specific evaluation function, which must be crafted by coding experts capable of parsing XML and defining precise conditions for task success. This dependency on manual coding hinders the rapid creation of evaluation functions. To address this limitation, we propose a business-level LLM evaluation system that leverages the advanced capabilities of large language models (LLMs) such as GPT and Gemini to enable semi-autonomous or even fully autonomous task evaluation.\nAs a first step, we utilize GPT-40's coding abilities to generate evaluation functions for tasks. Specifically, we supply GPT with our XML parsing code, an example evaluation function, and task-specific prompts to generate the required evaluation functions. While GPT-40 demonstrates strong coding capabilities, it can occasionally produce logic errors or incorrect conditions. For instance, when evaluating hotel searches on Booking.com, the search results dynamically change based on the selected date. However, GPT-40 might incorrectly attempt to match a static value, such as \"113 results,\" instead of using a regex pattern containing the word \"results\" to identify whether the searching process is finished. To quantify GPT-40's performance, Table 2 illustrates its impact on task evaluation. Although the proportion of fully correct functions is relatively low, the percentage of lines requiring human modification is also minimal. This suggests that GPT-40 significantly reduces the coding workload, enabling a more efficient and semi-autonomous evaluation process.\nThe first evaluation method delivers accurate results due to human validation of the evaluation functions. However, it depends on coding experts, which can be a constraint. To overcome this, we propose a fully LLM-based evaluation process. For operation and single-frame query tasks, GPT-40 and Gemini 1.5 Pro are provided with task instructions and the corresponding final state XMLs, enabling them to directly assess task completion. For multi-frame query tasks, a sequence of XMLs is used to evaluate their ability to handle more complex scenarios. Table 3 highlights the evaluation accuracy of both LLMs, demonstrating approximately 80% correctness. To improve reliability, we introduce a cross-validation process that combines the outputs of both LLMs. When the two models produce the same result, the probability of misjudgment decreases to around 0.03, ensuring high confidence. In cases of disagreement, human evaluation is applied. This approach significantly reduces the reliance on human labor compared to the first method while also eliminating the need for coding expertise."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Finetuned Agent", "content": "We train an agent based on InternVL2-8B (Chen et al., 2024) using both the AMEX and ANDROIDCONTROL datasets. The agent is then tested on the ANDROIDCONTROL static frame test set as well as the A3 dynamic arena. Table4 presents the evaluation results on the ANDROIDCONTROL test set, which is divided into four subsets: IDD, category unseen, app unseen, and task unseen. Each static frame evaluation is further categorized into two levels. The high-level evaluation provides only an overall task goal, such as \"Open the profile tab in YouTube,\" while the low-level evaluation includes additional step-wise instructions for the current screen, such as \u201cClick the profile icon at the bottom right of the screen.\" Naturally, low-level tasks are easier than high-level ones, as they offer more specific guidance.\nAlthough the agent performs relatively well on the ANDROIDCONTROL static test set, we observe poor performance in real-world scenarios, as shown in Table 5 and Table 6. Upon analyzing unsuccessful executions in A3, we identify that the history of actions might be a significant factor. (i) Previous studies have demonstrated that action history improves performance in static frame evaluations, as it is derived from ground-truth annotations and provided as input to the agent, which informs subsequent processes. However, in A3, real-world scenarios lack ground-truth action histories, which is a huge domain gap between training data and real-world evaluation. If the agent relies on its own action history, a single mistake can confuse the agent and cascade into a series of errors. (ii) In static evaluations, errors in action predictions do not affect subsequent states or the ground-truth history. However, in real-world evaluation, a wrong action can lead to an incorrect state, which might provide misleading information and requires the agent to self-correct, either by going back or starting over. Unfortunately, our trained agent currently lacks this self-correction capability.\nAnother challenge is the impracticality of information query tasks. This issue arises because no existing dataset provides this type of data during collection, leading the agent to fail in answering any queries."}, {"title": "4.2 Business-level LLM", "content": "We also employ the business-level LLM GPT-40 as an agent in A3. However, as highlighted in the previous Set-of-Mark (SoM) study (Yang et al., 2023), GPT-4o performs poorly when tasked with directly outputting or inputting coordinates. To address this, we apply the SoM technique to help identify element positions and coordinates. While SoM improves GPT-40's performance on CLICK actions, it continues to struggle with other actions, such as SCROLL. Despite these limitations, GPT-40 demonstrates the ability to complete some information query tasks due to its inherent capabilities, even without fine-tuning.\nWe also evaluate the performance of AppAgent (Zhang et al., 2023a), which includes an exploration phase before task automation and articulates its reasoning and planning during execution. These features notably enhance its effectiveness as an agent. AppAgent not only significantly outperforms the original GPT-40 but also surpasses our finetuned agent, likely due to the advanced capabilities of the GPT model underlying AppAgent."}, {"title": "4.3 Error Cases", "content": "To better demonstrate the obstacles that agents encounter in the real-world scenario, we provide some most common error cases during the observation of the evaluation.\n\u2022 Perform CLICK at wrong coordinate. This the most common error case where if the coordinate is wrong, either the screen doesn't change (click at nothing), or it goes to a wrong state, which requires self-correct ability to get back on the track. This type of errors is mainly due to the insensibility of LLMs to digits.\n\u2022 Perform meaningless action. This happens when an agent doesn't know what to do on the current state. For instance, it generates WAIT when the state is fully loaded and requires a CLICK or SCROLL. This is possibly due to the weak planning capability of the agent.\n\u2022 Start typing before the element is selected. We see many cases where the agent starts to type text when the search bar or other element is not clicked or selected. This is possibly due to the inconsistent annotation style in the existing datasets: sometime annotators click the element before typing while sometime they don't if the element is automatically selected.\n\u2022 Can't stop. We also see some cases where the agent already finishes the task but it still performs more actions that ruin the process."}, {"title": "5 Limitations", "content": "\u2022 The integrated tasks and evaluation functions are defined on specific versions of apps, which may lead to different evaluation results on different app versions.\n\u2022 Business-level LLMs such as GPT-40 and Gemini can only evaluate whether the whole task is complete, but cannot judge whether the subgoals are completely in the action chain. This remains to the future work to discover a better way to autonomously evaluate the agent performance other than success rate."}, {"title": "6 Discussion", "content": "\u2022 All datasets are used consistently with the licenses, such as Apache 2.0 of both ANDROIDCONTROL and AMEX.\n\u2022 No existing potential risk is shown during the evaluation."}, {"title": "7 Conclusion", "content": "We introduce Android Agent Arena (A3), a comprehensive and autonomous online evaluation system for GUI agents. A3 incorporates both human-validated task-evaluation pairs and an autonomous LLM-based cross-validation process. The tasks span a wider range of categories and applications, enabling the evaluation of agents' capabilities in both operation execution and information retrieval across three levels of difficulty. The autonomous evaluation process significantly minimizes human intervention and workload, offering a more efficient approach to scaling the number of evaluation tasks."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Task Examples", "content": ""}, {"title": "Easy", "content": "\u2022 Open Yelp and search for pizza nearby.\n\u2022 Search for 'Micheal Jackson' on YouTube."}, {"title": "Medium", "content": "\u2022 Is the book 'Romeo and Juliet' in my wishlist on Wish?\n\u2022 Go to 'Taylor Swift' page and subscribe on YouTube Music."}, {"title": "Hard", "content": "\u2022 Open Booking.com and search for stays in Beijing from Nov 27 to Nov 28. Then sort by price from low to high, and tell me the lowest price.\n\u2022 After sorting the results by distance for nearby BBQ on Google Maps, select the first store, and start navigation."}, {"title": "A.2 Prompts", "content": "The simplified prompt for GPT-40 to generate evaluation function code (Section 3.4.2) is as followed:\nGiven a task and a xml file of the\nscreen, generate an evaluation code\nfor the task to check whether the\ntask is completed correctly. Think\nof the conditions carefully,\nespecailly different elements and\nwhether to use regex expression.\nThe task is '{task}'. And here is the\nxml of the screen: {xml}.\nThe evaluation function should start\nwith the following code:\npython\n:::\nThe evaluation function should return\nTrue if the task is completed\ncorrectly and False otherwise.\nA base xml parser is also provided for\nyou to use:\n:::\npython\nfor example you can use the following\ncode to evaluate the task: 'Search\nMichael Jackson' on YouTube.'\npython"}, {"title": "A.3 Finetuned Agent", "content": "We use the 8B model of InternVL2 as the base model. We train the model on 8 A100 GPU for 27 hours on ANDROIDCONTROL and AMEX, following the default finetuning settings."}, {"title": "A.4 Demonstrations", "content": "Here we provide some demonstrations of errors during evaluation (See Figure 4 and Figure 5). We can see that the agent predicts several steps correctly but if one action is wrong, the agent would fail the task."}]}