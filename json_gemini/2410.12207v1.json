{"title": "DIVIDE-VERIFY-REFINE: ALIGNING LLM RESPONSES WITH COMPLEX INSTRUCTIONS", "authors": ["Xianren Zhang", "Xianfeng Tang", "Hui Liu", "Zongyu Wu", "Qi He", "Dongwon Lee", "Suhang Wang"], "abstract": "Recent studies show that LLMs, particularly open-source models, struggle to follow complex instructions with multiple constraints, hindering their adoption in mission-critical applications. Despite the importance, methods to improve LLMs\u2019 adherence to such constraints remain largely unexplored, and current research focuses primarily on evaluating this ability rather than developing solutions. While a few studies enhance constraint adherence through model tuning, this approach is computationally expensive and heavily reliant on training data quality. An alternative is to leverage LLMs' self-correction capabilities, allowing them to adjust responses to better meet specified constraints. However, this self-correction ability of LLMs is limited by the feedback quality, as LLMs cannot autonomously generate reliable feedback or detect errors. Moreover, the self-refinement process heavily depends on few-shot examples that illustrate how to modify responses to meet constraints. As constraints in complex instructions are diverse and vary widely (e.g., text length, number of bullet points, or inclusion of specific keywords), manually crafting few-shot examples for each constraint type can be labor-intensive and sub-optimal. To deal with these two challenges, we propose the Divide-Verify-Refine (DVR) framework with three steps: (1) Divide complex instructions into single constraints and prepare appropriate tools; (2) Verify: To address the feedback quality problem, these tools will rigorously verify responses and provide reliable feedback (e.g., Python scripts for format checking or pre-trained classifiers for content analysis); (3) Refine: To address the constraint diversity challenge, we design a refinement repository that collects successful refinement processes and uses them as few-shot demonstrations for future cases, allowing LLMs to learn from the past experience during inference. Additionally, recognizing that existing datasets lack complexity and have internal conflict, we develop a new dataset of complex instructions, each containing 1-6 constraints. Experiments show that the framework significantly improves performance, doubling LLama3.1-8B's constraint adherence and tripling Mistral-7B's performance on instructions with 6 constraints. The code and dataset are available at https://anonymous.4open.science/r/CODE_ICLR2025-52CE/README.md.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs), like ChatGPT, have shown significant improvements across a variety of language tasks (Ouyang et al., 2022; Touvron et al., 2023). The success of LLMs relies on their instruction-following ability to comprehend and execute complex instructions. Misinterpretations or failures to follow instructions can result in unintended outputs, which may have severe consequences (Mu et al., 2023; Zhou et al., 2023). This issue becomes particularly critical when LLMs are deployed as in high-stakes environments, such as legal documentation or technical writing. For example, when drafting legal contracts, LLMs must strictly adhere to constraints related to format, specific terminology, and precise language usage to avoid misinterpretations or legal liabilities. Similarly, in technical writing, adhering to strict format guidelines, word limits, and the inclusion of essential technical terms is critical to ensure clarity, consistency, and compliance with industry standards."}, {"title": "RELATED WORK", "content": "Instruction-Following of LLMs. Since the ability to follow instructions is crucial for the practical use of LLMs, many recent studies evaluate this capability from various perspectives (Dubois et al., 2024; Zhou et al., 2023; Jiang et al., 2024b; Chen et al., 2024b; Zhou et al., 2023; He et al., 2024b). They evaluate LLMs' instruction-following ability by testing on length (Dubois et al., 2024), format (Zhou et al., 2023), semantic and topic constraints (Chen et al., 2024b). Most works only test LLMs on simple instructions with only 1-2 constraints. Recently, some works generate instructions with multiple constraints (He et al., 2024a; Jiang et al., 2024b). They find that LLMs struggle to follow complex instructions as the number of constraints increases. Moreover, there is a big performance gap between the open-source models and the closed-source models on instruction- following. Upon finding these problems, some seminal works aim to improve instruction-following the ability of LLMs (Chen & Wan, 2023; Sun et al., 2024; Wang et al., 2024; He et al., 2024a). They use various prompting strategies to generate instructions and responses with advanced LLM models (e.g., GPT4) and then use the generated data to fine-tune open-source LLMs. Though most methods only consider instructions with few constraints, one of them (He et al., 2024a) focuses on improving the LLMs' ability to follow multiple constraints. They generate complex instruction datasets by merging instructions with external constraints. Then, they adopt GPT4 (teacher model) to modify the response of the open source model (student model) iteratively. The student model is finetuned by both the intermediate modification process and the final modified response. Although fine-tuning is an effective approach, it usually requires a large number of computation resources and heavily de- pends on the data quality. In addition, the fine-tuned model will still suffer from new constraints not seen before. Different from previous methods, our framework uses in-context learning with tool interaction to effectively identify and rectify unsatisfactory responses using the LLM itself, which is more practical and accessible.\nSelf-Correction of LLMs. Self-correction is a framework where LLMs refine their own responses during inference by reflecting on their initial responses (Shinn et al., 2024; Madaan et al., 2024). This process can be divided into two phases. Initially, LLMs are prompted to analyze and provide feedback on their own responses. Subsequently, based on the feedback LLMs refine the responses to correct their mistakes. However, recent studies report negative results indicating that LLMs cannot self-correct their own mistakes (Hong et al., 2024; Tyen et al., 2024; Kamoi et al., 2024; Gou et al., 2024). For example, Kamoi et al. (2024) reveal that top LLMs like GPT-4 and Claude 3 have low recall in detecting LLM errors, with LLMs significantly underperforming compared to humans."}, {"title": "THE PROPOSED FRAMEWORK: DVR", "content": "As shown in Fig. 2, we propose the Divide-Verify-Refine (DVR) framework which consists of three modules: (a) Divide instructions and prepare tools accordingly, (b) Verify responses and provide feedback, and (c) Refine and store responses in a repository. First, The tool preparation module aims to identify constraints, select appropriate tools, and fill out parameters. In this module, LLMs first decompose the complex instructions into single constraints. For each single constraint, the LLMs will prepare appropriate tools for verification. Second, in the verification and feedback module, the prepared tools will verify the response and give detailed feedback if the response does not adhere to the constraint. Third, in the self-refinement module, given the feedback and past refinement experience for the same constraint as few-shot examples, the response is refined to adhere to the target constraint. The successfully refined response will be stored in the refinement repository so that it can be retrieved as refinement examples in the future. Next, we introduce each model in detail."}, {"title": "DIVIDE: TOOL PREPARATION", "content": "To provide accurate feedback, we propose to adopt tools for verification. To enable LLMs to use external tools, we construct tools for different types of constraints. We build Python verifiers for structural constraints (e.g., the number of words, the number of paragraphs, or the number of bullet points). To handle constraints where existing tools are unavailable, we can request advanced code- generation models such as DeepSeek-Coder (Zhu et al., 2024) or Code-Llama Roziere et al. (2023) to generate required tools. We also adopt existing classifiers as verifiers for content constraints (e.g., the topic classifier (Antypas et al., 2022) and the sentiment classifier (Loureiro et al., 2022)).\nGiven an input instruction I, the LLM M first decomposes it into a series of individual constraints. We use a decomposition prompt Pdecomp asking LLMs for decomposition. With input instruction and decomposition prompt, LLM then generates a set of decomposed constraints: M(pdecomp, I) \u2192 {Ci}i=1,2,3..., where c\u2081 is the i-th single constraint. For each constraint ck, the LLM determines the appropriate tool by matching ck to a tool tk from the predefined toolset: M(pselect, Ck) \u2192 tk, where tk \u2208 {ti}i=1,2,3... is the selected tool for the constraint ck. The prompts for decomposition Pdecomp and tool selection Pselect are in Table 15 in Appendix A.9. After selecting the tools, the LLM sets the necessary parameters for each tool, such as specifying the required number of bullet points or the desired sentiment for the response. Finally, all tools relevant to instruction I are compiled into the set T\u2081 = {ti}i=1,2,3..., ready to be utilized in the subsequent verification and feedback phase."}, {"title": "VERIFY: VERIFICATION AND FEEDBACK", "content": "Given the instruction, the LLM will first generate the initial response R0 = M(pgenerate, I), where Pgenerate is the prompt for generation (detailed in Appendix A.9). We denote the current response as R and R = Ro for the first round of refinement and will be updated to the refined response in subsequent rounds. The current response is verified by each tool in toolset T\u2081 as follows:\n$f_i = t_i(R), \\forall t_i \\in T_I$ (1)\nwhere fi is the feedback from tool ti for constraint ci. If the response adheres to the constraint, the feedback is a boolean value \"true\". Otherwise, fi is a textual feedback that first identifies the error in the response and then suggests modification. For example, as shown in Fig. 2, the tool \"Bullet_points(4)\" counts the number of bullet points in the response and outputs \"true\" if there are 4 bullets; while the response only contains 2 bullets. It finds that the response does not satisfy the constraint and gives out the feedback \u201cThe response only contains 2 bullet points. 2 more bullet points should be added.\" This detailed feedback points out the errors in the response and gives directional information for LLMs to modify the response. We collect all feedback F\u2081 = {fi}i=1,2,3... which will be used to refine the response R."}, {"title": "REFINE: SELF-REFINE WITH FEEDBACK AND FEW-SHOT DEMONSTRATION", "content": "In the self-refinement phase, the LLM leverages the feedback collected to refine the response. To improve the performance, we propose to adopt representative demonstrations in the prompt to instruct LLMs on how to conduct refinement using feedbacks. As constraints vary widely, each type of constraint requires specific demonstrations for effective refinement. Manually creating few-shot examples for each constraint type is labor-intensive and impractical for real-world applications. To solve this issue, we propose to store the successful refinement process in the refinement repository. When LLMs need to refine a new response involving the same constraint type, the few-shot examples can be retrieved from the refinement repository as in-context examples.\nSpecifically, the refinement process targets one unsatisfied constraint at a time, cycling through a refine-verify-refine loop until all constraints are satisfied. For a given response R and the feedback f \u2208 F1, f #True, the refinement response can be written as follows:\nR' = M(Prefine, st, I, R, f) (2)\n= where Prefine is the prompt for refinement (detailed in Appendix A.9), st {(Ii, Ri, fi, Ri)}i=1,2,3... is the set of refinement examples selected from the refinement repository Q, which contains refinement examples having the same constraint type associated with f. There might be many refinement examples having the same constraint type with f available in"}, {"title": "EMPIRICAL VALIDATION", "content": "In this section, we conduct experiments to answer the following research questions: (RQ1) Can our DVR improve the ability of LLMs to follow complex constraints? (RQ2) How does the performance of LLMs differ across various types of constraints, and which constraints pose the greatest challenges? (RQ3) How does each module of DVR (the tool-assisted verification and the few-shot self-refinement library) individually contribute to improving LLMs' ability to follow constraints?"}, {"title": "EXPERIMENTAL SETUP", "content": "Datasets. We conduct experiments on two datasets: (i) We conduct experiments on CoDI (Control- lable Generation under Diversified Instructions) (Chen et al., 2024b). It has 500 instructions with 2 constraints. Each instruction has a topic constraint and a sentiment constraint. (ii) ComplexIn- struct: Since the complexity of CoDI is limited, we construct a new complex instruction datasets called ComplexInstruct. We use CoDI (topic instruction set) (Chen et al., 2024b) as seed instruc- tions which ask users to generate text on certain topics. Some instructions ask users to generate \"a paragraph of...\" or \"a sentence of...\" which already contain length constraints. To avoid con- flicts and hidden constraints, we remove these constraints by replacing the keywords \"paragraph\" and \"sentence\" with \"text\". Then, we synthesize complex instructions by adding constraints to these seed instructions (Zhou et al., 2023). To simulate instruction of different levels, we generate 6000 complex instructions with 1-6 constraints for each instruction as 6 levels (1000 instructions for each level). We have 21 types of constraints categorized into 8 general categories (such as length constraint, punctuation, and case change). Each type of constraint is diversified into 8 different expressions. The detailed information about the constraint types is in Appendix A.3 and Table 6.\nBaselines. We compare our method with representative and state-of-the-art baselines, which can be categorized into three main types: (i) Self-reflection based methods, which iteratively improve response via feedback from LLMs reflection, such as Reflexion (Shinn et al., 2024); (ii) Prompting based methods, which use different prompting strategies to get the best response, including Branch- solve-Merge (BSM) (Saha et al., 2024) and Universal Self-Consistency (U-SC) (Chen et al., 2024a); and (iii) Tool based methods, which use external tools for feedback or selection, such as Rejection sampling (Saunders et al., 2022), React (Yao et al., 2023), and CRITIC (Gou et al., 2024). For the refinement repository of our framework, we consider two variants, i.e., warm-start and cold-start. For warm-start, we have an additional set of instructions (6000 samples for ComplexInstruct and 500 samples for CoDI). Note that these data samples are totally independent with test set. Our framework will first run on these samples to collect examples to fill the refinement repository. For cold-start, since the refinement repository is empty at beginning, we use 5 fixed few-shot examples"}, {"title": "RQ1: ASSESSING THE CONSTRAINT-FOLLOWING ABILITY", "content": "To answer RQ1, we evaluate our framework on two datasets. We evaluate structural constraints (e.g., text length, number of sections, and bullet points) on ComplexInstruct and content constraints (e.g., topic and sentiment constraints) on CoDI respectively.\nFor ComplexInstruct, there are six difficulty levels. Each level corresponds to the number of con- straints in the instruction. For example, each instruction in Level 3 contains three constraints. There are 1000 instructions for each level. Results are shown in Table 1 and Table 2 (more results in Appendix A.4). (i) Single constraints vs Multi-constraints: For instructions in different difficulty levels, responses to instructions with more constraints tend to have a lower satisfaction rate. The satisfaction rate of instructions at Level 1 can approach close to 100%. However, at Level 6, the satisfaction rates for models like Llama3.1-8B and Mistral-7B drastically fall to 25% and 6.3%, re- spectively. This indicates that LLMs struggle to satisfy instructions with multiple constraints even though LLMs can satisfy them individually. (ii) Self-Reflection is unreliable: We can observe that Reflxion (Shinn et al., 2024) where LLMs reflect on and self-correct their responses, provides little improvement over Vanilla. This result indicates that LLMs themselves can not effectively iden- tify their errors in constraint-following tasks. Similarly, Universal Self-consistency (Chen et al., 2024a), which allows LLMs to choose the most consistent answers from a set of candidates, has"}, {"title": "RQ2: COMPARISON ACROSS DIFFERENT CONSTRAINT TYPES", "content": "Comparison across different constraint types is shown in Table 4 (warmstart). Coldstart results are provided in Table 9 in Appendix A.4. The 21 constraints in ComplexInstruct are categorized into 8 general categories as shown in the table. We have the following observations. (i) We can find that length constraints are the most challenging constraints for every language model. Length constraints have three levels: a minimum or maximum word count, a minimum or maximum sentence count, and an exact number of paragraphs. The reason might be that there is a lack of instructions containing length constraints during the instruction-tuning process. As a result, the language model struggles to understand the relationship between the output and the specified length in the instruction. Moreover, LLMs must plan from the beginning of the generation process to not only meet the length constraint but also ensure that the response remains complete and coherent. (ii) Language constraints, which"}, {"title": "RQ3: CONTRIBUTION OF INDIVIDUAL MODULES", "content": "In addition to comparing our method with existing baselines, we conduct an ablation study to assess the effectiveness of individual modules. Specifically, we investigate three variants: (i) w/o Detailed Feedback: The detailed feedback from the tool is removed but the refinement repository is kept to provide relevant few-shot examples showing the responses before and after refinement. Here, the refinement repository is empty in the beginning (coldsart). (i) w/o Repository: The refine- ment repository is removed, and only 5 fixed examples are used for the self-refine process. (i) w/o both: The refinement repository and detailed feedback are all removed. Tools only give whether the whole instruction is satisfied. Figure 4 shows performance gaps between each method and the Vanilla. Both detailed feedback and the refinement repository are crucial. Without the repository, performance gains are limited, as fixed few-shot examples aren't optimal for each refinement target. Detailed feedback is also important for LLMs, because it locates the error and provides the direc- tion for LLMs to modify their responses. Llama3-8B and Llama3.1-8B show higher gains on more difficult instructions. Mistral-7B's gains are modest, because of the limited capacity of Mistral-7B in following complex instructions, it can only follow 6.3% of level 6 instructions (shown Table 2). Despite this, the gains are notable from its low starting point. In conclusion, both detailed feedback and the refinement repository contribute to the performance of our framework."}, {"title": "HYPER-PARAMETER SENSITIVITY ANALYSIS", "content": "We also conduct a hyper-parameter sensitivity analysis of our framework, testing different numbers of refinement few-shots and trials for successful refinement on Llama3.1-8B. As shown in Figure 5, performance improves with more trials but saturates at five, with minimal gains beyond that. Similarly, increasing the few-shot examples boosts performance in the beginning. The performance saturates after 8 shots. On the CoDI dataset, performance improves rapidly with initial increases in trials and examples, indicating that the first few numbers of trials and few-shot examples are most effective for refining the response."}, {"title": "TOOL SELECTION ACCURACY", "content": "Correctly decomposing and selecting tools are essential for feedback and refinement. We define tool selection as a multi-label prediction task for LLMs, evaluated using hamming score, accuracy, preci- sion, recall, and F1-score. The total number of tools is 21. Results are shown in Table 5. Hamming loss, which measures the fraction of incorrect labels, is low across all models, indicating minimal mispredictions. Every model demonstrates a very high precision score, meaning that the tools they select are mostly correct, avoiding misleading feedback with incorrect tool selection. Accuracy, which measures the exact match between the selected tools and the ground truth, is the strictest metric. Despite this, all models achieve over 50% accuracy. Considering the limited performance of these models on constraint-following tasks, tool selection is a relatively easier task for LLMs. This performance gap makes it possible for our method to provide reliable feedback, collect past refinement examples and be effective in improving LLMs' constraint-following ability."}, {"title": "WOULD DVR AFFECT COMPREHENSIBILITY AND FLUENCY OF RESPONSES", "content": "In this subsection, we investigate if our framework would sacrifice comprehensibility and fluency in order to follow complex-constraints. We focus on evaluating key metrics such as readability, perplexity, and coherence. These metrics assess the comprehensibility and fluency of the responses. Results on ComplexInstruct (Table 11) and CoDI (Table 12) are in Appendix A.6. They both show that our framework has performance comparable to those of Vanilla, indicating that it does not degrade fluency and readability. The reason is that our method does not change any weights in LLMs, which maintains their ability in generating fluent and comprehensible text."}, {"title": "CONCLUSION", "content": "In conclusion, this paper presents the Divide-Verify-Refine (DVR) framework to enhance LLMs' ability to follow multi-constraint instructions. There are three steps in our framework: (1) Di- vide complex instructions into single constraints and assign appropriate tools for each constraint. (2) Verify: To deal with the feedback quality problem, these tools rigorously verify the response and generate reliable feedback. (3) Refine: To tackle the constraint diversity challenge, we design the refinement repository to store successful refinement processes, allowing LLMs to retrieve and learn from past examples. Our framework improves LLMs' adherence to complex multi-constraint instructions without the need for retraining, offering a scalable solution to enhance the practical usability of LLMs in real-world applications. Additionally, we construct a new dataset free from hidden or conflicting constraints, providing a more comprehensive and accurate evaluation of LLM performance on multi-constraint instructions."}, {"title": "ALGORITHM", "content": "To provide accurate feedback, we propose to adopt tools for verification. To enable LLMs to use external tools, we construct tools for different types of constraints. We build Python verifiers for structural constraints (e.g., the number of words, the number of paragraphs, or the number of bullet points). To handle constraints where existing tools are unavailable, we can request advanced code- generation models such as DeepSeek-Coder (Zhu et al., 2024) or Code-Llama Roziere et al. (2023) to generate required tools. We also adopt existing classifiers as verifiers for content constraints (e.g., the topic classifier (Antypas et al., 2022) and the sentiment classifier (Loureiro et al., 2022)).\nGiven an input instruction I, the LLM M first decomposes it into a series of individual constraints. We use a decomposition prompt Pdecomp asking LLMs for decomposition. With input instruction and decomposition prompt, LLM then generates a set of decomposed constraints: M(pdecomp, I) \u2192 {Ci}i=1,2,3..., where c\u2081 is the i-th single constraint. For each constraint ck, the LLM determines the appropriate tool by matching ck to a tool tk from the predefined toolset: M(pselect, Ck) \u2192 tk, where tk \u2208 {ti}i=1,2,3... is the selected tool for the constraint ck. The prompts for decomposition Pdecomp and tool selection Pselect are in Table 15 in Appendix A.9. After selecting the tools, the LLM sets the necessary parameters for each tool, such as specifying the required number of bullet points or the desired sentiment for the response. Finally, all tools relevant to instruction I are compiled into the set T\u2081 = {ti}i=1,2,3..., ready to be utilized in the subsequent verification and feedback phase.\nGiven the instruction, the LLM will first generate the initial response R0 = M(pgenerate, I), where Pgenerate is the prompt for generation (detailed in Appendix A.9). We denote the current response as R and R = Ro for the first round of refinement and will be updated to the refined response in subsequent rounds. The current response is verified by each tool in toolset T\u2081 as follows:\n$f_i = t_i(R), \\forall t_i \\in T_I$ (1)\nwhere fi is the feedback from tool ti for constraint ci. If the response adheres to the constraint, the feedback is a boolean value \"true\". Otherwise, fi is a textual feedback that first identifies the error in the response and then suggests modification. For example, as shown in Fig. 2, the tool \"Bullet_points(4)\" counts the number of bullet points in the response and outputs \"true\" if there are 4 bullets; while the response only contains 2 bullets. It finds that the response does not satisfy the constraint and gives out the feedback \u201cThe response only contains 2 bullet points. 2 more bullet points should be added.\" This detailed feedback points out the errors in the response and gives directional information for LLMs to modify the response. We collect all feedback F\u2081 = {fi}i=1,2,3... which will be used to refine the response R.\nIn the self-refinement phase, the LLM leverages the feedback collected to refine the response. To improve the performance, we propose to adopt representative demonstrations in the prompt to instruct LLMs on how to conduct refinement using feedbacks. As constraints vary widely, each type of constraint requires specific demonstrations for effective refinement. Manually creating few-shot examples for each constraint type is labor-intensive and impractical for real-world applications. To solve this issue, we propose to store the successful refinement process in the refinement repository. When LLMs need to refine a new response involving the same constraint type, the few-shot examples can be retrieved from the refinement repository as in-context examples.\nSpecifically, the refinement process targets one unsatisfied constraint at a time, cycling through a refine-verify-refine loop until all constraints are satisfied. For a given response R and the feedback f \u2208 F1, f #True, the refinement response can be written as follows:\nR' = M(Prefine, st, I, R, f) (2)\n= where Prefine is the prompt for refinement (detailed in Appendix A.9), st {(Ii, Ri, fi, Ri)}i=1,2,3... is the set of refinement examples selected from the refinement repository Q, which contains refinement examples having the same constraint type associated with f. There might be many refinement examples having the same constraint type with f available in"}, {"title": "BASELINE DETAILS", "content": "\u2022 Reflexion (Shinn et al., 2024): This method allows LLMs to self-reflect on their own re- sponses and provide valuable feedback for future outputs. With the feedback, LLMs will refine their responses.\n\u2022 Branch-solve-Merge (BSM) (Saha et al., 2024): BSM uses a \"Divide and Conquer\" ap- proach to break complex instructions as individual branches. Then the LLMs will merge the responses from branches as the final answer. Similarly, in our experiment, we use LLMs to generate a response for each single constraint and then merge them together.\n\u2022 Universal Self-Consistency (U-SC) (Chen et al., 2024a): This study extends the idea of Self-Consistency (Wang et al., 2023) to free-form generation. It first generates several candidate responses and then asks LLMs to select the most consistent one.\n\u2022 Rejection Sampling (Saunders et al., 2022): Since we have tools for reliable verification, the most simple method is to select the best one from a set of responses. Here, we give the maximum number of trials as 5.\n\u2022 ReAct (Yao et al., 2023): In ReAct, LLMs take actions based on the observation of the environment. Here, we adopt this method by letting the tools as the environment and giving LLMs boolean signals indicating whether the generated response adheres to all constraints in the instruction."}, {"title": "COMPLEXINSTRUCT", "content": "We have 21 types of constraints which can be divided into 8 general categories (Zhou et al., 2023):\n\u2022 Keywords:\n(1) Include keyword,\n(2) Include keyword at least/less than certain frequency,\n(3) Forbidden word,\n(4) At least/less than certain frequency of letters.\n\u2022 Length:\n(1) At least/less than certain number of words,\n(2) At least/less than certain number of sentences,\n(3) Exact number of paragraphs.\n\u2022 Detectable Content:\n(1) postscript,\n(2) Exact number of placeholders.\n\u2022 Detectable Format:\n(1) Number of bullet points,\n(2) Add title,\n(3) Answer from options,\n(4) Minimum of highlighted sections,\n(5) Json format.\n\u2022 Change Cases:\n(1) All uppercase,\n(2) All lowercase,\n(3) At least/less than certain number of all-capital words.\n\u2022 Startend:\n(1) End the text with a certain sentence,\n(2) Wrap whole response in double quotation.\n\u2022 Punctuation:\n(1) No commas in response.\n\u2022 Language:\n(1) Respond with certain language."}, {"title": "DETAILED EXPERIMENTS", "content": "We can observe that our methods consistently outperform baselines on differen LLMs."}, {"title": "LLM SELF-VERIFY ABILITY", "content": "We evaluate LLMs on their ability to verify whether the responses meet the given constraints. As shown in Table 10, LLMs often fail to verify their outputs accurately, suggesting they lack the capacity to provide reliable feedback or reflection on their own responses."}, {"title": "FLUENCY AND READABILITY", "content": "Here, we assume that we have tools for all existing constraints. However, users' requirements can be very diverse and we would not have certain tools for new constraints.\nAs a result, we can consider the following two future works. The first one is to handle constraints with dependency. If LLMs can correctly disentangle these constraints and find appropriate tools, our framework can also enhance LLMs' instruction-following ability in such a complex setting. The second future work is to deal with new constraints. One possible solution is to firstly find out if the constraint can be tested via python code. Then local LLMs can summarize constraints into code requirements and send them to advanced code generators for tool generation. Generated tools can also be saved locally for future use."}, {"title": "CASE STUDY", "content": "We use some examples to show how LLMs modify responses with the guidance of feedback. The red colored content is the errors from the original response and blue colored content is the corrected parts after refinement."}, {"title": "PROMPTS", "content": "I'm looking for text that explores arts or culture, can you assist? please explicitly add a note starting with P.S. There should be exactly 2 paragraphs in your response, separated by the markdown divider: ***. Make sure to include at least 2 placeholder represented by square brackets, such as [address], [name]. Highlight at least 2 text sections, i.e. *highlighted section*. There should be no commas in your reply.\nArt has the power to bring people together and transcend cultural boundaries. It can evoke emotions and spark conversations that might not be possible through other means. *At the [address] museum, visitors can experience this firsthand by exploring the diverse collection of art from around the world.*\n*** From paintings to sculptures to installations, each piece tells a unique story that can be inter- preted in many ways. *The work of [name] is a great example of this, as it challenges viewers to think critically about the world around them.* Whether you're an art enthusiast or just looking for a new perspective, the [address] museum is a must-visit destination. P.S. Don't forget to check out the museum's events calendar for upcoming exhibitions and performances!\nIt does not satisfy the constraint: There should be no commas in your reply.\nThe response contains 4 comma(s). Here are the detected commas: (museum, visitors) (tallations, each ) ( of this, as it) (perspective, the [address).\nPlease remove all commas.\nArt has the power to bring people together and transcend cultural boundaries. It can evoke emotions and spark conversations that might not be possible through other means. *At the [address] museum visitors can experience this firsthand by exploring the diverse collection of art from around the world.*\nFrom paintings to sculptures to installations each piece tells a unique story that can be inter- preted in many ways. *The work of [name] is a great example of this as it challenges viewers to think critically about the world around them.* Whether you're an art enthusiast or just looking for a new perspective the [address] museum is a must-visit destination. P.S. Don't forget to check out the museum's events calendar for upcoming exhibitions and performances!\ncurrent prompt\nOriginal Response: current response\nIt does not satisfy the constraint: current unsatisfied constraint\ncurrent feedback"}, {"title": "FAILURE CASES", "content": "Analyze these failure cases and find out two typical situations that our framework would fail:\n1) Combined Effect: In certain scenarios, multiple constraints can create a challenge for the LLMs to balance. As shown in the first example in the Table 16, in the case of limiting the frequency of the letter 't' while generating content about sports, the natural occurrence of the letter 't' in 'sports', combined with 2 sections requirements, makes it difficult to satisfy 't' letter frequency limit. Although these constraints are not inherently contradictory, their combined effect puts the model under pressure, resulting in failure to meet some requirements. (2) Incorrect decomposition and wrong tool selection: Incorrect decmoposition would lead to wrong tool selection. As shown in the second example in the Table 16, the constraint to add capitalized stress words fewer than four times was mistakenly decomposed into two separate tasks. This leads to the incorrect selection of tools, taking the \"stress words\" as keywords instead of capitalized words. This type of failure degrades the overall performance in following complex instructions."}]}