{"title": "REINFORCEMENT LEARNING CONSTRAINED BEAM SEARCH FOR\nPARAMETER OPTIMIZATION OF PAPER DRYING UNDER\nFLEXIBLE CONSTRAINTS", "authors": ["Siyuan Chen", "Hanshen Yu", "Jamal Yagoobi", "Chenhui Shao"], "abstract": "Existing approaches to enforcing design constraints in Reinforcement Learning (RL) applications\noften rely on training-time penalties in the reward function or training/inference-time invalid action\nmasking, but these methods either cannot be modified after training, or are limited in the types of\nconstraints that can be implemented. To address this limitation, we propose Reinforcement Learning\nConstrained Beam Search (RLCBS) for inference-time refinement in combinatorial optimization\nproblems. This method respects flexible, inference-time constraints that support exclusion of invalid\nactions and forced inclusion of desired actions, and employs beam search to maximize sequence\nprobability for more sensible constraint incorporation. RLCBS is extensible to RL-based planning\nand optimization problems that do not require real-time solution, and we apply the method to optimize\nprocess parameters for a novel modular testbed for paper drying. An RL agent is trained to minimize\nenergy consumption across varying machine speed levels by generating optimal dryer module and air\nsupply temperature configurations. Our results demonstrate that RLCBS outperforms NSGA-II under\ncomplex design constraints on drying module configurations at inference-time, while providing a\n2.58-fold or higher speed improvement.", "sections": [{"title": "1 Introduction", "content": "Many games and optimization problems feature complex rules or design constraints that limit how Reinforcement\nLearning (RL) agents interact with the environment. In recent literature, there exist two common approaches to ensure\ncompliance with these constraints: (1) Imposing penalties in the reward function when the RL agent violates a design\nconstraint during training [Yang et al.], and (2) \u201cMasking out\u201d invalid actions at training and/or inference-time [Vinyals\net al., Ye et al.] by setting the log-probabilities of invalid actions to -\u221e for a policy-based RL agent operating on\ndiscrete action spaces. However, both approaches have limitations in real-world scenarios.\nThe first approach, integrating constraints in the reward function, offers good flexibility in constraint types, as it is\npossible to penalize undesired actions and promote desired ones through the reward function. However, this implies that\nall constraints are to be encoded in the reward function before training, and changes in design constraints post-training\ncan lead to suboptimal solutions, with the RL agent either not exploring as close to the constraint boundary as possible\nor consistently violating more stringent constraints. Consequently, changes in design constraints necessitate fine-tuning\nor retraining the model with an updated reward function. This behavior hinders real-world applications, which will"}, {"title": "2 Method", "content": ""}, {"title": "2.1 Background", "content": ""}, {"title": "2.1.1 Beam Search for RL in Combinatorial Optimization", "content": "For a stochastic RL policy \u03c0 parameterized by weights @ and with a discrete action space A, given the current state of\nthe environment st at timestep t, the policy network outputs a categorical distribution across the action space:\n$\\\u03c0\u03bf(St) = {pe(ast)\u2200a \u2208 A}.$\n(1)\nThe action for the current timestep at can be sampled from the categorical distribution. The most popular method for\naction sampling at inference-time is greedy search. Specifically, the most favorable action for current timestep \u00e2t given\nthe observed state st is found by greedily selecting the most preferable action from the statistical distribution over the\naction space predicted by the actor:\n$\\\u00e2t = arg max\u03c0\u03bf(\u03b1, st) = arg maxa Apo(alst).$\n(2)\nSGBS [Choo et al.] and RLGBS drawn an analogy between RL models and LLMs showed that it is possible to apply\nbeam search, a popular approach for natural language generation, to RL action generation, provided that the simulation\nenvironment is deterministic and that real-time performance is not necessary. Through beam search, the method\nsacrifices real-time performance, but allowed for parallel exploration of multiple action sequence hypotheses. At the end\nof the beam search procedure, favorable hypotheses, as characterized by high cumulative product of action-probabilities:\n$\\a1:T = arg maxat\u2208A][Po(at|A1:t\u22121, 81:t), 01:0 = 0,$\n(3)\ncan be evaluated and selected based on the final cumulative episodic reward. Compared to greedy search, SGBS and\nRLGBS often result in higher reward with a relatively efficient polynomial-time complexity in the exponential search\nspace. The main difference between SGBS and RLGBS is that SGBS operates on problems with dense reward, while\nRLGBS operates on sparse reward problems (rt = 0 until reaching the stopping condition). As a result, SGBS takes\nadvantage of dense reward and prune beam candidates based on actual cumulative reward, while RLGBS relies purely\non the beam score for beam candidate selection."}, {"title": "2.1.2 Constrained Beam Search", "content": "The earliest example of constrained beam serach was proposed in [Anderson et al.] for image captioning, where a\nrecurrent neural network (RNN) was tasked with generating short discriptive sentences for images. In order to ensure\npotentially unseen objects in the image are correctly included in the caption, the authors used a convolutional neural\nnetwork (CNN) to perform object detection on the image before caption generation. The detected image tags are then\nenforced as disjunctive constraints during the search. A disjunctive constraint is a bundle of lexical constraints that can\nbe fulfilled by fulfilling just one of the member lexical constraint. For example, the constraint C\u2081 = {'desk', 'table'}\nis considered fulfilled when either 'desk' or 'table' appears in the generated text. Constraint enforcement is done by\nconstructing a finite-state machine (FSM) that enumerates all possible paths in satisfying all constraints, and this results\nin a FSM whose number of states grows exponentially to the number of constraints. At generation time, a grouped\nbeam search algorithm is used, where each group contains at most nu beams whose overall constraint fulfillment status\ncorresponds to a state in the FSM. As beam candidates make progress in constraint fulfillment, they are moved to the\ncorresponding beam group, and finally only beam candidates from the beam group that corresponds to \"all constraints\nfulfilled\" can be added to the finished hypotheses. Overall, the time complexity of the proposed CBS algorithm is\nO(T\u266d2C), where T is the length of the sentence, no is the number of beams and C is the number of constraints.\nA faster alternative to CBS, named grid beam serach, was proposed in [Hokamp and Liu] for machine translation\nproblems. This method studies non-disjunctive lexical constraints containing single words or phrases that must be\nfulfilled during text generation. This simplification of disjunctive constraints leads to a clearly defined \"length\" for\neach constraint. The authors constructed a grid with timestep t as x-axis and constraint fulfillment status c as y-axis\nto organize lexically constrained decoding. The grid beam search algorithm maintains C + 1 banks/rows in the grid,\nwith each bank containing up to no beams. Every beam starts from the bottom-left of the grid and can either move\nup by opening/continuing a constraint, or move right by generating other tokens that do not contribute to constraint\nfulfillment. It should be noted that once a constraint is started, the beam is forced to continue it without the option\nof abandoning a constraint and possibly starting later. Finally, only beams that have reached the top bank of the grid\n(fulfilled all constraints) are added to the finished hypotheses. The time complexity of grid beam search is O(T\u044c\u0421)."}, {"title": "2.2 Reinforcement Learning Constrained Beam Search", "content": "As the name suggests, RLCBS is CBS applied to action generation with RL models. We adopted BS and CBS source\ncode from the GenerationMixin module of Huggingface transformers [Wolf et al.], an open source library for natural\nlanguage processing with large language models. Huggingface transformer's CBS implementation is functionally\nequivalent to Vectorized DBA [Hu et al.]."}, {"title": "2.2.1 RL Adaptation", "content": "Figure 1 shows the schematic of the RLCBS algorithm. Although RLGBS is largely based on CBS for NLG, critical\ndifferences exist between action generation with an RL model and text generation with an LLM. While LLMs can\ngenerate text in a fully autoregressive manner, an RL agent relies on the simulation environment to predict the next states\nand continue the trajectory. For this purpose, a copy of the simulation environment serves as an oracle for sequence\ncontinuation and determining the stopping condition once the next-actions have been selected by CBS. During CBS,\nwe do not use reward feedback from the oracle and rely solely on the RL policy output for selecting beams to keep.\nHowever, once a beam candidate has been confirmed completed by the oracle simulation environment and added to the\ncompleted hypotheses, the final beam hypothesis to return is selected based on actual cumulative episodic reward as\ncalculated by the simulation environment."}, {"title": "2.2.2 Negative Constraints", "content": "Negative constraints (also referred to as preceding constraints in some literature) in RLCBS are handled by setting\nthe desired action's log-probability (logit) to -\u221e in logits processors. In NLG, negative constraints are often used to\nexclude certain \u201cbad words\" or unwanted phrases from appearing in the generated text. In RL, negative constraints can\nbe used to mask invalid actions or to set an upper limit of the number of times a certain class of action can appear in\nthe generated sequence. The logits processor can be considered a more versatile solution to invalid action masking in\nMaskable PPO [Tang et al.], as apart from masking unwanted actions, it is also possible to alter the action distribution\nto promote certain actions similar to guided beam search in NLG. We also use logits processors to renormalize logits\nafter applying any negative constraint so that the probabilities of unmasked tokens sum to 1 after setting the invalid\naction logits to -\u221e."}, {"title": "2.2.3 Positive Constraints", "content": "Examples of positive constraints include \u201cat least n actions of certain type must appear in the generated sequence\"\nor \u201cexactly n actions of certain type must appear in the generated sequence\u201d. This type of constraints is difficult to\nreliably enforce through logits manipulation, as it is unclear at what stage of the episode the constraint should be\nfulfilled, and na\u00efvely pushing progress of constraints at arbitrary stages of an episode could lead to suboptimal constraint\nplacement and consequently low cumulative episodic reward. In RLCBS, each positive constraint is encoded in a beam\nconstraint object whose abstracted definition is provided in Algorithm 1. For each beam candidate, the constraint object\nprovides a list of actions that advances constraint fulfillment, processes the newly generated action token, and provides\nthe steps remaining to complete the constraint. This information is required by the VDBA algorithm to correctly\nallocate banks across the beam candidate population. In the case of a disjunctive constraint with member constraints of\ndifferent lengths, i.e. C\u2081 = {\u2018desk', 'coffee table'}, the remaining length of the constraint is defined as the height of\nthe disjunctive trie.\nConstraints that specify the exact number of actions, such as \u201cexactly n actions of a certain type that must appear in the\ngenerated sequence\", can be enforced by adding a beam constraint and a logit processor to RLCBS at the same time."}, {"title": "2.2.4 Simulation Environment Caching", "content": "Due to the size and computational cost of modern LLMs, the speed of BS/CBS text generation is often dominated by the\nspeed of LLM inference. Conversely, the inference costs of RL models are often much lower due to their significantly\nlower number of parameters, and in the case of a simulation environment backed by an underlying physics-based model,\nthe cost of simulating state transitions in the simulation environment can become dominant. For the sake of argument,\nconsider the action-state sequence {so, ao, ...St\u22121, at-1, St} with stopping condition t = T. To complete the sequence\nusing conventional greedy search, a total of T timesteps need to be simulated by the environment, as only one path\nfrom so to sfin is explored. For CBS, since n\u044c beams are considered in parallel, the simulation environment must restart\nn\u028a times at each timestep to consider all beam candidates, leading to a total of $T(T \u2212 1)\u043f\u044c = O(T2n\u044c)$ simulation\nsteps before reaching the timestep t = T. To address the exponential growth in computational cost of physics-based\nsimulation during CBS, we introduce a global cache that serves multiple processes running the simulation environments\nin parallel. The simulation environment's underlying C++ physics-based model can be serialized as a state vector\nencoding all internal states of the environment. During CBS, we employ the Algorithm 2 where each simulation\nenvironment instance attempts to query the global key-value storage with the current action sequence as query key. In\ncase of a cache miss, shorter prefixes of the action sequence will be used until a cache hit or the prefix becomes empty.\nThe simulation environment then continues the simulation from the longest cached prefix, storing the state vectors\nobtained from the physics-based model after every simulation timestep. In single-thread execution, this scheme reduces\nthe total number of simulation steps from $T(T \u2013 1)\u043f\u044c$ to T\u0463. When multiple simulation environments are running"}, {"title": "3 Smart Dryer Simulation Environment", "content": "This study considers a modular Smart Dryer testbed, shown in Figure 2, which incorporates reconfigurable drying\nmechanisms to investigate the drying processes of pulp, paper, and other materials. The Smart Dryer is approximately\n9 m long with a drying chamber of length 6.34 m (20.8 ft). One of the contributions of this work is the Smart Dryer\ndrying simulation environment that facilitates training and evaluation of RL agents. The Smart Dryer simulation\nenvironment enables realistic and efficient data generation under randomized operating conditions, allowing for training\non-policy RL algorithms with a reasonable computational budget. In this section, we describe the underlying theoretical\ndrying model, the range of operating conditions, and a method for estimating overall energy consumption, which serves\nas the metric for evaluating the performance of the trained RL agent. A list of symbols referenced hereafter is given at\nthe end of the paper."}, {"title": "3.1 Physics-Based Drying Model", "content": "To simulate the effects of various process parameters on the paper while it travels through the dryer, the Smart Dryer\nsimulator leverages a theoretical drying model first proposed in [Seyed-Yagoobi et al., a] for simulating free-water\nremoval from paper in a conventional steam-heated, multi-cylinder dryer. The model is 1-D in space (thickness),\ntransient, and predicts moisture and temperature profiles inside the paper while it travels through the series of dryer\ndrums. Pilot machine trials verified the model on a multi-cylinder dryer incorporated with infrared emitters [Seyed-\nYagoobi et al., b].\nBased on the theoretical model proposed in [Seyed-Yagoobi et al., a], we constructed a simulation model for the\nSmart Dryer and performed preliminary experiment verification, which shows good agreement between simulation and\nexperiment results, as reported at the end of this section.\nThe drying model is based on mass and energy conservation governing equations in the thickness direction:\n$\\frac{\u04d8\u043c}{\u2202t} = \\frac{\u10dbJw}{dz} + \\frac{\u10dbJv}{dz} $\n(4)"}, {"title": "3.1.1 Boundary Conditions", "content": "One of the distinctive features of the Smart Dryer is the reconfigurable drying chamber which accommodates up to\n12 dryer modules in a row. Each of the 12 dryer positions can be occupied by one of the 4 dryer modules: slot jet\nreattachment (SJR) module, perforated plate (PP) module, dielectrophoresis (DEP) module, and solid plate (SP) module.\nAmong these, the SJR and PP modules receive hot air supply with temperature Ta and velocity va, while the DEP and"}, {"title": "3.2 Estimation of Energy Consumption", "content": "To simulate the effects of various drying technologies and control parameters on the paper being dried, as well as provide\na basis for optimization, a physics-based drying model was developed to numerically solve the paper temperature Tt and"}, {"title": "3.3 Experimental Validation", "content": "Experimental validation is conducted on the testbed under the specified conditions. The test samples are lab-made\npaper handsheets of refined hardwood, prepared following the TAPPI T 205 standard [of the Pulp and, TAPPI]. The\ntest is repeated three times with freshly made handsheets at room temperature. The prepared samples have a basis"}, {"title": "4 Experiment Setup", "content": "We approach the problem of combinatorial optimization of dryer module assignment and hot air supply temperature in\nthe Smart Dryer in a two-step process. The problem is first formulated as a Markov decision process without constraints\nand solved using RL techniques. Then, we define design constraints in RLCBS and apply the algorithm to RL action\ngeneration at inference time to decode high-quality action sequences that maximize cumulative reward while respecting\nthe inference-time constraints. The RL policy trained in the unconstrained environment is used to guide exploration in\nthe exponential search space while bound by negative and positive constraints."}, {"title": "4.1 RL Model Training", "content": "The Smart Dryer is discretized into up to 12 timesteps of equal lengths, each corresponding to the amount of time\nrequired for the paper to travel through the span of a drying module. For the RL task, it is assumed that dryer modules\ncan be freely reconfigured with hot air at potentially varied temperature levels supplied to each module. Although varied\ntemperature air supply for different modules is currently exceeding the capacity of the physical dryer, the intention\nis that the RL model will be trained in an environment with minimal constraints and additional constraints can be\nprogrammed into RLCBS at inference-time if needed.\nDuring RL training and inference, the simulation is carried out such that each dryer module represents a timestep. At\neach timestep t, the RL policy \u03c0 observes the state of the environment st and chooses an action at from the finite action\nspace A, which specifies the module type and air supply tempearture to be used in the next timestep. The environment\nproceeds by one timestep upon receiving the action input and provides the RL agent with the next state st+1 and a\nscalar reward rt as the feedback. The interaction between the RL agent and the environment repeats until a stopping\ncondition is reached at timestep T. The RL agent is trained to find the optimal policy that maximizes the cumulative\ndiscounted reward for all future timesteps t to tfin:\n$\\Rt = \u03a3\u03c4$\n(16)\nthrough actions at:T supplied to the environment.\nProximal Policy Optimization (PPO) [Schulman et al.], a popular on-policy algorithm, is used to learn the optimal RL\npolicy. It falls into the category of actor-critic RL algorithms where a policy (actor) and a value function (critic) are\njointly trained by interacting with the environment & over a series of discrete timesteps.\nThe actor is a neural network policy \u03c0\u03bf(st) parameterized by 0. It maps the state space S to a categorical distribution\nover the discrete action space A. Specifically, the actor network accepts a state vector containing temperature and\nDBMC at the top and bottom surface of the paper and machine speed. The output from the actor network is a multi-\ndiscrete distribution, comprised of two categorical distributions covering the 4 module types and 11 discrete temperature\nlevels equally displaced across the defined hot air temperate range of 80 to 190 \u00b0C\u00b9. The multidiscrete output setup\nsimplifies the combinatorial action space and allows for easy scaling to denser Ta grades in the future. Final action"}, {"title": "4.1.1 Reward Function", "content": "The main objective of training the RL agent is to minimize overall energy consumption, regardless of the machine speed\nsetting set at the beginning of each episode. However, cumulative energy consumption q, defined in Equation (15), cannot\nbe used directly for feedback to the RL agent, as it depends closely on the machine speed, with faster drying leading to\nhigher energy consumption even in optimal settings. To ensure consistent feedback that encourages minimizing energy"}, {"title": "4.1.2 SQP Reward Baseline", "content": "SQP is a well-known iterative method for constrained nonlinear optimization. Here, qsop represents the lowest possible\noverall process energy consumption achieved over a predefined sequence of 12 dryer modules and constant global\nprocess parameters, i.e., all dryer modules share the same hot air temperature Ta, flow rate va, IR on/off status and IR\nsurface temperature.\nUsing SQP, we attempt to solve the following constrained nonliear optimization problem:\nMinimize: q = f (Ta, va, TIR, Tp,init, DBMCp,init, SF),\nSubject to: DBMCp,fin \u2013 0.2 \u2264 0,\nDBMCp,fin = g(Ta, va, TIR, Tp,init, DBMCp,init, SF),\nWith: 80 \u00b0C \u2264 Ta \u2264 232 \u00b0C, 427\u00b0C < TIR \u2264 760 \u00b0C,\n$\\frac{Va}{Pa Anozzle} 200$\n(20)\nHere, f and g are objective and constraint functions backed by the simulation environment. Ta, TIR are normalized to\n[0, 1] before optimization. Air velocity va is determined collectively by fan power (air volumetric flow rate) and pa,\nwhich in turn is influenced by Ta. Machine speed, vm, controls total drying time and is manually specified for each\noptimization run by specifying speed factor, SF. Conversion between vm and speed factor SF is done as follows:\n$\\Um = Um,min + (Um,max - Um,min) \u00d7 (1 \u2013 SF),$\n(21)\nwhere Um,min  0.006 604 ms-1 corresponds to theoretical maximum drying time of 16 min and Um,max\n0.066 04 ms-\u00b9 corresponds to theoretical minimum drying time of 1.6 min. Finally, the dryer sequence configu-\nration is fixed at 6 SJR modules followed by 6 PP modules. We perform two sets of experiments where all IR modules\nare enabled in the first and disabled in the second.\nWe use a SQP implementation provided in MATLAB r2024a. Communication between MATLAB and the C++ physics-\nbased model is through an interface generated using MATLAB clibPublishInterfaceWorkflow. The baseline\nresults optimized by SQP are shown in Figure 4. The results are omitted at machine speed levels where SQP cannot\nfind a feasible solution. The reason for failing to find an feasible solution could include (1) failing to satisfy constraint\nDBMCp,fin - 0.2 \u2264 0 even when all controlled variables are maximized, which occurs when the machine speed is too\nfast; or (2) physics-based model fail to complete simulation under the prescribed set of controlled variables, which\nis most likely to occur when machine speed is too slow. We found that SQP is very sample efficient in this problem,\nrequiring on average less than 30 objective function calls to converge for each machine speed level. However, since\nSQP function calls must be made sequentially and cannot be fully parallelized, each session requires approximately 1 h\nwall time even with central differencing parallelization enabled.\nFrom the SQP optimized baseline, we note the following observations."}, {"title": "4.2 RLCBS with Inference-Time Design Constraints", "content": "We introduce three design constraints based on practical considerations of the physical drying testbed, encompassing\nboth negative and positive constraints:\n1. Force maximum 6 SJR modules: The generated sequence must include no more than six SJR modules.\nInitial experiments showed that SJR modules have the highest drying capacity and are more efficient than PP\nmodules, making them generally favored by the learned RL policy. However, SJR modules are more expensive\nto manufacture than PP modules. Additionally, due to their larger air outlet compared to PP modules, installing\ntoo many SJR modules in the dryer may result in uneven and insufficient airflow distribution to other modules.\n2. Force minimum 3 DEP modules: The generated sequence must include at least 3 DEP modules. Although\nDEP modules have relatively low drying capacity, they are highly energy efficient, consuming less than 100 W\nper unit. Including more DEP modules in the solution sequence is desirable, even if they are not preferred by\nthe learned RL policy.\n3. DEP/SP air temperature continuity: This constraint arises from the nature of DEP and SP modules: the\ndryer does not actively supply hot air to these positions, as air passage is blocked by the DEP module or by a\nsolid plate. Consequently, the setting of air temperature (Ta) for the DEP and SP modules must match that of\nthe preceding module in the solution sequence.\nIn RLCBS, negative constraints 1, 3 are implemented as logits processors, followed by logits renormalization. Specifi-\ncally, constraint 1 masks any actions that correspond to an SJR module (actions 11-21) once there are already 6 or more\nsuch actions in the action sequence prefix. Constraint 3 masks any actions that correspond to a DEP or SP module that\ndoes not share the same air temperature setting with the preceding module, but this constraint will not be enabled in the\nfirst step of each episode due to the lack of a preceding module in such case.\nPositive constraint 2 is implemented as a SequentialDisjunctiveConstraint that can be fulfilled by three\nactions that correspond to DEP modules (action 22-32) in the generated action sequence. Different from a\nPhrasalConstraint, the prescribed actions do not have to be adjacent in the sequence. Note that the SQP baseline for\nRL training uses a dryer module sequence configuration (6x SJR, 6x PP) that violates constraint 2. It is infeasible to train\nan RL agent for every combination of design constraints, as the reward function cannot be easily changed after training\nand each training session requires significant time investment due to the computational intensity of the physics-based\nsimulation. Nevertheless, the SQP baseline should remain reflective of the overall trend of energy consumption versus\nmachine speed even when some design constraints are violated."}, {"title": "4.3 NSGA-II Baseline", "content": "We use the Non-dominated Sorting Genetic Algorithm (NSGA-II) [Deb et al.] as a direct comparison of RLCBS in the\ntask of simulataneously optimizing dryer sequence optimization and module-specific hot air supply temperature. We\nselect NSGA-II as it supports all types of constraints discussed in this paper, and is widely recognized as a popular\nmulti-objective evolutionary algorithm that achieves good performance in a variety of combinatorial optimization\nproblems, allowing us to validate whether RLCBS has approached global optimum by comparing RLCBS and NSGA-II\nresults under the same experiment configuration."}, {"title": "5 Results", "content": "The RL agent was trained using the PPO algorithm for 10 million timesteps in the Smart Dryer physics-based simulation\nenvironment. Training experiments were parallelized over 30 processes and took approximately 20 days of wall time on\na PC with an AMD 5950X CPU and an Nvidia RTX3090 GPU. Physics-based simulation is performed on CPU and\nRL model training is performed on GPU. Due to fully randomized machine speed settings, the training process does\nnot benefit from the Redis global cache. As shown in Figure 5, the RL agent started with a negative reward and made\nprogress in the initial stages of training. However, after 5 million timesteps, the running mean of cumulative episodic\nreward plateaued below 0, indicating that the RL model could not outperform the SQP-optimized solution using greedy\nsearch.\nAlthough machine speed levels are fully randomized during training, we use fixed machine speed levels between 0.25\nand 0.75 in increments of 0.05 for evaluation, since the reward function in Equation (19) requires comparing the drying\noutcome of the RL solution with the SQP baseline calculated at these grid points, while any position in between requires\ninterpolation. With the test initial conditions aligned with precalculated positions, more accurate results can be reported\nas no interpolation error is introduced to the results.\nFor each machine speed level, we run 8 sessions using RLCBS with increasing number of beams n\u044c: 2, 4, 8, 16, 32,\n64, 128, 256. Due the presence of a Redis global cache, results from previous runs can be reused by subsequent runs,"}, {"title": "6 Discussion", "content": ""}, {"title": "6.1 Sample Trajectory", "content": "To study how constraints are implemented by RLCBS and NSGA-II in their solution sequences, we visualize the\nsolutions generated by both methods for the experiments described in Section 5. Figure 6 corresponds to Table 5, while\nFigure 7 corresponds to Table 6. In these figures, each row represents a solution sequence containing up to 12 dryer"}, {"title": "6.2 Comparison of SGBS, RLGBS, RLCBS", "content": "Comparison between RL-guided beam search methods supporting only negative constraints, SGBS [Choo et al.] and\nRLGBS, shows different approaches in beam candidate selections. Namely, SGBS is simulation-guided in that, although\nRL model is responsible for growing the search tree, pruning of beam candidates relies on simulation environment\nfeedback. Conversely, RLGBS relied on RL policy to grow and prune beam candidates.\nSGBS may be more robust in that beam candidates selected at each timestep are guaranteed to be the best-performing\nbeams in terms of cumulative reward. In the case of an RL model operating on a problem with a different distribution,\nSGBS is more likely to perform well due to consistant feedback from the environment so far. However, due to its\nreliance on reward feedback for beam pruning, SGBS is limited to tasks with dense reward. Additionally, even for\ndense reward tasks, it cannot be guaranteed that selecting beam candidates based on cumulative reward at intermediate\ntimesteps will lead to a globally optimal solution.\nRLGBS does not rely on reward feedback at all during search process, only that the best returned process at the end of\nsearch is selected based on cumulative reward. This strategy allowed it to operate on sparse reward tasks. However, as\nmentioned by the SGBS authors, relying solely on the RL policy for exploration may not be the most efficient approach.\nThis is partially remedied by the large beam sizes used by RLGBS (up to 256).\nIn developing RLCBS, we attempt to strike a compromise between the two approaches. RLCBS still relies on the RL\npolicy for beam growth and partly for beam candidate selection, but there are additional pruning at beam selection time\nto ensure that any failed beams (beam candidates that result in the simulation environment to arrive at a truncated state\nwith negative reward) are not passed to the next timestep. When finalizing beams, similar to RLGBS, we evaluate all\ncompleted hypotheses and return the best hypothesis based on cumulative reward. Finally, we attempt to refine a small\nnumber of returned hypotheses (1 for n\u044c \u2264 4, 4 for \u044c > 4) by swapping the last action of the sequence with every\npossible action, discard any sequence that results in violation of constraints, and re-evaluate the remaining sequences\nusing the simulation environment. Doing so requires evaluating at most 4 \u00d7 |A| = 176 new sequences for any beam\nsize, and therefore does not affect the overall runtime complexity of the algorithm."}, {"title": "6.3 Energy Savings", "content": "Compared to average energy consumption baselines set by SQP in Table 4, average energy savings by RLCBS are\n1.866% under constraint 3, and 0.737% under constraint 1, 2, 3. Although percentage savings are low, this is partly due\nto the baseline is already optimized by SQP. It proved difficult to provide a single-stage baseline using any na\u00efve policy,\nas due to the variation in the total drying time (2-5 min), any single-stage, constant parameter policy will fail in most\ntest cases due to underdrying or severe overdrying. Additionally, SQP being able to operate in the continuous domain\nfor the optimized variables may have been advantageous vs. RLCBS and NSGA-II under their current configuration\nof 11 discrete air temperature levels. A potential topic of future research may be to adapt RLCBS to RL agents with\ncontinuous action spaces.\nDespite the low percentage savings, the potential contribution to sustainability is substantial when considering the\nscale of application of paper manufacturing. Taking into account the rate of thermal energy consumption of 21 TJ"}, {"title": "6.4 Limitations", "content": "By exploring multiple beam candidates in parallel and considering the overall sequence feasibility over all timesteps,\nreal-time performance is sacrificed by replacing greedy search with beam search. However, this trade-off may be\njustified in problems where the cost of mistakes is high and real-time performance is not a critical requirement. For the\npaper drying process considered in this work, frequent adjustments to the control parameters are necessary to adapt to\nvariations in operating conditions. Yet, real-time performance is not essential as these parameter adjustments may only\nbe required on a batch or campaign basis, or the system's response time may not be fast enough to warrant real-time\ncontrol. In such cases, employing RLGBS to obtain higher cumulative episodic rewards may be desirable despite\nthe increased computational cost compared to greedy decoding. Conversely, in time-critical problems or problems\ninvolving non-deterministic environments, alternative methods like greedy search or one-step lookahead may be more\nappropriate, or the limitation could be circumvented by training a new RL agent using imitation learning on a dataset of\nhigh-quality sequences generated by RLGBS.\nAdditionally, the beam search decoder proposed in this work requires a copy of the simulation environment to determine\nthe next states for sequence continuation and checking stopping conditions. In practice, such an oracle simulation\nenvironment may not always be available. A possible solution is to train a surrogate model of the environment and use\nthe surrogate in place of the oracle during decoding. Further investigations on the impact of discrepancies between the\nfirst-principle oracle and surrogate models on the performance of generated actions may be needed.\nFinally, the simulation environment introduced in this work makes simplifying assumptions to the problem setup,\nincluding discrete temperature levels and assuming uniform air distribution to all drying modules. The energy savings\nare realized by adjusting the temperature of the hot air supply in a way that may not be feasible in the physical setup, and\nthis study is not concerned with the potential impact of optimally selected dryer module and air temperature sequence\nconfigurations on the properties of the paper. A process controller implemented in a real paper machine should also\nensure that product quality constraints are met. A potential direction for further research may be to impose quality\ntargets as constraints while reducing the energy consumption of the process."}, {"title": "7 Conclusion", "content": "We present Reinforcement Learning Constrained Beam Search (RLCBS) as a method to incorporate complex design\nconstraints to RL-generated actions at inference time. RLCBS enables policy-based RL methods to search for high-\nquality solutions efficiently in exponential search spaces, while injecting design constraints unseen at training time.\nOur experiments"}]}