{"title": "GraphSparseNet: a Novel Method for Large Scale Trafffic Flow Prediction", "authors": ["Weiyang Kong", "Kaiqi Wu", "Sen Zhang", "Yubao Liu"], "abstract": "Traffic flow forecasting is a critical spatio-temporal data mining task with wide-ranging applications in intelligent route planning and dynamic traffic management. Recent advancements in deep learning, particularly through Graph Neural Networks (GNNs), have significantly enhanced the accuracy of these forecasts by capturing complex spatio-temporal dynamics. However, the scalability of GNNs remains a challenge due to their exponential growth in model complexity with increasing nodes in the graph. Existing methods to address this issue, including sparsification, decomposition, and kernel-based approaches, either do not fully resolve the complexity issue or risk compromising predictive accuracy. This paper introduces GraphSparseNet (GSNet), a novel framework designed to improve both the scalability and accuracy of GNN-based traffic forecasting models. GraphSparseNet is comprised of two core modules: the Feature Extractor and the Relational Compressor. These modules operate with linear time and space complexity, thereby reducing the overall computational complexity of the model to a linear scale. Our extensive experiments on multiple real-world datasets demonstrate that GraphSparseNet not only significantly reduces training time by 3.51x compared to state-of-the-art linear models but also maintains high predictive performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Traffic flow forecasting represents a quintessential spatio-temporal data mining challenge, with profound utility in a spectrum of real-world applications, including intelligent route planning, dynamic traffic management, and smart location-based services [33]. The objective of this endeavor is to prognosticate future traffic patterns by leveraging historical traffic data, typically gleaned from the sensors embedded within transportation networks. The advent of deep learning has revolutionized this domain, with deep learning-based techniques, particularly those grounded in graph neural networks (GNNs), emerging as a dominant force. GNNs excel in capturing the intricate nonlinear dynamics inherent in spatio-temporal datasets, with their efficacy underpinned by the natural alignment between traffic data structures and graph-theoretic principles [14, 31]. In these models, graph nodes correspond to traffic sensors, while edges delineate the interconnections among these sensors.\nDespite the predictive prowess of GNN-based methodologies, they are not without their drawbacks, most notably the exponential growth in model complexity. The number of edges in a graph tends to increase exponentially with the number of nodes, posing a significant challenge for the scalability of GNNs to larger datasets. This challenge is exacerbated by the proliferation of sensors within traffic networks, driven by urban development and the pervasive integration of Internet of Things (IoT) technologies. The vast amounts of spatio-temporal data generated by these sensors pose a challenge in applying higher-accuracy GNN methods to larger-scale datasets.\nSeveral strategies have been proposed to expedite the computational efficiency of GNNs. For instance, the AGS method [5] introduces a sparsification technique that prunes a trained model, thereby reducing its complexity during the inference phase. However, this approach still incurs substantial computational overhead during the training phase. Other methods, such as GWNet and AGCRN [1, 35], employ Tucker decomposition to construct graph adjacency matrices with a reduced parameter count. Yet, these techniques merely curtail the number of trainable parameters in the adjacency matrix, without alleviating the computational burden associated with graph operations. The computational complexity of adjacency matrix multiplication, for example, remains $O(N^2)$,"}, {"title": "2 PRELIMINARY", "content": "2.1 Traffic Network\nTraffic network is represented by a undirected graph G = (V, E), where V is the set of nodes (sensors), N = |V| denotes the number of nodes, and E is the set of edges between two nodes. In our problem, we assume that each node records its traffic flow data as graph signal. A graph signal is $X_t \\in R^N$, where t denotes the t-th time step. The graph signal represents the traffic flow values at the t-th time step.\n2.2 Problem Definition\nGiven a traffic network G and its historical S step graph signal matrix $X_{1:S} = (X_1, X_2, ..., X_S) \\in R^{N \\times S}$, our problem is to predict its next T step graph signals, namely $X_{S+1:S+T} = (X_{S+1}, X_{S+2}, ..., X_{S+T}) \\in R^{N \\times T}$. We equationte the problem as finding a function F to forecast the next T steps data based on the past S steps historical data:\n$(X_{S+1}, X_{S+2}, ..., X_{S+T}) = F ((X_1, X_2, ..., X_S)).$ (1)"}, {"title": "3 ANALYSIS", "content": "In this section, we discuss the constraints that hinder the scalability of graph neural network methods. We also explain our motivations for simplifying models, supported by theoretical evidence and empirical findings."}, {"title": "4 PROPOSED MODEL", "content": "we commence by elucidating the overarching framework of the model. Subsequently, we introduce the specific module designs within the model.\n4.1 Framework of the model\nAs depicted in Figure 2, our model comprises two distinct modules: Feature Extractor and Relational Compressor. The Feature Extractor compresses the input data into an low-dimensional space and subsequently reconstructs it, while simultaneously updating an embedding that represents the node features. The Relational Compressor transforms the input data into an low-dimensional space, models the implicit adjacency relationships between different nodes in the low-dimensional space, and ultimately reconstructs the data back into its original space. The low-dimensional spaces involved in the two modules are mutually aligned. Both the Feature Extractor and Relational Compressor are stacked in a serial manner, and the outputs from each layer are concatenated with skip connections, which are then transformed into prediction through an output layer.\nIn our model, there are two types of embedding matrices. One is the input embedding $P \\in R^{N \\times d}$, and the other is the node embedding $Q \\in R^{N \\times d}$. d denotes the number of channel in the embedding. The node embedding is composed of trainable parameters, the elements of which are updated exclusively during the training process of the model and do not vary with the input data. In contrast, the input embedding is generated from the model's input data and automatically adjusts according to different inputs. The purpose of the node embedding is to model the characteristics of each node in the transportation network during the training process of the model. The input embedding encompasses not only the node feature information but also the spatial information (such as the relationship between nodes) and temporal information present in the input data. The objectives of transforming the input data into input embeddings are twofold: first, to obtain a dense representation for spatio-temporal features, and second, to enable the model to automatically learn the most salient features of the data.\n4.2 Feature Extractor\nThe advantages of Graph Neural Networks stem from two aspects: one is their powerful feature learning capability, which can accurately capture the features of each node, and the other is their ability to handle graph-structured data, allowing node features to be integrated based on adjacency relationships. By aggregating information from neighboring nodes to update the representation of a node, it can capture complex relationships between nodes, thereby better understanding graph-structured data. In the Feature Extractor module, our primary goal is to enable the model to learn the features of different nodes.\nIn the design of the Feature Extractor, we have two objectives: one is to update the node embeddings through this module to model the characteristics of each node in the traffic data, and the other is to attempt to compress the input embeddings, which contain spatio-temporal information, to a low-dimensional space, thereby preparing for subsequent feature fusion in low-dimensional space within the model.\nTo achieve both of these objectives, we have designed a module that compresses and then decompresses the input information based on node features. The input embeddings encompass only the local spatio-temporal characteristics of the input time series, as the data presented to the model at each instance constitutes a small slice of the entire dataset. Concurrently, the node embeddings remain invariant over time, thereby capturing and representing global information. Therefore, we concatenate the two types of embeddings as the input to the module.\n$X_{FE} = P||Q$ (20)\nHere, $X_{FE}$ represents the input of Feature Extractor. The concatenated input $X_{FE}$ contains both the local spatio-temporal information of the input data and the global features of each node. A reasonable approach to compressing data from an N-dimensional space to a low-dimensional space is to cluster based on node features. To this end, we use a matrix generated from the node features to compress the module input, as follows:\n$H_{FE} = V_1X_{FE}$ (21)\n$V_1 = W_1Q + B_1$ (22)\nHere, $H_{FE}$ represents the hidden state, $V_1$ is the compressing matrix. $W_1$ and $B_1$ are trainable parameter matrices. Subsequently, we designed the decompression process, which is similar to the compression. To introduce non-linear characteristics and control in the Feature Extractor, we added an activation function between the compression and decompression processes. We utilize the SoftMax function to present the hidden layer in the form of a probability distribution, thereby enhancing the training effectiveness of this module during the decompression process.\n$O_{FE} = V_2SoftMax(H_{FE})$ (23)\n$V_2 = W_2Q + B_2$ (24)\nOn one hand, the process of compression and decompression can update the a node embedding Q to learn the features of different nodes. The information contained within this embedding will be shared with Relational Compressor. On the other hand, the model is designed to learn a method of compressing the input to a size of C dimensions, which serves as an auxiliary function for Relational Compressor to map the input into a low-dimensional space. This alignment of the compression ratio in this module with that in Relational Compressor is crucial for the seamless integration and effective operation of the overall model.\n4.3 Relational Compressor\nAs we discussed in the previous analysis, learning the adjacency matrix A can be equivalent to learning matrices K and U. In the design of the Relational Compressor, our objective is to learn the"}, {"title": "6 RELATED WORK", "content": "Traffic flow forecasting stands as a quintessential challenge in spatio-temporal data prediction, with analogous tasks including the forecasting of shared bicycle demand, as well as the demand for buses and taxis, and the prediction of crowd flows, among others [2, 11, 19, 42]. Traditional statistical approaches such as ARIMA [32] and SVM [4], while prevalent in time series forecasting, often fall short due to their inability to account for spatial dimensions, making them less effective for complex spatio-temporal data sets. The advent of deep learning has introduced methods adept at handling the intricacies and non-linearities inherent in traffic data. Convolutional Neural Networks (CNNs), in particular, have become a staple in traffic flow forecasting [21, 23, 36, 40, 41]. These networks interpret traffic flow data as images, where each pixel represents the traffic count within a specific grid cell over a given time interval. By leveraging techniques originally developed for image recognition [31], CNNs can effectively model the spatial relationships between different grid regions. Recurrent Neural Networks (RNNs) have also been instrumental in the analysis of sequence data, bringing their sequence memorization capabilities to bear on traffic flow forecasting [28, 37, 43]. More recent advancements have seen Graph Neural Networks (GNNs) rise to prominence for their ability to manage the spatio-temporal correlations present in traffic flow data, achieving state-of-the-art results [8, 25, 27, 30]. GNNs, initially designed for graph structure analysis, have found widespread application in node embedding [24] and node classification [15]. In the realm of transportation systems, GNNs, including graph convolutional and graph attention networks, have been adapted to model graph structures and have achieved remarkable performance. For instance, DCRNN [18] employs a bidirectional diffusion process to emulate real-world road conditions and utilizes gated recurrent units to capture temporal dynamics. ASTGCN [7], on the other hand, utilizes dual attention layers to discern the dynamics of spatial dependencies and temporal correlations. STGCN, Graph WaveNet, LSGCN, and AGCRN [1, 12, 35, 38] represent a lineage of methods that build upon Graph Convolutional Networks (GCNs) to extract spatio-temporal information. Notably, Graph WaveNet introduces a self-adaptive matrix to factor in the influence between nodes and their neighbors, while LSGCN employs an attention layer to achieve a similar end. STSGCN, STFGNN, and STGODE [6, 17, 29] propose GCN methodologies designed to capture spatio-temporal information in a synchronous manner. MTGNN [34] introduces a graph learning module that constructs a dynamic graph by calculating the similarity between learnable node embeddings. DMSTGCN [10] captures spatio-temporal characteristics by forging dynamic associations between nodes. STPGNN [16] enhances predictive accuracy by taking into account special nodes within the road network. In addition to GNN-based methods, several approaches leveraging Transformers [3, 13] and pre-training [20, 39] have also been proposed. These methods have also shown promising results. However, both Transformer-based and pre-training approaches, along with GNNs, still encounter significant computational challenges when identifying nodes relationships, which limits the scalability of these models on large-scale datasets. Existing adaptive graph neural network methods often rely on fully connected graphs to bolster the model's learning capabilities. Yet, the exponential growth in the number of edges with an increase in nodes poses a challenge for these methods when generalizing to larger-scale datasets. To counter this, AGS [5] has proposed a method for significantly simplifying the adaptive matrix, thereby reducing the model's computational load. However, this approach is limited to the inference stage. In practical applications, the computational cost during the training phase often dwarfs that of the inference phase. A method that maintains linear complexity during training can significantly enhance the model's operational efficiency. BigST [9] introduces a method that employs kernel functions to linearly approximate graph convolution operations, yielding a graph prediction model with linear complexity. However, kernel-based methods can sometimes result in anomalous gradient values during training, impacting model convergence and, by extension, the model's performance. Amidst the ever-expanding scale of traffic data, there is an urgent need for graph neural network methods capable of delivering high-precision predictions at scale."}, {"title": "7 CONCLUSION", "content": "In this paper, we addressed the challenge of enhancing the scalability of GNN-based methods for large-scale traffic spatio-temporal data prediction. We identified that existing methods either do not fully address the computational complexity of graph operations or compromise model accuracy when simplifying GNN structures. By introducing GraphSparseNet (GSNet), we have provided a novel approach that leverages two module-the Feature Extractor and the Relational Compressor-to effectively manage and analyze large-scale traffic data. Our theoretical analysis and empirical evaluations highlight the significant advantages of GSNet over existing methods. By reducing model complexity to O(N), GSNet addresses the scalability issue while maintaining high predictive accuracy. Our extensive experiments on real-world traffic datasets demonstrate that GSNet not only achieves high predictive accuracy but also significantly improves training efficiency, outperforming state-of-the-art methods in both aspects. The model's efficiency is further validated by a substantial improvement in training time, outperforming current linear models by 3.51x. Future work may explore"}]}