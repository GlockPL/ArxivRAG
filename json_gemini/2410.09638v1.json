{"title": "On Goodhart's law, with an application to value alignment", "authors": ["El-Mahdi El-Mhamdi", "L\u00ea-Nguy\u00ean Hoang"], "abstract": "\"When a measure becomes a target, it ceases to be a good measure\", this adage is known as Goodhart's law. In this paper, we investigate formally this law and prove that it critically depends on the tail distribution of the discrepancy between the true goal and the measure that is optimized. Discrepancies with long-tail distributions favor a Goodhart's law, that is, the optimization of the measure can have a counter-productive effect on the goal.\nWe provide a formal setting to assess Goodhart's law by studying the asymptotic behavior of the correlation between the goal and the measure, as the measure is optimized. Moreover, we introduce a distinction between a weak Goodhart's law, when over-optimizing the metric is useless for the true goal, and a strong Goodhart's law, when over-optimizing the metric is harmful for the true goal. A distinction which we prove to depend on the tail distribution.\nWe stress the implications of this result to large-scale decision making and policies that are (and have to be) based on metrics, and propose numerous research directions to better assess the safety of such policies in general, and to the particularly concerning case where these policies are automated with algorithms.", "sections": [{"title": "Introduction", "content": "\"When a measure becomes a target, it ceases to be a good measure\". This quote by anthropologist Marilyn Strathern [Str97] is probably the most commonly known rephrasing of Goodhart's law, which originally stated that \u201cany observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes\u201d [Goo75]. This principle is critical in numerous areas where metrics are used as proxies for evaluation and decision making, from economics to management, public policy, peer-reviewing of scientific papers and grants, education, standard tests for school admission, content recommendation on social networks and the list could be as long and interdis- ciplinary as one could think of metrics-based decisions. Variants of this adage could also be found in popular wisdom and proverbs around the world, often describing situations where doing good according to some perceived form of the intention, leads to harm according to the real intention."}, {"title": "Related works", "content": "Goodhart's law. Goodhart's law was introduced in [Goo75], but it is closely related to Lucas'"}, {"title": "Model", "content": "Consider a large set \u03a9 of proposals. These can be research papers, strategic plans or weights of a neural network. For any \u03c9 \u2208 \u03a9, we assume that there is a \u201ctrue\u201d score G(w). The true goal is thus to maximize G(w) over \u03c9 \u03b5 \u03a9. However the G function may be unusable. Perhaps it is too long to compute, or to describe in a Turing machine, in particular in human understandable language. Perhaps it is also unknown.\nAs an example, it is arguably extremely challenging to formalise quantitatively what \u201cbeing a good scientist\" is. In practice, too often, instead of optimizing for this goal G, we resort to proxy measures such as \"impact of scientific work\", which in turn require even more proxies M such as \"number of citations\" or other controversial indexes and rankings.\nAs another example, consider the \u201ctrue\u201d objective function of a supervised learning problem. The problem is then typically to learn the parameters w\u2208 \u03a9 of a statistical model (e.g. the parameters of a linear regression or the weights of a neural network) so as to maximize accuracy on out-of-sample data. This will usually be of the form G(w) = Ex\u2190D[\u2212L(x,w)], where x is the data drawn from an unknown distribution D and L is a data-dependent loss function. Instead, in practice, we then turn to measures M that approximate G. Typically, in the supervised learning example, we may replace D by some sample S, which is usually known as the training set. Then, our measure of the performance of the model of parameters w \u2208 N is Ms(w) = - Exes L(x, w).\nIn such a case, we are very likely going to construct a measure M that is correlated with the goal G. The natural way to formalize this correlation is by first introducing a probability distribution \u1f6e over \u03a9. In the case of supervised learning, this would correspond to considering random machine learning models, and to determine whether, for such random models, M and G are correlated. Note, however, that the probability distribution over models can be quite sophisticated, e.g. neural networks obtained by random initialization then optimized after some 100,000 iterations of stochastic gradient descent.\nGiven the probability distribution \u1f6e, we can define expectations such as E[M] = \u0395\u03c9\u03c4\u1fc7[M(\u03c9)], as well as the correlation between Gand M by \n\n$\\rho = \\frac{Cov(M,G)}{\\sqrt{Var(M)Var(G)}}$."}, {"title": "When the measure becomes a target", "content": "The trouble though occurs when M becomes a target. In particular, when this is done, because of selection bias, there is now a focus on the choices of w for which M is large. In particular, if most options are discarded except for, e.g. the top a = 1% values of M, then the correlation between M and G within this biased sample may no longer be large. In particular, if this correlation becomes zero or negative, then the measure M ceases to be a good measure.\nTo formalize, let ma be the top a quantile for M, i.e. such that P[M > ma] = a. As a shorthand, whenever some probability, expectation, variance or covariance is conditioned on M > ma, we shall use an a subscript, i.e. we write Pa[E] = P[E|M > ma] and Ea[X] = E[X|M \u2265 ma]. We denote by pa the correlation between M and G for the top a quantile for M, i.e.\n\n$\\rho_a = \\frac{Cova(G, M)}{\\sqrt{Vara(G)Vara(M)}}$\n\nIn this paper, we analyze Goodhart's law by focusing on how pa behaves as a \u2192 0. Evidently this depends on the distribution of G and \u00a7. But as we shall see, for reasonable distributions of G and \u00a7, the correlation pa can be near-zero (which we call weak Goodhart) or negative (which we call strong Goodhart). We even show that, as a \u2192 0, the expected value of the goal can become infinitely negative (which we call infinite Goodhart).\nIn fact, the behavior of pa greatly depends on the tail distributions of G and \u00a7. In particular, how do the functions P[G > g] and [P[\u00c9 > x] behave as g and x get larger?"}, {"title": "Key finding", "content": "The key takeaway of this paper is that Goodhart's law strongly depends on the large deviation behavior of the discrepancy & between the measure M and the goal G. In particular, we show that we only have a very weak version of Goodhart's law (the correlation goes to zero very slowly as the proxy measure is maximized) when \u00a7 follows a normal distribution.\nHowever, for thicker tail distribution of the discrepancy, if a \u2248 \u03b5, then Goodhart's law can apply. In fact, especially if the discrepancy follows a power law distribution, the measure M can become negatively correlated to G. In such cases, it may become critical to stop maximizing M, as over-optimizing M would then be detrimental to G.\nYet recall that\n\n$\\epsilon = \\sqrt{\\rho^{-2} - 1}$\n\nThis thus suggests that it can be counter-productive to select a fraction of top candidates smaller than\n\n$\\alpha = \\sqrt{\\rho^{-2} - 1}$\n\nDisturbingly, if p = 0.9, then a \u2248 49%, meaning that we should not be more selective than merely picking up the top 49%. Even with p = 0.99, we should not have a < 14%. And to safely select the top a = 1%, we need a correlation between the goal and the metric that is as large as p = 0.99995."}, {"title": "Formal results", "content": null}, {"title": "Bounded goal, exponential discrepancy", "content": "We start with the easiest case, namely when the goal G is uniform on [0,1], and when \u00a7 follows some exponential distribution of parameter \u03bb. In other words, in this subsection, we assume that the probability density functions of G and gare given by pG(g) = 1 for g\u2208 [0,1], and pf(x) = \u03bbe-\u03bbx. Given that Var(G) = 1, to guarantee a signal-to-noise ratio equal to \u025b, we must then have \u03bb = 1/\u03b5\u03bd\u221a12.\nRecall the memoryless property of the exponential distribution. In particular, it implies that \u0395[\u03be]\u03be \u2265 x] = x + E[\u00a7] = x + 1/\u03bb. This memoryless property is why, as we overoptimize over M, nothing changes. More precisely, after some threshold for a, the correlation between G and M becomes zero, and the expected value of G given that we are within the top a values of M is constant. This constant can be as low as 1/2 for very large discrepancy \u025b. And it remains of the order 1 \u2013 O(\u03b5) for small discrepancy \u025b \u2192 0. All of this is formalized by the following theorem.\n\nTheorem 1. If G is uniform on [0,1] and \u0121 follows an exponential distribution such that the noise- to-signal ratio is \u025b, then for 0 <\n\n$\\alpha < \\frac{\\epsilon}{\\sqrt{12}} (1 - e^{-1/(\\epsilon \\sqrt{12})})$, we have\n\n$\\rho_a = 0$\n\nand\n\n$E_a [G] = \\frac{1}{\\lambda e^{\\lambda} - e + 1}$\n\nAs \u025b \u2192 0, we have Ea[G] = 1 \u2212 \u03b5\u03bd\u221a12 + \u039f(\u03b5\u00b2)."}, {"title": "Normal distributions", "content": "In this subsection, we assume that Gand & follow normal distributions. Without loss of generality, we assume G \u2190 N(0, 1) and \u00a2 \u2190 N(0,\u025b\u00b2). As a result, we know that M \u2190 N(0,1 + \u03b5\u00b2). We shall denote 7 = 6.283... the ratio of the perimeter of a circle to its radius.\nThe main result here is a very weak Goodhart's law. Namely, correlation goes to zero. But it does so extremely slowly.\n\nTheorem 2. If G and & follow normal distributions, then\n\n$\\rho_a ~ \\frac{1}{\\sqrt{2 ln \\frac{1}{\\alpha}ln ln \\frac{1}{\\alpha} - ln \\tau}}$\n\nIn particular, pa \u2192 0 as a \u2192 0."}, {"title": "Bounded goal, power law discrepancy", "content": "We now consider the case where G is uniform, but where \u00a7 is a power law distribution of parameters (\u03b2, \u03b7) with \u03b2 > 3 (the cases \u03b2 \u2264 3 do not yield finite correlations because of infinite variance). For technical reasons, we shall also assume \u1e9e \u2260 4. More precisely, we write\n\n$p(x) = \\frac{(\\beta - 1)\\eta^{\\beta-1}}{x^{\\beta}}$"}, {"title": "Power law goal, power law discrepancy", "content": "Now consider that the goal and the discrepancy follow power law distributions. We assume that the goal G has a probability density function pc [g] = (y \u2212 1)g^ for g > 1. We consider the same probability density function for &, that is, p(x) = (\u03b2 - 1)\u03b7\u03b2-1x-8 for x \u2265 \u03b7.\nNote that we now have\n\n$E[G] = \\frac{\\gamma}{\\gamma - 1}$\n\nand\n\n$Var(G) = \\frac{\\gamma}{(\\gamma-2)^2(\\gamma -3)}$\n\nFor the noise-to- signal ratio to equal \u025b, we need to have\n\n$\\eta = \\frac{\\beta-2}{\\gamma -2} \\sqrt{\\frac{(\\gamma-1)(\\beta-3)}{\\beta-1}} \\epsilon$.\n\nAs earlier, we assume \u03b2 > 3 and \u03b3 > 3.\nTheorem 5. Suppose that G and \u0121 follow a power law distribution with decay rates g\u00af^ and x-\u03b2. As a \u2192 0, we have\n\n$p_a ~ \\begin{cases}\n1, & \\text{if } \\gamma - \\beta < 0, \\\\\n-Ce^{-a^{\\frac{\\gamma - \\beta}{2(\\beta-1)}}}, & \\text{if } \\gamma - \\beta \\in (0,2), \\\\\n\\frac{D}{a^{\\frac{\\gamma - \\beta}{\\beta-1}}}, & \\text{if } \\gamma - \\beta > 2,\n\\end{cases}$"}, {"title": "Bounded goal, log-normal discrepancy", "content": "In this subsection we investigate what happens when G is uniform on [0, 1], and where the discrep- ancy & is a log-normal distribution of parameters (0, \u03b7), In other words, we assume\n\n$p_\\xi(x) = \\frac{1}{x \\eta \\sqrt{2\\pi}} exp\\left(-\\frac{(\\ln x)^2}{2\\eta^2}\\right)$\n\nNote that, to obtain a noise-to-signal ratio of \u025b, the parameter \u03b7 needs to verify\n\n$e^{2\\eta^2} - e^{\\eta^2} = \\epsilon^2$\n\nwhich is equivalent to\n\n$\\eta^2 = \\ln \\frac{1 + \\sqrt{1 + 4\\epsilon^2}}{2}$\n\nUnfortunately, we have not been able to analyze theoretically Goodhart's law in this setting. But surprisingly, numerical computations suggest that the maximization of the measure M yields a \"double descent\" for the goal. The expected value of the goal first increases, then decreases, and then increases again to reach better expected values of G than after the first increase.\nThere are important caveats though regarding both the reliability of numerical computations, as well as the robustness of this \u201cdouble descent\" for other (smaller) values of \u025b. More discussions are provided in Appendix F."}, {"title": "Worst-case Goodhart's law", "content": "How bad can Goodhart's law get? Is it possible for instance that, as we maximize the measure M, the goal G becomes infinitely bad? In this section, by allowing a dependency between the goal G and the discrepancy \u00a7, we answer in the affirmative.\nTheorem 6. There are settings where maximizing the measure M = G + \u00a7 is eventually infinitely bad for the goal G, even if, given any value of the goal G, the discrepancy \u0121 is centered and the noise- to-signal variance is \u025b. More precisely, even if Var(G) = 1, E[\u03be|G = g] = 0 and Var(\u03be|G = g) = \u025b2 for all g in the support of G, we can have Ea[G] \u2192 -\u221e.\nSketch of proof. The example we exhibit is one where -G follows an exponential distribution, i.e.\n\n$p_G(g) = e^{g}$ for g \u2264 0. We then consider power-law discrepancies whose power law decay satisfies\n\n$\\beta_g = 4 + \\frac{1}{-g}$\n\n(we also add a mass probability at a negative point -xg to guarantee zero mean, and normalize the coefficient ng so that the discrepancy & has variance \u025b2 for all values of g). As a result, for more negative values of g, the discrepancy \u0121 given G = g has a thicker tail distribution. We then show that, in this setting, Ea [G] \u2192 -\u221e.\nMore generally, this result suggests that, in general, if the discrepancy & has a very long tail for some value of G, over-optimizing M may imply that G will tend to take values for which the tail of & is the thickest. If such tails are thickest for very undesirable values of G, the over-optimization of M may thus favor very undesirable outcomes."}, {"title": "Why robust alignment may be very hard", "content": null}, {"title": "Illustrative example", "content": "Let us first start with an illustrative example with a simple interaction model between an algorithm and a user. We assume that the algorithm recommends a content xt at each time step t, where xt \u2208 R is simply a real number. The user that consumes the content, and retrieves some satisfaction that is larger if xt is close to the user's intrinsic preference \u03b8\u2208 R. Moreover, the user is asked whether they would prefer the content to take a larger or a smaller value of xt. However, here, we assume that the user's declared preference is biased by the user's addiction to the contents they have consumed in the past. This feature is key, as it corresponds to a feedback loop, which we argue to increase the risk of Goodhart's law.\nMore precisely, we assume that their instantaneous preference at time t is some combination of their intrinsic preference 0, with a factor a representing some form of addiction coefficient to \u03b8, altered by the accumulated exposition they had to previous content xj, j \u2264 t and is given by\n\n$\\theta_t = \\frac{\\alpha \\theta + \\sum_{j=1}^{t-1} x_j}{\\alpha + t}$\n\nIf xt < 0t, then the user answers that they wishe xt's to take larger values, i.e. Yt \u25b2 +1. Otherwise, the user replies yt \u2252 \u22121. The algorithm then chooses a next content xt+1 based on past proposals and user's responses. In fact, we limit ourselves to algorithms that output Xt+1 by computing\n\n$x_{t+1} \\leftarrow x_t + \\frac{w y_t}{t}$\n\nand that begin with xo 0. We then consider algorithms that maximize the user's estimated satisfaction within T = 100 rounds,\n\n$H(\\omega, \\theta, \\alpha) \\triangleq \\sum_{t=1}^T \\frac{1}{\\delta + (\\theta_t - x_t)^2}$\n\nwith 8 10-5.\nWe assume that the measure used to select which algorithm to deploy has a correct estimation of 0 = 0.2. However, let us assume that this measure gets the estimation of the addictiveness a wrong. The measure uses AM = 50, while the goal uses ag = 5, which means that the measure under-estimates how much the user will be addicted to the content they consume. In other words, we have M(w) = \u0397(\u03c9,\u03b8,\u03b1\u03bc), while G(w) = \u0397(\u03c9,\u03b8,\u03b1\u03c3). We then drew w from a log-normal distribution of parameters \u03bc = 0 and \u03c3 = 3 to cover a wide range of values of w."}, {"title": "On the hardness of robust alignment", "content": "It is common to test algorithms \u03c9 \u2208 \u03a9 with a fixed (series of) test T. In other words, the algorithm wis designed to optimize some given objective function M\u03c4(w) before deployment. This measure Mr(w) may be some performance measure on historical data. However, such a measure is likely to be very different from the actual performance G(w) of the algorithm w once deployed.\nOne critical difference between My and G is that, once deployed, the algorithm w may modify its environment, and thus the distribution D(w) of future data. It was, after all, the point of deploying the algorithm. Algorithms are deployed so as to modify (and hopefully improve) their environment.\nBut as a result, the performance of the algorithm once deployed is actually dependent on the algorithm in two ways. First, the algorithm achieves some accuracy on its task. But second, and sometimes more importantly, it creates a distributional shift that changes the set of tasks it will be tested on. As an example, a recommender system might change users' preferences by creating addiction in them, which will then facilitate click-through rate maximization. By doing so, the algorithm actually modifies T (the users on which it is tested) and creates the aforementioned distributional shift.\nWe can formalize this by considering that the performance of the algorithm is measured in terms of some loss function L that depends on data. So, typically, we would have \u039c\u03c4(\u03c9) = Ex-T[-L(x,w)], while G(w) = Ex\u2190D(w) [\u2212L(x, w)].\nIn the context of Goodhart's law, this is critical to note, as we now have a feedback loop between the outputs of the algorithm and its future inputs given by its environment. Such feedback loops might then lead to power law distributions (see [DMS00, ATHL05, BHS10] among others), which would open the door for some strong Goodhart's law. If this holds, the over-optimization of the measure might eventually be (extremely) counter-productive to the goal.\nNote that for this to hold, the algorithm wneed not be learning from the data x drawn from D(w). Our basic assumption here is merely that the algorithm w modifies its environment (thus"}, {"title": "Remarks and future works", "content": "This work raises numerous intriguing questions regarding the potential pitfalls of optimization, and the need of algorithms that are resilient to counter-productive over-optimization. In the following, we list numerous research directions for future work and provide pointers to where initial efforts are already undertaken.\nUnderstanding the log-normal discrepancy case. Numerical simulations suggest that for uniform goal and log-normal discrepancy, Goodhart's law features a double descent phenomenon. Whether or not we can mathematically prove this numerical finding remains an open question. If so, can we predict the point where the second descent starts?\nApplication to overfitting. Overfitting seems to be a case of Goodhart's law. However, it is not yet clear how our analysis could be best used to predict overfitting. In particular, it is not clear what probability distribution over parameters of a machine learning algorithm is most fit"}]}