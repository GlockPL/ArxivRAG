{"title": "POLYNOMIAL COMPOSITION ACTIVATIONS: UNLEASHING THE DYNAMICS OF LARGE LANGUAGE MODELS", "authors": ["Zhijian Zhuo", "Ya Wang", "Yutao Zeng", "Xiaoqing Li", "Xun Zhou", "Jinwen Ma"], "abstract": "Transformers have found extensive applications across various domains due to the powerful fitting capabilities. This success can be partially attributed to their inherent nonlinearity. Thus, in addition to the ReLU function employed in the original transformer architecture, researchers have explored alternative modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment representational capacity. In this paper, we propose a novel category of polynomial composition activations (PolyCom), designed to optimize the dynamics of transformers. Theoretically, we provide a comprehensive mathematical analysis of PolyCom, highlighting its enhanced expressivity and efficacy relative to other activation functions. Notably, we demonstrate that networks incorporating PolyCom achieve the optimal approximation rate, indicating that PolyCom networks require minimal parameters to approximate general smooth functions in Sobolev spaces. We conduct empirical experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. By substituting conventional activation functions with PolyCom, we enable LLMs to capture higher-order interactions within the data, thus improving performance metrics in terms of accuracy and convergence rates. Extensive experimental results demonstrate the effectiveness of our method, showing substantial improvements over other activation functions. Code is available at https://github.com/BryceZhuo/PolyCom.", "sections": [{"title": "INTRODUCTION", "content": "Transformers (Vaswani et al., 2017) have revolutionized the field of deep learning, facilitating unprecedented advancements in natural language processing (Radford et al., 2019), computer vision (Dosovitskiy et al., 2021), and beyond (Dong et al., 2018; Arnab et al., 2021). Characterized by their attention mechanisms, transformers excel at capturing intricate relationships within data, making them indispensable in contemporary machine learning applications. However, despite their widespread success, there remain opportunities for further refinement, particularly concerning the selection of activation functions. The activation function plays a crucial role in determining the output of each neuron within a neural network. Traditionally, simple nonlinearities such as Rectified Linear Unit (ReLU) (Nair & Hinton, 2010) and its variants (Hendrycks & Gimpel, 2016; So et al., 2021) have been favored due to their computational efficiency and ease of implementation. Although effective, these activation functions are inherently limited in their ability to model complex higher-order relationships within data. This limitation can be particularly restrictive in transformer architectures, where the ability to capture subtle and complex dependencies is essential.\nIn this paper, we introduce a novel category of polynomial composition activation functions (PolyCom), specifically engineered to enhance the performance of transformer architectures. In contrast to conventional activation functions, which are predominantly linear or piecewise linear, polynomial composition activations facilitate the modeling of more complex patterns within data. This augmentation in the activation function's expressiveness endows the model with superior expressive capacity, enabling it to capture higher-order interactions that might otherwise be neglected. Unlike other forms of polynomials ((Hornik et al., 1989; Trefethen, 2019)) that suffer from inadequate approximation, exploding values, and oscillatory behavior, we demonstrate that PolyCom possesses a more potent expressive capability than both ReLU and traditional polynomials and achieves optimal approximation within Sobolev space.\nWe posit that the integration of polynomial composition activations within transformer models can lead to enhanced performance in tasks requiring intricate data interpretation. To evaluate this hypothesis, we conducted comprehensive experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. These evaluations were performed across various benchmarks, assessing the performance of transformers employing polynomial composition activations in comparison to those utilizing traditional activation functions. The results indicate that the proposed method not only improves model accuracy, but also accelerates convergence rates, thereby suggesting that polynomial composition activations provide a substantive advantage in deep learning applications.\nThe main contributions of this paper are summarized in the following.\n\u2022 We propose a new activation function PolyCom which is a composition of the polynomial and other types of function. In particular, we introduce two instances of PolyCom: PolyReLU and PolyNorm, and details its integration into the transformer architecture.\n\u2022 Theoretically, we derive bounds on the number of trainable parameters required for PolyReLU networks to approximate ReLU networks, and vice versa. Additionally, we show that a PolyReLU network of size $O(e^{-d/n})$ can approximate any function in Sobolev spaces with error tolerance $e$, achieving optimal approximation rates.\n\u2022 Empirically, we validate the effectiveness of this new activation function on LLMs with both 1B dense models and MoE models with 1B active and 7B total parameters. The results of both models demonstrate that PolyCom can accelerate the converging speed and significantly outperform SwiGLU, GELU, and ReLU et al.\nThe outline of this paper is structured as follows: In Section 2, we present the mathematical formulation of PolyCom and discuss its integration within transformer architectures. Section 3 delivers a comprehensive theoretical analysis of PolyCom, emphasizing its enhanced expressivity and effectiveness. In Section 4, we provide a detailed account of our experimental results involving large language models (LLMs). Section 5 provides an overview of related work in the field of activation functions and their applications in transformer models. Finally, we conclude the paper and outline potential directions for future research."}, {"title": "POLYNOMIAL COMPOSITION ACTIVATION FUNCTION", "content": "In this section, we present the mathematical formulation of the polynomial composition activation function (PolyCom) and detail its integration into the transformer architecture."}, {"title": "PolyCom", "content": "The study of the polynomial activation function can be traced back to the seminal work of Hornik et al. (1989), which showed that neural networks with polynomial activation are not dense within the space of continuous functions. Additionally, empirical evidence has shown that deep neural networks employing pure polynomial activations tend to underperform (Trefethen, 2019). To overcome these limitations, we propose PolyCom, a novel composition of polynomial and other functions. Specifically, we explore two composition approaches:\n$\\begin{cases} \\text{Type I:} & x \\rightarrow \\sum_{i=0}^{r} \\alpha_i \\rho^{i}(x), \\\\ \\text{Type II:} & x \\rightarrow \\sum_{i=0}^{r} \\alpha_i \\rho(x^{i}), \\end{cases} \\alpha_i \\in \\mathbb{R},$\nwhere $r \\in \\mathbb{N}$ denotes the order of PolyCom and $\\rho$ represents an arbitrary functions such as ReLU, PReLU, Sigmoid, SiLU, or normalization. The key distinction between the two approaches lies in whether the function is composed before or after the power operation. It can be theoretically shown that both approaches have equivalent expressivity, provided that $\\rho$ is a non-linear function. This is because polynomial terms are symmetric with respect to composition, allowing both Type I and Type II to approximate similar function classes. In other words, rearranging the order of $\\rho$ and the polynomial powers does not affect the ability to approximate complex non-linear functions. In practice, we use a third-order PolyCom ($r = 3$) with trainable coefficients $\\alpha_i$. For initialization, we set $a_i = 1/r$ for $i = 1, 2, ..., r$ and $a_0 = 0$.\nFor Type I PolyCom, we specifically consider a composition involving the ReLU function due to its simplicity, which we term PolyReLU. An $r$-order PolyReLU is defined as:\n$\\text{PolyReLU}(x) = \\sum_{i=0}^{r} a_i \\text{ReLU}^{i}(x),$\nwhere$\\text{ReLU}^{i}(x) = \\max\\{x,0\\}^{i}$. This formulation can be seen as an extension of both ReLU and square ReLU.\nFor Type II PolyCom, we introduce PolyNorm, which normalizes the powers to ensure consistent magnitudes across terms:\n$\\text{PolyNorm}(x) = \\sum_{i=0}^{r} a_i \\frac{x^{i}}{\\|x^{i}\\|_2},$\nwhere $x^{i} = [x_1, x_2, ..., x_n]^T$ represents element-wise exponentiation, and $\\| \\cdot \\|_2$ denotes the $L_2$ normalization."}, {"title": "Integration into Transformer", "content": "The transformer architecture (Vaswani et al., 2017) consists of two alternating modules, Multi-Head Attention (MHA) and position-wise Feed-Forward Networks (FNN). Activation functions predominantly influence the performance of FFN layers. We begin by formalizing the common paradigm of FFN,\n$\\text{FFN}(x) = \\rho(xW_1)W_2$\nwhere $\\rho$ represents the activation function such as ReLU, GeLU, PolyReLU, or PolyNorm. We replace the traditional activation function with our proposed PolyCom variants to enhance model capacity and performance, as illustrated in Figure 2."}, {"title": "THEORETICAL ANALYSIS", "content": "As discussed in Section 2, PolyReLU and PolyNorm have equivalent expressivity. To streamline the analysis, we focus solely on the theoretical properties of PolyReLU, specifically its expressivity and effectiveness. Additional, nonlinear activations such as GeLU and SwiGLU can be locally approximated by Taylor polynomials around the origin, which allows us to primarily compare PolyReLU with ReLU and polynomial activations. To avoid confusion, we refer to networks that use ReLU activations as ReLU networks, and those that use PolyReLU activations as PolyReLU networks."}, {"title": "APPROXIMATING RELU NETWORKS BY POLYRELU", "content": "In this subsection, we present theoretical results on approximating ReLU networks using PolyReLU networks. The following lemma shows that ReLU, ReLU2, and polynomial activation are special cases of PolyReLU activation, highlighting the superior expressivity of PolyReLU. This implies that PolyReLU has stronger approximation abilities with fewer trainable parameters compared to ReLU and other polynomial activations."}, {"title": "Lemma 1", "content": "ReLU, ReLU2 and polynomial activation can be represented by PolyReLU."}, {"title": "Proof of Lemma 1", "content": "For ReLU activation, set $a_1 = 1, a_i = 0, \\forall i \\neq 1$, leading to PolyReLU(x) = ReLU(x).\nFor ReLU2 activation, set $a_2 = 1, a_i = 0, \\forall i \\neq 2$, giving PolyReLU(x) = ReLU2(x).\nFor a general polynomial activation, observe that for $\\forall x \\in \\mathbb{R}$ and $i \\in \\mathbb{N}$:\n$x^i = \\text{ReLU}^i(x) + (-1)\\text{ReLU}^i(-x), \\forall x \\in \\mathbb{R}, \\forall i \\in \\mathbb{N}$.\nThus, for any polynomial activation of order $r$,\nPoly(x) = PolyReLU$_1$(x) + PolyReLU$_2$(-x),\nwhere PolyReLU$_1$(x) = $\\sum_{i=0}^{r} a_i\\text{ReLU}^i(x)$ and PolyReLU$_2$(x) = $\\sum_{i=1}^{r} (-1)^i a_i\\text{ReLU}^i(x)$."}, {"title": "Theorem 1", "content": "Let $f : [-1,1]^d \\rightarrow [-1,1]$ be a ReLU network with depth L and width K. Then, there exists a PolyReLU network $g : [-1,1]^d \\rightarrow [-1,1]$ of size O(LK) such that\n$f(x) = g(x), \\text{ for } \\forall x \\in [-1,1]^d$."}, {"title": "APPROXIMATING POLYRELU WITH RELU NETWORKS", "content": "In this part, we give theoretical results on approximating PolyReLU networks using ReLU networks. The following Lemma 2 demonstrates that the PolyReLU activation can be approximated by a ReLU network within a given error tolerance."}, {"title": "Lemma 2", "content": "For the activation PolyReLU(x) = $\\sum_{i=0}^{r} a_i\\text{ReLU}^i(x), x \\in [-1,1]$ with $a_i \\in [-1, 1]$. Given any $\\epsilon \\in (0, 1)$, there exists a ReLU network $f : [-1,1] \\rightarrow [-1,1]$ with size $O(\\ln^2(1/\\epsilon))$, such that\n$\\max_{x\\in[-1,1]} |f(x) - \\text{PolyReLU}(x)| < \\epsilon$."}, {"title": "APPROXIMATION OF GENERAL SMOOTH FUNCTION", "content": "Similar to Yarotsky (2017); Boull\u00e9 et al. (2020), we also explore the universal approximation capabilities of PolyReLU networks in the context of Sobolev spaces (Adams & Fournier, 2003). Specifically, we show that PolyReLU networks achieve the optimal approximation rate within these spaces, meaning that PolyReLU networks require minimum parameters to approximate general smooth functions in Sobolev spaces, compared with networks with the other activation.\nThe definition of Sobolev space $W^{n,\\infty}([-1,1]^d)$ is stated below. The set $[-1,1]^d$ can be replaced by any compact set in $\\mathbb{R}^d$, we use it just for the sake of brevity."}, {"title": "Definition 1", "content": "(Sobolev Spaces). For $n, d \\in \\mathbb{N}$, Sobolev space $W^{n,\\infty}([-1,1]^d)$ is defined as\n$W^{n,\\infty}([-1,1]^d) = \\{ f \\in L^{\\infty} ([-1,1]^d) | \\| f \\|_{W^{n,\\infty}([-1,1]^d)} < \\infty \\}$,\nwith the norm which defined as the following\n$\\|f\\|_{W^{n,\\infty}([-1,1]^d)} = \\max_{n:||n||_1\\leq n} \\text{ ess sup}_{x\\in [-1,1]^d} \\| D^{n}f(x) \\|_{\\infty},$\nwhere $n \\in \\mathbb{N}^d$ and $D^{n}f$ is the respective weak derivative of $f$, and ess sup means the essential supremum in functional analysis.\nIntuitively, a Sobolev space is a space of functions endowed with a weaker notion of smoothness compared to differentiability and possessing generalized derivatives. The Sobolev space $W^{n,\\infty}([-1,1]^d)$ contains functions from $C^{m-1} ([-1,1]^d)$ which consists of functions whose derivatives of order $n - 1$ are Lipschitz continous. In the sequel, we mainly consider the unit ball within $W^{n,\\infty}([-1,1]^d)$, which is defined as follows\n$F_{n,d} = \\{ f \\in W^{n,\\infty} ([-1,1]^d) | \\| f \\|_{W^{n,\\infty}([-1,1]^d)} \\leq 1 \\}$.\nWith the above definitions established, we can present the following main results. We provide an upper bound on the size of PolyReLU networks required to approximate any function in $F_{n,d}$."}, {"title": "Theorem 3", "content": "Suppose that $d, n \\in \\mathbb{N}$ and $\\epsilon \\in (0,1)$. For any $f \\in F_{d,n}$, there exists a PolyReLU network $g$ with size $O(\\epsilon^{-d/n})$ that can approximate $f$ at a given error tolerance $\\epsilon$, i.e.,\n$\\max_{x\\in[-1,1]^d} \\| f(x) - g(x) \\|_{\\infty} < \\epsilon$."}, {"title": "EXPERIMENTS", "content": "In this section, we demonstrate the expressivity and effectiveness of PolyCom within the transformer through experiments on LLMs."}, {"title": "SETUP", "content": "Baseline. We evaluate PolyCom across two series of models: a 1B dense model and a Mixture of Experts (MoE) model with 1B active and 7B total parameters. The 1B dense model contains approximately 1.3 billion parameters with an architecture similar to Llama 2 (Touvron et al., 2023). For the MoE model, we use the OLMOE framework (Muennighoff et al., 2024), which activates 1.3B parameters out of a total of 6.9B parameters. Both models are trained from scratch. We compare the performance of PolyCom with several activation functions, including ReLU, square ReLU, GELU, and SwiGLU. All experiments are conducted on Nvidia A100-80G GPUs, 32 GPUs for the dense model, and 64 GPUs for the MoE model.\nModel Configuration. For the dense model, the transformer consists of 24 layers with hidden size $d_{model} = 2048$ and 16 attention heads. In the MoE model, the transformer is composed of 16 layers, with a hidden size of $d_{model} = 2048$, 16 attention heads, and 64 experts. To maintain a consistent number of trainable parameters across all activation functions, we adjust the intermediate size accordingly. Specifically, for SwiGLU, the intermediate size is set to two-thirds that of the other activations in all experiments.\nDatasets. The dense model is trained on the RedPajama-1T dataset \u00b9 (Computer, 2023), which was developed by the open-source AI community to enable competitive performance against proprietary models. The MoE model is trained on the OLMOE Mix dataset \u00b2 (Muennighoff et al., 2024).\nHyperparameters. Unless otherwise specified, we use a third-order PolyCom by default and initialize the coefficients as $a_i = 1/3$ for $i = 1,2,3$ and set $a_0 = 0$. Model weights are randomly initialized. For optimization, we apply the AdamW optimizer with $\\beta_1 = 0.9$ and B2 = 0.95. All models are trained on sequences of 4096 tokens. For the dense model, we set the initial learning rate to 3e-4, decaying to 1.5e-5 using a cosine scheduler. The MoE model starts with a learning rate of 4e-4, also decaying according to a cosine schedule.\nEvaluation To evaluate the performance of LLMs with PolyCom, we use a wide range of open benchmarks, including ARC-Easy (Clark et al., 2018), ARC-Challenge (Clark et al., 2018), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), SciQ (Welbl et al., 2017), CoQA (Reddy et al., 2019), Winogrande (Sakaguchi et al., 2021), MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), COPA (Gordon et al., 2012), CSQA (Talmor et al., 2019), OBQA (Mihaylov et al., 2018), and SocialIQA (Sap et al., 2019). We utilize the LM Eval Harness (Gao et al., 2023) for standardized performance evaluation."}, {"title": "RESULTS ON DENSE MODEL", "content": "Training Dynamics of 1B Dense Model. Figure 1 compares the training dynamics of the 1B dense model across different activation functions. As shown in the figure, models using PolyReLU and PolyNorm exhibit lower training loss and validation perplexity throughout the training process compared to models utilizing other activation functions. This indicates that PolyCom accelerates the convergence of LLMs. The models with PolyReLU and PolyNorm also consistently outperform others in downstream tasks by large margins, highlighting the advantage of PolyCom in improving the overall expressivity and effectiveness of LLMs.\nDowmstream Evaluation. Table 1 presents the training loss, validation perplexity, and downstream task accuracy (%) after processing 250 billion training tokens. The downstream tasks include ARC-Easy, ARC-Challenge, HellaSwag, PIQA, SciQ, and Winograde. More detailed results are provided in Appendix D. The results clearly demonstrate that the PolyCom family (PolyReLU and PolyNorm) outperforms the other activation functions. For instance, PolyNorm outperforms SwiGLU by an average margin of 1.21% across six downstream tasks. This underscores the expressivity and efficiency of PolyCom as an activation function in transformer models."}, {"title": "RESULTS ON MOE MODEL", "content": "Our experiments with MoE modes are based on OLMOE-1B-7B, which has 1 billion activate parameters and 7 billion total parameters (Muennighoff et al., 2024). Due to computational constraints, we compare only PolyNorm activation function, shown to perform best in dense models, with the widely used SwiGLU activation function, which is commonly employed in current LLM architectures.\nTraining dynamics of MoE model. In Figure 3, we report the training and validation loss of MoE models trained on 200 billion tokens. Models using PolyNorm consistently show lower losses compared to those using SwiGLU, indicating that PolyNorm enables faster learning. Figure 4 shows the downstream performance on HellaSwag, MMLU Var\u00b3, ARC-Challenge, and SciQ. PolyNorm outperforms SwiGLU on all tasks, with notable improvements, demonstrating superior generalization capabilities.\nDowmstream Evaluation. Table 2 presents the validation losses on 11 datasets. PolyNorm consistently achieves lower validation losses than SwiGLU across all datasets, with an average improvement of 0.02. In Table 3, we also observe that PolyNorm outperforms SwiGLU on 8 downstream tasks. These results highlight the superior performance of models using the PolyNorm activation function. Additional results can be found in Appendix E."}, {"title": "ABLATIONS AND ANALYSIS", "content": "Order of PolyCom. We first investigate the effect of different orders of PolyCom. We vary the order $r$ of PolyReLU in the set $\\{2, 3, 4\\}$ and plot the results in Figure 5(a). As seen, the convergence speed improves as the order increases. However, there is no noticeable difference between orders 3 and 4 in terms of convergence speed. Additionally, increasing the order can lead to computational overhead and overflow issues, particularly when using low-precision arithmetic. Based on these observations, we select $r = 3$ as the default order for PolyCom in our experiments, balancing both performance and computational efficiency.\nDifferent Polynomial Composition Functions. We evaluate the impact of different polynomial composition functions by comparing PolyReLU, PolyPReLU, PolyNorm, and PolyReLUNorm in Figure 5(b). Our results indicate that PolyNorm, which uses normalization as the composition function, achieves the lowest training loss and best overall performance. This suggests that normalization plays a key role in stabilizing training and enhancing the model's ability to generalize. In contrast, combining ReLU with normalization (PolyReLUNorm) provides intermediate results, suggesting that more complex compositions do not always lead to better outcomes.\nVariants of ReLU. In Figure 5(c), we compare different variants of the ReLU activation function, including ReLU and ReLU2. PolyReLU consistently outperforms both ReLU and ReLU2 across all tasks, highlighting the benefits of using polynomial composition. This result reinforces the hypothesis that introducing higher-order terms through PolyCom enables the model to capture more complex data interactions, thus improving the expressivity of the activation function without significantly increasing model size or complexity.\nRank of Weights. To understand how PolyCom enhances model performance, we analyze the rank of the weights in each FNN layer of the transformer. We use the effective rank (Roy & Vetterli, 2007) to measure the effective dimensionality of weights and its definition is in Appendix C.2. Figure 6 shows that PolyReLU and PolyNorm result in higher weight ranks compared to other activation functions such as SwiGLU, GELU, and ReLU. A higher rank in the weight matrices indicates greater capacity for representing complex patterns in the data. These findings suggest that PolyCom improves the expressibility of transformers by allowing the FNN layers to better utilize their parameters, ultimately leading to better generalization on downstream tasks.\nLayer-wise Similarity. We further analyze the layer-wise similarity of hidden states using cosine similarity, as illustrated in Figure 7. For both dense and MoE models, we compare SwiGLU with PolyNorm. The results reveal that PolyNorm consistently maintains lower layer-wise similarity compared to SwiGLU, indicating that PolyNorm promotes greater diversity between layers. This diversity likely enables the model to learn more complex representations, as deeper layers are not merely replicating the functionality of earlier ones. Notably, the gap in cosine similarity between PolyNorm and SwiGLU widens in the deeper layers, which are generally more crucial for downstream task performance. This increased diversity across layers enhances the model's ability to capture complex relationships, thereby improving the overall effectiveness of LLMs."}, {"title": "RELATED WORK", "content": "The design of activation functions has been a critical area of research in neural networks, directly influencing the performance and capabilities of deep learning models. Early activation functions like Sigmoid and Tanh were widely used due to their smooth nonlinear transformations (Goodfellow et al., 2016). However, these functions faced challenges such as vanishing gradients, making it difficult to train deep networks effectively. The introduction of the Rectified Linear Unit (ReLU) (Nair & Hinton, 2010) mitigated some of these issues by offering a simple, non-saturating nonlinearity, which has since become a standard in many deep learning applications. Variants of ReLU, such as Leaky ReLU (Maas et al., 2013) and Parametric ReLU (PReLU) (He et al., 2015), were developed to address the \"dying ReLU\" problem by allowing a small, non-zero gradient when the input is negative. Other functions, like the Exponential Linear Unit (ELU) (Clevert, 2015), aimed to provide smoother activation profiles, resulting in better generalization and faster convergence in certain tasks. Moreover, Manessi & Rozza (2018) proposed a combination of weighted base activation functions for further enhancement.\nPolynomial activation functions (Hornik et al., 1989; Oh et al., 2003), although less commonly used, have been studied in various contexts for their ability to model higher-order, complex relationships more effectively. For instance, Lokhande et al. (2020) introduced Hermite polynomial activations to improve pseudo-label accuracy, while Chrysos et al. (2020) proposed polynomial networks, \u03a0-nets, which apply to various domains such as image and audio processing. Building on this, Chrysos et al. (2023) utilized regularization techniques to enhance the performance of polynomial networks. These works highlight the potential of polynomial functions to increase the expressiveness of neural networks by capturing intricate, higher-order interactions. On the theoretical front, the expressivity and approximation power of polynomial functions have been rigorously explored (Kileel et al., 2019; Kidger & Lyons, 2020; Kubjas et al., 2024).\nThe choice of activation function in transformers has also become an important area of research. Originally developed for natural language processing, transformers (Vaswani et al., 2017) have been effectively adapted for diverse tasks, including image recognition, speech processing, and reinforcement learning. Despite their broad applicability, the activation functions predominantly utilized in transformers, ReLU and GELU, have seen minimal evolution. Recent studies, however, have begun to explore alternatives to these conventional activations. For example, the Swish activation (Ramachandran et al., 2017; Shazeer, 2020) and the Mish activation (Misra, 2019) are smooth and non-monotonic functions that offer potential benefits in model performance and training stability. Additionally, Gated Linear Units (GLU) were proposed by Dauphin et al. (2017), with SwiGLU (Shazeer, 2020), a prominent variant, being used in models such as LLaMA-Series (Touvron et al., 2023)."}, {"title": "CONCLUSIONS", "content": "In this paper, we introduce the Polynomial Composition Activation (PolyCom) and demonstrate its effectiveness within transformer models. By enabling the capture of higher-order interactions, PolyCom enhances both the accuracy and convergence rates of these models. Our experiments, conducted across different large language model architectures and multiple benchmarking datasets, confirm that PolyCom consistently outperforms conventional activation functions. Furthermore, ablation studies indicate that PolyCom increases model expressivity by elevating weight rank and reducing redundancy across layers. These findings underscore the significant potential of polynomial-based activations to improve transformer models, thereby paving the way for future research endeavors."}, {"title": "A OMITTED PROOFS", "content": "In this section, we provide the proofs that were omitted in the main body of the paper. The following proofs build upon the work of Yarotsky (2017); Telgarsky (2017); Boull\u00e9 et al. (2020)."}, {"title": "PROOF OF LEMMA 2", "content": "The proof of Lemma 2 leverages Lemma 3.4 from Telgarsky (2017), which we state below."}, {"title": "Lemma A.1", "content": "(Lemma 3.4 in Telgarsky (2017)). Let $\\epsilon \\in (0,1)$ be given. Suppose $p: [0,1]^d \\rightarrow [-1,1]$ be ar order polynomial with $s$ monomials and coefficients within $[-1,1]$. Then there exists a ReLU network $f : [0,1]^d \\rightarrow [-1,1]$ of size $O(\\min\\{sr \\ln(sr/\\epsilon), sd \\ln^2(dsr/\\epsilon)\\})$, such that $\\max_{x\\in[0,1]^d} |p(x) - f(x)| < \\epsilon$."}, {"title": "Proof of Lemma 2", "content": "Using this result, we now proceed with the proof of Lemma 2.\nFirst, we observe that PolyReLU(x) = Poly(ReLU(x), where Poly(x) = $\\sum_{i=0}^{r} a_i x$ for $x \\in [-1,1]$. By Lemma A.1, there exists a ReLU network $f_1 : [0,1] \\rightarrow [-1,1]$ of size $O(\\ln^2(1/\\epsilon))$ such that:\n$\\max_{x\\in [0,1]} |f_1(x) - \\text{Poly}(x)| < \\epsilon$.\nThus, we construct $f = f_1 \\circ ReLU$ for inputs $x \\in [-1, 1]$. This yields that:\n$\\max_{x\\in[-1,1]} |f(x) - \\text{PolyReLU}(x)| = \\max_{x\\in[-1,1]} |f_1 \\circ ReLU(x) - \\text{PolyReLU}(x)| = \\max_{x\\in[-1,1]} |f_1 (ReLU(x)) - \\text{Poly}(ReLU(x))| = \\max_{x\\in [0,1]} |f_1(x) - \\text{Poly}(x)| < \\epsilon$.\nSince $f_1$ is a ReLU network, the constructed function $f = f_1 \\circ ReLU$ is also a ReLU network, completing the proof."}, {"title": "PROOF OF THEOREM 1", "content": "The proof is an elementary extension of Lemma 1."}, {"title": "Proof of Theorem 1", "content": "Using Lemma 1, we can represent the ReLU activation on $\\mathbb{R}$ using a PolyReLU activation. Thus, we replace each ReLU activation in the ReLU network $f$ with PolyReLU to construct a new network $g$. Obviously, such $g$ satisfies the above requirements. Hence, the size and structure remain equivalent, and $g$ serves as the PolyReLU network equivalent to the ReLU network."}, {"title": "PROOF OF THEOREM 2", "content": "The lower bound of Theorem 2 follows directly from Theorem 11 in Liang & Srikant (2017), restated here for clarity:"}, {"title": "Lemma A.2", "content": "(Theorem 11 in Liang & Srikant (2017)). Suppose function $f : [0,1]^d \\rightarrow \\mathbb{R}$ is differentiable and strongly convex. Let $\\epsilon \\in (0,1)$ be given and $\\hat{f}$ be a ReLU network. If $\\max_{x\\in[0,1]^d} | f(x) - \\hat{f}(x)\\, then the network size of $\\hat{f}$ is at least $\\Omega(\\ln(1/\\epsilon))$."}, {"title": "Proof of Theorem 2", "content": "Denote $g_i$ as the i-th layer of PolyReLU neteeork $f$ for $1 \\leq i \\leq L$, such that:\n$g = g_L \\circ g_{L-1} \\circ ... \\circ g_1$."}, {"title": "B ACTIVATION FUNCTIONS", "content": "We provide definitions of several commonly used non-linear activation functions in Table 4."}, {"title": "EXPERIMENTAL DETAILS", "content": "Table 5 outlines the model architecture used for the 1B dense model. To ensure comparable numbers of training parameters across different activation functions, we adjust the intermediate sizes accordingly. For SwiGLU, the intermediate size is set to 5504, while for other activation functions, it is set to 8256.\nTable 6 outlines the model architecture used for the MoE models. Similarly, the intermediate size for SwiGLU is set to 1024, while for other activation functions, it is set to 1536."}, {"title": "DEFINITION OF EFFECTIVE RANK", "content": "We adopt the concept of effective rank from Roy & Vetterli (2007) to measure the effective dimensionality of a matrix. Given a matrix A with Singular Value Decomposition (SVD) $A = U \\Sigma V^T$, where $\\Sigma$ is a diagonal matrix containing singular values $\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_n \\geq 0$, we define the singular value distribution as $p_i = \\sigma_i/\\Sigma_{j=0}^n \\sigma_j$, $i \\in [n]$. The effective rank of A is then given by:\n$\\text{Erank}(A) = \\exp\\left(-\\sum_{i=0}^{n} p_i \\ln p_i\\right)$."}]}