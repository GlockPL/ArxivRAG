{"title": "Select2Plan: Training-Free ICL-Based Planning through VQA and Memory Retrieval", "authors": ["Davide Buoso", "Luke Robinson", "Giuseppe Averta", "Philip Torr", "Tim Franzmeyer", "Daniele De Martini"], "abstract": "This study explores the potential of off-the-shelf Vision-Language Models (VLMs) for high-level robot planning in the context of autonomous navigation. Indeed, while most of existing learning-based approaches for path planning require extensive task-specific training/fine-tuning, we demonstrate how such training can be avoided for most practical cases. To do this, we introduce Select2Plan (S2P), a novel training-free framework for high-level robot planning which completely eliminates the need for fine-tuning or specialised training. By leveraging structured Visual Question-Answering (VQA) and In-Context Learning (ICL), our approach drastically reduces the need for data collection, requiring a fraction of the task-specific data typically used by trained models, or even relying only on online data. Our method facilitates the effective use of a generally trained VLM in a flexible and cost-efficient way, and does not require additional sensing except for a simple monocular camera. We demonstrate its adaptability across various scene types, context sources, and sensing setups. We evaluate our approach in two distinct scenarios: traditional First-Person View (FPV) and infrastructure-driven Third-Person View (TPV) navigation, demonstrating the flexibility and simplicity of our method. Our technique significantly enhances the navigational capabilities of a baseline VLM of approximately 50% in TPV scenario, and is comparable to trained models in the FPV one, with as few as 20 demonstrations.", "sections": [{"title": "I. INTRODUCTION", "content": "Path planning for vehicles is a longstanding problem in robotics, traditionally addressed using model-based or Reinforcement Learning (RL) approaches [1], [2], [3]. However, methods that directly learn from experience often struggle when confronted with ambiguous or unfamiliar scenarios. Interestingly, recent research has shown that Large Language Models (LLMs) and Vision-Language Models (VLMs), demonstrate surprising reasoning capabilities that can be adapted for proposing robot paths in arbitrary scenes [4]. Indeed, these models excel at incorporating common-sense reasoning acquired during their long pretraining phase [5]. This ability is crucial in robotics operations, where the deployment scenario rarely aligns perfectly with the training dataset [6], [7]. While methods like LoRA [8] reduce the computational cost of fine-tuning LLMs and VLMs, they still require domain-specific data, which can be costly to obtain. In parallel, In-Context Learning (ICL) and Retrieval-Augmented Generation (RAG) have shown promising results in scoping the ability of LLMs at deployment time with no additional fine-tuning, mitigating these costs.\nOur novel framework Select2Plan (S2P) - combines Visual Question-Answering (VQA) and ICL with VLMs in a training-free manner, showing remarkable flexibility across various scenes, contexts, and setups.\nMore specifically, we formulate the planning problem as a VQA task using visual prompting. A high-level overview of the approach can be observed in Fig. 1. Inspired by [9] and [10], we generate a set of position candidates in the image space and use them as part of a query mechanism to a VLM, to extract the next robot move. We combine this approach with ICL to enhance the model's reliability: we retrieve similar successful samples and use them, along with the current annotated image, as context to support the model's generalization. In this way, we can generate a robust path, which can span multiple planning steps within a single response, in contrast to the iterative approach taken by [9]."}, {"title": "II. RELATED WORK", "content": "Our approach is at the intersection of robot navigation, planning, LLMS, VLMs, and ICL. Here, we review the most relevant works to highlight both similarities and differences, emphasizing the unique aspects of our methodology.\nTraditional methods for robot navigation tasks, such as Object Navigation and Visual Navigation, have historically relied on RL to train policies for complex tasks. Works like those of [1], [2], [3] employed deep RL models to learn navigation policies based on visual input. More recently, transformer-based models have emerged as a powerful alternative, often yielding better generalization due to their capacity to model long-range dependencies in data [14], [15], [16].\nLately, the attention has shifted to the use of LLMs and VLMs for several tasks, such as planning, navigation and manipulation [17], [18]. VLMs have shown great promise for high-level decision-making in robotics, as they integrate visual perception with language-based reasoning. [19] demonstrated how a pre-trained LLM can be utilized in zero-shot settings to control robots. [4], [18], [20] similarly explored how these models can handle real-world tasks without additional training, proving their flexibility in diverse scenarios. ICL has gained traction in tasks that require minimal data adaptation. Recent works, such as [21], have highlighted the effectiveness of ICL integrated with memory-based retrieval for robotics applications. However, these methods typically focus on a single setup and often require external modules for object recognition or advanced techniques to extract textual features. Our approach diverges by adopting a zero-training pipeline, which integrates Image-Based ICL coupled with VQA to further enhance the capabilities of VLMs, without the need of additional techniques and collecting just an handful of samples to achieve the task. This, allows our framework to generalize across both FPV and TPV setups without the need for specialized sensors or extensive pre-training. Our model can efficiently generate navigational plans from Image-Text pairs, making it more adaptable to different scenarios and goals and completely different setups than previous methods."}, {"title": "III. METHODOLOGY", "content": "After discussing the general framework, this section details the specific declinations for the two deployment scenarios: TPV and FPV. Figure 2 depicts the two in detail."}, {"title": "A. Vision Language Model and In-Context Learning", "content": "To provide context, we use a History-Injection ICL, where a fictitious chat conversation is created. In the chat, episodes retrieved from a memory database are split in query (annotated image and prompt) and answer, and injected into the model as turns of conversation as question and answer. This yields a multi-turn conversation of k turns, with k equal to the number of ICL samples. Finally, we explicitly ask the model, based on its previous responses, to process a new image.\nAs we can see from Fig. 2, our proposed approach is composed of five main components: an experiential memory, a sampler, an episodic memory, an annotator and a prompt templating engine, in addition to the VLM and the controller. We will discuss them in the following.\n1) Experiential Memory: The Experiential Memory is a collection of annotated images that can be used as context to influence the answer of the VLM via ICL. These experiences can be gathered in different ways, and we discuss different combinations in our result section, where we test with data coming from the same or different environments, or even directly processing online footage.\n2) Sampler: The sampler aims to select the most appropriate samples from the Experiential Memory to be presented to the VLM as context. We feed a Vision Transformer (ViT) a Swin Transformer [22] \u2013 with the live camera image to recover similar situations from the Experiential Memory. We represent the image \u2013 and the Experiential Memory samples as the average output of the last hidden layer and obtain a feature vector which we employ as the query.\nEmpirically, we observed that building a diverse context (both similar and different situations) benefits the model's generalisation to the current situation. To balance out the similarity, we incorporated a re-ranking algorithm adapted to our framework into the retrieving process. We employ a Maximal Marginal Relevance (MMR) [23], which aims to reduce redundancy and increase sample diversity according to a combined criterion of query relevance and novelty of information. MMR is defined as follows:\n$\\MMR(Q, M, C') = arg \\max [\\lambda < S_i, Q > + \\hfill \\newline \\qquad \\qquad \\qquad S_i \\in M \\backslash C \\qquad (1 - \\lambda) \\max_{S_j \\in C} < S_i, S_j > ] \\qquad (1)$", "latex": ["\\MMR(Q, M, C') = arg \\max [\\lambda < S_i, Q > + \\hfill \\newline \\qquad \\qquad \\qquad S_i \\in M \\backslash C \\qquad (1 - \\lambda) \\max_{S_j \\in C} < S_i, S_j > ] \\qquad (1)"]}, {"title": "B. Adaptations to the FPV Setting", "content": "We designed our framework with the AI2-THOR simulator [24] as the platform, where the discrete robot actions comprise the robot's and camera's movements. As shown in Fig. 2a, we enable the VLM to interpret visual annotations that resemble a control overlay inspired by video-game interfaces. We display the numbers 1 to 7 on a semicircle at the bottom of the image, providing rotational control, where 4 is the neutral MOVE_FORWARD and the others numbers represent various degrees of rotation.\nAdditionally, the model can select LOOK_UP and LOOK_DOWN commands, associated with the non-displayed numbers 8 and 9. Lastly, number 0 is associated with DONE command to end the episode.\nFollowing a structured procedure, we build the Experiential Memory by manually navigating the robot in the AI2-THOR environment. At each timestamp, the operator selects an appropriate action based on the current visual context i.e. a command number \u2013 and provides a natural language explanation, simulating a \u201cthink-aloud\" process. For instance, upon observing a microwave on the left, the explanation would state: \"A microwave is visible on the left, so the system will steer slightly to the left.\u201d The annotated image, the selected command, and the corresponding explanation are inserted in the Experiential Memory. This process aims to imitate human-like physical movements and capture the underlying thought processes that drive these actions.\nFinally, we request the VLM a text description of the environment - the list of objects in the frame and save it in the Episodic Memory. From it, we create a \u201ccircular compass\u201d, as shown in Fig. 3, which rotates along with the agent's rotations and can inform the model's decisions since certain objects can be found near affine items or go out of the field of view due to motion. We also include the last action, the current vertical view status and the previous command list: the VLM at each timestamp is asked not only to provide the current action, but also the next future one, to robustly follow the navigation strategy in act (Fig. 2a)."}, {"title": "C. Adaptations to the TPV Setting", "content": "We follow the setup of [11], [12] in considering a visual servoing system [25], [26], where a robot is controlled by the cloud via cameras installed in the infrastructure. Here, security cameras capture raw frames of the environment, which are then annotated and passed, together with samples from the Experiential Memory, to the VLM to predict the next few trajectory points for the robot to execute. Figure 2b visualises and summarises the framework's components.\nWe start the annotation process of the live frame by identifying the robot's position [11], through a YOLO model and place there the number 0 to make the task embodiment-agnostic. During the system's initial setup, a segmentation mask of the scene's floor is taken through Segment Anything [27] and saved as a binary mask. Importantly, this step is performed only once during the framework's first setup, so it does not handle new objects or obstacles appearing in the scene. Starting from the 0 position, we create concentric circles of numbers equally spaced and with increasing radius while using the mask, we filter out locations that are not traversable. Finally, we ask the VLM to choose a sequence of points from the robot to the end goal, which can be given as an object in the scene or a red circle - clear of obstacles see Fig. 2b.\nAn optional step is to crop the image to comprehend only the labels and the target to optimise the image size passed to the model, reducing costs and increasing focus on the important part of the picture.\nAfter receiving the sequence of points, we map back the numbers to coordinates in image space and follow the path generated through a PID controller [12]. During inference, the sequence produced is at most 3 or 4 points long but once 1 or 2 points are correctly tracked, the new sequence substitutes the old one."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "We preliminary explored open-source and closed-source VLMs; we empirically chose Gemini 1.5 Pro [28] for its accessibility, performance and huge context-width (gemini-1.5-pro-001). Hereinafter, with VLM we implicitly refer to the latter. The experimental setup differs slightly for FPV and TPV. In the following, we will discuss both of them and describe the metrics, data collection and baselines we will use to evaluate our system."}, {"title": "A. First Person View", "content": "Among the several simulators proposed to facilitate the Embodied Navigation tasks [29], [24], [30], we selected AI2-THOR's ObjectNav task, whose goal is to navigate towards a predefined target object.\nWe follow the evaluation procedure of [31], using the same metrics and setup, with different object classes for training and testing."}, {"title": "B. Third Person View", "content": "We evaluate our approach on a custom dataset recorded in four rooms of the Oxford Robotics Institute premises see Fig. 4 where we collected expert trajectories by teleoperating a TurtleBot3 rover. The trajectories recorded encompass a range of difficulty levels, from simple paths with minimal obstacles to more complex routes that include various dynamic and static obstacles, representing real-world navigation challenges. We annotate see Section III-C the images and manually mark the labels overlapping with obstacles or non-traversable locations as dangerous. We compare the results of our model against a zero-shot approach on this dataset. We test different Experiential Memories, composed of scenes from the same or different cameras, called scenarios A and D, respectively see Fig. 5. In addition, we will show results with trajectories performed by a human in the same scenes scenario H simulating the images usually captured by security cameras; this would demonstrate how a person move in an office room, avoiding obstacles such as chairs, boxes, etc. Finally, we will also use short clips of robots from the web [35], [36] \u2013 scenario O to validate how general and different from the target scenario the samples in the context can be, while still allowing the model to understand the task and mimic it successfully.\nThe main metric to evaluate our system is the Trajectory Score (TS), defined as:\n$\\TS = \\frac{\\Sigma_{i=0}^{N} P_{ci}}{\\max(\\text{len}(P_i), \\text{len}(G_i))} TS_i \\in [0,1]  (2)$", "latex": ["\\TS = \\frac{\\Sigma_{i=0}^{N} P_{ci}}{\\max(\\text{len}(P_i), \\text{len}(G_i))} TS_i \\in [0,1]  (2)"]}, {"title": "V. EXPERIMENTAL RESULTS", "content": "Our ICL-based approach in the TPV scenario achieved a maximum TS of 270.70 using context scenario A, which allowed unrestricted retrieval from the database. This performance significantly outperformed the zero-shot approach by approximately 40% demonstrating the robustness of our model when supported by a comprehensive context. Moreover, the model exhibited a 24% reduction in the selection of dangerous points, which would otherwise lead to potential collisions. This reduction is a critical enhancement, as it directly correlates to safer navigation, an essential factor for real-world autonomous applications.\nThe model's capability to avoid hazardous locations was evident in scenario A but improved also across other contexts, including scenarios from online videos and human-driven trajectories. This versatility underscores the model's adaptability and ability to generalize across varying sources of contextual information, demonstrating its robustness in understanding the task at hand. These findings are significant because they illustrate that effective navigation can be achieved with minimal data collection, or even none, leveraging just online data, significantly reducing the costs and time associated with data gathering and model training, making it a practical solution for real-world deployment.\nIn the FPV setup, S2P achieved an average Success Rate (SR) of 46.16% in the scenario most favourable to trained models: known scenes and known objects, where they unquestionably performed much better. This was expected, since our model retrieved from an extremely small database - just 1 episode per object type \u2013 while the best performing model [31] was trained on 8 millions of episodes. In more challenging scenarios, where generalization is required to handle novel scenes and objects, S2P consistently performed well, outperforming even the best one by approximately 10% in Success Rate (SR) on average. Additionally, the Success weighted by Path Length (SPL) metric showed an improvement of around 20% over the best-performing trained model in these scenarios. We hence reduce the number of samples needed by S2P to about 0.0005% with respect to [31]. These results validate the effectiveness of integrating ICL and VLMs for navigation tasks. Our approach achieves performance comparable to models trained extensively on specific tasks but with a handful of demonstrations. By leveraging the pre-training of VLMs, our method is also highly generalisable, capable of identifying different types of objects and adapting to various environmental settings. This adaptability makes our framework suitable for various applications, from robotic navigation in unfamiliar environments to automated surveillance systems using external cameras."}, {"title": "VI. CONCLUSIONS AND FUTURE WORKS", "content": "In conclusion, our experimental results underscore the potential of ICL-based frameworks combined with VLMS for autonomous navigation. Our model significantly outperforms zero-shot baselines and adapts effectively to diverse contexts with minimal data and no specialized training. By intentionally operating under conditions of data scarcity, we demonstrated the robustness of the framework and anticipate further performance improvements as more episodes are incorporated. This work establishes a strong foundation for scalable and flexible navigation systems, paving the way for more intelligent and adaptable autonomous technologies in real-world applications."}, {"title": "$\\D = \\sum_i D_i   D_i \\in {0,1}$", "content": null, "latex": ["\\D = \\sum_i D_i   D_i \\in {0,1}"]}]}