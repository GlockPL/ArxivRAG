{"title": "Collective Innovation in Groups of Large Language Models", "authors": ["Eleni Nisioti", "Sebastian Risi", "Ida Momennejad", "Pierre-Yves Oudeyer", "Cl\u00e9ment Moulin-Frier"], "abstract": "Human culture relies on collective innovation: our ability to\ncontinuously explore how existing elements in our environ-\nment can be combined to create new ones. Language is hy-\npothesized to play a key role in human culture, driving indi-\nvidual cognitive capacities and shaping communication. Yet\nthe majority of models of collective innovation assign no cog-\nnitive capacities or language abilities to agents. Here, we con-\ntribute a computational study of collective innovation where\nagents are Large Language Models (LLMs) that play Little\nAlchemy 2, a creative video game originally developed for\nhumans that, as we argue, captures useful aspects of innova-\ntion landscapes not present in previous test-beds. We, first,\nstudy an LLM in isolation and discover that it exhibits both\nuseful skills and crucial limitations. We, then, study groups\nof LLMs that share information related to their behaviour and\nfocus on the effect of social connectivity on collective per-\nformance. In agreement with previous human and computa-\ntional studies, we observe that groups with dynamic connec-\ntivity out-compete fully-connected groups. Our work reveals\nopportunities and challenges for future studies of collective\ninnovation that are becoming increasingly relevant as Gener-\native Artificial Intelligence algorithms and humans innovate\nalongside each other.", "sections": [{"title": "Introduction", "content": "Human culture evolves through the accumulation of arte-\nfacts, semantic repertoires, and behaviours that become\nmore complex over long time-scales (Creanza et al., 2017;\nSol\u00e9 et al., 2013; Whiten et al., 2021a). While popu-\nlar narratives often emphasize the contributions of lonely\ngeniuses (Montuori and Purser, 1995), a different story\nemerges if we consider historical data on cultural arte-\nfacts (Sol\u00e9 et al., 2013; Eldredge, 2011; Pereira et al., 2023),\ntheoretical analysis (Creanza et al., 2017; Sol\u00e9 et al., 2013),\nhuman behavioural experiments (Derex and Boyd, 2016;\nMason et al., 2008; Brackbill and Centola, 2020) and com-\nputational studies (Lazer and Friedman, 2007; Cantor et al.,\n2021; Brackbill, 2017; Fang et al., 2010). This body of work\nsuggests that human cultural evolution is an inherently col-\nlective process, akin to biological evolution (Creanza et al.,\n2017; Sol\u00e9 et al., 2013): innovations arise in a collective as\nindividuals modify and recombine existing ones in their en-\nvironment. In this work, we contribute novel computational\nevidence to support this hypothesis. We employ groups of\nLarge Language Models (LLMs) to solve innovation tasks,\nexamining how they perform in isolation and how their so-\ncial connectivity affects their collective behaviour.\nWhile many species exhibit culture (Whiten et al., 1999;\nAplin, 2019), cultural change (Aplin et al., 2015), and even\na continuously complexifying cultural repertoire (Whiten\net al., 2021b), humans are unique in their ability to accu-\nmulate innovations (Tennie et al., 2009; Derex, 2021; Boyd\nand Richerson, 1996). Studies aiming at understanding this\nphenomenon have drawn links between collective innova-\ntion and, among others, social learning capacities (Dun-\nbar, 1993a; Lotem et al., 2017), group size (Dunbar, 1993b;\nKline and Boyd, 2010; Derex et al., 2013) and social con-\nnectivity (Lazer and Friedman, 2007; Cantor et al., 2021;\nBrackbill, 2017; Fang et al., 2010; Nisioti et al., 2022).\nDespite a plurality of studies and methodologies, recent\npositions in cultural evolution (CE) warn that the field may\nhave neglected certain mechanisms, such as the role of in-\ndividual cognition in innovation and invention (Singh et al.,\n2021; Perry et al., 2021; Smolla et al., 2021). The majority\nof collective innovation studies model individual inventions\nsolely as random mutations and recombinations of existing\ninnovations (Mason et al., 2008; Cantor et al., 2021; Fang\net al., 2010). Yet a number of hypotheses in human stud-\nies point to the important role that language and, in general,\nadvanced cognitive mechanisms, have played in our evo-\nlution (Dunbar, 1993b; Boyd, 2018; Dunbar, 2014; Smith\net al., 2017). Equipping models of CE with language can\nenable the study of such hypotheses and reduce the artifi-\nciality of experimental set-ups by bringing them closer to\nthe ones used with human participants (Mesoudi, 2021).\nBeyond their capacity to model human language, LLMs\nhave shown an emergent ability to model certain aspects\nof human behaviour, such as fairness in economical deci-\nsions (Horton, 2023), content biases in information trans-\nmission (Acerbi and Stubbersfield, 2023), and convincing\nsocial interactions in realistic social simulation games (Park\net al., 2023). Moreover, LLMs are evermore present as copi-\nlots of human labour and have become actors in the process\nhuman cultural evolution (Brinkmann et al., 2023). A natu-\nral next question is whether they can also be useful in com-\nputational studies of cultural evolution as generative models\nof individuals (Perez et al., 2024).\nHere, we study how groups of LLMs solve collective in-\nnovation tasks. As a test-bed, we employ Little Alchemy\n2 (LA2) \u00b9, an existing creative game where players need to\ncombine items to create new ones. To identify which com-\nbinations are valid, Little Alchemy 2 employs a knowledge\ngraph where items are real-world entities and combinations\nare inspired by our physical reality (for example, 'fire' and\n'water' results in a new item, 'steam'). Little Alchemy 2\nwas recently proposed as a test-bed for the study of human\nexploration, as it poses challenges not present in classically-\nemployed bandit tasks (Br\u00e4ndle et al., 2023). Here, we test a\nsimilar proposal, namely that Little Alchemy 2 can be a use-\nful test-best for studying both human and computational cul-\ntural evolution. While Little Alchemy 2's knowledge graph\nis certainly not a comprehensive database of human cul-\ntural artefacts, it is significantly more realistic than previous\ntest-beds containing a few hand-crafted interactions among\nsymbolic items (Derex and Boyd, 2016; Cantor et al., 2021;\nMigliano et al., 2020). We present a visualization of a small\npart of the knowledge graph of LA2 in Figure 1.\nFirst, we examine an LLM in isolation to probe its\nproblem-solving capacities independently of group influ-\nences. We identify three key challenges related to a) fac-\ntual knowledge: to efficiently explore the search space an\nLLM needs to leverage semantic knowledge about the task,\nb) multi-step reasoning: to reach a target item a player of-\nten needs to craft multiple intermediate items, and c) ex-\nploration: a key feature of LA2 is its open-ended nature.\nAs is common in creative games, no instructions are given\nto the player, who starts with a couple of items and is left\nto explore a vast search space. As previous studies with\nhuman participants have shown, non-random exploration\nstrategies, such as empowerment, are required to explore ef-\nficiently (Br\u00e4ndle et al., 2023; Klyubin et al., 2005). To our\nknowledge, the abilities of LLMs to exhibit such forms of\nexploration have not been studied before.\nTo examine the knowledge and multistep reasoning abil-\nities of LLMs we first study innovation tasks that require\ncrafting a target item. These tasks were originally intro-\nduced by Jiang et al. (2020), and their complexity can be\ncontrolled by determining the number of intermediate items\nthe player needs to craft before reaching the target, termed\ntheir depth. Our experiments in tasks with a target suggest\nthat: a) LLMs leverage factual/ knowledge, as removing the\nnatural language semantics degrades performance b) multi-\nstep reasoning is challenging as performance significantly\ndrops when increasing the depth of the task.\nWe, then, focus on tasks without a target that employ the\nsame graph and starting set of items with LA2. We, first, ex-\namine the ability of a single LLM to explore the search space\nand compare it with a baseline agent that employs empower-\nment and is known to perform on par with humans (Br\u00e4ndle\net al., 2023; Klyubin et al., 2005). Our experiments indi-\ncate that the LLM agent performs on par with an agent that\nrandomly chooses combinations, suggesting that it does not\nexplore efficienly. Following the single-agent LLM experi-"}, {"title": "Related work", "content": "Various computational studies have investigated the dynam-\nics of collective innovation. These studies differ across three\nimportant dimensions. First, they may model individual\nand social learning mechanisms differently. The majority\nof CE models assumes that innovations arise solely through\nrandom mutations and recombinations (Mason et al., 2008;\nCantor et al., 2021; Fang et al., 2010), nullifying the cog-\nnitive capacities of individuals. Considering the problem-\nsolving abilities of LLMs, discussed right after, our model\ncan be seen as being rather closer to works that equip agents\nwith cognition, for example through reinforcement learn-\ning (Nisioti et al., 2022). Second, studies propose differ-\nent mechanisms as drivers of cultural accumulation. Here,\nworks may focus on the effect of social connectivity, with\nsome suggesting that more connectivity is better and size is\nthe sole determinant of collective performance (Mason and\nWatts, 2012), while others suggesting that partial, static or\ndynamic, connectivity confers an advantage to groups ex-\nploring search spaces with local optima (Derex and Boyd,\n2016; Nisioti et al., 2022; Lazer and Friedman, 2007) Third,\nthey employ different test-beds to capture the landscape of\ninnovation. Initially computational studies studied classical\nsearch problems such as line search (Mason et al., 2008; Ma-\nson and Watts, 2012) and the NK-problem (Lazer and Fried-\nman, 2007), thus ignoring the hierarchical, tree-like struc-\nture of innovation landscapes. Another line of works em-\nploys a test-bed inspired from drug discovery, where indi-\nviduals need to combine items to craft new ones (Derex and\nBoyd, 2016; Cantor et al., 2021; Nisioti et al., 2022), and is,\nthus, closest to the test-bed employed here. Yet this test-bed\ndiffers from ours in two ways: a) it is defined over symbolic\nitems and manually designed small-scale knowledge graphs,\nthus failing to capture the scale and semantics of human in-\nnovation b) it assumes that each combination incurs a reward\nfor the agent. In contrast, our open-ended tasks instruct the\nagent to explore and may, thus, capture the open-ended na-"}, {"title": "Collective innovation test-bed for LLMs", "content": "We, here, describe the test-bed we have designed to study\ncollective innovation with LLMs. We have minimally ex-\ntended Wordcraft, a Python-based gym reinforcement lear-\ning (RL) environment inspired by Little Alchemy 2 that was\noriginally introduced for evaluating the commonsense abil-\nities of RL agents (Jiang et al., 2020). Instead of images,\nitems in Wordcraft are expressed as text. Another difference\nwith the original game is that tasks are targeted: whereas a\ngame in Little Alchemy 2 starts with the player being given\na small set of items and no further guidance, tasks in Word-\ncraft determine, in addition to the initial set, a target item\nto craft. Our extension of Wordcraft is basically an intro-\nduction of open-ended tasks that follow the spirit of Little\nAlchemy 2 and an interface for mapping the gym environ-\nment to a textual form. To capture the diversity of the task\ndesign space of our test-bed, below we provide a general\ndefinition of its components and explain our particular im-\nplementation. These components are:\nA knowledge graph that represents the semantics of the\ntask space by indicating how items can be combined to cre-\nate new items. In this work we employ the knowledge graph\nof Little Alchemy 2, but, in the general case, studies can\nmanually define their own graphs (Nisioti et al., 2022) or\nderive graphs from text corpora (Jiang et al., 2020).\nA task-generation process that samples the initial set of\nitems from the knowledge graph and determines the goal of\nthe task. Here, we discriminate between open-ended and\ntargeted tasks. The former prompt the LLM to discover as\nmany items as possible without specifying a target. The lat-\nter prompt the LLM to craft a specific item and their com-\nplexity can be configured through two parameters: the num-\nber of items inserted in the initial set that are irrelevant for\ncrafting the current target, w, termed distractors, and the\nnumber of intermediate items that the LLM needs to craft\nbefore reaching the target, termed the depth.\nA textual repesentation of the task that has two parts: a)\nan intro prompt containing the rules of the game, as well as\nexamples of tasks and correct outputs from the LLM in order\nto elicit in-context learning (Brown et al., 2020) b) the cur-\nrent task state. As a task requires multiple crafting steps (in\nthe case of open-ended tasks on the scale of hundreds), we\nneed a way to keep the prompt size limited. For this reason,\ninstead of showing the complete crafting/discussion history\nfor a task we summarize its current state in the following\nform for targeted tasks:\nInventory: set of items available for crafting\nTarget: target item\nRemaining rounds: number of crafting steps before task\nis over\nTask valid combinations: a list of combinations already\nattempted that gave a new item\nTask invalid combinations: a list of combinations already\nattempted that did not give a new item\nIn the case of open-ended tasks, information about the tar-\nget is omitted. In addition to keeping the prompt size lim-\nited, this way of presenting the task can be seen as a form\nof external summarization (Krishnamurthy et al., 2024), as\nthe LLM does not need to memorize the item combinations\nit has attempted. We provide an illustration of intro and task\nprompts in an example task where two LLMs are solving an\nopen-ended task collectively on the right of Figure 1.\nMore formally, a task in our test-bed is described by a set\nof items that are initially available for crafting, Zo, a knowl-\nedge graph K and a target item g (that is empty in the case\nof open-ended tasks). The initial set I and target g are sam-\npled from the knowledge graph K at the beginning of an\nepisode that lasts for a fixed number of steps T. The agent\ncan execute actions in the form of two items, [11, 12] that de-\nnote the combination it attempts to make. If, according to\nthe knowledge graph, the action is a valid combination the\nenvironment returns a new item that is inserted in the inven-\ntory of the agent (if the item is already it is discarded, as\nitems can be re-used indefinitely). At each step t, the agent\nis presented with the current state of the environment and is\nprompted to execute an action."}, {"title": "Baselines", "content": "Here we describe previous models that do not make use of\nLLMs and have been employed in single-agent and collec-"}, {"title": "Singe-agent", "content": "Empowered agent Empowerment is a type of exploration\nstrategy that centers around the idea of choosing actions\nthat enable the generation of as many options as possible\nin the future (Klyubin et al., 2005). Previous lab studies\nwhere human participants played LA2 have shown that em-\npowerment is a powerful exploration strategy in this game\nand that it captures human behavior more accurately than\nother exploration strategies, such as random exploration and\nuncertainty-minimization (Br\u00e4ndle et al., 2023). While dif-\nferent implementations of empowerment are possible, here\nwe consider the one employed in (Br\u00e4ndle et al., 2023):\nat each timestep t an agent employing empowerment com-\nbines the two items in its inventory that will result in crafting\nthe most empowering item. The empowerment of an item is\ncomputed as the number of valid combinations it participates\nin. To choose an action, this agent accesses the knowledge\ngraph of the task (defined in the previous section), computes\nthe empowerment value of each potential combination and\nchooses the one with the highest value.\nRandom agent At each timestep t, this agent randomly\ncombines two items from its inventory."}, {"title": "Multi-agent", "content": "Random groups The majority of computational studies\nin collective innovation (Mason et al., 2008; Cantor et al.,\n2021; Fang et al., 2010; Nisioti et al., 2022) consider random\nagents (as defined in the previous paragraph) that can com-\nbine items they see in the inventories of their neighbours.\nThis implies that they have a perfect mechanism for copying\nsocial information. Agents can introduce new items by ran-\ndomly mutating a single item or combining multiple items.\nFor brevity, we refer to such groups of agents as random\ngroups.\nEmpowered groups Here we have multiple empowered\nagents that can combine items they see in the inventories of\ntheir neighbors."}, {"title": "Collective of LLMs with social connectivity", "content": "Here, we describe how we designed group sof LLM agents\nfor solving tasks in our innovation test-bed. We first describe\na single LLM agent and, then, describe how they share in-\nformation in a group with a certain social connectivity.\nAt each timestep t an LLM agent outputs text that con-\ntains the action it chooses to execute based on the prompt\ndescribing the task (described in the previous section). The\nprompt has been engineered to instruct the LLM to provide\nits output in the specific format:\nCombination: 'first item' and 'second item'\nReasoning: based on the information in the , do reasoning about why you chose this combina-\ntion\nInstructing the LLM to follow a certain format is a com-\nmon practice when it needs to interface with another system,\nsuch as an RL environment (Wang et al., 2023b). Prompt-\ning the LLM to reason on its response is also a common\ntechnique, termed chain-of-thought prompting, and has been\nshown to improve the reasoning abilities of LLMs (Wei\net al., 2022). As the LLM output probabilities over tokens,\nwe can control the randomness of its outputs through the\ntemperature of a soft-max function over probabilities.\nWe consider groups of identical LLMs (they all have iden-\ntical weights) that differ solely in the prompt that describes\nthe task. As we showed in Figure 1, the different LLMs are\npresented with the same intro prompt and are also assigned\nwith the same task. What differentiates LLMs is the state\nprompt, i.e., the current state of their inventory and their\nhistory of past combinations. Thus, any differences in the\noutputs of LLMs in a given group solely arise due to their\nown behavior in the current task and are bootstrapped by the\nrandomness in their sampling strategies.\nA group is characterized by its social connectivity, an\nundirected graph G that determines the local neighbourhood\nof an agent. In this work, we consider two types of con-\nnectivity: a) in fully-connected groups all LLM agents are\nneighbours with each other b) in dynamic groups the group\nis divided into sub-groups of two and agents visit another\nrandom sub-group with probability p for a fixed duration V\nAgents in a group interact solely by sharing information\nregarding their actions. In particular, social information is\nshared by augmenting the prompt with the following tags:\nOther players' valid combinations: a list of combinations\nalready attempted by other players in the agent's neigh-\nborhood that gave a new item\nOther players' invalid combinations: a list of combina-\ntions already attempted by other players in the agent's\nneighborhood that did not give a new item\nThus, similarly to previous computational studies (Mason\net al., 2008; Cantor et al., 2021; Fang et al., 2010; Nisioti\net al., 2022), information is shared implicitly based on the\nsocial connectivity without requiring an action on behalf of\nthe agents. We illustrate how socially-shared information is\npresented to the agents on the right of Figure 1 and the two\ntypes of social connectivity that we consider in our study on\nthe left of Figure 2."}, {"title": "Set-up", "content": "We perform an experimental analysis where we control for\ndifferent features of our set-up: a) we consider both targeted\nand open-ended tasks. For the former we randomly sample\ntasks with different number of distractors (w \u2208 {3,6}) and\ndifferent depth values (d \u2208 {1,2}). We allow six crafting\nsteps and measure success as the percentage of tasks where\nthe agent crafts the target item. We perform 10 trials and, for\neach one, we randomly sample 50 tasks. Tasks are identical\nacross trial. Thus, variance within a trial reveals the effect\nof task variability while variance across trials reveals vari-\nability in the agent's policy for the same task. For the open-\nended tasks we employ the same initial set with the one used\nin Little Alchemy 2 ('air', 'earth','fire','water'), allow 200\ncrafting steps and average across 10 trials. Here, we employ\nthe size of the agent's inventory as a proxy for success b) we\nstudy two different LLMs, GPT-3.5 turbo 2 and an open-\nsource model, Llama 2 3 and compare their performance\nto the two single-agent baselines (random agents and agents\nusing empowerment with a temperature of 0.1). We employ\na temperature value of 1.0 for the LLMs (we performed a\ngrid search for Llama 2 and did not observe large variations\nbut expect that a high temperature value is useful for elicit-\ning randomness across agents) c) we study groups with two\ntypes of social connectivity: a fully-connected group with 6\nagents and a dynamic group where agents are divided into\nsubgroups of two and, unless otherwise specified, perform\nvisits with a probability of p = 0.2 that last for V =50 steps.\nTo avoid repetitions, if an agent repeats an already-attempted\ncombination, we re-prompt until it chooses a novel one or 6\ncrafting steps have passed. We provide code for reproducing\nexperiments, including prompts, in an online repo."}, {"title": "Results", "content": "LLMs can exploit task knowledge\nTo gauge the ability of LLMs to understand and solve in-\nnovation tasks, we first examine the percentage of solved\ntargeted tasks, S, for varying task complexity with single-\nagent methods in Figure 3. Error bars indicate variance\ndue to both task and agent variability. We observe that\nwhen w = 3, d = 1 GPT-3.5 turbo solves almost all tasks\n(S = 0.9 \u00b10.1) while other methods succeed about half of\nthe time. As these tasks contain 5 items, they can be solved\nsuccessfully by exhaustive search within 10 steps and, by\nrandom search, about half of the time within the time budget.\nAgents employing empowerment outperform random ones,\nas empowering items have higher chances of being targets.\nLlama 2 performs significantly worse than GPT-3.5 turbo\nan observation that generalizes to all settings we examined.\nA major reason for this is the inability of Llama 2 to avoid re-\npeating combinations. In particular, we counted the number\nof times an LLM repeats a combination and observed that it\nwas close to zero for GPT-3.5 turbo but Llama 2 repeats it-\nself an overage of 4 times per task and in many cases, never\nfinds a novel combination despite the repetition mechanism.\nTo understand these differences in performances, we per-\nform two additional probing tasks that aim at examining the\nknowledge of LLMs. First, we test to what extent the perfor-\nmance of LLMs is influenced by the semantics of the task.\nWe do so by encoding all words in the knowledge graph\ninto random strings of 5 characters. We present how suc-\ncess changes for the two LLMs when removing semantics\nin Table 4, where we observe that the performance of both\nLLMs drops when semantics are removed. The drop is much\nsteeper for GPT-3.5 turbo, which is not surprising, as it per-\nforms much better than Llama 2 at the tasks with semantics.\nThe fact that semantics are crucial when there is one level\nindicates that the LLMs can predict which combination is\nthe most likely to give the target item in a single step.\nA yet more challenging and particularly useful skill, con-\nsidering the multi-level nature of tasks, is predicting the out-\ncome of crafting. To test for this, we create another probing\ntask: we prompt the LLM with valid combinations (gener-\nated by sampling combinations with the agent employing\nempowerment) and ask the LLM to predict the outcome of\nthe combination. We, then, compute the similarity between\nthese predictions with the actual crafting outcome using the\npre-trained glove model 'glove-twitter-25' 4. We present\nthese results on the right of Figure 4, where we include a\nbaseline that randomly samples items from the knowledge\ngraph as predictions for reference (similarity values range\nbetween 0 and 1). We observe that GPT-3.5 turbo is better"}, {"title": "LLMs struggle at multi-step reasoning", "content": "As we increase the complexity of the tasks we observe, in\nFigure 3 that: a) when the number of distractors increases\n(w = 6,d = 1) the gap between GPT-3.5 turbo and other\nmethods increases. This is not surprising as GPT-3.5 turbo\nexploits task knowledge best and the benefits of informed\nexploration become more apparent in larger search spaces\nb) when the depth increases (w = 3,d = 2) then the per-\nformance drop is more significant and the disparity between\nthe two LLM models decreases. Combined with the obser-\nvation that GPT-3.5 turbo cannot perfectly predict outcomes\nthis suggests that multi-step search poses a qualitatively dif-\nferent challenge that even advanced LLMs cannot solve."}, {"title": "LLMs struggle in open-ended tasks", "content": "We move to the open-ended task, where we observe that: a)\nLlama 2 only crafts an average of 6 items, performing worst\namong all methods. As discussed earlier, Llama 2 has the\ntendency to repeat combinations which is particularly detri-\nmental in open-ended tasks due to the long horizon without"}, {"title": "LLMs copy others imperfectly", "content": "We now move to experiments with groups where we first ex-\namine the collective behavior of GPT-3.5 turbo agents (we\ndo not experiment with Llama 2 groups due to their sub-\noptimal performance). A first question is whether LLM-\nagents benefit from innovating in a collective and, a require-\nment for this, is learning from the combinations of others\n(we have described how social information is shared when\npresenting the methods). A proxy for this is the tendency\nof agents to copy actions they see in others. To search for\nthis we count the number of crafting steps it takes for an el-\nement to appear in the inventory of an agent once it appears\nin the inventory of one of its neighbors. We compare the\nevolution of this variable for dynamic and fully-connected\ngroups in Figure 6 where we average within groups. We ob-\nserve that copying is not perfect: as time passes more and\nmore items accumulate that the agent could have crafted.\nAgents in fully-connected groups take more time to copy\ntheir neighbors. Potential reasons for this are that copy-\ning when having more neighbors takes more time, as more\nitems need to be copied, and that larger neighborhoods lead\nto longer prompts, making the task more difficult for LLMs."}, {"title": "Connectivity influences collective innovation", "content": "Finally, we compare groups with different connectivity in\nterms of their performance in open-ended tasks. Figure 7\nreveals that, for all groups, dynamic connectivity performs\nbest. In the case of GPT-3.5 turbo we present results with a\nperfect copy mechanism (when an agents crafts a new item,\nit appears in the inventories of all its neighbors). As we\nsaw in the previous paragraph, GPT-3.5 turbo agents do not\ncopy perfectly and are therefore less influenced by their con-\nnectivity (we performed the experiment with GPT-3.5 turbo\nagents without a copy mechanism and did not see a differ-\nence between dynamic and fully-connected groups). Our\nempirical observation agrees with previous works that em-\nployed groups similar to our random group baseline (Ma-\nson et al., 2008; Cantor et al., 2021; Fang et al., 2010), hu-\nmans (Derex and Boyd, 2016; Migliano et al., 2020) and\nreinforcement learning agents (Nisioti et al., 2022) and were\nperformed on manually-designed, small knowledge graphs.\nBy generalizing them to a larger knowledge graph grounded\nin the real world, our work further confirms that social struc-\nture matters in innovation tasks and suggests that our test-\nbed can prove useful in future studies of collective innova-\ntion with both human and artificial agents."}, {"title": "Discussion", "content": "We studied the ability of LLMs in innovation tasks, both\nin isolation and in groups with different social connectiv-\nity. We have shown that GPT-3.5 turbo can leverage the se-\nmantics of items to infer the outcomes of crafting but strug-\ngle when it comes to planning for multiple time steps and\nexploring in an open-ended way. Nevertheless, groups of\nLLMs exhibit an interesting phenomenon previously found\nin human and computational studies: they perform better\ncollectively when their social connectivity is partial and dy-\nnamic rather than static and fully-connected. We attributed\nthis phenomenon to the tree-like structure of the LA2 game:\nfollowing down some paths may lead you away from other\npaths and slow down exploration. We have shown that\ndynamically-connected LLM groups outperform both sin-\ngle LLMs, and fully-connected groups. In groups with dy-\nnamic connectivity, subgroups explore different paths and\nexchange members that share information about other paths,\nincreasing the diversity or breadth of exploration.\nOur analysis focused on probing for specific abilities and\nemergent behaviours in the studied groups of LLMs. How-\never, a larger-scale analysis is necessary to reveal the ef-\nfect of the different hyperparameters of the model, such as\nthe sampling strategy of LLMs, the configuration of the dy-\nnamic connectivity, and the task complexity. A limitation\nrevealed by our empirical study is that smaller, open-source\nmodels may fail at learning the task sufficiently well to lead\nto any interesting emergent behaviours. Thus, as other stud-\nies of the cognitive capacities of LLMs have shown, our\nwork suggests that collective innovation studies may re-\nquire larger models, such as GPT-4 or the introduction of\nadditional mechanisms for complementing their skills. We\nshould note that our experiments did not examine whether\npre-training equipped the LLMs with the ability to explore\nin-context, leverage common-sense knowledge or memorize\nthe solution of Little Alchemy 2. Nevertheless, our con-\nclusion that groups with dynamic connectivity out-compete\nsingle-agent and fully-connected ones remains valid.\nWe believe that this work has important implications. We\nhave shown that groups of LLMs with dynamic connectiv-\nity can overcome shortcomings of a single LLM and fully-\nconnected groups. This is a key insight, as dynamic commu-\nnication structures are less costly than fully-connected ones.\nLLMs are becoming ubiquitous, participating in various hu-\nman activities from finance to molecular discovery and writ-\ning fiction as copilots of human creativity and productiv-\nity (Mirowski et al., 2022; Brinkmann et al., 2023). We\nbelieve that this work is a first step towards understanding\nhow and with what kind of connectivity multi-agent LLM\nsystems could optimally and efficiently participate in explo-\nration, innovation, and cultural evolution."}]}