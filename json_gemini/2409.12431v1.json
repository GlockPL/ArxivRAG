{"title": "FlexiTex: Enhancing Texture Generation with Visual Guidance", "authors": ["Dadong Jiang", "Xianghui Yang", "Zibo Zhao", "Sheng Zhang", "Jiaao Yu", "Zeqiang Lai", "Shaoxiong Yang", "Chunchao Guo", "Xiaobo Zhou", "Zhihui Ke"], "abstract": "Recent texture generation methods achieve impressive results due to the powerful generative prior they leverage from large-scale text-to-image diffusion models. However, abstract textual prompts are limited in providing global textural or shape information, which results in the texture generation methods producing blurry or inconsistent patterns. To tackle this, we present FlexiTex, embedding rich information via visual guidance to generate a high-quality texture. The core of FlexiTex is the Visual Guidance Enhancement module, which incorporates more specific information from visual guidance to reduce ambiguity in the text prompt and preserve high-frequency details. To further enhance the visual guidance, we introduce a Direction-Aware Adaptation module that automatically designs direction prompts based on different camera poses, avoiding the Janus problem and maintaining semantically global consistency. Benefiting from the visual guidance, FlexiTex produces quantitatively and qualitatively sound results, demonstrating its potential to advance texture generation for real-world applications.", "sections": [{"title": "Introduction", "content": "The field of computer graphics aims to enhance the interaction with 3D assets, but the scarcity of high-quality 3D assets is a significant challenge. The creation of such assets requires extensive artistic skills and is a labor-intensive process. However, recent advancements in deep generative models introduce a new paradigm known as Artificial Intelligence Generated Content (AIGC), revolutionizing the creation and manipulation of 3D models. Among the various aspects of asset generation, texture generation plays a crucial role in adding expressiveness to shapes, and finding applications in industries such as AR/VR, film, and gaming. Furthermore, texture generation holds the potential to overcome the limitations associated with collecting high-quality textured 3D assets.\nRecent diffusion models show remarkable advancements in image generation. Noticing their potential on 3D tasks, researchers utilize them to generate textured 3D assets by optimization and in-painting, where they render 3D geometry into multiple views and update each view using rich 2D priors, However, these methods neglect cross-view correspondence in 3D space, leaving style inconsistency and seams. Recent works keep the style consistent and reduce the seams by utilizing batch inference and synchronize multi-view denoising on a shared texture, while still leaving heavy cross-view inconsistency. We argue that it is hard to keep cross-view alignment relying on ambiguous text prompts solely, and frequent feature aggregation introduces a serious variance bias, resulting in over-smoothness. Moreover, they suffer from the Janus problem, due to lack of geometry-aware ability and the bias introduced by ambiguous text descriptions.\nNoticing these, we propose a simple yet effective framework to achieve high-quality, precise texture generation, named FlexiTex, which supports text and image conditions both. First, we mitigate the ambiguity of text descriptions by converting them to more explicit modalities, i.e., image, which serves as a guide during batch inference and specifies the target object more accurately. Specifically, we introduce a Visual Guidance Enhancement module upon diffusion models to convert the text into an image and inject such a more specific informative guide into the batch inference process through cross-attention. By this, we ensure a more consistent direction during denoising and prevent variance degradation in joint sampling on the shared latent texture. Second, we apply a Direction-Aware Adaptation module to alleviate the Janus problem, where we inject direction prompts under different views into the model. This module makes the model more sensitive to direction information and encourages semantic alignment between views. Third, the proposed method is not only training-free but also more flexible than previous methods, as it accepts image prompts straightforwardly, by which it can finish texture transfer. This adaptability makes our framework suitable for various geometries and provides more diversity in the generated textures. We conduct comprehensive studies and analyses involving numerous 3D objects from various sources to demonstrate the effectiveness of FlexiTexin texture generation."}, {"title": "Related Work", "content": "Text-to-Image Diffusion Models. Recent years have witnessed the advancements of text-to-image diffusion models for the impressive generative capability to create high-fidelity images. Specifically, Stable Diffusion incorporates a text encoder from CLIP and generates realistic images based on input text prompts. Beyond the text conditioning, further enhances the model's capabilities. It allows the denoising network to be conditioned on additional input modalities, such as depth or normal maps. Furthermore, design the effective and lightweight IP-Adaptor to achieve image prompt capability for the pre-trained text-to-image diffusion models. It can be inserted into the current framework and achieve multi-modal image generation. In our work, we leverage ControlNet and Stable Diffusion to offer geometrically conditioned image priors, and IP-Adaptor to support image guidance.\nText-to-texture Synthesis. Generating textures on empty models is a challenging task, which requires precise alignment with geometry and semantic coherence. Early works aim to utilize geometric prior for texture generation. They inject positional information and train geometry-aware generative models from scratch. However, these methods cannot generalize well on various categories and show blurry textures, due to the limited 3D data with high-quality textures for training.\nOptimization-based methods utilize Score Distillation Sampling (SDS) on pre-trained diffusion models to update textures iteratively. Inpainting-based methods design a sequential inpainting strategy, which fills blank areas of current views based on existing contents from neighbor views. Other methods fine-tune a multi-view diffusion model to generate a 2 \u00d7 2 gird for multi-view consistency, while they are trained on synthetic datasets and waste rich prior gained from large-scale real image datasets, showing poor diversity.\nRecent studies focus on synchronization-based methods, as they claim that all views contribute equally to texture generation. SyncMVD firstly uses a shared latent texture to force consistent latent features from multiple views during denoising. Tex-Painter decodes all latent views and performs differentiable inverse rendering at each denoising step, aiming for a texture of higher resolution. Gene-"}, {"title": "Method", "content": "FlexiTexis designed to generate high-quality texture maps given an untextured mesh conditioned on either text or image. The overview is shown in Fig. 2. In this section, we first provide the backgrounds of diffusion models, mesh rendering, and texture warping in Sec. Following this, we introduce the Visual Guidance Enhancement module in Sec., which is designed to overcome issues related to over-smoothing. Then, we present the Direction-Aware Adaptation module in Sec., which is specifically designed to incorporate direction information during texture generation, thereby addressing the Janus problem."}, {"title": "Preliminary", "content": "We introduce the preliminaries here, including diffusion models, mesh rendering, and texture warping.\nStable Diffusion & Controlnet. The diffusion model consists of a forward process $q(.)$ and a reverse denoising process $p_{\\theta}(.)$. The forward process progressively corrupts the original data, denoted as $x_0$, with noise $\\epsilon \\sim \\mathcal{N}(0, I)$ to a noisy sequence $x_1,...,x_T$, following a Markov chain as:\n$q(x_t|x_{t-1}, y) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I)$,\n$x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\cdot \\epsilon$, (1)\nwhere $\\beta_t$ represents the variance schedule for $t = 1, ..., T$; $y$ is the condition. The reverse process denoises the pure data from $x_T$ as:\n$p_{\\theta}(x_{t-1}|x_t, y) = \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_t, t, y), \\Sigma_{\\theta}(x_t, t, y))$, (2)\nwhere $\\mu_{\\theta}$ and $\\Sigma_{\\theta}$ denote the mean and variance predictions, respectively, which are obtained from a trainable network parameterized by $\\theta$.\nBesides, we introduce ControlNet which injects low-level control, such as depth map $d$, during the denoising process of Stable Diffusion. The predicted noise of the U-Net with ControlNet is represented as $\\epsilon_{\\theta}(x_t, y, d, t)$\nTexture warping. Given a mesh $M$, a texture map $T$ and a viewpoint $C$, we can use the rasterization function $R$ to render an image. After rasterization, each valid pixel on rendered images corresponds to one on texture. However, these pixels are scattered in UV space after back projecting, necessitating Voronoi filling to fill up all blank regions in texture warping ($W^{-1}$). Given ${s_i | s_i = (u_i, v_i)}$ represents the UV coordinates of the i-th latent texel, we first generate the Voronoi diagram partitions the domain $T$ into a set of regions ${V_i}$, where each region $V_i$ is defined as:\n$V_i = {p \\in T | ||p - s_i|| \\leq ||p - s_j||, \\forall j \\neq i}$. (3)\nHere, $p$ is the 2D point position and $||.||$ denotes the Euclidean distance. Then we define a procedural function $P_i: V_i \\rightarrow \\mathbb{R}^3$ that generates the filling for the region $V_i$ based on the texels $s_i$ and the position $p$ within the region. Finally, we record visible masks on the texture map and apply specific boundary conditions near the edges to ensure a smooth and consistent filling. The final texture is represented as the union of the filled regions: $T = \\bigcup_{i=1}^N V_i$, where $N$ is the number of regions."}, {"title": "Visual Guidance Enhancement", "content": "In FlexiTex, we design a Visual Guidance Enhancement module to align the denoising inference on multiple views. Explicitly, for text input $x_{\\text{text}}$, we first utilize a text-to-image model to generate image prompt $X_{\\text{img}}$. Its corresponding semantic information $c_{\\text{img}}$ is injected into the denoising process in a cross-attention manner through IP-Adaptor. To be specific, given the image features $c_{\\text{img}}$, the output of new cross-attention $Z'$ is computed as follows:\n$Z' = \\text{Attention}(Q, K', V') = \\text{Softmax}(\\frac{Q (K')^T}{\\sqrt{d_v}}) V'$ , (4)\nwhere, $Q = Z W_q, K' = c_{\\text{img}} W_k$ and $V' = c_{\\text{img}} W_v$ are the query, key, and values matrices from the image features."}, {"title": "Direction-Aware Adaptation", "content": "We introduce Direction-Aware Adaptation which utilizes a Text Encoder with U-Net to provide spatial information and solve the Janus problem. We find that SD-XL is more sensitive to direction prompts. When we add prompts i.e., front/side/back view, it can understand these well and generate more semantically aligned results. Noticing that the orientation of the original shapes is not guaranteed, we first correct their orientation to ensure they are front-facing. This pre-processing step defines the standard orientation and determines the writing of subsequent direction prompts. Then we generate prompt ${C_{\\text{view}, i}}_{i=1}^N$ according to the elevation and azimuth for each view, organized as \"from <?> view\". Given such direction prompts, the denoising process for $view_i$ is conditioned on visual features $c_{\\text{img}}$ and direction features $c_{\\text{view}}$ via cross-attention, written as:\n$Z = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d_v}})V + \\text{Softmax}(\\frac{Q(K')^T}{\\sqrt{d_v}})V'$, (5)\nwhere $Q = ZW_q, K = c_{\\text{view}} W_k, V = c_{\\text{view}} W_v, K' = c_{\\text{img}} W_k, V' = c_{\\text{img}} W_v$. In our framework, the image prompt provides visual guidance on appearance and text description provides direction information, by which, the direction-aware single-view generative model can naturally become a multi-view generator during batch inference and synchronized sampling, as shown in Fig. 2."}, {"title": "Experiments", "content": "Setup\nImplementation Details. Our experiments are conducted on an NVIDIA A100 GPU. For denoising epoch, we use DDIM as the sampler. We set the number of iterations to 30 steps, the CFG scale (classifier-free guidance scale) for Direction-Aware Adaptation to 12, and the scale of Visual Guidance Enhancement to 0.6. Texture warping for latent views is used in the first 24 steps. We sample 8 views for a mesh, and the elevations and azimuths are (-180\u00b0, 15\u00b0), (-120\u00b0, -15\u00b0), (-60\u00b0, 15\u00b0), (0\u00b0, -15\u00b0), (60\u00b0, 15\u00b0), (120\u00b0, -15\u00b0), (-180\u00b0, -45\u00b0), (0\u00b0, 45\u00b0). We implement the rendering function by Pytorch3D .\nDataset. We collect 60 meshes with corresponding text prompts and 60 meshes with corresponding image prompts to evaluate the text-to-texture and image-to-texture generation ability, respectively. These meshed are randomly sampled from Objaverse, Objaverse-XL and ShapeNet."}, {"title": "Quantitative Analysis", "content": "FID & KID. Following GenesisTex, we use samples from pre-trained diffusion models as ground truth labels. We render depth maps from 16 cameras around the mesh as ControlNet inputs and we maintain the same prompts for inference. We modify the ground truth images' background to white to ensure the focus remains on the texture. We render each mesh from the same views, assessing quality via FID and KID. Tab. 1 shows our method outperforms baselines, indicating superior synthesis quality and closer alignment to ground truth, attributed to the explicit information provided by visual guidance.\nClipScore. For text-to-texture tasks, the CLIP score is derived by comparing rendered views with text prompts, while for image-to-texture tasks, it is computed by comparing rendered views with image prompts. Semantic consistency deviations may occur in text-to-texture tasks due to the text-to-image module, resulting in a slightly lower CLIP Score than TexPainter, as shown in Tab. 1. However, our method enhances texture quality and realism. For image-to-texture tasks, Paint3D improves alignment by injecting image features into UV map refinement, but struggles with complex UV maps due to semantic differences. Conversely, FlexiTex achieves the highest CLIP Score by employing visual guidance on multi-view inference, ensuring semantic consistency with input images.\nSpeed. Compared with optimization-based and inpainting-based methods requiring sequential sampling, our approach simultaneously generates multiple views once. Rapid texture warping further accelerates generation, in contrast to TexPainter using 40-minute due to enforced differentiable rendering in each denoising step."}, {"title": "Qualitative Analysis", "content": "As presented in Fig. 3 and Fig. 4, we qualitatively evaluate the quality of the generated texture conditioned on text and image, respectively. The Latent-Paint method produces textures that are broken by noise and impurities, as a consequence of the inherent limitation of SDS. Both Paint3D and Text2Tex exhibit noticeable blurriness at texture seams and suffer from the multi-face issue, thereby failing to maintain multi-view consistency. TexPainter and SyncMVD struggle to preserve clarity on texture, ending up with an over-smoothed and monotonous appearance. Through Visual Guidance Enhancement, FlexiTex overcomes the over-smooth problem and preserves more high-frequency details during generation. Direction-Aware adaptation also strengthens geometric alignment on side or back views, alleviating the Janus problem.\nFor image-to-texture tasks shown in Fig. 4, both TEXTure and PGC-3D struggle to retain the semantic information from image prompts, resulting in low-quality, noisy textures. Paint3D, while partially preserving semantic information, generates a significant number of texture fragments."}, {"title": "Ablation Studies", "content": "Effectiveness of Visual Guidance Enhancement. To examine the influence of various prompts on texture quality, we compare three types: simple prompts comprising no more than five words, refined prompts expanded by a Large Language Model (LLM) Llama-3 and Visual Guidance Enhancement using our method. As depicted in Fig. 5, textures derived from simple prompts can appear blurry or desaturated. While refined prompts marginally slightly address this issue, their results still lack vibrant details. However, with Visual Guidance Enhancement, we can observe a significant improvement in detail richness. Similar to VCD-Texture, we visualize the standard deviation (std) curve of the latent features from foreground parts during the denoising phase, in Table 6. Here, Visual Guidance Enhancement achieves the highest feature variance, indicating high-frequency details. Apart from the rich details brought by VGA, we also achieve better style consistency in Fig. 7. For example, for prompt 'a carton-style house', the original results can show grey on one side and orange on the other side, while with the VGA module, these two sides show consistent color.\nEffectiveness of Direction-Aware Adaptation. We visualize the effects of Direction-Aware Adaptation (DAA) on alleviating multi-face problem in Fig. 8. With reinforcement of specific direction on target meshes, the contents of each view are correct(e.g., a lizard shows one eye from the side view, the front of the iPhone is a screen, and a human only has one face). We also experiment on 200 human cases, 200 animal cases, and 200 object cases. Tab. 2 shows a lower multi-face percentage."}, {"title": "Conclusion", "content": "In this paper, we introduce FlexiTex, a novel framework designed for high-fidelity texture generation on 3D objects, accommodating both text and image prompts. We also mitigate multi-face issues across various types of objects. Experiments demonstrate that FlexiTex is superior to baseline methods in both quantitative and qualitative measures. However, our method does have certain limitations. Specifically, our generated results have not yet decoupled lighting information, leading to potential artifacts in highlights or shadows. Additionally, in areas where multiple views are inconsistent or unobserved, minor dirty spots may appear. Future research could explore material generation for re-lighting tasks and improved texture warping strategies, such as maintaining a color field for smooth transitions in conflict areas."}]}