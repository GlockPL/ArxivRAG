{"title": "Active Causal Structure Learning with Latent Variables: Towards Learning to Detour in Autonomous Robots", "authors": ["Pablo de los Riscos", "Fernando Corbacho"], "abstract": "Artificial General Intelligence (AGI) Agents and Robots must be able to cope with everchanging environments and tasks. They must be able to actively construct new internal causal models of their interactions with the environment when new structural changes take place in the environment. Thus, we claim that active causal structure learning with latent variables (ACSLWL) is a necessary component to build AGI agents and robots. This paper describes how a complex planning and expectation-based detour behavior can be learned by ACSLWL when, unexpectedly, and for the first time, the simulated robot encounters a sort of \"transparent\" barrier in its pathway towards its target. ACSWL consists of acting in the environment, discovering new causal relations, constructing new causal models, exploiting the causal models to maximize its expected utility, detecting possible latent variables when unexpected observations occur, and constructing new structures \u2013 internal causal models and optimal estimation of the associated parameters, to be able to cope efficiently with the new encountered situations. That is, the agent must be able to construct new causal internal models that transform a previously unexpected and inefficient (sub-optimal) situation, into a predictable situation with an optimal operating plan.", "sections": [{"title": "1 Introduction", "content": "In this paper, we start with an agent whose brain's dynamic decision network (DDN) has been learned in an environment in which \"transparent\" barriers were not previously present 3. Then, suddenly, a sort of \u201ctransparent\u201d barrier is introduced in the environment, on the pathway towards the agent's target, so that this becomes a new situation that the agent, initially, does not know how to efficiently handle. Then, the agent interacts within the environment and new unexpected observations take place. In order to cope with this new situation, the agent responds by generating a new hidden variable, that is causally related in the dynamic decision network with certain decision and chance variables. It also estimates the new conditional probability distributions related with"}, {"title": "1.1 Related Work", "content": "The main thrust of the work presented in this paper is related to the topic of causality in machine learning [42,43,45,46] and, more specifically, structure learning of causal models in the presence of latent variables, as it will be explained below. It is also related to the learning of behaviors in animals and autonomous robots [2,3,4,8,13,14,25,32]. Thus, our work is related, among other things, to structure learning in Bayesian networks [24,53], causal structure learning, structure learning with latent variables, and active structure learning. Causal structure learning (CSL) [20] aims at learning the causal graph (where edges are interpreted as direct causal effects) that best describes the dependence and causal structure in a given data set. Causal models allow to make predictions under arbitrary interventions. CSL, in general, takes into account different possible interventions, such as do-interventions and additive interventions. In this paper we focus on do-interventions that are selected by MEU. Causal sufficiency refers to the absence of latent variables [48]. Yet, when possible latent variables are considered, there are two common options for the modeling of latent variables. They can be modeled explicitly as nodes in the structural equations, or they can manifest themselves as a dependence between the noise terms, where the noise terms are assumed to be independent in the absence of latent confounding [20]. In this paper, we chose the first option, that is, the latent variables are modeled explicitly by the introduction of new nodes in the graph. We define these new nodes in the causal graph as hidden variables from the agent's perspective, since they never get to be observed.\nElidan et al.[15,31] describe structure learning with hidden variables. Elidan et al.[15] discover hidden variables by a structure-based approach based on the detection of cliques. On the other hand, Elidan et al.[31] identify hidden variables thanks to the ideal parent paradigm. First, they decide the existence of a new hidden variable by a"}, {"title": "2 Technical preliminaries", "content": "In this section, we introduce some concepts that will set the theoretical background for the rest of the work."}, {"title": "2.1 Partially Observed Markov Decision Process (POMDP)", "content": "A Markov decision processes (MDP) is a mathematical model used to describe the interactions between an agent and its environment. The model is based on the assumption that the agent is able to fully observe the environment, which allows the agent to be aware of its exact state at all times. Furthermore, when combined with the Markov assumption on the transition model, it ensures that optimal policies are always dependent on the current state. Yet, in general, agents are not able to completely observe the environment. Rather, they observe it partially, which means that the agent will not know exactly the state in which it is at any given moment. This aspect makes the problem considerably more complex to solve, as the actions indicated by the optimal"}, {"title": "2.2 Dynamic Decision Networks (DDN)", "content": "Just as it is necessary to work within a framework that allows modeling the interactions of the agent with the environment, it is also necessary to have a tool that allows representing and working with POMDP in a simpler and structured way. In this work, we will use dynamic decision networks, a probabilistic graphical model that extends decision networks to deal with the temporal relationships between observation and action variables and to come up with optimal policies.\nDefinition 2.2. A dynamic influence diagram (DID) or dynamic decision network (DDN) is a probabilistic graphical model consisting of a tuple DDN = (G, P), where G is a directed acyclic graph (DAG) and P the parameters modeling the joint probability distribution determined by the relationships given in G.\nThe DAG G = (Z, E) has three different types of nodes:"}, {"title": "2.3 Causality", "content": "Causal relations between random variables The term \"causality\" has been defined in various ways, and there is currently no consensus on a single unified definition. In this work, we will examine the relationship between two random variables, X and Y, which may be binary, categorical, or continuous. The three most commonly used definitions for causality are as follows:\nCounterfactual Causality: X is a cause of Y if specific values of X and Y were observed (X = x, Y = y), yet if a different value of X had been observed, (X = x'), then the value of Y would be different, (Y = y').\nInterventional Causality: X is a cause of Y if, by making the variable X take a certain value, X = do(x), then we have that:\n\\(P(Y|X = do(x)) = P(Y|X = x),\\)\nwhereas this is not the case for Y:\n\\(P(X|Y = do(y)) \\neq P(X|Y = y).\\)\nMechanistic Causality: X is a cause of Y if there exists a function f such that Y := f(X,N), that enables the generation of Y values based on X values and a random noise N.\nThese three definitions are related but not entirely equivalent. This work will focus on the definition of mechanistic causality, thus, it will be assumed that underlying mechanistic functions exist whenever two random variables are causally related. Based on the mechanistic causality definition, all possible causal relations between a pair of random variables can be classified as follows:\nDefinition 2.3. Causal relations between a pair of random variables.\nGiven a pair of random variables X and Y, there are four potential relationships between them:\n\\(R = X \\rightarrow Y \\Rightarrow \\exists f \\wedge \\exists N_y | Y := f(X,N_y)\\)\n\\(R = X \\leftarrow Y \\Rightarrow \\exists f \\wedge \\exists N_x | X := f(Y,N_x)\\)\n\\(R = X \\leftrightarrow Y \\Rightarrow \\exists f, g \\wedge \\exists N_x,N_y, Z | X := f(Z,N_x) \\wedge Y := g(Z,N_y)\\)\n\\(R = X | Y \\Rightarrow X \\text{ and } Y \\text{ are independent, there is no function that relates them to each other }\\)\nCausal relations between actions and random variables In the previous section, we described the causal relationship between two random variables. However, when we have a random variable and an action variable, it is necessary to qualify the concept of causality. To study this causal relationship, we will assume that the only possibilities for a random variable, O, and an action variable, D, are:\n\\(R=D\\rightarrow O\\Rightarrow \\text{ The activation function of D influences the values that O takes }\\)\n\\(R=D\\nleftrightarrow O\\Rightarrow \\text{ The activation function of D does not influence the values that O takes }\\)\nThe reason why the relationships O \u2192 D and O \u2194 D will not be considered is because this work will focus on the causality from the point of view of an agent. In this context, the action variables are not random, instead, they are the result of optimizing a utility function from the perspective of the agent. In other words, the cause of an action variable taking a value can be attributed to the agent's own behavior in response to the random values observed and the utility function being optimized and not the values of the random values itself. However, from an external point of view outside the agent,"}, {"title": "2.4 Structure Learning", "content": "One of the main problems in using Bayesian networks or Decision networks is that in real problems the structure and parameters of the network modeling the data are usually not available. This leads to the need to rely on expert knowledge to build and use these models. This has led to two main areas, parametric learning and structural learning from a data set. This section will focus on describing the problem of structural learning and the difficulties that need to be faced in order to provide a solution.\nProblem definition and challenges Given a set of variables X, and a set of fully observed data D, the goal is to learn the graph G* that models the conditional relations between the variables, corresponding to the joint probability distribution IP(X) underlying D.\nFirst, note that the structural learning problem is NP-hard. This is because the size of the space of possible DAGs of n nodes, |Gn|, is given by the following recursion:\n\\(\\left|G_{n}\\right|=\\sum_{i=1}^{n}(-1)^{i+1}\\binom{n}{i}2^{i(n-i)}\\left|G_{n-i}\\right|, n > 2,\\)\n\\(\\left|G_{0}\\right| = \\left|G_{1}\\right|=1,\\)\nthat grows super-exponentially with the number of nodes. On the other hand, in order to understand another one of the challenges faced by structural learning, two definitions must first be taken into account:"}, {"title": "4 Dealing with new Causal relations and Unexpected patterns", "content": "This section briefly describes, for completeness reasons, the main aspects of Causal Discovery, as well as the basic ingredients of our Theory of Surprise. Yet, we refer the reader to [38,39] for more detailed explanations."}, {"title": "4.1 Causal Discovery", "content": "Causal discovery in models of animal behavior consists, among other things, of learning the relation between the observed effects and the hypothetical causes that gave rise to them, for instance [12,13]. The field of causal discovery in machine learning and statistics is a very active open research problem [18,19,29,34,35,36,37,49]. In this paper, for simplicity reasons, we assume that the agent is already equipped with several causal relations. Nevertheless, we will introduce the basics and ideas behind the algorithms that we would use in the case of complete learning. We refer the reader to [38] for details on the algorithms that learns this DDN from scratch by interacting with the environment.\nIn order to quantify and detect relationships between random variables, it is necessary to give a value to the relationship, that tells us how strong or true it is. To do this, we will use the causal coefficients.\nDefinition 4.1. Causal Coefficients\nGiven two random variables X and Y, a causal coefficient C(X,Y) is a scalar value such that, the larger C(X, Y) is, the more truthful is the relation X \u2192 Y.\nC(X, Y) must satisfy the following properties:\n1. Antisymmetry:\n\\(C(X,Y) = \u2212C(Y, X)\\)\n2. Discriminant:\n\\(C(X, Y) > a > 0 \\rightarrow X \\rightarrow Y.\\)\nThe parameter a has the role of determining when the coefficient is large enough to consider the relationship X \u2192 Y to exist. If |C(X,Y)| < a, then the relationship between X and Y cannot be determined.\n3. Invariant to linear transformations:\n\\(C(aX + b, cY + b) = C(X, Y) \\forall a, b, c, d \\in R, a, c\\neq 0\\)\nFrom this definition, different coefficients can be designed, but the one we decided to use is the following:\n\\(C(X,Y) = \\frac{\\frac{H(X)}{log \\left| X \\right|}}{\\frac{H(Y)}{log \\left| Y \\right|}},\\)\nwhere H(X) is the entropy of the random variable X (17). On one hand, the coefficient (13), arises from the work of Janzing et al. [22], where it is shown that if X \u2192 Y"}, {"title": "5 Learning to Detour Overall Process", "content": "In our general learning theory we try to assume the least possible amount of a priori knowledge and let the agent, by interacting with its environment, learn all the necessary causal structure as it is needed on the fly. The overall learning to detour process goes through the following main phases:\n1. First phase: Causal Discovery\n (a) Random action execution interacting with the environment to generate samples\n (b) Learning of Dynamic Decision Network (DDN)\n   i. Execute the algorithm for intratemporal relationships outlined in section (4.1), and detailed in [38].\n   ii. For each action, execute the algorithm for intertemporal relationships outlined in section (4.1), and detailed in [38].\n (c) Learn the parameters of DDN CPTs, using MLE or MAP methods.\n2. Second phase: Learning Latent Variables:\n (a) MEU interaction with the environment to generate samples. In each iteration:\n   i. Collect evidence over observation variables\n   ii. Action selection by Maximum Utility Expectation (equation 23) using an optimizing algorithm (for example Shafer Shenoy for LIMIDs)\n   iii. Compute the Expected Utility and the probability distribution associated with the Utility values U(x) (equation 10) before executing the selected action."}, {"title": "5.1 Latent variable detection and new Hidden variable Structure learning", "content": "As we have mentioned, we propose latent variable detection when a significant surprise in the utility function takes place. It follows with hidden variable sub-graph structure learning to adapt the structure of the DDN graph to the new detected hidden variable and its topological structure with respect to the already existing chance variables in the DDN. The selection of candidates for the hidden variable's children and parents is also based on the relevance of each particular chance variable on the coefficient of surprise for the overall probabilistic graph joint distribution. For this paper we assume a \u201cXM\u201d topological structure that we will define later. The possible existence of a latent variable is detected by a sudden significant surprise in the utility function (the symptom). Yet the cause has to be inferred. In the Learning to Detour case, the latent cause corresponds to the width of the agent, which happens to be wider than the paling fence openings, thus making all the gaps non passable.\nUtility Surprise This section describes latent variable detection by Surprise in the Utility function. That is, after observing the actual Utility, the agent will compute the possible surprise in the Utility. In order to do so, we extrapolate the surprise coefficient defined in (22) to define the Utility surprise coefficient, where we also take into account whether a specific surprise generates an utility larger or smaller than expected.\nFirst of all, recall that the agent's way of acting is summarized by observing the values of the observation variables xt\u22121 at time t \u2013 1 and computing the probabilities on the observation variables at time t from these values, as well as the possible values of the action space Actt-1, denoted by IP(xt|xt\u22121, Actt\u22121). This enables the agent to calculate all possible values of U(xt) and then choose the decision a* that brings the maximum expected utility,\n\\(MEU(t) = \\max_{a} E_{P(x_{t} \\mid x_{t-1}, a)}[U(x_{t})] = E_{P(x_{t} \\mid x_{t-1}, a^*)}[U(x_{t})].\\)\nThe way the agent will detect the latent variables will be by comparing the utility value it receives at time t with the probability distribution P(xt|xt\u22121, Actt\u22121) it computed at time t \u2212 1. Two critical aspects must be taken into account in this comparison:\n On the one hand, it is important to notice whether the obtained value U(xt) is greater or smaller than the expected, MEU(t).\n On the other hand, we are interested in how surprising the value is compared to the distribution IP(xt|xt-1, Actt-1).\nAs a result, there are four possible scenarios in which to consider the probability that a latent variable was involved.\n large surprise and negative utility\n large surprise and positive utility\n low surprise and negative utility\n low surprise and positive utility\nCase 1 means that the utility value obtained had a very low associated probability, and if, in addition, the utility value is lower than the mean, then it means that there was some unobserved variable that influenced the agent's action, causing the utility not to be maximized as expected, and making it convenient for the agent to learn to predict when something similar might occur so that the decision is correct. The reason for studying whether the expected value is lower or higher is that, in the end, the agent's choice of decisions is conditioned to look for the highest expected value. So, if it turns out that the value obtained is lower, then the agent could have chosen another decision that might have yielded a higher utility. It is also irrelevant whether the dispersion of the distribution was high or low, since the surprise coefficient indicates whether the utility obtained was a surprise event or not.\nAs for the case 2, if the utility obtained is higher than expected, then we would have the opposite case, i.e. it has been discovered that there is a variable that influences and helps to obtain higher utility values, so it is convenient to learn to model when this may occur. However, in this paper, we will focus on detecting the latent variables that have a negative influence on the utility, and therefore the probability of influence will be close to 0, in the case that the utility obtained is higher than expected.\nIf the surprise coefficient is close to 0, as for the cases 3 and 4, then it means that the probability that there is a variable that the agent does not observe, and that changes its interaction with the environment will be low, since the agent is able to correctly predict the values of the observation variables that it will get after executing the actions. Taking all this into account, the coefficient that will be responsible for making this comparison will be an extrapolation of the one described in (22), which will also include whether the utility obtained is higher or lower than the one expected.\n\\(C_U (P \\mid\\mid x_t) = sign (U(x_t) \u2013 MEU(t)) \\cdot C_s(P \\mid\\mid x_t),\\)\nwhere U(xt) corresponds to the value of the utility function for the observation xt and P corresponds to the probability distribution P(xt|xt\u22121, Actt\u22121). This surprise"}, {"title": "5.2 CPT Estimation", "content": "Next, the parameters of the CPTs, after introducing the hidden variable in the DDN's graph, must be learned. That is, the hidden variable's children and parents CPTs parameter estimation is performed by Hard Weighted Expectation Maximization. Specifically, after experiencing an epoch, the agent computes the probabilities of influence of the hidden variable at each time observation, as well as the difference in utility between the current and the previous observation.\nThis is followed by the application of Hard Weighted EM, which is iterated until convergence. In the initial iteration, the value of the hidden variable in each observation is imputed using the probability of influence, and each observation is assigned a weight based on the difference in utility wi:\n\\begin{gathered}\nw_0 = 1,\\\\\nw_i = 1 + \\mid U(x_{i-1})-U(x_i) \\mid \\quad \\text{di = 1,..., tmax}\n\\end{gathered}\nThe introduction of these wi serves to enhance the weighting of observations that exhibit minimal surprise but a significant impact on utility, while reducing the weighting of observations that exhibit a high degree of surprise but lack relevance to the observed differences in utility. This approach enables the agent to learn a representation of HV that aligns with its existing knowledge, thereby enabling it to behave in a manner consistent with its previous behaviour in the absence of HV. For the rest of the iterations, the weights associated with the observations are kept, while the value of the hidden value is imputed using the DDN and the parameters estimated in the previous iteration for P(HV = 1).\nReturning to the Learning to Detour case, after learning the hidden variable HV, a new transition must be learned so that expected observation BT = 1 is realized when the Step Forward action is activated very close to the barrier. Also, after learning the"}, {"title": "5.3 Learning to Detour behavior", "content": "Thus, after learning the hidden variable HV, it can be observed from Figure 8. that the agent's behaviour has undergone an important transformation compared to the behaviour before learning.\nThe first thing to notice, is the shift from the agent's exclusive reliance on the Step Forward action (before learning) to a more nuanced approach (during learning), involving a reduction in the power of Step Forward decisions and the initiation of Step Aside actions to the right. This is due to the changes made to the CPT of BTt+1. As can be seen in the marginalized CPT of BTt+1 before and after learning (Table 2), there is a greater probability of a BT = 1 in the presence of the HV. This CPT, along with the CPT learned from HV (Table 3), enables the agent to act in this way.\nThe second significant event occurs when the agent collides with the barrier. During learning, the agent reduces the power of the Step Forward action. This is a consequence of the agent's adaptation, which has led to a reduction in the expected utility of activating Step Forward when the probability of HV = 1 is high. It is important to notice that at this moment, another important change takes place within our framework. That is, one of the key objectives is to enable the agent to learn from unexpected outcomes, which will enable it to make more accurate predictions and, therefore, avoid surprises in the future. In our case, Figures 9 and 10 illustrate that the surprises associated with BT (Barrier Tactile) and D (Depth) are reduced in comparison to those observed prior to learning. This clearly indicates that the predictions made by the agent are more accurate with respect to the environment when the HV factor is present. In summary,"}, {"title": "6 Discussion", "content": "This is also a first step towards a theory of Active Structure Learning for Resilient autonomous agents and robots. They must be able to construct causal internal models (Bongard et al.; Corbacho et al)[9,13]. As Bongard et al. [9] express, animals sustain the ability to operate after injury by creating qualitatively different compensatory behaviors. Although such robustness would be desirable in engineered systems, most machines fail in the face of unexpected damage. Corbacho et al.[13] is one of the first complete models of animal behavior learning after a lesion (that impairs the animal's motor capabilities) by internal causal model construction. The research here presented is also related to work on lifelong machine learning (Chen and Liu)[11]. That is, a machine learning paradigm that continuously learns by accumulating past knowledge that it then uses in future learning and problem solving. It is also related to the field of probabilistic robotics, which is concerned with perception and control in the face of uncertainty to endow robots with a new level of robustness in real-world situations [51]. It is also related to the field of autonomous mobile robot navigation (Arkin, 1998, 2005; Bekey, 2005; Pandey et al., 2017)[3,4,8,33]."}, {"title": "7 Work in progress", "content": "In this section we briefly describe work in progress that we do not include in this paper, due to space limitations, or level of maturity of the current development."}, {"title": "7.1 Other simulations and possible real world applications", "content": "One of the limiting aspects of the work presented in this specific paper, is that we have only included a specific example within the field of simulated robotics. Nevertheless, we have already started to work with other more realistic examples that will give us the opportunity to discover aspects where our framework can be improved. The first extension, we are already working on, consists on a robotic simulation of a Kephera robot, first within the Webots simulator, to later transfer it to the real physical robot. This"}, {"title": "7.2 Extending the theoretical framework of the Theory of surprise", "content": "We also want to make our theory of surprise more robust, and to that end, we need to continue to work on generalizing it to make it more versatile. Apart from the cases mentioned in the previous section, which entail their respective extensions of the framework, we are working on finding out how the entropy and information dispersion estimators tend to be distributed according to the sample, and whether these distributions depend on the cardinality. If this is the case, we would try to add a normalization factor to the surprise divergence, which would allow us to compare distributions that do not have the same cardinality, since this is currently possible with the surprise divergence as it is, but it is very uninformative since, like the Kullback-Leibler divergence, the result is\u221e. To deal with this, we try to solve it from Bayesian statistics, and assume that the parameter estimators come from samples of a Dirichlet variable with a uniform distribution over the parameters."}, {"title": "7.3 Exploration behaviour after surprise event", "content": "We are also working on an exploration algorithm that is activated each time a latent variable is detected. This would work in such a way that, when a latent variable is detected, we first observe the tuple with the values of the observation variables that have been observed at that time, and make a change in the utility function that prioritizes the agent to visit the tuples of values that are probabilistically close to the tuple of values that have been observed. If, when exploring these nearby states, there are also surprises observed with respect to the original utility function, then the tuples of nearby values that the agent would have to explore, would be added to a queue. In this way, we would avoid having the agent repeat the epochs from the beginning in order to learn, and the learning process would be done in a single epoch, but with a greater number of iterations. This idea is quite related to the concept of curiosity [41,47], so this could be an opportunity to formally relate the concept of surprise and curiosity of an agent in a future."}, {"title": "8 Future Work", "content": "The framework proposed in this paper allows for multiple generalizations and extensions. For completeness reasons, we list here some of them that we intend to address in future work.\n A comparative study of ACSLWL with some existing causal structural learning methods. Although we have seen that the framework gives good results in the designed environments, it remains to be compared whether the results with respect to other causal structural learning algorithms that also deal with latent variables. Before making the comparison, it would first be necessary to do tests in which causal and latent variable learning are done in a single process, unlike in this work where we have focused on latent variable learning for simplicity purposes.\n One potential issue that may arise from dealing with latent variables is that the complexity of the algorithms can grow and make them infeasible in real-world scenarios. Consequently, doing a complexity analysis, both at the computational and memory level, would be a good next step to make the proposed algorithms more robust.\n After the complexity analysis, the next step would be to optimize the algorithms we have presented here to make them more efficient. We are beginning to make online versions of the algorithms, as this would allow the algorithms to be quite more efficient and consistent, as well as bringing us closer to making online and automatic discovery of latent variables possible [10]. In this regard, we are working on online algorithms for estimating the entropy of a random variable and the transfer entropy of a process from the data observed by the agent. These algorithms are based on reformulations of the entropy formula, and we have yet to demonstrate the differences in complexity with respect to traditional estimation algorithms. Within this subsection, the development of the exploration algorithm (described in Work in"}]}