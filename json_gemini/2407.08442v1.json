{"title": "How Deep is your Guess? A Fresh Perspective on Deep Learning for Medical Time-Series Imputation", "authors": ["Linglong Qian", "Tao Wang", "Hugh Logan Ellis", "Robin Mitra", "Jun Wang", "Richard Dobson", "Zina Ibrahim"], "abstract": "We introduce a novel classification framework for time-series imputation using deep learning, with a particular focus on clinical data. By identifying conceptual gaps in the literature and existing reviews, we devise a taxonomy grounded on the inductive bias of neural imputation frameworks, resulting in a classification of existing deep imputation strategies based on their suitability for specific imputation scenarios and data-specific properties. Our review further examines the existing methodologies employed to benchmark deep imputation models, evaluating their effectiveness in capturing the missingness scenarios found in clinical data and emphasising the importance of reconciling mathematical abstraction with clinical insights. Our classification aims to serve as a guide for researchers to facilitate the selection of appropriate deep learning imputation techniques tailored to their specific clinical data. Our novel perspective also highlights the significance of bridging the gap between computational methodologies and medical insights to achieve clinically sound imputation models.", "sections": [{"title": "Introduction", "content": "In key areas such as finance [78], healthcare [25] and weather forecasting [26], predictive analytics frequently rely on extensive, diverse, and multimodal time series data. These datasets are complex, exhibiting characteristics such as skewed and long-tailed distributions and typically comprise a large number of multimodal and interrelated variables sampled at varying frequencies. The complexity is further compounded by the prevalence of non-random missingness, which is informative of the underlying data structure and highly influences the quality of predictive models performing downstream tasks.\nRecent advances in deep learning techniques have outperformed traditional statistical and machine learning imputation methods when applied to large and heterogeneous time-series datasets [91]. These models, herein referred to as deep imputers, do not make strong assumptions about the underlying distribution of the data and learn directly from the data itself, allowing them to capture complex patterns and relationships without being constrained by predefined distributions. Such flexibility is particularly valuable in dealing with large and heterogeneous time-series datasets, where the underlying distribution may be unknown or highly variable.\nThe literature contains a wide variety of deep imputers based on various architectures including convolutional neural networks (CNNs) [35], recurrent neural networks (RNNs) [77], and multi-layer perceptrons (MLPs) [2]. Several recent reviews exist, categorising existing methods in a number of ways. For example, [27] classifies deep imputers by the type of missingness handled (e.g., missing completely at random, missing at random or missing not at random) and the neural architectures employed. In contrast, [91] classifies imputers based on their abilities to model and quantify stochasticity. Furthermore, because of unique characteristics and challenges in healthcare datasets, some literature reviews have focused on surveys specific to medical time series such as [44] and [48]; particularly on benchmarking performance across multiple datasets and exploring how different deep imputers perform across different data types and levels of correlations among the variables modelled.\nExamining the literature on deep imputers, particularly those benchmarking model performance using medical datasets, often shows disparities in reported performance depending on the task, architecture, implementation approach and training data [24]. Our review has also uncovered a variety of data processing and masking techniques used to simulate missingness during experimental evaluation; those are yet to be systematically studied, leaving a critical gap in our understanding of the factors leading to the performance metrics reported in existing literature reviews. More importantly, the current level of scrutiny of the various types of deep imputers is insufficient to make an informed choice of the appropriateness of a given model for a specific task or dataset. To our knowledge, no comprehensive review thoroughly examines deep imputers beyond their architecture and general performance metrics.\nWe have therefore endeavoured to ask the following questions to support the assessment of the suitability of a given deep imputer for a given task: a) what are the characteristics of a given multivariate time series that make a particular category of deep imputers more suitable than others? b) what is the effect of the data processing steps adopted by a given model on the rigor of evaluation and subsequent performance? c) what are the distinct properties of medical time-series data and how effectively does the current paradigm of deep"}, {"title": "Background", "content": "The intended use of EHR data to support clinical care and inform treatment decisions introduces a unique set of characteristics that directly impact the imputation process. Specifically, EHR time series are inherently multimodal, capturing patient health dynamics through continuous measurements (e.g., heart rate), discrete events (e.g., medication changes) and ordinal data (e.g., cancer stages). These multimodal variables are recorded asynchronously at intervals which vary according to clinical needs. For example, heart rate is sampled more frequently than capillary blood glucose, and laboratory tests are conducted when clinicians anticipate a need [42].\nAs the variables recorded in EHRs jointly capture a patient's treatment journey, they exhibit multiple forms of dependencies. Many variables are cross-sectionally correlated, which can introduce redundancy by overemphasising certain features. For example, clinical practice often dictates ordering test panels and laboratory tests are seldom requested in isolation; ordering electrolyte tests typically includes kidney function tests, and calcium tests require concurrent albumin measurements to adjust calcium levels [68]. The correlations are particularly problematic given that clinical insights often lie within the extreme values of skewed distributions of clinical variables, such as abnormally high or low blood sugar. An imputation model, therefore, needs to accurately discern those informative outliers which mark physiological events, from noise.\nThe dependencies among EHR time-series also extend across the temporal dimension, encompassing short-term and long-term temporal dependencies that hold significant meaning within a patient's trajectory. Immediate physiological responses reveal acute body reactions, while prolonged interventions or chronic conditions manifest long-term effects. For example, a heart attack will result in immediate-term changes in blood pressure and heart rate, intermediate-term changes in renal function, and long-term varying effects on many features of interest if it leads to heart failure. Accurately modelling these temporal dependencies requires understanding the sequential patterns in EHRS, and balancing the impact of historical health events against recent ones. Moreover, temporal locality, which refers to physiological events occurring closely together in time, plays significant roles in clinical diagnosis. For instance, recurring patterns of irregular heartbeats persisting over short periods are characteristic of atrial fibrillation. Finally, many variables adhere to the notion of temporal invariance, whereby certain physiological patterns maintain clinical relevance regardless of the specific moment in the patient's timeline and regardless of temporal shift. For example, the presence of elevated troponin levels in blood tests consistently indicates heart injury, regardless of when the test is conducted within the patient's clinical timeline. Here, the imputation must maintain the salience of the variables adhering to temporal invariance.\nFinally, medical time-series distributions are often highly-skewed; they exhibit a natural class imbalance where the amount of training data available for a given outcome of interest is generally low. To illustrate, consider a warning system for in-hospital cardiac arrest, which requires training on patient records whose hospital stays culminate in a cardiac arrest. Cardiac arrest incidence is estimated to be as low as 2.3% of intensive care unit admissions [3], which makes the target population a minority with much less training data available compared to the majority (no cardiac arrest) class. Class imbalance is further amplified by the variability in clinical presentations of patients with the same disease [59], making the group of interest (those with a given clinical presentation) only a minority in any patient population."}, {"title": "EHR Missingness Beyond Traditional Frameworks", "content": "The complex characteristics of EHR data raise the question: how can one effectively model missingness and develop imputation strategies that preserve the dependencies and clinical meaning of the data? Following Rubin's classification [75], missing data mechanisms are traditionally classified into Missing Completely at Random (MCAR), Missing at Random (MAR), and Missing Not at Random (MNAR), reflecting the relationship between missingness in both observed and unobserved data. While Rubin's framework remains fundamental in missing data analysis and continues to be used for classifying imputation models in the latest literature reviews [48,91], it fails to fully capture the complexities of EHR data. In EHRs, missingness often results from the documentation practices involved in routine patient care and may patient-specific insights [7]. For example, a lab test might be missing because it was not ordered for a given patient, or because normal results are not typically recorded. Such practices blur the distinction between missing and observed variables [33,90] and complicate the practicality of the categorical distinctions of MCAR, MAR and MNAR [33].\nA more pertinent issue in EHRs is structured missingness [62], where the large volumes of heterogeneous and multimodal data, along with specific modes of data collection, cause missing values to exhibit associations and structural patterns. This non-random, multivariate associative pattern of missing values fundamentally hinders large-scale machine learning. In EHRS, structured missingness naturally arises from the asynchronous and decision-driven nature of healthcare data collection [90]. Over 60% of EHR data is missing not at random [54] due to irregular sampling intervals dictated by clinical decisions and carry meaningful insights."}, {"title": "Taxonomy of Deep Imputers", "content": "Our multidimensional exploration of the literature on deep imputers is depicted in Figure 1 and further detailed in Table 1. Our taxonomy is based on the following principles:\n1) The effectiveness of an imputation model in interpreting the complexity and missingness patterns of a given dataset relies on the fundamental connection between the dataset's characteristics and the model's inductive bias [32]. Inductive bias refers to the set of assumptions, preferences, or constraints that guide the learning process to reduce the space of possible solutions by prioritising one solution over another, independent of the observed data. This significantly influences the model's behaviour, generalisation capabilities, robustness, and ultimately shapes the resulting imputations. Inductive bias can take several forms related to the complexity of the learned representation, the underlying data distribution, or preferences regarding the learned parameters [32]. Since our review focuses on understanding the link between existing deep imputers and specific data properties and tasks, we highlight two types of inductive bias: preference bias, which dictates a model's assumptions in hypothesis selection by prioritising certain functions over others, effectively shaping its preferences for certain types, patterns, or relationships, and uncertainty bias, which dictates how a model accounts for uncertainty in its produced output.\n2) Modern deep imputers are a combination of neural architectures and frameworks. A network's architecture dictates the physical structure and design of the neural network itself, including the arrangement of layers, the type of neurons used, and how these neurons are connected, e.g. convolutional networks. On the other hand, a framework dictates how this structure is employed and trained to perform the imputation task. Frameworks are higher-level constructs that define the algorithmic approach that leverages a given neural architecture, e.g. a recurrent architecture could be trained within an encoder-decoder or a generative adversarial network (GAN) framework. Each architecture and framework has its own inductive bias, shaping the model's approach to handling missing data and influencing its performance in different scenarios.\nWe first examine existing imputation architectures, focusing on the inductive biases that influence their generalisation capabilities and effectiveness in addressing various types of missingness, data structures, and task complexities. We then distinguish between architecture and the conceptual or mathematical frameworks that utilise a neural network for imputation. Here, inductive bias dictates how a framework approaches specific challenges within the imputation task, shaping the effectiveness of different models across varied scenarios. Finally, different frameworks vary in their approaches to handling uncertainty in the resulting imputations. Here, a model's inductive bias reflects the methods employed to incorporate uncertainty into its imputation outputs, for example, whether uncertainty can be directly modelled through probabilistic means or variability in the model's outputs.\nIn our subsequent discussion, we outline the general inductive biases associated with each class of architectures and frameworks. For each class, we detail the deep imputation models found in the literature, discussing their specific inductive biases. We also highlight the latest developments aimed at enhancing these models to handle data that falls outside their inherent biases, as well as known areas for improvement and ongoing research. Our granular distinction aims to illuminate the strengths and limitations of existing methods for specific datasets, providing a roadmap to assess a model's suitability for various imputation contexts."}, {"title": "Neural Network Architectures", "content": "are intuitively fit for handling temporal sequences [60]. Their inherent inductive bias favours learning temporal dependencies between variables over time, as a result of the recurrent nature of their architecture, which allows them to maintain an internal state across time steps. This bias aligns with the sequential and dynamic nature of medical event sequences, enabling RNNs to effectively capture patient trajectories.\nVanilla RNNs are especially suited for capturing short-term temporal correlations in EHRs, but struggle with modeling long-term dependencies, especially when faced with known EHR issues such as irregular sampling and diverse record lengths. To address these, modified RNN architectures have been introduced in imputers, e.g., GRUD [11] which incorporates temporal decay into its architecture effectively managing time-sensitive missing data. Subsequent models, including MRNN [103] and BRITS [10] integrate solutions to handle irregular sampling into their architectures for more accurate and contextually relevant data imputation. Despite these advances, RNN imputers have yet to accommodate mixed variable types and multi-dimensional time-series [101]. Developing more sophisticated RNN variants that can seamlessly model the complex interplay between continuous and discrete variables within medical time series data remains a crucial next step."}, {"title": "Convolutional Neural Networks (CNNs)", "content": "possess an inductive bias towards capturing local patterns, embodying the principle of temporal locality for detecting acute physiological changes that can mark important clinical events, e.g. tachycardia. One-dimensional (1D) CNNs excel in identifying these pivotal moments by focusing on short-term local variations within the data [46], thus facilitating prompt clinical intervention and effective patient monitoring.\nThe exploration of two-dimensional (2D) CNNs extends the utility of CNNS to capture complex spatial relations across variables [37]. Innovations such as TimesNet [95] and Tiled CNNs [93] employ techniques like Gramian Angular Fields to encode multiple time series as images and leverage spatial correlations. This approach allows CNNs to unearth patterns that span multiple time points and variables, effectively revealing relationships characteristic of physiological signals. For example, multi-channel 2D CNNs can concurrently analyse ECG readings, respiratory rates, and oxygen saturation levels, offering a comprehensive representation of patient health status. However, while these models provide significant insights, they also introduce challenges related to the complexity of transforming and interpreting time series in 2D spaces, potentially obscuring the temporal sequence and causality inherent in the data. Therefore, making the transition from 1D to 2D CNNs requires careful consideration [85] to ensure that temporal information is preserved and accurately represented across time-series."}, {"title": "Transformers", "content": "deviate from traditional sequence processing methods by capturing long-range dependencies through self-attention, enabling the representation of global contextual relationships within data [88]. The intrinsic inductive bias of Transformers towards comprehensive contextuality aligns well with the multifaceted, long-sequence semantics of medical data, enhancing the recognition of complex, multivariate temporal patterns of patient trajectories across extended time frames. The global perspective of Transformers is particularly advantageous in identifying subtle, yet clinically significant patterns that might be overlooked by models with a narrower focus. However, adapting the Transformer to medical time series data requires bespoke modifications to preserve the strict sequential integrity that defines these datasets [13, 104] and to capture equally-useful short-term temporal associations [21]. Models such as DeepMVI [4], NRTSI [79], SAITS [23], GLIMA [84], and MTSIT [102] and CrosFormer [106] incorporate temporal encoding and locality-enhanced attention to maintain sequential coherence while leveraging global dependencies. However, the success of these sophisticated Transformer-based approaches heavily relies on the breadth and depth of available data. The scarcity of comprehensive, publicly accessible medical time series datasets remains a significant barrier, underlining the necessity for expanded data resources for healthcare analytics."}, {"title": "Graph Neural Networks (GNNs)", "content": "[107] stand out for their ability to model the complex relational structures prevalent in medical data, e.g., modeling the dependencies between health indicators such as heart rate, blood pressure, and laboratory results. The foundational inductive bias of GNNs towards capturing spatial relationships allows these models to represent health variables and their interdependencies as nodes and edges within a graph. This representation not only facilitates the preservation of temporal sequences but also enriches the representation to a broader context derived from interconnected health parameters.\nSeveral works have demonstrated the application of GNNs to address the challenges of medical time series imputation. Models like GACN [100] interweave Graph Attention Networks (GAT) [89] and temporal convolution layers to model spatio-temporal dependencies within the imputation process. Similarly, SPIN [57] and GRIN [19] leverage multi-layered attention mechanisms and graph recurrent imputation networks to enhance initial time series representations by using complex relational insights before imputation. Advanced architectures such as AGRN [14] and MDGCN [47] utilise bidirectional recurrent structures, integrating graph convolution with RNN cells to capture spatio-temporal relations. Here, the translation of time series into effective graph structures presents considerable challenges, notably the absence of natural graph representations in traditional time series datasets [105]. Although recent models like TSI-GNN [31] explored incorporating temporal information into bipartite graphs through an extension of graph representation learning, constructing and interpreting these graph models require significant computational efforts and domain knowledge, limiting their scalability and practicality. Furthermore, seamlessly incorporating temporal information into the GNN framework has proven difficult [96], especially aligning the static nature of graphs with the dynamic progression of medical time series [43]."}, {"title": "Learning Frameworks", "content": "In modern deep imputers, a network architecture provides the learning mechanisms for a learning framework, which guides the imputation process towards plausible generalisations of the data to capture and replicate complex data distributions. This synergy ensures that the generated data reflects realistic and clinically relevant patterns. Additionally, neural frameworks offer various approaches to quantifying confidence in the resulting imputation, crucial given that accurate modeling of EHR data directly impacts subsequent downstream tasks and clinical decision-making. Different neural frameworks employ varied paradigms for representing and handling uncertainty, which we include in our discussion to explore the diverse strategies for managing the inherent unpredictability of healthcare data."}, {"title": "Variational Autoencoders (VAES)", "content": "consist of an encoder and a decoder network. The encoder maps input data to a latent space distribution, while the decoder reconstructs data from this distribution. During training, VAEs optimise a variational lower bound on the log-likelihood of the data, ensuring the latent space captures key features of the input distribution. Using an expressive neural network architecture, VAEs effectively learn representations that reflect common properties of EHRS, such as skewness and multimodality.\nVAE variants such as the Heterogeneous Incomplete Variational Autoencoder (HI-VAE) [64], Mixed VAE (VAEM) [55], and MIWAE [58] handle missingness in diverse data types. GP-VAE [28] is a known early imputation model based on a Gaussian process. GP-VAE's performance plummets with heterogeneity in observations and extended missingness. V-RIN [63] aims to bypass GP-VAE's distribution-related imputation bias by incorporating an uncertainty-aware Gated Recurrent Unit (GRU) to blend temporal dynamics with the imputed data. Supnot-MIVAE [45] extends this approach by introducing an additional classifier to refine the evidence lower bounds, enhancing imputation accuracy for classification tasks. Shi-VAE [5] further expands these capabilities by including LSTMs for better temporal structure handling and effectively addressing missing data episodes.\nThe success of VAEs depends on their ability to create meaningful data representations that align with their assumed distribution. With EHR data, this may lead to oversimplifications which obscure vital clinical subtleties. While hybrid VAE models such as V-RIN and Shi-VA bypass the distribution problem by incorporating temporal dynamics, these models face significant challenges in producing interpretable, clinically relevant outputs. Furthermore, the computational intensity for training VAEs, especially when integrating temporal dynamics, remains a barrier for their wide adoption for large medical datasets."}, {"title": "Mixture Density Networks (MDNs)", "content": "[9] combine the predictive power of deep neural networks with the probabilistic precision of mixture models to model the conditional probability distribution of targets based on inputs. MDNs comprise a neural network architecture that outputs parameters of a mixture model (e.g., mean and variance, mixture weights) conditioned on the input data. This enables MDNs to predict a range of possible outcomes or data paths.\nMDNs assume that the data is generated from a mixture of probability distributions. This bias towards probabilistic rather than singular outcome representations aligns with the high variance in outcomes and treatment responses observed in medical time series. Consequently, MDNs can directly capture uncertainty through a mixture of weights and variances in the assumed distributions.\nA highly-performing example is CDNet [51], which effectively models imputed feature distributions and addresses the heterogeneity and irregularity of EHR data by integrating an MDN with a GRU and a Regularized Attention Network (RAN). In CDNet, the GRU captures time dependencies, while the MDN handles latent variable sampling through a mix of neural networks and distributions. This setup allows CDNet to potentially capture complex relationships within EHR data.\nA key challenge in all MDN models is the ability to optimally configure the model to fully exploit its theoretical potential for capturing complex relationships while avoiding overfitting and maintaining the interpretability and clinical relevance of the output."}, {"title": "Generative Adversarial Networks (GANs)", "content": "[17] establish a framework where two neural networks, a generator and a discriminator, compete in a zero-sum minimax game. The generator is tasked with replicating real data distributions to produce synthetic data samples that are indistinguishable from real data, while the discriminator learns to differentiate between real and synthetic samples. The training process involves iteratively updating the generator and discriminator networks to improve the quality of generated samples.\nGANs have an inductive bias towards generating realistically diverse data distributions, as the generator aims to fool the discriminator. This bias enables the adversarial model to effectively generate and impute incomplete multivariate time-series. However, GANs inherently lack direct mechanisms to quantify uncertainty within the imputations, and their application to establishing confidence in the generated data is still in its early stages [65].\nTwo prominent GAN examples found in the literature are GRUI-GAN [52] and E2GAN [53]. GRUI-GAN employs a modified GRU to account for the temporal irregularity of incomplete time series. The model adapts the GRU in both the discriminator and generator to learn the distribution of the entire dataset, the implicit relationships between observations, and the temporal information of the dataset. In the second phase, the input 'noise' of the GAN's generator is trained so that the generated time series closely resembles the original incomplete time series, increasing the likelihood of high-quality generated data. However, optimising the noise vector of GRUI-GAN has proven difficult [53]. To address this, E2GAN integrates the GAN structure with a denoising autoencoder, streamlining the imputation by bypassing direct noise optimisation. Advances continue with frameworks like NAOMI [50], which adopts a non-autoregressive approach to minimise cumulative errors in extended sequences, offering a more robust solution for datasets with significant missing-ness. Additionally, SSGAN [61] enhances the GAN paradigm by incorporating elements such as a temporal reminder matrix and additional classification layers to improve imputation quality.\nGANs have the potential to greatly enhance medical research by creating diverse and comprehensive datasets, including those representing unrepresented conditions and groups. Nevertheless, their inability to quantify confidence in the generated data necessitates additional methods, adding complexity to their practical use. The adversarial training process, though innovative, can induce instabilities like \"mode collapse\" observed in NAOMI and Sim-GAN, where the generator produces limited and repetitive outputs instead of capturing the full diversity of the data. Such limitations are especially significant in clinical settings, where the reliability of data and predictions is paramount."}, {"title": "Diffusion Models", "content": "[39,82] employ a stochastic process to gradually generate synthetic data, progressing from random noise towards distributions that mimic the observed data. During training, diffusion models learn to reverse this process by denoising synthetic samples to match observed data.\nDiffusion models assume that data evolves over time according to a diffusion process, inherently biasing them towards solutions that mimic this generating process and away from those that do not. Although diffusion models do not provide an explicit mechanism to quantify uncertainty, they iteratively incorporate noise into the diffusion process, allowing for the generation of stochastic samples that reflect the variability in the data. Additionally, diffusion models can estimate uncertainty by measuring the divergence between predicted and observed data distributions at each time step.\nExamples of diffusion models include NETRATE [30] and MedDiff [38], which integrate temporal dynamics and domain-specific knowledge into the diffusion process to better handle time-dependent variations and complex medical scenarios. CSDI [86] leverages observed data subsets to guide the generation process but encounters scalability issues due to the quadratic complexity induced by the transformer-based architecture [81]. To tackle this, SSSD [1] substitutes transformers with structured state-space models [34], while CSBI [15] and MIDM [92] enhance efficiency and accuracy by modelling the diffusion process as a Schr\u00f6dinger bridge problem [18] and sampling noise from the conditional distribution of observed representations. PriSTI [49] and DA-TASWDM [97] further push the boundaries by integrating spatio-temporal dependencies and dynamic temporal relationships. SPD [8] represents a paradigm shift by modelling time series from a continuous perspective, better aligning with the stochastic and irregular nature of medical data timelines.\nIn most diffusion models, computational efficiency, clinical relevance, and accuracy remain challenges. While models like PriSTI and DA-TASWDM mark significant advances in personalised and contextually relevant imputations, the lack of straightforward and explicit mechanisms to quantify and communicate uncertainty significantly hampers their practical utility."}, {"title": "Neural Ordinary Differential Equations (Neural ODEs)", "content": "embed a neural network modeled by some function $f$ into an ODE framework [12], which describes the temporal evolution of the data using differential equations. Once trained, an ODE model uses the learned function to simulate the dynamics of the data over time or make predictions about future states based on current observations.\nODE models assume that the underlying temporal evolution of the data data can be described by a system of ordinary differential equations. This bias favours continuous data transitions aligning with the functions learned by the model [66,76], enabling Neural ODEs to capture gradual transitions within patient timelines, potentially overcoming the issue of irregular sampling of EHR data [73]. These models do not directly facilitate uncertainty measurement, but uncertainty can be incorporated as a stochastic process into the differential equations, allowing for the propagation of uncertainty through time. Additionally, ODE models can estimate uncertainty by comparing model predictions with observed data and adjusting model parameters to minimise the discrepancy.\nThe utility of Neural ODEs have been demonstrated through various extensions and applications. ODE-GRU-D [36] extends GRU-D by using an ODE solver to precisely decipher the decay dynamics within time series, thus refining control over decay rates compared to the original GRU-D. Additionally, CRU [76] offers a probabilistic recurrent framework for irregularly sampled time series, utilising a linear stochastic differential equation (SDE) [87] within a latent space structure, thereby incorporating the analytic solutions of continuous-discrete Kalman filter [94] formulations with medical time series analysis. Similarly, CSDE [66] presents a novel probabilistic framework that overcomes the limitations of traditional dynamic models by incorporating Markov dynamic programming [40] and multi-conditional forward-backward losses, facilitating rigorous training and ensuring theoretical optimality.\nSolving the differential equations of Neural ODEs is computationally demanding and is sensitive to the initial set conditions. Furthermore, the robustness and domain relevance of Neural ODE-based imputations is critically reliant on the model's capability to accurately capture the underlying dynamics from medical datasets, which may be compounded by data sparsity. The sophisticated mathematical underpinnings of Neural ODEs can deter clinical applicability due to the abstract nature of their outputs, making it challenging for clinicians to derive clear, actionable insights."}, {"title": "Mind the Gaps", "content": "Having established our taxonomy of the state-of-the-art deep imputers, we now critically appraise the practical aspects of the current paradigm, particularly issues that have a direct impact on clinical utility. We first explore experimental design, identify issues that contribute to inconsistencies in model evaluation, and highlight visible gaps between the capabilities of existing deep imputers and the specific requirements of the medical domain. We then turn our discussion to model reliability, particularly our ability to quantify one's uncertainty in the resulting imputation. Here, we highlight the importance of post-hoc uncertainty quantification methods, particularly for models based on deterministic architectures. Finally, this section highlights the potential for integrating clinical insight with the imputation process to ensure that the generated values align with clinical protocols and are both statistically plausible and clinically meaningful. Our discussions underscore the need for more rigorous, transparent, and comprehensive approaches to performance and reliability evaluation, particularly in handling complex missingness patterns and data distributions."}, {"title": "Mind the Masking Gap", "content": "During experimental evaluation of a deep imputer, masking is used to simulate incomplete data conditions by designating certain data points as missing during training and evaluation. Masking provides a controlled way to test how an algorithm handles incomplete datasets and is thus essential for performance evaluation. Our examination of the literature identified a wide discrepancy in the preprocessing steps employed by different models and potential misalignments between the masking techniques used and the missingness assumptions the models are designed to handle. Our observations are summarised below.\nAs shown in Table 1, deep imputers have been designed to recognise different flavours of missingness (MCAR, MAR, MNAR). During experimental evaluation, however, all models shown in Table 1 use random masking (Figure 2 (a)) to generate missing datasets, predominantly producing MCAR scenarios. This approach oversimplifies the correlations embedded within the EHR time-series, which are reflected in complex missingness patterns across time and cross-sectionally. As discussed in section 2.1, these patterns arise from the underlying physiological processes and recording practices of clinical workers.\nInterestingly, the literature contains masking techniques that can capture spatio-temporal MNAR missingness patterns of medical datasets [20], including temporal masking (Figure 2 (b)), which captures missinngess patterns over time, spatial masking (Figure 2 (c)), which captures cross-sectional missingness and block masking (Figure 2 (d)), which combines the two to concurrently capture different flavours of temporal and cross-sectional correlations and dependencies. Despite their direct applicability to biomedical domains, they are rarely used to evaluate imputation models operating on medical datasets. The only examples using spatio-temporal masking of time-series come from the traffic domain [47, 99].\nThe above problem is exasperated by the lack of information in published work. With the exception of BRITS and CSDI, the use of random masking is not mentioned in the experimental design, and one must examine the accompanying code to discern it. While the use of random masking facilitates model evaluation, it contrasts with the complex and informative MNAR patterns observed in real-world EHRs [29] which many deep imputers have been designed to address. The discrepancy between the theoretical model and experimental evaluation technique therefore highly undermines a deep imputer's capacity, leaving it under-evaluated.\nThere are significant discrepancies and under-reporting of when masking is introduced during experimental evaluation. Data could be pre-masked before being ingested by the model or masked dynamically during the training phase. Traditional pre-masking methods, while more straightforward, limit the model's training to incomplete datasets, reducing its ability to learn from the entire range of clinical features and associated dependencies. In contrast, adopting in-mini-batch masking strategies promises a more dynamic approach by iteratively masking different subsets of the same dataset across training epochs. However, this approach risks overfitting, as the model may become too focused on the artificial missing patterns and fail to recognise the original data structures. Therefore, the decision of when masking is introduced can have a profound impact on the model's capacity to interpret the diverse missing patterns found in a given dataset [70]. Despite the potential impact on the results, this aspect of the experimental design is not reported in most deep imputers discussed in this survey, except for BRITS and GRUD, which mask before training, and CSDI and STAITS, which use in-mini-batch masking during training.\nThere are other decisions that highly influence the resulting masked data but are not discussed in most of the deep imputation literature. An important issue is the methodology used to implement masking. Generally, masking can be implemented using overlaying [22] or augmenting [16] as shown in Figures 2 e-f. Overlaying involves adding artificial missingness in addition to the original missingness the dataset contains, while augmentations only mask complete data, separating the artificial missingness generated from the original missingness. Choosing the type of masking has consequences during model training and evaluation. Although overlaying exposes the model to a broader array of missing data scenarios leading to more robust training and effective imputation strategies, it requires complex evaluation processes and increases the risk of overfitting. On the other hand, augmenting simplifies the model's learning process by allowing it to learn from the artificially introduced missingness without interference from the original missing patterns, but may not fully equip the model to handle the intricate missingness patterns in real-world data. In addition to the above, there is growing evidence that multiple masking, which refers to repeatedly applying masking operations to generate diverse examples during model training, is an effective strategy for improving imputation performance [72]. However, it is unclear how any of the deep imputers implement masking, creating a big gap in our understanding of the rigour of the evaluation techniques, especially in models designed to accommodate non-random EHR missingness."}, {"title": "Mind the Uncertainty Gap", "content": "Accurate imputation is crucial for downstream prediction tasks. In addition, the ability to quantify one's confidence in the resulting imputation enables understanding the limitations and reliability of the generated data. The correlation between imputation accuracy and uncertainty is subtle and model-dependent and has been shown to weaken with increased data diversity [41]. Therefore, a dedicated component for quantifying imputation uncertainty is particularly critical for high-stake medical downstream tasks. For example, [11] demonstrated that accurate imputation in GRUD significantly improves the prediction of patient outcomes in the ICU. Similarly, [28] showed that the ability of GP-VAE to quantify uncertainty improves the interpretability and trustworthiness of imputed laboratory tests by clinical staff.\nThe importance of establishing one's confidence in the resulting imputation is not currently reflected in the state-of-the-art of medical deep imputers. Neural architectures such as RNNs, CNNs, and transformers are different flavours of deterministic imputers with no inherent capabilities to quantify uncertainty. Highly performing models such as GRUD [11] and BRITS [10] effectively handle temporal dependencies and irregular sampling, but do not measure uncertainty. Although generative frameworks such as VAEs and MDNs are naturally probabilistic and can provide measures of confidence in the resulting imputation, they are computationally complex. Moreover, their uncertainty quantification mechanisms rely on distribution-specific assumptions and are highly dependent on the models' inductive bias, limiting the ability to generate interpretable insights that are directly applicable to the complex missingness patterns observed in medical time series.\nGiven the diversity of deep imputers and the distribution-dependent nature of non-deterministic models, there is a need for general post-hoc uncertainty quantification mechanisms that can be utilised regardless of the imputer's underlying architecture or framework. Such independent components will allow uncertainty quantification after training, avoiding potential performance degradation and associated model complexities. These methods can employ model-agnostic uncertainty estimates independent of the inductive bias of the imputation model, enhancing imputation robustness and reliability across different models and datasets.\nThere is a small but growing number of work proposing post-hoc uncertainty quantification for efficient and effective deep imputation architectures. A prominent example is DEARI [69], which extends BRITS by integrating a self-attention mechanism to enhance imputation accuracy and employs a post-hoc Bayesian marginalization strategy to provide reliable uncertainty bounds. CF-RNN [83] adapts the inductive conformal prediction framework to time-series forecasting, which constructs prediction intervals that may potentially uncover the response when conditioned on certain missing patterns. Another approach which remains unexplored is to adopt a multiple imputation framework [74], which involves creating multiple imputed datasets from the predictive distribution conditional on the observed data and combining the results to account for uncertainty. Applying the principles of multiple imputations in conjunction with deep imputers can enhance post-hoc uncertainty quantification by generating diverse imputation scenarios and ensuring robust estimates. This approach, though not immediately obvious in its application to deep learning models, merits investigation to improve the scalability and reliability of imputation models."}, {"title": "Mind the Knowledge Gap", "content": "While medicine is data-rich, it is also knowledge-rich and acknowledging the existing body of clinical knowledge can have a great impact on the reliability and interpretability of the imputed data. The incorporation of domain knowledge into neural network architectures can have a direct influence on alleviating the possible discrepancies between the imputer's inductive bias and the patterns embedded within the data. For instance, RNNs can be fine-tuned with clinical temporal patterns to capture treatment effects or disease progression, while CNNs can be adapted to embed clinical significance into spatial-temporal relationships. This idea is recognised by a few recent imputation attempts such as [71", "98": "where signal"}]}