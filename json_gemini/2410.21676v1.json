{"title": "How Does Critical Batch Size Scale in Pre-training?", "authors": ["Hanlin Zhang", "Depen Morwani", "Nikhil Vyas", "Jingfeng Wu", "Difan Zou", "Udaya Ghai", "Dean Foster", "Sham Kakade"], "abstract": "Training large-scale models under given resources requires careful design of parallelism strategies. In particular, the efficiency notion of critical batch size, concerning the compromise between time and compute, marks the threshold beyond which greater data parallelism leads to diminishing returns. To operationalize it, we propose a measure of CBS and pre-train a series of auto-regressive language models, ranging from 85 million to 1.2 billion parameters, on the C4 dataset. Through extensive hyper-parameter sweeps and careful control on factors such as batch size, momentum, and learning rate along with its scheduling, we systematically investigate the impact of scale on CBS. Then we fit scaling laws with respect to model and data sizes to decouple their effects. Overall, our results demonstrate that CBS scales primarily with data size rather than model size, a finding we justify theoretically through the analysis of infinite-width limits of neural networks and infinite-dimensional least squares regression. Of independent interest, we highlight the importance of common hyper-parameter choices and strategies for studying large-scale pre-training beyond fixed training durations.", "sections": [{"title": "1 Introduction", "content": "Efficient optimization is critical in pre-training large models (LMs) at scale (McCandlish et al., 2018; Shoeybi et al., 2019; Kaplan et al., 2020). In particular, large-batch training is key to accelerating training, as it enables more efficient parallelism across hardware accelerators (You et al., 2017; Goyal et al., 2018). Specifically, understanding the scaling behavior of the critical batch size (CBS) is essential for optimizing data parallelism, as it defines the point beyond which increasing the batch size no longer results in significant gains in computational efficiency. Below the CBS, approximately linear scaling is achievable\u2014doubling the batch size can proportionally reduce the number of optimization steps required to reach a target loss. However, beyond this threshold, further increases in batch size do not markedly improve gradient approximation quality and would lead to diminishing returns, making it essential to balance computational efficiency with model performance (McCandlish et al., 2018; Shallue et al., 2019). This trade-off presents a challenge for studying pre-training given resource constraints as practitioners are compelled to navigate difficult decisions in balancing compute, data, and training time.\nWe investigate the scaling laws governing CBS in the context of autoregressive transformer- based language model pre-training (Vaswani, 2017; Radford et al., 2018). Although CBS lacks a formal definition in the literature, establishing one is essential for enabling its study. Even with"}, {"title": "2 Experimental Design and Empirical Findings", "content": "We first describe the experimental settings. Refer Appendix D includes extra details. Throughout this paper, we use the abbreviations \u2018M' for million, \u2018B' for billion, and \u2018T' for trillion."}, {"title": "2.1 Experimental Settings", "content": "Model and training details. We train a series of autoregressive LMs with a context length of 512 in different sizes ranging from 85M, 151M, 302M, 604M to 1.2B (Appendix D Table 2) on C4 (Raffel et al., 2020) with optimizer-specific hyper-parameters reported in Table 3. We adopt the tokenizer of Eleuther Al's gpt-neox-20b that has a vocabulary of size 50280. We set the micro batch size smaller than the global one and use gradient accumulation to simulate the effects of large global batch sizes. We focus on distributed data parallelism scenarios where communication is frequent, which simplifies the evaluation and abstracts actual wall clock savings into a total number of optimization steps. More details on optimizer configurations and evaluation strategies are included in Appendix D."}, {"title": "Experimental design and outline", "content": "To study CBS in Chinchilla settings, we need to consider target loss on a holdout validation set and measure the amount of optimization steps required to reach it. We consider the validation loss of batch size 256 at step $t_{Chin} = C_{Chin} * N/(ctx\\_len * bsz) \\approx N/(25 * bsz)$ for each model size N in Table 2 (context length ctx_len set to be 512 for every model) (Appendix D Table 1). When scaling up model size jointly with data size, the above implies that each model size would have a different target loss.\nAchieving such a goal through the procedure above is challenging not only due to the combina- torially many hyper-parameter combinations but also the unknown training dynamics of each model size and batch size. In particular, as our focus is on the number of training steps needed to achieve a target validation loss, traditional learning rate decay strategies typically require predefining the total training duration (Loshchilov and Hutter, 2022; Hu et al., 2024; H\u00e4gele et al., 2024; Defazio et al., 2024). To address this, we propose using exponential weight averaging (EWA) (Polyak and Juditsky, 1992) to achieve the desired target validation loss, a simple approach that outperforms other popular choices (Figure 2). Below, we outline several key aspects step-by-step to approach the goal:\n1. Training beyond fixed durations or token amounts, allowing to resume training from checkpoints until the target validation loss is achieved (Section 2.2).\n2. Training with proper hyper-parameters: ensuring proper sweeps of momentum and learning rate (Appendix A); adopting well-tuned values for the \u1e9e2 parameter and the exponential weight averaging decay rate t, tailored to each batch size (Appendix B)."}, {"title": "2.2 Training Beyond Fixed Durations for Reaching Target Validation Loss", "content": "Benchmarking learning rate schedulers. In practice, LMs are usually trained using fixed token budgets (Hoffmann et al., 2022a), which can determine the total number of iterations the training would undergo. This training process can be easily decomposed into learning rate warmup and decay phases so that a lower learning rate is kept at the end of training to enable better optimization. However, our goal is to find the optimal performing run under various hyper-parameters and optimization conditions. This implies a nontrivial decision regarding selecting the maximum training duration (H\u00e4gele et al., 2024; Defazio et al., 2024). As training beyond fixed durations is particularly favorable in many large-scale pre-training scenarios, we benchmark recently proposed methods like schedule-free optimizer (Defazio et al., 2024), cosine, warmup-stable-decay schedule (WSD) (Hu et al., 2024) (or trapezoidal (Zhai et al., 2022) and our proposed constant+EWA strategy which maintains a running average of model weights $\\xi_{t+1} = \\tau \\cdot \\xi_{t}+(1-\\tau) \\cdot \\theta_{t}$ to improve optimization and use & for evaluations."}, {"title": "Takeaway on learning rate scheduling:", "content": "\u2022 EWA consistently improves model training efficiency. Training with EWA is slightly worse than Cosine learning rate decay for small batch sizes while outperforming Cosine for large batch sizes (>0.52M tokens)\n\u2022 Cosine scheduling, while beneficial for small-batch training, proves sub-optimal for large-batch scenarios. Its reliance on a fixed-duration schedule limits its adaptability, and even with appropriate learning rate decay, it underperforms our constant+EWA strategy in large-batch settings."}, {"title": "2.3 Ablation on Model Context Length", "content": "We adopt 512 as the context length of LMs for all of our experiments but it is unclear how it would impact the efficiency of training and whether the existence of CBS would vary when we enlarge the context length.\nSo we sweep over several larger windows $2^{10}, 2^{11}, 2^{12}$ (Section 2.3) using 151M models and find that they are qualitatively similar in terms of CBS while the 512 would be slightly worse in the absolute number of steps that required to reach the target loss. Overall all models in 4 context lengths have very similar relative optimization efficiency across various batch sizes and thus justifies our use of 512 for all the experiments."}, {"title": "Takeaway on model context length:", "content": "Different context lengths ($2^{9} ~ 2^{12}$) have similar efficiency w.r.t. batch size."}, {"title": "2.4 Ablation on Model Width and Depth", "content": "Model sizes can typically be scaled up in two main ways: by increasing the width, which involves enlarging the hidden size of the MLP, or by increasing the depth, which entails adding more layers to the network. As the main result in Figure 1 only involves a single way for scaling up models (Table 2), e.g. training 604M models this way has 2\u00d7 width than the 151M ones. To explore alternative scaling strategies, we investigate how the model behavior changes when we scale the 604M model by increasing the depth by 4x instead (detailed configurations in Table 5)."}, {"title": "Figure 4: Scaling up width and depth are equivalent in efficiency gain for compute-optimal training.", "content": "Firstly, as shown in Figure 4 (left, middle), under Chinchilla settings where data and model size are scaled proportionally, increasing either model depth or width leads to a similar increase in the CBS. Then through controlled comparison (Figure 4, right), we see that using two different ways to scale 151M models to 604M ones is equivalent in efficiency since both curves overlap.\nOur findings may offer practical insights for scaling models under a fixed token budget that is allocated in proportion to model size. This is particularly relevant because scaling model width is"}, {"title": "Takeaway on scaling transformer width and depth in compute-optimal regimes:", "content": "Increasing width and depth has similar effects in the increase of critical batch size for Compute-optimal pre-training."}, {"title": "3 Critical Batch Size Scaling Law", "content": ""}, {"title": "3.1 Formal Definition of Critical Batch Size", "content": "Recall that we define CBS as the batch size that leads to a 20% overhead compared to linear scaling. We will formalize it in this section. First of all, define $R(N, D, B)$ as the best loss achievable for a model of size N using a single pass on D tokens with a batch size B. This would be obtained by optimally tuning all other parameters of the optimizer, while keeping N, D, B fixed. Below is the formal definition of CBS:\nDefinition 1. Define $R_{opt}(N, D) = \\min_{B} R(N, D, B)$, $B_{opt}(N, D) = \\arg \\min_{B} R(N, D, B)$, as the minimal loss achieved optimizing over batch size and the optimal batch size respectively.\nLet's define $f_{N,D}(B)$ to be the number of steps required to reach $R_{opt}(N, D)$ as a function of batch size B. Clearly $f_{N,D}(B_{opt}) = D/B_{opt}$. To define the Critical Batch Size, $B^*(N, D)$, we can define a linear scaling curve $f^*_{N,D}(B) = D/B$. $f^*$ matches f at $B_{opt}$ and then scales down linearly as batch size goes up.\n$B^*(N, D)$ is defined as the maximum batch size $B' > B_{opt}(N, D)$ such that\n$f_{N,D}(B') \\leq 1.2 f_{N,D}(B')$.\nAs illustrated in Figure 5, $B^*(N, D)$ is the batch size at which the number of steps is 20% higher than what is predicted by the linear extrapolation from the optimal batch size. Note here that 20% can be replaced by any other suitable measure of increase from linear scaling."}, {"title": "3.2 Scaling Laws w.r.t. Model Size for Chinchilla-optimal Pre-training", "content": "As observed in all the results above, doubling the batch size for larger models allows them to more efficiently reduce the relative number of steps needed to reach the target loss. We ask whether those increased efficiencies can be predicted via a scaling law.\nWe begin our first step by fitting a power law of batch size (B) to the absolute number of steps (Y) to reach the target loss $log(Y) = log(a + \\frac{b}{B})$ and then derive the critical batch size. Then we derive the CBS via $B^* = \\frac{b}{5a} + 1.2 B_{opt}$, which is implied by a transition point where the total amount of data under this batch size would incur 20% overhead compared to linear scaling: $D_{total} = (a + \\frac{b}{B_{opt}}) * 1.2 B_{opt} = (a + \\frac{b}{B^o}) * B$, where \u03b1 = 1 and Bopt is set to be 256 that is observed in the linear scaling regime."}, {"title": "3.3 Decoupling CBS Scaling Laws w.r.t. Data Size and Model Size", "content": "Controlled comparison with the same data size. Firstly, we use the token size 3.072B of 151M models $t_{chin}$ to record the target validation loss for each model size, and train all the 302M, 604M, 1.2B models with a smaller duration again to reach this target loss. To optimize for performance when training on fewer tokens, we also tune the warmup steps accordingly. Figure 1 (right) shows that all the curves behave similarly and we observe no increase in CBS when enlarging the model size. Then we fit a scaling law that leads to $B^* = 621.341 * N^{0.087}$ (Figure 6, right), implying that CBS is relatively invariant across model sizes."}, {"title": "Takeaway on scaling laws for critical batch sizes:", "content": "Based on the scaling laws and controlled comparisons, we conclude that the increase in CBS in Chinchilla-optimal training is more strongly attributed to extended training durations rather than the increase of model size."}, {"title": "4 Theory on Scaling of Critical Batch Size", "content": "Our experimental results show that CBS increases with larger data sizes but remains (nearly) invariant when scaling up the model size. We now formally investigate this observation using theoretical analysis for both scenarios."}, {"title": "4.1 Fixed Data Size and Scaling Up Model Size", "content": "Various previous works have established infinite width limits of neural networks (Yang and Hu, 2021; Bordelon and Pehlevan, 2022). For initializations and architectures that obey these limits, we can theoretically claim, that for a fixed training time and batch size, the performance of the neural networks asymptotes with increasing width. The formal statement is provided below:\nTheorem 1. For SGD with a given batch size B (or for gradient descent, i.e., B \u2192 \u221e), training iterations t, an error tolerance \u0454 > 0, fixed learning rate schedule and data ordering, for any network and initialization satisfying Master Theorem (Theorem G.4) in Yang and Hu (2021), there exists a width w such that for any two networks $M_1, M_2$ having widths $w_1,w_2 > w$, $|R(\\mathcal{M}_{1},t) \u2013 R(\\mathcal{M}_{2},t)| \\leq \\epsilon$, where R(M, t) denotes the loss of network N at time t.\nProof of Theorem 1. The proof follows from the fact that the trajectory of the network approaches a limit as width tends to \u221e, and thus, by definition of limits, there exists a width w, such that for any two networks with a width greater than w, their loss at time t differs by at most e.\nNote that the assumption of a fixed learning rate schedule with increasing width might seem strong, but recent works (Yang et al., 2022) have shown, that one of these initializations, termed as Maximal Update Parameterization (\u00b5P), exhibits hyperparameter transfer with width. This initialization scheme has also recently gained popularity because of this property and has been used by many open-source implementations (Hu et al., 2024; Dey et al., 2023a,b; Liu et al., 2023). Moreover, works (Yang et al., 2022; Vyas et al., 2023) have empirically demonstrated that with \u00b5P, networks start exhibiting consistent loss curves at practical widths.\nMoreover, as the above theorem holds for a fixed batch size B as well as B \u2192 \u221e, we expect that there exists a finite width w such that the above theorem holds for all batch sizes B. Thus, for fixed training tokens, we would expect that the critical batch size won't scale with model width beyond a point. Although we have mostly talked about scaling model width, note that some recent results have also established such limits for infinite depth ResNets and transformers (Yang et al., 2024; Bordelon et al., 2024a,b), and thus the arguments above also hold for these networks."}, {"title": "4.2 Fixed Model Size and Scaling up Data Size", "content": "We now turn to study the impact of the batch size in mini-batch SGD for a well-specified Gaussian linear regression problem. Let (x, y) be a pair of covariates and responses from a population distribution. Let the population risk and the population distribution be\n$R(w) := E( \\|x^T w - y\\|^2), x \\sim \\mathcal{N}(0, H), y|x \\sim \\mathcal{N}(x^T w^*, \\sigma^2)$,\nwhere w is the trainable parameter, the expectation is over the population distribution, and $(H, w^*, \\sigma^2)$ specify the population distribution. Given D independent samples $(x_i, y_i)_{i=1}^D$ from the population distribution, we consider an estimate given by mini-batch SGD,\n$w_0 = 0, w_{t+1} = w_t - \\frac{\\gamma}{\\sqrt{B}} \\sum_{j=tB}^{(t+1)B-1} (x_j^T w_t - y_j) x_j, t = 0,..., n - 1$,\nwhere \u03b3 > 0 is a constant learning rate, B is the batch size, n := D/B is the number of steps, $w_0 = 0$ is the initialization (without loss of generality), and the output is the average of the iterates, $\\overline{w} := \\frac{1}{n} \\sum_{t=0}^{n-1} w_t$. The following theorem characterizes the CBS of mini-batch SGD that minimizes the number of steps (n) without sacrificing the rate of the attained excess risk."}, {"title": "Theorem 2", "content": "We write $f(D) \\leq g(D)$ if there is a positive constant c such that $f(D) \\leq c g(D)$ for every D > 1. We write $f(D) \\approx g(D)$ if $f(D) \\leq g(D) \\leq f(D)$.\nAssume the population distribution satisfies $o^2 \\approx 1$ and the following capacity and source conditions:\nfor a, b > 1 : $d_i \\approx i^{-a}$, $\\mathbb{E} \\langle v_i, w^* \\rangle^2 \\approx i^{-b}$, $\\mathbb{E} d_i \\langle v_i, w^* \\rangle \\langle v_j, w^* \\rangle = 0$ for i \u2260 j,\nwhere $(\\lambda_i, v_i)_{i>0}$ are the eigenvalues and the corresponding eigenvectors of H, and the expectation is over a prior of w*. Then for b > a, the CBS is $B^* \\sim D^{1-a/\\min{b,2a+1}}$, which allows the mini-batch SGD output w to attain the optimal rate of the expected excess risk (as data size D grows) with the smallest number of steps n.\nProof of Theorem 2. It follows from Theorem 3 and Corollary 1 in Appendix G."}, {"title": "Takeaway on the theory of batch size scaling when scaling up data and model size:", "content": "\u2022 As we scale up model size while keeping the data size fixed, \u00b5P suggests that critical batch size does not scale with model width beyond a point.\n\u2022 Fixing the model size, the critical batch size increases with the training duration. In the context of high- dimensional linear regression, where the variance error dominates the bias error, it is possible to choose a large batch size (as a function of the data size) for mini-batch SGD, allowing for reduced sequential runtime without compromising the rate at which excess risk is minimized."}, {"title": "5 Related Work", "content": "Scaling laws. Scaling laws describe the parametric relationships among key factors involved in training neural networks: model size N, dataset size D, training cost C, and final training loss R. These laws enable the prediction of training loss R based on available resources, making it possible to optimize resource allocation for efficient model training. For example, Hestness et al. (2017) found $R \\propto D^{-\\alpha}$, with \u03b1 \u2208 [0.07, 0.35]. Of the factors they varied, only task can change the exponent a. Changing the architecture optimizers, regularizers, and loss functions, would only change the proportionality factor, not the exponent; Henighan et al. (2020) studied statistical relations between N, D, C, R, over a wide range of values and found similar scaling laws, over the range of $N\\in [10^3, 10^9], C\\in [10^{12}, 10^{21}]$, and over multiple modalities (text, video, image, text to image, etc.). (Kaplan et al., 2020) states that N should be scaled faster than D. However, Chinchilla scaling (Hoffmann et al., 2022b) found that models are under-trained, and then suggests that when given an increased budget (in FLOPs), to achieve compute-optimal, the number of model parameters (N) and the number of tokens for training the model (D) should scale in approximately equal proportions. Recent efforts (Besiroglu et al., 2024; Porian et al., 2024; Pearce and Song, 2024) have been made in reproducing the scaling laws from (Hoffmann et al., 2022b) and the (Kaplan et al., 2020). Different from our focus on measuring the efficiency notion of CBS, most of them focus on deriving optimal hyper-parameters (Porian et al., 2024; Bi et al., 2024) including learning rate and batch size from"}, {"title": "6 Concluding Remarks", "content": "In conclusion, this study provides an extensive examination of the scaling laws for critical batch size in large-scale autoregressive language model pre-training. By systematically analyzing the relationship between model size, data size, and CBS, we found that while CBS slightly increases with data size, it remains relatively invariant to model size. Our findings contribute insights into optimizing data parallelism and computational efficiency in pre-training setups, particularly highlighting the role of hyper-parameters such as learning rate scheduling and optimizer settings. These insights offer practical strategies for scaling models while maintaining efficiency, which is critical in resource-constrained scenarios. We leave for future work the investigation of scenarios involving more heterogeneous data sources than C4, as well as cases where the ratio of model size to data size substantially exceeds the Chinchilla-optimal configuration."}]}