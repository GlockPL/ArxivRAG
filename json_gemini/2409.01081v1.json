{"title": "Beyond Efficiency: Molecular Data Pruning for Enhanced Generalization", "authors": ["Dingshuo Chen", "Zhixun Li", "Yuyan Ni", "Guibin Zhang", "Ding Wang", "Qiang Liu", "Shu Wu", "Jeffrey Xu Yu", "Liang Wang"], "abstract": "With the emergence of various molecular tasks and massive datasets, how to perform efficient training has become an urgent yet under-explored issue in the area. Data pruning (DP), as an oft-stated approach to saving training burdens, filters out less influential samples to form a coreset for training. However, the increasing reliance on pretrained models for molecular tasks renders traditional in-domain DP methods incompatible. Therefore, we propose a Molecular data Pruning framework for enhanced Generalization (MolPeg), which focuses on the source-free data pruning scenario, where data pruning is applied with pretrained models. By maintaining two models with different updating paces during training, we introduce a novel scoring function to measure the informativeness of samples based on the loss discrepancy. As a plug-and-play framework, MolPeg realizes the perception of both source and target domain and consistently outperforms existing DP methods across four downstream tasks. Remarkably, it can surpass the performance obtained from full-dataset training, even when pruning up to 60-70% of the data on HIV and PCBA dataset. Our work suggests that the discovery of effective data-pruning metrics could provide a viable path to both enhanced efficiency and superior generalization in transfer learning.", "sections": [{"title": "Introduction", "content": "The research enthusiasm for developing molecular foundation models is steadily increasing [1-5], attributed to its foreseeable performance gains with ever-larger model and amounts of data, as observed neural scaling laws [6] and emergence ability [7] in other domains. However, the computational and storage burdens are daunting in model training [8], hyperparameter tuning, and model architecture search [9-11]. It is therefore urgent to ask for training-efficient molecular learning in the community.\nData pruning (DP), in a natural and simple manner, involves the selection of the most influential samples from the entire training dataset to form a coreset as paragons for model training. The primary goal is to alleviate training costs by striking a balance point between efficiency and performance compromise. A trend in this field is developing data influence functions [12-14], training dynamic metrics [15-18], and coreset selection [19-21] for lossless - although typically compromised - model generalization. When it comes to molecular tasks, transfer learning, particularly the pretrain-finetune paradigm, has been regarded as the de-facto standard for enhanced training stability and superior performance [22-24]. However, existing DP methods are purposed for train-from-scratch setting, i.e., the model is randomly initialized and trained on the selected coreset. A natural question arises"}, {"title": "Preliminaries", "content": "In this section, we take a detour to revisit the traditional data pruning setting and pretrain-finetune paradigm before introducing the problem formulation of source-free data pruning.\nConsider a learning scenario where we have a large training set denoted as $D = \\{(x_i, y_i)\\}_{i=1}^{n}$, consisting of input-output pairs $(x_i, y_i)$, where $x_i \\in X$ represents the input and $y_i \\in Y$ denotes the ground-truth label corresponding to $x_i$. Here, $X$ and $Y$ refer to the input and output spaces, respectively. The objective of traditional data pruning is to identify a subset $D' \\subset D$, that captures the most informative instances. The model trained on this subset $D'$ should yield a slightly inferior or comparative performance to the model trained on the entire training set $D$. Thus they need to strike a balance between efficiency and performance.\nGiven source and target domain datasets $D_s$ and $D_t$, the goal of pretraining is to obtain a high-quality feature extractor $f$ in a supervised or unsupervised manner. While in the finetuning phase, we aim to adapt the pretrained $f$ in conjunction with output head $g$ to the target dataset $D_t$.\nConsidering the proficiency of molecular pre-trained models in capturing meaningful chemical spaces, their widespread usage in enhancing performance across diverse molecular tasks has become commonplace. This necessitates a reassessment of the conventional approach to DP within the molecular domain and, more broadly, within the field of transfer learning. Previous attempts [26, 27] in data pruning for transfer learning primarily focus on trimming upstream data, selecting samples that closely match the distribution of downstream tasks to align domain knowledge. However, this necessitates retraining the model from scratch, which is notably ill-suited for the molecular domain, where the continual influx of new molecules introduces novel functionalities and structures. To this end, we propose a tailored DP problem for molecular transfer learning:\n(Source-free data pruning). Given a target domain dataset $D_t$ and a pretrained feature extractor parameterized by $\\theta_s$, we aim to identify a subset $D'_t \\subset D_t$ for training, while being agnostic of the source domain dataset $D_s$, to maximize the model generalization."}, {"title": "Methodology", "content": "As with generic data pruning pipelines, the MolPeg framework is divided into two stages, scoring and selection. In the first stage, we define a scoring function to measure the informativeness of samples and apply it to the training set. In the subsequent stage, given the sample scores, we rank them in ascending order and maintain the high-ranking samples for training. Note that our pruning method is dynamically performed during the training process, rather than conducted before training.\nWe next introduce the MolPeg framework in detail. We track the training dynamics of two models with different update paces. For each training sample, we measure the difference in loss between the two models to quantify its importance, and then make the final selection based on this metric. In the following parts, we first intuitively introduce our design of the scoring function. Then, we further explore the theoretical support behind the effectiveness of the MolPeg. The connections with existing DP methods are discussed in Appendix F. The overall framework is illustrated in Figure 2."}, {"title": "The MolPeg framework", "content": "The design of the scoring function addresses two key issues, (1) how to achieve the perception of source and target domain and (2) how to measure the informativeness of the samples.\nSince we are unable to access the upstream dataset, the pre-trained model serves as the only entry point of the source domain. During the finetuning stage, apart from online encoder undergoing gradient optimization via back-propagation, we further maintain a reference encoder updated with exponential moving average (EMA) to perceive the cross-domain knowledge. Note that both encoders are initialized by pretrained model $\\theta_0 = \\xi = \\theta$, where $\\theta_t$ and $\\xi_t$ denotes the parameters of online and reference model at batch step t, respectively. They are updated as follows:\n$\\theta_{t+1} = \\theta_t - \\alpha \\nabla_\\theta L(D_t, \\theta_t) \\qquad \\xi_t = \\beta \\theta_t + (1-\\beta)\\xi_{t-1}$  (1)\nwhere $\\alpha$ is the learning rate and $\\beta\\in [0, 1)$ is the pace coefficient that controls the degree of history preservation. Here $D_t$ is the selected finetuning dataset for epoch t, and $\\nabla_\\theta L(D_t, \\theta_t)$ denotes the average gradient $E_{x_i \\in D_t} \\nabla_\\theta L(x_i, \\theta_t)$ for short. Intuitively, We control the influence of target domain on the reference encoder via EMA. With a small update pace $\\beta$, the online encoder prioritizes target domain, while the reference encoder emphasizes source domain."}, {"title": "Informativeness measurement and selection", "content": "By far we explicitly represent the inaccessible source do-main knowledge with the help of the reference model, facilitating us to further quantify the informativeness of each sample in the cross-domain context. Our motivation for measuring the sample informativeness comes from a recent work that improves the neural scaling laws [30]. They suggest that the best pruning strategy depends on the amount of initial data. When the data volume is large, retaining the hardest samples yields better pruning results than retaining the easiest ones; the conclusion is the opposite when the data volume is small. This contrasts with the conclusion that only the hardest samples should be selected [15]. From an intuitive perspective, simple samples are more representative, allowing the model to adapt to downstream tasks more quickly, while hard samples are crucial for model generalization since they are considered supporting vectors near the decision boundaries. This debate highlights that in data pruning, how to perform a mixture of easy and hard samples is a critical factor. As shown in Figure 3, when 60% samples in the HIV dataset are pruned, simply selecting the easiest or hardest samples leads to a performance drop in later epochs.\nTherefore, we opt to retain both easy and hard samples instead of singularly removing one type. To measure the information gap between domains, we adopt both online and reference encoder to infer each sample and calculate the absolute loss discrepancy between them:\n$D_t = \\{x \\in D_t | |L(x, \\theta_t) - L(x, \\xi_t)| \\geq \\delta\\},$ (2)"}, {"title": "Theoretical Understanding", "content": "In this section, we explore the theoretical underpinnings of the data selection process in MolPeg. Recall that our scoring function is defined by loss discrepancy, we further make use of Taylor expansion on the designed scoring function. Then, from the gradient perspective, i.e., the first-order expansion term, we derived the following propositions and the complete proof is provided in the Appendix E.\n(Slow parameter updating). Assume the learning rate is small enough, so that the parameter update $\\Delta \\theta_t = \\theta_{t+1} - \\theta_t$ is small for every time step, i.e. $|\\|\\Delta \\theta_t\\|| \\leq \\epsilon, \\forall t \\in N$, $\\epsilon$ is a small constant.\n(Interpretation of loss discrepancy). With Assumption 1, the loss discrepancy can be approximately expressed by the dot product between the data gradient and the \"EMA gradient\":\n$L(x, \\xi_t) - L(x, \\theta_t) = \\alpha \\nabla_{\\theta}L(x, \\theta_t)v^{EMA} + O(\\epsilon^2),$ (3)\nwhere $v^{EMA}$ denotes $\\sum_{j=1}^t(1 - \\beta)\\nabla_{\\theta}L(D_{t-j}, \\theta_{t-j})$, i.e. the weighted sum of the historical gradients, which we termed as \"EMA gradient\".\n(Gradient projection interpretation of MolPeg). In the context of neglecting higher-order small quantities, we define $D^+ \\subseteq D$ and $\\hat{D}^+ \\subseteq D$ denote the sets of samples for which the dot products between the data gradients and the \"EMA gradient\" are positive. Then, the gradient of the selected \"simple\" samples can be expressed as:\n$\\nabla_{\\theta}L(\\hat{D}^+, \\theta) = \\nabla_{\\theta}L({D}^+,\\theta) + a v^{EMA}, a \\geq 0.$ (4)\nSimilarly, we define $D^- \\subseteq D$ and $\\hat{D}^- \\subseteq D$ as samples that have negative dot products, then\n$\\nabla_{\\theta}L(\\hat{D}^-, \\theta) = \\nabla_{\\theta}L({D}^-,\\theta) + b v^{EMA}, b \\leq 0.$ (5)\na = 0 and b = 0 holds if and only if the loss discrepancy across ${D}^+$ and ${D}^-$ is uniform respectively, which are uncommon scenarios.\nTherefore, our data selection strategy essentially increases the weight of the (opposite) EMA gradient direction in the data gradient for easy (hard) samples. When ${D}^+$ predominates, indicating a majority of simple samples in the dataset, this simplified model is akin to the momentum optimization strategy, which utilizes the sum of the current data gradient and the weighted EMA gradient to update the"}, {"title": "Experimental Settings", "content": "To comprehensively validate the effectiveness of our proposed MolPeg, we conduct experiments on three datasets, i.e., HIV [28], PCBA [32], and QM9 [33], covering four types of molecular tasks. These tasks span two molecular modalities-2D graph and 3D geometry\u2014as well as two types of supervised tasks, i.e., classification and regression.\nGiven the potential issues of over-fitting and spurious correlations that may arise with limited samples when a large pruning ratio is adopted, we focus on relatively large-scale datasets containing at least 40K molecules. Below, we briefly summarize the information of the datasets. For a more detailed description and statistics of the dataset, please refer to Appendix A."}, {"title": "Implementation details", "content": "In this section, we provide a succinct overview of the implementation details for our experiments, including backbone models for different modalities, training details and evaluation protocols.\nGiven the two modalities involved in our experiment, we need corresponding backbone models for data modeling. Below is a concise introduction to the backbone models. For a more comprehensive understanding of the model architecture, please refer to the Appendix D.\n\u2022 For 2D graphs, we utilize the Graph Isomorphism Network (GIN) [34] as the encoder. To ensure the generalizability of our research findings, we adopt the commonly recognized experimental settings proposed by Hu et al. [35], with 300 hidden units in each layer, and a 50% dropout ratio. The number of layers is set to 5.\n\u2022 For 3D geometries, we employ two widely used backbone models, PaiNN [36] and SchNet [37], as the encoders for different datasets. For SchNet, we set the hidden dimension and the number of filters in continuous-filter convolution to 128. The interatomic distances are measured with 50 radial basis functions, and we stack 6 interaction layers. For PaiNN, we adopt the setting with 128 hidden dimension, 384 filters, 20 radial basis functions, and stack 3 interaction layers.\nWe adhere to the settings proposed by [35] for our experiments. In classification tasks, the dataset is randomly split, with an 80%/10%/10% partition for training, validation and testing, respectively. In regression tasks, the dataset is divided into 110K molecules for training, 10K for validation, and another 10K for testing. The Adam optimizer [38] is employed for training with a batch size of 256. For classification tasks, the learning rate is set at 0.001 and we opt against using a scheduler. For regression tasks, we align with the original experimental settings of PaiNN and SchNet, setting the learning rate to 5 \u00d7 10-4 and incorporating a cosine annealing scheduler.\nWe conduct a series of experiments between model performance and varied data quantities. Specifically, we divide the pruning ratio into six proportional subsets: [20%, 40%, 60%, 70%, 80%, 90%], and for each configuration, we randomly select five seeds and report the mean performance. For HIV datasets, performance is measured using the Area Under the ROC-Curve (ROC-AUC), while reporting the performance on PCBA in terms of Average Precision (AP) -higher values in both metrics indicate better performance. When assessing quantum property predictions in the QM9 dataset, the Mean Absolute Error (MAE) is used as the performance metric, with lower values indicating better accuracy."}, {"title": "Empirical Studies", "content": "Our empirical studies for classification tasks utilize the 2D graph modality as the input. We employ GIN as the backbone model and adopt GraphMAE [39] for model pre-training on the PCQM4Mv2 dataset. For a comprehensive comparison, we select the following two groups of DP methods as primary baselines in our experiments: static DP and dynamic DP, following [40]. The majority of previous methods fall into the former group, from which we select 14 competitive and classic DP methods as baselines, i.e., hard random pruning, CD [41], Herding [17], K-means [30], Least Confidence [42], Entropy [42], Forgetting [15], GraNd [18], EL2N [18], DeepFool [43], Craig [44], Glister [45], Influence [13] and DP [14]. Since dynamic pruning remains a niche topic, we identify four methods, to the best of our knowledge, to serve as baselines, i.e., soft random pruning, e-greedy [46], UCB [46] and InfoBatch [40], with MolPeg also falling into this category. Please refer to Appendix C for a more detailed introduction to the baselines."}, {"title": "Empirical analysis on classification tasks", "content": "Our systematic study suggests the following trends: (i) Dynamic DP strategies significantly outperform static DP strategies. Soft random, as a fundamental baseline in dynamic DP, consistently outperforms the baselines of static groups across almost all pruning ratios, even surpassing some strong competitors such as Glister and GraNd. We also observe that the performance advantage of dynamic DP becomes more pronounced when the pruning ratio is relatively large. Intuitively, compared to fixing a subset for training, dynamic pruning can perceive the full dataset during training, thereby possessing a larger receptive field and naturally yielding better performance. As more data samples are retained, the ability of both groups to perceive the full training set converges, leading to smaller performance differences between them. (ii) MolPeg achieves the state-of-the-art performance across all proportions. On the HIV dataset, we can achieve nearly lossless pruning by removing 80% of the samples, surpassing other baseline methods significantly. Similarly, on the larger-scale PCBA dataset, we can still achieve lossless pruning by removing 60% of the data. (iii) MolPeg brings superior generalization performance compared to fine-tuning on the full dataset. For example, on the HIV dataset, we achieve an ROC-AUC performance of 86 when pruning 40% of the data, surpassing the 85.1 achieved with training on the full dataset. This indicates that appropriate data pruning can better aid model generalization given a pre-trained model. However, as more downstream data is introduced, the improvement brought by our method diminishes, as shown by the 20% pruning proportion, due to introducing data samples that hinder model generalization."}, {"title": "Efficiency comparison", "content": "In addition to performance, time efficiency is another crucial indicator for DP. We conduct a performance-efficiency comparison of var-ious DP methods on the HIV dataset at a 60% pruning ratio, as shown in Figure 4. We define time effi-ciency as the reciprocal of the runtime multiplied by 1000. A higher value of this metric indicates greater efficiency. We can observe that despite MolPeg ex-periencing slight efficiency loss compared to random pruning, it demonstrates superior pruning performance. Compared to the current SOTA baseline model, Info-Batch, our method achieves better model generalization with comparable efficiency. Conversely, static pruning methods incur 1.6x to 2.1x greater time costs than ran-dom pruning, with model performance stagnating or declining. This underscores that MolPeg achieves supe-rior performance with minimal efficiency costs. Despite increased memory usage introduced by the reference model, EMA is commonly used to stabilize molecular training, which allows our method to utilize EMA-saved models without added memory overhead."}, {"title": "Results on QM9 dataset", "content": "Since regression is another common type of downstream molecular task, we also present the empirical results of MolPeg on two properties using the QM9 dataset, alongside comparisons with state-of-the-art methods. To ensure a fair comparison of experimental results, we employ the commonly used 3D geometry modality for modeling. We adopt GeoSSL [47] as the pretraining strategy and PaiNN as the backbone model, following the settings outlined by Liu et al. Empirical results are presented in Table 2. It can be observed that MolPeg consistently outperforms other DP methods. However, all DP methods unexpectedly demonstrate inferior performance than random pruning in certain pruning ratios (80% and 90%). We speculate this phenomenon is attributed to the PCQM4Mv2 dataset used for pre-training and the QM9 dataset having a close match in the distribution patterns of molecular features. Thus, any non-uniform sampling methods would lead to biased data pruning which exacerbates distribution shift and hinders domain generalization."}, {"title": "Sensitivity Analysis", "content": "We further conduct extensive sensitivity analysis to validate the robustness of MolPeg across different pre-training strategies, molecular modalities, and hyperparameter choices. All experiments below are conducted on the HIV dataset."}, {"title": "Conclusion", "content": "In this work, we propose MolPeg, a novel molecular data pruning framework designed to enhance generalization without the need for source domain data, thereby addressing the limitations of existing in-domain data pruning (DP) methods. Our approach leverages two models with different update paces to measure the informativeness of samples. Through extensive experiments across four downstream tasks involving both classification and regression tasks, we demonstrate that MolPeg not only achieves lossless pruning but also outperforms full dataset training in certain scenarios. This underscores the potential of MolPeg to optimize training efficiency and improve the generalization of pre-trained models in the molecular domain. Our contributions highlight the importance of considering source domain information in DP methods and pave the way for more efficient and scalable training paradigms in molecular machine learning."}, {"title": "Datasets and Tasks", "content": "In the following, we will elaborate on the adopted datasets and the statistics are summarized in Table 4.\n\u2022 PCQM4Mv2 is a quantum chemistry dataset curated by Hu et al. [49] based on the PubChemQC project [29]. It comprises 3,746,620 molecules and is extensively utilized in molecular pretraining tasks. We also adopt this widely recognized dataset for our molecular pretraining endeavors.\n\u2022 HIV dataset is designed to evaluate the ability of molecular compounds to inhibit HIV replica-tion [28] in a binary classification setting, consisting of 41,127 organic molecules.\n\u2022 PCBA is a dataset consisting of biological activities of small molecules generated by high-throughput screening [32]. It contains 437,929 molecules with annotations of 92 classification tasks.\n\u2022 QM9 is a comprehensive dataset, structured for regression tasks, that provides geometric, energetic, electronic and thermodynamic properties for a subset of GDB-17 database, comprising 134 thousand stable organic molecules with up to nine heavy atoms [33]. In our experiments, we delete 3,054 uncharacterized molecules which failed the geometry consistency check [50]. We include the U0 and ZPVE in our experiment, which cover properties related to stability, and thermodynamics. These properties collectively capture important aspects of molecular behavior and can effectively represent various energetic and structural characteristics within the QM9 dataset."}, {"title": "Computing infrastructures", "content": "Software infrastructures. All of the experiments are implemented in Python 3.7, with the following supporting libraries: PyTorch 1.10.2 [51], PyG 2.0.3 [52], RDKit 2022.03.1 [53].\nHardware infrastructures. We conduct all experiments on a computer server with 8 NVIDIA GeForce RTX 3090 GPUs (with 24GB memory each) and 256 AMD EPYC 7742 CPUs."}, {"title": "Related work", "content": "Data pruning (DP) has been an ongoing research topic since the rise of deep learning. Traditional data pruning strategies often focus solely on the task dataset, exploring ways to represent the distribution of the entire dataset with fewer data points, thereby reducing training costs. However, with the recent advancements in transfer learning, focusing solely on the task dataset has become insufficient. Consequently, some data pruning strategies have been developed for transfer learning scenarios. We classify these strategies into in-domain data pruning and cross-domain data pruning.\nMost existing data pruning methods fall into this category. We further divide them into two groups: static data pruning and dynamic data pruning following [40]. Static data pruning aims to select a subset of data that remains unchanged throughout the training process, while dynamic data pruning methods consider that the optimal data subset evolves dynamically during training. Guo et al. [54] classify existing static data pruning methods based on their scoring function into the following categories: geometry [41, 55, 16, 56, 57], uncertainty [42], loss [15, 18, 40], decision boundary [43, 58], gradient matching [59, 44], bilevel optimization [12, 45, 60], submodularity [61-63], and proxy [64, 42]. Despite dynamic data pruning is still in its early stages, it has demonstrated superior performance. Raju et al. [46] propose two dynamic pruning methods called UCB and e-greedy. These methods define an uncertainty value and calculate the estimated"}, {"title": "Backbone Model", "content": "Graph Isomorphism Network (GIN) [34] is a simple and effective model to learn discriminative graph representations, which is proved to have the same representational power as the Weisfeiler-Leman test [66]. Recall that each molecule is represented as G = (A, X, E), where A is the adjacency matrix, X and E are features for atoms and bonds respectively. The layer-wise propagation rule of GIN can be written as:\n$h_i^{(k+1)} = f_{atom}^{(k+1)}\\left(h_i^{(k)} + \\sum_{j \\in \\mathcal{N}(i)} h_j^{(k)} + f_{bond}^{(k+1)}(E_{ij})\\right).$ (6)\nwhere the input features $h_i^{(0)} = x_i$, $\\mathcal{N}(i)$ is the neighborhood set of atom $v_i$, and $f_{atom}, f_{bond}$ are two MultiLayer Perceptron (MLP) layers for transforming atoms and bonds features, respectively. By stacking K layers, we can incorporate K-hop neighborhood information into each center atom in the molecular graph. Then, we take the output of the last layer as the atom representations and further use the mean pooling to get the graph-level molecular representation:\n$Z_{2D} = \\frac{1}{N} \\sum_{i \\in v} h_i^{(K)}.$ (7)"}, {"title": "Embedding 3D geometries", "content": "We use the SchNet [37] as the encoder for the 3D geometries in HIV dataset. SchNet models message passing in the 3D space as continuous-filter convolutions, which is composed of a series of hidden layers, given as follows:\n$h_i^{(k+1)} = f_{MLP}\\left( \\sum_{j=1}^{N} f_{FG}(h_j^{(k)}, r_i, r_j) + h_i^{(k)} \\right),$ (8)\nwhere the input $h_i^{(0)} = a_i$ is an embedding dependent on the type of atom $v_i$, $f_{FG}(\\cdot)$ denotes the filter-generating network. To ensure rotational invariance of a predicted property, the message passing function is restricted to depend only on rotationally invariant inputs such as distances, which satisfying the energy properties of rotational equivariance by construction. Moreover, SchNet adopts radial basis functions to avoid highly correlated filters. The filter-generating network is defined as follow:\n$f_{FG}(x_j, r_i, r_j) = x_j \\cdot e_k(r_i - r_j) = x_j \\cdot exp(-\\gamma ||r_i - r_j||^2 - \\mu_k||^2).$ (9)\nSimilarly, for non-quantum properties prediction concerned in this work, we take the average of the node representations as the 3D molecular embedding:\n$Z_{3D} = \\frac{1}{N} \\sum_{i \\in v} h_i^{(K)},$ (10)"}, {"title": "Proof of Theoretical Analyses", "content": "(Slow parameter updating) Assume the learning rate is small enough, so that the parameter update $\\Delta \\theta_t = \\theta_{t+1} - \\theta_t$ is small for every time step, i.e. $|\\|\\Delta \\theta_t\\|| \\leq \\epsilon, \\forall t \\in N$, $\\epsilon$ is a small constant.\nWith the assumption of slow parameter update, we can prove that $|\\|\\xi_t - \\theta_t\\|| \\leq \\frac{\\epsilon}{\\beta}$.\n$\\xi_t - \\theta_t = (1 - \\beta)\\xi_{t-1} - (1-\\beta)\\theta_t$\n$= (1-\\beta)(\\xi_{t-1} - \\theta_{t-1}) - (1-\\beta)\\Delta\\theta_{t-1}$\n$= \\sum_{j=1}^t (1-\\beta)^j \\Delta \\theta_{t-j}.$ (11)\nFor the first two equations, we respectively use the definition of EMA parameter update in equation 1 and the definition of $\\Delta \\theta$. For the third equation, we iteratively employed the results from the previous two steps, along with the initial condition $\\xi_0 = \\theta_0$. With Assumption 1, we have\n$|\\| \\xi_t - \\theta_t\\|| \\leq \\sum_{j=1}^t (1-\\beta)^j \\epsilon \\leq \\frac{1-\\beta}{\\beta} \\epsilon \\leq \\frac{\\epsilon}{\\beta}.$ (12)\nFor the following results, we use the default setting in experiment $\\beta = 0.5$, i.e. $|\\| \\xi_t - \\theta_t\\|| \\leq \\epsilon$.\n(Interpretation of loss discrepancy) With Assumption 1, the loss discrepancy can be approximately expressed by the dot product between the data gradient and the \"EMA gradient\":\n$L(x, \\xi_t) - L(x, \\theta_t) = \\alpha \\nabla_{\\theta}L(x, \\theta_t)v^{EMA} + O(\\epsilon^2),$ (13)\nwhere $v^{EMA}$ denotes $\\sum_{j=1}^t(1 - \\beta)\\nabla_{\\theta}L(D_{t-j}, \\theta_{t-j})$, i.e. the weighted sum of the historical gradients, which we termed as \"EMA gradient\".\nFrom Lemma 1, since $|\\| \\xi_t - \\theta_t\\||$ is small, we can use Taylor expansion of the loss function at $\\theta_t$:\n$L(x, \\xi_t) - L(x, \\theta_t) = \\nabla_{\\theta}L(x, \\theta_t)(\\xi_t - \\theta_t) + O(|\\|\\xi_t - \\theta_t\\||^2)$\n$= \\nabla_{\\theta}L(x, \\theta_t) \\sum_{j=1}^t (1 - \\beta) L(D_{t-j}, \\theta_{t-j}) + O(|\\|\\epsilon\\||^2),$ (14)\nwhere we use equation 11 and the definition of online parameter update in equation 1.\n(Gradient projection interpretation of MolPeg) In the context of neglecting higher-order small quantities, we define $D^+ \\subseteq D$ and $\\hat{D}^+ \\subseteq D$ as samples that have positive dot products between the data gradient and the \"EMA gradient\", then\n$\\nabla_{\\theta}L(\\hat{D}^+, \\theta) = \\nabla_{\\theta}L({D}^+,\\theta) + a v^{EMA} + cv^{EMA}, a > 0, c \\in \\mathbb{R}.$ (15)\nSimilarly, we define $D^- \\subseteq D$ and $\\hat{D}^- \\subseteq D$ as samples that have negative dot products, then\n$\\nabla_{\\theta}L(\\hat{D}^-, \\theta) = \\nabla_{\\theta}L({D}^-,\\theta) + b v^{EMA} + d v^{EMA},b \\leq 0, d \\in \\mathbb{R}.$ (16)\nEquality holds if and only if the absolute loss discrepancy $|L(x,\\xi_t) - L(x, \\theta_t)| across $D^+$ and $D^-$ is uniform. This is a rare situation, and in such a case, our data selection strategy degenerates to random selection on $D^+$ and $D$."}, {"title": "Connections to Existing DP Methods", "content": "In the pretraining scenario, where the initialization is fixed, the GraNd score is defined as the norm of the gradient $|\\|\\nabla_{\\theta}L(x, \\theta_t)\\||$. With Assumption 1, we can deduce $|\\| \\xi_t - \\theta_t\\|| \\leq \\epsilon$ as shown in equation 12, then the data selected by MolPeg satisfies $\\delta \\leq |L(x, \\theta_t) - L(x,\\xi_t)| = |\\nabla_{\\theta}L(x, \\theta_t)(\\theta_t - \\xi_t) + O(\\epsilon^2)| \\leq \\epsilon |\\|\\nabla_{\\theta}L(x, \\theta_t)\\|| + O(\\epsilon^2)$. The data we select has a lower bound on the GraNd score $|\\|\\nabla_{\\theta}L(x, \\theta_t)\\|| \\geq O(\\frac{\\delta}{\\epsilon})$, making it more likely to be chosen by the GraNd score.\nOur strategy employs relative loss scales rather than absolute values, enabling a more flexible adaptation for transfer scenarios. For simple downstream samples for pretraining model, where L(x, xi) is small, both Infobatch and MolPeg eliminate samples with small online loss which are regarded as redundant for finetuning. However, for difficult samples for pretraining model, where L(x, \\xi_t) is large, our method diverges from Infobatch by preserving the crucial samples for transfer learning.\nIf we consider classification tasks and use accuracy loss, our method tends to select samples near the classification boundary. This can be related to the forgetting method, which aims to select samples that have been forgotten (i.e., initially classified correctly and then incorrectly) multiple times. For simplicity, let's explain this in the context of binary classification under Assumption 1. Further assume the class prediction probability f is l-Lipschitz continuous with respect to the parameters \u03b8, where f = (f(0), f(1)) and f(0) + f(1) = 1, we have $|\\|f(x,\\theta) - f(x,\\xi)\\|| \\leq l\\epsilon$. The loss function L(x, \u03b8) = | argmax_{i}\\{f(x, \u03b8)\\} - y|, y \u2208 {0, 1} is not continuous at the classification boundary where"}, {"title": "Pseudo-code of MolPeg", "content": "We provide the pseudo-code of MolPeg presented in Algorithm 1."}, {"title": "Discussions", "content": "Our data pruning strategy is specifically designed for molecular downstream tasks, but source-free data pruning is a task setting with broad applications in other fields as well. For example, in large language models (LLMs) and heavy-weight vision models, pretraining data is often difficult for users to obtain or even kept confidential. However, we have not validated our method in these more general scenarios. Therefore, verifying the effectiveness of MolPeg in natural language and vision tasks is one of our future research directions. Additionally, as the first work designed for the source-free data pruning setting, we have only made simple"}]}