{"title": "Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models", "authors": ["Kunat Pipatanakul", "Potsawee Manakul", "Natapong Nitarach", "Warit Sirichotedumrong", "Surapon Nonesung", "Teetouch Jaknamon", "Parinthapat Pengpun", "Pittawat Taveekitworachai", "Adisai Na-Thalang", "Sittipong Sripaisarnmongkol", "Krisanapong Jirayoot", "Kasima Tharnpipitchai"], "abstract": "This paper introduces Typhoon 2, a series of text and multimodal large language models optimized for the Thai language. The series includes models for text, vision, and audio. Typhoon2-Text builds on state-of-the-art open models, such as Llama 3 and Qwen2, and we perform continual pre-training on a mixture of English and Thai data. We employ various post-training techniques to enhance Thai language performance while preserving the base models' original capabilities. We release text models across a range of sizes, from 1 to 70 billion parameters, available in both base and instruction-tuned variants. Typhoon2-Vision improves Thai document understanding while retaining general visual capabilities, such as image captioning. Typhoon2-Audio introduces an end-to-end speech-to-speech model architecture capable of processing audio, speech, and text inputs and generating both text and speech outputs simultaneously.", "sections": [{"title": "1 Introduction", "content": "Foundation models are general models which can serve as the backbone in a wide range of AI applications involving language, vision, speech, and other modalities. There are a number of widely popular language model families, including open\u00b9 families such as Llama 3 (Grattafiori et al., 2024), Qwen2.5 (Yang et al., 2024a), Phi 3 (Abdin et al., 2024) and proprietary families, such as GPT-40, Claude 3, Gemini 1.5. However, although these models are multilingual and can work in a range of languages, they are developed as English-centric. Hence, the community has considered enhancing the performance of open foundation models on a small set of languages for their country or region.\nIn South East Asia (SEA), the efforts to enhance foundation language models for SEA languages include SeaLLM (Zhang et al., 2024), SEA-LION (Singapore, 2024), and Sailor (Dou et al., 2024). When it comes to the Thai language, there are model families such as OpenThaiGPT (Yuenyong et al., 2024), Typhoon (Pipatanakul et al., 2023), WangChan (Polpanumas et al., 2023), and Pathumma (NECTEC, 2024).\nTo continue our commitment in advancing Thai foundation models, this work introduces a new series of state-of-the-art Thai language and multimodal models, Typhoon2. Following our previous releases, these models are optimized for Thai and English, building on open-source models such as Llama 3 and Qwen2.5. The text models, Typhoon2-Text, are improved over Typhoon 1.5 in various aspects, including data filtering techniques for pre-training, complex instruction data development for improved post-training, long context, and function calling capabilities of the models. These text models are also now available in a range of sizes, consisting of 1B, 3B, 7B, 8B, and 70B parameters. We offer both pre-trained and instruction-tuned variants for each size. In addition, we introduce Typhoon2-Safety a safety text classifier designed to detect Thai-sensitive content and enhance the security of LM-integrated systems.\nFurthermore, the Typhoon2 series is multimodal. The vision model, Typhoon2-Vision, builds on the first Typhoon-Vision model by enhancing Thai document understanding capabilities, such as optical character recognition (OCR). The audio model, Typhoon2-Audio, extends the first Typhoon-Audio model, evolving into an end-to-end speech processing and generation model capable of understanding audio, speech, and text, while generating text and speech outputs in parallel. This report provides details and insights from our development. We make the weights of all models publicly available on Hugging Face Hub where the summary of our release is shown in Table 1."}, {"title": "2 Pre-training", "content": "This section of the report details the pre-training phase of Typhoon 2, which continues to build on the same motivation as its predecessor: to construct the highest-quality corpus that represents the Thai culture and language. Typhoon 2 builds upon the foundation laid by the previous iteration of Typhoon. The primary objective of this iteration is to develop a more diverse and high-quality dataset in Thai for pre-training. This goal is accomplished by implementing multiple data-gathering pipelines designed to target various domains and subsets within the Thai language."}, {"title": "2.1 Data Source & Typhoon1-Corpus", "content": "This section outlines the preparation process for Typhoon 1, which can be summarized in five steps. The first four steps involve preparing the base corpus, which will serve as a reference dataset used for filtering additional data in Section 2.2. The fifth step focuses on selecting the general Typhoon1-Corpus (Pipatanakul et al., 2023)."}, {"title": "2.1.1 Base Corpus Preparation", "content": "Scaling the Data: We initiate the process by increasing the number of Common Crawl (CommonCrawl) packages compared to the previous iteration, where we process a total of 40 packs of the CommonCrawl data.\nText Extraction: Although CommonCrawl provides a pre-extracted WET subset, several studies suggest that WET files exhibit lower quality compared to WARC files extracted using external tools such as Trafilatura (Li et al., 2024a; Penedo et al., 2023). In our study, we begin with a total of 40 packs of Thai CommonCrawl data with a cut-off date of September 2023 and perform HTML extraction from scratch using Trafilatura\u00b2. This process results in a significantly larger dataset, comprising approximately 3 TB of Thai text, or approximately 200 billion Llama3 tokens.\nStrict Deduplication: To ensure data quality and minimize redundancy, we implement a fuzzy deduplication pipeline using MinHash and LSH algorithms.\nHeuristic Filtering: To filter out pages containing excessive search engine optimization (SEO) content, very short documents, and low-quality texts, we evaluate each line using signals such as the ratio of numbers to text and the punctuation density. At the document level, document filtering is done based on other metrics, including newline ratios and overall document length. After deduplication and heuristic filtering, we have approximately 44B tokens of Thai text."}, {"title": "2.1.2 Typhoon 1 General Corpus", "content": "To ensure that the pre-trained data accurately represents the Thai language and culture, we employ a filtering process on the base corpus, involving a human-in-the-loop at the domain level in the same manner as Typhoon (Pipatanakul et al., 2023). This approach ensures that the content is both relevant and appropriate, preserving its authenticity. Consequently, we obtain 5 billion high-quality Thai tokens, which serve as the foundation for training the original Typhoon and Typhoon 1.5 models."}, {"title": "2.2 Gathering Diverse and High-Quality Thai Documents", "content": "To enhance our dataset, we observed a growing trend in pre-training large language models (LLMs) that emphasizes sourcing high-quality data from general corpora. Following this approach, we develop multiple pipelines to collect documents from a range of diverse domains, focusing on high-quality content absent from our existing general corpus."}, {"title": "2.2.1 Culturally Relevant Thai Text", "content": "A tailored methodology for content collection is developed to address cultural nuances in the text data, drawing upon principles of the fine-web educational approach (Penedo et al., 2024). Specifically, we use an LLM to annotate 50,000 randomly selected entries from the base corpus. Each entry is assessed for its cultural relevancy and educational value in understanding Thai culture, using a scale ranging from 1 to 5. Next, we fine-tune the classification head of BGE-M3 (Chen et al., 2024a) using the labeled dataset obtained from the previous process. We employ a smaller model to predict the Thai cultural value of samples within our base corpus. Subsequently, we filter only those with a predicted value of 4 or higher and use these instances to train Typhoon 2."}, {"title": "2.2.2 High-Quality Text Selection", "content": "Inspired by DCLM (Li et al., 2024a) and the importance of high-quality text filtering in Thai, we develop a fastText (Joulin et al., 2017) classifier tailored for the Thai language. Given the low-resource nature of Thai, we conduct multiple iterations of training to refine our classifier. First, we compile an instruction-following dataset in Thai, drawing from WangchanThai-Instruct (Vistec, 2024) and samples from our Typhoon Instruct dataset (Pipatanakul et al., 2023) as positive examples, while incorporating random examples, including toxic content and junk websites, as negative examples. Next, we apply the classifier developed in the initial iteration to classify approximately 200,000 samples from a base corpus. After manual inspection and filtering, we train another iteration of the fastText classifier. This iterative approach results in a high-quality Thai text classifier capable of filtering content with a predicted quality ratio exceeding 0.5 as a high-quality sample."}, {"title": "2.2.3 Synthetic Textbook", "content": "To address the lack of textbooks in our general corpus, we employ a method inspired by the Phi and Cosmopedia (Ben Allal et al., 2024; Gunasekar et al., 2023) approach. We use LLM to create an augmented dataset of 5,000 text samples with styles that emulate textbooks, blog posts, and academic materials. This augmented dataset is then used to fine-tune typhoon-1.5-8b-instruct on a text augmentation task involving raw text and style information. The fine-tuned model is used to augment 20% of our general corpus to resemble textbook-style content."}, {"title": "2.2.4 High-Educational Content", "content": "To gather high educational content in Thai, we use a similar approach as in collected culture-related text in Thai and fine-web edu (Penedo et al., 2024). Specifically, using an LLM to annotate 50K samples and fine-tune the BGE-M3 classification head. We filter only with a predicted value of 3 or higher due to the low level of educational content."}, {"title": "2.2.5 Other High-Quality Sources", "content": "We also incorporate highly educational sources, such as Thai Wikipedia, into our final pre-training dataset."}, {"title": "2.3 Data Mixture", "content": "To ensure good final model performance, it is imperative to determine the proportion of various data sources within the pre-training dataset. Since our approach involves continual pre-training (CPT), maintaining an awareness of the original pre-trained distribution is crucial.\nWe conduct a series of experiments to optimize the final model's performance. In the first stage, we independently evaluate each new data source to ensure that individual subsets contribute positively. We examine this by performing CPT on the 1.5B Qwen2.5 (Yang et al., 2024a) model using a similar recipe to Blakeney et al. (2024) and verify that each of the data sources improves one of the metrics or scores based on M3Exam (Zhang et al., 2023b) and/or ThaiExam (Pipatanakul et al., 2023).\nNext, we explore simple data mixture strategies. Our English dataset ratio, which is 50%, is inspired by previous studies, including Typhoon (Pipatanakul et al., 2023) and SambaLingo (Csaki et al., 2023) to mitigate catastrophic forgetting. For each large corpus, we repeat the data for 1 time, for smaller corpus (such as education documents and Wikipedia) we repeat the data up to three times. Additionally, we experimented with Doremi (Xie et al., 2023) but did not observe a significant improvement. Ultimately, our best approach is based on a simple data mixture strategy where our English subset is sourced from Shen et al. (2024); Weber et al. (2024), and each of the datasets is repeated 1 time, except for the education content (2 times) and Wikipedia (3 times). The Thai subset of our pretraining data mixture is illustrated in Figure 1."}, {"title": "2.4 Training", "content": "Based on our experiments, although current foundation models were already trained on some Thai texts, we do not extend the Thai tokenizer as done in Typhoon (Pipatanakul et al., 2023). This decision is because recent findings showed that adding more tokens to the tokenizer (which requires training their corresponding embeddings) can degrade overall performance (Dou et al., 2024; Zhao et al., 2024) despite yielding high generation efficiency. We use this configuration during the model pre-training.\nFor all models, the AdamW optimizer is used in conjunction with a cosine learning rate scheduler. Gradient clipping is applied with a threshold value of 1.0. Our training is conducted using a context length of 8192. The DeepSpeed ZeRO optimization framework at Stage 2 without offloading is employed, as it provides the highest throughput in our experimental setup. All models are pre-trained on a single node comprising eight H100 GPUs. The learning rate is optimized individually for each model. We select the Llama 3 series (Grattafiori et al., 2024) and Qwen 2.5 series (Yang et al., 2024a) as the base models due to their high performance on global leaderboards at the time of conducting pre-training. We perform full fine-tuning for models with 8B parameters or fewer and we apply LORA (Hu et al., 2022) with a rank of 32 for models with 70B parameters. Due to our resource constraints, we train on sub-sampling instead of the full dataset."}, {"title": "Base Model", "content": "Initially, we examined the performance of several 7\u20138B base models, as this is a standard size in academic scaling due to our resource constraints. Ultimately, we identified two families of base models that demonstrated significant potential for practical use:\n1.  Llama 3 series: A state-of-the-art open-source model developed by Meta. It is primarily trained for English performance. The instruction-tuned version also demonstrates excellent performance and is competitive with proprietary models.\n2.  Qwen2.5 series: A state-of-the-art open-source model developed by Alibaba. Its performance on knowledge-driven leaderboards is exceptional, surpassing many open-source and proprietary LLMs. It is optimized for English and Chinese as its primary languages.\nAs our Typhoon adaptation recipe is model-agnostic, we select both models as the base for the 7-8B category. Subsequently, we apply this recipe to other model sizes in the Llama 3 family since we observed a lower hallucination rate and better code-switching performance in our investigation."}, {"title": "2.5 Evaluation", "content": "We evaluate the models using the ThaiExam and M3Exam datasets. While this evaluation method has been widely used for pre-trained language models, the scores obtained using these datasets are highly above the average level of a typical Thai person\u00b3. This can be attributed to contamination and saturation due to overfitting (Fourrier et al., 2024). Nevertheless, we utilize this signal to measure whether the model has acquired knowledge of the Thai language and context, as well as a development signal for improvement."}, {"title": "2.6 Results and Findings", "content": "The evaluation results are shown in Table 2. We found that following insights from our experiments and evaluations.\n\u2022 Each filtering subset has its own performance gain: We observe a performance gain in the \"Science\" score on \u201cHigh-Quality Text Selection,\" while we achieve a \"Social\" score on \"Culture Related Text in Thai\" and a \"Thai\" score on the \"General subset\" during the first-stage data mixture setup on M3Exam.\n\u2022 CPT-performance depends on the based model: While CPT improves the model's performance, the based model's performance has a more significant effect on the overall score.\n\u2022 Data Mixture Effects in CPT Setup: Qwen2.5 and Llama 3.1 respond differently to data mixtures, requiring distinct configurations to achieve comparable performance improvements. Exploring this phenomenon remains an area for future work.\n\u2022 Avoid Optimizing Solely for Knowledge: While knowledge is a crucial aspect of LLMs, it is only one of many dimensions. Other objectives, such as instruction-following, task specificity, reasoning capabilities, and ease of parameter tuning, are equally vital for maximizing the overall utility of LLMs."}, {"title": "3 Post-training", "content": "In our post-training process, the goal is to improve Typhoon's usability. We focus on instruction-following abilities, such as multi-turn response capabilities, system prompt following, and reasoning. We also focus on enhancing Typhoon 2's capabilities on tasks such as function calling and long context for both Thai and English. This section details our approaches and findings in the post-training stage of Typhoon 2."}, {"title": "3.1 General Supervised Fine Tuning (SFT)", "content": "To make Typhoon 2 follow human instruction, we employ SFT as the principal strategy for aligning its outputs with human requirements. However, achieving effective human alignment is inherently multidimensional, as language models must accommodate a range of human values, preferences, and constraints. Recognizing this complexity, we design a comprehensive dataset encompassing multiple facets and specialized skills, enabling Typhoon 2 to meet diverse human needs better."}, {"title": "3.1.1 Data", "content": "We develop a dataset and combine it with the open-source dataset to ensure Typhoon 2's instruction-following performance based on these categories.\n\u2022 English Instruction-Dataset: We combine several public datasets based on multiple iterations of our Typhoon development. In this version, we incorporate SystemChat\u2074, Capybara5, OpenChat (Wang et al., 2024a) and a subset of the Tulu 3 (Lambert et al., 2024) dataset to retain the base model's English language comprehension. These datasets are selected primarily to align with human feedback patterns and various use cases in leveraging LLMs. For example, SystemChat supports system role-following features; Capybara facilitates multi-turn conversations, and OpenChat and Tulu 3 provide a rich diversity of instructions.\n\u2022 Thai General Instruction-Dataset: We utilize the Typhoon self-instruct instruction dataset for training to align it to Thai. Unlike the original Typhoon (Pipatanakul et al., 2023) development, we do not use any translated English instruction data as it introduces hallucination.\n\u2022 TyphoonIF Dataset: A new dataset is constructed based on AutoIF (Dong et al., 2024), available in both Thai and English. Seed constraints for the English and Thai ver-sions are manually crafted to represent typical prompts that humans might use when interacting with LLMs in specific tasks. Additionally, random queries are taken from airesearch/WangchanThaiInstruct (Vistec, 2024) and Suraponn/thai_instruction_sft6, along with randomly sampled queries from the English dataset as previously described in this subsection. Rejected samples, identified through an instruction evaluation function, are filtered and added to the final dataset. The final instruction set comprises 150,000 question/instruction and response pairs. These pairs are randomly designated as either system or user turns to enhance generalization. To further improve cross-lingual transfer ca-pabilities, instruction turns are randomly translated between Thai and English encouraging the models to align both Thai and English spaces to be similar to each other.\n\u2022 Typhoon Personality Dataset In addition, we curate an introductory prompt for Typhoon to incorporate its personality into the model."}, {"title": "3.1.2 Experimental Setup", "content": "Training: We perform full fine-tuning for SFT post-training, using the AdamW optimizer with a learning rate of 2e-5 for all experiments on the 7-8B model. We use a batch size of 16, and packing for a 32K context length during SFT. The SFT training is conducted for around 1,000 steps, resulting in approximately 700M tokens processed in total.\nEvaluation: We evaluate the performance of general SFT using three datasets, focusing on assessing general instruction-following performance and the usability of LLMs in Thai and English. The three datasets are as follows:\n\u2022 IFEval: We employ IFEval (Zhou et al., 2023), a method designed to evaluate instruction-following capabilities using a set of verifiable instructions. These in-structions are assessed against predefined rules implemented through test cases. The evaluation metric for IFEval is accuracy, which measures how well LLMs adhere to user-provided instructions. In addition to the standard IFEval (English version), we introduce IFEval-TH, a Thai version of IFEval. The original English instructions are translated into Thai, followed by a manual verification and correction process to ensure accuracy and content consistency. In this case, we evaluate the average of all four metrics from the original work.\n\u2022 Code-Switching: We observed that English monolingual and English-Chinese bilingual LLMs exhibit a high tendency to produce code-switching responses when prompted to respond in Thai. To quantify this behavior, we propose a simple code-switching evaluation designed to assess the model's propensity to output non-Thai characters when following Thai instructions. The evaluation in-volves scenarios where the input instructions are sampled from the test subset of airesearch/WangchanThai Instruct (Vistec, 2024). The assessment is conducted across two temperature settings, T = 0.7 and T = 1.0, to measure the model's consistency in producing Thai-majority responses.\n\u2022 MT-Bench: We utilize MT-Bench, a variant of the LLM-as-a-judge evaluation frame-work, which employs a strong LLM to assess responses to open-ended questions based on correctness, fluency, and adherence to instructions. For the ThaiLLM leaderboard (10X et al., 2024), we use the Thai version of MT-Bench developed by VISTEC (VISTEC, 2024), while the English version follows the LMSYS implementa-tion (Zheng et al., 2023).\nFor IFEval and code-switching (CS) evaluation, the metric is accuracy, defined as:\n1.  The response does not contain characters from other languages.\n2.  Thai characters constitute the majority of the response content.\nBaseline: We compare the Typhoon 2 model based on Llama 8B fine-tuning on the newly developed dataset with Typhoon 1.5 (Llama-based) and Llama 3.1 8B Instruct."}, {"title": "3.1.3 Results and Findings", "content": "We present our results and findings from the selection process of our SFT dataset in Table 4.\nWe found the following insights:\nImpact of English Data on Thai Performance: Our preliminary experiments indicate that the inclusion of a high-quality English dataset contributes to improving the performance of Thai language models. This suggests that cross-lingual benefits can be obtained when using high-quality data from a related or dominant language.\nThai-English Ratio: To enable the model to generate fluent responses in Thai, we initially experimented with a Thai-English ratio of 1:9. Despite the low proportion of Thai data, the model demonstrated the ability to respond adequately in Thai. However, increasing the ratio of Thai data leads to a performance improvement. After conducting multiple trials, we empirically found that the optimal Thai-to-English ratio is 3:7.\nImportance of Data Quality: The presence of low-quality data from a single source can lead to a performance degradation of up to 20% across the entire system.\nQuality over Quantity: We examine combining multiple large datasets, but the performance levels are only comparable to or even worse than those achieved with a smaller curated dataset tailored to specific functions and use cases."}, {"title": "3.2 Domain-Specific SFT", "content": "The model's performance on general domain instruction-following tasks is satisfactory across all datasets. However, a noticeable drop in domain-specific abilities, particularly in coding and mathematics, is observed. To address this issue, we examine incorporating math and coding instruction data in the experiment."}, {"title": "3.2.1 Data", "content": "Math & Code Dataset\nTo improve math and coding performance, we examine domain-specific datasets and select three based on their competitive results as follows,\n\u2022 Dart-math (Tong et al., 2024b): An augmented math dataset verified using a rejection sampling method, focusing on complex questions. The answers are sampled from DeepSeekMath (Shao et al., 2024).\n\u2022 ScaleQuest-Math (Ding et al., 2024): A technique to scale diverse math instruction using a small seed math problem. The responses are sampled through a combination of DeepSeekMath-RL (Shao et al., 2024) and Qwen2.5-Math (Yang et al., 2024b).\n\u2022 OpenCoder-Instruct (Huang et al., 2024): Stage 2 instruction dataset from the OpenCoder project contributes to the strong performance of fully open code LLMs. The dataset is created by leverage multiple methods to scale code instruction dataset such as Magicoder (Wei et al., 2024) and Wizardcoder (Luo et al., 2024).\nWe also translate a subset of each dataset into Thai using an early version of our Typhoon2 model. Invalid translations are filtered out by validating the responses against the final"}, {"title": "3.2.2 Experimental Setup", "content": "We evaluate the Typhoon 2 models' performance under various training scenarios, focus-ing on the impact of code and math domain-specific subsets. Three key evaluations are conducted as follows,\n\u2022 Training Data Impact: Models trained on the General-only subset are compared to those using General + Code & Math to assess the benefits of domain-specific data.\n\u2022 Math Performance: Typhoon 2, fine-tuned on General + Code & Math, is compared to Llama 3.1 8B Instruct and Qwen2.5 7B Instruct to evaluate math task effectiveness.\n\u2022 Code Performance: Typhoon 2's code performance, also fine-tuned on General + Code & Math, is compared against the same base models to assess the coding ability.\nTraining: The training process and hyperparameters are identical to those described in Sec-tion 3.1.2 for General-SFT training.\nEvaluation Data: We evaluate hard domain-specific tasks for LLMs, such as coding and math, which are also related to reasoning performance. We also incorporate evaluations from the general domain to ensure the model does not overfit specific domain tasks.\n\u2022 GSM8K: The Grade School Math 8K (Cobbe et al., 2021) consists of diverse grade school math word problems which are basic mathematical problems that require multiple-step reasoning.\n\u2022 MATH: The Mathematics Aptitude Test of Heuristics (Hendrycks et al., 2021) is a hard math dataset consisting of problems from mathematics competitions.\n\u2022 HumanEval: HumanEval (Chen et al., 2021b) consists of programming problems with a function signature, docstring, body, and several unit tests.\n\u2022 MBPP: MBPP (Austin et al., 2021) is a set of Python programming problems de-signed to be solvable by entry-level programmers, covering programming funda-mentals, standard library functionality, and related topics.\nEvaluation Implementation: The mathematical evaluation is zero-shot based on the dartmath implementation (Tong et al., 2024b). Code evaluation is zero-shot based on the evalplus implementation (Liu et al., 2023b) \u2013 we report the base subset result. The Thai subset is created by directly translating the original dataset into Thai using GPT-40, with automatic verification performed using the LLM-as-judge technique."}, {"title": "3.2.3 Results and Findings", "content": "The results and Findings"}, {"title": "3.3 Long Context", "content": "The capability to handle long contexts is essential for LLMs to process and understand complex and lengthy texts. Numerous real-world applications, such as summarizing academic papers and analyzing legal documents, demand models capable of handling inputs that go beyond standard context length limitations. However, training LLMs with extended context sizes is computationally demanding, often requiring significant training time and GPU resources. In practice, this limitation typically restricts the maximum context length to 32,768 tokens on four A100 (80GB) GPUs with Deepspeed ZeRO Stage 3. To enhance the long-context capabilities of Typhoon 2, we extend its context length from 8,192 tokens in Typhoon 1.5 to 32,768 tokens and generalize support for context lengths up to 128K tokens. We evaluate this capability through experiments on SFT tasks following CPT with an 8,192-token context size."}, {"title": "3.3.1 Data", "content": "We construct the dataset based on Section 3.1 combined with three primary sources to ensure both effective exploitation of long contexts and robust multilingual support. These three primary sources are:\n\u2022 LongAlign (Bai et al., 2024): This dataset is derived from the LongAlign framework, which is specifically designed to enhance LLMs for long-context understanding and instruction-following. While the full framework encompasses data creation, training strategies, and evaluation for long-context alignment, we selectively incorporate the dataset component for our work.\n\u2022 Anti-haystack7: This dataset is designed for enhancing the ability of LLMs to locate short and precise facts within long, noisy documents, emulating a \"needle in a haystack\" challenge.\n\u2022 IApp-Wiki-QA Dataset (Viriyayudhakorn & Polpanumas, 2021): To enhance Thai long-context capabilities, we utilize the Thai Wikipedia Question Answering dataset, consisting of 1,500 unique context records. We extend and reformat the data using the following processes:\nIrrelevant Content Addition: Random irrelevant contents are sampled from the ThaiSum dataset (Chumpolsathien, 2020), a news summarization dataset in the Thai language, to introduce noise and increase the context length.\nQuestion and Answer Integration: Questions and answers from the IApp-Wiki-QA dataset are randomly positioned within the extended context.\nToken Length Requirement: Each row in the dataset is structured to ensure that the Thai token count exceeds 30,000 tokens."}, {"title": "3.3.2 Experimental Setup", "content": "Evaluation: We evaluate the long context abilities using the Needle-in-a-Haystack (NIAH) method (Kamradt, 2023). This framework assesses a model's ability to retrieve specific \"needle sentences\" hidden within random segments of lengthy documents.\nThe evaluation involves completing sentences such as:\n\"The best thing to do in San Francisco is to eat a sandwich and sit in Dolores Park on a sunny day.\"\nwhere the corresponding input prompt is:\n\u201cWhat is the best thing to do in San Francisco?\u201d\nWe evaluate the long context abilities in both English and Thai. To ensure linguistic con-sistency, the Thai dataset was created using our in-house machine translation model to translate the English dataset.\nThis work considers two model architectures:\n\u2022 Llama3.1-based Model (Grattafiori et al., 2024): The model underwent continued pre-training with a context length of 8,192 tokens, using the original RoPE (Su et al., 2023) base frequency hyperparameter of 500,000. This was followed by supervised fine-tuning to extend the context length to 32,768 tokens.\n\u2022 Qwen2.5-based Model (Yang et al., 2024a): A similar training pipeline was applied, starting with pre-training using Qwen2.5's original RoPE (Su et al., 2023) config-uration, which employs a base frequency of 1,000,000. Additionally, this model leverages the YARN mechanism (Peng et al., 2024), optimized for long-context scenarios."}, {"title": "3.3.3 Results and Findings", "content": "Findings from Typhoon2-Llama3.1-8B,70B-Instruct Evaluation\nAs illustrated in Figures 2 and 3, both Typhoon2-Llama-3.1-8B,70B-Instruct models support a maximum context length of approximately 90,000 tokens. This is a reduction compared to the original Llama 3.1 model, which supports up to 128,000 tokens. We hypothesize that this limitation is due to two key factors:\n1.  The original Llama 3.1 model was trained incrementally across multiple stages, progressively extending its context length to 128,000 tokens.\n2.  Our CPT approach was restricted to a context length of 8,192 tokens, potentially limiting the model's ability to generalize to longer contexts, despite adjustments to the ROPE scaling (Su et al., 2023).\nAddressing these limitations could pave the way for future enhancements to the Llama-based Typhoon 2 models."}, {"title": "3.4 Function Calling", "content": "Function calling is essential for enhancing the capabilities of LLMs to tackle complex, real-world tasks. By enabling LLMs to interact with external tools and third-party services, function calling facilitates impactful applications such as workflow automation and financial analysis. To equip the Typhoon models with this capability, we utilize existing function-calling datasets described in the following subsection."}, {"title": "3.4.1 Data", "content": "We construct the general instruction dataset from Section 3.1 combined with three sources to ensure diversity, quality, and multilingual support:\n\u2022 APIGen (Liu et al., 2024d): This synthetic dataset emphasizes diversity and quality through a multi-stage hierarchical verification process. A Flan-style approach is used to generate outputs in various formats (e.g., JSON, YAML, XML). An in-house translation system is employed to translate data from English to Thai, ensuring robust bilingual representation.\n\u2022 ToolACE (Liu et al., 2024a): This dataset builds on a prior study, focusing on enhancing the function-calling capabilities of LLMs. Synthetic data is translated into Thai using the same in-house machine translation system, enabling bilingual support and increasing dataset complexity and diversity."}, {"title": "3.4.2 Experimental Setup", "content": "Evaluation: The trained models are evaluated using the Berkeley Function-Calling Bench-mark (BFCL) (Patil et al.", "components": "n\u2022 Abstract Syntax Tree (AST) Evaluation: This component measures the syntactic cor-rectness of generated function calls by comparing them to predefined specifications, focusing on function names, parameters, and data types"}]}