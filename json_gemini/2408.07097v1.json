{"title": "Attention Please: What Transformer Models Really Learn for Process Prediction", "authors": ["Martin K\u00e4ppell", "Lars Ackermann", "Stefan Jablonski", "Simon H\u00e4rtl"], "abstract": "Predictive process monitoring aims to support the execution of a process during runtime with various predictions about the further evolution of a process instance. In the last years a plethora of deep learning architectures have been established as state-of-the-art for different prediction targets, among others the transformer architecture. The transformer architecture is equipped with a powerful attention mechanism, assigning attention scores to each input part that allows to prioritize most relevant information leading to more accurate and contextual output. However, deep learning models largely represent a black box, i.e., their reasoning or decision-making process cannot be understood in detail. This paper examines whether the attention scores of a transformer based next-activity prediction model can serve as an explanation for its decision-making. We find that attention scores in next-activity prediction models can serve as explainers and exploit this fact in two proposed graph-based explanation approaches. The gained insights could inspire future work on the improvement of predictive business process models as well as enabling a neural network based mining of process models from event logs.", "sections": [{"title": "1 Introduction", "content": "Predictive Business Process Monitoring (PBPM) provides runtime support for the execution of a process with various predictions about the further evolution of a process instance, based on predictive models created from event logs [4]. Early knowledge of the likely course of a process instance offers significant advantages: upcoming steps can be prepared, and potential problems can be identified and mitigated early, e.g., in resource and time planning [4]. In the last years a plethora of deep learning architectures have been established as state-of-the-art for different prediction targets (e.g., next activity, outcome, remaining time), among others Convolutional Neural Networks [14], Long Short Term Memory Neural Networks (LSTM) [2,6], or more recently transformer architectures [1].\nHowever, deep learning models largely represent a black box, i.e., their reasoning or decision-making process is hidden [13]. Consequently, users often do not trust them, which hinders their application [3]. To address this issue, the discipline of explainable artificial intelligence (XAI) tries to find explanations for"}, {"title": "2 Preliminaries", "content": "In the following A denotes the set of viable activities in a business process. The\nexecution of a (business) process can be recorded in form of a trace \u03c3 = (e1, ..., en),"}, {"title": "2.1 Basic Terminology", "content": ""}, {"title": "2.2 Attention Mechanism", "content": "In deep learning the attention mechanism allows models to dynamically focus on\ndifferent parts of a input sequence for generating output. The fundamental idea\nbehind attention is to learn to assign different scores to the parts of the input,\nindicating how much focus the model should pay them when performing a task.\nHence, it allows to prioritize most relevant information leading to more accurate\nand contextual output. The approach investigated in this paper is based on the so\ncalled self-attention mechanism [21] utilized within the transformer architecture\nfor next activity prediction proposed by [1]. In the following, we describe parts of\nthe architecture relevant for the paper at hand, i.e., the input, the self-attention\nmechanism, and the output of the transformer. For a description of the remaining\nparts of the architecture, we refer to [21] and [1], respectively.\nAccording to [1] the transformer receives as input a list of activities (i.e., a\nprefix). This input is then embedded into a high-dimensional space of dimension\ndk by converting each element of the list into a so called embedding vector. This\nrepresentation leads to a continuous representation of the input and captures\npositional, semantic, and syntactic properties of the corresponding elements,\nand serves as input for the self-attention. It is important to note that the\nembeddings are automatically learned during training. In the attention mechanism\nan embedding vector xi is used in three different ways: as query, key, and value\nvector. We get them by multiplying xi with matrices WQ, WK, and Wv that\nare learned during the training process, and pack them for more efficiency into\nmatrices Q, V, and K (each column represents one vector) [21]. Next, we use\nQ, K, and V to calculate an attention score reflecting the importance of each\nelement (query) under consideration in relation to the others. First, each element\nof the input sequence is scored against the currently considered element by taking\nthe matrix product QKT. The values of the resulting matrix are divided by \\(\\sqrt{d_k}\\) before applying a row-wise softmax operation to normalize the values to avoid"}, {"title": "2.3 Interpretation of Attention Scores", "content": "Fig. 1 depicts attention score matrices for two heads, illustrating how elements\nof an input sequence (here the activities of the events) relate to each other\nthrough the model's attention mechanism. The interpretation of an attention\nscore matrix is identically for all heads. In the attention score matrix, rows and\ncolumns correspond to elements of the input sequence, with rows representing\nthese elements as queries and columns as keys. By examining a row, we can\nassess the importance the transformer model assigns to each other element in the\ninput sequence from the perspective of the row's element. In contrast, analyzing a\ncolumn reveals the degree of attention all other elements in the input sequence pay\nto the element associated with that column. Within the matrix higher attention\nscores appears lighter, while lower scores are shown darker, indicating the strength\nof the relationship between the elements in the respective row and column."}, {"title": "3 Related Work", "content": "In the following we briefly discuss XAI approaches in the PBPM domain with\nfocus on next activity prediction. They can roughly be classified into model-specific\nor model-agnostic approaches. While model-specific approaches are tailored to\nthe inner working of a model, model-agnostic approaches can be applied to any\nmachine learning model, regardless of its internals. Although model-agnostic\napproaches are highly versatile and universally applicable, they provide less\ndetailed insights in the decision processes of the particular model. The survey [20]\nidentifies 20 XAI approaches in the PBPM domain and categorizes them along\ndifferent dimensions. It shows, that the vast majority of approaches is model-\nagnostic and limited to local explanations. Hence, there is a lack of global\nmodel-specific explainer. When combining criteria of model-specific and next-\nactivity explainers, only three papers remain [8,18,22], that can be considered\nas directly related work. None of these used the transformer as their specific\nmodel, however [18] uses an attention mechanism within an LSTM, whereas this\nmechanism differs from those used by the transformer from [1]. The results in\nthis approach show accumulated attention scores for each position in a prefix.\nAlthough the authors refer to the work of [17] which states that attention\ncannot be interpreted \"as is\", they do not further consider tests proposed in [17]\nto verify and critically reflect whether the observed attention scores are truly\ninterpretable. Hence, it is still an open question in the PBPM domain, whether\nattention scores can be trusted as basis for explanation [10, 23]. We address\nthis question in Section 4. The Explainable Next Activity Prediction (XNAP)\napproach proposed in [22] provides explanations for LSTMs for next activity\nprediction using layer-wise relevance propagation resulting in assigning each\nactivity in the input prefix a relevance score. However, just as in [18] they do\nnot conduct any quantitative evaluation and limit themselves to a qualitative\nanalysis of the results. The approach proposed in [8] also relies on LSTM networks\nbut differs from [18] and [22] by outputting a directed and weighted graph as\nexplanation instead of relevance score for elements in the input prefix. In the"}, {"title": "4 Pre-Study: The Relevance of Attention Scores", "content": "Before using attention scores as pivotal component in an explanation approach\nfor next activity prediction, we verify their reliability in this context, i.e., that\nthey indeed highlight activities decisive for a prediction. We investigate whether\nattention scores in general affect the prediction (Exp. 1) and shed light on the\nextent to which particular attention scores contribute to a prediction (Exp. 2)."}, {"title": "5 Explanation Approaches", "content": "In this section we propose two global explainers that receive as input the trained\ntransformer prediction model M and a set of prefixes L to be explained. Both\nexplainer construct a comprehensible directed graph describing the control flow\nof the process that acts as an indicator to which extent the PBPM model\nunderstands the control-flow of the process. Since attention scores reliably predict\nthe next activity (cf. Section 4), we can base our explanation approach on them."}, {"title": "5.1 Backward Explainer", "content": "The first explainer derives explanations for individual traces, subsequently inte-\ngrating them into an overarching explanation for L via a directed graph G. This"}, {"title": "5.2 Attention Exploration Explainer", "content": "The Backward Explainer has one major drawback: It directly determines the\nedges of the graph when analysing a prefix. Once inserted an edge is retained,\nexcept if it is a shortcut that can be removed in the pruning step. Thus, existing\nedges are not updated based on information obtained from further prefixes. We\naddress this issue, by no longer relying on local graphs. Instead, we calculate a\nnew relevance score for each activity across all prefixes presented to the algorithm."}, {"title": "6 Evaluation", "content": "In this section, we examine the performance of our explainers using the event logs\nfrom the pre-study. Our approach is implemented as prototypical Python frame-\nwork that contains both explainers and all methods required for reproducibility2."}, {"title": "7 Concluding Remarks", "content": "This paper presents two transformer specific XAI approaches based on atten-\ntion scores for next-activity prediction. We show first, that attention scores\nindeed form are a reliable basis for explanations and presented two approaches\nexplaining predictions through graphs. These graphs show how attention scores\nfor activities relate to predictions. The evaluation using various quantitative\nmetrics demonstrates that the Attention Exploration Explainer outperforms the\nsimpler Backward Explainer. This provides compelling evidence that explainers\nbased on attention scores hold significant potential. This offers a wide range of\nuseful opportunities for the future development and application of transformer-\nbased predictions of the next activity. First, it provides transparency in the\ndecision-making process of the model increasing the trust of process participants\nin the predictions. Second, the global explanatory capabilities of the presented\napproaches allow to assess the process understanding in general and to avoid\ncontradictory explanations across different samples as in many local explainers.\nThird, the insights provided by the explainers, particularly in instances of erro-\nneous predictions, offer a valuable starting point for model improvement, which\nmay be achieved through enhanced training or the incorporation of additional"}]}