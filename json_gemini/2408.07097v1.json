{"title": "Attention Please: What Transformer Models Really Learn for Process Prediction", "authors": ["Martin K\u00e4ppell", "Lars Ackermann", "Stefan Jablonski", "Simon H\u00e4rtl"], "abstract": "Predictive process monitoring aims to support the execution of a process during runtime with various predictions about the further evolution of a process instance. In the last years a plethora of deep learning architectures have been established as state-of-the-art for different prediction targets, among others the transformer architecture. The transformer architecture is equipped with a powerful attention mechanism, assigning attention scores to each input part that allows to prioritize most relevant information leading to more accurate and contextual output. However, deep learning models largely represent a black box, i.e., their reasoning or decision-making process cannot be understood in detail. This paper examines whether the attention scores of a transformer based next-activity prediction model can serve as an explanation for its decision-making. We find that attention scores in next-activity prediction models can serve as explainers and exploit this fact in two proposed graph-based explanation approaches. The gained insights could inspire future work on the improvement of predictive business process models as well as enabling a neural network based mining of process models from event logs.", "sections": [{"title": "1 Introduction", "content": "Predictive Business Process Monitoring (PBPM) provides runtime support for the execution of a process with various predictions about the further evolution of a process instance, based on predictive models created from event logs [4]. Early knowledge of the likely course of a process instance offers significant advantages: upcoming steps can be prepared, and potential problems can be identified and mitigated early, e.g., in resource and time planning [4]. In the last years a plethora of deep learning architectures have been established as state-of-the-art for different prediction targets (e.g., next activity, outcome, remaining time), among others Convolutional Neural Networks [14], Long Short Term Memory Neural Networks (LSTM) [2,6], or more recently transformer architectures [1].\nHowever, deep learning models largely represent a black box, i.e., their reasoning or decision-making process is hidden [13]. Consequently, users often do not trust them, which hinders their application [3]. To address this issue, the discipline of explainable artificial intelligence (XAI) tries to find explanations for"}, {"title": "2 Preliminaries", "content": "In the following A denotes the set of viable activities in a business process. The execution of a (business) process can be recorded in form of a trace \\(\\sigma = (e_1, ..., e_n)\\), i.e., a temporally ordered sequence of events associated with the same process instance. Each event represents the execution of an activity in the process and is characterized by event attributes, such as the name of the corresponding activity. Without loss of generality (since the considered process transformer only process activities), we denote traces in the following as sequences of their activity names. We denote with \\(\\Pi_A\\) a function that returns for a given event its executed activity and with \\(|\\sigma|\\) the length of the trace \\(\\sigma\\). The set of all traces pertaining to the same (business) process is referred as (process) event log. For PBPM approaches, prefixes of a trace are used to represent running process instances:\nDefinition 1 (Prefix). Let \\(\\sigma = (e_1,...,e_n)\\) denote a trace and let \\(r \\in \\{1, ..., n - 1\\}\\). The prefix of length r is a function hd which returns the first r elements of a trace: \\(hd(\\sigma,r) = (e_1, ..., e_r)\\).\nThis prefix, is then used as input to a function \\(\\Omega\\) predicting the next activity, given by \\(\\Omega(hd(\\sigma,r)) = \\pi_a(e_{r+1})\\). A PBPM approach, in this paper a transformer model, aims to learn the function \\(\\Omega\\) using a given event log."}, {"title": "2.2 Attention Mechanism", "content": "In deep learning the attention mechanism allows models to dynamically focus on different parts of a input sequence for generating output. The fundamental idea behind attention is to learn to assign different scores to the parts of the input, indicating how much focus the model should pay them when performing a task. Hence, it allows to prioritize most relevant information leading to more accurate and contextual output. The approach investigated in this paper is based on the so called self-attention mechanism [21] utilized within the transformer architecture for next activity prediction proposed by [1]. In the following, we describe parts of the architecture relevant for the paper at hand, i.e., the input, the self-attention mechanism, and the output of the transformer. For a description of the remaining parts of the architecture, we refer to [21] and [1], respectively.\nAccording to [1] the transformer receives as input a list of activities (i.e., a prefix). This input is then embedded into a high-dimensional space of dimension \\(d_k\\) by converting each element of the list into a so called embedding vector. This representation leads to a continuous representation of the input and captures positional, semantic, and syntactic properties of the corresponding elements, and serves as input for the self-attention. It is important to note that the embeddings are automatically learned during training. In the attention mechanism an embedding vector \\(x_i\\) is used in three different ways: as query, key, and value vector. We get them by multiplying \\(x_i\\) with matrices \\(W_Q, W_K\\), and \\(W_v\\) that are learned during the training process, and pack them for more efficiency into matrices Q, V, and K (each column represents one vector) [21]. Next, we use Q, K, and V to calculate an attention score reflecting the importance of each element (query) under consideration in relation to the others. First, each element of the input sequence is scored against the currently considered element by taking the matrix product \\(QK^T\\). The values of the resulting matrix are divided by \\(\\sqrt{d_k}\\) before applying a row-wise softmax operation to normalize the values to avoid"}, {"title": "2.3 Interpretation of Attention Scores", "content": "Fig. 1 depicts attention score matrices for two heads, illustrating how elements of an input sequence (here the activities of the events) relate to each other through the model's attention mechanism. The interpretation of an attention score matrix is identically for all heads. In the attention score matrix, rows and columns correspond to elements of the input sequence, with rows representing these elements as queries and columns as keys. By examining a row, we can assess the importance the transformer model assigns to each other element in the input sequence from the perspective of the row's element. In contrast, analyzing a column reveals the degree of attention all other elements in the input sequence pay to the element associated with that column. Within the matrix higher attention scores appears lighter, while lower scores are shown darker, indicating the strength of the relationship between the elements in the respective row and column."}, {"title": "3 Related Work", "content": "In the following we briefly discuss XAI approaches in the PBPM domain with focus on next activity prediction. They can roughly be classified into model-specific or model-agnostic approaches. While model-specific approaches are tailored to the inner working of a model, model-agnostic approaches can be applied to any machine learning model, regardless of its internals. Although model-agnostic approaches are highly versatile and universally applicable, they provide less detailed insights in the decision processes of the particular model. The survey [20] identifies 20 XAI approaches in the PBPM domain and categorizes them along different dimensions. It shows, that the vast majority of approaches is model-agnostic and limited to local explanations. Hence, there is a lack of global model-specific explainer. When combining criteria of model-specific and next-activity explainers, only three papers remain [8,18,22], that can be considered as directly related work. None of these used the transformer as their specific model, however [18] uses an attention mechanism within an LSTM, whereas this mechanism differs from those used by the transformer from [1]. The results in this approach show accumulated attention scores for each position in a prefix. Although the authors refer to the work of [17] which states that attention cannot be interpreted \"as is\", they do not further consider tests proposed in [17] to verify and critically reflect whether the observed attention scores are truly interpretable. Hence, it is still an open question in the PBPM domain, whether attention scores can be trusted as basis for explanation [10, 23]. We address this question in Section 4. The Explainable Next Activity Prediction (XNAP) approach proposed in [22] provides explanations for LSTMs for next activity prediction using layer-wise relevance propagation resulting in assigning each activity in the input prefix a relevance score. However, just as in [18] they do not conduct any quantitative evaluation and limit themselves to a qualitative analysis of the results. The approach proposed in [8] also relies on LSTM networks but differs from [18] and [22] by outputting a directed and weighted graph as explanation instead of relevance score for elements in the input prefix. In the"}, {"title": "4 Pre-Study: The Relevance of Attention Scores", "content": "Before using attention scores as pivotal component in an explanation approach for next activity prediction, we verify their reliability in this context, i.e., that they indeed highlight activities decisive for a prediction. We investigate whether attention scores in general affect the prediction (Exp. 1) and shed light on the extent to which particular attention scores contribute to a prediction (Exp. 2)."}, {"title": "Experiment 1 Attention Mechanism Parameter Manipulation", "content": "We first investigate whether attention is necessary in the first place, following the experimental setup in [23]. We thus train a baseline model \\(M_b\\) with randomly initialized attention parameters (i.e., matrices \\(W_Q, W_K, W_v\\), and \\(W_O\\)) and a modified one \\(M_m\\) identical to the training setup of \\(M_b\\) except that we freeze the attention parameters to uniform weights during training. We repeat this procedure five times with differently initialized baselines and modified models to minimize the influence of chance. To ensure to get indeed different initializations, we use random seeds. Then each manipulated transformer \\(M_m\\) is compared against the corresponding baseline model \\(M_b\\). To do this, we send all test samples through all models and obtain for each sample and model the corresponding attention distribution as well as the prediction vector. Then for each sample we compare attention distribution and prediction vector obtained from \\(M_b\\) with those obtained from each modified model using JSD and TVD, respectively. The computed JSD and TVD values are afterwards averaged across all samples. The intention behind this setup is, that the higher the divergence in attention scores (larger JSD) and the smaller the divergence in output (lower TVD), the less they affect the prediction [10,23]. Therefore, if the same prediction can be made with completely different attention scores, they can hardly serve as explanation.\nFor interpretation, we plot the mean JSD against the mean TVD achieved for each model (see Fig. 2). Models positioned rightward and lower in the plots indicate less reliable attention scores. Hence, most of the event logs (BPIC12-W, Helpdesk, Sepsis) show good results. In the case of a right tendency (BPIC12), the TVD values are clearly greater than zero. Consequently, the attention scores can be trusted using the transformers trained on these event logs."}, {"title": "Experiment 2 Attention Score Masking", "content": "In a second experiment, we investigate how particular attention values affect the prediction (see Fig. 3)."}, {"title": "5 Explanation Approaches", "content": "In this section we propose two global explainers that receive as input the trained transformer prediction model M and a set of prefixes L to be explained. Both explainer construct a comprehensible directed graph describing the control flow of the process that acts as an indicator to which extent the PBPM model understands the control-flow of the process. Since attention scores reliably predict the next activity (cf. Section 4), we can base our explanation approach on them."}, {"title": "5.1 Backward Explainer", "content": "The first explainer derives explanations for individual traces, subsequently inte- grating them into an overarching explanation for L via a directed graph G. This process is explained in the remainder of this section and involves creating a local graph \\(G_\\sigma\\) for each prefix \\(\\sigma \\in L\\), which is then merged directly into G.\nCreating local graph: A crucial step for creating a local graph \\(G_\\sigma\\) for a prefix \\(\\sigma\\) is to use the head attention scores to identify the relevant activities \\(A_r\\) for the prediction \\(p_o\\). To do this we proceed as follows: We consider an activity as relevant for prediction \\(p_o\\), if its aggregated attention score over multiple modifications of \\(\\sigma\\) is large enough. To obtain different modifications of \\(\\sigma\\), we randomly apply modification operations. We pass each modification \\(\\sigma^m\\) to M to obtain prediction vectors \\(p_o^m\\) and the heads' attention score matrices \\(M_h\\). Then the cosine distance between \\(p_o^m\\) and \\(p_o\\) is computed and compared with a an a-priori defined threshold \\(d_{sim}\\), to check whether the prediction for \\(\\sigma^m\\) is still close to the prediction of \\(\\sigma\\). If \\(\\sigma^m\\) satisfies this condition, the attention scores for each element j in \\(\\sigma^m\\) are aggregated across the different heads:\n\\[S_j = \\sum_{m=1}^{|\\sigma^m|} \\left( \\sum_{n=1}^{|L_h|} M_{nj} \\right)\\]\nIn this formula first the heads' attention score matrices \\(M_h^o\\) are component- wise summed, and afterwards the jth column is summed. Hence, we get for each \\(\\sigma^m\\) an aggregated attention score vector \\(\\eta_o^m = (S_1,......, S_{|o^m|})\\) containing a total attention score for each event. This procedure is illustrated in Fig. 4. Next this total values are further aggregated per activity to a vector \\(\\psi\\) by summing up the total attention scores of each event belonging to a certain activity. Then \\(\\psi\\), is normalized so that its components are within range [0, 1]. We denote with \\(\\psi_\\sigma(a)\\) the total attention score for activity a. Finally, we filter the activities by a given threshold \\(d_{attr}\\) by keeping only activities with \\(\\psi_\\sigma(a) > d_{attr}\\) to get only those activities that have a particularly high total attention score. This filtering is needed as irrelevant elements most have a small but not zero attention score and thus would be considered otherwise. The identified relevant activities \\(A_r\\) and the most likely next activities \\(P_r\\) obtained from prediction vector \\(p_o\\) (i.e., activities a with \\(p_o(a) > \\delta_{pred})\\). We do not use a fixed number"}, {"title": "5.2 Attention Exploration Explainer", "content": "The Backward Explainer has one major drawback: It directly determines the edges of the graph when analysing a prefix. Once inserted an edge is retained, except if it is a shortcut that can be removed in the pruning step. Thus, existing edges are not updated based on information obtained from further prefixes. We address this issue, by no longer relying on local graphs. Instead, we calculate a new relevance score for each activity across all prefixes presented to the algorithm."}, {"title": "Determining relevance", "content": "To prevent relevance scores from increasing with each additional prefix and to reduce them when the attention score of an activity is consistently low, we introduce negative attention scores. In the transformer architecture, attention scores are inherently non-negative, with lower values indicating minimal impact. We encode the latter with a negative value. A positive value supports edge insertion, while a negative value is against it.\nTherefore we first determine for each prefix \\(\\sigma \\in L\\) a so called score matrix \\(K_\\sigma\\), with a column and a row for each activity \\(a \\in A\\). First, we extract for \\(\\sigma\\) the relevant activities \\(A_r\\) in the same way as in the Backward Explainer above. Additionally, we examine the positions of the relevant activities in \\(\\sigma\\) and store them in a set \\(I_\\sigma\\). Next, we use all subsets of \\(I_\\sigma\\) within two masking scenarios to quantify the individual and collective impact of activities on the prediction. For each subset \\(r \\in I_\\sigma\\), we consider two scenarios. In the \"masking out most\" scenario we mask the unimportant activities, i.e., all positions that are not contained in r. In contrast the \"masking out a few\" scenario masks the important activities, i.e., all positions in r. To obtain values for our score matrix \\(K_\\sigma\\), we now compare \\(\\sigma\\) with all of its masked versions (denoted as \\(\\sigma^m\\)). Therefore, we send \\(\\sigma\\) and \\(\\sigma^m\\) through M to obtain prediction vectors \\(p_o\\) and \\(p_o^m\\) respectively. As well as the most likely next activities \\(P_r\\) for prefix \\(\\sigma\\). Then, we compute as in the Backward Explainer the per activity aggregated attention score vectors \\(\\psi_o\\) and \\(\\psi_{o^m}\\) indicating how much focus the model gives to an activity in the normal and masked trace, respectively. The computation of the scores is described in detail in Alg. 1. In this calculation, a score is calculated for each activity in the masked prefix regardless of whether it is masked or not. For masked activities, the score is the product of the prediction score and attention scores. If the prediction for a masked activity matches its prediction in the normal trace, the score is negated, indicating reduced relevance due to the activity's negligible impact on the prediction. For non-masked activities, scores are derived differently: if an activity's prediction value is unchanged with masking, the score is the product of its attention score and normal trace prediction. If the prediction changes, the score is based on the product of the absolute differences in both attention scores and predictions, reflecting the influence of the activity in the masked trace. Eventually the calculated score is added to the corresponding cell (row for the predicted activity and column the masked activity) in the score matrix.\nFrom Alg. 1 we receive a score matrix for each masked version of \\(\\sigma\\), which are summed to obtain two scenario-specific score matrices, \\(K^{few}\\) and \\(K^{most}\\). The two matrices comprehensively assess the relevance of each activity in \\(\\sigma\\), considering the impact of its presence or absence in different combinations.\nBuilding the explanation graph: Finally we must aggregate the score matrices of both scenarios we have obtained for each prefix:"}, {"title": "6 Evaluation", "content": "In this section, we examine the performance of our explainers using the event logs from the pre-study. Our approach is implemented as prototypical Python frame- work that contains both explainers and all methods required for reproducibility2."}, {"title": "7 Concluding Remarks", "content": "This paper presents two transformer specific XAI approaches based on atten- tion scores for next-activity prediction. We show first, that attention scores indeed form are a reliable basis for explanations and presented two approaches explaining predictions through graphs. These graphs show how attention scores for activities relate to predictions. The evaluation using various quantitative metrics demonstrates that the Attention Exploration Explainer outperforms the simpler Backward Explainer. This provides compelling evidence that explainers based on attention scores hold significant potential. This offers a wide range of useful opportunities for the future development and application of transformer- based predictions of the next activity. First, it provides transparency in the decision-making process of the model increasing the trust of process participants in the predictions. Second, the global explanatory capabilities of the presented approaches allow to assess the process understanding in general and to avoid contradictory explanations across different samples as in many local explainers. Third, the insights provided by the explainers, particularly in instances of erro- neous predictions, offer a valuable starting point for model improvement, which may be achieved through enhanced training or the incorporation of additional\ntraining data. Moreover, an analysis of the graph structure can provide indications of potential weaknesses that may lead to the generation of erroneous predictions. Therefore, we answer our research question (Section 1) as follows: the trans- former's attention scores are effective for checking and visualizing its process understanding. However, since the trained prediction models only incorporate the activity event attribute, the process cannot be fully learned inherently. Limitations: Real-life event logs often suffer from quality issues such as noise, incompleteness, and inconsistencies, affecting the prediction and consequently the explanation quality in a negative way. In our pre-study we aggregate the attention scores of all heads to take a holistic perspective on the attention scores. Thus, it is unexplored what particular heads learn and how they potentially differ. In future work this should be investigated more in detail. Currently, the proposed explainers employ various thresholds to filter out less relevant relationships affecting the edges in the explanation graph. Thus, optimizing these thresholds holds vast potential to increase the explanation capabilities. An interesting starting point would be to adjust thresholds flexibly depending on the prefix considered. Future Work: In addition to the points already mentioned for future work, the used masking and modification operations within the relevance determination should be improved by using more advanced operations like in the LORELEY approach. Also the generation of process models out of prediction models should be consequently fostered as a novel way of process discovery."}]}