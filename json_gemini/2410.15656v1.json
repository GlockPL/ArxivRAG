{"title": "LightFusionRec: Lightweight Transformers-Based Cross-Domain Recommendation Model", "authors": ["Vansh Kharidia", "Dhruvi Paprunia", "Prashasti Kanikar"], "abstract": "This paper presents LightFusionRec, a novel lightweight cross-domain recommendation system that integrates DistilBERT for textual feature extraction and FastText for genre embedding. Important issues in recommendation systems, such as data sparsity, computational efficiency, and cold start issues, are addressed in methodology. LightFusionRec uses a small amount of information to produce precise and contextually relevant recommendations for many media formats by fusing genre vector embedding with natural language processing algorithms. Tests conducted on extensive movie and book datasets show notable enhancements in suggestion quality when compared to conventional methods. Because of its lightweight design, the model can be used for a variety of purposes and allows for on-device inference. LightFusionRec is a noteworthy development in cross-domain recommendation systems, providing accurate and scalable recommendations to improve user experience on digital content platforms.", "sections": [{"title": "I. INTRODUCTION", "content": "The need for strong recommendation systems has increased due to the spread of digital material across many media channels. Conventional recommender systems typically suffer from the cold start issue, data sparsity, or merely a lack of context because they are compartmentalized inside a particular domain (reads, watches, etc.). New things and users are always adding to this problem, thus finding creative solutions that leverage information from other fields is essential.\nTo combat these challenges, LightFusionRec is introduced, which is a new cross-domain recommendation system to unify state-of-the-art NLP methods with generic text embedding. The method uses DistilBERT to encode semantically rich text features from content descriptions and the FastText model for genre feature extraction. The hybrid model connects different content domains (e.g., movies and books), but is a lightweight solution that runs in an efficient manner, making it specifically suitable for on-device inference."}, {"title": "A. Challenges Addressed", "content": "Current SOTA cross-domain recommendation models, especially when it comes to content recommendation, face some key challenges that limit their scope for widespread adoption and usefulness. The model addresses several of those key challenges:\n1) Contextual understanding:\na) Challenge: Existing systems often fail to capture the nuanced context of content descriptions and the semantic relationships between genres. [18]\nb) Solution: By aligning DistilBERT-generated textual embedding with FastText-based genre vectors, LightFusionRec captures both the content description's meaning and the semantic relationships between genres.\n2) Getting cross-domain training data :\na) Challenge: Traditional recommendation models require a lot of cross-domain training data for the same set of users, which is extremely difficult to procure. [19]\nb) Solution: The model uses contextual understanding to recommend items that do not take any detailed user preferences or paired cross-domain data for a set of users while training.\n3) Data Sparsity and Cold-Start Problem:\na) Challenge: Difficulty in providing relevant recommendations to new users with a small interaction history. [1]\nb) Solution: Traditional recommender systems often struggle with recommending content for new users or items due to a lack of interaction history. LightFusionRec efficiently addresses the cold-start problem by focussing on these 2 parameters instead of user history:\n1) Genre of Content: This is represented by a genre vector (GV), which helps the model understand the types of"}, {"title": "II. RELATED WORKS", "content": "Cross-domain recommendation systems have emerged as a major research area that overcomes the limitations of classic single-domain recommendation systems. In this study, a literature review is presented on the evolution of recommendation algorithms. Recent research has primarily focused on improving the performance of NLP techniques used in recommendation models, learning more comprehensive user- and item-embedding representations, implementing a co-training model to integrate multi-domain and multitask information, and utilizing deep Neural Network architectures in recommendation models.\n\nTraditional recommender systems are often divided into two categories: content-based filtering and collaborative filtering. The collaborative filtering approach predicts user preferences based on interactions between users and items, whereas content-based filtering makes recommendations based on item attributes. Resnick et al.'s (1994) [10] work on collaborative filtering utilizing social information filtering was one of the early works that impacted the present-day recommendation algorithms, although these systems suffer limitations like scalability issues and the cold start problem [10].\n\nThe introduction of deep learning has resulted in a new revolution in recommender systems. He et al. (2017) [8] introduced Neural Collaborative Filtering (NCF), which is the first model to include neural networks for learning user-item interactions. Variational encoders (VAE) have also been used in collaborative filtering; see Liang et al. (2018) [10], which may capture more complex context information for user items.\n\nCross-domain recommendation systems use data from several domains to boost the accuracy of their recommendations. Hu et al. (2009) [12] conducted initial studies on the application of matrix factorization algorithms to transfer knowledge across disciplines. More recent approaches, like Man et al. (2017) [11], use deep learning models to collect latent properties across domains. Despite these advances, the challenge of successfully merging heterogeneous data from several fields remains.\n\nThe capability of recommender systems to understand and make use of textual material has been greatly enhanced by NLP approaches. Devlin et al. (2018) [13]proved the effectiveness of transformers in extracting contextual information from text with BERT (Bidirectional Encoder Representations of Transformers). Newer models that offer efficient solutions appropriate for real-time applications include DistilBERT, a simplified version of BERT that was presented by Sanh et al. (2020) [14]. To enhance content-based recommendations, these algorithms have been applied to extract rich textual information from item descriptions.\n\nThe semantic connections among various genres are captured by genre embedding, which offer a concise comprehension of item properties. Word2Vec, first presented by Mikolov et al. (2013) [15], established the foundation for learning word representations. FastText is a Word2Vec enhancement created by Bojanowski et al. (2017) [17] that takes sub-word information into account to further increase embedding quality. Recommendation algorithms have made better use of these embedding to comprehend genre semantics. Reddy et al. (2019) [4] developed a movie recommendation technique based on genre correlations.\n\nCross-domain recommendation research is leading the way with hybrid models that combine genre embedding and natural language processing approaches. By merging FastText-based genre vectors with text embedding produced by DistilBERT, the method expands upon this framework. By capturing contextual information from descriptions and semantic links between genres, this fusion overcomes the drawbacks of conventional models. Research has shown that hybrid techniques can effectively improve the precision of recommendations across different domains, as evidenced by the work of Wang et al. (2015) [16]."}, {"title": "G. Recent Progress in Single-Domain and Cross-Domain Recommendations", "content": "Advancements in recent times have expanded the limits of cross-domain recommendation systems. Saraswat et al. (2018) [6], for instance, employed keyword and genre similarity to make cross-domain suggestions. Recently, further work has been done to address the cross-domain recommendation problem of cold start (Li et al. (2018) [1], Cao et al. (2022) [9]). Transformers4Rec (Moreira et al., 2021) [2] is one of the most well-known of the newly invented transformer structures specifically designed for recommendations; nevertheless, it has not yet been modified for cross-domain recommendations.\n\n(2023) [7] provide Graph Transformers for Recommendations, a computationally costly model that requires vast amounts of data to execute, to demonstrate SOTA results in recommendation systems based on the transformers architecture. Using sentiment analysis for cross-domain recommendations, several cross-domain recommendation techniques, such Wang et al. (2020) [3], have taken use of current developments in NLP. The majority of large-scale recommendation systems in use today mostly rely on user history and data to provide recommendations. An increasing amount of research is being conducted on privacy-preserving techniques for recommendation systems as users grow more worried about their data and privacy. Federated learning is one method being used by researchers to make advantage of user data in recommendation models while protecting user privacy (Neumann et al., 2023) [5]."}, {"title": "III. PROPOSED METHOD", "content": "The architecture of LightFusionRec that was used to follow the methodology is shown in Fig 1.\n1) Inference Setup: Software: Python 3.7 or later\nDependencies: PyTorch 1.8.0+, Transformers 4.0.0+, FastText 0.9.2+, Scikit-Learn 0.24.0+, NumPy 1.20.0+, Pandas 1.2.0+\nSystem Requirements: CPU: Intel Core i3 or equivalent, Memory: 4 GB minimum (8 GB recommended), Storage: 512 MB free for model files and temporary data\nThe methodology flow is depicted in Fig. 2, which demonstrates how data was collected and prepared, then used Disitil-Bert for text feature extraction and FastText for genre vector preparation. Afterward, these steps were combined to train the reccomender model and precompute the book features. Assessment and Ablation Analysis Verify the Process's Accuracy."}, {"title": "2) Data Collection and Preprocessing:", "content": "The data sets used in the study comprise of movie and book descriptions, genres, and titles. The title is stored for output convenience, while the genres and description are used in model training. It was a conscious decision to use as minimal data as possible per record to ensure that the model is generalizable and performs well with minimal signifiers. The Amazon Reviews dataset is used for this paper. [21].\n1) Data Cleaning: Data cleaning was concluded by removing duplicate entries and removing entries with missing genre/description. The book dataset contained some entries where the title and/or description were not in English (some not even in a Latin script); Those entries were kept to ensure that the model does not break when it receives non-Latin input. After data cleaning, the dataset had 45k books and 65k movies.\n2) Genre Vector Preparation To capture the semantic relationships between genres, a FastText model was trained on the concatenated genres from both datasets. FastText, as proposed by Bojanowski et al. (2017) [17], extends Word2Vec by considering subword information, making it particularly suitable for learning embedding of short texts like genres.\nLet G be the set of all genres. For a genre $g \\in G$, its FastText embedding is denoted as:\n$v_g = FastText(g) \\in R^{50}$                                                                    (1)\nUse of FastText compared to other embedding methods like Word2Vec or Glove as FastText takes into account sub-word information, while GloVe and Word2Vec treat words as atomic units. FastText's approach leads to richer context representation, which helps the model distinguish between similar-sounding genres (e.g., \"romance\" and \"romantic-comedy\") by understanding sub-word differences. Word2Vec and GloVe lack this level of contextual distinction, which makes it harder for them to distinguish between similar sounding genres like 'romance' and 'romantic-comedy'."}, {"title": "3) Feature Fusion Feature Fusion", "content": "is for alignment of textual features and genre vectors which affects the recommendation quality. It is done by the following ways:\n4) Improved Content Understanding: Aligning textual features (e.g., keywords, descriptions) with genre vectors (categorical genre information) helps capture both semantic meaning and categorical preferences. This alignment allows for deeper insights into the nature of the content, which improves matching with user preferences.\n5) Contextual Recommendations: Genre vectors have context because of their textual features. A work categorized as \"science fiction,\" for instance, can also feature major textual themes like \"AI\" or \"space travel.\" Aligning these vectors provides a more sophisticated understanding of content that is relevant to the user's context, which helps to improve genre-based suggestions."}, {"title": "6) Enhanced Personalization", "content": "Users may have a preference for certain genres, nevertheless within those genres, they may have different interests. (e.g. within the \"romance\" category, certain users may like narratives with \"adventure\" or \"historical fiction\" themes). The alignment captures both general and specialized preferences, aiding in the provision of more individualized recommendations."}, {"title": "3) Text Feature Extraction using DistilBERT:", "content": "A lightweight variant of BERT called DistilBERT was utilized to extract rich textual features from content descriptions. Sanh et al. (2019) [14] introduced DistilBERT, which is 40% smaller and 60% faster than BERT while maintaining 97% of BERT performance.\nFor a content description d, the DistilBERT embedding is\n$e_d = DistilBERT(d) \\in R^{768}$                                                                 (2)\nTokenization and Encoding: The DistilBERT tokenizer was used to tokenize and encode the descriptions in input IDs and attention masks, with a maximum sequence length of 128 tokens. The inputs which were encoded are then fed into the DistilBERT model to get the last hidden states, focusing on the [CLS] token for its summary representation of the sequence."}, {"title": "4) Cross Domain Feature Fusion", "content": "To integrate textual and genre information, a cross-domain recommender model was designed that combines the features from DistilBERT and the FastText genre vectors. The model architecture includes several key components:\n1) Genre Projection The 50-dimensional genre vectors are projected to the 768-dimensional space to match the DistilBERT output. the genre vector were projected to match the dimensionality of the text embedding:\n$e_d = DistilBERT(d) \\in R^{768}$                                                              (3)\nwhere $W_g \\in R^{768x50}$ and $b_g \\in R^{768}$ are learnable parameters.\n2) Feature Concatenation and Fusion The textual and projected genre features are concatenated and passed through a fusion layer The fused feature f is computed as:\n$f = ReLU(W_f[e_d; P_g] + b_f)$                                                     (4)\nWhere $W_f \\in R^{768x1536}$ and $b_f \\in R^{768}$ are learnable parameters, and $[e_d;p_g]$ denotes the concatenation of $e_d$ and $p_g$."}, {"title": "5) Training the Recommender Model:", "content": "The training process involves optimizing the model to minimize the loss of cosine embedding between the final features of movies and books.\n1) Loss Function Cosine Embedding Loss was used in the model, which measures the similarity between the feature vectors. Cosine embedding loss is used:\n$L =\\begin{cases}\n1 - sim(m,b) & if \\ y=1\\\\\nmax(0,sim(m,b) - margin) & if \\ y=-1\\\\\n\\end{cases}$                                        (5)"}, {"title": "6) Pre-computing Book Features:", "content": "To expedite the recommendation process, book features are pre-computed using the trained model. This is essential to speed up inference, as by pre-computing the books, the model only needs to compute the sample movies that the user provides during inference. This limited inference activity can be performed on devices which do not possess a lot of compute power.\nBatch Processing: The books were processed in batches of 32 and their characteristics were extracted and stored for quick retrieval during the recommendation phase.\n$BF = Model (Book \\ IDs, AM,GV)$                                                                                    (6)"}, {"title": "7) Generating Recommendations:", "content": "The recommendation process involves calculating the similarities between the combined movie features and the precomputed book features.\n1) Movie Feature Extraction: For each input movie, features were extracted using the trained model and combined them to form a unified representation.\nCombined Movie Features $= \\frac{1}{n} \\sum_{i=1}^{n} MF_i$                                                   (7)\nwhere MF denotes the i-th Movie Feature.\n2) Similarity Calculation: The cosine similarity was calculated between the combined movie features and pre-calculated book features. In addition, the cosine similarities between genre and TF-IDF were also calculated and combined these metrics to generate final scores. For a movie m and a book b, their similarity is computed as:\nsim(m, b) = cosine(fm, fb)                                                                  (8)\n3) Top-K Recommendations: The top-K books with the highest combined scores were selected as recommendations."}, {"title": "B. Pseudo-Code", "content": "The below pseudocode details the steps to train the Light-FusionRec model including the optimization techniques that we've used like cosine annealing scheduler, AdamW optimizer with gradient clipping, etc. to help with replication of the model in different frameworks."}, {"title": "C. Evaluation", "content": "The model was evaluated using Root Mean Square Error (RMSE) and Mean Absolute Error (MAE), which are both effective metrics for assessing the accuracy of predictions. These metrics give the average size of errors in a set of predictions, allowing for a clear understanding of model performance across various thresholds. MAE and RMSE are calculated at the 20%, 50%, and 80% thresholds to ensure a comprehensive evaluation of the performance of the model.\n1) Evaluation Method: In the Amazon Reviews dataset, the list of users were extracted who had rated both movies and books. Then the books and movies were removed that the user had rated below 4 (out of 5), only keeping the books and movies that the users liked. The movies that each user liked were used to recommend books and based on those recommendations and the true values (the books that the user rated \\geq 4), The model was evaluated. The performance metrics that was used to measure are MAE, RMSE (Top 20%, Top 50%, and Top 80%) was shown in Table 1."}, {"title": "2) Ablation Study:", "content": "Ablation studies were performed on the model, The model was evaluated using only the DistilBERT model, only using genre similarity and only using TF-IDF. The model significantly outperformed each of these models. The result is shown in Table 2 and visualised in Figure 3 which shows that the MAE and RMSE values of LightFusionRec is the least, indicating a good result."}, {"title": "D. Interpretation of Results", "content": "The superior performance of LightFusionRec", "factors": "n"}]}