{"title": "Long-Range Biometric Identification in Real World Scenarios: A Comprehensive Evaluation Framework Based on Missions", "authors": ["Deniz Aykac", "Joel Brogan", "Nell Barber", "Ryan Shivers", "Bob Zhang", "Dallas Sacca", "Ryan Tipton", "Gavin Jager", "Austin Garret", "Matthew Love", "Jim Goddard", "David Cornett III", "David S. Bolme"], "abstract": "The considerable body of data available for evaluating biometric recognition systems in Research and Development (R&D) environments has contributed to the increasingly common problem of target performance mismatch. Biometric algorithms are frequently tested against data that may not reflect the real world applications they target. From a Testing and Evaluation (T&E) standpoint, this domain mismatch causes difficulty assessing when improvements in State-of-the-Art (SOTA) research actually translate to improved applied outcomes. This problem can be addressed with thoughtful preparation of data and experimental methods to reflect specific use-cases and scenarios.\nTo that end, this paper evaluates research solutions for identifying individuals at ranges and altitudes, which could support various application areas such as counterterrorism, protection of critical infrastructure facilities, military force protection, and border security. We address challenges including image quality issues and reliance on face recognition as the sole biometric modality. By fusing face and body features, we propose developing robust biometric systems for effective long-range identification from both the ground and steep pitch angles. Preliminary results show promising progress in whole-body recognition. This paper presents these early findings and discusses potential future directions for advancing long-range biometric identification systems based on mission-driven metrics.", "sections": [{"title": "1. Introduction", "content": "The increasing complexity of modern security scenarios necessitates the development of advanced biometric identification systems that can operate effectively over a wide range of distances, altitudes, and environmental conditions.\nThe Intelligence Community (IC) requires such technology for various critical applications, including counterterrorism, protection of vital infrastructure facilities, military force protection, and border security. However, effective implementations of current biometric systems are hindered by several major challenges.\nFirstly, image quality issues arising from factors such as motion blur, atmospheric turbulence, and resolution limitations significantly degrade system performance. Secondly, most algorithms have been developed based on low-pitch angle views of people, which may not be effective in scenarios characterized by significant pitch angles, such as those encountered when using UAVs or cameras mounted on buildings. Lastly, the over-reliance on face recognition"}, {"title": "2. Related Work", "content": "The limitations of accurate human identification at stand-off distances has been of interest of both the IC and research community for years. Earlier works such as [29] acknowledge that a major limiting factor is the lack of available datasets on which to train and evaluate new biometric models. Authors in [16] and [20] demonstrate the need for systems in low-resolution and unconstrained collection environments. Research such as [18], [25], [19], and [27] explore the impact that the number of pixels on the face as well as the quality of the face video itself have on facial recognition systems, which has spurred entities such as IARPA and DARPA to invest in datasets that can help improve biometric performance at long ranges and lower qualities [9, 8]\nThe utility for facial recognition systems that can perform accurately at long distances is also addressed in [4] and [21]. In recent years, the prevalence of high-quality, lower cost UAVs has lead to their widespread usage for various applications. Those systems with mid-to-high quality imaging sensors lend themselves especially to use cases involving human detection, identification and tracking. Research in [2], [3], [23], [26], and [22] investigate pairing facial detection and recognition systems with these types of UAVs. Research as seen in [10] even acknowledges the limitations of existing biometrics systems to address the challenges of uncontrolled lighting, distance and pitches demonstrate the need for new algorithms to overcome these challenges.\nThe current related body of research coupled with the growing needs of the IC demonstrate the need for the advances detailed in this paper."}, {"title": "3. Data Preparation", "content": "This section lays out the three components that comprise our methods to evaluate state-of-the-art solutions for identifying individuals at ranges and altitudes. The Data Collection section details our endeavor to produce a high-quality, statistically relevant dataset to provide both training and testing data for biometric recognition systems. The Data Curation and Partioning section provides insights into how the data is structured and organized to evaluate the various mission categories. Finally, the Common API section summarizes the custom API used across all solutions so that a fair and non-biased evaluation can be performed."}, {"title": "3.1. Data Collection", "content": "It is necessary to build a unique biometric image and video dataset to evaluate algorithm performance. This dataset consists of planned biometric collection events held in different parts of the United States during different seasons, ultimately resulting in diversity of subject demographics, terrain, and weather conditions. The dataset includes images and video of subjects in a variety of controlled and uncontrolled situations using a variety of sensors.\nSubjects participated in the experiments only for one day and were required to wear different clothing sets. In the field, for example, the probes had clothing set 1 and gallery had clothing set 2 for the same subject. This ensures that algorithms are performing recognition based on persistent biometric signals from the face, body, and patterns of movement and not simply recognizing their clothing.\nThe indoor phase consists of face and whole-body images captured from multiple pitch angles up to 50\u00b0 and the full rotation of yaw angles at 45\u00b0 increments. This phase also includes video captured by COTS sensors placed in a semicircle around the collection area to record subjects walking and using their cell phones from multiple angles."}, {"title": "3.2. Data Curation and Partitioning", "content": "In this section, we present the steps to curate the data from all the sensor measurements and to formalize the probe and gallery compositions for evaluation.\nStep 1: During data collection, researchers use a custom desktop application to record timestamps for when each subject begins and completes a recorded activity. In the first step of post-processing, individual images and video segments are associated with a particular subject based on these timestamps. Longer videos containing footage of multiple subjects are cut along these timestamps so that the resulting dataset is comprised of videos of one subject performing one activity in one clothing set. This is made possible via precise time synchronization of all sensors. The length of video segments varies depending on the activity being performed and, in the case of some activities, the subject's mobility and natural walking pace.\nStep 2: Additional data is recorded alongside videos during the data collection which describe weather conditions, atmospheric conditions, subject demographics, sensor positioning, and sensor hardware. This information is compiled such that each video segment can be associated with the corresponding metadata. Weather and atmospheric metadata that is tracked includes: temperature, wind chill, heat index, relative humidity, wind speed, wind direction, barometric pressure, and solar loading. This data, along with turbulent fluctuation recordings measured by a scintillometer, are recorded every minute during the data collection. Video segments are associated with the weather and atmospheric data aligned with the minute of its starting timestamp. Detailed sensor information such as minimum / maximum focal length, serial number, model number, manufacturer name, and sensor location are also included in the metadata describing each video segment.\nStep 3: Once the data has been organized as described in Steps 1 and 2, it is partitioned into training and test sets. This is done in a manner designed to create consistency of demographic distributions between the two sets. Every subject is assigned to either the training set or the test set, and the data is organized into its final directory structure. An XML file is generated for each piece of media to store the metadata described in Step 2 and the annotations described in Steps 4 and 5. A schema definition is used to validate XML files after metadata has been added to detect improperly formatted or corrupted data.\nStep 4: Automated annotations are generated using a chain of open-source and pretrained models. WB detection is done with YOLOv5 and a fine-tuned version is used on long-range and aerial videos [14]. 3D human mesh reconstruction with Meshtransformer and 2D keypoint estimation with DARK is performed on the WB detection results [30, 17]. Re-ID with DG-Net++ is then performed on the pose results to determine whether or not a WB detection is the intended subject or not [31]. The pose information helps narrow the gallery to reference images at a similar yaw angle to the detections. BoT-SORT is used for track generation, which leverages the Re-ID results for better track consistency [1]. Finally, various post-processing steps are performed such as estimating the head bounding box from the 3D mesh and 2D keypoints.\nStep 5: Select video frames are sent out for manual annotation either for validation or correction of suspected automated annotation errors. Mainly, these consist of a couple frames per track to verify the correct subject was denoted, a non-subject made it into the video, or the detection was a non-person. These results are then merged with the automated annotations. The metadata information such as type, method of collection along with examples are summarized in Table 1.\nStep 6: The test set is further partitioned into gallery and probe sets for evaluation. Gallery media is used to build a database of known identities and probe media is imagery of subjects whose identities are presumed unknown. Probe imagery is compared to gallery enrollments and the results are used to compute the metrics we use to evaluate system performance. The evaluation gallery sets contain data for distractor subjects who are not part of the probes to simulate a larger gallery. We perform additional data partitioning for each specific mission area. These more granular designations are described in detail in Section 5. Balancing the subjects for mission categories was applied during the probe selection process but variation in the number of subjects and the samples per mission still exist as shown in Table 2.\nStep 7: The probe set for mission analysis is organized into two major partitions: Face Restricted and Face Included. In the Face Restricted probe set, all faces are either"}, {"title": "4. Common API", "content": "The API leveraged in this work took inspiration from previous works in [13], [12], [5], and [6, 7]. It was built for this evaluation program and was guided by the same principles of maintaining operational relevance. To that end, the API has 3 main purposes: 1) To provide an algorithm harness with a front-end common set of callable functions that can abstract away complex back-end SOTA biometric algorithms, 2) to enforce strict standards on research code that help guarantee stable and predictable execution patterns, and 3) to operate as a data-passing middle-man, which allows the API to simulate the types of data and usage behaviors that would reflect those of a deployed system. Figure 2 shows a general overview of the API that is meant to interface with both streaming data and SOTA integrated biometric solutions to provide a realistic operational scenario. These end-to-end biometric solutions incorporate many internal algorithms across multiple biometric modalities, along with database-level storage and caching capabilities, to ultimately produce a single set of match scores for verification and search requests. For this reason, we refer to these SOTA solutions as Holistic integrated Solutions (HS). To evaluate an HS, the API ingests signature sets (sig-sets) similar to those introduced in [11] and used in numerous biometric challenges [24], which define and provide locations to subsets of video and imagery pertaining to the missions outlined in Section 5. The API then iterates over these sig-set-defined media subsets, loads the media, and subsequently streams the media to the HS. Because the API is constrainable, it can be configured to strip out, modify, or reformat media and its metadata to better simulate real-world operating scenarios. For instance, some scenarios may allow for metadata that provides camera information such as make, model, frame rate, operating conditions, etc., while other scenarios may need to simulate that metadata being unavailable to an HS at ingestion time."}, {"title": "5. Mission Areas", "content": "In this section, we explain the mission areas selected for this analysis and provide examples of the kinds of use cases that may be captured by each of them. Table 2 provides a brief overview of the mission areas discussed in this paper. Based on use case, some applications may focus on face recognition performance, whereas others might require the fusion of multiple modalities. The modality may also be determined by the biometrics available in galleries and watchlists. Currently, the barrier to entry for face recognition is relatively low due to the high availability of both frontal face imagery and highly accurate software systems. Other modalities such as body and gait are as yet less sophisticated and require significant performance advancement before they will be widely adopted. There is also a much smaller corpus of ground truth data for these modalities. Evaluating performance for these specific mission areas may also inform areas in which additional research is needed to achieve suitable accuracy for deployment in a real world application.\nExperimental Control: We use imagery captured by high resolution surveillance cameras positioned at eye-level. This relatively \"easy\" data is used to establish a performance baseline.\nClose Range Face: This area focuses on imagery with high resolution, frontal profile views of the face, simulating environments like building entrances, choke points, and security checkpoints. Typically, these areas are well-suited to face recognition because there is a high degree of certainty that people will pass through a contained space and face a certain direction. We use close range video from cameras positioned at eye-level or slightly elevated.\nClose Range Body: This mission area is a super set of Close Range Face that includes all views of the face along with body and gait signatures. These videos are taken from high resolution surveillance cameras at close-range positioned at eye-level or slightly elevated. This mission can be used as a baseline for comparing the effects of distance in biometric performance.\nLong Range Face: This category consists of data captured using long range cameras on the ground that are configured for facial imagery at a resolution suitable, though perhaps not ideal, for recognition. Long range face recognition is an active research area and encompasses surveillance use cases requiring that cameras be positioned at significant distances from their intended subjects. We are particularly interested in any recognition improvements that may be observed when leveraging body and gait imagery.\nLong Range Body (Fusion): This area includes all ground sensors at long range that are poorly configured for biometric recognition. This group is a super set of the long range face mission area which also includes body and gait, highlighting the improvements achieved by fusing these additional modalities along with the face.\nUAV: Imagery captured from UAV platforms introduces unique challenges relating to distance, pitch angle, and platform size and weight. Consistent, accurate performance us- ing this data would constitute a large step forward in the realm of biometric recognition and would support many surveillance and security use cases in which UAV platforms are already used or static camera equipment on the ground would be intractable.\nTurbulence: Atmospheric conditions can degrade outdoor image quality significantly and lead to poor recognition performance. Turbulence visibly distorts imagery, particularly at medium and long ranges, and may challenge any biometric application which relies on data captured outside from any nontrivial distance. This problem varies based on environmental conditions like climate, weather, and terrain.\nElevated Cameras: This area focuses on close range surveillance cameras positioned to look down from rooftops or masts at pitch angles exceeding 12\u00b0. In many instances, this is the most suitable way to place cameras due to space constraints or concern over damage to surveillance equipment. Extreme pitch angles are challenging since they do not capture the frontal or eye-level face imagery that typically produces the most accurate recognition.\nGait: Gait is a relatively nascent biometric modality when compared to those that are more established and widely adopted like the face. However, advancing gait recognition performance will serve many applications not only on its own but in conjunction with modalities like the face and body. Even as an auxiliary signal, gait patterns may provide more accurate recognition capabilities in challenging uncontrolled imaging conditions.\nFace Restricted: This area targets data from long-range cameras, elevated cameras, and cameras mounted to UAV platforms. Faces are totally or partially occluded, observed from extreme pitch angles, or are of lower resolution than 20 pixels in height. Improving recognition in the absence of clear facial imagery would be advantageous for a wide range of use cases in which subjects are captured in an uncontrolled environment not suitable for close range, ground level equipment installation. While previous research has shown that incorporation of degraded facial imagery can"}, {"title": "6. Analysis Design", "content": "Evaluating biometric systems based on specific mission areas informs development of applications targeting use cases that fall within these areas. It is useful to consider factors that may improve performance in one domain while hindering it in another. We design experiments targeting these mission areas in greater isolation than would be captured by a more general performance evaluation. Additionally, this analysis design is based on the idea that, under challenging conditions, systems will perform better when they can leverage imagery of both the face and the body than facial imagery alone. To this end, these experiments measure the performance of the selected algorithms on face, body, and/or WB fusion (face, body, and gait) recognition tasks using a dataset that consists of two distinct probe sets, Face Included and Face Restricted. For the purposes of evaluation we use probe videos from field data and gallery enrollments from controlled data. Probe videos contain long-range or elevated views for a single subject. After processing a video, a labeled database entry for the subject is made. It is assumed that each entry could have multiple tracklets and each tracklet would be associated with a template matching an entry in the gallery. Gallery enrollments are a collection of templates associated with subjects. An enrollment produces a single template entity that can be searched against and matched. And the template encapsulates features extracted from one or more pieces of input imagery from a subject. The imagery could be face and/or WB images and videos. Evaluations are implemented by creating probe and gallery databases. API commands are provided to verify against these databases. A ROC curve, which plots the true accept rate (TAR) vs the false accept rate (FAR) over the range of thresholds, is generated for each mission area. The experiments can be run in different modes such as face only, body only, gait only and fusion to highlight the strengths and weaknesses of systems as well as to show improvements, if any, when fusion is selected. As mentioned in Section 5, the main focus of this evaluation analysis is to use mission areas to better utilize the data to"}, {"title": "7. Results and Discussion", "content": "Face Recognition: The Close Range Face mission serves as a basis for comparison and consists of high-resolution imagery representative of high-quality video from current surveillance camera deployments. Face recognition algorithms should be able to perform well with existing technology. The Long Range Face mission presents additional challenges related to image quality, allowing us to test both the basic functionality of the algorithms and understand how new developments are improving accuracy in more challenging scenarios. ROC results are compared in Figure 3, which shows promising performance.\nWhole Body Matching: One of the primary goals of this research is to develop techniques for recognizing individuals based on their whole-body appearance. Combining face recognition with body geometry, gait, and other features should improve matching performance. This is particularly important given the image quality challenges in our dataset. Figure 4 compares results in this area and shows how performance decreases under different levels of difficulty. Close range shows very high accuracy, while longer ranges show systems performing well despite being poorly configured for recognition. Face restricted performance is surprisingly good given that these videos were selected in a way that well-established face recognition algorithms lacked usable faces, requiring the algorithms to rely heavily on body and gait features.\nImprovements from Fusion: Another critical area of interest is how the fusion of different biometric modalities improves recognition accuracy. Figure 5 illustrates how face only, body only, and gait only recognition is combined to produce improved fusion accuracy. The body only case outperforms the face only case for most of the systems and fusion of all modalities help the overall recognition performance. Based on this analysis, the challenges that come from identifying individuals at long range would be greatly"}, {"title": "8. Conclusion and Future Work", "content": "The design of thoughtful experiments that help move the needle on SOTA biometric algorithms requires a tight marriage between physical data collection processes and the detailed curation and partitioning of imagery and video therein. We hope this paper acts as a thought-provoking case study into that process, along with providing a rich analysis of anonymous SOTA algorithms tested against our experimental protocols.\nOur evaluation methods addresses challenges related to image quality and focuses on reducing the dependence on face recognition as the sole modality by providing data and tests focused on whole body matching. The evaluation utilized a diverse range of hardware solutions including COTS cameras, custom long-range cameras, and UAVs. Preliminary results indicate promising progress in WB recognition.\nIn future work, we intend to further address challenges related to image quality and dependence on face recognition as the primary modality in long-range biometric identification systems. We aim to promote development of robust systems that are less susceptible to these drawbacks. We also plan to focus on video recordings featuring larger groups of individuals as well as more intricate and dynamic scenarios. By doing so, we hope to continue encouraging development of more accurate and reliable methodologies for WB recognition technology. We are confident that these advanced systems will have significant implications for various mission areas, including law enforcement and national security. Further leveraging WB recognition capabilities will potentially improve situational awareness, public safety, and operational efficiency."}]}