{"title": "CoCoEvo: Co-Evolution of Programs and Test Cases to Enhance Code Generation", "authors": ["Kefan Li", "Hongyue Yu", "Tingyu Guo", "Shijie Cao", "Yuan Yuan"], "abstract": "Large Language Models (LLMs) have shown remarkable performance in automated code generation. However, existing approaches often rely heavily on pre-defined test cases, which become impractical in scenarios where such cases are unavailable. While prior works explore filtering techniques between programs and test cases, they overlook the refinement of test cases. To address this limitation, we introduce CoCoEvo, a novel LLM-based co-evolution framework that simultaneously evolves programs and test cases. CoCoEvo eliminates the dependency on pre-defined test cases by generating both programs and test cases directly from natural language problem descriptions and function headers. The framework employs specialized evolutionary operators, including LLM-based crossover and mutation operators for program evolution, along with a test case generation operator for test case evolution. Additionally, we propose optimization strategies such as a crossover rate scheduler to balance exploration and convergence, and a multi-objective optimization method for test case selection. Experimental results on multiple state-of-the-art LLMs demonstrate that CoCoEvo surpasses existing methods, achieving state-of-the-art performance in automated code generation and testing. These results underscore the potential of co-evolutionary techniques in advancing the field of automated programming.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, Large Language Models (LLMs) have undergone rapid development and have been widely applied in automated software development tasks such as code generation and test case generation. Models like GPT-4 [1], Llama3 [2], Qwen2.5 [3], and DeepSeek-V3 [4] have demonstrated exceptional code generation capabilities, significantly advancing the field of automated software development.\nDespite their impressive performance, LLMs do not always generate accurate or correct programs. To enhance the reliability of program generation, test cases are often utilized to evaluate code correctness and provide feedback. Existing approaches frequently rely on pre-defined test cases for evaluation. For instance, Sampling+Filtering [5] first generates a batch of candidate codes, evaluates them using pre-defined test cases, and selects the code with the highest score as the final solution. Other methods, such as Reflexion [6], Self-Repair [7], INTERVENOR [8], LDB [9], and PairCoder [10], utilize execution feedback on pre-defined test cases to refine or repair generated code. These methods generally assume that pre-defined test cases are accurate and can be fully trusted.\nHowever, pre-defined test cases are not always available. In test-driven development (TDD) [11], test cases are defined before program implementation. In real-world scenarios, software development frequently relies on natural language requirements, with no pre-existing test cases or example programs. Furthermore, creating pre-defined test cases can be labor-intensive, requiring significant manual effort. In situations where LLMs must generate test cases independently, the effectiveness of existing methods remains uncertain.\nSome approaches do not rely on pre-defined test cases. For instance, methods like CodeT [12] and MBR-Exec [13] generate both code and test cases using LLMs and filter the code based on the execution results of these test cases. Similarly, CodeCoT [14], which utilizes chain-of-thought prompting [15], enables LLMs to generate code and test cases simultaneously and evaluate and refine the code based on these cases. Additionally, AgentCoder [16] introduces a multi-agent system with a dedicated Tester agent to generate test cases for evaluating and repairing code. However, studies such as [12], [17] highlight that LLM-generated test cases can contain significant errors, potentially leading to unreliable evaluations and incorrect feedback. Moreover, many of these methods neglect to verify the quality of the generated test cases. While approaches like CodeT and MBR-Exec implement basic filtering mechanisms for test cases, other methods often use these generated test cases directly for code evaluation without ensuring their correctness.\nThere are also previous co-evolution methods based on code and test cases, which have primarily focused on code maintenance and program repair. For example, [18] recommends new test cases based on source code similarity, [19] identifies outdated test cases, and [20] constructs test oracles from bug reports using predefined templates. Similarly, [21] generates test cases based on predefined specifications to drive software evolution. However, these approaches depend heavily on pre-existing programs, test specifications, or templates, limiting their applicability in scenarios lacking such resources.\nTo address these limitations, we propose CoCoEvo, an LLM-based co-evolution framework that enables the simultaneous evolution of programs and test cases to produce more accurate outcomes (illustrated in Fig. 1). In CoCoEvo, both test cases and programs are generated by LLMs, requiring only natural language problem descriptions and function headers. No pre-defined programs, test cases, or templates are needed. The CoCoEvo framework consists of two alternating steps: program evolution and test case evolution. In program evolution, we introduce LLM-based crossover and mutation operators to generate program offspring. The current population of test cases is used to evaluate program fitness. In test"}, {"title": "II. RELATED WORK", "content": "A. Enhancing Code Generation Accuracy with Test Case\nThe integration of test cases has emerged as a pivotal strategy for improving the quality of code generated by LLMs. Numerous studies leverage test cases to filter, evaluate, and refine generated programs, leading to higher-performing and more reliable code. Below, we provide an overview of notable methodologies in this domain.\nLi et al. [5] introduce a Sampling+Filtering approach, where a large set of candidate programs is sampled and scored based on performance against a provided test set. Chen et al. [12] propose a framework in which both code and test cases are generated by LLMs simultaneously, using a dual agreement metric to identify and select better programs. Shi et al. [13] assess semantic similarity by executing generated programs on test cases and leveraging the execution results to guide program selection.\nBeyond evaluation, test cases play a critical role in iterative repair and refinement of generated code. Execution outcomes from test cases can be used to repair faulty programs. For instance, Shinn et al. [6] introduce the Reflexion framework, where failure messages from test executions are converted into verbal feedback for program repair. Similarly, Olausson et al. [7] evaluate LLMs' capabilities to repair programs using failed test feedback. Zhang et al. [23] allow LLMs to analyze test execution results to revise erroneous code. Zhong et al. [9] extend this idea by enabling step-by-step debugging. Huang et al. [14] adopt a chain-of-thought prompting [15] approach that generates code and tests sequentially, iteratively improving the code based on test feedback. Zhang et al. [10] introduce a pair programming framework where test feedback informs repair strategies and planning.\nThe generation of test cases differs across these methodologies. For example, CodeT [12] and CodeCoT [14] rely\nB. Automated Test Case Generation\nEarly works, such as [24], leveraged search-based methods to generate test cases. More recently, there has been a growing trend toward utilizing LLMs for test case generation. Studies such as [25], [26], and [27] have explored the effectiveness of LLMs in automating program testing. Similarly, [28] investigates methods for enhancing existing test cases using LLMs. Other approaches, such as TOGA [29], TOGLL [30], and the method described in [31], focus on generating test oracles based on predefined templates. Meanwhile, [32] introduces a model trained with deep reinforcement learning for text-to-test case generation. The application of LLMs in software development extends to generating test cases for various scenarios. For instance, [33] employs GPT-4 to generate security tests for identifying vulnerabilities, while [27] uses LLMs to assist in generating unit tests for automatic software testing. Similarly, [34] demonstrates the use of LLMs to create test cases aimed at reproducing general software bugs. LLMs are also integrated into code generation studies to improve the accuracy of generated code through test case generation. CodeT [12], for example, utilizes LLMs with zero-shot prompts to directly generate test cases. Reflexion [6] introduces a Test Agent powered by LLMs for test case generation.\nC. Evolution with LLMs for Code Generation\nThe integration of LLMs with evolutionary algorithms has been extensively explored to enhance code generation. For example, Bradley et al. [35] introduced OpenELM, an open-source Python library that designs specialized evolutionary operators for code generation. Chen et al. [36] utilized LLMS as adaptive mutation and crossover operators in evolutionary neural architecture search (NAS), optimizing neural network design by merging LLM capabilities with evolutionary processes. Liu et al. [37] proposed an automated approach for evolving optimization algorithms using LLMs, which eliminates the need for manual design by applying standard evolutionary operations such as initialization, selection, and mutation. In a related study, Liu et al. [38] developed a framework for evolving heuristics by integrating LLMs with evolutionary computation to co-evolve both natural language descriptions and executable code for heuristic design, further extending previous methodologies.\nMa et al. [39] introduced an algorithm combining LLMs with evolutionary search to generate and optimize reward functions for reinforcement learning tasks, incorporating a reward reflection mechanism for refinement. Meyerson et al. [40] demonstrated the use of few-shot prompting with LLMs to generate offspring in evolutionary algorithms, enabling a general crossover operator by concatenating parent solutions into a single prompt, which can adapt to various domains.\nSimilarly, Romera et al. [41] integrated LLMs with evolutionary algorithms to create context-aware program mutations, eliminating the need for manually defined mutation operators. Ye et al. [42] proposed ReEvo, which combines evolutionary search with LLM-driven reflections to generate optimization heuristics, efficiently exploring heuristic spaces while offering verbal gradients for optimization. Lastly, AutoTest [43] combines automated test case generation with code execution in an evolutionary framework, utilizing LLMs to co-generate code solutions and their corresponding test cases for enhanced optimization.\nAdditionally, various LLM-based co-evolution methods have been proposed. For instance, Chi et al. [19] identifies outdated test cases by co-evolving production and test code. Ruan et al. [20] constructs test oracles from bug reports using predefined templates. Chi et al. [19] leveraged LLMs with dynamic validation to automate co-evolution between production and test code, identifying obsolete test cases to improve software quality and reduce maintenance overhead. TestART [44] enhances unit testing for LLMs by combining automated test generation with repair, using template-based fixes and coverage feedback to improve test accuracy."}, {"title": "III. METHODS", "content": "A. Framework Overview\nAn overview of CoCoEvo is presented in Fig. 2. The evolutionary process begins with the random initialization of both the program population and the test case population, achieved through the random generating programs and test cases using LLMs. The co-evolution process follows a loop consisting of two alternating steps: program evolution and test case evolution, each of which includes a Generation-Evaluation-Selection sequence. During program evolution, the test case population is used to evaluate programs and calculate their fitness. Conversely, during test case evolution, the program population is utilized to evaluate test cases.\nFor programs, we introduce LLM-based crossover and LLM-based mutation operators for offspring generation. For test cases, we implement an LLM-based additional test case generation operator. Once the preset number of iterations is reached, the co-evolution loop terminates, and the program with the highest fitness score is returned as the final solution. Algorithm 1 provides the pseudo-code detailing the CoCoEvo process. Further implementation details for each algorithm step are elaborated in subsequent sections.\nB. Program Evolution\nThis section provides a detailed description of strategies employed in program evolution.\nProgram fitness function. During evaluation, programs and test cases undergo cross-evaluation, as illustrated in Fig. 3. The outcomes are represented by a matrix M:\n$M_{i,j} = \\begin{cases} 1, & \\text{program}_i \\text{ passed test}_j \\\\ 0, & \\text{program}_i \\text{ failed test}_j. \\end{cases}$ (1)\nWe compute the confidence score of each program, adopting the approach in [12]. Programs that pass the same set of test cases are grouped into a set $P_s$, while the corresponding test cases are grouped into a set $T_s$. The confidence of program i is then defined as\n$Conf_{p,i} = \\sqrt{|P_s| \\times |T_s|}$ (2)\nwhere $P_s$ represents the set containing program i, and $T_s$ represents the associated test set. The confidence score evaluates the likelihood of a program being correct. The fitness of program i is defined as\n$F_{P,i} = Conf_{P,i}$ (3)\nwhere a higher fitness score indicates a higher probability of correctness for the program.\nCrossover Rate Scheduler To address the multimodal nature of the program search space and mitigate the risk of premature convergence, a dynamic scheduler is employed to regulate crossover and mutation rates. A higher mutation rate is applied in the early stages to promote exploration, while the mutation rate decreases, and the crossover rate increases in later stages to refine convergence.\nUnlike traditional random mutation methods, the LLM-based program mutation operator leverages LLMs for code generation, which could retain key features of high-quality\n$x(r) = x_{final} + \\frac{1}{2}(x_{initial} - x_{final})(1 + cos(\\frac{\\pi r}{R})).$ (4)\nThis formulation ensures that the crossover rate increases smoothly from 0 to 1. The number of crossover operations and the number of mutation operations are defined as follows:\n$\\begin{cases} N_{r,c} = N_p \\times x(r) \\\\ N_{r,m} = N_p - N_{r,c} \\end{cases}$ (5)\nwhere $N_p$ is the program population size, $N_{r,c}$ and $N_{r,m}$ represent the number of program crossover and mutation operations in iteration round r, respectively. When the initialization is completed, the iteration round r is set to 1. After initialization, the iteration round r starts at 1 and increments by 1 after a co-evolution loop.\nC. Test Case Evolution\nTest fitness function. The evaluation of test cases involves two key metrics: confidence and discrimination. The confidence of test case j is calculated as\n$Conf_{T,j} = \\frac{1}{N_P} \\sum_{i=1}^{N_P} M_{i,j} F_{P,i}$, (6)\nwhere $N_P$ represents the total number of programs, $F_{P,i}$ represents the fitness of program i, and $M_{i,j}$ is the evaluation matrix indicating whether programi passed testj. Intuitively, confidence measures the degree of agreement between the program population and the test cases, with higher program fitness contributing more weight to the degree of agreement. Simple test cases may fail to differentiate between correct and erroneous programs, as both types may pass them. To address this, we introduce a discrimination metric. First, the pass rate of test case j across the program population is defined as\n$p_j = \\frac{1}{N_P} \\sum_{i=1}^{N_j} M_{i,j}$, (7)\nthen the discrimination of test case j is represented by the entropy of the pass rate:\n$Disc_{T,j} = -p_j log_2 p_j - (1 - p_j)log_2(1 - p_j)$. (8)\nTest cases achieve higher discrimination when their pass rate approaches 0.5, as they better differentiate among the program population. This indicator assesses whether the test cases can exert evolutionary pressure on the program population.\nThe selection of test cases is performed using the Pareto approach for multi-objective optimization [22], as illustrated in Fig. 5. The Pareto front is constructed based on the two optimization metrics, confidence and discrimination. Test cases with high confidence and low discrimination represent basic test cases that cover common scenarios, with a high probability of being correct. Test cases with high discrimination represent more challenging cases that are effective at exposing errors. We noticed that there are some test cases with very low confidence levels that sometimes have a very high degree of discrimination. To avoid this type of test case rom being selected, the test cases with confidence lower than the average in the selection results will be filtered out.\nD. LLM-based evolution operators\nProgram crossover operator.\nWe utilize LLMs to perform program crossover, as illustrated in Fig. 6. The process involves the following steps:\n1) Selection. Two programs are selected as crossover parents using the binary tournament selection method based on their fitness scores.\n2) Crossover. The selected programs are combined into a prompt. The LLM is instructed to analyze their similarities and differences and generate a new program that merges useful elements from both.\nProgram mutation operator. The LLM-based mutation operator intelligently rewrites a program to achieve the same functionality using a different approach. The steps are as follows:\n1) Selection. A program is randomly selected from the current population as the mutation parent."}, {"title": "IV. EXPERIMENTS", "content": "A. Dataset\nWe collected a dataset of 80 programming problems from the weekly and biweekly contests hosted on the LeetCode platform \u00b9, forming the LeetCode-Contest dataset. In the dataset, each problem includes a prompt comprising the function header and a natural language description in the form of a docstring, along with test cases and reference solutions. To ensure data integrity and avoid potential leakage, all selected problems were released after March 2024.\nTo more accurately assess the ability of LLMs to solve programming challenges, we took measures to prevent plagiarism of example test inputs and outputs in test case generation. Specifically, we removed all test input-output samples from the problem descriptions, retaining only the natural language"}, {"title": "V. RESULTS AND DISCUSSION", "content": "A. Comparison of Accuracy\nIn this section, we present the results of the accuracy comparison for the methods. Table III reports the pass@1 metric for each method across different LLMs on the LeetCode-Contest dataset. As shown in Table III, our proposed CoCoEvo method achieves the highest performance across all LLMs, demonstrating its superior ability to generate accurate code. This success can be attributed to the co-evolution framework employed by CoCoEvo, which facilitates a more effective iterative refinement between programs and test cases.\nAn interesting observation concerns the repair-based methods such as Self-Repair, Reflexion, and INTERVENOR. When these methods utilize test cases generated by LLMs instead of pre-defined ones, their effectiveness diminishes significantly. In some instances, their performance is even worse than that of the basic Sampling approach. This is because the erroneous test cases generated by LLMs can mislead these methods by providing inaccurate feedback, causing the repair process to make suboptimal adjustments to the code. Consequently, this results in lower accuracy and poorer overall performance. Similarly, for methods like Sampling+Filtering, where test cases are used to filter and select the best code, performance remains comparable to the Sampling method when the test cases are generated by LLMs. This highlights a critical limitation: these methods struggle to adapt effectively in scenarios where reliable pre-defined test cases are unavailable.\nFurthermore, methods that leverage the agreement between programs and test cases, such as MBR-Exec and CodeT, exhibit higher performance compared to Sampling and repair-based approaches. Among these, CodeT stands out, delivering consistent improvements in pass@1 across all LLMs. This suggests that considering the alignment and mutual validation of programs and test cases is a promising strategy for enhancing code generation accuracy.\nFig. 9 illustrates the changes of pass@1 during the evolution process. While fluctuations in performance are observed, the method demonstrates consistently strong performance on average. The figure shows that although the CoCoEvo method starts with a relatively low pass@1, its performance steadily improves over the course of evolution iterations, eventually surpassing that of the CodeT method. This demonstrates the effectiveness of the CoCoEvo method. By alternating the evolution of programs and test cases, the program population progressively converges toward the correct solution.\nFig. 10 illustrates the changes in the accuracy of the test population throughout the evolution process. At initialization, the CoCoEvo method achieves a slightly higher test accuracy than the randomly generated method. Specifically, the CoCoEvo method generates test cases once and extracts 10 test cases, whereas the random method involves generating test cases 10 times and extracting 10 test cases each time. This indicates that as the number of test case generations by LLMs increases, test accuracy tends to decrease slightly.\nInterestingly, For the CoCoEvo method, the peak test accuracy is observed in the second iteration. In subsequent iterations, the accuracy declines slightly, potentially due to the Pareto selection process, where test cases with higher discrimination but lower confidence are accepted. This may inadvertently introduce some incorrect test cases. Nevertheless, the decline is gradual, and the test accuracy remains consistently high, making this behavior acceptable.\nB. Comparison of Program Evolution Scheduler\nTo assess the effectiveness of the crossover rate scheduler, we compare the cosine scheduler with a constant scheduler in our experiments, using a mutation rate of 0.2 and a crossover rate of 0.8. These experiments were performed on the Qwen2.5-Coder-32B and the LeetCode-Contest dataset. The results, summarized in Table IV and Figure 11, highlight key differences between the cosine scheduler and the constant rate approach.\nAt the early stages of the evolution process, the constant method, characterized by a fixed crossover and mutation rate, demonstrates faster convergence and initially outperforms the cosine scheduler due to its higher crossover rate. However, this rapid convergence causes the Constant method to stagnate and become trapped in a local optimum, ultimately limiting its overall performance.\nIn contrast, the cosine scheduler, despite its lower initial performance, maintains consistent improvement throughout the evolution process. By adapting the crossover rate dynamically, the scheduler avoids premature convergence and\nachieves superior results in later stages. Notably, it surpasses the performance of the Constant method after the fifth iteration, ultimately delivering a higher pass@1 rate.\nThis comparison underscores the importance of dynamic scheduling in optimizing program evolution, particularly in scenarios prone to local optima.\nC. Comparison of Program Fitness Function\nTo evaluate the impact of different program fitness functions, we conducted a comparative analysis using the CodeT score and the simple pass rate as fitness functions. The base model for these experiments was Qwen2.5-Coder-32B with the LeetCode-Contest dataset serving as the benchmark. The results are summarized in Table V and illustrated in Figure 12. The findings reveal the insight that calculating the pass rate on test cases directly as the fitness function yields a lower pass@1 value compared to utilizing the CodeT score. This behavior can be attributed to the advantages of the CodeT score, which incorporates the congruence between programs and test"}, {"title": "D. Comparison of Test Fitness Function", "content": "In this section, we present the results of comparing various test fitness functions. Specifically, we evaluate Failure Rate, Pass Rate, Confidence, and Pareto as methods for test fitness calculation. The experiments are conducted using the Qwen2.5-Coder-32B model on the LeetCode-Contest dataset. The detailed results are depicted in Fig. 13, and additional information on test case accuracy is provided in Fig. 14.\nAmong the test fitness functions, Pass Rate directly calculates the proportion of test cases successfully passed by a program, while Confidence represents a weighted pass rate, as described in Section III-C. Similarly, Failure Rate measures the failure rate of a program on the test cases. For these three fitness calculation methods, greedy selection is employed as the offspring selection method.\nOur findings indicate that employing the Pareto method as the selection mechanism yields the best performance. The Pareto approach prioritizes test cases with both high discrimination and high confidence levels, thereby demonstrating superior effectiveness. In contrast, using Failure Rate as the fitness function results in a sharp decline in pass@1 performance. Previous studies on co-evolution have generally argued that the confidence level of a test case increases as more programs fail on it. However, we discovered that if the test cases contain errors, this method can lead to a population dominated by faulty test cases, which fail to provide accurate feedback to the programs.\nFurthermore, Fig. 13 reveals that employing Confidence as the test fitness function outperforms using Pass Rate by a small margin. The Confidence function weights the fitness of test cases based on program performance, with programs demonstrating higher fitness values contributing more heavily to the confidence score. This approach ensures that more reliable programs have a greater influence on test case evaluation.\nE. Ablation Study\nWe conducted ablation experiments using the Qwen2.5-Coder-32B model on the LeetCode-Contest dataset to evaluate the effectiveness of the test case evolution module. In these experiments, we removed the test case evolution module and"}, {"title": "VI. CONCLUSION", "content": "This paper introduced CoCoEvo, an LLM-based co-evolution framework that advances automated code generation by integrating simultaneous program and test case evolution. By eliminating the reliance on pre-defined test cases, CoCoEvo addresses a critical limitation in existing approaches, particularly in scenarios lacking comprehensive testing resources. The dynamic crossover rate scheduling mechanism and the multi-objective optimization for test case selection were key to the framework's success, enabling effective exploration and refinement throughout the evolution process. Experimental results on the LeetCode-Contest dataset, utilizing four leading LLMs, demonstrated that CoCoEvo achieved superior performance compared to traditional methods.\nBeyond its practical implications in software development, CoCoEvo opens new avenues for integrating evolutionary algorithms with LLMs. Future work will focus on enhancing the scalability of CoCoEvo for project-level code generation and incorporating additional quality metrics for test case evaluation. Furthermore, exploring its application in other domains, such as automated debugging and software maintenance, could further expand its impact."}]}