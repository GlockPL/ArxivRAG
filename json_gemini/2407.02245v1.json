{"title": "Safe CoR: A Dual-Expert Approach to Integrating Imitation Learning and Safe Reinforcement Learning Using Constraint Rewards", "authors": ["Hyeokjin Kwon", "Gunmin Lee", "Junseo Lee", "Songhwai Oh"], "abstract": "In the realm of autonomous agents, ensuring safety and reliability in complex and dynamic environments remains a paramount challenge. Safe reinforcement learning addresses these concerns by introducing safety constraints, but still faces challenges in navigating intricate environments such as complex driving situations. To overcome these challenges, we present the safe constraint reward (Safe CoR) framework, a novel method that utilizes two types of expert demonstrations-reward expert demonstrations focusing on performance optimization and safe expert demonstrations prioritizing safety. By exploiting a constraint reward (CoR), our framework guides the agent to balance performance goals of reward sum with safety constraints. We test the proposed framework in diverse environments, including the safety gym, metadrive, and the real-world Jackal platform. Our proposed framework enhances the performance of algorithms by 39% and reduces constraint violations by 88% on the real-world Jackal platform, demonstrating the framework's efficacy. Through this innovative approach, we expect significant advancements in real-world performance, leading to transformative effects in the realm of safe and reliable autonomous agents.", "sections": [{"title": "I. INTRODUCTION", "content": "The advance of autonomous driving technology promises to revolutionize the way people commute, offering safer, more efficient, and accessible transportation options. At the heart of this transformative potential is the importance of ensuring the safety and reliability of autonomous vehicles in diverse and dynamic driving environments. To achieve this, many researchers and engineers have proposed algorithms such as rule-based controllers [1], [2] and imitation learning methods [3]-[5]. Rule-based controllers provide a structured approach to decision-making based on predefined rules and conditions, while imitation learning allows the agents to mimic human driving behaviors by learning from vast amounts of driving data. However, these methods face significant challenges in handling situations that fall beyond predefined rules [6]. These scenarios, which are neither encapsulated within the training data nor foreseen in the predefined rule sets, pose critical hurdles in achieving the comprehensive coverage and reliability that autonomous driving aspires to achieve.\nTo address the limitations inherent in imitation learning and rule-based controllers, reinforcement learning (RL) [7], [8] has emerged as a compelling alternative. Unlike its predecessors, RL enables autonomous driving agents to learn optimal behaviors through trial and error, interacting directly with their environment. This method offers significant advantages, such as the ability to continuously improve and adapt to new situations over time, potentially covering the gaps left by imitation learning and rule-based systems. Although RL excels in adaptability and decision-making in complex scenarios, ensuring the safety of autonomous driving agents remains a critical challenge. However, the exploratory nature of RL, which often requires agents to make mistakes to learn, poses a significant risk in real-world driving contexts where safety is crucial. This fundamental concern highlights the need for innovative approaches within RL frameworks to balance exploration with the stringent safety requirements of autonomous driving.\nTo address the aforementioned issue, the concept of safe reinforcement learning (safe RL) [9], [10] has been introduced. This approach aims to incorporate safety constraints into the optimization process explicitly. By taking account of safety constraints into the policy optimization process, safe RL methods enhance the agent's ability to adhere to safety constraints, thereby improving safety during both the training phase and the final deployment. For instance, incorporating a lane-keeping reward directly into the reward function results in mediocre lane-keeping behavior. On the other hand, when the lane-keeping component is applied as a constraint within the safe RL framework, the agent demonstrates significantly improved lane-keeping performance. Despite these advancements, challenges persist in the application of safe RL algorithms for training agents to navigate complex driving environments safely.\nTo overcome these challenges, we propose a novel method called safe CoR, which innovatively combines two distinct types of expert demonstrations to refine existing safe RL algorithms. The first type, termed reward expert demonstrations, focuses exclusively on maximizing rewards without considering safety constraints. Conversely, the second type, safe expert demonstrations, prioritizes adherence to safety requirements above all, with subsequent consideration for reward maximization. By distinctly categorizing these experts-reward experts for their focus on performance optimization and safe experts for their dual focus on safety and reward maximization-we are able to calculate a constraint reward (CoR). This term aids in the update process, directing the agent to emulate the reward expert for maximizing rewards while using the safe expert as a regularizer to ensure constraint satisfaction. Through the strategic application of CoR, our method guides the agent toward reducing constraint violations (CV) while still achieving high levels of reward,"}, {"title": "II. RELATED WORK", "content": "Imitation learning is one of the main approaches in achieving autonomous driving agents. It is a method that guides agents to imitate the given demonstrations extracted from experts. One of the simplest approaches to imitation learning is behavior cloning (BC), which shows promising results in achieving generalization in real-world environments [13], [14]. Despite its promise, BC is particularly susceptible to compounding errors, a drawback that significantly hampers its effectiveness [15]. On the other hand, inverse reinforcement learning (IRL) [16] proposes another way to solve the problem of designing an autonomous agent, which is to learn the reward function from the expert demonstrations. Ho et al. [17] proposed an algorithm that integrates IRL and RL, enabling the agent to acquire expert behaviors and estimate the reward function concurrently. They mathematically proved the convergence of training both policies and discriminators alternatively and their research opened avenues for further researchers [4], [18], [19].\nAdditionally, there have been studies that combine imitation learning with online learning. Yiren et al. [20] experimentally demonstrated that expert demonstrations can assist agents in navigating challenging environments robustly.\nDespite these advancements, it is crucial to note that the mentioned methods have limitations as they do not directly account for safety constraints in the learning process.\nSafe reinforcement learning (safe RL) addresses the critical aspect of satisfying the safety of agents by integrating safety considerations into the learning process. The algorithm forces agents not only to maximize reward sums but also to satisfy given constraints simultaneously. This approach can be categorized into two methods: Lagrangian-based and trust-region-based methods.\nLagrangian-based method transforms the original safe RL problem into its dual problem. Ray et al. [12] proposed the proximal policy optimization-Lagrangian (PPO-Lagrangian) algorithm, which extends the traditional PPO framework by incorporating a Lagrangian multiplier approach to efficiently handle constraints, allowing for dynamic adjustment of the trade-off between policy performance and constraint satisfaction. Yang et al. [21] proposed the worst-case soft actor-critic (WCSAC), which relaxes constrained problems to unconstrained ones using Lagrangian multipliers. However, such algorithms suffer from being overly conservative in their updates when constraint violations occur excessively during the initial learning stage. Additionally, the usage of Lagrangian multipliers makes the learning process unstable.\nTrust-region-based method is an extended version of trust region policy optimization [22], which solves non-convex optimization by transforming it into a simple problem. Achiam et al. [9] introduced constrained policy optimization (CPO), which addresses the optimization of policy functions under safety constraints without transforming them into different forms of optimization problems. CPO uniquely maintains safety constraints by utilizing a trust region method, ensuring that policy updates remain within predefined safety limits, thereby facilitating the development of safe reinforcement learning policies. Kim and Oh proposed TRC and OffTRC [10], [23], assuming that the discounted cost sum follows a Gaussian distribution. They derived the closed-form upper bound of conditional value at risk (CVaR). Recently, Kim et al. [24] proposed a method that utilizes a distributional critic and gradient-integration technique to enhance the stability of the agent. However, the above algorithms still face challenges in learning agents for safe driving in complex environments."}, {"title": "III. PRELIMINARY", "content": "A constrained Markov decision process (CMDP) is a framework that extends the traditional Markov decision process (MDP) to incorporate an additional constraint. A CMDP is defined by the tuple\u3008S, A, p, P, R, C, \u03b3): state space S, action space A, initial state distribution p, transition probability P, reward function R, cost function C, and discount factor \u03b3. The expected reward sum J(\u03c0) can be written in the aforementioned terms as follows:\n$$J(\\pi) := E_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t)]$$\nwhere at ~ \u03c0(st) and St+1 ~ P(\u00b7|st, at). Similarly, to define constraints, the expected cost sum can be expressed as follows:\n$$C_{\\pi} := E_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t C(s_t, a_t)]$$\nThen the objective of safe RL can be represented as follows:\n$$maximize_{\\pi} J(\\pi) \\quad s.t. C_{\\pi} \\le \\frac{d}{1-\\gamma}$$\nwith the constraint threshold d.\nConstraint reward (CoR) is an additional objective term that assesses the relative distance of an agent state between two sets of state data [4]. By utilizing two disparate sets of states, denoted as SA and SB respectively, the agent can estimate its performance relative to these two sets of demonstrations. If the distance between the agent's state and the first set of states, SA, is less than the distance to the other set of states, SB, the CoR value exceeds 0.5. In contrast, when the agent's state is closer to SB than SA, the CoR is reduced to below 0.5. In the prior work [4], by defining SA as the collection of states associated with expert performance and SB as those corresponding to suboptimal or negative behavior, such as random policy, the CoR enables the training of agents to emulate expert trajectories over undesirable ones. For the states, the CoR is defined as follows:\n$$COR(s, S_A, S_B) = \\frac{(1 + \\frac{\\Delta_A}{\\alpha})^{-1}}{(1 + \\frac{\\Delta_A}{\\alpha})^{-1} + (1 + \\frac{\\Delta_B}{\\alpha})^{-1}}$$\n$$\\Delta_A = \\frac{1}{|S_A|} \\sum_{s_a \\in S_A} ||s - s_a||_2,$$\n$$\\Delta_B = \\frac{1}{|S_B|} \\sum_{s_b \\in S_B} ||s - s_b||_2,$$\nwhere ||\u00b7||2 is the l2 norm, and \u03b1 refers to a hyperparameter used to regulate the sensitivity of CoR."}, {"title": "IV. SAFE COR", "content": "The goal of this work is to combine the strengths of imitation learning (IL) with those of safe reinforcement learning (safe RL) by utilizing expert demonstrations. The most straightforward method of combining IL and RL is to redesign the actor's objective by incorporating an im-itation learning term, such as log-likelihood probability, E(s,a)~D [log \u03c0(a|s)], where D = {so,ao,...,SN,an} is a dataset of expert trajectories, as in [20]. However, challenges arise when applying this approach to safe RL. Using an expert focused solely on maximizing the reward, referred to as a reward expert, can lead the agent to violate given constraints. On the other hand, an expert trained through safe RL algorithms, represented as a safe expert, might suffer from the drawback of low reward, despite directly optimizing the constraint. In other words, relying solely on each type of expert does not align with the ideal framework we aim to build.\nOne approach to tackle these challenges is to utilize both demonstrations. In scenarios where safety is assured, the agent is encouraged to prioritize the influence of the reward expert over the safe expert for higher rewards. Conversely, when the agent struggles to adhere to a given constraint, it can be directed to emulate the behavior of the safe expert rather than the reward expert. Through this strategy, the agent can be steered towards an optimal balance between the guidance provided by the two experts. Building upon the foundational principles outlined in the preceding sections, constraint reward (CoR) can serve as a guidance. CoR(Sn, Sre, Sse), where Sre and Sse refer to the sets of reward expert's and safe expert's demonstrations, respectively, allows us to evaluate the relative distance between the two experts. As the agent's state aligns with the states from the reward expert, the CoR value increases. On the other hand, it decreases as the agent's states converge towards that of the safe expert. Thus, the CoR can be employed as an augmented reward component with the coefficient \u03bb for the objective function, as below:\n$$E[\\sum_{t=0}^{\\infty} \\gamma^t {R(s_t, a_t) + \\lambda COR(s_t, S_{re}, S_{se})}]$$\nWhile the enhanced reward objective helps the agent pursue a higher reward, it's necessary to regulate excessive guidance from the reward expert to ensure the agent's constraint satisfaction simultaneously, as previously discussed. To accomplish this goal, we can once more integrate the CoR into the constraint optimization process, thereby enforcing the stricter constraint on the agent as the CoR value increases. Finally, we redefine the safe RL problem in (3) as follows:\n$$maximize_{\\pi} J(\\pi) + \\lambda_r \\cdot CoR,$$\n$$s.t. C + \\lambda_c \\cdot CoR \\le \\frac{d}{1 - \\gamma},$$\nwhere\n$$COR := E_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t CoR(s_t, S_{re}, S_{se})],$$\n\u039br and \u039bc are the risk adjustment parameters of the CoR term added to the J(\u03c0) and C respectively. \u039b determines the significance of expert guidance relative to the original reward function. As its value increases, the optimization problem shifts towards the objective of IL. \u039bc influences the degree of constraint satisfaction, leading to an augmented cost function that encourages the agent to adopt a more conservative approach. Notably, an excessively large value can fundamentally alter the original optimization problem, as it might be equivalent to reducing the constraint threshold. Therefore, to ensure stable training, we assign values of 0.1 to \u039br and 0.01 to \u039bc."}, {"title": "V. EXPERIMENTAL SETUP", "content": "In this section, we detail the experimental setup by deploying our proposed framework across a combination of simulator environments and real-world platforms. Specifically, we employ two simulators, safety gym [12] and metadrive [11], alongside the real-world Jackal platform [10]. In the context of safety gym, we conduct comparative analyses among five safe RL algorithms, including their integration with the proposed framework. We apply PPO-Lagrangian [12] and WCSAC [21] as representatives of Lagrangian-based methods, and utilize CPO [9], OffTRC [23], and SDAC [24] as examples of trust-region-based methods. However, due to suboptimal performance observed with Lagrangian-based methods in the Jackal simulator, the real-world Jackal platform, and the metadrive simulator, our investigation focuses solely on three trust-region-based methods. For experiments conducted in the metadrive environment, we extend our comparison to include BC-SAC [20].\nIn the construction of our model, we integrate a Gaussian policy network consisting of two hidden layers. In the case of the metadrive, we utilize the hidden layer dimension of 256. In the case of the safety gym and Jackal platform, environments necessitate a higher dimensional setting of 512. This architectural decision extends to the value network, which mirrors the policy network's structure and size. The networks employ the ReLU function as their activation layer. For the model parameters, we propose to use the discount factor of 0.99, the maximum KL divergence of 0.001 per policy update step, and the learning rate of 0.0003. In the case of OffTRC, we established the risk level of CVaR as 0.25, to distinguish it from CPO. However, due to its poor performance at a risk level of 0.25, we assigned a risk level of 1.0 for SDAC. A detailed comparison between these variations will be conducted in Section VII. The sensitivity parameter (\u03b1) of CoR is adjusted to 3.0.\nThe safety gym [12] is an environment that provides several tasks for testing safe RL algorithms. Unlike typical RL environments, the safety gym provides a cost function from the environment, which is the measurement of how much the agent is acting safely. The predefined robots (point and car) conduct two tasks: the goal and the button. In the goal task, an agent tries to reach a randomly spawned goal point without going through eight hazard areas. The button task requires an agent to press a designated one among four buttons while avoiding five hazard areas and four moving obstacles. For both environments, the environmental settings including the reward and cost function are the same as in [24]. In all tasks, the number of constraint violations (CV) is counted when the agent enters hazard zones or collides with obstacles. The constraint threshold d is configured at 0.025 for the goal task and 0.02 for the button task.\nThe metadrive simulator, an autonomous driving platform, challenges an agent to navigate to a destination without deviating from the road, colliding with objects, or exceeding a maximum speed which we set to 30km/s for stable training. The simulation utilizes a state representation that includes LiDAR feedback, driving metrics, and navigation data, with actions defined by continuous variables for steering angle and acceleration. The reward function R(st, at) is composed of two key components: achieving higher speed and successfully navigating through prescribed waypoints. Conversely, the cost function C(st, at) is designed to increment by 1 for any instances of generating unsafe scenarios, such as off-road incidents or collisions. Additionally, we define the score function \u0398(st, at) using the coefficient lc(= 5) in Eq. (8) to assess the overall driving performance. The constraint threshold d is configured at 0.02\n$$\\Theta(s_t, a_t) := R(s_t, a_t) - l_cC(s_t, a_t).$$\nOur methodology involves training experts via TRC [10] with different constraint thresholds. We set the constraint threshold to 0.5 for the reward expert and 0.001 for the safe expert. Evaluation during test phases measures the average sum of score, reward, violations, and success probabilities over 100 episodes, each 1,000 steps in length. Upon an off-road incident, the environment is reset, where the agent's position is reverted to the start while the step count continues from its preceding value, ensuring step continuity despite the positional reset.\nFor the real-world Jackal experiment, we apply the sim-to-real method to the Jackal platform, using the pretrained agents from the Jackal simulator [10]. The Jackal simulator is an environment for training safe RL algorithms, where it"}, {"title": "VI. RESULTS", "content": "In this subsection, our objective is to evaluate and compare various safe RL algorithms alongside their enhanced counterparts that incorporate the proposed framework, specifically within safety-gym environments. The results from each environment are illustrated in Figure 2. Note that due to the superior performance of SDAC with the risk level of 1.0 compared to other baseline algorithms, the figure exclusively presents the results of SDAC enhanced by the safe CoR application. It suggests that the integration of the proposed framework with SDAC yields similar outcomes to the original SDAC in terms of reward sums. Nonetheless, when evaluating the cost rate and the sum of constraint violations in the training phase (total CV), the framework significantly outperforms the baseline algorithms across all environments.\nFurthermore, in order to assess the versatility and effectiveness of the proposed framework, we conducted an experiment on the PointGoal task with baseline algorithms augmented by the safe CoR. As demonstrated in Table I, the implementation of the proposed framework resulted in a beneficial impact on the overall performance. For both WCSAC and SDAC, the utilization of the framework led to a decrease in the sum of costs and constraint violations, while simultaneously enhancing the sum of rewards. Remarkably, while SDAC initially stood out as a state-of-the-art algorithm among the baselines, the proposed framework further enhanced its performance. For PPO-L, the safe CoR demonstrated improvements in both reward sum and total constraint violations, while still maintaining adherence to the constraint threshold. The decrease in reward sum observed in CPO with the safe CoR is considered reasonable, given that the original CPO algorithm faced challenges in satisfying constraints. However, in the case of OffTRC, the proposed framework influenced the reward sum to decrease, resulting in a considerably lower cost sum compared to the constraint threshold. Considering that OffTRC operated with a risk level of CVaR as 0.25, indicative of an already conservative training approach, the implementation of the safe CoR led the agent to adhere to excessively stringent constraint.\nThe final result of the simulation experiment is shown in Table II. The most crucial metric is the score as it indicates the agent's effort to progress along the waypoints. Additionally, the success ratio provides an overall measurement of the agent's performance in achieving the objective. The violation comprises the sum of crash and out-of-road situations.\nIn the case of SAC, it exhibited minimal learning in the environment, with no instance of reaching the destination. Incorporating expert demonstrations into SAC resulted in an improved driving score. However, the agent still encountered difficulties in reaching the final goal. For safe RL algorithms, CPO exhibited superior performance. However, when applying the safe CoR, OffTRC outperforms all algorithms. The implementation of OffTRC with the framework yielded a remarkable 29.6% enhancement in the driving score with a 61.9% reduction in violations. Notably, the number of crashes decreased significantly. In the case of CPO, there was a 4% improvement in the score along with a 14.5% decrease in violations. For SDAC, the application of the framework led to lower performance. However, considering the success ratios of both versions, it appears that SDAC faced challenges during training within the simulator.\nIn general, safe RL agents demonstrated competent performance, and when coupled with safe CoR, they exhibited improved performance compared to the original algorithms.\nIn this subsection, we conduct a comparative analysis of various safe RL algorithms and the effectiveness of the safe CoR framework applied to these algorithms, utilizing both the Jackal simulator and the real-world Jackal platform for our experiments. The outcomes from the Jackal simulator are depicted in Table III, and the results from the real-world Jackal platform are detailed in Table IV.\nThe data illustrates that the integration of the safe CoR framework with safe RL algorithms enhances performance, in terms of reducing constraint violations. Moreover, this"}, {"title": "VII. ABLATION STUDY", "content": "In our ablation study, we aim to quantitatively assess the differential impacts of the CoR term on the performance and safety metrics of the navigation task using SDAC [24]. For appropriate comparison, we included the results obtained when setting the risk level to 0.25. Furthermore, we assessed the influence of the proposed framework in comparison to the impact of log-likelihood probability, as in [20]. We employed TRPO [22] for the reward expert and TRC [10] for the safe expert.\nTable V illustrates the outcomes of applying our proposed framework to different components of the system. Implementing the framework exclusively within the reward function yields a positive effect on the overall score but adversely affects the cost sum. Conversely, when the framework is applied solely to the cost function, we observe enhancements in the score, cost sum, and constraint violations (CV). The outcomes derived from employing the log-likelihood probability demonstrate that the exclusive integration of a single type of expert does not exhibit significantly improved performance. When employing the reward expert, SDAC-TRPO, the agent encounters challenges in meeting the constraint threshold, despite exhibiting slightly enhanced performance in reward maximization. However, the utilization of the safe expert, SDAC-TRC, does not demonstrate improved performance in both reward maximization and safety metrics.\nThe most comprehensive benefits are observed when the CoR term is utilized as outlined in our proposed methodology. This approach results in optimal outcomes for safety metrics, surpassing the results of the other configurations. Although the score marginally decreases compared to its application solely in the reward function, this reduction is a deliberate trade-off to achieve lower constraint violations and cost sums. This strategy underscores the inherent balance between optimizing performance and enhancing safety within safe RL paradigms."}, {"title": "VIII. CONCLUSION", "content": "In this paper, the safe CoR framework is introduced as an innovative solution to the critical challenges of ensuring safety and reliability in the complex and dynamic environments encountered by autonomous agents. By ingeniously combining reward-focused and safety-oriented expert demonstrations, the safe CoR framework significantly has advanced the field of safe reinforcement learning (safe RL). Our empirical investigations across a variety of environments, including safety gym, metadrive, and the real-world Jackal platform, have demonstrated the framework's remarkable ability to enhance algorithmic performance, while concurrently reducing constraint violations. These results not only underscore the efficacy of the safe CoR framework in balancing performance with safety constraints but also highlight its capability to enhance the domain of autonomous agents and beyond. The contributions of this work, particularly the validation of our framework's superiority in real-world scenarios and its robust applicability across diverse environments, suggest the promising potential for advancements in the development of safe and reliable autonomous agents."}]}