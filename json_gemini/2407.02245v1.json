{"title": "Safe CoR: A Dual-Expert Approach to Integrating Imitation Learning\nand Safe Reinforcement Learning Using Constraint Rewards", "authors": ["Hyeokjin Kwon", "Gunmin Lee", "Junseo Lee", "Songhwai Oh"], "abstract": "In the realm of autonomous agents, ensuring\nsafety and reliability in complex and dynamic environments\nremains a paramount challenge. Safe reinforcement learning\naddresses these concerns by introducing safety constraints, but\nstill faces challenges in navigating intricate environments such\nas complex driving situations. To overcome these challenges,\nwe present the safe constraint reward (Safe CoR) framework,\na novel method that utilizes two types of expert demonstra-tions-reward\nexpert demonstrations focusing on performance\noptimization and safe expert demonstrations prioritizing safety.\nBy exploiting a constraint reward (CoR), our framework guides\nthe agent to balance performance goals of reward sum with\nsafety constraints. We test the proposed framework in diverse\nenvironments, including the safety gym, metadrive, and the\nreal-world Jackal platform. Our proposed framework enhances\nthe performance of algorithms by 39% and reduces constraint\nviolations by 88% on the real-world Jackal platform, demon-strating\nthe framework's efficacy. Through this innovative\napproach, we expect significant advancements in real-world\nperformance, leading to transformative effects in the realm of\nsafe and reliable autonomous agents.", "sections": [{"title": "I. INTRODUCTION", "content": "The advance of autonomous driving technology promises\nto revolutionize the way people commute, offering safer,\nmore efficient, and accessible transportation options. At the\nheart of this transformative potential is the importance of\nensuring the safety and reliability of autonomous vehicles\nin diverse and dynamic driving environments. To achieve\nthis, many researchers and engineers have proposed algo-rithms\nsuch as rule-based controllers [1], [2] and imitation\nlearning methods [3]\u2013[5]. Rule-based controllers provide\na structured approach to decision-making based on prede-fined\nrules and conditions, while imitation learning allows\nthe agents to mimic human driving behaviors by learning\nfrom vast amounts of driving data. However, these methods\nface significant challenges in handling situations that fall\nbeyond predefined rules [6]. These scenarios, which are\nneither encapsulated within the training data nor foreseen\nin the predefined rule sets, pose critical hurdles in achieving\nthe comprehensive coverage and reliability that autonomous\ndriving aspires to achieve.\nTo address the limitations inherent in imitation learning\nand rule-based controllers, reinforcement learning (RL) [7],\n[8] has emerged as a compelling alternative. Unlike its\npredecessors, RL enables autonomous driving agents to learn\noptimal behaviors through trial and error, interacting directly\nwith their environment. This method offers significant advan-tages,\nsuch as the ability to continuously improve and adapt\nto new situations over time, potentially covering the gaps\nleft by imitation learning and rule-based systems. Although\nRL excels in adaptability and decision-making in complex\nscenarios, ensuring the safety of autonomous driving agents\nremains a critical challenge. However, the exploratory nature\nof RL, which often requires agents to make mistakes to learn,\nposes a significant risk in real-world driving contexts where\nsafety is crucial. This fundamental concern highlights the\nneed for innovative approaches within RL frameworks to\nbalance exploration with the stringent safety requirements\nof autonomous driving.\nTo address the aforementioned issue, the concept of safe\nreinforcement learning (safe RL) [9], [10] has been intro-duced.\nThis approach aims to incorporate safety constraints\ninto the optimization process explicitly. By taking account of\nsafety constraints into the policy optimization process, safe\nRL methods enhance the agent's ability to adhere to safety\nconstraints, thereby improving safety during both the training\nphase and the final deployment. For instance, incorporating a\nlane-keeping reward directly into the reward function results\nin mediocre lane-keeping behavior. On the other hand, when\nthe lane-keeping component is applied as a constraint within\nthe safe RL framework, the agent demonstrates significantly\nimproved lane-keeping performance. Despite these advance-ments,\nchallenges persist in the application of safe RL\nalgorithms for training agents to navigate complex driving\nenvironments safely.\nTo overcome these challenges, we propose a novel method\ncalled safe CoR, which innovatively combines two distinct\ntypes of expert demonstrations to refine existing safe RL\nalgorithms. The first type, termed reward expert demonstra-tions,\nfocuses exclusively on maximizing rewards without\nconsidering safety constraints. Conversely, the second type,\nsafe expert demonstrations, prioritizes adherence to safety\nrequirements above all, with subsequent consideration for\nreward maximization. By distinctly categorizing these ex-perts-reward\nexperts for their focus on performance opti-mization\nand safe experts for their dual focus on safety and\nreward maximization-we are able to calculate a constraint\nreward (CoR). This term aids in the update process, directing\nthe agent to emulate the reward expert for maximizing\nrewards while using the safe expert as a regularizer to ensure\nconstraint satisfaction. Through the strategic application of\nCoR, our method guides the agent toward reducing constraint\nviolations (CV) while still achieving high levels of reward, il-"}, {"title": "II. RELATED WORK", "content": "A. Imitation learning\nImitation learning is one of the main approaches in achiev-ing\nautonomous driving agents. It is a method that guides\nagents to imitate the given demonstrations extracted from\nexperts. One of the simplest approaches to imitation learning\nis behavior cloning (BC)", "13": "n[14", "15": ".", "16": "proposes another way to solve the\nproblem of designing an autonomous agent", "al.\n[17": "proposed an algorithm that integrates IRL and RL", "4": [18], "19": ".", "20": "exper-imentally\ndemonstrated that expert demonstrations can as-sist\nagents in navigating challenging environments robustly.\nDespite these advancements", "methods": "Lagrangian-based and\ntrust-region-based methods.\nLagrangian-based method transforms the original safe RL\nproblem into its dual problem. Ray et al. [12", "21": "proposed the worst-case soft\nactor-critic (WCSAC), which relaxes constrained problems to\nunconstrained ones using Lagrangian multipliers. However,\nsuch algorithms suffer from being overly conservative in\ntheir updates when constraint violations occur excessively\nduring the initial learning stage. Additionally, the usage of\nLagrangian multipliers makes the learning process unstable.\nTrust-region-based method is an extended version of trust\nregion policy optimization [22", "9": "introduced constrained policy optimization (CPO)"}]}