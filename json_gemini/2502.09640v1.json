{"title": "Online Social Support Detection in Spanish Social Media Texts", "authors": ["Moein Shahiki Tasha", "Luis Ramos", "Zahra Ahani", "Ra\u00fal Monroy", "Olga kolesnikova", "Hiram Calvo", "Grigori Sidorov"], "abstract": "The advent of social media has transformed communication, enabling individuals to share their experiences, seek support, and participate in diverse discussions. While extensive research has focused on identifying harmful content like hate speech, the recognition and promotion of positive and supportive interactions remain largely unexplored. This study proposes an innovative approach to detecting online social support in Spanish-language social media texts. We introduce the first annotated dataset specifically created for this task, comprising 3,189 YouTube comments classified as supportive or non-supportive. To address data imbalance, we employed GPT-4o to generate paraphrased comments and create a balanced dataset. We then evaluated social support classification using traditional machine learning models, deep learning architectures, and transformer-based models, including GPT-4o, but only on the unbalanced dataset. Subsequently, we utilized a transformer model to compare the performance between the balanced and unbalanced datasets. Our findings indicate that the balanced dataset yielded improved results for Task 2 (Individual and Group) and Task 3 (Nation, Other, LGBTQ, Black Community, Women, Religion), whereas GPT-4o performed best for Task 1 (Social Support and Non-Support). This study highlights the significance of fostering a supportive online environment and lays the groundwork for future research in automated social support detection.", "sections": [{"title": "1. Introduction", "content": "Social support is usually conceptualized as an emotional, intangible and tangible aid procured from one's social connections, whereby the person feels loved, cared for, respected, and valued. A plethora of research has established the idea that social support is beneficial and greatly enhances both psychological and physical well-being, acting as an important factor of protection (Xia et al., 2012), significantly reducing the risk of mortality, with an impact comparable to factors such as obesity or inactivity (Kent de Grey et al., 2018).\nSocial support is positively related to psychological and physical health (Bellinia et al., 2019). Recent studies rightly point out that its functions are vast especially as highly developed social networks can reduce risks and help solve major medical problems (Kent de Grey et al., 2018). Additionally, social support has been associated with fewer symptoms of depression, anxiety, and stress (Baeza-Rivera et al., 2022).\nFor the last few years, social media has rapidly grown as an aid to help establish good relations and dialogue among family, friends and other acquaintances. Nonetheless, this growth has not been without some disadvantages. Its unregulated nature sometimes causes online environment that is hostile. Frequently, they are used for spreading hate speech and posting offensive comments which have varied negative effects on the society (Abdelsamie et al., 2024). Thus, it is necessary to design strategies that will help notice such kinds of content with a view for creating safer digital spaces as well as promoting decent conduct over the internet.\nDespite the impact it can have on people's well-being, the promotion of positive and supportive content has not received much attention in this area of research. In response to these challenges, our proposed approach offers a disjunct but underexplored strategy for combating negativity on social media platforms by promoting support comments. Instead of focusing efforts on detecting and filtering negative content, our approach seeks to cultivate a more positive and supportive online environment by encouraging users to provide emotional comfort, encouragement, and advice to those facing challenges.\nOnline social support refers to the emotional aid and consolation given via digital platforms such as social media. This kind of support is very important for individuals and groups that interact on social networks and face different challenges like victims of war, marginalized communities, minority populations etc. Digital platforms make it possible for users to connect with others who share similar experiences. That way they can get necessary resources, help, empathy, courage or encouragement they need. Online support networks are usually vital because they allow for anonymity and accessibility to people who do not have access to conventional supportive systems. They also enable real time participation which encourages a sense of belongingness, hence fostering psychological well-being by minimizing negativity effects towards the user.\nThe current study is based on the influence of social support on individual's health. YouTube was utilized to collect data starting from the search for some potential videos that could"}, {"title": "2. Definitions", "content": "(Xia et al., 2012) expose a definition of social support as mental and material support obtained from the social network, making one feel that he is cared for, loved, valued and appreciated. The definition of social support has broadened to include the virtual assistance and connections that individuals form online, commonly referred to as online social support (Ma et al., 2024), that involves behaviours, communication, and interactions that demonstrate care and appreciation for individuals, thus fostering a sense of belonging and helping to cope with life's challenges (Kolesnikova et al., 2025).\nThis study aligns with the previous definition of social support and explores the exchange of comments between users and audiences as a form of social support occurring within social media."}, {"title": "3. Related work", "content": "Given the importance of promoting positive and supportive discourse, research on this topic remains relatively scarce. This concept serves as a counterpart to hate speech. However, there is no directly comparable research specifically focused on the Spanish-language support speech in social media using NLP techniques.\nSocial support has been studied recently using diverse NLP techniques. (Ahani et al., 2024) proposed the detection of supportive speech on social media using NLP techniques and ML and DL models. They demonstrate that the integration of psycholinguistic, emotional and feeling characteristics with n-grams can detect social support, and it is also possible to distinguish whether it is directed at an individual or a group. The best results obtained for the different binary and multiclass tasks in all experiments range from 0.72 to 0.82. (Kolesnikova et al., 2025) also proposed the use of LLM, on the same dataset as in previous research, applying Zero-Shot learning to models such as GPT-3, GPT-4, and GPT-4-turbo, Transformer models available on Hugging Face's website were also used, with ROBERTa-base consistently outperforming others by improving previous metrics by up to 8%.\n(Zou et al., 2024) offer a comprehensive description regarding the self-disclosure processes and social support skills on online platforms, focusing on women suffering from infertility on Reddit. Their study combines several theories, such as Communication Privacy Management Theory, Functional Theory of Self Disclsoure, Social Support Theory, and Social Penetration Theory. These theories assist in assessing the consequences of personal disclosures on the amount and form of assistance offered in social media. Employing NLP techniques, the authors evaluate a corpus of Reddit posts and comments spanning across three years' worth of data. Precise text classification regarding self-disclosure and social support types is performed utilizing the BERT model. The model is shown to be effective with self-disclosure and social support through precision, recall, and F1 scores.\n(Erculj et al., 2019) proposed a text-mining approach to detect automatically discussion topics in the largest infertility forum in Slovenia and identify themes of social support types among patients coping with infertility. The study focused on an infertility forum where 13,2374 posts were made between 2002 and 2016. Topics of discussion were identified through the LDA method. The findings are suggestive that online forums, like health-related online groups, can offer critical support for patients with infertility issues, thus confirming hypotheses from earlier research. The results illustrate the efficacy of text-mining to understand and analyse online social support behaviour and help in communication within the health care system. Later, (Erculj and Pavs ic\u0103 Mrevlje, 2023) tested LDA, but in this study it was to analyze the women in need. The study suggests that increasing user engagement and possibly integrating more structured support mechanisms could enhance the effectiveness of online support communities for women in need."}, {"title": "4. Dataset development", "content": "Our study analyzes a corpus of YouTube comments aimed at Spanish-speaking audiences on videos covering diverse topics, including but not limited to, nationality, the Black community, women, religion, and LGBTQ+ issues. The videos chosen were those that garnered significant support, such as content related to the Olympic Games or those focused on social issues related to race, gender, and sexual orientation. The dataset consists of 3,189 comments, having been cleaned of duplicate comments and those not in Spanish.\nIn this regard, it must be pointed out that no other filtering or selection process was performed over the comments associated with the videos selected. This strategy made it possible to analyse issues pertaining to the articulation of support, but still prevailing in the comments focused on the actual quantitative distribution in the videos."}, {"title": "4.2. Annotator selection", "content": "For the selection of annotators, three male native Spanish-speaking candidates were recruited, two of whom were pursuing master's degrees in computer science. To ensure consistency and accuracy in the annotation process, each annotator was initially provided with a set of 100 sample tweets along with a comprehensive guide outlining the annotation protocols. This allowed the annotators to become familiar with the task before data generation and collection began.\nFollowing this, the labeled samples from the first two annotators were thoroughly reviewed and analyzed. To address any discrepancies or challenges, individual meetings and interviews were held with each annotator. During these discussions, the annotators provided insights into the issues and conflicts they encountered while labeling the data, which helped refine the annotation process.\nRecognizing the need for a highly experienced annotator to maintain the quality of the annotation, the authors selected a third annotator. This third annotator was one of the authors of the paper, a PhD student specializing in Natural Language Processing. As a native Spanish speaker with extensive knowledge of the subject matter, she contributed her expertise to ensure the consistency and accuracy of the annotation process. Her involvement played a crucial role in finalizing the labeled dataset."}, {"title": "4.3. Annotation guidelines", "content": "The Social Support detection task was structured as a three-step classification process. First, supportive comments were identified. Next, it was determined whether these supportive comments were directed toward an individual, a group, or a community. Finally, if the supportive comment was identified as being directed toward a group, the specific group was further identified. The guidelines for this process are described below.\n\u2022 Subtask 1 Binary social support detection: In this subtask, a given text is classified as supportive or non-supportive:\nSocial Support (label = SS): Statements of support promote understanding, empathy and positive actions. Therefore, a supportive comment is a statement or message that offers support, encouragement, admiration, advocacy, promotion, assistance, or defence. These comments are intended to provide emotional support, raise morale, recognize the achievements, or labour of others.\nNot Social Support (label = NSS): The text does not convey any form of support as specified by the previous definition.\n\u2022 Subtask 2 - Individual vs. Group: In this subtask, each supportive comment identified in the previouse subtask is categorized as either individual support or group support.\nIndividual: If the text expresses support for a specific person or individual (e.g., Alan Turing, Nikola Tesla, Donald Trump, Steve Jobs, etc.), it is labelled as Individual.\nGroup: If the text expresses support for a group of people, community, nation, etc. (e.g., Christians, Black community, LGBTQ or Other), it is labelled as Group.\n\u2022 Subtask 3 - Multiclass SS for Groups: In this subtask, the aim is to identify which community or group of people receive social support by classifying the group support comments identified in subtask 2 into the following categories:\nNation: This category includes texts that express support for a specific country, its people, or its sovereignty. Examples might include advocating for the rights of a nation, showing solidarity upon the occurrence of a national crisis, or celebrating a country's achievements. For instance, \"Sending love and strength to Ukraine during these challenging times\" would fall under this category.\nBlack community: This type of support acknowledges and uplifts the Black community, often focusing on racial equality, social justice, or celebrating Black cultural contributions. For instance: Honoring the resilience and achievements of the Black community-Black Lives Matter.\nLGBTQ: Messages in this classification aim to uplift the LGBTQ community by promoting equality, celebrating diversity, or supporting their rights and needs of representation. An example would be: Love knows no boundaries\u2014proud to stand with the LGBTQ community this Pride Month.\nReligion: Support in this category relates to specific religions or the general rights of religious communities, often emphasizing respect, solidarity, or advocacy against discrimination. For example: We stand with people of all faiths to ensure freedom of religion for everyone."}, {"title": "4.4. Annotation procedure", "content": "The annotation process provided detailed guidelines and sample data to the three selected annotators to efficiently create the proposed dataset. In this sense, the annotators followed a structured process, as illustrated in Figure 1. This process begins by determining whether the comments expressed support in terms of concern, advocacy, happiness, or care. If any was detected, then it is labelled as Support. The second level of analysis consists in distinguishing whether the identified support content was directed to an individual or a group. In the case of a group, the annotators also specified the affiliation of the group, such as Nation, Religion, Black Community, Women, LGBTQ or Other. On the contrary, if the comment did not show support, the annotators marked it as Non-Support. This process ensures the quality of the labels and data set."}, {"title": "4.5. Inter-annotator agreement", "content": "Inter-annotator agreement (IAA) assesses how much annotators agree, factoring in chance agreement. Cohen's Kappa Coefficient scores of 0.84 for subtask 1, 0.78 for subtask 2, and 0.62 for subtask 3 demonstrate the robustness of the datasets, reflecting the rigorous annotation process."}, {"title": "4.6. Statistics of the dataset", "content": "Table 1 presents the dataset statistics, highlighting the variation for each subtask. In subtask 1, the proportion of Non Support samples greatly exceeds the proportion of Support samples, suggesting that there are few people who are willing to show support on YouTube comments. Subtask 2 shows that there is a greater propensity to support groups rather than individuals. Subtask 3 reveals which trend topics have generated the most supportive comments. It can be seen that LGBTQ people have recieved more support than other topics, suggesting that this issue has triggered many supportive comments in the Spanish-speaking community; while the level of support for LGBTQ is considerably lower in other topics, this behaviour suggests that these topics depend on various factors such as social impact generated, affinity, and empathy. This behaviour can still be studied in depth to determine the origin of support."}, {"title": "5. Model training and Evalution", "content": "Before training, the dataset underwent preprocessing, including text cleaning, tokenization, and normalization, to improve model performance. To ensure a fair evaluation, we applied k-fold cross-validation, allowing for a robust assessment of both traditional and advanced machine learning models.\nTraditional models were trained using standard feature extraction techniques, while advanced models leveraged deep learning architectures and pre-trained transformers to enhance text representation. We computed precision, recall, and F1-score using both weighted and macro averaging. Macro F1 treats all classes equally by averaging their individual F1-scores, making it suitable for balanced datasets. In contrast, the Weighted F1-score addresses class imbalance by assigning more weight to classes with more instances, meaning that larger classes have a greater influence on the final score. However, we selected the macro F1-score as the primary evaluation metric because it offers a fair assessment of performance across all classes, giving equal importance to each class regardless of its frequency.\nThe formulas for Precision, Recall, and F1-score are as follows (Derczynski, 2016):\nPrecision = $\\frac{TP}{TP + FP}$\nRecall = $\\frac{TP}{TP + FN}$\nF1-score = 2\u00d7$\\frac{Precision X Recall}{Precision + Recall}$\nMacro F1-score calculates the F1-score for each class separately and then averages them:\nMacro-F1 = $\\frac{1}{N} \\sum_{i=1}^{N} F1-score_i$\nwhere N is the number of classes.\nWeighted F1-score takes class imbalance into account by weighting each class's F1-score based on its support:\nWeighted-F1 = $\\sum_{i=1} \\frac{support_i}{total instances} \\times F1-score_i$"}, {"title": "5.1. K-Fold Cross-Validation", "content": "K-fold cross-validation (CV) is a technique used to evaluate machine learning models (MLMs) by dividing the dataset into k folds. In each iteration, one fold serves as the test data, while the remaining folds are used for training. This process is repeated until the entire dataset has been tested. The results are typically averaged to calculate the mean score of the MLM.\nIn this study, we selected k=5 and compared the classification performance of both advanced and traditional machine learning models using their respective classification reports (Nti et al., 2021)."}, {"title": "5.2. Traditional machine learning models", "content": "For our classification task, we selected five traditional machine learning models to ensure a comprehensive evaluation of different algorithms. These models include Logistic Regression (LR), Support Vector Machine (SVM) with both radial basis function (RBF) and linear kernels, XGBoost, and Random Forest. These models are well-established in the field of text classification, known for their effectiveness in handling a variety of tasks (Ahani et al., 2024). To represent the text data effectively for these models, we used TF-IDF (Term Frequency-Inverse Document Frequency) as the feature extraction technique. TF-IDF helps to capture the importance of words in relation to the entire dataset, by weighing terms based on their frequency within a document and across the corpus (Roelleke and Wang, 2008)."}, {"title": "5.3. Deep learning", "content": "For our text classification task, we utilized deep learning models with different word embeddings to enhance performance. We implemented CNN and BiLSTM architectures with both GloVe and FastText embeddings, allowing the models to capture contextual meaning and semantic relationships effectively (Kolesnikova et al., 2025)."}, {"title": "5.4. Transformers", "content": "Transformers are deep learning models widely used in NLP tasks like translation, summarization, and text generation. They employ self-attention mechanisms to capture long-range dependencies, allowing efficient text processing.\nFor our text classification task, we utilized the Hugging Face Transformers library, leveraging pre-trained models such as XLM-ROBERTa, RoBERTuito, BERT, and DistilBERT to enhance accuracy and efficiency (Kolesnikova et al., 2025)."}, {"title": "5.5. GPT", "content": "GPT-4o is a powerful Transformer-based model pre-trained on vast text data. It excels in NLP tasks like text classification, sentiment analysis, and text generation.\nPre-training: The model learns language patterns by predicting words in large datasets without labeled data. Fine-tuning: It is further trained on specific datasets, like our support dataset, to enhance classification accuracy. Classification: GPT-4o analyzes text contextually, assigning relevant labels based on learned patterns. Contextual Understanding: Using attention mechanisms, it ensures precise and relevant classifications (Imamguluyev, 2023)."}, {"title": "5.6. Balanced Data Set", "content": "Initially, the dataset was divided into training and test sets, with 20% allocated for testing. To address class imbalance in the training data, we applied an oversampling technique using the GPT-4-O model. This model generated paraphrased versions of comments to augment the underrepresented classes, ensuring a more balanced distribution."}, {"title": "6. Results", "content": ""}, {"title": "6.1. Traditional machine learning", "content": "Table 3 presents the performance of various traditional machine learning models for Subtask 1. The results are reported in terms of weighted and macro scores for precision, recall, and F1-score, along with accuracy.\nAmong the models, the linear SVM achieved the highest weighted F1-score of 0.8445 and the best accuracy of 86.04%. It also demonstrated strong weighted precision and recall, highlighting its overall reliability in this task.\nThe XGBoost and Random Forest classifiers produced comparable results, with XGBoost slightly outperforming Random Forest in weighted recall (0.8464 vs. 0.8511) but achieving a marginally lower macro F1-score. Both models performed better than Logistic Regression (LR) and SVM with RBF kernels in terms of macro-level precision and recall.\nInterestingly, while Logistic Regression and SVM (RBF) delivered similar weighted F1-scores (0.8298 and 0.8311, respectively), their macro scores were slightly lower, indicating potential challenges in handling imbalanced class distributions compared to other models.\nOverall, XGBoost emerges as the most effective model, demonstrating superior performance with a Macro F1 score of 0.7577, highlighting its robustness for the classification task in Subtask 1"}, {"title": "6.2. Deep learning", "content": "Table 6 presents the performance of various deep learning models for Subtask 1. These models were evaluated using weighted and macro precision, recall, and F1-scores, as well as overall accuracy.\nThe BiLSTM (GloVe embeddings) achieved the highest weighted F1-score of 0.8273 and an accuracy of 83.62%, demonstrating its effectiveness in capturing contextual relationships in text. Its macro F1-score of 0.7314 further highlights its ability to balance predictions across all classes, outperforming the other models in this metric.\nThe BiLSTM (FastText embeddings) showed comparable performance, with a weighted F1-score of 0.8267 and an accuracy of 83.72%. While slightly behind the GloVe-based BiLSTM in macro F1-score (0.7281 vs. 0.7314), it demonstrated strong consistency across metrics, making it a reliable alternative.\nAmong the convolutional models, the CNN (FastText embeddings) outperformed its GloVe counterpart, achieving a weighted F1-score of 0.8203 and an accuracy of 83.91%. Its macro precision (0.7799) and recall (0.6936) suggest it effectively leverages word-level representations provided by FastText.\nThe CNN (GloVe embeddings), while achieving the lowest macro F1-score (0.5964) and weighted F1-score (0.7643), still demonstrated reasonable accuracy (80.69%). This indicates that while CNNs may excel at identifying local patterns, they may not capture global dependencies as effectively as BiLSTM models.\nOverall, the BiLSTM models, particularly with GloVe embeddings, emerged as the most effective for Subtask 1, emphasizing the importance of sequential modeling and rich word embeddings in text classification tasks."}, {"title": "6.3. Transformers", "content": "Table 9 shows the performance of various transformer-based models in Subtask 1, providing their weighted and macro scores for precision, recall, F1-score, and accuracy.\nThe model pysentimiento/robertuito-sentiment-analysis stands out with the highest performance in this subtask. It achieved a weighted F1-score of 0.8673 and an accuracy of 87.27%, significantly outpacing the others. Its macro F1-score of 0.7956 further highlights its ability to maintain strong performance across all classes, reflecting its robust handling of the task.\nAnother strong contender is papluca/xlm-roberta-base-language-detection, which achieved a weighted F1-score of 0.8386 and an accuracy of 84.60%. This model is also highly effective in balancing precision and recall, with macro precision and recall values of 0.7837 and 0.7482, respectively. Its performance is highly competitive, though slightly behind the robertuito-sentiment-analysis model.\nThe nlptown/bert-base-multilingual-uncased-sentiment model also shows strong results, achieving a weighted F1-score of 0.8416 and an accuracy of 85.29%. Its macro Fl-score is 0.7502, demonstrating a solid balance between precision and recall, positioning it as another strong performer.\nThe lxyuan/distilbert-base-multilingual-cased-sentiments-student model comes next with a weighted F1-score of 0.8329 and accuracy of 84.35%. While it trails the other transformers in terms of F1-score, it maintains a consistent performance across the task with a macro F1-score of 0.7372.\nLastly, papluca/xlm-roberta-base-language-detection (second instance) shows a very similar performance to its first instance, achieving a weighted F1-score of 0.8415 and an accuracy of 84.95%. This indicates that the XLM-ROBERTa model consistently delivers competitive results across different runs.\nIn summary, pysentimiento/robertuito-sentiment-analysis leads in terms of both weighted and macro scores, making it the top-performing transformer model for Subtask 1."}, {"title": "6.4. GPT", "content": "Table 12 shows the performance of the GPT4-0 model across three subtasks, providing both weighted and macro scores for precision, recall, F1-score, and accuracy.\nSubtask 1: The GPT4-o model performs exceptionally well with a weighted F1-score of 0.9006 and an accuracy of 89.97%. The model demonstrates a solid balance between precision and recall with macro F1-scores of 0.8531, making it a strong performer for this subtask.\nSubtask 2: For Subtask 2, GPT4-o achieves a weighted F1-score of 0.6322 and accuracy of 61.27%. The macro F1-score (0.6051) indicates some challenges in this task, particularly in terms of recall, where it drops to 0.6127. This suggests that GPT4-o may have more difficulty distinguishing certain classes in this subtask.\nSubtask 3: The model performs strongly in Subtask 3, with a weighted F1-score of 0.8472 and an accuracy of 84.45%. The macro F1-score of 0.7903 reflects a good balance of precision and recall, showing that the model handles this task well.\nIn summary, GPT4-o excels in Subtasks 1 and 3, with excellent precision, recall, and accuracy. However, in Subtask 2, the model's performance drops, particularly in recall. Nonetheless, it remains a strong model overall across the subtasks."}, {"title": "6.5. Balanced Data set", "content": "Table 13 presents the performance metrics of the robertuito-sentiment-analysis model across three tasks, using a balanced dataset. The evaluation includes weighted and macro scores for precision, recall, F1-score, and overall accuracy."}, {"title": "6.6. Best performance", "content": "According to the results of all models evaluated in this Study, the models presented in Table 14 demonstrate the best performance compared to others, as measured by macro F1 scores across different tasks. Specifically, the balanced dataset approach achieved the highest F1-scores for SubTask 2 and SubTask 3, while the GPT4-o model showed strong performance for SubTask 1. It is important to note that the balanced dataset experiment was also conducted with the obertuito-sentiment-analysis model, further enhancing the overall performance."}, {"title": "7. Error analysis", "content": "The table (15) presents the precision, recall, and F1-score for different labels across various tasks. For SubTask 1, the GPT4-O model achieved strong performance, particularly for the NSS label with an F1-score of 0.9358, while SS had a slightly lower F1-score of 0.7704. In SubTask 2, the balanced dataset showed excellent performance, with Group achieving an F1-score of 0.9455 and Individual scoring 0.8333. For SubTask 3, the balanced dataset also produced high F1-scores across different labels, including LGBTQ (0.9608) and Women (0.8421). Notably, the Nation label had an F1-score of 0.7143, reflecting some challenges. To better understand the model's performance, confusion matrix figures 2, 3, and 4 for the three tasks are provided, illustrating how the models performed across the different labels."}, {"title": "8. Discussion", "content": "Our findings underscore the effectiveness of NLP techniques in identifying social support within online discussions, showcasing their potential to foster a more positive digital environment. A comparison of machine learning, deep learning, and transformer-based models reveals that transformers, particularly pysentimiento/robertuito-sentiment-analysis, consistently outperform other approaches in Subtask2 and Subtask3 on a balanced dataset. For Task1, GPT-4o achieved the best performance on an unbalanced dataset. This suggests that contextual embeddings and large-scale pretraining play a critical role in enhancing classification accuracy for social support detection.\nThe use of GPT-4o to create a balanced dataset through paraphrasing led to significant improvements in model performance. By addressing class imbalances, the balanced dataset improved recall for minority classes. However, challenges remain in distinguishing between subtly supportive messages and neutral or ambiguous comments. Future research should focus on developing more refined annotation guidelines and advanced feature engineering techniques to improve classification granularity."}, {"title": "9. Conclusion and future work", "content": "This study presents the first comprehensive approach to detecting online social support in Spanish social media texts. Through a newly annotated dataset and extensive experimentation with various machine learning and deep learning models, we demonstrate the feasibility of automatic social support classification. Future research should focus on expanding the dataset with more diverse sources and refining the annotation process to capture a broader range of supportive expressions. Additionally, integrating real-time support detection into social media platforms could help identify and promote positive interactions, ultimately contributing to a more supportive online ecosystem. Another promising direction is the exploration of multimodal approaches that incorporate text, images, and video content to enhance the understanding of social support dynamics in online spaces."}, {"title": "10. Limitations", "content": "Despite the promising results, this study has several limitations. First, the dataset is limited to YouTube comments, which may not fully represent supportive communication across different social media platforms. Expanding the dataset to include platforms like Twitter, Facebook, and Reddit could improve the generalizability of the models. Second, the annotation process, while rigorous, is subject to human interpretation. The complexity and subtlety of social support expressions might lead to inconsistencies in labeling, which could impact model performance. Future studies should incorporate more annotators and develop clearer guidelines to reduce subjectivity. Third, while transformers demonstrated superior performance, their computational cost is significantly higher compared to traditional machine learning models. This limitation poses challenges for real-time deployment, particularly in resource-constrained environments. Future work should explore optimization techniques, such as model distillation, to reduce inference time without compromising accuracy. Finally, cultural and linguistic variations in social support expressions were not extensively analyzed in this study. A cross-cultural comparison of social support detection could provide deeper insights into how support is expressed differently across languages and communities."}, {"title": "Declarations", "content": ""}, {"title": "Funding", "content": "The work was done with partial support from the Mexican Government through the grant A1-S-47854 of CONACYT, Mexico, grants 20241816, 20241819, and 20240951 of the Secretar\u00eda de Investigaci\u00f3n y Posgrado of the Instituto Polit\u00e9cnico Nacional, Mexico. The authors thank the CONACYT for the computing resources brought to them through the Plataforma de Aprendizaje Profundo para Tecnolog\u00edas del Lenguaje of the Laboratorio de Superc\u00f3mputo of the INAOE, Mexico and acknowledge the support of Microsoft through the Microsoft Latin America PhD Award."}, {"title": "Conflict of Interest", "content": "I declare that the authors have no competing interests as defined by Nature Research, or other interests that might be perceived to influence the results and/or discussion reported in this paper."}, {"title": "Availability of data and materials", "content": "The dataset utilized in this study can be obtained upon request from the corresponding author. Please reach out to Moein Shahiki Tash at [mshahikit2022@cic.ipn.mx]."}]}