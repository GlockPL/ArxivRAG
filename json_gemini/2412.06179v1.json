{"title": "Annotations for Exploring Food Tweets From Multiple Aspects", "authors": ["Mat\u012bss Rikters", "Edison Marrese-Taylor", "Rinalds V\u012bksna"], "abstract": "This research builds upon the Latvian Twitter Eater Corpus (LTEC), which is focused on the narrow domain of tweets related to food, drinks, eating and drinking. LTEC has been collected for more than 12 years and reaching almost 3 million tweets with the basic information as well as extended automatically and manually annotated metadata. In this paper we supplement the LTEC with manually annotated subsets of evaluation data for machine translation, named entity recognition, timeline-balanced sentiment analysis, and text-image relation classification. We experiment with each of the data sets using baseline models and highlight future challenges for various modelling approaches.", "sections": [{"title": "1. Introduction", "content": "Despite the recently induced chaos due to the company leadership change, Twitter (now renamed to X\u00b9) has long been and still remains one of the most influential social networks not only around political or technological topics, but also for everyday lifestyle content and regular people posting about their daily lives. For years Twitter was also one of the only remaining useful social media platforms to the research community by providing real-time access to new posts with additional elevated access for academic research purposes at no cost\u00b2. This, however, also became deprecated in fall of 2023\u00b3 with no reasonable alternative so far, aside from the $5000.00 USD/month Pro tier.\nThe Latvian Twitter Eater Corpus (LTEC (Spro\u0123is and Rikters, 2020)) is a collection of tweets gathered by following the appearance of 363 keywords related to food and eating inflected in various valid word forms in the Latvian language. Data collecting was started in 2011 and has reached over 2.5 million tweets generated by more than 170,000 users. Each tweet in the dataset is represented by standard data fields like tweet date, text and id, author screen name, as well as additional metadata fields where available, such as location details, attached media information, a list of food and drink products mentioned in the tweet text in original surface forms and nominative singular forms (with respective English translations), and a separate subset of tweets with manually annotated sentiment classes - positive, neutral and negative.\nIn this paper we describe supplementing the LTEC with several new manually annotated and task- specific evaluation sets for text-image relation classification, machine translation, named entity recognition and an additional set for sentiment analysis. We also provide results from baseline ex- periments using each of these evaluation sets"}, {"title": "2. Related Work", "content": "The LTEC has been analysed from several aspects of sensory experiences related to the food prod- ucts mentioned in the tweet content by K\u0101le et al. (2021), as well as comparing the change of senti- ment in food tweets over time. Most recently (K\u0101le and Rikters, 2023) linked the timeline of food tweet sentiment changes with weather observation data in the area for further insights. We believe that our additions to the dataset can further spark analyt- ical research of food-related tweet data from other aspects, such as the images related to tweets.\nVempala and Preo\u0163iuc-Pietro (2019) explore the re- lations between Tweet text and attached images and categorise them in four different groups: a) the image adds to the text meaning and the text is represented in the image; b) the image adds to the text meaning and the text is not represented in the image; c) the image does not add to the text meaning and the text is represented in the image; and d) the image does not add to the text meaning and the text is not represented in the image. They also further analyse user demographic traits linked to each of the four image tweeting types."}, {"title": "3. Manually Annotated Datasets", "content": "In this section, we discuss the newly created man- ually annotated parts of LTEC. For the machine translation (MT) and named entity recognition (NER) tasks, we manually translated and anno- tated the same test set of 744 tweets, which already had manually assigned sentiment classes. This test set, however, was far too poor in terms of attached images, so for the text-image relation task we sam- pled 800 recent tweets with image attachments. Finally, for the extended and more balanced sen- timent analysis test set, we sampled 50 random tweets from each year between 2011 and 2020, to- talling 500.\nThe number of annotators was task-specific, hav- ing 12 annotators for sentiment analysis, a total of 3 for NER and image-text relation, and 2 for trans- lation. With some of them overlapping between tasks, there were a total of 16 unique annotators. A more detailed annotator profile is provided in the appendix along with annotation environment descriptions and annotation instructions."}, {"title": "3.1. Text-image Relation", "content": "While not available from the very beginning of Twitter, the option to embed images directly into a tweet was added in 2011 (CNN, 2011) after Twit- Pic had been the main external source of attached images for several years. The popularity of posting images along with food-related tweets has shifted over the years between 5 and 20 per cent of total monthly tweets, as can be seen in Figure 2. We use the same annotation schema as Vempala and Preo\u0163iuc-Pietro (2019) by grouping image-text re- lations into whether the image adds to the text meaning and whether the text is represented in the image. We manually annotate a test set of 800 im- age - tweet pairs. In our annotations we found that the majority do indeed textually describe what is represented in the image, and about half of the cases the image also adds to the meaning of the text. Only around 9% add to the meaning without describing the contents in the text, and a mere 6% do neither.\nFor example, given the image shown in Figure 1 and its accompanying text from the tweet (and its English translation), it is fairly clear that the text does not at all describe what is shown in the im- age, since we can see a cat sitting on a chair in the image and it is not mentioned anywhere in the text. However, the image most certainly adds to the meaning of the text upon seeing the angry look of the cat it becomes clear that the text rep- resents its personified thoughts."}, {"title": "3.2. Machine Translation", "content": "Manual translation of the 744 tweet test set from the original Latvian language into English was per- formed by a single translator. After the translation the sentences were reviewed by a professional post- editor as an additional quality assurance measure. Such texts tend to be challenging to comprehend and convey by modern models due to the use of specific terminology and abbreviations, which at times are used to maintain posts within certain character count limitations of social networks."}, {"title": "3.3. Named Entity Recognition", "content": "The 744 tweet test set was annotated in the CONLL-2003 format (Tjong Kim Sang and De Meulder, 2003) with four entity classes for per- sons, locations, organisations and miscellaneous (PER, LOC, ORG, MISC), mostly following MUC- 7 guidelines (Chinchor, 1998). Annotations were performed with a reduced entity set similar to the Balanced State-of-the-Art Multilayer Corpus for natural language understanding (Gruzitis et al., 2018) by converting the nine entity tag set to a four entity tag set. We kept the PERSON and ORGA- NIZATION tags, joined the LOCATION and GPE tags into a single LOCATION tag, and joined the remaining tags into a single MISC category. The manual annotation process was carried out by 2 an- notators using the INCEPTION (Klie et al., 2018) tool, with a third annotator resolving conflicts. Co- hen's Kappa (as reported by the INCEPTION) be- tween two main annotators for the NER task was 0.92. The test set contains 188 MISC entities, 99 LOC entities, 55 PER entities, and 68 ORG enti- ties. We also introduced a new FOOD tag to be used for marking food and drink related entities."}, {"title": "3.4. Sentiment Analysis", "content": "The original sentiment analysis test set in LTEC was sampled from tweets of only one year while the whole corpus spans over a decade. While transla- tions and named entities are somewhat more inde- pendent from the specific time period, sentiment can be more time-sensitive. For a more trustwor- thy evaluation, we decided to randomly select 50 tweets per year from each year between 2011 and 2020 into a more balanced additional 500 tweet evaluation set. Twelve human evaluators were asked to individually judge the sentiment for each of the 500 tweets. We used the majority vote of the human evaluators as the final annotation in cases where they disagreed on a particular eval- uation and considered two classifications as cor- rect in the 21 cases where the majority opinion was split in half (for example, 6 positive and 6 neutral). The overall agreement of the evaluators was 70.48% with a free marginal kappa (Randolph, 2005) of 0.56 (values from 0.40 to 0.75 are consid- ered intermediate to good agreement). This shows that the tweet texts are not always trivial enough to be unequivocally classified into just one of the three sentiment classes."}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Text-image Relation", "content": "Many modern multi-modal large language models have recently gained the ability to answer ques- tions about a given image in plain text chat form. In the text-image relation experiments we feed our data to the LLaVA (Liu et al., 2023) model and prompt it to answer a question about the type of text-image relation using the two prompts outlined in Table 1. We experimented with providing the model with the original tweet texts written in Lat- vian, as well as automatically translated versions of tweet texts into English using Tilde MT6, since the LLaVA model is using Vicuna (Zheng et al., 2023) as its language model part, which is inherently not multilingual. Prediction accuracy was only 20.69% when evaluated on the original texts, and improved slightly to 27.83% on English translations. The re- sults in Table 2 show more detailed results, which outline how the model tends to mostly answer the first question positively at around 84-88% of the time, while it should be around 57%. Meanwhile, the model seems to be answering the second ques- tion overly negatively at around 56-67%, where the answer should be closer to 15%, since in the vast majority of annotated tweets the content of the text was indeed represented in the image."}, {"title": "4.2. Machine Translation", "content": "We evaluated several state of the art MT mod- els using the dataset and compared automatic evaluation according to BLEU (Papineni et al., 2002) and ChrF (Popovi\u0107, 2015) scores using sacre- BLEU (Post, 2018). We compare the translations from publicly available translation services such as Google Translate7 and Tilde MT8, and open source pre-trained models like Opus MT (Tiedemann and Thottingal, 2020) and mBART (Tang et al., 2021). The results summarised in Table 3 indicate that such social media data is not particularly chal- lenging to translate. While mBART does notice- ably fall behind the others three, BLEU scores in the range of 40 to 50 usually indicate fairly us- able translations. Upon manual inspection there was one very noticeable frequent error made by the pre-trained models which was absent in the output from translation services and is directly related to typical modern social media texts - partial or full absence of emoji and emoticons."}, {"title": "4.3. Named Entity Recognition", "content": "The task of named entity recognition (NER) has been proven more challenging for languages with less widely available data, and especially morpho- logically rich languages such as Latvian. Recognis- ing named entities (NEs) in texts from social media adds an extra layer to the challenge, as such texts are not necessarily the most grammatically correct, and even tend to use specific terminology or abbre- viations to maintain the content of the post within the limits of the specific social media network (for Twitter formally 140, later changed to 280 sym- bols).\nWe trained 2 NER models using (Gr\u016bz\u012btis et al., 2019) dataset: a fine-tuned multilingual BERT (mBERT (Devlin et al., 2019)) and fine-tuned mBERT additionally pre-trained on tweets and added emoticons vocabulary10 (Thakkar and Pin- nis, 2020) using the Flair library (Schweter and Akbik, 2020).\nThe results in Table 4 show that a language model additionally pre-trained on Latvian tweets performs better when applied to in-domain data in a downstream task. The overall $F_1$ score is lower than SOTA (88.1) (Ul\u010dar and Robnik- \u0160ikonja, 2020) on the Fullstack (Gr\u016bz\u012btis et al., 2019) dataset, however, the LitLat score is ob- tained performing the NER on only 3 classes: per-"}, {"title": "4.4. Sentiment Analysis", "content": "With using the manually annotated 5,420 tweet training set to fine-tune a pre-trained multilingual BERT (Devlin et al., 2019) model for the senti- ment analysis task along with ~20,000 sentiment- annotated Latvian tweets from other sources11, the 744 tweet test set provided in LTEC can reach an accuracy of around 74%. The accuracy of the model according to the majority of human evalu- ators on our timeline-balanced 500 tweet set was even higher, reaching 86.40%. Since our annota- tion was performed by 12 annotators who did not always agree, we also compared each of their anno- tations against the majority vote. The accuracy of the average human evaluator compared to the ma- jority was 80.25%, which does not necessarily in- dicate human parity of the model, but rather that the correct choice is not always easy to decide."}, {"title": "5. Conclusion", "content": "In this paper, we introduced an extended version of evaluation sets for the Latvian Twitter Eater Cor- pus by supplementing the current sentiment analy- sis test set with named entity labels and full trans- lations into English. We also manually labelled two new evaluation sets for understanding relations that attached images have with the posted text, as well as a more timeline-balanced sentiment analy- sis test set.\nBy experimenting with evaluating pre-trained models using our new test sets we find that some tasks like named entity recognition and sentiment analysis can already handle such data fairly well. However, other tasks like machine translation and especially image-text relation classification still leaves much room for improvement.\nWe plan to publish the newly annotated data and merge them into the main repository of LTEC12, keeping everything under the current MIT license."}, {"title": "Limitations", "content": "In this work, we only considered highly domain- specific data written in a relatively less-spoken lan- guage, and possibly containing a noticeable de- gree of internet slang. Therefore, any results ob- tained from analysing such data need to be inter- preted with these considerations in mind and can- not be generalised to the broader scope of the Lat- vian language. Also, since hyper-parameter tuning on training large models is computationally very costly, we opt for choosing mostly default param- eters and base versions of open-source models for our experiments."}, {"title": "Ethics Statement", "content": "Our work fully complies with the ACL Code of Ethics 13. We use only publicly available datasets and relatively low compute amounts while con- ducting our experiments to enable reproducibility. All human annotators were fairly compensated for their efforts."}]}