{"title": "Detecting Systematic Weaknesses in Vision Models along Predefined Human-Understandable Dimensions", "authors": ["Sujan Sai Gannamaneni", "Rohil Prakash Rao", "Michael Mock", "Maram Akila", "Stefan Wrobel"], "abstract": "Studying systematic weaknesses of DNNs has gained prominence in the last few years with the rising focus on building safe AI systems. Slice discovery methods (SDMs) are prominent algorithmic approaches for finding such systematic weaknesses. They identify top-k semantically coherent slices/subsets of data where a DNN-under-test has low performance. For being directly useful, e.g., as evidences in a safety argumentation, slices should be aligned with human-understandable (safety-relevant) dimensions, which, for example, are defined by safety and domain experts as parts of the operational design domain (ODD). While straightforward for structured data, the lack of semantic metadata makes these investigations challenging for unstructured data. Therefore, we propose a complete workflow which combines contemporary foundation models with algorithms for combinatorial search that consider structured data and DNN errors for finding systematic weaknesses in images. In contrast to existing approaches, ours identifies weak slices that are in line with predefined human-understandable dimensions. As the workflow includes foundation models, its intermediate and final results may not always be exact. Therefore, we build into our workflow an approach to address the impact of noisy metadata. We evaluate our approach w.r.t. its quality on four popular computer vision datasets, including autonomous driving datasets like Cityscapes, BDD100k, and RailSem19, while using multiple state-of-the-art models as DNNs-under-test.", "sections": [{"title": "1 Introduction", "content": "With the recent advances in machine learning (ML), in particular, self-supervised techniques, there is an significant improvement in modelling of unstructured data like images. However, for safety-critical appli-cations, ML models need to be developed with a focus on trustworthiness by investigating and correcting potential failure modes. To that end, systematic errors of the DNNs need to be studied and rectified. Hidden stratification (Oakden-Rayner et al., 2020) and fairness-related bias (Buolamwini & Gebru, 2018; Wang et al., 2020; Li et al., 2023) due to spurious correlations (Xiao et al., 2020; Geirhos et al., 2020; Mahmood et al., 2021) and underrepresented subpopulations (Santurkar et al., 2020; Sagawa et al., 2019) are some examples of a potential failure modes where the error or weakness is systematic in nature. The existence"}, {"title": "2 Related Work", "content": "In this section, we discuss the recent progress in analysing systematic weakness using slice discovery methods (SDMs) (Eyuboglu et al., 2022) for structured and unstructured data and the relationship to interpretability and feature attribution methods."}, {"title": "3 Method", "content": "Within this section we explain our proposed modular workflow for weakness detection on the basis of human-understandable semantic dimensions. To this end, we introduce notation regarding metadata and slicing, discuss the generation of metadata, formulate DNN weakness within a Bayesian framework to account for the impact of noise, and lastly detail how such impact can be acknowledged within slice discovery algorithms.\nNotation: Consider a DNN-under-test (DuT) M trained on some computer vision task. Let D be the (test) data containing inputs and corresponding task-related ground-truth. For each sample $s_i \\in D$, using some per-sample performance metric (e.g., intersection over union (IoU)) and, if applicable, by applying some threshold, we obtain binarized DuT errors defined as $e_i \\in \\{0,1\\}$, i.e., a given sample can be either predicted correctly ($e_i = 0$) or incorrectly ($e_i = 1$) by the DuT. Here, we mildly deviate from conventional notation as instead of inputs to the DuT we discuss individual samples. While identical for image classification, in"}, {"title": "Metadata Generation", "content": "While there is great interest from safety experts and certification bodies in ODDs for safety argumentation, metadata that aligns with the ODD is scarcely available for most, particularly image, domains. Human annotation of such metadata is often out of scope for large datasets due to cost and time constraints. However, an automated metadata generation approach that captures different semantic dimensions of Z is feasible with existing technologies. For example, a multi-modal foundational model like CLIP (Radford et al., 2021) with its joint image and text embedding space could be a potential candidate for such automated annotation either out-of-the-box or after fine-tuning. For a given attribute a we can use CLIP as a zero-shot classification function G, which maps a given sample $s_i$ onto the attributes values, which represent the potential classes. As such, it therefore provides the coherence of the slices discussed above.\nTaking the ontology for pedestrians from the automotive domain as a baseline, a qualitative evaluation of CLIP's capability was done by Gannamaneni et al. (2023). While CLIP achieved SOTA level zero-shot performance for identifying different attributes like gender, skin-color, and age for portrait shots of human faces in the celebA dataset, they observed a drop in performance on real-world datasets containing pedestrians like in the Cityscapes dataset. The drop in performance can be attributed to more challenging conditions such as complex poses, low illumination, high occlusion. This observation, and our experiments below, show that the classification function G is subject to varying degrees and types of uncertainty, depending on the dimensions of Z: (i) the presence of data based (aleatoric) uncertainties, i.e., where the image resolution is low or the object in question is heavily occluded or distant leading to errors in generated metadata. (ii) the presence of model based (epistemic) uncertainties, i.e., where the function G has sub-optimal performance. While (i) can occur in case of both human and CLIP based annotation, (ii) more prominently occurs in non-human, automated labelling. Therefore, any method that aims to consider metadata generated using such techniques should take into account the incurred noisiness in the downstream tasks."}, {"title": "Bayesian Framework to Account for the Impact of Noise", "content": "To address the uncertain nature of classification, we extend the previous slice notation of the error to the joint? probability p(e, C, S), where C represents the outcome of the automated labelling for some attribute of a dimension while S denotes the corresponding ground-truth. For simplicity, we drop the indices and make the further assumption that S,C can be seen as binary, i.e. they may either be true (S, C) or not true (\u00acS, \u00acC), respectively. Using Bayes' Theorem and marginalizing over the C or S we can express\n$p(e|S) =p(e|C, S)r_c + p(e|\\neg C, S) (1 \u2013 r_c), (1)$\n$p(e|C) =p(e|C, S)p_c + p(e|C, \\neg S) (1 \u2212 p_c) . (2)$\nHere, p(e S) represents the true slice error while p(e|C) denotes the observed slice error. Furthermore, $p_c = p(S|C)$ and $r_c = p(C|S)$ are short-hands for precision and recall of the labelling function G as measured towards the ground-truth, and are used in the workflow, fig. 1, for the quality check. A more detailed derivation of the equations can be found in appendix E. Making these relations explicit allows us to investigate the hypothesis typically underlying Slice Discovery Methods in more detail. Namely, purely from an observed subset performance/weakness p(e|C) one may conclude that a related data property S constitutes a weakness of the DuT in the sense that also p(e|S) has a comparable performance/weakness. While in our approach the relation between S and C is explicit as the latter is given by a classifier for the former, in other approaches d'Eon et al. (2022); Eyuboglu et al. (2022); Jain et al. (2023) the relation is implicit as observed sets C are interpreted to indicate a meaning of S (typically referred to as slice label). Another assumption typically made is the independence between the labelling function G and the DuT. Such independence would imply that the errors of the DuT do not depend on the noise introduced by G. Specifically, for a semantic attribute, the error rates p(e|C, S) when G is correct and the error rate p(e|\u00acC, S) when it is not should be (approximately) equal. However, our experiments indicate that this is not always the case and we therefore"}, {"title": "Weak Slice discovery on Structured ODD data with SliceLine", "content": "We have now established methods to generate metadata and correct for noise during the metadata generation. With this as background, in algorithm 1, we propose a three staged approach for a Systematic Weakness Detector (SWD-1,2,3). In SWD-1, using the generated structured metadata and observed errors p(e|C), we can employ algorithms like SliceLine (Sagadeeva & Boehm, 2021) to provide a ranked list of top-k worst performing slices based on a scoring function that takes into account errors and sizes of slices (see eq. (5) in appendix D on how SliceLine works). As we have motivated, the observed errors might not always provide sufficient signal to identify the underlying error (see top row in fig. 2). Therefore, in SWD-2, utilizing eq. (4) to compensate for the noise in the metadata, we provide corrected errors instead of observed errors to SliceLine to provide a second, ranked list of top-k worst performing slices S. However, as it requires extensive human effort to identify all parameters in eq. (4), we make a cheaper approximation by only considering the independence assumption part of eq. (4), shown as computeCorrectedError() in algorithm 1, and estimate precision values based on human evaluation of metadata quality on n = 60 samples per attribute (see appendix H). These errors are the ones utilized in the scoring function of SliceLine. Based on the slice quality indicators discussed above, we are also able to discard invalid slices due to denominator values close to zero. In addition to SWD-1 and SWD-2, we also consider a merge of the resulting slices from both approaches as this might provide a complementary effect at a cost of loss of precision in identified weak slices as SWD-3. The merge step includes sorting based on slice score from scoring function, removal of duplicate slices, and filtering of invalid slices. The SliceLine hyperparameters therein include the level (maximal search depth), i.e. the maximal number of semantic dimensions considered simultaneously, as well as a cut-off for the necessary slice error $\\bar{e}_{\\mid S}$ to consider S a valid slice."}, {"title": "4 Proof of Concept with Synthetic Data", "content": "To demonstrate the efficacy of our proposed workflow fig. 1, evaluations on synthetic dataset are presented first. This is done to evaluate the impact of noise on the labelling process and the degree to which our approaches can compensate for it. The synthetic data is a tabular dataset containing columns for 9 \"real\" semantic dimensions for 200 000 samples filled with binary values. To each of the \"real\" dimensions (GT), a \"predicted\" metadata column is included, as a proxy for metadata that would be generated by CLIP in our workflow (see fig. 1). Finally, one column contains the binarized DuT errors (e). The first 4 dimensions are generated to be imbalanced with only 5% of the samples belonging to attribute \"1\". The other 5 dimensions are generated such that both attributes have equal distribution. The error column is chosen such that weak slices are induced for the ground truth attributes.\nWe consider three regimes of noise, i.e., different quality of labelling of the simulated annotation process: (i) a regime of \"good\" quality CLIP labelling, represented with pc being above 80%, (ii) a regime of \"medium\" quality CLIP labelling, represented with pc between 40% to 70%, and (iii) a regime of \u201cbad\u201d quality CLIP labelling, represented with pc being above 10% to 40%. For all three regimes, we consider 100 runs of the experiments to account for statistical influence. Further details about the dataset generation can be found in appendix B. In fig. 2, on the top row, the error distributions show how labelling quality impacts the spread of error between attributes for each semantic dimension, that is the upper and lower ends of the bars are given by the error rates for $\\bar{e}_S$, $\\bar{e}_{\\neg S}$ and similarly using C or the corrected errors. In the good labelling quality regime, as expected, observed errors and corrected errors both display the same spread as the GT error. But when labelling quality is medium or bad (where impact of eq. (4) is stronger), spread of observed error is significantly lower than that of corrected error. In contrast, corrected error overestimates what true error (GT) would be. From a safety perspective, we believe it is better to overestimate the systematic error within a DNN than to underestimate it which the corrected approach does. In the bottom row, we evaluate the results of SWD-1,2,3. This is shown by comparing how well the three approaches recover the top-k weak slices by comparison against k slices from Oracle, i.e., situation where we have access to perfect \"GT\" labelling quality annotation. Precision and recall are calculated for the three data quality regimes w.r.t. the ideal case by considering the overlap of slices at increasing levels of k. Note that precision and recall in this figure refer to quality metrics on weak slice discovery and not precision and recall of the CLIP labelling. While, at level 2, the maximum number of slices k is 162 for 9 binarized dimensions, we consider only slices fulfilling the cut-off requirement as a weak slice. Of those 162 slices, only ~ 30 are identified as weak slices. While under good labelling quality, slices identified by the three approaches basically have 100% overlap with the Oracle, under medium and strong label noise, SWD-3 shows significantly more recall than SWD-1 and marginally over SWD-2. This, however, comes with a small loss in precision. In cases of strong noise, SWD-1 only recovers a few slices where error signal is dominant which explains the high precision at the cost of low recall. SWD-3, on the other hand, has a reduced precision but recovers most of the weak slices identified by Oracle. For the rest of the paper, we primarily focus on slices identified by SWD-3."}, {"title": "5 Evaluations of real-world DNNS", "content": "In this section, we first present our experimental setup. We then show the evaluation of our systematic weaknesses detection method on a publicly available pre-trained model for the CelebA dataset. Here, the dataset's rich meta-data annotation allows us to investigate the influence of noisy metadata annotation. In addition, we compare against SOTA SDM methods to evaluate our claim that adherence to ODD descriptions is useful to end users (e.g., safety experts, ML developers). Subsequently, we present the insights gained by using our approach on DNNs trained on autonomous driving datasets."}, {"title": "5.1 Experimental Setup", "content": "Datasets and Models: Four pre-trained models, ViT-B-16 (Dosovitskiy et al., 2020), Faster R-CNN (Ren et al., 2015), SETR PUP (Zheng et al., 2021), PanopticFCN (Li et al., 2021) are evaluated using four public datasets (CelebA (Liu et al., 2015), BDD100k (Yu et al., 2020), Cityscapes (Cordts et al., 2016), and RailSem19 (Zendel et al., 2019)), respectively. We restrict the number of combinations (level) to 2 in this work. However, as discussed in appendix H, our approach allows for correction of errors even at higher levels of combinations. We utilize the cut-off for slice error as 1.5 *\u0113ls for all experiments except evaluation of PanopticFCN. Here, we utilize the cut-off for slice error as 1.0*\u0113ls as the global DuT error is quite high. For the detailed experimental setup, please refer to appendix A. To foster reproducibility, code and the prompts used for metadata generation with CLIP will be provided."}, {"title": "5.2 Evaluation of our Systematic Weaknesses Detection Method", "content": "Evaluating a ViT Model on CelebA: As our first experiment, we evaluate the weaknesses of the ViT-B-16 (Dosovitskiy et al., 2020) model (DuT) trained on ImageNet21k (Ridnik et al., 2021). We use the model for the targeted task of identifying the class \"person\" in the CelebA dataset (Liu et al., 2015) as a real-world proof of concept for our approach. Due to the extensive range of label categories in ImageNet (Deng et al., 2009) and the significant noise in labelling style, models trained on the full ImageNet dataset or its standard subset ImageNet1k (Russakovsky et al., 2015) can suffer from systematic weaknesses. For example, although the primary foreground object in an image might be a human, in some instances, the image can be labelled as belonging to the class \"person\" while in other, similar instances, the label might be about more granular classes like \"bride\" or \"guitarist\". To fix this issue, (Ridnik et al., 2021) proposed 11 hierarchies based on WordNet (Miller, 1995) semantic trees such that classes at higher hierarchy levels are superclasses that subsume classes at lower hierarchy levels. However, despite these efforts, considerable label noise in terms of"}, {"title": "5.3 Insights on SOTA Pedestrian Detection Models", "content": "Having shown the benefits of our proposed method, we evaluate a more safety-relevant task of pedestrian detection using models trained on real-world autonomous driving (AD) datasets to identify their system-"}, {"title": "6 Conclusion", "content": "In this work, we present a modular workflow for our Systematic Weakness Detector (SWD) to analyse the systematic weaknesses of DNNs that perform classification, object detection, and semantic segmentation tasks on image data. In the first step, we overcome the problem of missing metadata by generating metadata with a foundation model. Subsequently, in the second step, we perform slice discovery on the structured metadata, which comprises of DNN-under-test's per-object performance and previously acquired per-object metadata. Using our workflow, we transform the slice discovery of unstructured image data into an (approximate) slice"}]}