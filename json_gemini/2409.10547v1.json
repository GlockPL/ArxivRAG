{"title": "Efficient Chrome Extension for Phishing Detection Using Machine Learning Techniques", "authors": ["Leand Tha\u00e7i", "Arbnor Halili", "Kamer Vishi", "Blerim Rexha"], "abstract": "The growth of digitalization services via web browsers has simplified our daily routine of doing business. But at the same time, it has made the web browser very attractive for several cyber-attacks. Web phishing is a well-known cyberattack that is used by attackers camouflaging as trustworthy web servers to obtain sensitive user information such as credit card numbers, bank information, personal ID, social security number, and username and passwords. In recent years many techniques were developed to identify the authentic web pages that user visits and warn them when the webpage is a phish. In this paper, we have developed an extension for Chrome as the most favorite web browser, that will serve as a middleware between the user and phishing websites. The Chrome extension named \u201cNoPhish\" shall identify a phishing webpage based on several Machine Learning techniques. We have used the training dataset from \u201cPhishTank\" and extracted the 22 most popular features as rated by Alexa database. The training algorithms used are Random Forest, Support Vector Machine, and k-Nearest Neighbor. The performance results show that Random Forest delivers the best precision.", "sections": [{"title": "1 Introduction", "content": "Nowadays, cyber-attacks are considered to be the most prevalent attacks that threatens our security and privacy on the Internet. Starting from some simple actions up to critical cases like US Elections [1], cyber security has become a topic for every person and government around the world.\nAccording to the IBM Security X-Force Threat Intelligence Index report published in 2022, 41% of all cyber-attacks came from phishing, making it the top 1 technique most used for initial attack vector [2]. Comparing presence of this technique in report of 2021, there is a raise in 8 percent [3]. Phishing is a cyber-attack in which attackers try to steal information or sensitive data from victims, camouflaging as a reliable source. Additionally, phishing has also become a bridge for other attacks, tricking users into downloading malicious documents like viruses, malware, or ransomware.\nOne of our biggest motivations to further explore this topic was a gradual increase over the last two years of the number of phishing sites. Victims of attacks can be from different categories, from large companies to individuals as ordinary Internet users. The most frequent attack on large companies is targeting the employees of that company through phishing emails. In terms of attacks which do not target a specific person or company, but are made for large masses, the most type of frequent attack is the imitation of large companies. According to statistics released by Vade Secure for the 2021 [5] , the company which is most often imitated is Facebook which overcame \"Microsoft\u201d, which was far away in first place. As of this writing, based on same report, Facebook has 2.8 billion users, a large pool of potential victims ready to bite on Facebook phishing messages and web pages. There is also a large number of phishing attacks that don't impersonate Facebook directly. Instead, the emails impersonate another brand and include links to Facebook phishing pages, exploiting user trust in the Facebook brand.\nThe battle between hackers and defenders on this topic is on daily basis, and the result is never clear. There is no clear index or methodology specified that would classify who is dominating the Internet ecosystem however we are witnesses of such a situation. This battle is best illustrated where statistics of daily reporting within August 2022 for phishing are presented.\nBased on this growth of phishing sites we evaluate that there is always room for improvement in terms of defense against these attacks and any research in this regard means fewer victims. Obviously, attacks of this kind cannot be solved once and for all, constant efforts must be made to study new methods of attack and to adapt protection against them. In this paper, we tend that through detailed research and analysis to provide new and efficient solutions for application by using the latest technologies and techniques. In addition to the tool development and training part, this paper gives importance also to the simplicity of using the tool. The goal is that every user regardless of their professional background in this field, be able to get the right information about the site being visited.\nWork done for this solution combines knowledge from different fields of study, like Machine Learning, Web programming, and Web security. Machine Learning (ML) is seen as a mechanism that will enrich our tool with a way of learning and deciding based on a model that is created. The model will be created based on training data. Similar usage was seen earlier in a wide variety of applications, such as email filtering and computer vision, where it is difficult or impossible for conventional algorithms to perform the necessary tasks.\nThis paper's purpose is to propose a tool that can classify phishing sites. Classification falls under the Supervised Learning paradigm, where the model is trained with labeled data, a process that is often quite costly in all respects. Algorithms that will be discussed and that belong to this paradigm are:\n\u2022 SVM (Support Vector Machine) - As it is presented the classification boundaries are defined by the so-called support vectors. This is the perfect case when the data is sufficiently differentiable and there are no exceptions (outliers), respectively instances which are located outside the defined boundaries. If the model is trained using hard margins (hard margin classification) then the model works only if the data are linearly separated. For this reason, more flexible parameters of margins should be defined, respectively soft margin classification.\n\u2022 kNN (k-Nearest Neighbors) - The principle behind kNN is to find a predetermined number of training samples closest to the distance from the new point and to predict from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or variable based on local point density (radius-based neighbor learning). Distance, in general, can be any metric mass: the standard Euclidean distance is the most common choice. Neighbor-based methods are known as non-generalized Machine Learning methods, as they simply \"recall\u201d all their training records.\nDespite its simplicity, kNN has been successful in a large number of classification and regression problems, including handwritten figures and satellite image scenes. Being a non-parametric method, it is often successful in classification situations where the decision limit is very irregular.\n\u2022 Random Forest - Is a flexible, easy-to-use algorithm that produces, even without adjusting parameters for specific cases, an excellent result most of the time. Random Forest is one of the most widely used algorithms,"}, {"title": "2 Related Work", "content": "Sadly, in the battle against phishing attacks, the attacking side is always one step ahead, so the defensive side needs always to keep up with the new methods of attacking [10]. The process of constant sophisticating the tools and techniques for protecting users from phishing attacks is forcing attackers to adapt and evolve their techniques. Today phishing attacks are becoming more and more polymorphic [11]. The situation becomes even worse when it is known that the most perfect attacks occur within minutes and do not recur twice by the attacker without improving for the next time. The \"zero-day attacks\" that by definition are attacks that have not happened before, today are not uncommon [12]. These attacks are always launched with a new set of IP addresses and a new variant of the virus.\nThe most common method of detecting phishing sites is by updating URLs and IPs addresses listed as phishing in the antivirus database, this method is known as \"Blacklist\" [13]. To avoid blacklists attackers use creative techniques to cheat the system by modifying the URL to look legitimate through obfuscation and many techniques other simple methods including the \"fast-flux\" method [14]. Through this method proxies are automatically generated to host the web page and new URLs are generated through algorithms. The main drawback of this method is that, it can not detect \"Zero-day attacks\". Thus, there must be at least one victim who will report this site and will blacklist that page. One of the most popular tools of this type is \"Netcraft Extension\" [15]. This gadget consists of a large community, and expects the first victim to report a page in order to inform other users not to visit that page.\nAnother method for detecting phishing sites is a system with predefined rules. Where the tool analyzes the content of a site and based on the rules it has predefined and decides whether that site is considered phishing or not [16]. The problem with this type of system is, although in principle it can stop \"zero-day\" attacks, predefined features are not guaranteed to be found in every attack so the number of false positives is very high and usage remains very narrow. The most popular tool of this type of system is \"Phish Detector\" [17]. A tool that prevents phishing attacks in online banking. It is a \"rule-based\" system and claims zero false negative predictions. The disadvantage is that it is limited to the domain of online banking.\nThe third method, detecting phishing sites using Machine Learning, tends to stop \"zero-day\" attacks, including as many attacking domains as possible. Nowadays there are plenty of tools with this method, but there is always room for improvement. One of the early tools of this kind is the \"Cascaded Phish Detector\" which similar to this paper consists of a client-side component and a server-side component [18]. This gadget only takes into account the HTML content of the page, we think there are also other kind of features that can be extracted from a webpage and are informative as mentioned in Section 1.\nThe authors in [19] used Machine Learning to predict phishing. We find missing in this paper that there is no explanation of the ratio used to divide the training and testing set. Furthermore, there is no concrete implementation of this algorithm; it is not part of any tool, or we have not found any concrete implementation and ease of usage, which is one of the critical points of our work.\nAn interesting approach is found at [20]. Here an architecture which is mainly referred to a preprocessing of data is proposed. However, it only took into consideration one algorithm and could lead to a over fitting.\n[21] used a technique of search for classification of a web page as phishing or not. Pseudocode presented on the paper shows that algorithm instead of learning is using services through API and direct requests to make a decision. In this case Google search engine has the main role since top six Google search results are used to make decision. It worth mentioning that in most cases, top six Google search results change very rare, hence, it could present a high potential for wrong classification.\nWe found interesting the work of [21], which ranked Random Forest to be best (together with XGboost) among 12 algorithms used. However, even here there is no concrete implementation to link this result with a concrete tool.\nFurthermore, from our research made on current work and state of the art, best to our knowledge, we could identify a research gap in this intensive field. There are three principles that we took into consideration:\n\u2022 effectiveness of the solution,\n\u2022 easiness of implementation and usage, and\n\u2022 innovation.\nFrom studies and work done until now we found solutions that are effective (like [20] but there is no implementation or concrete use. We found work that was easy to use [19] but not enough effective. Our solution tends to fulfill all these three principles and tends to do it in a novel, innovative way, nor presented until now by any author."}, {"title": "3 Our Approach - NoPhish", "content": "Since the classification of whether the site is suspicious or not will be done by the tool trained with Machine Learning, the question arises on what the tool will base its prediction. The main thing to look for when detecting suspicious phishing sites is the URL of the site. Nowadays it is not enough just to look at whether the site uses the secure HTTPS protocol for communication. According to the latest report from APWG (Anti-Phishing Working Group), about 80% of phishing sites use the protocol HTTPS [4]. Therefore, some properties are extracted from the URL based on which a page can be considered suspicious. Other features are derived from the content of the site, where the authors of these sites often focus only on copy the look of the real page and leave many flaws which can be identified. And the last category of site features are extracted from searching this site in the most popular databases of statistics about web pages, such as the Alexa Database [22]. The most important part of the tool is definitely the training algorithm, based on which the tool makes decisions about how to classify the pages it handles. Within Machine Learning we have a large diversity of algorithms which are adequate for different situations. For this paper the variety is narrowed down to the 3 most adequate algorithms, which will be tested and compared. They are: Random Forest, SVM (Support Vector Machine), kNN (k-Nearest Neighbor). Chrome Extension will play the role of the middle man between the user and the classifier. Google Chrome offers the ability to add a tool add-on which can be used on any page you browse, it is called a Chrome Extension. The question is, why exactly Google Chrome? According to November 2020 statistics [23], Chrome browser dominates with 65.3% of worldwide use.\nThe tool 1 consists of two main components: the \"client-side\" component with which the user will interact and, the \"server-side\" component which will be the classifier that decides whether a site is considered phishing and the web server which will play the role of mediator between the two components and will create the environment for the execution of the classifier.\nSince the key point of this paper is Machine Learning, it was decided to use the Python language (version 3.9) for the implementation of the classifier component. One of the advantages of this language is definitely the large number of libraries. The Python language is today recognized as a leader in the field of Machine Learning, through the skicit-learn library the implementation of algorithms for Machine Learning is easier than ever. In addition Python also simplifies the data manipulation procedure for extraction of site features. The numpy library is used for manipulations with large multidimensional arrays, while matplotlib is used for graphical representations of results and statistical data needed for visualization. The \"client-side\" component will be developed as \"Chrome Extension\u201d for very easy user access. The reason for selecting the Google Chrome browser is the large percentage of users in the market."}, {"title": "3.1 Determining the features based on which pages will be classified", "content": "Because the dataset contains features that help detect sites not only for phishing but also for other risks. Twenty-two of the thirty features listed bellow are considered to be phishing information.\nIf an IP address is\n1. Using IP address instead of domain\nused as an alternative of the domain name in the URL, such as \"http://125.98.3.123/fake.html\", users can be sure that someone is trying to steal their personal information. Sometimes, the IP address is even transformed into hexadecimal code as shown in the following link \"http://0x58.0xCC.0xCA.0x62/2/paypal.ca/index.html\u201d.\n2. Long URL to Hide the Suspicious Part - Phishers can use long URL to hide the doubtful part in the address bar\n3. Using URL Shortening Services \u201cTinyURL\u201d - URL shortening is a method on the \u201cWorld Wide Web\u201d in which a URL may be made considerably smaller in length and still lead to the required webpage. This is accomplished by means of an \"HTTP Redirect\" on a domain name that is short, which links to the webpage that has a long URL.\n4. URL's having \u201c@\u201d Symbol - Using \u201c@\u201d symbol in the URL leads the browser to ignore everything preceding the \u201c@\u201d symbol and the real address often follows the \u201c@\u201d symbol\n66\n5. Redirecting using \"//\" - The existence of \"//\" within the URL path means that the user will be redirected to another website.\n6. Adding Prefix or Suffix Separated by (-) to the Domain - The dash symbol is rarely used in legitimate URLs. Phishers tend to add prefixes or suffixes separated by (-) to the domain name so that users feel that they are dealing with a legitimate webpage\n7. Sub Domain and Multi Sub Domains - If a page has many subdomains then it can be considered that the attacker is trying to disguise the site domain.\n8. Domain Registration Length - Based on the fact that a phishing website lives for a short period of time, we believe that trustworthy domains are regularly paid for several years in advance.\n9. Favicon - A favicon is a graphic image (icon) associated with a specific webpage. Many existing user agents such as graphical browsers and newsreaders show favicon as a visual reminder of the website identity in the address bar. If the favicon is loaded from a domain other than that shown in the address bar, then the webpage is likely to be considered a Phishing attempt.\n10. The Existence of \u201cHTTPS\u201d Token in the Domain - The phishers may add the \u201cHTTPS\" token to the domain part of a URL in order to trick users.\n11. Request URL - Request URL examines whether the external objects contained within a webpage such as images, videos and sounds are loaded from another domain. In legitimate webpages, the webpage address and most of objects embedded within the webpage are sharing the same domain.\n12. URL of Anchor - An anchor is an element defined by the <a href> tag. This feature is treated exactly as \"Request URL\". However, for this feature we examine:\n(a) If the <a href> tags and the website have different domain names,\n(b) If the anchor does not link to any webpage.\n13. Links in <Meta>, <Script> and <Link> tags - Given that our investigation covers all angles likely to be used in the webpage source code, we find that it is common for legitimate websites to use <Meta> tags to offer metadata about the HTML document; <Script> tags to create a client side script; and <Link>, tags to retrieve other web resources. It is expected that these tags are linked to the same domain of the webpage.\n14. Server Form Handler (SFH) - SFHs that contain an empty string or \"about:blank\" are considered doubtful because an action should be taken upon the submitted information. In addition, if the domain name in SFHs is different from the domain name of the webpage,"}, {"title": "Springer Nature 2021 - PreprintEfficient Chrome Extension for Phishing Detection Using ML Techniques 11", "content": "this reveals that the webpage is suspicious because the submitted information is rarely handled by external domains.\n15. Submitting Information to Email - Web form allows a user to submit his personal information that is directed to a server for processing. A phisher might redirect the user's information to his personal email. To that end, a server-side script language might be used such as \"mail()\" function in PHP. One more client-side function that might be used for this purpose is the \"mailto:\" function\n16. Abnormal URL - This feature can be extracted from WHOIS database. For a legitimate website, identity is typically part of its URL.\n17. IFrame Redirection - IFrame is an HTML tag used to display an additional webpage into one that is currently shown. Phishers can make use of the \"iframe\" tag and make it invisible i.e. without frame borders. In this regard, phishers make use of the \u201cframeBorder\u201d attribute which causes the browser to render a visual delineation.\n18. Age of Domain - This feature can be extracted from WHOIS database (Whois 2005). Most phishing websites live for a short period of time.\n19. DNS Record - For phishing websites, either the claimed identity is not recognized by the WHOIS database (Whois 2005) or no records founded for the hostname (Pan and Ding 2006). If the DNS record is empty or not found then the website is classified as \u201cPhishing\u201d, otherwise it is classified as \"Legitimate\".\n20. Website Traffic - This feature measures the popularity of the website by determining the number of visitors and the number of pages they visit.\n21. Google Index - This feature examines whether a website is in Google's index or not. When a site is indexed by Google, it is displayed on search results (Webmaster resources, 2014).\n22. Statistical-Reports Based Feature - Various companies like PhishTank or StopBadware form statistical reports exposing the IPs and domains most used for phishing. This feature checks if the host is part of these reports."}, {"title": "3.2 Choosing the most efficient algorithm for our specific case", "content": "For this paper we took under consideration three classification algorithms, they are: SVM, KNN and Random Forest. All will be tested and the algorithm that performs the best will be selected."}, {"title": "3.3 Feature Importance", "content": "The Random Forest algorithm also offers us the importance of each feature from the training. Through the feature_importance_object, the features are ranked based on how informative they were during the training. These percentages are calculated by the algorithm from MDI (Mean Decrease in Impurity) so as that feature reduces the impurity of the dataset.\nA problem that has emerged recently is that the importance of features based on impurity can inflate the importance of numerical features. Furthermore, the impurity-based feature importance of random forests suffers from being computed on statistics derived from the training dataset: the importances can be high even for features that are not predictive of the target variable, as long as the model has the capacity to use them to overfit.\nAs we can see from both the graph extracted from different methods that feature 11 which is the URL of the anchor tags is the most important feature. Logically the danger of phishing sites is the redirection into suspicious sites."}, {"title": "3.4 Client-side Component (The face of the tool)", "content": "The component with which the user will interact will be developed as a Chrome Extension. Chrome Extension development is simple, the three elements needed for a functional extension are:\n1. popup.html - where HTML elements are defined,\n2. popup.js - where functions are written in javascript, and\n3. manifest.json - where extensions details are written. The following code snippet represents the manifest.json file .\nThe main purpose of Chrome Extension is to be as simple as possible."}, {"title": "4 Experimental Results", "content": "In order to test our tool against current solutions to this problem we did some experimental work. First we selected our testing subject which will be twenty-seven websites, fourteen of them are verified phishing websites taken from PhishTank [6] and the other thirteen are well known websites proven that they dont have any malicious intent.\nThe tools that we did compare our work with were: Google Chrome's built in firewall and the firewall of a world wide agency whose name will be undisclosed for privacy reasons. The methodology of testing was as follows, each website was scanned with the three tools. In the end we gathered all the information and displayed it as the confusion matrix( as presented ) which we think is the best indicator of accuracy for this particular case."}, {"title": "5 Conclusion", "content": "From the evaluation results, one can conclude that developing a classifier by applying Machine Learning gives satisfactory results. Besides good results, there is a great potential for more extensive research in machine learning and phishing detection techniques.\nThis paper's results are satisfactory by achieving high accuracy with a small number of false-negatives. Furthermore, the Random Forest algorithm proved more efficient in this research; it does not mean that there could not be another more successful algorithm in the future in other circumstances with extensive evolution of attacks. The features on which the forecast is based should always represent the state-of-the-art attack properties. Such as the number four feature where if the URL contains \"//\" it is considered suspicious, and it can be removed in the future. Because some browsers nowadays have already stopped redirecting to other pages if the URL contains \"//\", but as long as this security \"bypass\" is present in all browsers can not be neglected."}, {"title": "6 Declarations", "content": "Ethical Approval\nNot applicable.\nCompeting interests\nPersonal nature interest (continuous work on security research)\nAuthors' contributions\nA.H. wrote the introduction sections text, contributed to related work, and the main manuscript text. K.V. wrote related work and contributed to the main manuscript text. L.TH. wrote mostly on the main manuscript and also dealt with experimental results. B.R. initiated the idea by writing the abstract and conclusion and was the main reviewer. All authors reviewed the manuscript.\nFunding\nNot applicable.\nAvailability of data and materials\nhttps://github.com/LeandThaqi/NoPhish"}]}