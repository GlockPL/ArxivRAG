{"title": "MIRFLEX: MUSIC INFORMATION RETRIEVAL FEATURE LIBRARY FOR EXTRACTION", "authors": ["Anuradha Chopra", "Abhinaba Roy", "Dorien Herremans"], "abstract": "This paper introduces an extendable modular system that compiles a range of music feature extraction models to aid music information retrieval research. The features include musical elements like key, downbeats, and genre, as well as audio characteristics like instrument recognition, vocals/instrumental classification, and vocals gender detection. The integrated models are state-of-the-art or latest open-source. The features can be extracted as latent or post-processed labels, enabling integration into music applications such as generative music, recommendation, and playlist generation. The modular design allows easy integration of newly developed systems, making it a good benchmarking and comparison tool. This versatile toolkit supports the research community in developing innovative solutions by providing concrete musical features.", "sections": [{"title": "1. INTRODUCTION", "content": "Music Information Retrieval (MIR) is a complex field focused on computational analysis and processing of musical data, with tasks like similarity estimation, genre classification, and recommendation. While recent advances in machine learning have led to powerful feature extraction methods, the fragmented nature of these tools poses challenges for researchers who must integrate multiple disparate systems. To address this, we present MIRFLEX, a unified feature extraction library designed for MIR research. MIRFLEX offers a diverse set of extractors covering key musical aspects such as key, beats, and genre, using both signal processing and machine learning techniques to generate comprehensive audio representations.\nThe primary objectives of this work are threefold:\n1. To offer a centralized and easily accessible collection of feature extraction tools, reducing the burden on researchers to implement and integrate disparate feature extraction techniques.\n2. To provide a comprehensive feature set that captures the multifaceted nature of musical data, enabling researchers to explore a wide range of music-related applications and queries.\n3. To contribute to the advancement of music information retrieval research by facilitating the rapid prototyping and development of new applications that leverage easily accessible and readily available musical features.\nThe proposed feature extraction library is available at 1"}, {"title": "2. INTEGRATED FEATURE EXTRACTORS", "content": "Our feature extractor comprises of the following features.\nWe also detail our selection process for the exact approach for feature extraction:"}, {"title": "2.1 Key Detection", "content": "We consider following approaches for key detection, Inception Key Net and CNNs with Directional Filters.\nInception Key Net [1] adapts the Inception V3 architecture and uses the Constant-Q transform to convert the audio to a time-frequency representation. Achieves state-of-the-art performance.\nCNNs with Directional Filters [2] approach compares shallow, domain-specific architectures with directional filters to deep VGG -style architectures with square filters, using constant-Q magnitude spectrograms.\nIn spite of the performance in Table 1, [1] does not have model weights publicly available. We integrate [2] using weights they provided 2."}, {"title": "2.2 Chord Detection", "content": "We consider the following options for chord detection:"}, {"title": "2.3 Down-beat Transcription / Tempo Estimation", "content": "We consider the following approaches for the Downbeat transcription/tempo estimation.\nCNNs with Directional Filters [2] uses Convolutional Neural Networks, with directional convolutional kernels instead of square ones. Single-Step Tempo Estimation CNN [8] frames tempo estimation as a multi-class classification problem, and uses conventional Convolutional Neural Network for the architecture. It can be used for audio clips of 11.9s and therefore is suitable for both local and global tempo estimation.\n1D State Space HMM [9] utilizes a 1D state space and a semi-Markov model for music rhythmic analysis. This approach can reduce the computation cost and provides 30 times speedup in processing.\nBeatNet: CRNN and Particle Filtering [10] combines Convolutional-Recurrent Neural Networks with particle filtering for online joint beat, downbeat and meter tracking. It can achieve real-time processing, with high accuracy, albeit computationally expensive.\nBased on its performance, we choose BeatNet."}, {"title": "2.4 Vocals / Instrumental Detection", "content": "The EfficientNet model, trained on Discogs, is used for instrument/vocals and vocals gender detection. The imple-"}, {"title": "2.5 Instrument, Mood / Theme, Genre Detection", "content": "Not many approaches open-source weights and implementations, we are not able to use any major latest model available. Consequently, the feature extraction system incorporates available weights and techniques implemented in the Essentia library. The Essentia library employs an array of Convolutional Neural Networks as the model architecture, utilizing the Jamendo baseline for the tasks of instrument detection, mood/theme detection, and genre detection."}, {"title": "3. CALL TO CONTRIBUTE", "content": "MIRFLEX is an extendable toolkit freely available on GitHub 4. Moreover, we invite the research community to contribute new feature extractors or extend the existing ones. By providing a common platform for feature extraction, MIRFLEX can facilitate the rapid prototyping and development of new MIR applications, while also enabling researchers to experiment with a diverse set of musical representations. The modular design of MIRFLEX allows for easy integration of new feature extractors, empowering researchers to expand the toolkit's capabilities and push the boundaries of music information retrieval.\nWe believe that MIRFLEX can serve as a valuable resource for the MIR community, fostering collaboration, reproducibility, and innovation in the field."}]}