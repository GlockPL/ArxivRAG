{"title": "Geospatial foundation models for image analysis: evaluating and enhancing NASA-IBM Prithvi's domain adaptability", "authors": ["Chia-Yu Hsu", "Wenwen Li", "Sizhe Wang"], "abstract": "Research on geospatial foundation models (GFMs) has become a trending topic in geospatial artificial intelligence (AI) research due to their potential for achieving high generalizability and domain adaptability, reducing model training costs for individual researchers. Unlike large language models, such as ChatGPT, constructing visual foundation models for image analysis, particularly in remote sensing, encountered significant challenges such as formulating diverse vision tasks into a general problem framework. This paper evaluates the recently released NASA-IBM GFM Prithvi for its predictive performance on high-level image analysis tasks across multiple benchmark datasets. Prithvi was selected because it is one of the first open-source GFMs trained on time-series of high-resolution remote sensing imagery. A series of experiments were designed to assess Prithvi's performance as compared to other pre-trained task-specific AI models in geospatial image analysis. New strategies, including band adaptation, multi-scale feature generation, and fine-tuning techniques, are introduced and integrated into an image analysis pipeline to enhance Prithvi's domain adaptation capability and improve model performance. In-depth analyses reveal Prithvi's strengths and weaknesses, offering insights for both improving Prithvi and developing future visual foundation models for geospatial tasks.", "sections": [{"title": "1. Introduction", "content": "Geospatial Artificial Intelligence (GeoAI) is an interdisciplinary field that integrates geospatial data science with artificial intelligence techniques to solve complex spatial problems (Janowicz et al. 2020, Li 2020). One of its most promising applications is in the realm of image analysis, particularly in interpreting and extracting valuable information from remote sensing imagery (Li, Wang, et al. 2022, Udawalpola et al. 2022). Remote sensing imagery has revolutionized the way we understand the physical process on the Earth's surface and in the atmosphere. Utilizing sensors mounted on satellites, drones, or aircraft, remote sensing captures high-resolution images. These images provide invaluable data across various sectors, including environmental monitoring (VoPham et al. 2018), disaster management (Mahmood 2022), agriculture (Garc\u00eda Pereira et al. 2020), and urban planning (Alastal and Shaqfa 2022), among others. Traditional methods of image analysis, such as thresholding techniques, often require manual intervention and are time-consuming (Zhou et al. 2019). This makes them less efficient for handling large datasets. GeoAI, on the other hand, leverages machine learning algorithms to automatically analyze and interpret geospatial images, thereby significantly improving the speed and accuracy of data extraction (Li and Hsu 2022). As GeoAI continues to evolve, it opens up new avenues for more automated and intelligent image analysis, making it a subject of keen interest for researchers and practitioners in the geospatial domains (Li and Hsu 2020, Gao et al. 2023).\nRecent advancement in deep learning have significantly improved the capabilities of geospatial image analysis, but these models come with their own set of challenges. One pressing issue is the requirement for large, annotated datasets for effective training (Deng et al. 2009, Lin et al. 2014). This is particularly challenging in specialized fields such as remote sensing, where the need for domain-specific knowledge, the large volume of data, and data variability due to factors such as seasonal changes make obtaining annotated data both time-consuming and expensive (Li et al. 2021). Furthermore, these models are generally task-specific, meaning that a model trained for one application may not easily generalize to another without substantial retraining and fine-tuning (Zhang et al. 2020, 2021). The computational cost is another significant barrier, as deep learning models often require specialized hardware for both training and inference (Shankar and Reuther 2022).\nThe recent advances in AI foundation models present a compelling solution to some of these limitations. Unlike deep learning models that require large training datasets for each specific task, foundation models are often pre-trained using self-supervised learning (SSL) on vast amounts of data (Zhou et al. 2023). This allows researchers to fine-tune these models on relatively smaller, task-specific annotated datasets, thereby reducing the annotation burden (Gu et al. 2023, Wang, Feng, et al. 2023, Li et al. 2024). Additionally, foundation models are designed to generalize across a variety of data analysis tasks, eliminating the need for separate models and the associated retraining for each specific application (OpenAI 2023, Touvron et al. 2023). While it is true that foundation models also require substantial computational resources for initial training, their ability to generalize across tasks means that the computational cost can be amortized over multiple applications, making them a more efficient choice for organizations that require solutions for a range of geospatial analysis tasks, as well as smaller research groups which do not have expertise or computational resources for building such large models.\nPrithvi, a geospatial foundation model developed by NASA and IBM, sets itself apart from other Al foundation models in vision tasks through several distinctive features (Jakubik et al. 2023). Pre-trained on contiguous US Harmonized Landsat Sentinel 2 (HLS) data, Prithvi is uniquely equipped to process remote sensing images in time series. This capability is often absent in other foundation models and enables Prithvi to well perform in various downstream tasks, such as burn scars segmentation, flood segmentation, and land cover classification. Additionally, Prithvi is designed to work with a 6-band input, including Red (R), Green (G), Blue (B), Narrow Near Infrared (NIR), Short Wave Infrared (SWIR) 1, and SWIR 2, as opposed to the conventional RGB imagery used in most AI foundation models. This multi-band capability enhances Prithvi's ability to capture a wider range of spectral information, thereby increasing its versatility and applicability across a diverse set of geospatial data and tasks.\nDespite advancement in GeoAI and foundation models, there is a notable gap in the literature concerning the performance evaluation of geospatial foundation models such as IBM's Prithvi in the realm of remote sensing image analysis, especially in environmental feature detection and segmentation. Unlike general-purpose AI foundation models, Prithvi is pre-trained on remote sensing images. This unique training dataset raises a presumption that Prithvi may offer inherent advantages in geospatial tasks over other pre-trained models. In addition, Prithvi's unique 6-band input capability, as opposed to the conventional RGB imagery, could have significant implications for its applicability and performance in real-world geospatial applications. To substantiate these presumptions, we raised the following research question: \"How does IBM's Prithvi perform in geospatial image analysis tasks as compared to other pre-trained task-specific A\u0399 models?\"\nTo answer this question, the study's objectives include a detailed performance evaluation of Prithvi on challenging image analysis tasks, such as object detection and instance segmentation. Four remote sensing datasets covering diverse geographical regions and features\u2014including a natural feature dataset, a global Mars crater dataset, an Arctic permafrost landscape dataset, and an agricultural land dataset (EuroCrops from central Europe) are used in the analysis. Since the Prithvi model primarily provides a feature extraction backbone (the encoder part), several new strategies were introduced to adapt it to downstream tasks. These include band adaptation, a multi-scale decoder, and a new fine-tuning strategy designed to maximize its predictive performance.\nThe remainder of the paper is structured as follows. Section 2 reviews research on GeoAI and recent development of foundation models for geospatial image analysis. Section 3 introduces the four datasets used in this work, providing an overview of their characteristics and geographical distribution. Section 4 details the adaptations and enhancements applied to the Prithvi model, outlining the methodology and analyses conducted to evaluate Prithvi's performance. Section 5 presents the evaluation and experimental results, followed by Section 6, which discusses the strengths and weaknesses of the Prithvi model. Section 7 concludes the paper with a summary of key insights and suggestions for future research."}, {"title": "2. Literature review", "content": "2.1. GeoAI and geospatial image analysis\nGeoAI is an interdisciplinary field that combines the predictive power of Al with the intricacies of geospatial data science, offering a unique approach to solving complex spatial problems (VoPham et al. 2018, PS Chauhan and Shekhar 2021). Within this context, the application of GeoAI in image analysis stands out for its unique challenges and opportunities. One key challenge is the complexity of the data involved. Unlike traditional image analysis, GeoAI deals with multispectral and multi-band images, often captured through advanced remote sensing technologies (Li and Hsu 2022). These datasets are not only large in scale but also diverse in nature, incorporating multiple sources such as satellites, aerial photographs, and ground-based sensors (Wang and Li 2021). This data heterogeneity, coupled with the temporal dynamics inherent in geophysical phenomena such as hurricanes and wildfire, requires spatially and spatiotemporally explicit algorithms capable of interpreting intricate spatial and temporal relationships. Meanwhile, GeoAI must account for various uncertainties, such as sensor errors and missing data, making the algorithms robust and adaptable. Despite these complexities, GeoAI has proven to be an indispensable tool in a range of applications, from environmental monitoring to urban planning (VoPham et al. 2018, Kamel Boulos et al. 2019, Liu and Biljecki 2022). Its ability to handle these multifaceted challenges sets it apart from traditional methods and makes it a focal point of contemporary geospatial research.\nThe integration of deep learning into GeoAI has significantly advanced the field of image analysis. Initially, the direct application of deep learning models to geospatial tasks yielded mixed results, often due to the complexities inherent in geospatial data (Lee 2019, Bhuiyan et al. 2020, Li and Hsu 2020). To overcome these limitations, the field has evolved to incorporate expert knowledge, thereby enhancing the models' interpretability and effectiveness in handling the unique challenges of geospatial data (Janowicz et al. 2020, Hsu et al. 2021, Li et al. 2021). This integration of domain expertise has been a pivotal advancement, allowing for more reliable solutions in GeoAI applications. Separately, the field has also embraced transfer learning, particularly useful in scenarios where acquiring extensive labeled datasets is time-consuming and costly. This approach allows researchers to fine-tune pre-trained models for specific geospatial tasks. Alongside these advances, there has been a growing focus on models that understand spatial hierarchies, crucial for complex tasks such as urban planning (Stubbings et al. 2019, Zhou et al. 2021). However, challenges remain in creating models that can generalize across multiple tasks without extensive model retraining. This sets the stage for the emergence of foundation models, which offer a more unified and adaptable framework for handling the complex and diverse nature of geospatial data.\n2.2. Visual foundation models\nFoundation models have emerged as a transformative force in computer vision, as they offer a robust framework and an appealing prospect to facilitate domain adaptation with low computational cost. Several innovative elements contribute to their transformative impact. First, foundation models process extensive and diverse datasets, setting them apart from traditional models that often operate on small and domain-specific data (Bommasani et al. 2021, Li, Xu, et al. 2023). This broad data scope is crucial for capturing representative image features and patterns, and it sets the stage for foundation models' second defining characteristic: pre-training methodologies. Due to the sheer volume of data, traditional supervised learning approaches are often impractical, leading to a new strategy of self-supervised learning (SSL) for pre-training the foundation models (Awais et al. 2023). Because of this, the models are capable to generalize across a multitude of tasks (Yuan et al. 2021, OpenAI 2023, Touvron et al. 2023). Third, foundation models also allow for fine-tuning from a domain-specific dataset to further boost its performance and domain adaptation (Zhou et al. 2023). Collectively, these defining characteristics make foundation models both revolutionary and complex tools in the image analysis landscape.\nIn the realm of computer vision and image analysis, foundation models have been categorized into various types based on their prompting mechanisms and data modalities, as outlined by Awais et al. (2023). Textually prompted models like CLIP (Contrastive Language-Image Pre-training; Radford et al. 2021) and ALIGN (A Large-scale ImaGe and Noisy-Text Embedding; Jia et al. 2021) interpret visual data through text-based prompts, leveraging extensive image-text datasets for pre-training and visual question answering. Visually prompted models such as Segment Anything Model (SAM; Kirillov et al. 2023) and SegGPT (Wang, Zhang, Cao, et al. 2023) utilize visual cues such as bounding boxes or segmentation masks and often rely on partially synthetic datasets with pseudo labels. Heterogeneous modality models like CLIP2Video (Fang et al. 2021) and AudioCLIP (Guzhov et al. 2022) integrate multiple types of data\u2014vision, text, and audio for a more comprehensive understanding of the visual world. Lastly, generalist models like VisionLLM (Wang, Chen, et al. 2023) exemplify the ability to generalize across a multitude of tasks when provided with appropriate prompts. These models not only embody the defining characteristics of foundation models but also showcase the adaptability and diversity that make them a cornerstone in modern computer vision research.\nWhile foundation models have made significant strides in general-purpose computer vision, they come with their own set of limitations (Awais et al. 2023). One key issue is their limited contextual understanding, which can lead to a lack of depth when tackling geospatial tasks. For example, a general-purpose model might excel at a wide array of object recognition tasks but may struggle with the semantic interpretations required in specialized scientific or industrial applications (Kirillov et al. 2023). In addition, the use of diverse training data without human assessment can introduce biases or inaccuracies, leading to a propagation of such errors in downstream analyses (Glocker et al. 2022, W\u00f3jcik 2022). This issue is further compounded by the difficulty in customizing these models to achieve expert-level performance in a specific scientific domain. These limitations have led to a growing interest in specialized foundational models that are trained on data pertinent to a research field (Alfassy et al. 2022, Nguyen et al. 2023, Tu et al. 2023, Wu et al. 2023). These models aim to marry the generalizability and adaptability of foundation models with the knowledge required for domain-specific tasks.\n2.3. Geospatial foundation models\nThe quest for precision and contextual sensitivity in specific scientific domains has catalyzed the development of specialized foundation models. In the realm of geospatial analysis, this pursuit has led to the emergence of Geospatial Foundation Models (GFMs; Mai et al. 2023). Unlike their general-purpose counterparts, GFMs are designed to interpret the complex patterns of the Earth's surface and atmosphere. They address challenges such as spatial heterogeneity (Sun et al. 2023), temporal dynamics (Yao et al. 2023) and the multidimensional nature of geospatial data (Jakubik et al. 2023), marking an advancement in how we analyze and understand our planet.\nIn developing GFMs, Transformers have emerged as the preferred architecture, attributed to their superior management of long-range dependencies and the implementation of a dynamic attention mechanism. This allows Transformers to focus selectively on image segments, emphasizing features crucial for specific tasks. Notably, the Vision Transformer (ViT; Dosovitskiy et al. 2021) revolutionized image analysis by treating images as sequences of patches, leading to its widespread adoption (Cha et al. 2023, Sun et al. 2023, Wang, Zhang, Xu, et al. 2023, Dimitrovski et al. 2024). Building on this, the Swin Transformer (Liu et al. 2021) introduces a hierarchical design that enhances image processing efficiency (Sun et al. 2023). However, addressing complex challenges such as multiscale issues in spatial and temporal dimensions requires further enhancements to these models. For example, Jakubik et al. (2023) improved temporal data handling by integrating temporal information as channels. Yao et al. (2023) developed a three-branch network utilizing the Video Swin Transformer (Liu et al. 2022) to harmonize spatial affinity, temporal continuity, and spatiotemporal interaction. In addition, the approach of refining established foundation models for specific geospatial tasks illustrates how techniques from conventional image analysis can be adapted to meet the unique demands of remote sensing and geospatial applications. For instance, SAM (Kirillov et al. 2023) excels in object segmentation within images without predicting category information. Yan et al. (2023) enhanced SAM's segmentation capabilities for category-specific tasks by incorporating a new mask decoder and introducing a prompt encoder designed for SAR imagery, leveraging SAR-specific prompts. Similarly, Chen, Liu, et al. (2024) leveraged SAM for instance segmentation in remote sensing images, augmenting the model with a novel prompt learning technique. These adaptations showcase the potential of Transformers in addressing the complex needs of remote sensing imagery analysis and geospatial applications, demonstrating their versatility and effectiveness across a broad range of geospatial contexts.\nTraining GFMs predominantly utilizes Masked Autoencoders (MAE; He et al. 2022) due to their effectiveness in self-supervised learning (SSL) methodologies for imagery, offering a scalable approach to training without the need for labeled data. This SSL method, by obscuring parts of the input images and learning to reconstruct them, enables the models to learn rich representations of geospatial features and dynamics autonomously. However, certain scenarios necessitate supervised training and fine-tuning, particularly for downstream tasks or when models are developed with specific functionalities in mind, like segmentation that requires category information (Yan et al. 2023, Yao et al. 2023).\nIn our examination of GFMs, IBM's Prithvi (Jakubik et al. 2023) stands out for its unique approach to GeoAI and geospatial data analysis, prompting us to select it for detailed evaluation. Prithvi is unique among AI foundation models for its design that accommodates a 6-band input, including Red, Green, Blue, NIR, SWIR 1, and SWIR 2. This capability allows it to capture a broader spectrum of spectral information than the conventional RGB imagery, enhancing its versatility and efficacy across various geospatial tasks. In addition to its advanced spectral analysis capabilities, Prithvi has the advantage in its scalability through processing a large dataset across the continental US. Furthermore, as an open-source model, Prithvi encourages wider access and community-driven enhancements. It includes the release of trained model weights, allowing researchers to directly fine-tune it for a variety of downstream tasks, thus amplifying its utility and applicability in real-world scenarios. Beyond prior evaluations focusing on its semantic segmentation capabilities in flood mapping (Li, Lee, et al. 2023), our assessment of Prithvi extends to its domain adaptability in other crucial image analysis tasks using multiple datasets. Additionally, several enhancement strategies are applied on top of the native Prithvi model to maximize its potential for such analyses."}, {"title": "3. Data", "content": "In our evaluation on the Prithvi model, we utilized a total of four datasets, tailored to support two distinct visual recognition tasks: object detection and instance segmentation. In the subsequent sections, we delve deeper into the details of each dataset:\nMars crater dataset. The Mars crater dataset, employed in the 2022 GeoAI Martian Challenge, represents a comprehensive and varied collection of 102,675 images sourced from a global mosaic of Mars. Constructed using Mars Odyssey's Thermal Emission Imaging System (THEMIS) daytime infrared (DIR) data, the mosaic delivers a 100 m resolution covering Mars's entire surface, as documented by Edwards et al. (2011). Each image captures a 25.6 km by 25.6 km area, presented in 256 \u00d7 256 pixels, offering a detailed and representative snapshot of Martian terrain. Over 301,912 craters are annotated by Geology experts with instance-level bounding boxes, drawing on the extensive Mars impact crater catalog by Robbins and Hynek (2012), a compilation from multiple rounds of manual reviews of infrared imagery and topographic data, documenting over 640,000 craters with detailed positional, morphological, and morphometric information. Following the process described by Hsu et al. (2021), the dataset was developed by extracting non-overlapping samples from the global mosaic, applying distortion correction, and addressing partially visible craters. The dataset showcases diverse crater sizes, from as small as 0.7 km to as large as 25.5 km in diameter. This diversity poses a unique challenge, requiring models to accurately discern features in both sparsely and densely cratered landscapes.\nEarth's natural feature dataset. The development of the natural feature dataset, as reported in the work by Li and Hsu (2020), represents a significant effort to compile a diverse collection of environmental features crucial for advancing research in geospatial analysis and landscape scene understanding. This foundational dataset for the current study includes over 100 manually labeled remote-sensing images for each of eight distinct natural features: craters, volcanoes, rivers (encompassing both meandering and non-meandering types), lakes, sand dunes, hills, and iceberg tongues. The initial phase involved utilizing geographical gazetteers, with an emphasis on the United States Geological Survey (USGS) Geographic Name Information System (GNIS), for accurate identification and categorization of various terrain objects.\nThis was followed by gathering and labeling images from multiple sources such as Google Earth, the USGS Earth Explorer, and relevant Google Images search results.\nThis dataset consists of a moderate number of images with a relatively low density of objects per image. The features in this dataset exhibit significant size variability, from medium sized to very large (see statistics in Table 1), reflecting the natural contours and diverse scales of these features. The varying image sizes add to the complexity of the detection task. The imagery has diverse spatial resolutions and spectral bands, including 1-meter optical imagery from the USGS National Agriculture Imagery Program, as well as sub-meter optical images and 2-meter multi-spectral images from DigitalGlobe's Worldview-2 satellite. Each image in this extensive dataset is accompanied by detailed annotation data, including bounding boxes to accurately delineate the terrain features of interest. The compilation of this dataset not only facilitates the current study but also supports a wide range of research avenues, particularly in the development of landscape scene recognition techniques.\nIce-wedge polygon dataset. The Ice-wedge polygon (IWP; Bhuiyan et al. 2020) dataset stands as a critical resource for mapping permafrost landscapes at a pan-Arctic scape. This dataset contains a collection of 867 image tiles with 34,931 annotated IWPs spread across a diverse array of tundra vegetation types, including sedge, tussock, and barren tundra (Li, Hsu, et al. 2022). This dataset is distinguished by its precision in annotation, featuring instance segmentation masks that accurately delineate each IWP, thereby facilitating fine-grained image analysis tasks. Originating from very high-resolution (0.5m) imagery captured by Maxar sensors, the dataset highlights the variability and complexity of the tundra landscape. The dataset has a higher density of feature distribution within the image scenes compared to other datasets (see statistics in Table 1). The sizes of these features vary significantly, ranging from one pixel to nearly covering the entire image scene.\nEuroCrops. The EuroCrops dataset is the most comprehensive open-access dataset in the European Union, featuring 944 image scenes and corresponding crop land labels captured in April 2019 (Schneider et al. 2021). The dataset, derived from two cloud-free Top of Atmosphere (TOA) Sentinel-2 images, offers a spatial resolution of 10 meters, focusing on central Denmark's agriculturally rich and flat terrains. Each image scene is sized at 128 by 128 pixels, with detailed labels on five cropland classes: spring cereal, winter cereal, maize, grassland, and \"other\". The features vary from very small (2 by 3 pixels) to quite large (128 by 128 pixels) within the fixed-size image scenes (128 by 128 pixels). This dataset has a moderate to high density of objects per image, with a wide range of object sizes (see statistics in Table 1). The 6-band nature of this dataset makes it particularly helpful for evaluating the advantages of Prithvi, which is also trained on six-band remote sensing imagery, to support agriculture research."}, {"title": "4. The Prithvi model, task-specific adaptation, and model enhancement", "content": "4.1. Model architecture and pre-training\nIn the development of NASA-IBM's Prithvi model, the pre-training phase plays a crucial role (Jakubik et al. 2023). The model is trained on HLS data, a dataset that fuses measurements from multiple satellite sensors, including NASA/USGS Landsat 8 and 9's Operational Land Imager (OLI) and Europe's Copernicus Sentinel-2A and Sentinel-2B's Multi-Spectral Instrument (MSI; Masek et al. 2021). To ensure the consistency and reliability of this data, the HLS project employs algorithms for atmospheric correction, cloud and cloud-shadow masking, and spatial co-registration. Specifically, Prithvi was trained on the HLSL30 product, which offers a 30-meter spatial resolution and is provided in a Cloud Optimized GeoTIFF (COG) format. The training data spanned the continental United States for the year 2017 and focused on six spectral bands, namely Blue, Green, Red, Narrow NIR, SWIR 1, and SWIR 2."}, {"title": "4.2. Task-specific adaptation of Prithvi: model head and fine-tuning", "content": "Upon completing the pre-training of Prithvi, the next step is to adapt it for specific downstream tasks. This is achieved by appending a task-specific decoder (also called a model head) to Prithvi's encoder to achieve different image analysis goals. shows our proposed image analysis pipeline that integrates Prithvi's pretrained encoder and is customized for object detection and instance segmentation tasks.\nTo achieve these image analysis goals, we developed the pipeline utilizing the model heads (decoder) inspired by the Mask R-CNN architecture (He et al. 2017). As depicted in Figure 3, when using the feature map generated by Prithvi's encoder as input, the task-specific adaptation module (labeled as 4.2 in the rightmost column of the figure) begins with the Region Proposal Network (RPN), which operates on the feature map extracted from the previous stage. The RPN employs a sliding window mechanism over the feature map to generate potential bounding box proposals for objects across various scales and aspect ratios. Following the generation of these region proposals, the workflow proceeds to process the Region of Interest (RoI). The Rol Align layer is employed here to convert these proposed regions of varying sizes into fixed-sized feature maps, enabling consistent processing by downstream layers. After the RoI Align layer, each region undergoes processing by two distinct branches to generate comprehensive outputs. The first branch, the detection branch, is tasked with bounding box refinement and object category classification. It refines the initial proposals to more accurately enclose the objects and classifies each object into its respective category. Following this, the second branch, the mask branch, comes into play specifically for mask prediction for each identified object. This branch is dedicated to determining the exact pixels within the refined bounding box that constitute the object, enabling the model to produce detailed masks that delineate the object's precise shape and boundaries.\nUpon this pipeline, the Prithvi model can be further fine-tuned with domain datasets for the desired tasks. When performing object detection, only the box branch is activated, whereas for instance segmentation, both the box and mask branches are employed. Training Prithvi on task-specific datasets enables the model to adapt its pre-trained knowledge to the unique characteristics of new datasets. This phase entails adjusting the weights across the entire model, including both the backbone and the appended head modules. Another strategy is to freeze the backbone weights and only train the decoder part to reduce training time and computational cost. In our study, we chose to fine-tune modules throughout the pipeline including Prithvi's backbone models so to achieve optimal performance. It is worth mentioning that this pipeline is also generalizable so the performance of Prithvi with other models can be compared by replacing the feature extractor encoder.\nTo further improve the adaptability and predictive performance of the Prithvi model in downstream tasks, and to enable the incorporation of the most common 3-band image as input, we enhanced the pipeline by introducing two modules: the band adaptation module and the multi-scale feature map generation module. In addition, this pipeline enables the flexible integration of other pre-trained backbone models for performance comparison. The next sections will introduce how each strategy works and conducts a comparative analysis of different models."}, {"title": "4.3. Band adaptation module", "content": "The Prithvi model is intrinsically designed to handle 6-band geospatial data, maximizing the use of the important information such multiband data provides. However, in many real-world scenarios, benchmark datasets (Bhuiyan et al. 2020, Schneider et al. 2021) may have a different band configuration than the Prithvi model. To increase the Prithvi model's applicability across diverse datasets, we developed three strategies (as shown in Figure 4) to adapt its original 6-band input to data with a different number of spectral and optical bands. The adaptation to the most commonly used 3-band RGB imagery is used as an example.\nThe first strategy, termed as the Zero-Padded Input, is depicted in Figure 4(b). This method involves augmenting 3-band data by appending three channels filled with zeros (depicted in black), simulating a 6-band input but devoid of any additional meaningful information. While this method seems to artificially inflate the data, it is computationally equivalent to adjusting the model's weight loading to retain only the weights associated with the existing bands. This is due to the convolutional nature of the patch embedding layer that transforms the input data. Importantly, we maintain the use of the original CNN kernels in the patch embedding layer, as demonstrated in the initially trained model shown in Although straightforward, this method might not fully tap into the model's capabilities, as it operates under the assumption that the missing three bands have a minimal bearing on the model's overall performance.\nAnother strategy, referred to as Channel Duplication, is detailed in Figure 4(c). In this method, the existing 3-band channels are replicated to create a 6-band input, with the duplication of red, green, and blue colors evident across the six bands. This method is based on the assumption that the initial 3 bands are sufficiently informative for the model's tasks and that the missing bands don't differ significantly in their feature-capturing capabilities compared to the available 3 bands. However, if the original 6-band model was designed to capture different types of features across all six bands, this method may not adequately substitute for that missing information. Like the Zero-Padded Input strategy, Channel Duplication also employs the same CNN kernels in the patch embedding layer, maintaining the same with the model's initial training status.\nLastly, we developed the Retrained Patch Embedding strategy, as illustrated in Figure 4(d). Rather than modifying the data to fit the model, this approach reconfigures the initial patch embedding layer of Prithvi to directly process 3-band data. Rooted in the belief that the model's architecture is versatile enough to adjust to fewer bands and that these 3 bands encompass all vital information, this method presents itself as a potentially more robust and sophisticated solution. As depicted in Figure 4(d), the CNN kernel within the patch embedding layer is reinitialized and tailored to process 3-band data, reducing the channels from 6 to 3 compared to earlier configurations. This modification not only streamlines the data processing but also reduces the model size, cutting down 590k parameters from the original model, thereby enhancing the efficiency."}, {"title": "4.4. Comparative analysis of Prithvi with other pre-trained models: benchmarking performance and adaptability in geospatial data processing", "content": "Leveraging the pipeline depicted in Figure 3, our goal in comparing Prithvi with established architecture is to pinpoint its strengths and potential areas for enhancement, with a particular focus on its training with geospatial data. To facilitate this evaluation, we have chosen three prominent, task-specific models: ViT (Li, Mao, et al. 2022), MViTv2 (Li, Wu, et al. 2022), and ResNet-50 (He et al. 2016). The comparison aims to highlight how Prithvi's unique approach to processing geospatial data compares to these well-established models. The architectures of these four models, including Prithvi, are detailed in Figure 5 (a)-(d), providing a visual reference to understand the structural differences.\nViT, as illustrated in Figure 5(a), represents a significant shift in computer vision, adopting the Transformer architecture originally developed for natural language processing (NLP). In this model, images are divided into patches, each processed akin to a token in NLP, enabling the Transformer to grasp global pixel relationships from the start. Similar to Prithvi, shown in Figure 6(c), both models operate predominantly at a single scale, producing single-scale feature maps, which aligns their approach to processing visual data. The key difference between them lies in how they pay attention to these patches. Prithvi uses a multi-head attention mechanism, which looks at the image patches in a way that considers the entire image context, akin to taking a step back to see the whole picture. On the other hand, ViT employs multi-head window attention, which means it focuses on smaller, windowed areas of the image at a time, similar to zooming in on specific details. This difference in attention methods underlines the unique ways ViT and Prithvi handle visual information, making their comparison particularly insightful for understanding how each could be best used in analyzing geospatial data.\nResNet-50, as depicted in Figure 5(b), distinguishes itself from transformer models like Prithvi and ViT by utilizing a CNN structure capable of generating hierarchical features. Unlike the single-scale focus typical of transformer architectures, ResNet-50\u2019s convolutional layers are organized hierarchically, enabling the extraction of features at multiple scales. A key aspect of ResNet-50\u2019s architecture is its residual connections, highlighted in Figure 5(b). These connections employ shortcut pathways that bypass one or more layers, directly addressing the vanishing gradient problem by allowing gradients to flow through the network more effectively. This innovation is particularly important as it enables the efficient training of deeper networks by ensuring that the added layers contribute positively to the overall performance, rather than complicating or degrading it. The operation on the residual connection is specifically designed to activate when there is a discrepancy between the input and output dimensions, ensuring smooth transitions and dimensional consistency across the network. By facilitating deeper and more efficient network architectures without loss in performance, ResNet-50 has set a benchmark in computer vision, making it an important model for comparative studies with transformer-based models.\nMViTv2, illustrated in Figure 5(d), extends the Vision Transformer architecture by incorporating a unique multi-scale attention module. This key feature enables MViTv2 to emulate the multi-scale feature generation typical of CNNs such as ResNet-50, blending the extensive contextual awareness of transformers with the precise, scale-sensitive processing characteristic of CNNs. This fusion creates a hybrid model that stands out in the architectural spectrum, offering a promising avenue for comparative analysis. MViTv2's ability to produce hierarchical, multi-scale features marks it as a significant model for enhancing geospatial data analysis, bridging the divide between the singular scale focus of traditional transformer models and the layered, hierarchical structure observed in CNNs. Notably, MViTv2 incorporates a specialized pooling operation within its residual connections, a mechanism designed to activate specifically when there is a discrepancy between the input and output dimensions. This adaptive feature ensures smooth transitions across dimensions, preserving essential information without compromising the integrity of the data being processed."}, {"title": "4.5. Multi-scale feature map generation module", "content": "In the domain of deep learning", "backbones": "the first type, as illustrated in Figure 6(a), is capable of"}]}