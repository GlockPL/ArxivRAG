{"title": "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling, Search Algorithms, and Relevant Frameworks", "authors": ["Xinzhe Li"], "abstract": "LLM test-time compute (or LLM inference) via search has emerged as a promising research area with rapid developments. However, current frameworks often adopt distinct perspectives on three key aspects-task definition, LLM profiling, and search pro-cedures-making direct comparisons challenging. Moreover, the search algorithms em-ployed often diverge from standard implementations, and their specific characteristics are not thoroughly specified. In this survey, we provide a comprehensive technical re-view that unifies task definitions and provides modular definitions of LLM profiling and search procedures. The definitions enable precise comparisons of various LLM in-ference frameworks while highlighting their departures from conventional search algo-rithms. We also discuss the applicability, performance, and efficiency of these meth-ods. For further details and ongoing updates, please refer to our GitHub repository: https://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md.", "sections": [{"title": "1 Introduction", "content": "Scaling test-time compute via search has recently enhanced the LLMs' power to a new level on reasoning tasks (Yao et al., 2023a; Hao et al., 2023), sequential decision-making tasks (e.g., robotics) (Putta et al., 2024), and graph-traversal tasks (e.g., path finding) (Meng et al., 2024). This survey aims to provide a comprehensive but integrated survey on existing frameworks for LLM-based search. The focus is on the work that the search processes are coupled with LLMs' test time compute rather than those using search and LLMs separately. For example, the plans (commonly in the form of PDDL) are prepared by LLMs to perform local search (Valmeekam et al., 2023b; Guan et al., 2023; Valmeekam et al., 2023a).\n\n1.1 Existing Surveys\n\nCurrent reviews on LLM search are limited from the following perspectives.\n\nNo dedicated, Detailed Survey Current surveys only contain paragraphs or sections to roughly touch on both technical aspects and their practical applicability, as summarized in Table 1.\n\nLimited Mention on LLM-Side Design Specifically, most of existing surveys (Huang et al., 2024; Wang et al., 2024b) mention little or a few implementations and dimensions for LLM profiling, which is not suitable for all the frameworks. Besides, the lack of examples hinders understanding.\n\nLimited Mention on Search Li (2024) give more detail regarding LLM profiling but lack details on search processes. Nonthelessness, most of existing surveys (Huang et al., 2024; Wang et al., 2024b) give a general sense of the computation process by mentioning which classical search algorithms the frameworks are built upon (e.g., depth-first search). However, details should be given because of their nuanced differences. Besides, many untypical twists to classic search algorithms are hidden. The deviations are not friendly for newcomers in the newly-developed area, e.g., those computer science graduates educated with typical search algorithms."}, {"title": "1.2 Survey Structure", "content": "To solve the above limitations, we provide unified task definitions and decouple the LLM-specific design (mainly prompting) from the control program (search procedures/algorithms). There exists a hierarchical structure between them: the low-level definitions provide a unified interface for the high-level components. The overall structure, accompanied by illustrative examples, is presented in Figure 1.\n\nIntroducing a Unified Task Definition Based on MDPs (\u00a7 2) Our definition standardizes different tasks in MDP structure. While MDPs naturally align with AI domains like robotics, special attention is given to adapting this definition for tasks traditionally not modeled as MDPs, such as graph traversal, reasoning, dialogue systems, and code generation. Notably, this MDP-based definition is also applicable to other LLM inference frameworks beyond search, including works like Li et al. (2022), Zhao et al. (2023), and Hu et al. (2024).\n\nComprehensively Summarizing LLM Profiling and Implementations (\u00a73) The design and imple-mentation of LLM profiling and prompting can be modularized into components commonly used in solving MDPs (Sutton & Barto, 2018): policies, value functions, and transition models. Correspondingly, 3 types of LLM-Profiled Roles (LMPRs) are defined.\n\nDefining Modular Search Procedures (\u00a7 4) Rather than directly showcasing individual search-based frameworks for LLM inference, we focus on modular and reusable components to reduce redundancy and enable more straightforward comparisons across frameworks. This approach promotes flexibility and minimizes overhead when adapting or extending search methods.\n\nReviewing Individual Frameworks (\u00a7 5) Based on the unified task and LMPR interface, we provide a comprehensive review of individual frameworks, organized by the search algorithms they are built upon. Our analysis highlights how LLM integration either diverges from or enhances traditional search algorithms. We identify and clearly present 11 frameworks, summarized in Table 7. This count exclusively includes frameworks that focus on test-time computation through search detailed in Section 5. Additionally, we highlight other test-time frameworks that function as components within search processes, such as ReAct (Yao et al., 2023b), CoT (Wei et al., 2022), and Self-Consistency (Wang et al., 2023), along with those discussed in Section 7."}, {"title": "1.3 Intended Audience and Use Cases", "content": "Reusable Modules While we strive to provide comprehensive coverage of the latest research, we ac-knowledge the rapid pace of advancements in the field, where variations in LLM profiling and search im-plementations may not be covered. Nonetheless, this survey offers a collection of classical and reusable implementations that can serve as solid foundations for future research and development."}, {"title": "2 Task (Re)formulation", "content": "Tasks solved by LLM-integrated search are inherited from both the \"LLM\" side (human language tasks) and the \"search\" side (structured tasks): 1) language reasoning: LLMs are naturally applied to reasoning tasks in language (Wei et al., 2022). 2) structured tasks: On the other hand, search algorithms are more conventionally utilized for structured tasks, such as web navigation, robotic navigation, gaming, and graph traversal (Russell & Norvig, 2010; Sutton & Barto, 2018). The convergent nature is that all of them belongs to sequential decision-making, e.g., reasoning often involves generating and evaluating sequences of logical steps or decisions to arrive at a conclusion.\n\nA MDP-Like Formulation To enable a clear comparison across different frameworks, this section for-mulates the tasks in Markov Decision Processes (MDPs) (S, A, T, R). In addition, observations O are often considered in Partially Observable Markov Decision Processes (POMDPs) (Li et al., 2022; Zhao et al., 2023; Hu et al., 2024):\n\n\u2022\nA set of states S, including the goal state $s_g$;\n\n\u2022\nA set of observations O, where $o_t$ is the partial observations of the state $s_t$ at time step t;\n\n\u2022\nA set of actions A, where $a_t \\in A$ is the action on $s_t$;\n\n\u2022\nTransitions $T (s_{t+1} | s_t, a_t)$, which define the dynamics from one state to another after executing an action;\n\n\u2022\nRewards R: A rewards evaluate the \"quality\" of a state-action pair or a trajectory towards the desired outcomes or goals.\n\nThis section only discuss task elements that are external to agents and exist independently of how agents operate or learn. We will see in the next section how the POMDP setting fits in LMPRs and agent definitions, where the Markovian assumption is broke.\n\nA Summary of Concrete Tasks Some structured tasks (e.g., recycling robot, gridworld, and chess) are always modeled as MDPs. These typical MDP tasks are actively studied in the domains of reinforcement"}, {"title": "2.1 Web Navigation", "content": "Another type of tasks is to navigate on websites for shopping and retrieving information (Zhou et al., 2024b)"}, {"title": "2.2 Graph Traversal in MDPs", "content": "A graph traversal problem, e.g., robotic navigation, is represented as a graph G = (V, E), where V is a set of vertices (or nodes), and E\u2282 V \u00d7 V represents the transitions between them. However, this definition is the common task settings for uninformed and informed search. However, these algorithms integrated with LLMs can be generalized beyond this definition. This is why recent work (Yao et al., 2023a) that uses these search algorithms along with LLM often uses MDP terminology, but does not include formal unification. Hence, we propose a conceptual framework that views graph traversal as a simplified version of MDPs. This framework can be described as follows:\n\n\u2022\nStates: Each node v \u2208 V is a state.\n\n\u2022\nActions: Actions are represented by edges E. The action space is generally considered homogeneous because the type of action is uniform, such as \"MOVE[arg]\", where \"[arg]\" represents parameters like direction or target node.\n\n\u2022\nTransitions: Following an edge from a node s via an action (edge) (s, s') \u2208 E always leads to the same node s'. Hence, the transitions T are deterministic:\n\n\u2022\nRewards R(s) are typically binary, with R(s) = 1 if s \u2208 G (i.e., s is a goal state) and R(s) = 0 otherwise.\n\n\u2022\nHeuristics for nodes h(s) can be interpreted as estimates of the value function in the MDP formu-lation, representing an estimated cost-to-go from node s to a goal node g \u2208 G.\n\nAs best as we know, this conceptualization is not explicitly stated in any peer-reviewed literature."}, {"title": "2.3 Language Reasoning in MDP", "content": "The formulations of language reasoning tasks are more diverse and creative. Although not exhaustive, the following paragraphs summarize forms that are particularly used in the current study of LLM-integrated search.\n\nReasoning (T via Concatenation) A reasoning process can be concretized as a Chain of Thoughts (COT) $T_1, T_2, ...$ (Wei et al., 2022), each expressed as a sequence of tokens via LLM generation. The reasoning steps can be not only naturally evolved but also deliberately specified. For creative writing, the first step can be specified as planning, and the second step is to generate according to the plan (Yao et al., 2023b) Following previous work (Li, 2024; Wang et al., 2024a), the MDP formulation includes:\n\n\u2022\nActions: An action is a thought consisting of several tokens, i.e., $a_1 = T_1$.\n\n\u2022\nStates: The initial state $s_1$ is defined by the task information, e.g., a user query, a problem description or the goal. The following states are defined as the concatenation of the following thoughts:\n\n$s_t = (s_{t-1}, a_{t-1}) = (s_1, a_1, ..., a_{t-1})$\n(1)\n\nApparently, directly concatnenating open actions leads to the open state space. When the reasoning is naturally evolved, the final state $s_g$ comes when the final thought $T_r$, or the entire chain expresses a valid response. It can be known in which step $s_g$ is reached for deliberate reasoning steps."}, {"title": "2.4 Code Generation in MDP", "content": "This is similar to language reasoning under Deterministic T via Concatenation. The only difference is that an action is a token in the vocabulary set of the LLM (rather than a thought consisting of several tokens). Such definition is originally proposed by Zhang et al. (2023). Under their definition, \u201cthe reward of state s is the pass rate of the program on the public test cases. The reward of a partial program is always 0.\""}, {"title": "2.5 Goal-Oriented Dialog in MDP", "content": "Previous work (Wang et al., 2020) frames goal-oriented dialog as MDP. Yu et al. (2023) begin using such formulation for LLM-integrated search. The formulation is demonstrated below.\n\n\u2022\nActions: An action $a \\in A$ indicates the intent, which is predefined. For example, the intent to convince the Persuadee using reasoning and factual evidence is defined as \"Logical Appeal\". This is commonly termed \"dialog act\" (Wang et al., 2020).\n\n\u2022\nStates: $s_t$ is defined as the dialogue history of previous t turns, containing dialog act and agent/user utterances\n\n$h_t = (agent_1 , u_1^{agent}, u_1^{usr}, ..., a_t^{agent}, u_t^{agent}, u_t^{usr})$\n(3)\n\n\u2022\nTransitions: It \"represents the dialogue state updates according to stochastic responses from the user to the agent.\" (Wang et al., 2020)"}, {"title": "2.6 Discussion", "content": "Accessibility of Action-State Transition In environments under deterministic transitions (e.g., dialog, code generation), the next state s' can be directly derived based on the selected action. In a dynamic environment, s' can be either sampled over the probability distribution or generated from Imprtransition Section 4 will demonstrate how this property affects search procedures.\n\nOverhead of Using MDP Definition Although the comprehensive definition provides a unified interface to discuss LIS frameworks, it increases the overhead when applied to graph traversal tasks, since several defining characteristics of an MDP are not necessary, e.g., state transitions and explicit definitions of actions.\n\nWhy Is There No Previous Work Defining Reasoning Tasks as MDPs? Typical MDPs are often defined for decision-making models which can only handle the tasks whose action space is constrained and finite. However, general-purpose models like LLMs naturally deal with infinite or/and hard-to-define action space, since LLMs can infer plausible actions with world knowledge and commonsense.\n\nDo Tasks Enable Action Undoing and State Back-Up? Some environments allow going back up to an earlier step after executing a sequence of actions (e.g., reasoning tasks), while other tasks may not (such as robotic tasks). This property is particularly important to discuss the applicability of LLM-integrated search methods. For environments under such property, the LIS agent can feel free to simulate future states for planning without worrying that the change of environments is irreversible. Details will be discussed in \u00a7 5)."}, {"title": "3 LLM-Profiled Roles", "content": "Following standard reinforcement learning terminology (Sutton & Barto, 2018), an agent designed to solve Markov Decision Processes (MDPs) typically incorporates the following components:\n\n\u2022\nPolicy $\u03c0(a_t | g, s_t)$: Determines the action $a_t$ to take given the current state $s_t$.\n\n\u2022\nValue Function $V^\u03c0(s) \u2192 R$: Estimates the expected return of state s under policy \u03c0.\n\n\u2022\nTransition Model $T(s_{t+1} | s_t, a_t)$: Represents the dynamics of the environment, predicting the next state $s_{t+1}$ given the current state $s_t$ and action $a_t$.\n\nThese definitions are broadly applicable across different agent designs. In this work, we adapt them to LLM-based search and focus on how to profile LLMs to work as/for these agentic components.\n\nBackground of LLM-Profiled Policy, Evaluator and Transition Model This section outlines the implementation of the three core components using three types of LMPRs. These roles are defined by Li (2024) as the LLM-profiled policy (Imprpolicy), evaluator (Impreval), and transition model (Imprtransition). For brevity, these notations are commonly adopted throughout this work.\n\nWhile prior studies such as Spiegel et al. (2024) and Feng et al. (2024) explored these LMPRs primarily in theoretical contexts and toy environments for reinforcement learning, this section extends these ideas by presenting detailed implementations in real-world tasks.\n\nPresentation of Prompting Examples To illustrate how LLMs are configured for different LMPR roles, we provide prompting examples throughout the paper. Model outputs are visually distinguished using shadow boxes for clarity. For example:"}, {"title": "3.1 LLM-Profiled Policy (LMPP)", "content": "Imprnaive_policy Given the observation $o_t$, Imprnaive_policy directly generates the next action $a_t$.\n\nImprreasoning_policy To generate $a_t$, this policy first produces a complete reasoning path that explains or justifies the generation of $a_t$. The reasoning path serves as an explicit intermediate step, enhancing interpretability and illuminating the decision-making process for $a_t$.\n\nImprreact_policy In contrast to Imprreasoning_policy, Imprreact_policy separates the reasoning step and the action-generation step into distinct inference passes. Each pass corresponds to an uninterrupted generation session. The reasoning text may include a planning path (e.g., $\u0101_{t+1}, ..., \u0101_\u03c4$), but only $\u0101_t$ is used for search. Another distinguishing feature is the more autonomous behavior of this policy, which does not strictly adhere to a fixed reasoning-then-acting sequence. Instead, it can dynamically alternate between reasoning and acting steps, such as reasoning-acting-acting. For example:\n\nYour task is to: put a cool tomato in microwave.\n\n>\n\nthink: To solve the task, I need to find a tomato, then cool it with the fridge, and\nfinally put it in the microwave. <more thoughts>\n\n\u039f\u039a.\n\n> go to countertop 1\n\n<observation>\n\n> go to countertop 2\n\nThe term \"react\" is attributed to the work of ReAct (Yao et al., 2023b). However, in their formulation, each thought is not explicitly treated as an action; instead, only tool invocations are considered actions in reasoning tasks. This distinction highlights the broader applicability of Imprreact_policy in our definition."}, {"title": "3.2 LLM-Profiled Transition Model (LMPT)", "content": "Imprtransition predicts outcomes according to LLMs' internal knowledge. The profiling can be categorized as generating: 1) Full state: The final goal is to return a full state/observation at the current step, as exemplified in Example 2.\n\n<profile information>\n[STATE 0] I have that, the white block is clear, the cyan block is clear, <more detail>\n[ACTION] Pick up the brown block.\n[CHANGE]\n\nThe hand was empty and is now holding the brown block, the brown block was on\nthe table and is now in the hand, and the brown block is no longer clear. [STATE 1]\nI have that, the white block is clear, the cyan block is clear, <more detail>\n\n2) Partial observation: The partial observation would be further processed to form the full state. One obvious task is reasoning via QAs.\n\nGiven a question, please decompose it into sub-questions. For each sub-question, please\nanswer it in a complete sentence, ending with \"The answer is\". When the original question\nis answerable, please start the subquestion with \"Now we can answer the question:"}, {"title": "3.3 LLM-Profiled Evaluator (LMPE)", "content": "LLMs can serve as flexible evaluators (LMPEs) by leveraging their generative and probabilistic capabilities. We propose categorizing these evaluators along three key dimensions:\n\n\u2022\nTask Formulation: Whether the evaluation is a binary classification, multi-choice QA, or a free-form judgment influences how the LLM's output or logits can be interpreted. These tasks are always formulated in LLMs' system-level prompts.\n\n\u2022\nState vs. Action Evaluation: This is analogous to state/action-state value functions in rein-forcement learning. Depending on whether the evaluator is assessing a static state st or a transition (st, at), the LLM must parse different context inputs to provide a valid judgment."}, {"title": "3.4 Discussion", "content": "Inference Cost of Imprpolicy In practice, the overall computational cost follows the pattern\n\n$Impr_{react\\_policy} > Impr_{reasoning\\_policy} > Impr_{naive\\_policy}$.\n\nThe gap between Imprreasoning_policy and Imprnaive_policy arises from the additional output tokens produced for reasoning. More importantly, when commercial API is used, Imprreact_policy exhibits an even higher cost because each separate reasoning or action-generation pass is effectively stateless with respect to the cached K-V pairs from previous passes, thereby preventing token-reuse optimizations.\n\nApplicability of Imprreact_policy. A central requirement for ReAct-style prompting (Imprreact_policy) is the availability of step-wise observations after each action. This imposes two prevalent scenarios:\n\n1. Tasks relying on simulators: When direct interaction with the real environment is impractical (e.g., actions on tasks are irreversible), a simulator can be substituted to generate the observation following each action. For instance, an LLM-based simulator (Imprtransition) might use commonsense knowledge to model environmental responses (e.g., turning on a water tap in a sealed sink). However, such simulators are unsuitable for tasks involving external or private data-like querying proprietary databases or retrieving up-to-date information\u2014since an LLM's internal knowledge typically cannot replicate these data sources.\n\n2. Action-reversible tasks. Certain problems can be retried or backtracked, allowing the agent to iteratively act, observe, and refine its actions, as discussed in Section 2. In Section 5, for exam-ple, LLM-based search frameworks such as LATS (Zhou et al., 2024a) leverage this property when interacting with real environments across multiple search steps to perform monte-carlo simulation.\n\nRisk of Impr value Although Imprvalue can effectively evaluate state or action quality, two challenges stand out:\n\n1. Mediocre discrimination abilities: As shown by Chen et al. (2024b), using logits as dense re-wards (e.g., in Imprpolicy&eval or Imprtransition&eval) can reveal that many open-source LLMs struggle to reliably distinguish \"good\" from \"bad\" examples.\n\n2. In-Context Reward Hacking (ICRH): According to Pan et al. (2024), an LLM evaluator (Imprvalue) may attempt to \u201cexplain away\u201d negative feedback by globally altering its reasoning and actions, potentially violating constraints. For example, to fix an INSUFFICIENTBALANCEERROR, the LLM might suggest unauthorized money transfers from other accounts, thus compromising safety or policy compliance.\n\nNot All Generation with \"Reasoning\" is Truly Augmented. By design, LLMs generate tokens in an auto-regressive manner, meaning earlier tokens are not influenced by later ungenerated tokens. Hence, although reasoning tokens after actions (or evaluation) can make the model outputs more interpretable, they do not always alter subsequent decisions or evaluations. In Xie et al. (2023), for instance, a chain of thoughts\n\n$a_t, a_{t+1}, ....., \u0101_\u03c4$\n\nis produced, where $\u0101_{t+1},..., \u1fb6_\u03c4$ are \u201cunrecorded\" actions. Crucially, $a_t$ is unaffected by any future a tokens, making this effectively a naive policy rather than a true reasoning-augmented approach.\n\nSimilarly, consider the evaluator in Example 9:\n\nGiven a question and some sub-questions, determine whether the last sub-question is useful\nto answer the question. Output 'Yes' or 'No', and a reason."}, {"title": "4 Search Procedures", "content": "This section presents the reusable search procedures applied across various frameworks, including both non-LMPR-specific and LMPR-based procedures. Unlike Section 3, which focused on configuring LMPRs, here we demonstrate how these LMPRs are integrated into the operational processes. However, some content may overlap slightly for coherence.\n\nSearch Nodes: Integrating States, Action, and Rewards In this section, we shift our focus to search and clarify how the fundamental search \"node\" is defined with respect to states and actions. Some methods (e.g., ToT (Yao et al., 2023a)) treat a node as a particular state in a search tree, with transitions determined by the actions taken. To ensure generality, we unify states, actions, and even their estimated values (or rewards) in a single node structure (e.g., RAP (Hao et al., 2023)), facilitating partial expansions or multi-step lookahead. To align with object-oriented design, we represent a node as n with attributes n.action, n.state, n.parent, and n.val, representing the action, state, parent node, and value, respectively."}, {"title": "4.1 First-Order Procedures", "content": "First-order procedures operate independently, without relying on other procedures. They serve as the foun-dational components upon which more complex procedures are built, ensuring a modular and scalable frame-work for LLM-based search operations. The first three are based on LLM-Profiled Policy (LMPP), evaluator (LMPE), and transition model (LMPT), respectively, while others not necessarily depend on LMPRs."}, {"title": "4.2 Higher-Order Procedures", "content": "Value-Based Selection The first type is top-k selection. The top k states or actions are picked from a large pool of candidates based on their estimated values. A state-value function V(s') or an action-value function Q(s, a) is used to generate values. Commonly, they are implemented by LMPE+ evaluation. Note that the if statement for value assignment also allows specialized ways to assign values without necessarily relying on n'.state."}, {"title": "5 Frameworks Based on Search Algorithms", "content": "This section summarizes how different frameworks utilize search algorithms, leveraging the LMPRS and search procedures introduced in Table 7. Note that, some MCTS-specific procedures (e.g., MCTS selection,"}, {"title": "5.1 Beam Search", "content": "Beam search can be employed for reasoning tasks through two procedures iteratively:\n\n1. LMPP Expansion: Each set of beam nodes Nt is passed to the LMPP expansion proce-dure. Internally, SAMPLE_LMPP calls either SAMPLE_ACTIONS_ONE_AT_A_PASS or SAM-PLE_ACTIONS_BATCH, while SIMULATE can be a simple concatenation transition: each node nt \u2208 Nt has its parent state nt.parent expressed as a sequence of actions ($a_1,...,a_{t\u22121}$), an ac-tion at assigned to nt.action, and the new node's state (nt.state) as ($a_1,...,a_{t\u22121}, a_t$). The resulting set of expanded nodes is denoted $N_{t+1}^{sample}$.\n\n2. TopK-Based Selection: From $N_{t+1}^{sample}$, a value-based selection procedure (e.g., Procedure 4) is applied to pick the top-k nodes (the beam size). This subset is returned as $N_{t+1}$.\n\n3. The process repeats until reaching a terminal state.\n\nBelow are two frameworks that illustrate how beam search is adapted:\n\nXie et al. (2023)\n\n\u2022\nUses a value function implemented by $Impr_{values} + Impr_{policy&eval1}$.\n\nPathFinder (Golovneva et al., 2023)\n\n\u2022\nComputes a summed similarity score as the value for each candidate node, comparing its state with those of other beams $N_{t+1}^{sample}$. The similarity function can be as simple as n-gram overlap."}, {"title": "5.2 Breadth-First Search", "content": "Tree-of-Thoughts (ToT) (Yao et al., 2023a)\n\n\u2022\nSimilar to beam search, breadth-first search (BFS) is performed by iteratively applying LMPP Expansion and TopK-Based Selection.\n\n\u2022\nA key difference is that, in BFS, all nodes at depth t undergo the same number of actions before expanding further levels. This enforces uniform depths across expansions."}, {"title": "5.3 Depth-First Search", "content": "Tree-of-Thoughts (ToT) (Yao et al., 2023a) Yao et al. (2023a) also apply depth-first search (DFS) for LLM inference, relying on the LMPP Expansion and Threshold-Based Selection. Key points include:\n\n\u2022\nThreshold-Based Selection: One action (node) is sampled at a time, but it is not compared with other nodes. Instead, once its value is evaluated by whether it surpasses a threshold, that path is followed to its conclusion.\n\n\u2022\nLMPE Evaluation for Deadend: The system uses a deadend judgment to halt exploration of unpromising paths, which can be considered as another LMPE evaluation.\n\n\u2022\nBacktracking: Upon reaching a deadend, the system reverts to an earlier node and continues exploring other previously expanded but untried branches.\n\n\u2022\nPath Maintenance: Because of backtracking, the framework must track partial paths, whereas BFS or beam search only needs to maintain the selected nodes at each depth."}, {"title": "5.4 Best-First Search", "content": "Best-first search typically uses a heuristic function h(s) to estimate how promising a state will reach the goal.\n\nKoh et al. (2024)\n\n\u2022\nUses LMPP Expansion for the selected node or the initial root node, where actions are executed in the environment (web interface) to simulate the next states. The generated nodes are saved in N.\n\n\u2022\nEmploys TOPK_SELECT (k=1) through LMPE+ evaluation on each node in saved nodes n \u2208 N. The node value n.val is derived from evaluating its parent's state.\n\n\u2022\nContinues until either the search tree reaches a specified budget \u1e9e or the state value exceeds a threshold \u03b8."}, {"title": "5.5 A*", "content": "A* is similar to best-first search but augments the heuristic h(s) with the accumulated cost/utility g(s) to reach a node from the start. The evaluation function is\n\n$f(s_t) = g(s_t) + Ah(s_t)$,\n(9)\n\nwhere A balances the two terms. summarizes how two frameworks implement this formula differently:\n\nLLM-A* (Meng et al., 2024)\n\n\u2022\nDesigned for path-finding tasks (e.g., mazes).\n\n\u2022\ng(st): Computed incrementally as the path cost from $s_0$ to $s_t$. Formally,\n\n$g(s_t) = - \\sum_{i=1}^{t} Cost(s_i)$,\n(10)\n\n\u2022\nh(s) under LMPE+ evaluation: The main modification beyond the typical A* is to integrate a LMPE to the h(s). Specifically, Imprpolicy&eval3 (see Table 4) is applied to evaluate the Euclidean distance from $s_n$ back to the recently visited $s_{llm} \u2208 Impr_{policy}$, along with the typical Euclidean distance between $s_n$ (expanded neighbour node) and $s_g$."}, {"title": "5.6 Monte Carlo Tree Search", "content": "Monte Carlo Tree Search typically involves selection, expansion, path simulation, and backpropagation. Most reviewed frameworks follow these steps. Below are some differences in notable frameworks:\n\nRAP (Hao et al., 2023)\n\n\u2022\nApplicable to BlocksWorld, Crosswords, and other reasoning tasks.\n\n\u2022\nSAMPLE_ACTION depends on whether the action space is finite and predefined (e.g., Blocks World, where exhaustive retrieval is used) or open-ended (e.g., Crosswords, where LMPP sampling is used).\n\nLATS (Zhou et al., 2024a)\n\n\u2022\nTargets tasks with reversible actions (e.g., certain reasoning problems).\n\n\u2022\nExecutes actions in the actual environment during path simulation, requiring actions to be reversible to allow repeated trials.\n\nLLM-MCTS (Zhao et al., 2023)\n\n\u2022\nDesigned for robotic tasks.\n\n\u2022\nUses random sampling and a domain-specific simulator for path simulation, producing next states and rewards.\n\n\u2022\nAdopts a domain-specific P(s, a) in PUCT, which is derived from LMPP sampling to form an action distribution.\n\nPG-TD (Zhang et al., 2023)\n\n\u2022\nSpecializes in code generation.\n\n\u2022\nTreats the prior distribution P(a | s) in PUCT as the LMPP token probabilities for the next token, given the partial program.\n\n\u2022\nUses LMPT simulation to generate partial programs, but internally adopts beam search to complete the path until a leaf node is reached. They refer to the whole process as \u201cevaluation\u201d)."}, {"title": "6 Discussion", "content": "In this section, we analyze how search frameworks for LLM inference deviate from traditional search algo-rithms, where and how they apply, and the resulting impact on performance and efficiency."}, {"title": "6.1 Deviations from Typical Search Algorithms", "content": "Beyond Finite, Fixed Search Space Typical search algorithms, e.g., BFS (breath), deals with fixed search space and needs to keep track of all possibilities at each depth level, the large or infinite search space can lead to excessive memory consumption. LMPP sampling based on LLM priors makes BFS overcome this limitation.\n\nBeyond Finite, Fixed Search Spaces Classical BFS or DFS typically requires enumerating all successors at each depth, which can lead to massive memory usage in large or infinite search spaces. By contrast, LMPP sampling (based on LLM priors) can manage successor expansions more selectively, reducing the need to store every possibility at each level. Also, it is possible to handle tasks with an open and infinite action space.\n\nMaking \"Uninformed\" Search Informed Traditionally, BFS and DFS are considered uninformed, exploring the search space without heuristics. LLM-based frameworks labeled as BFS or DFS often incor-porate LMPP sampling or LMPE+ evaluation, effectively introducing heuristic knowledge from the LLM. Moreover, anticipating dead ends in DFS is feasible with LLM-based heuristics. Classic DFS only identifies dead ends when it exhausts neighbor nodes. With an LLM, the search can backtrack early if the model predicts an unpromising or \"dead-end\" scenario.\n\nCompromised Optimality in A* A* requires an admissible heuristic h(s"}]}