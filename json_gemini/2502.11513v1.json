{"title": "MaZO: Masked Zeroth-Order Optimization for Multi-Task Fine-Tuning of Large Language Models", "authors": ["Zhen Zhang", "Yifan Yang", "Kai Zhen", "Nathan Susanj", "Athanasios Mouchtaris", "Siegfried Kunzmann", "Zheng Zhang"], "abstract": "Large language models have demonstrated exceptional capabilities across diverse tasks, but their fine-tuning demands significant memory, posing challenges for resource-constrained environments. Zeroth-order (ZO) optimization provides a memory-efficient alternative by eliminating the need for backpropagation. However, ZO optimization suffers from high gradient variance, and prior research has largely focused on single-task learning, leaving its application to multi-task learning unexplored. Multi-task learning is crucial for leveraging shared knowledge across tasks to improve generalization, yet it introduces unique challenges under ZO settings, such as amplified gradient variance and collinearity. In this paper, we present MaZO, the first framework specifically designed for multi-task LLM fine-tuning under ZO optimization. MaZO tackles these challenges at the parameter level through two key innovations: a weight importance metric to identify critical parameters and a multi-task weight update mask to selectively update these parameters, reducing the dimensionality of the parameter space and mitigating task conflicts. Experiments demonstrate that MaZO achieves state-of-the-art performance, surpassing even multi-task learning methods designed for first-order optimization.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have revolutionized natural language processing, enabling breakthroughs in various applications (Anthropic, 2024; DeepMind, 2024; OpenAI, 2024; Bai et al., 2023). However, the large sizes of LLMs pose significant memory challenges during training. Traditional first-order (FO) optimization uses backpropagation, which requires substantial memory to store intermediate activations and gradients (Rostam et al., 2024; Kundu et al., 2024). This issue is especially pronounced in fine-tuning tasks on resource-constrained platforms (e.g. low-end GPUs or edge devices) (Zhuang et al., 2024). Moreover, certain hardware platforms lack software support (e.g. automatic differentiation) for backpropagation (Bergholm et al., 2018), further restricting FO methods. Although parameter-efficient fine-tuning methods have alleviated some of these challenges, they still require multiple times the memory of inference (Bai et al., 2024a; Zhang et al., 2024b). Zeroth-order (ZO) optimization provides a memory-efficient alternative by estimating gradients via forward passes only. Recent advances, such as MeZO (Malladi et al., 2023), have reduced memory usage to inference levels while achieving strong performance in LLM fine-tuning. However, the gradient variance in ZO methods is proportional to the number of perturbed parameters, which makes ZO methods struggle with high-dimensional parameter spaces, leading to slower convergence, increased gradient estimation variance, and hard to scale up (Chen et al., 2024b). Although recent work (Liu et al., 2024b; Yang et al., 2024b; Chen et al., 2023; Liu et al., 2024b; Yu et al., 2024) has addressed some of these issues, most ZO methods focus on single-task learning, leaving their application to multi-task learning largely unexplored.\nMulti-task learning is a key paradigm in LLMs to enable shared representations across diverse downstream tasks. This approach improves generalization, reduces the need for task-specific models, and improves performance in a wide range of applications (Zhang et al., 2023; Radford et al., 2019). Despite its advantages, multi-task learning also introduces inherent challenges, particularly when tasks exhibit conflicting objectives. These conflicts arise when the optimization signals from different tasks are misaligned, leading to competing gradients that prevent the model from learning effectively across all tasks (Sener and Koltun, 2018; Mahapatra and Rajan, 2020; Crawshaw, 2020; Zhou et al., 2022; Shi et al., 2023)."}, {"title": "2 Preliminaries and Related Work", "content": null}, {"title": "2.1 Zeroth-Order Optimization", "content": "Zeroth-order (ZO) optimization estimates gradients using forward passes only. A common approach for ZO gradient estimation is the simultaneous perturbation stochastic approximation (Spall, 1992), which serves as a randomized gradient estimator. Consider a model with parameters $\\theta \\in \\mathbb{R}^d$ and a loss function $L(\\theta)$. Using Taylor expansion, the randomized gradient can be estimated by perturbing $\\theta$ with random noise $z \\sim \\mathcal{N}(0, I_d)$ and computing forward and reverse losses:\n$\\nabla L(\\theta) = \\frac{L(\\theta + \\epsilon z) - L(\\theta - \\epsilon z)}{2\\epsilon} z$, (1)"}, {"title": "2.2 Multi-task Learning", "content": "Multi-task learning aims to improve generalization performance by jointly learning $T$ related tasks through shared parameters (Chen et al., 2024a). Classical multi-task learning minimizes a weighted combination of task-specific losses:\n$\\mathcal{L}(\\theta) = \\sum_{t=1}^{T} w_t L_t(\\theta), \\text{ s.t. } \\sum_{t=1}^{T} w_t = 1, w_t \\geq 0$, (3)"}, {"title": "3 The MaZO Framework", "content": null}, {"title": "3.1 Challenges in ZO Multi-Task Fine Tuning", "content": "Under ZO optimization, multi-task learning faces unique challenges. Specifically, task-specific ZO gradient estimates exhibit fundamental collinearity, as the aggregated multi-task learning gradient aligns with the shared random perturbation $z$:\n$g = \\sum_{t=1}^{T} w_t g_t = \\sum_{t=1}^{T} w_t (\\frac{L^+({\\theta + \\epsilon z}) - L^{-}({\\theta - \\epsilon z})}{2\\epsilon}) z$. (4)"}, {"title": "3.2 Multi-Task Weight Update Mask", "content": "We first introduce the multi-task weight update mask, assuming the weight importance scores are precomputed. We defer the computation of weight importance scores to the next subsection. In ZO optimization, the variance of an estimated gradient increases with the number of training parameters. Therefore, it is crucial to identify and focus on critical parameters for effective optimization while freezing others (Liu et al., 2024b; Guo et al., 2024).\nSuppose that we have a weight importance score matrix $S^t$ for each task $t$ and a sparsity level $\\rho$. We unfreeze the top $k = [(1 - \\rho) \\cdot N]$ parameters in each row, where $N$ is the total number of parameters in that row. The importance scores are compared row-wise due to the approximations involved in gradient and Hessian estimation following Sun et al. (2023), which will be detailed in Section 3.4.\nSince importance scores across tasks are not directly comparable due to differing scales, we normalize the scores row-wise for each task:\n$\\hat{S}_{ij}^t = \\frac{S_{ij}^t - \\min(S_i^t)}{\\max(S_i^t) - \\min(S_i^t)},$ (6)\nwhere $S_i^t$ denotes the $i$-th row of $S^t$; $\\hat{S}_{ij}^t$ is the normalized score for parameter $j$ in row $i$ for task $t$. The overall score across tasks is computed as:\n$S = \\sum_{t=1}^{T} \\hat{S}^t$. (7)\nWe select the top $k$ parameters based on $S$ in each row to fine-tune, while freezing the others. This selection is represented by a binary mask matrix $M$, where $M_{ij} = 1$ indicates that parameter $j$ in row $i$ is unfrozen. The final parameter update is computed as:\n$\\Delta W_{\\text{masked}} = \\Delta W \\odot M,$ (8)\nwhere $\\odot$ denotes element-wise multiplication. When applied to LoRA (Hu et al., 2021), this becomes:\n$\\Delta W_{\\text{masked}} = (A \\cdot B) \\odot M,$ (9)\nwhere A and B are the decomposed matrices of LoRA."}, {"title": "3.3 Weight Importance", "content": "The overall importance score for task $t$ combines the normalized global and greedy scores with a weight regularization term:\n$S^t = \\alpha S_{\\text{global}}^t + \\alpha S_{\\text{greedy}}^t + \\beta |W|,$ (10)\nwhere $\\alpha$ and $\\beta$ are hyperparameters controlling the contributions of each component and $|W|$ is the absolute value of weight. We now describe the computation of two complementary metrics: the global score and the greedy score."}, {"title": "3.3.1 Global Score", "content": "The global score is inspired by the Optimal Brain Surgeon method (Frantar and Alistarh, 2023; Sun et al., 2023; Das et al., 2023). Unlike pruning, which sets the parameters to zero, our approach freezes certain parameters while updating others via perturbation. Consider the Taylor expansion of the loss function of task $t$:\n$\\delta L_t = (g^t)^T \\delta \\theta + \\frac{1}{2} \\delta \\theta^T \\cdot H^t \\cdot \\delta \\theta + O(||\\delta \\theta||^3),$ where $H^t$ is the Hessian matrix of task $t$ and $g^t = \\frac{\\partial L_t}{\\partial \\theta}$. Freezing a parameter at position $m$ imposes the constraint $I_m \\delta \\theta = 0$, where $I_m$ is an indicator function. The optimization problem becomes:\n$\\min_{m} \\{ \\min_{\\delta \\theta} (g^t)^T \\delta \\theta + \\frac{1}{2} \\delta \\theta^T \\cdot H^t \\cdot \\delta \\theta \\text{ s.t. } I_m \\delta \\theta = 0 \\}.$ (11)"}, {"title": "3.3.2 Greedy Score", "content": "Although the global score provides a theoretical measure of parameter importance, it may not suffice because the model may not converge to the optimal situation due to the large variance in the ZO gradient. Therefore, we also introduce a greedy score as a practical complement, which considers the immediate impact of freezing a parameter in a single optimization step.\nFor a gradient descent update with learning rate $\\eta$ and random direction $z$, the parameter update of task $t$ is approximated as:\n$\\delta \\theta \\approx -\\eta zz^T g^t.$ (13)\nSubstituting $\\delta \\theta$ and taking the expectation over random directions $z$, we obtain the expected change in loss:\n$\\mathbb{E}(\\delta L_t) = - (g^t)^T g^t \\cdot \\eta + \\frac{\\eta}{2} \\sum_{i=0}^{M} (g_i^t)^2 + \\frac{\\eta^2}{4} \\sum_{i=0}^{M} (g_i^t) H_{ii}^t (g_i^t)^2$ where $M$ is the number of parameters in a LLM. When we freeze a parameter at position $m$, the change of loss (greedy score) will increase by:\n$(\\mathcal{S}_{greedy})_m = \\delta L_m = \\frac{\\eta}{4} H_{mm}^t (g_m^t)^2 \\frac{\\eta}{4} \\sum_{j=0}^{M} H_{mj}^t (g_m^t) (g_j^t)^2$ (14)"}, {"title": "3.4 Implementation", "content": "To avoid the huge cost of computing the full gradient and Hessian, we adopt a row-wise approximation strategy. For a linear layer $y = Wx$, focusing on a single row $w_i$, the output is $y_i = w_i^T x$. Performing a Taylor expansion of the loss $L$ with respect to $y_i$, we find that both the first-order gradient $\\nabla L(y_i)$ and second-order derivative $\\nabla^2 L(y_i)$ are scalars. Substituting $\\Delta y_i = \\Delta w_i^T x$, the gradient and Hessian with respect to $w_i$ are:\n$g_t = \\frac{\\partial \\mathcal{L}}{\\partial w_i} = \\nabla L(y_i) x$, (15)\n$H^t = \\frac{\\partial^2 \\mathcal{L}}{\\partial w_i^2} = \\nabla^2 L(y_i) (xx^T)$. (16)"}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experimental Setup", "content": "We perform multi-task fine-tuning on two widely used decoder-only pretrained language models: LLaMA-2-7B (Touvron et al., 2023) and Mistral-7B (Jiang et al., 2023).\nTasks. We evaluate our approach on a diverse set of natural language understanding (NLU) and natural language generation (NLG) tasks from the GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) benchmarks. Specifically, for NLU, we include SST-2, BoolQ, RTE, WSC, WiC, MultiRC, and COPA, covering various classification and reasoning tasks. For NLG, we use SQUAD for question answering. To ensure computational feasibility, we sample a subset of each dataset with fixed training, validation, and test splits. Details on datasets and evaluation metrics are in Appendix G.\nBaselines. We compare MaZO with several baselines. First, we include vanilla ZO optimization combined with traditional multi-task learning (MTL-ZO) techniques as a direct comparison to MaZO in the ZO setting. Second, we evaluate single-task learning (STL-ZO), where models are trained individually on each task to provide an upper bound for task-specific performance without multi-task conflicts, as well as a single-task transfer baseline, where the model is trained on a single task (SST-2) using vanilla ZO optimization and evaluated across all tasks to highlight the limitations of single-task training in multi-task scenarios. Third, we include LoRA fine-tuning (Hu et al., 2021), a parameter-efficient approach, and extend MaZO to update LORA matrices under ZO settings, demonstrating its flexibility. Finally, we compare MaZO against state-of-the-art first-order (FO) multi-task learning methods, including CoBa (Gong et al., 2024), FAMO (Liu et al., 2024a), and MTL-LORA (Yang et al., 2024a), to its compatibility with ZO optimization. These baselines provide a comprehensive comparison for assessing MaZO's effectiveness and robustness in addressing the challenges of ZO-based multi-task learning."}, {"title": "4.2 Results on LLaMA-2-7B", "content": "MaZO Outperforms Competitors. The results for LLaMA-2-7B are presented in Table 1. Vanilla multi-task ZO optimization shows only slight improvements over the zero-shot baseline, highlighting its inability to effectively address multi-task conflicts under ZO settings. Similarly, vanilla single-task ZO optimization with a shared model fails to generalize effectively across multiple tasks, underscoring the inherent challenges of ZO optimization in multi-task scenarios.\nIn contrast, our proposed MaZO framework achieves the highest average performance across all tasks and demonstrates a balanced performance profile. These results validate MaZO's ability to mitigate inter-task conflicts and optimize multi-task learning by selectively focusing on critical parameters. The effectiveness of MaZO is further evident in its superior performance in both full-model ZO fine-tuning and LoRA-based fine-tuning, with particularly pronounced gains in the latter. This underscores MaZO's flexibility and its compatibility with parameter-efficient fine-tuning techniques.\nDimensionality Reduction Enhances Multi-Task Learning. The application of LoRA to ZO fine-tuning significantly improves the performance of multi-task learning. This improvement can be attributed to LoRA's ability to reduce the dimensionality of the parameter space, thereby lowering the variance of gradient estimates. These findings reinforce the validity of MaZO's masking strategy, which optimizes multi-task learning by focusing on a reduced set of critical parameters."}, {"title": "4.3 Results on Mistral-7B", "content": "The results for Mistral-7B in Table 2 reveal trends similar to those observed with LLaMA-2-7B. Despite the relatively low zero-shot performance of Mistral-7B, vanilla multi-task learning ZO fails to deliver substantial improvements. This underscores the inherent challenges of ZO-based multi-task learning. In contrast, MaZO consistently outperforms all other methods. Its ability to mitigate ZO-specific challenges is evident in its superior performance, further validating MaZO as a state-of-the-art solution for ZO-based multi-task learning."}, {"title": "4.4 Computational Performance", "content": "Figure 4 shows that MaZO converges faster and achieves a significantly lower loss compared to traditional multi-task ZO fine-tuning methods. This holds true both with and without LoRA. This improvement can be attributed to the mask mechanism in MaZO, which focuses on optimizing the most critical parameters, thereby reducing gradient noise, balancing the inter-task conflicts, and accelerating convergence.\nThe significantly better convergence and accuracy of MaZO is obtained at marginal computing and memory overhead. Specifically, the mask search time introduced by MaZO is negligible compared to the overall training time. When evaluated in the LlaMMA-7B model, MaZO incurs a slight increase in memory usage (approximately 10%) compared to baseline multi-task learning ZO methods. This is primarily due to the additional storage required for the weight update mask. However, this increase does not significantly impact the overall memory efficiency, especially when combined with LoRA, where the parameter space is already reduced."}, {"title": "4.5 Various Weight Importance Metrics", "content": "To further validate the effectiveness of MaZO, we compare its performance with three alternative weight scoring methods: random selection, magnitude-based scoring, and Wanda scoring. Detailed implementation of these methods is described in Appendix F. For a fair comparison, we fix the sparsity level at 50%, consistent with the sparsity used in the Wanda score."}, {"title": "4.6 Ablation Study", "content": "Finally, we explore the optimal hyperparameter settings for MaZO, includeing \u03b1, \u03b2, sparsity level, and the LoRA rank. To streamline the process, we perform grid searches for each hyperparameter while keeping the others constant. For most experiments, we fine-tune the model on SST-2, BoolQ, COPA, and SQUAD, encompassing binary classification, multiple-choice, and generation tasks, providing diverse evaluation scenarios. However, for the LORA rank, we evaluate performance across all tasks.\n\u03b1 and \u03b2. To optimize \u03b1 and \u03b2, we fix the sparsity level at 50% and perform full-model fine-tuning (without LoRA). The search is conducted in two stages. First, \u03b2 is set to zero, and \u03b1 is tuned, resulting in an optimal value of \u03b1 = 10. Next, with \u03b1 fixed, \u03b2 is tuned, yielding an optimal value"}, {"title": "5 Conclusion", "content": "In this work, we have presented MaZO, a novel framework that harnesses masked zeroth-order optimization for the multi-task fine-tuning of LLMs. By incorporating weight importance score alongside a multi-task weight update mask, MaZO effectively reduces gradient variance and mitigates conflicts among tasks. Our experimental results demonstrate that MaZO not only surpasses current zeroth-order optimization methods but also outperforms leading multi-task learning methods designed for first-order optimization across a range of NLP tasks. Furthermore, our parameter-level approach is not limited solely to zeroth-order optimization, offering potential integrations with a variety of other optimization strategies."}, {"title": "6 Limitations", "content": "While MaZO demonstrates strong empirical performance, several limitations warrant discussion. First, the computation of weight importance introduces additional computational overhead compared to vanilla ZO methods. However, this cost remains negligible relative to the memory and computational demands of model weights and activations. Second, the effectiveness of MaZO is partially contingent on the quality of gradient and Hessian approximations. While our current approximations are effective, they could be further refined through more sophisticated estimation techniques to enhance performance. Third, our experiments are limited to medium-scale models (7B parameters) due to computational constraints. Although the method is theoretically applicable to larger models, the interplay between mask sparsity and model scale has not been systematically studied and represents an avenue for future research. Finally, we do not provide a theoretical convergence analysis for the masking approach. However, Sparse MeZO (Liu et al., 2024b) has already conducted a comprehensive and rigorous analysis of general masking scenarios in zeroth-order optimization. We refer interested readers to their work for detailed theoretical insights, and therefore do not duplicate these efforts here."}, {"title": "A Additional Explanation on Hessian and Gradient Approximation", "content": "Consider a linear layer in an LLM that computes:\n$y = Wx,$ (17)\nwhere $W \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^n$, and $y \\in \\mathbb{R}^m$. Focusing on one particular linear component, let us analyze a single row $w_i \\in \\mathbb{R}^n$ of $W$. The corresponding output is given by:\n$y_i = w_i^T x,$ (18)\nwhich is a scalar.\nTo analyze the sensitivity of the loss $L$ with respect to $w_i$, we perform a second-order Taylor expansion of $L$ with respect to $y_i$:\n$L(y_i + \\Delta y_i) \\approx L(y_i) + \\nabla L(y_i) \\Delta y_i + \\frac{1}{2} \\nabla^2 L(y_i) (\\Delta y_i)^2$. (19)\nSince $y_i$ is a scalar, its second derivative $\\nabla^2 L(y_i)$ is also a scalar.\nNow, the change in $y_i$ due to a change in the weights is\n$\\Delta y_i = \\Delta w_i^T x.$ (20)\nSubstituting this into the second-order term yields:\n$\\frac{\\partial^2 L}{\\partial w_i^2} \\approx \\nabla^2 L(y_i) (xx^T).$ (21)\nSince we are primarily interested in comparing weight importance along the row direction, the absolute scale of the Hessian is not crucial. In practice, we can drop the multiplicative factor $\\nabla^2 L(y_i)$ (or, equivalently, assume it to be a constant) and write:\n$\\frac{\\partial^2 L}{\\partial w_i^2} \\propto xx^T.$ (22)\nSimilarly, one can derive a first-order approximation for the gradient. By retaining only the first-order term of the Taylor expansion, we have:\n$L(y_i + \\Delta y_i) \\approx L(y_i) + \\nabla L(y_i) \\Delta y_i.$ (23)\nWith $\\Delta y_i = \\Delta w_i^T x$, the gradient with respect to $w_i$ becomes:\n$\\frac{\\partial L}{\\partial w_i} \\approx \\nabla L(y_i) x.$ (24)\nSimilarly, since we are only interested in the relative value, the factor is dropped:\n$g \\propto x.$ (25)\nThis derivation shows that, by considering each row independently (row-wise), we avoid the immense complexity involved in computing the full Hessian matrix (which is high-dimensional and difficult to characterize even under diagonalization assumptions). In other words, computing the Hessian row-wise allows us to circumvent the problem of determining the eigenvalues or even a reliable diagonal approximation of the full Hessian."}, {"title": "B Pseudo-code of MaZO", "content": "The pseudo-code of the whole MaZO LLM fine-tuning framework is shown as Algorithm 1."}, {"title": "C Baseline", "content": null}, {"title": "C.1 CoBa: Convergence Balancer for Multitask Finetuning", "content": "CoBa (Convergence Balancer) (Gong et al., 2024) is a novel multi-task learning (MTL) method designed for large language models (LLMs). It dynamically adjusts task weights during training to ensure balanced convergence across tasks, while maintaining computational efficiency.\nConsider an LLM parameterized by $\\theta \\in \\mathbb{R}^M$, trained on $T > 2$ tasks. The loss function for task $t$ at iteration $i$ is denoted as $L_t(\\theta; i) : \\mathbb{R}^M \\rightarrow \\mathbb{R}_{\\geq 0}$. The overall optimization objective is:\n$\\min_{\\theta \\in \\mathbb{R}^M} L(\\theta; i) = \\sum_{t=1}^{T} w_t(i) L_t(\\theta; i),$ (26)\nwhere $w_t(i)$ is the weight of task $t$ at iteration $i$. A uniform weight assignment $w_t(i) = \\frac{1}{T}$ ensures equal attention to all tasks but often leads to varying convergence rates. CoBa dynamically adjusts $w_t(i)$ to balance these rates, prioritizing generalization by deriving weights from validation losses instead of training losses. CoBa is built upon three main components:\nRelative Convergence Score (RCS) dynamically allocates smaller weights to tasks that converge faster and larger weights to slower-converging tasks. It is computed as:\n$\\text{RCSt}(i) = \\text{softmaxt} \\bigg(\\frac{\\alpha_t(i)}{\\sum_{t'=1}^{T} \\alpha_{t'}(i)} \\bigg),$ (27)\nwhere $\\alpha_t(i)$ is the convergence slope of task $t$, derived from the normalized validation loss ratio over a sliding window of $N$ iterations. The softmax operation ensures differentiation across tasks, with faster-converging tasks receiving lower weights.\nAbsolute Convergence Score (ACS) addresses task divergence by reducing weights for diverging tasks and increasing weights for converging tasks. It is computed as:\n$\\text{ACSt}(i) = \\text{softmaxt} \\bigg(-N \\sum_{j=i-N+1}^{i} |\\alpha_t(j)|\\bigg),$ (28)\nwhere normalization is performed along the historical iteration dimension, isolating a task's own trajectory. ACS ensures tasks with consistent convergence receive higher weights, while diverging tasks are penalized.\nDivergence Factor (DF) determines the relative influence of RCS and ACS on the final task weights. It is defined as:\n$\\text{DF}(i) = \\min \\bigg(\\text{softmax} \\bigg(\\frac{\\gamma \\alpha_{\\text{max}}(i)}{\\sum_{j=1}^{T} |\\alpha_t(j)|} \\bigg), 1 \\bigg),$ (29)\nwhere $\\alpha_{\\text{max}}(i)$ is the largest convergence slope across all tasks at iteration $i$. DF ensures RCS dominates when all tasks are converging, while ACS takes precedence when divergence is detected.\nThe final task weights $w_t(i)$ are computed as:\n$w_t(i) = \\text{DF}(i) \\cdot \\text{RCSt}(i) + (1 - \\text{DF}(i)) \\cdot \\text{ACSt}(i),$ (30)\nallowing a seamless transition between RCS and ACS dominance based on task convergence trends.\nThe convergence slope $\\alpha_t(i)$ for task $t$ is calculated based on the normalized validation loss ratio $\\hat{L}_t^{\\text{val}}(\\theta; i)$. Specifically, we fit a linear model to the validation loss ratios over a sliding window of $N$ iterations. The observations are defined as:\n$x_t(i) = [i, 1], X_{t(N; i)} = [x_t(s_0), ..., x_t(i)],$ (31)\n$y_{t(N; i)} = [\\hat{L}_t^{\\text{val}}(\\theta; s_0), ..., \\hat{L}_t^{\\text{val}}(\\theta; i)],$ (32)\nwhere $s_0 = \\max(0, i - N + 1)$ is the starting step of the sliding window. The goal is to compute the coefficient vector $c_t(N; i) = [\\alpha_{t(N; i)}, \\beta_{t(N; i)}]$ that minimizes the mean squared error (MSE) between the predicted and actual validation loss ratios:\n$c_t = \\text{arg} \\min_{c_t} \\frac{1}{2} (X_t c_t - y_t)^T (X_t c_t - y_t).$ (33)"}, {"title": "C.2 FAMO: Fast Adaptive Multitask Optimization", "content": "Fast Adaptive Multitask Optimization (FAMO) is a dynamic weighting method designed to address the challenges of multitask learning (MTL), where directly optimizing the average loss across tasks often leads to under-optimization of certain tasks. FAMO ensures balanced task loss reduction using only O(1) space and time per iteration, making it computationally efficient and scalable."}, {"title": "C.3 MTL-LORA", "content": "MTL-LORA (Multi-Task Learning LoRA) is a parameter-efficient fine-tuning method designed to enhance the multi-task learning (MTL) capabilities of large language models (LLMs). It builds upon the Low-Rank Adaptation (LoRA) framework by addressing the challenges of task interference and suboptimal information sharing in multi-task scenarios.\nLORA is a parameter-efficient fine-tuning method that freezes the majority of a pre-trained model's parameters and introduces trainable low-rank matrices to approximate gradient updates. For a weight matrix W \u2208 Rd\u00d7k in the original model, LORA decomposes the gradient update AW into two low-rank matrices B \u2208 Rd\u00d7r and A \u2208 Rr\u00d7k, where r < min(d, k). The updated weight matrix"}, {"title": "Algorithm 1 MaZO LLM Fine-Tuning Framework", "content": null}, {"title": "H Discussion of Collinearity", "content": "Task-specific ZO gradients: For each task $t \\in \\{1, ..., T\\}$, the zeroth-order gradient estimate is given by\n$g^t = \\frac{L_t(\\theta + \\epsilon z) - L_t(\\theta - \\epsilon z)}{2 \\epsilon} z = \\alpha_t z,$ (37)\nwhere $a_t$ is a scalar. Thus, every $g^t$ is a scalar multiple of the same random direction $z$.\nSpan of all task gradients: The space spanned by the set of all task gradients is\n$\\text{span}\\{g^1, g^2, ..., g^T\\} = \\text{span}\\{z\\}.$ (38)\nTherefore, the dimension of this span is\n$\\dim(\\text{span}\\{g^1, g^2, ..., g^T\\}) = 1.$ (39)\nAggregated gradient: The combined gradient used for the update is\n$g = \\sum_{t=1}^{T} w_t g^t = \\bigg(\\sum_{t=1}^{T} w_t \\alpha_t\\bigg) z.$ (40)\nwhich clearly lies in the one-dimensional subspace spanned by $z$.\nGradient covariance matrix: Define the covariance matrix of the task gradients as\n$C = \\sum_{t=1}^{T} \\pi_t (g^t - g) (g^t - g)^T,$ (41)\nwhere $\\pi_t$ are probability weights (or simply $1/T$ for uniform weighting) and the mean gradient is\n$g = \\sum_{t=1}^{T} \\pi_t g^t.$ (42)\nSince $g^t = a_t z$, we have"}]}