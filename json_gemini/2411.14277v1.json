{"title": "Neuro-Symbolic Query Optimization in Knowledge Graphs", "authors": ["Maribel ACOSTA", "Chang QIN", "Tim SCHWABE"], "abstract": "This chapter delves into the emerging field of neuro-symbolic query optimization for knowledge graphs (KGs), presenting a comprehensive exploration of how neural and symbolic techniques can be integrated to enhance query processing. Traditional query optimizers in knowledge graphs rely heavily on symbolic methods, utilizing dataset summaries, statistics, and cost models to select efficient execution plans. However, these approaches often suffer from misestimations and inaccuracies, particularly when dealing with complex queries or large-scale datasets. Recent advancements have introduced neural models, which capture non-linear aspects of query optimization, offering promising alternatives to purely symbolic methods. In this chapter, we introduce neuro-symbolic query optimizers, a novel approach that combines the strengths of symbolic reasoning with the adaptability of neural computation. We discuss the architecture of these hybrid systems, highlighting the interplay between neural and symbolic components to improve the optimizer's ability to navigate the search space and produce efficient execution plans. Additionally, the chapter reviews existing neural components tailored for optimizing queries over knowledge graphs and examines the limitations and challenges in deploying neuro-symbolic query optimizers in real-world environments.", "sections": [{"title": "1. Introduction", "content": "Knowledge graphs (KGs) and other data models can be interrogated using queries formulated in a query language. For example, for knowledge graphs modeled with the Resource Description Framework (RDF), queries are formulated using the SPARQL query language. To compute the answer for a given query, a query processor comprises different components that implement strategies to speed up the query execution while producing correct results. Such strategies include resorting to access methods to retrieve portions of the stored dataset efficiently, re-ordering the operators that occur in the query to minimize the number of intermediate results, or implementing specific algorithms to implement operators that produce intermediate results quickly."}, {"title": "2. Preliminaries", "content": "In query processing, the optimizer is the component that selects the strategies to execute a given query over a dataset. These strategies are encoded in a query plan, which is represented as a tree. The leaves of this tree correspond to access methods to retrieve relevant data from the dataset, while the internal nodes are operators to combine intermediate results and produce the final query answer. Formally, the query optimization problem can be defined as a search problem [1] whose goal is to identify a plan from the space of possible plans that minimizes some objective function, e.g., the query runtime or latency. Ibaraki and Kameda [2] formally demonstrated that identifying the optimal plan is an NP-complete problem. To explore the search space and identify efficient plans, traditional query optimizers implement symbolic components based on dataset summaries, statistics, cost models, (algebraic) rules, or heuristics.\nHowever, even with detailed statistics and advanced techniques, symbolic query optimizers are prone to errors due to misestimations or inaccurate models. Producing suboptimal plans has tremendous effects on query latency, where execution times can be affected by orders of magnitude or even prevent a query from being answered in a reasonable amount of time. This problem is exacerbated when evaluating complex queries or dealing with very large KG. To overcome these limitations, neural models for query optimization have been proposed for query engines over relational databases and knowledge graphs. Most of these solutions focus on replacing a specific component in the query optimizer by using machine learning models based on neural networks or deep learning, as they are able to capture complex relationships of the factors that affect the query performance, e.g., data correlations, complexity of the query operators, and (hardware) resource usage. This chapter goes a step beyond the literature and introduces the notion of neuro-symbolic query optimizers, where we discuss the integration of neural and symbolic components during query optimization. Furthermore, this chapter provides details of existing solutions for neural components to optimize queries over knowledge graphs. We finally present a discussion of existing limitations and future directions of neuro-symbolic query optimizers.\nThe remainder of this chapter is structured as follows. \u00a72 introduces the preliminaries, including symbolic methods for query optimization and the general architecture of query optimizers. \u00a73 briefly presents existing neural and neuro-symbolic methods for relational databases. \u00a74 describes the architecture of neuro-symbolic optimizers for knowledge graphs. \u00a75 discusses challenges and future directions for deploying neuro-symbolic optimizers in real-world systems. \u00a76 concludes this chapter."}, {"title": "2.1. Symbolic Methods for Query Optimization", "content": "Symbolic query optimization typically refers to a type of optimization that involves manipulating and reasoning about the symbolic representation of queries and their plans. This approach focuses on theoretical and logical transformations. The database literature distinguishes three types of optimizers related to symbolic query optimization: heuristics-based, rule-based, and cost-based optimization."}, {"title": "2.2. General Architecture of Query Optimizers", "content": "Figure 1 outlines the components of query optimizers. The cost-based optimizer components include the cardinality estimator (\u00a72.2.1) and the cost model (\u00a72.2.2). The planner (\u00a72.2.3) explores potential plans to identify the optimal execution strategy using the cost model (in the case of cost-based optimizers) or pre-defined strategies encoded as heuristics or rules (in the case of other symbolic optimization techniques)."}, {"title": "2.2.1. Cardinality Estimator", "content": "This component determines the cardinality or number of results produced by a subplan. In query optimization, the cardinality of a subplan is typically assumed to be directly"}, {"title": "2.2.2. Cost Model", "content": "In cost-based optimization, the cost model is used to estimate the resources required to execute query plans. In this context, the cost is a numerical value which is associated with each plan. Although this cost does not necessarily represent the exact runtime of a query, it is correlated with it: a higher cost indicates a longer query runtime.\nDifferent database systems implement different models or formulas to estimate the cost of a plan. Still, a typical cost model formula considers several factors, such as I/O cost, CPU cost, and sometimes memory usage. For example, for a plan that involves scanning a table and applying a filter, the cost might be estimated as the linear combination of number of sequential pages read from disk plus the number of tuples processed. The following is an example of such a cost model implemented in PostgreSQL [6]:\n$Cost(P) = (C_{seq-page} \\cdot #disk\\_pages\\_read)+(C_{cpu-tuple} \\cdot #tuples\\_scanned)$ (1)\nAs shown in Figure 1 and Eq. (1), the cost model relies on the dataset statistics stored in the catalog and the cardinality estimator to compute the efficiency of a plan.\nIn practice, developing a cost model that accurately reflects the true execution cost of different query plans can be complex. Finding suitable values for the constants C used in a cost model is not straightforward as these are coupled to the database configuration, the query workload diversity, and even the hardware where the database is deployed. Furthermore, all of these factors can change over time due to changes in the dataset, queries, and technical infrastructure. Therefore, finding optimal values for constants and other parameters in a cost model requires a combination of theoretical knowledge, empirical analysis, and continuous tuning to ensure the cost model remains accurate over time."}, {"title": "2.2.3. Planner", "content": "For a given query, the planner traverses the search space of possible plans by comparing equivalent plans, i.e., plans whose executions produce the same results. The space of equivalent plans is given by [7]: (i) the algebraic space, which is the set of rules that preserve plan equivalence [36] (e.g, operator commutativity and associativity), (ii) the rewriter applies simple transformations to the query to produce more efficient plans, e.g., by using views, flattening out subqueries, redundant subexpression elimination, etc. and (iii) the method-structure space that contains the available implementations of access methods and logical operations specified in the query.\nDuring the plan traversal, the planner compares equivalent plans to find an optimal plan. For this, planners may implement different search strategies [86, 96], including exhaustive search using Dynamic Programming [144], greedy algorithms that prune large sub-spaces of plans and typically run in polynomial time, randomized algorithms that generate an initial plan randomly which is later refined, and hybrid search strategies such as Iterative Dynamic Programming (IDP) [96] combining DP with greedy steps.\nIn cost-based optimization, the planner relies on the cost model to guide the exploration of plans and identify the best ones. Similarly, optimizers with different architectures may also implement search strategies like the ones discussed here, but they use heuristics or rules to enumerate and compare equivalent plans."}, {"title": "3. Literature in Neural Components in Relational Databases", "content": "In the context of neural approaches for relational databases, the cardinality estimators, cost models, and plan traversal components are enhanced with learned models. Each of these components benefits from learning different aspects of query processing via machine learning and neural networks. In the following sections, we will explore how the learned models are integrated into optimizers to improve the query performance."}, {"title": "3.1. Learned Cardinality Estimation", "content": "Cardinality estimators that rely on machine learning may follow two paradigms: the query modeling (\u00a73.1.1) and the data modeling (\u00a73.1.2). Query modeling approaches learn a mapping function between a query or a plan and its cardinality, treating cardinality estimation as a regression problem. Instead, data modeling treats cardinality estimation as a density estimation problem by focusing on learning joint data distributions [8]. An overview of existing solutions for learned cardinality estimations is shown in Table 1."}, {"title": "3.1.1. Query-Driven Models", "content": "Query-driven models are applied when the full dataset is unavailable or too large to learn from. These approaches use query logs to predict the cardinality of the new queries with similar patterns and, therefore, it is regarded as a supervised learning process.\nQuery-driven modeling approaches perform (1) a sampling phase to extract information related to schema and attribute values tables, columns from the schema, values from each column for predicates [8], or the physical plans generated by the optimizer [10] are gathered. To pass this information to a neural network, the approaches build a (2) query featurization where the operations included in the query are transformed into numerical"}, {"title": "3.1.2. Data-Driven Models", "content": "These models learn to predict the query cardinality from data distributions by learning a joint data distribution of each data point in the dataset [8]. Existing modeling approaches greatly differ in the model selection, as shown in Table 1.\nIn the following, we briefly discuss unsupervised methods for data-driven modeling that are neural-based. Prominent approaches based on neural networks are Sum-Product Networks (SPNs) and autoregressive models. SPNs [15] split data into clusters and groups based on similarity and correlation, respectively, using SUM and PRODUCT operators to efficiently compute joint data distributions and cardinalities. Autoregressive models treat each tuple or row as a sequence [16, 17]. To predict the value of the next attribute in the tuple, it factorizes the joint distribution and computes conditional distributions based on the preceding attributes. It then aggregates these probabilities for data samples matching a query to estimate the query's cardinality [8].\nAlthough these approaches can achieve high accuracy, their inference time is much longer because of the adoption of more specialized models. For example, the total time of"}, {"title": "3.2. Learned Cost Models", "content": "Learned cost models based on neural (\u00a73.2.1) and neuro-symbolic (\u00a73.2.2) approaches have been widely investigated and proven to outperform traditional methods [8, 19-23]."}, {"title": "3.2.1. Neural Cost Models", "content": "Neural-based approaches to learned cost models are mainly supervised methods and are applied to replace cost models by predicting performance metrics. The input is generally presentations of queries, operators or other required characteristics which are fed into the neural networks. NN-based [19], tree convolution [20, 22] and tree-structured deep neural network [10] are used to capture the complex relationships and patterns in query plans. As for output, various performance metrics, such as latency, are predicted [20, 22]."}, {"title": "3.2.2. Neuro-symbolic Cost Models", "content": "Although learning-based approaches with neural networks for cost estimation have been intensively studied these years, the inherent limitations, such as training overhead, poor generalization among databases as well as the lack of explainability for end-to-end predictions, are unavoidable [23]. Therefore, traditional formula-based cost models -as the example from Eq. (1)\u2013 have now improved with advanced learning-based models [23]. Instead of manually setting the values for constants and parameters in formula-based cost models, the neuro-symbolic cost models learn optimal settings offline. As new data and query workloads come into the system, parameters are further refined and adjusted dynamically with online learning to adapt to changing conditions or configurations. In this way, transferability can also be achieved by a lightweight learning scheme. Besides, inherent advantages of formula-based cost models are utilized, such as training is based on the existing knowledge of the cost model and the interpretability of the cost model, which makes the training process more efficient and interpretable [23]."}, {"title": "3.3. Learned Plan Traversal", "content": "One of the main challenges in cost-based query optimization is that the plan enumeration phase is sensitive to errors in the cardinality estimations and the cost model. To avoid this, learned plan traversal approaches skip these aspects, and learn a model to effciently identify the subspace of plans where optimal plans may reside.\nCurrent learned planners are neural-based using deep reinforcement learning (DRL) [21]. With DRL, an agent learns to chose the policy which make the cumulative rewards maximun. In the context of query optimization, the state and action of reinforcement learning are the current subplans and the combination of two subplans into a new plan. In this way, the planner learns a decision policy to map states to actions, with the maximum expected reward [24]. Depending on learning from past queries or during the execution of the current query, learned plan travesal can be categorized into offline-learning and online-learning methods."}, {"title": "4. Towards Neuro-Symbolic Optimizers for Knowledge Graphs", "content": "The problem of query optimization and solutions for knowledge graphs (KGs) takes a lot of inspiration from the ones developed for relational databases. However, existing data models for KGs present fundamental properties that greatly differ from the relational model. This hinders the direct application of database techniques to KGs. In the context of query optimization, three of these fundamental properties of KGs are:\n1. Connectedness: KGs are graph-based structures where entities are represented as nodes, which are interconnected via edges that represent relationships between them. In graph models such as RDF, where the atomic data structure is a triple, the relationships or connections in the KG are considered first-class citizens.\n2. Semi-structuredness: KGs follow a schema-less paradigm without imposing the same structures or restrictions over the entire datasets. Due to their schema-less nature, KGs often present irregular data structures where some nodes are over-specified with multiple attributes or predicates while others are highly incomplete.\n3. Unexpected data correlations: The data distributions in real-world KGs follow a power law, e.g., a small number of nodes have a high degree of connections, while most nodes have a few connections. This is very common in RDF graphs [32] due to the presence of frequent properties defined in RDF/S and OWL.\nThe properties (1), (2), and (3) make the process of learning patterns and joint data distributions from the KG very challenging. In particular, property (3) impedes applying techniques from relational databases, where data is typically more uniformly distributed."}, {"title": "4.1. Architecture of a Neuro-Symbolic Query Optimizer", "content": "Query optimizers can integrate learned or neural components in different ways. To characterize this, we present in Figure 2 a spectrum comprising a progression from traditional, fully symbolic query optimizers to fully learned, neural network-based optimizers, with various hybrid neuro-symbolic approaches in between.\n*   Fully Symbolic or Traditional Optimizer This represents the classic, rule-based query optimizer found in most systems (cf. \u00a72). It relies entirely on symbolic methods, such as dataset statistics, predefined rules, and heuristics to make optimization decisions.\n*   Fully Learned Optimizer At the far end of the spectrum, this represents a fully learned query optimizer that relies entirely on neural networks or other machine learning models to perform all aspects of query optimization, from cost estimation to plan selection.\n*   Region of Neuro-Symbolic Optimizers This middle area represents a blend of symbolic and neural approaches. It includes optimizers that combine learned models with traditional symbolic components, including:\n    *   Learned Cardinality Estimator: A neural model is used to estimate the cardinalities of intermediate query results.\n    *   Neuro-Symbolic Cost Model: Integrates a learned model into the cost estimation process, either by learning the weights of the factors in the cost model or by combining neural estimates with traditional cost models. Currently, there are no approaches for neuro-symbolic cost models over KGs.\n    *   Learned Cost Model: The entire cost model is replaced by a learned model, which predicts the cost of executing query plans using machine learning techniques.\n    *   Learned Planner: The planning process itself is guided by a learned model, which suggests query plans based on data obtained from estimates or performance measurements obtained online or from past executions.\nThe following sections focus on the architecture of components to build neuro-symbolic optimizers and discuss limitations and open challenges of existing solutions."}, {"title": "4.2. Learned Cardinality Estimation", "content": "Different architectures can be applied to learn cardinality estimations over KGs as shown in Figure 3. In query-driven architectures (cf. Figure 3a), the model is trained in a supervised way using labeled training data, i.e., queries and their corresponding cardinalities. For this, the approaches first obtain a representation of the terms (nodes and edges) that occur in the queries and learn a representation for the queries themselves. These representations can be transformed, e.g., into a vector, and then passed to a supervised model to predict query cardinalities. In data-driven architectures (cf. Figure 3b), the model can be trained in an unsupervised way by learning the latent probability distributions of the nodes and edges in the KG. For this, the approaches traverse the KG to perform subgraph sampling. The sampled subgraphs are then used in an autoregressive model to learn the probabilities of the KG terms (nodes and edges); these probabilities are finally transformed into cardinality values by performing some scaling transformation."}, {"title": "4.2.1. Node and Edges Representation", "content": "The first step is to represent the entities (nodes) and predicates (edges) that occur in the input data (i.e., queries or the KG itself) in a suitable way for neural models. This is done by transforming the nodes and edges into vector-based representations.\n*   Numerical Vector Encoding A simple approach to represent entities and predicates is to assign them a numerical ID, and then represent these IDs with binary or one-hot encoding vectors. This representation is simple but does not include any information about the meaning or interconnections of entities or predicates in the KG, forcing the subsequent model to learn the semantics of these.\n*   Embedding-enhanced Representation To represent the individual entities and predicates that occur in the query, it is possible to use Knowledge Graph Embeddings (KGE), instead of assigning them arbitrary numerical IDS. KGE can be trained from parts of the entire KG, and capture rich information about the entities and predicates, which can"}, {"title": "4.2.2. Graph Representation and Cardinality Estimation", "content": "Performing cardinality estimation over KGs is inherently different from the relational case. In KGs, both the dataset and the queries are graphs\u00b2, therefore, the models greatly benefit from representations that explicitly encode the connectivity of nodes and edges in the dataset. Multiple ways exist of encoding KGs and queries to suitably represent them to neural models to perform cardinality estimation, differing mainly in their expressivity.\n*   Adjacency Tensors with Multilayer Perceptrons This is a rather simple encoding to represent the input data using an adjacency matrix. This representation can only be used in query modeling approaches, as the adjacency matrix of the entire KG is typically a very large and sparse structure. To represent a query, it is sufficient to use three matrices [33]. One is an adjacency tensor, to represent the graph structure of the query. The size of this tensor is fixed a priori with the maximum number of edges allowed by the model in any query. The other two matrices are also of fixed size to encode the entities and predicates that occur in a query, respectively, using vectors or embeddings as detailed in \u00a74.2.1. These three matrices can be flattened into vectors and processed using Multilayer-Perceptrons (MLP), combined, and transformed into a cardinality estimate using another MLP [33]. This is a simple and efficient architecture, yet it has several drawbacks. First, the fixed dimensions of the MLPs implicate only queries with a (maximum) fixed number of nodes and edges can be processed by a given model, requiring training multiple models. Second, processing the adjacency tensor as a flattened vector using an MLP does not account for the permutation invariance of (query) graphs. That is, a permutation of the entities in the adjacency will still represent the same query graph but will look vastly different to the model. This requires the model to have more training data to learn this invariance, or to compute query canonical representations where permutations of the same query always produce the same representation.\n*   Graph Neural Networks Instead of representing the KG or the graph queries using matrices, a fitting approach is to compute a representation using a Graph Neural Network (GNN). While this approach can also be applied to the entire KG, it can be challenging to scale up to very large KGs. Therefore, existing approaches instead apply it to query graphs [34, 35]. Using GNNs to represent graph structures and their connectedness has two main advantages. First, a single GNN model can process arbitrary query shapes and sizes by default, so there is no need for multiple models. Second, GNNs are permutation-invariant and thus capture commutative of joins or AND operations in query graphs, which makes these models more data efficient. The learned node representation with the GNN can then be combined, e.g., with a global sum to perform cardinality estimations using directly an MLP [34] or an attention layer followed by an MLP [35]."}, {"title": "4.2.3. Limitations and Open Challenges", "content": "Current neural models for cardinality estimation show impressive results in terms of accuracy, but they still present several limitations. First, while the studied models are rather small in terms of parameters, training on large corpora still requires extensive time and computational resources. In particular, supervised approaches require a representative and diverse query workload to be able to generalize to new queries. Still, real-world query workloads are not always accessible, and generating diverse queries is not necessarily straightforward and can also be challenging in terms of time and resource consumption. Second, while many methods show degraded yet acceptable performance in dynamic KGs, i.e., new entities are added to the KG, substantial changes to the KG \u2013 i.e., large amounts of added entities, schema changes \u2013 require retraining the models, which hinders scalability. Hence, future work needs to develop more efficient models for rapid adaptability to dynamic data changes, e.g., embeddings or representations that are easily updatable and tailored to cardinality estimation as a downstream task."}, {"title": "4.3. Learned Cost Models", "content": "Current learned cost models for KGs aim to directly predict the latency or runtime of a plan before execution. These models can be directly used by the query optimizer to aid the search for plans with minimal query latency. Compared to the relational case, models for KGs face the challenges of a possibly large number of operations in the query, resulting in a larger plan to be processed and difficulty in gathering concise statistics due to the semi-structuredness and unexpected data correlations that occur in KGs. Figure 4 shows the general architecture of learned cost models for KGs."}, {"title": "4.3.1. Query Plan Representation", "content": "In contrast to queries, query plan representations include the sequence of operations and other technical details about the execution strategies that the system will use to evaluate a query. To enable a trained model to predict the latency of a specific query plan, it is necessary to encode the plan into a format that the model can process, as discussed below.\n*   Algebraic and Graph Similarity Vectors Query plans can be represented using fixed vectors that encode their characteristics. Algebraic features, including the number of triple patterns, operators and their order occurring in the plan, and depth of the query tree, can be used to coarsely characterize the plan [36]. The structure of a query graph can be coarsely encoded by calculating its graph edit distances to a set of representative queries and using those distances as features [36].\n*   Query Tree Representation Apart from these coarse representations, a plan can also be represented directly as a tree. The structure then explicitly encodes the operators and their order, and the nodes can be encoded as vectors that hold information such as the cardinality of the subtree, as well as nodes and edges instantiated in the query, etc."}, {"title": "4.3.2. Cost Model Predictors", "content": "Various machine learning architectures leverage the representation of query plans to estimate their latency accurately. Some architectures include simple models like k-nearest neighbors and support vector machines. More advanced methods, such as tree convolution, utilize the structural information inherent in query trees. These methods demonstrate the evolution from coarse representations to more sophisticated techniques that better capture the complexities of query execution plans.\n*   K-Nearest Neighbors and Support Vector Machines Based on the algebraic and graph similarity vectors, various simple models like k-nearest neighbors or support vector machines can be trained to predict the latency of the represented plan [36].\n*   Tree Convolution As the fixed-size algebraic and graph similarity vectors only encode the plan coarsely, the explicit query tree representation is a more powerful approach. For this, approaches like tree convolution can be used to process tree-structured data and, thus, capture all information in the representation [37]. Such a model can also be trained in a supervised way using the tree representations of queries and the corresponding latency. In experiments, using a tree convolution with algebraic and graph-similarity features leads to a three-fold improvement over just using the coarse features."}, {"title": "4.3.3. Limitations and Open Challenges", "content": "Compared to approaches for relational databases, fewer approaches exist for learned cost estimation over KGs. While existing approaches already display good results in terms of latency prediction accuracy, they can be further enhanced in terms of explicit and fine-grained representation of the query tree regarding operators and statistics about involved nodes and edges (e.g., using KG embeddings). The existing approaches also do not treat the problem of dynamic data or changes in the underlying hardware where the KG is stored, which can lead to significant changes in latency. For the latter, devising neuro-symbolic cardinality estimators is a promising direction, as they allow for decoupling the impact on the cost of the algebraic aspects of the query plan and the technical characteristics of the hardware used to store and manage the KG."}, {"title": "4.4. Learned Plan Traversal", "content": "Learned end-to-end models for plan traversal in KG aim to learn a greedy heuristic for operator ordering that minimizes the query execution latency. Similar to relational databases, existing works in KG also frame this task as a reinforcement learning (RL) problem. Figure 5 shows the general architecture of a learned planner. In the RL models, the current state of the environment is a partially completed plan, and the model can choose as actions viable query operators that have not yet been included in the plan. The model is trained to optimize for a reward that correlates with the cost of the produced query plans, e.g., the number of intermediate results or the plan latency.\nUnlike the relational case, the heterogeneity of KGs prevents encoding the complete database into the query plan representation, as done in the work by Marcus et al. [27]. Furthermore, queries in KGs usually contain a significantly larger number of operations (especially joins) than queries over relational databases, making them harder to optimize."}, {"title": "4.4.1. Plan Representation", "content": "Learned plan traversal models for KGs need to represent (sub)plans in a suitable way for reinforcement learning (RL) models. The following paragraphs discuss two common methods for encoding plans: matrix representation and query tree representation.\n*   Matrix Representation One way to encode a (sub)plan is using a square matrix, where each entry encodes an atomic operator [38], e.g., a triple pattern for SPARQL queries or a subgraph pattern for other KG data models. Entries on the diagonal represent operators still missing in the plan, while entries on the off-diagonal either represent performed operators or placeholders indicating that the operator can or cannot be performed. In the case of triples, they can be encoded using IDs for the nodes, edges, and variables. While simple, this representation does not explicitly model the query tree and has no semantic information about the KG terms.\n*   Query Tree Representation Like learned cost models, another way is to explicitly represent the plan as a tree, where the nodes can be enhanced with nodes and edges that occur in the query, potential KG embeddings thereof [39], and subplan cardinalities."}, {"title": "4.4.2. Neural Architectures", "content": "Learned planners leverage reinforcement learning to enhance and devise efficient query plans. The following paragraphs describe two specific neural network approaches \u2013 Multilayer Perceptrons (MLPs) and Tree-LSTMs \u2013 employed to predict the next operation in a query plan, each with unique capabilities and advantages."}, {"title": "4.4.3. Limitations and Open Challenges", "content": "While current approaches for learned plan traversal show results comparable to dynamic programming while attaining a linear runtime, issues similar to learned cardinality and cost methods remain. Reinforcement learning is typically sample inefficient, and thus, a large set of query plans and their corresponding runtime is needed. Furthermore, changes to the data or the database setup make resampling the dataset and retraining the models necessary. Future approaches thus need to enhance how such models are optimized, e.g., by using fast and efficient representation learning and incorporating fast-to-obtain statistics to enable faster and less frequent learning of the models themselves. In this regard, approaches not based on reinforcement learning are also a promising avenue."}, {"title": "5. Challenges and Future Directions for Neuro-Symbolic Query Optimization", "content": "This chapter has presented the architecture of neuro-symbolic optimizers, where one or several symbolic components of the optimizer are entirely replaced by neural components. Another paradigm of using neural models during query optimization is to build a hybrid architecture where neural networks have a supporting role while the decision-making process is still delegated to the symbolic components. For instance, neural networks could be used to estimate the cost of a plan, which is then validated or adjusted using traditional rule-based or statistical methods. This hybrid approach can offer the benefits of neural models (e.g., learning from data) while maintaining the interpretability of symbolic methods. This hybrid architecture, however, introduces overhead during query optimization as it requires executing additional components, which hinders the overall query performance. For this reason, the goal is then to build neuro-symbolic optimizers where the neural components are first-class citizens. Besides the technical challenges already discussed in \u00a74, in the following, we discuss further challenges and future directions to deploy neuro-symbolic query optimizers in real-world systems.\nGeneralization One of the key challenges in neuro-symbolic query optimization is ensuring that the models generalize well across different types of queries, dataset structures, and data distributions. A model that performs well on one type of workload may not necessarily generalize to others, particularly in the presence of novel query patterns or changes in the underlying dataset."}, {"title": "Future Directions:", "content": "*   Hybrid data-query neural architectures: Data- and query-driven models for query optimization present inherent strengths and limitations. In particular, data-driven models can be challenging to scale up to large datasets, while query-driven models may overfit the training workload. Hence, hybrid data-query architectures can help the query-driven model generalize beyond the specific queries seen during training.\n*   Robust feature extraction: Improving the feature extraction process to ensure that the neural network can capture the most relevant and generalizable information of the query and database, which will reduce overfitting to specific types of queries.\n*   Meta-learning: Explore meta-learning approaches that enable the neural components in the query optimizer to learn how to learn. This enables quick adaption to new scenarios with minimal retraining."}, {"title": "Training Overhead and Scalability to Large Datasets", "content": "Compared to traditional optimizer components that compute summaries or statistics, training neural components often requires a large amount of input data and significant computational resources. The training process can be time-consuming, especially for large datasets and complex queries. In particular, data-driven neural architectures may suffer from scalability over large datasets, as learning from KGs with billions of statements is currently impractical. Furthermore, especially in the case of query-driven neural architectures, training data (i.e., a query workload) that is representative and diverse is not always available."}, {"title": "Future Directions:", "content": "*   Training data generators: Develop advanced data generators to efficiently produce large training data, e.g., KG with certain structures or queries and their costs, that is diverse and representative of various dataset states and query patterns. Furthermore, these data generators should be able to resemble realistic query workloads, incorporating common query patterns, edge cases, and noise that the optimizer might encounter in production environments.\n*   Hybrid data-query neural architectures: Hybrid architectures can incorporate query-level or other statistics during learning. This relieves the data-driven model from learning from large datasets and increases training efficiency.\n*   Efficient training algorithms: Research on more efficient training algorithms tailored to (knowledge graph) databases. For example, leveraging few-shot or transfer learning would reduce the overhead associated with training.\n*   Incremental learning: Focus on developing incremental learning methods that allow the optimizer to adapt and update its knowledge without retraining the neural components whenever new data or queries are introduced."}, {"title": "Interpretability", "content": "In traditional query optimizers, the (symbolic) components are inherently interpretable. This is an important feature for database administrators who need to understand the system's decision-making process to perform fine-tuning or correcting actions. Yet, neural components often function as opaque boxes. This lack of interpretability makes it difficult for users to understand, trust, and diagnose the optimizer's behavior, particularly when it makes unexpected or suboptimal decisions."}, {"title": "Future Directions:", "content": "*   Hybrid explainability techniques: Develop methods to provide explanations for the neural components' decision-making processes during query optimization. These methods may translate neural predictions into symbolic rules or use attention mechanisms that highlight important features of the queries or the dataset that impacted the component's output.\n*   User-centric tools: Build tools that allow users to query the decision process of neuro-symbolic components, thus improving trust and transparency."}, {"title": "Uncertainty", "content": "Traditional optimizers are sensitive to misestimations and other errors that introduce uncertainty along the query optimization process and lead to suboptimal plans. Besides these inaccuracies, neuro-symbolic optimizers can also be sensitive to noise, variations in the input, or biases introduced during learning, e.g., underrepresented query types in the training workload. This uncertainty can degrade the query performance, making it crucial to ensure that these systems are robust and can quantify their own uncertainty in decision-making."}, {"title": "Future Directions:", "content": "*   Uncertainty quantification: Develop techniques to quantify and communicate the uncertainty in the neural predictions", "optimization": "Robust query optimization for relational databases [40", "41,42": "aims at identifying alternative", "robust\" to optimization errors or unexpected adverse runtime conditions. Implementing neural-symbolic robust techniques requires incorporating adversarial examples during training to understand the vulnerabilities of the neural components and devise efficient plans.\n*   Neuro-symbolic adaptive query processing": "Adaptive query processing (AQP) [3"}]}