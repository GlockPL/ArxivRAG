{"title": "ProxiMix: Enhancing Fairness with Proximity Samples in Subgroups", "authors": ["Jingyu Hu", "Jun Hong", "Mengnan Du", "Weiru Liu"], "abstract": "Many bias mitigation methods have been developed for addressing fairness issues in machine learning. We found that using linear mixup alone, a data augmentation technique, for bias mitigation, can still retain biases present in dataset labels. Research presented in this paper aims to address this issue by proposing a novel pre-processing strategy in which both an existing mixup method and our new bias mitigation algorithm can be utilized to improve the generation of labels of augmented samples, which are proximity aware. Specifically, we proposed ProxiMix which keeps both pairwise and proximity relationships for fairer data augmentation. We conducted thorough experiments with three datasets, three ML models, and different hyperparameters settings. Our experimental results showed the effectiveness of ProxiMix from both fairness of predictions and fairness of recourse perspectives.", "sections": [{"title": "1. Introduction", "content": "Machine learning has been used as an effective decision-making aid in more and more fields. However, concerns have been raised about the potential unjust or biased predictions by models, which can harm individual and societal values [1]. Most popular ML models are considered black-box, making it difficult to understand their internal decision-making processes. To address this issue, there is a growing focus on achieving fair and trustworthy ML by developing explainable and interpretable techniques [2, 3, 4], auditing models to detect hidden bias [5, 6], as well as mitigating the spotted bias [7, 8].\nAmong various mitigation methods, mixup-based methods have attracted increasing attention from the community. Mixup [9] is a data augmentation method that linearly interpolates two samples to generate synthesized data for model generalization. Some research have investigate the combination of mixup with subgroup analysis for addressing fairness issues in datasets, applying it as an augmentation strategy in preprocessing [10] or a loss regularization term in training [11]. However, one limitation of mixup is that if the original labels in the dataset are biased, this bias can persist in the labels of mixed samples. The generated data labels can introduce additional bias to models."}, {"title": "2. Related Work", "content": "The fairness problem can be divided into individual and group levels. Individual fairness measures the bias by checking if similar predictions can be made for similar individuals. Group fairness compares the treatments of fairness in unprivileged and privileged groups. Fairness is achieved when the treatments are equal between groups. Prediction-based and recourse-based fairness are two perspectives for evaluating model fairness. In this paper, we focus on group fairness in machine learning.\nFairness of Prediction Outcomes Most fairness metrics are based on predicted outcomes. Demographic Parity (DP) [13] based metrics use predicted outcomes to assess whether different demographic groups are equally favored by the model. It aims for equal proportions of positive outcomes across subgroups. The DP difference between groups is called Statistical Parity Difference (SP), and DP ratio between groups is called Disparate Impact (DI). In addition to depending on predictions only, there are some fairness metrics [14] that consider both predicted and actual outcomes. Equality of Opportunity (EO) measures the True Positive Rate (TPR) of subgroups. Equalized odds (Eodds) compares both True Positive Rate (TPR) and False Positive Rate (FPR) of each groups.\nFairness of Recourse Another recent research trend is to apply Explainable Artificial Intelligence (XAI) methods to address fairness issues. One of the key components in this area is counterfactual explanation (CE), sometimes also called as algorithm recourse. CE focuses on explaining why a particular outcome occurred instead of an alternative plausible outcome. [15, 16]. Recourse refers to identifying the closest counterfactuals that could alter the result with minimal feature changes. Several algorithms have been developed to generate such counterfactual explanations for machine learning models [17, 18, 19]. The concept of fairness of recourse are proposed by [20] and defined as the disparity of the mean cost to achieve the desirable recourse among the unprivileged subgroups. [6, 21] proposed metrics based"}, {"title": "Bias Mitigation Methods", "content": "Bias mitigation methods can be categorized into three stages: pre-processing, in-processing, and post-processing [8, 23]. Pre processing mitigations aim to reduce bias by modifying and creating a fairer training dataset [24, 25, 26]. In-processing mitigation occurs during training by adding regularization and constraints to models [11, 27]. Mitigations in the post-processing stage like calibration are applied after a model has been successfully trained [21, 28]. Both pre-processing and post-processing-based methods are model-agnostic as they occur before and after the model training.\nOver-sampling in the pre-processing stage refers to changing the distribution of the training dataset by adding more samples. Duplicating instances of the unprivileged group is one straight-forward strategy [29, 30]. [31, 32] generate synthetic samples around the unprivileged group to mitigate bias. MixSG [10] takes both the privileged and unprivileged groups into consideration when synthesizing new data using mixup, but the potential bias in generated labels has not been discussed yet."}, {"title": "3. Preliminaries and Problem Statement", "content": "Notations Given the dataset $D = \\{(X, Y, Z)\\}_{i=1}^{N}$ with N samples, where X is a set of feature space, and each feature x in X has a set of values in $d_{r_i}$, label $Y \\in Y := \\{0,1\\}$, and a sensitive attribute $Z \\in Z := \\{0,1\\}$. The dataset is divided into training set $D_{train}$ and test set $D_{test}$. We use $D_{train}$ to fit a classifier model $f : X \\rightarrow Y$ and $D_{test}$ to assess the model's prediction and fairness performance. Fairness is measured by the model's performance on the difference between subgroups identified by Z. We define the unprivileged/minority group when Z=0, and Z=1 is the privileged/majority group.\nMixup Strategy in Fairness Mixup [9] is a data augmentation technique that involves blending pairs of samples to create new synthetic training examples. The premise of mixup is that linear combinations of features will result in the same linear combinations of target labels. Thus, mixup applies stochastic linear combinations to samples $S_0 (x_0, y_0), S_1(x_1, y_1)$ to generate a new sample $\\tilde{S}(\\tilde{x}, \\tilde{y})$, with random parameters A drawn from a Beta distribution.\n$\\tilde{x} = \\lambda * x_0 + (1 - \\lambda) * x_1, \\quad \\text{where } x_0, x_1 \\text{ are input vectors}$ (1)\n$\\tilde{y} = \\lambda * y_0 + (1 - \\lambda) * y_1, \\quad \\text{where } y_0, y_1 \\text{ are target labels}$ (2)\nTo address fairness concerns, previous research has explored the practice of sampling $S_0$ and S\u2081 from different subgroups, applying this step to both pre-processing stage like mixSG [10] and in-processing stage like fairMixup [11] as bias mitigation methods.\nBias Persist After Mixup The premise of mixup lies in the linear relationship between features and labels. The challenge here is if the original labels in the dataset are biased, the labels of mixed samples can retain this bias. The newly generated biased samples can impact the fairness of the trained model."}, {"title": "4. Methodology and Experiment Design", "content": "To address the issue of possible biased label for mix-up, we proposed a method called ProxiMix for improvments. It synthesizes $D_{new} = \\{(X, Y, Z)\\}_{i=1}^{K}$ from $D_{train}$ with the consideration of both pairwise and proximity samples, to reduce dataset bias. Fitting the model with fairer dataset $D'_{train} = D_{train} \\cup D_{new}$ is expected to improve its fairness performance."}, {"title": "4.1. ProxiMix Algorithm", "content": "The Importance of Proximity Awareness Given a sample $S_0$ from group $D_{train}(Z = 0)$, and another sample $S_1$ from $D_{train}(Z = 1)$, the proximity samples set of $S_1$ is defined as $D_p = \\{S_{p_0}, S_{p_1},..., S_{p_m} \\}$. The label value of each sample can be either 0 or 1. We illustrate three cases when mixing up two samples $S_0$ and $S_1$: (1) Case 1: Labels of $S_0$, $S_1$ and all of their proximity samples are the same. (2) Case 2: Labels of $S_0$ and $S_1$ are the same, but there exist different labels among proximity samples $D_p$. (3) Case 3: Labels of $S_0$ and $S_1$ are different.\nIn Case 1, linear mixing and proximity yield the same results because there are no impurities between the two samples. In Case 2, both samples $S_0$ and $S_1$ have the same label. This implies that direct mixing will result in all labels becoming 0 regardless of the mixing ratio. This approach ignores the samples from 1 in between and can potentially introduce bias when predicting subgroups with the 1 label. In Case 3, the mixed label depends on the mixing rate when using mixup directly. Specifically, the mixed label becomes 1 when the mixing rate exceeds 0.5. However, we can see in the example that the majority of the proximity samples $D_p$ between 0 and 1 belong to 0. It suggests that the probability of being classified as 0 should be higher. Considering the proportion of proximity labels can enhance the probability of being classified as 0."}, {"title": "ProxiMix Algorithm Design", "content": "ProxiMix consists of two parts: we first introduce proximity-based mixed label $Y_{sim}$ and then combine $Y_{sim}$ with $Y_\\lambda$ from the existing mixup [10] using d-adjusted balancing degree.\nAs discussed above, the current mixup approach does not account for potential biases in labels. Our proposal aims to determine the mixed label by considering the proportions of labels in proximity samples. Specifically, when mixing two samples, $S_0$ and $S_1$, we calculate their Euclidean distance with their one-hot encoded features, denoted as $P_{dis} = ||x_0 - x_1||$, to measure their proximity. Then, we select all the samples that are within the $P_{dis}$ distance from $S_0$ to form a potential proximity samples set ProxiSet. The final mixed label for $S_0$ and $S_1$ is assigned based on the label with the larger proportion within the $S_0 \\cup$ ProxiSet.\nWe combine our proximity-based $Y_{sim}$ with $Y_\\lambda$ from the current mixup to form the new definition of mixed Y, achieved by calculating $d * Y_\\lambda + (1 - d) * Y_{sim}$, where d is a balancing degree between 0 and 1. The algorithm pseudocode is described in Algorithm 1."}, {"title": "4.2. Experiment Setting", "content": "Fig. 3 presents the overall workflow of our experiment. The parameter balancing degree d in our mixup algorithm is tested with values ranging from 0 to 1, in steps of 0.1. The proximity samples for each round are set to 25. we consider proximity when there are at least 5 neighbors to ensure credibility. The mixing ratio A is randomly generated from the Beta(1,1) distribution.\nDatasets The experiment is conducted on three datasets for classification problems: (1) Adult income dataset [33]: predicting whether a person's annual income exceeds 50K (high/low-income); (2) Law school dataset [34]: predicting whether a person's in law school will fail/pass the exam; (3) Credit default dataset [35]: predicting whether a person's credit payment will be on-time/overdue.\nModels Three models including logistic regression (LogReg), decision trees (DT) and multi-"}, {"title": "5. Results", "content": "In section 5.1, we fix the balancing degree d of ProxiMix and examined the impact of different sampling modes for subgroups on the outcomes. In section 5.2, we fix the sampling mode and explored the impact of different balancing degrees d on the results. To ensure the consistency of findings, Section 5.3 assesses the effectiveness of ProxiMix from counterfactual cost perspective."}, {"title": "5.1. Sampling Mode Preferences in ProxiMix with Fixed Balancing Degree", "content": "ProxiMix is built on the mixup concept, which involves continuously selecting and mixing two samples to generate new data. To identify which combinations of samples had a more positive impact on the model's performance, we divide the dataset into different subgroups and sample from them.\nThere are four subgroups with considerations on both labels and values of a single sensitive feature in Z. The first sample selected from each group $D_{train}(Y = y, Z = z)$ is notated as C1, C3, the second sample selected from the subgroup $D_{train}(Y = y)$ which has the opposite sensitive label is notated as C'1',C2',C3',C4', respectively."}, {"title": "5.2. The Impact of Balancing Degree in ProxiMix", "content": "The above section discussed the different sampling strategies with a balanced mixup (d = 0.5). This section explores how different d in ProxiMix can impact model performance. Here, we fix strategy Ci C; while changing balance degree d.\nMost combinations positively affect a model fairness, with an optimal d that maximizes fairness improvements. The best performance is achieved at d=0.7 for the C1 \u2299 C1' strategy, while for C3 \u2299 C3', the optimal performance is reached at d=0.2.\nWe noticed the best fairness DP% and Eodds% occurs at d = 1 under C4C3'. However, both TPR of female and male groups decline when d exceeds 0.5. [36] mentions a similar scenario"}, {"title": "5.3. Counterfactual Cost across Different Groups", "content": "We now evaluate the effectiveness of our algorithm from the XAI perspective, and the results are consistent with the above observations. First, we calculate the average (avg) and standard deviation (std) of the counterfactual cost across female (F) and male (M) subgroups. Then, we compare the cost gaps between the two groups. A smaller gap indicates fairer counterfactual explanations within different groups. In the Adult dataset, C2 C1' remains to show more significant bias mitigation performance. In the Law school dataset, as we disscussed above, the improvment is limited because the bias in the original dataset is not significant."}, {"title": "6. Conclusion", "content": "This paper proposed a new debiasing algorithm called ProxiMix. It extends the mixup technique by considering labels from proximity samples in the subgroup to mitigate potential bias in the preprocessing stage. Our experiments evaluated the performance of ProxiMix with different sampling combinations and balancing degrees. The results prove that adding proximity-based labels improves fairness performance, and there exists optimal balancing degree for achieving the most significant enhancement. These observations were further supported by the experimental results on the cost comparison of counterfactual explanations. In future work, we plan to extent ProxiMix to multi-class tasks and consider intersectional fairness."}, {"title": "A. Appendices: Dataset Description", "content": "The Adult Income dataset is also known as the Census Income dataset. Its documentation 5 provides a detailed description of 14 features in the dataset. We omitted some features, such as 'fnlwgt', and the final features we used after data cleaning are as follows."}, {"title": "A.1. Adult Income Dataset", "content": "The Adult Income dataset is also known as the Census Income dataset. Its documentation 5 provides a detailed description of 14 features in the dataset. We omitted some features, such as 'fnlwgt', and the final features we used after data cleaning are as follows."}, {"title": "A.2. Law School Dataset", "content": "The Law School dataset contains admission records for law schools. We followed the description provided in [37] and the data cleaning pipeline in [21], extracting the following features for the experiment."}, {"title": "A.3. Credit Default Dataset", "content": "The Credit Default dataset, also known as the credit card clients dataset, explores default payments on credit cards. Followings are the features and descriptions."}, {"title": "B. Appendices: Results", "content": ""}]}