{"title": "Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking", "authors": ["Greta Warren", "Irina Shklovski", "Isabelle Augenstein"], "abstract": "The pervasiveness of large language models and generative AI in online media has amplified the need for effective automated fact-checking to assist fact-checkers in tackling the increasing volume and sophistication of misinformation. The complex nature of fact-checking demands that automated fact-checking systems provide explanations that enable fact-checkers to scrutinise their outputs. However, it is unclear how these explanations should align with the decision-making and reasoning processes of fact-checkers to be effectively integrated into their workflows. Through semi-structured interviews with fact-checking professionals, we bridge this gap by: (i) providing an account of how fact-checkers assess evidence, make decisions, and explain their processes; (ii) examining how fact-checkers use automated tools in practice; and (iii) identifying fact-checker explanation requirements for automated fact-checking tools. The findings show unmet explanation needs and identify important criteria for replicable fact-checking explanations that trace the model's reasoning path, reference specific evidence, and highlight uncertainty and information gaps.", "sections": [{"title": "1 Introduction", "content": "The acceleration of misinformation and disinformation in recent years means that the role of fact-checkers in verifying public information continues to grow in scale and urgency [2]. Fact-checking organisations have struggled to keep up with the increasing volume and sophistication of disinformation spread online [11, 34]. These concerns have been aggravated by the rise of generative AI. For example, publicly available Large Language Models (LLMs) such as ChatGPT\u00b9 enable the generation of text to facilitate the production of false news articles and information on social media at vast scale and speed [15], while recent developments in synthetic media generation enable production of convincing images, audio and video to mislead the public [115]. The threat to societal stability posed by misinformation has bolstered calls for the development of AI-based tools to increase fact-checkers' capacity to debunk false claims, by fully or partially automating the fact-checking process [32]. This need becomes critical as a recent report indicates that the number of fact-checking projects worldwide has plateaued in recent years and decreased in 2024 [106]. Despite the increasing amount of misinformation in the world requiring debunking, fact-checking projects are buckling under insufficient funding and political pressures [63, 87]. The development of Natural Language Processing (NLP) technologies for fact-checking is ramping up (see [32, 50, 67] for surveys), but their successful adoption relies on how effectively they can meet the needs of the fact-checkers they are intended for [70, 98].\nPrior research has demonstrated that fact-checking is a complex endeavour requiring expert knowledge, research skills, and capacity to judge sources appropriately [47, 48, 81]. Fact-checkers themselves also report being broadly sceptical of AI tools and automation, believing that such technology is incapable of handling the intricacies and complexities of fact-checking (e.g., [62, 81, 85]). Previous work notes that fact-checker scepticism towards automated fact-checking tools could be alleviated if the tools could produce explanations and rationale for their outputs [81]. Yet, it is unclear what specific information these tools must provide to be truly useful and how fact-checker demands for explainability and transparency might align with technical capabilities. The fact-checking process tends to have discrete stages with different goals, comprising different tasks [47, 62, 81]. We speculate that at each of these stages, different types of automation and explanations may be required. However, little research has focused on how technical tools might fit into fact-checker processes and how explanations might assist and support fact-checkers in decision-making. To address this gap we conducted an interview study with 10 fact-checking professionals from five continents, addressing the following research questions:"}, {"title": "2 Related work", "content": "Fact-checking has its origins in journalism and has emerged as a distinct practice in the last 20 years, spurred on by the proliferation of news through online and social media, and increased political polarisation [10, 49, 104]. Current literature distinguishes between external fact-checking, practised by independent fact-checking organisations (e.g., Snopes,2 Stowarzyszenie Demagog,\u00b3 BOOM4), which involves analysing and verifying public claims such as those made in political statements, news reports, and on social media and internal fact-checking, practised by traditional news organisations, cross-checking and correcting other journalists' work before publication to filter inaccuracies and to protect the publisher from potential liabilities [49, 62]. In this work, we focus on the needs of independent (external) fact-checkers, although we anticipate that our findings will be applicable to internal fact-checkers, whose work tasks typically comprise a subset of the fact-checking tasks. The practice of fact-checking encompasses close collaboration between fact-checkers and news editors who supervise the work, copy editors responsible for the quality of fact-checks, investigators and researchers with expertise in data analysis and visualisation tools, and social media managers who disseminate and maximise impact and engagement of fact-checks [62]. Fact-checkers are typically tasked with a wide remit of duties, such as monitoring social media, fielding reader requests, extracting and prioritising claims, researching, consulting data and domain experts, assigning veracity labels, and writing up fact-checks [47, 62, 81]. Ethnographic and interview studies have documented how fact-checkers worldwide follow broadly similar processes [47, 62, 66, 81]. Previous work has grouped these processes into four steps that comprise an archetypal fact-checking pipeline: (i) choosing claims to check; (ii) searching for evidence; (iii) assigning a verdict; and (iv) writing and communicating the fact-check [32, 50, 62, 81].\nPrevious qualitative studies have provided rich accounts of fact-checkers' workflows and the challenges they face [81], detailing the human and organisational infrastructures and stakeholder groups that underlie them [62], their core values [36] and tensions between their epistemological ideals and practices [22]. However, the precise decision-making and reasoning processes employed by fact-checkers in their work remain unclear. Studies frequently describe the antipathy of fact-checkers towards integrating automated fact-checking techniques in their work, referring to their lack of utility in practice and the absence of explanations provided by opaque systems [78, 81]. Yet existing literature does not present tangible solutions for model developers regarding how such flaws can be addressed to support how fact-checkers reason and make decisions about the claims they check. We aim to develop a more fine-grained understanding of how fact-checkers evaluate information, assign verdicts, and explain these processes (RQ1), as a prerequisite to designing computational systems and tools that support and enhance decision-making."}, {"title": "2.2 The state of automated fact-checking", "content": "Automated fact-checking systems tend to follow a fact-checking pipeline similar to the one described in the section above [32, 37, 50]. The primary tasks are: (i) claim detection and claim filtering [56, 57]; (ii) evidence retrieval [29, 73]; (iii) veracity prediction [7, 16, 110]; and (iv) explanation generation [12, 68]. Claim detection involves identifying checkable claims from sources such as social media, news articles or live political debates [57]. Successful techniques involve human-in-the-loop and semi-supervised active learning approaches where the user of a system can provide feedback or labels for selected instances [43, 111]. Additional methods for claim filtering and prioritisation identify urgent claims (based on virality or harmfulness) [1, 84, 114]. Stance detection techniques [17, 44, 94] are used to classify whether a given piece of evidence supports or refutes a claim. Although veracity prediction is performed based on the retrieved evidence, the evidence retrieval step is typically conducted in a coarse-grained way using standard search engines, which are optimised for relevance rather than for veracity prediction, leading to subpar retrieval performance [32, 52, 54]. Previous work in information retrieval (IR) has examined the challenge of retrieving credible and relevant information from online sources [29]. Automated veracity prediction techniques attempt to determine the veracity of a claim given provided evidence, tending to rely on secondary evidence documents such as news articles [44, 93], Wikipedia [110] or retrieved online sources [16]. These models vary in the number of veracity labels they consider. Where some retain"}, {"title": "2.3 Explainable AI for fact-checking", "content": "Various desiderata for explanations of AI systems have been proposed in recent years [70, 86, 105]. Of these, researchers have stipulated eight criteria specific to fact-checking explanations [67]. These criteria propose that fact-checking explanations should be actionable (i.e., provide steps towards desirable outcome), causal (i.e., use a causal model), coherent (i.e., follow natural laws, be rule-based or otherwise deterministic), context-full (i.e., presented in the context of the claim), interactive (i.e., allow users to provide feedback to the system), parsimonious (i.e., communicate necessary information with minimal redundancy), chronological (i.e., reflect when a statement was made and the information available at that time), and impartial (i.e., avoid partisan language or opinions). While these criteria are useful for NLP researchers developing basic explanation techniques, an important caveat is that these criteria are what model developers intuit as satisfactory fact-checking explanation, rather than being grounded in the needs of the people that might use these explanations, such as fact-checkers using automated tools. This limitation is a further example of the disconnect between automated fact-checking research and real-world applications and illustrates a neglect of stakeholder needs [98], crucial for explanations to be truly useful [70, 74]. Automated fact-checking encompasses different groups of stakeholders (e.g., fact-checkers, content-moderators, model developers, and laypeople), each with distinct explanation needs, tailoring to contextual factors, goals, and levels of expertise [42, 62, 70, 74]. However, existing empirical evaluations of explainable fact-checking are almost exclusively directed at and executed with laypeople from a limited selection of Western countries, rather than expert fact-checkers with diverse and varied contexts and perspectives. One such study indicated that neither feature-attribution nor example-based explanations of automated veracity prediction had an effect on laypeople's perceptions of the veracity of a news story or their intent to share it, but increased their tendency to over-rely on the AI system when it provided incorrect predictions [75]. A separate study also found no effect of example-based explanations on people's accuracy in predicting the veracity of a claim [77]. Illustrating that current explanation methods have little utility for fact-checkers, a recent study found that free-text explanations for an automated disinformation detection system improved the performance of laypeople in identifying false information, but not those of journalists [100]. Together, these studies suggest that existing explainability techniques are, at best, ineffective and, at worst, misleading and vulnerable to misuse, potentially bolstering misinformation instead of dispelling it. Both empirical [100] and literature-based [98] analyses indicate that automated explainable fact-checking research is shaped by researcher assumptions rather than by direct engagement with fact-checkers, which hinders their utility [32, 67]. We seek to address these shortcomings by identifying the explanation needs of fact-checkers and how they can be addressed by explainable fact-checking systems (RQ3)."}, {"title": "3 Method", "content": "The central goal of this research is to explore fact-checking as a primary use case for explainable NLP systems, through a collaboration between NLP and Human-Computer Interaction (HCI) researchers. We jointly developed new questions and adapted those used in prior work on fact-checkers [81] for our interview guide for semi-structured interviews with professional fact-checkers and journalists (see Appendix B). To elicit discussion of fine-grained decision-making, we asked fact-checkers to describe their processes and reasoning by referencing a claim that they had worked on recently (RQ1). For each step in the process, we asked participants about their use of automated fact-checking tools and their effectiveness (RQ2). To uncover specific explanation needs for automated"}, {"title": "3.1 Participant recruitment", "content": "We recruited 10 fact-checking professionals (see Table 1 for an overview of demographics) by advertising the study on mailing lists for fact-checkers and investigative journalists (the International Fact-Checking Network (IFCN)6 and Hacks/Hackers7), social media websites (Twitter/X and LinkedIn), and at an international conference for professional fact-checkers (GlobalFact11) in June 2024. Our recruitment material consisted of a brief description of the study, the research team, a link to a webpage containing more detailed information and an online registration form.\nIn line with prior studies [81], recruiting fact-checkers proved challenging. The interviews took place in June and July 2024, during which several major election campaigns (e.g., the European Parliament elections, Indian general election, United States presidential nominations) took place, which may have exacerbated recruitment challenges. We received a large volume of spurious responses (>200), which were identified by rapid form-completion times, invalid links to professional portfolios, and incoherent or inappropriate responses to open-ended questions. After filtering spam responses, we received 31 genuine expressions of interest. We emailed these respondents with information about the study, the information sheet, and an informed consent form. Participants returned the informed consent form, scheduled an interview slot with the first author and filled out a short pre-interview questionnaire (Appendix A)."}, {"title": "3.2 Data collection", "content": "All interviews were conducted remotely via Zoom and lasted approximately 1 hour. Conversations focused on (i) how fact-checkers describe and explain their work processes, (ii) their use of AI and technological tools in their work, and (iii) their information and explanation needs in automated fact-checking (Appendix B). All interviews were recorded and transcribed with participant permission. Participants were compensated with a USD $50 online gift voucher."}, {"title": "3.3 Data analysis", "content": "We analysed the interview transcripts using an iterative bottom-up, inductive approach inspired by grounded theory [30], using NVivo 14 software. The first author coded the transcripts using line-by-line open coding, (e.g., \"triangulate verdicts\", \"using multiple tools for diverse results\"). Axial coding (e.g., \"multi-tool usage\")"}, {"title": "3.4 Methodological limitations", "content": "Similar to prior research [81] our sample size is small, given the relatively small global population of fact-checkers. Furthermore, we only conducted interviews in English, and though our sample"}, {"title": "4 Findings", "content": "We set out to document specific work practices of fact-checkers related to evaluating claims and explaining their decisions in greater detail than has been previously documented (RQ1). We also aimed to investigate fact-checker use of automated fact-checking tools (RQ2) to identify specific information and explainability criteria tailored to automated fact-checking (RQ3). For each step of the fact-checking process [32, 50], Figure 1 depicts the type of AI tools used by fact-checkers, the AI methods underlying them, the explanation needs expressed by fact-checkers, and the existing methods that have been proposed in the explainability literature for the specific sub-task. From participant descriptions of their decision-making processes and tool use, we found that AI tool usage was present in each part of the fact-checking pipeline. In line with previous findings [32, 81], fact-checker tool use was fragmented; available tools had limited scope and performed one specific function (e.g., bot-detection for Twitter/X accounts), rather than automating multiple tasks. We also found that the explanations fact-checkers required from AI tools differed depending on the fact-checking task, and that these explanation needs only partially aligned with explanation methods proposed by the literature."}, {"title": "4.1 Claim detection", "content": "Participants described selecting claims from trending social media posts, such as TikTok, Facebook and Twitter/X, as well as receiving tips from the public via tiplines, which could be particularly useful for claims circulating on private messaging platforms such as WhatsApp.\nP9, who had an editorial role, described the process of his organisation's weekly editorial meeting: \"Journalists submit claims for likely verification, which I review before they jump on it to do their research [...] we discuss what are the trending claims, the ones that are fact-checkable. [Then] we assign them to our fact-checkers.\"\nSix participants (P2, P2, P6, P7, P9, P10) reported using AI tools for monitoring social media for detecting check-worthy and potentially harmful claims, such as Logically Accelerate, Rolli10, CrowdTangle11, as well as Full Fact Al's tool12 for live debate claim-monitoring. Despite the proliferation of such tools, cost played a key role for fact-checkers coming from lower-resourced locations (P5, P7, P9, P10):\n\"Unfortunately, we can't afford to pay for the premium features to use some of those [live debate claim detection] tools again. That's why we restrict ourselves to the open source that are freely available online\" - P9, Investigative Journalist, Nigeria"}, {"title": "4.1.3 Explanation needs for automated claim detection", "content": "Participants were generally positive about tools used for claim detection, noting that they helped to expedite identification of potential checkable claims, reduce work required to monitor multiple media channels, and surface claims they may have otherwise missed. Most reported not giving much thought to how these tools work, but those that did, tended to develop their own folk theories [33] about them. For example, P9 described his theories about how the Full Fact tool for live debate claim detection worked from observing patterns in the claims it detected:\n\"Any claim that has the 'most popular', 'most especially', that has this kind of a clause, 'highest', 'lowest', 'the best', the most this, the least this. I've seen it concurrently picking out words or phrases that has those adjectives. So that's how I [...] determine, 'Okay, this is how this algorithm is working' [...] especially any sentences that has figures involved in them, it also flags a potential claim for us to verify.\" - P9, Investigative Journalist, Nigeria\nMost participants were not interested in understanding how these tools worked, but appreciated what they perceived as support for a time-consuming task. Claim detection is a relatively low-stakes task where there are many tools, and thus, opportunities for triangulation. Inaccuracies in a particular tool were not deemed a significant drawback [32]. Where participants developed their own folk theories, such theories helped them decide when and how to use the tools and how to evaluate the delivered suggestions. Of course, risks associated with automated tools for claim detection remain. For example, existing biases in claim selection (e.g., [102]) may be magnified, or new biases may be introduced in the claim selection process [38]. Here, explanations for how and why particular claims are selected may be helpful, but capacity to coordinate the use of multiple tools and to address language deficiencies in tool performance may be more important."}, {"title": "4.2 Evidence retrieval", "content": "For our participants, evidence retrieval was the core of the fact-checking task. The crucial aspect of this stage was retrieval of primary sources for evidence, such as court documents, official statements, a person who appears to be the source of the claim, or photos and videos of an alleged event:\n\"We don't cite, for example, media citing source, we try to find always the most primary, most original source of data [... If] we're talking about migration statistics, we try to look at the migration officials, maybe ask them for an official statement.\" P3, Fact-Checker, Poland"}, {"title": "4.2.2 Automated tools for evidence retrieval", "content": "Participants used various automated fact-checking tools for assistance in identifying sources, extracting evidence from large text, video, or audio files, and, in some cases, retrieving evidence. For example, P4 and P7 discussed using AI tools for geolocation estimation, P1 described a colleague using a tool for facial recognition. P3 and P9 mentioned Bot-o-meter, a tool that identified bot-based Twitter/X accounts. Participants (P1, P3, P4, P5, P7, P8) also discussed negative experiences using LLM-based chat-bots such as ChatGPT and Microsoft Copilot for evidence retrieval:\n\"We've tested it, you know we asked ChatGPT a lot of questions about public figures in the Ukrainian context and it provided us with wrong answers shows you that you have to double-check; you cannot just trust that\" - P1, Investigative Journalist, Ukraine\nInstead, ChatGPT was more useful for summarising or sorting through evidence. For example, P7 described creating \"custom GPTs\" to assist with extracting evidence from large files:\n\"In [Chat]GPT, you can upload the documents, a file, and you can then converse with it. You can just ask the questions regarding the document\" - P7, Director & Journalist, Poland\nOverall, fact-checkers tended to be wary of relying on automated tools because they needed to be able to check that the evidence is relevant and from reliable sources. Perhaps the biggest barrier to tool usage was language-specific limitations of current AI systems. Fact-checkers who worked with languages other than English (P2, P6, P8, P9, P10) noted that many automated fact-checking tools were less reliable (if at all) for non-English text and video, as well as for non-Western accents in English. As P9 explained:\n\"Most of the AI tools that have been developed so far do not understand African accents. So they were not able to identify [...] the way I talk, I have my Nigerian accent. I am not British and not American; I'm not European either!\" \u2013 P9, Investigative Journalist, Nigeria\nP2 explained that while tools trained in English can be ineffective, where tools in local languages are available, they are appreciated:\n\"There was a tool here in Argentina that was called TranscribeMe... and it was a tool curated by Argentinians. So that worked awesomely.\"\nAt the same time, two fact-checkers (P8, P10) also mentioned that the quality of misinformation generated by AI (e.g., deepfake video or audio) was also noticeably lower for languages other than English, making it easier to identify and debunk as false. For the same reason, AI-generated disinformation in these languages can be less prevalent:\n\"Most of the tools [...] people are using to generate deepfakes [...] they haven't been trained on Indian language models [...] I'm not 100% confident ... [but] that could be why we did not see much deepfakes, and... maybe it's also got to do with the cost as well [...] let's say one party wants to defame the other party by creating, you know, generative [AI] then they have to incur a lot of cost.\" - P10, Fact-Checker, India\nLLMs are known to perform less well on low-resource languages [21, 69], though recent research has demonstrated that their performance can be improved by increasing model size [5]. However, larger LLMs also tend to require paid subscriptions (e.g., ChatGPT) or, when open source, remain expensive and compute-intensive to run. Poorer performance on non-English claims is an acknowledged issue by automated fact-checking research, however progress remains slow [117]."}, {"title": "4.2.3 Explanation needs for automated evidence retrieval", "content": "Although evidence retrieval is by no means a low-stakes task, it is a laborious one, so fact-checkers seemed willing to use a number of tools without needing too many explicit explanations. The explanations they required tended to focus on just enough information to verify claims and check reasoning. Most fact-checkers described having a robust editorial process, in which their work is cross-checked by"}, {"title": "4.3 Verdict decision", "content": "Everything that fact-checkers do culminates in a verdict. As with any complex decision, despite all the evidence it is often difficult for fact-checkers to articulate how, in the end, the decision is actually made [27, 96]. Several fact-checkers struggled to articulate their precise reasoning process, referring to gut-feeling or \"instinct\" (P7). Participants described sometimes having an intuitive sense that a claim was false, based on years of experience:\n\"How do I explain this? Because [...] when you've been doing this work for almost 6 years now, your mind just gets ingrained and it processes the misinformation quite quickly.\" - P10, Fact-checker, India\nFor most, the responsibility of deciding on a final verdict is not an individual one. Fact-checking involves highly collaborative workflows and robust editorial processes [47, 62]. P4 recounted \"having to do that fact-check process every time we did a new draft of the story [...] I redid the fact-check like 7 times overall\". P3 described his organisation's procedures of peer review and collective decision-making, and their strength in mitigating potential bias in assigning verdicts:\n\"This [biased decision-making] we try to diminish [...] all people in the team have to agree for that article to be published, because we have different points of view. So when we agree, then there is a bigger chance that there is no risk of bias, because there's always risk of bias.\" - P3, Fact-Checker, Poland\nNevertheless, when pressed, fact-checkers pointed to evidence as the foundation for their decision rationale. They described laying out a clear and logical evidence-based argument when communicating the final verdict to the public. Part of the challenge in deciding whether something is true or false, is that most misinformation claims are neither. A good lie, after all, always has a grain of truth in it. Fact-checkers (P3 and P7) acknowledged that on one hand, assigning labels such as \"true\", \"false\", \"half-true\" had significant advantages; they are clear and are easily and instantly understood by people. On the other hand, such labels can gloss over nuance and \"make reality much simpler than it is\" (P7). Instead of categorical verdicts, a number of fact-checking organisations (e.g., Pagella Politica; 13 Full Fact14) publish verdicts one or more sentences in length. These verdicts have the advantage of being able to specify which parts of the claim are true or false, allowing for more nuanced conclusions. P7 suggested that more descriptive verdicts may have more powerful long-term effects on readers:\n\"What does it mean that something is false? [...] I think I would prefer [readers] to remember the verdict in form of text [for example,] 'so this claim is against most recent reports. And it can include words such as 'false', 'truth', 'manipulation', 'unverifiable', but [it's] not the sole purpose\" - P7, Director & Journalist, Poland\nHowever, P7 also noted that: \"text-like claim and verdicts, they will encourage different type of emotional manipulation, meaning, for example, usage of some kind of verbs, which might have not neutral approaches. Which, again, someone might argue that might lead to some kind of polarisation.\"\nDescriptions for verdict labels are essentially explanations sometimes, we want them to be simple, while at other times we want more complex answers. The appropriate complexity depends on the particular claim, the context, the audience, and many other factors [61, 76]. Claims that require fact-checking can be politically sensitive or controversial, and verdict formats, whether one-word or more elaborate judgements, have simultaneous advantages and drawbacks. Our participants were conscious that even with longer explanations, sometimes all people will remember is the term \"true\" or \"false\", although at other times the fuller explanation is what matters, and this is not always predictable."}, {"title": "4.3.2 Automated tools for veracity prediction", "content": "When asked about the use of AI tools for veracity prediction, our participants were as sceptical as those in prior studies [62, 81]. They were concerned about bias, lack of nuance, and that the task is simply too complex for an automated system:\n\"AI can do some technical work. It can maybe verify images or videos or visuals, but it won't be able to"}, {"title": "4.3.3 Using Al tools for detecting Al manipulation", "content": "The introduction of generative AI has resulted in a proliferation of misinformation produced using these tools [41]. Our participants felt they needed to use tools to detect such manipulation or at least to \"confirm\" their own intuitions about AI-generated or manipulated media. P5 and P10 described using tools such as Hive Moderation, 15 DeepFake-O Meter, 16 and Truemedia.org17 by uploading media or text to the platforms, which return a verdict on its authenticity, often accompanied by a percent confidence level, such as \"Input is likely to contain AI-generated content - 99.9%\". Some participants described paying particular attention to the level of confidence accompanying an output:\n\"I think the confidence [...] does help, [...] it makes me a bit more confident as well. I was [...] 50% sure...that this is likely to be AI-generated. But now the tool is also giving me confidence.\" - P10, Fact-Checker, India\nHere too, participants were cognisant that AI tools could be unreliable, especially when the confidence level output did not match their feeling that \"something is off\" (P5) about a particular piece of media. Multiple participants (P2, P3, P5, P6, P10) described triangulating verdicts by using several AI tools to predict the veracity of the same input, check the same image or video, seek better quality"}, {"title": "4.3.4 Explanation needs for automated veracity prediction", "content": "While our participants clearly sought explanations for how automated systems might assign verdicts to misinformation, they often pointed to three aspects that an explanation should include - the why, the how, and the who.\nThe Why: Explaining model verdicts. As well as explaining their process, automated fact-checking models must also explain how the evidence they gathered justifies the predicted verdict. A common theme was that fact-checking explanations must provide the sources used in verifying the claim (P2, P3, P4, P5, P6, P7, P9, P10). More specifically, fact-checkers discussed the function of explanations as"}, {"title": "4.4 Communication: explaining fact-checks", "content": "After reaching a verdict, fact-checkers typically write an article consisting of the claim, the verdict, and the explanation of how they arrived at the verdict. Participants related how explanations are key to communicating their work to the public:\n\"Without [explanations] it would be censorship. In my opinion, if you have a tool which would be just assessing true-false, true-false without explaining... this would not be okay [...] if a big company would employ a tool that would be just a classifier that would [be] just striking, and I think some of them already do, I don't think it's okay. I think there needs to be even [a] simple explanation why the decision was made\" - P7, Director & Journalist, Poland\nAs such, the same considerations fact-checkers have for evaluating their own explanations of the final verdict are relevant to how we might need to think about the explanations provided by automated fact-checking systems. There are three main considerations that our participants brought up in how they construct explanations of the fact-check for the public: replicability, the type of verdict, and the complexity of the claim fact-checked.\nExplanations must be replicable. Replicability was identified as the most prominent theme. All participants discussed the importance of including links to sources, public data, and tools used or referenced in the fact-checking process, with the view to including sufficient information that readers can reconstruct and replicate the fact-check. P1 described this practice of providing and signposting evidence as \"self-explanatory\", while P6 identified this commitment to \"show the work\" as the central tenet of fact-checking:\n\"I think fact-checking is a different branch [of journalism] for that reason... Our work has to be shown, like the kind of math class thing back in the day, you know. Show your work. How did you arrive at this? We have to be able to do this.\" P6, Senior Fact-Checker & Project Manager, Ireland/USA\nTo date, explainable automated fact-checking techniques have focused on explaining only the predicted verdict, that is, how the evidence proves whether a claim is true or false [67]. Yet focusing"}, {"title": "4.4.2 Automated tools for communicating fact-checks", "content": "The availability of LLM-based tools that can produce well-appointed text has led to much debate on the future of journalism in general [90]. Fact-checkers were also acutely aware of these systems and their capabilities. Participants for whom English was not their first language (P2, P3, P5, P7, P8) noted that they at times used LLM-based chatbots such as ChatGPT and Microsoft Copilot18 to improve the quality of their writing, summarise fact-checks (e.g., bullet points or a short paragraph at the top of the article), edit articles, and disseminate fact-checks.\nFor example, P7 used the paid subscription to ChatGPT and mentioned using the tool for \"cross-checking, whether there is something to be improved [...] sources would have to be verified, but still... the argumentation could be relatively good\". P8 mentioned using ChatGPT to write scripts for video versions of the fact-check article to be disseminated on social media.\nParticipants (P4, P5, P7) also mentioned ethical concerns about using Al in fact-checking, specifically, the use of news articles for LLM training without permission from journalists:\n\"In the journalism world, people are relatively sceptical in terms of ChatGPT, like generative AI, because, it's stolen work, the journalists' work, to learn, without compensating them, and that's the reality.\" - P7, Director & Journalist, Poland\nSuch ethical considerations could also manifest as stigma for those using the tools:\n\"There's a sense of pride in not using these kinds of tools [...] I remember we had some discussion about ChatGPT, and [...] people were like, 'No, don't use it because somebody will tell us, you're using... some other people's work. P7, Director & Journalist, Poland\nThe fact that LLM-driven systems are only able to produce text because people have produced a lot of this text first is intractable. Where these tools could be useful, such considerations remained important, especially in an environment that constantly deals with sensitive and political issues while often struggling with precarity (see also [113])."}, {"title": "4.4.3 Explanation needs for fact-checkers", "content": "Our participants made it clear that there is a difference between the kinds of explanations they might furnish for their readers and the explanations that they themselves required of the systems they used. Fact-checkers require a high threshold of certainty in their verdicts to maintain the confidence of their readers and the general public in their work [81", "used": "n\"Confidence, depending how, if you would have an idea how to present it, would be good, maybe showing the thought process of a tool [..."}]}