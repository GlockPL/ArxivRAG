{"title": "Neurosymbolic AI for Travel Demand Prediction: Integrating Decision Tree Rules into Neural Networks", "authors": ["Kamal Acharya", "Mehul Lad", "Liang Sun", "Houbing Song"], "abstract": "Travel demand prediction is crucial for optimizing transportation planning, resource allocation, and infrastructure development, ensuring efficient mobility and economic sustainability. This study introduces a Neurosymbolic Artificial Intelligence (Neurosymbolic AI) framework that integrates decision tree (DT)-based symbolic rules with neural networks (NNs) to predict travel demand, leveraging the interpretability of symbolic reasoning and the predictive power of neural learning. The framework utilizes data from diverse sources, including geospatial, economic, and mobility datasets, to build a comprehensive feature set. DTs are employed to extract interpretable if-then rules that capture key patterns, which are then incorporated as additional features into a NN to enhance its predictive capabilities. Experimental results show that the combined dataset, enriched with symbolic rules, consistently outperforms standalone datasets across multiple evaluation metrics, including Mean Absolute Error (MAE), R2, and Common Part of Commuters (CPC). Rules selected at finer variance thresholds (e.g., 0.0001) demonstrate superior effectiveness in capturing nuanced relationships, reducing prediction errors, and aligning with observed commuter patterns. By merging symbolic and neural learning paradigms, this Neurosymbolic approach achieves both interpretability and accuracy. The data and code can be accessed on GitHub.", "sections": [{"title": "I. INTRODUCTION", "content": "Travel demand prediction plays a critical role in transportation planning, infrastructure development, and resource allocation [1]. Accurate forecasting of travel demand between regions, such as counties, enables stakeholders to optimize transportation networks, reduce congestion, and make informed decisions regarding public infrastructure investments.\nAs urbanization accelerates and mobility patterns become increasingly complex, robust and reliable methods for demand prediction are essential for meeting the dynamic needs of commuters while improving the efficiency of transportation systems.\nPredicting travel demand between counties presents a significant challenge, as it requires modeling complex relationships between spatial, economic, and mobility factors. Traditional methods, such as statistical and time-series models [2], often struggle to capture the nonlinear and multidimensional interactions inherent in real-world travel data [3]. While these approaches provide simplicity and interpretability, they fail to address the intricate dependencies that drive travel demand. Neural networks (NNs), on the other hand, excel at learning these complex patterns due to their advanced capabilities. However, their \"black-box\" nature limits interpretability, which is critical in transportation applications where transparency and accountability are essential. With the increasing availability of high-resolution mobility and economic datasets [4] [5], there is a growing demand for advanced methodologies that can effectively uncover and model these hidden patterns while maintaining a balance between accuracy and interpretability.\nIn this paper, we propose a novel Neurosymbolic approach as shown in the Figure 1 that integrates decision tree (DT) rules with NNs to enhance both the interpretability and predictive power of travel demand prediction models. DTs are used to extract interpretable if-then rules that capture key travel patterns and relationships between features, while NNs are employed to model nonlinear interactions and improve accuracy. By encoding DT rules as additional input features"}, {"title": "II. RELATED WORK", "content": "Traditional methods for predicting travel demand have relied heavily on statistical models, such as gravity models [8] and logistic regression techniques [9]. Regression analysis has also been utilized to predict the demand for Regional Air Mobility (RAM) [10] and Advanced Air Mobility (AAM) [11]. These models often assume linear relationships and may not effectively capture the complex, nonlinear interactions present in transportation systems.\nWith the advent of machine learning, more sophisticated models have been employed to improve prediction accuracy. NNs have been widely used due to their ability to model non-linear relationships and learn from large datasets [12]. In the context of travel demand prediction, NNs have demonstrated superior performance over traditional statistical methods by capturing intricate patterns in mobility data [13]. They have also been employed as a supportive tool to enhance genetic algorithms, leading to improved outcomes [14]. However, a significant limitation of NNs is their lack of interpretability. The \"black-box\" nature of these models makes it challenging for practitioners to understand the underlying decision-making processes, which is crucial in transportation planning where transparency and explainability are essential [15]. To address this black-box puzzle, two distinct branches of AI come into play: XAI [6] and Neurosymbolic AI [7]. XAI focuses on providing explanations for AI models, typically after the training process. In contrast, Neurosymbolic AI integrates neural learning with symbolic reasoning directly within the model's architecture, creating a more inherently interpretable framework.\nVarious recent research has explored the application of XAI in various aspects of trip demand prediction, from forecasting overall travel demand to understanding individual mode choices. Hu et al. [16] tackled the challenge of predicting population inflow using mobile device location data, comparing various tree-based models and interpretation techniques. Their study revealed that boosting trees outperformed other models and that while feature importance rankings were consistent across models, the choice of importance measures and hyper-parameter settings did influence the results. Moving from a nationwide perspective to a city-wide focus, Kim et al. [17] investigated the dynamic relationship between taxi and ride-hailing services in New York City. Their two-stage modeling approach combined linear regression with a long short-term memory network to predict taxi demand, effectively capturing the influence of ride-hailing services, day of the week, weather conditions, and holidays. This study demonstrated the potential of interpretable deep learning models for developing active demand management strategies, such as quota control systems, to balance demand between different modes and mitigate congestion. Kim [18] examined travel mode choice in Seoul, employing extreme gradient boosting (XGB) [19] and interpretation techniques like variable importance, interaction analysis, and accumulated local effects (ALE) [20] plots. The study not only demonstrated the accuracy of XGB but also revealed the importance of trip- and tour-related variables, age, and number of trips on tour in influencing mode choice decisions. Another study [21], achieves explainable or interpretable AI by employing two key techniques: variable importance analysis and SHapley Additive exPlanations (SHAP) [22] analysis. Variable importance analysis identifies the input variables, such as trip distance, traveler's age, and weather conditions, that have the most significant influence on the model's predictions, revealing the strongest predictors of travel mode choice. SHAP analysis then quantifies the impact of each feature, like annual income and car/bicycle ownership, on the prediction for individual instances, providing a deeper understanding of how these factors work together to shape travel mode decisions.\nIn the transportation domain, no studies have been found that focused on Neurosymbolic approaches for travel demand prediction. Despite the potential advantages, there is a gap in the literature regarding the application of Neurosymbolic AI to travel demand prediction. Existing studies have only investigated post training accuracy and interpretability of demand"}, {"title": "III. METHODOLOGY", "content": "The Figure 2 illustrates the methodological workflow for predicting travel demand using DT and NN. It begins with data integration from diverse sources, including geospatial, economic, and mobility datasets, to form the Initial Dataset. This dataset is processed and refined into the Final Dataset, ensuring feature relevance and quality. Rules are extracted using DTs with varying depths, resulting in a Dataset of the Rules, which is further refined based on variance thresholds to produce the Dataset of the Selected Rules.\nA. Data Collection and Preprocessing\nThe data for this study was collected from various sources to ensure the inclusion of key factors influencing trip distribution. Table I lists the various datasets used in this research.\nThe multiscale dynamic human mobility flow dataset [23] provides extensive spatiotemporal data on U.S. population movements since January 1, 2019. Compiled from anonymized mobile phone location data sourced by SafeGraph, this dataset documents origin-to-destination (O-D) population flows at the census tract level on daily bases. Its detailed granularity and temporal coverage make it a valuable resource for applications in transportation planning. For this study, the daily dataset from March 15, 2021, to April 15, 2021, was extracted. The Open Source Routing Machine (OSRM) was utilized to obtain geographical features for counties within Tennessee, while economic data was sourced from the Boyd Center for Business and Economic Research. County profiles and rankings were obtained from datasets provided by the Tennessee Commission on Children and Youth. All datasets underwent cleaning, standardization, and were merged using county-level identifiers. Missing values were imputed using median substitution, and numerical variables were scaled to facilitate comparability across features. Ultimately, 11 features were prepared for each county.\n\u2022 Land Use Counts: Consists of two features: NaturalAreaCounts and PublicAreaCounts representing natural area counts like forest, agricultural etc. and public area counts which includes places like residential, commercial, industrial etc. respectively.\n\u2022 Points of Interest (POI): Counts of POIs like educational places, commercial, healthcare, entertainment etc. within the counties.\n\u2022 Roads: Classified into two categories: MajorRoads and OtherRoads.\n\u2022 Transportation: Includes the total count of airports, railway stations, bus stations, etc.\n\u2022 Economic Features: Consists of three features: Unemployment Rate, Employed Population, and Sales Tax Revenue.\n\u2022 Ranking: Overall ranking of the counties in TN based on health, economic well-being, education etc.\n\u2022 Population: Provides population count for each county.\nAbove features of both origin and destination counties were merged with the mobility dataset to obtain the initial dataset which consists of 22 columns for representing the features of origin and destination, 2 columns that provides time and distance required to travel from one county to another, one column that specify the day either weekday or weekend and last column provide the number of flows between the counties.\nA multicollinearity check was performed to ensure that all features in the final dataset had a variance inflation factor (VIF) < 10. This step eliminated redundant or highly correlated variables. After doing so we end up with the dataset that contains 10 features along with the population flow column, making it an 11 column dataset.\nB. Decision Tree Rule Extraction\nWe employed DT as a method to derive interpretable rules that elucidate the relationships between features and target variables. The process of rule extraction and subsequent feature generation involves several stages, as outlined below:\n1) Construction of Decision Trees with Various Depths: DTs of varying depths, ranging from 3 to 15, were constructed to balance the trade-off between model interpretability and predictive accuracy. Shallow trees (e.g., depth 3) emphasize interpretability by isolating the most significant features driving the outcomes, while deeper trees (e.g., depth 15) capture complex, nonlinear relationships. This variation enabled a comparative evaluation of model complexity versus performance, ensuring the retention of valuable information from both simple and complex feature interactions.\n2) Extraction of Rules from Decision Trees: After training the DTs, the paths from root nodes to leaf nodes were traversed to extract if-then rules. Each path represents a rule delineating a subset of the data based on conditions imposed on input features. These rules encapsulate the underlying patterns and thresholds, enabling a clear understanding of the decision-making process for specific predictions. \nC. Generating Dataset of the Rules\nThe extracted rules from the DT were then applied to the dataset to generate a new set of features. These features were encoded as binary indicators, where each feature corresponds to a rule: a value of 1 indicates that the rule's conditions are met, and a value of 0 indicates otherwise. We generated various datasets corresponding to the various depths and different number of rules from it.\nTo ensure the relevance and practicality of the newly generated rule-based features, a systematic filtering process was implemented. Rules with minimal variance\u2014specifically those falling below thresholds of 0.01, 0.001, and 0.0001\u2014were excluded. As a result, we have four different datasets for rules: one that has all rules and other three with the rules having variances of 0.01, 0.001 and 0.0001 respectively for different depth of DTs."}, {"title": "D. Neural Network Implementation", "content": "To integrate the rules into the NN, we generated 60 distinct datasets derived from the rules extracted from DTs. These datasets were used to train the NN, alongside the final dataset, to assess how different combinations of rules and tree depths influence the NN's performance."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "A. Evaluation Metrics\nIn this study, we utilized three evaluation metrics to assess the performance of our model: R-squared (R2), Mean Absolute Error (MAE), and Common Part of Commuters (CPC).\n1) Mean Absolute Error (MAE): MAE is a metric that measures the average magnitude of errors in the model's predictions, without considering their direction (i.e., positive or negative). It is given by the formula:\n$MAE = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y_i}|$\nwhere:\n\u2022 $y_i$ is the actual value,\n\u2022 $\\hat{y_i}$ is the predicted value,\n\u2022 n is the number of data points.\n2) R-squared: R2, also known as the coefficient of determination, measures the proportion of variance in the dependent variable that is explained by the independent variables in the model. Mathematically, it is expressed as:\n$R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$\nwhere:\n\u2022 $y_i$ is the actual value,\n\u2022 $\\hat{y_i}$ is the predicted value,\n\u2022 $\\bar{y}$ is the mean of the actual values,\n\u2022 n is the number of data points.\nAn R2 value closer to 1 indicates that the model explains a higher proportion of the variance, implying better fit.\n3) Common Part of Commuters (CPC): The Common Part of Commuters (CPC), also known as the S\u00f8rensen-Dice index, is a metric used to measure the similarity between the generated flows (yg) and real flows (yr) in flow generation models. CPC is computed as:\n$CPC = \\frac{2\\sum_{i,j} min(y_g (l_i, l_j), y_r (l_i, l_j))}{\\sum_{i,j}y_g(l_i, l_j) + \\sum_{i,j}y_r(l_i, l_j)}$\nwhere:\n\u2022 $y_g(l_i,l_j)$ represents the generated flow from location $l_i$ to location $l_j$,\n\u2022 $y_r(l_i,l_j)$ represents the real flow from location $l_i$ to location $l_j$,\n\u2022 The summations are over all possible origin-destination pairs (i, j).\nCPC ranges from 0 to 1, where 1 indicates a perfect match between the generated and real flows, and 0 signifies no overlap between them. When the total generated outflow equals the real total outflow, the CPC metric becomes equivalent to the accuracy of the model, measuring the fraction of trips assigned to the correct destination.\nB. Results\nThe Figure 3 illustrates the performance of the NN model evaluated on three datasets: the Final Dataset, the Rules Dataset, and the Combined Dataset (Rules + Final Dataset). The results are analyzed using three metrics: (MAE), R2, and CPC, across varying DT depths (3-15). The Figure 3a demonstrates a significant reduction in MAE as the DT depth increases for the Rules Dataset and the Combined Dataset. The Rules Dataset initially exhibits high error, but deeper trees capture more granular patterns, reducing MAE. The Combined Dataset consistently achieves the lower MAE than the Final Dataset. However, the rules only dataset started to perform better consistently than baseline dataset in higher depths, notably after depth of 9. The R2 results of Figure 3b reveal the similar pattern. The Rules Dataset shows steady growth in R2, reflecting its ability to capture relationships more effectively as tree depth increases. The Combined Dataset achieves the highest R2 values across most depths, highlighting the advantage of integrating rules with the original dataset. The CPC results from Figure 3c follow a similar trend, with the Rules Dataset and the Combined Dataset showing significant improvement as the tree depth increases. The Rules Dataset starts with lower CPC values, but as the rules grow more complex, it aligns better with the observed commuter patterns. The Combined Dataset achieves the highest CPC values at all depths, indicating superior alignment with commuting patterns.\nThe Figure 4 illustrates the performance of the NN model evaluated on various sets of rules selected based on different variance thresholds (0.01, 0.001, 0.0001) in combination with the Final Dataset. The performance is compared using three metrics: MAE, R2, and CPC, across DT depths (3\u201315). The MAE in Figure 4a shows variability across different variance thresholds and depths. The rules selected with a variance threshold of 0.0001 consistently result in lower MAE values, indicating better predictive performance, particularly at greater depths. Conversely, rules selected with higher variance thresholds (0.01 and 0.001) exhibit higher MAE, especially at shallow depths. All the dataset are better than the baseline one. The R2 metric in Figure 4b highlights the fluctuation in model performance based on different variance thresholds. The rules with a variance threshold of 0.0001 achieve the highest R2 values at most depths, indicating superior model fit. Rules with thresholds of 0.01 and 0.001 show fluctuating R2, reflecting inconsistencies in capturing relationships as depth increases. Similar pattern are seen in the Figure 4c that evaluates the alignment of predicted and observed commuter patterns. Rules with a variance threshold of 0.0001 exhibit consistently higher CPC values, particularly at greater depths, demonstrating their"}, {"title": "V. DISCUSSION", "content": "The experimental results demonstrate the effectiveness of integrating DT rules into the NN framework. Across all metrics- MAE, R2, and CPC-the combined dataset (Final Dataset + Rules) consistently outperformed the standalone datasets. This highlights the complementary nature of the original features and the extracted DT rules. Specifically, rules selected at lower variance thresholds (e.g., 0.0001) significantly enhanced performance, underscoring the importance of granular rule selection in capturing nuanced relationships within the data.\nA. Impact of Integrating Decision Tree Rules\nThe integration of DT rules into the NN significantly improved its interpretability and predictive capability. The rules extracted from DTs provide explicit if-then conditions, enabling the model to incorporate domain knowledge and decision boundaries effectively. These rules not only complement the NN's ability to capture non-linear relationships but also mitigate overfitting by introducing structured and meaningful constraints. The experiments reveal that integrating rules led to lower MAE, higher R2, and improved CPC, particularly at greater tree depths and finer variance thresholds.\nB. Analysis of Rule Depth and Quantity\nThe depth of the DTs and the number of rules extracted at each depth played a crucial role in the model's performance. As tree depth increased, the total number of rules grew exponentially, offering a richer representation of the data. However, this came with diminishing returns, as excessively large rule sets introduced noise and complexity. Rules selected at lower variance thresholds effectively balanced rule quantity and relevance, achieving superior performance. For instance, at depths 10-14, rules selected with a 0.0001 variance threshold consistently outperformed those selected with 0.01 or 0.001 thresholds, demonstrating that more granular rule sets capture finer patterns in the data.\nC. Potential Reasons for Observed Improvements\nSeveral factors contribute to the observed improvements in the model's performance:\n\u2022 Feature Augmentation: The integration of DT rules augmented the original feature set with interpretable and data-driven constraints, enhancing the model's ability to generalize.\n\u2022 Granular Rule Selection: Rules selected at lower variance thresholds introduced finer-grained patterns, enabling the model to capture subtle relationships that might be overlooked in the original dataset.\n\u2022 Hybrid Framework: Combining DT-based interpretability with the NN's capacity for learning non-linear relationships created a synergistic effect, leading to robust predictions.\n\u2022 Reduced Overfitting: By incorporating rules, the model avoided over-reliance on noisy or irrelevant features, resulting in better generalization across depths.\nD. Limitations\nWhile the proposed methodology demonstrates promising results, certain limitations must be acknowledged to provide a balanced perspective and guide future improvements.\n1) Data-Related Constraints: One key limitation lies in the potential biases or constraints inherent in the datasets used for this study. The reliance on data from specific sources, such as mobility datasets and economic indicators, may limit the generalizability of the findings to regions or scenarios with differing data availability or quality. Additionally, the static nature of the dataset may not fully capture temporal dynamics, such as seasonal or real-time fluctuations in travel demand.\n2) Rule Selection and Scalability: The process of rule extraction and selection introduces challenges related to scalability and computational complexity. As tree depth increases, the number of generated rules grows exponentially, necessitating careful thresholding to ensure computational feasibility."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "This study proposed and evaluated a hybrid framework that integrates DT-based symbolic rules with NNs for travel demand prediction. The key findings demonstrate that incorporating DT rules significantly enhances model performance across multiple metrics, including MAE, R2, and CPC. Rules selected at finer variance thresholds (e.g., 0.0001) were particularly effective in capturing nuanced relationships, leading to improved predictive accuracy and alignment with observed commuter patterns. The integration of symbolic rules provided an interpretable structure while leveraging the non-linear learning capabilities of NNs, resulting in a synergistic performance boost. This approach aligns with the principles of Neurosymbolic AI, where symbolic rules enhance model transparency and provide actionable insights, while NNs enable the model to learn complex patterns from the data. The DT rules offer clear decision boundaries that complement the data-driven flexibility of NNs. This combination mitigates overfitting, augments the feature space, and improves generalizability.\nFuture research will aim to overcome the limitations identified in this study by exploring dynamic rule selection methods, adaptive variance thresholds, and more efficient mechanisms for integrating symbolic rules into NN architectures. Expanding the dataset to incorporate diverse, real-time data sources will enhance the framework's applicability across various scenarios. Furthermore, integrating explainable NN architectures, such as attention mechanisms or self-explaining models, could improve transparency and interpretability within the hybrid approach."}]}