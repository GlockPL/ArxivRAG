{"title": "MMBee: Live Streaming Gift-Sending Recommendations via Multi-Modal Fusion and Behaviour Expansion", "authors": ["Jiaxin Deng", "Shiyao Wang", "Yuchen Wang", "Jiansong Qi", "Liqin Zhao", "Guorui Zhou", "Gaofeng Meng"], "abstract": "Live streaming services are becoming increasingly popular due to real-time interactions and entertainment. Viewers can chat and send comments or virtual gifts to express their preferences for the streamers. Accurately modeling the gifting interaction not only enhances users' experience but also increases streamers' revenue. Previous studies on live streaming gifting prediction treat this task as a conventional recommendation problem, and model users' preferences using categorical data and observed historical behaviors. However, it is challenging to precisely describe the real-time content changes in live streaming using limited categorical information. Moreover, due to the sparsity of gifting behaviors, capturing the preferences and intentions of users is quite difficult. In this work, we propose MMBee based on real-time Multi-Modal Fusion and Behaviour Expansion to address these issues. Specifically, we first present a Multi-modal Fusion Module with Learnable Query (MFQ) to perceive the dynamic content of streaming segments and process complex multi-modal interactions, including images, text comments and speech. To alleviate the sparsity issue of gifting behaviors, we present a novel Graph-guided Interest Expansion (GIE) approach that learns both user and streamer representations on large-scale gifting graphs with multi-modal attributes. It consists of two main parts: graph node representations pre-training and metapath-based behavior expansion, all of which help model jump out of the specific historical gifting behaviors for exploration and largely enrich the behavior representations. Comprehensive experiment results show that MMBee achieves significant performance improvements on both public datasets and Kuaishou real-world streaming datasets and the effectiveness has been further validated through online A/B experiments. MMBee has been deployed and is serving hundreds of millions of users at Kuaishou.", "sections": [{"title": "1 Introduction", "content": "Due to the rapid development of mobile device hardware and the Internet, live streaming has become a prevalent social service for people's daily lives. As one of the most popular live streaming platforms in China, Kuaishou has reached 386.6 million daily active users and the revenue generated by the live streaming business reached RMB 9.7 billion as of the third quarter of 2023, which heavily relies on Kuaishou's continuous optimization of the live streaming ecosystem and improvement of the recommendation system. As shown in Figure 1, on live streaming platforms, content creators can share their produced video content with users in real-time, and users can interact with streamers and peers through live comments or discussions. They can even send virtual gifts to their favorite streamers, which is one of the main sources of revenue for the live-streaming business. Therefore, the task of live streaming gifting prediction is vital not only for enhancing user experience and streamer revenue but also for increasing the business effectiveness of the platform.\nRecent years have witnessed several relevant methods for recommendation [12, 21, 25, 35, 36] and gifting prediction [11, 32] in live streaming. For example, MARS [11] introduces a two-stage recommendation approach applied in the Multi-Stream Party scenario, aiming to maximize reward earnings while optimizing user personal experience at the same time. However, this approach ignores the close connection between users' gifting behavior and the rapidly changing live content in the living room. To address this issue, MTA [32] designs a novel orthogonal module that fully utilizes the multi-modal features in live streaming. However, MTA treats the gift prediction as a time series prediction problem which does not consider users' personalization. Although typical behavior-based methods like SIM [20] can achieve personalized recommendations for gifting prediction, they may face the challenge of behavior sparsity in the context of live streaming. According to [6], DNN-based methods typically require a minimum of 5-10 historical behavior sequences to learn meaningful representations for modeling user interests. However, the average length of user's gifting behavior is as low as 0.3 anchors in our scenario. Therefore, gifting prediction requires a comprehensive consideration that combines user personalization under sparse behaviors and real-time content modeling to achieve optimal recommendation effectiveness.\nTo address these challenges, we propose MMBee: an efficient live streaming gifting prediction method based on real-time Multi-Modal Fusion and Behaviour Expansion. Specifically, we first design a Multi-modal Fusion Module with Learnable Query (MFQ). It helps the model to perceive the real-time content changes in live streaming through processing the complex visual frames, comments and audio in each streaming segment. In addition, aiming to address the sparsity problem in gifting prediction, we propose a novel Graph-guided Interest Expansion (GIE) approach. We first construct large-scale gifting graphs based on the history of gifting interactions. Then a graph pre-training scheme via contrastive learning (GraphCL) is adopted to learn general and robust streamer and user representations. Apart from these learned self-supervised embeddings, we further extend behavior sequences through metapaths with the graph structural information and optimize the representations in an end-to-end manner with online recommendation model. Both of the self-supervised and end-to-end learning schemes help model jump out of the specific historical gifting behaviors for potential preferences exploration and largely enrich the behavior representation. Finally, to meet the low latency requirements of the online serving system, we propose a decoupled graph offline training and online inference strategy. MMBee has now been deployed on the live-streaming recommendation system of Kuaishou, serving millions of active users every day.\nOverall, our contributions are shown as follows:\n\u2022 The proposed Multi-modal Fusion with Learnable Query (MFQ) module leverages the dynamic multimodal content of live streaming and captures the distinct characteristics among streamers.\n\u2022 Graph-guided Interest Expansion (GIE) module largely enriches the observed history behaviors of users and streamers with both self-supervised graph representation learning and metapath-based behavior expansion to alleviate the sparsity problem.\n\u2022 We validate the effectiveness of MMBee through extensive offline experiments on Kuaishou's 3 billion scale industrial dataset and public dataset. Online A/B tests further show that MMBee brings significant online benefits and we build efficient industrial infrastructure to deploy MMBee on the real-world online live streaming recommendation."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Live Streaming Gifting Recommendation", "content": "Existing works on live streaming gifting recommendation systems primarily view the whole live room as recommendation target and model the interaction between streamers and viewers only with categorical data. For instance, MARS [11] proposes a novel recommendation scenario called Multi-Stream Party (MSP) and designs two-phase methods to jointly maximize the reciprocal response of donations and optimize MSP personal satisfaction. LSEC-GNN [36] models the live stream e-commerce scenario using GNN and fully leverages the interaction information among streamers, users, and products. However, previous research ignores that dramatic content changes can occur even within the same live room thus it is vital to make full use of the multi-modal feature in live streaming. Aiming to solve this issue, MTA [32] introduces a novel orthogonal projection model to capture the cross-modal information interaction of real-time content. However, MTA formulates the gifting prediction task as a time series prediction problem and neglects the personalization modeling of users' interests. In conclusion, there still exists great room for improvement in existing methods for live-streaming gifting prediction."}, {"title": "2.2 Personalized Recommendation", "content": "The most widely adopted personalized recommendation methods in the industry are based on deep neural networks. For instance, DIN [42] models users' diverse interests in different target items by introducing attention mechanisms. SIM [20] proposes an online two-stage retrieval method that models relevant behaviors from a user's long-term history based on the features of the current candidate item. However, in live-streaming gifting scenarios, it is challenging to achieve satisfactory results with these methods due to the sparsity issue in streamers and user interactions. Recently, several works combined with GNNs have introduced multimodal features to enrich the embedding of graph nodes. For instance, MMGCN"}, {"title": "3 Preliminaries", "content": "In live streaming platforms, we use users to represent the viewers who watch live streaming and use authors to represent streamers. Vu = {u\u2081, u\u2082, \u00b7\u00b7\u00b7, u\u2098} is the set of users and Va = {a\u2081, a\u2082, \u00b7\u00b7\u00b7, a\u2096} is the set of authors who are broadcasting at the current time, where m is the numbers of users and k is the numbers of authors. Previous studies treat the whole live streaming room as the recommendation target while ignoring the real-time change of streaming content. Thus, different from traditional recommendation tasks, we divide each live room into multiple consecutive 30s live segments and the live segment of author a at the current time is denoted by \u03b4\u2090. We formulate that all live streaming segments of the current moment are the recommendation target and Ma = {v\u2090, s\u2090, t\u2090} is the multi-modal raw data tuple, where v\u2090, s\u2090 and t\u2090 represent the visual frames, speech and comment text gathered from frame \u03b4\u2090.\nGiven a set of triples (u\u2c7c, a\u2c7c, y\u2c7c), y\u2c7c = 1 means that u\u2c7c send gift to a\u2c7c, otherwise y\u2c7c = 0. Thus, the gift-through-rate (GTR) prediction problem is to predict whether user u\u1d62 will send gift to a\u1d62 given the multi-modal raw data M\u2090\u2081 = {v\u2090\u2081, s\u2090\u1d62, t\u2090; } in the current live streaming segment \u03b4\u2090\u2081:\n$$p = f(a_i, u_i, M_{a_i})$$\nwhere p is termed as the gift through rate (GTR) and f (\u00b7) is the GTR prediction model. In this work, we choose SIM [20] as our foundational model, considering its widespread usage in the industry and its online efficiency and effectiveness. The objective function utilized in our method is the negative log-likelihood function, which is defined as follows:\n$$L = - \\frac{1}{N} \\sum_{i=1}^N (y_i \\log p_i + (1 - y_i) \\log(1 - p_i))$$ where y\u1d62 denotes the ground truth label indicating whether current segment gets donation and p\u1d62 \u2208 [0, 1] is the predicted GTR."}, {"title": "4 Multi-modal Fusion with Learnable Query", "content": "For each live streaming segment, three frames are evenly sampled from each segment and necessary filtering process is conducted to clean the gathered ASR (Automatic Speech Recognition) and comment text. Then, we extract the multi-modal feature of raw data with Kuaishou's internal pre-trained 8 billion parameters multi-modal model K7-8B\u00b9 and the extracted multi-modal feature sequences tuple of visual, speech and comment at the current moment of author a are represented with X\u1d65, X\u209b and X\u209c, respectively.\nSince processing and integrating information from different modalities is quite important [2, 10, 38], we propose a multi-modal fusion with learnable queries to ensure efficient modality interactions. Inspired by [32, 33], we adopt the orthogonal projection (OP) operation to maximize the complementation effects between different modalities. For example, take X\u1d65 as target modality, we calculate the relevant scores between the visual modality X\u1d65 with another two modalities by using correlation operations:\n$$Corr_{vs} = Softmax(X_vX_s)$$\n$$Corr_{vt} = Softmax(X_vX_t)$$\nwhere Softmax() is the softmax operation. Then, the irrelevant parts are obtained through 1 \u2212 Corr operation. Finally, the fused latent feature of visual modality Y\u1d65 is performed with:\n$$Y_v = OP(X_v, X_s, X_t) = X_v + X_s(1 - Corr_{vs}) + X_t(1 - Corr_{vt})$$\nNote that 1 \u2212 Corr represents the dissimilarity vector that measures the difference between two modes' representation. It helps to preserve the parts of other modalities that are orthogonal to the target modality and remove duplicate information to prevent redundancy. Then, as shown in the online stage of Figure 2, we utilize the orthogonal latent features in a hybrid fusion [23] manner applied with cross-attention and self-attention [27] alternately. The fused feature hf is gotten with:\nh\u1d65 = CrossAttention(X\u2090W\u1d65\u146b, Y\u1d65WK, Y\u1d65WV), Y\u1d65 = OP(X\u1d65, X\u209b, X\u209c)\nh\u209b = CrossAttention (X\u2090W\u209b\u146b, Y\u209bWK, Y\u209bWV), Y\u209b = OP(X\u209b, X\u209c, X\u1d65)\nh\u209c = CrossAttention(X\u2090W\u209c\u146b, Y\u209cWK, Y\u209cWV), Y\u209c = OP(X\u209c, X\u209b, X\u1d65) (5)\nhf = h\u1d65 \u2295 h\u209b \u2295 h\u209c\nhf = SelfAttention(h\u1d65W\u1d6b, h\u209bW\u1d6b, h\u209cW\u1d6b)\nHowever, the fused feature hf can only reflect the content-level representation, thus lacking the connection to distinctive characteristics across various types of authors. To address this issue, we produce several learnable query[13, 43] tokens q\u2098 \u2208 R\u1d3a\u00d7\u1d48 to extract streamer-aware content patterns. Note that each author keeps a set number of learnable query embeddings which are randomly initialized. N represents the number of query tokens for each author. The learnable query first interacts with fused multi-modal features through cross-attention layers as:\nh\u2098 = CrossAttention(q\u2098W\u1d6b, hfWK, hfWV)\nThen the queries interact with each other through self-attention layers to fuse the necessary information among different patterns:\nh\u2098 = SelfAttention(h\u2098W\u1d6b, h\u2098WK, h\u2098WV)\nThe multi-modal fusion module benefits from the learnable queries in two major aspects: 1) Each author has learnable tokens that store their specific highlight content patterns. The tokens can be activated at certain moments of awesome content, which is quite useful for gifting prediction. 2) These queries help align the multimodal representations with the ID embedding based recommendation space, thereby maximizing their mutual information. Consequently, the integration of learnable queries further enhances model's ability to capture real-time content."}, {"title": "5 Graph-guided Interest Expansion", "content": ""}, {"title": "5.1 User-to-Author and Author-to-Author Graph", "content": "Based on the users' donation history, we first construct a User-to-Author(U2A) graph G\u2081 (Vu U Va, E\u2081) that represents the correlation between users and authors, where Vu and Va are the sets of users and authors respectively and E\u2081 represents the donation relationship between users and authors. As illustrated in Figure 3 (a), the circle represents the user, and the square represents the author. If a user has previously made a donation, an edge exists between the user and the donated author in this graph. The weight of the edge is the amount of donated money and an author node has the attribute of aggregated multi-modal feature. In this way, the large User-to-Author graph is constructed.\nBased on the aforementioned U2A graph, we further construct the Author-to-Author (A2A) Graph G\u2082 (Va, E\u2082) to represent the interdependence among authors, where E\u2082 denotes the Swing similarity [34] relationship among authors. In this graph, each node represents an author, and the edge weight represents the Swing similarity between the authors. The similarity between author i and author j is given below:\n$$s(i, j) = \\sum_{\\scriptsize U\\in \\text{U}_i \\cap \\text{U}_j} \\sum_{\\scriptsize u\\in \\text{U}_i \\cap \\text{U}_j}  \\frac{1}{\\alpha + I_u I_v}$$\nwhere U\u1d62 is the set of users who have made donations on author i and I\u1d64 is the set of authors that donated by user u.\nA2U graph is established through donation relationship between users and authors. The design of edge weights and sampling strategies helps enrich the representations of authors who have a rich history of being donated. However, there are some new or cold-start authors. Their limited donation history makes it difficult to benefit from the A2U graph. Fortunately, A2A graph is built from the swing similarity defined in Equation 8, which finds substitutable authors based on the substructures of user-author donation bi-partitive graph. It is useful for linking cold-start author to warm-start author and encouraging the engagement of cold-start authors, so A2A graph is quite necessary.\nAfter constructing U2A and A2A graphs, we first leverage the graph node representation learning approach to train graph embedding layer in Section 5.2. Next, we propose metapath based behavior expansion process to enrich sparse behavior sequences in Section 5.3. To provide a precise demonstration of the abovementioned methods, we first establish the following definition:\nDEFINITION 1 (\u041c\u0415\u0422\u0410\u0420\u0410\u0422\u041d[5]). Metapath is defined as a relation sequence to capture the specific structural relation between objects. In A2U and A2A graph, we define five metapaths: three metapaths P u2a2u, Pu2a2u2a, Pu2a2a begin from target user, for example p u2a2a ="}, {"title": "5.2 Node Representation Pre-training with GraphCL", "content": "Previous studies [3, 18, 19, 37] have shown that graph node embedding algorithms are beneficial for recommendation systems for tackling data sparsity problem because these methods are able to effectively capture the user-author relatedness from graph structures. To leverage the connectivity information of the whole graph, we apply the graph contrastive learning (GraphCL) framework to train the graph embedding layer. Aiming to cluster similar nodes together while pushing away dissimilar ones, we loop through all nodes in the whole graph G\u2081 and obtain positive sample set Vp through the metapath-guided neighbor process and the negative nodes set Vn are sampled randomly. The positive and negative nodes are utilized with the Cross-Entropy loss LCE and InfoNCE [17] LNCE loss for optimizing the parameters of the node embedding layers. Algorithm 1 shows the core of our approach and the trained graph node embedding implies the connectivity information from the whole graph. The InfoNCE loss is defined with Equation 9.\n$$L_{NCE} = -\\frac{1}{|V_p|} \\sum_{v_i\\in V_p} \\log \\frac{\\exp(\\Theta(v_i)^T\\Theta(v_p))}{\\exp(\\Theta(v_i)^T\\Theta(v_p)) + \\sum_{v_j \\in V_n} \\exp(\\Theta(v_i)^T\\Theta(v_j))}$$"}, {"title": "5.3 Metapath-guided Behavior Expansion through End-to-End Training", "content": "When analyzing the node number distribution of the constructed A2U graph, we observe that the average outdegree of user nodes is 0.32. It becomes difficult for widely used behavior-based models like SIM to study meaningful representations and explore potential gifting preferences. Furthermore, the graph embedding in Section 5.2 is trained in a self-supervised manner which is not directly optimized for the recommendation model. To address these challenges, we expand the behavior sequence of the target user and author using various pre-defined metapaths [5]. Due to the computation cost, we perform up to 3-hop neighbors on both U2A and A2A Graph. We enumerate all possible metapaths and five metapaths with the highest scores are selected using commonly used feature importance filtering methods as follows:\n\u2022 N(2)\n(ut) begins with the target user ut and follow this metapath. The retrieved behavior sequence is a set of users who share the same authors as the target user. Therefore, this metapath gets similar users who share the similar interests of the target user."}, {"title": "5.4 System Deployment", "content": "As shown in Figure 4, our recommendation model and graph embedding layer are trained on Kuaishou's large-scale distributed training system. Each day, hundreds of millions of users visit Kuaishou, actively watching and interacting with live-streaming content, resulting in the generation of hundreds of millions of logs for watching and interaction. These logs are collected, preprocessed in real-time, and utilized for training the model. Our training system incrementally updates the model parameters by incorporating the latest user-author interactions, multi-modal content features, and trained graph embedding. The trained parameters are synchronized to the online inference model for online serving. To train graph embedding, we first gather the users' historical donation behavior and utilize it to build the User-Author and Author-Author donation graphs. The topology of these two graphs is stored in a key-value based storage system called KGNN\u00b2. Then the graph embedding trainer requests the KGNN server with Algorithm 1 for training the node embedding layer and the KGNN storage updates once a day.\nDuring the training and inference processes of the recommendation model, it needs to request the metapath-guided neighbors of the target user and author. As shown by the red dashed line in Figure 4, one approach is to dynamically request the KGNN storage. However, this method can impose significant computational overhead on the KGNN server and result in great time delays when walking on the entire graph. To address this issue, as shown by the green dashed line in Figure 4, we apply the pre-requested expansion manner and store the metapath-guided neighbors of all nodes in the graph in the Graph Behavior Offline Storage in advance. As a result, the online recommendation model can directly access the Graph Behavior Offline Storage to retrieve the sequence without having to walk on the graph."}, {"title": "6 Experiment", "content": ""}, {"title": "6.1 Dataset", "content": ""}, {"title": "6.1.1 Kuaishou Dataset", "content": "We first test our method on company internal dataset called Kuaishou Dataset. It includes about 3 billion user interaction logs with live-streaming content in Kuaishou App. This dataset is collected as follows: We first apply a 30s sliding window to generate the streaming segment samples. If the user requests the recommendation service and makes a donation at time t, then only the segment containing t will be taken as the positive training sample while other samples will be ignored. On the contrary, if the recommended live broadcast has impressed but users' donation behavior does not occur until exiting, the segment when user exit will be adopted as negative sample [14]. With this process, the sparsity of of Kuiashou dataset is 99.969% which is reasonable. Kuaishou dataset is composed of two parts: Dtrain and Dtest, where Dtrain is users' real interaction logs from 7 days of all live streaming content during that period for the training phase. The Dtest is sampled from the following one-day logs after Dtrain is collected, which is used to test model's performance."}, {"title": "6.1.2 Public Dataset", "content": "To prove the effectiveness of our proposed MFQ and GIE module, we also compare our method on two public short video recommendation datasets: TikTok and MovieLens. The statistics of datasets are shown in Table 1."}, {"title": "6.2 Baseline", "content": "On the Kuaishou dataset, we choose two widely used baselines MMOE [16] and SIM [20] for comparison. We evaluate the performance of our method by comparing it with the following recommendation method that is integrated with MMOE and SIM:\n\u2022 BDR [39] consists of User-to-User and Author-to-Author graphs, enabling simultaneous prediction from both perspectives."}, {"title": "6.3 Evaluation Metrics", "content": "For offline evaluation on Kuiashou dataset, we use the training set Dtrain to train all methods and evaluate the performance of all methods on the test set Dtest. We report the average performance over hours. We adopt three widely adopted metrics: AUC, UAUC and GAUC [40] to evaluate the performance of different methods."}, {"title": "6.4 Overall Performance", "content": "Table 2 shows the performance of all models on the Kuaishou dataset. Note that given the large number of users and samples in Kuaishou dataset, an improvement of 0.5% in AUC, UAUC, and GAUC during offline evaluation holds significant value to bring obvious online gains for business. Table 3 presents the performance of several competitors on public Tiktok and Movielens datasets.\nFirst, our method surpasses all baselines by a significant margin on Kuaishou dataset. Our method MFQ significantly outperforms traditional live streaming recommendation models BDR and MTA in UAUC and GAUC for two main reasons. Firstly, BDR ignores the modeling of multi-modal content, while MTA lacks the connection to distinctive characteristics across various types of authors. In contrast, our MFQ successfully leverages the multi-modal content of the target live-streaming room and adopts learnable queries to extract streamer-aware content patterns. Additionally, our method GIE also outperforms the graph-based method EgoFusion which provides evidence that the metapath-guided behavior expansion process greatly enhances behavior representation and explores potential donation preferences.\nSecondly, our method exhibits generalizability to a common behavior-based model. Our method has seamlessly integrated into two widely used behavior-based methods, MMoE and SIM, both of which demonstrate significant performance improvements. Moreover, MMBee is not limited to these two behavior-based models and can be easily adapted to other methods such as DIN [42] and DIEN [41] as well.\nThirdly, our method is not restricted to gifting prediction tasks and it also proves effectiveness in multi-modal recommendation tasks. As shown in Table 3, our method exhibits great improvement when compared to several strong multi-modal recommendation baselines. This gain mainly comes from two folds: (1) The metapath-guided neighbors in our method enable better capture of user preferences, but other graph-based methods only rely on implicit learning from graph embeddings. (2) The MFQ module enhances the fusion of multi-modal features from short videos and clusters different videos with learnable queries initialized with item embedding, thereby benefiting further performance improvement of the recommendation model."}, {"title": "6.5 Ablation Study", "content": "Graph-level Ablation: In order to investigate the importance of different metapath neighbors and the effect of graph embedding training, we remove five expanded sequences in turn and evaluate the performance of ablated graph embedding features. The results are presented in Table 7, where we use (-) to represent the removed part or feature. For example, h u2a2u (-) means removing the metapath neighbors N(2) (ut) in recommendation model, (-) denotes removing the learned graph node embedding layers but remaining the expanded sequence and hg (\u2212) represents removing all features of graph modeling. From table 7, we can observe that hg(-) drops -0.1100% of AUC and (\u2212) also leads to a significant drop in performance which means that the GIE modeling is a very important supplement to the observed history behaviors. This suggests that the explicit metapath-based behavior expansion process and implicit graph node embedding learning are all beneficial to model's performance. Furthermore, among five expanded behavior sequences, we observed the metapath of Pa2u2a and Pu2a2u2a are the most important sequences among them.\nMulti-modal Ablation: We also investigate the influence of the multi-modal feature in MFQ module. Specifically, hm (-) denotes removing all multi-modal content and qm (-) represents removing the learnable query and cross attention. Table 7 shows that when removing the multi-modal feature MMBee suffers significant performance drops. We further study the influence of different modalities and report the ablation results in Table 4. We find visual modality has the most important impact, causing the most performance degradation when removed. The speech and comment modality have a lesser impact factor but still show an innegligible effect on the model's overall performance."}, {"title": "6.6 Visualization Study", "content": "We conduct experiment to visualize the learnable query representations in MFQ. We randomly sample 10,000 authors and visualize these representations using t-SNE [26] in 2 dimensions, as illustrated in Figure 5. The points in this graph represent the sampled authors, and it is obvious that there are several distinct clustering centers and we mark two of them by the yellow and red boxes. To demonstrate the characteristics of each clustering center, we provide some visual frames for further explanation. We observe that authors in the yellow box tend to be chatting authors, while gaming authors tend to appear in the red box. These phenomena support our assumption that learnable query can represent distinctive characteristics of various types of authors."}, {"title": "6.7 Study of Online Response Time", "content": "We investigate the online response time when recommendation requests the KGNN server and Figure 6 (left) shows the different response time when requesting different metapath behaviors. It is obvious that the max lag can reach 8.79 ms but this is not allowed in real-world applications. So we applied the pre-request of expansion behaviors and stored it in advance (described in Section 5.4) so the online recommendation model could access the embedding server instead of walking through the graph on the fly. We evaluate the efficiency of offline storage by comparing the time cost between the baseline system and the system equipped with MMBee. The response time (in milliseconds) with millions of queries per second during Jan. 24, 2024 is presented in Figure 6 (right), where the yellow and green lines represent the response time of the baseline system and MMBee. Empirical evidence shows that the response time of MMBee is only about 1 ms more than that of the baseline system on average, which is brought by the extra expanded graph behavior retrieving and computational overhead of inference."}, {"title": "6.8 Online Result", "content": "To evaluate the online performance of MMBee, we conduct strict online A/B tests on Kuaishou's business scenarios of live streaming main page spanning from 2023/10/05 to 2023/10/09 and we compare the performance of MMBee and SIM with 1% main traffic for experiments. Note that MMBee integrates our proposed MFQ and GIE into SIM backbone. We use NGU (Number of users who sent gifts) and NGC (the total number of gifts sent) as main online metrics. Online evaluation shows that MMBee has achieved 2.862% on NGU and 4.775% lift on NGC metric, which indicates that MMBee achieves much better recommendation results and brings considerable revenue increments for the platform."}, {"title": "7 Conclusion", "content": "In this paper, we propose a novel real-time multi-modal fusion and behavior expansion model called MMBee for live streaming gifting prediction. The model efficiently leverages real-time multi-modal features and effectively exploits metapath-guided expanded behaviors to enhance the performance of GTR prediction. We address two important challenges in live streaming gifting prediction, namely the multi-modal modeling and behavior sparsity, by introducing the Multi-modal Query Fusion (MFQ) and Graph-guided Interest Expansion (GIE) modules. Extensive experiments on real-world datasets demonstrate the excellent performance of MMBee."}]}