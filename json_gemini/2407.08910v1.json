{"title": "PAIL: Performance based Adversarial Imitation Learning Engine for Carbon Neutral Optimization", "authors": ["Yuyang Ye", "Lu-An Tang", "Haoyu Wang", "Runlong Yu", "Wenchao Yu", "Erhu He", "Haifeng Chen", "Hui Xiong"], "abstract": "Achieving carbon neutrality within industrial operations has become increasingly imperative for sustainable development. It is both a significant challenge and a key opportunity for operational optimization in industry 4.0. In recent years, Deep Reinforcement Learning (DRL) based methods offer promising enhancements for sequential optimization processes and can be used for reducing carbon emissions. However, existing DRL methods need a pre-defined reward function to assess the impact of each action on the final sustainable development goals (SDG). In many real applications, such a reward function cannot be given in advance. To address the problem, this study proposes a Performance based Adversarial Imitation Learning (PAIL) engine. It is a novel method to acquire optimal operational policies for carbon neutrality without any pre-defined action rewards. Specifically, PAIL employs a Transformer-based policy generator to encode historical information and predict following actions within a multi-dimensional space. The entire action sequence will be iteratively updated by an environmental simulator. Then PAIL uses a discriminator to minimize the discrepancy between generated sequences and real-world samples of high SDG. In parallel, a Q-learning framework based performance estimator is designed to estimate the impact of each action on SDG. Based on these estimations, PAIL refines generated policies with the rewards from both discriminator and performance estimator. PAIL is evaluated on multiple real-world application cases and datasets. The experiment results demonstrate the effectiveness of PAIL comparing to other state-of-the-art baselines. In addition, PAIL offers meaningful interpretability for the optimization in carbon neutrality.", "sections": [{"title": "1\nIntroduction", "content": "In industry 4.0 era, the imperative of achieving carbon neutrality\nis closely intertwined with the pursuit of economic efficiency [7].\nThe evolving carbon credit systems, designed to regulate and mon-\netize carbon emissions, have constituted a significant component of\noverall economic measurement [13]. The operational optimization\ntasks of industry 4.0 are required to achieve the dual objectives of\neconomic efficiency and environmental sustainability [22]. This\nshift towards sustainable practices emphasizes not only the impor-\ntance of advanced automation but also the technical innovation\nfor carbon friendly operations. For example, many industry plants\nhave deployed numerous sensors to collect environmental data like\ntemperature and pressure. Based on the streaming data, the existing\noperating systems can automatically conduct actions to maximize\nthe production efficiency. Such systems are required to balance the\ngoals of product maximization and carbon emission minimization\nfor sustainable development goals [23]. Therefore, designing and\ndeveloping a new operational system capable of making effective de-\ncisions to balance both economic and environmental objectives and\nachieve the sustainable development goals (SDG), have emerged as\na major concern across many industrial applications.\nUnfortunately, to the best of our knowledge, there is no solution\nfor SDG optimization in industry systems yet, partially due to the\nfollowing challenges.\n\u2022 Historical Dependency: The challenge of optimizing Sus-\ntainable Development Goals (SDG) in industrial systems is\nmarkedly accentuated by the historical dependency of ac-\ntion rewards. In these settings, the impact of actions are\ndeeply intertwined with past operations, complicating the\nreward estimation process. Traditional Deep Reinforcement\nLearning (DRL) [16, 25] methods, which rely on predefined\nKey Performance Indicators (KPIs) for rewards, struggle in\nthe context of carbon emissions related SDGs. The SDG ob-\njectives, unlike straightforward economic metrics, cannot\nbe easily quantified on a step-by-step basis but are rather\nassessed cumulatively after the whole operational sequences.\nThis historical interdependency complicates the identifica-\ntion of individual action rewards, challenging accurate re-\nward estimation and optimization efforts.\n\u2022 Complex Action Space: The optimization of SDGs involves\nnavigating a complex, multi-dimensional continuous action\nspace, characterized by the combination of various action\ntypes and operational values. This complexity is heightened\nby the dynamic nature of the impacts each action may have,\nfurther influenced by preceding actions within this contin-\nuous space. Accurately estimating the rewards for actions\nin such a nuanced and variable environment poses a sig-\nnificant challenge, necessitating sophisticated optimization\nalgorithms that can adeptly manage the intricacies of action\nspaces in the industrial systems.\n\u2022 Sequence Diversity: In SDG optimization within multi-\ndimensional continuous action spaces, rapid development\nof imitation learning (IL) [17] allows for learning from high-\nperformance policies without predefined reward functions.\nHowever, this approach struggles with sequence diversity\nand the vague definition of high performance in industrial\nsettings. The scarcity of high-performance examples and the"}, {"title": "2\nPreliminary", "content": "2.1\nProblem Definition\nIn industrial systems, measurements that are instantly readable can\nserve as states, alongside actionable adjustments. These states and\nactions enable us to optimize towards Sustainable Development\nGoals (SDGs), focusing on achieving carbon neutrality. Our objec-\ntive is to enhance SDG through strategic actions on the system.\nThe primary objective of proposed solution is to optimize the\nactions performed in an industrial system and achieve the highest\nSDG. During the model learning phase, we should estimate the\nimprovement of SDG by taking the learned action as a reward\nsignal. On the other hand, we use the improvement of SDG as a\nperformance indicator during the evaluation phase. To facilitate this\nprocess, we also need to learn a function capable of predicting the\nSGD value over time. Consequently, we divide the task of carbon\nneutral optimization into two sub-problems as follows.\nDEFINITION 1 (PROBLEM DEFINITION). Given a set of industrial\noperation trajectories denoted by T = {\u03c41, T2, ..., \u03c4\u03b7}, where each\ntrajectory ti is a sequence of actions and states in the industry sys-\ntem, associated with a corresponding Sustainable Development Goal\n(SDG) yi. The objective is twofold: (1) To predict the final SDG for a\ngiven complete trajectory, represented as \u0177 = f(t). (2) Let tk be the\ntimestamp of starting inference, for each t > tk, to learn an optimal\npolicy \u03c0\u03c1(at | St, ht) that infers action at based on the current state\nst and historical context ht, aiming to reach the highest SDG.\nWithout loss of generality, we assumes all trajectories have a\nuniform length T. In the pre-processing step, the system can align\nthe trajectories of different lengths by adding dummy time windows\nin the end. In real applications, we usually leave the first 20-30% time\nas \"only monitoring\". After the timestamp of \"starting inference\",\nthe system uses all the historical data and current state to generate\nthe action to be carried out in current timestamp. By iteratively\nobtaining the successor state from probability distribution, the\nsystem can produce an action sequence for all the remaining time."}, {"title": "2.2 Imitation Learning", "content": "Generative Adversarial Imitation Learning (GAIL) is a method that\nintegrates both behavior cloning and inverse reinforcement learn-\ning using a Generative Adversarial Networks (GAN) [12] based\nstructure, which aims to minimize the Jensen-Shannon divergence\nbetween the expert policy TE and the generated policy \u03c0\u03c1.\n$\\text{DJS}(\\pi_E||\\pi_\\theta) = \\frac{1}{2} D_{KL}\\left(\\pi_E||\\frac{\\pi_E + \\pi_\\theta}{2}\\right) + \\frac{1}{2} D_{KL}\\left(\\pi_\\theta||\\frac{\\pi_E + \\pi_\\theta}{2}\\right)$ (1)\nThis model consists of two key components: a discriminator D\nand a policy generator \u03c0\u03c1. The generator aims to generate actions\nthat mimic the expert behavior, while the discriminator aims to\ndistinguish between the agent actions and the expert actions. For-\nmally, the discriminator seeks to solve the optimization problem as\nfollows,\n$\\max _{D \\epsilon (0,1)^{S \\times A}} E_{\\pi_E} [\\log D(s, a)] + E_{\\pi_\\theta} [\\log(1 \u2013 D(s, a))]$, (2)\nwhere Te is the expert's policy, \u03c0 is the policy of the agent, and\n(s, a) are state-action pairs. In contrast, the generator aims to fool\nthe discriminator by producing actions that are indistinguishable\nfrom those of the expert, achieved by solving the following problem,\n$\\min _{\\pi_\\theta} E_{\\pi} [\\log(1 \u2013 D(s, a))] + \\lambda H(\\pi)$, (3)\nwhere H(\u03c0) is the entropy of the policy, and A \u2265 0 is a coefficient\nthat encourages exploration by the agent. By alternating between\ntraining the discriminator and the generator, GAIL learns a policy\n\u03c0 that is able to mimic the expert's behavior. Following the idea of\nimitation learning (IL), this work delves into the industrial context\nof carbon neutrality, proposing a novel algorithm designed to tackle\nthe specific challenges previously outlined."}, {"title": "3 Methodology", "content": "In this section, we provide the technical details of proposed PAIL so-\nlution. As shown in Figure 1, PAIL has three major modules, namely\nPolicy Generator, Environment Simulator, and Policy Optimization.\n3.1 Policy Generator\nIn many industrial contexts, the decision-making process is signifi-\ncantly influenced by historical states and actions. Take oil extraction\nas an example, the prior action, such as shutting off a gas valve, can\nsignificantly shape the probabilities of subsequent actions on the\nsame equipment (e.g., shutting it off again or initiating a lift). In light\nof such critically important temporal dependencies, we design an\nadapted Transformer architecture to generate the action sequences\nwith discerning temporal correlations in the trajectory, as shown in\nFigure 2. The multi-head self-attention architecture enables simulta-\nneous processing of multiple trajectories. It is particularly beneficial\nfor the capture of subtle and long-range inter-dependencies in the\ntrajectory. This attribute is essential for precise modeling along tem-\nporal dimension that context and historical trends are paramount.\nFurthermore, given the task of forecasting new action sequences\nwith historic trajectories, the self-attention mechanism exhibits\nsuperiority to dynamical adjusting focused segments of the input."}, {"title": "3.2\nEnvironment Simulator", "content": "The decision-making process often requires a precise prediction of\nthe state evolutions following conducted actions in the trajectory.\nIn pursuit of modeling this predictive framework, we employ the\nVariational Auto-Encoder (VAE) [19], a deep generative model that"}, {"title": "3.3 Policy Optimization", "content": "The major training objective of PAIL is to make the recommended\nactions close to the ones from trajectories of high SGD. Meanwhile,\nthe system should make the value of each action as large as possible\nto improve the generalization ability. To this end, we design a dual\npolicy optimization strategy consisting of two components, namely\nthe discriminator and performance estimator.\n3.3.1 Discriminator of PAIL. PAIL draws inspiration from the foun-\ndational principles of Generative Adversarial Networks (GAN) [12]"}, {"title": "4 Experiments", "content": "In this section, we introduce the details of our experiments con-\nducted on two industry datasets of carbon neutral optimization.\n4.1\nExperimental Settings\n4.1.1 Dataset. In this study, we conducted an empirical analysis\nusing two distinct datasets. The first dataset was collected from\nthe gas production industry. The dataset consists of 1,100 produc-\ntion processes, each process is with 30 steps (i.e., time window to\ncarry out actions). The sensors record key parameters like system\ntemperature and air pressure. From a broader set of metrics, we\nselectively filtered the dataset to include the most informative 51\nparameters, which collectively define the state space. Additionally,\nthe dataset delineates the action space available at each decision\npoint, encompassing five distinct types of actions. The final Sus-\ntainable Development Goal (SDG) is measured by considering both\neconomic efficiency and carbon credits. Each data sample has only\none SDG at the end to evaluate the overall process.\nThe second dataset was sourced from a supply chain system,\ncomprising 500 transaction sequences. Each sequence is divided\ninto 30 time windows. The data captures purchase actions by 5\nfactories aimed at meeting buyer demands, including details on\nthe acquired volume, average cost, and CO2 emissions for each\ntransaction. Accordingly, the buying behavior of each factory is\nidentified as the primary action within this multi-dimensional space.\nTo complement this, 15 additional critical indicators have been se-\nlected and incorporated to construct the state space. Consequently,\nan SDG indicator is also included to assess the entire transaction\nsequence, taking into account both the transaction amount and\ncarbon credits.\nFor both datasets, we selected the sequences of top 10% as high-\nperformance trajectories and used all of them to train the model.\nFor the remainder data, we partitioned them at a 4:1 training/test\nratio for the experiment.\n4.1.2 Metrics. To evaluate the proposed model, we used a variety\nof metrics. Firstly, we employed a pre-trained trajectory value esti-\nmator for predicting the improvement ratio of SDG compared to\nthe benchmark, utilizing off-policy evaluation to gauge the value of\nlearned policy without real interactions. Specifically, we employed\nthe Doubly Robust Off-policy Value Evaluation for an unbiased\npolicy value estimation, merging direct method estimation with im-\nportance sampling for greater accuracy. Consistent with previous\nimitation learning studies, we measured the discrepancy between\ngenerated and optimal policies using KL divergence, and assessed\npolicy diversity through the complement cosine similarity of gen-\nerated action sequences. Moreover, we used the F1-score to assess\nimprovements in terms of individual trajectories by setting a thresh-\nold that classified the top 50% as positive samples and the remainder\nas negative before updating the policy, in order to track transitions\nfrom negative to positive statuses while monitoring the retention\nof positive samples."}, {"title": "4.2 Performance Analysis", "content": "Figure 3 and 4 show the performance of PAIL and baselines eval-\nuated by the four metrics on both datasets. From the results, we\nhave the following observations: (1) DQN demonstrated limited\neffectiveness in environments with sparse reward signals, high-\nlighting a critical area for learning policy with developing imitation\nlearning based methods. (2) Behavioral Cloning (BC) suffered across\nall metrics due to poor generalization caused by compounding er-\nrors and heavy reliance on high-quality expert data, which cannot\nbe available in our problem definition. (3) While classical GAIL\nand its variants showed reasonable performance on metrics like\nKL divergence, their overall effectiveness was compromised in en-\nvironments requiring nuanced action choices, likely due to poor\ngeneralization, which suggests a critical need for strategies that\nextend beyond mere replication of observed behaviors in the con-\ntext of carbon neutrality. (4) Among previous imitation learning\nmethods, AggreVaTeD distinguished itself by utilizing historical\ninformation to enhance learning outcomes, and UID-GAIL also\nshowed promising results by adaptively utilizing imperfect demon-\nstrations to learn policy, suggesting the value of historical context\nand auxiliary guidance in industrial operation optimization. (5) Fi-\nnally, PAIL outperformed existing benchmarks across all metrics,\ndemonstrating superior aggregate and individual performance, as\nwell as enhanced generalization capabilities to unseen sequences."}, {"title": "4.3 Ablation Study", "content": "To evaluate the contributions of various components in the PAIL\nframework, we developed several variants of PAIL. Their descrip-\ntions and modifications are listed as follows:"}, {"title": "4.4 Parameters Tuning", "content": "4.4.1 Impact of ts. In order to assess the robustness of PAIL, here\nwe evaluated our model under different starting point ts. As Fig-\nure 6 shows, PAIL exhibited relatively poor performance on both\ndatasets under conditions where the threshold ts was either large\nor small. This trend could be attributed to inadequate learning from\nlimited historical data and sequence features at lower ts values, and\nrestricted update capability at higher ts values.\n4.4.2 Impact of A. Subsequently, we examined the influence of\nparameters A to evaluate the importance of two objectives for pol-\nicy refinement. As Figure 7 shows, the performance significantly\ndeclined when the lambda parameter was either too large or too\nsmall, demonstrating the effectiveness of our designed discrimi-\nnator module and performance estimator module in aiding policy\ngeneration. When A was too small, despite the TD learning module\nutilizing information from the discriminator, relying solely on the\nlearned Q-function proved to be insufficiently robust. Furthermore,\nthe performance degradation observed with a large lambda fur-\nther indicated that an approach based solely on imitation learning\nprinciples leaded to generalization issues.\n4.4.3 Impact of l.: Figure 8 depicts the investigate on the impact\nof the lookback size l on model performance when considering the\nhistorical information. Initially, as the lookback length I increased,"}, {"title": "4.5 Case Study", "content": "In this section, we present a case study where we identified several\ntrajectories with significant SGD improvements. As Figure 9 shows,\nwe created a heatmap based on the importance of distinct time steps\nto the final SGD. A segment of this heatmap is highlighted, where\ndarker colors indicate greater importance of a given time step. Sub-\nsequently, we quantified the similarity between the updated action\nvector at each step and the original action vector. Our observation\nrevealed that areas with darker colors corresponded to lower simi-\nlarity scores, indicating more substantial changes in actions. This\npattern further substantiated that our method could effectively alter\nsequences in a manner that enhances the corresponding SGD per-\nformance, highlighting its potential to optimize learning processes\nthrough strategic modifications in action sequences."}, {"title": "5 Related Work", "content": "5.1 KDD for Social Good\nRecent advances in machine learning and data mining have sig-\nnificantly contributed to various social good domains, including\nhealthcare [21, 42, 56], smart cities [51, 52], and financial services [8,\n24, 49, 50]. Key studies [15, 33, 39] have underscored Al's critical\nrole in enhancing decision-making across socioeconomic spheres,\nparticularly at the nexus of economic efficiency and environmen-\ntal sustainability. Research, notably [23], has leveraged advanced\nmachine learning to link economic growth with energy consump-\ntion, aligning with the United Nations Sustainable Development\nGoals (SDGs) and highlighting the contribution of AI to sustainable\ndevelopment. A broad spectrum of AI-driven research focuses on op-\ntimizing energy consumption through intelligent devices [10], user\nbehavior modeling [23, 46-48], and smart home technologies [37].\nExtending beyond energy, this body of work explores water con-\nservation and system optimization, alongside enhancements in\nelectric vehicle fuel efficiency [40, 44] and strategic charging sta-\ntion placement [38, 53]. Our study applies imitation learning to\nimprove industrial processes towards carbon neutrality, providing\nnovel AI insights for social good in line with the SDGs, showcasing\nthe dual benefits of energy conservation and economic advantage\nfor a sustainable future.\n5.2 Imitation Learning\nRecent advances in deep reinforcement learning (DRL) have sig-\nnificantly propelled efforts to optimize sequential decision-making\nacross various fields, including autonomous vehicles [20, 32], treat-\nment recommendations [21, 30, 57], and economics [18, 45], ex-\ntending to industry operation system optimization [16, 25]. The\nprimary aim of DRL research has been to develop a learning policy\ndriven by specific rewards. Yet, crafting such reward signals has of-\nten been challenging due to the necessity for complex calculations,\nespecially in long-term optimization scenarios where immediate\nindicators are insufficient.\nThus, Imitation Learning (IL) has emerged as a vital subset of\nreinforcement learning (RL), emphasizing learning from expert poli-\ncies to derive reward signals. This approach has been applied in\ndiverse areas from video games [9] to autonomous driving [3, 5] and\nrobotics [4, 36]. IL includes Behavioral Cloning (BC), which maps\nobservations to actions through supervised learning [29], and In-\nverse Reinforcement Learning (IRL), which infers experts' implicit\nreward functions to grasp their decision-making process [1, 28].\nAlthough BC has suffered from compounding errors [31] and IRL\nfrom computational complexities, Generative Adversarial Imitation\nLearning (GAIL) has combined their strengths to match the learner's\nstate-action distributions with those of the expert, mitigating their\nindividual drawbacks [17, 55]. Subsequent GAIL variants, such as\nAGAIL [34], which learns from incomplete demonstrations using\npartial actions, and Triple-GAIL [9], which introduces skill selec-\ntion and multi-modal imitation, have shown enhanced adaptability\nin complex settings. Nonetheless, IL's dependence on high-quality\ndemonstrations and its sensitivity to distributional shifts, where\npolicies may face unseen states, presents notable challenges. These\nlimitations have highlighted concerns over IL's generalization ca-\npabilities, underscoring the difficulty in ensuring optimal policy"}, {"title": "6\nConclusion", "content": "In this study, we address the critical challenge of achieving carbon\nneutrality in industrial operations, presenting a novel Performance-\nbased Adversarial Imitation Learning (PAIL) engine. By leveraging\nImitation Learning techniques, we design a Transformer based\npolicy generator to produce operational policies without relying\non pre-defined action rewards. Then, through dual reward mech-\nanism refinement with utilizing an adapted discriminator and a\nQ-learning based performance estimator, PAIL updates policies\naligned with the optimal sustainable development goals (SDG).\nExtensive experiments across various real-world scenarios demon-\nstrate the effectiveness of PAIL compared to other state-of-the-art\nmethods, offering both enhanced performance and interpretability\nin achieving carbon neutrality."}]}