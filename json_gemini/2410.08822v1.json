{"title": "SOLD: REINFORCEMENT LEARNING WITH SLOT OBJECT-CENTRIC LATENT DYNAMICS", "authors": ["Malte Mosbach", "Jan Niklas Ewertz", "Angel Villar-Corrales", "Sven Behnke"], "abstract": "Learning a latent dynamics model provides a task-agnostic representation of an agent's understanding of its environment. Leveraging this knowledge for model-based reinforcement learning holds the potential to improve sample efficiency over model-free methods by learning inside imagined rollouts. Furthermore, because the latent space serves as input to behavior models, the informative representations learned by the world model facilitate efficient learning of desired skills. Most existing methods rely on holistic representations of the environment's state. In contrast, humans reason about objects and their interactions, forecasting how actions will affect specific parts of their surroundings. Inspired by this, we propose Slot-Attention for Object-centric Latent Dynamics (SOLD), a novel algorithm that learns object-centric dynamics models in an unsupervised manner from pixel inputs. We demonstrate that the structured latent space not only improves model interpretability but also provides a valuable input space for behavior models to reason over. Our results show that SOLD outperforms DreamerV3, a state-of-the-art model-based RL algorithm, across a range of benchmark robotic environments that evaluate for both relational reasoning and low-level manipulation capabilities. Videos are available at https://slot-latent-dynamics.github.io/.", "sections": [{"title": "INTRODUCTION", "content": "Advances in reinforcement learning (RL) have showcased the ability to learn sophisticated control strategies through interaction, achieving superhuman performance in domains ranging from board games (Silver et al., 2016) to drone racing (Kaufmann et al., 2023). While these approaches excel in settings where explicit models of the environment are available or abundant data can be collected, learning complex control tasks in a sample-efficient manner remains a significant challenge. Model-based RL (MBRL) has emerged as a promising approach to address this limitation by constructing models of the environment's dynamics. Notably, the Dreamer framework (Hafner et al., 2020; 2021; 2023) has demonstrated improved sample efficiency over model-free baselines by learning behaviors solely through imagined rollouts.\nWhile these research efforts have produced world models capable of accurately predicting the dy-namics of visual tasks, they rely on a holistic representation of the environment's state. In contrast, humans perceive the world by parsing scenes into individual objects (Spelke, 1990), anticipating how their actions will influence specific components of their surroundings. This ability is crucial in complex tasks that require reasoning about multiple objects and their interactions, which is common in robotic manipulation. Learning structured representations of an environment through objects in-troduces a powerful inductive bias. The learned representations not only enhance the interpretability of the dynamics prediction but also foster decision-making by providing a structured input space for behavior models to reason over. Despite these advantages, the integration of object-centric repre-sentations and world models remains largely underexplored. To the best of our knowledge, no prior work has introduced a method that performs object-centric model-based RL directly from pixels."}, {"title": "BACKGROUND", "content": "Slot Attention for Video (SAVi) SOLD employs SAVi (Kipf et al., 2022), an encoder-decoder architecture with a structured bottleneck composed of N permutation-equivariant object embeddings denoted as slots, in order to recursively parse a sequence of video frames 0_{0:1} = {0_0, ..., 0_\u30f6} into their object representations Z_{0:7} = {Z_0, ..., Z_7}, Z_t \u2208 R^{N\u00d7D_z}. At time t, SAVi encodes the input video frame o_t into a set of feature maps F_t \u2208 R^{L\u00d7D_h}, where L is the size of the flattened grid (i.e. L = width \u22c5 height), and uses Slot Attention (Locatello et al., 2020) to iteratively refine the"}, {"title": "SLOT-ATTENTION FOR OBJECT-CENTRIC LATENT DYNAMICS", "content": "We propose Slot-Attention for Object-centric Latent Dynamics (SOLD), a method that combines model-based RL akin to the Dreamer framework (Hafner et al., 2020; 2021; 2023) with object-centric representations. The three core components of our algorithm are: the object-centric world model, which predicts the effects of actions on the environment, the critic, which estimates the value of a given state, and the actor that selects actions to maximize this value."}, {"title": "WORLD MODEL LEARNING", "content": "World models compress an agent's experience into a predictive model that forecasts the outcomes of potential actions. By simulating rollouts within the internal model, agents can learn desired behaviors in a sample-efficient manner. When the inputs are high-dimensional images, it is helpful to learn compact state representations, enabling prediction within this latent space. This type of model, called latent dynamics model, allows for efficient prediction of many latent sequences in parallel.\nMost prior works rely on generating a single, holistic representation of the environment's state, which contrasts with findings from cognitive psychology. Humans perceive scenes as compositions of objects (Spelke, 1990) and reason about how their actions affect distinct parts of their environ-ment. Furthermore, environment dynamics can be compactly explained in terms of objects and their interactions Battaglia et al. (2016). Therefore, we propose to structure the latent space by decom-posing visual environments into their constituent parts.\nComponents To create a world model that operates on object-centric latent representations, we build on top of OCVP (Villar-Corrales et al., 2023). We begin by pretraining SAVi on a dataset of 10^6 frames from random episodes. Having a sufficiently large initial dataset is crucial for meaningful object-centric representations to emerge. We do not freeze the pretrained encoder-decoder models, allowing slots to adapt to novel configurations that do not occur during random pre-training. The sequence of object slots Z_{0:t} alongside the action commands a_{0:t} serve as inputs to our transformer-based dynamics model which predicts the slot representation of the next frame 2_{t+1}:\nEncoder:\nZ_t = e_n(o_t),\nDecoder:\no_t = d_n(Z_t),\nDynamics model:\nZ_{t+1} = p_y(Z_{0:t}, a_{0:t}), and\nReward predictor:\nr_t ~ p_s(r_t | Z_{0:t}).\nObject-centric dynamics learning For the dynamics model, we follow the sequential attention pattern proposed in Villar-Corrales et al. (2023), which disentangles relational and temporal atten-tion to decouple the processing object dynamics and interactions. During training, we provide the slot representation of S seed frames as context. We append the predictions to the context and apply this process in an autoregressive manner to predict the subsequent T frames. We do not employ teacher forcing so that the dynamics model learns to handle its own imperfect predictions. To shape the predicted representations, we reconstruct the subsequent frame \\hat{o}_{t+1} and extract the SAVi repre-sentations of the actual frame Z_{t+1} to compute the hybrid dynamics loss:\nL_{dyn}() = \\sum_{t=S}^{S+T-1} [|Z_t-en (O_t)||^2+ ||\u00f4_t - O_t|||^2].\nFor all loss terms, we specify the parameter group that is being optimized and omit stop-gradients for other models to avoid cluttering the notation.\nReward model learning The reward predictor solves a regression problem where the prediction depends on the slot representations but is not directly tied to any single slot. To address this, we"}, {"title": "BEHAVIOR LEARNING", "content": "Our strategy of using the world model for behavior learning builds upon the Dreamer framework. At the core of this method lies the process of latent imagination, visualized in Figure 2b, which trains the actor and critic networks purely on imagined trajectories predicted by the world model. Since both the actor and critic operate on the latent state, they benefit from the structured representation learned by the world model. The architecture of both models mirrors that of the reward predictor, consisting of a SAT backbone that processes the slot histories followed by an MLP head:\nActor:\na_t ~ \u03c0_\u03c1(a_\u03c4 | Z_{0:t}),\nCritic:\nR_t = E[v_\u03c6(R_t | Z_{0:t})].\nCritic learning To account for rewards beyond the imagination horizon T = 15, the critic is trained to estimate the expected return under the current actor's behavior. Since no ground truth is available for these estimates, we compute bootstrapped A-returns (Sutton & Barto, 2018), R_\u03bb, via temporal difference learning. These returns integrate predicted rewards \u00ee and values \u00ce_\u0125 to form the target for the value model:\nR = ft+1+\u03b3((1\u2212\u03bb)Rt+1+\u03bbR1t+1) where R = RT,\nwhich is trained to minimize the resulting loss:\nL_{critic}(\u03c6) = -\\sum_{t=0}^{T-1} log \u03c5_\u03c6(R | Z_{0:t}).\nWe decouple the gradient scale from value prediction through same approach as in the reward model, predicting a categorical distribution over exponentially spaced bins. To stabilize learning, we regu-larize the critic's predictions towards the outputs of an exponentially moving average (EMA) of its own parameters (Mnih et al., 2015; Hafner et al., 2023).\nActor learning The actor is optimized to select actions that maximize its expected return while encouraging exploration through an entropy regularizer. Its model architecture is similar to the critic and reward predictor, but instead of regressing a scalar value, it predicts the parameters of the action distribution. Specifically, the MLP head outputs the mean \u00b5_t and standard deviation \u03c3_\u03c4 to parameterize a normal distribution N(\u03bc_\u03c4, \u03c3_\u03c4|z0:t) over possible actions. The trade-off in the actor's loss function weights expected returns with maintaining randomness in the actor outputs and"}, {"title": "RESULTS", "content": "We evaluate SOLD on a suite of robotic manipulation tasks, designed to test for both complex relational reasoning as well as low-level manipulation capabilities. Further, we apply our method to environments that are not designed as object-centric tasks. Specifically, we test on two environments from Meta-World (Yu et al., 2019) and DM-Control (Tassa et al., 2018), respectively.\nBaselines We design the experiments to achieve two main objectives: first, to specifically assess the impact of the object-centric paradigm in our method by comparing it to a baseline that replaces the object-centric encoder-decoder modules with a standard convolutional architecture (Ours w/o OCE); and second, to evaluate our approach against the best available competitor from the literature by benchmarking it against DreamerV3 (Hafner et al., 2023), a state-of-the-art MBRL algorithm known for its strong performance across a wide range of tasks. Here, we choose the 12 million pa-rameter version, to match the parameter count of our own model. Further details about the baselines are provided in Appendix C.\nEnvironments We introduce a suite of eight object-centric robotic control environments designed to test both relational reasoning and manipulation capabilities. These environments feature two"}, {"title": "OBJECT-CENTRIC DYNAMICS LEARNING", "content": "The object-centric representations learned by SAVi can be seen in the context-frames of Figure 3. The slots effectively decompose the visual scene, with most slots representing distinct objects and three slots capturing different parts of the respective robots. This part-whole segmentation demon-strates that the slots can meaningfully identify separate parts of a larger object, representing the gripper jaws in the first example and different parts of the kinematic chain of the Sawyer robot in the second. Notably, the sharp mask predictions show that each slot isolates information about the specific object it represents (see also Section E). This property is crucial for object-centric behavior learning, as it enables subsequent components to reason about task-relevant objects while ignoring irrelevant information.\nFurther, the open-loop predictions visualized in Figure 3 demonstrate that the object decomposition learned by the SAVi model is preserved throughout the prediction sequence. On the PickAndPlace-Distinct task, the movement of the robot and the blocks is predicted with high accuracy, showcasing the model's ability to capture complex physical interactions. Furthermore, the model effectively handles occlusions, as evidenced by the continued precise prediction of the spherical red target. In the second example, the slots capture the intricate shape of the hammer and nail-box. The predictions remain reliable over a long horizon, even during interactions between the robot and the hammer, and between the hammer and nail."}, {"title": "BEHAVIOR LEARNING", "content": "To assess SOLD's performance across our suite of robotic control tasks, we train each method three times with different random seeds on each environment. The final success rate achieved by each method is shown in Figure 4. SOLD consistently outperforms the non-object-centric baseline, often"}, {"title": "CONCLUSION", "content": "We present SOLD, an object-centric model-based RL algorithm that learns directly from pixel inputs. By employing structured latent representations through slot-based dynamics models, our method offers a compelling alternative to traditional, holistic approaches. While object-centric rep-resentations have been valued for their role in forward prediction (Villar-Corrales et al., 2023), we demonstrate their synergistic benefits in accelerating the learning of behavior models. SOLD achieved strong performance across visual robotics environments, significantly outperforming the state-of-the-art DreamerV3, particularly in tasks requiring relational reasoning. Additionally, the learned behavior models exhibit interpretable attention patterns, explicitly focusing on task-relevant parts of the visual scene."}, {"title": "Limitations & Future Work", "content": "One limitation of our world model is that it generates predictions in a deterministic manner. This can be a drawback in environments that are inherently stochastic or highly unpredictable. We believe this is a key reason why SOLD outperforms DreamerV3 on complex robotic manipulation tasks but struggles to match its performance on simpler tasks like Cartpole-Balance, where minor variations in action sequences can lead to vastly different outcomes over long horizons. Addressing this limitation by incorporating stochasticity into the prediction model presents a promising direction for future work. A second limitation arises from the object-centric encoder-decoder model we use. While SAVi performs well on the tasks we evaluated, scaling it to complex real-world data remains a significant challenge. However, the core ideas of our method are independent of the specific object-centric encoder-decoder model, and future work can easily integrate more advanced models that address these scalability concerns."}, {"title": "NOTATION", "content": "Dz\nThe slot dimension\nN\nThe number of slots\nZt\nA set of slots Zt \u2208 RN\u00d7Dz at time-step t\n20:t\nA history of slot-sets up to time-step t\ne\u03c8\nA SAVi encoder that maps ot to Zt\nd\u03c8\nA SAVi decoder that reconstructs ot from Zt\nFt\nFeatures obtained by encoding images\nL\nNumber of spatial locations in F\nReinforcement Learning\nS\nThe number of seed frames\nT\nThe imagination horizon\not\nAn image observation\nat\nAn action command\nrt\nA reward\n\u03b3\nA scalar discount factor\nH\nThe entropy of a probability distribution\nfMLP\nAn MLP head that belongs to parameter group \u03b1\n\u03b1\nA processed output token\nht"}, {"title": "IMPLEMENTATION DETAILS", "content": "In this section, we describe the network architecture and training details for each of the SOLD com-ponents. Our models are implemented in PyTorch (Paszke et al., 2019), have 12 million learnable parameters, and are trained on a single NVIDIA A-100 GPU with 40GB of VRAM. A summary of the model implementation details is listed in Table 1."}, {"title": "SLOT ATTENTION FOR VIDEO", "content": "We closely follow Kipf et al. (2022) for the implementation of the Slot Attention for Video (SAVi) decomposition model, including their proposed CNN-based encoder ey and decoder dy, the transformer-based predictor and the Slot Attention corrector. We employ between 2 and 10 (depending on the environment) 128-dimensional object slots, whose initialization is learned via backpropagation. We empirically verified that learning the initial slots performs more stable than the usual random initialization. Furthermore, we use three Slot Attention iterations for the first video frame in order to obtain a good initial decomposition, and a single iteration for subsequent frames, which is enough to update the slot state given the observed features."}, {"title": "OBJECT-CENTRIC DYNAMICS MODEL", "content": "Our object-centric dynamics model is based on the OCVP-Seq (Villar-Corrales et al., 2023) archi-tecture, which is a transformer encoder employing sequential and relational attention mechanisms in order to decouple the processing of temporal dynamics and interactions, and has been shown to achieve interpretable and temporally consistent predictions. We use 4 transformer layers employing 256-dimensional tokens, 8 attention heads, and using a hidden dimension of 512 in the feed-forward layers."}, {"title": "SLOT AGGREGATION TRANSFORMER", "content": "The Slot Aggregation Transformer (SAT) forms the architectural backbone for the reward, value and action models. This module aggregates information from object slots across multiple time steps to produce output tokens that are subsequently fed to MLP heads in order to predict rewards, values, or actions. An overview of our SAT module is depicted in Figure 8.\nSAT is a causal transformer encoder module that receives as input a history of object slots, as well as a learnable output token [out] for each time step, which is responsible for producing the final output for the corresponding time step. Additionally, we append to the SAT inputs a number of reg-ister tokens [reg] per time-step, which have been shown to aid with processing in attention-based models by offloading intermediate computations from the output tokens and helping the module focus on relevant slots (Darcet et al., 2024).\nTo encode the positional information into SAT, we employ Attention with Linear Biases (Press et al., 2022) (ALiBi), which introduces linear biases directly into the attention scores, effectively encoding token recency. This approach helps the model deal with sequences of varying length, as well as gen-eralize to longer sequences than those seen during training, thus outperforming absolute positional encodings.\nFor our experiments, SAT is implemented with 4 transformer encoder layers with causal self-attention, RMS-Normalization layers (Zhang & Sennrich, 2019), 8 attention heads, a token dimen-sion of 256, and a hidden dimensionality in the feed-forward layers of 512. We set the number of learnable register token per time step to 4. Furthermore, we enforce in our causal attention masks that tokens belonging to time step t cannot directly interact with previous output and register tokens."}, {"title": "TRAINING DETAILS", "content": "SAVi Pretraining SAVi is pretrained for object-centric decomposition on approximately one mil-lion frames for 400,000 gradient steps. We use the Adam optimizer (Kingma & Ba, 2015), a batch size of 64 and a base learning rate of 10^\u20134, which is first linearly warmed-up during the first 2,500 training steps, followed by cosine annealing (Loshchilov & Hutter, 2017) for the remaining of the training procedure. We perform gradient clipping with a maximum norm of 0.05.\nSOLD Training SOLD is trained using the Adam optimizer (Kingma & Ba, 2015) and different learning rates for each component: 10^{-4} for the dynamics and rewards models, and 3 \u22c5 10^{-5} for training the action and value models, as well as for fine-tuning the SAVi encoder. To stabilize training, we perform gradient clipping with maximum norm of 0.05 for the SAVi model, 3.0 for the transition model, and 10.0 for the reward, value, and action models. For all components, we also use learning rate warmup for the first 2,500 gradient steps. Additionally, we implement the exponential moving average (EMA) for the target value network with a decay rate of 0.98. We use an imagination horizon of 15 steps for behavior learning, and the \u03bb-parameter is set to 0.95."}, {"title": "BASELINES", "content": "In our experiments we compare our approach with two different baseline models, namely the SoTA model-based RL baseline DreamerV3 and a Non-Object-Centric variant of our proposed model:"}, {"title": "ENVIRONMENTS", "content": "In this section, we provide further details about our proposed suite of environments, which includes eight object-centric robotic control tasks designed to test relational reasoning and manipulation ca-"}, {"title": "ADDITIONAL RESULTS", "content": "Figure 9 depicts the object-centric decomposition of a video frame obtained by SAVi. SAVi parses the input frame into per-object RGB reconstructions and alpha masks, which can be combined via a weighted sum in order to accurately reconstruct the observed video frame. Notably, SAVi assigns an object slot to the scene background, five slots to different blocks, one slot to the red target, and one slot to the robot arm. The sharp object masks demonstrate that SAVi isolates object-specific information in each slot, which is beneficial for downstream applications such as behavior learning, allowing the agent to reason about object properties and their relationships while abstracting task-irrelevant details.\nWe visualize action-conditional open-loop predictions on the Push Specific (Figure 10), Button-Press (Figure 11), Hammer (Figure 12), Cartpole-Balance (Figure 13), and Finger-Spin (Figure 14) environments. More precisely, we depict the ground truth sequence, the predicted video frames, the predicted instance segmentation of the scene, obtained by assigning a distinct color to each object mask, and the object reconstruction for each slot.\nIn all examples, our model parses the scene into sharp and accurate object representations, and models the action-conditional object dynamics and interactions in order to accurately predict the future video frames while preserving object-centric representations. We emphasize SOLD's ability to capture complex physical interactions, such as pushing a block to a target location (10), pressing a button (11) or hitting a nail with a hammer (12). Furthermore, we showcase that SOLD can generalize to diverse non-object-centric environments (13 and 14)."}]}