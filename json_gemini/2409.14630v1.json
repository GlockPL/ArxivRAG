{"title": "EQ-CBM: A Probabilistic Concept Bottleneck with Energy-based Models and Quantized Vectors", "authors": ["Sangwon Kim", "Dasom Ahn", "Byoung Chul Ko", "In-su Jang", "Kwang-Ju Kim"], "abstract": "The demand for reliable AI systems has intensified the need for interpretable deep neural networks. Concept bottleneck models (CBMs) have gained attention as an effective approach by leveraging human-understandable concepts to enhance interpretability. However, existing CBMs face challenges due to deterministic concept encoding and reliance on inconsistent concepts, leading to inaccuracies. We propose EQ-CBM, a novel framework that enhances CBMs through probabilistic concept encoding using energy-based models (EBMs) with quantized concept activation vectors (qCAVs). EQ-CBM effectively captures uncertainties, thereby improving prediction reliability and accuracy. By employing qCAVs, our method selects homogeneous vectors during concept encoding, enabling more decisive task performance and facilitating higher levels of human intervention. Empirical results using benchmark datasets demonstrate that our approach outperforms the state-of-the-art in both concept and task accuracy.", "sections": [{"title": "1 Introduction", "content": "With the increasing demand for reliable AI, explaining deep neural networks (DNNs) has gained significant attention across various research fields. For decades, post-hoc explanation methods [2, 23, 30, 33, 35] have been the mainstay due to their clarity and compatibility. However, these methods fall short in fully interpreting black-box DNNs as they provide explanations detached from the intrinsic decision-making processes [19, 24] of models. Recently, concept-based model interpretation has emerged as a promising alternative. Kim et al. [15] define concepts as discriminative vectors necessary for a model to understand objects, presented as orthogonal vectors known as concept activation vectors (CAVs). Building on this, concept bottleneck models (CBMs) [3, 6, 12, 17, 18, 20, 32, 34, 40, 42] have been proposed to explain the decision-making processes"}, {"title": "2 Backgrounds", "content": ""}, {"title": "2.1 Concept Bottleneck Models", "content": "CBMs [3,6,12,17,18,20,32,34,40,42] were designed to enhance the interpretability of DNNs by leveraging human-understandable concepts. These models transform raw input images into a set of intermediate concept representations, which are then used to make final tasks without relying on additional image features. To explain CBMs in detail, we first define the notations. A dataset $D$ consists of $N$ triplets, each containing an image $x$, a ground truth concept label set $C^*$, and a ground truth class label $y^*$: $D = (x, C^*, y^*)$. The ground truth concept label set $C^*$ includes concept labels for $K$ individual concepts, $C^* \\in {0,1}^K$.\nCBMs typically comprises three main components:\n1. Backbone network $f : x \\rightarrow z$: This network extracts a latent vector $z$ from the input image $x$.\n2. Concept encoder $g: z \\rightarrow C$: This encoder maps the latent vector $z$ to a set of concepts $C$.\n3. Downstream layer $h: C \\rightarrow y$: This layer predicts the final class $y$ using only the predicted concepts $C$.\nTo maintain structural transparency in our model, we employ a lightweight backbone network, ResNet34 [13], for $f$, and a single fully-connected layer for $h$. This configuration ensures that the model's decision-making process remains interpretable and transparent. By using these interpretable concepts for final classification tasks, CBMs inherently promote transparency and clarity in model predictions. This transparency allows for human intervention to correct model failures or misalignments between concepts and the final task."}, {"title": "2.2 Energy-based Models", "content": "EBMs [4, 5, 7-9, 11, 16, 26, 29, 41, 43] are probabilistic frameworks designed to represent complex distributions. The core principle of EBMs is to associate an energy score with each possible state of variables, where lower energy scores correspond to more probable states. This approach provides a flexible and expressive means to capture the underlying dependencies within the data."}, {"title": "3 EQ-CBM", "content": "We introduce EQ-CBM, a framework that enhances CBMs by utilizing probabilistic concept encoding with EBMs with qCAVs. Our approach addresses the limitations of previous CBMs by improving robustness and interpretability in complex decision-making tasks. As depicted in Fig. 2, the proposed method comprises three main components: qCAVs (Q), probabilistic concept encoders ($g_\\theta$), and a sampling module.\nIn this section, we describe each component and its integration into the overall framework, highlighting the advantages of using energy-based modeling and vector quantization for concept representation."}, {"title": "3.1 Quantized Concept Activation Vectors", "content": "Vector quantization is a powerful technique widely used to discretize continuous vectors into a finite set of representative vectors, known as a codebook. Notable applications of vector quantization include VQ-VAE [36], which avoids posterior collapse and enables high-quality generative modeling. By mapping continuous data to discrete codebook vectors, vector quantization captures the underlying structure of the data by aligning representations with a finite set of learned codebook vectors. This alignment is particularly beneficial for interpretability and robustness, as it simplifies the analysis and manipulation of the model's internal representations."}, {"title": "3.2 Probabilistic Concept Encoder", "content": "The probabilistic concept encoders $g_\\theta$ abstract each concept by applying variational inference techniques to extract a variational latent vector $v_k$ from a normal distribution. Specifically, as depicted in Fig. 2, from $z$, we infer the mean $\\mu_\\alpha$ and variance $\\sigma_\\beta$, and sample noise $\\epsilon$ from Gaussian distribution to learn diverse representations. This process enables the model to capture the variability and complexity of real-world concepts more effectively.\nOur energy function $E_\\theta$ then models the joint distribution between the variational latent vector $v_k$ and the qCAVs. Instead of updating qCAVs through backpropagation gradients, we use an exponential moving average (EMA) to"}, {"title": "3.3 Energy-based Concept Encoding", "content": "To effectively capture the relationships between variational latent vectors $v_k$ and qCAVs, we employ an energy-based approach for concept encoding. EBMs offer a robust and flexible framework for modeling complex dependencies and probabilistic relationships. By utilizing an energy function $E_\\theta$, we measure the compatibility between $v_k$ and predefined qCAVs, enabling a more comprehensible and interpretable representation of concepts.\nThe core idea behind energy-based concept encoding is to define a joint energy-concept landscape where low energy scores indicate high compatibility between $v_k$ and the qCAVs. This approach allows the model to probabilistically infer concept activations given $v_k$, thereby effectively capturing the variability and complexity of real-world scenarios. As shown in Fig. 3, we integrate EBMs into the concept encoders, which operates on the variational latent vector $v_k$. This energy function returns a low value when $v_k$ is closely related to the $q_k$, facilitating the selection of the most relevant concept vectors for the final task. The problem we aim to solve is therefore represented as the joint distribution of $c_k$, $v_k$, and $q_k$ as follows:\n$\\log P_\\theta (c_k, v_k, q_k) = \\log P_\\theta (v_k, q_k) + \\log P_N (c_k| v_k, q_k),$"}, {"title": "4 Experiments", "content": "To evaluate our model, we used four datasets:\nCUB [38], CelebA [22], AwA2 [39], and TravelingBirds [20]. The CUB dataset includes 12K bird images across 200 species, with 6K for training and 6K for testing, each labeled with 312 attributes. Following previous works [6, 17, 20], we used 112 attributes as concepts. CelebA contains 202K facial images of 10K celebrities, each with 40 attributes, from which we used six attributes as concepts. The AwA2 dataset has 37K images of 50 animal species, annotated with 85 attributes.\nThe TravelingBirds dataset, derived from the CUB dataset [38], replaces image backgrounds with diverse scenes [44] to test model robustness in real-world uncertainties. All images were resized to 299\u00d7299 pixels across datasets. All experiments were conducted on a system with an AMD 5955WX CPU and an Nvidia A6000 GPU using five random seeds. Detailed hyperparameters are depicted in Table 1."}, {"title": "4.1 Metrics", "content": "To comprehensively evaluate the performance, we utilized several key metrics: Concept Accuracy, Task Accuracy, and Uncertainty. Each metric provides insight into different aspects of model performance, from the precision of concept predictions to the robustness of the model under uncertain conditions."}, {"title": "4.2 Concept and Task accuracy", "content": "To evaluate the effectiveness of our proposed model, we compared its performance against several CBMs using three datasets: CUB, CelebA, and AwA2. We measured both concept prediction accuracy and task (classification) accuracy. \nAs shown in Table 2, our proposed EQ-CBM outperforms existing approaches in both concept and task accuracy across all three datasets. For the CUB dataset, EQ-CBM achieved a concept accuracy of 96.580% and a task accuracy of 79.310%. In comparison, the best-performing baseline, ECBM, achieved a concept accuracy of 96.536% but a lower task accuracy of 77.148%. Similarly, for the CelebA dataset, our model attained concept and task accuracies of 90.617% and 56.600%, respectively, significantly surpassing the best-performing baseline, CEM, which had a task accuracy of 42.618%. In the AwA2 dataset, EQ-CBM reached concept and task accuracies of 99.129% and 95.965%, respectively. The closest competitor, Coop-CBM, achieved 98.875% in concept accuracy and 95.927% in task accuracy.\nThese results demonstrate that our model not only maintains high concept accuracy but also achieves superior task performance, validating the effectiveness of incorporating EBMs with qCAVs. Notably, our approach consistently"}, {"title": "4.3 Concept Intervention", "content": "To further evaluate the robustness and interpretability of our model, we conducted a series of concept intervention experiments. The goal of these experiments is to assess how human intervention in correcting concept predictions influences the overall task accuracy. As illustrated in Fig. 4, we compared the performance of our model (EQ-CBM) with several methods, including CBM, CEM, and ECBM, under varying levels of intervention. The intervention ratio on the x-axis represents the proportion of corrected concept predictions, while the y-axis shows the corresponding task accuracy.\nFor the CUB dataset, as the intervention ratio increased, EQ-CBM consistently outperformed other models, achieving nearly 100% task accuracy at higher intervention ratios. This demonstrates the effectiveness of our model in leveraging homogeneous concepts from qCAVs to enhance human intervention. Similarly, in the CelebA dataset, our model showed significant improvements in task accuracy with increasing intervention ratios, outperforming all baseline methods. Notably, EQ-CBM achieved a task accuracy of 100% with full intervention, highlighting its robustness and capacity to incorporate human feedback effectively. In the AwA2 dataset, EQ-CBM again outperformed the baseline methods across all intervention levels. With an intervention ratio of 1.0, our model reached a task accuracy of 100%, indicating its superior ability to utilize qCAVs for improved the level of human intervention."}, {"title": "4.4 Visualization of Encoded Concepts", "content": "To assess how well the encoded concepts capture the underlying data structure, we utilized t-SNE to visualize the concepts produced by each model just before the final task linear classifier. Figure 5 shows the t-SNE plots of these concepts for CEM [6], ECBM [40], and our proposed EQ-CBM on the CUB dataset.\nIn Fig. 5a, the encoded concepts produced by CEM exhibited significant overlap among different concepts, indicating weak separation and potential confusion in concept interpretation. This overlap suggests that the model might struggle to differentiate between certain concepts, leading to less accurate and interpretable predictions. In Fig. 5b, while ECBM achieves better separation than CEM, there is still considerable clustering within certain concept groups. This partial overlap could impact the model's ability to clearly distinguish between similar concepts, potentially affecting its task performance and interpretability. In contrast, our proposed EQ-CBM, as depicted in Fig. 5c, demonstrated a much clearer separation of concepts. The t-SNE plot reveals distinct clusters for each concept, indicating that EQ-CBM effectively captures the unique characteristics of each concept and represents them in a well-separated latent space. Specifically, these concepts are sampled qCAVs, which provide a homogeneous representation of concepts by incorporating variability and uncertainty. This clear separation enhances the model's interpretability and allows for more accurate concept-based predictions."}, {"title": "4.5 Uncertainty Robustness", "content": "To evaluate the robustness of our proposed model under uncertain conditions, we conducted experiments using the TravelingBirds dataset, a variant of the"}, {"title": "4.6 Ablation Study", "content": "We performed an ablation study to evaluate the impact of different components of our model on both concept and task accuracy. The results are shown in Ta-ble 3. Without EMA, the model's concept accuracy dropped to 92.081% and task accuracy significantly decreased to 23.745%. This indicates that EMA is crucial for stabilizing and ensuring consistent convergence of qCAVs. Removing the energy-based modeling component led to a concept accuracy of 95.454%, demonstrating that energy-based modeling is essential for capturing nuanced relationships between concepts, resulting in higher accuracy. Training without variational inference showed a slight decrease in task accuracy to 78.681%, with concept accuracy at 96.518%. This suggests that variational inference helps in"}, {"title": "4.7 Concept Interpretation", "content": "To demonstrate the interpretability of our model, we visualized several concepts and their corresponding scores and uncertainties on the CUB dataset, as shown in Fig. 8. Each image depicts a bird species with its corresponding concepts, predicted concept scores $c_k$, and uncertainties $u_k$.\nFor the California Gull, the model incorrectly identifies the belly_color due to shadows, resulting in high uncertainty. The upper_tail_color also shows high uncertainty because it is occluded. The eye_color is correctly identified but with high uncertainty due to its multicolored nature. For the Heermann Gull, the eye_color and bill_color are correctly identified but have high uncertainty because of the small size of the eyes and the multicolored bill. In the Acadian Flycatcher image, the nape_color prediction has some uncertainty due to a shadow that makes the nape appear gray. Although the predicted concept is incorrect, the uncertainty score indicates that the model is unsure about this prediction. Similarly, the eye_color prediction also shows high uncertainty due to the shadow effect. For the Vermilion Flycatcher, the model predicts the"}, {"title": "5 Conclusion", "content": "In this paper, we introduced EQ-CBM, a novel framework designed to enhance CBMs through probabilistic concept encoding using EBMs with qCAVs. Our approach addressed the limitations of deterministic concept encoding in existing CBMs by enabling robust probabilistic inference, thereby improving both concept and task accuracy. By integrating EBMs, our framework effectively captured the underlying dependencies and uncertainties within the data, leading to more accurate and interpretable models. qCAVs ensured the selection of homogeneous vectors during concept encoding, enhancing both task performance and human intervention capabilities. Experiments on datasets such as CUB-200-2011, AwA2, CelebA, and TravelingBirds demonstrated the superiority of EQ-CBM in achieving a better balance between interpretability and accuracy compared to existing CBMs. The TravelingBirds dataset further showcased the robustness of our model under challenging conditions. Our ablation studies underscored the importance of each component within our framework.\nFuture work will focus on improving the scalability of our approach to efficiently handle larger datasets, providing a robust and flexible framework for enhancing the decision-making processes of DNNs."}, {"title": "A Model Complexity", "content": "In this section, we present a detailed comparison of the model complexity for various concept bottleneck models (CBMs), including our proposed EQ-CBM.\nTable A1 shows the number of parameters, FLOPs, and inference latency for each method. Additionally, it highlights the concept accuracy and task accuracy. These metrics emphasize the computational efficiency and performance of our approach relative to existing methods."}, {"title": "B Nearest Neighbor Concept Analysis", "content": "To further validate the interpretability and robustness of our proposed EQ-CBM model, we conducted a nearest neighbor (NN) concept analysis. This experiment visualized the top-5 NNs for selected concept embeddings, allowing us to assess how well the model captured and represented these concepts across different images. Figure A1 illustrates the NN results for four selected concepts. For each concept, the query image (highlighted with a red border) is shown on the left, followed by its top-5 NN images identified by the model."}, {"title": "C Energy Distribution Analysis", "content": "We present an analysis of the energy distributions generated by EBMs in our probabilistic concept encoders for various bird concepts. Figure A2 illustrates the energy ($\\overline{e}_k$) distributions for selected bird concepts across three different datasets: CUB, CUB_Black, and CUB_Random. The CUB dataset [38] is the original data used for training, while CUB_Black and CUB_Random are subcategories of the TravelingBirds dataset [20] used solely for inference, where the backgrounds were replaced with black and random real-world scenes [44], respectively."}]}