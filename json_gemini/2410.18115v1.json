{"title": "Point Cloud Compression with Bits-back Coding", "authors": ["Nguyen Quang Hieu", "Minh Nguyen", "Dinh Thai Hoang", "Diep N. Nguyen", "Eryk Dutkiewicz"], "abstract": "This paper introduces a novel lossless compression method for compressing geometric attributes of point cloud data with bits-back coding. Our method specializes in using a deep learning-based probabilistic model to estimate the Shannon's entropy of the point cloud information, i.e., geometric attributes of the 3D floating points. Once the entropy of the point cloud dataset is estimated with a convolutional variational autoencoder (CVAE), we use the learned CVAE model to compress the geometric attributes of the point clouds with the bits-back coding technique. The novelty of our method with bits-back coding specializes in utilizing the learned latent variable model of the CVAE to compress the point cloud data. By using bits-back coding, we can capture the potential correlation between the data points, such as similar spatial features like shapes and scattering regions, into the lower-dimensional latent space to further reduce the compression ratio. The main insight of our method is that we can achieve a competitive compression ratio as conventional deep learning-based approaches, while significantly reducing the overhead cost of storage and/or communicating the compression codec, making our approach more applicable in practical scenarios. Throughout comprehensive evaluations, we found that the cost for the overhead is significantly small, compared to the reduction of the compression ratio when compressing large point cloud datasets. Experiment results show that our proposed approach can achieve a compression ratio of 1.56 bit-per-point on average, which is significantly lower than the baseline approach such as Google's Draco with a compression ratio of 1.83 bit-per-point.", "sections": [{"title": "I. INTRODUCTION", "content": "Point cloud offers a flexible and visually rich repre-sentation of 3D data, with the ability to capture detailed spatial information. This makes point cloud invaluable in a wide range of applications, including automotive LiDAR for autonomous driving, 3D scene understanding in robotics, and immersive experiences in virtual reality and augmented reality. Despite the flexibility and usefulness of the point cloud, the unstructured format of the point cloud poses significant challenges in data storage and often requires efficient data compression techniques [1]. For example, a single sweep of LiDAR sensors in autonomous driving can produce 100,000 points, resulting in 84 billion points per day [2]. Moreover, a single point in the point cloud may require 32 or 64 bits to represent the geometric attributes, i.e., (x, y, z) position in a 3D coordinate, let alone the color attributes, resulting in a massive amount of data storage. However, the unordered nature of point clouds presents a significant challenge for efficient compression. Consequently, point cloud compression has emerged as a critical area of research, focusing on transforming raw point cloud data into a more structured format that can be more efficiently compressed [1]. A typical point cloud compression process can be illus-trated in Fig. 1, beginning with \"spatial processing\", which converts raw point cloud data into ordered formats such as tree-based structures or voxel grids. The spatial processing step is often a lossy process and is critical as it facilitates the subsequent transformation of the data into a binary sequence. Once organized in the tree-based or voxel format, the data can be processed by a lossless compression codec, which involves an important step of entropy estimation. The entropy estimation step is critical to quantify the uncertainty or randomness in the data, typically using statistical tools like histograms or machine learning models to approximate Shannon's entropy [2]\u2013[5]. The lossless compression codec then utilizes the estimated entropy, typically conditioned on the marginal probability of the data, to produce the final binary sequence, as depicted in Fig. 1. The most widely used lossless compression codecs are arithmetic coding [6] and the recently developed asymmetric numeral systems (ANS) [7], thanks to their implementation simplicity and low compression overhead. The main idea of these codecs is to build the cumulative distribution functions (CDFs) and the inverse CDFs from the learned marginal distribution of data, denoted as P(x) in Fig. 1. Once the learned distribu-tion functions are shared between the encoder (sender) and decoder (receiver), data samples are mapped into a binary sequence by transforming the value P(x) into an interval between 0 and 1 [7], [8]. Accurate entropy estimation is crucial, as it directly im-pacts the efficiency of lossless compression codecs like arithmetic coding and ANS. While considerable research works have focused on enhancing entropy estimation through advanced deep learning models, e.g., [3]\u2013[5] and references therein, the resulting reduction in compression ratios (defined"}, {"title": "II. SYSTEM MODEL AND PROPOSED METHODOLOGY", "content": "Recall that the main building blocks of a point cloud compression pipeline is illustrated in Fig. 1 and our main focus in this work is the entropy estimation step and lossless compression step. For the spatial processing step, we employ a simple voxelization processing, in which the entire point cloud is divided into equal-sized 3D cubes called \"voxels\". A voxel can be considered as a counterpart of \"pixel\u201d in 2D image/video processing. At the end of the spatial processing step, a raw point cloud is transformed into M = $2^d \\times 2^d \\times 2^d$ voxels in which d is the axis-wise depth value of the processed point cloud. For example, with d = 7, we use 7 bits per dimension to represent the $2^7$ = 128 positions of the points in a 3D coordinate. The resulting M voxels now are well structured and can be used for the entropy estimation step. Note that further complicated spatial processing, such as density embedding [5] and building Octrees [2] or KD-trees [12], can be applied for more efficient data presentation but it's not our main focus of this work as we will later show that with simplistic voxelization step described above, we can still outperform other tree-based approaches in terms of compression ratio."}, {"title": "B. Entropy Estimation with CVAE Model", "content": "In Fig. 2, we illustrate our approach to compress the processed point clouds, i.e., voxels, with bits-back cod-ing. As illustrated in Fig. 2(a), we develop our entropy estimation model for M voxels based on a convolutional variational autoencoder (CVAE). Note that the \"CVAE'S encoder\" and \"CVAE's decoder\" terms in Fig. 2 should not be confused with the encoder-decoder terms used for the arithmetic coding and ANS codecs. We will use the terms \"CVAE's encoder\u201d and \u201cCVAE's decoder\" to explicitly refer to the components of the CVAE model, thus avoiding potential confusion with general encoder-decoder terms used for lossless compression codecs like arithmetic codec and ANS. It is also noted that, in this work, we only consider compressing the geometric attributes of the point clouds, i.e., the positional values of the floating points in the 3D coordinate, and discard the color information of the points to focus extensively on demonstrating the bits-back coding capabilities. We formulate the lossless compression problem of M voxels as follows. Given an alphabet A (i.e., codebook) of voxels having value either 1 or 0, i.e., A = {0,1}, a processed point cloud having M voxels can be represented as a symbol x = [$x_1$,$x_2$,...,$x_M$] (see footnote\u00b9 below), in which each element $x_m$ (1 < m < M) receives value either 0 or 1, i.e., $x_m$ \u2208 {0,1}. In the context of entropy estimation, let's define the real-valued probability of the variable $x_m$ receiving value i (i = 0, 1) as P($x_m$ = i), where $\\sum_{i=0}^{i=1}$P($x_m$ = i) = 1. We have the Shannon information content of an outcome $x_m$ is h($x_m$ = i) = $\\log{\\frac{1}{P(x_m=i)}}$, and the binary entropy function of $x_m$ is\n    \n    H($x_m$) = $\\sum_{i=0}^{i=1} P(x_m = i) \\log{\\frac{1}{P(x_m=i)}}$  (1)\n    \n    Note that from the above equation and hereafter we use log to denote the base two logarithm (i.e., usually denoted as $log_2$) for notational simplicity. For compression of M variable $x_m$, where each $x_m$ is an element of the voxelized point cloud x, the entropy of \u00e6 can be calculated by [8, Equation 8.1]:\n\n    H(x) = $\\sum_{x_m \\in A} P(x_1,x_2,...,x_M) \\log{\\frac{1}{P(x_1, x_2,..., x_M)}}$ (2)\n    \nIn practice, one often requires probabilistic models to estimate the exact marginal probability P(x) = P($x_1$,$x_2$,..., $x_M$) of the vector x = [$x_1$,$x_2$,...,$x_M$]. Let's denote $P_\\theta$($x_m$ = i|$x_1$,...,$x_{m-1}$) being the estimated con-ditional probability with parameters \u03b8 to the true conditional probability P($x_m$ = i|$x_1$,..., $x_{m\u22121}$). Once the conditional probabilities are estimated, the approximate marginal proba-bility $P_\\theta$(x) can be calculated using the chain rule, i.e.,\n\n      $P_\\theta(x) = \\prod_{m=1}^{M} P_\\theta(x_m | x_1, x_2, ..., x_{m-1})$ (3)\n\nAs the conditional probabilities are now can be approx-imated with potential estimation errors, the code length of the compressed symbol \u00e6 is greater than the optimal code length, i.e., the entropy H(x) defined in (2). In particular, the average code length of the lossless compressed sequence x with a probabilistic model $\\theta$ can be calculated by [8, Equation 5.23]:\n\n      L(x) = H(x) + $\\sum_{x_m \\in A} P(x) \\log{\\frac{P(x)}{P_\\theta(x)}}$   (4)\n\nIn the above equation, the sum $\\sum_{x_m \\in A} P(x) \\log{\\frac{P(x)}{P_\\theta(x)}}$ is a Kullback-Leibler (KL) divergence, which is a non-negative value describing the relative distance between the\nNote that the symbol \u00e6 can be viewed as a flattened array of voxels for illustration purpose only. In actual implementation, a 3-dimensional array can be utilized for preserving the spatial structure of the point cloud data."}, {"title": "C. Bits-back Coding for Voxelized Point Cloud Data", "content": "The bits-back coding method was first described by Frey and Hinton in [9], and was further developed by Townsend et al. in [10]. The main idea behind bits-back coding is that the encoder (sender) and decoder (receiver) of a given codec, e.g., arithmetic coding [6] or asymmetric numeral system (ANS) [7], will share a latent variable model P(z) (a prior), instead of the marginal probability $P_\\theta$(x). The marginal probability $P_\\theta$(x) then can be inferred by using Bayes' update rule with the prior P(z), the posterior $Q_\\phi$(z | x), and the likelihood $P_\\theta$(x | z), which will be described later. The use of a latent variable model is useful to exploit the \"correlation\", or \"side information\", between the data samples when compressing a batch of such data samples together. The correlation between data samples will gain advantages, in terms of compression ratio, when compressing large batches of data, which is usually the case of com-pressing data with deep learning-based probabilistic models. From the optimal coding rate principle through minimizing the KL divergence in (4), we formalize the bits-back coding approach as follows. Starting from equation (4), we now introduce a latent variable z which has a multivariate normal (Gaussian) prior P(z) = N(z; 0, I), where I is the identity matrix. As we now trying to infer true probability P(x) from our voxel data with a latent variable model, the marginal probability $P_\\theta$(x) can be written as the total probability of observing x under all possible values of the latent variable z, i.e.,\n  $P_\\theta(x) = \\int_{z} P_\\theta(x, z)dz$.    (5)"}, {"title": "III. PERFORMANCE EVALUATION", "content": "We first describe two point cloud datasets that we use to validate our bits-back coding approach, followed by the compression process as illustrated in Fig. 1. The point cloud datasets are the ShapeNet dataset [14] and SUN RGB-D dataset [15]. The ShapeNet dataset contains various 3D ob-ject models in different categories such as tables, cars, chairs, and airplanes. The SUN RGB-D dataset contains various raw point cloud data resulting from the reconstruction of RGB-D images, in which each RGB-D image is a pair of an RGB image and a depth image of an indoor scene. We create synthetic point clouds from these above datasets as follows. From the 3D objects in the ShapeNet dataset, we sample uniformly 20,000 points on the surface of the 3D mesh objects in the dataset. For the RGB-D dataset, we randomly sample 20,000 points from the existing raw point clouds. In other words, each point cloud in our synthetic dataset contains 20,000 points in a 3D coordinator. We use 32 bits of floating-point format (float32) to demonstrate each geometric attribute for each point within the cloud. The final synthetic point cloud dataset is rescaled and normalized within the interval [-1.0, 1.0]. This rescaling and normalization process follows standard practice in common deep learning-based approaches in [3], [4]. The information about the datasets is summarized in Table I. After having the synthetic point cloud datasets, we can perform spatial processing with simple voxelization as fol-"}, {"title": "B. Experiment Results", "content": "We visualize the main processing steps on point cloud datasets in Fig. 4. For illustration purposes, we take one data sample, i.e., a point cloud of an airplane, from the ShapeNet's test set and one data sample, i.e., a point cloud of a bedroom scene, from the SUN RGB-D's test set. In the second column, we observe that the higher value of bit-depth d results in a better voxel representation of the point cloud. In the third column, we visualize the reconstruction of the voxel data at the output of the CVAE model. The results show that the 3D shape of the objects and scenes are well preserved, but details such as the tail of the airplane and the structure of the bedroom scene are not well recovered. The reason is that the CVAE is optimized based on the variational lower bound of the training data. This means the CVAE will have better generation capabilities, i.e., preserving the 3D shape of the objects and scenes, but lack the fine details of some spatial regions. Note that the blurry voxel reconstruction of the CVAE only affects the compression ratio of the voxel data and since we are using a lossless compression method, the decompressed voxel representation is identical to the original voxel representation, as shown in the third and fifth columns of Fig. 4. This observation suggests that one can potentially develop a more complex entropy estimation model, i.e., building more complex deep neural networks, to gain the 3D spatial details at the third column in Fig. 4, similar to approaches in [3] and [4]. However, building more complex entropy models is not our main focus and we will let this as a potential research direction, as we can always build better models on top of our framework."}, {"title": "2) Impacts of compressing large point cloud dataset:", "content": "Next, we evaluate the performance of the proposed bits-back coding scheme by varying the number of point clouds to be compressed in Fig. 5. We use two other approaches that are \"sequential coding\" and \"Draco\" as baselines for comparison. Sequential coding, as described in Section II-B, is the technique to sequentially compress the new data sample into an existing compressed message. This technique assumes that both the encoder and decoder can access the approximate probability $P_\\theta$(x). This can be done by sharing the deep neural network's parameters for the encoder and decoder. After that, the decoder can sequentially decode the received message by using the pre-trained neural network to produce all conditional probabilities, thus estimating the final marginal probability $P_\\theta$(x) using the chain rule in (3) [3], [4]. The second baseline is Draco, a compression library developed by Google that utilizes a tree-based entropy estimation [12]. In particular, after the voxelization step, Draco then builds a KD-tree (k-dimensional tree) for remov-ing empty voxels and creating hierarchical regions (nodes), providing a quick way to estimate the entropy of the point cloud across different regions and scales. In Fig. 5, we show that by increasing the number of point clouds to be compressed, the compression ratios (measured in average bit-per-point) of our bits-back coding approach and the sequential coding approach decrease quickly, mean-ing that we need a lower number of bits to compress the en-"}, {"title": "3) Impacts of bit-depth:", "content": "As described above, the sequen-tial coding approach [3], [4] can achieve a near-optimal com-pression ratio because the encoder and decoder have access to the approximate probability model in each compression iteration. In the following, we will analyze the drawbacks of this approach in practical scenarios and emphasize the advantages of our proposed bits-back coding approach. In Fig. 6, we evaluate the impacts of the bit-depth parameter d on the system performance in terms of compression ratio. In particular, as we increase the bit-depth value from d = 5 to d = 7, the compression ratio increases as shown in Fig. 6(a). The reason for this trend is the bit-depth parameter indicates the resolution of the voxel representation of the point cloud, as discussed in Section III-B.1. As the resolution (i.e., the number of voxels) increases, the compression codec will need to assign a higher number of bits to encode the corresponding point cloud. Note that the compression ratio on the ShapeNet dataset (left figure of Fig. 6(a)) is higher than that on the SUN RGB-D dataset (right figure). The difference in compression ratio is caused by the geometric patterns, or the difference in entropy, of the point cloud datasets. We observe that the point clouds in the SUN RGB-D dataset are more scattered around the x, y, z axes in the 3D space, while the point clouds in the ShapeNet dataset are densely distributed around the central region. The spatial patterns suggest that the uncertainty in the estimated entropy H(x) result in a difference in the average code length, as described in equation (4)."}, {"title": "IV. CONCLUSION", "content": "In this work, we have proposed a novel bits-back coding method for compressing large point cloud datasets. The novelty of the approach is based on the bits-back coding method, which utilizes the latent variable model over the model parameters to encode and decode point cloud data. By utilizing the prior, likelihood, and approximate posterior of the CVAE model during the decoding step, our approach achieves a competitive compression ratio compared to the sequential coding approach with deep neural networks. The use of the bits-back coding method enables us to achieve a comparable compression ratio to the other sequential coding with deep learning models while consuming lower memory and communication costs. Notably, the bits-back coding approach does not require access to the learned entropy model $P_\\theta$(x) during the decoding process, thus enabling lightweight and practical decoder implementation. Future research directions could be adopting advanced spatial pro-cessing techniques, such as building tree-based models, using posterior estimation with flow-based models, and building deeper neural networks for better entropy estimation."}, {"title": "APPENDIX", "content": "In particular, the CVAE's encoder consists of three Convo3D (3D convolution) layers, followed by two fully-connected layers. The main idea of using stacked Convo3D layers is to subsequently reduce the dimensions of the input data before feeding it into the fully connected layers. For high-dimensional data like our voxels, where the bit-depth value d = 7 results in dimensions of M = $2^7 \\times 2^7 \\times 2^7$, the input data expands to a flattened array with 221 features (more than 2 million). Processing such a large flattened array using a vanilla VAE with only fully-connected layers becomes impractical due to the immense computational demand. Instead, by using the Convo3D layers, the input dimension is reduced from M to 2,000 features before forwarding these 2,000 features into the fully connected layers. The parameters of each Convo3D and fully-connected layer are illustrated in detail in Fig. 2(b). In this work, our implementation of different bit-depth values with d = 5, d = 6, and d = 7, with only simple modifications of the kernel size and stride parameters of the Convo3D layers. For higher bit-depth (i.e., d > 7), it is possible to further implement such bigger and deeper models. In return, it would require large computing power and running time while does not provide further insights from our work, which extensively focuses on investigating the potential of the bits-back coding. After passing the M voxel features through the CVAE's encoder, the number of features is reduced to 500, which is used for constructing a latent space model that consists of 50 features. The addition module before the CVAE's latent space in Fig. 2(b) is to illustrate the reparameterization trick, in which the latent space is subjected to a Gaussian prior, i.e., P(z) ~ N(z; 0, I) [13]. Following the latent space layer are two fully connected layers with the same sizes as the ones of the CVAE's encoder. Finally, the last three layers are the ConvoTran3D (3D transposed convolution) layers. The ConvoTran3D layers are utilized to subsequently increase the dimensional features of the input vector, so that the final output of the CVAE model has the same dimension, i.e., M, as the input vector. We then train the CVAE for the two datasets described in Table I. The training process is done by using Adam optimizer with a learning rate of 0.001. The loss function is conventional ELBO in (8). The activation function used across the hidden layers is ReLu. The final output layer's activation function is Sigmoid as we are modeling the binarized voxel data. The Sigmoid output of the CVAE model then can be used as a parametric Bernoulli distribution to produce the voxels having values of either 0 or 1. We train the CVAE model for 500 training epochs. After training, the CVAE model can produce an estimated probability for the input voxel as shown in Fig. 4. Our details of implementation and reproducibility can be found at https://github.com/hieunq95/gpcc-bits-back."}]}