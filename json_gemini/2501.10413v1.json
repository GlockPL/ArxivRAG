{"title": "Cooperative Search and Track of Rogue Drones using Multiagent Reinforcement Learning", "authors": ["Panayiota Valianti", "Kleanthis Malialis", "Panayiotis Kolios", "Georgios Ellinas"], "abstract": "This work considers the problem of intercepting rogue drones targeting sensitive critical infrastructure facilities. While current interception technologies focus mainly on the jamming/spoofing tasks, the challenges of effectively locating and tracking rogue drones have not received adequate attention. Solving this problem and integrating with recently proposed interception techniques will enable a holistic system that can reliably detect, track, and neutralize rogue drones. Specifically, this work considers a team of pursuer UAVs that can search, detect, and track multiple rogue drones over a sensitive facility. The joint search and track problem is addressed through a novel multiagent reinforcement learning scheme to optimize the agent mobility control actions that maximize the number of rogue drones detected and tracked. The performance of the proposed system is investigated under realistic settings through extensive simulation experiments with varying number of agents demonstrating both its performance and scalability.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid technological progress in unmanned aerial vehicles (UAVs) and the surging global demand have opened up a wide range of applications for the use of UAVs in both the military and more importantly the civilian sectors [1]. However, the simplicity of operating UAVs poses significant security, and public safety risks [2]. The unauthorized or illegal use of drones over sensitive, restricted, and critical infrastructures necessitates the development of robust and precise counter-drone solutions that can be effective in intercepting rogue drones [3].\nEvidently, current interception technologies are still in their infancy, with only a handful of systems capable of effectively neutralizing rogue drones. The most popular countermeasures include the deployment of radio frequency (RF) jammers that can disrupt a drone's GNSS (localization/navigation) or RF signals (mostly used for telemetry) [4]. Nevertheless, the interference levels caused by these jamming systems pose serious side effects to the normal operation of authorized systems operating within sensitive areas [5], [6].\nMoreover, the interception task is only part of a multifaceted and complex problem of detecting, tracking, and neutralizing rogue drones, due to the underlying challenges entailed by the dynamic behavior of heterogeneous drones, accuracy and efficiency of detection systems, and flexibility and scalability of neutralization systems [7]. These challenges have already been identified in recent reports by the U.S. Homeland Security Committee, which discusses potential countermeasures and emphasizes the importance of developing interceptor UAVs with precise detection, tracking, and interception mechanisms as a holistic system that can effectively neutralize this threat [8].\nIn accordance, this work investigates the deployment of a team of pursuing UAVs (i.e., agents) over a confined area (sensitive facility) with the aim of searching, detecting, and tracking multiple rogue drones (i.e., targets). In particular, in this work we propose a multiagent reinforcement learning (RL) scheme for simultaneously detecting and tracking multiple rogue drones by a team of autonomous UAVs agents. We focus on a realistic scenario in which the rogue drones and the pursuer UAV agents move in a confined space, where each agent is equipped with a range-finding sensor that exhibits a limited sensing range for detecting the rogue drones. More specifically, the agents can detect the presence of the rogue drones only if they are inside their sensing range. In the proposed framework the UAVs aim at: (a) learning how to effectively be deployed and search their controlled area to accurately detect and track rogue drones, and (b) best choosing their mobility actions to maximize the time they have these targets in their field-of-view (FOV). The main contributions of this work are as follows:\n\u2022\tThe search-and-track problem is formulated as a Markov Decision Process (MDP), and a novel approach based on multiagent reinforcement learning is proposed. Specifically, multiple Q-learning agents cooperate, thus allowing the team of pursuer UAV agents to coordinate and optimize their mobility control actions at each time-step in order to maximize the aggregated number of targets detected over the operating time period.\n\u2022\tTwo reward functions are proposed that encourage the coordination between the learning agents to closely achieve the constraints and the objectives of the cooperative search-and-track problem.\n\u2022\tExtensive simulation experiments demonstrate the effectiveness of the proposed approach for various scenarios with changing numbers of pursuer agents.\nThe rest of the paper is organized as follows. Section II"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Rogue drone detection and tracking technologies mainly rely on radars, RF signals, computer vision, acoustic signals, and sensor fusion [9]. Existing anti-drone systems typically utilize fixed terrestrial sensors; for example, [10] describes a solution that uses radars, while [11] uses RF analysis and [12] fuses acoustic signals, computer vision, and RF signals. Unfortunately, ground-based solutions are subject to limitations, especially in harsh environments like cities, due to obstacles.\nMore recent works in [13], [14] describe a prototype passive radar system, deployed on the ground and onboard a UAV. This prototype is based on signals of opportunity and software-defined radio, for detecting-and-tracking malicious drones operating over sensitive regions.\nCooperative teams of UAV agents unlock significantly more capabilities than what is possible by a single UAV. The work in [15] proposes a dynamic radar network, composed of UAVs outfitted with radars, that is able to distributively detect-and-track rogue drones in real time. Further, the authors in [16] propose an approach based on deep reinforcement learning for tracking multiple targets using multiple UAV agents. However, the aforementioned works only focus on target tracking and no searching for targets is performed, i.e., prior information about the targets is assumed. The authors in [17], [18] coordinate multiple robots to explore an area by observing multiple static points of interest (POIs) using reinforcement learning. There is no assumption of prior information about the POIs and the coordination is achieved through a novel informative reward function.\nComplementing the state of the art, this work investigates the joint search-and-track problem (i.e., to locate and track rogue drones) that arises when multiple pursuer agents aim to intercept multiple rogue drones. As elaborated in the previous section, this important task precedes the track-and-intercept task that we have previously investigated in [19], where multiple UAV agents track-and-intercept the operation of multiple rogue drones by transmitting jamming signals to interrupt their communication links (RF) and sensing receivers (e.g. GNSS)."}, {"title": "B. Background on Reinforcement Learning", "content": "Reinforcement learning [20] is a paradigm in which an agent learns to make decisions by interacting with an environment. Through trial-and-error, the agent explores various actions and receives a numerical feedback from the environment in the form of a reward that is relative to its actions. The goal of an RL problem is to find an optimal policy, i.e., a mapping from states to actions, that maximizes the cumulative rewards over time.\nQ-learning, is a widely-used algorithm in which an optimal policy is learnt by estimating the values of state-action pairs, $Q(s, a)$, and storing them in a table, referred to as the Q-table. The update rule for Q-learning is as follows:\n$Q(s, a) \\leftarrow Q(s, a) + \\alpha[r + \\gamma \\max_{a'} Q(s', a') \u2013 Q(s, a)]$, (1)\nwhere $\\alpha$ is the learning rate and $r$ is the reward received for transitioning from state $s$ to state $s'$ by taking action $a$.\nSince many RL applications involve large or continuous state and action spaces, they usually require function approximation. Tile coding [20] is a commonly used linear state function approximator that exhaustively partitions the state space into a set of overlapping tilings and tiles.\nA cooperative multi-agent system is comprised of several decision-making agents operating in a common environment, interacting with each other to achieve a common objective [21]. This common objective is typically complex, and multi-agent RL (MARL) [22] is a promising way to address the emerging complexity. A typical reward function in MARL problems is the global reward G which represents the system's performance and motivates the agents to act in the system's interest; however, all agents receive the same reward, irrespective of whether their actions improved the system performance, leading to poor quality information (i.e., low signal-to-noise ratio). This is referred to as the credit assignment problem and can be tackled with reward shaping, which refers to a collection of methods (e.g., [17], [23], [24]) which modify (or \u201cshape\u201d) the original reward function.\nIn this work we have adopted the popular difference rewards method. Contrary to others, it has some theoretical properties, it does not assume any domain knowledge, and it is generic/domain-agnostic. A difference reward $D_j$ [17] is a shaped reward signal that quantifies each agent's individual contribution to the system's performance, by removing a large amount of the noise created by the actions of other agents active in the system, and is given by:\n$D_j(s, a) = G(s,a) \u2013 G(s_{-j}, a_{-j})$, (2)\nwhere $G(s,a)$ is the global reward and $G(s_{-j}, a_{-j})$ is the counterfactual term, i.e., the global reward for a theoretical system without the contribution of the j-th agent."}, {"title": "III. SYSTEM MODEL", "content": "In this work, a set of rogue drones (i.e., targets), denoted by $I = {1,2,..., M}$, maneuver in the environment with discrete-time dynamics. The state vector of a target $i$ at time-step $t$ is denoted by $x^i_t = [x^i_t, \\dot{x}^i_t, y^i_t, \\dot{y}^i_t] \\in R^4$ containing the position $(x^i_t,y^i_t)$ and velocity $(\\dot{x}^i_t,\\dot{y}^i_t)$ in 2D Cartesian coordinates. The targets start from a random point in the perimeter of the area and move linearly towards a POI with constant velocity magnitude."}, {"title": "B. Agent Dynamics", "content": "A set of controllable UAV agents, denoted by $J = {1, 2, ..., N}$, cooperate to counter the operation of the rogue drones. At time-step $t$, the kinematic state of agent $j$ is denoted by $s^j_t = [x^j_t, y^j_t]^T \\in R^2$, i.e., the position in 2D Cartesian coordinates, and its dynamics are given by:\n$s_{t+1}^j = s_t^j + \\begin{bmatrix} \\Delta R[l_1] cos(\\frac{2 \\pi l_2}{N_{\\Theta}}) \\\\ \\Delta R[l_1]sin(\\frac{2 \\pi l_2}{N_{\\Theta}}) \\end{bmatrix}$, $l_1 = 1, ..., |\\Delta R|$\n$l_2 = 0, ..., N_{\\Theta}$\n(3)\nwhere $\\Delta R$ is the vector of possible radial step sizes (with $\\Delta R[l_1]$ returning the value at index $l_1$), $\\Delta \\Theta = \\frac{2 \\pi}{N_{\\Theta}}$, and the parameters $(|\\Delta R|, N_{\\Theta})$ determine the number of possible mobility control actions. The mobility control action $a_{l_1,l_2}$ represents the direction and step size of the movement along the x and y axis. At each time-step $t$, each UAV agent chooses one mobility control action from the discrete set $U = {a_{l_1,l_2} | l_1 = 1, ..., |\\Delta R|, l_2 = 0, ..., N_{\\Theta}}$, as computed by Eq. (3), while agent $j$'s set of all admissible mobility controls is denoted by $U^j = {u^j_1, u^j_2, ..., u^j_{|U|}}$."}, {"title": "C. Agent Sensing Model", "content": "The UAV agents are able to detect nearby targets only if they are inside their limited FOV. More specifically, a target $i$ at state $x^i_t$ is detected by agent $j$ at state $s^j_t$, if the Euclidean distance between them at time-step $t$, $d^{ij}_t$, is below the detection radius $R_0$; i.e., if $d^{ij}_t = ||Hx^i_t - s^j_t||_2 < R_0$ holds, where $H$ is a matrix that extracts the position coordinates from a target's state vector."}, {"title": "D. Problem Formulation", "content": "The objective of the search-and-track problem tackled in this work is to maximize the aggregated number of targets detected by the agents over an episode, given by:\n$(P1) max F = \\sum_{t=1}^{T} \\sum_{i \\in I} N^i_t$ (4)\ns.t. $u^j_t \\in U^j, \\forall j \\in J \\forall t$\n(5)\nwhere $F$ denotes the team's utility function, i.e., the global system performance, which is the overall team's objective to maximize. $N^i_t$ denotes whether target $i$ is being detected by an agent at time-step $t$, and is given as:\n$N^i_t = \\begin{cases} 1, \\ if \\exists j \\in J: d^{ij}_t < R_0 \\\\ 0, \\ otherwise \\end{cases}$\n(6)\ni.e., it is equal to 1 if at least one agent $j \\in J$ detected target $i$ at time-step $t$ and 0 otherwise, and $d^{ij}_t$ is the Euclidean distance between the $i$-th target and $j$-th agent (computed as given in Section III-C). Overall, the optimization problem (P1) finds the mobility control actions $u^j_t$ for each agent $j \\in J$ and each time-step $t$, that maximize the sum of the number of targets detected over an episode. It should be noted that in order to optimally solve (P1), an assumption must be made about prior knowledge of the target trajectories, i.e., the target states $x^i_t$ for each time-step $t$."}, {"title": "IV. PROPOSED APPROACH", "content": "This section presents the RL-based approach for searching and tracking multiple targets using multiple agents. Prior knowledge about the targets, such as their locations, the number of targets, and their kinematic model is not known to the agents and the agents must coordinate to maximize the team's utility function, $F$. Figure 1 illustrates the problem tackled in this work."}, {"title": "A. Proposed Cooperative Q-learning Algorithm", "content": "In order to address the described search-and-track problem using RL, it is first formulated as an MDP [20]:\nState: The state vector of agent $j$ at time-step $t$ is de-termined by the positions of the agents and the posi-tion of each target $i$ (information about a target's posi-tion is available only if the target is within agent $j$'s FOV, as discussed in Section III-C), and is denoted by $s^j_t = [s^j, m^{j}_{i1}, ..., m^{j}_{i4}, p^{j}_{i1}, ..., p^{j}_{14}, n^{j}_{11}, ..., n^{j}_{i4}, q^{j}_{11}, ..., q^{j}_{34}]_t$.\nThe state vector of the $j$-th agent comprises five components:\n\u2022\t$s^j_t$: absolute position coordinates of agent $j$\n\u2022\t$m^{j}_{il}$: number of targets in the l-th area of agent $j$\n\u2022\t$p^{j}_{il}$: average Euclidean distance between agent $j$ and the targets in the l-th area of agent $j$\n\u2022\t$n^{j}_{il}$: number of agents in the l-th area of agent $j$\n\u2022\t$q^{j}_{il}$: average Euclidean distance between agent $j$ and the remaining agents in the l-th area of agent $j$\nwhere $l \\in {1,2,3,4}$. Essentially, at each time-step $t$ each agent $j$ splits the 2D space in four areas by assuming Cartesian axes centered at its current position coordinates and computes relative distances from the agents and targets.\nImportantly, the size of the state vector is constant and independent of the number of agents and targets and thus allows the framework to scale well with respect to the number of learning agents and targets.\nAction: At each time-step $t$, each agent $j$ selects an action $a^j_t$ from the discrete action space $A = U$, which includes choosing a mobility control as defined in Section III-B. Illegal actions are restricted, e.g., an action that moves an agent outside of the environment's area.\nReward: After observing its state and taking an action, each agent $j$ receives a scalar reward, $r^j_t$, from the environment. Various reward functions are investigated for the search-and-track problem:\nThe global reward, $G_t$, is defined as the number of individual targets the team as a whole detected at time-step $t$, given by:\n$G_t = \\sum_{i \\in I} N^i_t$, (7)\nwhere $N^i_t$ is given by Eq. (6).\nThe difference reward, $D^j_t$, for agent $j$ is computed by applying Eq. (2):\n$D^j_t = G_t - G^{(-j)}_t$, (8)\nwhere the counterfactual term, $G^{(-j)}_t$, is calculated by assuming that agent $j$ does not detect any target for time-step $t$, as given by:\n$G^{(-j)}_t = \\sum_{i \\in I} N^i_t$, $N^i_t = \\begin{cases} 1, \\ if \\exists j' \\in J, j' \\neq j: d^{ij'}_t < R_0 \\\\ 0, \\ otherwise. \\end{cases}$\n(9)\nThe RL-based training algorithm is summarized in Alg. 1. Each UAV agent is an independent learner, having its own Q-table which is initialized with zero values (line 1). For every episode, which lasts for $T$ consecutive time-steps, the environment is initialized randomly, encouraging the agents to learn various strategies via trial-and-error (line 3). Each agent selects an action according to an $\\epsilon$-greedy policy, i.e., selects an action that corresponds to the maximum Q-value in the current state with probability $1 - \\epsilon$ or to a random action with probability $\\epsilon$ (line 6). The exploration parameter $\\epsilon$ and learning rate $\\alpha$ decay over time in order to encourage learning and exploration at initial episodes and exploitation afterwards (lines 12-13). Each agent executes its action, receives the reward, and observes the new state before updating the Q-value of the current state-action pair (lines 7-9)."}, {"title": "V. PERFORMANCE EVALUATION", "content": "The UAV agents and the rogue drones operate in an area of size 50 m by 50 m. The targets are randomly initialized in the perimeter of the area where: target 1 is initialized with $x^1_1$ ranging from 0 to 50 m and $x^1_2 = 0$ m, and target 2 is initialized with $x^2_1$ ranging form 0 to 50 m and $x^2_2 = 50$ m. The agents are initially positioned in the middle of the area, i.e., at [25, 25] m. The agent's dynamical model has radial displacement $\\Delta R = [0,1,3]$ m, and $N_{\\Theta} = 4$ which gives a total of 9 control actions, including the initial position of the agent. The POI coordinates are random inside a 10 m $\\times$10 m box in the middle of the area. An agent can detect a target only if the target is within distance $R_0 = 5$ m from the agent.\nRL: The agents learn for $N_e = 10^5$ episodes, where each episode is comprised of $T = 30$ time-steps. The Q-learning parameters are set as follows: the learning rate is set to $\\alpha = 0.2$ and $alpha\\_decay\\_rate = 0.99997$, and the exploration parameter is set to $\\epsilon = 0.3$ and $epsilon\\_decay\\_rate = 0.99997$, while the discount factor is set to $\\gamma = 0.9$. For representing the state space, each agent has its own tile coding function approximator comprising $2^6$ grid tilings. The tile width in $m^{j}_{il}$ and $n^{j}_{il}$ dimensions is set to 1, while in $s^j, p^{j}_{il}$ and $q^{j}_{il}$ dimensions is set to 25 m. The values for the Q-learning and tile coding parameters are selected after performing parameter tuning.\nEvaluation: The experiments are repeated over 20 independent runs. All training curve plots show average values across 20 independent runs with a median filter applied."}, {"title": "B. Simulation Results", "content": "We begin the evaluation of the proposed approach with a comparison between the learning performance of the pro-posed reward functions (discussed in Section IV-A) when utilizing 2 and 4 agents. Figure 2 shows the global system performance normalized in the range [0,1]. As observed in Fig. 2, the learning patterns improve as the training episodes progress for both reward functions. However, the difference reward, $D_t$, outperforms the global reward, $G_t$, in terms of the learning speed, scalability, and overall performance, with its advantage being more evident in the 4-agent case, since the higher number of agents increases the noise created by the other agents active in the system, making it more difficult to learn from $G_t$. At the last training episode, $D_t$ converges to approximately 0.9 which is very close to the optimal value, i.e., 1. The optimal value, however, requires prior knowledge about the targets (as discussed in Section III-D). Even so, due to its higher performance, the experiments that follow, utilize the difference reward, $D_t$, as the reward function $r^j_t$.\nFigure 3 shows a simulated scenario of the policy learned with 4 agents and 2 targets during 3 consecutive time periods, i.e. $t = 1,..., 10 s, t = 11,..., 20 s, t = 21, ..., 30 s$.\nThe agents' positions are initialized in the middle of the area, i.e., at [25, 25] m. The initial target state vectors are [0, 0.69, 47, -0.72] and [50, -0.78,4,0.62] for targets 1 and 2 (in units m, m/s, m, and m/s, respectively). The POIs are located at [21, 25] m and [28,21] m for targets 1 and 2, respectively. The trajectories of the agents and the targets are shown in Figs. 3a - 3c, for the three time periods, where the bright green line indicates whether a target is detected. During the first time period, the 4 agents spread out towards 4 different directions to maximize the coverage of the area and reduce the overlap in their sensing range and therefore maximize the probability of detecting the targets. At the beginning of the second time period, at time-step $t = 11$ s, agents 4 and 1 detect targets 1 and 2, respectively, and continue to detect them by tracking them for the rest of the period. The remaining agents, i.e., agents 2 and 3, maneuver towards the center of the area to search for targets, since during this time period it is more likely to find a target there. During the last time period, both targets continue being tracked by the agents.\nTo further evaluate the performance of the proposed approach, search patterns are generated by running 1000"}, {"title": "VI. CONCLUSIONS", "content": "This work has examined the utilization of multiple UAV agents to cooperatively search-and-track multiple rogue drones. We have proposed an RL-based scheme, that utilizes a simple reward function which encourages the coordination between the learning agents. The learned strategies enable the agents to select mobility control actions for each agent to maximize the aggregated number of targets detected over each episode, considering the agents' limited sensing range for detecting the targets. Performance evaluation results have provided a comparison between two reward functions (global and difference rewards) and demonstrated the superiority of the difference reward function in terms of scalability, learning speed, and final joint performance.\nAs future work we plan to integrate these results with our previous track-and-jam work to provide a holistic search-track-jam system for rogue-drone interception. Moreover we aim at field-testing these algorithms in a small scale deployment with commercial-off-the-shelf drones using the onboard LiDAR ranging sensors and software-defined-radios as payloads to enable jamming. The goal is to execute the integrated search-track-jam scheme to test its real-time performance."}]}