{"title": "Multi-View Neural Differential Equations for Continuous-Time Stream Data in Long-Term Traffic Forecasting", "authors": ["Zibo Liu", "Zhe Jiang", "Shigang Chen"], "abstract": "Long-term traffic flow forecasting plays a crucial role in intelligent transportation as it allows traffic managers to adjust their decisions in advance. However, the problem is challenging due to spatio-temporal correlations and complex dynamic patterns in continuous-time stream data. Neural Differential Equations (NDEs) are among the state-of-the-art methods for learning continuous-time traffic dynamics. However, the traditional NDE models face issues in long-term traffic forecasting due to failures in capturing delayed traffic patterns, dynamic edge (location-to-location correlation) patterns, and abrupt trend patterns. To fill this gap, we propose a new NDE architecture called Multi-View Neural Differential Equations. Our model captures current states, delayed states, and trends in different state variables (views) by learning latent multiple representations within Neural Differential Equations. Extensive experiments conducted on several real-world traffic datasets demonstrate that our proposed method outperforms the state-of-the-art and achieves superior prediction accuracy for long-term forecasting and robustness with noisy or missing inputs.", "sections": [{"title": "I. INTRODUCTION", "content": "Given historical traffic flow measurements at a selected set of locations in a road network, long-term spatio-temperal traffic forecasting aims to train a deep-learning model to predict the traffic flow at those locations far ahead into the future. Consider a scenario in a road network where the long-term forecasting model could utilize one hour of recent past data to predict the traffic conditions up to eight hours ahead, where the amount of future predicted data is eight times the past measurements. Whereas the short-term forecasting model could only predict one hour of traffic data ahead, which is far less than what the long-term forecasting model did. The capability of long-term traffic forecasting can play a crucial role in intelligent transportation. For example, early warnings about future traffic conditions may help traffic planners (such as map apps), traffic signal management, and agencies of law enforcement, medical assistance and disaster response to adjust their decisions and routes [1], [2].\nExisting methods for traffic flow forecasting can be categorized into discrete-time or continuous-time methods. Discrete-time methods include recurrent neural networks (RNNs) [3]-[5], or convolutional neural networks (CNNs) with graph neural networks (GNNs) [6]\u2013[11]. These methods cannot fully capture the dynamic patterns of traffic flows in continuous time, especially for discrete traffic measurements with noise and missing values. In addition, GNNs are shown to contain the risk of over-smoothing when trained on limited data [12], [13]. Continuous-time methods are largely based on neural differential equations (NDES) [14]\u2013[17]. These methods train a neural network to capture the continuous-time traffic dynamics expressed by a differential equation [18]\u2013[21]. Specifically, a neural network is trained to fit the gradient function. However, existing NDE models do not fully capture some complex dynamic patterns such as delayed propagation of traffic conditions, dynamic spatial dependency in traffic dynnamics, and abrupt shifts in local traffic trend, which are elaborated below.\nFirst, their designs lack consideration of the complex cascading effects with delayed propagation of traffic conditions that are typical in traffic flows. For example, consider a scenario where a slow-moving vehicle on a highway prompts deceleration, which triggers a cascade effect as the following drivers also reduce speed with a delay. The time, location, and extent of this delayed pattern are unknown beforehand. Second, the spatial dependency between locations in a road network is temporally dynamic due to rush hours and accidents. How to explicitly and quantitatively capture such dependency remains a challenge. Third, there exist abrupt shifts in traffic flow patterns that are local and deviate from the current broad traffic conditions, due to certain events such as sudden road closures caused by car accidents or bad weather. This requires us to incorporate additional input on local traffic trend which is missing in the existing spatio-temporal models. The impact of considering delayed propagation, treating spatial dependency explicitly, and incorporating local traffic trend is less significant on short-term traffic forecasting, which most existing work focuses on [6], [8]\u2013[11], [18], [19], [22]\u2013[25], but is more significant on long-term traffic forecasting, which this work focuses on.\nWe propose a new deep-learning framework called Multi-View Neural Differential Equations (MNDE) for long-term spatio-temporal traffic forecasting. It uses NDEs as the backbone to capture dynamic traffic patterns in continuous time. More importantly, our framework contains separate NDE modules to capture the complex delayed propagation and dynamic spatial dependency. Specifically, our model incorporates dynamic spatial correlations by calculating intermediate correlations between locations over time as a new NDE input. Finally, we model abrupt shifts in traffic dynamics through a"}, {"title": "II. PRELIMINARIES AND PROBLEM STATEMENT", "content": [{"title": "A. Preliminaries", "content": "Definition 1: Flow Rate Measurements. A road system has a set N of chosen locations where sensors are deployed to measure traffic. Denote n = |N|. All the vehicles that pass a location form a traffic flow; a sensor at the location measures the flow rate, i.e., the number of passing vehicles at the location during each preset time interval. Let X = [Xi,j, i \u2208 N, j\u2208 {0, 1, .., l \u2013 1}]T be the flow-rate measurement matrix across all locations over I consecutive time intervals, where Xij is the flow-rate measurement at location i during the jth time interval. For simplicity, we normalize each time interval as one unit of time. Denote the I measurements at location i as Xi = [Xi,j, j \u2208 {0,1, ..,1 \u2013 1}]T. Hence, X = [X\u00bf, i \u2208N]T.\nTo facilitate the construction of neural differential equations, we need to interpolate the discrete flow-rate measurements Xi at each location i to a continuous time function Xi(t), t\u2208 [0,1 \u2013 1], with Xi(j) = Xi,j, \u2200j \u2208 {0,1, ..,1 \u2212 1}. One common method is the natural cubic spline method [16], [26] that produces a segmented curve: Xi(t) = ai,j + bi,j (t - tj) + Ci,j (t - tj)\u00b2 + di,j (t-tj)\u00b3, t \u2208 [tj, tj+1], j\u2208 {0, 1, .., 1 \u2013 2}, where ai,j, bi,j, Ci,j, di,j are interpolation coefficients. Those coefficients can be computed based on the spline interpolation condition: Xi(t = j) = Xi,j, the continuity condition: Xi(t = j + 1) = Xi,j+1, and the second derivative condition: Xi''(t = j) = Xi''(t = tj + 1) = 0. We denote X(t) = [X\u00bf(t),i \u2208 N]T as a vector of flow-rate functions at all locations. We sometimes abbreviate X(t) as X for simplicity."}, {"title": "Definition 2: Neural Differential Equation (NDE) [?], [27].", "content": "Let's begin with the Neural Ordinary Differential Equation (NODE) [18], [28]\u2013[30]. Consider a latent variable, which is the embedding of X generated from a fully connected (FC) layer, denoted as H = [Hi, i \u2208 N]T.\nH(t) = H(0) + \\int_{0}^{t} H'(\\tau)d\\tau \\qquad (1)\nOne can build a NODE neural network, denoted as f\uff61(H(t) : \u03b8), which takes H(t) as input and produces an approximate value for gradient function H'(t), where \u03b8 denotes the parameters, which can be optimized through training.\nH(t) \u2248 H(0) + \\int_{0}^{t} f_{\\theta}(H(\\tau): \\theta) d\\tau \\qquad (2)\nBy approximating the integral with a summation over small steps and iteratively applying f\uff61, we can compute H(t) iteratively, starting from H(0), through an ODE solver [?].\nIt is shown that the controlled version of NODE, Neural Controlled Differential Equation (NCDE) [31], [32], generates more accurate results. It employs a neural network fc(H(t) : 0c), taking H(t) as input and producing an output, which is then multiplied by the control term X'(t) to approximate H'(t), where \u03b8\u03b5 denotes the parameters.\nH(t) \u2248 H(0) + \\int_{0}^{t} f_{c}(H(\\tau): \\theta_c)X'(t)d\\tau \\qquad (3)\nThe integral in (3) is a Riemann-Stieltjes integral problem [33] whereas that in (2) is a Riemann integral problem."}]}, {"title": "B. Problem Statement", "content": "Design a deep learning network, denoted as MNDE (Multi-view NDE), which takes the past measurements over l time intervals as input and produces forcast on the flow rates of the next l' time intervals. The number of history intervals l is far less than the number of future intervals l'.\nIn most work [18], [20], [23], [34]-[36], both l and l' are set to 1 hour for short-term forecast. Note that I should not be too large to avoid excessively large models and computation costs that come with them. Moreover, research has shown that too large l may actually degrade forecast accuracy [37]. For long-term forecast, l' is set much larger than l.\nInput:\n\u2022 X = [Xi,j,i \u2208 N,j \u2208 {0,1,..,1 \u2212 1}]T as the past measurements over l time intervals\n\u2022 V = [Vi,j, i \u2208 N, j \u2208 {0, 1, .., l\u2032 \u2013 1}]T as the ground truth on the flow rates measured over the next l' intervals\nOutput:\n\u2022 Forecast flow rates \u0177 =MNDE(X,0) \u2208 Rn\u00d7l\u2032\nObjective:\n\u2022 Optimize the parameters of MNDE such that the error between forecast flow rates \u0177 and the ground truth y is minimized"}, {"title": "III. RELATED WORKS ON TRAFFIC FORECASTING", "content": "In this section, we discuss various traffic transportation fore-casting approaches, encompassing techniques ranging from traffic-focused machine learning methods to advanced deep learning strategies and neural differential equation methods."}, {"title": "A. Statistics and Machine Learning Approaches", "content": "There are a variety of traditional statistical and machine learning methods for traffic transportation forecasting. Some prominent example models are (1) Autoregressive integrated moving average model (ARIMA) [38], [39] integrates the autoregressive model with moving average operation; (2) Seasonal autoregressive integrated moving average (SARIMA) [40] adds a specific ability to ARIMA for the recognition of seasonal patterns; and (3) K-nearest neighbor model (KNN) [41] which predict traffic of a node based on its k-nearest neighbors. A common shortcoming of these models is their focus on temporal dependencies while often neglecting spatial factors, which are crucial in understanding traffic patterns across different areas. Furthermore, their reliance on human-designed features restricts their capability to autonomously discover more predictive and task-specific features, limiting their adaptability and effectiveness."}, {"title": "B. Deep Learning Approaches", "content": "There are extensive works on deep learning for traffic flow forecasting. Due to space limits, we only summarize some representative works. Diffusion convolutional recurrent neural network (DCRNN) [3], for instance, uses diffusion graph convolutional network (GCN) [42], which captures spatial correlations through bi-directional exchanges of information across nodes, with a GRU-based network [43] adept at temporal correlation analysis. However, while GRU excels in short-term correlations, it's less effective for long-range dependencies. Addressing this, spatio-temporal graph convolutional networks (STGCN) [6] and graph wavenet for deep spatial-temporal graph modeling (GraphWaveNet) [8] apply convolutional operations in both spatial and temporal dimensions, offering more nuanced insights into traffic patterns. Deep spatio-temporal residual networks (STResNet) [44] takes the deep Resnet structure to capture temporal closeness, period, and trend properties of traffic patterns. Spatial-temporal synchronous graph convolutional network (STSGCN) further advances this by focusing on localized spatio-temporal subgraphs, enhancing the model's ability to understand immediate, localized correlations. However, since the model formulation does not incorporate global information, this model still had limitations when it came to long-term forecasting. In addition to the spatial graphs from predefined road networks, spatial-temporal fusion graph neural networks (STFGNN) [9] later introduce the use of dynamic time warping [45] for data-driven spatial networks which helped the model to learn representations from different data-driven and domain-driven views. Automated dilated spatio-temporal synchronous graph network (Auto-DSTSG) [34] implements the auto ML technique to search an optimal graph structure based on the idea of spatiotemporal synchronous graph modeling from STSGCN. Dynamic graph convolutional recurrent network (DGCRN) [46] utilizes the generated graph, which can effectively cooperate with a predefined graph while improving the prediction performance. Spatio-temporal differential equation network (STDEN) [20] borrows the idea in physics and introduces potential energy fields [47] and derives a differential equation to further describe the physical dynamics of the traffic potential energy fields. Graph multi-attention network (GMAN) [48] adapts an encoder-decoder architecture, where both the encoder and the decoder consist of multiple spatio-temporal attention [49] blocks to model spatio-temporal pattern. Spatio-temporal joint graph convolutional networks (STJGCN) [50] encompasses the construction of both pre-defined and adaptive spatio-temporal joint graphs between time steps, which represent comprehensive and dynamic spatio-temporal correlations. Spatio-temporal meta-graph learning for traffic forecasting (MegaCRN) [51] utilizes the attention mechanism to build a novel Meta-Node Bank to achieve an excellent embedding for the downstream network. Spatio-temporal self-supervised learning for traffic flow prediction (ST-SSL) [11] is benefited from self-supervised learning, by adding spatial and temporal heterogeneity-aware self-supervised signals. Spatio-temporal adaptive embedding makes vanilla transformer SOTA for traffic forecasting (STAEformer) [52] presents the spatio-temporal adaptive embedding that can yield outstanding results with vanilla transformers. Deep expansion learning for periodic time series forecasting (DEPTS) [22] expends the periodic state capture the complicated temporal pattern. The above works are the univariate forecasting, which is to predict the future value of a single variable based solely on its historical data. Whereas, multivariate forecasting involves using historical data from multiple variables to predict the future value of a target variable. They assume the target is influenced by multiple factors and there are complex relationships between the variables. Rethinking Multivariate Time Series Forecasting from a Pure Graph Perspective (FourierGNN) [25] treats multivariate time series from a pure graph perspective and proposes the Fourier graph operator to perform matrix multiplication in Fourier space. Spatial-temporal identity: a simple yet effective baseline for multivariate time series forecasting (STID) [35] proposes an effective baseline for multi-variant time series forecasting by attaching spatial and temporal identity information, based on simple multi-layer perceptrons."}, {"title": "C. Neural Differential Equation Methods", "content": "Neural Differential Equations (NDES) [27] can optimize neural networks in a continuous manner using differential equations. They are capable to do high-capacity function approximation, utilize continuous depth neural networks. There are a few types of NDEs: (1) neural ordinary differential equation (NODE) such as ODE-based RNN models [28], [29] (e.g., ODE-RNN, ODE-GRU, ODE-LSTM) and latent ODE models [53]; (2) neural controlled differential equation (NCDE) [31], [32] which is usually used to learn functions for the analysis of irregular time-series data; and (3) neural stochastic differential equation [54], [55] which is usually employed for generative models that can represent complex stochastic dynamics.\nMore recently, Spatial-temporal graph ode networks for traffic flow forecasting (STGODE) [18] brings us the attention to the NODE application in transportation forecasting area. It breaks through the limit of network depth and improves the capacity of extracting longer-range spatial-temporal correlations. Based on STGODE, Graph neural controlled differential equations for traffic forecasting (STGNCDE) [19] provides a new perspective of continuous depth generation by employing NCDE to capture the spatio-temporal dependencies, where the controlled term helps the model gain more powerful representations."}, {"title": "IV. PROPOSED APPROACH", "content": "This section introduces our Multi-View Neural Differential Equation (MNDE) framework, illustrated in Fig. 1. We give an overview below before providing the details for each of its components. The framework begins by interpolating discrete flow-rate measurements into continuous time functions and then learns three latent embeddings (through fully connection layers) on top of dense samples from X. These embeddings capture different latent dynamic patterns of the traffic flows, including the temporal dynamics (HF(t)), spatio-temporal dynamics (HST(t)), and dynamic edge interactions (HE(t)), respectively, where superscript C stands for current pattern. Each of these three current dynamic patterns is modeled by an NDE (shown in the top of the figure). The spatio-temporal dynamics are computed based on the temporal dynamics (T in the figure) and the spatial dynamics from a graph convolutional network component (S in the figure). Another graph convolutional network component (E in the figure) captures the edge dynamics. We integrate these components together to construct the current neural differential equations module (CNDE). We feed the output of CNDE back as the input for a number of times \u2013 3 times in our experiments but 2 times in the figure to save space \u2013 to create deeper networks for better capturing of the dynamic patterns. To capture the delayed propagation effect in traffic flow dynamics, we introduce a new delayed neural differential equations module (DNDE), which extends the CNDE module with a delayed factor in temporal integration to capture the delayed temporal dynamics (HP(t)), delayed spatio-temporal dynamics (HBr (t)), and delayed edge interactions (HD(t)), where superscript D stands for delayed pattern. Again, we feed the output of DNDE back as the input a number of times \u2013 3 times in our experiments but 2 times in the figure to save space \u2013 for deeper networks. We also add a differentiation module (DM), which uses self-attention on temporal gradients to capture the abrupt shift patterns. The learned embeddings of different latent dynamics are finally aggregated to make the final forecasting."}, {"title": "A. Temporal Interpolation and Latent Embeddings", "content": "After interpolating input measurements from discrete times to continuous time through the cubic spline method, we perform a dense temporal sampling for a much expanded input than the original measurements, as is illustrated in the figure with the sampling steps r,r' < 1; we set r and r' to 1/3 in our experiments. The dense samples are fed into three separate fully connection layers to generate embeddings, HF(0) \u2208 Rnxxc, HST(0) \u2208 Rnxxc, and HE(0) \u2208 Rnxnxxc', where c and c' are the embedding dimensions. These embeddings become the initial states of three NDEs for temporal dynamics HF(t), spatio-temporal dynamics HST(t), and dynamic-edge patterns HE(t), respectively. The dimension of HE(0), i.e., n \u00d7n\u00d7c',"}, {"title": "B. Current Neural Differential Equations (CNDE) Module", "content": "The CNDE module consists of three NDEs for temporal dynamics, spatio-temporal dynamics, and dynamic-edge patterns, respectively, which are explained below.\nTemporal dynamics: The temporal function T is a crucial component in the recursive implementation of the NDE for the latent temporal dynamics HF(t), as in (4), to generate the final result HF(l). T takes Hf(t), initially HF(0), as input and performs three fully connected layers with a skip connection to produce a tensor of dimension Rnxc. We do not use a recurrent neural network layer in T, as the temporal information is incorporated by the multiplier X'(t) in the NDE model.\nH\u00a3(1) = HF (0) + \\int_{0}^{l} T(Hf(t))X'(t)dt \\qquad (4)\nSpatial-Temporal dynamics: To capture the spatio-temporal dynamics, we multiply the spatial function S and temporal function T within the second NDE. Following [19], S is designed based on a graph neural network (GCN) structure [56]. The final results of spatial-temporal dynamics, HST(l), is computed by (5), following the general NDE in (3), with fc built by multiplying S and T. As shown in (5), S as a function of HST(t) consists of a GCN layer and a fully connected layer (F), followed by a skip connection. The adjacency matrix As and the parameters Ws, bs are learnable. Here is a non-linear activation function (e.g., ReLU).\nHg\u2081(1) = HST (0) + \\int_{0}^{l} S(HST(t))T(HST(t))X'(t)dt \\qquad (5)\nS(HST(t)) = F(GCN(HST(t))) + F(HST(t))\n= F(\\sigma(AsHST(t)Ws+bs)) + F(HS\u2081(t)) \\qquad (6)\nEdge dynamics:\nTraffic flows at different locations are not isolated in a real-world road system, where the dynamic patterns of some flows are associated under similar temporal conditions (such as peak hours), similar residential/commercial environment, or same vehicles that travel through them. We model such potential association with an artificial edge between each pair of locations and capture the dynamic edge patterns with one extra NDE, HE(t). As shown in (7), we formulate an edge function E in a similar construction as the S function, consisting of a GCN layer and a fully connected layer (F), followed by a skip connection. The difference is that input latent embedding, HE(t) with initial value of HE(0), is of a different size, n \u00d7 n \u00d7 c', in order to reflect pair-wise features between locations. Our method is different from the existing work [6]-[9] in that we use dynamic edge features in the GCN operation, instead of considering edges in a static or fixed adjacency matrix. The advantage is that such a design can learn the edge-level temporal knowledge from the node (location) level representation.\nH(1) = H (0) + \\int_{0}^{l} E(H(t))dt \\qquad (7)\nE(HE(t)) = F(GCN(H(t))) + F(H(t))\n= F(\\sigma(AEH(t)WE + be)) + F(HE(t)) \\qquad (8)"}, {"title": "C. Delayed Neural Differential Equations Module", "content": "In order to characterize the delayed propagation of traffic flow dynamics (where traffic from an upstream location affects a downstream location at a later time), we introduce a new delayed NDE module. It is known to the transportation community [57], [58] that traffic flow dynamics are determined by both the current state and the delayed state in a road system. Consider an example where congestion is developed at one segment of a highway. As the vehicles slow down, those behind will slow down. This chain reaction of slowing down propagates backward along the highway. Consequently, drivers further downstream will experience a delayed slowdown (i.e., a smaller flow rate) and even congestion. Therefore, the downstream traffic dynamics are determined by both the delayed state from upstream and the current local state. None of the prior work in deep learning based traffic forcasting [19], [23], [59] has considered the delayed state.\nThe Delayed NDE (DNDE) module is illustrated under the current NDE model (CNDE) in Figure 1. The design of DNDE, as shown in (9)-(11), is controlled by X'(t) with t\u2208 [0, d], where d should be set less than l, allowing the integration to focus on the data into the past, without the interference of the data closer to the present, i.e., in the range of t \u2208 (d, l]. Through independent validation datasets, we find that d < 1/3 gives adequate discrepancy between the patterns captured by DNDE and CNDE.\nHP(d) = HP (0) + \\int_{0}^{d} T(HP(t))X'(t)dt \\qquad (9)\nHg\u2081(d) = Hg(0) + \\int_{0}^{d} S(Hr(t))THB(t)) X'(t)dt \\qquad (10)\nHD(d) = HE (0) + \\int_{0}^{d} E(HD(t))dt \\qquad (11)"}, {"title": "D. Differentiation Module", "content": "In order to capture abrupt shift patterns like sudden road closure due to car accidents or severe weather in a road system (unforeseen incidents that unfold suddenly), we design a differentiation module to learn from the local temporal gradients of traffic flow data. The module aims to conduct fine-grained analysis to capture nuanced fluctuations and interactions. This is different from the existing work that focuses on regularly occurring events such as rush hour congestion [8], [10]. Specifically, we concatenate the sampled temporal gradients of traffic flow data, denoted as D = [X'(0); X'(r'); ...; X'(l)] \u2208 Rnx\u310a, where r' (e.g., 1/3) is the increment step for sampling. Then we apply a self-attention layer [49], as shown in (12), on the input D to produce an output D', Rnxl' given by (13). Recall that l' is the length of the output for each flow in the problem statement. The self-attention layer focuses its learning on the nuances, where the weight matrices Wq \u2208 R\u2606xc, Wk \u2208 R\u2606xc', and W\u2208 Rxl represent the attention operations for query Q, key K, and value V, respectively. The number of attention heads is set by h, and c' is the embedding dimension.\nQ = DWq + bq, K = DWk + bk, V = DW\u2084 + bv \\qquad (12)\nD' = softmax (\\frac{Q^TK}{\\sqrt{h}}) V \\qquad (13)"}, {"title": "E. Aggregation Module", "content": "We aggregate the final latent embeddings from the CNDE module, the DNDE module, and the differentiation module to make our long-term flow rate forecast. We first use CNN and MLP to transform the outputs of the CNDE and DNDE Modules, i.e., HST(1), HE(1), HBr(d), and HP(d), to the space of Rnxl'. They are denoted as po, P1, P2 and p3, respectively, after transformation. We do not use HF(l) and HP(d) because the temporal dynamic patterns are already incorporated into HSr(l) and HBr(l). Denote D' as p4. The aggregation formula is given in (14), where each transformed output is multiplied by the sum of other transformed outputs that are first normalized using the softmax operation. This aggregation technique offers several advantages, including the ability to choose critical forecasting features and non-linear aggregation. This non-linear operation [30] is used to account for correlations in the higher dimensions.\n\u0177 = \\frac{K}{K(K-1)} \\sum_{m} (\\sum_{n\u2260m} softmax(p_n)) \\qquad (14)"}, {"title": "F. Loss Function", "content": "The Huber loss function is commonly employed in regression problem scenarios, such as traffic flow forecasting. This function is a hybrid of the L1 and L2 loss functions and is well documented in literature [61]. The Huber loss function is composed of two parts, which form a piece-wise function: (1) robustness: a squared term that has a smooth and small gradient when the disparity between the true and predicted values is small (i.e., less than a d threshold value), and (2) accuracy: a restricted gradient term when the true and predicted values are considerably different. We choose Huber loss due to its balance between accuracy and robustness. The standard form of the Huber loss function is represented in equation (15).\nL(Y, Y) = \\begin{cases}\n\\frac{1}{2}(Y - \u0176)^2, & |Y - \u0176| \u2264 \u03b4\\\\\n\u03b4 |Y - \u0176| - \\frac{\u03b4^2}{2}, & otherwise\n\\end{cases} \\qquad (15)\nwhere d is a hyperparameter value set for the intended threshold; y is the true future spatio-temporal data; and \u0177 is the predicted future data."}, {"title": "V. EXPERIMENTS AND NUMERICAL RESULTS", "content": [{"title": "A. Settings and Datasets", "content": "Dataset: We use three widely used public traffic flow datasets, PEMS03, PEMS04, PEMS08, PEMS-BAY, and METR-LA 1 which are collected from the Freeway Performance Measurement System in California, USA. The details of data statistics"}]}, {"title": "B. Prior Work", "content": "For all prior works, we run their codes and follow their recommended configuration. In order to fit in the long-term forecasting scenario, we set all the under models with history length l = 12, and future length 1' = 96. All results are obtained by averaging the outcomes of four separate experiments.\n\u2022 Graph WaveNet [8]: Graph WaveNet integrates adaptive graph convolution with 1D dilated casual convolution to capture spatio-temporal dependencies.\n\u2022 STGCN [6]: Spatio-Temporal Graph Convolutional Network combines graph structure convolutions with 1D temporal convolutional kernels to capture spatial dependencies and temporal correlations, respectively.\n\u2022 STSGCN [23]: Spatio-Temporal Synchronous Graph Convolutional Networks decompose the problem into multiple localized spatio-temporal subgraphs, assisting the network in better capturing of spatio-temporal local correlations and consideration of various heterogeneities in spatio-temporal data.\n\u2022 STFGNN [9]: Spatio-Temporal Fusion Graph Neural Networks uses Dynamic Time Warping (DTW) algorithm to gain features, and follow STSGCN [23] in using sliding window to capture spatial, temporal, and spatio-temporal dependencies.\n\u2022 STGODE [18]: Spatio-Temporal Graph ODE Networks attempt to bridge continuous differential equations to the node representations of road networks in the area of traffic forecasting.\n\u2022 MegaCRN [51]: Meta-Graph Convolutional Recurrent Network utilize the attention mechanism to build a novel Meta-Node Bank to achieve an excellent embedding for downstream network.\n\u2022 Z-GCNETS [24]: Time Zigzags at Graph Convolutional Networks attempt to bridge continuous differential equations to the node representations of road networks in the area of traffic forecasting.\n\u2022 Spacetimeformer [64]: Long-Range Transformers tokenize the spatio-temporal series and then learn interactions between space, time, and value information jointly along this extended sequence.\n\u2022 FourierGNN [25]: This model rethinks time series forecasting from a pure graph perspective and proposes the Fourier graph operator to perform matrix multiplication in Fourier space.\n\u2022 ST-SSL [11]: The Spatio-Temporal Self-Supervised Learning approach enhances traffic pattern representations with auxiliary self-supervised learning paradigms.\n\u2022 STGNCDE [19]: Spatio-Temporal Graph NCDE Networks first introduce NCDE to traffic forecasting area. It utilizes the power of controlled path and its characteristic of differentiable to achieve a excellent performance."}, {"title": "C. Model Performance", "content": "Table 1 presents the comparison between the proposed MNDE and the prior work in terms of MAE, MAPE and RMSE at the end of the 2nd hour, the 4th hour and the 8th hour (after the input flow-rate measurements).\nOur proposed MNDE demonstrates superior performance across two datasets, three forecasting horizons, and three evaluation metrics. For example, at the 8th-hour time interval, our model exhibits significant enhancements in comparison to the second-best model, with 24% and 15% improvement observed in the MAPE metric.\nBasic GCN models (GraphWaveNet and STGCN) perform poorly in long-term forecasting. This is likely due to the constraints imposed by limited network depth and the associated risk of over-smoothing [12], [13]."}, {"title": "D. Ablation Study", "content": "To investigate the effect of different components of MNDE, we conduct ablation experiments on PEMS08 with several different variants of our model. CNDE1(ST): We interpolate the discrete flow data into continuous flow data and fed it into the model which only includes one pass of a simplified CNDE Module with only the temporal dynamics (T) and the spatial-temporal dynamics (ST). CNDE3(ST): It is the same s CNDE1(ST) except that the CNDE module is looped three times. CNDE3(STE): It uses the complete version of CNDE with the edge dynamics (E). The output of the edge and spatio-temporal dynamics are aggregated by the aggregation module. CNDE3(STE)+DNDE: It uses both CNDE module and the DNDE module, whose outputs are aggregated. MNDE: It adds differentiation module to capture the localized abrupt shift patterns.\nFig. 2 presents the values of MAE, MAPE and RMSE under the variants of MNDE over PEMS08 dataset at the 8th hour time interval. The results show that the DNDE module and the differentiation module contribute the most in reducing the forecast errors, as MNDE and CNDE3(STE)+DNDE improve accuracy significantly over their respective predecessors, whereas the edge dynamics and the looping of the CNDE module still contribute, though by smaller amounts, as CNDE3(STE) and CNDE(STE) improve accuracy incrementally over their respective predecessors. This highlights the importance of capturing the delayed propagation of traffic conditions and the local traffic trends, which we are the first to introduce in long-term traffic forecasting."}, {"title": "E. MNDE v.s. STGNCDE", "content": "We conduct a case study that compares the proposed MNDE and the best existing work STGNCDE at a randomly chosen location from the PEMS04 road network. The location has 4 days of flow rate measurements. Figure 3 shows the long-term flow rate prediction of MNDE (STGNCDE) over time at the location, together with the ground truth, where each prediction is produced by using one hour input data that is of 8 hours ago, for long-term forecasting. We can see that the grey line (flow-rate predictions by STGNCDE) sometimes deviates significantly from the red line of ground truth, while the blue line (predictions by MNDE) follows the ground truth much better. The reason is that the design of STGNCDE only contains CNDE (without the explicit edge dynamics E), while MNDE contains CNDE, DNDE and differentiation module (DM), with DNDE capturing delayed propagation of traffic dynamics and DM capturing localized traffic trend."}, {"title": "F. Robustness Study", "content": "To evaluate the robustness of our MNDE, we perform an experiment where we partially remove some of the input data by randomly substituting some original flow rate measure-ments with \u201cNaN\u201d, simulating malfunctioned sensors with missing measurements or rejected measurements due to too much noise. Following [16"}]}