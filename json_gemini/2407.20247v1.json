{"title": "How Homogenizing the Channel-wise Magnitude Can Enhance EEG Classification Model?", "authors": ["Huyen Ngo", "Khoi Do", "Duong Nguyen", "Viet Dung Nguyen", "Lan Dang"], "abstract": "A significant challenge in the electroencephalogram EEG lies in the fact that current data representations involve multiple electrode signals, resulting in data redundancy and dominant lead information. However extensive research conducted on EEG classification focuses on designing model architectures without tackling the underlying issues. Otherwise, there has been a notable gap in addressing data pre-processing for EEG, leading to considerable computational overhead in Deep Learning (DL) processes. In light of these issues, we propose a simple yet effective approach for EEG data pre-processing. Our method first transforms the EEG data into an encoded image by an Inverted Channel-wise Magnitude Homogenization (ICWMH) to mitigate inter-channel biases. Next, we apply the edge detection technique on the EEG-encoded image combined with skip connection to emphasize the most significant transitions in the data while preserving structural and invariant information. By doing so, we can improve the EEG learning process efficiently without using a huge DL network. Our experimental evaluations reveal that we can significantly improve (i.e., from 2% to 5%) over current baselines.", "sections": [{"title": "Introduction", "content": "Electroencephalograms (EEGs) hold great potential for medical advancements, including identifying neurological disorders and enabling mind-computer interfaces [2], [5], [25]. Nonetheless, decoding the complex patterns embedded in EEG data necessitates the utilization of artificial intelligence (AI). AI models can learn from and interpret vast datasets, unlocking secrets within brain waves. However, applying a deep learning model to EEG classification presents unique challenges due to the temporal and non-linear nature of EEG signals, which can lead to overfitting and unreliable results.\nResearchers are working on refining model architecture and enhancing feature extraction to fully exploit the combined temporal and spatial nature of multi-channel EEG data. Recurrent neural networks (RNNs) [26], [20] and LSTMS [24,28,27] and [9] excel in capturing the temporal evolution of brain activity across channels, but they can overlook the crucial spatial aspect, which can lead to overfitting. Convolutional neural networks (CNNs) [7], [15], EEG-based architectures [14], [21] are better at extracting spatial features from multi-channel data, but their dependence on 1D convolution limits their ability to capture intricate temporal relationships between channels. Feature enhancement methods, such as siamese networks [21], [24] learning a joint representation between EEG and visual stimuli, may not generalize well to other EEG-based classification problems. Converting EEG signals into grayscale heatmaps [19] can also distort the temporal structure of the data, leading to information loss. Moreover, despite state-of-the-art models achieving progress in signal classification, the core challenge of multi-channel signals, particularly EEG, remains largely unaddressed [4], [8]. The intricate nature of EEG data, with amplitude and varying frequency across channels, can lead to overlapping signals and the subsequent masking of vital information in certain channels [13,12].\nThe paper introduces a two-step approach to pre-process EEG signals, aiming to improve the accuracy of EEG signal classification. It uses Inverted Channel-wise Magnitude Homogenization (ICWMH) to transform 1D EEG signals into high-dimensional representations, while FEVSC enhances information density by extracting features beyond standard temporal dynamics. The method also introduces a skip connection mechanism, enriching the feature set and providing a more comprehensive picture of brain activity. Our contributions can be summarized as follows:\nAn inverted channel-wise magnitude homogenization is proposed to address the variance in signal amplitudes across channels, ensuring equalized contribution among channels.\nA feature enrichment via skip connection leverages the sequential nature of EEG signals to extract additional features, especially long-range dependencies, enriching the overall feature set.\nExtensive experiments comparing our method to various baselines proved its effectiveness, and ablation tests were used to assess its adaptability."}, {"title": "Methodology", "content": ""}, {"title": "Preliminary and Notations", "content": "We consider a dataset comprising N training samples {(x\u00b2, y\u00b2)}, with x\u00b2 \u2208 Rdx and y\u00b2 \u2208 Rdy denotes the ith EEG sampled signal and the corresponding ground truth. Each image x\u00b2 comprises H \u00d7 W pixels denoted by x\u00b2 = {x\u00b2(a, b)}, with a and b as pixel indices.The dataset encompasses M ~ p(M) classes, where p(M) denotes the categories' probability distribution. For a sample belonging to class m we have (xm, ym), where m \u2208 [1,..., \u039c]. To achieve EEG classification, we employ a model parameterized by a weight matrix W = [w\u2081,...,wm]. The objective is to optimize this model by minimizing the Cross-Entropy Loss function (Equation 2), which effectively measures the discrepancy between the predicted class probabilities and the true class labels. The specific problem tackled in this work can be formally described by Equation (1)."}, {"title": "Overall Architecture", "content": "The proposed methodology (Figure 1) for EEG signal classification uses a two-step preprocessing approach to improve signal quality and feature extraction. This approach tackles the inherent variability in power distribution observed across EEG channels. First, the Inverted Channel-wise Magnitude Homogenization (ICWMH) technique is used to normalize signal amplitude, ensuring a balanced input for subsequent processing stages. This transformation encodes each EEG signal sample denoted as x\u00b2 into an EEG-encoded image suitable for the CNN model. The encoded image is then enriched using a Feature Enrichment via Skip Connection (FEvSC) approach, which uses edge detection to capture variant information from the image. This information is then integrated back into the encoded image using the Hadamard sum, providing a more comprehensive representation of the CNN model. The enriched image, incorporating both original data and edge-derived features, is stacked into a three-layer structure, which is fed into the convolution operator of the CNN for classification, resulting in improved classification performance."}, {"title": "Inverted Channel-wise Magnitude Homogenization (ICWMH)", "content": "The input data x\u00b2 \u2208 RC\u00d7L (refer to Figure 1) is a composition of multiple stochastic random processes x(t), where C, L is the number of channels and channel signal length, respectively. The overall problem is maximizing the log-likelihood between the distribution of channel-wise signal p(y\u00b2x,...,x) and the estimated class distribution p(x\u00b2y\u00b2) (refer to Equation 3). Figure 1 illustrates the differences in power P, or signal strength, across channels owing to the underlying neuronal activity and the amplitude of measured neural currents. Furthermore, the variance distribution of these signals (p\u2260p \u2200 h\u2260k) highlights how the dominant frequencies differ across brain regions. Consequently, the gradients across channels \u2207wF(x) are dominated by the overwhelming one.\nTo address the issue of dominant magnitude across channels, ICWMH (see Figure 1) aims to equalize signal magnitudes before training. This ensures each channel contributes equally, mitigating the dominance of specific channels by their overwhelming magnitude. ICWMH employs a bottle-neck normalization process to squeeze the multi-channel signals within a fixed range. This technique serves a dual purpose which is removing redundant features within the local receptive field, preventing individual channels from dominating solely due to their magnitude, and balancing the contribution of each channel (represented by P(x) for channel c) by equalizing their influence on the learning process. Following this normalization, ICWMH leverages an interpolation step to enhance further the quality of information. This additional refinement ensures that while balancing the effect of overwhelming channels (indicated by a ratio of crucial frequency properties are preserved (refer to Equation 4).\nThis translates to a learning process where no single channel holds undue sway, allowing for the extraction of richer and more informative features from the entire multi-channel spectrum."}, {"title": "Variant Feature Extractor", "content": "In the EEG-encoded image, the salient features come from the change between image regions. Thus, by applying edge detection on the image (Figure 2), we can get the salient features with low computation complexity. To do so, we first define the pixel-wise gradient magnitude:\nwhere a and b are pixel row and column indices, and the pixel-wise gradient direction is as follows:\nBased on the two aforementioned features, we can find the salient representations as follows:"}, {"title": "Feature Enrichment via Skip Connection (FEvSC)", "content": "The Feature Enrichment via Skip Connection (FEvSC) approach is proposed to improve the classification performance of an EEG model. It uses the strengths of the first proposed technique, ICWMH, and edge detection methods to enrich the feature space of EEG data. The approach as described in Equation 7 aims to mitigate redundancy while preserving essential structural and variant features inherent in EEG signals, enhancing predictive accuracy.\nIn this equation, I represents an affine transformation influenced by general edge detection principles. Edge detection in EEG signal analysis helps isolate significant boundaries and transitions, indicating critical neural activities and accentuating high-frequency components associated with subtle brain activities. This enhances contrast and clarity of signal features, highlighting areas of neural activity crucial for accurate classification. The FEVSC approach uses skip connections in its neural network architecture, allowing direct access to both raw and processed EEG signals. These enriched features provide refined input, aiding in learning complex patterns for accurate EEG classification. Combining edge detection and skip connections offers a promising direction for improving EEG classification system performance."}, {"title": "Experiment", "content": ""}, {"title": "Datasets", "content": "Perceive Lab [24], [21]. The Perceive Lab dataset contains EEG responses from 6 subjects who viewed 2,000 unique objects (40 classes from ImageNet [6]) during 10 seconds to achieve 11,964 EEG segments. Each sample has 128 channels and 500 time-step data. The dataset is sampled in 50Hz with notch filtering, channel-wise z-score normalized, and bandpass filtered across three frequency ranges. High-gamma-dataset (HGD) [23]. The High-Gamma Dataset is a 128-electrodes dataset from 14 healthy subjects, consisting of 1000 four-second trials of executed movements divided into 13 runs per subject. The four classes of movements were left, right, feet, and rest. The datasets are each divided into training (80%), validation (10%), and test (10%) sets."}, {"title": "Models", "content": "We reimplemented and evaluate LSTM networks, stacked bi-directional LSTMs [24,28,27,9], EEGNet [14] and EEGChannelNet [21] model, Siamese networks [21], 2D EGG-encoded grayscale heatmaps [19]. We trained these models under identical conditions with the learning rate of 9e 04, the batch size is 64, and the optimizer is Adam under 100 epochs. This comprehensive exploration provided valuable insights into the effectiveness of different EEG classification approaches."}, {"title": "Problems", "content": "This study presents a two-step preprocessing method for enhancing feature representation. First, data is normalized using Inverted Channel-Wise Magnitude Homogenization (ICWMH). Second, the variant feature is extracted through edge detection, using Gaussian blur to reduce noise and employing Adaptive Edge and Canny Edge Detection techniques."}, {"title": "Hyperparamter Tuning", "content": "In particular, the training configuration for Perceive Lab dataset employs Canny edge detection with thresholds (50, 120) in bilinear interpolation mode, along with a Gaussian blur kernel size of (3, 3). For the HGD dataset, the baseline configuration utilizes adaptive edge detection with mean thresholding in bilinear interpolation mode and the same Gaussian blur kernel size."}, {"title": "Comparison to State-of-the-art methods", "content": "ICWMH with Edge Detection shown in Table 1, achieves an accuracy of approximately 66% on a dataset of 40-class images from the Perceive Lab challenge. This surpasses previous approaches, such as GIE (Grayscale Image Encoded), which achieved 64% accuracy. The key improvement lies in its additional edge detection step, which extracts critical features within the EEG data, boosting accuracy to 65.78%. This approach outperforms standard and specialized architectures like LSTMs and EEGNet by a large margin and surpasses EEChannelNet by nearly 20%. ICWMH+ED's ability to extract richer information from EEG data paves the way for advancements in brain-computer interfaces and neurological disease diagnosis. When applied to the HGD dataset, ICWMH+ED achieves an accuracy of 57.18%, double the accuracy of the grayscale-encoded image method without edge detection. This demonstrates the promise of ICWMH+ED in EEG classification tasks, even exceeding previous RNN models by an accuracy of 7%."}, {"title": "Ablation Study", "content": ""}, {"title": "Interpolation Method", "content": "The study examines the impact of different interpolation methods on EEG classification performance (refer to Table 2), focusing on 'bilinear' and 'nearest' techniques. The 'bilinear' method [18] achieves baseline accuracy for both datasets, while the 'nearest' method [22] results in lower accuracy, especially in the HGD dataset, where the accuracy drops significantly to half of the baseline. The choice of interpolation significantly impacts the classification model's effectiveness."}, {"title": "Edge Threshold", "content": "The study investigates different threshold values [1], [3] in EEG data. The threshold of (50, 120) set Perceive Lab dataset achieves the highest accuracy (65.78%). Lower threshold settings show a slight decrease in accuracy (64.74%), while higher threshold settings result in a significant drop in accuracy (51.75%), potentially omitting crucial information and leading to underfitting. The HGD dataset performs best with a threshold of (40, 120), achieving 56.81% accuracy. A stricter threshold setting leads to lower accuracy, especially with an increased upper threshold, causing underfitting and a significant drop in accuracy."}, {"title": "Gaussian Blur Kernel", "content": "The paper explores the impact of different Gaussian blur kernel sizes [17] on EEG classification accuracy. The baseline kernel size of (3,3) achieves the highest accuracy at 65.78% for Perceive Lab and 57.18% for the HGD dataset. However, as kernel size increases to (5,5) and (7,7), accuracies decrease, possibly due to excessive smoothing, which can lead to a loss of critical signal detail. This highlights the importance of selecting an appropriate level of Gaussian blurring for EEG image preprocessing."}, {"title": "Edge Threshold", "content": "The experiment also investigates the effectiveness of adaptive thresholding methods [16] for edge detection. Adaptive Mean Thresholding achieves an accuracy of 62.98% for Perceive Lab and baseline accuracy for the HGD dataset, effectively balancing local variations in illumination in EEG imaging. Adaptive Gaussian Thresholding, however, achieves lower accuracy for both datasets, suggesting a more localized approach to thresholding."}, {"title": "Conclusion", "content": "This paper presents a groundbreaking EEG classification method using Inverted Channel-wise Magnitude Homogenization (ICWMH) and Edge Detection. The method achieves approximately 66% accuracy rate in 40 classes of classification tasks, highlighting the importance of improved feature representation and balanced channel input. The process converts EEG signals into expanded dimensional representations and integrates long-range dependencies, extracting a broader set of features. The study emphasizes the need for careful hyperparameter calibration and the delicate interplay between noise suppression and feature retention in EEG signal classification success. The methodology could serve as a new benchmark in the field."}, {"title": "Related Works", "content": "Model Architecture Design Within the domain of EEG classification, significant research efforts have centered on optimizing established models like Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) for enhanced performance [11]. [24] explored the potential of Long Short-Term Memory (LSTM) networks and their stacked variants, while [28] focused specifically on LSTMS-B with Swish activation and bagging ensembles. Further investigations by [9] and [27] extended to BiLSTMs and attention-based models for visual object classification based on EEG signals. While RNNs excel at capturing temporal dynamics, concerns regarding potential overfitting due to limited spatial information extraction remain. This contrasts with CNNs, which demonstrate strong efficacy in EEG classification by effectively extracting relevant brain activity features. [14] solidified the value of CNNs in this domain through their compact EEGNet architecture specifically designed for EEG-based brain-computer interfaces. Further emphasizing the versatility of CNNs for EEG data, [21] proposed EEGChannelNet, employing 1D CNNs for robust feature extraction. These advancements reflect the ongoing pursuit of improved performance and adaptability in EEG classification through continued model optimization and innovation.\nFeature Enhancement Prior research has significantly improved EEG feature data for better classification accuracy. [21] and [24], [10] pioneered the use of a Siamese network architecture to learn a joint embedding between EEG signals and images. This approach maximizes the similarity between embeddings from both modalities, thereby enhancing the model's representational power for EEG-based visual classification tasks. Further advancing EEG data utilization, [19] introduced an innovative approach to transforming EEG signals into grayscale heatmap representations. This conversion from 1D signals to a 2D image format leverages the strengths of Convolutional Neural Networks (CNNs) by making relevant features more readily extractable for classification tasks. These advancements demonstrate the ongoing focus on enriching EEG feature data to unlock its full potential for accurate and reliable brain-computer interaction."}]}