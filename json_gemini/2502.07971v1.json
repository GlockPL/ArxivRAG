{"title": "RETREEVER: TREE-BASED COARSE-TO-FINE REPRESENTATIONS FOR RETRIEVAL", "authors": ["Shubham Gupta", "Zichao Li", "Tianyi Chen", "Cem Subakan", "Siva Reddy", "Perouz Taslakian", "Valentina Zantedeschi"], "abstract": "Document retrieval is a core component of question-answering systems, as it enables conditioning\nanswer generation on new and large-scale corpora. While effective, the standard practice of encoding\ndocuments into high-dimensional embeddings for similarity search entails large memory and compute\nfootprints, and also makes it hard to inspect the inner workings of the system. In this paper, we\npropose a tree-based method for organizing and representing reference documents at various granular\nlevels, which offers the flexibility to balance cost and utility, and eases the inspection of the corpus\ncontent and retrieval operations. Our method, called RETREEVER, jointly learns a routing function\nper internal node of a binary tree such that query and reference documents are assigned to similar\ntree branches, hence directly optimizing for retrieval performance. Our evaluations show that\nRETREEVER generally preserves full representation accuracy. Its hierarchical structure further\nprovides strong coarse representations and enhances transparency by indirectly learning meaningful\nsemantic groupings. Among hierarchical retrieval methods, RETREEVER achieves the best retrieval\naccuracy at the lowest latency, proving that this family of techniques can be viable in practical\napplications.", "sections": [{"title": "Introduction", "content": "Information retrieval enables us to efficiently search for relevant information across millions of documents. With\ntechniques like retrieval-augmented generation, document retrievers have become a key component in transforming\nlarge language models (LLMs) into tailored experts without the need for exhaustive fine-tuning [Lewis et al., 2020,\nIzacard and Grave, 2021, Gao et al., 2023]. They enable LLM-generated content to be grounded in retrieved knowledge,\nthereby alleviating hallucinations [Shuster et al., 2021]. Moreover, retrieval allows LLMs to scale to massive datasets\nwithout increasing their internal parameters.\nA popular approach for document retrieval is to represent documents as high-dimensional embeddings and use nearest-\nneighbor techniques to find relevant documents for a given query embedding [Karpukhin et al., 2020]. While effective,\nthe high dimensionality of these document embeddings results in a large memory footprint and heavy computation\nat inference time. The documents are also stored without any human-readable structure or understandable grouping,\nmaking it difficult to derive insights from the corpus or verify the retrieval process. In this paper, we propose\nRETREEVER, a tree-based method where documents are organized and represented at various granular levels, offering\ncoarse-to-fine representations, learned entirely end-to-end by optimizing for retrieval performance as the learning\nobjective. These representations offer the flexibility to balance cost and utility. Instead of training and storing a separate\nmodel for each desired representation size, a full-capacity tree can be learned, allowing excerpts to be encoded at any\nequal or lower level based on computational constraints during inference.\nRETREEVER takes inspiration from the hierarchical retrieval literature, where tree organizers are built and navigated\nvia calls to an LLM by cross-encoding queries and contexts [Chen et al., 2023, Sarthi et al., 2024, Edge et al., 2024].\nSuch retrievers have the advantage of preserving the existing organization of the data, such as document divisions and\nentity relationships, and grouping documents semantically into a readable tree organization. However, their reliance\non LLMs make them slow and expensive to train and run, hence impractical even for medium-size text corpora. In"}, {"title": "Related Work", "content": "Bi-encoders Retrieval models typically rely on sparse or dense vector embeddings to select the most relevant documents\nfor a query, reducing the problem to a nearest-neighbor search. Dense retrieval models Karpukhin et al. [2020], Khattab\nand Zaharia [2020] rely on encoders, neural networks designed to embed the semantic meaning of text into dense\nvector representations. State-of-the-art sentence embedding models often leverage bidirectional encoders, such as\nBERT Devlin et al. [2019] and BGE Xiao et al. [2024]), while others are built upon pretrained large language models,\nsuch as LLM2Vec BehnamGhader et al. [2024]). Some encoders, such as BGE-large Xiao et al. [2024], are specifically\ndesigned for retrieval, offering a sentence embedding framework optimized for such tasks. Other notable examples are:\nDPR Karpukhin et al. [2020], which finetunes a BERT Devlin et al. [2019] dual encoder to separately encode queries\nand documents into a single dense vector by contrastive learning; ColBERT Khattab and Zaharia [2020], Santhanam\net al. [2021], which finetunes BERT to encode queries and documents into multiple dense vector representations and\nby matching individual token embeddings between queries and documents. Unlike sparse retrieval methods, such as\nBM25 or TF-IDF Salton and Buckley [1988], these dense representations are less interpretable, expensive to compute\n(generally because of attention operations) and take up a large amount of storage space. They do however perform\nbetter on many downstream tasks such as QA, specially for complex queries.\nHierarchical retrieval Hierarchical retrieval approaches aim to balance test-time efficiency and accuracy by organizing\nthe retrieval process into multiple stages, typically involving coarse-to-fine filtering of candidate documents. Many\nhierarchical methods face challenges related to computational cost and performance trade-offs, and scale poorly with\nthe size of the corpus. MemWalker Chen et al. [2023] addresses LLM context limitations by structuring long texts\ninto a hierarchical memory tree. It first segments the text into chunks, summarizing each into a node, which are\nthen recursively merged into higher-level summaries until forming a root node. At query time, the LLM navigates\nthis tree interactively, using iterative prompting to locate the most relevant segments, MemWalker Chen et al. [2023]\nreframes retrieval in the context of addressing LLM context window limitations. It first segments long contexts into\nsmaller chunks, summarizing each into hierarchical nodes that form a memory tree. At query time, the LLM traverses\nthis tree using iterative prompting, efficiently locating the most relevant segments without exceeding its context limit.\nRaptor Sarthi et al. [2024] uses a clustering algorithm to group similar documents and then applies an LLM to recursively\nsummarize and re-embed chunks of text, constructing a tree with differing levels of summarization from the bottom up,\nresulting in a structured, multi-layered tree representation of the original documents. GraphRAG Edge et al. [2024]\nuses an LLM to build a graph-based text index by first deriving an entity knowledge graph from source documents and\nthen pregenerating community summaries for related entities. For a given question, partial responses are generated\nfrom each community summary and combined into a final summarized response. While hierarchical retrieval methods\nimprove response quality using LLMs and provides inspectable structures, they incur high computational costs and slow\nprocessing times, especially with large datasets. This trade-off makes them less suitable for real-time or resource-limited\nscenarios, emphasizing the need for more efficient solutions. RETREEVER overcomes these limitations by constructing\nand navigating a tree with no LLM calls."}, {"title": "Proposed Method", "content": "RETREEVER consists of (1) a frozen encoder E that returns embeddings for a given chunk of text, and (2) a learnable\nbinary tree T that organizes encoded pieces of text into a hierarchy and routes queries to their relevant contexts (see\nFigure 1). In this section, we describe how the tree hierarchy is learned by continuous optimization and how search is\nperformed at test time. In what follows, we use the term context to refer to the text segments organized by RETREEVER.\nTree Formulation A perfect binary tree T is a binary tree where all levels are completely filled. The nodes t \u2208 T of\nthe tree satisfy the following two properties: (i) each node has either no children or exactly two, one left child and one\nright child; (ii) each node, except the unique root, has exactly one parent. There exist two types of tree nodes: branching\n(or split) nodes TB, which route their inputs to their left or right child, and leaf nodes (or simply leaves) TL, which have\nno children and constitute the terminal state of the tree T := TB \u222aTL.\nGiven positive query-context pairs {(qi, c\u00ee) \u2208 P}1 where qi, c\u2081 are embedding pairs generated by the encoder E,\nour goal is to learn a perfect binary tree T of chosen depth D that assigns qi and ci to the same leaf node ti \u2208 TL. Such\nleaf assignments denoted T(xi) are obtained by routing a given input xi \u2208 X (e.g., xi := qi the query embedding)\nsequentially through branching nodes until it reaches a specific leaf node in the tree. Specifically, each branching node\nt\u2208 TB is parametrized by a split function se\u2081 that routes an input to its left child node if a certain condition is met, or to\nits right one otherwise. We denote by zi \u2208 {0, 1}|7| the route taken by the input, where zi,t equals 1 if the input has\nreached node t, and 0 otherwise.\n\u0422\u0432\nHowever, hard assignments would result in piece-wise constant functions with null gradients, making back-propagation\nineffective. Therefore, we make use of probabilistic relaxations, inspired by works such as Irsoy et al. [2012], to make\nthe tree learning problem differentiable, hence to allow the learning of the split parameters \u0398 := {0t}11 jointly by\ngradient descent and so that they are optimal for retrieval. Such manipulations effectively relax the hard routes into soft\nones zi \u2208 [0, 1]|7|, where each element zi,t can now be interpreted as the probability of the input traversing node t\n(see Figure 1).\nChoice of Split Function A split function se\u2081 : X \u2192 [0, 1] can be implemented in several ways, as long as it outputs a\nrouting score, that determines the probability of routing an input left or right. Its simplest form is a linear projection,\nsuch as the one used in Zantedeschi et al. [2021]: given a split node t, its left child tleft and right child tright, the split\nfunction is defined as the dot product so\u2081 (xi) = 0txT (interpreting \u03b8t \u2208 X as a hyper-plane). We experiment also with\nmore complex functions, such as neural networks (see Appendix A.1)."}, {"title": "Tree Propagation", "content": "Note that any split function defined above outputs a score that is independent of the scores from the\nother nodes. However, the probability of reaching a node should be constrained by the probability of having reached\nits parent, the parent of its parent, and so on, to avoid degenerate trees where a descendant has higher probability of\nbeing reached than its ancestors. The simplest way of enforcing such tree constraints is by propagating the scores of\nthe ancestors down to the node of interest by multiplying the probabilities along the path. We refer to this type of\ntree propagation as product propagation. Given a node t and its ancestors At (the split nodes along the path from\nthe root to t), the probability of reaching t is T(xi)t = Zt \u220fa\u2208A\u0142 Za. Notice that the product propagation naturally\nenforces that the sum of node probabilities at any depth is always constant and equal to 1. Alternatively, one can learn\nhow to propagate probability scores through the tree. We refer to this type of tree propagation as learned propagation\nand describe it in Appendix A.3. In either tree propagation, evaluating each split function sequentially, in order of\ndepth, would unnecessarily slow down training and inference, in particular for trees of large depth. Equivalently, in our\nimplementation we leverage parallel computing and the independence of split functions to evaluate all split functions in\nparallel, regardless of their depth, and then apply the selected tree propagation."}, {"title": "Training by Contrastive Learning", "content": "The task now is to learn end-to-end the parameters (that include those of the split\nand propagation functions), so that any query is routed optimally to the leaf that contains its corresponding ground-truth\ncontext. Such an optimal assignment is achieved when positive query-context pairs are independently routed similarly\nthrough the tree, leading to similar leaf assignments. However, optimizing solely for perfect positive assignments is\nlikely to lead to the trivial solution where all contexts and queries are routed to the same leaf, resulting in a collapsed\ntree with a single active path. To avoid such a solution, we define an objective that additionally encourages maximal use\nof the tree, by spreading unrelated contexts and queries across different tree leaves. Building on the contrastive learning\nliterature Oord et al. [2018], Radford et al. [2021], we optimize the following Softmax-based InfoNCE loss,\n$\\frac{1}{2P}\\left(\\log \\frac{e^{\\text{sim}(q_i, c_i)}}{\\sum_{j=1}^P e^{\\text{sim}(q_i,c_j)}} + \\log \\frac{e^{\\text{sim}(c_i,q_i)}}{\\sum_{j=1}^P e^{\\text{sim}(c_i,q_j)}}\\right)$,\nwhere P is the set of query-context pairs, and we take the similarity sim : [0,1]|TL] \u00d7 [0,1]|TL] \u2192 R to be the\nnegative Total Variation Distance (nTVD) between two leaf assignments: sim(a, b) = $\\sum_{l=1}^{|T_L|} a_l \\cdot b_l$, where\na = (a1, a2,..., a|T\u2081|) and b = (b1, b2, ..., b|T\u2081|) are the leaf assignment distributions, and |TL | is the number of leaf\nnodes in the tree. In Eq.(1), the left term encourages any query to have a leaf assignment similar to the assignment\nof its ground-truth context and different from any other context in the batch. Conversely, the second term encourages\ncontexts to be routed similarly as their positive queries and differently from their negative ones.\nNotice that learning node assignment probabilities, as opposed to unconstrained features (e.g., in MRL [Kusupati et al.,\n2022]), naturally provides a hierarchical and soft clustering of the documents which can be inspected by the user. We\nshow in Section 5 that learned clusterings capture semantic groupings, where assigned documents share topics and\nkeywords, even though RETREEVER does not directly optimize for semantic coherence."}, {"title": "Coarse-to-Fine Representations", "content": "Because of the hierarchical structure of the tree and its constraints, optimizing\nassignments at the leaf level, as expressed in Eq.(1), implicitly optimizes the assignments at intermediate levels, making\nthem suitable coarse representations. To boost the retrieval performance of such coarse representations, we devise an\noptimization scheme where at each iteration a random level of the tree is selected and the constrastive loss is optimized"}, {"title": "Indexing and Search", "content": "Once the tree is trained, we index new content by encoding each context excerpt with the encoder\nE and then routing it through the tree T to obtain assignments for the chosen tree level. Such assignments can then be\ninterpreted as a new representation of an excerpt, where each dimension represents the alignment between the excerpt\nand a learned semantic group. To retrieve related contexts for a query, we build a vector index based on these level\nassignments, process the query, and return its nearest neighbors based on the nTVD metric used at training."}, {"title": "Retrieval Experiments", "content": "In this section, we assess the retrieval performance of RETREEVER at different representation levels and as compared\nto flat and hierarchical retrieval methods. Our results indicate that using the learned node assignments for retrieval (1)\ngenerally preserves the representation power of the encoder at the finer level, (2) results in more expressive embeddings\nat the coarsest levels and (3) strikes the best latency-accuracy trade-off among hierarchical retrieval methods.\nMetrics and datasets To evaluate retrieval results, we measure the following standard metrics: RECALL@K, which\nassesses the proportion of ground-truth documents that are present within the top-k results returned by the retriever;\nNormalized Discounted Cumulative Gain at rank k (NDCG@K) which accounts for ground-truth ranking, as relevant\ndocuments appearing earlier are considered more valuable. We use four open-domain question-answering datasets\nfor our experiments: NATURAL QUESTIONS (NQ) [Kwiatkowski et al., 2019], HOTPOTQA [Yang et al., 2018b],\nTOPIOCQA [Adlakha et al., 2022], and REPLIQA Monteiro et al. [2024b]. A sample from our datasets consists of a\nquery (natural-language question) and one ground-truth context. We follow the pre-processing done in Monteiro et al.\n[2024a], with the difference that for HOTPOTQA we concatenate all relevant contexts and discard irrelevant ones to\nobtain context. We make use of a validation set for model selection and hyperparameter tuning. For REPLIQA, we use\nthe first split REPLIQA0 for training and validation (10% of the split) and REPLIQA1 for testing. For datasets with a\nvalidation set but not a test partition, we use the validation set for testing and create a validation set by holding out 10%\nof randomly selected training samples. Then, for training RETREEVER and the baselines we make use of the training\nquery-context pairs, and for testing, we build the index with all the contexts from training, validation and test splits, if\nnot specified otherwise."}, {"title": "Full Representations", "content": "In Table 1 we report recall@10 and NDCG@10 for all representation learning methods,\nusing their full representations (at leaf level for RETREEVER and HIER-GMM). We first observe that BGE and\nRETREEVER have generally similar performance on all datasets, which suggests that learning a tree retains the\nencoder's representational power. To assess whether this is a general property of models learning soft clustering, we\nreport the retrieval performance of HIER-GMM, using its node assignments as representations. The results indicate that\nthe extracted representations are clearly not suited for retrieval purposes, stressing the need to directly optimize the\nretrieval objective and not a proxy one, as we do.\nAdding constraints to the learning problem typically results in lower performance as compared to an unconstrained\nproblem. To quantify the performance degradation due to our tree constraints, we also report the performance of\nprobabilistic assignments as extracted by a flat model, that we name RETREEVER-NO TREE. The only difference with\nRETREEVER is that the tree structure and propagation are removed from the model. When lifting the tree constraints,\nwe indeed observe a performance improvement on 5 out of 8 cases. However, such improvement comes at the price of\nlosing the coarse-to-fine benefits of a tree structure."}, {"title": "Tree Congruence", "content": "We first verify whether RETREEVER learns representations that are congruent with its tree structure, despite being\ntrained solely on query-context alignment labels.\nWe begin by investigating whether the node embeddings of the cross-attention splits align with the tree structure, i.e.,\nwhether their similarity inversely correlates with their distance in the tree. In Figure 4(left) we plot the average cosine\nsimilarity between the node embeddings of a node and its descendants as a function of their tree distance (expressed\nas their hop distance). Cosine similarity clearly decreases as the distance increases, demonstrating that the learned\nembeddings reflect the tree structure. See Appendix B.4 for an extended analysis.\nNext, we assess whether semantically similar contexts are assigned to nearby nodes. To do so, we measure the average\ncosine similarity between all pairs of context embeddings (extracted with BGE-large) grouped by the depth of their"}, {"title": "Topics and Keywords Coherence", "content": "To further inspect the learned structure in a human-understandable way, we resort to topic models for extracting and\nanalyzing the distribution of topics and keywords of the contexts assigned to nodes across the tree. To better understand\nhow RETREEVER organizes contexts, we present a visualization of the RETREEVER tree trained on NQ, highlighting\nthe topics and keywords associated with several nodes (Figure 2). For each node t, we collect all contexts assigned to\nthe leaves of the subtree rooted at t and extract topics and keywords using the method from Kilgarriff [2009]. A quick\ninspection of these keywords reveals the hierarchical structure reflects the semantic relationships of the contexts. For\nexample, the node whose contexts are on the topic of media (node 5) has child nodes focused on publishing and TV.\nFurthermore, the path from node 5 to node 54 illustrates a consistent refinement of topics and keywords, progressing\nfrom media down to Television seasons."}, {"title": "Discussion and Future Work", "content": "In this work, we introduced a retrieval system that generates tree-based representations optimized for retrieval. Our\napproach provides flexible control over the trade-off between representation size, inference latency, and retrieval\naccuracy. Unlike black-box models, RETREEVER allows us to inspect the model by examining how information is\ntraversed and stored at different levels. A natural extension of this work is to learn the tree structure including the width\nand depth adaptively, or pruning the tree dynamically based on retrieval needs. Another direction is to develop tools\nfor analyzing the hierarchical structure, such as a summary decoder that explains what a node represents based on\nits learned embeddings. An important challenge is adapting a learned tree to new tasks or datasets-whether certain\nsubtrees can be updated or new ones grown while keeping the rest of the tree unchanged, or if full retraining is necessary."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal\nconsequences of our work, none of which we feel must be specifically highlighted here. We however believe that\ncontributing towards transparent and verifiable retrievers should benefit society in the long term, and is essential in\nfostering trust and accountability of retrieval systems."}, {"title": "RETREEVER's Additional Design Options", "content": "RETREEVER can be instantiated in many different ways. In this section we report a selection of the design choices we\nexplored in our experiments and that are implemented in our codebase."}, {"title": "Split Functions", "content": "A split function se\u2081 : X \u2192 [0,1] determines the routing probability of an input x\u2081 \u2208 X through node t. The primary\nrequirement for a split function is that it outputs a scalar \u2208 R, based on which we compute the probabilities of routing\nthe input to node t's left or right children tleft and tright (see Appendix A.3). Below, we describe different types of split\nfunctions that can be used."}, {"title": "Linear Split Function", "content": "The simplest form of a split function is a linear projection, similar to the approach used in Zantedeschi et al. [2021]. For\na given split node t, with left and right children tleft and tright, the split function is defined as:\n$\\text{S}_{\\theta_t} (x_i) = \\theta_t x_i,$\nwhere \u03b8t \u2208 X represents a learnable hyperplane. Each split node learns a separate hyperplane, and there are no shared\nparameters across the tree."}, {"title": "MLP Split Function", "content": "A more expressive alternative is to use a learnable neural network, modeled as a Multi-Layer Perceptron (MLP). This\nMLP Se : X \u2192 R|TB| maps an input x\u2081 to a routing probability for each branching node in the tree, where TB is the\nset of branching nodes. The MLP consists of multiple layers with nonlinear activations, such as ReLU, and incorporates\ndropout for regularization. Unlike the linear split function, which maintains separate parameters per node, the MLP\nsplit function shares parameters across different nodes while still allowing for node-specific learning."}, {"title": "Cross-Attention Split Function", "content": "While the linear and MLP split functions operate on dense passage embeddings for the entire document, the cross-\nattention split function allows us to leverage token-level embeddings for more expressive routing. This method utilizes\na cross-attention mechanism between learnable node embeddings and text tokens to determine the routing probabilities\nat each split node.\nLet the input text xi \u2208 Rndxdemb consist of na embedded tokens, encoded by the encoder E. Instead of a simple\nprojection, we introduce learnable node embeddings et \u2208 Rne\u00d7d'emb for each node t. These node embeddings interact\nwith the text tokens via a cross-attention mechanism, where the node embeddings serve as queries, and the text tokens\nprovide keys and values.\nWe define the attention mechanism as follows:\nAttention(Q, K, V) = softmax $\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V,$\nwhere:\nQ = $e_t W_q^T$\nK = $x_i W_k$\nV = $x_i W_v$\nHere, Wq \u2208 Rdk \u00d7 d'emb, Wk \u2208 Rdk\u00d7demb, and W \u2208 Rdk\u00d7demb are learnable projection matrices shared across the tree,\nwhile dk represents the dimension of the projected queries and keys. This formulation describes a single-head attention\nmechanism but can be naturally extended to multi-head attention by introducing independent projection matrices for\neach head and concatenating the resulting outputs.\nThe transformed node embeddings are then aggregated via a node-specific function to produce the final routing score.\nThis aggregation is discussed in detail in Appendix A.2, and shown in Figure 5. This mechanism significantly increases\nthe expressivity of the split function compared to a simple linear projection. The node embeddings and projection"}, {"title": "Cross-attention score aggregation", "content": "The cross-attention split function, described in Appendix A.1.3, uses node embeddings to compute a score for each\nnode. This function consists of two components:\n1. The cross-attention mechanism, which processes the input by attending to node embeddings as described\nabove.\n2. The node scorer, which outputs a single score for each node based on the output of the attention mechanism.\nHere, we discuss different choices for the node scorer.\nFor a given sample, the output y of the cross-attention has the shape [nt, ne, dk], where nt is the number of tree nodes,\nne is the number of embeddings per node, and dk is the dimension of the attention output projection. Since we need a\nsingle score per tree node, y must be reduced to the shape [nt]. This transformation can be performed in multiple ways,\nas detailed below:\n\u2022 Mean then Linear: In this approach, we first aggregate all embeddings for a given node by performing mean\npooling over the ne dimension, resulting in a single embedding per node. A shared linear layer is then applied\nto map this embedding into a score, producing a final score for each node.\n\u2022 Per-Node Linear Map, then Mean of Scores: Instead of using a shared transformation, this method learns a\nseparate linear function for each node to map its ne embeddings into individual scores. The final score for\nthe node is then computed as the mean of these scores. This approach allows each node to focus on different\naspects of its embeddings, leading to more flexible representations. This scoring is illustrated in Figure 5.\n\u2022 Incorporating Tree Structure: This method extends the previous approach by modifying the computed node\nscores to account for the hierarchical structure. Specifically, in addition to the score obtained per node, we\nrefine these scores using a small MLP trained per node. This MLP takes as input the scores of the node and all\nof its ancestors, and outputs a modified score, allowing the scoring mechanism to incorporate hierarchical\ninformation learned by the tree."}, {"title": "Tree propagation.", "content": "Note that any split function defined above outputs a score that is independent of the scores from the other nodes.\nHowever, the probability of reaching a node should be constrained by the probability of having reached its parent, the\nparent of its parent, and so on. The simplest way of enforcing such tree constraints is by propagating the scores of the\nancestors down to the node of interest by multiplying the probabilities along the path. We refer to this type of tree\npropagation as product propagation. Consider a node t, its left and right children tleft and tright, and its ancestors At\n(the split nodes along the path from the root to t). The probabilities of routing an input left or right based on node t's\nsplit score are:\n$\\text{Z}_{t_{\\text{left}}} (X_i) = \\sigma(s_{\\theta_t}(X_i))$\n$\\text{Z}_{t_{\\text{right}}} (X_i) = 1 \u2013 \\sigma(s_{\\theta_t}(X_i))$\nwhere \u03c3() is the sigmoid function. Applying the sigmoid in this way ensures that the output probabilities of a split node\nalways form a valid distribution (non-negative densities that sum to 1). Then, we constrain the probability of reaching\nnode tleft (and similarly node tright) is defined as:\n$\\text{T}(X_i)_{t_{left}} = Z_{t_{left}}\\prod_{a \\in A_t} Z_a.$"}]}