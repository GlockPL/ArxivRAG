{"title": "Evaluating Robustness of LLMs on Crisis-Related Microblogs across Events, Information Types, and Linguistic Features", "authors": ["Muhammad Imran", "Kai Chen", "Abdul Wahab Ziaullah", "Ferda Ofli"], "abstract": "The widespread use of microblogging platforms like X (formerly Twitter) during disasters provides real-time information to governments and response authorities. However, the data from these platforms is often noisy, requiring automated methods to filter relevant information. Traditionally, supervised machine learning models have been used, but they lack generalizability. In contrast, Large Language Models (LLMs) show better capabilities in understanding and processing natural language out of the box. This paper provides a detailed analysis of the performance of six well-known LLMs in processing disaster-related social media data from a large-set of real-world events. Our findings indicate that while LLMs, particularly GPT-40 and GPT-4, offer better generalizability across different disasters and information types, most LLMs face challenges in processing flood-related data, show minimal improvement despite the provision of examples (i.e., shots), and struggle to identify critical information categories like urgent requests and needs. Additionally, we examine how various linguistic features affect model performance and highlight LLMs' vulnerabilities against certain features like typos. Lastly, we provide benchmarking results for all events across both zero- and few-shot settings and observe that proprietary models outperform open-source ones in all tasks.", "sections": [{"title": "1 Introduction", "content": "Microblogging platforms like X (formerly Twitter) are vital during large-scale disasters [30]. They facilitate real-time communication for the public to share firsthand experiences, report damage to infrastructure, and most importantly, seek assistance [23, 2]. Moreover, local governments are increasingly leveraging these non-traditional data sources to enhance their situational awareness and quickly identify humanitarian needs, and inform their response strategies accordingly [28, 20].\nDespite their accessibility, data from social media platforms are often highly noisy [13]. During large-scale disasters, the volume of messages can reach millions per day, filled with irrelevant content and chatter [6]. This deluge makes it challenging for local authorities to identify reports critical for humanitarian response. Previous research has addressed this issue by developing supervised"}, {"title": "2 Related Work", "content": "In crisis informatics literature, several studies introduced large-scale crisis-related microblog datasets and presented baseline results using both classical machine learning algorithms (e.g., Random Forest, Support Vector Machines, etc.) as well as deep learning models (e.g., RNNs, LSTMs, CNNs, etc.) [24, 17]. Later, researchers undertook an effort to consolidate available datasets and tasks for benchmarking transformer-based models such as BERT [9], DistilBERT [27] and ROBERTa [21], and showed that the transformer-based models typically outperform [4]. A more comprehensive crisis-related dataset along with benchmarking results were presented in [3]. Likewise, a more recent study [31] presented a BERT-based ensemble model, FF-BERT, for the classification of flash flooding messages. Their evaluations examined various BERT-based ensemble models on a specially curated dataset of 21,180 paragraphs of text. Meanwhile, [12] developed QuakeBERT and showed better performance to assess physical and social impacts of an earthquake through microblogs.\nPrevious research has shown that transformer-based models outperform traditional ML algorithms on various metrics. Recent efforts have focused on using more powerful LLMs across diverse fields and tasks. For instance, LLMeBench has assessed LLMs on multiple NLP tasks such as sentiment analysis and summarization [8]. Additionally, studies like [35] have applied LLMs to crisis-related tasks, evaluating models like Mistral 7B [18] for their ability to analyze disaster-related tweets. Further, Llama-2 and Mistral have been fine-tuned for disaster response guidance, as presented in [26]. This paper builds upon these findings by analyzing LLMs on a crisis-related dataset, exploring LLMs' performance across various disaster types, information types, and the linguistic features of the messages, to identify their capabilities and weaknesses."}, {"title": "3 Assessment Methodology", "content": "The increasing complexity and frequency of natural disasters worldwide necessitate Al models, particularly LLMs, that can effectively generalize across various types of disasters (e.g., floods,"}, {"title": "3.1 Dataset and Models", "content": "To answer our research questions, we utilize HumAID [3] dataset comprising 77,196 tweets from 19 different natural disasters that occurred in 11 distinct countries (3 native English-speaking and 8 non-English-speaking) between 2016 to 2019. The tweets in the dataset are labeled by paid crowdsourcing workers into ten distinct information categories (acronyms): (1) caution and advice (CA)\u2014(2) sympathy and support (SS)\u2014(3) requests or urgent needs (RUN)\u2014(4) displaced people and evacuations (DPE)\u2014(5) injured or dead people (IDP)\u2014(6) missing or found people (MFP)\u2014(7) infrastructure and utility damage (IUD)\u2014(8) rescue volunteering or donation effort (RVDE)\u2014(9) other relevant information (ORI)\u2014 and (10) not humanitarian (NH). We drop the \u201cother relevant information\" class from our analysis as it mainly contains general event-related information that does not belong to other categories. The dataset is already split into train, development, and test sets. We use the test split (N=15,160) for our experiments. \nModels We select six well-known LLMs (three proprietary and three open-source) for this study. We choose GPT-3.5 [5], GPT-4 [1], and GPT-40 [25] from OpenAI as our proprietary models and Llama-2 13B [29], Llama-3 8B [10] and Mistral 7B [18] as our open-source models. All six models are known for their language understanding capabilities across various NLP tasks."}, {"title": "3.2 Experimental Design", "content": "We evaluate the LLMs for the classification task in two settings: zero-shot and few-shot. In the zero-shot setting, the models operate without any class-specific examples, relying solely on their pre-trained capabilities to perform the task. In the few-shot setting, models receive examples for each class to improve class-specific performance. For instance, in a three-shot experiment, we provide the model with three carefully selected tweets per class from the training set, totaling 30 examples for ten classes. For all experiments, we set the temperature parameter to zero. We use the following prompt across all experiments, except for Llama-2 and Mistral, where we provide additional instructions to control for verbosity."}, {"title": "4 Results and Discussion", "content": null}, {"title": "4.1 Disaster Type Analysis", "content": "Our first research question examines how LLMs perform across different types of disasters. We analyze data from 19 events, grouped into four event types: 5 earthquakes, 7 hurricanes, 3 wildfires, and 4 floods. We present results for both proprietary and open-source models and compare their performance in zero-shot and few-shot (i.e., 1, 3, 5, and 10) settings."}, {"title": "4.2 Information Type Analysis", "content": "Our second research question examines LLMs' capabilities in processing diverse types of information related to humanitarian response and situational awareness during disasters. Our analysis contains nine distinct information categories, detailed in Section 3.1, with their cumulative distribution across all events depicted in Figure 1(b). While we conducted experiments across all six models in all few-shot settings (except for Llama-2 10-shot), the following results focus solely on the two top-performing models, GPT-40 and Mistral, from the proprietary and open-source categories, respectively. The complete set of results, including all six models, are provided in Appendix A."}, {"title": "4.3 Native vs. Non-Native English Analysis", "content": "Our third research question examines the performance of LLMs in processing social media content from native-English-speaking versus non-English-speaking countries. Our dataset includes 11 events from native-English-speaking countries and 8 events from non-English-speaking countries.\nFigure 5 displays the F1-scores for all models, including both proprietary and open-source. It is evident that all models achieve better performance in processing data from native-English-speaking"}, {"title": "4.4 Linguistic Feature Analysis", "content": "Our fourth research question explores whether various linguistic features, such as word count, hashtag count, and emoji usage in tweets, affect the performance of LLMs. Previous studies have shown that such features significantly influence the performance of traditional machine learning and deep learning models [7]. We aim to determine if this holds true for LLMs, as well. We defined 17 linguistic features and analyzed their frequency distributions across all classes. We observed that tweets discussing requests and urgent needs often include more mentions of other user accounts, particularly NGOs and official accounts.\nNext, we perform a logistic regression analysis to ascertain how different linguistic features affect model performance. To avoid the undesirable effects of multicollinearity, we exclude highly correlated linguistic features like character, word, and alphabet counts and work with a reduced set of features as our independent variables and consider the binary (correct/incorrect) validation of the predicted class labels with the ground truth as our dependent variable. we see that numbers, hashtags, mentions, face, and heart emojis have positive correlations with model performance whereas character, special character and typo counts have the opposite effect. For example, with a relatively small but statistically significant coefficient, increasing character counts tends to negatively impact model performance. On the contrary, again with relatively small but statistically significant coefficients-, number, hashtag, and mention counts play positive roles in improving predictive performance."}, {"title": "Hashtag Positioning Impacts", "content": "Next, we investigate whether the placement of hashtags within messages affects LLM performance. We categorize messages into three groups: (i) messages with hashtags only (strictly) at the beginning, (ii) messages with hashtags in the middle, and (iii) hashtags only (strictly) at the end. Interestingly, we observed that hashtags placed in the middle of messages frequently result in higher error rates. Most probably, these errors stem from the disruption in sentence structure caused by mid-sentence hashtags, which can confuse models by introducing unexpected breaks or context shifts. This phenomenon appears more pronounced in proprietary models compared to open-source models. Specifically, proprietary models such as GPT-3.5 (in all shot settings except zero-shot), GPT-4, and GPT-40 consistently exhibited difficulties in accurately interpreting messages with mid-sentence hashtags. The models often misclassify or overlook critical context surrounding the hashtag, leading to erroneous predictions. Additionally, there is a notable difference in performance for GPT-4, GPT-4o, and Llama-3 zero-shot and Mistral 10-shot configurations when hashtags are exclusively at the beginning of messages. Conversely, the positioning of hashtags at the end of messages does not significantly affect LLMs' performance."}, {"title": "4.5 Event-wise and Overall Performance", "content": "In addition to our main analyses addressing the specified research questions, we conduct two additional experiments to assess LLMs' performance for individual events and their overall performance. These results also help benchmark the LLMs against this dataset."}, {"title": "5 Ethical Considerations", "content": "The datasets used in this study consist of publicly available tweets posted by individuals or organizations during various natural disasters. The data was collected in strict adherence to the terms and conditions set forth by the Twitter (now X) API to ensure ethical compliance. To safeguard individuals' privacy, any personally identifiable information, including names, addresses, phone numbers, or other sensitive details, was systematically anonymized before data processing. Moreover, no attempts were made to infer or store additional demographic or personal information about the users."}, {"title": "6 Conclusion and Future Work", "content": "We presented a comprehensive evaluation of prominent large language models in processing social media data from 19 major natural disasters across 11 countries, including 8 non-native and 3 native English-speaking regions. Our findings highlight varying strengths and limitations of LLMs in managing diverse disaster types, information categories, and linguistic complexities. Specifically, the models demonstrated notable difficulties with flood-related data and frequently misclassified critical information categories such as requests and urgent needs and caution and advice. Furthermore, our analysis identified key factors such as message length, typographical errors, and the presence of special characters as significant challenges that impair model performance. Importantly, we observed that providing few-shot examples yielded limited performance gains for most models. This could be due to the high variability in social media content, even from the same class. Finally, we provided benchmarking results, aiming to inform further research into LLMs' vulnerabilities and assist in developing more robust models for disaster information processing.\nFuture work: We aim to extend our qualitative analyses to understand the reasons behind LLMs' underperformance for specific disaster types and information categories, with a focus on identifying actionable solutions to address these issues. Beyond text-based models, our future research will explore the potential of large vision-language models in processing multimodal social media data, such as combining textual and visual content, to provide a more holistic understanding of disaster events. This exploration is particularly relevant for enhancing emergency management systems in complex real-world scenarios."}, {"title": "A Class-wise results", "content": "Tables 4 and 5 present the class-wise results across various shots for all the models."}]}