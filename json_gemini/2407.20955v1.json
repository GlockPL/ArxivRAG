{"title": "EMOTION-DRIVEN PIANO MUSIC GENERATION VIA TWO-STAGE\nDISENTANGLEMENT AND FUNCTIONAL REPRESENTATION", "authors": ["Jingyue Huang", "Ke Chen", "Yi-Hsuan Yang"], "abstract": "Managing the emotional aspect remains a challenge in\nautomatic music generation. Prior works aim to learn var-\nious emotions at once, leading to inadequate modeling.\nThis paper explores the disentanglement of emotions in\npiano performance generation through a two-stage frame-\nwork. The first stage focuses on valence modeling of lead\nsheet, and the second stage addresses arousal modeling by\nintroducing performance-level attributes. To further cap-\nture features that shape valence, an aspect less explored by\nprevious approaches, we introduce a novel functional rep-\nresentation of symbolic music. This representation aims to\ncapture the emotional impact of major-minor tonality, as\nwell as the interactions among notes, chords, and key sig-\nnatures. Objective and subjective experiments validate the\neffectiveness of our framework in both emotional valence\nand arousal modeling. We further leverage our framework\nin a novel application of emotional controls, showing a\nbroad potential in emotion-driven music generation.", "sections": [{"title": "1. INTRODUCTION", "content": "With the recent advancements in symbolic music genera-\ntion [1-6], there has been a growing interest in controlling\nhigh-level musical features throughout the generation pro-\ncess. Among these features, emotion-driven music gener-\nation [7-13] aims to generate music that conveys specific\nemotions, representing a crucial aspect for music appreci-\nation and analysis. The downstream applications of such\nmodels have also been explored, such as music therapy for\nhealthcare and educational purposes [14] and soundtrack\ngeneration for videos and movies [15].\nEmotion could be represented in two dimensions from\nthe literature [16]: valence and arousal. Valence refers to\nthe positiveness of an emotion and arousal refers to energy\nor activation [17-19]. These two dimensions can be fur-\nther divided into four quadrants (4Q), namely high valence\nhigh arousal (Q1), low valence high arousal (Q2), low va-\nlence low arousal (Q3), and high valence low arousal (Q4).\nIn this paper, we focus on the emotion-driven piano per-\nformance generation of these four quadrants. Throughout\nprior works, we observe crucial challenges from the per-\nspectives of both model design and musical inductive bias.\nFirst, previous emotion-driven piano performance gen-\neration models [7, 8] attempt to learn emotion quadrants\nand expressions in an end-to-end paradigm. In terms of\nmodel design, this approach poses training difficulty on the\ngeneration model, leading to the instability in achieving\nresults of desired emotions. For example, many existing\nworks [7, 9, 11] could effectively control the arousal lev-\nels of music, while their performance of valence modeling,\nespecially in generating low valence (i.e., negative) music,\nis still poor. In terms of music, the creation process of\nmusic typically involves multiple stages, such as the lead\nsheet composition for melodies and chord progressions,\nand performance generation for textures and expressive-\nness. Consistently, emotion can be evoked through a com-\nbination of musical elements (e.g., melody, chord, texture).\nFor example, major/minor chords have been found to seize\ndifferent valence trends in psychological studies [20] and\nperformance-level attributes like articulation, tempo, and\nvelocity are more related to arousal [21, 22]. It is worth to\nexplore the potential relation between the disentanglement\nof the generation process and the emotion expression.\nSecond, previous emotion-driven generation models\nhave received limited attention regarding the influence of\ntonality on emotion modeling. It has been widely shown\nthat major-minor tonality in composition is highly related\nto valence perception [22-25]. For example, as depicted in\nFigure 1, the histogram of musical keys derived from the\nemotion-labeled music dataset EM\u039fPIA [7] supports the\ndistribution skews to major keys for high valence clips and\nopposite trend for low valence ones. Furthermore, different\ntonalities may reveal similar patterns in the relative rela-\ntionships between melodies and chords, while the distribu-\ntion of melodies, chords, and tonalities can exhibit distinct\nshapes across different emotions. Current representations\nof symbolic music, such as REMI [2] and CP-Word [26],\ndo not explicitly incorporate such interactions nor address\nits connection to emotion adequately. Therefore, it is nec-\nessary to consider a functional format of symbolic music\nrepresentation considering the relationships between notes,\nchords and key signatures to better model the tonality in the\nemotion-driven music generation process.\nIn this paper, we contribute to combat above challenges:"}, {"title": "\u2022", "content": "We employ a two-stage Transformer-based model on\nemotion-driven piano performance generation. The first\nstage focuses on valence modeling via lead sheet com-\nposition, while the second stage addresses arousal mod-\neling by introducing performance-level attributes."}, {"title": "\u2022", "content": "We propose a novel functional representation for sym-\nbolic music, encoding both melody and chords with Ro-\nman numerals relative to musical keys, to consider the\ninteractions among notes, chords and tonalities [27]."}, {"title": "\u2022", "content": "Experiments demonstrate the effectiveness of our frame-\nwork and representation on emotion modeling. Addi-\ntionally, our method enables new capabilities to con-\ntrol the arousal levels of generation under the same lead\nsheet, leading to more flexible emotion controls."}, {"title": "2. RELATED WORK", "content": "2.1 Emotion-driven Piano Performance Generation\nPrior works apply emotion conditions on deep-learning\nmodels to guide the generation of piano performance [7, 8,\n11], or develop searching methods to generate music of de-\nsired emotions [28, 29]. Musical elements via feature dis-\nentanglement [9] or supervised clustering [10] can further\nbe regarded as a bridge between emotion labels and per-\nformances for generation. In contrast, our framework em-\nploys a two-stage generation approach to reduce the com-\nplexities of one-stage generation, fostering a more nature\nprocess of music creation as well as a better incorporation\nbetween emotion labels and generation results."}, {"title": "2.2 Tonality, Functional Harmony, and Emotion", "content": "Musical keys and functional harmony have been explored\nin the field of roman numeral analysis [30-32]. The analy-\nsis of how modes and tonalities relate to mid-level percep-\ntual features (e.g., dissonance, tonal stability, minorness)\nand affect the emotional perception of music pieces has\nalso been discovered [22, 24].\nWhile some music generation works attempted to com-\nbine key information into data representation [33], loss\nfunction [34] and text conditions [6], none of them ex-\nplore the relation between musical keys and emotional per-\nception. In this paper, we leverage both functional har-\nmony knowledge and class-octave based pitch representa-\ntion [35] to design a new data representation, incorporat-\ning the relationships between notes, chords and keys for\nemotion-driven music generation."}, {"title": "3. METHOD", "content": "In this section, we will first introduce the functional rep-\nresentation of symbolic music as the main generation unit.\nThen we introduce the two-stage model as the main com-\nponent of the emotion disentanglement and generation."}, {"title": "3.1 Functional Representation", "content": "Figure 2 illustrates our proposed functional representation.\nIts design is initially based on REMI [2], a widely used\nevent-based representation for symbolic music. We incor-\nporate different note and chord events assisting to better\nlearn the joint information of emotion and key signature."}, {"title": "3.1.1 Emotion and Key Events", "content": "We follow CTRL [36] to set up the condition within the\nautoregressive generation process in Transformer architec-\nture. To denote distinct emotions and affect overall prop-\nerties, we begin the event sequence with <Emotion_*>.\nevent to indicate the emotion label of music clips. The\n<Key_*> event is appended after <Emotion_*> to pro-\nvide the musical key property, with the total of 24 keys (12\ntonic notes with two modes in EMOPIA [7])."}, {"title": "3.1.2 Bar, Sub-Beat, Tempo and EOS Events", "content": "Similar to REMI, a <Bar> event denotes the new start of\na bar; a <Sub-Beat_*> event denotes one of 16 pos-\nsible discrete beat locations within a bar; a <Tempo_*>.\nevent denotes local tempo changes every four beats; and\nan <EOS> event denotes the end of sequence."}, {"title": "3.1.3 Chord Events", "content": "A musical chord name typically consists of root note and\nchord quality. For example, Fmaj represents the chord\nF-A-C with root F and major quality. Such symbols de-\nscribe correct note information in chord within the tonality,"}, {"title": "3.1.4 Note-related Events", "content": "A note is denoted by <Pitch_*>, <Duration_*> and\n<Velocity_*> events, where <Pitch_*> event indi-\ncates the onset of pitches from A0 to C8. Inspired by [35,\n37], we decompose <Pitch_*> into <Octave_*> and\n<Degree_*> events according to the note octave and de-\ngree in the certain key scale. The conversion rule from\n<Pitch_*> to <Degree_*> is the same as that of chord\nroots in Figure 3. For example, pitch D#4 is decom-\nposed into <Octave_4> and <Degree_III> in c mi-\nnor scale, but <Degree_I> in D# major scale. Such\ndegree-octave pitch representation narrows the difference\nbetween melodies, thus improves the learning of connec-\ntions between emotions, chords, and melodies, as demon-\nstrated in Figure 4."}, {"title": "3.2 Two-stage Emotion Disentanglement", "content": "We use the idea of Compose & Embellish [38] to generate\nmusic in two stages: lead sheet first, and then piano perfor-\nmance. While Compose & Embellish is emotion-agnostic,\nwe extend it so that the lead sheet model involves valence\nmodeling and the performance model arousal modeling."}, {"title": "3.2.1 Valence Modeling", "content": "The top left section of Figure 5 denotes the first stage,\nwhere only emotion events <Emotion_Positive> and\n<Emotion_Negative> are considered as conditions.\nThe former includes music pieces of Q1 and Q4 (high va-\nlence) and the latter includes those of Q2 and Q3 (low va-\nlence). The lead sheet model first predicts a key event k\nconditioned on the given emotion event e, and then gener-\nates the lead sheet sequence $M = {m_1,\\ldots,m_T}$ of length\nT, as melody and chord progression, conditioned on previ-\nous tokens step-by-step:\n$p(k, M|e) = p(k|e) \\prod_{t=1}^T p(m_t|e, k, M_{<t}),$ (1)\nwhere $p(k|e)$ and $p(m_t|e,k, M_{<t})$ are jointly learned\nthrough the Transformer-based generation model [26,\n38]. Performance-related events <Velocity_*> and\n<Tempo_*> are removed in the first stage (i.e., lead sheet\ngeneration), as we mainly focus on the contributions of\nkey, pitch and chord for valence perception."}, {"title": "3.2.2 Arousal Modeling", "content": "The top right section of Figure 5 denotes the second stage.\nGiven the lead sheet M, the performance model generates\nperformance X conditioned on the true emotion label (Q1\nto Q4). As the valence aspect has already been modeling\nin the first stage, this stage focuses on the generation of\nmusical textures for the lead sheet, and more importantly,\non how to perform it through variations of tempo, ve-\nlocity, articulation, and other performance-level attributes\nthat largely influence perceived arousal [21, 22", "interleaved": "n\nthe form of {\u2026\u2026<Track_M>, $M_i$, <Track_X>, $X_i$\u2026\u2026"}]}