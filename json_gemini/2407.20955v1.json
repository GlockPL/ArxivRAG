{"title": "EMOTION-DRIVEN PIANO MUSIC GENERATION VIA TWO-STAGE DISENTANGLEMENT AND FUNCTIONAL REPRESENTATION", "authors": ["Jingyue Huang", "Ke Chen", "Yi-Hsuan Yang"], "abstract": "Managing the emotional aspect remains a challenge in automatic music generation. Prior works aim to learn various emotions at once, leading to inadequate modeling. This paper explores the disentanglement of emotions in piano performance generation through a two-stage framework. The first stage focuses on valence modeling of lead sheet, and the second stage addresses arousal modeling by introducing performance-level attributes. To further capture features that shape valence, an aspect less explored by previous approaches, we introduce a novel functional representation of symbolic music. This representation aims to capture the emotional impact of major-minor tonality, as well as the interactions among notes, chords, and key signatures. Objective and subjective experiments validate the effectiveness of our framework in both emotional valence and arousal modeling. We further leverage our framework in a novel application of emotional controls, showing a broad potential in emotion-driven music generation.", "sections": [{"title": "1. INTRODUCTION", "content": "With the recent advancements in symbolic music generation [1-6], there has been a growing interest in controlling high-level musical features throughout the generation process. Among these features, emotion-driven music generation [7-13] aims to generate music that conveys specific emotions, representing a crucial aspect for music appreciation and analysis. The downstream applications of such models have also been explored, such as music therapy for healthcare and educational purposes [14] and soundtrack generation for videos and movies [15].\nEmotion could be represented in two dimensions from the literature [16]: valence and arousal. Valence refers to the positiveness of an emotion and arousal refers to energy or activation [17-19]. These two dimensions can be further divided into four quadrants (4Q), namely high valence high arousal (Q1), low valence high arousal (Q2), low valence low arousal (Q3), and high valence low arousal (Q4).\nIn this paper, we focus on the emotion-driven piano performance generation of these four quadrants. Throughout prior works, we observe crucial challenges from the perspectives of both model design and musical inductive bias.\nFirst, previous emotion-driven piano performance generation models [7, 8] attempt to learn emotion quadrants and expressions in an end-to-end paradigm. In terms of model design, this approach poses training difficulty on the generation model, leading to the instability in achieving results of desired emotions. For example, many existing works [7, 9, 11] could effectively control the arousal levels of music, while their performance of valence modeling, especially in generating low valence (i.e., negative) music, is still poor. In terms of music, the creation process of music typically involves multiple stages, such as the lead sheet composition for melodies and chord progressions, and performance generation for textures and expressiveness. Consistently, emotion can be evoked through a combination of musical elements (e.g., melody, chord, texture). For example, major/minor chords have been found to seize different valence trends in psychological studies [20] and performance-level attributes like articulation, tempo, and velocity are more related to arousal [21, 22]. It is worth to explore the potential relation between the disentanglement of the generation process and the emotion expression.\nSecond, previous emotion-driven generation models have received limited attention regarding the influence of tonality on emotion modeling. It has been widely shown that major-minor tonality in composition is highly related to valence perception [22-25]. For example, as depicted in Figure 1, the histogram of musical keys derived from the emotion-labeled music dataset EMOPIA [7] supports the distribution skews to major keys for high valence clips and opposite trend for low valence ones. Furthermore, different tonalities may reveal similar patterns in the relative relationships between melodies and chords, while the distribution of melodies, chords, and tonalities can exhibit distinct shapes across different emotions. Current representations of symbolic music, such as REMI [2] and CP-Word [26], do not explicitly incorporate such interactions nor address its connection to emotion adequately. Therefore, it is necessary to consider a functional format of symbolic music representation considering the relationships between notes, chords and key signatures to better model the tonality in the emotion-driven music generation process.\nIn this paper, we contribute to combat above challenges:\n\u2022 We employ a two-stage Transformer-based model on emotion-driven piano performance generation. The first stage focuses on valence modeling via lead sheet composition, while the second stage addresses arousal modeling by introducing performance-level attributes.\n\u2022 We propose a novel functional representation for symbolic music, encoding both melody and chords with Roman numerals relative to musical keys, to consider the interactions among notes, chords and tonalities [27].\n\u2022 Experiments demonstrate the effectiveness of our framework and representation on emotion modeling. Additionally, our method enables new capabilities to control the arousal levels of generation under the same lead sheet, leading to more flexible emotion controls.\nAs a minor contribution, we also refine key signature labels and extract lead sheet annotations for the EMOPIA dataset [7] to ensure the correct training of the two-stage framework. We share the data, open source our code\u00b9 and present generation samples in the demo page. 2"}, {"title": "2. RELATED WORK", "content": "Prior works apply emotion conditions on deep-learning models to guide the generation of piano performance [7,8, 11], or develop searching methods to generate music of desired emotions [28, 29]. Musical elements via feature disentanglement [9] or supervised clustering [10] can further be regarded as a bridge between emotion labels and performances for generation. In contrast, our framework employs a two-stage generation approach to reduce the complexities of one-stage generation, fostering a more nature process of music creation as well as a better incorporation between emotion labels and generation results.\nMusical keys and functional harmony have been explored in the field of roman numeral analysis [30-32]. The analysis of how modes and tonalities relate to mid-level perceptual features (e.g., dissonance, tonal stability, minorness) and affect the emotional perception of music pieces has also been discovered [22, 24].\nWhile some music generation works attempted to combine key information into data representation [33], loss function [34] and text conditions [6], none of them explore the relation between musical keys and emotional perception. In this paper, we leverage both functional harmony knowledge and class-octave based pitch representation [35] to design a new data representation, incorporating the relationships between notes, chords and keys for emotion-driven music generation."}, {"title": "3. METHOD", "content": "In this section, we will first introduce the functional representation of symbolic music as the main generation unit. Then we introduce the two-stage model as the main component of the emotion disentanglement and generation."}, {"title": "3.1 Functional Representation", "content": "Figure 2 illustrates our proposed functional representation. Its design is initially based on REMI [2], a widely used event-based representation for symbolic music. We incorporate different note and chord events assisting to better learn the joint information of emotion and key signature."}, {"title": "3.1.1 Emotion and Key Events", "content": "We follow CTRL [36] to set up the condition within the autoregressive generation process in Transformer architecture. To denote distinct emotions and affect overall properties, we begin the event sequence with <Emotion_*> event to indicate the emotion label of music clips. The <Key_*> event is appended after <Emotion_*> to provide the musical key property, with the total of 24 keys (12 tonic notes with two modes in EMOPIA [7])."}, {"title": "3.1.2 Bar, Sub-Beat, Tempo and EOS Events", "content": "Similar to REMI, a <Bar> event denotes the new start of a bar; a <Sub-Beat_*> event denotes one of 16 possible discrete beat locations within a bar; a <Tempo_*> event denotes local tempo changes every four beats; and an <EOS> event denotes the end of sequence."}, {"title": "3.1.3 Chord Events", "content": "A musical chord name typically consists of root note and chord quality. For example, Fmaj represents the chord F-A-C with root F and major quality. Such symbols describe correct note information in chord within the tonality, but they overlook the variations in chord functions of the same chord across different tonalities. For example, while Fmaj serves the tonic function in F major scale, it serves the subdominant function in C major scale. Moreover, the chord progression follows these functional harmony rules to establish tonality and convey musical emotion [27].\nTo introduce chord functions in the emotion modeling, we adopt Roman numerals from Roman Numeral Analysis [31] to notate chord roots in Figure 3. Given the <Key_*> event, root notes in the absolute pitch are directly converted into Roman numerals based on their scale degrees relative to the key (i.e., relative pitch). For roots outside the scale, we employ a direct conversion for I#, II#, IV#, V# and VI# appearing in major keys, but randomly assign III# and VII#, which only appear in minor keys, as one of their neighboring degrees during the encoding and decoding process. This design ensures the notation to be key-independent and make every conversion of notes reasonable to the music theory. The notations of chord qualities remain unchanged, and the chord event <Chord_*> appears every four beats."}, {"title": "3.1.4 Note-related Events", "content": "A note is denoted by <Pitch_*>, <Duration_*> and <Velocity_*> events, where <Pitch_*> event indicates the onset of pitches from A0 to C8. Inspired by [35, 37], we decompose <Pitch_*> into <Octave_*> and <Degree_*> events according to the note octave and degree in the certain key scale. The conversion rule from <Pitch_*> to <Degree_*> is the same as that of chord roots in Figure 3. For example, pitch D#4 is decomposed into <Octave_4> and <Degree_III> in c minor scale, but <Degree_I> in D# major scale. Such degree-octave pitch representation narrows the difference between melodies, thus improves the learning of connections between emotions, chords, and melodies, as demonstrated in Figure 4."}, {"title": "3.2 Two-stage Emotion Disentanglement", "content": "We use the idea of Compose & Embellish [38] to generate music in two stages: lead sheet first, and then piano performance. While Compose & Embellish is emotion-agnostic, we extend it so that the lead sheet model involves valence modeling and the performance model arousal modeling."}, {"title": "3.2.1 Valence Modeling", "content": "The top left section of Figure 5 denotes the first stage, where only emotion events <Emotion_Positive> and <Emotion_Negative> are considered as conditions. The former includes music pieces of Q1 and Q4 (high valence) and the latter includes those of Q2 and Q3 (low valence). The lead sheet model first predicts a key event k conditioned on the given emotion event e, and then generates the lead sheet sequence M = {m\u2081,\u2026mr} of length T, as melody and chord progression, conditioned on previous tokens step-by-step:\n$p(k, Me) = p(ke) \\prod_{t=1}^{T}p(m_{t}|e, k, M_{<t}),$ (1)\nwhere p(kle) and_p(mt|e,k, M<t) are jointly learned through the Transformer-based generation model [26, 38]. Performance-related events <Velocity_*> and <Tempo_*> are removed in the first stage (i.e., lead sheet generation), as we mainly focus on the contributions of key, pitch and chord for valence perception."}, {"title": "3.2.2 Arousal Modeling", "content": "The top right section of Figure 5 denotes the second stage. Given the lead sheet M, the performance model generates performance X conditioned on the true emotion label (Q1 to Q4). As the valence aspect has already been modeling in the first stage, this stage focuses on the generation of musical textures for the lead sheet, and more importantly, on how to perform it through variations of tempo, velocity, articulation, and other performance-level attributes that largely influence perceived arousal [21, 22]. During the training and inference phases, with the positions of <Bar> events, M and X are further segmented into {M1,..., Mb} and {X1,\uff65\uff65\uff65, Xb}, where b is the number of bars. The segmented sequences are \u201cinterleaved\" in the form of {\u2026\u2026<Track_M>, Mi, <Track_X>, X\u00a1\u00a8\u00a8\u00a8 } with additional <Track_*> events to distinguish M and X tracks. In that, the target performance bar Xi is appended to its corresponding conditions Mi, as mapping each lead sheet segment to its corresponding performance segment [26]. With the emotion condition and key event from lead sheet as prefix tokens, the performance model is summarized as\n$p(X|e, k, M) = \\prod_{i=1}^{b}p(X_{i}|e, k, M_{<i}, X_{<i}) .$ (2)"}, {"title": "3.2.3 Training Objectives", "content": "Lead sheet and performance models are trained separately by both optimizing the negative log-likelihood loss of the sequence. Since existing emotion-labeled music datasets are not large, we leverage large-scale music datasets without emotion annotations to pretrain both models for better music understanding. During pretraining, the emotion event is marked as <Emotion_None>. We then finetune two models on the emotion-labeled dataset (detail in Section 4) to learn composition and performance styles specific to different emotion contexts."}, {"title": "3.2.4 Two-stage Inference", "content": "The left bottom section of Figure 5 denotes the inference process of both models. In the first stage, the lead sheet model predicts the key event and generates the lead sheet sequence step-by-step given <Emotion_Positive> or <Emotion_Negative> event, creating a musical motif for the specific valence preference. Even though our framework has the capability to generate any-key music of specific emotions, we observe that some generation results, such as a high-valence and high-arousal song with a minor key scale, may go beyond the current definition of emotion in [16], where the valence naturally has a strong correlation to the major-minor tonality (Figure 1). Therefore, we limit major keys to <Emotion_Positive> and minor keys to <Emotion_Negative> during the inference stage. We acknowledge that this can be overly simplifying. Since this paper focuses mainly on the valence-arousal disentanglement during the generation process, we leave this exploration of generating any emotion within any key as an advanced topic for future research.\nIn the second stage, the performance model generates piano performance with desired valence and arousal combination given the lead sheet from the first stage. For example, to generate a music piece of Q3, a \u201cNegative\u201d lead sheet and a \"Q3\" emotion event are selected as conditions. Additionally, this two-stage framework enables the flexibility to generate different arousal levels of piano performance under the same lead sheet, delivering some scenarios when the music need to shift quickly to complement the scenes in movies or daily videos (detail in Section 4 and the demo page)."}, {"title": "4. EXPERIMENTS", "content": "As presented in Table 1, we collect different datasets for pretraining and finetuning phases as mentioned in Section 3.2.3. For pretraining the lead sheet model, we use the HookTheory dataset [39,40], where we choose 18,206 lead sheets with high-quality and human-transcribed melody, chord and key annotations in 4/4 time signature. We simplify 249 chord quality classes into 11 types 3 as the same set in the other datasets below. For pretraining the performance model, we use the Pop1k7 dataset [26], consisting of 1747 transcribed pop piano performances. Since Pop1k7 does not contain lead sheet annotations, we refer [38] to extract melodies using the skyline algorithm [41], recognize chords using the chorder library [42], and detect key signatures using [43] in MIDI Toolbox [44].\nFor finetuning the models with emotion conditions, we use the EMOPIA dataset [7], consisting of 1,071 music clips with human-annotated emotion labels. Similar to Pop1k7, we obtain the lead sheets of EMOPIA by extracting melodies using the algorithm in [45] and recognizing chords using the algorithm in [46]. Empirically, we observe that specifically in the EMOPIA dataset, melodies and chords extracted by these alternative algorithms are more correct compared to the skyline algorithm and the chorder library. Additionally, we found the key signature labels in EMOPIA are not fully correct since they are also obtained by the detection algorithm with error rates. Since the valence modeling is strongly related to the musical keys and modes, we manually correct the key annotations of 367 clips in EMOPIA to ensure a high quality of lead sheets.\nAll datasets are randomly divided into respective training and validation sets at the ratio of 9:1. As a result in our functional representation, the vocabulary size of events is 215 for lead sheet and 324 for piano performance."}, {"title": "4.2 Model Settings", "content": "The lead sheet model is a 12-layer Transformer Decoder [47] with 8 heads, 512 hidden dimensions and relative positional encoding [48]. The performance model is similar to the lead sheet model except with Performer attention [49]. The total parameter sizes are 41 million and 38 million respectively.\nBoth models are trained with the batch size of 4, the maximum sequence length of 512 (lead sheet model) or 3072 (performance model), and the Adam optimizer with \u03b2 = (0.99, 0.9). We adopt a 200-step warm-up to achieve the maximum learning rate of 1e-4 for pretraining and 1e-5 for finetune. All models are implemented by PyTorch and trained on one NVIDIA Tesla V100 GPU. The lead sheet model took around 180,000 steps to converge and the performance model took around 200,000 steps. Nucleus sampling [50] is employed in the inference phase. We referred [38,51] to choose the sampling hyperparameters \u03c4 = 1.2, p = 0.97 for the lead sheet model and T = 1.1, p = 0.99 for the performance model."}, {"title": "4.3 Baseline and Ablations", "content": "We consider the emotion-driven piano performance generation model in EMOPIA [7] as our baseline, which generates music in an end-to-end paradigm instead of two stages. To ensure the fair comparison of generation performance, we trained the baseline model under ths same datasets in both pretraining and finetune phases, and replaced the original CP-Word representation with REMI as the former usually yields better generation performance and more comparable to our proposed functional representation. Two other related works [8, 9] are not included in comparison due to the main reason that we focus more on the evaluation of the two-stage framework in valence and arousal modeling; and the partial reason that they are not open-source or releasing the reproducible model weights.\nWe conduct a comprehensive ablation study to evaluate if each proposed design benefits the emotion modeling of music generation. Specially, these designs include: 1) the two-stage generation, 2) the functional representation, and 3) the dataset pretraining. In the following sections, models are denoted as <representation(stage)>. For example, REMI(one) denotes the one-stage generation model with REMI representation as the baseline, and REMI(two) denotes the two-stage generation as one variant."}, {"title": "4.4 Objective Evaluation and Results", "content": "Even though previous studies [7\u20139] employ metrics, such as Pitch Range (PR) and Number of Pitch Classes (NPC), to evaluate the generation performance, they do not provide any evidences on the superiority of melody development, chord progression, and texture arrangement of music. Therefore, a model with more similar PR and NPC values to those of the target dataset does not necessarily promise a better generation quality than others.\nInstead of using such metrics, we wish to evaluate the consistency between the input musical conditions and the generation results. We introduce key consistency to assess if a model can generate music pieces that adhere to the desired input key signatures, which is highly correlated to the lead sheet development and valence modeling. Specifically, key consistency measures the match between the key condition <Key_*> and the actual key detected in the generation using the algorithm [44] with an 81% accuracy rate. We compare REMI and our functional representation to determine if the functional representation can improve the key consistency via more close and interactive designs on key, melody, and chord. Since this metric requires the key as conditions, we add the <Key_*> event in REMI after <Emotion_*> (as REMI+key in Table 2) when training the model. The non-pretrained versions are also included for comparison. Each model generates 200 lead sheets (100 high and 100 low valence) and 400 performance samples (100 per emotion quadrant) for evaluation. From Table 2, the functional representation outperforms REMI (i.e., REMI+key) across all components and even achieves compatible accuracy to real data over the lead sheet component. This demonstrates the effectiveness of the functional representation, by representing notes and chord roots relative to key events for key modeling. In contrast, REMI struggles with associating chord events with keys due to the ignorance of chord labels serving different functions in different key scales. Moreover, pretraining process introduces musical priors to enhance the learning of key relationships with other musical elements, improving key consistency for both representations."}, {"title": "4.5 Subjective Evaluation and Results", "content": "We leverage an online listening test to assess the emotion modeling ability of models. The test was conducted to collect user responses on three parts: 1) valence modeling, 2) arousal modeling, and 3) 4Q emotion modeling. During this test, the quality of the generated music has also been assessed implicitly as it is a prerequisite to the emotion expression in the music. 22 participants were engaging in this test, 5 with less than 2 years of musical training, 8 with 2-5 years, 3 with 5-10 years, and 6 with more than 10 years."}, {"title": "4.5.1 Valence Modeling", "content": "In this part, each participant listened to 16 generated tracks of piano performance from four models [four tracks (two high valence and two low valence) per model]: 1) Real data; 2) REMI (one); 3) REMI (two); 4) Functional (two). For each track, participants rated its positiveness from -2 (low valence) to 2 (high valence) with the step size 1.\nThe left of Figure 6 ('(a)') presents the mean opinion scores for the valence-oriented test, where the Functional (two) model significantly outperforms both REMI (two) and REMI (one) models. The REMI (two) model shows a slight improvement over REMI (one) due to its two-stage design. Our proposed Functional (two) model even marginally exceeds real data in low valence scores, which could be due to the potential subjective biases in the negative emotion as discussed in [10]. And our model achieves both great performance in high valence and low valence results, demonstrating a good balance in valence modeling."}, {"title": "4.5.2 Arousal Modeling", "content": "In the second part, the functional (two) and REMI (two) models are chosen to compare their arousal modeling performance. Specifically, we wish to explore whether they can generate piano performance with either high or low arousal under the same lead sheet based on the given conditions (Q1 and Q4 for positive lead sheets, Q2 and Q3 for negative ones). Two pairs of generated tracks are randomly drawn for each model and each valence level, where every pair includes two tracks of different arousal conditions. For each track, participants rated its arousal level from -2 (low arousal) to 2 (high arousal) with the step size 1."}, {"title": "4.5.3 4Q Emotion Judgement", "content": "In the last part, participates needed to choose the best option from four options (4Q) for each track, with 8 tracks in total for the two models the last section (4 tracks per model and 1 track per emotion).\nFigure 7 presents the confusion matrices of two models. The Functional (two) model achieves the higher overall accuracy than that of REMI (two) (71.5% vs. 31.0%). When examining each emotion category, Functional(two) demonstrates superior performance in Q3 and Q4 than Q1 and Q2. Furthermore, music pieces generated from it with high valence conditions are misidentified almost based on their arousal levels; for instance, pieces intended for Q1 are almost mistaken for Q4 and vice versa. In contrast, for REMI(two), the misclassifications are across all categories, demonstrating its limitations in modeling the four emotion classes although through two-stage generation.\nAll above evaluations support that the combination of two-stage framework and functional representation is effective in controlling the emotion of the music it generates to a certain extent."}, {"title": "5. CONCLUSION AND FUTURE WORK", "content": "In this paper, we first explore emotion disentanglement through a two-stage Transformer-based framework for emotion-driven piano performance generation. Then we propose a novel functional representation for symbolic music to capture the interactions among musical keys, modes, chords, and melodies in relation to the emotion contexts. An objective metric is designed to qualify the key modeling of the proposed method, and subjective evaluations further confirm its ability to convey desired emotional perception. In the future, we wish to focus on enhancing the flexibility of emotional music generation across all musical keys and investigating new applications fostered by our framework, such as the controls of valence and arousal attributes under similar music motifs."}]}