{"title": "SCENE: Evaluating Explainable AI Techniques Using Soft Counterfactuals", "authors": ["Haoran Zheng", "Utku Pamuksuz"], "abstract": "Explainable Artificial Intelligence (XAI) is essential for enhancing the transparency and accountability of AI models, especially in natural language processing (NLP) tasks. This paper introduces SCENE (Soft Counterfactual Evaluation for Natural language Explainability), a novel evaluation method that leverages large language models (LLMs) to generate Soft Counterfactual explanations in a zero-shot manner. By focusing on token-based substitutions, SCENE creates contextually appropriate and semantically meaningful Soft Counterfactuals without extensive fine-tuning. SCENE adopts Validitysoft and Csoft metrics to evaluate the effectiveness of model-agnostic XAI methods in text classification tasks. Applied to CNN, RNN, and BERT architectures, SCENE provides valuable insights into the strengths and limitations of various XAI techniques.", "sections": [{"title": "1 Introduction", "content": "The majority of literature describes counterfactual explanations as \"the smallest change to the feature values that changes the prediction to a predefined output\" (Molnar, 2020, p. 263), or, in other words, what could have happened differently. Its biggest advantage is being very \"human-friendly,\" as it requires neither additional assumptions nor complex operations in the background. (Molnar, 2020)\nIn this approach, specific values in an input instance are perturbed while all other variables remain unchanged. This process aims to see how these adjustments impact the outcome. Any perturbation that leads to an output change is identified as a counterfactual. By comparing the original instance with these counterfactuals, we gain insights into the relationship between inputs and outputs. (Wachter et al., 2017; Robeer et al., 2021). Furthermore, in the case of text data, particularly in classification tasks, perturbations usually involve replacing significant words or tokens with other highly significant ones that correspond to a different output class (Robeer et al., 2021; Wu et al., 2021).\nThe fundamental challenge for the field of causal inference, as highlighted by Holland (1986), is that it is impossible to observe both original instance and its alternative simultaneously for a single unit of analysis. This unit of analysis refers to the smallest entity about which we aim to make counterfactual investigations. This challenge renders causal inference more complex than statistical inference and unachievable without making certain identification assumptions (Feder et al., 2022).\nAlthough counterfactual predictions are typically unobservable, in scenarios where the causal system is the predictor (the text sequence classifier in our case) itself, it is feasible to generate counterfactuals. In the context of this paper, which focuses on text classification tasks that fit these criteria, we can measure treatment effects by comparing predictions under both factual and counterfactual conditions (Feder et al., 2022). Consequently, this approach enables us to evaluate the effectiveness of existing explainable AI (XAI) techniques in identifying significant tokens."}, {"title": "2 Dataset Description and Task Overview", "content": "The data selected for this analysis is the Stanford Sentiment Treebank (SST2) (Zaidan et al., 2007). This dataset consists of realistic movie reviews with binary labels, positive and negative. The complexity of the dataset, with an average review length of 773 words, mirrors real-world textual data. With its detailed labels and diverse linguistic styles, SST2 is ideal for benchmarking sentiment analysis algorithms. In this study, we focus on sentiment analysis as a text classification task, leveraging the annotations and varied linguistic expressions within SST2 to evaluate the performance of various model-agnostic XAI techniques."}, {"title": "3 Evaluation Metrics for XAI Techniques", "content": "Significant contributions have been made in previous works towards developing metrics for evaluating saliency explanations. One common approach involves erasure-based methods, where significant tokens are masked or pruned, and the original model outputs are compared with the post-erasure outputs holistically. Notable examples include the work by Serrano and Smith (2019), which measured the percentage of decision changes after zeroing out the attention weights of \"important\" elements. Building on this concept, DeYoung et al. (2019) introduced the metrics of Comprehensiveness and Sufficiency to systematically evaluate erasure-based methods. These metrics quantify the impact on the model's predictions when the most important features are excluded or solely included.\nHowever, previous works have raised some concerns with erasure-based approach. Feng et al. (2018) reveal not only that neural models can still make high-confidence predictions on reduced inputs, but also that these inputs are nonsensical to human observers, thereby highlighting interpretability issues. Intuitively, consider classifying the sentence \"I love this movie\" in a sentiment analysis task. If the token \u201clove\u201d is stripped or masked, resulting in \"I [MASK] this movie,\" the sentence becomes incomplete and lacks actual sentiment and semantic meaning. These \"damaged\" versions of the inputs can fall outside the data distribution of the model's training sets, leading to imprecise evaluations of faithfulness (Ge et al., 2021).\nAnother fundamental sanity check is the completeness axiom, also known as the Summation-to-Delta Property, as established by Shrikumar et al. (2017) and Sundararajan et al. (2017). This axiom asserts that the sum of the attributions (explanations) equals the difference between the model's prediction for the original input and a baseline. This principle ensures that the explanation fully accounts for the model's decision, providing a comprehensive and theoretically sound foundation for attributing importance scores to individual tokens. Intuitively, this means that if the model's prediction changes significantly when a token is perturbed, the token's attribution should accurately reflect this change, adhering to the principle of completeness.\nInspired by the completeness axiom, the metric of infidelity was introduced (Yeh et al., 2019). Infidelity measures the discrepancy between the predicted impact of perturbations on the model's output and the actual impact observed. In the context of sentiment analysis, which is the focus of this paper, the goal is to identify the most important tokens influencing sentiment. To recreate the experiment, we implicitly use the original input text embeddings as the baseline. The perturbation function is designed to introduce small random noise to these embeddings, thereby creating perturbed versions of the input. Specifically, noise sampled from a normal distribution is subtracted from the original embeddings.\nWhere:\n$E_{\\mu_I}[(\\delta(f, x) - (f(x) - f(x - I)))^2]$\nf is a black box function\nO is the XAI technique\nI is a random variable with probability measure $\\mu_I$\nWhile this mathematical operation on word embeddings is theoretically sound and highly effective for computational purposes, it results in high-dimensional vectors composed of embeddings that often do not correspond to real words. Consequently, this poses challenges for human observers in semantically interpreting how specific perturbations affect the model's predictions. Furthermore, although to a lesser degree, the concern of plausibility remains, as the perturbed versions of the inputs can still fall outside the original data distribution.\nDespite these challenges, using the original input embeddings as the baseline has the advantage of maintaining the context and structure of the input text, ensuring that the perturbations are applied in a meaningful manner. Since the evaluation of infidelity involves comparing the"}, {"title": "3.1 Previous Works", "content": "Lastest, human agreement is often used as a benchmark to evaluate saliency explanations (DeYoung et al., 2019; Atanasova, 2024). Human agreement is measured by how well the identified tokens align with those annotated by humans. For models that make binary (yes/no) decisions about the relevance of saliency explanations, simpler metrics like accuracy or exact match are sufficient. In our case, most of the XAI methodologies generate continuous attribute importance scores, for which metrics such as the area under the precision-recall curve (AUPRC) and average precision, which evaluate the overlap between the model's extracted important tokens and the human-provided ones, are more appropriate. In this paper, we chose to use the human annotations collected by DeYoung et al. (2019) and the mean of the average precisions (MAP) to measure the performance of various XAI techniques (Atanasova, 2024).\nWhere:\n$MAP = \\frac{1}{N} \\sum_n\\sum_\\alpha (R_{n,a} - R_{n,a-1})P_{n,\\alpha}$\nN is the number of instances\n$P_{na}$ and $R_{n,a}$ are the precision and recall of the a-th threshold and the n-th instance."}, {"title": "3.2 Counterfactuals as an Alternative", "content": "To address the interpretability issues exhibited in other metrics, we explore counterfactual explanations as an alternative. The common approach for modern counterfactual generation has been to structure it as an optimization problem, aiming to find the minimal changes to certain features of an input that will alter the final output while keeping most features constant. This method aims to compute human-understandable and realistic counterfactuals effectively, addressing the need for easy comprehension by human observers (Wachter et al., 2017).\nHowever, due to the high dimensionality and discrete nature of text data, this approach has its limitations. First and foremost, formulating the task of minimizing the distance between two inputs as an optimization problem is challenging because it necessitates calculating gradients for a discrete input (Belinkov and Glass, 2019). Moreover, creating traditional counterfactuals that change the output class becomes increasingly difficult with longer text inputs. For instance, consider a lengthy movie review from a reviewer who passionately loves a particular movie. If we replace only a few words or tokens with high positive sentiments with negative sentiments, the overall sentiment classification of the review is unlikely to change, thus rendering \"flipping the class\" unrealistic in such cases.\nTo address these challenges, we propose Soft Counterfactuals, a new class of counterfactuals where a final output change is not necessarily required. In this relaxed definition, instead of finding \"the closest possible world,\" we provide a diverse range of relevant and insightful \"close possible worlds\" (Wachter et al., 2017). Soft Counterfactuals allow for granular insights, such as detecting subtle changes in feature values that influence outcome probabilities, rather than requiring a complete shift from one outcome to another. Furthermore, without relying on an optimization function to minimize the distance, we utilize the power of transformers to ensure that these counterfactuals are realistic and contextually appropriate (Vaswani et al., 2017).\nAnother challenge in generating counterfactuals is resource constraints. Previous studies have explored human-generated counterfactuals for text data, but the high cost makes this approach impractical for large-scale operations. On average, workers spend 4 to 5 minutes revising a single input, and they are still prone to systematic omissions (Kaushik et al., 2019; Wu et al., 2021).\nTo overcome this obstacle, researchers have explored automated approaches, particularly leveraging the advancements in large language models (LLMs) that have gained traction in recent years. Notable examples include Wu et al. (2021), who introduced Polyjuice, a fine-tuned GPT-2 model trained on multiple datasets with paired sentences. Polyjuice employs control codes (e.g., negation, lexical changes) and a fill-in-the-blank structure to generate specific types of counterfactuals. Similarly, Robeer et al. (2021) created realistic counterfactuals using a Counterfactual GAN architecture. These methods and their counterparts often require auxiliary models and/or training data, which is not always feasible due to the lack of task-specific datasets or the resource-intensive nature of fine-tuning models (Bhattacharjee et al., 2024).\nBhattacharjee et al. (2024) proposed an alternative, utilizing zero-shot state-of-the-art LLMs to generate counterfactuals, but this approach can raise further concerns about the interpretability of the counterfactual generation process. Despite significant strides in generating realistic and high-quality counterfactuals with LLMs, most efforts have been directed towards using counterfactuals as"}, {"title": "3.2.1 Counterfactuals Generation", "content": "a data augmentation tool. However, when the goal is to create standardized evaluation metrics for XAI techniques on text classification problems, the task becomes more straightforward.\nIn SCENE, we do not require counterfactuals to be perfect paraphrased representations of the original instances. Instead, we focus on token-based substitutions and compare the outputs holistically. Therefore, an off-the-shelf (zero-shot) BERT model for masked language modeling (BertForMaskedLM) is sufficient for the task (Devlin et al., 2018; Ribeiro et al., 2020). This approach is computationally efficient, requires no further fine-tuning, and can leverage GPU advantages.\nIn summary, SCENE is an approach that uses masked language modeling (MLM) for token-based substitution. We select and mask certain number of tokens, then replace them with likely alternatives to craft Soft Counterfactuals using a zero-shot BertForMaskedLM. This approach allows SCENE to measure faithfulness, as defined by Atanasova (2024), which refers to whether an explanation technique's predictions are faithful to the model's inner workings and not based on arbitrary choices. Additionally, SCENE measures confidence indication, as defined by Atanasova (2024), which refers to whether a token contributed significantly to the prediction."}, {"title": "3.2.2 Counterfactual Evaluation", "content": "The flexibility of Soft Counterfactuals offers convenience in generating realistic counterfactuals for text data but also poses challenges in evaluating the results. In this paper, we adopt the $Validity_{soft}$ and Counterfactual Evaluation Score$_{soft}$ ($C_{soft}$) formulations defined by Ge et al. (2021) to evaluate XAI techniques' ability to provide relevant saliency explanations. In other words, these formulations assess the techniques' ability to identify tokens with high significance.\n$\\begin{equation} Validity_{soft} = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{K} \\sum_{k=1}^{K} |p(y_n | x_n) - p(y_{n, k}^{cf} | x_{n, k}^{cf})| \\end{equation}$\nWhere:\nN is the number of original instances\nK is the number of Soft Counterfactuals generated for one instance\np($y_n | x_n$) is the probability of the predicted label given the original input for instance n\n$p(y_{n, k}^{cf} | x_{n, k}^{cf})$ is the probability of the predicted label given the Soft Counterfactual input for instance n\n$Validity_{soft}$, inspired by the commonly used Validity metric for counterfactual evaluations, measures the average change in probabilities from the original predictions to the predictions made with counterfactual inputs for the same class. This metric assesses the extent of probability shifts the higher the score, the better the method is at selecting the significant tokens (Mothilal et al., 2020; Ge et al., 2021).\n$\\begin{equation} C_{soft} = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\sum_{k=1}^{K}(p(y_n | x_n) - p(y_{n, k}^{cf} | x_{n, k}^{cf}))}{\\sum_{k=1}^{K} dist(x_n, x_{n, k}^{cf})} \\end{equation}$\nWhere:\nN is the number of original instances\nK is the number of Soft Counterfactuals generated for one instance\np($y_n | x_n$) is the probability of the predicted label given the original input for instance n\nP ($y_{n, k}^{cf} | x_{n, k}^{cf}$) is the probability of the predicted label given the Soft Counterfactual input for instance n\ndist($x_n, x_{n, k}^{cf}$) is the distance between the original instance and its Soft Counterfactuals\n$C_{soft}$ builds on the foundation of $Validity_{soft}$ by incorporating a distance function to evaluate the faithfulness of the saliency explanations (Ge et al., 2021). For the distance function, SCENE uses Universal Sentence Encoder, proposed by Cer et al. (2018). In this approach, the encoder (BERT-uncased for this paper) employs attention mechanisms to generate context-aware representations of words in a sentence, taking into account both the sequence and the identities of surrounding words. These context-aware word representations are then averaged to produce a sentence-level embedding (Cer et al., 2018). The higher the $C_{soft}$ score, the better the method is at providing faithful and informative explanations.\nIt is worth noting that $Validity_{soft}$ and $C_{soft}$ are designed for binary classification tasks and may not fully capture the behavior of counterfactual outputs in multi-class classification scenarios. For instance, an increase in $p(y_{n, k}^{cf} | x_{n, k}^{cf})$ does not necessarily lead to a decrease in the original output p($Y_{n, k} | x_{n,k}^{cf}$); both probabilities can increase while the probabilities of other classes decrease (Ge et al., 2021)."}, {"title": "4 Experimental Design", "content": "This section presents the experimental design of SCENE and the methods used to measure its performance. The experiment involves several stages: extraction of"}, {"title": "4.1 Token Extraction", "content": "To identify the most significant tokens from the XAI results, SCENE extracts the top tokens based on their importance weights for each technique, respectively. Initially, the test set is passed through various XAI methods to ensure that attributions for all tokens are computed and saved for each method. Subsequently, the text is tokenized, with the first and last tokens removed to focus on the core content. The tokenized text is then paired with corresponding weights derived from three possible methods: mean, L2, or direct weighting. To ensure clarity, subword tokens indicated by '##' and their preceding tokens are filtered out. Additionally, non-alphabetic tokens are excluded unless they are single-character words such as 'I' or 'a'. The remaining tokens are sorted by their importance weights in descending order, and the top V tokens are selected. This approach allows for a systematic ranking of the most relevant tokens, providing a framework for analyzing the saliency explanations generated by ten XAI methods across three different popular model architectures: CNN, RNN, and Transformers.\nWe selected ten XAI techniques to represent the most prominent model-agnostic XAI methods as of 2024. These include perturbation-based approaches such as LIME and SHAP, as well as backpropagation-based techniques like Integrated Gradients and Guided Backpropagation. It is worth noting that many XAI methods use gradients usually are not adapted for text explanations. For this reason, the attributions are calculated as a function of the embeddings and not the input tokens. The techniques include:\nLIME: Proposed by Ribeiro et al. (2016), Local Interpretable Model-agnostic Explanations (LIME) creates locally faithful approximations around a specific prediction with a simpler model that is easier to interpret.\nSHAP: Proposed by Lundberg and Lee (2017), SHapley Additive exPlanations (SHAP) applies Shapley values, concepts from cooperative game theory, to measure the contribution of individual features.\nGradient X Input: Proposed by Shrikumar et al. (2017), Gradient X Input serves as a straightforward method for visualizing each input feature's contribution to the output by multiplying the gradient of the output with respect to each input feature by the value of the input feature itself.\nGrad L2 Norm: Closely related to Gradient X Input, Gradient L2-norm computes the L2-norm of the gradient of the model's output with respect to the inputs, which indicates the sensitivity of the model to changes in the input embeddings (Munn and Pitman, 2022).\nIntegrated Gradients: Proposed by Sundararajan et al. (2017), Integrated Gradients calculates the attribution to each input feature by multiplying the integrated gradient along that dimension by the difference between the actual input and the baseline.\nSaliency: Proposed by Simonyan et al. (2013), Saliency creates a saliency map that highlights the regions of an image most influential to the classification decision of a neural network by utilizing a single backpropagation pass to calculate the gradients.\nGuided Backpropagation: Proposed by Springenberg et al. (2014), Guided Backpropagation visualizes the parts of an image that most activate a given neuron in a neural network, combining the standard backpropagation approach with an additional guidance mechanism to provide clearer visualizations.\nGuided GradCAM: Proposed by Selvaraju et al. (2017), Guided Grad-CAM is a variation of Gradient-weighted Class Activation Mapping (Grad-CAM) that generates coarse localization maps by computing the gradient of the target concept's score with respect to the feature maps of the last convolutional layer. This method highlights important regions in the image.\nDeepLIFT: Proposed by Shrikumar et al. (2017), Deep Learning Important FeaTures (DeepLIFT) compares the activation of each neuron to its \"reference activation\""}, {"title": "4.2 Creating Soft Counterfactuals", "content": "To generate Soft Counterfactuals with relevant and informative token replacements, SCENE masks the important tokens identified by XAI techniques and substitutes them with alternative words. These previously identified important tokens are masked, and the masked text is then re-tokenized. Using a zero-shot BertForMaskedLM model, we predict possible replacements for the masked tokens, filtering out non-alphabetic words and those identical to the original tokens. From the top predictions, we randomly select a subset to generate K Soft Counterfactual samples. This process results in a collection of Soft Counterfactual texts, each with specified replacements for the masked tokens, enabling an in-depth analysis of the impact of token-level changes on model predictions. Additionally, a random seed is used to ensure reproducibility and a diverse range of Soft Counterfactuals.\nSCENE generates K Soft Counterfactuals where V significant tokens are replaced by likely alternatives. In our experiments, the parameters were set to K = 10 and V = 5."}, {"title": "4.3 Evaluation of Soft Counterfactuals", "content": "Finally, SCENE computes the $Validity_{soft}$ and $C_{soft}$ (defined in 3.2.2) for the ten selected XAI methods across three different model architectures. We then present these results with two other metrics-infidelity and human agreements-to provide a comparison with SCENE.\nAdditionally, two possible ways to summarize these results are provided: the average of each token embedding attribution and the L2 norm. We also record the average time each XAI methodology spends on each test dataset instance, including both the inference and the calculation of attributions by the XAI methodology."}, {"title": "5 Results", "content": "For the CNN architecture, the metrics reveal different strengths for each technique. In terms of Human Agreement, Saliency (both Mean and L2) performs well in capturing important attributes, making it more aligned with human interpretation of the model's decisions. When measuring Infidelity, Deconvolution (both Mean and L2) outperformed others by remaining more stable after perturbations, indicating a higher fidelity to the model's behavior.\nSHAP (both Mean and L2) showed strength in terms of the counterfactual metrics but exhibited high Infidelity and had a higher time cost, which may limit its practicality in real-time applications. Finally, gradient-based methods demonstrated greater efficiency regarding the time metric, providing faster explanations compared to other techniques."}, {"title": "5.1 CNN", "content": "Among the evaluated XAI techniques for RNN, Saliency (L2) stands out with the highest MAP at 0.3843, surpassing most other methods in this metric. Most L2 methods show similar MAP scores around 0.384, indicating comparable precision performance when considering L2 regularization. While most methods demonstrate average processing times under 0.05, LIME (both Mean and L2) requires significantly more time. Additionally, Saliency (both Mean and L2) exhibit slightly higher average infidelity (0.0006), suggesting they may be less reliable in generating consistent explanations compared to other techniques."}, {"title": "5.2 RNN", "content": "XAI methods applied to a BERT model reveal that Grad L2 Norm, Guided Backpropagation (L2), and Deconvolution (L2) offer the best performance, characterized by high Human Agreement, low Infidelity, reasonable average time, and strengths in $Validity_{soft}$ metrics. In contrast, methods such as LIME (both Mean and L2) performs poorly due to low MAP, high infidelity, low $Validity_{soft}$, low $C_{soft}$, and high average time. Applying LIME in a repetitive setting could imply high computing times and further processing issues"}, {"title": "5.3 Transformer", "content": "The exploration and application of counterfactual explanations within the realm of XAI and NLP have demonstrated significant potential in bridging the interpretability gap between complex Al models and human understanding. This paper presented SCENE (Soft Counterfactual Evaluation for Natural language Explainability), a novel approach that leverages LLMs to generate Soft Counterfactuals in a zero-shot manner, thereby providing an alternative method for evaluating XAI techniques.\nBy focusing on token-based substitutions and employing a zero-shot BertForMaskedLM model, SCENE effectively circumvents the challenges associated with high-dimensional text data and the need for extensive fine-tuning. This streamlined approach not only ensures the generation of contextually appropriate and semantically meaningful Soft Counterfactuals but also maintains computational efficiency.\nThe introduction of the $Validity_{soft}$ and $C_{soft}$ metrics within SCENE provides a standardized framework for evaluating the effectiveness of XAI techniques in identifying significant tokens. These metrics, inspired by established concepts in causal inference, offer a nuanced understanding of the impact of token-level changes on model predictions, thus validating the reliability of the saliency explanations generated by different XAI methods.\nMoreover, the empirical results from applying SCENE across various model architectures, including CNN, RNN, and BERT transformers, indicate that different XAI techniques exhibit unique strengths and limitations when applied to these models. For instance, Saliency and gradient-based methods provide quick and interpretable insights, while Deconvolution maintain high fidelity to the model's behavior. These insights are critical for practitioners aiming to select the most suitable XAI technique based on specific requirements such as interpretability, computational efficiency, and robustness to input perturbations.\nIn conclusion, SCENE represents an alternative in the evaluation of XAI techniques for NLP tasks. By addressing the limitations of existing methods and leveraging the capabilities of LLMS, SCENE provides a powerful tool for generating realistic counterfactuals and assessing the effectiveness of XAI methods."}, {"title": "6 Conclusion", "content": "Future work will focus on advancing SCENE by refining evaluation metrics to better capture the behavior of counterfactual outputs, especially in multi-class settings, addressing the unique challenges posed by complex label structures and interactions. Additionally, integrating SCENE with diverse NLP tasks such as named entity recognition, machine translation, and question answering will broaden its applicability. Enhanced counterfactual generation techniques, including exploring state-of-the-art large language models (LLMs) and fine-tuning for specific domains, may further improve the quality and relevance of generated counterfactuals. By addressing these areas, we aim to enhance the robustness, flexibility, and impact of SCENE, contributing to the development of more interpretable, transparent, and accountable AI systems."}, {"title": "7 Future Work", "content": "We would like to express our gratitude to Pamela Gonz\u00e1lez Mu\u00f1iz and Joselo Pe\u00f1a Contreras for their assistance in facilitating the experimentation process and testing SCENE on the CNN and Transformer structures. We also extend our thanks to Dr. Batuhan Gundogdu and Dr. Yuri Balasanov for their helpful discussions and feedback."}, {"title": "8 Acknowledgments", "content": null}]}