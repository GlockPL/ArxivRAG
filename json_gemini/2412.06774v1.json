{"title": "Visual Lexicon: Rich Image Features in Language Space", "authors": ["XuDong Wang", "Xingyi Zhou", "Alireza Fathi", "Trevor Darrell", "Cordelia Schmid"], "abstract": "We present Visual Lexicon, a novel visual language that encodes rich image information into the text space of vocabulary tokens while retaining intricate visual details that are often challenging to convey in natural language. Unlike traditional methods that prioritize either high-level semantics (e.g., CLIP) or pixel-level reconstruction (e.g., VAE), ViLex simultaneously captures rich semantic content and fine visual details, enabling high-quality image generation and comprehensive visual scene understanding. Through a self-supervised learning pipeline, ViLex generates tokens optimized for reconstructing input images using a frozen text-to-image (T2I) diffusion model, preserving the detailed information necessary for high-fidelity semantic-level reconstruction. As an image embedding in the language space, ViLex tokens leverage the compositionality of natural languages, allowing them to be used independently as \u201ctext to-kens\" or combined with natural language tokens to prompt pretrained T2I models with both visual and textual inputs, mirroring how we interact with vision-language mod-els (VLMs). Experiments demonstrate that ViLex achieves higher fidelity in image reconstruction compared to text embeddings-even with a single ViLex token. Moreover, ViLex successfully performs various DreamBooth tasks in a zero-shot, unsupervised manner without fine-tuning T2I models. Additionally, ViLex serves as a powerful vision encoder, consistently improving vision-language model performance across 15 benchmarks relative to a strong SigLIP baseline.", "sections": [{"title": "1. Introduction", "content": "How should we represent an image? This is a fundamental question in computer vision. Over decades of progress, there have been two primary approaches: representations optimized for understanding high-level semantics [14, 50, 53], or for high-fidelity image reconstruction [23, 34, 86], often used in image generation [55, 60]. Understanding-focused models like CLIP [53] and DINO [8] capture high-level semantics but lose pixel-level details. Conversely, reconstruction-focused models, such as VAEs [34], retain fine visual details but lack semantic richness, making them less effective for tasks like vision-language modeling. In this paper, we aim to address the question: \u201ccan a single representation excel in both image reconstruction and semantic understanding?\"\nTo bridge this gap, we introduce ViLex that encodes im-ages into a Visual Lexicon within the text space. ViLex model is designed to capture both high-level semantics such as object categories and spatial layouts \u2013 while pre-serving rich visual details like styles, and textures that are difficult or even impossible to articulate in natural language.\nWe achieve this, as illustrated in Figure 2, by leverag-ing a self-supervised learning strategy based on a frozen, pretrained text-to-image (T2I) diffusion model, which acts as the source of supervisory signals. Although initially de-veloped for generative purposes, many recent works [3, 19, 35, 79] have discovered that diffusion models [27, 56, 60] inherently capture both semantic and detailed visual infor-mation through their denoising process.\nTo incorporate rich visual information into our ViLex model, we repurpose diffusion models as decoders within an autoencoder [4, 25, 34, 73] framework. ViLex embed-dings are mapped into the latent space of the T2I diffu-sion model's vocabulary tokens-specifically, the index-to-embedding lookup matrix of the text encoder [53, 54], which converts text token IDs into embeddings. Using diffusion models as decoders for semantic-level image re-construction, rather than traditional VAE decoders [34] or MAE [23] designed for pixel-level reconstruction, encour-ages the model to learn meaningful semantic representa-tions that are highly transferable to diverse visual scene un-derstanding tasks. This design enables ViLex to harness the rich visual knowledge embedded in diffusion models while maintaining a lightweight structure, making it well-suited for a broad range of understanding tasks beyond diffusion models' original generative applications.\nThe ViLex model consists of a vision encoder that ex-tracts visual representations from the input image and an attention pooling layer that transforms the visual represen-tation into visual lexicon tokens. During training, ViLex model is optimized with an image reconstruction loss, re-ceiving gradients from the frozen diffusion model and its text encoder to fine-tune the visual lexicon tokens for accu-rately reconstructing images with similar appearance. Ad-ditionally, we propose the TailDrop strategy during training, where the last k visual lexicon tokens are randomly dropped to encourage the earlier tokens to encapsulate richer seman-tic information. During inference, the number of tokens can be dynamically adjusted to meet user requirements.\nOur ViLex model is designed to support both image generation and understanding tasks. For image genera-tion: ViLex tokens can be directly used as \u201ctext-prompts\", enabling the re-creation of semantically and visually sim-ilar images. Experiments on COCO image reconstruc-tion demonstrate that our ViLex significantly outperforms its counterparts image-guided DALL-E 3 [5] and DeDiffu-sion [76] in terms of the layout, semantic, and style consis-tency with the input image, based on human studies. No-tably, even with just a single ViLex token, the FID score of ViLex remains lower than that of DeDiffusion [76], show-casing the representational power of Visual Lexicon. Ad-ditionally, ViLex embeddings can seamlessly integrate with text prompts, for example, \"an image similar to [ViLex to-kens], in Van Gogh style\u201d, enabling multimodal image gen-eration and DreamBooth [58] tasks in a zero-shot fashion by prompting a frozen T2I model with both visual and tex-tual inputs. For image understanding: replacing the strong semantic-pretrained backbone SigLIP [88] with ViLex's vi-sion encoder in vision-language models [6] leads to im-provements across various vision-language tasks, including image and video captioning [10, 78], visual question an-swering [21], and referring segmentation [83].\nThe main contributions of our work are:\n\u2022 We propose ViLex, an image encoder that maps images into the text space of text-to-image diffusion models. The resulting image embeddings capture both high-level se-mantics and intricate visual details that are otherwise challenging to convey in natural language.\n\u2022 ViLex enables zero-shot unsupervised DreamBooth by prompting T2I models with both ViLex tokens and text prompts, without requiring fine-tuning a T2I model or"}, {"title": "2. Related Work", "content": "Image Representation Learning is a fundamental task in computer vision. There are two popular approaches: representing an image with features optimized for visual scene understanding [14, 20, 37, 50, 53, 72, 75, 77, 88] or with features optimized for high-fidelity image reconstruc-tion [23, 34, 51, 86], which is often used in image gener-ation [52, 60]. Understanding-focused representations like those in CLIP [53], SigLIP [88], DINO [8], and DINOv2 [8, 50] capture high-level semantic information but lose pixel-level details. Conversely, reconstruction-focused features, commonly from AutoEncoder-based models (AEs), like VAE [34], MAE [23], and BEiT [2], retain fine image de-tails but often lack semantic richness, limiting their utility in downstream tasks like vision-language modeling [6, 36]. AutoEncoders, while effective for pixel-level fidelity, of-ten struggle with discriminative tasks. Their focus on re-constructing local, semantically agnostic details leads to suboptimal performance in tasks demanding rich, discrim-inative representations, such as linear evaluation on Ima-geNet [2, 9, 15]. We intend to propose a new vision en-coder that provides image representations for both image understanding and generation tasks.\nImage Inversion for Diffusion Models. The goal of im-age inversion is to determine the text prompt that can be used for generating a specific source image. Prompt-inversion [44, 48] uses gradient descent to move from the pixel space to the text-embedding space. Techniques like Dreambooth [58, 59] and textual-inversion [18] learn spe-cial text tokens for given instances, but require gradient-based training for each individual image, making them slow at inference time. Also, DreamBooth needs to determine the LORA adapters for model architecture changes and is not generic for visual understanding. Recently, DeDiffu-sion [76] proposed training a model to generate the inverse text using Gumbel softmax, but the quality of reconstruction is limited by what can be represented by text tokens. Our approach bypasses discrete language-based text representa-tions, enabling higher-quality reconstructions and efficient single-pass inference.\nRepresentation Learning with Diffusion Models has been explored by several previous works [12, 31, 74, 79, 80, 84]. For instance, 1-DAE [12] uses the diffusion loss as a self-supervised learning objective, while ODISE [79] employs diffusion-pretrained features for zero-shot panop-tic segmentation. DIVA [74] shows that finetuning a CLIP backbone [53] with gradients from diffusion models en-hances localization capabilities. In contrast to these meth-ods, we harness the built-in knowledge of T2I models to learn visual features, effectively framing the generation of text embeddings that reconstruct an image as a powerful representation learning objective.\nImage Tokenization, commonly associated with varia-tional autoencoders (VAEs) [34], is crucial for compressing images into a lower-dimensional space for diffusion model training. VQVAEs [56, 81] utilizes a discrete codebook for quantizing latent representations, while recent works like MagViTv2 [85], FSQ [47], and BSQ [89] improve quan-tization with direct binary encoding. More recent image-tokenizers [38, 71, 86] propose new encoding strategies, in-cluding scale prediction [38, 71] and 1D compression [86]. Unlike these tokenizers, which predict features as noise, our model predicts features conditioned on the diffusion model. Thus, instead of aiming for lossless reconstruction, our fo-cus is on recreating images with high semantic fidelity."}, {"title": "3. Describe an Image with a Visual Lexicon", "content": "In this section, we introduce ViLex that maps images di-rectly into the text space, while effectively preserving com-plex visual details that are difficult to express in natural lan-guage. Our representation effectively acts as a new \u201clan-guage\" for text-to-image generation and a strong vision en-coder for downstream vision-language tasks. We will in-troduce our approach in \u00a7 3.1, and the technical details in \u00a7 4.1. Figure 3 presents the overview of ViLex."}, {"title": "3.1. Visual Lexicon", "content": "Approach overview. ViLex aims to capture high-level se-mantic representations \u2013 such as object categories and lay-outs \u2013 while also preserving rich visual details like styles, patterns, and textures that are difficult or even impossible to describe in natural language.\nTo accomplish this, we train ViLex through a self-supervised learning pipeline, with a frozen pretrained text-to-image (T2I) diffusion model serving as the source of su-pervisory signals. This approach enables ViLex to extract visual representations that encompass both semantic-level understanding and intricate visual features. Unlike previous works that directly use diffusion models as feature extrac-tors [3, 19, 35, 79], we take a different approach. In our framework, diffusion models act as decoders in an autoen-coder [4, 25, 34, 73] pipeline, which allows us to bake the semantic richness learned by these models into a vision en-coder. As a result, ViLex benefits from the detailed and rich visual representations of diffusion models while be-"}, {"title": "4. Experiments", "content": "4.1. Technical Details\nWe describe main technical details here and provide the full details in the supplement.\nText-to-image diffusion model. Following DeDiffu-sion [76], we use Imagen [60] as the base text-to-image dif-fusion model, adapting the U-Net architecture from [49, 57] with 600M parameters, an embedding dimension of 256,"}, {"title": "5. Conclusions", "content": "We introduce ViLex, a visual lexicon that maps images di-rectly into the text space, while effectively preserving com-plex visual details that are difficult to express in natural lan-guage. Our representation can be seamlessly integrated into text prompts from natural language for both multimodal image generation and downstream vision-language tasks. ViLex can also improve both image understanding and re-construction capabilities of pretrained vision encoders by fine-tuning them using ViLex's training approach."}, {"title": "A1. Technical Details", "content": "We introduced the main technical and implementation de-tails of our ViLex model in the main paper, here we provide a more comprehensive explanation.\nText-to-image diffusion model. Following DeDiffu-sion [76], we use Imagen [60] as the base text-to-image dif-fusion model, adapting the U-Net architecture from [49, 57] with 600M parameters, an embedding dimension of 256, and an input resolution of 64\u00d764. The text encoder of Ima-gen is OpenCLIP ViT-H/14 [13, 32] with a vocabulary size of 49408. The U-Net conditions on text embeddings via a pooled embedding vector, which is added to the diffusion timestep embedding. Imagen is further conditioned on the full sequence of text embeddings by incorporating cross-attention over the text embeddings at multiple resolutions. The Imagen model uses v-prediction [61] as its objective, with a batch size of 2048, and is trained for 3 million steps. As a baseline model, Imagen achieves an FID of 6.52 on 30K 64\u00d764 MS-COCO 2014 validation images [60]. During image generation inference, we use a super-resolution model, such as an SDXL upsampler, to upsample the image resolution from 64x64 to 512x512 for better visualizations.\nModel architecture of ViLex. ViLex consists of two com-ponents: a ViT-based image encoder and a transformer-based attention pooling module. Both components are un-frozen during the training process. For the image encoder, we use a pretrained SigLIP-So400M@224 [88]. SigLIP uti-lizes ViT-base as the backbone and is pretrained on the WebLI dataset [11] using a sigmoid loss and trained on English image-text pairs, with input images resized to 224x224. The model architecture of the ViT-base is shape-optimized on 400M training samples for improving the model effi-ciency and speed. In our method, the attention pooler is im-plemented as a single multi-head attention layer with learnable queries, using the encoder output as both keys and val-ues. This allows the attention pooling module to effectively aggregate embeddings of varying lengths. The attention pooling module contains n learnable queries, where n \u226475, along with [SOS] and [EOS] tokens to ensure the total token count remains within the 77-context length limit defined by the CLIP text encoder [32, 53]. The attention pooling layer comprises 5 transformer blocks, which are always randomly initialized.\nModel training. The training data is obtained from We-bLI [11], enabling training on either images alone or with image-text pairs. We found that joint image-text training and our TFG are essential for enabling multimodal image generation. However, training without text captions does not negatively impact performance on downstream vision-language tasks. Following [60, 76], we use Adafactor op-timizer [63] and a weight decay of 0.01. Training is per-formed with a batch size of 2048 over 300K steps, which takes approximately 2.5 days on 64 TPUv5 chips. We found that double the training steps (from 300k to 600k) can fur-ther improve the model performance on increasing the per-formance of a pretrained vision encoder. The ViT is ini-tialized with a pretrained SigLIP model and the attention pooling layers are randomly initialized. We use learning rate 1 \u00d7 10-5 for the image encoder and 3\u00d710-4 for the attention pooling layers, with a cosine learning rate decay and a 10K-step linear warmup, and a weight decay of 0.01. After training, our ViLex encoder maps an image to ViLex representations. We next evaluate two capabilities of these frozen ViLex representations: image generation and visual understanding.\nPaliGemma experiments. To evaluate the effectiveness of the proposed ViLex approach in enhancing a pretrained vision encoder for vision-language tasks, we integrate our vision encoder into the PaliGemma [6] framework and replace the vision encoder with either the fine-tuned SigLIP-So400M [88] from ViLex or the official version with-out model fine-tuning, freezing the vision encoder and fine-tuning the model on downstream tasks. Following PaliGemma's official pipeline, we transfer the model to a variety of individual academic benchmarks using a unified transfer approach with minimal hyperparameter tuning. To ensure fair comparison, we applied the same hyperparam-eter sweeping strategy for both the baseline and our fine-tuned vision encoder, reporting the best results for each. This structured approach allows us to fairly assess the im-pact of the proposed ViLex method on a wide range of vision-language tasks. The sweeping parameters for these tasks are as follows: COCOCap [40] (COCO image cap-tioning task) and COCO-35L [70] (COCO captions trans-lated in 35 languages): learning rate (4e-6, 5e-6, 6e-6), epochs (5, 10), dropout (0, 0.02, 0.05). TextCaps [64] (im-age captioning with reading comprehension): learning rate (4e-6, 6e-6), and training epochs (5, 10). For SciCaps [28] (captions for scientific figures): learning rate (6e-5, 7e-5), dropout (0.1, 0.2), and label smoothing (0.1, 0.2). For VQAv2 [21] (visual question answering): label smoothing (0.0, 0.1), dropout (0.0, 0.1), and weight decay (0, 1e-6). For TextVQA [65] (visual reasoning based on text in im-ages): learning rate (4e-6, 6e-6). For OKVQA [46] (outside knowledge VQA), ScienceQA [43] (science question an-swering), and VizWizVQA [22] (VQA from people who are blind): learning rate (8e-6, 1e-5), and dropout (0.0, 0.02)."}, {"title": "A2. Human Study", "content": "We conduct human studies to evaluate the quality of gener-ated images using an image-to-image pipeline, focusing on three criteria: Semantic Alignment, Style Alignment, and Layout Alignment. For Semantic Alignment, participants judge which generated image more accurately captures the original content and semantic details, such as object cate-gories. Introducing new instances or omitting existing ones from the input image is considered less desirable. For Style Alignment, participants assess which generated image best retains the artistic style and visual aesthetics of the original. For Layout Alignment, participants evaluate which gener-ated image maintains a composition and positioning of ob-jects that closely matches the input image.\nThe results of this evaluation are reported in Table 2 of the main paper. Detailed instructions and the question for-"}, {"title": "A3. Ablation Study", "content": "Training Steps. We observed that extending the fine-tuning steps of the vision encoder using our ViLex pipeline leads to improved performance across nearly all evaluated benchmarks, as shown in Table A1. Specifically, increas-ing the training steps from 150k to 300k yields significant gains. Further extending the training to 600k steps provides marginal improvements compared to the 300k-step results. The largest improvements are observed in datasets that de-mand stronger spatial understanding, such as the referring expression segmentation datasets RefCOCO/+/g.\nNumber of attention pooling layers. Although increas-ing the number of attention pooling layers improves image reconstruction performance (as indicated by a lower FID score), it also introduces a trade-off with image understand-ing capabilities. As shown in Table A2, we found that using 5 attention pooling layers provides the optimal balance between image generation quality and developing an effective vision encoder for visual scene understanding.\nVision encoders. The ViLex approach effectively enhances various vision encoders for downstream visual scene under-standing tasks. We initialize the vision encoder of ViLex with either the CoCa [82] pretrained ViT or the SigLIP [88] pretrained ViT-So400M. Similar to our experiments in pre-vious sections, We observed consistent performance im-provements for both image understanding tasks, such as COCOCaps [40], and video understanding tasks, such as"}, {"title": "A4. Demo Results", "content": "Semantic-level image reconstruction. In this section, we present additional demo results in Figure A2, showcasing a set of images generated with varying diffusion noises and different random seeds. These images demonstrate high se-mantic and visual consistency, leveraging ViLex tokens as \"text\" prompts for text-to-image diffusion models. How-ever, as shown in the results, our model occasionally misses small objects in the scene. This limitation primarily stems from using a low-resolution text-to-image diffusion model as the base during the ViLex model's pretraining phase. We hypothesize that this issue could potentially be mitigated by employing a higher-resolution T2I model as the base model.\nPrompting a frozen T2I model with both visual and tex-tual prompts. In the main paper, we have demonstrated that ViLex tokens can serve as a novel visual \"language\" for multimodal image generation. Unlike methods such as DreamBooth [58, 59] and textual inversion [18], which re-quire: (1) learning specialized text tokens for specific in-stances, (2) gradient-based training for each individual im-age, and (3) the use of LORA adapters [29] to modify the model architecture, DreamBooth must be fine-tuned sepa-rately for each object (or each set of images correspond-ing to the same object). In contrast, ViLex enables several DreamBooth tasks like image re-contextualization, artistic rendition and accessorization, as illustrated in Figure A3, Figure 5 and Figure 6, by simply prompting a frozen T2I model with a combination of our visual prompts (i.e., ViLex tokens) and natural language text prompts. This approach does not require changes to the architecture of a pretrained text-to-image generation model or any fine-tuning of the T2I model itself. All tasks are performed in a zero-shot and unsupervised manner."}, {"title": "Training loss.", "content": "We adopt the standard diffusion [27, 66, 67] training objective to optimize ViLex, backpropagating the reconstruction loss to update its parameters. In a diffusion model, the denoising objective aims to learn a model  \u03f5 \u03b8 ( x t , t ) that predicts the noise  \u03f5  added to data  x 0  at timestep t. Given a noisy sample  x t , the objective minimizes the difference between the predicted and true noise:\nL denoise  = E  x 0 , \u03f5 , t  [ \u2225 \u03f5 \u2212  \u03f5 \u03b8 ( x t , t )\u2225 2 ],\nwhere  x t  =\u221a \u03b1 t x 0  +\u221a 1 \u2212 \u03b1 t \u03f5 , with  \u03b1 t  controlling the noise schedule. This loss enables the model to reverse the diffusion process, gradually reconstructing  x 0  from  x t ."}, {"title": "CFG, two sets of samples are generated:", "content": "In CFG, the final prediction \u03f5 guided  is computed as:\n\u03f5 guided  =  \u03f5 \u03b8 ( x t ) + w \u22c5 ( \u03f5 \u03b8 ( x t , c ) \u2212  \u03f5 \u03b8 ( x t ))"}, {"title": "Text-Free Guidance", "content": "The TFG noise prediction  \u03f5 tfg  is then computed as:\n\u03f5 tfg  =  \u03f5 \u03b8 ( x t , v ) + w tfg \u22c5 ( \u03f5 \u03b8 ( x t , [ v , c ]) \u2212  \u03f5 \u03b8 ( x t , v ))"}]}