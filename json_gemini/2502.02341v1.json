{"title": "Test Time Training for 4D Medical Image Interpolation", "authors": ["Qikang Zhang", "Yingjie Lei", "Zihao Zheng", "Ziyang Chen", "Zhonghao Xie"], "abstract": "4D medical image interpolation is essential for improving temporal resolution and diagnostic precision in clinical applications. Previous works ignore the problem of distribution shifts, resulting in poor generalization under different distribution. A natural solution would be to adapt the model to a new test distribution, but this cannot be done if the test input comes without a ground truth label. In this paper, we propose a novel test time training framework which uses self-supervision to adapt the model to a new distribution without requiring any labels. Indeed, before performing frame interpolation on each test video, the model is trained on the same instance using a self-supervised task, such as rotation prediction or image reconstruction. We conduct experiments on two publicly available 4D medical image interpolation datasets, Cardiac and 4D-Lung. The experimental results show that the proposed method achieves significant performance across various evaluation metrics on both datasets. It achieves higher peak signal-to-noise ratio values, 33.73dB on Cardiac and 34.02dB on 4D-Lung. Our method not only advances 4D medical image interpolation but also provides a template for domain adaptation in other fields such as image segmentation and image registration. The code is available at TTT4DMII.", "sections": [{"title": "I. INTRODUCTION", "content": "4D medical image interpolation focuses on generating intermediate frames from a sequence of medical images, helping to create smoother and more detailed representations of organs or tissues over time. This technique is especially useful in capturing subtle changes during procedures like heartbeats or lung movement, which are critical for accurate diagnosis and treatment. While video interpolation methods are widely applied in fields like film and animation, the unique constraints and requirements of medical imaging make it challenging to apply these methods to 4D medical image interpolation.\nIn CT scans, patients are exposed to higher levels of radiation, which can increase the risk of secondary cancers, making data collection challenging. In MRI, the data acquisition rate is slow, leading to motion artifacts such as blurring caused by factors like patient movement, unstable breathing, and difficulty maintaining a steady position during long scan times.\nTo tackle these challenges, many state-of-the-art methods have been proposed. Some of them focus on capturing periodic organ motion by improving model structure [1, 2] or leveraging Transformer architectures to address the limitations of CNNs in modeling long-range spatial dependencies [3], whereas others put their attention on optimization methods to enhance training and inference efficiency [4, 5]. In addition, some researchers propose novel auxiliary losses to better improve the medical image interpolation task [6].\nDespite great progress previous works have made, very few of them care about distribution shifts. For example, during deployment, due to different imaging devices, the data received by the model and the data used for training do not come from the same distribution. This discrepancy often leads to poor performance, which inspires us to borrow the idea from Test-Time Training (TTT) and propose a TTT-based training paradigm.\nThe basic idea of TTT is to use self-supervised learning to adapt the model to a new distribution [7, 8]. We modify the model during test time to enhance its performance on specific instance. The only issue is that the test data lacks the corresponding labels. But we can use self-supervised learning to generate pseudo-labels directly from the input data itself. As shown in figure 1, the TTT framework resembles a sideways \"Y\" structure. The head represents the feature extractor, while the upper and lower branches correspond to the main task network and the self-supervised auxiliary network, respectively. During training, TTT optimizes both the main network and the self-supervised network. At test time, each test input is first processed through the upper branch to let the model adapt to the new distribution using the self-supervised auxiliary task, after which predictions are made for the main task.\nOn top of this, we introduce the TTT design for 4D medical image interpolation(TTT4MII) and incorporate two self-supervision tasks which are commonly used in computer vision: rotation prediction and image reconstruction. We consider the rotation prediction task a four-class classification task. To be specific, we randomly rotate the input (0\u00b0, 90\u00b0, 180\u00b0, or 270\u00b0) and pass it through the model to perform the classification task. For the image reconstruction task,"}, {"title": "II. RELATED WORK", "content": "4D medical image interpolation tackles the challenge of generating high-resolution temporal data, which is often limited by factors such as radiation exposure and scan time. VoxelMorph provides a fast, learning-based framework for generating deformation fields, optimizing registration and interpolation tasks with convolutional networks [10]. Another approach by Kim introduced a diffusion deformable model, which integrates diffusion and deformation modules to generate intermediate frames along a geodesic path while preserving spatial topology [11]. Additionally, Kim proposed UVI-Net, an unsupervised framework that directly interpolates temporal volumes without intermediate frames, demonstrating robustness with minimal training data [12]. However, a few of these methods consider the impact of domain shifts, which can significantly degrade model performance in practical clinical settings."}, {"title": "B. Test Time Training", "content": "TTT improves model adaptability to distribution shifts by updating parameters during inference [13]. In image classification, Sun formulated TTT as a self-supervised task on test samples, achieving robust performance under domain shifts [7, 8, 14]. In anomaly detection and segmentation, Costanzino leveraged TTT to use features from test data for training a binary classifier, improving segmentation accuracy without labeled anomalies [15]. In video object segmentation, Bertrand incorporated mask cycle consistency in TTT to counter performance drops caused by video corruptions and sim-to-real transitions [16]."}, {"title": "C. Self-supervised Learning", "content": "Self-supervised learning leverages automatically generated labels from data itself to learn meaningful representations without requiring manual annotations. A common strategy in SSL is to design auxiliary tasks with specific loss functions that guide the model to extract relevant features [17-19]. For instance, Doersch proposed a jigsaw puzzle task where an image is split into patches, and the network predicts their spatial arrangement, promoting an understanding of spatial structure [20]. Similarly, Gidaris used rotation prediction as an auxiliary task, where the model identifies the rotation angle (0\u00b0, 90\u00b0, 180\u00b0, or 270\u00b0) applied to an image, enhancing its sensitivity to geometric transformations [21]. More recently, Chen introduced contrastive learning via SimCLR, which uses a contrastive loss to maximize agreement between augmented views of the same instance, learning invariant representations across transformations [22]. These approaches highlight the versatility of self-supervised learning in capturing robust data representations."}, {"title": "III. METHODS", "content": "In this section, we describe our proposed TTT4MII method. It can be implemented on top of any generic model architecture, such as feature extractor-prediction head or encoder-decoder frameworks."}, {"title": "A. Problem Setup", "content": "Formally, given a video V consist of n frames, represented as {Io, I1,..., In-1}, where each Ii corresponds to a 3D medical image at time $T = \\frac{i}{n-1}$. Given two consecutive frames Io and In\u22121 at T = 0 and T = 1, respectively, our objective is to predict an intermediate frame \u00cet at any temporal point T = t, where 0 < t < 1."}, {"title": "B. Network Architecture", "content": "Our architecture is designed to allow the interpolation network and the self-supervised auxiliary network to share features. The architecture includes a feature extractor f, which is shared by both the main task head h and the self-supervised auxiliary head g. To be specific, f is a 3D AlexNet, while h is UVI-NET, the interpolation head for predicting the next frame. We define f as the feature extractor with parameters fo and has the main task head with parameters he, such that model(I;0) = h(f(I)), where I represents the input frames. Intuitively, our method aims to update f (and thus fe) during test time using gradients from the auxiliary head g, allowing f to adapt to the test distribution. The self-supervised auxiliary head g with parameter ge, takes the output of f as its input (as shown in Figure 1). In this work, we employ two self-supervised tasks: rotation prediction and image reconstruction."}, {"title": "C. Rotation Prediction and Masked Autoencoders", "content": "We use rotation prediction as one of the self-supervised auxiliary tasks. To be specific, we rotate the input frame by one of four possible angles (0\u00b0, 90\u00b0, 180\u00b0, and 270\u00b0) and input this rotated frame to the model. The task is defined as a four-way classification problem, where the model predicts the rotation angle of the input image. During TTT, the auxiliary task generates pseudo-labels for rotation, and the model adapts to the test distribution by minimizing the cross-entropy loss:\n$L_g = \\frac{1}{N} \\sum_{i=1}^N \\sum_{c=1}^4 \\theta_{i,c} log(p_{i,c})$\nwhere $p_{i,c}$ is the predicted probability for the c-th rotation angle of the i-th input frame, $\\theta_{i,c}$ is the ground truth label for self-supervision, and N is the number of input frames. This auxiliary task allows the model to learn spatial and structural representations beneficial for the interpolation process, helping the model adapt to unseen data by updating its parameters.\nAs an alternative self-supervised task, we use image reconstruction. In image reconstruction, a simple yet effective method is MAE, which masks a large proportion of patches to create a challenging task, helping the model learn generalizable features. We extend the original MAE architecture by replacing the ViT with 3D-ViT so that it can handle 3D inputs. Each input image x is first divided into many small patches. Then, we randomly mask 80% of the patches in x and feed the remaining patches into an autoencoder. The self-supervised objective $L_g(g \\circ f(x), x)$ compares the masked patches reconstructed by $g \\circ f(x)$ with the original masked patches in x and compute the pixel-wise mean squared error."}, {"title": "D. Test Time Training", "content": "Here, we introduce three different schemes of TTT: a) Na\u00efve TTT, b) Online TTT, and c) Mini-batch TTT. The objective for all three is to optimize the loss function:\nmin E = $L_g(g \\circ f(x), x)$\nNa\u00efve TTT: In this setting, we adapt the model using all the test data batches at once before making predictions. This scheme assumes access to a collection of unlabeled batches b1,b2,..., bm and the pre-trained model weights \u03b80. The model performs multiple epoch across all batches to optimize the weights, producing an optimized model (f0,90). This naive idea is that the model adapts globally access all visible data, without considering the independence of each individual"}, {"title": "IV. EXPERIMENTAL SETTINGS", "content": "We conduct our experiments on the Cardiac and the 4D-Lung dataset.\nThe Cardiac dataset includes 100 heart scans, capturing motion between the end-diastolic and end-systolic phases. The dataset is split into 90 scans for training and 10 scans for testing. On average, there are 10 frames between these two phases, providing the temporal resolution necessary for evaluating interpolation methods.\nThe 4D-Lung dataset consists of 82 chest CT scans from 20 lung cancer patients, taken at the end-inspiratory and end-expiratory phases. The training data comprises scans from 18 patients, while the remaining 2 are used for testing. The dataset is preprocessed to highlight lung-specific features, with normalization, windowing, and bed removal. All images are resized to 128\u00d7128\u00d7128 for consistency."}, {"title": "B. Evaluation Metrics", "content": "To evaluate the performance of our model, we use five common metrics for image interpolation: PSNR (Peak Signal-to-Noise Ratio), NCC (Normalized Cross-Correlation), SSIM (Structural Similarity Index), and NMSE (Normalized Mean Squared Error). PSNR measures the quality of the reconstructed images by comparing their pixel-wise differences. NCC assesses the similarity between the predicted and ground truth frames by measuring correlation. SSIM evaluates the structural similarity, considering luminance, contrast, and texture. NMSE quantifies the difference between the predicted and true images. The metrics we use can provide a comprehensive evaluation of both the quality and perceptual fidelity of interpolated images."}, {"title": "C. Implementation Details", "content": "Our experiments are conducted on an Ubuntu 22.04 environment using a NVIDIA RTX 4090 GPU. During training time, We train our models for 200 epochs with a learning rate of 2 \u00d7 10-4 and 50 epochs for testing time. Using a batch size of 1 is not only necessary to fit the data within the available GPU memory but also more reflective of real-world scenarios, where models are often applied to single patient images or scans at a time.\nThe base model we use is UVI-Net [12], two U-Net architecture composed of a reconstruction model and an optical flow calculator. After the training phase, we freeze the parameters of the decoder during testing. To implement the auxiliary self-supervised task, we introduce a simple prediction head consisting of two fully connected layers to predict the rotation angle of input images. And we extend MAE to a 3D variant, so the encoder can adapt to the new distribution of unseen data. We randomly mask 80% of the input patches and task the encoder with reconstructing the missing regions during training. The auxiliary task facilitates back propagation to update the encoder parameters. During TTT, the model adapts to unseen test data by leveraging this self-supervised optimization. Once TTT concludes, we proceed with interpolation and evaluate the results. This approach make the encoder adapt to the test distribution, improving the robustness and accuracy of the interpolation process. As shown in Figure 1, the encoder is updated during test time using the auxiliary task of rotation prediction or 3D MAE, which enhances the model's robustness and accuracy when processing unseen data.\nWe assess the impact of different self-supervised task and TTT scheme on interpolation task in Cardiac and 4D-Lung datasets. And We compare the results of our method with previous methods to show TTT provides a measurable improvement in accuracy and efficiency."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "As shown in table I and table II, The experimental results demonstrate the effectiveness of our proposed method across two datasets: Cardiac and 4D-Lung. We compare our method with previous methods. Key evaluation metrics include PSNR, NCC, SSIM, and NMSE, with higher PSNR, NCC, and SSIM indicating better performance and lower NMSE indicating reduced error and perceptual dissimilarity.\nOn the Cardiac dataset, our method achieves the highest PSNR (33.73), NCC (0.571), and NMSE (2.230), surpassing the strongest baseline, UVI-Net, which achieves a PSNR of 33.59 and an NCC of 0.565. Our approach also records the lowest NMSE (2.230), reflecting improved interpolation accuracy and perceptual quality. Compared to the widely used VM model, our method delivers a significant improvement of over 2.7 dB in PSNR.\nOn the 4D-Lung dataset, our method similarly outperforms all baselines. It achieves the highest PSNR (34.02), SSIM (0.320), and NCC (0.981). Additionally, our method reduces NMSE to 0.551, further confirming its superior performance in handling lung motion. These results show notable improvements over UVI-Net, which achieves a PSNR of 34.00 and an SSIM of 0.980. Compared to VM, our method demonstrates a 1.7 dB improvement in PSNR.\nThe results show that our TTT framework with auxiliary loss not only improves interpolation accuracy but also generalizes well to unseen data, achieving consistent improvements"}, {"title": "B. Ablation study", "content": "We further analyze the impact of different self-supervised tasks on our TTT framework. In particular, we compare the performance of Rotation Prediction and 3D-MAE as auxiliary self-supervised learning tasks. The results are presented in Table IV, where we evaluate their effects under three different TTT schemes: Na\u00efve TTT, Online TTT, and Mini-Batch TTT.\nAs shown in Table IV, the use of 3D-MAE consistently leads to better interpolation accuracy across all TTT schemes."}, {"title": "C. Qualitative Analysis", "content": "We present a visualization to compare interpolation results in different Settings. Figure 3 illustrates the interpolation results under six different settings, combining two self-supervised tasks with three TTT schemes. The top row displays the predicted frames alongside the ground truth, highlighting the overall structural similarity and temporal coherence of the interpolated results. The bottom row further extracts and visualizes the differences between the predictions and the ground truth, providing a detailed view of where each setting excels or falls short. The results reveal several key observations. First, both self-supervised tasks produce predictions that are visually close to the ground truth, with rotation prediction yielding slightly sharper boundaries in regions of high motion, such as the lung lobes. Image reconstruction, on the other hand, demonstrates better performance in preserving fine-grained textures, particularly in static or slowly moving regions. The difference maps further underscore these trends, with rotation prediction combined with Mini-batch TTT achieving the lowest error in dynamic regions, while image reconstruction paired with Na\u00efve TTT excels in static areas. These qualitative findings align with our quantitative results, demonstrating the robustness of our framework across different self-supervised tasks and adaptation schemes."}, {"title": "VI. CONCLUSION AND FUTURE WORKS", "content": "In this study, we introduced a novel TTT framework for 4D medical image interpolation, leveraging self-supervised auxiliary tasks to address domain shifts during inference. Our method significantly improves interpolation accuracy and robustness by adapting the model to new distribution at test time without requiring any labels. Experiments on the Cardiac and 4D-Lung datasets demonstrate the effectiveness of our approach, with consistent improvements in key metrics such as PSNR, SSIM, and NCC, alongside a notable reduction in NMSE. And our framework is highly flexible, supporting multiple self-supervised tasks and adaptation schemes, making it a strong candidate for real-world clinical use.\nDespite these advancements, several challenges remain. The computational cost of TTT may limit its practicality in time-sensitive scenarios. Additionally, while our experiments focused on cardiac and lung datasets, the generalization of the framework to other anatomical structures and imaging modalities remains to be explored. Future work will focus on two key directions: (1) designing more efficient self-supervised tasks that reduce adaptation time while maintaining performance and (2) evaluating its application to broader medical imaging tasks, including segmentation and registration. By addressing these challenges, we aim to further bridge the gap between"}]}