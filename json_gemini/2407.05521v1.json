{"title": "Accelerating MRI Uncertainty Estimation with Mask-based Bayesian Neural Network", "authors": ["Zehuan Zhang", "Matej Genci", "Hongxiang Fan", "Andreas Wetscherek", "Wayne Luk"], "abstract": "Accurate and reliable Magnetic Resonance Imaging (MRI) analysis is particularly important for adaptive radiotherapy, a recent medical advance capable of improving cancer diagnosis and treatment. Recent studies have shown that IVIM-NET, a deep neural network (DNN), can achieve high accuracy in MRI analysis, indicating the potential of deep learning to enhance diagnostic capabilities in healthcare. However, IVIM-NET does not provide calibrated uncertainty information needed for reliable and trustworthy predictions in healthcare. Moreover, the expensive computation and memory demands of IVIM-NET reduce hardware performance, hindering widespread adoption in realistic scenarios. To address these challenges, this paper proposes an algorithm-hardware co-optimization flow for high-performance and reliable MRI analysis. At the algorithm level, a transformation design flow is introduced to convert IVIM-NET to a mask-based Bayesian Neural Network (BayesNN), facilitating reliable and efficient uncertainty estimation. At the hardware level, we propose an FPGA-based accelerator with several hardware optimizations, such as mask-zero skipping and operation reordering. Experimental results demonstrate that our co-design approach can satisfy the uncertainty requirements of MRI analysis, while achieving 7.5 times and 32.5 times speedup on an Xilinx VU13P FPGA compared to GPU and CPU implementations with reduced power consumption.", "sections": [{"title": "I. INTRODUCTION", "content": "Radiotherapy has been commonly used in cancer treatment, which employs ionizing radiation to destroy cancer cells. However, the radiation also affects surrounding normal tissue, thus the precision and the dose of radiation must be carefully adjusted to reduce side effects. Traditionally, imaging and radiation treatments are conducted on separate days using different machines. The development of MR-Linac, an advanced machine capable of imaging tumours and the related organs immediately before delivering radiation, has the potential to revolutionize cancer treatment by adaptive radiotherapy [1] making use of real-time imaging information of the tumour regions to improve targeted therapy while minimizing radiation-induced side effects. Recent efforts have been made to accelerate image reconstruction for adaptive radiotherapy based on magnetic resonance-guided techniques [2]. However, these techniques lack support for uncertainty estimation, a critical component for clinicians in treatment planning. Accurate estimation of uncertainty helps prevent overconfident predictions, thereby improving the reliability and trustworthiness of medical decisions.\nTo address this issue, several probabilistic deep learning approaches [3] can be employed to enhance adaptive radiother-"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "A. Magnetic Resonance Imaging\nMRI is a non-invasive medical technique to assess the health of patients without physical penetration into their bodies [11]. MRI in medical applications works by utilizing a powerful magnetic field and radio waves to generate images of the inside of the body [11]. In the process of MRI, a patient is placed within strong magnetic fields. The magnetic fields cause the protons, primarily those in the abundant hydrogen atoms in the body's water and fat tissues, to respond. This process emits signals and it is possible to localize the origin of these signals and create a 3D spatial mapping of different tissues in the body. An important parameter often mentioned is a b-value [12], which specifies the strength of the diffusion sensitization. For simplification, we can think of b-value as a \"scale\" measurement. Larger b-values capture slow moving water molecules and smaller diffusion distances.\nThe MRI-generated anatomical images undergo a comprehensive analysis to acquire a thorough understanding of the body's internal condition, which is of great importance for clinical treatment and medical research. MRI analysis is a fundamental problem in the medical field, the effectiveness of which determines the treatment outcomes of many diseases. For instance, cancer is a lethal disease with significant morbidity and mortality [13], [14]. A frequently used form of treatment is radiotherapy that utilizes ionizing radiation to eradicate malignant cells. The efficacy largely depends on delivering sufficient radiation dose to the tumour without harming vital organs. If the position of tumors can be precisely located through MRI analysis, the performance of radiotherapy would be significantly augmented. Accordingly, comprehensive MRI analysis possesses the huge potential to guarantee the accurate diagnoses and facilitate treatments [15].\nB. IVIM Model and IVIM-NET\nIn the field of quantitative MRI, the intravoxel incoherent motion (IVIM) model [12], [16], which is able to provide"}, {"title": "C. Methods of Uncertainty Estimation", "content": "Bayesian Neural Network (BayesNN). For BayesNNs, weights are treated as probability distributions instead of point values to predict the posterior of outputs. This allows estimating uncertainty by quantifying the distribution of possible outputs for a given input, rather than just a single point estimate. The commonly employed approaches are Markov Chain Monte Carlo method [28] and Variational Inference [29]. The Markov Chain Monte Carlo method can be considered the best available solution to sample from exact posterior distributions, but the substantial amount of operations is prohibitively expensive for most deep learning models [4], rendering it unpopular. Variational Inference demonstrates superior scalability. The core idea is the use of another distribution to approximate"}, {"title": "III. ALGORITHM-HARDWARE CO-OPTIMIZATION FLOW", "content": "An overview of the algorithm-hardware co-optimization flow is shown in Fig. 1. It is proposed to convert DNN to mask-based BayesNN characterized by hardware-efficient architectures, and to provide hardware optimization strategies to realize efficient model deployment on FPGA.\nPhase 1: Preparation. In the first phase, a neural network architecture should be given. Theoretically, most main-stream networks equipped with dropout [42] layers, which are the popular methods for regularization, are all compatible. Also, uncertainty requirements tailored to the situation and particular constraints are formulated. The uncertainty requirements serve as a basis to assess the uncertainty quality as well.\nIn addition, synthetic datasets are required in the workflow. Typically, models are trained on collected real datasets that have been manually labeled. While this approach is effective for evaluating accuracy, it presents difficulties in assessing uncertainty estimates due to the absence of ground truths of uncertainty for the collected data. To resolve this issue, the utilization of synthetic data is a must. A multitude of datasets are simulated based on domain-specific knowledge. Furthermore, different levels of noise are also injected into simulated data in accordance with predefined uncertainty requirements to generate distinct synthetic data, each representing a scenario. More simulated situations enable a more comprehensive evaluation of the network's performance across diverse scenarios. In this way, precise labels and noise levels can be obtained easily for synthetic data, which serves as a viable solution to the issue of collecting measured data and ground truths.\nPhase 2: Algorithm. The second phase processes model design, training and evaluations. To convert the given architecture to a BayesNN, the Masksembles approach is selected for this purpose, since it covers a range of ensemble-like models of which Monte Carlo Dropout [6] and Deep Ensembles [32] are extreme examples [10] as stated in II-C. The hyperparameter settings of the Masksembles can also be regulated to ensure high-quality uncertainty estimates. Moreover, it is a plug-in module that can be directly inserted into an existing neural network, requiring only minor modifications to the network. Hence, it is characterized by its general applicability. In addition, since the masks are fixed, the position of neurons to be retained or dropped can be determined explicitly, thereby eliminating the randomness during inference, making it more efficient for hardware design.\nThen, the model is trained on synthetic datasets. Given that the dropped positions are predetermined, it works like an enhanced version of conventional dropout techniques. A grid search is conducted for the dropout rate ranging from 0.1 to 0.9 with a step size of 0.1, and the sampling number is varied among 4, 8, 16, 32, 64 to find the optimal hyperparameters.\nAfter training, synthetic data with ground truths are used for evaluations. Evaluation results show whether the network satis-"}, {"title": "IV. ALGORITHM DESIGN OF UIVIM-NET", "content": "Fig. 2 illustrates the architecture of IVIM-NET, consisting of 4 identical separate sub-networks, and each sub-network mainly consists of 3 parts. In the first part, one linear layer is followed by batch normalization, the ReLU function, and the dropout layer. The second part is the same as the first part. The third part is only one linear layer, called the encoder. The outputs of the encoder are fed to the Sigmoid function to be the outputs of this sub-network. Finally, the conversion function, denoted as C(.), transforms the sub-network outputs to the corresponding parameters as results. The width of linear layers equals to the number of b-values of input voxels. Regarding uncertainty requirements, it is expected that output uncertainty shrinks with less noise.\nIn order to produce synthetic data, the physical equation\n\n$\\frac{S}{S_{b=0}} = fe^{-bD^*} + (1 - f)e^{-bD}$\n\nis exploited to simulate data. The ranges of So, D*, D, and f are set according to real scenarios, then these parameters are randomly specified to calculate signal density S for each data point. Afterwards, noise is injected to corrupt clean data, which frequently occurs in real collected data. Gaussian noise with mean 0 and standard deviation $S_0/SNR$ is added with different signal-to-noise ratios (SNR) which determine the noise levels simulating different scenarios. As a result, synthetic data with different noise levels are produced.\nTo create uIVIM-NET, the dropout blocks are replaced"}, {"title": "V. HARDWARE DESIGN", "content": "In Phase 3, we develop a parallel and pipelined uIVIM-NET accelerator by tailoring the accelerator architecture specifically to the model. Fig. 3 gives an overview architecture consisting of an I/O manager, an intermediate layer cache, multiple identical processing elements (PEs), and a controller. The I/O manager serves as a repository for input data and outputs from PEs, while the intermediate layer cache is utilized to temporarily store intermediate results. The controller manages the internal state of the accelerator, overseeing the progress of the computation, coordinating the sourcing and storage of data, and determining when to retrieve a sample.\nA. Analysis of Parallelism\nThe uIVIM-NET features several parallelism opportunities. First, input parallelism can be achieved by processing multiple elements in the input data concurrently, especially when processing all elements simultaneously in one voxel. Second, output parallelism is attainable by designing different PEs to handle independent output neurons in FC layers. Third, sub-network parallelism can be exploited by creating parallel processing blocks for each of 4 independent separate sub-networks, requiring more DSP resources. Fourth, sample parallelism can be achieved by processing all sampling obtained from evaluating each sub-network multiple times simultaneously. In summary, our design prioritizes input and output parallelism due to resource constraints, while sub-network"}, {"title": "C. Processing Module", "content": "Processing modules are responsible for calculating the outputs of layers in the model. As shown in Fig. 3, input data are passed into processing units (PU) first, which includes a block of parallel multipliers followed by a tree of adders to finish dot product calculations, and then bias is added. The activation function performs ReLU. The processing modules are logically organized into identical PEs, with each PE dedicated to computing for a single output neuron, all operating in parallel. Each forward pass is computed layer by layer. Different layers in multiple forward passes share PEs.\nFor each PU in PEs, there is a dedicated memory block to store weights and biases of the uIVIM-NET. Fig. 4 shows the comparison of the previous common scheme and our scheme. In previous accelerators [35], [36] that target for BayesNN based on MC-Dropout, it requires sampling weights following the Bernoulli distribution. In order to achieve sampling randomly and drop corresponding weights, the weights have to be determined during runtime by the Bernoulli Sampler module, and the Dropout module drops corresponding neurons. In this way, it increases the consumed resources, and also increases the latency and power due to more incurred operations. In order to overcome this bottleneck, we propose a mask-zero skipping storage strategy. For the uIVIM-NET, since the dropped positions of weights have been predetermined and are fixed, it is allowed to only store weights which are not dropped, avoiding the need for other modules. Moreover, it is a must to keep some copies, the number of which equals the number of sampling.\nTo achieve maximum performance, we adopt fine-grained pipelining within PUs to maximize throughput. We insert $R_A$ and $R_M$ internal pipeline registers to every adder and every multiplier. Implementing internal pipeline registers minimizes the path between registers, thus allowing for an increase in clock speeds, which in turn speeds up the computation. Pipeline stages are independent; therefore, every adder and multiplier can start processing a new value every clock cycle, despite the latency of every computation being >1. Therefore, if the number of PEs is $N_{PE}$, the number of b-values is $N_b$,"}, {"title": "D. Controller", "content": "To coordinate all modules within the accelerator, the controller uses an internal state machine to dispatch control signals to evaluate each sub-network.\nDuring the inference stage, each sub-network requires multiple forward passes to obtain different sampling. Typically, processing sampling serially is adopted as the operation order, denoted as the sampling-level scheme, as illustrated in Fig. 5. In this order, for each voxel, weights of each sampling must be loaded multiple times to complete sampling, which unavoidably incurs frequent loading operations, increasing the power significantly [8]. To address this issue, we introduce a new batch-level operational order. As described previously, the generated masks are fixed, which means multiple weight sampling should not change for all the voxels. Hence, there is no need to reload each sampling weight repeatedly for each voxel since the same weight configurations can be implemented many times.\nTherefore, for the batch-level scheme, only the weights of one sampling are loaded for evaluations of the whole batch of voxels. Then, once this batch of evaluations is complete, the weights of the next sampling are loaded for evaluations, continuing until all samplings are processed for all voxels in this batch. Afterwards, a new cycle of evaluation begins with the next batch of voxels. Using this scheme, weights of each sampling are only loaded once per batch of voxels. If the number of sampling is N, for each batch, the sampling-level scheme requires N \u00d7 batchsize weight loadings while the batch-level scheme requires N weight loadings. Our scheme reduces the weight loading operations by batchsize times, thereby decreasing power consumption."}, {"title": "VI. EVALUATION", "content": "A. Experimental Setup\nWe use an Intel Xeon Silver 4110 as our CPU platform and an Nvidia GeForce GTX 1080 Ti as our GPU platform to run the uIVIM-NET using Pytorch (1.10.0) framework for the software baseline. Signals are generated using the equation (1) by drawing So, D*, D, and f randomly from reasonable ranges in real cases according to domain experts, with added Gaussian noise. Synthetic datasets with 5 different levels of noise, corresponding to SNR values of 5, 15, 20, 30, and 50, were generated, with each dataset containing 10,000 synthetic data. For each data, S/So is calculated as inputs of the model.\nOur accelerator is designed using Xilinx Vivado 2021.2 written in Verilog, targeting the Xilinx VU13P device, running at 250MHz. The quantization scheme is 16-bit fixed-point representation with 4 integer bits. The simulation results, resource utilization and power consumption after implementation on the Vivado tool are reported. The accelerator features 32 PEs, with each PE capable of processing voxels up to 128 elements, which is enough to support real scenarios, a published IVIM dataset [43]\u2013[45] with 104 b-values. On chip, 20k voxels are stored with a batch size of 64 and a sampling number of 4.\nB. Algorithm Performance\nRoot means squared error (RMSE) between, the reconstruction and predicted individual IVIM parameters, and their respective ground truth values is calculated to assess the overall accuracy. The evaluation results of different SNR values are plotted in Fig. 6. Furthermore, the standard deviation (std) divided by the mean of samples is used to assess the"}, {"title": "VII. CONCLUSION", "content": "This paper presents an algorithm and hardware co-design approach applicable to existing DNNs. This methodology is adopted to optimize medical imaging models to estimate uncertainty and to customize an accelerator for fast MRI analysis. Experiments demonstrate the effectiveness in various scenarios, which provide useful insights for applications. Our customized accelerator also exhibits remarkable computational speed and energy efficiency, outperforming both CPUs and GPUs as well as existing FPGA designs.\nOn the basis of the current research, further exploration holds significant potential as well. It is promising to adopt the proposed accelerator to support image-guided treatment, which would involve integrating with other functions such as dose calculation to provide a real-time system for adaptive radiotherapy [46].\nFurthermore, apart from MRI analysis, uncertainty information is also helpful for other applications such as intelligent robots and autonomous driving. The active agents and controller need to make decisions based on incomplete knowledge, and the assumption that the inference situation has the same distribution as training is often invalid in many real scenarios. It is believed that with uncertainty information, more robust performance could be attained. Our proposed framework can be extended to cover these applications."}]}