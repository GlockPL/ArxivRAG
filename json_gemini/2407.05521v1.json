{"title": "Accelerating MRI Uncertainty Estimation with Mask-based Bayesian Neural Network", "authors": ["Zehuan Zhang", "Matej Genci", "Hongxiang Fan", "Andreas Wetscherek", "Wayne Luk"], "abstract": "Accurate and reliable Magnetic Resonance Imaging (MRI) analysis is particularly important for adaptive radiotherapy, a recent medical advance capable of improving cancer diagnosis and treatment. Recent studies have shown that IVIM-NET, a deep neural network (DNN), can achieve high accuracy in MRI analysis, indicating the potential of deep learning to enhance diagnostic capabilities in healthcare. However, IVIM-NET does not provide calibrated uncertainty information needed for reliable and trustworthy predictions in healthcare. Moreover, the expensive computation and memory demands of IVIM-NET reduce hardware performance, hindering widespread adoption in realistic scenarios. To address these challenges, this paper proposes an algorithm-hardware co-optimization flow for high-performance and reliable MRI analysis. At the algorithm level, a transformation design flow is introduced to convert IVIM-NET to a mask-based Bayesian Neural Network (BayesNN), facilitating reliable and efficient uncertainty estimation. At the hardware level, we propose an FPGA-based accelerator with several hardware optimizations, such as mask-zero skipping and operation reordering. Experimental results demonstrate that our co-design approach can satisfy the uncertainty requirements of MRI analysis, while achieving 7.5 times and 32.5 times speedup on an Xilinx VU13P FPGA compared to GPU and CPU implementations with reduced power consumption.", "sections": [{"title": "I. INTRODUCTION", "content": "Radiotherapy has been commonly used in cancer treatment, which employs ionizing radiation to destroy cancer cells. However, the radiation also affects surrounding normal tissue, thus the precision and the dose of radiation must be carefully adjusted to reduce side effects. Traditionally, imaging and radiation treatments are conducted on separate days using different machines. The development of MR-Linac, an advanced machine capable of imaging tumours and the related organs immediately before delivering radiation, has the potential to revolutionize cancer treatment by adaptive radiotherapy [1] making use of real-time imaging information of the tumour regions to improve targeted therapy while minimizing radiation-induced side effects. Recent efforts have been made to accelerate image reconstruction for adaptive radiotherapy based on magnetic resonance-guided techniques [2]. However, these techniques lack support for uncertainty estimation, a critical component for clinicians in treatment planning. Accurate estimation of uncertainty helps prevent overconfident predictions, thereby improving the reliability and trustworthiness of medical decisions.\nTo address this issue, several probabilistic deep learning approaches [3] can be employed to enhance adaptive radiother-apy with uncertainty estimation. BayesNN [4]\u2013[7] stands out as a highly effective approach, which has gained popularity. However, since BayesNN necessitates multiple forward passes to obtain results, the computational load is typically several times higher than that of DNN, posing a challenge for real-time processing which is critical for adaptive radiotherapy. To meet the clinical requirements and help towards wider adoption, reliable and trustworthy predictions with well-calibrated estimations and fast processing speed must be attained. Therefore, it is imperative to optimize the algorithm to fully leverage the potential of existing models for MRI analysis, and design a customized accelerator to support adaptive radiotherapy.\nHowever, there are several challenges. First, the frequent runtime sampling essential for BayesNNs execution leads to considerable resource and latency overhead, reducing the efficiency of uncertainty estimation. Second, conventional BayesNN introduces inherent randomness into the model to compute uncertainty, and weight configurations can only be determined during runtime, which complicate the development of efficient hardware solutions. Third, BayesNN usually involves multiple sampling for each data item during inference, so weights need to be reloaded multiple times, resulting in high power consumption [8], [9].\nThis paper proposes a novel accelerated approach based on Bayesian neural networks, to achieve high-performance MRI analysis with uncertainty estimation. It is intended to be the first step to support uncertainty estimation in adaptive radiotherapy. An algorithm-and-hardware co-design flow is developed to endow DNN with the ability to estimate the uncertainty of predictions in real-time while adhering to low power consumption requirements. To eliminate the runtime sampling required by conventional BayesNNs, we adopt Masksembles [10], an efficient mask-based BayesNN for MRI uncertainty estimation. To facilitate the seamless adoption of Masksembles for MRI analysis, we propose a novel transformation design flow that effectively converts IVIM-NET to Masksembles-IVIM, abbreviated as uIVIM-NET. By employing pre-defined fixed masks in Masksembles, we circumvent the inherent randomness of conventional BayesNNs to allow us to efficiently skip invalid operations for further hardware optimization. At the hardware level, benefiting from the fact that weight configurations are determined in advance, we adopt the mask-zero skipping scheme to drop the specified weights offline. In addition, in order to avoid frequent weight"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "A. Magnetic Resonance Imaging\nMRI is a non-invasive medical technique to assess the health of patients without physical penetration into their bodies [11]. MRI in medical applications works by utilizing a powerful magnetic field and radio waves to generate images of the inside of the body [11]. In the process of MRI, a patient is placed within strong magnetic fields. The magnetic fields cause the protons, primarily those in the abundant hydrogen atoms in the body's water and fat tissues, to respond. This process emits signals and it is possible to localize the origin of these signals and create a 3D spatial mapping of different tissues in the body. An important parameter often mentioned is a b-value [12], which specifies the strength of the diffusion sensitization. For simplification, we can think of b-value as a \"scale\" measurement. Larger b-values capture slow moving water molecules and smaller diffusion distances.\nThe MRI-generated anatomical images undergo a comprehensive analysis to acquire a thorough understanding of the body's internal condition, which is of great importance for clinical treatment and medical research. MRI analysis is a fundamental problem in the medical field, the effectiveness of which determines the treatment outcomes of many diseases. For instance, cancer is a lethal disease with significant morbidity and mortality [13], [14]. A frequently used form of treatment is radiotherapy that utilizes ionizing radiation to eradicate malignant cells. The efficacy largely depends on delivering sufficient radiation dose to the tumour without harming vital organs. If the position of tumors can be precisely located through MRI analysis, the performance of radiotherapy would be significantly augmented. Accordingly, comprehensive MRI analysis possesses the huge potential to guarantee the accurate diagnoses and facilitate treatments [15].\nB. IVIM Model and IVIM-NET\nIn the field of quantitative MRI, the intravoxel incoherent motion (IVIM) model [12], [16], which is able to provide"}, {"title": "C. Methods of Uncertainty Estimation", "content": "internal microscopic information, shows great potential [17]\u2013[24]. The IVIM model captures key information of internal microscopic parameters and can be used to explain signal attenuation caused by microscopic motions, which are primarily characterized as a function of the diffusion of water within tissue (diffusion) and the blood flow (perfusion).\nTraditionally, to fit parameters of IVIM to observed data, the least squares method and Bayesian inference [25] are used on a pixel-by-pixel basis. However, these approaches suffer from long fitting times and poor repeatability of the fitted model parameters, limiting wide clinical use. The limitations of traditional fitting methods have promoted the exploration of advanced techniques to overcome these bottlenecks.\nRecently, the growing of deep learning areas has greatly contributed to the advancements in the medical field. The advent of DNN spurred the creation of the IVIM-NET, designed to estimate the parameters of the IVIM model, achieving state of the art performance both in prediction quality and speed [26], [27], thereby holding great promise for enhancing the clinical applicability of the IVIM model in MRI analysis.\nIVIM-NET [26], [27] is introduced to solve the inverse IVIM problem:\n$\\frac{S}{S_{b=0}} = f e^{-b D^{*}} + (1 - f) e^{-b D}$     (1)\nwhere $S$ is the signal intensity of measurements. A set of $S$ values are measured under different conditions determined by $b$, where $b$ represents the b-value of measured voxels. $S_{b=0}$ is the signal intensity when b-value is 0, while $D$, $D^{*}$ and $f$ represent the signal attenuation caused by the Brownian motion of water molecules, the signal attenuation caused by blood flow and the fraction of incoherently flowing blood flow in the tissue, respectively.\nThe IVIM-NET architecture consists of 4 separate subnetworks, with each sub-network dedicated to predicting one parameter: $D$, $D^{*}$, $f$, and $S_{b=0}$. Each sub-network has an identical fully-connected layer architecture. Input data are normalized measurements, $S/S_{b=0}$ of voxels, and the dimension of inputs equals the number of b-values of input data. After every hidden layer, batch normalization and a ReLU activation function are applied.\nC. Methods of Uncertainty Estimation\nBayesian Neural Network (BayesNN). For BayesNNs, weights are treated as probability distributions instead of point values to predict the posterior of outputs. This allows estimating uncertainty by quantifying the distribution of possible outputs for a given input, rather than just a single point estimate. The commonly employed approaches are Markov Chain Monte Carlo method [28] and Variational Inference [29]. The Markov Chain Monte Carlo method can be considered the best available solution to sample from exact posterior distributions, but the substantial amount of operations is prohibitively expensive for most deep learning models [4], rendering it unpopular. Variational Inference demonstrates superior scalability. The core idea is the use of another distribution to approximate"}, {"title": "III. ALGORITHM-HARDWARE CO-OPTIMIZATION FLOW", "content": "the posterior. Gaussian distributions are commonly utilized as proxy distributions to finish the inference [7], [30]. An alternative to directly estimating model parameters is to approximate inference from multiple predictions of the model, which saves computational overhead. In this approach, the Monte Carlo Dropout (MC-Dropout) method [6] which utilizes Bernoulli distributions is popular since it does not require large modifications to existing network architectures. But it often estimates uncertainty with lower quality [31]. Most methods require the utilization of specific distributions to finish samplings, thereby introducing inherent randomness, which can make hardware design difficult.\nEnsemble method. The ensemble method for uncertainty estimation in deep learning involves using multiple models to make predictions, and then aggregating the predictions to estimate uncertainty with the variance of the individual predictions serving as a measure of uncertainty. This can be done by combining the outputs of multiple models, or by training an ensemble of models to make predictions. The Deep Ensembles method [32] is a kind of this methods, and is able to achieve well-calibrated uncertainty estimation. But ensemble methods typically require heavy operational costs due to implementing a large set of networks.\nMasksembles. MC-Dropout and Deep Ensembles are popular approaches for uncertainty estimation, while there is a trade-off between the algorithm performance and computing costs between these methods [10]. Masksembles [10] is proposed to combine the advantages, and can be utilized to generate models capable of estimating uncertainty, which are defined as mask-based BayesNN in this paper. It generates a set of less correlated masks in advance, which keep fixed during training and inference as well. The masks are followed after the fully-connected (FC) layer or feature maps to keep or drop the corresponding neurons or channels. During inference, the masks are also applied. For each input, a set of sampling results are obtained to calculate predictions and uncertainty. Since the masks generated are less correlated, the quality of estimated uncertainty is comparable to Deep Ensembles. As a result, it improves performance while maintaining low computing costs. For more details about the ways to generate the masks, please refer to the work [10].\nD. BayesNN Accelerators\nThe development of customized BayesNN accelerators has attracted much attention [33]\u2013[39]. The work [33] is the first to accelerate BayesNNs based on Variational Inference, which elaborated on ways to generate random numbers in detail. BYNQNet [34] exploits the sampling-free method and implements the model on the PYNQ-Z1 board. The approach adopts moment propagation for inference at a low hardware cost. ASBNN [40] explores the relationship among multiple forward passes to achieve approximate calculations. The methods in [35] [36] involve thorough explorations on the structured sparsity of Monte Carlo Dropout-based convolutional BayesNNs, and designed FPGA-based hardware accelerators. [41] combined Monte Carlo Dropout and Multi-Exit methods and designed accelerators with spatial-temporal mapping strategies. Nonetheless, previous designs have not been applied to support real-time MRI analysis.\nIII. ALGORITHM-HARDWARE CO-OPTIMIZATION FLOW\nAn overview of the algorithm-hardware co-optimization flow is shown in Fig. 1. It is proposed to convert DNN to mask-based BayesNN characterized by hardware-efficient architectures, and to provide hardware optimization strategies to realize efficient model deployment on FPGA.\nPhase 1: Preparation. In the first phase, a neural network architecture should be given. Theoretically, most main-stream networks equipped with dropout [42] layers, which are the popular methods for regularization, are all compatible. Also, uncertainty requirements tailored to the situation and particular constraints are formulated. The uncertainty requirements serve as a basis to assess the uncertainty quality as well.\nIn addition, synthetic datasets are required in the workflow. Typically, models are trained on collected real datasets that have been manually labeled. While this approach is effective for evaluating accuracy, it presents difficulties in assessing uncertainty estimates due to the absence of ground truths of uncertainty for the collected data. To resolve this issue, the utilization of synthetic data is a must. A multitude of datasets are simulated based on domain-specific knowledge. Furthermore, different levels of noise are also injected into simulated data in accordance with predefined uncertainty requirements to generate distinct synthetic data, each representing a scenario. More simulated situations enable a more comprehensive evaluation of the network's performance across diverse scenarios. In this way, precise labels and noise levels can be obtained easily for synthetic data, which serves as a viable solution to the issue of collecting measured data and ground truths.\nPhase 2: Algorithm. The second phase processes model design, training and evaluations. To convert the given architecture to a BayesNN, the Masksembles approach is selected for this purpose, since it covers a range of ensemble-like models of which Monte Carlo Dropout [6] and Deep Ensembles [32] are extreme examples [10] as stated in II-C. The hyperparameter settings of the Masksembles can also be regulated to ensure high-quality uncertainty estimates. Moreover, it is a plug-in module that can be directly inserted into an existing neural network, requiring only minor modifications to the network. Hence, it is characterized by its general applicability. In addition, since the masks are fixed, the position of neurons to be retained or dropped can be determined explicitly, thereby eliminating the randomness during inference, making it more efficient for hardware design.\nThen, the model is trained on synthetic datasets. Given that the dropped positions are predetermined, it works like an enhanced version of conventional dropout techniques. A grid search is conducted for the dropout rate ranging from 0.1 to 0.9 with a step size of 0.1, and the sampling number is varied among 4, 8, 16, 32, 64 to find the optimal hyperparameters.\nAfter training, synthetic data with ground truths are used for evaluations. Evaluation results show whether the network satis-"}, {"title": "IV. ALGORITHM DESIGN OF UIVIM-NET", "content": "fies uncertainty requirements across all situations, or whether it exhibits subpar performance in certain situations. These results could provide valuable insights for practical applications.\nIf all uncertainty requirements are satisfied, the design flow continues to Phase 3 for hardware design; otherwise, it implements iterative improvements, such as changing the given model architecture, and then restarting the flow.\nPhase 3: Hardware. In the third phase, the trained model and the fixed mask generated in Phase 2 are obtained. We model latency and resource consumption, and concentrate on efficient hardware architecture design.\nFor latency and resource models, they mainly depend on the size of the network, the pipeline design and the DSP resource consumed. We propose simple analytic formulations for their estimation. If high performance is desirable, we can map the network onto an FPGA with efficient hardware optimizations.\nThe model obtained contains hardware-efficient characteristics, which are exploited to address pain points of BayesNN in hardware design. We propose mask-zero skipping strategies to configure weights offline, eliminating randomness completely. We also reorder operations to avoid frequent weight loading, significantly reducing power consumption. Further details are elaborated in Section V.\nIV. ALGORITHM DESIGN OF UIVIM-NET\nFig. 2 illustrates the architecture of IVIM-NET, consisting of 4 identical separate sub-networks, and each sub-network mainly consists of 3 parts. In the first part, one linear layer is followed by batch normalization, the ReLU function, and the dropout layer. The second part is the same as the first part. The third part is only one linear layer, called the encoder. The outputs of the encoder are fed to the Sigmoid function to be the outputs of this sub-network. Finally, the conversion function, denoted as $C(\u00b7)$, transforms the sub-network outputs to the corresponding parameters as results. The width of linear layers equals to the number of b-values of input voxels. Regarding uncertainty requirements, it is expected that output uncertainty shrinks with less noise."}, {"title": "V. HARDWARE DESIGN", "content": "In order to produce synthetic data, the physical equation (1) is exploited to simulate data. The ranges of $S_0$, $D^*$, $D$, and $f$ are set according to real scenarios, then these parameters are randomly specified to calculate signal density $S$ for each data point. Afterwards, noise is injected to corrupt clean data, which frequently occurs in real collected data. Gaussian noise with mean 0 and standard deviation $S_0/SNR$ is added with different signal-to-noise ratios (SNR) which determine the noise levels simulating different scenarios. As a result, synthetic data with different noise levels are produced.\nTo create uIVIM-NET, the dropout blocks are replaced by masks given by the Masksembles, and these are enabled during training and inference. For training, the loss function is in line with that of IVIM-NET. Specifically, each network is responsible for estimating a specific parameter that can be utilized to reconstruct inputs. The loss is calculated as the mean-square error (MSE) between the input and the reconstructed input derived using equation (1).\nIn the evaluation stage, for every input, the network is evaluated multiple times to obtain a set of sampling of predictions of individual parameters and reconstruction. The mean is the final prediction value, and the standard deviation provides a measure of the associated uncertainty.\nV. HARDWARE DESIGN\nIn Phase 3, we develop a parallel and pipelined uIVIM-NET accelerator by tailoring the accelerator architecture specifically to the model. Fig. 3 gives an overview architecture consisting of an I/O manager, an intermediate layer cache, multiple identical processing elements (PEs), and a controller. The I/O manager serves as a repository for input data and outputs from PEs, while the intermediate layer cache is utilized to temporarily store intermediate results. The controller manages the internal state of the accelerator, overseeing the progress of the computation, coordinating the sourcing and storage of data, and determining when to retrieve a sample.\nA. Analysis of Parallelism\nThe uIVIM-NET features several parallelism opportunities. First, input parallelism can be achieved by processing multiple elements in the input data concurrently, especially when processing all elements simultaneously in one voxel. Second, output parallelism is attainable by designing different PEs to handle independent output neurons in FC layers. Third, sub-network parallelism can be exploited by creating parallel processing blocks for each of 4 independent separate subnetworks, requiring more DSP resources. Fourth, sample parallelism can be achieved by processing all sampling obtained from evaluating each sub-network multiple times simultaneously. In summary, our design prioritizes input and output parallelism due to resource constraints, while sub-network"}, {"title": "D. Controller", "content": "parallelism and sample parallelism are not picked. It is possible to process the latter serially and we adopt optimizations based on the batch-level scheme.\nB. Memory\nThe I/O manager stores input data and outputs from PEs. It is implemented using BRAM blocks, so the entirety of the inputs should be held in on-chip memory. If the amount of voxels exceeds the limit, it is possible to store input data in batches since all the voxels are independent.\nThe intermediate layer cache is used to store temporary results. Since the model is computed layer by layer, the results of early layers are stored in the cache. Moreover, if the size of a linear layer is larger than the number of processing elements, the intermediate layer cache stores partial results of computing linear layers serially as well.\nC. Processing Module\nProcessing modules are responsible for calculating the outputs of layers in the model. As shown in Fig. 3, input data are passed into processing units (PU) first, which includes a block of parallel multipliers followed by a tree of adders to finish dot product calculations, and then bias is added. The activation function performs ReLU. The processing modules are logically organized into identical PEs, with each PE dedicated to computing for a single output neuron, all operating in parallel. Each forward pass is computed layer by layer. Different layers in multiple forward passes share PEs.\nFor each PU in PEs, there is a dedicated memory block to store weights and biases of the uIVIM-NET. Fig. 4 shows the comparison of the previous common scheme and our scheme. In previous accelerators [35], [36] that target for BayesNN based on MC-Dropout, it requires sampling weights following the Bernoulli distribution. In order to achieve sampling randomly and drop corresponding weights, the weights have to be determined during runtime by the Bernoulli Sampler module, and the Dropout module drops corresponding neurons. In this way, it increases the consumed resources, and also increases the latency and power due to more incurred operations. In order to overcome this bottleneck, we propose a mask-zero skipping storage strategy. For the uIVIM-NET, since the dropped positions of weights have been predetermined and are fixed, it is allowed to only store weights which are not dropped, avoiding the need for other modules. Moreover, it is a must to keep some copies, the number of which equals the number of sampling.\nTo achieve maximum performance, we adopt fine-grained pipelining within PUs to maximize throughput. We insert $R_A$ and $R_M$ internal pipeline registers to every adder and every multiplier. Implementing internal pipeline registers minimizes the path between registers, thus allowing for an increase in clock speeds, which in turn speeds up the computation. Pipeline stages are independent; therefore, every adder and multiplier can start processing a new value every clock cycle, despite the latency of every computation being >1. Therefore, if the number of PEs is $N_{PE}$, the number of b-values is $N_b$,"}, {"title": "VI. EVALUATION", "content": "which is also the dimension of inputs, and the adder tree is $L$ levels deep. The total latency of a PU is the time it takes to perform multiplication, evaluate the adder tree, accumulate the result of $[N_b/N_{PE}]$ parts over time, and add bias:\n$Latency\\  of\\ PU = R_M + L \\times R_A + ([N_b/N_{PE}] - 1) + R_A$\n$= R_M + R_A(L + 1) + [N_b/N_{PE}] - 1$  (2)\nD. Controller\nTo coordinate all modules within the accelerator, the controller uses an internal state machine to dispatch control signals to evaluate each sub-network.\nDuring the inference stage, each sub-network requires multiple forward passes to obtain different sampling. Typically, processing sampling serially is adopted as the operation order, denoted as the sampling-level scheme, as illustrated in Fig. 5. In this order, for each voxel, weights of each sampling must be loaded multiple times to complete sampling, which unavoidably incurs frequent loading operations, increasing the power significantly [8]. To address this issue, we introduce a new batch-level operational order. As described previously, the generated masks are fixed, which means multiple weight sampling should not change for all the voxels. Hence, there is no need to reload each sampling weight repeatedly for each voxel since the same weight configurations can be implemented many times.\nTherefore, for the batch-level scheme, only the weights of one sampling are loaded for evaluations of the whole batch of voxels. Then, once this batch of evaluations is complete, the weights of the next sampling are loaded for evaluations, continuing until all samplings are processed for all voxels in this batch. Afterwards, a new cycle of evaluation begins with the next batch of voxels. Using this scheme, weights of each sampling are only loaded once per batch of voxels. If the number of sampling is $N$, for each batch, the sampling-level scheme requires $N \u00d7 batchsize weight loadings while the batch-level scheme requires $N weight loadings. Our scheme reduces the weight loading operations by batchsize times, thereby decreasing power consumption.\nVI. EVALUATION\nA. Experimental Setup\nWe use an Intel Xeon Silver 4110 as our CPU platform and an Nvidia GeForce GTX 1080 Ti as our GPU platform to run the uIVIM-NET using Pytorch (1.10.0) framework for the software baseline. Signals are generated using the equation (1) by drawing $S_0$, $D^*$, $D$, and $f$ randomly from reasonable ranges in real cases according to domain experts, with added Gaussian noise. Synthetic datasets with 5 different levels of noise, corresponding to SNR values of 5, 15, 20, 30, and 50, were generated, with each dataset containing 10,000 synthetic data. For each data, $S/S_0$ is calculated as inputs of the model. Our accelerator is designed using Xilinx Vivado 2021.2 written in Verilog, targeting the Xilinx VU13P device, running at 250MHz. The quantization scheme is 16-bit fixed-point representation with 4 integer bits. The simulation results, resource utilization and power consumption after implementation on the Vivado tool are reported. The accelerator features 32 PEs, with each PE capable of processing voxels up to 128 elements, which is enough to support real scenarios, a published IVIM dataset [43]\u2013[45] with 104 b-values. On chip, 20k voxels are stored with a batch size of 64 and a sampling number of 4.\nB. Algorithm Performance\nRoot means squared error (RMSE) between, the reconstruction and predicted individual IVIM parameters, and their respective ground truth values is calculated to assess the overall accuracy. The evaluation results of different SNR values are plotted in Fig. 6. Furthermore, the standard deviation (std) divided by the mean of samples is used to assess the"}, {"title": "VII. CONCLUSION", "content": "uncertainty for each data. This metric measures the relative variance. The evaluation results of different SNR values are also plotted in Fig. 7.\nThe figures show that less noise in input voxels (evaluation SNR is higher) leads to smaller RMSE (higher accuracy) and low uncertainty (more confident), and typically, networks trained with less noisy data tend to exhibit higher levels of accuracy and confidence, which is in line with expectations. The results demonstrate the effectiveness of the framework in successfully converting the existing IVIM-NET to uIVIM-NET empowered with the ability to estimate the uncertainty of predictions. The findings on synthetic data can be a valuable reference to provide guidance in real medical scenarios, thus exhibiting significant potential to enhance MRI analysis. For instance, clinicians are able to set numerical thresholds to determine diagnosis with high uncertainty based on the experimental results on synthetic data, and further adopt more comprehensive medical examinations to treat patients.\nC. Hardware Performance\na) Comparison with existing designs: To demonstrate the benefits of our converted algorithm and customized hardware architecture and optimizations as a whole, we make comparisons against the existing designs in Table I. Previous work [33]\u2013[36] accelerated BayesNNs, but they were not evaluated for medical analysis. As these designs were evaluated on different BayesNNs, it is unfair to compare them in terms of speed unless the same network is executed. Therefore, we choose the energy efficiency, throughput per watt consumed, as metrics. We quote the hardware performance from the original papers. Table I shows that our design achieves more than 2x energy efficiency compared with the work [33] and [34] which accelerate BayesNN only consisted of FC layers similar to ours, indicating significant improvements. The accelerators [35], [36] are optimized for convolutional BayesNN requiring denser operations, resulting in higher power consumption. Compared with those, our design also exhibits higher energy efficiency, demonstrating the effectiveness of the design flow. The advantages of low power and high energy efficiency in our design could be attributed to the hardware optimizations. Firstly in previous work [33], [35], [36], random number generators are designed to determine samplings and dropout operations are implemented during runtime, while the omission of the Sampler and Dropout modules in our architecture results in a significant reduction in power. Secondly, by utilizing a batch-level scheme, the frequent loading of weights is avoided, thus leading to low power consumption.\nTo further showcase the benefits in comparison to existing work, an estimated comparison is presented. Firstly, our approach is a co-design framework. The adoption of Masksembles enhances the software performance and yields advantages in hardware design as well. It is worth noting that previous work [33], [35], [36] only focused on accelerating MC-Dropout BayesNN, so the potential from algorithmic aspect is unexplored. Secondly, hardware optimizations applied to algorithmic architecture generated from the conversion flow reduce power consumption as shown and discussed before. Thirdly, the whole flow of our methodology is more general and can be extended to other mainstream neural networks, thus showcasing significant potential for widespread applications.\nb) Comparison with CPU and GPU: A hardware performance comparison of our FPGA design with both CPU and GPU implementations is shown in Table II.\nEach batch takes our FPGA accelerator 0.28ms on average. The acceleration performance outperforms that of the GPU and CPU by a factor of 7.5 and 32.5, respectively, while using only a fraction of the power. To further illustrate this advantage, we calculated the energy cost of each batch of voxels across different platforms. As indicated in Table II, the proposed design demonstrates substantial energy cost savings per batch, with reductions of approximately 34.4\u00d7 and 82.8\u00d7 when compared to the GPU and CPU versions, respectively. To support uncertainty estimation in adaptive radiotherapy, the processing speed should achieve 0.8ms/batch. The fast speed of our accelerator meets the real-time requirement, indicating considerable promise for practical applications.\nc) Resource Utilization: More experiments are conducted to evaluate resource utilization and performance of the accelerator with varying numbers of PEs. The results, shown in Fig. 8, confirm the main limiting factor of the speed of our design is the amount of available DSP resources. Specifically, when 32 PEs are deployed, the accelerator consumes 67% of all available DSPs with 11.78W power.\nThe relationship between consumed resources and processing speed can also be observed. The utilization of BRAM and IO resources remains relatively constant, as the consumption of BRAM resources primarily depends on the storage of voxels and model weights, while the consumption of IO resources is primarily influenced by the ways to access the outcomes. The number of DSPs for each PE is constant, so DSP resources consumed are proportional to the number of PEs. The processing speed can be estimated based on equation (2), which matches the practical results. With higher parallelism, the accelerator achieves faster speed but also increases power consumption and resource utilization. The relationship presents a trade-off. The parallelism can be determined according to resources available on chip and performance requirements.\nVII. CONCLUSION\nThis paper presents an algorithm and hardware co-design approach applicable to existing DNNs. This methodology is adopted to optimize medical imaging models to estimate uncertainty and to customize an accelerator for fast MRI analysis. Experiments demonstrate the effectiveness in various scenarios, which provide useful insights for applications. Our customized accelerator also exhibits remarkable computational speed and energy efficiency, outperforming both CPUs and GPUs as well as existing FPGA designs.\nOn the basis of the current research, further exploration holds significant potential as well. It is promising to adopt the proposed accelerator to support image-guided treatment, which would involve integrating with other functions such as dose calculation to provide a real-time system for adaptive radiotherapy [46].\nFurthermore, apart from MRI analysis, uncertainty information is also helpful for other applications such as intelligent robots and autonomous driving. The active agents and controller need to make decisions based on incomplete knowledge, and the assumption that the inference situation has the same distribution as training is often invalid in many real scenarios. It is believed that with uncertainty information, more robust performance could be attained. Our proposed framework can be extended to cover these applications."}]}