{"title": "Diff4VS: HIV-inhibiting Molecules Generation with\nClassifier Guidance Diffusion for Virtual Screening", "authors": ["Jiaqing Lyu", "Changjie Chen", "Bing Liang", "Yijia Zhang"], "abstract": "The AIDS epidemic has killed 40 million people and\ncaused serious global problems. The identification of new HIV-\ninhibiting molecules is of great importance for combating the\nAIDS epidemic. Here, the Classifier Guidance Diffusion model\nand ligand-based virtual screening strategy are combined to\ndiscover potential HIV-inhibiting molecules for the first time.\nWe call it Diff4VS. An extra classifier is trained using the HIV\nmolecule dataset, and the gradient of the classifier is used to guide\nthe Diffusion to generate HIV-inhibiting molecules. Experiments\nshow that Diff4VS can generate more candidate HIV-inhibiting\nmolecules than other methods. Inspired by ligand-based virtual\nscreening, a new metric DrugIndex is proposed. The DrugIndex\nis the ratio of the proportion of candidate drug molecules in the\ngenerated molecule to the proportion of candidate drug molecules\nin the training set. DrugIndex provides a new evaluation method\nfor evolving molecular generative models from a pharmaceutical\nperspective. Besides, we report a new phenomenon observed\nwhen using molecule generation models for virtual screening.\nCompared to real molecules, the generated molecules have a\nlower proportion that is highly similar to known drug molecules.\nWe call it Degradation in molecule generation. Based on the\ndata analysis, the Degradation may result from the difficulty of\ngenerating molecules with a specific structure in the generative\nmodel. Our research contributes to the application of generative\nmodels in drug design from method, metric, and phenomenon\nanalysis.", "sections": [{"title": "I. INTRODUCTION", "content": "The AIDS epidemic is a grave global public health issue to-\nday, having resulted in approximately 40 million deaths. More-\nover, AIDS places a significant strain on economic progress\nand contributes to the stigmatization of specific groups [1].\nDespite the availability of diverse anti-HIV medications, chal-\nlenges such as adverse effects, drug resistance, and exorbitant\ntreatment costs persist. Consequently, the discovery of novel\nHIV-inhibiting compounds stands as an urgent and imperative\nendeavor [2].\nWith the development of deep learning, researchers began\nto try to use neural network models to generate specific con-\ntent, such as images or text. VAE(Variational Autoencoders)\n[3],GAN(Generative Adversarial Networks) [4], Diffusion [5],\n[6], and other generative models have shown powerful ca-\npabilities. At the same time, several molecular generation\nmodels have been proposed, such as charRNN [7], JT-VAE [8],\nLatentGAN [9], DiGress [10]. The application of generative\nmodels in drug design is a valuable research direction [11].\nVirtual screening is a mature computer-aided drug design\ntechnique that plays a crucial role in modern drug discov-\nery pipelines [12]. Ligand-based virtual screening methods\nare employed to identify novel hit candidates by retrieving\ncompounds with high similarity to known drug molecules.\nA recent study combines molecular generative models with\nvirtual screening and has made some progress [13].\nIn this paper, we introduce a pioneering approach: Classifier\nGuidance Diffusion for virtual screening, termed Diff4VS.\nThis method enhances the model's capability to generate\nmolecules that are more similar to known drugs, thereby\nincreasing the proportion of HIV-inhibiting candidates. By\nintegrating classifiers to guide the generative process, Diff4VS\nrepresents a significant advance in molecule generation models\nfor virtual screening. Comparison experiments show that our\nmethod generates more HIV-inhibiting candidates than other\nmodels. Ablation experiments show that Classifier Guidance\nwith Binary Cross Entropy(BCE) Loss is effective.\nBesides, evaluating the performance of molecular genera-\ntive models remains a formidable challenge [14]. Traditional\nmetrics often fall short in capturing the nuanced require-\nments of drug-likeness, which is paramount in the practical\napplication of generated molecules. Inspired by ligand-based\nvirtual screening, a new metric DrugIndex is proposed. The\nDrugIndex is the ratio of the proportion of candidate drug\nmolecules in the generated molecule to the proportion of\ncandidate drug molecules in the training set. This metric\nis crucial for ensuring that the generated molecules possess\ncharacteristics that are conducive to drug design.\nAdditionally, through our experiments, we report and dis-\ncuss a new phenomenon observed during virtual screening\nprocesses. We found that molecules generated by current\nmolecule generation models exhibit a lower proportion of\nhigh similarity to known drug molecules compared to real\nmolecules. We call it Degradation in molecule generation.\nBased on the data analysis, the Degradation may result from\nthe difficulty of generating molecules with a specific structure\nin the generative model.\nIn summary, the contributions of this paper are summarized\nas follows:"}, {"title": "II. RELATED WORK", "content": "The core idea of Variational Autoencoders(VAE) [3] is to\nmap the input data to latent variables in the latent space via an\ncoder and the latent variables back to the original data space\nia a decoder. Both encoders and decoders are neural network\nodels trained to maximize the similarity between the original\\ata and the reconstructed data. Researchers have proposed\neveral VAE models that can generate molecules, such as JT-\nAE [8]. Conditional Variational Autoencoder(cVAE) [15] is\nan extended model based on VAE to generate samples with\nconditions."}, {"title": "B. Generative Adversarial Networks for Molecule Generation", "content": "The working principle of Generative Adversarial Net-\nworks(GAN) [4] is based on the idea of adversarial training\nrom game theory. The generator receives random noise as\ninput and tries to generate samples similar to the real data.\nThe classifier, in turn, receives both real samples and sam-\nples generated by the generator and tries to distinguish their\nsources. During the training process, the generator gradually\nimproves the quality of the generated samples. At the same\ntime, the classifier improves the accuracy of discrimination by\ncontinuously distinguishing between real data and generated\ndata. Researchers have proposed several GAN models that\ncan generate molecules, such as LatentGAN [9]. Conditional\nGenerative Adversarial Networks (cGAN) is a variant of GAN."}, {"title": "C. Diffusion Models For Molecule Generation", "content": "The Diffusion model [5], [6] constructs two parameterized\nMarkov chains to diffuse the data with predefined noise and\nreconstruct the desired samples from the noise. In the forward\nchain, the Diffusion model gradually adds pre-designed noise\nto real samples until the samples conform to a specific distri-\nbution, such as a Gaussian distribution. At the same time, real\nsamples and noisy samples are used to train a neural network\nto predict the distribution of noise. Starting from the specific\ndistribution, the reverse chain uses a trained neural network\nto sample noise from the predicted distribution and remove\nthe noise from samples. Researchers have proposed several\nDiffusion models that can generate molecules, such as DiGress\n[10]."}, {"title": "III. METHOD", "content": "In this section, we provide a detailed introduction to our\nproposed method, Diff4VS. As shown in Fig. 1, our method\nmainly consists of four parts. We introduce them in Sec-\ntion III-A, Section III-B, Section III-C and Section III-D. Then\nwe introduce our new metric in Section III-\u0395."}, {"title": "A. Diffusion for Virtual Screening", "content": "Ligand-based virtual screening detects molecules similar\nto known drugs from datasets. However, it is limited by\nmolecular datasets. If there is a lack of molecules similar\nto known drugs in the dataset, virtual screening cannot find\ncandidates. Therefore, the use of known drug molecules to\nguide generative models to generate candidates for virtual\nscreening is noteworthy.\nWe are the first to combine conditional Diffusion with\nvirtual screening. This new paradigm is shown in Fig. 1-\nA. First, with millions of molecules, the Diffusion model is\ntrained to generate molecules. Second, with molecules in HIV\ndataset, an extra classifier is trained to guide Diffusion for\nHIV-inhibiting molecule generation. Third, ligand-base virtual\nscreening technologies are used to detect HIV-inhibiting can-\ndidates from generated molecules."}, {"title": "B. Discrete Diffusion", "content": "A molecule can be viewed as a graph G, atoms are vertexes\nand bonds are edges. Inspired by Austin et al [16], one-hot\nencoding is used to represent the class of atoms and bonds.\nNoise is added separately on each atom and bond feature X.\nInstead of adding Gaussian noise, the feature of each atom\nand bond are multiplied by a matrix Q. $[Q]_{i j}^{t}$ represents the\nprobability of jumping from state i to state j at round t, and\n$Q = \\Pi_{t=1}^{-t} Q^{t}$. Eq 1 is used to add noise.\n$q(X_t | X_{t-1}) = X_{t-1} Q^t$ and $q(X_t | X_0) = X_0 Q^t$ (1)\nInspired by Vignac et al [10], the transition matrices defini-\ntion for atoms and bonds are from the marginal distribution.\nNote that the bonds of molecules are undirected, only the\nupper-triangular part of B is added with noise and then the B\nwill be symmetrized.\nIn the reverse process, $q(x_{t-1}|G_t)$ is calculated for denois-\ning. There is a limited variety of atoms and bonds. So the\nkinds of atoms and bonds can be enumerated. X is the set\nof kinds. For feature $x_{t-1}$, $q(x_{t-1}|G_t)$ can be calculated with\nEq 2.\n$q(x_{t-1}|G_t) = \\sum_{x \\in X} q(x_{t-1} | x_0 = x, x_t)p(x\u00b0 = x | G_t)$ (2)\nVignac et al [10] proved that $q(x_{t-1} | x_0 = x, x_t)$ is propor-\ntional to $x_t (Q^t)^{x-1}$. According to Yang et al [17],\nwhen $q(x_{t-1}|x_t, x_0)$ has a closed-form expression, $x_0$ can be\nused as the target of the neural network.\nAs a result, a neural network $f_\\theta(G_t, t)$ is trained to predict\n$q_\\theta(G^0 | G_t)$ in the forward process of discrete Diffusion. With\n$p_\\theta(x = x | G_t)$, $q_\\theta(x_{t-1}|G_t)$ can be calculated.\nInspired by [18], a loss function with invariance is used in\nEq 3. p is the predicted probabilities for each atom and bond\nin $G^0$. A controls the relative importance of nodes and edges,\nand we set it to 5.0 in our experiments. The training process\nof the discrete Diffusion model is shown in Algorithm 1."}, {"title": "C. HIV-inhibiting Molecule Generation", "content": "We take HIV-inhibiting molecule generation as condition\n$Y_c$. The value of $y_c$ is either 0 or 1. Since $G_t$ and $y_c$ are\ngiven, and $p(y_c | G_t)$ is independent of $G_{t-1}$, 1/$p(y_c | G_t)$ is a\nconstant Z. As a result, $\\hat{q}(G_{t-1} | G_t, y_c)$ is shown in Eq 4.\n$\\hat{q}(G_{t-1} | G_t, y_c) = \\hat{q}(G_{t-1} | G_t) q(y_c | G_{t-1}, G_t) Z$ (4)\nThe noise-adding process is the same whether it is a\nconditional generation or not. As a result,$\\hat{q}(G_{t-1} | G_t)$ is equal\nto $q(G_{t-1} | G_t)$. And $\\hat{q}(G_{t-1} | G_t, y_c)$ can be calculated with\n$q(G_{t-1} | G_t) q(y_c | G_{t-1})Z$.\nWe show the way to calculate $q(G_{t-1} | G_t)$ in Section III-B,\nand $q(y_c | G_{t-1})$ is equivalent to the result of a classifier\nwith noise. That's why Classifier-Guidance Diffusion trains a\nclassifier $g_\\theta$ with noise. However, there are so many possible\nvalues of $G_{t-1}(Atoms: 8^n \\times Bonds: 4^{(n*n)})$. As a result,\n$q(y_c | G_{t-1})$ cannot be obtained directly from the classifier $g_\\theta$.\nInspired by Dhariwal et al [19] and Vignac et al [10],\nwe view G as a continuous tensor of order $n + n^2$(n atoms\nand $n^2$ bonds). Using the first-order Taylor expansion around\n$G_t$, C is a constant independent of $G_{t-1}$, and $\\hat{q}(y_c | G_{t-1})$ is\napproximately equal to Eq 5.\n$\\log \\hat{q}(y_c | G_{t-1}) \\approx \\log \\hat{q}(y_c | G_t) + <\\nabla_G, G_{t-1} - G_t>$\n$= <\\nabla_G, G_{t-1}> + C$ (5)\nWe make the additional assumption that $\\nabla_G \\log \\hat{q}(y_c | G_t)$ is\nproportional to $-\\nabla_G(y_c \\log y_\\theta + (1 - y_c) \\log(1 - y_\\theta))$. There-\nfore, we use the Binary Cross Entropy(BCE) as the loss func-\ntion for the classifier $g_\\theta$ and then estimate $\\nabla_G \\log \\hat{q}(y_c | G_t)$\nwith the gradient of the classifier $g_\\theta$. So we can calculate\n$q(y_c | G_{t-1}, G_t)$ and $\\hat{q}(G_{t-1} | G_t, y_c)$. Based on $\\hat{q}(G_{t-1} | G_t, y_c)$,\nwe sample $G_{t-1}$. This process is repeated and finally, $G_0$\nis generated. The sampling process of the discrete Diffusion\nmode is shown in Algorithm 2."}, {"title": "D. Network Architecture", "content": "A neural network $f_\\theta(G_t,t)$ is trained to predict $q_\\theta(G^0 | G_t)$\nin the forward process of discrete Diffusion. Inspired by Vi-\ngnac et al [10], cycles and spectral features are added as extra\ninputs to improve performance. We use graph transformer\nnetwork [20] with FiLM layers [21] to predict $q_\\theta(G^0 | G_t)$.\nBesides, we use the same graph transformer for classifier\nguidance. The network architecture is shown in Fig 1-D."}, {"title": "E. Proposed Metric", "content": "Molecule generation models have found one important\napplication in the field of drug molecule discovery. Table I\noutlines several common evaluation metrics used to assess the\nperformance of generative models. However, beyond QED,\nthese evaluation metrics do not specifically focus on the ability\nof the models to generate drug-like molecules. Therefore, the\nexisting metrics are unable to meet the requirements of drug\nmolecule design with generative models.\nInspired by ligand-based virtual screening approaches, we\ndesign a novel evaluation metric to better assess the capacity of\nmodels to generate drug-like molecules. $f_a$ denotes the morgan\nfingerprint of the molecule a. The similarity between molecule\na and b is defined in Eq. 6. The similarity of a molecule b\nto a set of molecules A is defined in Eq. 7. Besides, Eq. 8\ncalculates the proportion of molecules in set B that are similar\nto molecules in the known drug molecule set A.\n$MolSim(a, b) = TanimotoSimilarity(f_a, f_b)$ (6)\n$SetMolSim(A, b) = arg \\underset{\\alpha \\in A}{max} MolSim(a, b)$ (7)\n$DrugLike(A, B) = \\frac{\\{b \\in B | SetMolSim(A,b) > 0.5\\}}{|B|}$ (8)\nThe set of known drug molecules is denoted as X, the set of\nmolecules used to train the generative model is denoted as Y,\nand the set of molecules generated by the generative model is\ndenoted as Z. The new metric we define is as shown in Eq. 9.\nWe refer to it as the Drug Index. The Drug Index reflects the\nability of generative models to generate drug-like molecules.\n$DrugIndex(X, Y, Z) = \\frac{DrugLike(X, Z)}{DrugLike(X, Y)}$ (9)\nThe Drug Index is inspired by virtual screening and is\ndesigned to be intuitive and easy to implement. Using the Drug\nIndex, one can evaluate molecule generation models that are\nsuitable for computer-aided drug design tasks such as virtual\nscreening."}, {"title": "IV. EXPERIMENTS", "content": "First, we compare our method with others from Sec-\ntions IV-A through IV-D. Then we conduct ablation experi-\nments in Section IV-E and present some generated candidate\nmolecules in Section IV-F. Additionally, we report and discuss\nthe Degradation phenomenon in Section IV-G. Finally, we\ndiscuss the limitations of our work in Section IV-H. All the\ncode and results will be made available on Github.\u00b9."}, {"title": "A. Dataset", "content": "All molecule generation models are trained on the MOSES\n24] dataset and its properties are listed in Table II.\nThe HIV dataset is a part of the MoleculeNet [25] dataset\nwhich tested the ability to inhibit HIV replication. There are\n41127 molecules in the HIV dataset. Each molecule has an\nactive or inactive label indicating whether it inhibits HIV\nreplication. Referring to the restrictions of molecular weight,\natom type, and bond type in Table II, we construct a subset\nHIV-a. As shown in Table III, the HIV-a dataset contains\n12116 molecules.\nWe perform simple upsampling on the HIV-a dataset,\nreplicating the positive samples until the number of positive\nand negative samples is roughly balanced. The classifier $g_\\theta$\nof Diff4VS is trained on the HIV-a dataset with noise for\nguidance. The hyperparameter A is set to 1000.\nIn addition to these 12,116 molecules, the remaining 29,011\nmolecules were used to train Graph Neural Networks (GNNs)\nfor evaluating the generated molecules, as detailed in Sec-\ntion IV-C. We refer to these 29,011 molecules as the HIV-b\ndataset."}, {"title": "B. Comparison across Generative Models", "content": "We generate 90,000 molecules using charRNN [7], JT-VAE\n8], LatentGAN [9], DiGress [10], and our method. We used\nthe known HIV inhibitor molecules from the MoleculeNet\ndataset [25] as the reference drug molecules. The HIV Drug\nIndex (HIV DI), Quantitative Estimate of Druglikeness (QED),\nand other properties of these generative models are shown in\nTable IV.\nAccording to Table IV, the HIV Drug Index provides a more\neffective way to compare the ability of different generative\nmodels to generate HIV drug molecules. Unlike the HIV Drug\nIndex, the difference in the average QED values of molecules\ngenerated by different generative models is often negligible,\nmaking it challenging to discern differences in their ability to\ngenerate drug-like molecules. In contrast, the newly designed\nDrug Index metric exhibits certain advantages over using QED\nalone.\nBesides, our method is the only one with a Drug Index\ngreater than 100%. The Drug Index of our method is 10.43%\nhigher than the second-best method. This demonstrates that\nour method can generate more molecules that are structurally\nsimilar to existing HIV drugs. Furthermore, virtual screening\nwith our method can identify more candidate drug molecules.\nAccording to the information in Table IV and Fig. 2,\nour method of generating molecules has similar properties\nsuch as Weigh, logP, and SA compared to other generation\nmethods, and the distribution is close to that of MOSES. This\nindicates that the guidance of our classifier has not disrupted\nthe distribution of the original generative model. The average\nQED of our method is the lowest, it is because the average\nQED of active molecules in the HIV dataset is also lower than\nMOSES(0.40 vs 0.81). Therefore, the lowest QED does not\nnegate the ability of our method to generate anti-HIV drug\ncandidates."}, {"title": "C. Comparison of GNNs for Virtual Screening", "content": "Neural networks are also a way of ligand-based virtual\nscreening. We select four representative GNNs and train\nthem on the HIV-b dataset. The four GNNS are GAT(Graph\nAttention Network) [26], GCN(Graph Convolution Network)\n27], MPNN(Message Passing Neural Network) [28] and At-\ntentiveFP [29]. The learning rate of the four models is set to\n0.001, and the number of epochs is set to 500. Models are\nimplemented by DeepChem [30]. We do simple upsampling\non the HIV-b dataset, replicating the active samples until there\nare roughly the same number of active and inactive samples.\nTo objectively reflect the ability of each model to identify\nactive molecules, we conducted 5-fold cross-validation exper-\niments. When the probability given by the model is greater\nthan 50%, the input molecule is considered to be active. The\nresults are shown in Table V. There are significantly more\ninactive samples than active samples in the HIV-b test set.\nTherefore, even if the F1-score is lower than 0.5, the model\nstill demonstrates a certain level of discriminative ability.\nWhen the ROC-AUC is larger than 0.7, the virtual screening\nperformance is considered fair, as suggested by prior studies\n31]. The ROC-AUC results also indicate that graph neural\nnetworks (GNNs) can be a viable approach for the virtual\nscreening of HIV drug molecules."}, {"title": "D. Comparison of Drug Index using GCNS", "content": "To further verify the effectiveness of the our proposed metric\nand method, we use GCNs for additional verification. We train\nthree GCNs on the HIV-b dataset. When the sum of the output\nprobabilities of the three models is greater than 100%(i.e., 2\nout of the 3 models consider the molecule to be active), the\ninput molecule is deemed active. Instead of Eq. 7, the output\nof GCNs is used to calculate DrugLike and DrugIndex. The\nresults are presented in Table VI."}, {"title": "E. Ablation Experiment", "content": "To robustly validate the efficacy of our approach, we con-\nducted ablation studies. Our primary focus was to investigate\nthe impact of the classifier guidance in the diffusion model\nand our innovative adoption of the BCE loss function on the\nfinal results. We established three models: the first excluded\nthe classifier guidance module entirely, the second employed\nMSE as the classifier's loss function, and the third utilized\nBCE as the classifier's loss function.\nThe results are shown in Table VII, we observed that the\nHIV Drug Index (HIV DI) for the model without classifier\nguidance and the model employing MSE as the classifier's loss\nfunction stood at 86.11% and 83.51% respectively. Strikingly,\nthe HIV DI for the model utilizing BCE as the classifier's\nloss function soared to 104.62%, significantly outperforming\nthe other two. This underscores the crucial role of the choice of\nloss function for classifier guidance, as an inappropriate selec-\ntion can lead to diminished rather than enhanced performance.\nFurthermore, it solidifies the efficacy of our innovative adop-\ntion of the BCE loss function, reaffirming that our approach\nfor virtual screening is capable of yielding a more extensive\nrepertoire of HIV-inhibiting candidate molecules."}, {"title": "F. Generated Candidate Molecules", "content": "Ten pairs of similar molecules are shown in Fig. 3 with\nthe top row of active molecules from the HIV dataset and the\nbottom row of molecules generated by our model. Our model\ngenerates molecules that are very similar to known HIV drug\nmolecules. This further demonstrates the effectiveness of our\nmethod."}, {"title": "G. Observed Phenomenon \"Degradation\" and Discussion", "content": "In Table V and Table VI, it can be observed that the Drug\nIndex of the generative models(except our method) are all\nbelow 100%. Compared with the molecules in training set\nMOSES, the proportion of generated molecules that are similar\nto known HIV drugs is lower. We are the first to observe\nand report this phenomenon. We call it the Degradation in\nmolecule generation.\nTo explain this phenomenon, we do the following experi-\nment. First, we calculate Morgan fingerprint of all molecules\nwith a radius of 10. Then, we use the k-means algorithm to\ncluster the active molecules in the HIV-a dataset according to\nfingerprint similarity. The k is set to 30. In the end, we use\nthe k-nearest neighbor(KNN) algorithm to classify molecules\nthat are similar to known drugs. These molecules are from the\nMOSES dataset as well as generated by charRNN [7], JT-VAE\n8], LatentGAN [9], DiGress [10], and our method. The result\nis shown in TableVIII.\nAccording to Table VIII, the following conclusions can be\ninferred.\n1) Degradation from Difficulty in Specific Structures:\nAccording to cluster 13, we can find that there are only\nmolecules from MOSES in this cluster and no molecules\nfrom the generative models. Although the training set con-\ntains similar molecules, the generative model has difficulty\ngenerating the structure of this cluster of HIV drug molecules.\nTherefore, we can speculate that the inability to generate these\nspecific structures is the reason for the poor performance.\nActually, some scholars have found that generation models\noften struggle to generate aromatic structures [10], which is\nconsistent with our conclusion.\n2) Training Set Needs Improvement: For most clusters,\nthere are neither molecules from the MOSES dataset nor\nmolecules generated by the existing generative models. The\nmolecules in the MOSES dataset lack certain molecular\nstructures, so the generation models trained on this dataset\nstruggled to produce molecules with these specific structural\nfeatures. As a result, improving the training data for generative\nmodels is an important consideration in the drug discovery\nprocess.\n3) Our Method Makes Sense: Our method uniquely gener-\nates candidate molecules within the cluster 22. This demon-\nstrates the effectiveness of our approach in leveraging existing\nHIV-inhibiting molecules to guide the generation model.\nTo further validate our conclusion, we find that many\nclusters containing HIV molecules without any molecules\nfrom MOSES or the generative models possessed a similar\nstructure to Fig. 4, where two rings of length 5 or 6 share at\nleast one chemical bond with each other.\nWe have compiled the proportion of molecules with this"}, {"title": "H. Limitation", "content": "1) Lack of Data: The training set for the Classifier\nGuidance contains only 328 HIV-inhibiting molecules. Al-\nthough Classifier Guidance Diffusion requires less data than\nClassifier-Free Diffusion, it is still challenging to train a\nreliable classifier for molecules with noise. Molecular drug\ndata is obtained through costly and time-consuming wet lab\nexperiments, and this lack of data hinders the development of\neffective conditional generation models for drug design. Fine-\ntuning Methods that can achieve good performance with less\ndata, such as LoRA [33], may help address this problem.\n2) Flaws in Theory: In Eq. 5, we treat Gas a contin-\nuous tensor and perform a Taylor expansion. However, the\nunderlying molecular structure is stored discretely in G. This\ncontinuous approximation inevitably results in the loss of\nthe discrete nature of the original information. Additionally,\nwe do not have access to $\\nabla_G \\log (y_\\theta | G_t)$ and thus make a\nfurther assumption. While the ablation experiments in Section\nIV-E lend support to the validity of our assumption, the\ntheoretical framework still exhibits fundamental limitations.\nThe theoretical innovation of conditional molecule generation\nmodels is also a direction worth studying in the future."}, {"title": "V. CONCLUSION", "content": "In this paper, we are the first to combine conditional\nmolecule generation models with virtual screening. With the\nHIV dataset, an extra classifier is trained to guide Diffusion"}]}