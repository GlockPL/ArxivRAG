{"title": "A Novel Psychometrics-Based Approach to Developing Professional Competency\nBenchmark for Large Language Models", "authors": ["Elena Kardanova", "Alina Ivanova", "Ksenia Tarasova", "Taras Pashchenko", "Aleksei Tikhoniuk", "Elen Yusupova", "Anatoly Kasprzhak", "Yaroslav Kuzminov", "Ekaterina Kruchinskaia", "Irina Brun"], "abstract": "The era of large language models (LLM) raises questions not only about how to train models,\nbut also about how to evaluate them. Despite numerous existing benchmarks, insufficient\nattention is often given to creating assessments that test LLMs in a valid and reliable manner.\nTo address this challenge, we accommodate the Evidence-Centered Design (ECD)\nmethodology and propose a comprehensive approach to benchmark development based on\nrigorous psychometric principles. In this paper, we have made the first attempt to illustrate this\napproach by creating a new benchmark in the field of pedagogy and education, highlighting\nthe limitations of existing benchmark development approach and taking into account the\ndevelopment of LLMs. We conclude that a new approach to benchmarking is required to match\nthe growing complexity of AI applications in the educational context.\n\nWe construct a novel benchmark guided by the Bloom's taxonomy and rigorously designed by\na consortium of education experts trained in test development. Thus the current benchmark\nprovides an academically robust and practical assessment tool tailored for LLMs, rather than\nhuman participants. Tested empirically on the GPT model in the Russian language, it evaluates\nmodel performance across varied task complexities, revealing critical gaps in current LLM\ncapabilities. Our results indicate that while generative AI tools hold significant promise for\neducation-potentially supporting tasks such as personalized tutoring, real-time feedback, and\nmultilingual learning\u2014their reliability as autonomous teachers' assistants right now remain\nrather limited, particularly in tasks requiring deeper cognitive engagement.", "sections": [{"title": "1 Introduction", "content": "In evaluating the performance of Large Language Models (LLMs), benchmarks play a crucial\nrole, representing standardized sets of tasks with established evaluation criteria (Guo et al.,\n2023; Naveed et al, 2024). Despite the diversity of existing benchmarks covering a wide range"}, {"title": "Global Practices in Benchmarking LLMs", "content": "LLMs are at the forefront of contemporary Al technologies, with computational capabilities\nand architectures in constant evolution. Evaluating LLM performance is a rapidly growing\narea. Benchmarks for LLM evaluation offer several advantages, allowing for the evaluation of"}, {"title": "2 Benchmark development process", "content": ""}, {"title": "2.1 Differences in development process for humans and LLMs", "content": "If one compares tests for humans and LLM-targeted benchmarks, the most apparent difference\nis their size. The latter often have thousands of tasks (i.e., Hendrycks et al., 2020), while the\nformer rarely exceed a dozen items. Tasks for LLMs may contain up to 10-20 distractors, while\na typical multiple-choice question intended for human examinees contains 4-7 answer options.\nThe limitations of human attention and focus necessitate a radically different approach to test\ndevelopment compared to benchmark development.\n\nFrom a psychometrics point of view, test development emphasizes a clear delineation of the\ntarget construct and expert-driven item generation. This ensures decent sampling of target\nbehavior and a clear link between the behavior the task must elicit and the target construct.\nPsychometrically grounded tests focus on item creation and refinement, whereas LLM\nbenchmarks focus more on dataset compilation and quality control (Davis, 2016; Narayanan &\nKapoor, 2023).\n\nPsychometrics-based tests require pilot studies and detailed data analysis (within classical test\ntheory or item response theory approaches) with subsequent changes before the actual use by\nthe audience. The International Test Commission (ITC) Guidelines on Quality Control in\nScoring, Test Analysis and Reporting of Test Scores require for any test form reporting basic\nstatistics about item and test functioning and guidelines for interpreting the test scores (ITC,\n2012). LLM benchmarks focus more on error and agreement analysis and less on detailed\nstatistical validation (Fang et al., 2024).\n\nLogic of the test and benchmark development process is presented in Table 1."}, {"title": "2.2 The psychometric approach to benchmark development", "content": "Recent research in the field of LLMs has shown a particular interest in evaluating the extent to\nwhich different LLMs can assist in assessing knowledge at critical learning checkpoints (e.g.,\nduring professional examinations or certification processes). Such studies have already been\nconducted in various domains, including medicine (Gilson, Safranek, Huang, Socrates, Chi,\nTaylor, Chartash, 2022; Wang, Gong, Jia, Xu, Zhao, ..., Li, 2023a), law (Fei, Shen, Zhu, Zhou,\nHan, Zhang, Ge, 2023), and chemistry (Guo, Nan, Liang, Guo, Chawla, Wiest, Zhang,\n2023), among others. In the introduction we already mentioned why this is insufficient to\nevaluate LLM's performance in a target domain.\n\u0648...\n\nIn this paper we adopt the psychometrics approach to the benchmark development for LLM.\nPsychometrics offers a unique approach to test development\u2014construct-oriented approach\n(Wang, et al, 2023b). This approach is based on the causal theory of measurement, which\nsuggests that item responses are caused by the property being measured, and, for example, the\nnumber of items is not important itself, but the proof that responses to items are not caused by\nanything other than the target characteristic is crucial. In this case, the number of items may be\nsmaller, but their quality is important, they must relate to all structural components of the\nconstruct being measured, and all alternative explanations for responses to items must be\neliminated. Psychometrics provides a mechanism for how to develop tests from the\noperationalization of the construct to the analysis of the test results.\n\nIn this work, we also draw upon the Evidence-Centered Design (ECD) framework used in\neducational assessment that has been developed in psychometrics (Mislevy & Haertel, 2006;\nZieky, 2013, Oliveri & Mislevy, 2019). This theoretical approach allows one to identify\nobservable evidence of construct's manifestation and thereby move from a general definition\nof the construct to the specific variables on which test items are created. In particular, this"}, {"title": "The Proficiency Model", "content": "The Proficiency Model delineates the characteristics or competencies that the assessment aims\nto evaluate. Its purpose is to establish a clear linkage between the benchmark and its intended\nobjective. To accomplish this, benchmark developers must define the professional\ncompetencies of LLM, substantiate the asserted connection, and provide a robust rationale for\nthe conceptualization of these competencies. When applying to the task of evaluation of LLM\nprofessional competencies, it means we should formulate the educational outcomes \u2013 what we\nwould like to assess. This is done through the process of operationalization, that is the first step\nin the process of any test development, including benchmark for LLM evaluation. It involves\nformulating educational outcomes in terms of measurable indicators to describe the substantive\narea accurately and unambiguously. This ensures that the selected/developed tasks allow for\nvalid conclusions about the model's knowledge/skills/abilities in that substantive area. The\neducational outcomes should be action-oriented, observable, and clearly articulated."}, {"title": "The Task Model", "content": "The Task Model facilitates the verification that the developed benchmark is indeed suitable for\nmeasuring the competencies of LLM as outlined in the Proficiency Model. Benchmark\ndevelopers must justify, through the characteristics of the test items, how each item elicits"}, {"title": "3 Case Study", "content": "To illustrate how our approach can be implemented into a real benchmark development we\napplied it to develop the benchmark for the domain of pedagogy and education."}, {"title": "3.1 Implementation of psychometric approach using the pedagogy and education\nbenchmark as an example", "content": "Generative AI-based tools may have great potential when used in education. Existing research\nhighlights that potential applications of generative AI for teaching and learning could create\npersonalized recommendations and virtual tutors, generate answers to students' questions,\nimprove teaching models, assessment systems, and education ecology, provide useful\nsuggestions for teachers (Su & Yang, 2023), provide real-time feedback and assessment, create\nmultilingual learning materials for students with diverse linguistic backgrounds (Alasadi &\nBalz, 2023), allow for personalized tutoring, automated essay grading, language translation,\ninteractive learning, adaptive learning (Baidoo-Anu & Owusu Ansah, 2023), etc. Indeed, in a\nsituation of interaction between a teacher and students or when performing other functions\nrelated to teaching or organizing the educational process, the competent use of generative AI\ntools can not only help the teacher with routine tasks, but also make the learning process more\nengaging (Noroozi et al, 2024). But is the current level of generative AI models sufficient to\nhelp a teacher and can a teacher rely on it as an assistant?"}, {"title": "3.1.1 Educational outcomes formulation", "content": "It was important for us to assess the competence of LLMs in a specific professional domain,\nevaluating the integrated set of knowledge and skill that are mobilized in a particular context\nto solve a task and achieve a defined outcome. According to ECD, educational outcomes were\nformulated in terms of measurable indicators to accurately and unequivocally describe the\ncontent area, ensuring that the selected or developed items would allow for valid inferences\nregarding the model's knowledge, abilities, and skills within this content area. The educational\noutcomes should be action-oriented, observable, and articulated in clear terms.\n\nEducational outcomes within the domain were formulated with a focus on addressing potential\naspects of LLMs' professional involvement: assisting teachers in their work with students and\nserving as a consultant (assistant).\n\nTo formulate educational outcomes, a comprehensive analysis of the literature pertaining to\nundergraduate students' standards across the necessary fields of study was conducted. This\nanalysis encompassed both groups of general professional competencies and universal\ncompetencies, as well as the content of professional qualification examinations, theoretical"}, {"title": "3.1.2 Selection of test content", "content": "The test content was selected based on the qualification characteristics described in the\nprofessional standard for teachers and general education outcomes of the LLM as teacher's\nassistant and consultant. The test content was organized as a structured list of test content units\nthat belong to each content area of professional field.\n\nTo achieve this, first, the content was divided by experts into 16 content areas. This provided\nan initial structuring of the field, which not only facilitated the logical progression of mastering\nbut also identified key content nodes\u2014main concepts of the domain. To delineate the topics,\ncurrent sources, textbooks, and university syllabi were utilized. The final list of content areas\nincluded the following:\n\n1. Traditional approaches to teaching and learning;\n\n2. Developmental Didactics;\n\n3. Project-Based Learning.\n\n4. Educational Technologies;\n\n5. Instructional Design.\n\n6. Developmental Psychology of Education;\n\n7. Social Psychology of Education."}, {"title": "3.1.3 Taxonomy levels description", "content": "Within the framework of classical Bloom's taxonomy, three levels were selected for the case\nstudy: 1) reproduction, 2) understanding, 3) application.\n\nThe reproduction level in the context of LLM testing involves tasks related to the retrieval of\nfacts, key concepts, and fundamental knowledge. At this stage, the model demonstrates its\ncapability to reproduce information from its training data, source documentation, and open and"}, {"title": "3.1.4 Item development process", "content": "After selection the test content areas and a choice of the taxonomy levels we developed a\nblueprint as a detailed test plan connecting the educational outcomes, content units and\ntaxonomy levels. The blueprint serves as a basis for item development, what we started at the\nnext stage.\n\nIt was decided to limit the benchmark to multiple choice (MC) questions. This form of items\nis used in many benchmarks for LLM evaluation, MC items can be checked automatically and\nare therefore objective, scored dichotomously. This item format is also the most popular in\nhumans' assessment.\n\nAll items in the benchmark are original. Each item was developed based on the test blueprint,\nreflecting a certain content element and a certain taxonomy level. The item developers were"}, {"title": "3.2 Empirical testing of the developed benchmark", "content": ""}, {"title": "3.2.1 Pilot testing procedures", "content": "In the current version of the paper, we chose to empirically test our benchmark using GPT-4,\nwhich demonstrates superior capabilities in natural language understanding and generation.\nBeing guided by the approach taken by Khondaker et al. (2023) and Abdelali et al. (2023) we\nhave used the following system prompt: \"You are a professional teacher-practitioner, you can\nbe a teacher's assistant and a student's assistant, you work in Russia. You are taking a bachelor's\nlevel exam in pedagogy and education. You will be asked questions with multiple-choice\nanswers. Your task is to choose all correct options. In your answer, return only the letters\".\n\nWe use the GPT-4 model through the OpenAI API. We decode with temperature 1 with top_p\n= 0. We automatically extracted the responses of GPT-4 and scored them dichotomously,\nawarding 0 points to partially correct answers."}, {"title": "3.2.2 Pilot testing results", "content": "The proficiency of LLM is estimated based on percentages of correct responses. GPT-4 scored\n39.2% percent correct on the Pedagogy and Education Benchmark with a score of 1541 out of\n3963. The performance of GPT-4 in each content domain is shown in Table 2."}, {"title": "4 Conclusion", "content": "In this paper, we present a psychometrics-based methodology that addresses limitations in\nmany popular benchmarks for large language models evaluation. Our approach is deeply rooted\nin principles of test development, enabling a more nuanced evaluation of LLM performance.\nBy implementing a stepwise framework, we developed a unique testing dataset tailored for\nLLM evaluation in the field of pedagogy and teaching, which differs significantly from both\ntests aimed at human test-takers and most LLM benchmarks. This dataset, curated by\neducational experts, was designed to specifically assess the performance of LLMs in the\nRussian language, broadening the scope of LLM benchmarking by ensuring linguistic diversity\nand applicability across multiple environments.\n\nOur methodology introduces several unique features for LLM assessment, including those\ninfluenced by the application of the Evidence-Centered Design (ECD) methodology. Firstly,\nthe incorporation of a blueprint allows for the placement of the indicators, content units, and\ntaxonomy levels. The blueprint defines the type of items aimed at measuring content units in"}]}