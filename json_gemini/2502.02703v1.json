{"title": "Developing multilingual speech synthesis system for Ojibwe, Mi\u2019kmaq, and\nMaliseet", "authors": ["Shenran Wang", "Changbing Yang", "Mike Parkhill", "Chad Quinn", "Christopher Hammerly", "Jian Zhu"], "abstract": "We present lightweight flow matching multilin-\ngual text-to-speech (TTS) systems for Ojibwe,\nMi'kmaq, and Maliseet, three Indigenous lan-\nguages in North America. Our results show\nthat training a multilingual TTS model on three\ntypologically similar languages can improve\nthe performance over monolingual models, es-\npecially when data are scarce. Attention-free\narchitectures are highly competitive with self-\nattention architecture with higher memory ef-\nficiency. Our research not only advances tech-\nnical development for the revitalization of low-\nresource languages but also highlights the cul-\ntural gap in human evaluation protocols, calling\nfor a more community-centered approach to hu-\nman evaluation.", "sections": [{"title": "1 Introduction", "content": "Many world languages are currently endangered,\nespecially those spoken by historically marginal-\nized and Indigenous communities. Language re-\nvitalization and reclamation is an ongoing effort\nto ensure continued language vitality for commu-\nnity self-determination and well-being (Oster et al.,\n2014; McCarty, 2018; Bird, 2020). Among recent\nefforts of language revitalization, TTS technology\nis valued as a potential tool to assist the education\nof Indigenous languages, as TTS models can flexi-\nbly synthesize diverse learning materials to guide\npronunciation learning (Pine et al., 2022, 2024).\nIn general, speech synthesis for Indigenous lan-\nguages is underdeveloped compared to the major-\nity of languages. The main barrier to developing\nTTS technologies for Indigenous communities with\noral traditions is still the lack of data (Pine et al.,\n2022, 2024). There are recent efforts to develop\nspeech synthesis systems for low-resource and In-\ndigenous languages, including Mundari (Gumma\net al., 2024), Kanien'k\u00e9ha (also known as Mohawk;\nIroquoian), Gitksan (Tsimshianic), SEN COTEN\n(Coast Salish) (Pine et al., 2022, 2024), Plains Cree\n(Central Algonquian) (Harrigan et al., 2019) and\nOjibwe (Hammerly et al., 2023). Yet there is still\nroom for improvement and development in this\nspace.\nIn this study, we continue this line of effort and\ndevelop TTS systems for Ojibwe, Mi\u2019kmaq, and\nMaliseet, the latter two of which haven't received\nany attention from the speech processing commu-\nnity yet. Our study explicitly tackles several chal-\nlenges in designing speech technology for Indige-\nnous communities.\n\u2022 First, it is generally impractical to bring In-\ndigenous members to labs for recording, so we\ndemonstrate a community-centered approach\nto allow speakers to record their own voices\nat their own pace.\n\u2022 Secondly, as collecting Indigenous speech at\nscale is difficult, we show that training a flow\nmatching multilingual TTS models (Mehta\net al., 2024) with typologically similar lan-\nguage varieties can help improve the synthesis\nperformance in low-resource settings.\n\u2022 Thirdly, since the TTS system is likely to\nbe deployed in common computing devices,\nwe also implemented attention-free architec-\ntures, including FNet (Lee-Thorp et al., 2022),\nMamba2 (Dao and Gu, 2024) and Hydra\n(Hwang et al., 2024) that closely match the\nperformance of self-attention models in TTS\nbut are generally more efficient in deploy-\nment.\n\u2022 Finally, we also discuss the need to adapt cur-\nrent experimental paradigms to better work\nwith Indigenous communities.\nThe code is available at: https://github.com/\nShenranTomWang/TTS."}, {"title": "2 Data Collection", "content": "Languages We worked closely with speakers\nfrom three Indigenous languages of Canada:\nOjibwe, Mi'kmaq, and Maliseet. The three lan-\nguages are genetically related. Ojibwe is spoken\naround the Great lakes of North America and is\npart of the Central branch of the Algonquian fam-\nily, while Mi'kmaq and Maliseet are spoken in\nthe Maritimes and are classed within the Eastern\nbranch of the Algonquian family. According to\nestimates from the 2021 Statistics Canada Survey,\nthere are 25,440 speakers of Ojibwe, 9,000 speak-\ners of Mi'kmaq, and 790 speakers of Maliseet\n(Robertson, 2023). All language communities are\nactively involved in significant efforts to document\nand ensure the continued vitality of their languages.\nData collection Most Indigenous speakers flu-\nent in their own languages are senior speakers. It\nis infeasible to bring them to a sound-proof lab\nfor recording at a university. Instead, we adopted\na community-centered approach that allows the\nspeakers to have full control over the speech record-\ning process in the comfort of their own home, fol-\nlowing the protocol from a prior study (Hammerly\net al., 2023).\nIn each case, we used texts identified by the\ncommunity members as representative of their di-\nalect and writing system as the basis for the data\nset. These texts were then split into individual\nutterances (complete sentences or phrases) and\nloaded into the prompting and recording program\nSpeechRecorder (Draxler and J\u00e4nsch, 2004). The\nprogram allows speakers to read and record utter-\nances at their own pace, easily re-record in the case\nof an error or disfluency, and package and upload\nrecorded utterances into secure cloud storage as\nthey complete them.\nData partition We resampled the recorded audio\nto 22,050Hz. For each speaker, we reserved 100\nrandom samples for validation and another 100\nrandom samples for test. The rest of the speech\nsamples were used for model training. The detailed\nstatistics of our data were summarized in Table 1.\nSince each of our datasets has a different size, we\napplied oversampling to our multilingual training\ndataset by duplicating training samples such that\nthey contain roughly the same duration for each\nspeaker."}, {"title": "3 Method", "content": "3.1 MatchaTTS\nOur system is built upon Matcha-TTS (Mehta et al.,\n2024), a fast TTS model based on conditional flow\nmatching, a class of probabilistic generative model\ncapable of generating high-fidelity image and audio\n(Lipman et al., 2023). The original Matcha-TTS\nconsists of a text encoder, a duration predictor, and\na flow matching decoder. The text encoder trans-\nforms the text input into hidden states, which are\nthen upsampled to the output length based on the\nduration predictor. The flow matching decoder pre-\ndicts the final mel spectrogram through iterative\ndenoising steps conditioning on the upsampled hid-\nden states.\nThe original MatchaTTS model was only de-\nsigned for single-speaker TTS. For multilingual\nspeech synthesis, we added learnable speaker and\nlanguage embeddings for each unique speaker and\nlanguage, a common technique for multilingual\nmodels (Cho et al., 2022). Both embeddings were\nconcatenated with the output of the text encoder,\nwhich was then fed into the flow-matching decoder\nfor mel-spectrogram prediction. By default, the\nflow-matching decoder uses 10 inference steps to\nperform inference.\n3.2 Sequence mixing layers\nThe multilingual MatchaTTS utilizes attention for\nsequence mixing with 40M parameters, yet its\nquadratic complexity is not ideal for efficient de-\nployment. Here we also explore different attention-\nfree layers that can also mix information across\nsequences to improve the efficiency of MatchaTTS.\nWe replace self-attention with each of the follow-\ning layers. For cross-attention, we concatenate the"}, {"title": "inputs and put them through each layer.", "content": "Mamba2 Mamba2 (Dao and Gu, 2024) is a se-\nlective state-space model (SSM)(Gu et al.; Gu and\nDao, 2023) that can perform sequence mixing with\nsubquadratic complexity. SSMs have been shown\nto be effective in speech generation tasks (Zhang\net al., 2024; Miyazaki et al., 2024). In Mamba2,\nthe selective SSM can be formulated as follows:\n$h_t=A h_{t-1}+B x_t$\n$Y_t = C h_t$\nwhere Bt and Ct are input-dependent weights\nand $A_t = a_tI$ is a diagonal matrix. The input-\ndependent weights allow Mamba2 to selectively\nfocus on the information across time steps, mak-\ning it effective for sequence processing. Mamba2\nis closely related to transformers. If $A_t = I$, it\nis equivalent to the formulation of linear attention\n(Katharopoulos et al., 2020; Dao and Gu, 2024).\nIn our TTS model, we replaced the attention\nmodules of MatchaTTS with Mamba2 blocks. No-\nticeably, Mamba2 modules have more parameters\nthan attention modules. In order to keep the total\nnumber of parameters consistent, we shrunk the\nencoder and decoder hidden dimension size by $\\frac{3}{4}$,\nresulting in around 38M parameters in total."}, {"title": "Hydra", "content": "Hydra As the original Mamba2 is uni-\ndirectional, Hydra (Hwang et al., 2024) is a\nbidirectional extension of Mamba2 but still\nmaintains the subquadratic complexity. Below we\nprovide an overview of Hydra.\nState-space models, as discussed before, can be\nformulated by:\n$y = SSM(A, B, C)(x) = Mx$\nWhere x is the input, y is the output. Our goal is\nthen to find the matrix M with desired properties.\nCurrent SSMs such as Mamba2 use semiseparable\nmatrices for M. Hydra takes a step further and uses\nquasiseparable matrices for M, whose computation\ncomplexity remains subquadratic and has the nice\nproperties of being able to process inputs in order\nand reverse. Formally, a matrix is N-quaiseparable\niff any submatrix from either the strictly upper or\nlower triangle has a rank of at most N. Specifically,\nquasiseparable matrices can be decomposed into\nsemi-separable matrices via:\n$QS(x) = shift(SS(x)) + flip(shift(SS(flip(x)))) + Dx$\nWhere QS(\u00b7) and SS(\u00b7) denote matrix mulplica-\ntions of quasiseparable and semiseparable matrices\nrespectively, flip(\u00b7) denotes the action of revers-\ning the input, shift(\u00b7) refers to the action of shift-\ning the input one position to the right (padding"}, {"title": "with 0 at the beginning), and D is a diagonal ma-", "content": "trix. The SS() operation allows for the selection\nof any SSMs and we selected the selective SSM in\nMamba2. This allows Hydra to perform bidirec-\ntional sequence mixing in linear complexity.\nWhile Hydra has not been applied to TTS yet, its\nbidirectionality makes it potentially more powerful\nthan Mamba2. Hydra layers were used to replace\nall attention modules in MatchaTTS. Hydra also\nhas more parameters than attention, therefore we\nalso shrunk the encoder and decoder hidden dimen-\nsion size by $\\frac{3}{4}$, resulting in around 39M parameters\nin total.\nDiscrete Fourier Transform Discrete Fourier\nTransform has proven to be a viable sequence mix-\ning method with a complexity of O(L log L)(Lee-\nThorp et al., 2022) and works well for speech (Chen\net al., 2024). We replaced all attention modules of\nthe MatchaTTS with the FFT layer in FNet.\nThe FFT layer performs a 2D Fast Fourier Trans-\nform, on hidden dimensions and on the sequence\ndimension of the input and eventually takes the real\npart of the output. Formally, it can be formulated\nas:\n$y = R(F_{seq}(F_h(x)))$\nHere, R() denotes the action of obtaining real parts\nof the input, and $F_{dim}(\u00b7)$ denotes the action of\nperforming FFT on the dim dimension of input.\nBy the duality of the Fourier transform, FNet\ncan be thought of as alternating between multipli-\ncations and convolutions. Since this operation is\nparameter-free, the FNet model has only around\n31M parameters."}, {"title": "4 Experiments", "content": "Training As these languages all use a phoneti-\ncally transparent Latin alphabet, we used a simple\ncharacter-based tokenizer to tokenize all sentences.\nPunctuations were all removed except for the apos-\ntrophe in Ojibwe, which plays a role in Ojibwe\nphonology. Monolingual models were trained for\neach individual speaker, whereas multilingual mod-\nels were trained on all speakers with different se-\nquence mixing layers. All experiments were run on\na single A100 40GB GPU for a fixed 200 epochs.\nFull training details are available in Appendix A.\nVocoder For waveform generation, we trained\na Vocos vocoder (Siuzdak, 2024) on all training\nsamples. Vocos is a frequency domain vocoder"}, {"title": "that closely matches the performance of time-", "content": "that closely matches the performance of time-\ndomain vocoders like Hifi-GAN (Kong et al., 2020)\nand diffusion-based vocoder like Fregrad (Nguyen\net al., 2024) but with much higher throughput.\nSince vocoder is not the focus, we provided their\nevaluation results in Appendix D."}, {"title": "5 Results and Discussions", "content": "Objective Evaluation We perform our objec-\ntive evaluation results with Fundamental Fre-\nquency Root Mean Square Error (F0 RMSE), Log-\namplitude RMSE (LAS RMSE), Mel Cepstral Dis-\ntortion (MCD), Perceptual Evaluation of Speech\nQuality (PESQ), Short-Time Objective Intelligibil-\nity (STOI), Voiced/Unvoiced F1 (VUV F1) and\nMFCC Frechet Distance (FID), similar to contem-\nprary works (Li et al., 2024; Lv et al., 2024).\nResults in Table 2 suggest that multilingual mod-\nels generally outperform monolingual models in\nall languages. Training on typologically similar\nlanguages does help alleviate the lack of data for\nindividual languages, since the model can learn\nfrom the commonalities in these languages. Such\nfindings can also provide guidance for the future\ncollection of Indigenous speech datasets. We can\nprioritize dataset diversity over quantity, as a large\nquantity of speech data from a single language is\nalso hard to collect.\nWhile the self-attention MatchaTTS dominates\nmost objective metrics, other attention-free archi-\ntectures also match its performance closely. No sin-\ngle model dominates all objective metrics. Hydra's\nperformance is particularly close to self-attention,\nsuggesting that it is a strong competitor. Its bidirec-\ntional nature also allows it to outperform Mamba2.\nFNet underperforms all other models due to its\nparameter-free nature.\nIn terms of computational efficiency, as shown\nin Table 3, all attention-free architectures are much\nmore memory-efficient than self-attention models,\nand memory saving is more prominent when the\nbatch size is large. However, the attention-free\narchitectures do not necessarily reduce computa-\ntion time, presumably because our model is small\nenough that their advantages are not obvious.\nSubjective Evaluation Despite these challenges,\nin evaluating the current work, we designed sepa-\nrate mean opinion score (MOS) surveys for each\nlanguage. For each TTS voice, the survey in-\ncluded 10 generated utterances from each of the\nfive models and 10 utterances of natural speech."}, {"title": "The detailed design is described in Appendix E.", "content": "The detailed design is described in Appendix E.\nWe were able to recruit three raters for Ojibwe but\none did not complete the survey. For Mi'kmaq and\nMaliseet, we were not able to obtain MOS rating\ndue to the limited number of speakers. Generally\nspeaking, the MOS ratings are largely consistent\nwith the objective metrics (see Table 2).\nAs recently discussed in Pine et al. (2024), there\nare many challenges and questions to be raised\nwhen conducting a subjective evaluation of speech\nsynthesis with Indigenous communities. We also\nfind that, due to the gap in cultural norms, the use\nof standard measures like MOS and the current ex-\nperimental paradigm may not always be viable in\ndetermining the quality of synthetic speech. For\nexample, despite our instructions, one Ojibwe rater\nrated 5 for all Ojibwe NJ's voices, regardless of\nwhether it was natural or synthetic. We believe\nthis may have been due to a reluctance to comment\nnegatively on the voice, even when it was synthetic.\nThe concept of participating in controlled experi-\nments and judging synthetic voices, in general, is\nnot a natural task, and cultural norms can amplify\nthis. This implies that researchers working with In-\ndigenous communities should design more creative\nmeasures that also conform to the cultural norms of\nthe relevant community. We plan to conduct such\nwork as we continue development of these systems"}, {"title": "6 Conclusion", "content": "In this paper, we report our ongoing efforts to de-\nvelop TTS systems with and for the Indigenous\ncommunity. Our experiments demonstrate that\ntraining multilingual TTS models on similar lan-\nguages can partially compensate for the lack of data\nfor individual languages. In the future, we will be\nworking with the relevant communities and schools\nto deploy these systems for Indigenous language\neducation."}, {"title": "7 Ethical statements", "content": "Our research would not have been possible with-\nout the support of the Indigenous communities\ninvolved. The subjective evaluation experiments\nwere approved by the institutional ethics review\ncommittee. All Indigenous participants in the study,\nincluding the voice donors and raters, participated\nvoluntarily and received fair compensation for their\ncontributions.\nThe goal of our research is to develop TTS tools\nfor Indigenous communities. We are currently ac-\ntively working with learners and teachers learning\nthese Indigenous languages at school. However,\nTTS technology might potentially be misused for\nimpersonation and deception, which can be partic-\nularly dangerous for the Indigenous communities\nas they are not frequently exposed to such tech-\nnologies. We will continue to work alongside these\ncommunities to inform them about the benefits as\nwell as security concerns of speech technologies."}, {"title": "8 Limitation", "content": "Our study is still limited in several aspects. First,\nas all speech recordings were recorded at the speak-\ners' own residence, there are still ambient noises in\nsome of the recordings. These background noises\nlimit the overall performance of TTS systems. Sec-\nondly, we were not able to successfully conduct\nhuman MOS ratings, which complicates the inter-\npretation of the results.\nSecondly, while we would like to make the col-\nlected data publicly available for replication and\nlanguage documentation research, we were unable\nto do so this time, as we were not able to obtain\nthe consent of the Indigenous voice donors at this\nmoment. The primary concern is the malicious use\nof the data that might harm the communities. How-\never, we will continue to work with them and aim\nfor more open-source corpora in the long run.\nOur research currently focuses mostly on ma-\nchine learning system development. To make\nspeech technology truly beneficial to the Indige-\nnous communities, more human-centered designs\nthat take into consideration the community-specific\ncultural norms will also be needed to deploy these\nsystems to the benefit of the Indigenous communi-\nties (Noe and Kirshenbaum, 2024)."}]}