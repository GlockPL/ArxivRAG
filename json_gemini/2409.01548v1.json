{"title": "VOXHAKKA: A DIALECTALLY DIVERSE MULTI-SPEAKER TEXT-TO-SPEECH SYSTEM\nFOR TAIWANESE HAKKA", "authors": ["Li-Wei Chen", "Hung-Shin Lee", "Chen-Chi Chang"], "abstract": "This paper introduces VoxHakka, a text-to-speech (TTS)\nsystem designed for Taiwanese Hakka, a critically under-resourced language spoken in Taiwan. Leveraging the YourTTS\nframework, VoxHakka achieves high naturalness and accuracy and low real-time factor in speech synthesis while\nsupporting six distinct Hakka dialects. This is achieved by\ntraining the model with dialect-specific data, allowing for\nthe generation of speaker-aware Hakka speech. To address\nthe scarcity of publicly available Hakka speech corpora, we\nemployed a cost-effective approach utilizing a web scraping\npipeline coupled with automatic speech recognition (ASR)-\nbased data cleaning techniques. This process ensured the\nacquisition of a high-quality, multi-speaker, multi-dialect\ndataset suitable for TTS training. Subjective listening tests\nconducted using comparative mean opinion scores (CMOS)\ndemonstrate that VoxHakka significantly outperforms ex-isting publicly available Hakka TTS systems in terms of\npronunciation accuracy, tone correctness, and overall natu-ralness. This work represents a significant advancement in\nHakka language technology and provides a valuable resource\nfor language preservation and revitalization efforts.", "sections": [{"title": "1. INTRODUCTION", "content": "For low-resource languages, Text-to-Speech (TTS) systems\nplay a crucial role in language preservation and revitalization.\nSpeaker-aware and zero-shot TTS systems contribute to preservation efforts by not only capturing the unique\nphonetic nuances of the language but also by enabling the re-production of voices belonging to culturally significant indi-viduals, even when data is scarce. Furthermore, these sys-tems facilitate language promotion by enabling the creation\nof diverse audio content, including audiobooks, short films,\neducational materials for non-native speakers, and newscasts\nfeaturing virtual anchors. This accessibility to engaging audio\ncontent can be instrumental in raising awareness and promot-ing the use of low-resource languages.\nMotivated by this need, we introduce VoxHakka, a freely\navailable, Creative Commons (CC)-licensed, high-quality\nmulti-speaker TTS system designed specifically for Tai-wanese Hakka (\u81fa\u7063\u5ba2\u5bb6\u8a9e).\nTaiwanese Hakka, a major language in Taiwan, is spo-ken by the Hakka community, comprising approximately\n15-20% of the total population (2-3 million people). How-ever, despite its significant speaker base, the number of fluent\nspeakers is dwindling, particularly among younger genera-tions who often favor Mandarin or Taiwanese Hokkien (\u81fa\n\u7063\u53f0\u8a9e). This shift in language use has resulted in a dearth\nof readily accessible, open, and accurately labeled Hakka\nlanguage resources.\nFurthermore, Taiwanese Hakka exhibits significant di-alectal variation, primarily categorized into six major dialects:\nSixian (\u56db\u7e23), Hailu (\u6d77\u9678), Dapu (\u5927\u57d4), Raoping (\u9952\u5e73),\nZhaoan (\u8a54\u5b89), and Nansixian (\u5357\u56db\u7e23). Sixian and Hailu\nare the most prevalent, accounting for 57.5% and 35.8% of\nspeakers, respectively. This linguistic diversity is reflected in\nHakka's rich phonology and intricate grammatical structure.\nThe language features seven tones, with variations in number\nand values across dialects. Its consonant system demonstrates\ncomplexity through contrasts in voicing, aspiration, and a di-verse range of vowels, including monophthongs, diphthongs,\nand nasalized vowels. The presence of syllable-final conso-nants (codas), particularly in checked-tone syllables ending\nin /p/, /t/, or /k/, distinguishes Hakka from other Sinitic\nlanguages. This, coupled with complex tone sandhi rules in-fluenced by syllable combinations, contributes to the intricate\nprosodic patterns of the language. Importantly, each Hakka\ndialect exhibits unique characteristics in its tones, vowels,\nconsonants, and tone sandhi rules, further underscoring the\nimportance of dedicated language resources for each.\nThese factors-data scarcity and pronunciation complex-ity-pose significant challenges for developing high-quality\nHakka TTS systems. Consequently, there is a dearth of robust\nHakka TTS systems suitable for widespread use in academia\nor industry. Existing systems often focus solely on the Six-ian dialect and offer limited speaker diversity (typically fewer\nthan three speakers). Furthermore, these systems lack zero-"}, {"title": "2. ACQUISITION OF SPEECH FOR TTS", "content": "High-quality TTS models rely on clean, well-articulated\nspeech data with a consistent speaking rate. We describe our\nmethodology for acquiring and processing data from web\nsources to create a suitable dataset for TTS model training.\nFigure 1 illustrates this data preparation pipeline.\n1. Web Scraping: We utilize web scraping techniques to\ngather audio files and their corresponding transcriptions\nfrom government websites and affiliated resources, such\nas the Dictionary of Taiwanese Hakka, the Bank of Hakka\nExamination, and Hakka Radio. This data can be cate-gorized into two types:\n\u2022 Well-transcribed Data: Primarily sourced from ex-ample sentences and language learning materials,\nthis data exhibits high transcription accuracy with\nstrong alignment between audio and text.\n\u2022 Ill-transcribed Data: Obtained from sources like ra-dio press releases, this data often contains discrepan-cies between the audio content and its accompanying\ntext. However, the audio quality generally remains\nsuitable for TTS training.\n2. ASR Training: We leverage well-transcribed data, along-side a six-dialect lexicon from the Dictionary of Taiwanese\nHakka, to train an Automatic Speech Recognition (ASR)\nsystem. This system's acoustic model encompasses the\nfull range of Hakka phonemes and tones.\n3. Speech Cleanup: Employing the trained ASR system's\nacoustic model (implemented using the Kaldi toolkit [6]),\nwe refine the transcriptions of the ill-transcribed data. For\neach phrase, a biased language model is generated using its\ninitial transcription. By adjusting discounting constants,\nwe control the influence of this biased language model\nduring transcription refinement. Larger constants priori-\ntize acoustic information, yielding transcriptions closely\nmatching the actual pronunciation.\n4. Forced Alignment: To minimize extraneous silences and\nprovide phoneme-level timing information, essential for\ncertain TTS architectures, we perform forced alignment on\nthe cleaned transcriptions using the trained ASR system.\nThis yields precise timing information for each phoneme,\nincluding silences.\n5. Silence Trimming: We segment phrases based on long si-lences (greater than 0.05 seconds), preserving up to 0.025\nseconds of silence before and after each segment to ensure\nnatural-sounding transitions.\n6. Final Data: Finally, the well-transcribed data and the pro-cessed, cleaned data are combined to form the final dataset\nfor training the VoxHakka TTS model."}, {"title": "3. TTS MODEL TRAINING", "content": "While a well-constructed dataset is essential, further process-ing of text and speech data (cf. Figure 1) is necessary before\ntraining the TTS model. Figure 2 illustrates our TTS model\ntraining pipeline.\n1. Speech Concatenation: The data preprocessing steps\ndescribed previously yield short speech segments with\nminimal silence. To enhance the synthesis of longer,\nmore natural-sounding sentences, and to enable the model\nto learn appropriate pauses between phrases (indicated\nby commas in text), we implement a sentence concate-nation strategy. As shown in Figure 3, a long sentence\nis segmented into shorter waveforms (e.g., waveform 1,\nwaveform 2, and waveform 3) with corresponding text\nsegments (text 1, text 2, and text 3). These segments are\nconcatenated pairwise, introducing a 0.05-second pause\n(0.025 seconds from each segment) at each junction. The\nconcatenated waveforms are paired with their correspond-ing concatenated text, with a comma representing the\npause (e.g., \"text 1, text 2\").\n2. Grapheme-to-Phoneme (G2P) Conversion: Unlike\npinyin-based writing systems, Taiwanese Hakka utilizes\nChinese characters, including numerous specialized char-acters. Therefore, a robust G2P conversion mechanism,\nmapping Hakka characters to their corresponding phonetic\nrepresentations, is crucial. We have developed a compre-hensive G2P conversion table, leveraging the govern-ment's online Hakka dictionary, encompassing all Hakka\nglyphs and their phonetic transcriptions across the six ma-jor dialects. For characters with multiple pronunciations,\nwe prioritize the most frequent variant at this stage.\n3. TTS Model Training: We employ YourTTS [2], a\nlightweight VITS-based model [7] incorporating language\n(or dialect) embedding, as our TTS system. YourTTS\nis well-suited for CPU deployment due to its smaller\nmodel size. Importantly, its speaker embedding mecha-"}, {"title": "4. EXPERIMENTS", "content": "Our data primarily originates from three publicly available\nresources on the Internet: the Dictionary of Taiwanese Hakka\n(DICT), the Bank of Hakka Examination Examples (EXAM),\nand Hakka Radio (RADIO). DICT and EXAM primarily\nconsist of well-transcribed complete sentences. In contrast,\nRADIO's transcriptions often exhibit inconsistencies with\nthe spoken content, sometimes including Mandarin pronun-ciations instead of Hakka. We address these inconsistencies\nusing our speech cleanup techniques. Tables 1 and 2 provide\ndetailed statistics for these datasets.\nAs evident from the tables, we collected a total of 180.53\nhours of transcribed Hakka speech from online sources. RA-DIO constitutes the largest contributor, providing over half of\nthe data for both the Sixian and Hailu dialects. While RADIO\ncontains fewer individual utterances compared to DICT and\nEXAM, each utterance represents a complete news report, av-eraging around 3 minutes in length-significantly longer than\nthe sentences in DICT and EXAM. After applying speech\ncleanup and silence trimming, 77.72% of the scraped speech\nis deemed suitable for TTS training. Silence trimming also\nleads to a significant increase in the number of characters per\nsecond (#Char/Sec), a metric reflecting speaking rate."}, {"title": "4.2. TTS Model Structure of VoxHakka", "content": "Following the YourTTS architecture, we incorporate 4-dimensional trainable language embeddings into the input\ncharacter embeddings and sum the external speaker em-beddings with the text encoder outputs and decoder out-puts, which are then sent to the duration predictor and\nvocoder. Similar to the VITS model, our decoder comprises\nfour stacked affine coupling layers, each consisting of four\nWaveNet residual blocks [9]. For vocoding, we employ HiFi-GAN version 1 [10] with the discriminator modifications\nproposed by Kim et al. [7].\nTo enable efficient end-to-end training, we connect the\nTTS model and vocoder using a variational autoencoder\n(VAE) [11] with a posterior encoder composed of 16 non-causal WaveNet residual blocks.\nFor model training and HiFi-GAN discriminator opti-mization, we use the AdamW optimizer [12] with betas of\n0.8 and 0.99, weight decay of 0.01, and an initial learning rate\nof 0.0002, decaying exponentially by a gamma of 0.999875.\nOur model hyperparameters largely align with those in\nthe original YourTTS paper. However, we utilize eight trans-former encoder layers and omit the Speaker Consistency Loss\n(SCL). Model training is conducted using four NVIDIA L40S\nGPUs, with a batch size of 54 per GPU."}, {"title": "4.3. Evaluation", "content": "To ensure a fair and comprehensive evaluation of pronuncia-tion accuracy and intonation, we collaborated with a Hakka\nlinguistics expert to design 15 test sentences in Hakka char-acters (Table 3). These sentences were specifically crafted to\nhighlight potential strengths and weaknesses in TTS systems.\nAs a benchmark for natural pronunciation, we recorded a fe-male professional Hakka broadcaster reading these sentences.\nDespite VoxHakka's ability to synthesise the speech of six\ndialects of Hakka, there are no publicly available TTS sys-tems other than the Sixian dialect for comparative evaluation.\nTherefore, as far as subjective listening is concerned, we only\nconsider the Sixian dialect as a dialectal target for the evalua-tion. However, in our Demo Page, we still show the synthesis\nresults of five dialects other than Sixian for the reader's refer-ence or future comparison."}, {"title": "4.3.1. Systems to be Compared", "content": "We compared our VoxHakka system with two commercially\navailable Hakka TTS systems: Cyberon and Bronci. Both\nsystems focus exclusively on the Sixian dialect and provide\nlimited information regarding their model architectures and\ntraining corpora.\nCyberon's system requires input in Hakka Pinyin (Tai-wanese Hakka Romanization System) and offers adjustable\nspeaking rate. However, it offers only a single female speaker.\nWe utilized our G2P system to convert the 15 test sentences\ninto Hakka Pinyin for input to Cyberon's system.\nBronci's system accepts Hakka characters directly. Based\non the provided female speaker's voice characteristics, we\nhypothesize that this system might have utilized the Hakka\nAcross Taiwan (HAT) corpus [13] for training, as the speaker's\nvoice closely resembles that of a female speaker in the HAT\ncorpus. However, it is essential to note that the HAT corpus\nis not freely available."}, {"title": "4.3.2. Our Synthesized Utterances", "content": "To emphasize VoxHakka's capabilities in zero-shot settings,\nwe opted not to synthesize speech using speaker characteris-tics present in our training corpus. Instead, we adopted two\napproaches:\n1. We synthesized a female voice in Mandarin (not Hakka)\nusing Google's Text-to-Speech AI.\n2. We recorded a female speaker uttering the test sentences\nin the Sixian Hakka dialect."}, {"title": "4.3.3. Comparative Mean Opinion Scores", "content": "We conducted a listening test employing Comparative Mean\nOpinion Scores (CMOS) to evaluate listener preferences\nacross three dimensions: naturalness, pronunciation accu-racy, and tone correctness. CMOS values range from -2\n(system significantly worse than human speech) to +2 (sys-tem significantly better than human speech) on a continuous\nscale. Sixteen listeners participated in the evaluation, each an-swering 60 \"Which is better?\" questions (15 test sentences X\n4 system comparisons = 60 questions) per evaluation aspect,\ntotaling 180 questions per listener."}, {"title": "4.3.4. Results", "content": "Table 4 presents a comparative evaluation of the different TTS\nsystems for the Sixian dialect of Taiwanese Hakka, focusing\non Naturalness, Pronunciation Accuracy, and tone correct-ness. Human speech serves as the baseline, assigned a score\nof 0 for all metrics. The two commercial systems, Cyberon\nand Bronci, demonstrate relatively lower scores, ranging from\n-1.37 to -1.7, indicating a perceptual gap compared to human\nspeech across all three dimensions.\nOur proposed VoxHakka system consistently outperforms\nboth commercial systems. Employing speaker embeddings\nderived from both Mandarin and Hakka speech, VoxHakka\nachieves competitive scores. Notably, the VoxHakka (Hakka\nSpk Emb.) configuration, utilizing Hakka-specific speaker\nembeddings, attains the highest naturalness score (-0.91)\namong all systems, surpassing even the Mandarin-based\nVoxHakka variant. This finding highlights the effective-ness of language-specific speaker embeddings in capturing\nthe unique nuances of Hakka speech.\nDespite its overall strong performance, both VoxHakka\nversions exhibit slightly lower pronunciation accuracy scores\n(-1.43 and -1.5) compared to Cyberon. This suggests a poten-tial avenue for future improvement, specifically targeting the"}, {"title": "5. CONCLUSION", "content": "We have presented VoxHakka, a freely available, CC-BY\n4.0 licensed multi-speaker TTS system for all six major\ndialects of Taiwanese Hakka. Addressing the challenges\nof data scarcity and pronunciation complexity, we devel-oped a comprehensive system encompassing web-based data\ncollection and processing, a robust G2P conversion mecha-nism, and a carefully tuned TTS model based on YourTTS.\nEvaluations using CMOS testing demonstrate VoxHakka's\nsuperior performance compared to commercially available\nHakka TTS systems, particularly in terms of naturalness.\nImportantly, VoxHakka's zero-shot capabilities, enabled by\nlanguage-specific speaker embeddings, open new possibili-ties for personalized and adaptable Hakka speech synthesis.\nFuture research will focus on refining VoxHakka's pro-nunciation accuracy by exploring improvements in acoustic\nmodeling. Additionally, we aim to expand the system's\ncapabilities by incorporating emotional expressiveness and\nprosodic control. By making VoxHakka openly accessible,\nwe hope to facilitate research, educational initiatives, and\ncreative applications that contribute to the preservation and\nrevitalization of Taiwanese Hakka."}]}