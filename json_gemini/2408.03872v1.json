{"title": "Inter-Series Transformer: Attending to Products in Time Series Forecasting", "authors": ["Rares Cristianb", "Pavithra Harsha\u00aa", "Clemente Ocejob", "Georgia Perakisb", "Brian Quanza", "Ioannis Spantidakisb", "Hamza Zerhounib"], "abstract": "Time series forecasting is an important task in many fields ranging from supply chain management to weather forecasting. Recently, Transformer neural network architectures have shown promising results in forecasting on common time series benchmark datasets. However, application to supply chain demand forecasting, which can have challenging characteristics such as sparsity and cross-series effects, has been limited.\nIn this work, we explore the application of Transformer-based models to supply chain demand forecasting. In particular, we develop a new Transformer-based forecasting approach using a shared, multi-task per-time series network with an initial component applying attention across time series, to capture interactions and help address sparsity. We provide a case study applying our approach to successfully improve demand prediction for a medical device manufacturing company. To further validate our approach, we also apply it to public demand forecasting datasets as well and demonstrate competitive to superior performance compared to a variety of baseline and state-of-the-art forecast methods across the private and public datasets.", "sections": [{"title": "1. Introduction", "content": "Time series forecasting is a fundamental problem in machine learning with applications across many domains. Common applications of time series forecasting include supply chain management [1], financial modeling [2], weather forecasting [3], and many more. Since many of these problems have been around for far longer than more modern problems in machine learning such as robotic manipulation, often the models used in production are more theoretical in foundation and rely less heavily on data.\nThat is, they often encode strong priors, or inductive biases [4, 5], from the assumptions made and the corresponding simple parametric form the models take. Furthermore, they may not be designed to fully utilize the broader set of related data often available these days for forecasting particular time series."}, {"title": "2. Related Work", "content": "Time series forecasting has emerged in recent years as a crucial topic in machine learning. There has been a growing interest in using time series models for forecasting in various domains such as transportation [14-17, 19, 23-26, 29, 31, 35, 36], energy [16, 17, 19, 23, 24, 26-29, 31-33, 35, 36],\nweather [28, 29, 32, 33, 35, 36] or retail sales [14]. As evidenced by the recent M5 competition [41], there is also growing interest in developing new machine learning and deep learning time series methods for supply chain demand forecasting specifically. However, the literature on this kind of time series models in the context of retail demand forecasting is limited. We provide a brief overview of existing work to establish the motivation and context for our proposed approach."}, {"title": "2.1. Traditional time series models", "content": "Many traditional time series forecasting methods are still used in industry [6-9]. Exponential smoothing [6, 7] produces a prediction that is a weighted sum of past observations with exponentially decreasing weights for past data points. Holt-Winters [7], that falls into the exponential smoothing family, adds model components to enable capturing trend and seasonality. Autoregressive Integrated Moving Average (ARIMA) [8, 9] is a large class of models combining an auto-regressive model, a moving average model and a differencing step to forecast stationary and non-stationary time series.\nAs we will see later, these traditional methods often have limitations compared to machine learning and deep learning time series models, especially when it comes to large and complex datasets. In the framework of the M5 competition [41], the results achieved show that the traditional time series methods were outperformed by state-of-the-art machine learning models.\nThe book [9] provides a comprehensive reference to a range of traditional time series forecasting methods including exponential smoothing, ARIMA and state space models."}, {"title": "2.2. RNN and CNN-based time series models", "content": "A Recurrent Neural Network (RNN) is a type of neural network used for sequential data with the ability of capturing past information stored by having a recurrent hidden state whose activation at each time is dependent on the previous time state and current time input. RNN-based models can be used for language translation, natural language processing or image identification as well as for time series forecasting showing promising results in this area. RNN-based models have been extensively applied to time series forecasting [10-19]. One variant of Recurrent Neural Network is the Long-Short Term Memory model (LSTM) [11, 12]. The purpose of this method is to overcome the challenge of long-term dependencies in time series forecasting by introducing a sophisticated hidden layer that controls the flow of information required to predict the output in the network. By maintaining a memory of past information, the hidden layer regulates the flow of data and stores the relevant information to use for the next steps. This method can capture long-term patterns in the time series and make more accurate predictions for future time points. DeepAR [14] is a forecasting method based on autoregressive RNNs which learns seasonal behaviors and dependencies on given covariates across time series to make probabilistic forecast. By using a shared model across multiple time series (i.e., a multi-task forecast approach), DeepAR can generate forecasts for time series with limited historical data.\nTemporal Latent Auto-Encoder [19] is a recent model designed to tackle high-dimensional multivariate time series forecasting and model cross-series correlations by combining nonlinear factorization of time series and a temporal latent space LSTM forecast model.\nConvolutional neural networks (CNNs) [42] are a variant of deep neural networks (DNNs) that use a sequence of convolutional layers, pooling layers, and fully-connected layers. CNN-based approaches have been extensively applied and evaluated across a wide spectrum of sequence modeling tasks such as Natural Language Processing (NLP) or speech recognition, and have been shown to outperform RNN-based methods in some cases [43-45]. CNN models have also demonstrated effectiveness in the domain of time series modeling and forecasting [16, 46-48]. The temporal convolutional network (TCN) [49] is a CNN-based model using causal convolutions, that is, convolution applied across time such that the convolution output for a given time point depends only on the current and previous time points. The TCN architecture can take a sequence of any length and map it to an output sequence of the same length via multiple layers of convolution (and padding sequences as necessary).\nNotably, TCNs have demonstrated superior empirical performance compared to RNNs and LSTMs in various sequence modeling tasks. For instance, DeepGLO [48], a TCN-based model, showed superior results compared to LSTM and DeepAR in time series prediction tasks such as electricity and traffic forecasting. The DeepGLO method is a hybrid model that combines a global matrix factorization model regularized by a temporal convolution network, along with another temporal network that can capture local properties of each time-series and associated covariates. Moreover, the WaveNet architecture has been adapted for conditional time series forecasting [47] by using stacked dilated convolutions providing access to a wide range of historical data, and multiple convolutional filters in parallel to separate time series facilitating rapid data processing and exploiting the correlation structure between multivariate time series. LSTNet [16] combines a traditional linear autoregressive model with RNN and CNN to extract short-term local dependency patterns among variables and to discover long-term patterns for time series trends. It leverages the strengths of both the convolutional layer to discover the local dependency patterns among multi-dimensional input variables and the recurrent layer to capture complex long-term dependencies.\nThe recurrence mechanism of RNNs restricts the amount of historical information that can be used for predictions and can complicate training as well, which can limit the capability of such models to capture long-term and complex temporal patterns effectively. Similarly, CNN models typically use dilation to increase their receptive fields, necessitating deeper networks to capture longer-term dependencies."}, {"title": "2.3. Transformer-based time series models", "content": "Transformer-based methods have been proposed to overcome the limitations of previous DNN approaches for time series forecasting and tackle the challenges posed by having modern-day datasets, which are often larger and more complex. The Transformer model [20] avoids using a recurrence mechanism and relies mainly on a self-attention mechanism to capture cross-sequence (time) interactions and generate an output. This model achieves very good results in natural language processing, computer vision and time series forecasting [22-36, 50] which may in part be due to its ability to directly capture long-range dependencies and interactions in sequential data. Temporal Fusion Transformer (TFT) [25] is a Transformer-based model incorporating variable selection networks to choose pertinent input variables at each time step and static covariate encoders to integrate static features into the model. It processes known and observed inputs using a sequence-to-sequence layer and implements a novel self-attention mechanism with interpretable multi-head attention. This enables TFT to learn long-term relationships across different time steps, making it a powerful tool for capturing complex temporal dependencies in the data. The self-attention mechanism also facilitates the interpretation of feature importance, allowing for the identification of critical factors affecting the forecasting task.\nPyraformer [30] is a recent Transformer-based model that introduces the pyramidal attention module to describe the temporal dependencies of different ranges leveraging a pyramidal graph and its attention mechanism. Autoformer [29] replaces the self-attention mechanism in the Transformer by an Auto-Correlation mechanism with dependency discovery and information aggregation at the series level to tackle long-term time series forecasting with intricate temporal patterns. FEDformer [35] is a frequency enhanced Transformer, decomposing the input into a trend component with a moving average kernel and a seasonal component. These models are specifically designed for long sequence forecasting. Using the seasonal-trend decomposition structure of FEDformer, DLinear combines it with simple linear layers. Two one-layer linear networks are applied to each component and the two features are summed to get the final prediction. This simple linear approach has outperformed the state-of-the-art (SOTA) FEDformer for multivariate and univariate forecasting as well as other recent SOTA Transformer-based solutions on different common benchmark datasets.\nNote for the multivariate (multiple time series) case these transformer approaches typically treat multiple time series values as part of the input vector for one time point, and an initial embedding linearly maps the combination to a new embedded vector which is subsequently transformed via attention across time points. Recently PatchTST [51] instead applied a shared transformer backbone independently per channel (time series), i.e., in a multi-task manner, and on patches of time points, to transform each channel independently, with a simple linear transformation at the end to get a final prediction for each series, and found this approach to often out-perform the previous channel-mixing transformer forecasting approaches."}, {"title": "2.3.1. Key model differences with our approach", "content": "In summary past transformer-based forecast models were either applied per-series (univariate forecasting) or typically by jointly embedding all variables per time point (multivariate forecasting), with PatchTST using per-series transformation for the Transformer part with a shared network (multi-task approach) but also using a static linear mapping on the transformed variables at the end. On the one hand, per-series approaches like PatchTST and univariate transformer models often achieve better results than multivariate modeling for many datasets (especially when trained in a multi-task manner across time series) as they can leverage more data (from the multiple time series independently) to train the shared-parameter network and avoid overfitting that can occur with multivariate modeling, especially when there are many time series, and can also be more computationally efficient. However, on the other hand, per-series models are unable to leverage cross-series information and capture joint time series distribution relationships in forecast outputs. I.e., while multivariate time series forecast modeling can capture complex relationships between time series it is also more prone to overfitting and more expensive to train. Furthermore, the static, typically linear, encoding per time point as typically used in multivariate transformer and other neural net forecasting approaches, and as used in PatchTST after independent per-series transformation, limits the relationships that can be captured and modeled between time series - for example, a different kind of transformation, or different information among time series should be shared, depending on the context given by the particular time window being considered.\nInstead with our novel transformer forecasting approach, we aim to capture the best of both worlds, of cross-time-series multivariate modeling along with per-series, shared network, multi-task modeling. Instead of using a static linear transformation of multiple time series values, we apply self-attention across time-series, for the current segment of each time series - to capture the relationships between time series in a dynamic and more flexible and powerful way. Afterward, a shared transformer network is applied per-series to finish transforming each individual time series and derive the final forecast. This whole network can then be trained in a multi-task fashion - i.e., applied to each time series individually (each time also feeding in the other time series initially for the cross-series attention transformation part), so it can leverage the benefit of having effectively larger data for training by sharing parameters across time series as in multi-task approaches like PatchTST. Note that while some past work has explored inter-series attention as well, e.g., for epidemiological analysis [52], unlike such prior work, we apply this in a more constrained way as just an initial transformation of directly aligned time series segments only, and critically and uniquely still use a shared multi-task forecast network applied per series after."}, {"title": "2.3.2. Experiment differences with our approach", "content": "Additionally, most transformer forecasting papers provide results of using these novel transformer methods on widely used, publicly available time series forecasting benchmark datasets, many of which are derived from real-world practical applications, or else on specific domains like epidemiology or logistics. However, the application of these methods to supply chain demand forecasting is very limited. Indeed, most of the papers do not include any retail dataset in the evaluation of their method. The TFT paper [25] is one of the only papers introducing a deep learning method for time series forecasting applied on various domains including a sales forecast dataset in the set of results, but this is not the focus of the study. These models have yet to be explored and demonstrate broad success in supply chain demand forecasting, which can have challenging characteristics such as sparsity and cross-series effects. As a key distinguishing part of this work we benchmarked many transformer forecast methods, including TFT, with the private dataset provided by the medical device manufacturing company as well as the best of the methods on other retail datasets.\nOur method and a diverse set of the models described will be applied on retail datasets to help provide an analysis of the application of these time series methods for supply chain demand forecasting. This includes an in-depth case study conducted on a supply chain demand forecasting dataset for a medical device manufacturing company. The study provides results for multiple horizons and target metrics, with the objective of catering to the business needs of the company using real-world data. Finally, we evaluated the effectiveness of our proposed approach on two publicly available real-world sales datasets, to provide a better understanding of its performance and its comparison with other methods."}, {"title": "3. Methodology", "content": null}, {"title": "3.1. Problem Definition", "content": "Let a generic matrix be denoted by a bold capital letter, a vector by a bold lower case letter, and any scalars including individual entries of a vector or matrix by an non-bolded letter. Then, multivariate time-series can be represented by a matrix (with each row being an individual time series / variate) and univariate time series by a vector (with each entry being a time point). For a matrix X, we denote the i-th column of the matrix with a subscript and lowercase letter xi. Let us consider a situation where we have several products for which we want to forecast demand. Each product can be represented as a multivariate time-series denoted by Y\u00b9, where j represents a specific product. The columns of Y\u00b9, denoted as (y\u2081, \u2026\u2026, y), represent the T time points of our time series, while the rows correspond to the different variates. We consider having n variates. Specifically, at each time point i, the vector y consists of the n observed values for the different variables considered. For example, such values consist of useful information such as the sales of the product at a particular location or the price of the product. We consider the problem of forecasting h future values (YT+1,1, \u2026\u2026\u2026, YT+\u0127,1) where each y represents the value for the dimension I we are interested in, at time point i for product j. In this case, we are interested in the same dimension across all future time periods and we call this the label or target variable. In the demand forecasting case, this may correspond to the sales of the particular product."}, {"title": "3.2. Preliminaries", "content": "The Transformer model is based on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention [20]. Through the attention mechanism and the positional encoding, the model is able to appropriately attribute the impact of each element in our input on each other element and compute a representation of a sequence by relating different positions in the sequence. The Transformer architecture consists of stacked self-attention layers and point-wise fully connected layers (i.e., the latter applied per time point, in the case of time series), that sequentially transforms the (vector) representations (also known as encodings), for elements of a sequence with each layer."}, {"title": "Attention Mechanism", "content": "The main component of the Transformer responsible for its success is the attention mechanism which can model the relationships between elements in a sequence. The original Transformer paper [20] uses two different attention mechanisms which are the same at the core: the self-attention mechanism and the encoder-decoder attention. An attention function can be described as mapping a query and a set of key-value pairs to an output, where each query, key and value corresponds to an element in a sequence, e.g., a time point in the case of time series. The input consists of queries and keys of dimension dk, and values of dimension du for all elements in a sequence. Q, K and V are respectively a packed set of queries, a packed set of keys and a packed set of values (with each item in the sequence corresponding to a row). dmodel is the model dimension to which we project our input. The output for each sequence element is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query for that element with the corresponding key. In matrix form the set of outputs is given by the following function:\nAttention(Q, K, V) = softmax(QKT/\u221adk)V, (1)\nwith softmax applied per row. In Transformers, these queries, keys, and values are typically determined for each sequence element by linearly projecting the current representation of each element using a learned weight matrix (i.e., which are parameters of the model) for each of the query, key, and value representations, i.e., Q = XWQ, K = XWK, and V = XWV, where X is the current representation of the sequence. Thus the output of the self-attention operation results in a new, transformed encoding / representation for each element in the sequence.\nInstead of performing a single attention function, an extension to this mechanism is to linearly project the queries, keys and values h times with different learned linear projections to dk, dk and dv dimensions respectively. The attention function is then performed on each of these projected versions of queries, keys and values, which are concatenated and once again projected. This multi-head attention can allow learning different relationships for different parts of a sequence. We label this the MultiHead attention function, described with the following:\nMultiHead(Q, K, V) = concat(head1, ..., head\u0127)WO (2)\nwhere head\u1d62 = Attention(QW, KWK, VW)\nwhere W \u2208 Rdmodelxdk, WK \u2208 Rdmodelxdk, WV \u2208 Rdmodelxdv and WO \u2208 Rhdv\u00d7dmodel. For self attention as used in Transformers, Q = K = V = X is the input to MultiHead."}, {"title": "Positional Encoding", "content": "Positional encoding [53\u201356] assigns a unique encoding vector to each time step, which is added to the input embedding vector (initial encoding) at that time step. The positional encoding has the same dimension dmodel as the embeddings, so that the two can be summed. The encoding vector captures information about the position of the time step in the sequence, such as its relative position to other time steps. This enables the model to distinguish between different time steps and understand the temporal ordering of the sequence."}, {"title": "Encoder", "content": "The encoder is composed of a stack of blocks, each composed of self-attention layers and position-wise feed forward network layers (i.e., the same network applied independently to each sequence element) followed by the residual connection and layer normalization to assist the training stability. The self-attention component of each block of the encoder is in charge of computing the attention weights between all of the elements in the block's input sequence and transforming the elements based on these attentions, as described above. The encoder thus performs a sequence of transformations to the input sequence representations, and then passes this information onto the decoder for it to roll-out the final predictions."}, {"title": "Decoder", "content": "The decoder architecture is similar to the encoder architecture. It is also composed of a stack of identical blocks with the same components. In addition to the two sub-layers in each encoder block, the decoder adds a third sub-layer, which performs multi-head attention over the output of the encoder stack as well. In this way sequential decoder outputs are generated based on both the encoded representations of the input sequence, and the output sequence of the decoder generated so far.\nPlease refer to [20] for more details about the Transformer architecture."}, {"title": "3.3. Model Architecture", "content": "We design a new Transformer-based model, referred to as Inter-Series Transformer, to overcome the different challenging characteristics described in Section 2. As mentioned in Section 2.3.1, a key differentiating aspect of our approach is combining controlled, cross-series attention based transformation of different time series, along with per-time-series multi-task modeling via a shared network (for temporal transformation), to capture the best of both worlds.\nNote, in the context of supply chain demand forecasting, each time series typically corresponds to a product or product group / category (or product and location combination) and each could also itself be a multivariate time series, incorporating other features like price or promotions.\nIn our proposed architecture, there are four main differentiating / new components compared to the vanilla Transformer architecture and typical / past Transformer forecasting approaches:\n\u2022 Inter-Series Attention Layer: we introduce a new custom attention layer to get a better informed representation of the target time series by learning dynamics between the different time series / products and incorporating the other time series into the prediction. As shown in Figure 1, this new component is the first layer (or layers) of our custom Transformer and takes all the time series as inputs as detailed in 3.3.1. Note that unlike past Transformer approaches that apply attention just across time, our can leverage attention to capture cross-product / time series effects. Additionally, unlike applying attention across both all time series and time points at all layers, which could more easily lead to overfitting, ours enables capturing cross time-series effects in a more controlled manner (transforming just the target time series themselves up-front).\n\u2022 Multi-task, shared per-series transformer: by limiting cross-product / series attention to initial layers and select time series, we control complexity, and enable using a shared Transformer network afterwards to separately transform each individual multivariate time series (e.g., per product), capturing the temporal effects and effectively expanding the amount of data used to train this shared network.\n\u2022 Projection to High Dimensional Representations for various features: we address mixed feature and feature type inputs to create a more comprehensive and informative input for the Transformer by learning separate mappings to high dimensional representations for different feature types.\n\u2022 Abandonment of Positional Encoding: we abandon the positional encoding and we capture relative positioning by defining and incorporating specific features that change with time."}, {"title": "3.3.1. Attention layers", "content": "As mentioned in the previous section, the traditional Transformer architecture involves two different attention layers: self-attention and encoder-decoder attention. These two layers allow for the model to learn complex relationships between the different elements in the input/target sequences. However, given that the traditional application was on natural language, this architecture treats each different sequence as somewhat independent to each other. In the context of time-series forecasting within a singular retailer, there are multiple sequences/time series occurring in overlapping time periods, e.g., corresponding to different products or product groups (or product-location combinations). It is important to recognize cross-series effects such as the cannibalization of one product by another. For this reason, we introduce a new attention layer that incorporates other time-series into the prediction for the current time-series."}, {"title": "Inter-Series Attention Layer", "content": "We introduce our own custom attention layer which utilizes the same form described above in order to help learn dynamics between different products. We refer to this as Inter-Series attention Layer. As we will see next, this layer takes the Attention function in 3.2 and changes the traditional form of the inputs to learn complex dynamics through attention weights between separate time series in addition to different time periods.\nThe main goal of this layer is to learn attention weights between the context window of the product for which we are providing a forecast and the context windows of all other time series (products) to produce a better informed representation of our desired time series context window. This can also help with sparsity as sparser time series can pay attention to larger-volume time series to improve their predictions, as we will show in our experiment results. To yield a single time series representation from the attention between all products, we must input our target time series context window as the query vector and the context windows of all other time series as the key and value vectors. Therefore, this creates a difference in the input shape between the Inter-Series attention layer and the other attention mechanisms described in 3.2. In the self-attention layer, the query vector and the key/value vectors are all of shape context-window \u00d7 dmodel. In the encoder-decoder attention layer, the query vector and the key/value vectors are respectively of shape forecast-window \u00d7 dmodel and context-window \u00d7 dmodel.\nThis is intuitive since we are matching like-sequences with each other and relating positions in the sequences to each other based on a high-dimensional representation of our original sequence. In our Inter-Series attention layer, the query vector is instead of shape 1 \u00d7 context-window, corresponding to the context window of the target time series, and the key/value vectors stacked are of shape total-series \u00d7 context-window, corresponding to the context windows of all the other time series. We chose to remove the projection to higher dimension since we are only concerned with the target of our other products and want to preserve the notion of a context window in the output. This layer results in an output of dimension 1 \u00d7 context-window, meant to represent a better-informed version of our target time series within the context window. It is worth noting that the overall architecture can also capture past information through other means such as using other features or incorporating past windows of values. We augment the original target series in the context window with the informed feature output by the Inter-Series attention Layer and proceed through the model as normal, more formally:\nP = all products target time series (3)\nPq = target time series of product q\nX = features of product q\nXIS = MultiHead(Pq, P, P)\nY = Transformer([Pq, Xq, XIS])\nHere, XIS is the informed feature output of our Inter-Series attention. As detailed in the equations below, the query Pq is the context window of the target time series, and the key/value P are the context windows of all the time series. As shown in Figure 1, the first layer is our custom Inter-Series attention layer that outputs XIS. This new informed feature XIS is then concatenated with X\u0105, the matrix feature of product q, and Pq, its target time series. This concatenation is then fed as input into a Transformer model. If our Inter-Series attention layer is not used, the initial input would simply consist of the concatenation of Pq and Xq, resulting in Y = Transformer([Pq, Xq]).\nNote, to keep our architecture simple / control complexity, in our experiments we limited our networks to a single layer of this inter-series attention, but it's also possible to apply this for multiple layers to enable a more complex transformation of the target time series based on the other series."}, {"title": "Multi-task per-series transformation", "content": "Note, as depicted in Figure 1, after the inter-series attention layer application, a single transformer network is applied to the combined, transformed representation of the target series, q. This applies time series transformation as is commonly done with Transformers, across time, to capture temporal affects and derive a forecast for future time points. Notably, this network is shared for all time series (e.g., all products q). That is, the same network parameters are used for all time series, and thus all data can be used to learn the parameters of this shared network - which as mentioned in the Related Work, Section 2, can often enable better performance / avoid overfitting of multivariate / multi-time-series output approaches. We call this multi-task application because one shared model is used for each task (product / time series) and applied separately for each, and the model network can also learn to adapt its behavior based on the identifying features and representation elements for each time series, as needed (i.e., as driven by the data)."}, {"title": "3.3.2. Projection to High Dimensional Representation", "content": "In NLP tasks, inputs are typically sequences of single discrete (categorical) values that need to be quantized through an embedding process to create fixed vector-value representations for each word. Yet, in the context of time series forecasting, features may contain both discrete and real-valued quantities, and of different types (for example multiple different categorical variables), presenting a more complex challenge, not addressed by the original Transformer. Projecting the features to a meaningful representation is a critical aspect for Transformer models that use real-valued features as well.\nTo address this issue, we map categorical and real-valued features independently from each other to increased dimensions. For real-valued features, we use a linear layer of weights to learn the optimal mapping. Specifically, we address the issue of low-dimensional real-valued features by replacing the original embedding layer with a linear layer of weights, which allows us to learn the optimal mapping for projecting the inputs to the desired dimensionality. This process creates a set of high-dimensional continuous input features. For categorical features, we employ the original embedding layers to create feature vectors for different features. Finally, the feature vectors and the projected continuous inputs are concatenated before being passed through the Transformer.\nOur approach allows us to take advantage of the full range of feature types and dimensions available in the data, creating a more comprehensive and informative input for the Transformer model. By mapping the different types of features independently, we can ensure that each feature is represented optimally in the model, enhancing its ability to capture the complex patterns present in time series forecasting data."}, {"title": "3.3.3. Abandonment of Positional Encoding", "content": "The positional encoding layer is responsible for assigning a relative position to each element in a given sequence. In the context of time series forecasting, information such as the date and time of day can be critical for producing accurate forecasts. While one approach to incorporating date information into the positional encoding involves adding it as input features and enforcing the positional encoding, this can potentially corrupt the original input and negatively impact the model's stability and ability to learn relationships, as observed in 6.2. To address this issue, we propose removing the positional encoding altogether and relying solely on the date-time features to capture relative positioning. Specifically, we map the date into two separate features, one capturing age (year) and the other capturing the month of the year. Note, for more granular time series, a similar process could be performed for other more granular time segments. To ensure stable training in the deep network, we scale the age feature using the natural log and the month feature to be between -0.5 and 0.5. This approach allows the model to learn the relative positioning of elements in the sequence based on the known fact that each element follows the prior by one month."}, {"title": "4. Experimental Setup", "content": null}, {"title": "4.1. Datasets", "content": "Our study involves evaluating our proposed model and other time series methods on a private dataset provided by a medical device manufacturing company, which is split into two parts, type 1 and type 2, as explained below. This dataset serves as a guiding case study to improve the limitations of time series forecasting in retail settings. To further validate our approach, we apply our method to two publicly available retail datasets, namely Walmart Stores Sales and Walmart M5 [57]. This allows us to compare the effectiveness of our Inter-Series Transformer model with other time series forecasting techniques, which have been discussed in section 2, on large-scale sales datasets. By evaluating our approach on both private and public datasets, we can gain a more comprehensive understanding of its potential impact and applicability in real-world retail forecasting scenarios."}, {"title": "Private Small Retail Dataset", "content": "The primary dataset used to evaluate our models is a small retail sales dataset of a medical device manufacturer. Products can be identified with three unique identifiers in a hierarchical structure. Our dataset consists of products across different distribution centers in the world, and each time series corresponds to a specific product at a specific distribution center. We use this approach to avoid having multiple data points at a single time point for a product, which could potentially result in the loss of information through aggregation.\nMost of the literature on time series forecasting methods focuses on extremely large datasets, which have been shown to be effective with deep learning techniques that can learn a general understanding of time series mechanisms. However, these approaches may not be applicable in smaller private settings such as ours. Our data consists of two types of products identified at level 1 and resulting in two separate datasets. The first dataset comprises 65 time series corresponding to type 1 products, which exhibit a general increasing trend. The second dataset comprises 50 time series corresponding to type 2 products with a general decreasing trend. Retail datasets often exhibit unpredictable patterns due to external forces that we may not be able to model. Additionally, time series can drop in and out at different time points due to new product introductions, adding to the complexity of forecasting. Our Transformer method employs learned embedding vectors to ensure that new time series can still generate predictions through their identifier features, along with self-attention which is applicable for variable length sequences and multi-task learning so the shared network is trained across the variety of series and this can work effectively on new series."}, {"title": "Retail Datasets", "content": "In addition to conducting experiments on the private dataset provided by a third-party company, we also evaluated the effectiveness of our custom Transformer and other benchmark models on two publicly available Walmart datasets. This allowed us to measure the performance of our model in comparison to other models on much larger datasets, providing insights from our custom attention mechanisms on a massive number of products.\nThe first dataset we used is the Store Sales forecasting dataset provided by the Walmart Recruiting team for a Kaggle competition. This dataset contains time series of sales for 98 departments at 45 different Walmart stores. While there is no information on specific products, we can consider each department as a separate product, giving us a total of 4,410 time series for potential input during training. One key difference between this dataset and the private one is that we do not have access to any department-specific features, only store-specific ones.\nThe second dataset we experimented with is the M5 dataset obtained from Walmart [57], which is even larger in size compared to the previous Walmart dataset. The data is separated into 3,049 different products sold by Walmart in the US at different stores, and includes aggregated series as well based on the category and department of the product and state location of the stores. This results in a total of 44,280 time series to model on. The significant increase in data makes it difficult to obtain good performance with sufficient epochs, but we present results on a small number of epochs across several algorithms."}, {"title": "4.2. Training Procedure & Metrics", "content": "To maximize the performance of the Inter-Series Transformer model, we conducted hyperparameter tuning on several key parameters such as the number of encoder / decoder layers, model dimension, embedding dimension, batch size, and the"}]}